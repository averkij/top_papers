{
    "paper_title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation",
    "authors": [
        "Xingyang Li",
        "Muyang Li",
        "Tianle Cai",
        "Haocheng Xi",
        "Shuo Yang",
        "Yujun Lin",
        "Lvmin Zhang",
        "Songlin Yang",
        "Jinbo Hu",
        "Kelly Peng",
        "Maneesh Agrawala",
        "Ion Stoica",
        "Kurt Keutzer",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $O(n \\log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $O(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\\times$ longer while reducing training costs by up to 4.4$\\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\\times$ compared to dense attention inference."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 2 5 8 9 1 . 6 0 5 2 : r Radial Attention: O(n log n) Sparse Attention with Energy Decay for Long Video Generation Xingyang Li Muyang Li Tianle Cai Haocheng Xi"
        },
        {
            "title": "First Intelligence",
            "content": "https://github.com/mit-han-lab/radial-attention Nothing spreads without loss; every signal, every influence, every attention Inspired by thermodynamic principles decays with distance. Figure 1: We present Radial Attention, sparse attention mechanism with O(n log n) computational complexity. Radial Attention accelerates pre-trained HunyuanVideo [1] by 1.9 at its default video length while maintaining comparable video quality. When generating 4 longer videos, it reduces tuning costs by up to 4.4 and speeds up inference by up to 3.7 versus dense attention."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, scalable sparse attention mechanism with O(n log n) complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard O(n2) dense attention and more expressive than linear attention. Specifically, Radial Attention employs simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size indicates equal contributions. Preprint. Under review. shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to 1.9 speedup over the original dense attention. With minimal tuning, it enables video generation up to 4 longer while reducing training costs by up to 4.4 compared to direct fine-tuning and accelerating inference by up to 3.7 compared to dense attention inference."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models have achieved remarkable success in generating high-quality images [2, 3]. Recent advances have extended their capabilities to video generation, producing visually compelling and temporally coherent results [4, 5, 6, 7, 1]. However, these improvements come at significant computational cost. Unlike image generation, video synthesis involves an additional temporal dimension, dramatically increasing the number of tokens to process. As self attention scales quadratically with sequence length, training and inference on long videos become prohibitively expensive, limiting model practicality and scalability. Figure 2: Radial Attention reduces the computational complexity of attention from O(n2) to O(n log n). When generating 500-frame 720p video with HunyuanVideo, it reduces the attention computation by 9, achieves 3.7 speedup, and saves 4.6 tuning costs. Several prior works have sought to mitigate this challenge using sparse attention. For instance, as illustrated in Figure 3(a), Sparse VideoGen (SVG) [8] employs an online profiling strategy that classifies each attention head as either spatial or temporal and then applies the corresponding sparse mask. While this approach can accelerate inference, it poses challenges during training, especially for longer videos. The profiling may misclassify attention heads on unseen data distributions, and such errors can be reinforced during optimization, leading to degraded performance. Other approaches replace the softmax attention with linear alternatives [9, 10]; however, these typically require substantial architectural modifications, where modest fine-tuning is typically insufficient to recover the original video quality. In physics, it is well known that signals and waves naturally experience energy decay as they propagate through space and time. Inspired by this principle, we observe similar phenomenon in attention: post-softmax attention scores between token pairs tend to diminish as their spatial or temporal distance increases (see Figure 4(b)). We term this phenomenon Spatiotemporal Energy Decay and model the decay as an exponential function of both spatial and temporal distances. Based on this model, we unify spatial and temporal attention heads in SVG [8] into Radial Attention, scalable sparse attention mechanism with O(n log n) computational complexity (see Figure 2). Radial Attention employs static sparse attention mask to translate the concept of energy decay into corresponding computation density decay. The mask design is simple yet effective: each token attends to others at similar spatial locations, while the attention window shrinks exponentially with temporal distance, as illustrated in Figure 3(b). Moreover, since Radial Attention only prunes unimportant token relations without modifying the underlying softmax attention mechanism, it enables efficient adaptation of pre-trained video diffusion models to longer sequences using lightweight fine-tuning, such as LoRA [11]. Compared to fullparameter fine-tuning with dense attention, it achieves better video quality, as LoRA focuses on updating parameters most critical for temporal coherence and visual fidelity. The length-extension LoRA is also compatible with existing style LoRAs (see Section 5.2). When generating videos at the default length, Radial Attention accelerates leading video diffusion models of Wan2.1-14B [7], HunyuanVideo [1] by up to 1.9 speedup. When generating 4 longer videos, Radial Attention reduces tuning costs by up to 4.4 and accelerates inference by up to 3.7 without sacrificing quality. Some visual examples on HunyuanVideo can be found in Figure 1. 2 Figure 3: Attention pipelines of SVG [8] and our Radial Attention. Softmax is omitted for clarity. (a) SVG dynamically selects either spatial or temporal attention for each head to speed up inference. However, it does not overcome the original models length limitation and cannot be trained on unseen distributions like longer videos. (b) Our Radial Attention uses static mask that unifies spatial and temporal attention with O(n log n) computational complexity. This static design enables efficient longer-video adaptation."
        },
        {
            "title": "2 Related Work",
            "content": "Video diffusion models. Diffusion models have achieved state-of-the-art (SOTA) results in image synthesis [2, 3, 12, 10]. Researchers further extend them to the video domain. Early approaches [13, 14, 15, 16] adapted 2D UNets [2, 17] to handle frame sequences by adding temporal modules. Ever since the advent of Sora [4], the community has largely shifted to use DiT [18] as the backbone. Latte [19] first proposed decoupled spatial and temporal attention for modeling video sequences. To better capture long-range dependencies and jointly model spatial-temporal dynamics, recent SOTA models have adopted 3D dense attention [20, 21, 5, 22, 1, 7, 6]. However, dense attention is computationally intensiveoften orders of magnitude more demanding than decoupled attentionand its cost scales quadratically with the number of frames, posing substantial challenges for both training and deployment. Efficient video generation. Many techniques developed to accelerate image diffusion modelssuch as timestep distillation [23, 24], caching [25, 26], quantization [27, 28, 29], and distributed inference [30, 31, 32]also apply to video diffusion. However, video models often rely on 3D dense attention, shifting the bottleneck from feedforward to attention layers. Recent works like SageAttention [33, 34, 35, 36] and FlashAttention-3 [37] show that quantizing attention can significantly speed up inference. In large language models (LLMs), sparse attention has been widely explored to reduce attention complexity [38, 39, 40, 41, 42, 43, 44, 45, 46]. For instance, Long LoRA [39] combines two local sparse attention patterns with shifting to achieve global receptive field in video understanding. PowerAttention [45] restricts attention to power-of-two token distances, yielding O(n log n) complexity. However, these methods ignore the inherent spatial and temporal structure in video data, making them suboptimal for video generation (see Section 5.2). To better exploit this structure, several video-specific sparse attention methods have been proposed [8, 47, 48, 49]. For example, STA [47] uses sliding 3D windows for local attention, and SVG dynamically selects spatiotemporal patterns for each head. Both improve efficiency but struggle with long videos: STAs fixed receptive field limits long-range dependencies, while SVGs runtime profiling becomes unreliable for unseen long video distributions. In contrast, our Radial Attention employs static O(n log n) pattern shared across all heads. This design accelerates both training and inference, allowing for efficient longer video extension. Long video generation. Due to the quadratic cost of dense attention, generating long videos remains resource-intensive, regarding both training and inference. RIFLEx [50] extends video length by modifying RoPE [51] frequencies to tackle temporal repetition and motion deceleration, allowing 2 extrapolation with the pre-trained models. However, it still suffers from poor video quality (e.g., blurring) when generating longer videos. Dalal et al. generate short video segments and stitch them together via test-time training layers [52]. Framepack [53] adopts an autoregressive strategy, generating short clips sequentially based on context frames that are encoded into fixed number of tokens. Other approaches replace dense attention with linear attention [10, 9, 54, 55, 56, 57, 58, 59], offering faster computation and global receptive fields. However, linear attention struggles to capture local details [60], often degrading quality. Our Radial Attention strikes middle ground between O(n2) dense attention and O(n) linear attention, achieving O(n log n) complexity while preserving the visual fidelity. Moreover, it can be efficiently fine-tuned from existing models using LoRA [11], enabling scalable longer-video generation with minimal overhead. 3 Attention with O(n log n) complexity. Preliminary efforts in this direction include Reformer [61], which approximates dense attention using locality-sensitive hashing to bucket similar keys/queries; H-Transformer [62], which imposes hierarchical structure on the attention matrix; Multi-resolution attention [63], which recursively refines attention where scores are large; and Fast Multipole Attention [64], which adapts the classical fast multipole method for hierarchical interactions. However, these methods are often hardware-unfriendly and demonstrate limited scalability in large-scale settings. In contrast, our method uses simple static attention mask that is hardware-friendly and scales efficiently while maintaining strong modeling capability."
        },
        {
            "title": "3 Preliminary",
            "content": "Diffusion models synthesize videos by sampling Gaussian noise XT (0, I) in latent space and progressively denoising it through neural network to produce clear latent X0, which is subsequently decoded into the final video using pre-trained decoder. Compared to images, videos introduce an additional temporal dimension, significantly increasing the number of latent tokens. For instance, generating 5-second 720p video in HunyuanVideo [1] requires approximately 115K tokens. Aggressively compressing the latent space often compromises video quality, thereby imposing lower bound on token reduction [3]. To capture spatiotemporal correlation in video generation, recent models [7, 1, 22, 5] use 3D dense attention, which computes interactions between all token pairs. Given tokens with embedding dimension d, attention is computed as: Attention(Q, K, ) = softmax (cid:19) (cid:18) QK , (1) where Q, K, Rnd are the query, key, and value matrices. The attention matrix QK has size n, leading to O(n2) time and memory complexity. While techniques like FlashAttention [65, 66] reduce memory complexity, the quadratic time complexity remains bottleneck, especially for long or high-resolution videos. As result, designing more efficient attention mechanisms is crucial for scaling video diffusion models. To mitigate this computational burden, sparse attention restricts interactions to subset of token pairs. Formally, this is achieved by adding sparsity mask {, 0}nn to the attention logits: SparseAttention(Q, K, ) = softmax (cid:18) QK + (cid:19) . (2) Entries set to are ignored in the softmax computation. Various schemes have been proposed to construct the mask. Static methods, such as STA [47], apply pre-defined sparsity pattern across all inputs. However, they often provide limited expressiveness. In contrast, dynamic schemes like SVG [8] adapt the sparsity pattern based on input content to improve fidelity. However, dynamic masking introduces overhead to determine the sparsity pattern on the fly and does not apply to training. Can we design static attention pattern that matches the expressiveness of dynamic methods and can also be used in training?"
        },
        {
            "title": "4 Method",
            "content": "The key insight of Radial Attention is that attention scores between tokens decay with increasing spatial and temporal distance. This motivates us to allocate computation based on the inherent spatiotemporal correlations in video data. In Section 4.1, we characterize the spatiotemporal energy decay phenomenon in attention. Then, in Section 4.2, we formally define Radial Attention, which translates energy decay into corresponding compute density reduction, enabling on-hardware speedup. We also analyze its complexity and approximation error, showing that the complexity is O(n log n) and our attention mask effectively captures essential information in the attention. Finally, in Section 4.3, we demonstrate how to extend pre-trained models to longer video sequences using Radial Attention."
        },
        {
            "title": "4.1 Spatiotemporal Energy Decay in Attention",
            "content": "In Figure 4(a), we show two post-softmax attention maps from HunyuanVideo [1]. Following the terminology in SVG [8], the left map is referred to as spatial attention, where each token primarily 4 Figure 4: (a) Example spatial and temporal attention maps from HunyuanVideo (defined in Section 4.1). (b) Attention score distributions. (b1): Average score between tokens at the same spatial location decreases with temporal distance (b2): Average attention score within frame decreases with spatial distance. Spatial and Temporal Attention refer to the distributions derived from the corresponding maps in (a). Average means averaging over multiple random maps and diffusion steps. The plots indicate that spatial attention shows high temporal decay and relatively low spatial decay, while temporal attention exhibits the opposite. attends to nearby tokens within the same frame or adjacent frames. The right map represents temporal attention, where each token focuses on tokens at the same spatial location across different frames. Figure 4(b) illustrates the attention score distributions of these two maps, along with third curve of averaged attention scores over multiple heads and diffusion steps. In Figure 4(b1), we show the average attention score between tokens at the same spatial location but with increasing temporal distance. In Figure 4(b2), we show the average score between tokens in the same frame as spatial distance increases. In both cases, attention scores exhibit clear decay pattern with increasing distance between the query and key tokens. We refer to this phenomenon as Spatiotemporal Energy Decay. Moreover, regression analysis suggests that this decay closely follows an exponential distribution (see Section 5.3). Specifically, following the notation from Section 3, assume the video latent consists of frames, each containing tokens (in total = tokens). Consider query token located at the k0-th spatial position of the i0-th frame. The corresponding attention score after applying softmax, denoted by [0, 1]n, is given by = softmax(Qi0s+k0K). Then there exist constants α, β > 0 and Crel > 0, for each key token at spatial position in frame satisfying pjs+l Creleαji0βlk0pi0s+k0 . (3) Parameters α and β control temporal and spatial decay, respectively. High β (strong spatial locality) and low α model temporal attention, while high α and low β capture spatial attention, as shown in the empirical plots in Figure 4(b). This motivates our unified sparsity pattern that leverages both spatial and temporal decay in principled manner."
        },
        {
            "title": "4.2 Radial Attention: Convert the Energy Decay to Compute Density Decay",
            "content": "Radial Attention simulates energy decay through compute density decay to save computation. Temporal density decay. Along the temporal dimension, Radial Attention applies simple the compute density between tokens in frame and frame is set to exponential decay rule: ( 1 2 )log2(max(ij,1)). This forms structured pattern as illustrated in Figure 5(a), where the attention map is divided into 2log2(max(f, 2)) 1 diagonal bands centered on the main diagonal (band 0). Bands above and below the diagonal are indexed as 1, 2, 3, . . . and 1, 2, 3, . . ., respectively. Each bands width doubles relative to the previous one, ensuring that the total computation per band remains bounded by constant. The attention from tokens in frame to frame lies in band sign(j i) log2 max(i j, 1). The central band (band 0) retains full compute density (100%), while each successive band moving outward has half the compute density of the preceding one producing radial decay effect with progressively lighter colors. Spatial density decay. As observed in Figure 4 and formalized in Equation 3, most attention energy is concentrated on tokens at similar spatial locations across frames. We preserve these high-energy interactions, which yield diagonal-like structures within each frame-to-frame attention block. Due to temporal decay, the computed diagonal width of these blocks shrinks as the temporal distance between frames increases. Specifically, as shown in Figure 5(b), the diagonal width for attention between frame . If it falls below 1, instead of further narrowing the and frame is given by 2log2 max(ij,1) 5 Figure 5: (a) The compute density pattern. The attention map is divided into 2log2(max(f, 2)) 1 bands (here, the number of frames = 12) based on the temporal distance between tokens. The central band has full compute density, while each successive outer band has half the density of the previous one. Except for band 1, each band also doubles the diagonal width of its predecessor. (b) The corresponding attention mask for (a). The compute density is reflected in the compute diagonal width of each frame-to-frame block. When the diagonal width drops below 1, we reduce the frequency of diagonals. We additionally add an attention sink. (c) An example mask used in HunyuanVideo, illustrating the final sparsity pattern in practice. diagonal, we reduce the frequency of diagonals. Specifically, we only retain diagonals in those blocks where j mod 2log2 max(ij,1) = 0 to keep the same amortized attention density decay. Formal definition. The attention mask used in Radial Attention is defined as follows. We construct 4D attention mask {, 0}f ss, where each element Mi,j,k,l = 0 indicates that the token at spatial position in frame is permitted to attend to the token at position in frame j. Conversely, Mi,j,k,l = denotes that attention between the token pair is suppressed. The mask is constructed according to: Mi,j,k,l = 0, 0, . otherwise if 2log2 max(ij,1) and l + 1 if j mod 2log2 max(ij,1) = 0 and = 2log2 max(ij,1) (4) The final attention mask {, 0}nn used in the attention operation of Equation 2 is obtained by flattening frame and spatial indices: Mis+k,js+l = Mi,j,k,l. Figure 5(c) shows detailed example mask we use in HunyuanVideo for generating 253-frame 720p video. This strategy preserves spatial interactions with high temporal proximity while employing sparse sampling for distant frames to maintain efficiency. To further enhance video quality, we also incorporate an attention sink [38, 8], ensuring that every token attends to tokens in the first frame. Relation to SVG. Radial Attention unifies spatial and temporal attention in SVG [8] using single attention mask. Specifically, the central band (band 0 in Figure 5(a)) of our mask already captures dense spatial interactions, effectively subsuming the spatial attention in SVG. For temporal attention, SVG overlooks temporal decay, allocating unnecessary computation to distant frames with limited relevance. In contrast, Radial Attention reduces attention to these peripheral regions and reallocates the budget toward tokens nearer in time, achieving both improved efficiency and enhanced modeling of local temporal dynamics. Complexity analysis. The computational cost of our method is proportional to the number of zeros in the attention mask . When the number of frames is large, we derive the following upper bound: #zeros in 4s2f (cid:124) (cid:123)(cid:122) (cid:125) central band and sink + log2 (cid:88) 2r+1f r= (cid:124) (cid:123)(cid:122) diagonal width1 2s2 2r (cid:125) log2 1 (cid:88) + 2log2 s+1f (5) r=log2 s+1 (cid:124) (cid:123)(cid:122) diagonal width<1 (cid:125) 4s2f log2 = 4sn(log2 log2 s). (6) detailed derivation of Equation 5 can be found in Appendix A.1. From Equation 6, we observe that for long videos (i.e., large ) with fixed spatial resolution s, the total computational complexity scales 6 Table 1: Quantitative results at the default video length. Under the same computation budget, our method consistently outperforms STA and PA in PSNR, SSIM, and LPIPS, matches the video fidelity of SVG, and achieves 1.8 speedup on HunyuanVideo and Wan2.1-14B on single H100 GPU. Model Method PSNR () SSIM () LPIPS () Vision Reward () TFLOPs Latency (s) Speedup Hunyuan Video (117 frames) Wan2.1-14B (69 frames) Original STA (FA3) PA SVG Ours Original STA (FA3) PA SVG Ours 26.7 22.1 27.2 27.3 22.9 22.4 23.2 23.9 0.866 0.764 0.895 0.886 0.830 0.790 0.825 0.842 0.167 0.256 0.114 0.114 0.171 0.176 0.202 0. 0.141 0.132 0.140 0.144 0.139 0.136 0.132 0.126 0.114 0.128 612 331 339 340 339 560 322 324 324 323 1649 719 1002 867 876 1630 812 978 949 2.29 1.65 1.90 1.88 2.01 1.67 1.71 1.77 as O(n log n). Empirical results on HunyuanVideo, shown in Figure 2, confirm this trend. Notably, when generating 509-frame 720p video, our Radial Attention reduces attention computation by 9 compared to dense attention. Error analysis. Following Equation 3, we derive an error bound for the attention score corresponding to query token located at position k0 in the i0-th frame. Let = softmax(Qi0s+k0 + Mi0s+k0) denote the masked attention score. The ℓ1 error between the exact and approximated attention scores is bounded as follows: (cid:13)p p(cid:13) (cid:13) (cid:13)1 Crel (cid:34) 2 +1(cid:1) 8 eβ(cid:0) (1 eα)(1 eβ) + 4 1 + eβ 1 eβ eα(s+1) 1 eα (cid:35) = O(Crele min(β/2,α)s). (7) Proof details are provided in Appendix A.2. As Equation 7 shows, the error decreases exponentially with larger decay rates α and β. In Section 5.3, we further empirically compare this error bound to that of SVG, showing that Radial Attention achieves smaller errors, thereby validating its effectiveness. Hardware-friendly block sparsity. To ensure efficient execution on modern hardware, attention is computed over blocks rather than individual 1 1 tokens [67, 8, 40, 43, 44, 65]. Our implementation adopts block size of 128 128."
        },
        {
            "title": "4.3 Low-Rank Adaptation for Long Videos",
            "content": "Although we employ an efficient attention mechanism, the pre-trained model was originally trained on short videos. Recent works [50] have explored training-free methods for extending generation to longer videos, but their performance remains limited due to length distribution mismatch. Training directly on long videos, meanwhile, is computationally prohibitive in both time and memory. Radial Attention alleviates this challenge by reducing the training time complexity to O(n log n). Importantly, it preserves critical inter-token relations in the softmax attention, allowing the original pre-trained weights to remain largely intact. As result, only minimal fine-tuning is required. Therefore, to further minimize training overhead, we incorporate low-rank adapters (LoRA) [11, 39] into the attention mechanism. Specifically, LoRA is applied to the query, key, value, and output projections of the attention layers, enabling efficient fine-tuning with significantly reduced memory and computational costs. Empirically, we find that LoRA fine-tuning with Radial Attention not only minimizes overhead but also improves video quality by refining only the most critical weights and concentrating attention more effectively. See Section 5.3 for detailed results."
        },
        {
            "title": "5.1 Setups",
            "content": "Models. We benchmark our method on three popular text-to-video diffusion models: Mochi 1 [22], HunyuanVideo [1], and Wan2.1 [7], which contain 10, 13, and 14 billion parameters, respectively. Mochi 1 can generate up to 5-second video with 480p resolution and 162 frames. HunyuanVideo can generate up to 5-second video with 720p resolution and 125 frames. Wan2.1-14B can generate up to 5-second video with 720p and 81 frames. 7 Table 2: Quantitative results at the extended (2 and 4) video lengths. With minimal fine-tuning, our method maintains the quality regarding Vision Reward and multiple VBench dimensions (Subject Consistency, Aesthetic Quality, and Image Quality) when the length grows. It also achieves high sparsity, reducing training costs by up to 4.4 and delivering up to 3.7 inference speedup. Model #Frames Method Sparsity Training Time (h) Training Speedup Inference Time (s) Inference Speedup Vision Reward () VBench S.C. A.Q. I.Q. 125 (1) Original 253 (2) Hunyuan Video 509 (4) Original RIFLEx Spatial Temporal Long LoRA PA [45] SANA Full Ours Original RIFLEx Spatial Temporal Long LoRA PA [45] Full Ours 163 (1) Original 331 (2) Mochi 1 667 (4) Original Spatial Temporal Long LoRA PA [45] SANA Full Ours Original Spatial Temporal Long LoRA PA [45] Full Ours 81 (1) Original Wan2.1 -14B 161 (2) Original Full Ours 0.00% 0.00% 0.00% 80.5% 80.7% 80.6% 80.4% 0.00% 80.8% 0.00% 0.00% 88.3% 88.2% 88.4% 88.2% 0.00% 88.3% 0.00% 0.00% 76.1% 76.3% 76.0% 77.8% 0.00% 76.4% 0.00% 85.2% 85.4% 86.0% 86.5% 0.00% 85.5% 0.00% 0.00% 0.00% 73.6% 16.0 16.2 16.6 16.7 12.8 45.0 16.2 20.7 21.1 20.9 21.8 93.6 21.4 8.57 8.54 9.07 8.53 8.22 15.0 8. 17.4 17.6 19.0 17.3 49.2 17.4 28.0 14.5 2.81 2.78 2.71 2.69 3.52 1.00 2.78 4.52 4.44 4.48 4.29 1.00 4.37 1.75 1.76 1.65 1.76 1.82 1.00 1.78 2.83 2.80 2.59 2.84 1.00 2.83 1.00 1.93 797 797 335 338 363 334 285 797 339 2895 2895 755 774 803 766 2895 781 112 302 186 189 210 183 166 302 185 992 382 393 426 381 992 386 5735 5735 2847 0.119 0.959 0.643 0. 1.00 1.00 2.38 2.36 2.20 2.39 2.80 1.00 2.35 1.00 1.00 3.83 3.74 3.61 3.78 1.00 3.71 1.00 1.62 1.60 1.44 1.65 1.82 1.00 1.63 1.00 2.60 2.52 2.33 2.60 1.00 2.57 1.00 1.00 2.01 0.122 0.128 0.054 0.104 0.112 0.109 -0.205 0.124 0.126 0.054 0.037 0.112 0.083 0.130 0.128 0.133 0.134 0.071 0.040 0.088 0.075 0.095 0.101 -0.201 0.095 0.110 -0.091 0.091 0.028 0.086 0.107 0.099 0. 0.135 0.109 0.150 0.145 0.953 0.969 0.979 0.963 0.958 0.967 0.907 0.955 0.968 0.988 0.989 0.922 0.972 0.936 0.950 0.977 0.973 0.603 0.622 0.607 0.620 0.620 0.608 0.300 0.616 0.623 0.545 0.539 0.598 0.597 0.618 0.590 0.590 0. 0.611 0.614 0.670 0.658 0.685 0.653 0.442 0.648 0.663 0.451 0.456 0.664 0.646 0.689 0.648 0.635 0.672 0.973 0.623 0.672 0.937 0.935 0.936 0.950 0.946 0.905 0.923 0. 0.916 0.930 0.931 0.944 0.956 0.934 0.958 0.551 0.596 0.591 0.596 0.610 0.334 0.610 0.615 0.383 0.611 0.556 0.584 0.633 0.613 0.618 0.466 0.595 0.593 0.630 0.626 0.568 0.594 0.602 0.322 0.585 0.536 0.543 0.650 0.613 0.638 0. 0.623 0.672 0.946 0.966 0.981 0.598 0.590 0.607 0.614 0.689 0.677 Benchmarks. We use Vision Reward [68] (higher is better) to approximate the human rating of the generated videos. For pre-trained models evaluated at their default video lengths, we further report PSNR and SSIM to quantify numerical similarity, and LPIPS [69] to assess perceptual differences between the outputs of the original models and the benchmarked methods. For longer-video generation, we additionally use VBench-long[70] to evaluate our fine-tuned models. Specifically, we report metrics of subject consistency, aesthetic quality, and imaging quality, where the original models exhibit notable degradation. Baselines. We compare Radial Attention against the following methods: SVG [8]: Accelerates video models with sparse attention by dynamically classifying attention heads as spatial or temporal and applying corresponding masks. Spatial/Temporal: The respective attention masks used in SVGs spatial and temporal heads, as described in Section 4.1. STA [47]: Applies sliding window attention to capture spatially and temporally local dependencies. PowerAttention (PA) [45]: sparse attention mechanism with O(n log n) complexity for LLMs, attending only to tokens at power-of-two distances. LongLoRA [39]: Uses shifted local attention to efficiently extend the context window of LLMs. SANA[10]: An efficient diffusion model backbone with linear attention. We replace softmax attention with SANAs for adapting to longer videos. 8 Figure 6: Examples of videos generated by Radial Attention and the original Wan2.1-14B in the default video length. Radial Attention mirrors the video quality of the original model. RIFLEx[50]: Training-free video length extrapolation by adjusting the frequency of RoPE [51]. Implementation details. For default-length inference, we evaluate HunyuanVideo on 117 frames at 768p resolution (7681280), and Wan2.1 on 69 frames at the same resolution. Following SVG, we apply dense attention during the first 12 steps as warm-up phase for all models. Additionally, we keep dense attention in the first DiT block to maintain quality for Wan2.1-14B. Unless otherwise specified, we use FlashAttention-2 [66] as the default attention backend. The latency measurements are conducted on single NVIDIA H100 GPU. For longer-video generation, we fine-tune the model with videos that are 24 longer than the default length from OpenVid-1M [71]. Specifically, we sample 2k top-scoring videos in aesthetic and motion scores for each extended length. We use 8 H100 GPUs for training, which takes around 1621 hours for HunyuanVideo, 817 hours for Mochi 1, and 15 hours for Wan 2.1. Inference latency for Wan 2.1 is measured on single H100, while HunyuanVideo and Mochi 1 are evaluated using 8 H100s. We use FlashInfer [72] for inference and Block-Sparse Attention [73] during training. See Appendix for more details."
        },
        {
            "title": "5.2 Main Results",
            "content": "Training-free inference acceleration. Table 1 presents quantitative comparison of Radial Attention against three strong sparse attention baselines on HunyuanVideo [1] and Wan2.1-14B [7] at their default generation lengths. Corresponding visual results are provided in Figure 6 and Appendix C.1. Under the same compute budget (measured in TFLOPs), Radial Attention preserves the video quality of dense attention while consistently outperforming STA and PA on similarity metrics (PSNR, SSIM, LPIPS), and matches the quality of SVG. While PA shares similar O(n log n) complexity with our design, it ignores the spatio-temporal locality inherent in video data, making it suboptimal in practice. Regarding efficiency, we adopt the same system optimization used in SVG [8]. Specifically, on single H100, our Radial Attention achieves 1.9 and 1.8 end-to-end speedups for HunyuanVideo and Wan 2.1, respectively, matching the theoretical compute budget savings (1.8 and 1.7 fewer TFLOPs). Although STA yields slightly higher speedup by using FlashAttention-3 (FA-3) [37], it suffers from noticeably degraded visual quality. Our current implementation uses FA2 [66]. Upgrading to FA3 is orthogonal to our algorithm and is left as future work. Long video generation. Table 2 provides results for video generation at 2 and 4 the original lengths, with visualizations available in Figure 7 and Appendix C.2. For Wan2.1-14B, only 2 extrapolation is reported due to its significantly higher computational and memory costs. To ensure fairness, all sparse attention baselines use similar sparsity ratios. When generating longer videos, the original models without further tuning exhibit significant quality degradation, especially for 4 video length extension. While RIFLEx improves performance at 2 length extrapolation, its quality deteriorates beyond that, indicating limited extension capability. Spatial and temporal sparse attentions suffer from limited reception fields; on the other hand, LongLoRA and PA, though with global reception field, fail to capture spatial-temporal correlations, Interestingly, PA exhibits large gain in Vision Reward after resulting in degraded quality. fine-tuning, suggesting that its original sparse pattern misaligns with the pre-trained attention distribution. Fine-tuning allows the model to adapt to the imposed attention sparsity, improving alignment and quality. SANA, which replaces softmax attention with linear attention, requires 9 Figure 7: Visual comparison of HunyuanVideo with 4 length extension (509 frames). LoRA fine-tuned models using Radial Attention achieve higher vision rewards, outperforming dense attention baselines while providing significant speedups. Figure 8: Radial Attention LoRA is compatible with existing style LoRAs. On HunyuanVideo, it extends video length by 4 while maintaining vision reward comparable to that of the original-length LoRA video. massive retraining and fails under fine-tuning-based video length extension. In contrast, Radial Attention achieves quality on par with LoRA fine-tuned dense attention models. Notably, it even slightly improves the Vision Reward over the pre-trained model at the default video length. Thanks to O(n log n) complexity, Radial Attention delivers substantial inference and training speedups over the original dense attention, as detailed in Table 2 and Figure 2. For instance, when generating 4 longer videos, we can save up to 4.4 training costs and achieve up to 3.7 inference speedup. Compatibility with existing LoRAs. key advantage of Radial Attention is its seamless compatibility with pre-trained task-specific LoRAs (e.g., artistic style transfer). As illustrated in Figure 8, combining our extended-length LoRA with existing style LoRAs preserves visual quality while enabling longer-video generation. We observe that the content style generated by the merged LoRA exhibits subtle differences from the original LoRAs. This discrepancy is primarily attributed to the relatively small dataset used for training the extended-length LoRA, which may introduce slight style bias that interacts with the style LoRA. We expect that training the length-extension LoRA on more comprehensive dataset would help mitigate this issue."
        },
        {
            "title": "5.3 Ablation Study & Analyses",
            "content": "Effectiveness Low-Rank Adaptation. Figure 9(a) compares Vision Reward between full fine-tuning and LoRA as video length increases. For dense attention, LoRA fine-tuning generally lags behind full fine-tuning until 4 length extension. However, with our proposed Radial Attention, LoRA finetuning matches or even outperforms full fine-tuning, suggesting that Radial Attention not only scales better computationally, but also makes the model easier to fine-tune for longer-video generation. Attention error. We evaluate the average attention output Mean Squared Error (MSE, lower is better) of Radial Attention on Wan2.1-14B, comparing it to SVG [8] and STA [47]. Radial Attention achieves an MSE of 3.9 103, significantly lower than 4.4 103 for SVG and 1.5 102 for STA, demonstrating the effectiveness of our mask in preserving attention fidelity. Regression Results. We perform regression analysis using the model = exp(ax + b) to fit the average attention decay curves in Figure 4. As illustrated in Figure 9, the fitted curves achieve an R2 value of over 0.985, indicating that the exponential functions can well model the decay."
        },
        {
            "title": "6 Conclusion & Discussion",
            "content": "In this work, we propose Radial Attention, an O(n log n) sparse attention mechanism for efficient long video generation. We observe Spatiotemporal Energy Decay in video diffusion models, which motivates unified attention pattern with sub-quadratic complexity. At default video lengths, Radial Attention achieves up to 1.9 speedup while maintaining video quality. For videos up to 4 longer, Radial Attention preserves video fidelity and delivers up to 4.4 and 3.7 speedups in training and inference, respectively, with minimal LoRA fine-tuning. This work contributes toward scalable, 10 Figure 9: (a) Radial Attention outperforms dense attention in generation quality. When combined with LoRA, it further improve the quality while significantly reducing training costs. (b) We model the attention decay curves using the exponential function = exp (ax + b). It fits the data well, achieving R2 > 0.985. high-quality video generation and offers foundation for efficient long-range attention in broader sequence modeling tasks. Limitations. The assumption of exponential decay for attention scores (Equation 3) simplifies the complex spatiotemporal dependencies in natural video data. While aiding theoretical analysis, future work could improve efficiency and performance by more deeply understanding and modeling the underlying data structure. As shown in Equation 6, our method still has quadratic complexity with respect to resolution. Future work will explore more scalable attention mechanisms for high-resolution video generation. Currently, we use Radial Attention only for fine-tuning to extend video length, but it could also be used for pre-training to natively support long videos, similar to NSA [74] and MoBA [75]. We leave this exploration to future work."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank MIT-IBM Watson AI Lab, National Science Foundation, Hyundai, and Amazon for supporting this research."
        },
        {
            "title": "A Derivations and Proofs",
            "content": "A.1 Complexity In this section, we provide the detailed complexity derivation in Equation 5, which scales as O(n log n). Since the computational cost of masked attention is proportional to the number of zeros in the attention mask , we only need to derive an O(n log n) upper bound for the latter. Central band&attention sink. Firstly, recall from Figure 5(a) that we apply dense attention on these frame-to-frame attention blocks within the central band and attention sink. The attention sink refers to the pattern that every token attends to all tokens in the first frame. Using the same notation as in Section 4, where is the total number of tokens, is the number of tokens per frame, and is the number of frames (so = s), we define the attention mask for this region as (1) {, 0}f ss: (1) i,j,k,l = (cid:26)0, if j 1 or = . otherwise (8) Here, (1) i,j,k,l indicates whether the k-th token in frame is allowed to attend to the l-th token in frame j, with 0 denoting allowed attention and indicating disallowed attention. Since the attention sink spans blocks and the central band includes at most 3f blocks, the total number of nonzero entries in this region is bounded by: #zeros in (1) 4 s2 = 4s2f. (9) Bands with diagonal width 1. The second part is those bands with diagonal width 1, except the central band. The mask for this region can be defined as (2) {, 0}f ss: (2) i,j,k,l = (cid:26)0, if 2log2 max(ij,1) and l + 1 . otherwise 2log2 max(ij,1) (10) Thus, since there are at most log2 bands in this region, the number of zeros in these bands is bounded by: #zeros in (2) log2 (cid:88) r=1 2r+1sn (cid:124) (cid:123)(cid:122) (cid:125) area bound for band 2/2r (cid:124)(cid:123)(cid:122)(cid:125) compute density bound of band log2 (cid:88) r=1 2r+2s2f 2r = 4s2f log2 s. (11) (12) (13) Bands with diagonal width < 1. The last part is those bands with 2 log2 max(ij,1) < 1, where we reduce the frequency of diagonals. The mask for this region (3) {, 0}f ss is given by: (cid:40) (3) i,j,k,l = if j mod 2log2 max(ij,1) 0, . otherwise = 0 and = (14) Since there are at most (log2 1) (log2 + 1) bands satisfying this condition, we have the number of zeros in these bands bounded by: #zeros in (3) log2 1 (cid:88) 2log2 s+1 (cid:125) (cid:123)(cid:122) (cid:124) number of diagonals (log2 log2 s)4s2f. r=log2 s+1 (cid:124)(cid:123)(cid:122)(cid:125) area bound of each diagonal (15) (16) Combining Equation 9, Equation 13, and Equation 16 together, we have the aggregate upper bound of the number of zeros in Radial Attentions mask: # of zeros in 4s2f log2 4s n(log2 log2 s), which scales O(n log n) with the number of frames for long video generation. (17) A.2 Error Bound The design of Radial Attention is inspired by the spatial-temporal structure in video. In this section, we formulate this intuition by theoretically bounding the asymptotic approximation error of Radial Attention with respect to the decay speed of the attention value in the spatial and temporal dimensions. We focus on bounding the approximation error of single row of the attention matrix. We fix reference query token at position k0 of frame i0, and write the unnormalized row of the attention matrix as aj,l = exp(Qi0s+k0 js+l). where Qi0s+k0 refers to the query vector at position k0 in frame i0, Kjs+l refers to the key vector at position in frame j, and refers to the number of tokens per frame."
        },
        {
            "title": "Assumptions",
            "content": "(A1) Relative exponential decay. To capture the intuition that the closer frames have stronger correlation and each token typically attends to tokens in other frames at similar spatial positions, we assume there exist Crel > 0 and (α, β) > 0 such that 0 aj,l Crel eαji0βlk0 a0, a0 := ai0,k0 > 0. where α characterizes the temporal decay rate and β characterizes the spatial decay rate. (A2) Infinite temporal grid & finite spatial grid. To conduct asymptotic analysis, we let (temporal) but keep {1, . . . , s} (spatial). Extending to only enlarges the sums we bound."
        },
        {
            "title": "Notation",
            "content": "Z := (cid:88) (cid:88) jZ l=1 aj,l, Zkeep := (cid:88) aj,l, Zout := Zkeep. (j,l) : Mi0,j,k0,l=0 Exact and masked softmax rows: pj,l = aj,l/Z, pj,l = aj,l1{ =0}/Zkeep. The total variation error can be calculated as follows by standard algebraic argument, Because a0 itself is in the sum, a0. Hence (cid:13) (cid:13)p p(cid:13) (cid:13)1 ="
        },
        {
            "title": "Zout\nZ",
            "content": "."
        },
        {
            "title": "Zout\nZ",
            "content": "Zout a0 . (1) (2) Mask geometry For temporal offset := i0 0, define the bandwidth w(t) := 2log2 max(t,1) {1, 2, 4, . . . , s}. The mask keeps spatial index iff k0 w(t) and the frame is one of the sub-sampled frames; otherwise Mi0,j,k0,l = . Two kinds of approximation errors, therefore, appear: (i) Spatial tails inside kept frames For each t, the discarded spatial part satisfies (cid:88) eβd d>w(t) eβ(w(t)+1) 1 eβ . Because w(t) 2 when s, T1 := 2Crela0 (cid:88) eαt (cid:88) eβd t0 d>w(t) 4Crela0 (1 eα)(1 eβ) eβ(cid:0) 2 +1(cid:1) . (3) (ii) Frames skipped by the subsampling rule For > s, only every K(t) = (cid:6)2log2 t/s(cid:7) frame is kept; the remainder contributes T2 := 2Crela0 1 + eβ 1 eβ (cid:88) t>s eαt 2Crela0 (1 + eβ) (1 eβ)(1 eα) eα(s+1). (4) Total variation error Combine all equations above: (cid:13)p p(cid:13) (cid:13) (cid:13)1 Crel (cid:34) 2 +1(cid:1) 8 eβ(cid:0) (1 eα)(1 eβ) + 4 1 + eβ 1 eβ eα(s+1) 1 eα (cid:35) = O(Crele min{β/2,α}s) . This characterizes how the decay rates affect the approximation error."
        },
        {
            "title": "B Additional Implementation Details",
            "content": "In terms of our LoRA fine-tuning for longer-video generation, we fine-tune HunyuanVideo [1] and Mochi 1 [22] at global batch size of 1 with sequence parallelism, and train Wan 2.1 [7] with global batch size of 8. All tuning experiments are conducted on 8 H100 GPUs. During training, we keep the first two DiT blocks with dense attention. Since there are 60, 48, and 40 blocks for HunyuanVideo, Mochi 1, and Wan2.1-14B, this only incurs negligible overhead. We train HunyuanVideo for 2 and 4 length video generation for 2400 and 1200 steps, respectively. We train Mochi 1 for 5000 steps for both 2 and 4 length video generation. We train Wan2.1-14B for 2500 steps for 2 length video generation. The LoRA rank is 128 for all training tasks."
        },
        {
            "title": "C Visualization of the generated videos",
            "content": "In this section we compare Radial Attention against various baselines in video quality, and list our speedup in both training and inference. For the complete video comparisons, see our supplementary materials and this link. C.1 Default Video Length We provide visual comparison between the original dense attention, STA [47], and our Radial Attention on HunyuanVideo [1] and Wan2.1-14B [7]. We conduct experiments under 768p, 117 frames settings for HunyuanVideo, and 768p, 69 frames settings for Wan2.1-14B. As shown in Figure and Figure B, Radial Attention has higher PSNR compared to STA [47], effectively maintaining the high fidelity of the original videos. C.2 Longer-video Length We provide visual comparison between the aforementioned baselines and Radial Attention on HunyuanVideo [1], Mochi 1 [22], and Wan2.1-14B [7]. We conduct experiments under the default resolution settings, which are 720p for HunyuanVideo and Wan2.1-14B, and 480p for Mochi 1. Table A: We compare Radial Attention against another O(n log n) attention baseline, Harmonic Series (HS). Radial Attention consistently outperforms it across all metrics. Model Method PSNR () SSIM () LPIPS () VisionReward () HunyuanVideo (117 frames) HS Ours 27.0 27.3 0.881 0.886 0.119 0.114 0.136 0.139 Moreover, we generate videos at 4 longer length for HunyuanVideo (21 seconds, 509 frames) and Mochi 1(22 seconds, 667 frames), and 2 longer length for Wan2.1-14B (10 seconds, 161 frames). We use Vision Reward [68] to evaluate the generated videos. Figure C, Figure D, and Figure demonstrate that Radial Attention achieves the highest average Vision Reward score compared to the baselines, well preserving the video quality even at longer-video settings. C.3 Comparison with Other O(n log n) Sparsity Patterns We additionally conduct an ablation study to validate the effectiveness of the sparsity pattern in our proposed O(n log n) attention mask. Specifically, we compare Radial Attention with the Harmonic Series Decay Attention (HS), which features computed diagonal width inversely proportional to its distance from the main diagonal. Table presents quantitative results comparing Radial Attention with HS, demonstrating the superiority of Radial Attention."
        },
        {
            "title": "D Broader Impacts",
            "content": "Radial Attention significantly reduces computational costs for video diffusion models, enabling longer-video generation with minimal fine-tuning efforts while maintaining quality. This paves the way for high-quality video creation tools for education and creative arts. Since Radial Attention accelerates self-attention to O(n log n) complexity, it can accelerate video diffusion models and decrease energy consumption, leading to greener AI applications. This also helps the popularization of generative models. However, malicious users can misuse our method to create deepfakes and spread misinformation. The technology may also exacerbate the digital divide between those with and without access to the minimal necessary computational resources. To address these concerns, we advocate for responsible deployment, adherence to ethical standards, and the development of effective detection methods. We encourage the research community to continue advancing both efficient generation techniques and safeguards to ensure these powerful tools benefit society while minimizing potential harms. We will explicitly specify the usage permission of our code and models with proper licenses."
        },
        {
            "title": "E License",
            "content": "Here, we show all the licenses for our used assets. Wan 2.1 [7], Mochi 1 [22], Diffusers, and STA [47] are under Apache-2.0 license. The license of HunyuanVideo [1] is here. SVG [8] and OpenVid-1M do not have an explicit license. 15 Figure A: Comparison of Dense Attention and Radial Attention on HunyuanVideo Text-to-Video generation at the default length (5 seconds, 117 frames, 768p). 16 Figure B: Comparison of Dense Attention and Radial Attention on Wan2.1-14B Text-to-Video generation at the default length (4 seconds, 69 frames, 768p). Figure C: Comparison of all baselines and Radial Attention at 4 default length (21 seconds, 509 frames) Text-to-Video video generation from HunyuanVideo. Radial Attention achieves the best Vision Reward score with good visual quality and consistency. In contrast, Original HunyuanVideo and RIFLEx generate blurred videos with poor visual quality. Temporal Head generates distorting figures. Spatial Head, Long LoRA, and PowerAttention generate temporally inconsistent video backgrounds. Dense Attention generates less dynamic videos. 18 Figure D: Comparison of all baselines and Radial Attention at 4 default length (22 seconds, 667 frames) Text-to-Video video generation from Mochi 1. Radial Attention achieves the highest Vision Reward score because it has excellent visual quality and consistency. In contrast, Original Mochi 1 generates blurred videos with poor visual quality. Spatial Head, Temporal Head, Long LoRA, PowerAttention, and Dense Attention generate videos with either inconsistent backgrounds or inconsistent figures. 19 Figure E: Comparison of all baselines and Radial Attention at 2 default length (10 seconds, 161 frames) Text-to-Video video generation from Wan2.1-14B. Radial Attention achieves the highest Vision Reward score because it has excellent visual quality and consistency. In contrast, Original Wan2.1-14B generates blurred videos with poor visual quality. Dense Attention generates videos with inconsistent figures."
        },
        {
            "title": "References",
            "content": "[1] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2, 3, 4, 7, 9, 14, 15 [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 3 [3] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 4 [4] OpenAI. Sora: Video generation models as world simulators, 2024. 2, 3 [5] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, [6] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 2, 3 [7] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 3, 4, 7, 9, 14, 15 [8] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. In ICML, 2025. 2, 3, 4, 6, 7, 8, 9, 10, 15 [9] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity. In CVPR, 2025. 2, 3 [10] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. ICLR, 2025. 2, 3, 8 [11] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 2, 3, 7 [12] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 3 [13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 2022. 3 [14] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 3 [15] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In ICLR, 2024. 3 [17] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234241. Springer, 2015. 3 21 [18] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 3 [19] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [20] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. [21] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in $200k. arXiv preprint arXiv:2503.09642, 2025. 3 [22] Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. 3, 4, 7, 14, 15 [23] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. CVPR, 2025. 3 [24] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. In NeurIPS, 2024. 3 [25] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In CVPR, 2024. 3 [26] Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In CVPR, 2025. 3 [27] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models. In ICCV, 2023. 3 [28] Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, et al. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. In ICLR, 2025. 3 [29] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Junxian Guo, Xiuyu Li, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank component for 4-bit diffusion models. In ICLR, 2025. [30] Muyang Li, Tianle Cai, Jiaxin Cao, Qinsheng Zhang, Han Cai, Junjie Bai, Yangqing Jia, Ming-Yu Liu, Kai Li, and Song Han. Distrifusion: Distributed parallel inference for high-resolution diffusion models. In CVPR, 2024. 3 [31] Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, and Xibo Sun. Pipefusion: Patch-level pipeline parallelism for diffusion transformers inference. arXiv preprint arXiv:2405.14430, 2024. 3 [32] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. 3 [33] Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In ICLR, 2025. 3 [34] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In ICML, 2025. 3 [35] Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, and Jianfei Chen. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025. 3 [36] Jintao Zhang, Xiaoming Xu, Jia Wei, Haofeng Huang, Pengle Zhang, Chendong Xiang, Jun Zhu, and Jianfei Chen. Sageattention2++: more efficient implementation of sageattention2. arXiv preprint arXiv:2505.21136, 2025. 3 [37] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. NeurIPS, 2024. 3, 9 22 [38] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In ICLR, 2024. 3, 6 [39] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In ICLR, 2024. 3, 7, [40] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. NeurIPS, 2024. 3, 7 [41] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In ICML, 2025. 3 [42] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. In ICML, 2024. 3 [43] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. In ICML, 2025. 3, 7 [44] Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified sparse attention. In MLSys, 2025. 3, 7 [45] Lida Chen, Dong Xu, Chenxin An, Xintao Wang, Yikai Zhang, Jiangjie Chen, Zujie Liang, Feng Wei, Jiaqing Liang, Yanghua Xiao, et al. Powerattention: Exponentially scaling of receptive fields for effective sparse attention. arXiv preprint arXiv:2503.03588, 2025. 3, 8 [46] Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, and Song Han. Duoattention: Efficient long-context llm inference with retrieval and streaming heads. In ICLR, 2025. 3 [47] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025. 3, 4, 8, 10, 14, 15 [48] Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. [49] Xin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan, Nan Duan, Yibo Zhu, Daxin Jiang, and Hong Xu. Dsv: Exploiting dynamic sparsity to accelerate large-scale video dit training. arXiv preprint arXiv:2502.07590, 2025. 3 [50] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. In ICML, 2025. 3, 7, 9 [51] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 3, 9 [52] Karan Dalal, Daniel Koceja, Gashon Hussein, Jiarui Xu, Yue Zhao, Youjin Song, Shihao Han, Ka Chun Cheung, Jan Kautz, Carlos Guestrin, et al. One-minute video generation with test-time training. arXiv preprint arXiv:2504.05298, 2025. [53] Lvmin Zhang and Maneesh Agrawala. Packing input frame context in next-frame prediction models for video generation. arXiv preprint arXiv:2504.12626, 2025. 3 [54] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. 3 [55] Dongchen Han, Xuran Pan, Yizeng Han, Shiji Song, and Gao Huang. Flatten transformer: Vision transformer using focused linear attention. In ICCV, 2023. [56] Songhua Liu, Zhenxiong Tan, and Xinchao Wang. Clear: Conv-like linearization revs pre-trained diffusion transformers up. arXiv preprint arXiv:2412.16112, 2024. 3 [57] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR, 2020. 3 23 [58] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. In Forty-first International Conference on Machine Learning, 2024. 3 [59] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In Forty-first International Conference on Machine Learning, 2024. [60] Han Cai, Junyan Li, Muyan Hu, Chuang Gan, and Song Han. Efficientvit: Lightweight multi-scale attention for high-resolution dense prediction. In ICCV, 2023. 3 [61] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. 4 [62] Zhenhai Zhu and Radu Soricut. H-transformer-1d: Fast one-dimensional hierarchical attention for sequences, 2021. [63] Zhanpeng Zeng, Sourav Pal, Jeffery Kline, Glenn Fung, and Vikas Singh. Multi resolution analysis (mra) for approximate self-attention, 2022. 4 [64] Yanming Kang, Giang Tran, and Hans De Sterck. Fast multipole attention: divide-and-conquer attention mechanism for long sequences, 2024. 4 [65] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. 4, [66] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. 4, 9 [67] Muyang Li, Ji Lin, Chenlin Meng, Stefano Ermon, Song Han, and Jun-Yan Zhu. Efficient spatially sparse inference for conditional gans and diffusion models. In NeurIPS, 2022. 7 [68] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, et al. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation. arXiv preprint arXiv:2412.21059, 2024. 8, 15 [69] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 8 [70] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2311.17982, 2025. 8 [71] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. In ICLR, 2025. 9 [72] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving. In MLSys, 2025. 9 [73] Junxian Guo, Haotian Tang, Shang Yang, Zhekai Zhang, Zhijian Liu, and Song Han. Block Sparse Attention. https://github.com/mit-han-lab/Block-Sparse-Attention, 2024. [74] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. 11 [75] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025."
        }
    ],
    "affiliations": []
}