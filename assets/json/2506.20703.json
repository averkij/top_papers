{
    "paper_title": "Generative Blocks World: Moving Things Around in Pictures",
    "authors": [
        "Vaibhav Vavilala",
        "Seemandhar Jain",
        "Rahul Vasanth",
        "D. A. Forsyth",
        "Anand Bhattad"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization."
        },
        {
            "title": "Start",
            "content": "GENERATIVE BLOCKS WORLD: MOVING THINGS AROUND IN PICTURES Vaibhav Vavilala1 1University of Illinois Urbana-Champaign 2Toyota Technological Institute at Chicago Seemandhar Jain1 Rahul Vasanth1 D.A. Forsyth1 Anand Bhattad2 5 2 0 2 5 ] . [ 1 3 0 7 0 2 . 6 0 5 2 : r Figure 1: Generative Blocks World. Given an input image (bottom left), we extract set of 3D convex primitives (top left) that provide an editable and controllable representation of the scene. These primitives are used to generate new images that respect geometry, texture, and the text prompt. The first column shows the original input and its primitive decomposition. Subsequent columns show sequential edits: translating the cat to the left (second column), translating it to the right (third column), moving the yarn in front of the cat and shifting the camera toward the scene center (fourth column), and scaling up the cats head (burgundy primitive; fifth column). Our method enables semantically meaningful, 3D-aware image editing through intuitive manipulation of these learned primitives."
        },
        {
            "title": "ABSTRACT",
            "content": "We describe Generative Blocks World to interact with the scene of generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by flow-based method which is conditioned on depth and texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization."
        },
        {
            "title": "INTRODUCTION",
            "content": "Modern large generative models can generate realistic-looking images from minimal input, but they offer limited control. Recent works have shown that intrinsic scene properties essential for renderingsuch as normals, depth, albedo, and illuminationemerge within the learned representations of these large generative models [3, 4, 12, 52]. Yet despite these emergent capabilities, modifying geometry, lighting, or viewpoint often disrupts appearance or object identity. Traditional rendering systems offer precise control through explicit geometric representations and physically based shading models but require extensive authoring effort and technical expertise. Our goal is to bring the control of traditional rendering to modern generative models without the overhead of explicit modeling. The system described here enables an author to modify the camera viewpoint of scene while preserving its 1 content, and to relocate objects or parts while maintaining their high-fidelity appearance (see Fig. 1). Achieving this, however, requires addressing two fundamental challenges in view synthesis and editing. At high level, these operations should be simple. For many pixels, accurate camera moves are easy: acquire an accurate depth map, project texture onto that map, then reproject into the new camera. Similarly, moving objects or parts is conceptually straightforward: project texture onto the depth map, adjust the depth map, then reproject. But this idealized pipeline breaks down in practice due to two key obstacles: (i) many target pixels are not visible in the source view, so texture must be extrapolated; and (ii) editing depth maps directly is very difficult and unintuitive. To address these challenges, we propose representing scenes as small assemblies of meaningful parts or primitives. This idea has deep roots. Roberts Blocks World [40] viewed simple scenes as handful of cuboids. Biederman [6] suggested that humans recognize and reason about objects as compositions of primitive parts. For our purposes, such assemblies must approximate the scenes depth map well enough to enable view-consistent texture projection. Primitive decompositions have been widely studied in computer vision for recognition, parsing, and reconstruction [17, 20, 21, 22, 32, 46, 47], but their application to content generation has been limited. Moreover, reliable primitive fitting is very recent phenomenon [47, 49]. Our work exploits these advances to control modern generative models, enabling precise, structured, and editable image synthesis. We represent scenes using convex geometric primitives and use them to control image synthesis, allowing edits such In homage to as camera moves, object moves, and detail adjustments, while maintaining structure and appearance. computer vision history, we call our framework Generative Blocks World, though our learned primitives are more complex than simple cuboids. Generative Blocks World decomposes an input image into sparse set of convex polytopes using an extension of recent convex-decomposition procedure [11, 47]. These convex primitives provide sufficient geometric accuracy to enable view-consistent texture projection. final rendering using pretrained depth-conditioned Flux DiT [28] preserves textures that should be known and inpaints missing textures. Our primitives are accurate enough that we dont need to train the generative depth-to-image model on the particular statistics of our primitives. Good primitive decompositions have very attractive properties. They are selectable: individual primitives can be intuitively selected and manipulated (see Fig. 1). They are object-linked: segmentation by primitives is close to segmentation by objects, meaning an editor is often able to move an object or part by moving primitive (Fig. 1; Fig. 3; Fig. 4). They are accurate: the depth map from properly constructed primitive representation can be very close to the original depth map (Section 3.1), which means primitives can be used to build texture hints (Section 3.2) that support accurate camera moves (Fig. 2; Fig. 5). They have variable scale: one can represent the same scene with different numbers of primitives, allowing an editor to adjust big or small effects (Fig. 7; Fig. 8; Fig. 12). Contributions. We describe pipeline that fuses convex primitive abstraction with SOTA flow-based generator, FLUX. Our pipeline uses natural texture-hint procedure that supports accurate camera moves and edits at the object-level, while preserving identity. We provide extensive evaluation demonstrating superior geometric control, texture retention, and edit flexibility relative to recent state-of-the-art baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Primitive Decomposition. Early vision and graphics pursued parsimonious part-based descriptions, from Roberts Blocks World [40] and Binfords generalized cylinders [7] to Biedermans geons [6]. Efforts to apply similar reasoning to real-world imagery have been periodically revisited [20, 32, 5] from various contexts and applications. Modern neural models revive this idea: BSP-Net [8], CSG-Net [42], and CVXNet [11] represent shapes as unions of convex polytopes, while Neural Parts [46], SPD [58], and subsequent works [30] learn adaptive primitive sets. Recent systems extend from objects to scenes: Convex Decomposition of Indoor Scenes (CDIS) [47] and its ensembling/Boolean refinement [49] fit CVXNet-like polytopes to RGB-D images, using hybrid strategy. CubeDiff [25] fits panoramas inside cuboids. Our work leverages CDIS as the backbone, but (i) improves robustness to in-the-wild depth/pose noise and (ii) couples the primitives to Rectified Flow (RF) renderer, enabling controllable synthesis rather than analysis alone. Conditioned Image Synthesis. Conditional generative networks such as Pix2Pix [23], CycleGAN [57], and SPADE [37] pioneered layout-to-image translation. Diffusion models now dominate; seminal works include Stable Diffusion [41], ControlNet [55], and T2I-Adapter [34]. Subsequent work has shown that multiple spatial controls can be composed for restoring images [48], and recent methods can exert local and global color edits [50]. In this work, we use pretrained depth-conditional FLUX model using depth maps derived from primitives. 2 Point-Based Interactive Manipulation. Point-based manipulation offers direct, intuitive control over 2D image attributes. DragGAN [35] allows users to deform an objects pose or shape by dragging handle points on 2D generative manifold. This concept was subsequently adapted to more general diffusion models by methods like DragDiffusion [43], DragonDiffusion [33], StableDrag [10], DiffusionHandles [36], and Dragin3D [19], which improved robustness, controllability, and fidelity. Diffusion Self-Guidance can exert layout control and perform object-level edits [14]. However, these methods fundamentally operate by deforming pixels or lack true understanding of 3D scene structure. They can perform in-place pose/shape edits and even simple translations but struggle to perform 3D-consistent manipulations, such as moving objects within scene or moving the camera while respecting perspective, occlusion, and texture. In contrast, we show promising results in such scenarios and offer flexible control of primitives, providing both fine-grained control when using large number of primitives and coarse, object-level control when using smaller number of primitives. Object-Level and Scene-Level Editing. Many recent works embed 3D priors into generative editing but focus on single objects: StyleNeRF [18], SJC [51], DreamFusion [38], Make-A-Dream [45], and 3D-Fixup [9]. Methods like Obj3DiT [31] use language to guide transformations (e.g., rotation, translation) by fine-tuning model on large-scale synthetic dataset. In contrast, Generative Blocks World generalizes to complex editing tasks that are not easy to describe precisely in text form. An alternative paradigm, seen in Image Sculpting [54] and OMG3D [56], offers precise control by first reconstructing 2D object into an explicit 3D mesh, which is then manipulated and re-rendered using generative models. While offering high precision, these multi-stage pipelines can be complex and are often bottlenecked by the initial reconstruction quality. Our method provides more streamlined approach by operating on abstract primitives, avoiding the complexities of direct mesh manipulation while still providing strong geometric control. Primitive-Based Scene Authoring. Recently, LooseControl [2] showed how to train LoRA weights on top of pretrained Depth ControlNet, enabling box-like primitive control of image synthesis. The LoRA weights bridge the domain gap between box-like primitive depth maps and standard depth maps as one might obtain from, e.g., DepthAnything [53]. In contrast, this paper demonstrates primitive fits that do not require fine-tuning diffusion models because the underlying primitive representation is highly accurate. We similarly adopt depth-to-image generator, but our conditioning signal is structured geometrya set of editable primitives rather than dense mapsyielding stronger semantic correspondence and causal behavior. More recently, Build-A-Scene [13] uses the same primitive generator and image synthesizer as LooseControl; thus, it suffers from the same problems in depth accuracy. Generative Blocks World differs by (i) decomposing each object into handful of convex polytopes, giving finer yet still abstract control; (ii) supporting camera moves; and (iii) allowing new scenes to be authored via primitive assembly."
        },
        {
            "title": "3 METHOD",
            "content": "Generative Blocks World generates realistic images conditioned on parsimonious and editable geometric representation of scene: set of convex primitives. The process consists of four main stages: (i) primitive extraction from any image via convex decomposition (Sec. 3.1), (ii) generating an image conditioned on the primitives (and text prompt), (iii) user edits the primitives and/or camera, and (iv) generates new image conditioned on the updated primitives, while preserving texture from the source image (Sec. 3.3). We describe each component in detail below. See Fig. 2 for an overview. 3.1 CONVEX DECOMPOSITION FOR PRIMITIVE EXTRACTION Our primitive vocabulary is blended 3D convex polytopes as described in [11]. CVXnet represents the union of convex polytopes using indicator functions O(x) [0, 1] that identify whether query point R3 is inside or outside the shape. Each convex polytope is defined by collection of half-planes. half-plane Hh(x) = nh + dh provides the signed distance from point to the h-th plane, where nh is the normal vector and dh is the offset parameter. While the signed distance function (SDF) of any convex object can be computed as the maximum of the SDFs of its constituent planes, CVXnet uses differentiable approximation. To facilitate gradient learning, instead of the hard maximum, the smooth LogSumExp function is employed to define the approximate SDF, Φ(x): The signed distance function is then converted to an indicator function : R3 [0, 1] using: Φ(x) = LogSumExp{δHh(x)} C(xβ) = Sigmoid(σΦ(x)) 3 Figure 2: Pipeline Overview. Top left: We use pretrained convex decomposition models [49] to extract primitives from an input image at multiple scales. Bottom: Users can manipulate these primitives and the camera to define new scene layout. We render the modified primitives into depth map and generate texture hint image. These serve as inputs to pretrained depth-to-image model [28], which requires no fine-tuning (Top right). The resulting image respects the modified geometry, preserves texture where possible, and remains aligned with the text prompt. The collection of hyperplane parameters for primitive is denoted as = {(nh, dh)}, and the overall set of parameters for convex as β = [h, σ]. While σ is treated as hyperparameter, the remaining parameters are learnable. The parameter δ controls the smoothness of the generated convex polytope, while σ controls the sharpness of the indicator function transition. The soft classification boundary created by the sigmoid function facilitates training through differentiable optimization. The neural architecture of our primitives model is the standard ResNet-18 Encoder Eθ followed by 3 fullyconnected layers that decode into the parameters of the primitives Dθ. While the model is lightweight, the SOTA of primitive prediction requires different trained model for each primitive count K. Recent work has adapted primitive decomposition to real-world scenes (as opposed to well-defined, isolated objects, such as those in ShapeNet [47]). These methods combine neural prediction with post-training refinement: an encoder-decoder network predicts an initial set of convex polytopes, which is followed by gradient-based optimization to align the primitives closely to observed geometry. This approach is viable because the primary supervision for primitive fitting is depth map (with heuristics that create 3D samples, and auxiliary losses to avoid degenerate solutions). Note that ground truth primitive parameters are not available (as they could be in many other computer vision settings e.g., segmentation [26]). This is why the losses encourage the primitives to classify points near the depth map boundary correctly instead of directly predicting the parameters. Rendering the primitives. We condition the RF model on the primitive representation via depth map, obtained by ray-marching the SDF from the original viewpoint of the scene. Depth conditioning abstracts away potential chatter in the primitive representation from e.g. over-segmentation, while simultaneously yielding flexibility in fine details (depth maps typically lack pixel-level high-frequency details). Depth-conditioned image synthesis models are well-established e.g. [55]. Because its hard to edit depth map, but easy to edit 3D primitives, our work adds new level of control to the existing image synthesis models. As we establish quantitatively in Table 2, our primitive generator is extremely accurate, and our evaluations show that we get very tight control over the synthesized image via our primitives. This means that whatever domain gap there is between depth from primitives and depth from SOTA depth estimation networks is not significant. Scaling to in-the-wild scenes. We collect 1.8M images from LAION to train our primitive prediction models. To obtain ground truth depth supervision, we use DepthAnythingv2 [53]. To lift depth map RHW to 3D point cloud using the pinhole camera model, each pixel (u, v) with depth du,v maps to 3D point (X, Y, Z) as: 4 = (u cx) du,v fx , = (v cy) du,v fy , = du,v where (cx, cy) is the principal point (typically W/2, H/2), and (fx, fy) are the focal lengths along the image axes. DepthAnythingv2 supplies metric depth module with reasonable camera calibration parameters. These 3D samples are required to supervise primitive fitting. In fact, at test-time, we can directly optimize primitive parameters using the training losses since these 3D samples are available. Primitive fitting details. We use the standard ResNet-18 encoder (accepting RGBD input) followed by 3 fullyconnected layers to predict the parameters of the primitives. We train different networks for different primitive counts {4, 6, 8, 10, 12, 24, 36, 48, 60, 72}, and allow the user to select their desired level of abstraction. Alternatively, the ensembling method of [49] can automatically select the appropriate number of primitives. Depending on the primitive count, the training process takes between 40-100 mins on single A40 GPU, and inference (including generating the initial primitive prediction, refinement, and rendering) can take 1-3 seconds per image. While traditional primitive-fitting to RGB images fits cuboids [27], we find that polytopes with more faces and without symmetry constraints yield more accurate fits. Thus, we use = 12 face polytopes. We do not use Manhattan World loss or Segmentation loss; the former helped on NYUv2 [44] but not on in-the-wild LAION images and the latter showed an approximately neutral effect in the original paper [47]. 3.2 DEPTH-CONDITIONED INPAINTING IN RECTIFIED FLOW TRANSFORMERS Here, we describe our image synthesis pipeline. We build upon the state-of-the-art Flux, rectified flow model [15, 28]. Forward Noising Process. In the forward process, clean latent representation x0 (derived from an input image via variational autoencoder, VAE) is progressively noised over timesteps to produce sequence x1, x2, . . . , xT . The noise schedule is defined by sigmas σt, typically linearly interpolated from 1.0 to 1 . The forward process is governed by: (cid:113) xt = 1 σ x0 + σtϵ, ϵ (0, I), where σt controls the noise level, and ϵ is Gaussian noise. For conditional inputs like depth map, the control image is encoded into latents via the VAE and concatenated with the noisy latents xt during the reverse process. Adding Spatial Conditions. Older ControlNet implementations [55] train an auxiliary encoder that adds information to decoder layers of base frozen U-Net. Newer implementations, including models supplied by the Black Forest Labs developers, concatenate the latent xt and condition (e.g., depth map) as an input to the network, yielding tighter control. FLUX.1 Depth [dev] re-trains the RF model with the added conditioning; FLUX.1 Depth [dev] LoRA trains LoRA layers on top of frozen base RF model. Both options give tight control and work well with our primitives, though LoRA exposes an added parameter loraweight [0, 1] tuning how tightly the depth map should influence synthesis. This is helpful when the primitive abstraction is too coarse relative to the geometric complexity of the desired scene. Reverse Diffusion Process. The reverse process starts from noisy latent xT (0, I) and iteratively denoises to approximate x0. The DiT-RF model uses transformer architecture with: Double-stream layers: process image tokens (noisy latents and control image latents) and text tokens (prompt embeddings) separately with cross-attention. Singlestream layers: jointly process all tokens to capture interactions. The model predicts noise ϵθ(xt, t, c, p), where is the control image and includes text embeddings and pooled projections. The scheduler updates the latents: using rectified flow techniques to optimize the denoising trajectory. xt1 = SchedulerStep(xt, t, ϵθ), Role of Hint and Mask. core contribution of this work is an algorithm to generate hint image to initialize the image generation process, as well as confidence mask (see Sec 3.3). The hint and mask influence the generation within timesteps tend tstart, which are hyperparameters. The mask [0, 1] specifies regions where the hint should guide the output. The hint is encoded into latents xhint via the VAE. During denoising, the latents are updated as: where xhint,t is the noised hint latent at timestep t: xt = (1 m) xhint,t + xt, xhint,t = SchedulerScaleNoise(xhint, t, ϵ). Thus, the hint image is noised to match the current timesteps noise level before incorporation, ensuring consistency with the denoising process. Outside [tend, tstart], the hint and mask are ignored. 5 Figure 3: Editable Primitives as Structured Depth Prior for Generative Models. Our method uses 3D convex primitives as an editable intermediate representation from which depth maps are derived. These depth maps (shown as insets in the top row) are used to condition pretrained depth-to-image generative model. The top row shows primitive configurations after sequential editstranslation, scaling, deletion, and camera motionalongside their corresponding derived depth maps. The bottom row shows the resulting synthesized images. Unlike direct depth editing, which is unintuitive and underconstrained, manipulating primitives offers structured, interpretable, and geometry-aware interface for controllable image generation. Figure 4: Comparison with Drag Diffusion [43]. Given scene (first column), we attempt to reposition objects using recent point-based image editing method by drawing drag handles (second column). However, drag points are ambiguous: it is unclear whether the intended operation is translation or scaling. As result, the output lacks geometric consistency (third column). E.g., the clock changes shape, and pushing it deeper into the scene fails to reduce its size appropriately; fine details on the can are lost. In contrast, Generative Blocks World infers 3D primitives (fourth column) that can be explicitly manipulated (fifth column), producing plausible image that respects object geometry, scale, positioning, and texture (last column). 3.3 TEXTURE HINT GENERATION FOR CAMERA AND OBJECT EDITS number of methods have been proposed to preserve texture/object identity upon editing an image. common and simple technique is to copy the keys and values from style image into the newly generated image (dubbed style preserving edits). For older U-Net-based systems, this is done in the bottleneck layers [2]. For newer DiTs, this is done at selected vital layers [1]. In our testing, key-value copying methods are insufficient for camera/primitive moves (see Fig. 6). Further, because of our primitives, we have geometric representation of the scene. Here we demonstrate routine to obtain source hint image xhint as well as confidence mask that can be incorporated in the diffusion process. The hint image is rough approximation of what the synthesized image should look like using known spatial correspondences between primitives in the first view and the second. The confidence mask indicates where we can and cannot trust the hint, commonly occurring near depth discontinuities. We rely on the diffusion machinery to essentially clean up the hint, filling gaps and refining blurry projected textures so it looks like real image. The result of our process is an image that respects the text prompt, source texture, and newly edited primitives/camera. Figure 5: Comparison with LooseControl [2]. Camera moves present serious problems for existing work. Four scenes (left side of each pair), synthesized from the depth maps shown. In each case, the camera is moved to the right (right side of each pair), and the image is resynthesized. Note how, for LooseControl, the number of apples changes (first pair); the level of water in the glass changes and there is an extra ice cube (second pair); the duck changes (third pair); an extra rock appears (fourth pair). In each case, our method shows the same scene from different view, because the texture hint image is derived from the underlying geometry, and strongly constrains any change. Creating point cloud correspondences We develop method that accepts point clouds at the ray-primitive intersection points, convex map integer array indicating which primitive was hit at each pixel, list of per-primitive transforms (such as scale, rotate, translate), and hyperparameter max distance for discarding correspondences. This procedure also robustly handles camera moves because the input point clouds are representations of the same scene in world space. Creating texture hint Given correspondence map of each 3D point in the new view relative to the original view, we can apply this correspondence to generate hint image that essentially projects pixels in the old view onto the new view. This is the xhint supplied to the image generation model, taking into account both camera moves and primitive edits like rotation, translation, and scaling. The point cloud correspondence ensures that if primitive moves, its texture moves with it. In practice, this hint is essential for good texture preservation (see Fig. 6). Correspondence and hint generation take about 1-2 seconds per image; 30 denoising steps of FLUX at 512 resolution take about 3 seconds on an H100 GPU. 3.4 EVALUATION the image that was We seek error metrics to establish (1) geometric consistency between the primitives requested vs. synthesized and (2) texture consistency between the source and edited image. For (1) we compute the AbsRel between the depth map supplied to the depth-to-image model (obtained by rendering the primitives) and the estimated depth of the synthesized image (we use the hypersim metric depth module from [53] to get linear depth). Consistent with standard practice in depth estimation, we use least squares to fit scale and shift parameters onto the depth from RGB (letting the primitive depth supplied to the DM be GT). To evaluate texture consistency, we apply ideas from the novel view synthesis literature and our existing point cloud correspondence pipeline. Given the source RGB image and the synthesized RGB image (conditioned on the texture hint), we warp the second image back into the first images frame using our point cloud correspondence algorithm. If we were to synthesize an image in the first renders viewpoint using the second render, this is the texture hint we would use. In error metric calculation, the first RGB image is considered ground truth, the warped RGB image from the edited synthesized image is the prediction, and the confidence mask filters out pixels that are not visible in view 1, given view 2. This evaluation procedure falls in the category of cycle consistency/photometric losses that estimate reprojection error [16, 24, 29, 39]. 7 Figure 6: Projection-Based Texture Hints Preserve Object Identity After Edits. This figure compares our projectionbased texture hints against StableFlow [1], which uses vital-layer key-value injection. First two columns: input primitives and image. Third: edited primitives. Fourth: synthesis from original depth, revealing consistent geometry but altered texture. Fifth: StableFlows approach often changes texture or object identity. Sixth: our projection-based hints maintain texture fidelity despite edits. Seventh: combining both approaches can improve fine detail recovery (e.g., the treasure chest). 3.5 HYPERPARAMETER SELECTION There are number of hyperparameters associated with our procedure, and we perform grid search on held-out validation set to find the best ones. When generating correspondence maps between point clouds, we let max distance= 0.005. In our confidence map, we dilate low-confidence pixels with score less than τ = 0.01 by 9 pixels, which tells the image model to synthesize new texture near primitive boundaries that are often uncertain. We set (tstart, tend) to (1000, 500) by default, though tend can be tuned per test image by the user. Applying the hint for all time steps can reduce blending quality near primitive boundaries; not applying the hint for enough time steps could weaken texture consistency. Allowing some time steps to not follow the hint enables desirable super resolution behavior e.g. when bringing primitive closer to the camera. See supplementary for detailed algorithms for creating the hint and confidence mask. Inpainting the hint image After warping the source image to the new view, we find it helpful to inpaint low-confidence regions of the hint xhint before supplying it to the image model. We considered several possibilities, including cv2 telea and cv2 ns from the OpenCV package, as well as simply leaving them as black pixels. We find that Voronoi inpainting, variation of nearest neighbor inpainting, worked well. The voronoi inpainting function performs image inpainting by filling in regions of low confidence in hint image using colors from nearby high-confidence pixels, based on Voronoi diagram approach. Given hint image of shape [H, W, 3] and confidence mask of shape [H, ] (after resizing if necessary), we identify valid pixels where the confidence satisfies Ci,j τ , with τ being the threshold (default 0.01). For each pixel (i, j) in the image, we assign the color of the nearest valid pixel (k, l), determined by Euclidean distance, effectively performing nearest-neighbor interpolation. Mathematically, the inpainted image is defined as: i,j = Ik,l where (k, l) = arg min (m,n)V (cid:112)(i m)2 + (j n)2, and = {(m, n) Cm,n τ } represents the set of high-confidence pixel coordinates. This process leverages KDtree for efficient nearest-neighbor searches, ensuring that each pixel adopts the color of the closest reliable pixel, thus preserving local color consistency in the inpainted result. For FLUX image generation we begin with the default settings from the diffusers FLUX controlled inpainting pipeline 1. We set the strength parameter (controlling starting noise strength) to 1.0 and guidance to 10. We use 30 num steps for denoising. In comparative evaluation, we use the default settings from the authors."
        },
        {
            "title": "4 RESULTS",
            "content": "In Fig. 4, we show how users can manipulate depth map inputs to depth-to-image synthesizers with our primitive abstractions. We can use these primitives to edit images, as shown in Fig. 5. Notice how we can get precise control over the 1https://huggingface.co/docs/diffusers/en/api/pipelines/control_flux_inpaint 8 (a) = 6 parts. (b) = 8 parts. Figure 7: Applying the same primitive edit for different text prompts at coarse scale (K {6, 8} parts). The first row in each subplot contains source primitives and depth (first two columns); the confidence mask for hint generation, followed by four source RGB images. The second row shows the modified primitives and depth, followed by the hint image xhint, followed by the four corresponding edited images. At coarse scales, moving primitive can move lot of texture at once. Observe how our hint generation procedure automatically yields confidence masks and hints, assigning low confidence to boundaries of primitives that moved (e.g., the dogs hair) and reveals holes when moving objects. The image model cleans up the low-confidence regions and even handles blurry/aliased texture in the hint when tend > 0, meaning that the hint is not used for some denoising steps. Method AbsRelsrc AbsReldst PSNR SSIM Ours LooseControl [2] 0.072 0.143 0.076 0. 18.7 6.65 0.874 0.670 Table 1: Comparison of image reconstruction and generation metrics between our method and LooseControl. AbsRelsrc and AbsReldst are absolute relative errors evaluating how well the generated images adhere to the requested primitive geometry (source and modified, respectively). PSNR and SSIM are evaluated by reprojecting the second synthesized image back to the original camera viewpoint (see Sec 3.4 and measuring texture consistency with the source. Observe how our procedure simultaneously offers tight geometric adherence to the primitives while preserving the source texture. Results obtained by averaging 48 test images with random camera moves. Because [2] does not offer primitive extraction code, we supply our own primitives to both methods for evaluation. We use = 10 parts for this evaluation. synthesized geometry while respecting texture, which existing methods struggle to do. We quantitatively evaluate this property in Table 1, demonstrating we hit both goals conclusively. Existing texture preservation methods for multi-frame consistency typically rely on key-value transfer from one image to another. This, unfortunately, does not preserve details very well, only high-level semantics and style. We ablate the advantage of our texture preservation approach in Fig. 6. When there are few primitives, moving one primitive affects big part of the scene; when there are lot of primitives, we can make fine-scale edits. We show several such examples in Figs. 7, 8. 9 (a) = 24 parts. (b) = 60 parts. Figure 8: Applying the same primitive edit for different text prompts at fine scale (K {24, 60} parts). Observe in the first two rows how all synthesized images respect the enlarged green primitive, while background texture is preserved. In the bottom two rows, we compose several edits using large number of primitives (K = 60), enabling fine-scaled edits. We scale up the light blue primitive while scaling down the light green primitive on the left-hand side. We then translate the dark blue primitive on the right-hand side towards the bottom center of the image. We also slightly translate the camera upward. Observe how in the subsequent columns, the edited result respects the geometry specified by the primitives while following the high-level texture of the source image. However, notice how composing four edits challenges our procedure, as the texture preservation isnt as tight. For example, in the final column, tiled pattern appears on the floor that wasnt in the source."
        },
        {
            "title": "5 DISCUSSION",
            "content": "This work demonstrates that we can utilize 3D primitives to achieve precise geometric control over image generation model outputs, and even preserve high-level textures more effectively than existing methods that rely on key-value transfer. central reason this works is that good primitive decompositions offer several useful properties: they are selectable, allowing intuitive manipulation of scene components; they are object-linked, with boundaries that often correspond to semantic parts; they are scalable, enabling both coarse and fine-grained edits (with fewer and higher source primitive counts); and they are accurate enough to yield depth maps that support high-quality texture projection. Moreover, our pipeline is designed to be user-friendly: since our primitive decomposition is fast, users can easily choose between coarse and fine control by adjusting the number of primitives, and seamlessly switch between decompositions to suit the editing task and scene context. We believe we have unlocked new interactive controls for image synthesis with our Generative Blocks World. While our method handles problems near primitive boundaries robustly, objects with holes that are not tightly modeled by the primitives (e.g., underneath chair or coffee mug handle) are challenging in our current formulation; additional segmentation and masking would be required (or simply using more primitives). Depth-of-field blurring/bokeh may not get resolved or sharpened when bringing out-of-focus objects into focus. Significant object rotations may also fail (see 10 Figure 9: Failure cases. Top: Illumination misalignments. Our texture hints operate in pixel space and cannot model illumination effects outside primitive boundaries (e.g., reflections or cast shadows). As result, moving or scaling objects may not consistently update their associated lighting effects. For example, the bread stack is translated correctly, but its reflection remains unchanged (see fourth column). Middle: Poor decomposition. Primitive fitting may fail in cluttered scenes or near image boundaries, where sparse depth points hinder the separation of adjacent objects (e.g., the bottle and paper towel are merged). This leads to inaccurate depth maps and poor control. Bottom: Rotation artifacts. Large object rotations (e.g., 50) disrupt texture consistency and geometry, possibly due to distribution shift in the texture hints, resulting in distortions or hallucinated content (e.g., warped Blocks World text). Figure 10: Primitive edits can conflict with the text prompt. Some geometric edits require changing the text prompt, for example, when removing an object. The fourth column mentions brick in the text prompt, but that primitive was removed, resulting in brick pieces in the inpainted region. In the fifth column, we remove the brick from the text prompt, which removes the brick pieces but it still leaves behind white stone. In the final column, we use our texture hints but without StableFlow, getting clean surface. The StableFlow key-value sharing approach placed brick and stone textures where we didnt want them. We conclude that our texture hints are critical, but combining them with StableFlow [1] key-value sharing can help in some cases, hurt in others. Fig. 9). In an interactive workflow, manually expanding the confidence mask to include problematic regions e.g., unwanted reflections that dont move with primitive, can fix some issues. Future work that applies our point correspondences within the network layers themselves (e.g., in vital layers) may yield more robust solutions. Our method does not yet account for view-dependent lighting effects and does not enforce temporal consistency across frames for video synthesis. Our results focus on editing generated images. While we can extract texture hints from real images, our experiments show that edited images should start from the same noise tensor and prompt as the source image to achieve good results. Therefore, good image inverters that work with depth-conditioned diffusion transformers are needed. Additionally, certain extreme edits that are at odds with the text prompt are likely to cause problems (e.g., if the prompt mentions an object is on the right, but user manipulates the primitives to move the object to the left). Changing the text prompt could work in some circumstances  (Fig. 10)  , or the DiT will inpaint missing regions with content that doesnt harmonize well with the rest of the image. This is due to the delicate link between the text prompt, hint image, initial noise tensor, and depth map."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This material is based upon work supported by the National Science Foundation under Grant No. 2106825 and by gift from Boeing Corporation."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel CohenOr. Stable flow: Vital layers for training-free image editing. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), June 2025. [2] Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In ACM SIGGRAPH 2024 Conference Papers, SIGGRAPH 24, New York, NY, USA, 2024. Association for Computing Machinery. doi: 10.1145/3641519.3657525. URL https://doi.org/10.1145/3641519. 3657525. [3] Anand Bhattad, Daniel McKee, Derek Hoiem, and David Forsyth. Stylegan knows normal, depth, albedo, and more. Advances in Neural Information Processing Systems, 36, 2023. [4] Anand Bhattad, James Soole, and David Forsyth. Stylitgan: Image-based relighting via latent control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [5] Anand Bhattad, Konpat Preechakul, and Alexei A. Efros. Visual jenga: Discovering object dependencies via counterfactual inpainting, 2025. URL https://arxiv.org/abs/2503.21770. [6] Biederman. Recognition by components : theory of human image understanding. Psychological Review, (94), 1987. [7] TO Binford. Visual perception by computer. In IEEE Conf. on Systems and Controls, 1971. [8] Zhiqin Chen, Andrea Tagliasacchi, and Hao Zhang. Bsp-net: Generating compact meshes via binary space partitioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [9] Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liangyan Gui, Matheus Gadelha, Paul Guerrero, and Nanxuan Zhao. 3D-Fixup: Advancing Photo Editing with 3D Priors. In Proceedings of the SIGGRAPH Conference Papers. ACM, 2025. doi: 10.1145/3721238.3730695. [10] Yutao Cui, Xiaotong Zhao, Guozhen Zhang, Shengming Cao, Kai Ma, and Limin Wang. Stabledrag: Stable dragging for point-based image editing. In European Conference on Computer Vision. Springer, 2024. [11] Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, and Andrea Tagliasacchi. Cvxnet: Learnable convex decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [12] Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, and Anand Bhattad. Generative models: What do they know? do they know things? lets find out! arXiv preprint arXiv:2311.17137, 2023. [13] Abdelrahman Eldesokey and Peter Wonka. Build-a-scene: Interactive 3d layout control for diffusion-based image In The Thirteenth International Conference on Learning Representations, 2025. URL https:// generation. openreview.net/forum?id=gg6dPtdC1C. [14] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. 2023. [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [16] Qihang Fang, Yafei Song, Keqiang Li, Li Shen, Huaiyu Wu, Gang Xiong, and Liefeng Bo. Evaluate geometry of radiance fields with low-frequency color prior. In Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence and Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence and Fourteenth Symposium on Educational Advances in Artificial Intelligence, AAAI24/IAAI24/EAAI24. AAAI Press, 2024. doi: 10.1609/aaai.v38i2.27938. URL https://doi.org/10.1609/aaai.v38i2.27938. [17] David F. Fouhey, Abhinav Gupta, and Martial Hebert. Data-driven 3d primitives for single image understanding. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2013. [18] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: style-based 3d aware generator for highresolution image synthesis. In International Conference on Learning Representations, 2022. 12 [19] Weiran Guang, Xiaoguang Gu, Mengqi Huang, and Zhendong Mao. Dragin3d: Image editing by dragging in 3d space. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), June 2025. [20] Abhinav Gupta, Alexei A. Efros, and Martial Hebert. Blocks world revisited: Image understanding using qualitative geometry and mechanics. In European Conference on Computer Vision (ECCV), 2010. [21] Varsha Hedau, Derek Hoiem, and David Forsyth. Thinking inside the box: using appearance models and context based on room geometry. In Proceedings of the 11th European Conference on Computer Vision: Part VI, ECCV10, Berlin, Heidelberg, 2010. Springer-Verlag. [22] Derek Hoiem, Alexei A. Efros, and Martial Hebert. Recovering surface layout from an image. International Journal of Computer Vision, 75(1), 2007. doi: 10.1007/s11263-006-0031-y. URL https://doi.org/10.1007/ s11263-006-0031-y. [23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2017. [24] Yoonwoo Jeong, Jinwoo Lee, Seokyeong Lee, Doyup Lee, and Minhyuk Sung. NVS-Adapter: Plug-and-play novel view synthesis from single image. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [25] Nikolai Kalischek, Michael Oechsle, Fabian Manhardt, Philipp Henzler, Konrad Schindler, and Federico Tombari. Cubediff: Repurposing diffusion-based image models for panorama generation, 2025. URL https://arxiv. org/abs/2501.17162. [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023. doi: 10.1109/ICCV51070.2023.00371. [27] Florian Kluger, Hanno Ackermann, Eric Brachmann, Michael Ying Yang, and Bodo Rosenhahn. Cuboids revisited: Learning robust 3d shape fitting to single rgb images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [28] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [29] Feifei Li, Qi Song, Chi Zhang, Hui Shuai, and Rui Huang. PoI: Pixel of interest for novel view synthesis assisted scene coordinate regression. arXiv preprint arXiv:2502.04843, 2025. [30] Haolin Liu, Yujian Zheng, Guanying Chen, Shuguang Cui, and Xiaoguang Han. Towards high-fidelity single-view holistic reconstruction of indoor scenes. In European Conference on Computer Vision. Springer, 2022. [31] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Krishna, Aniruddha Kembhavi, and Tanmay Gupta. Object 3dit: Language-guided 3d-aware image editing. Advances in Neural Information Processing Systems, 36, 2023. [32] Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei A. Efros, and Mathieu Aubry. Differentiable Blocks World: Qualitative 3D Decomposition by Rendering Primitives. In NeurIPS, 2023. [33] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. [34] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. Proceedings of the AAAI Conference on Artificial Intelligence, 38(5), Mar. 2024. doi: 10.1609/aaai.v38i5.28226. URL https://ojs. aaai.org/index.php/AAAI/article/view/28226. [35] Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 conference proceedings, 2023. [36] Karran Pandey, Paul Guerrero, Matheus Gadelha, Yannick Hold-Geoffroy, Karan Singh, and Niloy J. Mitra. Diffusion handles enabling 3d edits for diffusion models by lifting activations to 3d. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. doi: 10.1109/CVPR52733.2024.00735. [37] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 13 [38] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2023. URL https://openreview.net/forum?id=FjNys5c7VyY. [39] Yuxin Qin, Xinlin Li, Linan Zu, and Ming Liang Jin. Novel view synthesis with depth priors using neuISSN 2073-8994. doi: ral radiance fields and cyclegan with attention transformer. Symmetry, 17(1), 2025. 10.3390/sym17010059. URL https://www.mdpi.com/2073-8994/17/1/59. [40] L. G. Roberts. Machine Perception of Three-Dimensional Solids. PhD thesis, MIT, 1963. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. doi: 10.1109/CVPR52688.2022.01042. [42] Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. Csgnet: Neural shape parser for constructive solid geometry. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. [43] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [44] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Proceedings of the 12th European Conference on Computer Vision - Volume Part V, ECCV12, Berlin, Heidelberg, 2012. Springer-Verlag. doi: 10.1007/978-3-642-33715-4 54. URL https://doi.org/10. 1007/978-3-642-33715-4_54. [45] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023. [46] Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Computer Vision and Pattern Recognition (CVPR), 2017. [47] Vaibhav Vavilala and David Forsyth. Convex decomposition of indoor scenes. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023. doi: 10.1109/ICCV51070.2023.00842. [48] Vaibhav Vavilala, Rahul Vasanth, and David Forsyth. Denoising monte carlo renders with diffusion models, 2024. URL https://arxiv.org/abs/2404.00491. [49] Vaibhav Vavilala, Florian Kluger, Seemandhar Jain, Bodo Rosenhahn, Anand Bhattad, and David Forsyth. Improved convex decomposition with ensembling and boolean primitives, 2025. URL https://arxiv.org/abs/2405. 19569. [50] Vaibhav Vavilala, Faaris Shaik, and David Forsyth. Dequantization and color transfer with diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2025. doi: 10.1109/WACV61041. 2025.00932. [51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. doi: 10.1109/CVPR52729.2023.01214. [52] Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, and Anand Bhattad. Luminet: Latent intrinsics meets diffusion models for indoor scene relighting. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2025. Depth anything v2. [53] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, and Hengshuang Zhao. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 26cfdcd8fe6fd75cc53e92963a656c58-Paper-Conference.pdf. Jiashi Feng, 14 [54] Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, and Saining Xie. Image sculpting: Precise object editing with 3d geometry control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 2023. doi: 10.1109/ICCV51070.2023. 00355. [56] Ruisi Zhao, Zechuan Zhang, Zongxin Yang, and Yi Yang. 3d object manipulation in single image using generative models, 2025. URL https://arxiv.org/abs/2501.12935. [57] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, 2017. [58] Chuhang Zou, Alex Colburn, Qi Shan, and Derek Hoiem. Layoutnet: Reconstructing the 3d room layout from single rgb image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018."
        },
        {
            "title": "A APPENDIX",
            "content": "Here we present detailed algorithms for creating the texture hint, as well as additional evaluation. Figure 11: Our model is compatible with most depth-image synthesizers. While pretrained FLUX works out of the box, LoRA weights on top of the base FLUX model are available ( FLUX.1 Depth [dev] LoRA), exposing new loraweight parameter (scaling the activations of the LoRA layers). This is intriguing in the context of our primitives, because they can either be used to coarsely model scene geometry (e.g. loraweight near 0.8, second last column), leaving details to the image synthesizer, or they can tightly control the result when loraweight is close to 1 (final column). Number of Parts (K) AbsRel Error 4 6 8 10 12 24 36 48 60 72 0.0376 0.0330 0.0295 0.0282 0.0265 0.0223 0.0203 0.0202 0.0194 0.0195 Table 2: AbsRel depth error metrics for varying numbers of 3D box primitives (12-face polytopes). Lower values indicate better depth map approximation quality. While theory would predict AbsRel 0 as (e.g. one primitive per pixel), in practice we run into bias-variance problems fitting more than 60 primitives. Generating primitives is efficient (approx. 1-3 seconds per image on the GPU including finetuning and rendering) so it is feasible for the user to select from few candidates based on the desired level of abstraction. No other primitive-conditioned image synthesis method offers variable abstraction. 16 Figure 12: Given the same depth map, we extract primitives at variable resolution (from 4-72 parts). We show the depth maps in each second row, and synthesized result in each 3rd row. Observe how no matter the resolution, the FLUX-LoRA model (we use loraweight = 0.8) gives an image that follows the primitive conditioning. We conclude that wide array of primitive densities is tolerable to depth-to-image models, enabling meaningful artistic edits. 17 Figure 13: Additional move camera evaluations. Generative Blocks World can simultaneously adhere to source texture and requested primitives. 18 Figure 14: Additional move camera evaluations. Our method can simultaneously adhere to source texture and requested primitives. 19 Figure 15: Additional move camera evaluations. Our method can simultaneously adhere to source texture and requested primitives. 20 Figure 16: We repeat the analysis of StableFlow [1], which applies U-Net based key-value caching of older-generation Diffusion models to newer Diffusion Transformers. Specifically, their work analyzes FLUX.1 [dev]; given that our work uses depth maps to communicate geometric information to our image generation model, we analyze Vital Layers in FLUX.1 Depth [dev] and FLUX.1 Depth [dev] LoRA, finding the top 5 multimodal and single modal layers to be essentially identical. We try using the vital layers we identified for texture transfer, finding this method to be inadequate (see Fig. 6). 21 ALGORITHM 1: Point Cloud Correspondence Generation Input: P1, P2: point clouds; M1, M2: convex maps; : primitive transforms; C: centers; dmax = 0.005: max distance threshold Output: R: correspondence map; W: confidence map Function ApplyTransform(p, c, T): ; if contains translation then p Ttrans; end if contains rotation angle θ then c, cos(θ), sin(θ); x, x s, + ; end if contains scaling factor scale then p/scale; end return + c; 0HW 2 ; 0HW ; for unique(M1) do if < 0 or or / M1 or / M2 then continue; end I1 {(y, x) : M1[y, x] = p} ; I2 {(y, x) : M2[y, x] = p} ; Q1 {P1[y, x] : (y, x) I1} ; for (y2, x2) I2 do P2[y2, x2] ; if then // Center the point // Y-axis rotation // Initialize correspondence map // Initialize confidence map // Pixel indices for primitive in map 1 // Pixel indices for primitive in map 2 // 3D points for primitive // Query point from second cloud ApplyTransform(q, C[p], [p]) ; // Apply transformation end Q1 q2 ; arg mini d[i] ; dmin d[i]; if dmin dmax then 1, 1) I1[i] ; (y 1, R[y2, x2] [x 1]; W[y2, x2] 1 min(dmin/dmax, 1) ; // Compute distances to all points // Find nearest neighbor // Get corresponding pixel coordinates // Confidence score end end end return R, W; ALGORITHM 2: Hint Generation from Correspondence Maps Input: Isrc RCHsWs: source image; RHrWr2: correspondence map; RHrWr : confidence map; Mhit {0, 1}HrWr : hit mask Output: RCHsWs : generated hint image Function BilinearSample(I, y, x): // Floor coordinates // Interpolation weights // Scale factors // Initialize hint image // Skip hit pixels // Get correspondence // Get confidence // Skip low-confidence correspondences // Scale to source resolution // Boundary check // Normalized offset C, H, shape(I); clip(x, 0, 1.001), clip(y, 0, 1.001); x0, y0 x, ; x1, y1 min(x0 + 1, 1), min(y0 + 1, 1); wx, wy x0, y0 ; vtop I[:, y0, x0] (1 wx) + I[:, y0, x1] wx; vbot I[:, y1, x0] (1 wx) + I[:, y1, x1] wx; return vtop (1 wy) + vbot wy; λh Hs/Hr, λw Ws/Wr ; 0CHsWs ; for [0, Hr) do for [0, Wr) do if Mhit[y, x] = 1 then continue ; end (xc, yc) R[y, x] ; W[y, x] ; if < 0.1 then continue ; end ysrc yc λh, xsrc xc λw ; ystart λh, yend (y + 1) λh; xstart λw, xend (x + 1) λw; for ys [ystart, yend) do for xs [xstart, xend) do if ys / [0, Hs) or xs / [0, Ws) then continue ; max(yendystart,1) ; max(xendxstart,1) ; end αy ysystart αx xsxstart ysample ysrc + αy λh; xsample xsrc + αx λw; H[:, ys, xs] BilinearSample(Isrc, ysample, xsample); end end end end return H;"
        }
    ],
    "affiliations": [
        "Toyota Technological Institute at Chicago",
        "University of Illinois Urbana-Champaign"
    ]
}