{
    "paper_title": "ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking",
    "authors": [
        "Haofeng Liu",
        "Mingqi Gao",
        "Xuxiao Luo",
        "Ziyue Wang",
        "Guanyi Qin",
        "Junde Wu",
        "Yueming Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 1 8 5 8 0 . 5 0 5 2 : r ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking Haofeng Liu1, Mingqi Gao2, Xuxiao Luo1, Ziyue Wang1, Guanyi Qin1, Junde Wu3, and Yueming Jin1((cid:66)) 1 National University of Singapore, Singapore, Singapore 2 Southern University of Science and Technology, Shenzhen, China 3 University of Oxford, Oxford, United Kingdom haofeng.liu@u.nus.edu, ymjin@nus.edu.sg Abstract. Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex realworld surgical scenarios. In this paper, we introduce ReSurgSAM2, twostage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates diversity-driven memory mechanism that maintains credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2. Keywords: Robotic-assisted surgery Referring segmentation Longrange video object tracking Video-language learning."
        },
        {
            "title": "Introduction",
            "content": "Surgical video scene segmentation is vital in computer-assisted surgery. Through precise identification and differentiation of surgical instruments and tissues, this technology can provide cognitive assistance to surgeons for decision-making [11]. Most existing methods in surgical scene segmentation rely only on video data [24,25]. Despite achieving real-time and satisfactory performance, these systems can only generate semantic masks for all instruments and tissues collectively, without providing surgeons the ability to interactively identify and track specific objects of 2 Liu et al. interest. Referring segmentation, which aims to automatically identify and segment the specific object according to textual expression, is beneficial for various surgical applications [25,19]. Integrated with AR technology, it could enhance surgical education by enabling trainees to interactively explore specific instruments through AR overlays [7,16]. During intraoperative surgery, such ability enables the system to focus on regions of interest specified by surgeons, which can optimize workflow, providing accurate and personalized navigation support, contributing to safer and higher-quality outcomes of patient care. To enable this capability, RSVIS [19] takes the first step to investigate referring instrument segmentation in the surgical domain. They propose videoinstrument synergistic network to learn both video-level and instrument-level knowledge to improve performance. However, this method relies only on shortterm information of three consecutive frames, leading to inherent challenges in long-term tracking. Referring segmentation in surgery still remains underexplored. In natural domains, the typical task is referring video object segmentation (RVOS). The online RVOS methods [20,8] can offer real-time processing, however, they lack robust long-term tracking essential for surgical scenarios, as these methods are typically developed for short videos (under 10 seconds). This limitation is particularly critical in surgical procedures, which generally last long durations in hours, with dynamic scene variations and instrument movement. In contrast, offline RVOS alternatives [21,22] achieve better performance through extended temporal integration but require future frame information, making them unsuitable for intraoperative applications. Recently, Segment Anything Model 2 (SAM2) [13] has gained attention for its interactive framework with satisfactory tracking capabilities, presenting promising potential to enhance the referring segmentation task. By providing visual prompts on initial frames as permanent memory, it can perform consistent tracking with memory attention. However, visual prompts (e.g., bounding boxes or multi-points) rely on the objects presence in the first few frames and impose labeling burdens on surgeons during operations. Since the target object may not be present at the beginning of surgery, such interactions are suboptimal [11]. Instead, textual expressions offer greater flexibility and are the closest form to audio. Integrating textual prompts into SAM2 marks crucial step toward hands-free interaction in surgery [9,19]. Furthermore, identifying the reliable initial frame for tracking is critical for RVOS, as inaccurate segmentation masks can introduce error accumulation, significantly degrading overall performance [3]. Finally, SAM2 employs greedy strategy that selects the nearest frames as memory without assessing their reliability, limiting long-term temporal modeling [10]. To address these challenges, we propose ReSurgSAM2, novel two-stage framework built on SAM2 that sequentially performs text-referred target detection followed by tracking, enabling efficient and accurate RVOS in long-range surgical videos. During the detection stage, we identify the reliable initial frame using our Cross-Modal Spatial-Temporal Mamba (CSTMamba) and Credible Initial Frame Selection (CIFS) strategy. CSTMamba efficiently captures dediReSurgSAM2: Referring Segment Anything in Surgical Video 3 cated spatial-temporal dependencies across video frames while integrating multimodal features, enabling precise specified object detection and segmentation for robust initial frame selection. Leveraging these accurate detection results, CIFS selects the optimal frame for tracking initialization based on confidence. During tracking, our Diversity-Driven Long-term Memory (DLM) mechanism empowers SAM2 to track the object reliably throughout the entire surgical video by conditioning on diverse and dependable memory bank. Extensive experiments on datasets containing instruments and tissues demonstrate significant performance improvements over existing methods, maintaining real-time at 61.2 FPS."
        },
        {
            "title": "2.1 ReSurgSAM2 Framework",
            "content": "SAM2 [13] extends SAM [6] to video domains by incorporating temporal memory attention while preserving its segmentation capabilities. It implements shortterm memory mechanism through queue, which conditions the current frame features on both the permanent initial frame and nearest predictions through greedy selection. For each frame, the mask decoder generates predictions with two key scores: the intersection-over-union (IoU) score and the occlusion score. The IoU score estimates the alignment between the prediction with ground truth, while the occlusion score employs signed confidence scheme - positive values signify object presence, negative values indicate absence, and the magnitude reflects confidence. This dual-scoring system enables object tracking even during occlusions. While SAM2 demonstrates promising performance in general video domains, it faces limitations in adapting to surgical RVOS, including visionlanguage integration, reliable initial frame identification, and long-term tracking. To overcome the above limitations, we propose ReSurgSAM2, specialized two-stage framework for surgical applications that seamlessly integrates textreferred target detection with tracking, as illustrated in Fig. 1. Given the t-th frame ft R3HW from an image stream and its linguistic expression e, we separately employ SAM image encoder and the frozen CLIP text encoder with trainable MLP to extract features, with our goal of obtaining the segmentation mask mt RHW for the text-referred target. In the first stage, our model enhances detection reliability by utilizing CSTMamba and mask decoder to generate high-fidelity segmentation mask. The scores of the mask are subsequently fed into the CIFS, which performs credible frame selection prepared for the tracking phase to mitigate error accumulation. Upon identification of the optimal initial frame by CIFS, the model switches to the tracking stage, where the prompt encoder accepts the CLS token from text features, and the model ensures reliable and consistent object tracking throughout the video by integrating vanilla short-term memory with long-term memory using the DLM. 2."
        },
        {
            "title": "Identify the Reliable Initial Frame with CSTMamba and CIFS",
            "content": "Cross-Modal Spatial-Temporal Mamba (CSTMamba): Referring segmentation using single frame often yields suboptimal result, and propa4 Liu et al. Fig. 1: Overview of ReSurgSAM2. The model begins with the text-referred target detection using CSTMamba to provide credible frames for selection using CIFS. Upon detecting the initial frame, CIFS activates the tracking stage, in which DLM offers diverse and reliable memory for consistent long-term tracking. gating this result as an initial reference for tracking potentially causes error accumulation. Therefore, precise target detection is crucial for our two-stage framework. While transformer-based approaches can enhance segmentation by leveraging self-attention, this mechanism suffers from quadratic complexity [17], limiting the real-time surgical applications. Recently, Mamba [4] has emerged as promising alternative with linear complexity and selective information propagation. STMamba [23] leverages these advantages for video segmentation; however, it lacks cross-modal capabilities for RVOS and is constrained by Mambas linear scanning mechanism, which restricts its ability to capture fine-grained pixel-level information. To address these limitations, we propose the CSTMamba, which integrates sensory memory bank, the CSTMamba block, cross-modal attention mechanism to facilitate comprehensive cross-modal spatial-temporal modeling. To enable temporal interaction, the sensory memory bank stores the most recent two frame features extracted from the image encoder. Then, the CSTMamba takes the language features and video features from the current frame and sensory memory as input for cross-model spatial-temporal modeling, and outputs the fused image feature E(ft) and CLS token. The video input can be written as [E(ft), E(ft1), E(ft2)] R3H 256, where E() is the image encoder and [] is concatenation, and are the feature height and width. The CSTMamba block, designed for comprehensive spatial-temporal modeling, is illustrated in Fig. 1. It integrates STMamba with 7 7 2D depth-wise ReSurgSAM2: Referring Segment Anything in Surgical Video 5 convolution (DWConv) to capture global features with selective scanning and fine-grained local features with an expanded receptive field. Building upon this foundation, an inverted bottleneck [15] expands the MLP block to four times the input dimension, effectively enhancing feature representation by utilizing the enlarged middle layer to improve spatial-temporal interactions. Additionally, bidirectional cross-modal attention mechanismstext-to-vision (T2V) and vision-to-text (V2T) [18]facilitate cross-modal interactions. The CSTMamba processes visual-temporal information and cross-modal signals simultaneously, generating enriched spatial-temporal features and fused CLS token with strong cross-modal representations for final mask prediction. Credible Initial Frame Selection (CIFS): When automatically selecting an initial reference frame for tracking, it is essential to identify highly reliable frame to mitigate error accumulation [10]. However, in surgical environments, the small inter-class similarity among instruments and among tissues increases the risk of false detections, leading to unreliable segmentation. To address this challenge, we implement CIFS that requires the model to predict the object as present with high confidence based on IoU score and occlusion score across Nw consecutive frames before frame selection begins. Before selection, it uses the sliding window to detect the qualified frames, which can be formulated as: = {fj [t Nw + 1, t] iouj > δiou sigmoid(oj ) > δo}, (1) where is the sliding window, oj and iouj are the predicted occlusion score and IoU score for the t-th frame, δiou and δo are their respective thresholds, and sigmoid() maps the score to range [0, 1]. Upon = Nw, the one with the highest IoU score is selected as the initial reference among these Nw qualifying frames. With the selection of an optimal and credible initial frame, ReSurgSAM2 enters its second stage, where it performs robust tracking by leveraging this reference frame to propagate predictions throughout the remaining sequence, ensuring both semantic fidelity and temporal consistency."
        },
        {
            "title": "2.3 Diversity-driven Long-term Memory",
            "content": "In surgical environments, videos typically have long durations with dynamic scene variations and instrument movement. However, SAM2s greedy strategy of selecting only the most recent frames as memory hinders effective long-term tracking, leading to redundancy and potential viewpoint overfitting, thereby limiting its ability to capture anatomical changes. To overcome this challenge, we propose the DLM mechanism that enhances SAM2s vanilla memory bank by strategically selecting frames from candidate pool. This mechanism enriches the memory bank by using the candidate pool to extend the temporal coverage range and collecting the frames that capture diverse spatial-temporal information to mitigate viewpoint overfitting. Additionally, it ensures the inclusion of high-confidence frames to minimize error accumulation. The proposed DLM mechanism updates the candidate pool in inference as: = {ft iout > γiou}, (2) 6 Liu et al. Table 1: Dataset statistics for Ref-EndoVis17 and Ref-EndoVis18. Dataset Ref-EndoVis17(tool) Ref-EndoVis18(tool) Ref-EndoVis18(tissue) Training Testing Sequence Frame Object Pair Sequence Frame Object Pair 2265 1384 2100 1639 1639 4873 3787 2995 900 596 596 20 34 25 10 15 7 7 11 3 4 4 where is the candidate pool, ft is the t-th frame, iout is its predicted IoU score, and γiou is confidence threshold. Each element in is indexed by pi, where pi refers to the i-th candidate. This mechanism selects high-confidence frames as memory candidates for mitigating error propagation. When the candidate pool reaches its capacity Np, we store the most diverse candidate based on its cosine similarity to the latest long-term memory frame: = arg min piP (pi) (lk) (pi)M (lk) , (3) where () is the memory encoder, pi is i-th candidate frame, lk is the latest frame in long-term memory bank L, and is the selected frame. After selection, the pool is cleared to extend the temporal coverage range of the long-term memory, and the updated memory bank is utilized for the (t + 1)-th frames memory attention mechanism. To enhance efficiency, we maintain queue with capacity Nl for long-term memory, keeping the initial frame permanently in the long-term memory. By concatenating SAM2s vanilla short-term memory with our long-term memory using the DLM mechanism, ReSurgSAM2 maintains memory bank that is reliable and diverse, boosting consistent long-term tracking."
        },
        {
            "title": "3.1 Dataset and Implementation Details",
            "content": "The experiments were conducted with Ref-EndoVis17 and Ref-EndoVis17 building upon EndoVis17 [2], EndoVis18 dataset [1] and RSVIS [19]. The EndoVis17 comprises 3000 frames across 10 sequences, including eight training sequences, eight test sequences from identical scenes, and two additional test sequences, with Table 2: Quantitative comparison with state-of-the-art methods. Setting Method Ref-EndoVis17(tool) Ref-EndoVis18(tool) Ref-Endovis18(tissue) &F &F 42.3 53.78 ReferFormer [21] Offline 62.41 62.28 62.55 71.09 70.96 71.23 61.84 32.3 Offline 60.97 60.76 61.18 67.56 67.79 67.33 63.53 71.48 55.58 MUTR [22] 22.1 RSVIS [19] Online 61.22 61.37 61.07 68.35 68.55 68.15 65.69 72.91 58.47 25.6 OnlineRefer [20] Online 60.32 60.29 60.34 72.19 71.88 72.50 70.56 77.58 63.55 RefSAM [8] Online 63.56 63.77 63.35 72.86 73.40 72.31 71.90 77.66 66.14 25.4 Online 77.73 77.77 77.69 80.62 80.94 80.31 75.09 80.93 69.25 61.2 ReSurgSAM2 69.9 &F FPS ReSurgSAM2: Referring Segment Anything in Surgical Video 7 Fig. 2: Qualitative comparison in Ref-EndoVis17(left) and Ref-EndoVis18(right). instrument labels. The EndoVis18 dataset consists of 15 sequences with comprehensive scene segmentation annotations. As introduced in RSVIS, both datasets were reannotated with consistent instance-specific labels to facilitate RVOS. Building upon RSVIS [19], we performed meticulous refinement to address inconsistencies and omissions in instrument labeling that would otherwise compromise experimental validity. We further enriched the datasets by incorporating tissue-specific annotations from EndoVis18, including kidney parenchyma, covered kidney, and small intestine. For Ref-EndoVis17, we merged sequences from the identical scenes to prevent cross-contamination between training and test sets, with sequences 2, 5, and 6 designated as the test set. Following RSVIS [19], we allocated sequences 2, 5, 9, and 15 as the test set for Ref-EndoVis18. This division ensures balanced object distribution across training and testing. Table 1 presents statistics for both datasets, with \"pair\" denoting the text-mask pair. ReSurgSAM2 employs the Hiera-small backbone [14] initialized with SAM2 pre-trained weights [13], with an input size of 512. During training, we follow SAM2 to conduct prompt segmentation by loading three frames for text-referred object detection, followed by seven frames for tracking. The model is trained for 30 epochs utilizing the same training strategies as SAM2. For inference, unlike RSVIS, we generate text expressions at the first appearance of each object to accommodate the whole surgical video. The hyperparameters are set as follows: δo = 0.9, δiou = 0.7, γiou = 0.95, Nw = 5, Np = 5 and Nl = 4. Unlike semantic segmentation using challenge IoU [2,1], RVOS tracks the object across the whole video, including when it is occluded. Therefore, for evaluation metrics, we adopt and [12] for accuracy, where assesses region accuracy and evaluates boundary accuracy following RVOS [5], with &F representing their mean, and the frame-per-second (FPS) for efficiency. All metrics are higher-is-better. All experiments utilized the same training data on an NVIDIA A6000 GPU. Liu et al."
        },
        {
            "title": "3.2 Comparison and Ablation Study",
            "content": "Comparison: To validate the effectiveness of ReSurgSAM2, we conducted comprehensive comparisons against state-of-the-art methods summarized in Table 2. The comparison methods include offline methods (ReferFormer and MUTR) and online methods (RSVIS, OnlineRefer, and RefSAM). Offline methods achieve stable performance by processing 64 frames concurrently during inference, thus reducing false detections. In contrast, RSVIS only relies on short-term information, leading to suboptimal long-term tracking. While OnlineRefer and RefSAM exhibit modest long-term tracking capabilities by query propagation, the performance remains suboptimal on long-range sequences in Ref-EndoVis17. In comparison, ReSurgSAM2 shows superior cross-modal and long-term tracking capabilities in RVOS, with substantial &F improvements: 14.17 on Ref-EndoVis17, 7.76 on Ref-EndoVis18 tool, and 3.19 on Ref-EndoVis18 tissue datasets. The qualitative comparison with RSVIS and RefSAM is illustrated in Fig. 2. RSVIS lacks robust instrument discrimination in complex scenarios, causing incomplete segmentation. Both RSVIS and RefSAM exhibit limited tracking stability during rapid object movements and scene variations, due to their limited long-term modeling. In contrast, ReSurgSAM2, equipped with robust initialization and diverse long-term memory, performs reliable and consistent tracking. Ablation Study: We conduct comprehensive ablation studies on Ref-EndoVis17 to evaluate the effectiveness of each proposed component, with results presented in Table 3. For ablation, the two-stage RVOS framework uses the first detected frame with iout > 0.7 and sigmoid(ot) > 0.9 as reference to activate tracking of stage 2. This design helps capture short-term temporal dependencies, leading to &F gain of 2.64. Integrating CSTMamba strengthens spatial-temporal referring segmentation to generate more reliable reference for tracking, achieving 4.77 improvement in &F. Furthermore, the CIFS strategy further enhances &F by 6.14, as it selects more reliable reference as memory, mitigating error accumulation. DLM further enhances vanilla memory mechanism with long-term temporal modeling by maintaining diverse and long-range memory bank, contributing 3.03 boost in &F. To validate DLMs effectiveness, we explored various memory bank designs: an extended short-term memory based on the vanilla memory bank, and long-term memory with interval sampling every five frames (storing three frames each), as shown in Table 4. By integrating all proposed components, ReSurgSAM2 ultimately achieves 77.73 in &F while maintaining real-time performance at 61.2 FPS. Stage 2 CSTMamba CIFS DLM &F FPS 61.15 61.46 60.84 70.1 63.79 63.77 63.82 68.2 68.56 68.51 68.61 67.5 74.70 74.67 74.72 63.1 77.73 77.77 77.69 61.2 Method &F Vanilla 74.70 74.67 74.72 Extended 74.68 74.64 74.72 Interval 75.32 75.27 75.37 DLM 77.73 77.77 77. Table 3: Ablation studies on Ref-Endovis17. Table 4: Ablation on memory bank design. ReSurgSAM2: Referring Segment Anything in Surgical Video"
        },
        {
            "title": "4 Conclusion",
            "content": "This paper presents ReSurgSAM2, two-stage framework via credible long-term tracking for surgical referring segmentation. While existing methods are limited in efficiency and long-term tracking, our approach addresses these limitations through reliable initial frame identification and long-term memory mechanism building upon SAM2. Extensive experiments on surgical datasets demonstrate that ReSurgSAM2 significantly outperforms existing methods, offering practical and efficient solution for real-time surgical video analysis."
        },
        {
            "title": "References",
            "content": "1. Allan, M., Kondo, S., Bodenstedt, S., Leger, S., Kadkhodamohammadi, R., Luengo, I., Fuentes, F., Flouty, E., Mohammed, A., Pedersen, M., et al.: 2018 robotic scene segmentation challenge. arXiv preprint arXiv:2001.11190 (2020) 2. Allan, M., Shvets, A., Kurmann, T., Zhang, Z., Duggal, R., Su, Y.H., Rieke, N., Laina, I., Kalavakonda, N., Bodenstedt, S., et al.: 2017 robotic instrument segmentation challenge. arXiv preprint arXiv:1902.06426 (2019) 3. Cuttano, C., Trivigno, G., Rosi, G., Masone, C., Averta, G.: Samwise: Infusing wisdom in sam2 for text-driven video segmentation. arXiv preprint arXiv:2411.17646 (2024) 4. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023) 5. Khoreva, A., Rohrbach, A., Schiele, B.: Video object segmentation with language referring expressions. In: Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part IV 14. pp. 123141. Springer (2019) 6. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 40154026 (2023) 7. Kovoor, J.G., Gupta, A.K., Gladman, M.A.: Validity and effectiveness of augmented reality in surgical education: systematic review. Surgery 170(1), 8898 (2021) 8. Li, Y., Zhang, J., Teng, X., Lan, L., Liu, X.: Refsam: Efficiently adapting segmenting anything model for referring video object segmentation. arXiv preprint arXiv:2307.00997 (2024) 9. Liu, H., Zhang, E., Wu, J., Hong, M., Jin, Y.: Surgical sam 2: Real-time segment anything in surgical video by efficient frame pruning. In: Advancements In Medical Foundation Models: Explainability, Robustness, Security, and Beyond (2024) 10. Liu, Y., Yu, R., Yin, F., Zhao, X., Zhao, W., Xia, W., Yang, Y.: Learning qualityaware dynamic memory for video object segmentation. In: European Conference on Computer Vision. pp. 468486. Springer (2022) 11. Moglia, A., Georgiou, K., Georgiou, E., Satava, R.M., Cuschieri, A.: systematic review on artificial intelligence in robot-assisted surgery. International Journal of Surgery 95, 106151 (2021) 12. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., SorkineHornung, A.: benchmark dataset and evaluation methodology for video object segmentation. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 724732 (2016) 10 Liu et al. 13. Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024) 14. Ryali, C., Hu, Y.T., Bolya, D., Wei, C., Fan, H., Huang, P.Y., Aggarwal, V., Chowdhury, A., Poursaeed, O., Hoffman, J., et al.: Hiera: hierarchical vision transformer without the bells-and-whistles. In: International conference on machine learning. pp. 2944129454 (2023) 15. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: Inverted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 45104520 (2018) 16. Sheik-Ali, S., Edgcombe, H., Paton, C.: Next-generation virtual and augmented reality in surgical education: narrative review. Surgical technology international 33 (2019) 17. Tay, Y., Dehghani, M., Bahri, D., Metzler, D.: Efficient transformers: survey. ACM Computing Surveys 55(6), 128 (2022) 18. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) 19. Wang, H., Yang, G., Zhang, S., Qin, J., Guo, Y., Xu, B., Jin, Y., Zhu, L.: Videoinstrument synergistic network for referring video instrument segmentation in robotic surgery. IEEE Transactions on Medical Imaging 43(12), 44574469 (2024) 20. Wu, D., Wang, T., Zhang, Y., Zhang, X., Shen, J.: Onlinerefer: simple online baseline for referring video object segmentation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 27612770 (2023) 21. Wu, J., Jiang, Y., Sun, P., Yuan, Z., Luo, P.: Language as queries for referring video object segmentation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 49744984 (2022) 22. Yan, S., Zhang, R., Guo, Z., Chen, W., Zhang, W., Li, H., Qiao, Y., Dong, H., He, Z., Gao, P.: Referred by multi-modality: unified temporal transformer for video object segmentation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38, pp. 64496457 (2024) 23. Yang, Y., Xing, Z., Yu, L., Huang, C., Fu, H., Zhu, L.: Vivim: video vision mamba for medical video segmentation. arXiv preprint arXiv:2401.14168 (2024) 24. Yue, W., Zhang, J., Hu, K., Xia, Y., Luo, J., Wang, Z.: Surgicalsam: Efficient class promptable surgical instrument segmentation. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 38, pp. 68906898 (2024) 25. Zhou, Z., Alabi, O., Wei, M., Vercauteren, T., Shi, M.: Text promptable surgical instrument segmentation with vision-language models. Advances in Neural Information Processing Systems 36, 2861128623 (2023)"
        }
    ],
    "affiliations": [
        "National University of Singapore, Singapore, Singapore",
        "Southern University of Science and Technology, Shenzhen, China",
        "University of Oxford, Oxford, United Kingdom"
    ]
}