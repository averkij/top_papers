{
    "paper_title": "Robot Learning from a Physical World Model",
    "authors": [
        "Jiageng Mao",
        "Sicheng He",
        "Hao-Ning Wu",
        "Yang You",
        "Shuyang Sun",
        "Zhicheng Wang",
        "Yanan Bao",
        "Huizhong Chen",
        "Leonidas Guibas",
        "Vitor Guizilini",
        "Howard Zhou",
        "Yue Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \\href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details."
        },
        {
            "title": "Start",
            "content": "Jiageng Mao, Yanan Bao Huizhong Chen Sicheng He Hao-Ning Wu Yang You Shuyang Sun Zhicheng Wang Leonidas Guibas, Vitor Guizilini Howard Zhou Yue Wang Google DeepMind USC Stanford Toyota Research Institute 5 2 0 2 0 1 ] . [ 1 6 1 4 7 0 . 1 1 5 2 : r Fig. 1: PhysWorld: framework for robot learning from video generation. Given an image and task prompt as inputs (column #1), our method generates task-conditioned video (column #2) and reconstructs the underlying physical world to ground generated visual demonstrations into physically feasible robot actions (column #3), enabling zero-shot robotic manipulation in the real world (column #4). Abstract We introduce PhysWorld, framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given single image and task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit the project webpage for details. I. INTRODUCTION Recent advances in generative models enable the synthesis of photorealistic videos directly from images and language instructions. Trained on large-scale Internet data, video generation models exhibit strong generalization across diverse scenarios. For robotics, such models offer powerful source of visual guidance for manipulation. Given robots observation and task instruction, video generator can produce demonstration that depicts task completion. These generated videos inherently capture object dynamics and embodiment motions, which can be leveraged to learn generalizable robotic manipulation policies. Despite the great promise of video generation, translating generated pixel motions into executable robotic actions remains highly challenging. Previous works [1][5] learn inverse dynamics or policy models to align generated video frames with real robotic actions. Yet, such methods generally rely on large-scale real-world demonstrations for alignment, while collecting them at scale is costly and labor-intensive. Other methods [6][8] propose to extract robotic actions by directly following visual cues, e.g. flows, sparse tracks, or object poses, from generated videos. Nevertheless, directly retargeting video motions to robots neglects underlying physical constraints, often leading to inaccurate manipulations. We argue that the key bottleneck of bridging generated videos and robotic actions lies in physical feasibility. Video generation, despite its generalization power, only provides visual plausibility rather than physical accuracy for robotic tasks, whereas robots operating in the real world require physically accurate actions to interact with objects correctly. We tackle this dilemma by introducing proxy physical world model built from generated videos. This world model provides realistic physical feedback, enabling scalable robot learning to imitate generated video motions in physically consistent manner. To this end, we propose PhysWorld, framework for physically grounded robot learning from video generation. The core of PhysWorld lies in the synergy between physical world reconstruction and video generation: video generation provides pixel-level visual guidance for task execution, while the physical world model offers realistic feedback for learning from the generated visual guidance. In particular, given single RGB-D image and task prompt, our method first generates task-conditioned video depicting how the task is completed visually. Next, we propose novel method for constructing physically interactable scene from the video. Finally, we introduce an object-centric residual reinforcement learning approach that bridges video generation and physical world reconstruction, producing physically accurate robotic actions. Our framework requires only single RGB-D image and language command, yet outputs executable actions that follow the instruction to complete the task. By explicitly modeling physics, PhysWorld eliminates the need for realworld data collection and achieves zero-shot generalizable robotic manipulation, while significantly improving manipulation accuracy over previous methods. We evaluate PhysWorld on diverse set of real-world robotic manipulation tasks. Experimental results show that combining video generation with physical world modeling yields substantial improvements in accuracy across all tasks. PhysWorld enables physically grounded and generalizable robotic manipulation, consistently outperforming existing approaches by large margin. We will release code and project resources to facilitate further research. II. RELATED WORKS Video generation for robotics. Video generation [9][11] holds great promise for robotics. It has been explored for goal generation [12], planning [2], [13], dynamics learning [14], [15], and policy learning [1], [3][8]. To extract robotic actions from generated videos, several works [1] [5], [14], [15] train action models from generated video frames using large amounts of real robotic demonstrations, but collecting such data is costly. In contrast, PhysWorld removes the need for real-world data collection and enables zero-shot robotic manipulation. Other approaches [6] [8] directly extract actions by following visual cues from generated videos, such as optical flows [6], sparse tracks [7], or object poses [8]. However, pixel-level imitation neglects physical plausibility and often results in inaccurate realworld manipulation. PhysWorld instead introduces proxy physical world model, allowing agents to imitate generated video motions with physical feedback, thereby improving the accuracy and feasibility of real-world robotic manipulation. Robot learning from videos. Videos contain rich motion and task information that can be leveraged for training robotic policies. Researchers tackle this problem by learning transferable representations [16][26], tracking embodimentagnostic motion representations [27][40], real-to-sim [41] [44], or reinforcement learning [45], [46]. PhysWorld shares similar insights with other works on object pose tracking [8], [36], real-to-sim reconstruction [43], [44], [46], [47], and reinforcement learning [43], [46]. However, these methods generally rely on ad-hoc laboratory settings for real-to-sim reconstruction and human demonstration collection, which limits their generalization to in-the-wild generated videos that often contain motion blur or visual hallucinations. In contrast, PhysWorld only requires single generated video for real-to-sim reconstruction and can effectively learn physically accurate robotic actions from generated videos. Real-to-sim-to-real. Real-to-sim-to-real methods reconstruct physical scene from observations and embed it in simulators for policy learning. To obtain complete object and scene textured meshes [48] or Gaussian splats [49], prior works [50][62] require dedicated multi-view captures for reconstruction, making them difficult to apply to monocular generated videos. In contrast, PhysWorld leverages generative priors to model physical scenes from single-view video, enabling physical world modeling directly from generated videos without additional multi-view capture. III. METHOD We study the problem of open-world robotic manipulation. Our system takes as input an RGB-D image and languagebased task command, and outputs physically feasible robotic actions to complete the task. At its core, our approach unifies video generation and physical world modeling: video generation provides pixel-level visual guidance for task execution, while the physical world model offers realistic feedback for learning from the generated visual guidance. In Section III-A, we describe how to model the physical world from generated videos, and in Section III-B, we detail how to learn robotic actions from the physical world model. A. Physical World Modeling from Video Generation Video generation models trained on Internet data have demonstrated remarkable capability in generating visual demonstrations across diverse tasks and scenarios. However, these generated demonstrations only provide pixel-level guidance for task completion, while robots operate in the 3D space and are under physical constraints. To bridge this gap, we propose to first model the physical world from generated videos, transforming pixel-level guidance into physically grounded representations that can be executed by robots as accurate and feasible actions. Such transformation is nontrivial, as generated videos provide only partial observations of the physical world and often contain visual artifacts. In this paper, we introduce novel method that effectively tackles this problem with generative priors. Specifically, given generated video, we first estimate 4D spatiotemporal representation. We then generate textured meshes for objects and the background, endow them with physical properties, and align them with the 4D representation to construct the physical scene. Finally, we extract 4D motions from the video as targets for policy learning. The details of each step are presented in the following sections. Video generation. Our method supports variety of video generation models [9][11], [63], as long as they are imageto-video models with text control. Given an input image I0 and task command, video generation model produces future frames {I1, . . . , IT } demonstrating how the task Fig. 2: PhysWorld pipeline. Given an RGB-D image and task prompt, our framework (i) generates task-conditioned video, (ii) reconstructs geometry-aligned 4D representation from the generated video, (iii) generates textured object and background meshes, (iv) assembles them into physically interactable scene through property estimation, gravity alignment, and collision optimization, (v) learns object-centric residual RL policies that transform visual demonstrations into feasible robotic actions, and (vi) deploys to the real world. will be completed. In this work, we primarily use Veo3 [63] for video generation due to its high output quality, while additional models are evaluated in the ablation studies. Geometry-aligned 4D reconstruction. Generated videos provide pixel-level demonstrations, and converting them into 4D spatio-temporal representations is necessary for robots that operate in the physical world. To obtain an accurate structure and motion estimate from videos, we initialize the dynamic scene reconstruction with MegaSaM [64], which produces temporally consistent depth estimate {D } for each frame. However, MegaSaMs estimates are not well aligned with real-world metric scales. To address this, we leverage the real-world captured depth image D0 to calibrate the outputs. Specifically, we solve for global scale and shift (α, β) such that αD 0 + β D0 over all valid pixels, by minimizing robust regression objective: 0, . . . , min α,β (cid:88) pΩ wp (cid:0)α 0(p) + β D0(p)(cid:1) , (1) t}T where Ω denotes the set of valid pixels and wp are Huber weights that downweight outliers. The calibrated parameters (α, β) are then applied to all frames {D t=0, producing metric-aligned depth maps {Dt}T t=0 that enable consistent 4D spatio-temporal reconstruction of the scene geometry. With known camera parameters, we can also obtain dynamic point clouds {Pt}T t=0 through un-projection. Textured mesh generation. 4D reconstruction provides structures and motions from generated videos, but the depth or point cloud representation is not directly usable for physics simulation. Mesh is the standard geometry representation in simulators. Previous real-to-sim methods typically rely on pipelines such as Polycam or BundleSDF [65] to reconstruct these meshes from complete multi-view scans. However, pipelines are unsuitable for generated monocular videos, where objects and scenes are only partially visible. To address this challenge, we propose generative approach for recovering complete object and background meshes. Given the first image and its point cloud geometry I0, P0, we first separate objects from the background in I0. The object pixels are removed, and the missing regions are filled using masked image inpainting [66], resulting in completed background imagery and individual object crops o. For each object, we apply an image-to-3D generator [67] to o, producing canonical textured mesh o. For background reconstruction, we require geometry corresponding to the completed background image b. This means inferring geometry in the regions originally occluded by objects. We address this with an object-on-ground assumption: objects are supported by the background, so their occluded regions are either planar supporting surfaces or extend to infinity (bounded by scene limits). Concretely, we cast camera rays through occluded pixels and compute their nearest intersections with either the supporting plane or scene boundaries, thereby filling in with consistent geometry. With b, b, we then reconstruct the background mesh via height-map triangulation and apply as the texture. Finally, object and background meshes {M o, b} are assembled into complete scene by aligning and resizing them to match the observed point cloud P0 through registration. Physical scene reconstruction and alignment. From the generated videos, we obtain decomposed scene meshes {M o, b}. To make these meshes physically interactable, Fig. 3: Qualitative evaluation of physical scene modeling from generated videos. three additional steps are required: physical property estimation, gravity alignment, and collision optimization. Physical property estimation assigns appropriate physical parameters, such as mass and friction coefficients, to scene components. Inspired by [68], we leverage commonsense knowledge from vision-language models (VLMs) to estimate these properties. Specifically, we query VLM with the object category to obtain typical physical parameters, and assign the predicted values to each object and the background for subsequent physical simulation. Gravity alignment is to transform {M o, b} from camera the scene is consistent with the to world frame so that world gravity axis, which is crucial for physically plausible simulation. We estimate the ground plane normal from segmented plane points using RANSAC, and compute the minimal rotation that aligns with the world up axis ez: Rgrav = exp(cid:0)[u] θ(cid:1), θ = arccos(nez), = ez ez , (2) where [u] is the skew-symmetric matrix of u. Applying Rgrav to all meshes aligns the scene with gravity in the world frame for subsequent physical simulation. Collision optimization is to optimize the placement of each object with respect to the background mesh so that all objects maintain minimum clearance to avoid initial collisions. We voxelize the background mesh into signed distance field (SDF) ϕbg. For each object o, let Vo = {vo,1, . . . , vo,No} be its mesh vertices. We introduce vertical translation τo along the gravity-opposing axis and solve min {τo} (cid:88) o"
        },
        {
            "title": "1\nNo",
            "content": "No(cid:88) (cid:104) i=1 max(cid:0)0, ϕbg(vo,i + τo ez)(cid:1)(cid:105)2 , (3) where ez is the unit z-axis. This objective penalizes penetrations, i.e., negative SDF values, and is minimized by gradient descent using Adam with gradient clipping and early stopping. This procedure ensures that all objects are adjusted relative to the background so that no initial collisions occur and consistent clearance is preserved for simulation. Finally, we obtain physically interactable digital twin from the generated video. This physical model is essential for the subsequent learning process, as it provides the physically grounded feedback required to transform visual demonstrations into executable robotic actions. B. Object-Centric Learning from the Physical World Model With the physical world model established, the core step is to learn robotic policy that can follow the generated video demonstrations. Video generation produces two types of motion: embodiment motion and object motion. Prior methods [46] primarily retarget embodiment motions, but this often incurs high errors due to inaccurate motion transfer. The issue is further exacerbated for generated videos, which frequently contain hallucinated robots or human hands. In contrast, object motions are less prone to such artifacts and provide clearer visual guidance for task execution. Motivated by this, we focus on object-centric learning and introduce residual reinforcement learning approach that tracks object motions under physical constraints. Learning targets. Transforming generated visual demonstrations into 4D spatio-temporal learning objectives is necessary for training robotic policies. The commonly used learning objectives are optical flow [6], object tracks [7], and object poses [8]. In this paper, we adopt object poses as tracking targets, since object pose estimation is generally more robust than other motion representations. Our framework also supports other forms of motion supervision, which we leave for future exploration. Given the estimated 4D scene t=0 and {Pt}T representations {Dt}T t=0, together with object meshes o, we use FoundationPose [69] to recover perframe object poses {xo = [po , qo ]}T t=0, (4) R4 is its R3 is the object position and qo where po orientation quaternion. These object pose trajectories {xo } are incorporated as supervision for policy learning, enabling the robot to track object motions in generated videos. Residual reinforcement learning. straightforward approach [8] to tracking object poses is to combine grasping model [70] for object pickup with motion planner [71] for subsequent placement. However, this strategy often struggles in complex manipulation tasks: grasping itself is prone to failure, and motion planning can also fail when initialized from improper poses. As result, completing task may require repeated grasping and planning attempts, leading to inefficiency and reduced reliability. Reinforcement learning is promising alternative that can learn robust policies from physical feedback, but it often requires carefully designed rewards and long training time to converge. To address this, we propose residual reinforcement learning method that Fig. 4: Quantitative evaluation of PhysWorld on real-world manipulation tasks. ot = (cid:2)xee and xobj combines the merits of both paradigms: grasping and motion planning provide baseline actions that narrow the search space, while an RL policy learns residual corrections on top of the baseline, enabling robust adaptation under feedback from the physical world model. Formally, given observation ot, the executed action is at = abase + πθ(ot), (5) where abase is the baseline action from grasping and planning as in [8], and πθ(ot) is the residual policy that learns corrective adjustments. This residual formulation accelerates policy learning and improves robustness by leveraging feedback from the physical world model. Importantly, the success of the baseline itself is not required, since the learned residuals can rectify imperfect baseline actions to achieve task success. Observation and action space. We adopt state-based policy for efficient learning. At each step t, the policy πθ(ot) observes , xobj , τt, xo , xgrasp, dpre, xbase (cid:3), (6) where xee are the current end-effector and object poses, τt [0, 1] is the normalized time index, xo is the target object pose from the generated video. {xgrasp, dpre, xbase } are baseline actions from grasping and planning: xgrasp is grasp proposal, dpre is pre-grasp offset, and xbase is the planned end-effector pose at time t, The policy outputs residual action [pt, ωt] consisting of translation pt R3 and rotation ωt R3. The executed command refines the baseline pose xbase : = pbase pcmd = [pbase + pt, qcmd , qbase = exp([ωt]) qbase , qcmd = [pcmd where xbase output end-effector pose command for robotic control. ], and xcmd , (7) ] is the Rewards. We aim for simple rewards that can generalize to diverse tasks. Specifically, an object tracking reward rtrk encourages the robot to align the object with its target pose from the video: = wposekpospobj rtrk po 2 + woriekoriqobj qo 2 . (8) grasp reward rgrasp ensures stable grasp and movement by penalizing excessive distance between the end-effector and the object when grasping and holding the object: rgrasp (cid:104) = wgrasp 1 pee pobj 2 > τ (cid:105) , (9) where pee threshold, and 1[] is the indicator function. is the end-effector position, τ is distance"
        },
        {
            "title": "A planning reward rplan",
            "content": "discourages infeasible actions by assigning negative reward when inverse kinematics or motion planning fail. We train the policy πθ(ot) within the physical world model using the above reward terms, and adopt PPO [72] as the learning algorithm. Leveraging baseline actions significantly accelerates convergence, as the policy only needs to learn residual corrections. IV. EXPERIMENTS Our experiments aim to evaluate the efficacy of PhysWorld as framework for unifying video generation and physical world modeling, quantify its ability to generalize without task-specific robot demonstrations, and analyze key design choices and limitations. To this end, we organize our study to answer the following empirical questions, in order: (Q1) Video Generation: Does video generation enable more generalizable robotic manipulation? (Sec. IV-A) (Q2) World Modeling: Does physical world modeling improve robustness in manipulation tasks? (Sec. IV-B) (Q3) Learning: Does object-centric residual RL enhance policy effectiveness compared to other methods? (Sec. IV-C) A. Video Generation Enables Generalizable Manipulation Qualitative evaluation. To answer whether video generation enables more generalizable robotic manipulation, we evaluate PhysWorld on diverse set of real-world manipulation tasks, including: 1. Wipe the whiteboard; 2. Water the flowers; 3. Put the book in the bookshelf; 4. Pour the fish from the pan onto the plate; 5. Put the lid on the pot; 6. Put the spoon in the pan; 7. Put the shoe in the shoebox; 8. Pour the candies from the spoon onto the plate; 9. Sweep the paper scraps into the dustpan; 10. Pour the tomato from the pan onto the plate. qualitative evaluation of PhysWorld on real-world manipulation tasks is shown in Figure 5. The generated, task-conditioned videos provide rich, task-level visual guidance across diverse scenes, which our physical world model then grounds into executable actions, requiring no additional robot data and enabling zero-shot robotic manipulation in the real world. Video generation quality. To analyze how video generation quality impacts downstream manipulation, we compare 4 image-to-video models: Veo3 [63], Tesseract [11], CogVideoX1.5-5B [9], and Cosmos-2B [10], on the same Fig. 5: Qualitative evaluation of PhysWorld on real-world manipulation tasks. TABLE I: Generation quality of different video generation models. We measure the ratio of usable videos among all generated videos. Models Usable ratio (%) Veo3 [63] Tesseract [11] CogVideoX1.5-5B [9] Cosmos-2B [10] 70% 36% 4% 2% set of tasks. For each combination of model and task, we generate 10 videos and compute the fraction that are usable, i.e., those from which object poses can be recovered robustly. Table reports the usable-video ratio across tasks. Veo3 achieves the highest overall ratio, and robotic data finetuning (e.g., Tesseract) tends to outperform generic generators. These results indicate that higher-quality, task-consistent video generation is necessary for reliable manipulation. B. World Modeling Improves Manipulation Robustness Physical scene reconstruction quality. Figure 3 shows the reconstructed models from generated videos. Our method integrates geometry-aligned 4D reconstruction with generative priors to recover the underlying physical scenes from monocular inputs. The resulting scenes are geometryconsistent and physically interactable, providing reliable physical feedback for robot learning. Effectiveness of world modeling. To evaluate the effectiveness of introducing physical world models, we evaluate 10 real-world manipulation tasks, each with 10 rollouts, and report the success rate of each task. We compare our method against 3 zero-shot methods without physical world modeling: (i) RIGVid [8]: it directly tracks object poses from generated videos and leverages off-the-shelf grasping models and motion planning for robotic control; (ii) Gen2Act [7]: we adopt modified version in [8], which extracts sparse point tracks as tracking objectives; (iii) AVDC [6]: it leverages depth and optical flow estimation to represent object and embodiment motions. Figure 4 summarizes the quantitative comparison: PhysWorld attains the highest average success rate (82%), significantly outperforming the second-best method [8] (67%) by large margin. This indicates that learning from physical world model provides corrective feedback that reduces compounding errors from grasping and planning, especially in phases like picking, insertion, and pouring. Moreover, leveraging object poses as tracking targets significantly outperforms those using point tracks [7] and optical flows [6]. This implies that object pose estimation provides more robust object motion signals from generated videos than point tracks and optical flow, which often suffer from drifting under occlusion and blurred motion. Failure mode analysis. To further investigate where the performance gains come from, Figure 6 breaks failure cases into 4 categories: grasping, tracking, dynamics, and reconstruction. Comparing with [8], introducing the physical world model substantially reduces grasping failures from 18% to 3% and eliminates tracking failures from 5% to 0%, which indicates the importance of physical feedback Fig. 6: Failure mode analysis. of world models. Our method introduces 7% reconstruction errors. This is mainly because we reconstruct the physical scene from monocular, generated videos, and the completed geometry in occluded regions may be misaligned with realworld geometry. However, we argue that the problem can be mitigated by performing multiview reconstruction of the environment in advance. C. Object-Centric Learning Enhances Policy Effectiveness learning from videos: Object-centric vs. embodiment-centric learning. We compare two paradigms of (i) embodiment-centric learning, which reconstructs human hand mesh and maps finger keypoints to the robot endeffector as robot movement trajectories, and (ii) objectcentric learning, which trains policies to follow object motions. As shown in Table II, object-centric learning is remarkably stronger (Put the book in the bookshelf : 90% vs. 30%; Put the shoe in the shoebox: 80% vs. 10%). The main reason is that generated videos often hallucinate hands or exhibit inconsistent hand kinematics, whereas object motion is more stable and easier to estimate under occlusion. Objectcentric learning therefore transfers more reliably to robots and aligns better with our physics-grounded training. Residual RL vs. RL from scratch. We further compare residual RL with training policy from scratch under the same physical world model and for the Pour the tomato from the pan onto the plate task (see Figure 7). Residual RL converges within few hundred iterations and obtains higher object tracking rewards under the same budget. The baseline grasp-and-plan actions constrain exploration to small, feasible neighborhood, while the world model provides corrective feedback that the residual policy uses to refine trajectories. In contrast, RL from scratch can also succeed [17] but requires longer training time and more careful reward designs. Hence, residual RL with physical world models enables faster learning and improves the robustness of manipulation. TABLE II: Object-centric vs. embodiment-centric learning Task Embodiment-centric Object-centric Put the book in the bookshelf Put the shoe in the shoebox 30% 10% 90% 80% V. CONCLUSION We introduced PhysWorld, framework that bridges video generation and robot learning through physical world modeling. By reconstructing physically interactable scene from Fig. 7: Residual RL vs. RL from Scratch. generated videos and learning object-centric residual RL policies, PhysWorld transforms generated visual demonstrations into physically feasible robotic actions, enabling zeroshot generalizable manipulation in the real world. Future work includes synthesizing physically accurate videos with this framework for training robotic video generation models. Limitations. Physical world modeling is bounded by the fidelity of physical simulators and may introduce additional sim-to-real gaps. However, from the evidence in Figure 4, we still believe in the necessity of introducing world model to provide reliable physical feedback for more robust learning."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Liang, R. Liu, E. Ozguroglu, S. Sudhakar, A. Dave, P. Tokmakov, S. Song, and C. Vondrick, Dreamitate: Real-world visuomotor policy learning via video generation, CoRL, 2024. [2] B. Wang, N. Sridhar, C. Feng, M. Van der Merwe, A. Fishman, N. Fazeli, and J. J. Park, This&that: Language-gesture controlled video generation for robot planning, ICRA, 2025. [3] Y. Tian, S. Yang, J. Zeng, P. Wang, D. Lin, H. Dong, and J. Pang, Predictive inverse dynamics models are scalable learners for robotic manipulation, ICLR, 2025. [4] J. Jang, S. Ye, Z. Lin, J. Xiang, J. Bjorck, Y. Fang, F. Hu, S. Huang, K. Kundalia, Y.-C. Lin et al., Dreamgen: Unlocking generalization in robot learning through neural trajectories, arXiv e-prints, 2025. [5] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel, Learning universal policies via text-guided video generation, NeurIPS, 2023. [6] P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum, Learning to act from actionless videos through dense correspondences, ICLR, 2024. [7] H. Bharadhwaj, D. Dwibedi, A. Gupta, S. Tulsiani, C. Doersch, T. Xiao, D. Shah, F. Xia, D. Sadigh, and S. Kirmani, Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation, CoRL, 2025. [8] S. Patel, S. Mohan, H. Mai, U. Jain, S. Lazebnik, and Y. Li, Robotic manipulation by imitating generated videos without physical demonstrations, arXiv preprint arXiv:2507.00990, 2025. [9] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng et al., Cogvideox: Text-tovideo diffusion models with an expert transformer, arXiv preprint arXiv:2408.06072, 2024. [10] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding et al., Cosmos world foundation model platform for physical ai, arXiv preprint arXiv:2501.03575, 2025. [11] H. Zhen, Q. Sun, H. Zhang, J. Li, S. Zhou, Y. Du, and C. Gan, Tesseract: learning 4d embodied world models, ICCV, 2025. [12] M. Shridhar, Y. L. Lo, and S. James, Generative image as action models, arXiv preprint arXiv:2407.07875, 2024. [13] Y. Du, M. Yang, P. Florence, F. Xia, A. Wahid, B. Ichter, P. Sermanet, T. Yu, P. Abbeel, J. B. Tenenbaum et al., Video language planning, ICLR, 2024. [14] M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel, Learning interactive real-world simulators, NeurIPS, 2023. [15] S. Li, Y. Gao, D. Sadigh, and S. Song, Unified video action model, CoRL, 2025. [16] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn, Bc-z: Zero-shot task generalization with robotic imitation learning, in CoRL, 2022. [17] S. Bahl, A. Gupta, and D. Pathak, Human-to-robot imitation in the wild, RSS, 2022. [18] P. Sharma, L. Mohan, L. Pinto, and A. Gupta, Multiple interactions made easy (mime): Large scale demonstrations data for imitation, in CoRL, 2018. [19] C. Wang, L. Fan, J. Sun, R. Zhang, L. Fei-Fei, D. Xu, Y. Zhu, and A. Anandkumar, Mimicplay: Long-horizon imitation learning by watching human play, CoRL, 2023. [20] K. Kedia, P. Dan, A. Chao, M. A. Pace, and S. Choudhury, One-shot imitation under mismatched execution, ICRA, 2025. [21] R. Mendonca, S. Bahl, and D. Pathak, Structured world models from human videos, RSS, 2023. [22] L. Chen, R. Paleja, and M. Gombolay, Learning from suboptimal demonstration via self-supervised reward regression, in CoRL, 2021. [23] P. Sharma, D. Pathak, and A. Gupta, Third-person visual imitation learning via decoupled hierarchical controller, NeurIPS, 2019. [24] H. Kim, J. Kang, H. Kang, M. Cho, S. J. Kim, and Y. Lee, Uniskill: Imitating human videos via cross-embodiment skill representations, CoRL, 2025. [25] V. Jain, M. Attarian, N. J. Joshi, A. Wahid, D. Driess, Q. Vuong, P. R. Sanketi, P. Sermanet, S. Welker, C. Chan et al., Vid2robot: End-toend video-conditioned policy learning with cross-attention transformers, RSS, 2024. [26] M. Xu, Z. Xu, C. Chi, M. Veloso, and S. Song, Xskill: Cross embodiment skill discovery, in CoRL, 2023. [27] C. Wen, X. Lin, J. So, K. Chen, Q. Dou, Y. Gao, and P. Abbeel, Any-point trajectory modeling for policy learning, RSS, 2024. [28] N. Heppert, M. Argus, T. Welschehold, T. Brox, and A. Valada, Ditto: Demonstration imitation by trajectory transformation, in IROS, 2024. [29] M. Xu, Z. Xu, Y. Xu, C. Chi, G. Wetzstein, M. Veloso, and S. Song, Flow as the cross-domain manipulation interface, CoRL, 2024. [30] M. Hong, A. Liang, K. Kim, H. Rajaprakash, J. Thomason, E. Bıyık, and J. Zhang, Hand me the data: Fast robot adaptation via hand path retrieval, arXiv preprint arXiv:2505.2045, 2025. [31] H. Xiong, Q. Li, Y.-C. Chen, H. Bharadhwaj, S. Sinha, and A. Garg, Learning by watching: Physical imitation of manipulation skills from human videos, in IROS, 2021. [32] J. Ren, P. Sundaresan, D. Sadigh, S. Choudhury, and J. Bohg, Motion tracks: unified representation for human-robot transfer in few-shot imitation learning, ICRA, 2025. [33] M. Lepert, J. Fang, and J. Bohg, Phantom: Training robots without robots using only human videos, arXiv:2503.00779, 2025. [34] G. Papagiannis, N. Di Palo, P. Vitiello, and E. Johns, R+ x: Retrieval and execution from everyday human videos, ICRA, 2025. [35] J. Kerr, C. M. Kim, M. Wu, B. Yi, Q. Wang, K. Goldberg, and A. Kanazawa, Robot see robot do: Imitating articulated object manipulation with monocular 4d reconstruction, CoRL, 2024. [36] C.-C. Hsu, B. Wen, J. Xu, Y. Narang, X. Wang, Y. Zhu, J. Biswas, and S. Birchfield, Spot: Se (3) pose trajectory diffusion for object-centric manipulation, ICRA, 2025. [37] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani, Track2act: tracks from internet videos enables generalizable Predicting point robot manipulation, in ECCV, 2024. [38] Y. Zhu, A. Lim, P. Stone, and Y. Zhu, Vision-based manipulation from single human video with open-world object graphs, CoRL, 2024. [39] B. Wen, W. Lian, K. Bekris, and S. Schaal, You only demonstrate once: Category-level manipulation from single visual demonstration, RSS, 2022. [40] H. Bharadhwaj, A. Gupta, S. Tulsiani, and V. Kumar, Zero-shot robot manipulation from passive human videos, arXiv:2302.02011, 2023. [41] A. Mandlekar, S. Nasiriany, B. Wen, I. Akinola, Y. Narang, L. Fan, Y. Zhu, and D. Fox, Mimicgen: data generation system for scalable robot learning using human demonstrations, CoRL, 2023. [42] J. Wang, K. Liu, D. Guo, X. Zhou, and C. G. Atkeson, One-shot video imitation via parameterized symbolic abstraction graphs, ICRA, 2025. [43] W. Ye, F. Liu, Z. Ding, Y. Gao, O. Rybkin, and P. Abbeel, Video2policy: Scaling up manipulation tasks in simulation through internet videos, ICML, 2025. [44] P. Dan, K. Kedia, A. Chao, E. W. Duan, M. A. Pace, W.-C. Ma, and S. Choudhury, X-sim: Cross-embodiment learning via real-to-sim-toreal, CoRL, 2025. [45] L. Smith, N. Dhawan, M. Zhang, P. Abbeel, and S. Levine, Avid: Learning multi-stage tasks via pixel-level translation of human videos, RSS, 2019. [46] T. G. W. Lum, O. Y. Lee, C. K. Liu, and J. Bohg, Crossing the human-robot embodiment gap with sim-to-real rl using one human demonstration, CoRL, 2025. [47] A. Allshire, H. Choi, J. Zhang, D. McAllister, A. Zhang, C. M. Kim, T. Darrell, P. Abbeel, J. Malik, and A. Kanazawa, Visual imitation enables contextual humanoid control, arXiv:2505.03729, 2025. [48] S. Zhao, J. Mao, W. Chow, Z. Shangguan, T. Shi, R. Xue, Y. Zheng, Y. Weng, Y. You, D. Seita, L. Guibas, S. Zakharov, V. Guizilini, and Y. Wang, Robot learning from any images, CoRL, 2025. [49] B. Kerbl, G. Kopanas, T. Leimkuehler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering, TOG, 2023. [50] T. Dai, J. Wong, Y. Jiang, C. Wang, C. Gokmen, R. Zhang, J. Wu, and F.-F. Li, Automated creation of digital cousins for robust policy learning, CoRL, 2024. [51] A. Zook, F.-Y. Sun, J. Spjut, V. Blukis, S. T. Birchfield, and J. Tremblay, Grs: Generating robotic simulation tasks from real-world images, in CVPR Workshops, 2024. [52] J. Abou-Chakra, K. Rana, F. Dayoub, and N. Sunderhauf, Physically embodied gaussian splatting: realtime correctable world model for robotics, CoRL, 2024. [53] M. M. L. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, and P. Agrawal, Reconciling reality through simulation: real-tosim-to-real approach for robust manipulation, RSS, 2024. [54] X. Han, M. Liu, Y. Chen, J. Yu, X. Lyu, Y. Tian, B. Wang, W. Zhang, and J. Pang, Re3sim: Generating high-fidelity simulation data via 3d-photorealistic real-to-sim for robotic manipulation, ArXiv, 2025. [55] Y. Fang, Y. Yang, X. Zhu, K. Zheng, G. Bertasius, D. A. Szafir, and M. Ding, Rebot: Scaling robot learning with real-to-sim-to-real robotic video synthesis, IROS, 2025. [56] H. Lou, Y. Liu, Y. Pan, Y. Geng, J. Chen, W. Ma, C. Li, L. Wang, H. Feng, L. Shi, L. Luo, and Y. Shi, Robo-gs: physics consistent spatial-temporal model for robotic arm with hybrid representation, ICRA, 2025. [57] N. Pfaff, E. Fu, J. P. Binagia, P. Isola, and R. Tedrake, Scalable real2sim: Physics-aware asset generation via robotic pick-and-place setups, IROS, 2025. [58] X. Li, K. Hsu, J. Gu, K. Pertsch, O. Mees, H. R. Walke, C. Fu, I. Lunawat, I. Sieh, S. Kirmani, S. Levine, J. Wu, C. Finn, H. Su, Q. H. Vuong, and T. Xiao, Evaluating real-world robot manipulation policies in simulation, CoRL, 2024. [59] M. M. L. Torne, A. Jain, J. Yuan, V. Macha, L. L. Ankile, A. Simeonov, P. Agrawal, and A. Gupta, Robot learning with super-linear scaling, RSS, 2025. [60] S. Patel, X. Yin, W. Huang, S. Garg, H. Nayyeri, F.-F. Li, S. Lazebnik, and Y. Li, real-to-sim-to-real approach to robotic manipulation with vlm-generated iterative keypoint rewards, ICRA, 2025. [61] S. Zhu, L. Mou, D. Li, B. Ye, R. Huang, and H. Zhao, Vrrobo: real-to-sim-to-real framework for visual robot navigation and locomotion, IEEE RA-L, 2025. [62] X. Li, J. Li, Z. Zhang, R. Zhang, F. Jia, T. Wang, H. Fan, K.-K. Tseng, and R. Wang, Robogsim: real2sim2real robotic gaussian splatting simulator, ArXiv, 2024. [63] Google, Veo3, 2025. [64] Z. Li, R. Tucker, F. Cole, Q. Wang, L. Jin, V. Ye, A. Kanazawa, A. Holynski, and N. Snavely, Megasam: Accurate, fast and robust structure and motion from casual dynamic videos, in CVPR, 2025. [65] B. Wen, J. Tremblay, V. Blukis, S. Tyree, T. Muller, A. Evans, D. Fox, J. Kautz, and S. Birchfield, Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects, in CVPR, 2023. [66] J. Zhao, S. Zhou, Z. Wang, P. Yang, and C. C. Loy, Objectclear: Complete object removal via object-effect attention, arXiv preprint arXiv:2505.22636, 2025. [67] J. Xiang, Z. Lv, S. Xu, Y. Deng, R. Wang, B. Zhang, D. Chen, X. Tong, and J. Yang, Structured 3d latents for scalable and versatile 3d generation, in CVPR, 2025. [68] A. J. Zhai, Y. Shen, E. Y. Chen, G. X. Wang, X. Wang, S. Wang, K. Guan, and S. Wang, Physical property understanding from language-embedded feature fields, in CVPR, 2024. [69] B. Wen, W. Yang, J. Kautz, and S. Birchfield, Foundationpose: Unified 6d pose estimation and tracking of novel objects, in CVPR, 2024. [70] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie, and C. Lu, Anygrasp: Robust and efficient grasp perception in spatial and temporal domains, IEEE T-RO, 2023. [71] B. Sundaralingam, S. K. S. Hari, A. Fishman, C. Garrett et al., Curobo: Parallelized collision-free robot motion generation, in ICRA, 2023. [72] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv:1707.06347, 2017."
        }
    ],
    "affiliations": [
        "DeepMind",
        "Google",
        "Stanford",
        "Toyota Research Institute",
        "USC"
    ]
}