{
    "paper_title": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation",
    "authors": [
        "Bo Wang",
        "Mengyuan Xu",
        "Yue Yan",
        "Yuqun Yang",
        "Kechen Shu",
        "Wei Ping",
        "Xu Tang",
        "Wei Jiang",
        "Zheng You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Precise lesion resection depends on accurately identifying fine-grained anatomical structures. While many coarse-grained segmentation (CGS) methods have been successful in large-scale segmentation (e.g., organs), they fall short in clinical scenarios requiring fine-grained segmentation (FGS), which remains challenging due to frequent individual variations in small-scale anatomical structures. Although recent Mamba-based models have advanced medical image segmentation, they often rely on fixed manually-defined scanning orders, which limit their adaptability to individual variations in FGS. To address this, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It introduces adaptive scan scores to dynamically guide the scanning order, generated by combining group-level commonalities and individual-level variations. Experiments on two public datasets (ACDC and Synapse) and a newly proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and dataset are available at https://github.com/YqunYang/ASM-UNet."
        },
        {
            "title": "Start",
            "content": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation Bo Wang, Mengyuan Xu, Yue Yan, Yuqun Yang*, Member, IEEE, Kechen Shu, Wei Ping, Xu Tang, Senior Member, IEEE, Wei Jiang, Zheng You 1 5 2 0 2 0 1 ] . [ 1 7 3 2 7 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Precise lesion resection depends on accurately identifying fine-grained anatomical structures. While many coarse-grained segmentation (CGS) methods have been successful in large-scale segmentation (e.g., organs), they fall short in clinical scenarios requiring fine-grained segmentation (FGS), which remains challenging due to frequent individual variations in small-scale anatomical structures. Although recent Mamba-based models have advanced medical image segmentation, they often rely on fixed manually-defined scanning orders, which limit their adaptability to individual variations in FGS. To address this, we propose ASM-UNet, novel Mamba-based architecture for FGS. It introduces adaptive scan scores to dynamically guide the scanning order, generated by combining group-level commonalities and individual-level variations. Experiments on two public datasets (ACDC and Synapse) and newly proposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that ASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and dataset are available at https://github.com/YqunYang/ASM-UNet. Medical image segmentation, Mamba, fine-grained segmentation, anatomical variations."
        },
        {
            "title": "Index Terms",
            "content": "I. INTRODUCTION REcently, precise resection of the lesion has garnered increasing attention, with the goal of completely removing the lesion while preserving as much healthy tissue as possible. This approach promotes faster and more effective postoperative recovery and reduces the risk of long-term complications [1], [2]. To enable precise resection, artificial intelligence technology is employed to automatically segment medical images and construct 3D models, assisting clinicians in accurately identifying the optimal starting point for resection and predicting the extent of lesion and surrounding vessels [3][5]. This ultimately facilitates greater surgical precision in the resection of the lesion. In CT image segmentation of organs such as the heart, liver, and lungs, the large size of the imaging area contributes to high segmentation accuracy in terms of total volume [6][8]. This supports surgical planning for resection procedures involving larger anatomical regions. However, such coarse-grained segmentation (CGS) is insufficient for precise resection. For example, as shown in Fig. 1, the biliary tract system can be further refined into multiple anatomical structures. During gallbladder resection, it is crucial to accurately distinguish the common hepatic duct (CHD), right hepatic duct (RHD), and common bile duct (CBD) at fine-grained level to prevent surgical errors. Therefore, achieving accurate fine-grained segmentation (FGS) of medical images has become critical topic in precision surgery. Nevertheless, individual anatomical variation remains major challenge in FGS. As illustrated in the left part of Fig. 2, group-level commonality, such as the general distribution patterns of most organs, is consistently observed across individuals of different ages and sexes, which facilitates large-scale CGS. In contrast, when the anatomical scale is reduced from the entire thoracoabdominal cavity to single organs or functional systems, inter-individual differences become more pronounced. Taking the biliary system as Corresponding author: Yuqun Yang (email: yqunyang@163.com). 2 Fig. 1. Illustration of fine-grained anatomical structures within the biliary tract system under the coarse-grained organ segmentation. It consists of multiple anatomical structures, including gallbladder (GB), cystic duct (CD) and others. The definitions of all abbreviations are provided in Table I. Illustration of six common anatomical variants of the biliary tract system. Beyond these, numerous undefined variants also occur in Fig. 2. clinical practice. an example, it can be classified into six common types [9], [10], as shown in the right part of Fig. 2, along with other undefined variants. These variations correspond to different distributions of anatomical structures such as RHD and CHD, which are critical components for precise resection. Moreover, such variations often occur at small anatomical scale. For example, the CHD typically has diameter of less than 10 mm. Therefore, although achieving accurate FGS is essential for precision surgery, it remains challenging task due to significant individual anatomical variations. Among the various approaches proposed for medical image segmentation, convolutional neural network (CNN)-based models have become the dominant paradigm [11]. In particular, U-Net [12] and its variants [13][15] have achieved encouraging performance, benefiting from their encoderdecoder architecture and multi-scale feature fusion. However, CNNs inherently struggle to capture long-range dependencies, 3 limiting their effectiveness in segmenting complex anatomical structures. To address this issue, researchers have introduced the Transformer architecture [16]. With its self-attention mechanism, the Transformer excels at modeling global context and long-range dependencies, making it particularly well-suited for tasks that require holistic understanding of medical images [17]. As result, many Transformer-involved methods [18][20] have been proposed, achieving promising accuracy across variety of medical image segmentation tasks. Recently, to address the high computational complexity of Transformers, which scales quadratically with input length (O(n2)), Mamba [21] has been introduced. Mamba leverages selective state space model (SSM) and linear-time (O(n)) operations, enabling efficient parallel processing while preserving the capability to model long-range dependencies. Building on these advantages, researchers have adopted the Mamba architecture to develop medical image segmentation models [22][24], achieving both effective and efficient performance. However, challenges remain in applying these models to medical imaging tasks. Unlike text, which follows one-dimensional sequential order (e.g., forward or backward), medical image data is inherently three-dimensional and can be scanned in multiple directions, such as horizontal, vertical, or diagonal [25] [27]. Although scanning-based feature extraction can effectively emphasize anatomical details, existing methods typically rely on manually designed and fixed scanning orders. This rigidity limits their adaptability to the diverse and complex characteristics of FGS, which often exhibit intricate structures, irregular shapes, and varying spatial positions. For example, accurately identifying CHD often requires scanning along RHD and LHD to locate their intersection, where an appropriate scanning order helps ensure precise localization. However, due to individual variations, the fixed scanning orders used in existing Mambabased methods struggle to follow this identification process effectively. Therefore, how to generate adaptive scanning orders for the different individuals has become an important problem. To address this issue, we propose novel adaptive scanning mechanism that predicts scan score for each medical image (i.e., each individual), which is then embedded into the feature representation. The scan order is dynamically determined by sorting these scores, allowing the model to follow adaptive scanning paths. Considering the group-level commonality in large anatomical scale and individual-level variation in smaller ones, we design dual-scan scoring strategy: group scan score shared across all images and an individual scan score generated specifically for each image. These two scores are integrated to jointly and adaptively determine the final scan order, enabling the model to effectively capture both group-level commonality and individual-level variation, thereby improving segmentation accuracy at both coarseand fine-grained levels. To evaluate the performance of our method, we conduct experiments on two public datasets, Synapse multi-organ segmentation (Synapse) [28] and the automatic cardiac diagnosis challenge (ACDC) [29], as well as on our proposed biliary tract multi-class segmentation (BTMS) dataset for FGS. The BTMS dataset poses significant segmentation challenge due to its eight fine-grained categories within the small and complex anatomical structures of the biliary tract system (see Figure 1). The main contributions of this work are as follows: We propose an adaptive scanning Mamba (ASM) that dynamically determines scan paths by jointly considering group-level commonality and individual-level variation, enhancing segmentation performance. We introduce biliary tract multi-class segmentation dataset, BTMS, which includes eight categories at small anatomical scales, serving as meaningful and challenging benchmark for FGS. We conduct extensive experiments on two public datasets and our BTMS dataset, demonstrating the effectiveness and generalizability of the proposed method on both coarseand fine-grained segmentation tasks. II. RELATED WORK To provide comprehensive overview, this section covers two key areas relevant to our study: common approaches in medical image segmentation and recent advances in Mamba-based methods. 4 A. Medical Image Segmentation Fully Convolutional Networks (FCNs) [30] use convolution and upsampling for pixel-level prediction but lack sufficient skip connections, making it hard to retain spatial details. To address this issue, U-Net [12] adopts symmetric encoder-decoder architecture with skip connections at each scale, effectively combining low-level local information with high-level semantic features. It has since become strong backbone in medical image segmentation. Since then, many variants of U-Net have been proposed. For example, U-Net++ [13] refines skip connections to enhance multi-scale feature aggregation, while U-Net3+ [14] introduces full-scale skip connections to improve feature integration. Among them, nnU-Net [31] stands out as self-configuring framework that automatically adapts network architecture, training, and preprocessing to given dataset, achieving strong performance without manual tuning. To fully explore the potential of CNNs, ConvNeXt [32] modernizes the ResNet architecture with series of design improvements, achieving strong performance in image segmentation while retaining the simplicity and efficiency of convolutional models. Despite these advancements, modeling long-range dependencies remains difficult, prompting the introduction of Transformers to boost performance. For example, MissFormer [18] leverages Transformer architecture to effectively integrate local context and global dependencies, enhancing performance in medical image segmentation. Similarly, CoTr [33] and nnFormer [19] integrate CNNs with Transformers to achieve accurate medical image segmentation. In UNETR [34], the Transformer is employed as the encoder to capture global multi-scale contextual information. Building on this framework, several variants such as Swin UNETR [35] and UNETR++ [4] have been proposed, achieving promising segmentation performance. While Transformers have demonstrated strong capability in modeling global dependencies, their inherent computational complexity leaves considerable room for more efficient solutions. B. Mamba-based Methods Mamba [21] was proposed to address key limitations of Transformer models, such as quadratic computational complexity and inefficiency in modeling long-range dependencies. By leveraging SSMs, Mamba achieves linear complexity while maintaining strong performance through efficient sequence modeling. As result, various Mamba-based methods have been developed for medical image segmentation. For instance, VM-UNet [36] incorporates visual state space block into the U-shaped architecture to effectively capture global contextual information. Building upon Mamba, Swin-UMamba [37] exploits the pretraining capability of ImageNet to enhance segmentation performance. Its variant, Swin-UMamba [24], further adopts self-supervised learning strategy to bridge the domain gap between natural and medical images, thereby significantly improving generalization on small datasets while maintaining strong performance across multiple benchmarks. These methods have demonstrated remarkable effectiveness in 2D medical image segmentation. Furthermore, Vivim [38] applies Mamba to video segmentation with temporal module to compress long sequences. In addition to the 2D images, researchers extend to employing Mamba for segmenting 3D medical images. For instance, U-Mamba [39] combines the local feature extraction capabilities of CNNs with the long-range dependency modeling strength of SSMs. Following this trend, NnMamba [40] further unifies CNNs and SSMs to effectively capture long-range dependencies in 3D medical image analysis. SegMamba [23] introduces the tri-directional spatial Mamba (TSMamba) module to capture multi-scale global information, while LKM-UNet [41] enhances local modeling through large kernel Mamba blocks, expanding the receptive field for more precise spatial feature extraction. Although these Mamba-based methods demonstrate promising performance in medical image segmentation, their reliance on fixed scanning order still limits generalization in fine-grained segmentation tasks, which is the problem our method is designed to solve. III. DATASET The biliary system plays critical role in the human digestive system, with primary functions including the transport and storage of bile produced by the liver, the regulated release of bile into the duodenum, and TABLE ANATOMICAL CATEGORIES WITH ABBREVIATIONS. CATEGORIES MARKED WITH * ARE CHALLENGING TO SEGMENT ID Category Name Abbreviation 0 1 2 3 4 5 6 7 8 Background Gallbladder Cystic Duct Common Bile Duct Common Hepatic Duct Right Hepatic Duct Right Posterior Hepatic Duct Right Anterior Hepatic Duct Left Hepatic Duct - CB CD CBD CHD RHD RPHD RAHD LHD TABLE II STATISTICS OF PUBLICLY AVAILABLE DATASETS"
        },
        {
            "title": "Annotated Biliary Classes",
            "content": "3Dircadb1 [42] Synapse [28] WORD [43] 512512 512512 512512 20 50 150 3,209 6,811 30,495 [1.0, 4.0] [2.5, 5.0] [2.5, 3.0]"
        },
        {
            "title": "BTMS",
            "content": "512512 100 20,419 [0.625, 1.5] 1.6 3.0 3.0 1. [74, 260] [85, 198] [151, 343] [169, 729] 130 127"
        },
        {
            "title": "GB\nGB\nGB",
            "content": "GB, CD, CBD, CHD, RHD, RPHD, RAHD, LHD the prevention of intestinal content reflux. It comprises several important anatomical structures, including the CB, CD, CBD, CHD, RHD, RPHD, and RAHD, whose abbreviations are explained in Table I. These anatomical structures exhibit substantial complexity and variability due to individual anatomical variations. Precise delineation of intrahepatic bile ducts (e.g., RPHD, RAHD, LHD), extrahepatic bile ducts (e.g., CHD, CBD), and associated structures (e.g., CB) is essential for various clinical applications, including preoperative planning in liver transplantation, minimizing surgical complications during laparoscopic hepatectomy, and improving diagnostic accuracy in biliary diseases. Therefore, fine-grained segmentation of the biliary system holds great clinical significance. However, existing publicly available datasets for the biliary system remain at coarse-grained level. As shown in Table II, all the datasets (e.g., 3Dircadb1 [42], Synapse [28], and WORD [43] ) only provide annotations for the gallbladder (GB), lacking detailed segmentation of the complete biliary tree. In contrast, our BTMS dataset offers fine-grained annotations for eight biliary structures, enabling more comprehensive anatomical understanding. Moreover, BTMS contains 100 cases with 20,419 images, which is significantly larger than 3Dircadb1 (20 cases / 3,209 images) and Synapse (50 cases / 6,811 images), and comparable in scale to WORD (150 cases / 30,495 images) but with much finer anatomical labeling. Additionally, BTMS has higher-resolution slices (median thickness: 1.2 mm) and more slices per case (median: 269), providing richer spatial detail for accurate biliary system analysis. Three case examples, along with the explanation of color coding and anatomical locations, are shown in Fig. 3. In the experiments, the dataset is split into training, validation, and test sets in 4:3:3 ratio. Notably, CD and RHD are marked with * in Table due to their significant segmentation difficulty, which arises from anatomical variations and their small-scale structures. For example, CD is extremely short and thin, making it difficult to locate. RHD shares similar characteristics, and its segmentation is even more challenging because its position can vary considerably or even disappear (e.g., Type-2 in Figure 2) entirely due to changes at the intersection of RAHD and RPHD, further increasing the difficulty compared to CD. Fig. 3. Examples from our BTMS dataset. Each row corresponds to single case, with the right section providing simplified explanation of color coding and anatomical location. Fig. 4. Overview of the proposed network architecture ASM-UNet. A. Overview IV. METHODOLOGY As shown in Figure 4, the proposed model is built upon the U-Net architecture, following the design principles of the literature [31], and consisting of six encoder blocks, six decoder blocks, and adaptive scanning Mamba (ASM) blocks. Note that the ASM block is inserted after the first encoder block and before the last decoder block. This design choice is motivated by the fact that feature maps at these stages have large spatial dimensions (corresponding to very long sequences) and limited channel information, making it difficult to capture global context effectively while incurring high computational cost. threedimensional medical image with dimensions is used as the input to the model, while the output is segmentation map of shape C. Here, , H, D, and represent the width, height, depth, and number of segmentation categories, respectively. During data flow, the downsampling rates and output channels of the six encoder blocks are set to [1, 2, 2, 2, 2] and [32, 64, 128, 256, 320], respectively, and those of the decoder blocks are configured correspondingly. Specifically, each encoder Fig. 5. Diagram of adaptive score generation and feature reordering. The adaptive scan score is obtained by combining both the group and individual scan scores, and is used to adaptively guide the scanning order. block consists of two convolutional layers, followed by normalization and ReLU activation, with kernel size of 333. The first convolutional layer uses stride of 2 for downsampling, while the second uses stride of 1. Decoder blocks perform upsampling using transposed convolution, followed by concatenation with the corresponding feature map from the encoder block via skip connection. The combined feature map is then processed through two convolutional layers (with 3 3 3 kernels and stride 1), followed by normalization and ReLU activation, to produce the input for the next decoder block. B. Adaptive Scanning Mamba The ASM module is designed to enhance generalization performance by employing an adaptive scanning mechanism instead of fixed one. As illustrated in Fig. 4, the process begins by flattening the feature map along the spatial dimensions (width, height and depth), i.e., from (W, H, D, L) to (W D, L), which is then fed into several blocks containing adaptive score generation and feature reordering modules. Then, the reordered feature maps are passed through series of stacked Mamba layers. Next, the multiple output feature maps are restored to their original order (i.e., undo reordering) and averaged at the pixel level. Finally, the averaged feature map is reshaped along the spatial dimensions to produce the output of the ASM module. Here, the adaptive scan score for guiding the scan order is generated by adding group scan score and individual scan score. 1) Group scan score: Since group-level commonalities are consistently observed among individuals, such as the spatial distribution and volume of most organs, emphasizing these patterns can help the model better capture the global anatomical context. Therefore, group scan score is defined to guide the scanning order at the global level. Since it aims to leverage group-level commonalities without being influenced by individual-specific characteristics, the score is generated through learnable parameter within the model, namely the learnable group scan embedding, as illustrated in the adaptive score generation block of Fig. 5. The length ng of the embedding is hyperparameter, and is initialized as vector sampled from normal distribution. To accommodate feature maps of arbitrary spatial size, the embedding is expanded and adjusted based on the spatial dimensions (d = s) using linear interpolation during feature learning. Finally, the Sigmoid function is applied to normalize the embedding into score, ensuring that its values lie within the range [0, 1]. 2) Individual scan score: Since small-scale anatomical structures often exhibit variability, they contribute to differences between individuals, i.e., individual variation. Therefore, accurately capturing individualspecific information is essential for performing the FGS task. To achieve this, an individual scan score generator is designed, as shown in Fig. 5. It takes as input the feature maps of specific patient and generates an individual scan score by comprehensively analyzing the medical image using three different fixed scan orders [21] to ensure generation accuracy. The length of the generated score is the same as that of the input feature maps, i.e., = s. This design is inspired by the observation that, when clinicians examine medical images, an initial fixed scanning order (for example, from top to bottom or from right to left) helps them memorize the anatomical content at each location. Based on this internal memory, they can perform dynamic scanning of regions potentially related to disease, more effectively analyze their relationships, and draw diagnostic conclusions. In our method, three fixed scan orders serve as the models initial scanning phase, similar to how clinicians first build an understanding of anatomical structures such as the left and right hemi-liver, which are important for identifying the RHD and LHD. Based on this global understanding, the individual scan score is then generated to guide more adaptive scanning process for accurately identifying small-scale structures such as the CHD and CD, ultimately improving the performance of the FGS task. After both group and individual scan scores are generated, the adaptive scan score can be obtained by adding them. The adaptive scan score is concatenated with the corresponding feature embedding for adaptively guiding the scanning order. A. Datasets V. EXPERIMENT To comprehensively evaluate the effectiveness and generalizability of our method, namely ASM-UNet, we conduct experiments on two widely-used public datasets ACDC [29] and Synapse [28] for medical image segmentation, and our fine-grained BTMS dataset introduced in Section III. For ACDC dataset, it serves as benchmark for cardiac segmentation, containing annotations for three anatomical structures: the right ventricle (RV), myocardium (MYO), and left ventricle (LV). It consists of 2,978 slices from 150 cases, in which training and test sets contain 100 and 50 cases, respectively. For Synapse dataset, it provides diverse collection of annotated medical images for multi-organ segmentation, including eight structures: aorta (AO), gallbladder (GB), left kidney (LK), right kidney (RK), liver (LIV), pancreas (PAN), spleen (SPL) and stomach (STO), comprising total of 3,779 slices from 30 patients. B. Data Preprocessing and Implementation Details All images are resampled to an isotropic resolution based on the median voxel spacing of the dataset. Intensity normalization is performed using z-score standardization, where the mean and standard deviation are computed from the foreground voxels. Data augmentation techniques are applied, including random flipping with probability of 50%, random rotation up to 30 degrees, and random scaling with factor ranging from 0.7 to 1.4. Training is conducted using patch-based strategy with batch size of 2. The optimization process employs the Adam optimizer with an initial learning rate of 0.01, which follows polynomial decay schedule. The loss function consists of combination of dice loss and cross-entropy loss with equal weighting. The model is trained for maximum of 1000 epochs, with validation performed every 50 epochs. If no improvement is observed for 60 consecutive validation steps, early stopping is applied to terminate training. To ensure fair comparisons, the hyperparameters of the comparative methods are set as closely as possible to those reported in their original papers. All experiments are implemented using PyTorch and executed on two NVIDIA RTX 3090 GPUs, each equipped with 24 GB of memory. 9 TABLE III COMPARISON WITH ALTERNATIVE METHODS ON TWO PUBLIC DATASETS Methods ACDC Synapse Avg. LV RV MYO Avg. AO GB LK RK LIV PAN SPL STO VIT+CUP [44] R50-VIT+CUP [44] TransUNet [44] Swin-UNet [45] LeVit-UNet-384s [46] MISSFormer [18] CoTr [33] UNETR [34] Swin UNETR [35] nnFormer [19] SegFormer3D [47] 81.45 87.57 89.71 90.00 90.32 88.27 90.63 88.61 89.41 92.06 90.96 81.46 86.07 88.86 88.55 89.55 85.78 89.72 85.29 87.54 90.94 88.50 70.71 81.88 84.54 85.62 87.64 85.35 87.45 86.52 86.30 89.58 88.86 92.18 94.75 95.73 95.83 93.76 93.68 94.72 94.02 94.38 95.65 95.53 67.86 71.29 77.48 79.13 78.53 81.96 80.78 79.56 83.51 86.57 82. 70.19 73.73 87.23 85.47 87.33 86.99 85.42 89.99 90.75 92.04 90.43 45.10 55.13 63.16 66.53 62.23 68.65 68.93 60.56 66.72 70.17 55.26 74.70 75.80 81.87 83.28 84.61 85.21 85.45 85.66 86.51 86.57 86.53 67.40 72.20 77.02 79.61 80.25 82.00 83.62 84.80 85.88 86.25 86.13 91.32 91.51 94.08 94.29 93.11 94.41 93.89 94.46 95.33 96.84 95.68 42.00 45.99 55.86 56.58 59.07 65.67 63.77 59.25 70.07 83.35 73. 81.75 81.99 85.08 90.66 88.86 91.92 88.58 87.81 94.59 90.51 89.02 70.44 73.95 75.62 76.60 72.76 80.81 76.23 73.99 78.20 86.83 81.12 ASM-Unet 92.61 92.61 90. 94.75 87.27 92.98 73.35 87.15 87. 96.37 82.87 95.13 82.82 TABLE IV COMPARISON WITH ALTERNATIVE METHODS ON OUR BTMS DATASET. MEANS THAT THE METHOD IS MAMBA-BASED Methods Avg. Coarse Avg. Fine (w/o Hard) Avg. Fine (w/ Hard) GB CD* CBD CHD RHD* RPHD RAHD LHD VIT-CUP [44] R50-VIT-CUP [44] TransUNet [44] Swin-UNet [45] CoTr [33] UNETR [34] Swin UNETR [35] nnFormer [19] SegFormer3D [47] SegMamba [23] NnMamba [40] U-Mamba [39] LKM-UNet [41] ASM-UNet 39.98 48.49 60.92 57.11 74.14 64.99 62.40 67.38 64.28 74.65 71.92 74.20 72. 76.74 C. Comparison Studies 17.16 21.97 31.92 25.91 46.87 37.26 36.31 41.42 37.33 42.94 43.86 46.23 45.40 48.79 12.87 20.48 28.99 19.43 40.53 31.31 30.21 36.16 30. 35.75 37.51 41.69 40.73 42.57 45.19 51.38 69.65 60.54 80.43 65.98 69.53 68.10 68.52 79.49 79.18 83.37 77.74 - 5.97 7.46 - 23.04 11.58 8.12 19.59 8.62 11.54 14.89 29.31 26. 4.88 5.63 15.72 10.67 47.66 38.31 33.23 40.58 39.51 40.77 39.58 43.82 42.65 16.04 16.50 28.31 16.58 48.17 31.84 27.68 40.93 31.44 42.29 43.46 45.77 49.23 - 26.09 32.99 - 19.92 15.31 15.74 21.12 11.39 16.88 21.99 26.82 26. 10.24 16.75 22.79 20.73 30.39 24.65 26.76 29.91 25.40 27.34 30.12 29.87 31.51 12.62 21.95 26.46 22.79 32.58 29.76 28.29 31.98 28.08 30.03 31.11 33.47 29.47 14.01 19.61 28.56 24.15 42.01 33.01 32.35 37.04 31.04 37.69 39.71 41.10 41. 84.59 23.41 48.36 51.60 24.37 32. 32.70 42.98 We perform comprehensive comparison of our method with thirty other models on three datasets. The comparative methods consist of the nine widely-used methods TransUNet [44], Swin-UNet [45], LeViTUNet [46], MISSFormer [18], CoTr [33], UNETR [34], Swin UNETR [35], nnFormer [19], and SegFormer3D [47], as well as four Mamba-based methods SegMamba [23], NnMamba [40], U-Mamba [39], LKM-UNet [41]. The quantitative results based on the Dice metric are reported in Table III and Table IV. ACDC and Synapse: Table III presents the segmentation results on the ACDC and Synapse datasets. Our proposed ASM-UNet achieves the highest average Dice scores on both datasets, outperforming the secondbest method by 0.55% on ACDC and 0.70% on Synapse. In terms of per-class segmentation performance, ASM-UNet achieves the highest Dice score in the majority of categories across both datasets. Although ASM-UNet does not achieve the top Dice score for MYO (ACDC), LIV (Synapse), and PAN (Synapse), its performance remains within 1% of the best-performing method, demonstrating its competitive accuracy. notable exception is the STO category in Synapse, where nnFormer outperforms ASM-UNet by 4.01%. 10 Fig. 6. Segmentation results of all compared methods. The region highlighted with yellow bounding box is enlarged for better visualization. Nevertheless, ASM-UNet achieves the second-best result in this category and delivers superior performance across most other classes. BTMS: To further validate the effectiveness of ASM-UNet, we conduct additional fine-grained segmentation experiments on our BTMS dataset, which poses significant segmentation challenges. The results are summarized in Table IV, where fine-grained segmentation performance is reported in two forms: Avg. Fine (w/o Hard) and Avg. Fine (w/ Hard), with Hard referring to two particularly challenging categories as defined in Table I. Additionally, we report the coarse-grained performance as Avg. Coarse, which considers only foreground (1) versus background (0) segmentation. According to the results, our ASM-UNet achieves the highest performance across all three average metrics: 76.74% in coarse-grained segmentation, 48.79% in fine-grained segmentation without hard categories, and 42.57% with them. Compared to the second-best methods, ASM-UNet achieves improvements in Dice scores of 2.09% over SegMamba, 1.92% over CoTr, and 0.78% over U-Mamba, respectively. Notably, ASM-UNet consistently outperforms all competing methods across most individual anatomical categories, including GB (84.59%), CBD (48.36%), CHD (51.60%), RPHD (32.51%), and LHD (42.98%). Here, the Dice score for GB is significantly higher than that of other categories due to its larger volume. This highlights the challenge of segmenting large-scale anatomical structures in FGS task. For the two hard categories, all methods achieve Dice scores below 35%, with VIT-CUP and Swin-UNet even yielding scores as low as 0% (marked with - in Table), primarily due to the tiny volumes and high variability in spatial positions of these structures. To investigate the underlying cause of the low accuracy in FGS, we conduct an additional evaluation using CGS, denoted as Avg. Coarse. In this setting, the eight anatomical categories in both the predictions and ground truth are merged into single foreground class. The results show substantial increase in Dice scores compared to FGS, indicating that the models are generally capable of detecting the overall biliary system. However, they struggle to accurately distinguish between the individual fine-grained anatomical structures. This highlights the significantly greater challenge posed by FGS relative to common CGS, primarily due to the small size, ambiguous boundaries, and high inter-class similarity among the finegrained categories. 11 D. Ablation Studies Given the challenging nature of the BTMS dataset, we conduct ablation studies to evaluate the contributions of different modules within our model. The results are presented in Table V, where Mamba refers to the baseline model using only the pure Mamba module without scan score enhancements, IS denotes the incorporation of the individual scan score, and GS indicates the addition of the group scan score. TABLE ABLATION STUDIES ON DIFFERENT MODULES, WHERE GS AND IS REPRESENT GROUP AND INDIVIDUAL SCAN SCORES, RESPECTIVELY; MAMBA DENOTES THE FIXED SCANNING STRATEGY Models Mamba IS GS Avg. Fine (w/ Hard) Avg. Fine (w/o Hard) M1 M2 M3 M4 M5 40.23 40.85 41.34 41.87 42.57 46.68 46.97 47.46 48.21 48.79 M1 denotes the baseline UNet model, while M2 through M5 correspond to variants equipped with different combinations of the proposed modules. M5, which integrates all components, represents our complete model ASM-UNet. According to the results in Table V, we observe clear performance improvement as each module is incrementally added. Introducing the Mamba module alone (M2) improves the Dice score from 40.23% to 40.85% on hard categories and from 46.68% to 46.97% on non-hard categories, indicating that Mamba provides solid foundation over the baseline UNet (M1). The addition of IS (M3) further boosts performance to 41.34% (hard) and 47.46% (non-hard), demonstrating its effectiveness by capturing individual differences. When the GS is added (M4), the model achieves 41.87% on hard categories and 48.21% on non-hard categories, suggesting that utilizing group commonality contributes positively to segmentation quality. Finally, our complete model ASM-UNet (M5), which integrates both IS and GS modules, achieves the highest performance with 42.57% and 48.79% on hard and non-hard fine-grained segmentation, respectively. These results not only demonstrate the individual effectiveness of each module but also highlight their complementary nature in enhancing the models ability to distinguish fine-grained and challenging anatomical structures of biliary system. E. Parameter Analysis TABLE VI PARAMETER ANALYSIS ON THE IMPACT OF BRANCH NUMBERS IN ASM BLOCK FOR SEGMENTATION Number of Branches Avg. Fine (w/ Hard) Avg. Fine (w/o Hard) 0 1 2 3 4 40.23 41.91 42.01 42.57 42.24 46.68 48.70 48.46 48.79 49.17 As illustrated in Fig.4, multiple branches can be configured to generate adaptive scan scores, which are then fed into the stacked Mamba layers for feature learning. To assess the impact of branch numbers on segmentation performance, we conduct the experiments for parameter analysis, and the results are summarized in TableVI. As shown in the table, introducing multiple branches consistently improves 12 Fig. 7. Three examples of coarse-grained segmentation results produced by different methods. Fig. 8. Three examples of segmentation results for the two hard categories produced by different methods. performance over the zero-branch baseline, highlighting the effectiveness of incorporating adaptive scanning representations. In particular, employing three branches yields the highest Dice score (42.57%) on segmentation with hard categories, while four branches achieve the best result (49.17%) on nonhard categories. These findings indicate that moderate branching enhances the models ability to capture diverse semantic patterns. However, increasing the number of branches beyond certain point may lead to diminishing returns or reduced robustness, particularly in more challenging scenarios. This is because averaging over too many branches may dilute discriminative features, making it harder to preserve the unique characteristics that distinguish hard categories from the others. Therefore, we select three branches as the default setting for fine-grained segmentation experiments on the BTMS dataset. F. Visual Discussion To further analyze the advantage of our method, we show the visual results of FGS and CGS in Fig. 6 and 7, respectively. For FGS, it is evident that our method produces results closely aligned with the ground truth, particularly in delineating the boundaries between RAHD, RHD, and CHD. While LKMUNet and U-Mamba also achieve promising results, they tend to generate more false positives compared to our ASM-UNet. In contrast, the other methods struggle to distinguish fine-grained categories and often fail to produce reliable segmentation in these regions. For CGS, the reduced task complexity allows all methods to produce visually reasonable results. This further demonstrates that existing methods are generally effective for standard CGS tasks. However, they exhibit limited capability in distinguishing finegrained anatomical categories. Despite the relative ease of CGS, our method still achieves the best visual performance, delivering continuous and precise segmentation that closely aligns with the ground truth. 13 Finally, we independently analyze the segmentation results for the two hard categories, CD and RHD, as shown in Fig.8. Despite employing various advanced methods, all models face considerable challenges in correctly identifying and reconstructing these categories. This difficulty stems from their tiny size, irregular morphology, and highly variable spatial distribution. In particular, it remains challenging to distinguish these structures from the background and to preserve their anatomical continuity, such as accurately predicting the CD as narrow duct connecting the GB and CHD. These findings highlight the limitations of current methods and point to the need for further research on robust, structure-aware segmentation approaches tailored to such fine-grained and spatially inconsistent anatomical components. VI. CONCLUSION In this paper, we propose novel Mamba-based model ASM-UNet for FGS tasks. In our approach, group scan score is introduced to capture group-level commonalities, while an individual scan score is designed to model case-specific variations, which are commonly observed in small-scale anatomical structures. These two scores are combined to generate an adaptive scan score, which guides the Mamba modules in performing feature learning with adaptive scanning, replacing manual and fixed scanning strategies. This design significantly enhances the models robustness and generalizability in the FGS task. To evaluate the effectiveness of ASM-UNet, we conduct comprehensive comparison studies on two public datasets, ACDC and Synapse. Additionally, to further assess the upper bound of our methods capability, we construct challenging new dataset, BTMS, which partitions the biliary system into eight fine-grained anatomical structures. Extensive experiments demonstrate that ASM-UNet consistently outperforms stateof-the-art methods in both coarse-grained and fine-grained segmentation tasks. However, all methods, including ours, show limited performance on two particularly challenging categories, CD and RHD, due to their small size and highly variable spatial distribution. Addressing these cases remains an open challenge and an important direction for our future research."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Jiao, J. Zhang, W. Wen, W. Qin, and X. Chen, Prostate-specific membrane antigen-targeted surgery in prostate cancer: accurate identification, real-time diagnosis, and precise resection, Theranostics, vol. 14, no. 7, p. 2736, 2024. [2] R. Li et al., Nir-ii aiegens nanosystem for fluorescence and chemiluminescence synergistic imaging-guided precise resection in osteosarcoma surgery, Aggregate, vol. 6, no. 1, p. e658, 2025. [3] R. Azad et al., Medical image segmentation review: The success of u-net, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 1007610095, 2024. [4] A. Shaker, M. Maaz, H. Rasheed, S. Khan, M.-H. Yang, and F. S. Khan, Unetr++: delving into efficient and accurate 3d medical image segmentation, IEEE Transactions on Medical Imaging, vol. 43, no. 9, pp. 33773390, 2024. [5] Y. Yang et al., Fsvs-net: few-shot semi-supervised vessel segmentation network for multiple organs based on feature distillation and bidirectional weighted fusion, Information Fusion, p. 103281, 2025. [6] X. Fang and P. Yan, Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction, IEEE Transactions on Medical Imaging, vol. 39, no. 11, pp. 36193629, 2020. [7] J. Ma et al., Abdomenct-1k: Is abdominal organ segmentation solved problem? IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 10, pp. 66956714, 2021. [8] H. Liu et al., Cosst: Multi-organ segmentation with partially labeled datasets using comprehensive supervisions and self-training, IEEE Transactions on Medical Imaging, vol. 43, no. 5, pp. 19952009, 2024. [9] T. Nakamura et al., Anatomical variations and surgical strategies in right lobe living donor liver transplantation: lessons from 120 cases1, Transplantation, vol. 73, no. 12, pp. 18961903, 2002. [10] G. Varotti et al., Anatomic variations in right liver living donors1, Journal of the American College of Surgeons, vol. 198, no. 4, pp. 577582, 2004. [11] B. Kayalibay, G. Jensen, and P. van der Smagt, Cnn-based segmentation of medical imaging data, arXiv preprint arXiv:1701.03056, 2017. [12] O. Ronneberger, P. Fischer, and T. Brox, U-net: Convolutional networks for biomedical image segmentation, in Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer, 2015, pp. 234241. [13] Z. Zhou, M. M. Rahman Siddiquee, N. Tajbakhsh, and J. Liang, Unet++: nested u-net architecture for medical image segmentation, in Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: 4th International Workshop, DLMIA 2018, and 8th International Workshop, ML-CDS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 20, 2018, Proceedings 4. Springer, 2018, pp. 311. [14] H. Huang et al., Unet 3+: full-scale connected unet for medical image segmentation, in ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2020, pp. 10551059. 14 [15] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, Uctransnet: rethinking the skip connections in u-net from channel-wise perspective with transformer, in Proceedings of the AAAI conference on artificial intelligence, vol. 36, no. 3, 2022, pp. 24412449. [16] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, vol. 30, 2017. [17] O. Oktay et al., Attention u-net: Learning where to look for the pancreas, arXiv preprint arXiv:1804.03999, 2018. [18] X. Huang, Z. Deng, D. Li, X. Yuan, and Y. Fu, Missformer: An effective transformer for 2d medical image segmentation, IEEE Transactions on Medical Imaging, vol. 42, no. 5, pp. 14841494, 2022. [19] H.-Y. Zhou et al., nnformer: volumetric medical image segmentation via 3d transformer, IEEE Transactions on Image Processing, vol. 32, pp. 40364045, 2023. [20] B. Wang et al., BiCLIP-nnFormer: Virtual Multimodal Instrument for Efficient and Accurate Medical Image Segmentation, Instrumentation, vol. 12, no. 2, 2025. [21] A. Gu and T. Dao, Mamba: Linear-time sequence modeling with selective state spaces, arXiv preprint arXiv:2312.00752, 2023. [22] H. Gong et al., nnmamba: 3d biomedical image segmentation, classification and landmark detection with state space model, in 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI). IEEE, 2025, pp. 15. [23] Z. Xing, T. Ye, Y. Yang, G. Liu, and L. Zhu, Segmamba: Long-range sequential modeling mamba for 3d medical image segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 578588. [24] J. Liu et al., Swin-umamba: Adapting mamba-based vision foundation models for medical image segmentation, IEEE Transactions on Medical Imaging, early access, 2024. [25] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, Vision mamba: Efficient visual representation learning with bidirectional state space model, arXiv preprint arXiv:2401.09417, 2024. [26] T. Huang, X. Pei, S. You, F. Wang, C. Qian, and C. Xu, Localmamba: Visual state space model with windowed selective scan, arXiv preprint arXiv:2403.09338, 2024. [27] Y. Ren, X. Li, M. Guo, B. Li, S. Zhao, and Z. Chen, Mambacsr: Dual-interleaved scanning for compressed image super-resolution with ssms, arXiv preprint arXiv:2408.11758, 2024. [28] B. Landman, Z. Xu, J. Igelsias, M. Styner, T. Langerak, and A. Klein, Miccai multi-atlas labeling beyond the cranial vaultworkshop and challenge, in Proc. MICCAI Multi-Atlas Labeling Beyond Cranial VaultWorkshop Challenge, vol. 5, 2015, p. 12. [29] O. Bernard et al., Deep learning techniques for automatic mri cardiac multi-structures segmentation and diagnosis: is the problem solved? IEEE Transactions on Medical Imaging, vol. 37, no. 11, pp. 25142525, 2018. [30] J. Long, E. Shelhamer, and T. Darrell, Fully convolutional networks for semantic segmentation, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 34313440. [31] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, nnu-net: self-configuring method for deep learning-based biomedical image segmentation, Nature methods, vol. 18, no. 2, pp. 203211, 2021. [32] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, convnet for the 2020s, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 11 97611 986. [33] Y. Xie, J. Zhang, C. Shen, and Y. Xia, Cotr: Efficiently bridging cnn and transformer for 3d medical image segmentation, in Medical Image Computing and Computer Assisted InterventionMICCAI 2021: 24th International Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part III 24. Springer, 2021, pp. 171180. [34] A. Hatamizadeh et al., Unetr: Transformers for 3d medical image segmentation, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2022, pp. 574584. [35] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. R. Roth, and D. Xu, Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images, in International MICCAI brainlesion workshop. Springer, 2021, pp. 272284. [36] J. Ruan and S. Xiang, Vm-unet: Vision mamba unet for medical image segmentation, arXiv preprint arXiv:2402.02491, 2024. [37] J. Liu et al., Swin-umamba: Mamba-based unet with imagenet-based pretraining, in International conference on medical image computing and computer-assisted intervention. Springer, 2024, pp. 615625. [38] Y. Yang, Z. Xing, and L. Zhu, Vivim: video vision mamba for medical video object segmentation, arXiv preprint arXiv:2401.14168, 2024. [39] J. Ma, F. Li, and B. Wang, U-mamba: Enhancing long-range dependency for biomedical image segmentation, arXiv preprint arXiv:2401.04722, 2024. [40] H. Gong, L. Kang, Y. Wang, X. Wan, and H. Li, nnmamba: 3d biomedical image segmentation, classification and landmark detection with state space model, arXiv preprint arXiv:2402.03526, 2024. [41] J. Wang, J. Chen, D. Chen, and J. Wu, Lkm-unet: Large kernel vision mamba unet for medical image segmentation, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2024, pp. 360370. [42] L. Soler et al., 3d image reconstruction for comparison of algorithm database, URL: https://www. ircad. fr/research/data-sets/liversegmentation-3d-ircadb-01, 2010. [43] X. Luo et al., Word: large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image, Medical Image Analysis, vol. 82, p. 102642, 2022. [44] J. Chen et al., Transunet: Rethinking the u-net architecture design for medical image segmentation through the lens of transformers, Medical Image Analysis, vol. 97, p. 103280, 2024. [45] H. Cao et al., Swin-unet: Unet-like pure transformer for medical image segmentation, in European conference on computer vision. Springer, 2022, pp. 205218. [46] G. Xu, X. Zhang, X. He, and X. Wu, Levit-unet: Make faster encoders with transformer for medical image segmentation, in Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Springer, 2023, pp. 4253. [47] S. Perera, P. Navard, and A. Yilmaz, Segformer3d: an efficient transformer for 3d medical image segmentation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 49814988."
        }
    ],
    "affiliations": [
        "Institute or affiliation not explicitly listed in the provided text"
    ]
}