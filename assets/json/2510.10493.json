{
    "paper_title": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution",
    "authors": [
        "Norbert Tihanyi",
        "Bilel Cherif",
        "Richard A. Dubniczky",
        "Mohamed Amine Ferrag",
        "Tamás Bisztray"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present the first large-scale study exploring whether JavaScript code generated by Large Language Models (LLMs) can reveal which model produced it, enabling reliable authorship attribution and model fingerprinting. With the rapid rise of AI-generated code, attribution is playing a critical role in detecting vulnerabilities, flagging malicious content, and ensuring accountability. While AI-vs-human detection usually treats AI as a single category we show that individual LLMs leave unique stylistic signatures, even among models belonging to the same family or parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000 Node.js back-end programs from 20 large language models. Each has four transformed variants, yielding 250,000 unique JavaScript samples and two additional representations (JSIR and AST) for diverse research applications. Using this dataset, we benchmark traditional machine learning classifiers against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom architecture derived from the 770M-parameter CodeT5 model with its decoder removed and a modified classification head. It achieves 95.8% accuracy on five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks, surpassing other tested models such as BERT, CodeBERT, and Longformer. We demonstrate that classifiers capture deeper stylistic regularities in program dataflow and structure, rather than relying on surface-level features. As a result, attribution remains effective even after mangling, comment removal, and heavy code transformations. To support open science and reproducibility, we release the LLM-NodeJS dataset, Google Colab training scripts, and all related materials on GitHub: https://github.com/LLM-NodeJS-dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 3 9 4 0 1 . 0 1 5 2 : r The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution Bilel Cherif Richard A. Dubniczky Technology Innovation Institute Eötvös Loránd University Abu Dhabi, United Arab Emirates Budapest, Hungary bilel.cherif@tii.ae richard@dubniczky.com Norbert Tihanyi Technology Innovation Institute Abu Dhabi, United Arab Emirates norbert.tihanyi@tii.ae Mohamed Amine Ferrag United Arab Emirates University Al Ain, United Arab Emirates mohamed.ferrag@uaeu.ac.ae Tamás Bisztray University of Oslo Oslo, Norway tamasbi@ifi.uio.no Abstract In this paper, we present the first large-scale study exploring whether JavaScript code generated by Large Language Models (LLMs) can reveal which model produced it, enabling reliable authorship attribution and model fingerprinting. With the rapid rise of AI-generated code, attribution is playing critical role in detecting vulnerabilities, flagging malicious content, and ensuring accountability. While AI-vs-human detection usually treats AI as single category we show that individual LLMs leave unique stylistic signatures, even among models belonging to the same family or parameter size. To this end, we introduce LLM-NodeJS, dataset of 50,000 Node.js back-end programs from 20 large language models. Each has four transformed variants, yielding 250,000 unique JavaScript samples and two additional representations (JSIR and AST) for diverse research applications. Using this dataset, we benchmark traditional machine learning classifiers against fine-tuned Transformer encoders and introduce CodeT5-JSA, custom architecture derived from the 770M-parameter CodeT5 model with its decoder removed and modified classification head. It achieves 95.8% accuracy on five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks, surpassing other tested models such as BERT, CodeBERT, and Longformer. We demonstrate that classifiers capture deeper stylistic regularities in program dataflow and structure, rather than relying on surface-level features. As result, attribution remains effective even after mangling, comment removal, and heavy code transformations. To support open science and reproducibility, we release the LLM-NodeJS dataset, Google Colab training scripts, and all related materials on GitHub: https://github.com/LLM-NodeJS-dataset. Keywords AI-generated, code stylometry, LLM, authorship attribution"
        },
        {
            "title": "1 Introduction\nAuthorship attribution [22, 24], often referred to as stylometry, is\nthe task of determining the author of a document, text, or piece of\nsource code by analyzing stylistic and structural features. In most\nexisting studies on human versus AI authorship, the AI-generated\ncategory is represented by a single model [9, 19, 29, 33] or, at best,\na small set of models [8], thereby limiting the robustness of the\nresulting methods [31].",
            "content": "Corresponding author 1 In human code attribution, typically only limited samples per author are available [1, 11], while large language models (LLMs) allow the generation of virtually unlimited, task-specific datasets for mapping their coding style. However, because many LLMs are trained on overlapping code corpora [25], they may inherit broad mixture of stylistic and structural characteristics. With the growing presence of AI-generated content online, researchers have also raised concerns about recursive training and eventual style convergence among models [27, 35]. If such convergence occurs, model-level attribution, or even model family detection may become increasingly difficult. Big-tech leaders predict that AI will soon generate most of our source code [4, 5, 28]. Thus, robust attribution in LLMs is urgently needed for critical domains like software forensics, malware analysis, cybercrime investigations, and academic integrity [30]. Recent work by Bisztray et al. [6] shows that different LLMs leave distinguishable fingerprints when writing code, achieving 94.5% accuracy in telling apart code generated by five leading LLMs. These findings highlight that attribution research must extend beyond the binary AI-vs-human setting to richer tasks such as model-level attribution, family-level attribution, co-authorship attribution, obfuscation-robust attribution, and malware/threat actor attribution. These tasks differ not only in dataset accessibility but also in their sensitivity to model evolution, as LLM fingerprints can change rapidly with the introduction of new models. In [6], code stylometry focused on distinguishing code among five LLM families (Claude-3.5, DeepSeek-v3.1, Gemini-2.5, GPT4.1, LLaMA-3.3-70B). Extending this line of work, we focus on JavaScripta flexible, dynamically typed language whose diverse styles and widespread tooling make attribution more challenging. We further investigate whether stylometric attribution is possible within the same model family (GPT-4o, GPT-4o-mini, GPT-4.1, GPT5, gpt-oss) and explore scaling to larger classification tasks (1020 classes). To this end, we introduce the first diverse AI-generated JavaScript dataset for authorship attribution and address the following research questions: Bisztray T., Cherif B., Dubniczky R. A., Gruschka N., Borsos B., Ferrag M. A., Kovács A., Mavroeidis V., Tihanyi N. Figure 1: The seven-step methodology for LLM authorship attribution in JavaScript code. RQ1: Which machine learning and transformer-based approaches achieve the most robust performance for black-box, model-level JavaScript source code attribution? RQ2: What signals do deep-learning methods exploit to distinguish between different models? To answer these questions, we follow the methodology shown in Figure 1. On high level, our pipeline begins with prompt creation for back-end Node.js coding tasks and code generation from 20 stateof-the-art LLMs. The outputs are syntax-checked, deduplicated, and transformed into multiple JavaScript variants. These are then used to train both traditional Machine Learning (ML) models and transformer-based models for authorship attribution, which are subsequently evaluated and validated on an independent dataset. The detailed methodology is introduced in Section 3. In this paper, we present the following key contributions: LLM-NodeJS dataset: We release public dataset of 50,000 syntactically correct and executable Node.js back-end programs, expanded with four additional code variants (minified, mangled, obfuscated, deobfuscated), resulting in 250,000 JavaScript samples. Covering 20 state-of-the-art models and diverse web tasks, the dataset also includes two additional representations JavaScript Intermediate Representation (JSIR) and Abstract Syntax Tree (AST), providing the first large-scale foundation for wide range of JavaScript research, not limited to code authorship. Classifier evaluation: Using the LLM-NodeJS dataset, we benchmark traditional ML classifiers (Random Forest, XGBoost, KNN, Logistic Regression, Linear SVM) and finetuned transformers (i.e., BERT, CodeBERT, Longformer), including custom CodeT5-Authorship model with modified classification head and removed decoder layers. We show that JavaScript attribution is highly effective, achieving over 95% accuracy across five models within the same GPT family (GPT-4.1, GPT-4o, GPT-4o-mini, GPT-5-nano, GPT-5-mini, GPT-oss-120b) and nearly 90% for 20-class attribution. Robustness to code variants: Attribution remains accurate on minified (91%), terser-mangled (93%), and deobfuscated (85%) code, while obfuscation lowers accuracy due to token inflation. These results show that classifiers rely on deeper structural signalssuch as AST structure and dataflow graphsrather than shallow stylistic features explored in prior work Open science: To support reproducibility, we release the LLM-NodeJS dataset along with all Google Colab notebooks and training scripts on the project GitHub page: https:// github.com/LLM-NodeJS-dataset."
        },
        {
            "title": "Key Takeaway",
            "content": "The key insight of our work is that individual LLMs leave distinctive, structurally grounded fingerprints, enabling reliable model-level attribution beyond traditional AI-vshuman detection. As AI-generated code becomes standard, the key question shifts from whether it was written by AI to which AI produced it, highlighting the need to treat the AI-generated category as diverse rather than uniformly representing it with one model. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 outlines the methods used to construct the dataset and to train the classifiers. Section 4 presents experimental results, Section 5 discusses limitations and future work, while Section 6 concludes the paper."
        },
        {
            "title": "2 Related Work\nThis section reviews authorship attribution methods for source\ncode, organized by technique family. We then discuss benchmark\ndatasets and tasks.",
            "content": "2 The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution"
        },
        {
            "title": "2.1.3 Pre-trained and Generative Models. Pre-trained models (i.e.,\nBERT, CodeBERT, RoBERTa) learn contextual embeddings from\nlarge code corpora, yielding state-of-the-art attribution with mini-\nmal manual features [13, 16, 41]. Fine-tuned CodeT5 variants achieve\nstrong accuracy even across closely related authors [21].",
            "content": "Generative LLMs (e.g., GPT-4) support zero/few-shot attribution, reaching 6569% on hundreds of authors [12]. Beyond human code, Bukhari et al. distinguished AIfrom human-written code at 92% [8], while Idialu et al. reported F1 0.91 for GPT-4 vs. human Python [23], though most treat AI as single-source class. Bisztray et al. extended this to the AI-vs-AI setting, with modified CodeT5 surpassing 95% accuracy across five LLMs from different model families [6]. Together, these results show both human and machine code leave detectable stylistic signatures."
        },
        {
            "title": "2.2.4 Execution and Binary-Level Approaches. Dynamic analysis\nleverages runtime traces (system calls, memory, performance) to\nreveal algorithmic signatures resistant to superficial edits [40]. How-\never, sandboxing is costly and mostly limited to malware foren-\nsics [14, 15, 37]. Attribution also extends to binaries: stylistic mark-\ners often survive compilation [20, 34], and despite compiler vari-\nability, accuracy remains above chance [15].",
            "content": ""
        },
        {
            "title": "3.1 Dataset Creation- The LLM-NodeJS dataset\nWe designed a set of 250 complex back-end Node.js tasks inspired\nby real-world scenarios encountered by professional developers.\nFormally, let",
            "content": "T = {𝑇1,𝑇2, . . . ,𝑇250} denote the set of tasks. Each task 𝑇𝑖 is decomposed into collection of smaller subtasks, 𝑇𝑖 = {𝑡𝑖1, 𝑡𝑖2, . . . , 𝑡𝑖𝑚𝑖 }, where 𝑚𝑖 is the number of subtasks in task 𝑇𝑖 . These subtasks represent functionally meaningful components (e.g., writing GitHub uploader, creating URL shortener service with local SQLite database) and introduce substantial structural and stylistic diversity, ensuring that no two generated programs in the LLM-NodeJS dataset are identical. All generated JavaScript implicitly uses strict mode and ES6 imports instead of require. For example, one task involved creating JWT validator service that verifies RSA-signed tokens and returns JSON status, comprising dozens of smaller subtasks. For each task 𝑇𝑖 , we generated 𝑘 = 10 independent implementations per model, yielding D𝑚 = 𝑘 = 10 250 = 2,500 samples per model 𝑚. Across twenty LLMs spanning eight model families (c.f. Figure 1), this resulted in total of 50,000 unique programs. Only JavaScript snippets that were syntactically correct and passed \"node check\" command were included in the dataset. To assess the effect of obfuscation on attribution accuracy, we created six variants of each sample: (1) JavaScript Minified whitespace, comments, and formatting removed. (2) Terser Mangled - identifiers shortened systematically via Terser. (3) Obfuscated JavaScript - transformed using techniques such as control-flow flattening or string encoding. (4) De-Obfuscated JavaScript - partially restored versions improving readability through identifier recovery and reformatting. (5) JavaScript JSIR - intermediate representation for program analysis and attribution. Bisztray T., Cherif B., Dubniczky R. A., Gruschka N., Borsos B., Ferrag M. A., Kovács A., Mavroeidis V., Tihanyi N. (6) JavaScript AST - structured tree representation capturing syntax and hierarchy. training time to 111 minutes, with only 13% accuracy drop, which is still acceptable given the substantial training time gain. The dataset includes the validated original programs plus four syntactically correct variants (minified, mangled, obfuscated, deobfuscated), comprising total of 250,000 JavaScript samples, as well as two additional representations (JSIR and AST) that support wide range of JavaScript analysis research."
        },
        {
            "title": "3.2.2 Transformer-based Models . We evaluate three encoder-only\narchitectures [39], including BERT, CodeBERT, and Longformer. En-\ncoder models are typically favored for classification, while decoder\nmodels are preferred for generation with long context.",
            "content": "All transformer fine-tuning experiments were optimized for NVIDIA A100 (80 GB) GPUs. Batch sizes, gradient accumulation, and mixed-precision settings were adjusted to fully utilize available memory while avoiding overflow. In particular, training was performed in bf16 precision with fused AdamW optimization and cosine restarts for learning rate scheduling. For BERT, we used an effective batch size of 32 (achieved through 16 2 gradient accumulation), whereas Longformer leveraged its larger context window with batch size of 32 and no accumulation."
        },
        {
            "title": "3.2.3 Custom CodeT5-JSA Architecture. In [6], the authors trans-\nformed CodeT5+ (770M parameters) into an encoder-only model\nby removing the decoder layer and attaching a lightweight classifi-\ncation head (H1) consists of:",
            "content": "Linear layer GELU Dropout(0.2) Linear layer For the ten and twenty class scenario the above setting has started dropping accuracy, therefore here we revised the classification head: Dropout(0.2) Linear layer Tanh Dropout(0.2) Linear layer In both settings the last layer is followed by Softmax activation to produce class probabilities. Replacing the GELU activation layer with Tanh and introducing an initial dropout layer improved stability and overall accuracy. For example on the deobfuscated twenty class scenario accuracy jumped from 72% to 79.94%. In terms of optimisation, if full FP32 is used, the training time is 958 minutes, whereas switching to bfloat16 reduces the 20-class"
        },
        {
            "title": "4 Experimental Results\nTo answer our research questions, we conducted our experiments\nin two stages. The first is geared towards research question 1, where\nwe investigate which machine learning and transformer-based ar-\nchitectures perform best in attributing JavaScript code (along with\nthe obfuscated versions) to the original generator model.",
            "content": "Next, we aim to investigate if attribution performance holds even when there are more classes, and whether there are certain models, that are too similar to each-other to be accurately distinguished."
        },
        {
            "title": "4.1 Dataset diversity analysis\nBefore starting the classification process, we assessed the dataset’s\ndiversity. As discussed in the methodology section, each task is\nposed ten times to the same model, enabling us to measure vari-\nability in the generated JavaScript programs. To quantify this di-\nversity—both within and across model families—we used three\nCodeBLEU components: N-gram match (lexical overlap on tokens,\nidentifiers, and keywords), Syntax match (structural overlap via\nASTs), and Dataflow match (semantic overlap via variable de-\npendencies and data/control flow, e.g., Jaccard over dependency\ngraphs). Table 1 reports median similarities for GPT-4o, GPT-5, and\nGemini. We summarize intra–vs.–inter model separation by the\naverage gap Δ ((cid:205) intra-model medians - (cid:205) inter-model medians).\nLower similarity indicate greater diversity in the code, whereas for Δ\nhigher values reflect stronger discriminative power between models.",
            "content": "Table 1: Median CodeBLEU Similarities and IntraInter Gaps for JavaScript Programs"
        },
        {
            "title": "Model Pair",
            "content": "N-gram Match"
        },
        {
            "title": "Dataflow Match",
            "content": "Intra-Model Comparisons Gemini Gemini GPT-4o GPT-4o GPT-5 GPT-5 0.29 0. 0.16 0.58 0.60 0.60 Inter-Model Comparisons Gemini GPT-4o Gemini GPT-5 GPT-4o GPT-5 Intra-Model Avg. Inter-Model Avg. Gap (Δ) 0.09 0. 0.01 0.25 0.04 0.20 0.49 0. 0.38 0.59 0.43 0.16 0.40 0. 0.35 0.31 0.16 0.11 0.40 0. 0."
        },
        {
            "title": "4.1.1 Key Results. Dataflow similarity provides the strongest dis-\ncrimination between same and cross-model pairs, representing\nthe clearest model-specific signal at the semantic level. Programs\ncompared within the same model are semantically aligned (e.g.,\nGPT-4o × GPT-4o median 0.46), whereas cross-family pairs can be\nmarkedly divergent (GPT-4o × GPT-5 median 0.11). N-gram simi-\nlarity shows a comparable gap but is sensitive to surface-level edits.",
            "content": "4 The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution Syntax yields the smallest gap, not because it lacks discriminative power, but because structural patterns are relatively stable across tasks. Implications for attribution. These results indicate that model 4.1.2 attribution will be most reliable when grounded in deeper semantic and structural signals rather than surface-level cues. Even when stylistic features are obfuscated or lexical patterns altered, the underlying logic and structural organization of code often remain detectable. Dataflow similarity thus provides the most obfuscationresilient indicator of model identity, capturing consistent reasoning and dependency patterns. Syntax similarity offers complementary stability, reflecting structural regularities that persist despite superficial transformations, while n-gram similarityalthough highly discriminative in clean conditionsdegrades rapidly under lexical variation."
        },
        {
            "title": "4.1.3 Token length distribution. Figure 2 shows the distribution\nof tokenized sequence lengths in our dataset, which informs ar-\nchitecture choice: short to medium-length contexts fit well within\nencoder-only models.",
            "content": "Figure 2: Tokenized Dataset Statistics, where PX marks that percentage of samples fall below that token count. In practice, trade-offs between context length, attention mechanism, and pretraining objectives determine how effectively stylistic signals are captured. Prior work [6] finds that representation quality can outweigh raw context window size. For example, 512-token modified CodeT5 outperforms Longformer (which has larger context-window)."
        },
        {
            "title": "4.2 JavaScript Authorship Attribution\nHere, we present our key findings across different scenarios: (i)\n5-class attribution, (ii) 10-class attribution, and (iii) 20-class attribu-\ntion.",
            "content": "Five-Class Attribution for Different Model Families. We first 4.2.1 replicate the setup of [6], where the base BERT model achieved an accuracy of 0.92 on five-way attribution task assigning program code to one of five generator LLMs from distinct model families. Our selected models are gpt-5-mini, gemini-2.5-flash-lite, qwen-2.5-coder-32b, llama-3.3-70b, and deepseek-v3.1. This dataset consists of 12,500 JavaScript programs (5 2,500), split into 80% training and 20% validation. In this setting, BERT𝐵 achieved 97.9% accuracy, F1, and precision/recall, confirming that LLMs from different families exhibit distinct and learnable stylistic signatures in JavaScript code generation, just like in C. Since we already achieved very high accuracy in this scenario, we shifted our focus to more challenging problem, where all five models belong to the same family. Five-Class Attribution for Same Model Families. Next, we test 4.2.2 five-class attribution with GPT variants only. Here, BERT𝐵 still attains 90.2% accuracy, indicating that even closely related LLMs leave detectable stylistic fingerprints. For complete comparison of model performance on this task, see the summary Table 2. Because we observe an approximate 8% drop in accuracy compared to the different model family classification task, we evaluated range of models to determine the best achievable performance. Figure 3 suggests that most errors arise from confusion between closely related variants of the same release (like GPT-4o vs. GPT4o-mini, or among GPT-5 variants), while architecturally distinct versions can be distinguished more easily. Variants of the same model likely share similar pre-training corpora and compute budgets. We hypothesize that their Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) pipelines are also largely aligned. For example, GPT-4o and GPT-4o-mini may have been trained primarily on mid-2023 data, while GPT-5 variants likely incorporate early-2025 GitHub snapshots and more diverse coding examples, along with refined SFT and RLHF stages. As AI-generated code becomes increasingly widespread, it remains uncertain whether model styles will converge into uniform pattern or remain distinguishable through continued humanengineering interventions. We predict that as systems approach AGI, models will co-adapt and self-optimize, gradually blurring stylistic boundaries. Nonetheless, architectural and tokenization Figure 3: Confusion matrix for five-class attribution within the GPT family using BERT𝐵 on js_original 5 Bisztray T., Cherif B., Dubniczky R. A., Gruschka N., Borsos B., Ferrag M. A., Kovács A., Mavroeidis V., Tihanyi N. Table 2: 5/10/20 class attribution by JavaScript Variant (different model families) Model Name Type 5-Class (GPT family only) 10-Class 20-Class Parameters Acc(%) Prec(%) F"
        },
        {
            "title": "Time",
            "content": "Acc(%) Prec(%) F"
        },
        {
            "title": "Time",
            "content": "Acc(%) Prec(%) F"
        },
        {
            "title": "Time",
            "content": "CodeT5-JSA Longformer𝐵 CodeBERT BERT𝐵 XGBoost"
        },
        {
            "title": "KNN",
            "content": "CodeT5-JSA Longformer𝐵 CodeBERT BERT𝐵 XGBoost Random Forest Linear SVM Logistic Regression KNN CodeT5-JSA Longformer𝐵 CodeBERT BERT𝐵 XGBoost Random Forest Linear SVM Logistic Regression KNN"
        },
        {
            "title": "LLM",
            "content": "ML ML ML ML ML"
        },
        {
            "title": "LLM",
            "content": "LLM LLM LLM ML ML ML ML ML LLM LLM LLM LLM ML ML ML ML ML 95. 94.28 94.28 90.24 88.72 86.24 82. 80.40 71.32 90.02 89.56 89.28 86. 86.48 83.04 79.08 76.88 67.84 89. 88.32 86.76 84.08 86.48 83.44 78. 77.76 70.28 95.89 94.38 94.40 90. 88.76 86.27 82.60 80.44 71.41 90. 89.67 89.54 86.81 86.51 83.05 79. 76.80 67.92 90.40 88.42 86.78 84. 86.53 83.47 78.58 77.72 70.30 95. 94.28 94.27 90.21 88.74 86.25 82. 80.40 71.21 90.03 89.56 89.26 86. 86.49 82.99 78.97 76.79 67.55 89. 88.32 86.74 84.08 86.49 83.41 78. 77.67 70.06 ORIGINAL JAVASCRIPT RESULTS 93.76 93.60 93.58 55: 28:15 127:23 07:00 06:54 00:49 00: 00:01 00:10 00:01 93.62 91.88 89. 86.60 81.20 76.44 73.82 60.74 92. 92.11 89.76 86.67 81.85 75.83 73. 61.49 92.65 91.86 89.55 86.46 80. 75.83 73.33 59.99 262:25 13:15 13: 02:23 01:15 00:02 00:14 00:01 MANGLED JAVASCRIPT RESULTS 89. 89.68 89.64 55:58 28:16 127:46 06: 06:34 00:51 00:31 00:01 00:11 00: 89.58 89.36 86.24 84.80 79.72 74. 71.60 57.74 89.87 89.89 86.46 84. 80.55 74.59 71.27 58.77 89.69 89. 86.30 84.68 79.31 74.31 71.17 56. 253:41 13:00 12:58 150.54 68.30 2. 16.60 0.01 DEOBFUSCATED JAVASCRIPT RESULTS 90.00 56:08 89.78 89. 28:12 127:14 88. 76 06:41 06:55 00: 00:30 00:01 00:08 00:00 84.50 82. 83.16 79.22 73.56 71.40 60.40 89. 84.60 82.84 83.28 80.06 73.34 71. 61.47 88.84 84.33 82.63 83.00 78. 73.03 70.96 59.64 254:13 13:17 12: 156.11 65.69 2.63 14.59 0.01 85. 86.79 85.87 81.99 75.78 68.25 58. 56.43 48.03 83.61 80.78 81.28 76. 73.29 66.93 55.77 53.68 45.39 79. 78.15 75.46 72.61 72.98 66.35 54. 53.43 47.63 86.52 86.95 86.22 82. 75.73 68.28 58.17 55.61 49.70 83. 81.02 81.49 76.70 73.08 66.88 54. 52.82 45.91 79.79 78.02 75.33 72. 72.88 66.51 53.39 52.44 48.05 85: 111:29 Layers:24, Token:512 86.69 85.74 81.93 75. 67.19 57.73 55.71 46.34 83.41 80. 81.13 76.47 73.04 65.98 54.27 52. 43.20 79.60 77.87 75.03 72.40 72. 65.42 53.12 52.66 45.63 507:52 Layers:12, Token: 25:08 24:55 08:31 02:56 00:12 00: 00:01 Layers:12, Token:512 Layers:12, Token:512 Estimators: 400 Trees: 400 Max Iteration: Max Iteration: 2000 Neighbors: 5 112:02 Layers:24, Token:512 507:23 Layers:12, Token: 25:26 25:07 Layers:12, Token:512 Layers:12, Token:512 595.40 Estimators: 168.78 Trees: 400 15.28 21.45 0.01 Max Iteration: Max Iteration: 2000 Neighbors: 5 112:44 Layers:24, Token:512 508:35 Layers:12, Token: 25:16 24:44 Layers:12, Token:512 Layers:12, Token:512 504.17 Estimators: 166.82 Trees: 400 13.91 19.42 0.01 Max Iteration: Max Iteration: 2000 Neighbors: 5 differences are expected to preserve some degree of distinguishability, if not through code style, then through higher-level moral, ethical, or policy biases imparted by their creators. remaining efficient, illustrating favorable balance between accuracy and runtime. Surprisingly, on mangled and deobfuscated JavaScript code XGBoost managed to outperform the Bert𝐵 model on several occasions."
        },
        {
            "title": "4.2.3 Ten and Twenty-Class Attribution. As the attribution task\nexpands from five to ten and twenty classes, overall accuracy de-\ncreases across all models. Transformer-based encoders (CodeT5-JSA,\nLongformer𝐵, CodeBERT) consistently outperform classical ML base-\nlines across all settings.",
            "content": "Among all models, CodeT5-JSA shows the best overall performance, maintaining 94% accuracy in the ten-class setup and above 88% in the twenty-class case for the original JavaScript. Longformer𝐵 achieves comparable results but at substantially higher computational cost. Traditional ML classifiers such as XGBoost and Random Forest remain competitive yet degrade more rapidly as class count increases, and linear methods (SVM, LogReg) fall below 60% at twenty classes. XGBoost consistently outperformed other ML models while"
        },
        {
            "title": "4.2.4 Different JavaScript Variants. To assess robustness, we evalu-\nate attribution on terser-mangled and deobfuscated code (Table 2).\nSince performance on mangled and minified JavaScript was nearly\nidentical, we report results for the mangled variant only. Although\nthese transformations eliminate key stylistic cues—such as variable\nand function names—accuracy remains high. This indicates that\nsurface-level features like naming patterns or token frequencies are\nnot the primary drivers of attribution. Instead, the classifiers exploit\ndeeper structural and semantic regularities that persist through\ncode transformations. When code is deobfuscated, accuracy recov-\ners substantially, confirming that restoring syntactic structure is\nsufficient to reestablish most discriminative patterns.",
            "content": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution These results extend the observations of [6] from to JavaScript, demonstrating that attribution remains feasible both when the number of models scale, and with different transformations. Even as lexical information is deliberately suppressed, model-specific signatures persist through structural and dataflow patterns. Overall, these findings confirm that attribution grounded in semantic and syntactic organization remains tractable and robust even under aggressive obfuscation and fine-grained class expansion."
        },
        {
            "title": "4.3 Cross-check Validation\nAll JavaScript samples in the LLM-NodeJS dataset were generated\nthrough OpenRouter.ai. To ensure that this service does not wa-\ntermark its outputs, we created a new dataset of 500 JavaScript\nsamples corresponding to completely unseen tasks. These samples,\nwhich include Node.js–specific programs, were generated using\nthe original OpenAI API—entirely independent from the Open-\nRouter.ai service—and do not overlap with the 50,000 samples used\nfor training or evaluation.",
            "content": "We re-evaluated the five-class classifier using CodeBERT on the original JavaScript variant. It maintained high attribution performance, with only 12% drop compared to the results observed in our experiments. This confirms that attribution generalizes well to previously unseen code distributions. The validation dataset is publicly released as CROSS_CHECK_DATASET.json on GitHub."
        },
        {
            "title": "5 Limitations and Future Work\nThis study leaves several open questions that point to promising\ndirections for future research:",
            "content": "Model scale and architectures: Our experiments were limited to moderate-sized Transformer encoders such as BERT and CodeT5. Larger or extended-context architectures capable of processing JSIR and AST variants were not explored due to computational constraints. Future work should examine how model size, context length, and decoderonly architectures affect attribution accuracy. We did not conduct experiments on AST and JSIR representations, but made these variants available for the research community. Language and data diversity: The dataset is restricted to JavaScript, whereas prior work examined [6]. Extending attribution to other languages (C++, Rust, Python, Java) or to cross-language setupstraining on one language and testing on anotherwould help determine the generality of model fingerprints. Broader datasets covering varied coding domains and prompt styles would also strengthen external validity, as we only focused on web-deveopment. Attribution generalization: In the multi-class setting, the classifier assumes that all samples originate from the 20 studied LLMs. Future work should explore open-set recognition and continual learning to handle unseen models, as well as temporal drift studies to assess the stability of fingerprints as LLMs evolve. Interpretability and robustness. Attribution robustness should be tested not only under simple code transformations, but under adversarial modifications like transpilation, control-flow rewriting. 7 Ethical and practical considerations: Attribution systems may be vulnerable to deliberate style spoofing or adversarial evasion. Future work should investigate these risks and develop safeguards for responsible use in code forensics and AI provenance detection."
        },
        {
            "title": "6 Conclusion\nWe introduced the LLM-NodeJS dataset, a corpus of 250,000 vali-\ndated Node.js programs comprising 50,000 original JavaScript pro-\ngrams, each with four variants (minified, terser-mangled, obfus-\ncated, and deobfuscated), along with additional AST and JSIR rep-\nresentations for every sample. Using this dataset, we compared\nclassical ML algorithms with fine-tuned Transformer encoders (e.g.,\nBERT, CodeBERT, and Longformer) as well as a custom modified\narchitecture based on the 770M-parameter CodeT5 model.",
            "content": "RQ1: Can we reliably attribute JavaScript programs to their generating LLM? Answer: Yes. On original JavaScript, CodeT5-JSA attains 95.8% accuracy in the five-way GPT-only setting, 94.6% on the ten-class task, and 88.5% on twenty classes  (Table 2)  . Performance on terser-mangled, minified, and deobfuscated code remains close (typically single-digit drops), indicating that attribution does not depend primarily on surface naming or formatting. RQ2: What signals do the models rely on? Answer: Deeper structural and semantic cues are just as important as style. Our similarity analysis shows that dataflow provides the clearest same-vs.-cross model separation, along with n-gram, however, the absence of the latter still allows accurate attribution. These explain the robustness of attribution on mangled/minified code and the recovery after deobfuscation: classifiers exploit persistent logic, dependency structure, and control/dataflow rather than superficial lexical features. Overall, our results demonstrate that LLM-generated JavaScript carries persistent, model-specific fingerprints that remain detectable across families, class scales (ten and twenty classes), and common code transformations. While accuracy naturally declines as stylistic boundaries narrow, semantically grounded signals continue to support reliable attribution. References [1] Mohammed Abuhamad, Tamer AbuHmed, Aziz Mohaisen, and DaeHun Nyang. 2018. Large-Scale and Language-Oblivious Code Authorship Identification. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS 18). Association for Computing Machinery, New York, NY, USA, 101114. doi:10.1145/3243734. [2] Mohammed Abuhamad, Tamer Abuhmed, David Mohaisen, and Daehun Nyang. 2021. Large-scale and Robust Code Authorship Identification with Deep Feature Learning. ACM Trans. Priv. Secur. 24, 4 (July 2021), 23:123:35. doi:10.1145/ 3461666 [3] Bander Alsulami, Edwin Dauber, Richard Harang, Spiros Mancoridis, and Rachel Greenstadt. 2017. Source Code Authorship Attribution Using Long Short-Term Memory Based Networks. In Computer Security ESORICS 2017, Simon N. Foley, Dieter Gollmann, and Einar Snekkenes (Eds.). Springer International Publishing, Cham, 6582. doi:10.1007/978-3-319-66402-6_6 [4] Sam Altman. 2025. Sam Altman says AI is doing 50% of coding in compahttps://www.indiatoday.in/technology/news/story/sam-altman-saysnies. Bisztray T., Cherif B., Dubniczky R. A., Gruschka N., Borsos B., Ferrag M. A., Kovács A., Mavroeidis V., Tihanyi N. ai-is-doing-50-percent-coding-in-companies-warns-students-about-careerchoices-2696937-2025-03-21. Accessed: 2025-09-09. [5] Dario Amodei. 2025. Vibe Coding Is Coming for Engineering Jobs. https: //www.wired.com/story/vibe-coding-engineering-apocalypse. Accessed: 202509-09. [6] Tamas Bisztray, Bilel Cherif, Richard Dubniczky, Nils Gruschka, Bertalan Borsos, Mohamed Amine Ferrag, Attila Kovacs, Vasileios Mavroeidis, and Norbert Tihanyi. 2025. Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution. arXiv preprint arXiv:2506.17323 (2025). [7] Egor Bogomolov, Vladimir Kovalenko, Yurii Rebryk, Alberto Bacchelli, and Timofey Bryksin. 2021. Authorship Attribution of Source Code: Language-Agnostic Approach and Applicability in Software Engineering. In Proceedings of the 29th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 21). Association for Computing Machinery, Athens, Greece, 932944. doi:10.1145/3468264.3468606 [8] Sufiyan Bukhari, Benjamin Tan, and Lorenzo De Carli. 2023. Distinguishing AIand Human-Generated Code: Case Study. In Proceedings of the 2023 Workshop on Software Supply Chain Offensive Research and Ecosystem Defenses (Copenhagen, Denmark) (SCORED 23). Association for Computing Machinery, New York, NY, USA, 1725. doi:10.1145/3605770.3625215 [9] Luana Bulla, Alessandro Midolo, Misael Mongiovì, and Emiliano Tramontana. 2024. EX-CODE: Robust and Explainable Model to Detect AI-Generated Code. Information 15, 12 (2024). doi:10.3390/info15120819 [10] Aylin Caliskan, Fabian Yamaguchi, Edwin Dauber, Richard Harang, Konrad Rieck, Rachel Greenstadt, and Arvind Narayanan. 2018. When Coding Style Survives Compilation: De-anonymizing Programmers from Executable Binaries. In Proceedings 2018 Network and Distributed System Security Symposium. Internet Society, San Diego, CA, 15 pages. doi:10.14722/ndss.2018. [11] Aylin Caliskan-Islam, Richard Harang, Andrew Liu, Arvind Narayanan, Clare Voss, Fabian Yamaguchi, and Rachel Greenstadt. 2015. De-anonymizing programmers via code stylometry. In Proceedings of the 24th USENIX Conference on Security Symposium (Washington, D.C.) (SEC15). USENIX Association, USA, 255270. [12] Soohyeon Choi, Yong Kiam Tan, Mark Huasong Meng, Mohamed Ragab, Soumik Mondal, David Mohaisen, and Khin Mi Mi Aung. 2025. Can Find You in Seconds! Leveraging Large Language Models for Code Authorship Attribution. doi:10.48550/arXiv.2501.08165 arXiv:2501.08165 [cs] version: 1. [13] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT: Pre-Trained Model for Programming and Natural Languages. doi:10.48550/ arXiv.2002.08155 arXiv:2002.08155 [cs]. [15] [14] Alberto Ferrante, Eric Medvet, Francesco Mercaldo, Jelena Milosevic, and Corrado Aaron Visaggio. 2016. Spotting the Malicious Moment: Characterizing Malware Behavior Using Dynamic Features. In 2016 11th International Conference on Availability, Reliability and Security (ARES). IEEE, Salzburg, Austria, 372381. doi:10.1109/ARES.2016.70 Jason Gray, Daniele Sgandurra, Lorenzo Cavallaro, and Jorge Blasco Alis. 2024. Identifying Authorship in Malicious Binaries: Features, Challenges & Datasets. ACM Comput. Surv. 56, 8, Article 212 (April 2024), 36 pages. doi:10.1145/3653973 [16] Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with Data Flow. doi:10.48550/arXiv.2009.08366 arXiv:2009.08366 [cs]. [17] Dixiao Guo, Anmin Zhou, Liang Liu, Shan Liao, and Lei Zhang. 2022. Method of Source Code Authorship Attribution Based on Graph Neural Network. In Proceedings of 2021 Chinese Intelligent Automation Conference, Zhidong Deng (Ed.). Springer Singapore, Singapore, 645657. [18] Dixiao Guo, Anmin Zhou, Liang Liu, Shan Liao, and Lei Zhang. 2022. Method of Source Code Authorship Attribution Based on Graph Neural Network. In Proceedings of 2021 Chinese Intelligent Automation Conference, Zhidong Deng (Ed.). Springer, Singapore, 645657. doi:10.1007/978-981-16-6372-7_70 [19] Andrea Gurioli, Maurizio Gabbrielli, and Stefano Zacchiroli. 2025. Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry . In 2025 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE Computer Society, Los Alamitos, CA, USA, 394405. doi:10.1109/SANER64311.2025.00044 [20] Xie He, Arash Habibi Lashkari, Nikhill Vombatkere, and Dilli Prasad Sharma. 2024. Authorship Attribution Methods, Challenges, and Future Research Directions: Comprehensive Survey. Information 15, 3 (March 2024), 131. doi:10.3390/ info15030131 Number: 3 Publisher: Multidisciplinary Digital Publishing Institute. Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Finetuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Iryna Gurevych and Yusuke Miyao (Eds.). Association for Computational Linguistics, Melbourne, Australia, 328339. doi:10.18653/v1/P18-1031 [21] [22] Baixiang Huang, Canyu Chen, and Kai Shu. 2025. Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges. doi:10.48550/arXiv.2408. 08946 arXiv:2408.08946 [cs] version: 2. [23] Oseremen Joy Idialu, Noble Saji Mathews, Rungroj Maipradit, Joanne Atlee, and Mei Nagappan. 2024. Whodunit: Classifying Code as Human Authored or GPT-4 generated-A case study on CodeChef problems. In Proceedings of the 21st International Conference on Mining Software Repositories. 394406. [24] Patrick Juola. 2006. Authorship attribution. Found. Trends Inf. Retr. 1, 3 (Dec. 2006), 233334. doi:10.1561/1500000005 [25] Kocetkov, Denis and Kumar, Divyanshu and Shoeb, Abu Awal Md and others. 2022. The Stack: 3 TB of Permissively Licensed Source Code. Hugging Face Dataset. https://huggingface.co/datasets/bigcode/the-stack Accessed: 2025-0909. [26] Christopher McKnight and Ian Goldberg. 2018. Style Counsel: Seeing the (Random) Forest for the Trees in Adversarial Code Stylometry. In ACM Workshop on Privacy in the Electronic Society (WPES). ACM, Toronto, ON, Canada, 138142. [27] Melanie Mitchell. 2023. Data Contamination and Model Collapse in AI Systems. AI Magazine 44, 4 (2023), 358364. doi:10.1002/aaai. [28] Satya Nadella. 2025. As much as 30% of Microsoft code now written by AI, CEO Satya Nadella says. https://nypost.com/2025/04/30/business/microsoft-ceosatya-nadella-says-30-of-code-now-written-by-ai. Accessed: 2025-09-09. [29] Phuong T. Nguyen, Juri Di Rocco, Claudio Di Sipio, Riccardo Rubei, Davide Di Ruscio, and Massimiliano Di Penta. 2024. GPTSniffer: CodeBERT-based classifier to detect source code written by ChatGPT. Journal of Systems and Software 214 (Aug. 2024), 112059. doi:10.1016/j.jss.2024.112059 [30] Paul W. Oman and Curtis R. Cook. 1989. Programming Style Authorship Analysis. In Proceedings of the 17th Annual ACM Computer Science Conference (CSC 89). Association for Computing Machinery, New York, NY, USA, 320326. [31] Wei Hung Pan, Ming Jie Chok, Jonathan Leong Shan Wong, Yung Xin Shin, Yeong Shian Poon, Zhou Yang, Chun Yong Chong, David Lo, and Mei Kuan Lim. 2024. Assessing ai detectors in identifying ai-generated code: Implications for education. In Proceedings of the 46th international conference on software engineering: software engineering education and training. 111. [32] Erwin Quiring, Alwin Maier, and Konrad Rieck. 2019. Misleading authorship attribution of source code using adversarial learning. In Proceedings of the 28th USENIX Conference on Security Symposium (Santa Clara, CA, USA) (SEC19). USENIX Association, USA, 479496. [33] Musfiqur Rahman, SayedHassan Khatoonabadi, Ahmad Abdellatif, and Emad Shihab. 2024. Automatic Detection of LLM-generated Code: Case Study of Claude 3 Haiku. doi:10.48550/arXiv.2409.01382 arXiv:2409.01382 [cs] version: 1. [34] Nathan Rosenblum, Xiaojin Zhu, and Barton P. Miller. 2011. Who Wrote This Code? Identifying the Authors of Program Binaries. In Computer Security ESORICS 2011, Vijay Atluri and Claudia Diaz (Eds.). Springer, Berlin, Heidelberg, 172189. doi:10.1007/978-3-642-23822-2_10 Ilia Shumailov, Yarin Gal, Vitaly Shmatikov, Nicolas Papernot, and Ross Anderson. 2023. The Curse of Recursion: Training on Generated Data Makes Models Forget. Nature Machine Intelligence 5 (2023), 14111422. doi:10.1038/s42256-023-00782-5 [36] Lucy Simko, Luke Zettlemoyer, and Tadayoshi Kohno. 2018. Recognizing and Imitating Programmer Style: Adversaries in Program Authorship Attribution. In Proceedings on Privacy Enhancing Technologies (PETS), Vol. 2018. 127144. doi:10.1515/popets-2018-0007 [35] [37] Qige Song, Yongzheng Zhang, Linshu Ouyang, and Yige Chen. 2022. BinMLM: Binary Authorship Verification with Flow-aware Mixture-of-Shared Language Model . In 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE Computer Society, Los Alamitos, CA, USA, 10231033. doi:10.1109/SANER53432.2022.00120 [38] Norbert Tihanyi, Tamas Bisztray, Mohamed Amine Ferrag, Ridhi Jain, and Lucas C. Cordeiro. 2024. How secure is AI-generated code: large-scale comparison of large language models. Empirical Software Engineering 30, 2 (Dec. 2024), 47. doi:10.1007/s10664-024-10590-1 [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS17). Curran Associates Inc., Red Hook, NY, USA, 60006010. [40] Ningfei Wang, Shouling Ji, and Ting Wang. 2018. Integration of Static and Dynamic Code Stylometry Analysis for Programmer De-anonymization. In Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security (AISec 18). Association for Computing Machinery, New York, NY, USA, 7484. doi:10.1145/3270101.3270110 [41] Yue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. 2021. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. doi:10.48550/arXiv.2109.00859 arXiv:2109.00859 [cs]. [42] Sarim Zafar, Muhammad Usman Sarwar, Saeed Salem, and Muhammad Zubair Malik. 2020. Language and Obfuscation Oblivious Source Code Authorship Attribution. IEEE Access 8 (2020), 197581197596. doi:10.1109/ACCESS.2020."
        }
    ],
    "affiliations": [
        "Eötvös Loránd University, Budapest, Hungary",
        "Technology Innovation Institute, Abu Dhabi, United Arab Emirates",
        "United Arab Emirates University, Al Ain, United Arab Emirates",
        "University of Oslo, Oslo, Norway"
    ]
}