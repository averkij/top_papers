{
    "paper_title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
    "authors": [
        "Mingyang Wei",
        "Dehai Min",
        "Zewen Liu",
        "Yuzhang Xie",
        "Guanchen Wu",
        "Carl Yang",
        "Max S. Y. Lau",
        "Qi He",
        "Lu Cheng",
        "Wei Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 1 7 4 3 0 . 1 0 6 2 : r EPIQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning Mingyang Wei1, Dehai Min2, Zewen Liu1, Yuzhang Xie1, Guanchen Wu1, Carl Yang1, Max S.Y. Lau1, Qi He3, Lu Cheng2, Wei Jin1* 1Emory University, 2University of Illinois Chicago, 3Microsoft {mingyang.wei, zewen.liu, yuzhang.xie, guanchen.wu}@emory.edu {j.carlyang, msy.lau, wei.jin}@emory.edu {dmin10, lucheng}@uic.edu, qhe@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patientlevel reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate textgrounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrievalbased difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction."
        },
        {
            "title": "Introduction",
            "content": "The COVID-19 pandemic underscored the challenge of extracting reliable insights from rapidly expanding epidemiological literature (Wang and Tian, 2021; Diéguez-Campa et al., 2020). Evidence-informed public health practice requires decisions grounded in the best available scientific evidence, yet such decisions target communities or populations rather than individual pa- *Correspondence: wei.jin@emory.edu 1Benchmark and code are available at https://github. com/myweiii/EpiQAL. tients and often demand synthesizing heterogeneous, context-dependent study findings (Brownson et al., 2009; Orton et al., 2011). Biomedical question answering (QA) systems have been developed to help users retrieve and summarize evidence from large article collections (Bauer and Berleant, 2012; Tsatsaronis et al., 2015; Wallace, 2019), but these systems primarily support clinical knowledge retrieval and patient-level decision making. Epidemiological reasoning, by contrast, requires population-level statistical and causal inference about disease burden, transmission dynamics, and intervention effects (Glass et al., 2013). This gap motivates QA benchmarks tailored to epidemiological inference. suitable benchmark must satisfy two properties. First, it should be controlled, limiting shortcut cues that allow models to exploit superficial patterns such as lexical overlap between questions and contexts (Shinoda et al., 2021). Second, it should be trustworthy, anchoring answers to verifiable study evidence rather than relying solely on annotator judgment. Current QA resources only partially meet these requirements. Exam-style clinical benchmarks such as MedQA and MedMCQA (Jin et al., 2021; Pal et al., 2022) primarily test medical knowledge, offering limited coverage of study-level inference over population distributions. Literature-grounded datasets like PubMedQA (Jin et al., 2019) link questions to research text but rely on abstracts and constrained label spaces, whereas epidemiological questions may admit multiple valid conclusions and require richer methodological context. Epidemic-focused datasets such as COVID-QA, CoQUAD, and EPIC-QA (Möller et al., 2020; Raza et al., 2022a; Goodwin et al., 2022) provide valuable resources, yet they are frequently disease-specific, adopt extractive formats vulnerable to surface matching, and lack systematic verification that inferences reflect authentic epidemiological reasoning. Moreover, expert annotation remains costly, limiting both scale and topic coverage. We present EpiQAL, Epidemiological QA over the Literature, the first benchmark that systematically evaluates epidemiological QA by combining broad topic coverage, multi-answer evaluation, and document-grounded answer derivation. Building EpiQAL requires addressing four challenges. (1) Scope. Epidemiological research spans diverse phenomena from outbreak detection to vaccine effectiveness evaluation. benchmark limited to single disease cannot assess generalization across the field. (2) Grounding. Epidemiological conclusions must be traceable to study evidence. Without such grounding, it is difficult to distinguish genuine inference from hallucination. (3) Verification. Epidemiological questions often admit multiple valid answers. Validating multianswer correctness at scale without exhaustive expert annotation requires automated quality control. (4) Difficulty. Models can exploit superficial cues such as lexical overlap between question stems and correct options, succeeding without genuine comprehension. Our framework addresses each challenge. For scope, we develop taxonomy of six categories and twenty-five topics with epidemiology experts, covering phenomena from surveillance and outbreak investigation to transmission modeling and forecasting. For grounding, we adopt subset-specific strategies that require correct options to be supported by explicit document evidence, including masked-input setting that withholds the Discussion section at test time. For verification, we design checking model group where multiple LLMs independently verify factual consistency, routing uncertain cases to human review. For difficulty, we employ difficulty screening and stem refinement that replaces salient entities with descriptive phrases (Bai et al., 2024; Wu et al., 2025). EpiQAL comprises three subsets probing different capabilities. EpiQAL-A measures textgrounded factual recall where correct answers are explicitly stated in the document. EpiQAL-B targets multi-step inference linking document evidence with epidemiological principles. EpiQAL-C evaluates conclusion reconstruction under masked inputs where the Discussion section is withheld at test time. Together, these subsets enable finegrained diagnosis of model behavior across evidence retrieval, inferential reasoning, and synthesis. Our contributions are as follows. We formalize epidemiological QA as distinct problem requiring population-level reasoning over study evidence. We develop an expert-curated taxonomy ensuring broad coverage across epidemiological subdomains. We propose an automated construction framework integrating multi-LLM verification, difficulty control, and targeted human review. We release EpiQAL with three subsets and benchmark ten open LLMs under multi-answer evaluation protocol."
        },
        {
            "title": "2 Related Work",
            "content": "Biomedical QA benchmarks. Existing biomedical QA benchmarks vary in format, evidence source, and domain scope. Exam-style benchmarks such as MedQA and MedMCQA use singleanswer multiple-choice questions to test broad medical knowledge (Jin et al., 2021; Pal et al., 2022). BioASQ provides expert-curated questions with summaries and exact answers grounded in biomedical literature (Krithara et al., 2023), while PubMedQA links questions to abstracts but adopts constrained yes/no/maybe label space that limits expressiveness (Jin et al., 2019). Epidemic-focused benchmarks such as COVID-QA, CoQUAD, and EPIC-QA ground questions in pandemic-related evidence but are typically disease-specific and use extractive formats (Möller et al., 2020; Raza et al., 2022b; Goodwin et al., 2022). In contrast, EpiQAL covers diverse epidemiological topics, supports multi-answer evaluation, and includes maskedinput setting for conclusion reconstruction. Automatic QA construction and quality control. Automatic QA construction has evolved from template-based generation to neural pipelines conditioned on passages (Du et al., 2017), with recent work improving distractor plausibility for multiplechoice formats (Lee et al., 2025). To reduce annotation artifacts and shortcut cues, model-in-the-loop collection and adversarial filtering select harder or less biased instances (Bartolo et al., 2020; Kiela et al., 2021; Bras et al., 2020), while multi-judge LLM verification helps mitigate single-model biases in quality control (Liu et al., 2023; Ma et al., 2025). For settings admitting multiple valid answers, benchmarks such as HotpotQA adopt setbased F1 and Exact Match metrics (Yang et al., Figure 1: Overall framework for EpiQAL construction. The pipeline begins with subset-specific input processing (upper left), followed by QA generation and multi-model verification that routes uncertain cases to human review (upper right). For EpiQAL-B&C, difficulty judging screens overly easy instances and triggers stem refinement when needed (lower). EpiQAL-A outputs directly after verification. 2018), and LIQUID demonstrates automatic multianswer evaluation at scale (Lee et al., 2023). LongBench v2 further incorporates difficulty screening into benchmark construction (Bai et al., 2024). EpiQAL builds on these advances by combining taxonomy-guided generation with multi-LLM verification and difficulty control."
        },
        {
            "title": "3.1 Task Formulation",
            "content": "We now define two tasks: dataset generation and benchmarking. Dataset generation. Given source document D, the goal is to produce question Q, set of correct options Oc, and set of distractors Od. We formulate this as constrained generation where model Mg operates under constraint schema that specifies topic scope, reasoning requirements, and option construction rules: (Q, Oc, Od) = Mg(D, E; G) (1) Here denotes optional external knowledge. For EpiQAL-B, consists of epidemiological relations from knowledge graphs used only during construction; for EpiQAL-A and EpiQAL-C, is empty. Section 3.4 details the constraint schema and its subset-specific instantiations. Benchmarking. The evaluation task is multiple choice QA where multiple options may be correct. Let denote the test-time input. For EpiQALA and EpiQAL-B, = D. For EpiQAL-C, the Discussion section Dd is masked so that = Dd. Given D, question Q, and candidates = Oc Od, tested model Mt predicts an answer set A: = Mt( D, Q, O). We allow = to represent abstention, and include instances where Oc = so that no option is correct. This design penalizes indiscriminate guessing. Evaluation uses set-based Exact Match: EM = 1[A = Oc]."
        },
        {
            "title": "3.2 Framework Overview",
            "content": "Epidemiological reasoning spans spectrum from retrieving stated facts to synthesizing conclusions from partial observations. To diagnose where models succeed or fail along this spectrum, we design three subsets that isolate distinct capabilities: textgrounded recall in EpiQAL-A, multi-step inference in EpiQAL-B, and conclusion reconstruction under masked inputs in EpiQAL-C. Figure 1 illustrates the construction pipeline. All three subsets share core structure of input processing, QA generation, and multi-model verifiTable 1: Comparison of the three subsets in EpiQAL. EpiQAL-A EpiQAL-B EpiQAL-C"
        },
        {
            "title": "Core Capability\nKnowledge Source\nTaxonomy Guided\nExternal Knowledge\nTest Input\nDifficulty Control",
            "content": "Fact recall Document Yes No Full document No Multi-step inference Document + KG Yes Generation only Full document Yes Conclusion reconstruction Paper structure No No Document w/o Discussion Yes cation, while EpiQAL-B and EpiQAL-C undergo additional difficulty control. The pipeline proceeds as follows: (1) subset-specific input processing derives supervision from taxonomy guidance or paper structure; (2) generation model Mt produces Q, Oc, Od under explicit constraints that enforce evidence grounding; (3) multi-LLM checking group verifies factual consistency and option validity, routing uncertain cases to human review; (4) difficulty control screens overly easy instances and refines question stems when needed. Section 3.3 details how each subset instantiates this pipeline. These components address the construction challenges identified in Section 1. The expert taxonomy ensures broad topic coverage, addressing scope. Subset-specific constraints and evidence requirements yield traceable answers, addressing grounding. Multi-model verification with human review enables scalable quality control, addressing verification. Difficulty control reduces surface-level shortcuts, addressing difficulty. The following subsections detail each component."
        },
        {
            "title": "3.3 Subset Design",
            "content": "We instantiate the framework into three subsets that share unified multiple-choice formulation but differ in supervision source and test-time input D. Table 1 summarizes the key differences. EpiQAL-A: Text-grounded recall. EpiQAL-A contains retrieval-based questions whose correct options Oc are explicitly stated in the source document D. Each correct option must be directly supported by verbatim spans. Distractors Od are document-grounded confounders that match surface form but differ in role, population, or context. EpiQAL-B: Multi-step inference. EpiQAL-B targets inference that links multiple cues in with epidemiological knowledge. During construction, external knowledge from knowledge graphs elicits inference-oriented questions, but evaluation provides only = D. Correct options Oc express derived implications rather than passage restatements. Distractors Od contain reasoning-level flaws such as causal reversal or entity misattribution. EpiQAL-C: Masked-input reasoning. EpiQALC evaluates reconstruction of author-stated conclusions when the Discussion section Dd is masked, so = Dd. Correct options Oc are salient conclusions extracted from Dd, but must be supportable by evidence in D. Distractors Od are plausible under the paper narrative but unsupported, contradictory, or logically inverted. Appendix A.4 provides detailed distractor design principles for each subset. 3."
        },
        {
            "title": "Input Constraints",
            "content": "Epidemiology Taxonomy. To ensure broad coverage across epidemiological subdomains, we develop taxonomy with domain experts that defines question scope and guides generation for EpiQAL-A and EpiQAL-B. The taxonomy reflects the workflow of epidemiological inquiry, emphasizing population-level evidence synthesis rather than individual-level clinical reasoning. The taxonomy is organized into six high-level classes covering complementary stages of epidemiological investigation. Surveillance and Descriptive Epidemiology characterizes disease occurrence through rates, temporal trends, and demographic patterns. Outbreak Investigation and Field Response addresses case confirmation, attack rates, source attribution, and immediate control measures. Determinants and Exposures examines how exposure arises across behavioral, environmental, and social contexts. Susceptibility and Immunity describes who is susceptible, correlates of protection, and vaccine effectiveness. Modeling, Methods, and Evaluation covers transmission modeling, study design, bias handling, and program evaluation. Projections and Forecasts produces forward-looking predictions and supports decision making. Each class contains multiple topics that provide finer-grained control over question intent. For EpiQAL-A and EpiQAL-B, we sample topic and use its description to steer evidence selection, question phrasing, and option design. EpiQAL-C derives supervision from paper structure rather than taxonomy guidance, as its goal is to reconstruct author-stated conclusions regardless of topic. The complete taxonomy with all 25 topics and their descriptions is provided in Appendix A.2. Domain Knowledge Augmentation. EpiQALB incorporates external knowledge during construction to encourage multi-evidence inferenceoriented questions and harder distractors. We extract disease entities from the source document D, link them to biomedical knowledge graphs, and summarize related triples into natural language signals. These signals help elicit questions whose solution requires bridging document evidence with epidemiological principles. At evaluation time, is withheld, so that success requires models to use parametric knowledge rather than relying on provided signals. Appendix A.3 details the construction procedure."
        },
        {
            "title": "3.5 Constrained QA Generation",
            "content": "We define constraint schema to control question and option construction. The schema consists of three components: topic constraint, logic constraint, and option constraints. External knowledge is provided separately for EpiQAL-B (Section 3.4). The schema structure is shared across subsets, while subset-specific instantiations differentiate text-grounded recall, multi-step inference, and masked conclusion reconstruction. Topic constraint. Topic constraint includes Taxonomy Constraint and Paper Structure Constraint. For EpiQAL-A and EpiQAL-B, the selected taxonomy topic restricts generation to the intended epidemiological phenomenon. EpiQAL-C derives supervision from paper structure and does not use topic guidance. Logic constraint. The logic constraint specifies what constitutes valid reasoning demand in the question stem and is the main mechanism for differentiating the three subsets. In EpiQAL-A, stems are restricted to retrieval-style questions whose answers are explicitly stated in D. In EpiQAL-B, stems require synthesis-style questions that combine multiple pieces of document evidence with epidemiological principles. In EpiQAL-C, stems require reconstruction of an author-stated conclusion by reasoning over observations when Dd is masked. Option constraint. The constraint on correct options Oc enforces evidence consistency, with subset-specific rules. For EpiQAL-A and EpiQALB, Oc must be supported by document evidence. EpiQAL-B further requires that Oc express derived implications rather than restatements of passage facts. For EpiQAL-C, Oc are salient conclusions extracted from Dd. The constraint on distractors Od requires semantic and stylistic similarity to Oc while introducing controlled errors. EpiQAL-A uses document-grounded confounders that match surface form but differ in role or context. EpiQALB uses reasoning-level adversarial errors such as entity misattribution or causal reversal. EpiQALC uses plausible traps that are unsupported by D, contradictory, or logically inverted."
        },
        {
            "title": "Appendix F provides the generation prompts for",
            "content": "each subset."
        },
        {
            "title": "3.5.1 Multi-model Verification\nAutomatically generated QA instances may contain\nfactual errors, label inconsistencies, or reasoning\nflaws. We address this through multi-model verifi-\ncation combined with targeted human review.\nChecking model group. A group of LLMs inde-\npendently verifies each generated option in Oc∪Od.\nCheckers assess two properties: whether the option\nis consistent with its assigned label given the cited\nevidence, and whether the implied reasoning is co-\nherent. Checkers operate at the option level rather\nthan re-solving the full question, which allows effi-\ncient verification at scale.",
            "content": "To ensure that correctness does not depend on construction-only information, we require that accepted options be evidence-consistent with the testtime input D. For EpiQAL-A and EpiQAL-B, = D. For EpiQAL-C, = Dd. Although EpiQAL-C correct options are extracted from Dd, checkers require that they be supported by spans in D. We run each checker multiple times with stochastic decoding and aggregate decisions into vote ratio [0, 1] representing the fraction of keep votes. Two thresholds govern the decision process: options below the lower threshold are rejected automatically, options above the upper threshold are accepted, and options in between are flagged for human review. This tiered approach balances automation with quality control. Human Review. Full manual auditing is infeasible at scale, so we reserve expert effort for uncertain cases. For flagged options, human reviewers inspect the evidence attribution and option label, then either approve or discard the instance. This policy concentrates expert attention on high-risk cases while keeping overall annotation cost modest."
        },
        {
            "title": "3.6 Difficulty Control",
            "content": "For EpiQAL-B and EpiQAL-C, quality also depends on whether items demand nontrivial reasoning. We apply difficulty control only to these two subsets because EpiQAL-A targets text-grounded recall rather than reasoning depth. Difficulty control consists of two steps: difficulty judging to identify overly easy items, and stem refinement to reduce shortcut cues. Difficulty judging. We estimate instance difficulty using pool of models ranging from small to large. For each model, we compare the predicted answer set with the reference set Oc using set-based F1 and Exact Match (Appendix A.1), then combine them into difficulty score: DiffScore = 1 (α F1 + (1 α) EM) where α [0, 1] controls the trade-off between partial overlap and exact set recovery. We average DiffScore across the model pool. Items below threshold are treated as easy and passed to stem refinement. Stem refinement. Stem refinement is rewriting step that replaces salient entities in the question stem with descriptive phrases. This reduces surface matching between and Oc, requiring models to reason about the described concept rather than pattern match on entity names. For example, question mentioning cutaneous leishmaniasis might be rewritten to describe it as vector-borne skin disorder caused by Leishmania parasites transmitted via sandfly bites. The rewritten stem preserves answerability while increasing discriminative difficulty. No retrieved text is provided to models at evaluation time. The refinement procedure iteratively extracts core entity from Q, retrieves its definition from web sources, and replaces the entity with summarized description. This process repeats until DiffScore exceeds the threshold or maximum number of iterations is reached. Appendix B.1 provides the detailed procedure, and Appendix B.2 analyzes the effect of refinement iterations on model performance."
        },
        {
            "title": "4 Experiment",
            "content": "We evaluate EpiQAL from three perspectives. First, we report dataset statistics and construction efficiency. Second, we benchmark diverse set of open-source models on the resulting subsets. Third, we analyze the results and discuss implications for epidemiological QA evaluation."
        },
        {
            "title": "4.1 Generation Settings",
            "content": "Generation and verification. We use Qwen330B-A3B-Instruct-2507 as the generation model. For EpiQAL-B, we extract disease entities using GLiNER and link them to knowledge graphs via SapBERT, with Llama-3.3-70B-Instruct summarizing retrieved triples. Generated options are verified by checking group of four models from different families (GLM-4.5-Air, Mistral-Large, Llama-3.370B, Qwen3-30B), with decisions aggregated by vote ratio. Difficulty control uses pool of nine models ranging from 3B to 110B parameters. Implementation details are provided in Appendix C. Corpus. We build corpus from the Journal Archive of PLOS Neglected Tropical Diseases (PLO, 2007), collecting approximately 10,600 research articles containing abstracts, main text, author summaries, and acknowledgements. For the main experiments, we use randomly sampled subset of 500 articles. All content is used under the original open license. Table 2 summarizes dataset statistics. Each subset contains 500 instances with varying numbers of options and correct answers. We allow instances with an empty correct answer set, which penalizes guessing by requiring explicit abstention. Across all subsets, fewer than 4% of options require human review, demonstrating the efficiency of multimodel verification. Additional analyses of class and topic coverage are provided in Appendix D."
        },
        {
            "title": "4.2 Evaluation Protocol",
            "content": "We evaluate all models in closed-book setting, providing only the subset-specific input document D, the question Q, and the candidate options O. Models are instructed to select all correct options in fixed output format. Although EpiQAL-C instances have on average one correct option, we do not reveal this to models, preventing them from exploiting the single-answer structure as shortcut. We score only the final answer line and allow an empty set to represent abstention when no option is correct. We report set-based Exact Match (ApTable 2: Dataset statistics for each subset."
        },
        {
            "title": "Subset",
            "content": "Samples Avg. #Options Avg. #Correct Options % Human Review EpiQAL-A EpiQAL-B EpiQAL-C 500 500 500 3.508 2.898 3.020 1.432 1.064 0.998 3.2% 3.4% 1.8% pendix A.1), which equals 1 if the predicted set exactly matches the reference set and 0 otherwise. In EpiQAL-C, the Discussion section is removed before evaluation. We use temperature 0.3 and report results with and without Chain-of-Thought prompting. Chain-of-Thought adds reasoning instruction while preserving the same final answer format. We evaluate ten open models from five families: Phi-4-mini-instruct from Microsoft (Microsoft et al., 2025); Llama-3.2-3B-Instruct, Llama3.1-8B-Instruct, and Llama-3.3-70B-Instruct from Meta (Grattafiori et al., 2024); Mistral-7B-Instructv0.3 and Mistral-Large-Instruct-2411 from Mistral AI (Jiang et al., 2023); Qwen3-8B, Qwen3-30BA3B-Instruct-2507, and Qwen3-32B (Yang et al., 2025); and GLM-4.5-Air from Zhipu AI (Team et al., 2025). Table 3 reports F1 Score and Exact Match on all three subsets."
        },
        {
            "title": "4.3 Discussion",
            "content": "Table 3 reports F1 and Exact Match across all three subsets. Current LLMs show limited capabilities on epidemiological reasoning. The best-performing models achieve Exact Match scores of 0.812 on text-grounded recall, 0.760 on multi-step inference, and 0.800 on conclusion reconstruction. These numbers fall well below the near-ceiling performance that state-of-the-art LLMs achieve on many general NLP benchmarks. Most models score below 0.70 on EpiQAL-B and EpiQAL-C, and the smallest model Llama-3.2-3B scores below 0.15 on both subsets. Epidemiological reasoning, which requires integrating scattered evidence with domain principles, remains unsolved by current LLMs. Multi-step inference is the key bottleneck. Among the three reasoning types, multi-step inference proves most difficult. EpiQAL-B scores range from 0.094 to 0.760, and most models cluster below 0.70. Text-grounded recall and conclusion reconstruction yield higher scores, suggesting that models can retrieve explicit facts and generate plausible conclusions but struggle to integrate multiple pieces of evidence into coherent inferences. This bottleneck likely reflects fundamental limitation in how current architectures combine information across long contexts with background knowledge. Model rankings shift across subsets. No single model dominates all three subsets. Mistral-Large leads on EpiQAL-A at 0.812 but drops to 0.574 on EpiQAL-B without CoT. Mistral-7B ranks below average on EpiQAL-A at 0.632 but achieves the best scores on both EpiQAL-B and EpiQAL-C. Qwen3-30B-A3B shows the largest CoT gains on EpiQAL-B, improving from 0.568 to 0.720. These shifts suggest that text retrieval, evidence integration, and conclusion reconstruction engage different model capabilities. single aggregate score would obscure these distinctions. Scale alone does not guarantee success. Mistral7B outperforms Mistral-Large on both EpiQAL-B and EpiQAL-C by substantial margins. Llama3.1-8B approaches Llama-3.3-70B on multi-step inference despite having fewer than one-eighth the parameters. At the same time, Llama-3.2-3B collapses on reasoning-intensive subsets while larger Llama models perform reasonably. These patterns suggest capability threshold below which models cannot perform epidemiological reasoning, but above which further scaling yields diminishing returns. Instruction tuning quality and architectural choices appear to matter more than raw parameter count. Answer precision explains Mistral-7Bs success. Mistral-7B achieves only moderate F1 scores but leads on Exact Match for EpiQAL-B and EpiQALC. The explanation lies in its F1-EM gap. Mistral7B shows gaps of just 0.019 on EpiQAL-B and 0.034 on EpiQAL-C, meaning it selects correct options without over-selecting plausible distractors. Llama-3.1-8B achieves comparable F1 but shows gaps exceeding 0.35, losing substantially on Exact Match because it hedges by selecting additional options. For tasks where false positives carry significant costs, model that abstains when uncertain may outperform one that maximizes coverage. Chain-of-Thought helps inference but not reTable 3: F1 ScoreExact Match accuracy for each model across subsets, with and without Chain-of-Thought prompting."
        },
        {
            "title": "Microsoft",
            "content": "EpiQAL-A EpiQAL-B EpiQAL-C w/o CoT CoT w/o CoT CoT w/o CoT CoT Phi-4-mini-instruct 0.7720.494 0.7790. 0.6540.240 0.7140.384 0.7260.410 0.7160.402 Meta-Llama Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct Llama-3.3-70B-Instruct"
        },
        {
            "title": "Mistral AI",
            "content": "0.4730.308 0.8490.668 0.8260.676 0.3870.274 0.8560.698 0.8220.696 0.4020.120 0.6230.262 0.7780.588 0.2010.094 0.7510.584 0.8060.656 0.2700.124 0.5870.204 0.7790.552 0.2860.096 0.6940.382 0.8200. Mistral-7B-Instruct-v0.3 Mistral-Large-Instruct-2411 0.7360.632 0.9100.812 0.7420.632 0.9110.810 0.7790.760 0.8060.574 0.7420.732 0.8280.650 0.8140.780 0.7940. 0.8110.800 0.8010."
        },
        {
            "title": "Qwen",
            "content": "Qwen3-8B Qwen3-30B-A3B-Instruct-2507 Qwen3-32B 0.8430.712 0.8930.784 0.8880.780 0.8650.764 0.8920.796 0.8860.768 0.6810.442 0.7710.568 0.8210.676 0.7470.562 0.8460.720 0.8140.672 0.6630.478 0.7200.526 0.7470. 0.7080.500 0.7690.586 0.7500."
        },
        {
            "title": "Zhipu AI",
            "content": "GLM-4.5-Air 0.8630.766 0.8490.754 0.7280.586 0.7540.612 0.7050. 0.6350.526 trieval. CoT prompting substantially improves performance on EpiQAL-B for most models. Llama3.1-8B improves from 0.262 to 0.584, and Qwen330B-A3B improves from 0.568 to 0.720. On EpiQAL-A, CoT produces no consistent benefit. On EpiQAL-C, results are mixed. Explicit reasoning steps appear to help when models must integrate multiple evidence pieces but add little value for direct retrieval. Two exceptions stand out. First, CoT harms Llama-3.2-3B across all subsets, suggesting that small models lack the capacity to benefit from explicit reasoning. Second, CoT slightly degrades Mistral-7B on EpiQAL-B from 0.760 to 0.732, possibly because explicit reasoning interferes with its already-calibrated implicit inference. Generator bias does not dominate results. EpiQAL-B is constructed using Qwen model as the generator, raising the possibility of generatorfavoring artifacts. However, Mistral-7B from different model family achieves the highest score on this subset. Qwen models perform competitively but do not lead. This cross-family result suggests that the benchmark measures genuine reasoning capabilities rather than superficial alignment with the generators style. implications. Practical For fact extraction, Mistral-Large and Qwen3-32B perform best without needing CoT. For multi-step inference, Mistral7B outperforms larger models and does not require CoT. For conclusion reconstruction with incomplete evidence, Mistral-7B again leads. Deployments with strict precision requirements should prefer models with small F1-EM gaps. Systems with limited compute should avoid models below 7B parameters for reasoning tasks. These findings highlight the value of task-specific evaluation over reliance on general benchmarks or scale assumptions."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced EpiQAL, benchmark for evidencegrounded epidemiological question answering over research articles. Our construction framework combines an expert-curated taxonomy, subsetspecific constraints for evidence grounding, multimodel verification, and difficulty screening. This yields three complementary subsets that isolate textgrounded recall, multi-step inference, and conclusion reconstruction. Experiments across ten open models reveal that current LLMs show limited capabilities on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multistep inference but yields mixed results elsewhere. These findings support using EpiQAL as diagnostic suite for epidemiological QA capabilities. We release EpiQAL along with construction code and baseline evaluations to facilitate future work on evidence-grounded reasoning for public health."
        },
        {
            "title": "Limitations",
            "content": "This work has several limitations. First, our source corpus is drawn solely from PLOS Neglected Tropical Diseases, which may underrepresent domains such as respiratory surveillance, chronic disease epidemiology, and health policy. Second, we generate 500 instances per subset due to computational constraints. Scaling up may surface new failure modes on long-tail topics with sparse evidence. Third, EpiQAL-B is constructed using single generation model from the Qwen family. Although the top performer on this subset is Mistral-7B from different family, future work could explore crossfamily or mixture-based generation to further reduce potential generator-related artifacts. Fourth, despite multi-model verification and targeted human review, the benchmark may contain residual errors from LLM-based generation. Fifth, we evaluate open models up to approximately 110B parameters. Results may not transfer to larger proprietary systems. Finally, EpiQAL remains proxy for realworld public health analysis, which often requires integrating multiple documents and incorporating temporal and geographic context beyond singlearticle reasoning."
        },
        {
            "title": "References",
            "content": "2007. Plos neglected tropical diseases. Open Access Journal Archive. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204. Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the ai: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662678. Michael Bauer and Daniel Berleant. 2012. Usability survey of biomedical question answering systems. Human genomics, 6:17. Asma Ben Abacha and Dina Demner-Fushman. 2019. question-entailment approach to question answering. BMC Bioinform., 20(1):511:1511:23. Balu Bhasuran, Qiao Jin, Yuzhang Xie, Carl Yang, Karim Hanna, Jennifer Costa, Cindy Shavor, Wenshan Han, Zhiyong Lu, and Zhe He. 2025. Preliminary analysis of the impact of lab results on large language model generated differential diagnoses. npj Digital Medicine, 8(1):166. Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, and Yejin Choi. 2020. Adversarial filters of dataset biases. Preprint, arXiv:2002.04108. Ross Brownson, Jonathan Fielding, and Christopher Maylahn. 2009. Evidence-based public health: fundamental concept for public health practice. Annual review of public health, 30:175201. Carlos Diéguez-Campa, Iván Pérez-Neri, Gustavo Reyes-Terán, Iliana Flores-Apodaca, Jorge Castillo Ledon Pretelini, Omar Mercado-Bautista, Ricardo Alvarez Santana, Marco Zenteno, Brigham Bowles, and Angel Lee. 2020. The 2020 research pandemic: bibliometric analysis of publications on covid-19 and their scientific impact during the first months la pandemia de investigación del 2020: Un análisis bibliométrico de las publicaciones sobre covid-19 su impacto científico durante los primeros meses. Archivos de cardiología de México, 1. Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13421352, Vancouver, Canada. Association for Computational Linguistics. Thomas Glass, Steven Goodman, Miguel Hernán, and Jonathan Samet. 2013. Causal inference in public health. Annual review of public health, 34. Travis Goodwin, Dina Demner-Fushman, Kyle Lo, Lucy Wang, Hoa Dang, and Ian Soboroff. 2022. Automatic question answering for multiple stakeholders, the epidemic question answering dataset. Scientific Data, 9. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300. Daniel Scott Himmelstein, Antoine Lizee, Christine Hessler, Leo Brueggeman, Sabrina Chen, Dexter Hadley, Ari Green, Pouya Khankhanian, and Sergio Baranzini. 2017. Systematic integration of biomedical knowledge prioritizes drugs for repurposing. eLife, 6:e26726. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880, Online. Association for Computational Linguistics. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567 2577, Hong Kong, China. Association for Computational Linguistics. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada. Association for Computational Linguistics. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp, Robin Jia, Mohit Bansal, Christopher Potts, and Adina Williams. 2021. Dynabench: Rethinking benchmarking in nlp. Preprint, arXiv:2104.14337. Anastasia Krithara, Anastasios Nentidis, Konstantinos Bougiatiotis, and Georgios Paliouras. 2023. Bioasqqa: manually curated corpus for biomedical question answering. Scientific Data, 10. Seongyun Lee, Hyunjae Kim, and Jaewoo Kang. 2023. Liquid: framework for list question answering In Proceedings of the Thirtydataset generation. Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI23/IAAI23/EAAI23. AAAI Press. Yooseop Lee, Suin Kim, and Yohan Jo. 2025. Generating plausible distractors for multiple-choice questions via student choice prediction. Preprint, arXiv:2501.13125. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeintensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA. Curran Associates Inc. Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, and Nigel Collier. 2021. Self-alignment pretraining for biomedical entity representations. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 42284238. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Chiyu Ma, Enpei Zhang, Yilun Zhao, Wenjun Liu, Yaning Jia, Peijun Qing, Lin Shi, Arman Cohan, JudgYujun Yan, and Soroush Vosoughi. 2025. ing with many minds: Do more perspectives mean less prejudice? on bias amplifications and resistance in multi-agent based llm-as-judge. Preprint, arXiv:2505.19477. Microsoft, :, Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, Dong Chen, Dongdong Chen, Junkun Chen, Weizhu Chen, Yen-Chun Chen, Yi ling Chen, Qi Dai, and 57 others. 2025. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. Preprint, arXiv:2503.01743. Dehai Min, Zhiyang Xu, Guilin Qi, Lifu Huang, and Chenyu You. 2025. UniHGKR: Unified instructionaware heterogeneous knowledge retrievers. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 45774594, Albuquerque, New Mexico. Association for Computational Linguistics. Timo Möller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020. COVID-QA: question answering dataset for COVID-19. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Linguistics. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 48854901, Online. Association for Computational Linguistics. Lois Orton, Ffion Lloyd-Williams, David TaylorRobinson, Martin OFlaherty, and Simon Capewell. 2011. The use of research evidence in public health decision making processes: Systematic review. PLOS ONE, 6(7):e21704. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. 2022. Medmcqa: large-scale multisubject multi-choice dataset for medical domain question answering. In Proceedings of the Conference on Health, Inference, and Learning, volume 174 of Proceedings of Machine Learning Research, pages 248260. PMLR. Anusri Pampari, Preethi Raghavan, Jennifer Liang, and Jian Peng. 2018. emrqa: large corpus for question answering on electronic medical records. Preprint, arXiv:1809.00732. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Shaina Raza, Brian Schwartz, and Laura Rosella. 2022a. Coquad: covid-19 question answering dataset system, facilitating research, benchmarking, and practice. BMC Bioinformatics, 23. Shaina Raza, Brian Schwartz, and Laura Rosella. 2022b. Coquad: covid-19 question answering dataset system, facilitating research, benchmarking, and practice. BMC Bioinformatics, 23. Consoli S, Coletti P, Markov P, Orfei L, Biazzo I, Schuh L, Stefanovitch N, Bertolini L, Ceresa M, and Stilianakis N. 2025. An epidemiological knowledge graph extracted from the world health organizations disease outbreak news. SCIENTIFIC DATA, 12(1):970. Kazutoshi Shinoda, Saku Sugawara, and Akiko Aizawa. 2021. Can question generation debias question answering models? case study on questioncontext lexical overlap. In Proceedings of the 3rd Workshop on Machine Reading for Question Answering, pages 6372, Punta Cana, Dominican Republic. Association for Computational Linguistics. Chang Su, Yu Hou, Suraj Rajendran, Jacqueline R. M. A. Maasch, Zehra Abedi, Haotan Zhang, Zilong Bai, Anthony Cuturrufo, Winston Guo, Fayzan F. Chaudhry, Gregory Ghahramani, Jian Tang, Feixiong Cheng, Yue Li, Rui Zhang, Jiang Bian, and Fei Wang. 2022. Biomedical discovery through the integrative biomedical knowledge hub (ibkh). medRxiv."
        },
        {
            "title": "5 Team, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu\nHou, Bin Chen, Chengxing Xie, Cunxiang Wang,\nDa Yin, Hao Zeng, Jiajie Zhang, Kedong Wang,\nLucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao,",
            "content": "Xiaohan Zhang, Xuancheng Huang, Yao Wei, and 152 others. 2025. Glm-4.5: Agentic, reasoning, Preprint, and coding (arc) foundation models. arXiv:2508.06471. George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael Alvers, Dirk Weißenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, Yannis Almirantis, John Pavlopoulos, Nicolas Baskiotis, Patrick Gallinari, Thierry Artieres, Axel-Cyrille Ngonga Ngomo, Norman Heino, Eric Gaussier, Liliana Barrio-Alvers, and Georgios Paliouras. 2015. An overview of the bioasq largescale biomedical semantic indexing and question answering competition. BMC Bioinformatics, 16:138. Byron C. Wallace. 2019. What does the evidence say? models to help make sense of the biomedical literIn Proceedings of the Twenty-Eighth Interature. national Joint Conference on Artificial Intelligence, IJCAI-19, pages 64166420. International Joint Conferences on Artificial Intelligence Organization. Panpan Wang and Deqiao Tian. 2021. Bibliometric analysis of global scientific research on covid-19. Journal of Biosafety and Biosecurity, 3(1):49. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. 2025. Webdancer: Towards autonomous information seeking agency. Preprint, arXiv:2505.22648. Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, and Carl Yang. 2025. Kerap: knowledge-enhanced reasoning approach for accurate zero-shot diagnosis prediction using multi-agent llms. Preprint, arXiv:2507.02773. Yuzhang Xie, Jiaying Lu, Joyce Ho, Fadi Nahab, Xiao Hu, and Carl Yang. 2024. Promptlink: leveraging large language models for cross-source biomedical concept linking. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2589 2593. Ran Xu, Hui Liu, Sreyashi Nag, Zhenwei Dai, Yaochen Xie, Xianfeng Tang, Chen Luo, Yang Li, Joyce C. Ho, Carl Yang, and Qi He. 2025. SimRAG: Selfimproving retrieval-augmented generation for adapting large language models to specialized domains. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 11534 11550, Albuquerque, New Mexico. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois. 2024. GLiNER: Generalist model for named entity recognition using bidirectional transformer. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 53645376, Mexico City, Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A Additional Method Details",
            "content": "A.1 Evaluation Metrics Let setmodel denote the set of options predicted by model and setref denote the reference option set. We compute F1 = 2 setreference setmodel setreference + setmodel (2) ExactMatch = (cid:40) if setmodel = setreference 1, 0, otherwise (3) A.2 Epidemiology Taxonomy This appendix provides the complete taxonomy introduced in Section 3.4. Each of the six classes contains multiple topics, and each topic includes an expert-curated description specifying its semantic scope. These descriptions serve as explicit constraints during question generation for EpiQALA and EpiQAL-B, steering the generation model toward the intended epidemiological competency. The taxonomy also supports topic-level analysis of model performance. Table 4 lists the six classes with their descriptions. Tables 5 through 7 provide all 25 topics organized by class. A.3 External Knowledge Construction This appendix describes how external knowledge is constructed for EpiQAL-B. The procedure consists of four steps: entity extraction, entity linking, triple retrieval, and summarization. We first extract disease entities from the source document using GLiNER (Zaratiana et al., 2024). Extracted mentions are then normalized via entity linking using SapBERT (Liu et al., 2021), which is SOTA biomedical entity linking method (Xie et al., 2024), to encode mentions and retrieve candidate entities. We retrieve related triples from two knowledge graphs: eKG-DONs (S et al., 2025), which compiles outbreak reports from official sources, and iBKH (Himmelstein et al., 2017; Su et al., 2022), which encodes broader biomedical relations. Finally, language model summarizes the retrieved triples into compact natural language statements used as generation signals (Xie et al., 2025). These signals are used only during dataset construction to steer the generation model toward inference-oriented questions. They are not provided to models at evaluation time. A.4 Distractor Design We design distractors to be plausible under the provided study context while remaining incorrect for the specific question intent. Across all subsets, we enforce semantic type matching with correct options, stylistic consistency, and diversity so that different distractors reflect different confusable alternatives rather than near duplicates. We attach evidence spans and brief rationales during construction to support verification and error analysis. EpiQAL-A. Distractors in EpiQAL-A are passagegrounded confounders. They are valid entities or facts stated in the same document, matching the semantic category and tone of correct options. They are incorrect because they refer to different role, population, setting, time window, or study context than what the question requires. This design discourages guessing by surface cues while preserving retrieval-based task in which all options are locally supported by explicit spans. EpiQAL-B. Distractors in EpiQAL-B are reasoning-level adversaries. They share the grammatical structure and semantic category of correct options but express misleading implications that require reasoning process. We introduce subtle flaws using three main categories: Entity or attribution shift: conclusion that holds for another entity in the passage is incorrectly applied to the target entity. Causal direction reversal: the direction of an implied effect is flipped while keeping entities Cls Class Description 1 2 3 4 6 Surveillance and Descriptive Epidemiology Outbreak Investigation and Field Response Determinants and Exposures Susceptibility and Immunity Modeling, Methods, and Evaluation Projections and Forecasts Describes population occurrence from routine data, including rates, time place person patterns, aberration signals, and basic system performance, without causal analysis or forecasting. Handles outbreak specific confirmation, field case definitions, line lists, attack rates and curves, chain and source hypotheses, and immediate control with situation reports. Explains how exposure arises across settings, covering behavioral, environmental, occupational, and social determinants, delineates canonical transmission routes and contact structures, interprets exposure response with attention to measurement methods, units, detection limits, and thresholds, and situates risks within One Health interfaces involving reservoirs and vectors. Describes who is susceptible and why, links serologic measures to correlates of protection, evaluates effectiveness after vaccination or prior infection and its waning with reinfection, hybrid immunity, and variant escape, including the effects of vaccine dose number and intervals, and assesses severity risk using clinical and contextual prognostic factors. Provides analytical methods for transmission modeling and inference, real time debiasing of surveillance data, study design and causal effects, measurement and bias handling, and program performance and burden evaluation. Produces forward looking forecasts and scenarios, evaluates and combines models, and supports decision making, it does not reconstruct recent under reported data. Table 4: Epidemiology taxonomy classes and study context fixed."
        },
        {
            "title": "B Stem Refinement",
            "content": "Principle mismatch: correct passage fact is combined with an incorrect epidemiological principle to yield plausible but wrong implication. Construction-time external signals may validate the flawed reasoning chain but are not embedded as explicit hints in the distractor text. EpiQAL-C. Distractors in EpiQAL-C are maskedinput traps tailored to the Discussion masking setup. We draw candidates from either the non-Discussion sections or the Discussion, then refine them into self-contained sentences that are plausible but incorrect when only the non-Discussion sections are available. We use five primary trap categories: Limitations or future work: unproven hypotheses that are not established as conclusions. External literature dependence: claims supported only by cited outside work in the Discussion. Background restatement: common knowledge rather than study-specific findings. Incorrect conclusion: same entity but wrong conclusion under the question. Causal reversal: reversed causal direction under the study context. For each distractor, we attach evidence revealing why it is not valid answer under the masked-input setting. B.1 Procedure Stem refinement is retrieval-based rewriting step applied during dataset construction. We adapt the recursive retrieval approach from Wu et al. (2025) by iteratively replacing entities with their descriptions. The procedure works as follows. First, we prompt model to extract core entity from the question stem as replacement candidate. Second, we construct synthetic query to search for the entitys definition and characteristics, retrieving the top Kr relevant snippets from the web. Third, model summarizes these snippets into concise description that replaces the original entity in the stem. This process repeats until the DiffScore exceeds threshold θd or reaches the maximum number of iterations Tr. No retrieved text is provided to models at evaluation time; only the rewritten stem is used. B.2 Effect on Model Performance To isolate the effect of refinement, we construct controlled variants of EpiQAL-C by applying 0 to Tr refinement iterations to the same base instances, regardless of whether they would be refined in the final pipeline. We evaluate each model with Chainof-Thought prompting at temperature 0. Results are shown in Table 8. Cls Class Top Topic Description Surveillance and Descriptive Epidemiology 2 Outbreak Investigation and Field Response 1 2 4 1 2 3 4 Frequency measures and standardization Time Place Person patterns, seasonality and clustering Aberration and outbreak detection System performance, deduplication and record linkage Diagnostic verification, field case definitions and line lists Event specific attack rates and epidemic curves Outbreak hypothesis mapping and source attribution"
        },
        {
            "title": "Immediate control and\nsituation reporting",
            "content": "Defines prevalence, incidence, person time, and applies standardization to make rates comparable. Describes temporal trends, spatial distribution, and demographic profiles using routine population surveillance. Builds statistical baselines and thresholds to flag unusual increases in counts, rates, or positivity, focuses on signal detection rather than source attribution. Assesses sensitivity, timeliness, and completeness, manages deduplication and linkage across multiple data sources. Confirms the pathogen, applies field case definitions, and builds and cleans line lists. Quantifies spread in defined groups and interprets epidemic curves for the event. Links cases by time, place, and shared exposures to identify likely sources and transmission chains, integrating line lists, environmental sampling, traceback, and genomic evidence. Implements urgent measures and documents current status with concise situation reports. Table 5: Epidemiology taxonomy topics, Classes 1 and 2 As shown in Table 8, model performance decreases after refinement and generally continues to decline with additional iterations, though the decrease becomes smaller over time. This pattern suggests that iterative entity replacement increases reasoning difficulty by expanding the information models must integrate. Considering the trade-off between generation efficiency and difficulty gain, we set Tr = 3. B.3 Example Table 9 shows representative instance before and after refinement. Refinement replaces salient entities with descriptive phrases that preserve answerability but remove direct lexical anchors. This requires models to map descriptions back to the correct concepts and integrate evidence from the passage. In Table 9, underlined text marks the entity selected for replacement at each iteration, and bold text indicates the retrieved description that replaces the original surface form. In Iteration 1, cutaneous leishmaniasis is replaced with descriptive paraphrase. Iteration 2 expands Leishmania parasites into higher-level description while preserving question intent. In Iteration 3, neglected tropical diseases is replaced, further reducing lexical overlap between the stem and source evidence. To answer correctly, models must identify which epidemiological entity the description refers to and use passage evidence to select the correct options, rather than relying on surface-form matching."
        },
        {
            "title": "C Experimental Details",
            "content": "C.1 Compute and Inference Settings Experiments run on NVIDIA H100 and H200 GPUs. Llama-3.3-70B-Instruct, and GLM-4.5-Air use four-bit inference, and all other models use default precision settings. C.2 Generation efficiency. All experiments run on two NVIDIA H100 GPUs. Generating 500 samples requires 43.78 hours for EpiQAL-A, 78.83 hours for EpiQAL-B, and 114.61 hours for EpiQAL-C, corresponding to approximately 5.3, 9.5, and 13.8 minutes per sample respectively. EpiQAL-B and EpiQAL-C take longer than EpiQAL-A due to additional verification steps and difficulty control. Compared with expertauthored annotation, the pipeline substantially reduces human cost by routing only small fraction of options to review. Cls Class Top Topic Description 3 Determinants and Exposures Susceptibility and Immunity 1 2 3 4 2 3 4 Contextual determinants of exposure Transmission modes and contact patterns Exposure response interpretation Zoonotic and One Health interfaces, reservoirs and vectors Susceptibility stratification and special populations Serology and correlates of protection Protection effectiveness, waning, reinfection and immune escape"
        },
        {
            "title": "Severity risk and prognostic\nfactors",
            "content": "Integrates individual behaviors with environmental, occupational, and social and structural conditions that shape exposure probability and inequities. Describes general routes of spread and population contact structures across settings. Specifies the exposure metric, determines whether values are above or below assay limits and thresholds, and interprets exposure to infection, severity, or transmissibility patterns as reported in the passage. Identifies animal reservoirs, vectors, and human animal environment interfaces where spillover can occur. Identifies groups more susceptible to infection based on demographic and clinical traits and setting specific contexts. Estimates seroprevalence and relates immune markers to protection thresholds and population level immunity. Describes protection after vaccination or prior infection, its change over time, risks of reinfection, hybrid immunity, and variant related escape, considers how vaccine dose number and dose intervals influence vaccine effectiveness and its waning over time. Assesses risk of severe outcomes conditional on infection and stratifies prognosis by host factors. Table 6: Epidemiology taxonomy topics, Classes 3 and 4 C.3 Preprocessing. We extract structured sections when available and normalize raw text by removing reference lists and non-content artifacts. Documents are assembled in fixed section order to reduce variance across instances. We drop papers with missing main text or abnormal formatting that prevents reliable section parsing. C.4 Model Configuration. Generation model. We use Qwen3-30B-A3BInstruct-2507 as the generation model. For disease entity extraction, we use GLiNER (Zaratiana et al., 2024). For entity linking in EpiQAL-B construction, we use SapBERT (Liu et al., 2021) to encode mentions and retrieve candidate disease entities from knowledge graphs. To summarize knowledge graph triples into natural language signals, we use Llama-3.3-70B-Instruct. Generation temperature is set to 0 for reproducibility. Checking model group. We verify generated options using instruction-tuned models from different families: GLM-4.5-Air, Mistral-Large-Instruct2411, Llama-3.3-70B-Instruct, and Qwen3-30BA3B-Instruct-2507. Each checker runs 3 times with temperature 1.0, and decisions are aggregated into the vote ratio defined in Section 3.5.1. We set the rejection threshold θc = 0.7 and acceptance threshold θh = 0.8. Difficulty judging pool. To estimate difficulty as described in Section 3.6, we evaluate pool of models ranging from small to large: Phi-4-mini-instruct, Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, Qwen3-8B, Llama-3.1-8B-Instruct, Qwen3-30BA3B-Instruct-2507, Qwen3-32B, Llama-3.3-70BInstruct, and GLM-4.5-Air. We compute DiffScore with α = 0.7 and average across models. The difficulty threshold is θd = 0.9, maximum refinement iterations Tr = 3, and retrieval budget Kr = 6 snippets."
        },
        {
            "title": "D Dataset Analysis",
            "content": "This appendix provides additional analysis of dataset composition for EpiQAL-A and EpiQAL-B, which use taxonomy-guided generation. EpiQALC derives supervision from paper structure rather than taxonomy and is not included in this analysis. Cls Class Top Topic Description 5 Modeling, Methods, and Evaluation 6 Projections and Forecasts 1 3 4 5 1 2 4 Transmission modeling and inference Real time debiasing and delay adjustment Study design and causal effects Measurement and bias handling Program performance and impact evaluation Near term forecasting Scenario projections Forecast evaluation and model combination"
        },
        {
            "title": "Decision oriented\nforecasting and risk\ncommunication",
            "content": "Uses mechanistic or statistical models to estimate transmission parameters and infer transmission patterns. Reconstructs recent incidence by adjusting for reporting delays, right truncation, and under ascertainment. Selects designs and identification strategies and defines effect measures for causal estimation. Addresses measurement validity, misclassification and measurement error, confounding and selection, generalizability, survey weighting, and sample size. Assesses coverage and implementation fidelity, audits routine data quality, evaluates real world effectiveness, and estimates disease burden. Produces short horizon probabilistic forecasts for upcoming values and quantifies forecast uncertainty. Projects future trajectories under stated assumptions about policy, behavior, or immunity. Assesses forecast quality using proper scoring rules, calibration, and sharpness diagnostics, and develops or applies methods to combine multiple forecasting models to improve predictive accuracy, stability, and robustness across contexts. Maps forecast probabilities to operational thresholds or cost loss trade offs and communicates uncertainty for decision making. Table 7: Epidemiology taxonomy topics, Classes 5 and 6 D.1 Class Distribution Figure 2 shows the distribution of instances across the six taxonomy classes. Both subsets achieve broad coverage, with Surveillance and Descriptive Epidemiology and Determinants and Exposures being the most frequent classes. This distribution reflects the prevalence of these topics in the source corpus of neglected tropical disease research. D.2 Topic Distribution Figure 3 shows the distribution across all 25 topics. Coverage is generally balanced, though some variation exists due to the natural distribution of topics in the source articles. Topics related to transmission modes, susceptibility, and disease burden appear most frequently."
        },
        {
            "title": "E Additional Related Work",
            "content": "Machine reading comprehension. Early work on machine reading comprehension cast question answering as span selection within controlled contexts, enabling precise evaluation of extractive models (Rajpurkar et al., 2016; Joshi et al., 2017). With the rise of instruction-tuned large language models, generation-based QA has become competitive, yet multiple choice formats remain attractive because they encourage targeted reasoning while preserving objective scoring (Nie et al., 2020; Hendrycks et al., 2021). Scientific articles often restate conclusions with considerable lexical overlap, meaning that purely extractive setups can overestimate genuine inference. This observation motivates evaluation formats that probe reasoning beyond surface matching. Additional biomedical QA resources. Beyond the benchmarks discussed in the main text, several resources address specific clinical needs. emrQA constructs QA pairs from electronic medical records using expert templates (Pampari et al., 2018). MedQuAD compiles question-answer pairs from trusted medical websites organized by topic (Ben Abacha and Demner-Fushman, 2019). These datasets primarily target patient-level clinical reasoning rather than population-level epidemiological inference. Retrieval augmentation and knowledge resources. Retrieval-augmented generation grounds model outputs in retrieved passages and is often Table 8: Exact Match accuracy on EpiQAL-C across stem refinement iterations, w/o CoT."
        },
        {
            "title": "Original",
            "content": "Iter 1 Iter 2 Iter 3 Phi-4-mini-instruct 0.452 0. 0.426 0.410 Meta-Llama Llama-3.2-3B-Instruct Llama-3.1-8B-Instruct 0.130 0.274 0.096 0. 0.094 0.238 0.124 0."
        },
        {
            "title": "Mistral AI",
            "content": "Mistral-7B-Instruct-v0.3 0.830 0.806 0.780 0."
        },
        {
            "title": "Qwen",
            "content": "Qwen3-8B Qwen3-30B-A3B-Instruct 0.542 0.544 0.502 0.518 0.470 0.522 0.478 0."
        },
        {
            "title": "Zhipu AI",
            "content": "GLM-4.5-Air 0.578 0.558 0.554 0.558 Table 9: An example of stem refinement. The options are unchanged, and only the question stem is rewritten."
        },
        {
            "title": "Version\nOriginal",
            "content": "Iteration 1 Iteration 2 Iteration 3 Question stem Which of the following best captures the primary implication of integrating patient-reported experiences and preferences into the early-stage development of medicinal products for neglected tropical diseases, based on the qualitative findings from multi-country study on cutaneous leishmaniasis? Which of the following best captures the primary implication of integrating patient-reported experiences and preferences into the early-stage development of medicinal products for neglected tropical diseases, based on the qualitative findings from multi-country study on vector-borne skin disorder caused by Leishmania parasites, characterized by painless, chronic ulcers or nodules on exposed body parts, primarily resulting from sandfly bites and affecting millions globally? Which of the following best captures the primary implication of integrating patient-reported experiences and preferences into the early-stage development of medicinal products for neglected tropical diseases, based on the qualitative findings from multi-country study on vector-borne skin disorder caused by protozoan parasites from over 20 species transmitted to humans via bites of infected phlebotomine sandflies, primarily causing chronic skin lesions through vector-borne transmission, affecting millions globally? Which of the following best captures the primary implication of integrating patient-reported experiences and preferences into the early-stage development of medicinal products for diverse group of communicable diseases caused by parasitic, bacterial, fungal, viral, and protozoan pathogens, predominantly affecting impoverished populations in tropical and subtropical regions and perpetuating cycles of poor health, social marginalization, and economic hardship, based on the qualitative findings from multi-country study on vector-borne skin disorder caused by protozoan parasites from over 20 species transmitted to humans via bites of infected phlebotomine sandflies, primarily causing chronic skin lesions through vector-borne transmission, affecting millions globally? Figure 2: Class distribution for EpiQAL-A and EpiQAL-B. used to mitigate hallucination (Lewis et al., 2020; Izacard and Grave, 2021; Bhasuran et al., 2025)."
        },
        {
            "title": "Structured resources such as Hetionet and iBKH\nencode biomedical entities and relations that can",
            "content": "Figure 3: Topic distribution for EpiQAL-A and EpiQAL-B. support downstream reasoning (Himmelstein et al., 2017; Su et al., 2022). For epidemiology-oriented knowledge, eKG-DONs compiles outbreak reports from official sources (S et al., 2025). Recent work studies instruction-aware retrieval across heterogeneous sources (Min et al., 2025) and integration of knowledge graphs with multi-agent reasoning (Xie et al., 2025; Xu et al., 2025). In EpiQAL-B construction, we operationalize structured relations by summarizing knowledge graph triples into natural language signals used only during generation; these signals are withheld at evaluation time."
        },
        {
            "title": "F Prompt",
            "content": "Tables 10, 11, and 12 show the emphasized generation prompts for EpiQAL-A, EpiQAL-B, and EpiQAL-C, respectively. Table 10: Prompts used for EpiQAL-A Generation. Question Generation: Your task is to generate retrieval-based question using the provided passage. The question should be answerable by directly locating information in the passage, without requiring inference or external knowledge. [... ...] Step 3: Write one question that requires readers to locate and retrieve specific information from the passage. The question should have clear, unambiguous answer that appears explicitly in the passage. Step 4: Apply quality requirements. good retrieval question should target specific factual content rather than vague or general information, have an answer that is explicitly stated in the passage in locatable form, and not be answerable by general knowledge alone without reading the passage. Step 5: Apply question stem constraints. The question stem should not copy phrases directly from the passage that would make the answer obvious, should not be so broad that multiple unrelated answers could apply, and should be grammatically complete and clear. [... ...] Correct Option Generation: Your task is to generate correct options for retrieval-based question. The correct options should be answers that can be directly found in the passage. You will be given the passage, the question, and the evidence from question generation. [... ...] Step 3: Generate one or more correct options. Each option must be directly supported by explicit text in the passage. Do not infer or add information not present in the passage. Step 4: Apply option constraints. Each option should use concise wording that captures the answer without copying the entire evidence sentence. Each option should be semantically complete, though it does not need to be full sentence. Each option must not contradict any information in the passage. Step 5: If generating multiple options, ensure each represents distinct correct answer from different parts of the passage. Options should not overlap or be redundant. [... ...] Ditractor Generation: Your task is to generate distractors for retrieval-based question. Distractors should be plausible-sounding answers that appear in the passage but do not correctly answer the specific question asked. They test whether readers can precisely locate the correct information rather than guessing based on keyword matching. You will be given the passage, the question, and the correct options. [... ...] Step 2: Identify content in the passage that could be confused with the correct answer. Good distractors share these characteristics: - They belong to the same semantic category as the correct option such as both being locations, numbers, time periods, or names - They appear in the passage and are factually accurate within the passage context - They relate to different entity, time, place, or context than what the question specifically asks about Step 3: Generate distractors using only information from the passage. Each distractor must be valid fact stated in the passage but incorrect as an answer to this specific question. [... ...] Table 11: Prompts used for EpiQAL-B Generation. Question Generation: Your task is to generate multiple-choice style question that requires multi-step reasoning. The question should be grounded in the passage, guided by the topic, and optionally informed by external domain knowledge. [... ...] Step 2. Identify passage-anchored detail that the question must rely on. This should be specific fact, number, observation, or finding that appears in the passage. The question must be impossible to answer without this anchored detail. Step 3. Select at least two pieces of evidence from the passage that must be combined to answer the question. These pieces of evidence should come from different sentences or different parts of the text. Step 4. Evaluate whether the external domain knowledge is relevant. If any meaningful connection exists, you must incorporate relevant information from the external domain knowledge as part of your evidence. Step 5. Establish the reasoning chain among your selected evidence. Before writing the question, plan how the evidence pieces connect logically. Step 6. Before finalizing your question, verify that it truly requires multi-step reasoning. Step 7. Verify that the question asks about something the passage does not directly answer. Step 8. Write one question stem that requires the reasoning chain you planned. Step 9. Ensure the question leaves room for multiple plausible answer directions. [... ...] Correct Option Generation: Your task is to generate correct options for multiple-choice question that requires multi-step reasoning. The options should be derived conclusions that emerge from integrating the provided evidence, not facts that can be directly retrieved from the passage. [... ...] Step 3. Draft one or more correct options. Each option must satisfy these requirements: - It must be conclusion that requires integrating at least two pieces of the provided evidence - It must not be direct paraphrase of any single sentence in the passage - It must not be verifiable by reading only one evidence piece - It must require applying an epidemiological principle or methodological concept to interpret the evidence - It must use different vocabulary from the passage where possible while preserving accuracy [... ...] Ditractor Generation: Your task is to generate distractors for multiple-choice question that requires multi-step reasoning. Distractors must look structurally identical to the correct options but contain subtle logical flaw that can only be detected through careful reasoning. [... ...] Step 3. Identify multiple vulnerable points in the reasoning chain where reader might go wrong. Consider these categories of errors: - Confusing related but distinct concepts - Applying valid method to an incompatible study design - Mixing up the target variable with superficially similar variable - Using correct terminology but violating underlying assumptions - Drawing conclusions that would require different data than what is available [... ...] Table 12: Prompts used for EpiQAL-C Generation. Correct Option Extraction: Your task is to extract one conclusion from the provided Discussion section that will serve as the Correct Option for reasoning test. Readers will see only the Passage Body and must identify which conclusion can be derived from it. [... ...] Step 3. Apply the novelty requirement. The conclusion must not be explicitly stated anywhere in the Passage Body. Reject candidates where the same statement appears in the Results or other sections. Step 4. Apply the derivability requirement. The conclusion must be logically derivable from evidence in the Passage Body by applying general epidemiological principles. Reject conclusions that require: - Results from other studies cited in the Discussion but not described in the Passage Body - Specific facts about diseases, treatments, or populations not mentioned in the Passage Body - Comparisons to external benchmarks or statistics not provided in the Passage Body Step 5. Apply the complexity requirement. Prefer conclusions that: - require integrating multiple pieces of evidence from the Passage Body - require applying epidemiological principles to interpret the data - represent key finding rather than minor observation Step 6. Apply exclusion criteria. Reject conclusions that: - are direct numerical summaries already stated in the Results - describe study limitations or future research directions - are speculative statements without clear evidential basis in the Passage Body - are generic statements applicable to any similar study [... ...] Question Generation: Your task is to generate question stem for single-choice reasoning test. The question must be answerable only by the provided Correct Option, which is conclusion derived from the Passage Body through epidemiological reasoning. [... ...] Step 3. Design question that requires readers to integrate the evidence pieces and apply the same epidemiological reasoning to arrive at the Correct Option. The question should set up reasoning task without revealing the answer direction. Step 4. Apply difficulty requirements. good question should: - require integrating multiple pieces of evidence rather than relying on single fact - require applying epidemiological principles to interpret the data - not be answerable by simply locating sentence in the Passage Body Step 5. Apply concealment requirements. The question stem: - must not use any words or phrases that appear in the Option field - must not use synonyms or paraphrases that directly hint at the conclusion - must not indicate the type of answer expected such as prognosis, risk, or recommendation - must not reveal which evidence pieces are relevant [... ...] Ditractor Generation: Your task is to generate distractors for reasoning test. Distractors should be plausible-sounding conclusions that cannot actually be derived from the Passage Body alone. [... ...] Step 2. Identify candidate distractor statements from the Discussion section. Good distractors fall into one of these categories: - External dependency: Conclusions that require information from other studies cited in the Discussion but not described in the Passage Body - Speculation: Statements about future research directions, untested hypotheses, or possibilities using hedging language such as may, might, or could - Limitations: Statements about study limitations or methodological caveats - Background only: Statements that merely restate general background knowledge - Causal reversal: statement created by reversing or misinterpreting the cause-effect relationship implied in the correct option Step 3. Verify each candidate meets two requirements: - It cannot answer the question. If candidate could be derived from the Passage Body through valid reasoning, discard it. - It should be relevant to what the question asks. Prefer distractors that address similar aspects as the question and the correct option. [... ...]"
        }
    ],
    "affiliations": [
        "Emory University",
        "Microsoft",
        "University of Illinois Chicago"
    ]
}