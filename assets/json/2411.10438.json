{
    "paper_title": "MARS: Unleashing the Power of Variance Reduction for Training Large Models",
    "authors": [
        "Huizhuo Yuan",
        "Yifeng Liu",
        "Shuang Wu",
        "Xun Zhou",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training deep neural networks--and more recently, large models--demands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained a less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose a unified optimization framework, MARS (Make vAriance Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via a scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw a connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by a large margin."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 1 8 3 4 0 1 . 1 1 4 2 : r MARS: Unleashing the Power of Variance Reduction for Training Large Models Huizhuo Yuan Yifeng Liu Shuang Wu Xun Zhou Quanquan Gu Abstract Training deep neural networksand more recently, large modelsdemands efficient and scalable optimizers. Adaptive gradient algorithms like Adam, AdamW, and their variants have been central to this task. Despite the development of numerous variance reduction algorithms in the past decade aimed at accelerating stochastic optimization in both convex and nonconvex settings, variance reduction has not found widespread success in training deep neural networks or large language models. Consequently, it has remained less favored approach in modern AI. In this paper, to unleash the power of variance reduction for efficient training of large models, we propose unified optimization framework, MARS (Make vAriance Reduction Shine), which reconciles preconditioned gradient methods with variance reduction via scaled stochastic recursive momentum technique. Within our framework, we introduce three instances of MARS that leverage preconditioned gradient updates based on AdamW, Lion, and Shampoo, respectively. We also draw connection between our algorithms and existing optimizers. Experimental results on training GPT-2 models indicate that MARS consistently outperforms AdamW by large margin."
        },
        {
            "title": "Introduction",
            "content": "Adaptive gradient methods such as Adam (Kingma, 2014) and AdamW (Loshchilov, 2017) have become the predominant optimization algorithms in deep learning. With the surge of large language models, the majority of the renowned models, including GPT-2 (Radford et al., 2019), GPT-3 (Brown, 2020), PaLM (Chowdhery et al., 2023) and Llama 3 (Dubey et al., 2024) are trained with adaptive gradient methods. Numerous efforts have been made to improve adaptive gradient methods from both first-order and second-order optimization perspectives. For example, You et al. (2019) introduced LAMB, layerwise adaptation technique that boosts training efficiency for BERT (Devlin, 2018). Using symbolic search, Chen et al. (2023) developed Lion, achieving faster training and reduced memory usage. Liu et al. (2023) designed Sophia, leveraging stochastic diagonal Hessian estimators to accelerate training. Gupta et al. (2018) proposed Shampoo, which performs stochastic optimization Equal contribution ByteDance Research, San Jose, CA, USA; e-mail: huizhuo.yuan1@bytedance.com The work was done during Yifengs internship at ByteDance Inc. Department of Computer Science, University of California, Los Angeles, CA, USA; e-mail: liuyifeng@cs.ucla.edu ByteDance Inc., Beijing, China; email: wushuang.58@bytedance.com ByteDance Inc., Beijing, China; email: zhouxun@bytedance.com ByteDance Research, Los Angeles, CA, USA; e-mail: quanquan.gu@bytedance.com 1 over tensor spaces with preconditioning matrices for each dimension. Anil et al. (2020) further refined Shampoo to give scalable, practical version. Recently, Vyas et al. (2024) showed that Shampoo is equivalent to Adafactor (Shazeer and Stern, 2018) in the eigenbasis of Shampoos preconditioner, and introduced SOAP, which stabilizes Shampoo with Adam. Importantly, recent studies (Kaddour et al., 2024; Zhao et al., 2024) have shown that these optimizers perform on par with AdamW in LLM pretraining, yet do not outperform it. This suggests the ongoing challenge in developing adaptive gradient methods superior to Adam and AdamW for large-scale model training. Since adaptive gradient methods face challenges of high stochastic gradient variance, and language model training inherently involves high-variance optimization problem (McCandlish et al., 2018), it is natural to consider variance reduction techniques to address this challenge. There exists large body of literature on variance reduction for stochastic optimization, such as SAG (Roux et al., 2012), SVRG (Johnson and Zhang, 2013) and STORM (Cutkosky and Orabona, 2019), which can improve the convergence of stochastic optimization. However, variance reduction has not found widespread success in training deep neural networks or large language models. Defazio and Bottou (2019) discussed why variance reduction can be ineffective in deep learning due to factors such as data augmentation, batch normalization, and dropout, which disrupt the finite-sum structure required by variance reduction principles. Nevertheless, in training language models, data augmentation, batch normalization, and dropout are nowadays rarely used, which opens the door for applying variance reduction techniques in optimizing these models. This naturally leads to the following research question: Can variance reduction technique be applied to improve the performance of training large models? In this paper, we answer the above question affirmatively by introducing novel optimization framework called MARS (Make vAriance Reduction Shine), which incorporates variance reduction into adaptive gradient methods. Notably, we introduce scaling parameter into the stochastic recursive momentum (STORM) (Cutkosky and Orabona, 2019) to adjust the strength of variance reduction and define new gradient estimator. This gradient estimator undergoes gradient clipping and is subsequently subjected to exponential averaging. When the variance reduction strength is set to 1, it recovers the vanilla STORM momentum. In addition, the second-order momentum update is defined by the reweighted intermediate variable. These together ensure optimization stability throughout the training process. We summarize our major contributions of this paper as follows: We propose unified framework for preconditioned variance reduction, namely MARS. At its core, MARS comprises two major components: (1) scaled stochastic recursive momentum, which provides variance-reduced estimator of the full gradient for better gradient complexity; and (2) the preconditioned update, which approximates the second-order Newtons method for better per-iteration complexity. By combining preconditioned gradient methods with variance reduction, MARS achieves the best of both worlds, accelerating the search for critical points in optimization. The MARS framework is versatile, accommodating all existing full matrix or diagonal Hessian approximations. Under this framework, we utilize three distinct designs of the preconditioning matrix, resulting in three specific instances of our MARS framework: MARS-AdamW, MARSLion, and MARS-Shampoo. Each variant demonstrates compatibility with their corresponding 2 preconditioning in AdamW, Lion, and Shampoo, showing that MARS can seamlessly integrate with and do variance-reduction on these established methods. Empirically, we evaluated MARS-AdamW (or simply MARS) on GPT-2 fine-tuning tasks using the OpenWebText dataset. MARS demonstrated superior performance on GPT-2 large: it reached validation loss of 2.58 within 27 billion tokens, whereas AdamW required 50 billion tokens to achieve the same level. The final validation loss achieved by MARS on GPT-2 large is 2.53, compared to 2.56 for AdamW. Furthermore, on the downstream task Hellaswag, MARS improved accuracy to 44.20%, outperforming AdamWs 42.31% after training on 50 billion tokens, highlighting its superior performance in training large models. In this paper, we assume xt denotes the parameter of the language model at step and Notations ξ1, ..., ξT Ξ are sequence of independent random variables which denote the training data for each step. For some objective function that is differentiable, we assume E[f (x, ξt)x] = (x) for x, t. In our algorithm, the training data of the current step ξt and previous step ξt1 are used for attaining different gradient for the same parameter xt, so we just explicitly indicate these variables for function ."
        },
        {
            "title": "2 Preliminaries\nIn this section, we review the preliminaries of stochastic optimization, including standard stochastic\ngradient methods and variance reduction.",
            "content": "We consider minimizing an objective function () : Rd as follows: (x) = EξD[f (x, ξ)], min (2.1) where (x, ξ) is possibly nonconvex loss function, Rd is the optimization variable, ξ is random vector (e.g., training data point) drawn from an unknown data distribution D. We assume the access to the first-order oracle, which returns an unbiased estimator of the gradient E[f (x, ξ)] = (x). The standard stochastic gradient descent (SGD) algorithm yields: xt+1 = xt ηtf (xt, ξt), (2.2) where ηt > 0 is the learning rate or step size. SGD needs O(ε4) stochastic gradient evaluations (i.e., gradient complexity or incremental first-order oracle complexity) to find ϵ-approximate first-order stationary points, i.e., (x)2 ϵ (Ghadimi and Lan, 2013). To accelerate the convergence of SGD, variance reduction techniques have been extensively researched in both the machine learning and optimization communities over the past decade, resulting in numerous algorithms for convex optimizationsuch as SAG (Roux et al., 2012), SVRG (Johnson and Zhang, 2013), SAGA (Defazio et al., 2014), and SARAH (Nguyen et al., 2017a)as well as for nonconvex optimization, including SVRG (Allen-Zhu and Yuan, 2016; Reddi et al., 2016), SNVRG (Zhou et al., 2020), SPIDER (Fang et al., 2018), and STORM (Cutkosky and Orabona, 2019), among others. Notably, for nonconvex optimization, SNVRG (Zhou et al., 2020), SPIDER (Fang et al., 2018) and STORM (Cutkosky and Orabona, 2019) can improve the gradient complexity of SGD from O(ε4) to O(ε3), demonstrating provable advantage. 3 At the heart of variance reduction techniques is variance-reduced stochastic gradient, exemplified by the method proposed by Johnson and Zhang (2013) as follows: mt = (xt, ξt) ((cid:101)x, ξt) + ((cid:101)x), (cid:101)x is an anchoring point (a.k.a., reference point) that updates periodically. This variancewhere reduced stochastic gradient can reduce the variance of the stochastic gradient by adding correction term ((cid:101)x, ξt) + ((cid:101)x) based on less frequently updated reference point (cid:101)x and its full gradient ((cid:101)x). It can be shown that the variance mt can be controlled by xt (cid:101)x2, which will diminish as both xt and (cid:101)x converges to the stationary points when the algorithm makes progress. Subsequent improvements in variance reduction techniques were introduced in SARAH (Nguyen et al., 2017a) and SPIDER (Fang et al., 2018), which get rid of the anchor point and result in the following momentum update: mt = (xt, ξt) (xt1, ξt) + mt1, xt+1 = xt ηtmt. (2.3) In the context of training neural networks, xt Rd represents the trained weights in the neural network, ξt represents random data, and mt is the variance-reduced (VR) first-order momentum. The stochastic gradient difference term (xt, ξt) (xt1, ξt) cancels out common noise brought by ξt, while pushing the gradient estimation from the estimator of (xt1) to the estimator of (xt). However, mt needs to be reset periodically to full gradient (or large batch stochastic gradient) (xt), which we refer to as an anchoring step, analogous to the anchor point in SVRG. Subsequently, Cutkosky and Orabona (2019) introduced Stochastic Recursive Momentum (STORM), variant of standard momentum with an additional term, achieving the same convergence rate as SPIDER while eliminating the need for periodic anchoring: mt = β1mt1 + (1 β1)f (xt, ξt) + β1 (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) (2.4) where β1 > 0 is momentum parameter, and β1(f (xt, ξt) (xt1, ξt)) is the additional term that has variance reduction effect. Note that if xt xt1, STORM becomes approximately the standard momentum. Alternatively, (2.4) can be rewritten as an exponential moving average (EMA) of the first order momentum from previous step/iteration and the stochastic gradient with gradient correction term: mt = β1mt1 + (1 β1) (cid:104) (xt, ξt) + β1 1 β1 (cid:124) (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) (cid:125) (cid:123)(cid:122) gradient correction (cid:105) . (2.5) Theoretically, when assuming access to an unbiased stochastic first-order oracle to the objective function (x), STORM achieves the nearly optimal gradient complexity of O(ε3) for non-convex and smooth optimization problems (Arjevani et al., 2023)."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce MARS (Make vAriance Reduction Shine), family of precondtioned optimization algorithms that perform variance reduction in gradient estimation."
        },
        {
            "title": "3.1 MARS Framework",
            "content": "We first introduce our framework for preconditioned, variance-reduced stochastic optimization, which unifies both first-order (e.g., AdamW, Lion) and second-order (e.g., Shampoo) adaptive gradient methods. Preconditioned Variance Reduction. Variance reduction methods achieve faster convergence than SGD, yet identifying optimal learning rates remains practical challenge. Particularly, different parameters often exhibit varying curvatures, requiring tailored learning rates for each. One approach to addressing this issue is to use the Hessian matrix to precondition gradient updates, integrating curvature information into the updates. The idea stems from minimizing the second-order Taylor expansion at xt: (xt+1) (xt) + (xt)(xt+1 xt) + 1 2 (xt+1 xt)2F (xt)(xt+1 xt), (3.1) resulting in the update formula xt+1 = xt H1 (xt), where Ht := 2F (xt) Rdd is the Hessian matrix. In our paper, we encapsulate the preconditioned gradient H1 (xt) update within more generalized framework of Online Mirror Descent (OMD) as in Gupta et al. (2018), leading to the following update rules: (cid:26) xt+1 = arg min xRd ηt mt, + xt2 Ht (cid:27) , 1 2 (3.2) where ηt > 0 can be viewed as base learning rate. Combining (3.2) with the STORM momentum, we obtain the following preconditioned variance-reduced update: (cid:104) mt = β1mt1 + (1 β1) (xt, ξt) + (cid:26) xt+1 = arg min xRd ηt mt, + 1 2 xt2 Ht . β1 1 β1 (cid:27) (cid:0)f (xt, ξt) (xt1, ξt)(cid:1)(cid:105) , (3.3) (3.4) Remark 3.1. SuperAdam (Huang et al., 2021) also incorporates the STORM into the design of adaptive gradient methods. However, their precondition matrix can be viewed as special case of our general framework. SuperAdams design focuses on diagonal precondition matrix and draws heavily from the design used in Adam (Kingma, 2014), AdaGrad-Norm (Ward et al., 2020), and AdaBelief (Zhuang et al., 2020). Furthermore, their preconditioner matrix is designed following Adams structure but does not account for the revised definition of variance-reduced momentum, resulting in significant mismatch between the first-order and second-order momentum. We will further clarify these differences when discussing specific instances of our framework. In practice, alongside our preconditioned variance-reduced update (3.3), we Algorithm Design. introduce scaling parameter γt to control the scale of gradient correction in variance reduction. We also introduce new gradient estimator ct, which is the combination of stochastic gradient and the scaled gradient correction term: ct = (xt, ξt) + γt (cid:124) β1 1 β1 (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) (cid:125) (cid:123)(cid:122) scaled gradient correction . (3.5) 5 When γt = 1, (3.5) reduces to the second term of (3.3). On the other hand, when γt = 0, (3.5) reduces to the stochastic gradient. Thus, ct can be seen an gradient estimator with adjustable variance control. Following standard techniques in deep learning practice, we also perform gradient clipping on ct, which is calculated by: (cid:101)ct = Clip(ct, 1) = (cid:40) ct ct2 ct if ct2 > 1, otherwise. (3.6) We note that the Second-order Clipped Stochastic Optimization (Sophia) algorithm (Liu et al., 2023) also incorporates clipping in their algorithm design. However, their approach does clipping upon the preconditioned gradient with clipping-by-value, while our method applies clipping to the intermediate gradient estimate using the more standard technique of clipping-by-norm. After the gradient clipping, the VR momentum mt can be calculated as the EMA of (cid:101)ct. The resulting MARS algorithm is summarized in Algorithm 1. Algorithm 1 MARS 1: input: x0, β1, {γt}, {ηt} 2: Set m0 0 and x1 x0 3: for = 1, to do 4: Sample ξt and let ct = (xt, ξt) + γt if ct2 > 1, then (cid:101)ct = ct else (cid:101)ct = ct 5: ct2 6: mt = β1mt1 + (1 β1)(cid:101)ct 7: 8: end for xt+1 = arg minx (cid:8)ηt mt, + 1 2 xt2 Ht β1 1β1 (cid:9) (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) In practice, calculating the Hessian matrix is computationally Full Matrix Approximation. expensive or even intractable due to the complexity of second-order differentiation and the significant memory cost of storing Ht, especially when the parameters in neural network constitute highdimensional matrix. Many existing algorithms employ various approximations of the Hessian. For instance, K-FAC (Martens and Grosse, 2015) and Shampoo (Gupta et al., 2018) approximate the Gauss-Newton component of the Hessian (also known as the Fisher information matrix), using layerwise Kronecker product approximation (Morwani et al., 2024). Additionally, Sophia (Liu et al., 2023) suggests using Hutchinsons estimator or the Gauss-Newton-Barlett estimator for approximating the Hessian. We take various designs of the preconditioning matrix into account and broaden the definition of Ht in (3.4) to encompass various specifically designed preconditioning matrix in the rest of the paper. Diagonal Matrix Approximation. Even when using approximated Hessian matrices, secondorder algorithms mentioned above remain more computationally intensive compared to first-order gradient updates. Thus, another line of research focuses on approximating the Hessian matrix through diagonal matrices, as seen in optimization algorithms like AdaGrad (Duchi et al., 2011), RMSProp (Tieleman, 2012), AdaDelta (Zeiler, 2012), Adam (Kingma, 2014) and AdamW (Loshchilov, 2017), etc. This approach to diagonal preconditioning effectively transforms the updates into first-order method, assigning adaptive learning rates to each gradient coordinate. For example, in 6 AdaGrad (Duchi et al., 2011), the preconditioned matrix is defined by: [Ht]ii = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) τ =0 (cid:2)f (xτ , ξτ )(cid:3)2 . On the other hand, Adam can be seen as using diagonal Ht, where each diagonal element is the EMA of [f (xt, ξt)]2 : [Ht]ii = β[Ht1]ii + (1 β)[f (xt, ξt)]2 . (3.7) Therefore, the update simplifies to elementwise adaptive gradient update, i.e., [xt+1]i = [xt]i η[mt]i/[Ht]ii. Our unified framework accommodates both types of preconditioning: full Hessian approximation and diagonal Hessian approximation. Different definitions of Ht give rise to different algorithms. Notably, full-matrix approximations of the Hessian are potentially more powerful than diagonal approximations, as they can capture statistical correlations between the gradients of different parameters. Geometrically, full-matrix approximations allow both scaling and rotation of gradients, whereas diagonal matrices are limited to scaling alone. 3."
        },
        {
            "title": "Instantiation of MARS",
            "content": "In previous subsection, we introduced our preconditioned variance reduction framework in Algorithm 1 and discussed various approaches for approximating the Hessian matrix. In this subsection, we introduce practical designs of MARS under different choices of Ht. While here we only present three instantiations: MARS-AdamW, MARS-Lion, and MARS-Shampoo, we believe there are many other instances of MARS can be derived similarly."
        },
        {
            "title": "3.2.1 MARS-AdamW",
            "content": "The first instance of MARS is built up on the idea of Adam/AdamW (Loshchilov, 2017). To automatically adjust the learning rate and accelerate convergence, Adam (Kingma, 2014) adopts the adaptive preconditioned gradient in (3.7) together with bias correction and ℓ2 regularization. AdamW (Loshchilov, 2017) further changes the ℓ2 regularization to decoupled weight decay. Overall, the full AdamW updates can be summarized as follows: mt = β1mt1 + (1 β1)f (xt, ξt), vt = β2vt1 + (1 β2)(cid:0)f (xt, ξt)(cid:1)2, (cid:98)mt = mt 1 βt 1 xt+1 = xt ηt , (cid:98)vt = (cid:18) (cid:98)mt vt 1 βt 2 , (cid:19) . + λxt (cid:98)vt + ϵ (3.8) (3.9) (3.10) (3.11) We see that except for the small ϵ introduced for computational stability, and the decoupled weight decay λxt, AdamW can be seen as step of mirror descent update (3.2) with mt defined in (3.8), vt defined in (3.9), and Ht defined by Ht := (cid:114) diag (cid:16) (cid:17) vt 1 βt 1 (cid:112)1 βt . (3.12) 7 In MARS-AdamW, we implement the preconditioned variance-reduced update as in (3.4), and utilize the same definitions for Ht, ϵ, and weight decay as those specified in AdamW. For vt, different from the EMA of squared gradients g2 in AdamW, we redefine it to fit our variance-reduced stochastic gradient. Specifically, we denote the summation of the stochastic gradient and the scaled gradient correction term by ct and define vt as the EMA of c2 as follows: ct := (xt, ξt) + γt β1 1 β1 mt = β1mt1 + (1 β1)ct, vt = β2vt1 + (1 β2)c2 . (cid:0)f (xt, ξt) (xt1, ξt)(cid:1), (3.13) (3.14) (3.15) Here, γt is scaling parameter parameter that controls the strength of gradient correction. When γt = 0, the algorithm reduces to AdamW. Conversely, when γt = 1, (3.14) aligns with the STORM momentum. Combining (3.13), (3.14), (3.15) together with (3.12) and the mirror descent update (3.2), we derive the MARS-AdamW algorithm in Algorithm 2. In practice, γt is often set between 0 and 1. Moreover, we employ gradient clipping-by-norm to ct at Line 5, following the standard gradient clipping technique performed in neural network training. Remark 3.2. Compared with SuperAdam (Huang et al., 2021), one key difference is that our algorithm defines the second-order momentum vt as the exponential moving average of the square norm of ct rather than the square norm of the stochastic gradient. This new definition of secondorder momentum is crucial for accommodating the right scale of updates on coordinate-wise basis. Moreover, as we mentioned in Algorithm 1, we introduce scaling parameter γt and implement gradient clipping on ct. In Section 4, we will demonstrate empirically that the changes contribute to effective performance in large language model training. Finally, our algorithm utilizes bias correction and weight decay while SuperAdam does not. Remark 3.3. Careful readers might have noticed that in each iteration of our algorithm, we need to calculate the stochastic gradient twice for different data batches ξt1 and ξt with the same parameters. In order to overcome this problem, we propose to use (cid:0)f (xt, ξt) (xt1, ξt1)(cid:1) to approximate (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) in (3.13) and ct will be approximated by: ct (xt, ξt) + γt β1 1 β1 (cid:0)f (xt, ξt) (xt1, ξt1)(cid:1). To avoid confusion, we refer to the approximate version as MARS-approx. While MARS and MARS-approx differ in their updates and may theoretically exhibit distinct convergence guarantees, our experiments show that MARS provides only marginal improvements over MARS-approx in practice. Thus, we recommend using MARS-approx for practical applications. Connection with Adan. Adan (Xie et al., 2024) is another adaptive gradient methods improved upon Adam with reformulated Nesterovs accelerated SGD (See Lemma 1 in Xie et al. (2024) for more details). The Adan algorithm takes the following momentum updates: yt = β1yt1 + (1 β1)f (xt, ξt), zt = β2zt1 + (1 β2)(cid:0)f (xt, ξt) (xt1, ξt1)(cid:1), mt := yt + β2zt. (3.16) (3.17) (3.18) When β2 = β1, this reduces to mt = β1mt1 + (1 β1)(cid:2)f (xt, ξt) + β1 (cid:0)f (xt, ξt) (xt1, ξt1)(cid:1)(cid:3), which is special case of MARS-approxs momentum with γt = 1 β1. It is worth noting that although motivated by the Nesterovs momentum, Adans momentum updates in (3.16), (3.17) and (3.18) cannot recover Nesterovs momentum unless β1 = β2. Algorithm 2 MARS-AdamW 1: input: x0, λ, β1, β2, {γt}, {ηt} 2: Set m0 0, v0 0 and x1 x0 3: for = 1, to do 4: Sample ξt and let ct = (xt, ξt) + γt if ct2 > 1, then else β1 1β1 (cid:101)ct = ct (cid:101)ct = ct 5: ct2 6: mt = β1mt1 + (1 β1)(cid:101)ct vt = β2vt1 + (1 β2)(cid:101)c2 7: , (cid:98)mt = mt 1βt 1 xt+1 = xt ηt (cid:98)vt = vt 1βt 2 (cid:16) (cid:98)mt + λxt 8: (cid:17) (cid:98)vt+ϵ 9: 10: end for (cid:0)f (xt, ξt) (xt1, ξt)(cid:1)"
        },
        {
            "title": "3.2.2 MARS-Lion",
            "content": "Using symbolic program search, Chen et al. (2023) introduced simpler algorithm Lion compared to AdamW, which employs sign operation to maintain uniform magnitude across all parameters. The updates for Lion are illustrated as follows: mt = β2ut1 + (1 β2)f (xt, ξt), ut = β1ut1 + (1 β1)f (xt, ξt), xt+1 = xt ηt (cid:16) sign(mt) + λxt (cid:17) . (3.19) (3.20) (3.21) Instead of employing an EMA of gradient norms as in (3.9) and (3.12) of AdamW, the sign preconditioning mechanism in Lion utilizes Ht := (cid:113) diag(m2 ). (3.22) Following the same definition of Ht as in (3.22), we present MARS-Lion in Algorithm 3. Connection with Lion. Lion turns out to be special case of MARS-Lion. The momentum updates in Lion can be seen as an approximate implementation of our updates. To facilitate this claim, we present lemma that follows directly from straightforward arithmetic calculations. Lemma 3.4. For any sequence {gt Rd}t=0,1,..., consider the following updates of mt for any constant factors a1, a2, b1, and b2: ut = a1ut1 + a2gt, mt = b1ut + b2gt. 9 (3.23) (3.24) Algorithm 3 MARS-Lion 1: input: x0, λ, β1, {γt}, {ηt} 2: Set m0 0 and x1 x0 3: for = 1, to do 4: Sample ξt and let ct = (xt, ξt) + γt if ct2 > 1, then (cid:101)ct = ct else (cid:101)ct = ct 5: ct2 6: mt = β1mt1 + (1 β1)(cid:101)ct 7: 8: end for xt+1 = xt ηt sign(mt) + λxt (cid:17) (cid:16) β1 1β1 (cid:0)f (xt, ξt) (xt1, ξt)(cid:1) The updates are equivalent to mt = a1mt1 + (b1a2 a1b2 + b2)gt + a1b2(gt gt1). Setting gt = (xt, ξt), a1 = β1, a2 = 1 β1, b1 = β2, b2 = 1 β2 in Lemma 3.4, and shifting the index of ut by taking ut = ut1, ut1 = ut2 in Lemma 3.4, we can show that Lion momentum updates in (3.19) and (3.20) are equivalent to the following single momentum update: (cid:0)f (xt, ξt) (xt1, ξt1)(cid:1)(cid:105) β1(1 β2) 1 β1 On the other hand, setting γt = 1 β2 in the core updates of MARS (3.13) and (3.14), we obtain the mt update: mt = β1mt1 + (1 β1) (cid:104) (xt, ξt) + (3.25) . (cid:104) mt = β1mt1 + (1 β1) (xt, ξt) + β1(1 β2) 1 β1 (cid:0)f (xt, ξt) (xt1, ξt)(cid:1)(cid:105) . (3.26) The only difference between (3.25) and (3.26) lies in the stochasticity used, specifically, ξt versus ξt1 when calculating (xt1, ). Therefore, ignoring the gradient clipping at Line 5, we can see Lion as special case of MARS-Lion when γt = 1 β2, and using approximate gradient calculation on (xt1, ξt). In practice, we observe little difference between using (xt1, ξt1) derived from the STORM momentum and its approximation (xt1, ξt)."
        },
        {
            "title": "3.2.3 MARS-Shampoo",
            "content": "Shampoo (Gupta et al., 2018) introduces preconditioning approach that operates on the eigenspace of matrices. Given the gradient matrix Gt := ft(xt, ξt) Rmn, the update rules of Shampoo are displayed as follows: Lt = Lt1 + GtG , Rt = Rt1 + Gt, xt+1 = xt ηtL1/4 GtR1/4 , (3.27) where xt Rmn (slightly abusing notation) represents the corresponding weight matrix. It has been shown that the two-sided preconditioning in (3.27) is equivalent to preconditioning on the flattened vector gt := vec(Gt) with Kronecker product (Gupta et al., 2018; Morwani et al., 2024) Ht := (cid:16) (cid:88) τ =1 GtG (cid:17)1/4 (cid:79) (cid:16) (cid:88) t Gt (cid:17)1/4 . τ =1 10 In practice, an exponential moving average (EMA) is often used in place of the direct summation. (cid:1)1/4. This is The update rule in (3.27) can be simplified to xt+1 = xt ηt equivalent to performing preconditioning on the eigenspace of Gt: (cid:1)1/4Gt (cid:0)GtG (cid:0)G Gt Ut, Σt, Vt = SVD(Gt), xt+1 = xt ηtUtV . (3.28) Therefore, we borrow the eigenspace preconditioning from Shampoo, and design our algorithm to precondition on any matrix-shaped update as in (3.28). In particular, we present our algorithm in Algorithm 4. Algorithm 4 MARS-Shampoo 1: input: x0, λ, β1, {γt}, {ηt} 2: Set m0 0 and x1 x0 3: for = 1, to do 4: 5: mt = β1mt1 + (1 β1)ct 6: Ut, Σt, Vt = SVD(mt) xt+1 = xt ηt(UtV 7: 8: end for + λxt) sample ξt and let ct = (xt, ξt) + γt( β1 1β )(cid:0)f (xt, ξt) (xt1, ξt)(cid:1) To reduce the time complexity of SVD decomposition, Bernstein and Newhouse (2024) summarized four different approaches for computing (3.28) including SVD, sketching (Martinsson and Tropp, 2020), Newton iteration (Lakić, 1998; Higham, 2008; Anil et al., 2020), and Newton-Schulz iteration (Schulz, 1933; Higham, 2008). Our algorithm design accommodates any of these SVD solvers to best fit specific computational needs. Connection with Muon. Muon (Jordan et al., 2024) is recently proposed algorithm that utilizes the Newton-Schulz iteration (Higham, 2008; Schulz, 1933) to solve the SVD problem. It has demonstrated superior performance in terms of convergence speed when compared with AdamW and Shampoo in training large language models. The update rules of Muon are demonstrated as follows: ut = µut1 + (xt, ξt), mt = µut + (xt, ξt), Ot = NewtonSchulz (mt) , xt+1 = xt ηt(Ot + λxt). (3.29) (3.30) Applying Lemma 3.4 to (3.29) and (3.30), with gt = (xt, ξt), a1 = µ, a2 = 1, b1 = µ, b2 = 1, we obtain an equivalent single update of momentum: mt = µmt1 + (xt, ξt) + µ(cid:0)f (xt, ξt) (xt1, ξt1)(cid:1). (3.31) On the other hand, taking β1 = µ, γt = 1 µ = 1 β1 in MARS, (3.13) and (3.14) reduces to mt = µmt1 + (1 µ)f (xt, ξt) + µ(1 µ)(cid:0)f (xt, ξt) (xt1, ξt)(cid:1). 11 By dividing both sides of the above equation by 1 µ, we obtain mt 1 µ = µ mt1 1 µ + (xt, ξt) + µ(cid:0)f (xt, ξt) (xt1, ξt)(cid:1). (3.32) In can be seen that (3.32) is rescaled version of (3.31), except that the stochastic gradients (xt, ξt) and (xt1, ξt) are taken both at ξt."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "All our experiments are done based on the nanoGPT (Karpathy, 2022) implementation of the GPT-2 (Radford et al., 2019) architecture, and on the OpenWebText (Gokaslan et al., 2019) dataset. The training and validation sets contain approximately 9 billion and 4.4 million tokens, respectively, all preprocessed using the GPT-2 tokenizer. We conduct experiments on three scales of GPT-2 models: small (125M parameters), medium (355M parameters), and large (770M parameters). Per the nanoGPT configurations, we disabled biases, applied GeLU activations, and set the Dropout rate (Srivastava et al., 2014) to 0.0. We utilized 16 NVIDIA A100 GPUs for training the small models. For the medium and large models, training was conducted on 32 NVIDIA A100 GPUs and 32 NVIDIA H100 GPUs, respectively. (a) Training Loss (b) Validation Loss (c) Wall-clock time Figure 1: The training and validation loss curves, plotted against both training tokens and wall-clock time on GPT-2 small model (125M)."
        },
        {
            "title": "4.2 Results",
            "content": "In Figures 1, 2, and 3, we demonstrate the training and validation losses as function of training tokens and wall-clock time for various model sizes1. Across the small, medium, and large GPT-2 models, MARS consistently surpasses both the AdamW and Muon baselines in training and validation losses. The performance gap becomes more pronounced with increasing model size. Notably, MARS 1The training loss curves are smoothed using an Exponential Moving Average. 12 (a) Training Loss (b) Validation Loss (c) Wall-clock time Figure 2: The training and validation loss curves, plotted against both training tokens and wall-clock time on GPT-2 medium model (355M). (a) Training Loss (b) Validation Loss (c) Wall-clock time Figure 3: The training and validation loss curves, plotted against both training tokens and wall-clock time on GPT-2 large model (770M). exhibits both rapid initial decay and sustained superiority throughout the training process. Further, we explore the performance of additional learning rate choices in Appendix B. Notably, the best validation loss of MARS achieved in our GPT-2 small experiments is 2.852. For comparison, SophiaH (Liu et al., 2023) achieves validation loss of 2.901, and Sophia-G (Liu et al., 2023) achieves 2.875 under the same settings and datasets as ours. These results demonstrate that our reported performance is highly competitive with state-of-the-art optimizers. In Figure 1(c), 2(c), and 3(c), we compare the wall-clock time of different algorithms. We observe that our algorithm, MARS, has slightly higher per-iteration cost compared to AdamW but is much faster than Muon. Additionally, MARS consistently demonstrates lower validation losses than both AdamW and Muon within equivalent training durations. We also test our optimization algorithm on the downstream task of commonsense natural language inference using the Hellaswag dataset (Zellers et al., 2019), evaluated with 0-shot. The results, shown in Figure 4, are obtained from GPT-2 medium and large models pre-trained for 100,000 steps (50B tokens). The models pre-trained with MARS outperformed those pre-trained with AdamW and Muon optimizers, validating an enhanced downstream performance within the same number of 13 Figure 4: The accuracy for 0-shot evalution on Hellaswag (Zellers et al., 2019) dataset for AdamW, Muon and MARS on GPT-2 medium (355M) and large (770M) models. pre-training steps."
        },
        {
            "title": "4.3 MARS and MARS-approx.",
            "content": "We then conduct experiments to compare the performance of MARS and MARS-approx (MARSAdamW version) on GPT-2 small and medium models, the results are shown in Figure 5. Models trained with MARS exhibit slightly better performance than those trained with MARS-approx, though the performance gap in terms of training tokens is marginal. This suggests that the approximate version is practical alternative in scenarios where computational cost is priority, as it introduces only minimal performance loss. However, in settings where achieving higher validation accuracy is critical, the exact version is recommended. Figure 5: Validation loss curves for MARS and MARS-approx on GPT-2 small (125M, left) and medium (355M, right)."
        },
        {
            "title": "5 Related Work\nIn this section, we provide a review of additional related works, including some previously mentioned,\nto help readers gain a deeper understanding of the history and development of adaptive gradient\nmethods and variance reduction techniques.",
            "content": "Adaptive Gradient Methods. RProp (Riedmiller and Braun, 1993) is probably one of the earliest adaptive gradient methods by dynamically adjusting the learning rate. AdaGrad (Duchi et al., 2011; McMahan and Streeter, 2010) adjusts the learning rate based on the geometry of the training data observed during earlier iterations. To tackle with the issue of diminishing gradient in AdaGrad, Tieleman (2012) introduced RMSProp by incorporating the idea of exponential moving average. significant advancement came with Adam (Kingma, 2014), which integrated RMSProp with Nesterovs momentum (Nesterov, 1983, 2013) achieving superior performance and becoming prevalent optimizer in deep neural network training. Later, Loshchilov (2017) proposed to decouple weight decay from gradient calculations in Adam and introduced AdamW, an optimization algorithm having become the predominant optimization algorithm in contemporary deep learning applications. To fix the convergence issue of Adam, Reddi et al. (2019) introduced the AMSGrad optimizer, which maintains running maximum of past second-order momentum terms to achieve non-increasing step sizes. Subsequently, Chen et al. (2018) unified AMSGrad and SGD within the Padam framework by introducing partial adaptive parameter to control the degree of adaptiveness. Notably, AdamW and its variations have been widely used in the training of popular large language models, including OPT (Zhang et al., 2022), Llama 3 (Dubey et al., 2024), and DeepSeek-V2 (Liu et al., 2024). Variance Reduction Methods. SAG(Roux et al., 2012) and SDCA(Shalev-Shwartz and Zhang, 2013) were among the first attempts to apply variance reduction techniques to accelerate the convergence of SGD. Subsequently, simpler algorithms like SVRG(Johnson and Zhang, 2013) and SAGA(Defazio et al., 2014) were introduced, achieving the same improved convergence rates. SARAH (Nguyen et al., 2017a) further simplified these approaches by employing biased recursive gradient estimation, which reduces storage requirements while achieving the complexity bounds for convex optimization problems. For non-convex optimization, besides SVRG (Allen-Zhu and Yuan, 2016; Reddi et al., 2016) and SARAH (Nguyen et al., 2017b), SPIDER (Fang et al., 2018) integrates Normalized Gradient Descent (Nesterov, 2013; Hazan et al., 2015) with recursive estimation of gradients, while SNVRG (Zhou et al., 2020) introduces multiple reference points for semi-stochastic gradient calculation for improved variance reduction and convergence rate. SpiderBoost (Wang et al., 2019) refines SPIDER by enabling the use of significantly larger constant step size while preserving the same near-optimal oracle complexity. Subsequently, STORM (Cutkosky and Orabona, 2019) was proposed to further simplifies the SPIDER and SNVRG algorithms through the use of stochastic recursive momentum. This was later improved into parameter-free variant, namely STORM+(Levy et al., 2021). Variance Reduction for Adaptive Gradient Methods. Few works have explored the application of variance reduction techniques to adaptive gradient methods. To the best of our knowledge, the only exceptions are Adam+ and SuperAdam. Adam+ (Liu et al., 2020) attempts to reduce the variance of first-order moment in Adam by estimating the gradient only at extrapolated points. SuperAdam (Huang et al., 2021), an adaptive gradient algorithm that integrates variance reduction with AdamW to achieve improved convergence rates. However, these variance-reduced adaptive gradient methods have primarily been validated on basic computer vision tasks, such as MNIST (Schölkopf 15 and Smola, 2002) and CIFAR-10 (Krizhevsky et al., 2009), and simple natural language modeling tasks, like SWB-300 (Saon et al., 2017), using straightforward architectures such as LeNet (LeCun et al., 1998), ResNet-32 (He et al., 2016), 2-layer LSTMs (Graves and Graves, 2012), and 2-layer Transformers (Vaswani, 2017). As result, significant gap remains in the successful application of variance reduction techniques to adaptive gradient methods, particularly in the rapidly evolving domain of large language models."
        },
        {
            "title": "6 Conclusion\nIn this work, we introduce MARS, a unified framework for adaptive gradient methods that integrates\nvariance reduction techniques to improve the training of large models. Our approach combines\nthe adaptive learning rate introduced by preconditioning with the faster convergence enabled by\nvariance reduction. Within our framework, we have developed three optimization algorithms based\non the ideas of AdamW, Lion, and Shampoo. Through extensive empirical experiments on GPT-2\npre-training tasks, we demonstrate that MARS consistently outperforms baseline algorithms in terms\nof both token efficiency and wall-clock time. Our results establish a generic framework for combining\nadaptive gradient methods with variance reduction techniques, contributing to the advancement of\noptimizers in large model training.",
            "content": "Acknowledgments We thank Yuan Cao, Yiming Dong, and Zhuoqing Song for the valuable discussions throughout various stages of this work."
        },
        {
            "title": "References",
            "content": "Allen-Zhu, Z. and Yuan, Y. (2016). Improved svrg for non-strongly-convex or sum-of-non-convex objectives. In International conference on machine learning. PMLR. Anil, R., Gupta, V., Koren, T., Regan, K. and Singer, Y. (2020). Scalable second order optimization for deep learning. arXiv preprint arXiv:2002.09018 . Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro, N. and Woodworth, B. (2023). Lower bounds for non-convex stochastic optimization. Mathematical Programming 199 165214. Bernstein, J. and Newhouse, L. (2024). Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325 . Brown, T. B. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165 . Chen, J., Zhou, D., Tang, Y., Yang, Z., Cao, Y. and Gu, Q. (2018). Closing the generalization gap of adaptive gradient methods in training deep neural networks. arXiv preprint arXiv:1806.06763 . Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y. et al. (2023). Symbolic discovery of optimization algorithms. Advances in neural information processing systems 36. 16 Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S. et al. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24 1113. Cutkosky, A. and Orabona, F. (2019). Momentum-based variance reduction in non-convex sgd. Advances in neural information processing systems 32. Defazio, A., Bach, F. and Lacoste-Julien, S. (2014). Saga: fast incremental gradient method with support for non-strongly convex composite objectives. Advances in neural information processing systems 27. Defazio, A. and Bottou, L. (2019). On the ineffectiveness of variance reduced optimization for deep learning. Advances in Neural Information Processing Systems 32. Devlin, J. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A. et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783 . Duchi, J., Hazan, E. and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research 12. Fang, C., Li, C. J., Lin, Z. and Zhang, T. (2018). Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. Advances in neural information processing systems 31. Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM journal on optimization 23 23412368. Gokaslan, A., Cohen, V., Pavlick, E. and Tellex, S. (2019). Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus. Graves, A. and Graves, A. (2012). Long short-term memory. Supervised sequence labelling with recurrent neural networks 3745. Gupta, V., Koren, T. and Singer, Y. (2018). Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning. PMLR. Hazan, E., Levy, K. and Shalev-Shwartz, S. (2015). Beyond convexity: Stochastic quasi-convex optimization. Advances in neural information processing systems 28. He, K., Zhang, X., Ren, S. and Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. Higham, N. J. (2008). Functions of Matrices. Society for Industrial and Applied Mathematics. Huang, F., Li, J. and Huang, H. (2021). Super-adam: faster and universal framework of adaptive gradients. Advances in Neural Information Processing Systems 34 90749085. 17 Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. Advances in neural information processing systems 26. Jordan, K. et al. (2024). KellerJordan/modded-nanogpt: NanoGPT (124M) quality in 8. minutes. https://github.com/KellerJordan/modded-nanogpt. Kaddour, J., Key, O., Nawrot, P., Minervini, P. and Kusner, M. J. (2024). No train no gain: Revisiting efficient training algorithms for transformer-based language models. Advances in Neural Information Processing Systems 36. Karpathy, A. (2022). NanoGPT. https://github.com/karpathy/nanoGPT. Kingma, D. P. (2014). Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980 . Krizhevsky, A., Hinton, G. et al. (2009). Learning multiple layers of features from tiny images . Lakić, S. (1998). On the computation of the matrix k-th root. ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik: Applied Mathematics and Mechanics 78 167172. LeCun, Y., Bottou, L., Bengio, Y. and Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE 86 22782324. Levy, K., Kavis, A. and Cevher, V. (2021). Storm+: Fully adaptive sgd with recursive momentum for nonconvex optimization. Advances in Neural Information Processing Systems 34 2057120582. Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D. et al. (2024). Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 . Liu, H., Li, Z., Hall, D., Liang, P. and Ma, T. (2023). Sophia: scalable stochastic second-order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342 . Liu, M., Zhang, W., Orabona, F. and Yang, T. (2020). Adam +: stochastic method with adaptive variance reduction. arXiv preprint arXiv:2011.11985 . Loshchilov, I. (2017). Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 . Martens, J. and Grosse, R. (2015). Optimizing neural networks with kronecker-factored approximate curvature. In International conference on machine learning. PMLR. Martinsson, P.-G. and Tropp, J. A. (2020). Randomized numerical linear algebra: Foundations and algorithms. Acta Numerica 29 403572. McCandlish, S., Kaplan, J., Amodei, D. and Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162 . McMahan, H. B. and Streeter, M. (2010). Adaptive bound optimization for online convex optimization. arXiv preprint arXiv:1002.4908 . 18 Morwani, D., Shapira, I., Vyas, N., Malach, E., Kakade, S. and Janson, L. (2024). new perspective on shampoos preconditioner. arXiv preprint arXiv:2406.17748 . Nesterov, Y. (1983). method for solving the convex programming problem with convergence rate o(1/k2). Proceedings of the USSR Academy of Sciences 269 543547. Nesterov, Y. (2013). Introductory lectures on convex optimization: basic course, vol. 87. Springer Science & Business Media. Nguyen, L. M., Liu, J., Scheinberg, K. and Takáč, M. (2017a). Sarah: novel method for machine learning problems using stochastic recursive gradient. In International conference on machine learning. PMLR. Nguyen, L. M., Liu, J., Scheinberg, K. and Takáč, M. (2017b). Stochastic recursive gradient algorithm for nonconvex optimization. arXiv preprint arXiv:1705.07261 . Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I. et al. (2019). Language models are unsupervised multitask learners. OpenAI blog 1 9. Reddi, S. J., Hefny, A., Sra, S., Poczos, B. and Smola, A. (2016). Stochastic variance reduction for nonconvex optimization. In International conference on machine learning. PMLR. Reddi, S. J., Kale, S. and Kumar, S. (2019). On the convergence of adam and beyond. arXiv preprint arXiv:1904.09237 . Riedmiller, M. and Braun, H. (1993). direct adaptive method for faster backpropagation learning: The rprop algorithm. In IEEE international conference on neural networks. IEEE. Roux, N., Schmidt, M. and Bach, F. (2012). stochastic gradient method with an exponential convergence _rate for finite training sets. Advances in neural information processing systems 25. Saon, G., Kurata, G., Sercu, T., Audhkhasi, K., Thomas, S., Dimitriadis, D., Cui, X., Ramabhadran, B., Picheny, M., Lim, L.-L. et al. (2017). English conversational telephone speech recognition by humans and machines. arXiv preprint arXiv:1703.02136 . Schölkopf, B. and Smola, A. J. (2002). Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press. Schulz, G. (1933). Iterative berechnung der reziproken matrix. Z. Angew. Math. Mech. 13 5759. Shalev-Shwartz, S. and Zhang, T. (2013). Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research 14. Shazeer, N. and Stern, M. (2018). Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning. PMLR. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I. and Salakhutdinov, R. (2014). Dropout: simple way to prevent neural networks from overfitting. The journal of machine learning research 15 19291958. Tieleman, T. (2012). Lecture 6.5-rmsprop: Divide the gradient by running average of its recent magnitude. COURSERA: Neural networks for machine learning 4 26. Vaswani, A. (2017). Attention is all you need. Advances in Neural Information Processing Systems . Vyas, N., Morwani, D., Zhao, R., Shapira, I., Brandfonbrener, D., Janson, L. and Kakade, S. (2024). Soap: Improving and stabilizing shampoo using adam. arXiv preprint arXiv:2409.11321 . Wang, Z., Ji, K., Zhou, Y., Liang, Y. and Tarokh, V. (2019). Spiderboost and momentum: Faster variance reduction algorithms. Advances in Neural Information Processing Systems 32. Ward, R., Wu, X. and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex landscapes. Journal of Machine Learning Research 21 130. Xie, X., Zhou, P., Li, H., Lin, Z. and Yan, S. (2024). Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models. IEEE Transactions on Pattern Analysis and Machine Intelligence . You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K. and Hsieh, C.-J. (2019). Large batch optimization for deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962 . Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 . Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. and Choi, Y. (2019). Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830 . Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V. et al. (2022). Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 . Zhao, R., Morwani, D., Brandfonbrener, D., Vyas, N. and Kakade, S. (2024). Deconstructing what makes good optimizer for language models. arXiv preprint arXiv:2407.07972 . Zhou, D., Xu, P. and Gu, Q. (2020). Stochastic nested variance reduction for nonconvex optimization. Journal of machine learning research 21 163. Zhuang, J., Tang, T., Ding, Y., Tatikonda, S. C., Dvornek, N., Papademetris, X. and Duncan, J. (2020). Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Advances in neural information processing systems 33 1879518806. Proofs of Lemma 3. Proof of Lemma 3.4. Substituting (3.23) into (3.24), we obtain mt = b1 (cid:0)a1ut1 + a2gt (cid:1) + b2gt = a1b1ut1 + (b1a2 + b2)gt. On the other hand, shifting the index of (3.24) by 1, we have mt1 = b1ut1 + b2gt1. 20 (A.1) (A.2) Combining (A.1) and (A.2), we obtain the iterative update of mt from its previous value mt1 as: mt = a1mt1 a1b2gt1 + (b1a2 + b2)gt = a1mt1 + (b1a2 a1b2 + b2)gt + a1b2(gt gt1). This completes the proof."
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Learning Rates In our experiments, we explore various learning rate choices on GPT-2 small. AdamW with learning rate of 6 104 demonstrates superior performance in terms of validation loss during the initial 37 billion tokens of training. However, higher learning rate of 3 103 yields significantly better results in the final 13 billion tokens. We then further extend our experiments by comparing AdamW and MARS-AdamW under identical hyperparameters, both using the higher learning rate of 3 103. The results, presented in Figure 6, show that even against an alternative baseline with hyperparameters optimized for AdamW, MARS-AdamW consistently outperforms AdamW. We observe that there is still potential for further parameter tuning and performance improvements with MARS. Figure 6: Validation loss with respect to training tokens for AdamW with learning rates 6 104, 3 103 and MARS with learning rate 3 103 on GPT-2 small model (125M). The model trained with MARS achieves validation loss of 2.852. B.2 Comparison between MARS and Lion The key difference between MARS-AdamW and MARS-Lion lies in their precondition matrix. We evaluate MARS-AdamW against MARS-Lion under identical hyperparameters on GPT-2 small. As shown in Figure 7, MARS-AdamW outperforms MARS-Lion in both training and validation loss. Therefore, we mainly use MARS-AdamW in the rest of our experiments. (a) Training Loss (b) Validation Loss Figure 7: The training and validation loss curves, plotted against training tokens for Lion and MARS on GPT-2 small model (125M). B.3 Sensitivity to γ. To explore the impact of γt, we tested various γt schedules, including constant and linearly changing schedules. Other hyper-parameters are set the same as the main experiments, and the results are summarized in Table 1. Specifically, the constant γ values of 0.025 and 0.05 exhibit the best performance among the γt-schedules evaluated. We are using γt schedule of 0.025 across all our other experiments. Table 1: The validation loss for GPT-2 small (125M) model with different γ-schedules. γ-schedule γ Validation loss Constant Changing 0.025 0.05 0.0250.05 0.10 00.1 00. 2.871 2.871 2.872 2.873 2.891 2.899 Hyper-parameter Choices Table 2 summarizes the architectural hyperparameters for GPT-2 models with 125M (small), 355M (medium), and 770M (large) parameters. Table 3 lists the general hyperparameters used across all experiments, while Tables 4, 5 and 6 present the training hyperparameters for the small, medium, and large models, respectively. 22 Table 2: Architecture hyperparameters for GPT-2 series models (Radford et al., 2019). Model #Param #Layer nhead demb GPT-2 small GPT-2 medium GPT-2 large 125M 355M 770M 12 24 36 12 16 768 1024 1280 Table 3: General hyper-parameters for the experiments. Hyper-parameter Steps Batch size in total Context length Gradient clipping threshold Dropout Learning rate schedule Warm-up steps Base seed Value 100,000 480 1024 1.0 0.0 Cosine 2000 Table 4: Hyper-parameters for GPT-2 small experiment. We use γt 0.025 for MARS. Hyper-parameter AdamW Muon MARS Max learning rate Min learning rate (β1, β2) 6e-4 3e-5 2e-2 3e- (0.9, 0.95) µ = 0.95 6e-3 3e-5 (0.95, 0.99) Table 5: Hyper-parameters for GPT-2 medium experiment. We use γt 0.025 for MARS. Hyper-parameter AdamW Muon MARS Max learning rate Min learning rate (β1, β2) 3e-4 6e-5 1e-2 6e-5 (0.9, 0.95) µ = 0.95 3e-3 6e-5 (0.95, 0.99) Table 6: Hyper-parameters for GPT-2 large experiment. We use γt 0.025 for MARS. Hyper-parameter AdamW Muon MARS Max learning rate Min learning rate (β1, β2) 2e-4 1e-5 6.67e-3 1e-5 (0.9, 0.95) µ = 0.95 2e-3 1e-5 (0.95, 0.99)"
        }
    ],
    "affiliations": []
}