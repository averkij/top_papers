{
    "paper_title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models",
    "authors": [
        "Yu Zeng",
        "Wenxuan Huang",
        "Shiting Huang",
        "Xikun Bao",
        "Yukun Qi",
        "Yiming Zhao",
        "Qiuchen Wang",
        "Lin Chen",
        "Zehui Chen",
        "Huaian Chen",
        "Wanli Ouyang",
        "Feng Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 0 3 1 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "AGENTIC JIGSAW INTERACTION LEARNING FOR ENHANCING VISUAL PERCEPTION AND REASONING IN VISION-LANGUAGE MODELS Yu Zeng1, Wenxuan Huang3,4, Shiting Huang1, Xikun Bao1, Yukun Qi1, Yiming Zhao1, Qiuchen Wang1, Lin Chen1, Zehui Chen1, Huaian Chen1, Wanli Ouyang2,4, Feng Zhao1 1University of Science and Technology of China 2Shanghai AI Laboratory 3East China Normal University 4The Chinese University of Hong Kong"
        },
        {
            "title": "ABSTRACT",
            "content": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Vision-Language Models (VLMs) have recently achieved remarkable success across wide range of multimodal tasks, including image captioning (Chen et al., 2024a;c; Li et al., 2024b), visual question answering (Bai et al., 2025; Zhu et al., 2025; Hurst et al., 2024; Comanici et al., 2025), and scene understanding (Wang et al., 2025b;a). By effectively associating visual and textual information, these models exhibit strong multimodal perception and reasoning capabilities. However, their performance remains severely limited on tasks that require comprehensive visual understanding and structured reasoning. In particular, we find that current VLMs often perform near random even on relatively simple 2 2 jigsaw tasks (Carlucci et al., 2019; Chen et al., 2023), which demand both perceptual accuracy and logical inference. This suggests that existing pretraining and fine-tuning strategies are insufficient for developing robust perceptual and reasoning abilities. Equal Contribution Corresponding author"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Description of the action space. (a) illustrates swapping two jigsaw pieces and observing the updated jigsaw state; (b) shows cropping specific region of the jigsaw for closer inspection; and (c) depicts zooming into selected area to examine fine-grained details. Recent studies have explored reinforcement learning (RL) (Schulman et al., 2017; Rafailov et al., 2023; Shao et al., 2024) as way to enhance these capabilities by enabling models to learn through interaction, trial-and-error, and feedback. While RL-based approaches show promise, they are fundamentally constrained by the scarcity and limited scalability of high-quality vision-language RL data. Current methods for constructing multimodal RL datasets (Huang et al., 2025; Yang et al., 2025; Lu et al., 2023) generally fall into two categories: human expert supervision and automated synthesis. Human-supervised datasets are either prohibitively expensive or too small in scale for large-scale training, while automated approaches that rely on closed-source models suffer from limited quality, capability constraints, and substantial API costs. These challenges collectively hinder the development of VLMs with strong reasoning and generalization abilities. To address these limitations, we propose AGILE, an Agentic jiGsaw Interaction Learning framework for Enhancing visual perception and reasoning in VLMs. Our approach leverages the structured nature of the jigsaw puzzle as proxy task for perception and reasoning, modeling the jigsawsolving process as progressive interaction between the model and its environment. At each step, the model generates Python code to perform actions within well-defined action space (swapping two image tiles, observing the current jigsaw state, cropping regions for detailed observation, or zooming in for fine-grained analysis). This interaction-driven process enables the model to iteratively refine its perceptual and reasoning capabilities while receiving explicit feedback at every step. By simulating the step-by-step dynamics of jigsaw solving, the model is encouraged to capture structural relationships among visual components and acquire more robust perception and reasoning skills. key feature of our approach is the use of code and rule-based data generation, which offers two major advantages. First, the difficulty of the jigsaw task can be precisely controlled by adjusting factors such as the number of correctly placed tiles in the initial state and the overall jigsaw size. Second, because the ground-truth solution is inherently available, the synthetic dataset can be scaled to arbitrary sizes with strict supervision, overcoming the data scarcity inherent in human-annotated or closed-source datasets. The combination of interactive training and scalable data generation yields an efficient and effective framework for advancing visual perception and reasoning in VLMs. We validate the effectiveness of our approach through comprehensive experiments. Our method substantially improves performance across jigsaw tasks of varying complexity, for instance, raising accuracy from 9.5% to 82.8% on the 2 2 setting, while also demonstrating strong generalization to wide range of vision tasks, including high-resolution image understanding, real-world scene analysis, fine-grained recognition, visual reasoning, and hallucination benchmarks. Moreover, we show that scaling the training data further boosts performance, and that under equal data budgets, jigsaw-based training achieves results comparable to or even surpassing those obtained with general QA data. These findings underscore the potential of jigsaw tasks in addressing the scarcity of multimodal RL data. By combining interactive training with scalable data generation, our method significantly enhances both perceptual and higher-order reasoning abilities, pointing to promising new direction for advancing VLMs. Our main contributions are as follows: We introduce AGILE, an agentic jigsaw interaction learning framework that formulates jigsaw solving as stepwise interactive process, thereby driving incremental improvements in both visual perception and reasoning capabilities of VLMs."
        },
        {
            "title": "Preprint",
            "content": "We present scalable jigsaw-based data generation method that yields high-quality multimodal reinforcement learning datasets with controllable difficulty, providing an efficient solution to the current shortage of high-quality training data. Extensive experiments demonstrate that our approach substantially improves performance on jigsaw tasks of varying complexity while exhibiting strong generalization across diverse vision benchmarks, highlighting its effectiveness in enhancing both perception and reasoning in VLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reinforcement Learning for Vision Language Models. Reinforcement learning (RL) (Schulman et al., 2017; Rafailov et al., 2023; Shao et al., 2024) has been widely applied to improve reasoning in language models, as exemplified by the success of DeepSeek-R1 (Guo et al., 2025) in mathematical reasoning. Building on this progress, recent studies have extended RL to VLMs, with rule-based RL in multimodal domains emerging as promising direction. For perception enhancement, R1-V (Chen et al., 2025b) applies RL to object counting, while Perception-R1 (Yu et al., 2025) leverages object matching and IoU as reward signals to improve grounding. DeepEyes (Zheng et al., 2025) shows how RL can encourage models to invoke visual tools, thereby expanding perceptual abilities. For reasoning, MMEureka (Meng et al., 2025) demonstrates the effectiveness of rule-based RL in mathematical tasks. From data perspective, Vision-R1 (Huang et al., 2025) and R1-OneVision (Yang et al., 2025) convert visual information into textual representations to build multimodal chainof-thought (CoT) datasets that support stronger reasoning. Despite these advances, the lack of scalable, high-quality RL datasets remains fundamental bottleneck, severely limiting further improvements in both perception and reasoning for VLMs. Enhancing Generalization via Proxy Tasks. Rule-based reinforcement learning (RL) has shown promise but typically requires large amounts of high-quality, verifiable data. Logic-RL (Xie et al., 2025a) addressed this by introducing the Knights and Knaves (K&K) puzzle, which offers controllable difficulty and algorithmically generated ground truth, enabling models to acquire reasoning skills transferable to mathematical tasks. Enigmata (Chen et al., 2025a) extended this approach to broader set of text-based puzzles (e.g., cryptographic, arithmetic, logical), demonstrating that solving such tasks with RL improves general reasoning without external data. RPT (Dong et al., 2025) further reframed next-token prediction as reasoning task, using verifiable rewards to enhance predictive ability and strengthen the basis for reinforcement fine-tuning. Inspired by these proxy-task successes in LLMs, recent work has begun exploring similar ideas for VLMs. Code2Logic (Tong et al., 2025) and ViGaL (Xie et al., 2025b) leverage code-synthesized games to improve mathematical reasoning, while ViCrit (Wang et al., 2025d) enhances perceptual robustness by reinforcing models to detect hallucinated entities via modified captions. Jigsaw-R1 (Wang et al., 2025e) proposed puzzle-based RL paradigm, but due to training limitations, its models still performed poorly even on 2 2 jigsaw puzzles, failing to fully exploit the proxy-task benefits. To address this gap, we propose agentic jigsaw interaction learning framework, which casts jigsaw solving as an interactive process between the model and environment. This enables iterative refinement of perception and reasoning with explicit stepwise feedback, allowing the model to better capture visual relationships and develop stronger perceptual and reasoning skills."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we provide comprehensive description of our proposed agentic jigsaw interaction learning framework. We first introduce the jigsaw task and describe how the model interacts with the environment to accomplish it (Sec. 3.1), then present the construction of the training data (Sec. 3.2). Finally, we outline the training paradigm, including both the cold-start stage and the reinforcement learning stage (Sec. 3.3). 3.1 JIGSAW TASK AND ENVIRONMENT INTERACTION Jigsaw Task. Given an input image, we partition it into an grid of jigsaw pieces, where task difficulty can be flexibly adjusted by varying m. If the image height or width is not divisible by m,"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the AGILE framework. (a) depicts the interaction process between the model and the external environment, together with the implementation of the GRPO algorithm; (b) shows the collection of high-quality jigsaw trajectory data; and (c) illustrates the modelenvironment interaction during the jigsaw rollout process. the image is resized so that its dimensions are exact multiples of the grid. The grid is then randomly shuffled, and each piece is assigned an index in row-major order ranging from 1 (top-left) to m2 (bottom-right). The shuffled configuration is denoted as IShuf le = {I1, I2, . . . , Im2}, (1) where each Ik corresponds to one jigsaw block. Since the shuffling process is explicitly recorded during data generation, we can recover the ground-truth jigsaw layout as IGT = {Iπ(1), Iπ(2), . . . , Iπ(m2)}, (2) where π is permutation over {1, 2, . . . , m2}. During jigsaw solving, the model will maintain current state IState = {Iπ(1), Iπ(2), . . . , Iπ(m2)}, (3) where π denotes the current arrangement of pieces. At each step, the model iteratively swaps two pieces, observes the resulting configuration, and aims to reconstruct the ground-truth layout IGT . Environment Interaction. The model interacts with the environment throughout the jigsawsolving process in an iterative manner. As illustrated in Figure 1, , we predefine set of API for modelenvironment interactions in Python. The model expresses its actions by generating Python code, which is executed by the environment to produce the corresponding resulting image. The model then uses these results to reason further and decide on subsequent actions, repeating this process iteratively until the jigsaw is completed. Specifically, at each step, the model can perform the following actions: Swap: Given the current jigsaw state IState, the model calls the Swap function to exchange the positions of any two jigsaw pieces. Observe: Given IState, the model calls the Observe function to obtain the current jigsaw progress IObs, which is then used to determine the next action. Crop & Zoom: Given IObs, the model crops and zooms into local region to inspect finer details and inform subsequent actions. To better illustrate the overall procedure, the entire rollout process is presented in Algorithm 1."
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Interactive Jigsaw-Solving Procedure of VLMs with the Environment Input: Query x, IShuf le, Policy model πθ, External environment V, Maximum iteration steps . Output: Final interaction trajectory y. 1: Initialize trajectory , step counter 0. 2: while < do 3: 4: 5: 6: Sample response yt πθ( x, IShuf le, y). Append assistant response yt to trajectory: + yt. if <code> </code> detected in yt then Parse the Python code (e.g., Swap, Observe, Crop, Zoom), execute it in the environment V, and obtain feedback Ot. else if <answer> </answer> detected in yt then return final trajectory y. 7: 8: 9: 10: 11: 12: end while 13: return final trajectory y. end if Append environment feedback Ot as user input: + Ot. Update step counter + 1. 3.2 DATA CONSTRUCTION In Section 3.1, we have introduced the jigsaw task and described how the model interacts with the environment to complete it. Based on this task design, we observe that the model (e.g., Qwen2.5VL-7B) exhibits limited foundational capabilities, manifested in poor instruction-following and incorrect Python code generation, which hinders proper interaction with the environment and introduces substantial training noise. Directly applying reinforcement learning (RL) thus results in low learning efficiency. To bridge this gap and more effectively train our model, we employ Gemini 2.5 Pro1 to collect expert trajectories for the cold-start phase, endowing Qwen-2.5VL-7B with basic interactive jigsaw-solving capabilities. For this purpose, we design structured prompts to guide Gemini 2.5 Pro in interacting with the environment to complete the jigsaw tasks (The prompt templates are provided in Appendix B. ). To further ensure data quality, we apply an additional filtering process: first, selecting samples where Geminis outputs match the ground truth, and then manually verifying each interaction step in the correct samples to ensure rationality and consistency. To ensure that our model can perform diverse jigsaw reasoning actions during reinforcement learning, we carefully balance the training data. Specifically, trajectories are balanced with respect to both the number of steps (48) and the types of actions involved (e.g., Swap, Observe, Crop, Zoom). This design ensures that the model is sufficiently exposed to the full action space and learns to handle various interactions with the environment effectively. Overall, we curate dataset comprising 1.6K high-quality reasoning trajectories for the jigsaw task."
        },
        {
            "title": "3.3 TRAINING PARADIGM: COLD-START AND REINFORCEMENT LEARNING",
            "content": "Cold-start. During the cold-start phase, we leverage the 1.6K high-quality jigsaw-solving trajectories constructed in Section 3.2 to equip the model with basic instruction-following and Python code generation capabilities, ensuring effective interaction with the environment. Reinforcement Learning. Our framework adopts Group Relative Policy Optimization (GRPO), which utilizes the average reward of multiple sampled outputs as baseline, instead of relying on learned value function. The policy model is optimized by maximizing the following objective: JGRPO(θ) = xD, {yi}G i=1πold(x;V) (cid:34) 1 (cid:88) i=1 1 t=1 I(yi,t) (cid:80)yi yi (cid:88) min (cid:18) πθ(yi,t x, yi,<t; V) πold(yi,t x, yi,<t; V) ˆAi,t, t=1:I(yi,t)=1 (cid:19) (cid:35) ˆAi,t β DKL(πθ πref ) . (4) clip (cid:18) πθ(yi,t x, yi,<t; V) πold(yi,t x, yi,<t; V) (cid:19) , 1 ϵ, 1 + ϵ 1The version is Gemini-2.5-Pro-Preview-05-06."
        },
        {
            "title": "Preprint",
            "content": "where the rollout module samples group of trajectories {y1, y2, . . . , yG} from the old policy πold for each input question through interaction with the external environment V. The advantage term ˆAi,t is computed based on the relative rewards of outputs within each group. Our reward system comprises three components: an accuracy reward, format reward, and step reward. The total reward is computed as the sum of these components. Accuracy Reward: We compare the models generated answer IAnswer with the ground truth IGT . If all jigsaw image blocks are correctly placed, the accuracy reward is 1; otherwise, it is 0. Format Reward: The model receives reward of 1 if its output follows the required structured format, with the reasoning process, code, and final answer correctly enclosed within the <think>, <code>, and <answer> tags, respectively. Step Reward: We encourage the model to complete the jigsaw in as few steps as possible. For 2 2 jigsaw, if each step swaps any two image blocks, at most three steps are theoretically required to place all blocks correctly. Excessive actions may lead the model to perform invalid steps, reducing the proportion of effective perception and reasoning. Furthermore, to prevent the model from prematurely hacking the step reward during early RL training, the step reward is applied only when the jigsaw is correctly completed. If the jigsaw is incorrect, the model is penalized by assigning the maximum step penalty. Formally, the step reward is defined as: Rstep = λ (cid:16) I{Racc=1} stepnum + I{Racc=0} stepmax (cid:17) , (5) where stepnum is the number of steps the model actually used to complete the jigsaw, stepmax is the maximum allowed steps, λ denotes the step penalty coefficient, which is set to 0.05, and I{} is the indicator function. The final reward formulation is shown in Equation 6: = α Racc + β Rformat + γ Rstep (6) where the coefficients α, β, and γ weight the relative importance of accuracy, format, and step rewards, respectively. In our experimental setup, α, β, and γ are set to 0.8, 0.2, and 1.0, respectively."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we present comprehensive experimental study. We first describe the implementation details, including datasets, training procedures, and inference settings (Sec. 4.1). We then evaluate the effectiveness of our agentic jigsaw interaction learning framework on both jigsaw-solving and general vision tasks (Sec. 4.2). Finally, we perform ablation studies to assess the impact of jigsaw data scale and to contrast the benefits of jigsaw data with those of general QA data (Sec. 4.3). 4.1 IMPLEMENTATION DETAILS Datasets and Models. Our training data consist of two components corresponding to the cold-start and reinforcement learning (RL) stages. In the cold-start stage, we employ 1.6K high-quality jigsaw puzzle trajectories collected in Sec. 3.2 to endow the model with basic interactive jigsaw-solving skills. In the RL stage, we construct dataset of 15.6K images spanning diverse domains, including high-resolution visual search, OCR-based text recognition, real-world scenes, and structured diagrams (see Appendix for details). Each image is partitioned into 2 2 patches and randomly shuffled to ensure that all patches are initially misplaced. All experiments are conducted using Qwen2.5-VL-7B (Bai et al., 2025) as the base model. Training and Inference Setups. We conduct supervised fine-tuning (SFT) on llama-factory (Zheng et al., 2024) and reinforcement learning (RL) training on verl (Sheng et al., 2024), both with fullparameter tuning. For inference and evaluation, we adopt VLMEvalKit (Duan et al., 2024) as the framework. All experiments are performed on 8 NVIDIA A100 GPUs with 80GB memory each. Detailed training hyperparameters are provided in the Appendix D."
        },
        {
            "title": "Preprint",
            "content": "Table 1: Jigsaw Acc result. LN indicates the difficulty level, where denotes the initial number of correct pieces. smaller corresponds to more scrambled jigsaw and higher difficulty. The best results are highlighted in bold, and the second-best results are underlined. Model Random GPT-4o Gemini-2.5-Pro MiMo-VL-7B-RL InternVL3-8B InternVL3-78B Qwen2.5VL-72B Qwen2.5VL-7B +Cold-Start +RL 2 L0 4.5 38.7 43.3 11.3 3.7 3.3 22.7 6.3 12.0 78.7 L1 3.7 37.7 46. 14.3 3.7 4.3 24.3 6.0 32.0 83.0 L2 Avg. 4.2 47.0 49.7 17.0 5.0 3.0 35. 16.3 22.0 86.7 4.1 41.1 46.4 14.2 4.1 3.5 27.4 9.5 22.0 82.8 L0 0.0 1.0 7. 0.3 0.0 0.0 0.3 0.0 0.0 5.0 L1 0.0 1.3 8.7 0.0 0.0 0.0 1.3 0.0 0.3 7. L2 0.0 2.7 9.7 0.0 0.0 0.0 2.3 0.0 0.0 11.0 L3 0.0 3.0 10. 0.0 0.0 0.0 2.7 0.0 0.0 14.0 3 3 L4 0.0 3.0 11.0 0.3 0.0 0.0 2. 0.0 0.0 17.7 L5 0.0 7.0 15.0 0.3 0.0 0.0 3.0 0.7 0.7 25.3 0.0 7.7 23.0 1.0 0.0 0.0 6.7 1.3 0.3 38.0 L7 Avg. 0.0 13.7 32. 3.7 0.0 0.0 12.7 1.3 0.0 48.0 0.0 4.9 14.6 0.7 0.0 0.0 3.9 0.4 0.2 20.8 Table 2: Jigsaw Score result. LN indicates the difficulty level, where denotes the initial number of correct pieces. smaller corresponds to more scrambled jigsaw and higher difficulty. The best results are highlighted in bold, and the second-best results are underlined. Model Random GPT-4o Gemini-2.5-Pro MiMo-VL-7B-RL InternVL3-8B InternVL3-78B Qwen2.5VL-72B Qwen2.5VL-7B +Cold-Start +RL 2 2 25.2 54.8 55.3 21.8 24.6 21.3 40.7 14.4 40.6 86.4 L1 24.7 59.1 60.1 27.2 24.3 26.5 46. 30.1 45.6 88.7 L2 Avg. L0 24.8 63.2 61.6 29.2 25.3 28.4 56. 43.8 45.3 91.9 24.9 59.0 59.0 26.1 24.7 25.4 47.6 29.4 43.8 89.0 11.4 25.9 31.9 5.5 7.5 7.1 21. 5.8 6.5 41.1 L1 11.3 27.3 31.8 6.9 10.1 10.9 25.6 12.0 6.9 46.8 11.0 34.1 35.7 8.0 15.7 14.1 30.8 20.3 7.8 52.6 3 3 L4 11.1 42.3 43. 15.9 20.3 23.1 34.7 32.7 10.4 65.3 L3 10.9 36.3 39.4 10.7 16.8 18.5 33.4 25.6 10.2 58. L5 11.4 48.0 48.7 19.5 25.1 25.7 40.0 40.9 13.2 71.0 L6 11.3 54.4 61. 20.6 28.2 29.0 46.3 50.2 15.7 77.9 L7 Avg. 10.8 64.0 68.6 28.9 31.7 37.0 55. 60.9 17.0 83.6 11.2 41.5 45.1 14.5 19.4 20.7 36.0 31.1 11.0 62.1 4.2 MAIN RESULTS Significant Improvements on the Jigsaw Task. To comprehensively evaluate performance on the jigsaw task, we curate test set of 300 images spanning diverse scenarios, including high-resolution visual search, OCR-based text recognition, and real-world scenes. Each image is partitioned into 2 2 and 3 3 grids, and the performance is measured using two metrics: Acc, which equals 1 only when all patches are correctly placed, and Score, defined as the ratio of correctly placed patches to the total number of patches. As shown in Table 1 and 2, the base model Qwen2.5-VL7B performs poorly without training (achieving only 9.5% accuracy even under the simplest 2 2 setting) while the proprietary Gemini2.5-Pro and the larger Qwen2.5-VL-72B models also struggle on this task. After supervised fine-tuning (SFT) with 1.6K cold-start trajectories and reinforcement learning (RL) with 15.6K images, Qwen2.5-VL-7B achieves substantial gains: on the 2 2 setting, Acc improves from 9.5% to 82.8% and Score from 29.4% to 89.0%; on the more challenging 3 3 setting, it also generalizes well, with Acc increasing from 0.4% to 20.8% and Score from 31.1% to 62.1%, significantly surpassing Gemini2.5-Pro and Qwen2.5-VL-72B. These results demonstrate that modeling jigsaw solving as an interactive multi-turn dialogue enables the model to progressively enhance its visual perception and reasoning abilities, thereby achieving superior performance on the jigsaw task. Generalization to Downstream Visual Tasks. The proposed agentic jigsaw interaction learning framework delivers substantial improvements on the jigsaw task. By leveraging explicit step-by-step feedback for iterative refinement, it enables the model to capture visual relations more effectively and to develop stronger reasoning capabilities."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Main results. Performance comparison of different models on the 9 benchmarks. Abbreviations: MME-RW (MME-RealWorld-Lite), RWQA (RealWorldQA), HRB4K (HRBench4K), HRB8K (HRBench8K), HalBench (HallusionBench), MMMU (MMMU VAL), Avg. denotes the average performance across all 9 benchmarks. represents the relative performance gain achieved by RL compared to the base model Qwen2.5-VL-7B. The best results are highlighted in bold, and the second-best results are underlined. Model MME-RW RWQA HRB4K HRB8K VStar MMVP BLINK HalBench MMMU Avg. LLaVA-OV-7B InternVL2.5-8B InternVL2.5-78B Qwen2.5-VL-72B Qwen2.5-VL-7B + Cold-Start + RL (vs. Qwen2.5-VL-7B) 48.4 48.2 49.7 44. 44.6 46.2 48.4 +3.8 69.5 69.4 78.4 75.3 68.5 68.4 70.2 +1.7 65.3 68.0 74.5 80.1 68.8 71.0 73.0 +4.2 58.4 63.3 72.5 77. 65.3 68.4 70.5 +5.2 73.3 71.7 75.9 85.9 76.4 77.5 80.6 +4.2 77.3 75.7 83.0 81.7 74.3 76.7 78.0 +3.7 52.6 54.9 63.6 61. 56.4 55.7 58.0 +1.6 36.6 49.9 57.1 53.5 50.1 49.8 51.9 +1.8 48.2 53.6 65.4 66.9 54.8 54.0 55.8 +1.0 58.8 61.6 68.9 69. 62.1 63.1 65.2 +3.1 To further assess whether jigsaw training enhances performance on general vision downstream tasks, we evaluate the model on 9 benchmarks: high-resolution image understanding (HRBench4K (Wang et al., 2025c), HRBench8K (Wang et al., 2025c), VStarBench (Wu & Xie, 2024)), real-world scene understanding (MME-RealWorld (Zhang et al., 2024), RealWorldQA (xAI, 2024)), fine-grained visual recognition (MMVP (Tong et al., 2024), BLINK (Fu et al., 2024)), visual reasoning (MMMU (Yue et al., 2024)), and hallucination evaluation (HallusionBench (Guan et al., 2024)). As shown in Table 3, the jigsaw-trained model demonstrates strong visual generalization, achieving notable gains on HRBench4K (+4.2%), HRBench8K (+5.2%), and VStarBench (+4.2%). On average, it surpasses the base model Qwen2.5-VL-7B by 3.1% across all 9 benchmarks, providing compelling evidence that jigsaw-based training effectively enhances the models ability to capture visual relations and strengthen reasoning skills, thereby improving its performance on general vision downstream tasks. 4.3 ANALYSIS Impact of Training Data Scale. Figure 3: The left y-axis denotes the accuracies on HRBench4K and RealWorldQA, while the right y-axis corresponds to the accuracy on the jigsaw task. Figure 4: Comparison with General QA Data. The total number of samples is consistently maintained at 20K across both experimental setups. Impact of Jigsaw Training Data Scale. To investigate the effect of data scale, we systematically vary the amount of training data and evaluate model performance on both jigsaw task and general vision tasks. As shown in Figure 3, scaling up the data yields substantial performance gains: jigsaw task accuracy increases from 22.0% to 82.8%, while HRBench4K and RealWorldQA see improvements of 2.0% and 1.8%, respectively. These results highlight clear data-to-performance trend, demonstrating that larger training sets translate directly into stronger perceptual and reasoning abilities. Crucially, because jigasw data in our reinforcement learning stage are generated via"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Case Study. Jigsaw-solving reasoning and behaviors exhibited by our model. scalable programmatic synthesis, AGILE can easily leverage larger datasets. This scalability not only provides richer learning signals for continual capability growth but also offers practical and sustainable solution to the scarcity of high-quality multimodal RL data. Performance Comparison: Jigsaw vs. General QA Data. We compare the effectiveness of jigsaw data against conventional General QA data for model training. As shown in Figure 4, models trained with jigsaw data consistently outperform those trained with QA data across multiple benchmarks. Notably, combining 10K jigsaw samples with 10K QA samples yields superior performance on general vision benchmarks compared to training with 20K QA samples alone. These results highlight the unique role of jigsaw data in enhancing fundamental visual generalization, demonstrating the efficiency and controllability of jigsaw-solving as proxy for visual perception and reasoning. key advantage of our approach lies in the programmatic synthesis of jigsaw data, which enables effortless construction of large-scale, high-quality datasets, process that is otherwise costly and labor-intensive for QA data. Thus, jigsaw data not only serves as an effective alternative to General QA data but, in some cases, proves to be the superior choice. This finding underscores the potential of jigsaw tasks in alleviating the scarcity of multimodal RL data and opens promising new direction for advancing multimodal model development. Case Study. In Figure 5, we showcase several jigsaw-solving reasoning patterns and behaviors exhibited by our model. These cases illustrate the emergence of high-quality perceptual and reasoning strategies in the jigsaw task, including comprehensively interpreting the visual content of individual pieces to infer their spatial relations, employing cropping and zooming to examine and validate edge alignment, and reasoning about semantic consistency across pieces. Such behaviors demonstrate human-like reasoning, thereby effectively enhancing the models perceptual and reasoning capabilities."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we propose novel agentic jigsaw interaction learning framework that formulates jigsaw solving as an interactive process to enhance visual perception and reasoning in large VisionLanguage Models (VLMs). By treating jigsaw solving as an iterative interaction, the model progressively refines its perceptual and reasoning capabilities through exploration and feedback. Our approach significantly improves performance on jigsaw tasks of varying complexity and demonstrates"
        },
        {
            "title": "Preprint",
            "content": "strong generalization to broader general visual tasks, including visual question answering and scene understanding. Furthermore, by analyzing the performance gains from increasing the scale of jigsaw data and comparing it with general QA data, we show that jigsaw serve as an effective proxy for alleviating the scarcity of high-quality RL data, highlighting the potential of task-driven proxy training for stimulating complex multimodal perception and reasoning in VLMs."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work adheres to ethical research standards in data collection, model training, and evaluation. All datasets used in this study are publicly available research datasets (e.g., COCO, TextVQA, HRBench, RealWorldQA), which were collected and released under their respective licenses. No private or personally identifiable information (PII) was used."
        },
        {
            "title": "7 REPRODICIBILITY STATEMENT",
            "content": "We are committed to ensuring the reproducibility of our results. Comprehensive implementation details, including training data and hyperparameter settings, are provided in Appendices and D. To further facilitate reproducibility, we will release the training code, datasets, and evaluation scripts upon publication."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Fabio Carlucci, Antonio DInnocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22292238, 2019. Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, et al. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles. arXiv preprint arXiv:2505.19914, 2025a. Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/ R1-V, 2025b. Accessed: 2025-02-02. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In European Conference on Computer Vision, pp. 370387. Springer, 2024a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024b. Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024c. Yingyi Chen, Xi Shen, Yahui Liu, Qinghua Tao, and Johan AK Suykens. Jigsaw-vit: Learning jigsaw puzzles in vision transformer. Pattern Recognition Letters, 166:5360, 2023. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Qingxiu Dong, Li Dong, Yao Tang, Tianzhu Ye, Yutao Sun, Zhifang Sui, and Furu Wei. Reinforcement pre-training. arXiv preprint arXiv:2506.08007, 2025. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pp. 1119811201, 2024. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pp. 148166. Springer, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for In Proentangled language hallucination and visual illusion in large vision-language models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14375 14385, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025."
        },
        {
            "title": "Preprint",
            "content": "Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024a. Xiaotong Li, Fan Zhang, Haiwen Diao, Yueze Wang, Xinlong Wang, and Lingyu Duan. Densefusion-1m: Merging vision experts for comprehensive multimodal perception. Advances in Neural Information Processing Systems, 37:1853518556, 2024b. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In European Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. conference on computer vision, pp. 740755. Springer, 2014. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Minesh Mathew, Viraj Bagal, Rub`en Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 16971706, 2022. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. CoRR, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, In Proceedings of the IEEE/CVF and Marcus Rohrbach. Towards vqa models that can read. conference on computer vision and pattern recognition, pp. 83178326, 2019. Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, et al. Code2logic: Game-code-driven data synthesis for enhancing vlms general reasoning. arXiv preprint arXiv:2505.13886, 2025. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. Qiuchen Wang, Ruixue Ding, Zehui Chen, Weiqi Wu, Shihang Wang, Pengjun Xie, and Feng Zhao. Vidorag: Visual document retrieval-augmented generation via dynamic iterative reasoning agents. arXiv preprint arXiv:2502.18017, 2025a. Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, and Feng Zhao. Vrag-rl: Empower vision-perception-based rag for visually rich information understanding via iterative reasoning with reinforcement learning. arXiv preprint arXiv:2505.22019, 2025b."
        },
        {
            "title": "Preprint",
            "content": "Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 79077915, 2025c. Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, et al. Vicrit: verifiable reinforcement learning proxy task for visual perception in vlms. arXiv preprint arXiv:2506.10128, 2025d. Zifu Wang, Junyi Zhu, Bo Tang, Zhiyu Li, Feiyu Xiong, Jiaqian Yu, and Matthew Blaschko. Jigsaw-r1: study of rule-based visual reinforcement learning with jigsaw puzzles. arXiv preprint arXiv:2505.23590, 2025e. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. xAI. Grok-1.5 Vision Preview. https://x.ai/news/grok-1.5v, 2024. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025a. Yunfei Xie, Yinsong Ma, Shiyi Lan, Alan Yuille, Junfei Xiao, and Chen Wei. Play to generalize: Learning to reason through game play. arXiv preprint arXiv:2506.08011, 2025b. Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A DATA DISTRIBUTION",
            "content": "As shown in Figure 6, our RL training corpus for jigsaw is derived from multiple sources, covering diverse range of visual scenarios: Figure 6: Distribution of jigsaw RL training data. High-resolution and visual search images (39.7%, 6.2K samples): To enhance the models fine-grained perception in high-resolution images, we collect data from the VStar (Wu & Xie, 2024) dataset, high-resolution natural scenes in DeepEyes (Zheng et al., 2025), and selected samples from HRBench (Wang et al., 2025c). This improves the models ability to capture subtle visual cues and recognize small object attributes. Text recognition and structured scenes (33.3%, 5.2K samples): To strengthen the models capacity for text perception and recognition, we include text-rich images from diverse domains, such as the TextVQA (Singh et al., 2019) training set, InfoVQA (Mathew et al., 2022) book covers and posters, as well as structured visual reasoning tasks involving tables and subject-specific charts (Yue et al., 2024; Li et al., 2024a; Chen et al., 2024b). Dense natural and real-world scenes (26.9%, 4.2K samples): To enhance the models recognition and understanding in complex and real-world environments, we collected images from COCO2017 (Lin et al., 2014) and RealWorldQA (xAI, 2024) datasets."
        },
        {
            "title": "B PROMPTS",
            "content": "In this section, we present the prompt used to collect high-quality jigsaw trajectories, as illustrated in Figure B. The same prompt is employed to prompt the Qwen2.5-VL-7B model during both the SFT and RL stages. Prompt for Interactive Collection of High-Quality Jigsaw Trajectories System Prompt: You are skilled and experienced puzzle master. will divide an image evenly into 2 2 grid, get 4 image blocks, and then shuffle them. The image blocks will be labeled A, B, C, and D. Initially, the image blocks are named and arranged in the following order: [A, B, C, D] That is, the initial layout of the image blocks is as follows: D Your Task:"
        },
        {
            "title": "Preprint",
            "content": "Your goal is to reconstruct the original image by observing and analyzing the visual content between the image blocks. Pay attention to the details in each image block and establish visual connections between adjacent image blocks to achieve the goal of completing the puzzle correctly. For example: - Continuity of text information: If the image contains text or logos, observe whether the text continues naturally between adjacent blocks (e.g., characters connect smoothly and the font direction is consistent). This is one of the key clues to determine whether the image blocks should be adjacent. - Structure and shape consistency: Identify possible structural elements in the image (such as buildings, roads, object outlines, etc.), and infer which image blocks should be visually spliced together to form complete and reasonable shape. - Continuation of edge visual details: Observe the color, texture or pattern (such as sky, grass, lines or shadows) at the edge of the image block to determine whether it can be visually connected to the adjacent block naturally. - Direction and angle consistency: Pay attention to the direction of image elements (such as human faces, object directions, text angles, etc.) to ensure the rationality of the overall visual direction of the image. You can swap any two image blocks each time to achieve the correct layout. Inference Process: 1. State Representation: At each step, you need to maintain state list representing current arrangement of image blocks. Example: state = [B, C, A, D] This corresponds to: Top left (index 0): Top right (index 1): Bottom left (index 2): Bottom right (index 3): 2. Visual Analysis: Carefully analyze the image patch content and visual details to determine the image patch location. To better complete the puzzle task, you can use additional image operations to enhance visual perception: - Crop the image area of observation image to more closely observe the visual correlation between different regions in multiple image patches. Used to find clues during the puzzle or verify whether the puzzle is complete (For example: it looks like the puzzle is not completely fixed, maybe because of the lower left and upper right corners need to crop these two areas separately to get some visual of observation image 2. information to further judge. First will crop the lower left area <code>...</code>By observing the cropped lower left area, it shows that part of the text OF may be related to the appyness text in the upper right corner. To further verify and manage, will now crop the upper right image <code>...</code>. By observing the cropped image, will swap image blocks and C...) - Zoom area to enlarge visual details (selectively observe the details of the cropped image). These operations can help you make more informed decisions."
        },
        {
            "title": "Preprint",
            "content": "3. Move Execution: After making decision, generate Python code snippet to swap tiles. Use the observation(state) function to get the updated layout image for the next step. Available Image Operations: You can use the following Python functions to assist in inference: 1. Crop region from an image using normalized coordinates (from 0 to 1). crop_box = [x1, y1, x2, y2] crop_image_{id} = crop(image, crop_box) 2. Zoom in an image by specified factor (e.g., 1.5 zoom means 150% zoom). zoom_image_{id} = zoom(image, zoom_factor) 3. Swap tiles (e.g., top left and bottom left) and call observation function to see the jigsaw progress. state[0], state[2] = state[2], state[0] observation_image_{id} = observation(state) Replace id with an integer to uniquely identify each operation result. The image parameter must point to an existing image (e.g., observation image 1, crop image 1, etc.). Format: All Python code must be enclosed in <code>...</code>tags. All reasoning steps must be enclosed in <think>...</think>tags. The final answer must be enclosed in <answer>...</answer>tags. Important: 1. For both swap and image manipulation, always explain why you are doing this before generating code. 2. Stop as soon as you generate code snippet. will execute the code and return the generated image for the next step. 3. You can only perform one image operation or observation of the puzzle state per turn. 4. Final answer: When you are sure that the puzzle is fully reconstructed (all edges are aligned and the visual information is complete and continuous), return the final state list in the <answer> block. 5. Please pay close attention to the rationality of your cropping area and ensure that the cropped area is reasonable criterion for completing the puzzle. User Prompt: The four images are respectively labeled A, B, C, and D. Please complete the jigsaw. Image A: <image>Image B: <image>Image C: <image>Image D: <image>"
        },
        {
            "title": "C CASE STUDY",
            "content": "In Figures 7 - 10, we present representative high-quality jigsaw trajectories collected to illustrate how our model performs reasoning and interacts with the environment. These cases specifically demonstrate: (1) how the model generates Python code to interact with the environment, such as swapping any two jigsaw pieces, observing jigsaw progress, and cropping/zooming in on regions for finer visual inspection; and (2) how the environment provides feedback that guides the models subsequent reasoning. Figure 7: Representative high-quality jigsaw trajectory case 1 (part 1)."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Representative high-quality jigsaw trajectory case 1 (part 2)."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Representative high-quality jigsaw trajectory case 2 (part 1)."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Representative high-quality jigsaw trajectory case 2 (part 2)."
        },
        {
            "title": "D THE IMPLEMENTATION OF TRAINING",
            "content": "The detailed training hyperparameters are provided in Tables 4 and 5, and all experiments are conducted on 8 NVIDIA A100 GPUs, each equipped with 80 GB of memory."
        },
        {
            "title": "Preprint",
            "content": "Table 4: Key hyperparameters for SFT. Table 5: Key hyperparameters for RL. Name Finetuning type Freeze vision tower Freeze multi-modal projector Freeze language model Cutoff len Image max pixels Epochs Batch size Gradient accumulation steps Learning rate LR scheduler type Warmup ratio Value Full True True False 16384 401408 2.0 32 4 1.0e-5 cosine 0. Name"
        },
        {
            "title": "Max turns\nRollout num\nTrain batch size\nMini batch size\nMicro batch size per GPU\nLearning rate\nKL loss coefficient\nTotal epochs\nMax prompt length\nSingle response max tokens\nMax response length\nGPU memory utilization",
            "content": "Value 5 8 64 64 2 2.0e-6 0.0 1 8192 2048 20000 0."
        },
        {
            "title": "E ANALYSIS OF WANDB CURVES IN JIGSAW OPTIMIZATION",
            "content": "In this section, we present the training curves of our RL process and provide an analysis of the training dynamics. We monitor the evolution of rewards (including accuracy and format rewards), response length, number of interaction turns, and validation accuracy. (a) Reward mean. (b) Response length mean. (c) Turn number mean. (d) Validation accuracy. Figure 11: Visualization of Wandb curves in jigsaw RL optimization. At the beginning of training, both validation accuracy and reward values are low, indicating that the model has very limited jigsaw-solving ability in the early stage. As training progresses, the number of interaction turns increases briefly, suggesting that the model explores more interactions (e.g., swapping tiles, cropping for observation) in order to achieve higher accuracy. With continued training, the reward values steadily increase, reflecting stable improvement in jigsaw-solving capability through RL. After sufficient training, the model gradually reduces both the number of turns and response length, demonstrating enhanced perceptual and reasoning ability, such that it can solve jigsaw correctly with fewer interactions."
        },
        {
            "title": "F LIMITATIONS",
            "content": "Despite our best efforts, this work has several limitations. Multi-turn jigsaw interactions inevitably In the 3 3 setting, increase context length and introduce significant computational overhead. the context often exceeds the models maximum window size, restricting our RL training to the 2 2 case. Future work could investigate more efficient interaction mechanisms with external environments or integrate memory modules to mitigate context length constraints, thereby enabling RL training on larger and more complex jigsaw tasks."
        },
        {
            "title": "G USE OF LLMS",
            "content": "Yes. We use LLMs solely to assist in language polishing and improving readability. All technical content, experiments, and analyses are conducted by the authors."
        }
    ],
    "affiliations": [
        "East China Normal University",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "University of Science and Technology of China"
    ]
}