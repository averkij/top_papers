{
    "paper_title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation",
    "authors": [
        "Yushi Lan",
        "Shangchen Zhou",
        "Zhaoyang Lyu",
        "Fangzhou Hong",
        "Shuai Yang",
        "Bo Dai",
        "Xingang Pan",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces a novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs a Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both text- and image-conditioned 3D generation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 3 3 0 8 0 . 1 1 4 2 : r Prepreint, work in progress GAUSSIANANYTHING: LATENT DIFFUSION FOR 3D GENERATION"
        },
        {
            "title": "INTERACTIVE POINT CLOUD",
            "content": "Yushi Lan, Shangchen Zhou, Zhaoyang Lyuδ, Fangzhou Hong, Shuai Yangγ, Bo Daiδ, Xingang Pan, Chen Change Loy S-Lab, Nanyang Technological University, Singapore δShanghai Artificial Intelligence Laboratory, γPeking University https://nirvanalan.github.io/projects/ga/ Figure 1: Our method generates high-quality and editable surfel Gaussians through cascaded 3D diffusion pipeline, given single-view images or texts as the conditions."
        },
        {
            "title": "ABSTRACT",
            "content": "While 3D content generation has advanced significantly, existing methods still face challenges with input formats, latent space design, and output representations. This paper introduces novel 3D generation framework that addresses these challenges, offering scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our framework employs Variational Autoencoder (VAE) with multi-view posed RGB-D(epth)-N(ormal) renderings as input, using unique latent space design that preserves 3D shape information, and incorporates cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GAUSSIANANYTHING, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multiview image inputs. Notably, the newly proposed latent space naturally enables geometry-texture disentanglement, thus allowing 3D-aware editing. Experimental results demonstrate the effectiveness of our approach on multiple datasets, outperforming existing methods in both textand image-conditioned 3D generation. 1 Prepreint, work in progress"
        },
        {
            "title": "INTRODUCTION",
            "content": "3D content generation holds great potential for transforming the virtual reality, film, and gaming industries. Current approaches typically follow one of two paths: either 2D-lifting method or the design of native 3D diffusion models. While the 2D-lifting approach (Shi et al., 2023b; Liu et al., 2023b) benefits from leveraging 2D diffusion model priors, it is often hindered by expensive optimization, the Janus problem, and inconsistencies between views. In contrast, native 3D diffusion models (Jun & Nichol, 2023; Lan et al., 2024; Zhang et al., 2024) are trained from scratch for 3D generation, offering improved generality, efficiency, and control. Despite the progress in native 3D diffusion models, several design challenges still persist: (1) Input format to the 3D VAE. Most methods (Zhang et al., 2024; Li et al., 2024) directly adopt point cloud as input. However, it fails to encode the high-frequency details from textures. Besides, this limits the available training dataset to artist-created 3D assets, which are challenging to collect on large scale. LN3Diff (Lan et al., 2024) adopt multi-view images as input. Though straightforward, it lacks direct 3D information input and cannot comprehensively encode the given object. (2) 3D latent space structure. Since 3D objects are diverse in geometry, color, and size, most 3D VAE models adopt the permutation-invariant set latent (Zhang et al., 2023a; Sajjadi et al., 2022; Zhang et al., 2024) to encode incoming 3D objects. Though flexible, this design lacks the image-latent correspondence as in Stable Diffusion VAE (Rombach et al., 2022), where the VAE latent code can directly serve as the proxy for editing input image (Mou et al., 2023b;a). Other methods adopt latent tri-plane (Wu et al., 2024; Lan et al., 2024) as the 3D latent representation. However, the latent tri-plane is still unsuitable for interactive 3D editing as changes in one plane may not map to the exact part of the objects that need editing. (3) Choice of 3D output representations. Existing solutions either output texture-less SDF (Wu et al., 2024; Zhang et al., 2024), which requires additional shading model for post-processing; or volumetric tri-plane (Lan et al., 2024), which struggles with high-resolution rendering due to extensive memory required by volumetric rendering (Mildenhall et al., 2020). In this study, we propose novel 3D generation framework that resolves the problems above and enables scalable, high-quality 3D generation with an interactive Point Cloud-structured Latent space. The resulting method, dubbed GAUSSIANANYTHING, supports multi-modal conditional 3D generation, including point cloud, caption, and image. Specifically, we propose 3D VAE that adopts multi-view posed RGB-D(epth)-N(ormal) renderings as the input, which are easy to render and contain comprehensive 3D attributes corresponding to the input 3D object. The information of each input view is channel-wise concatenated and efficiently encoded with the scene representation transformer (Sajjadi et al., 2022), yielding set latent that compactly encodes the given 3D input. Instead of directly applying it for diffusion learning (Zhang et al., 2024; Li et al., 2024), our novel design concretizes the unordered tokens into the shape of the 3D input. Specifically, this is achieved by cross-attending (Huang et al., 2024b) the set latent via sparse point cloud sampled from the input 3D shape, as visualized in Fig. 2. The resulting point-cloud structured latent space significantly facilitate shape-texture disentanglement and 3D editing. Afterward, DiT-based 3D decoder (Peebles & Xie, 2023; Lan et al., 2024) gradually decodes and upsamples the latent point cloud into set of dense surfel Gaussians (Huang et al., 2024a), which are rasterized to high-resolution renderings to supervise 3D VAE training. After the 3D VAE is trained, we conduct cascaded latent diffusion modeling on the latent space through flow matching (Albergo et al., 2023; Lipman et al., 2023; Liu et al., 2023c) using the DiT (Peebles & Xie, 2023) framework. To encourage better shape-texture disentanglement, point cloud diffusion model is first trained to carve the overall layout of the input shape. Then, pointcloud feature diffusion model is cascaded to output the corresponding feature conditioned on the generated point cloud. The generated featured point cloud is then decoded into surfel Gaussians via pre-trained VAE for downstream applications. In summary, we contribute comprehensive 3D generation framework with point cloud-structured 3D latent space. The redesigned 3D VAE efficiently encodes the 3D input into an interactive latent space, which is further decoded into high-quality surfel Gaussians. The diffusion models trained on the compressed latent space have shown superior performance in text-conditioned 3D generation and editing, as well as impressive image-conditioned 3D generation on general real world data. Prepreint, work in progress"
        },
        {
            "title": "2 RELATED WORK",
            "content": "3D Generation via 2D Diffusion Models. The success of 2D diffusion models (Song et al., 2021; Ho et al., 2020) has inspired their application to 3D generation. Score distillation sampling (Poole et al., 2022; Wang et al., 2023) distills 3D from 2D diffusion model, but faces challenges like expensive optimization, mode collapse, and the Janus problem. More recent methods propose learning the 3D via two-stage pipeline: multi-view images generation (Shi et al., 2023b; Long et al., 2024; Shi et al., 2023a) and feed-forward 3D reconstruction (Hong et al., 2024b; Xu et al., 2024; Tang et al., 2024). Though promising results have been achieved, their performance is bounded by the multi-view generation results, which usually violate view consistency (Liu et al., 2023b) and fails to scale up to higher resolution (Shi et al., 2023a). Moreover, this two-stage pipeline limits the 3D editing capability due to the lack of 3D-aware latent space. Native 3D Diffusion Models. Native 3D diffusion models (Zhang et al., 2023a; Zeng et al., 2022; Zhang et al., 2024; Lan et al., 2024; Li et al., 2024) are recently proposed to achieve high-quality, efficient and scalable 3D generation. native 3D diffusion pipeline involves two-stage training process: encoding 3D objects into the VAE latent space (Kingma & Welling, 2013; Kosiorek et al., 2021), and latent diffusion model on the corresponding latent codes. Though straightforward, existing methods differ in VAE input formats, latent space structure and output 3D representations. While most methods adopt point alone as the VAE input (Zhang et al., 2023a; 2024; Li et al., 2024), our proposed method encodes hybrid 3D information through convolutional encoder. Moreover, comparing to the latent set (Zhang et al., 2023a; Sajjadi et al., 2022) representation, our proposed method adopts point cloud-structured latent space, which can be directly used for interactive 3D editing. Besides, rather than producing textureless SDF, our method directly decodes the 3D latent codes into high-quality surfel Gaussians (Huang et al., 2024a), which can be directly used for efficient rendering. Point-based Shape Representation and Rendering. The proliferation of 3D scanners and RGBD cameras makes the capture and processing of 3D point clouds commonplace (Gross & Pfister, 2011). In the era of deep learning, learning-based methods are emerging for point set processing (Qi et al., 2016; Zhao et al., 2021), up-sampling (Yu et al., 2018), shape representation (Genova et al., 2020; Lan et al., 2023b), and rendering (Pfister et al., 2000; Yifan et al., 2019; Lassner & Zollhofer, 2021; Xu et al., 2022; Kerbl et al., 2023). Moreover, given its affinity for modern network architectures (Huang et al., 2024b; Zhao et al., 2021), more explicit nature against other 3D representations (Chan et al., 2022; Mildenhall et al., 2020; Muller et al., 2022), efficient rendering (Kerbl et al., 2023), and even high-quality surface modeling (Huang et al., 2024a), point-based 3D representations are rapidly developing towards the canonical 3D representation for learning 3D shapes. Thus, we choose (featured) point cloud as the representation for the 3D VAE latent space, and 2D Gaussians (Huang et al., 2024a) as the output 3D representations. Feed-forward 3D Reconstruction and View Synthesis. To bypass the per-scene optimization of NeRF, researchers have proposed learning prior model through image-based rendering (Wang et al., 2021; Yu et al., 2021). However, these methods are primarily designed for view synthesis and lack explicit 3D representations. Sajjadi et al. (2022; 2023) propose Scene representation transformer (SRT) to process RGB images with Vision Transformer (Dosovitskiy et al., 2021) and infers set-latent scene representation. Though benefiting from the flexible design, its geometryfree paradigm also fails to generate explicit 3D outputs. Recently, LRM-line of work (Hong et al., 2024b; Tang et al., 2024; Wang et al., 2024) have proposed feed-forward framework for generalized monocular reconstruction. However, they are still regression-based models and lack the latent space designed for generative modeling and 3D editing. Besides, they are limited to 3D reconstruction only and fail to support other modalities."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "2D Gaussian Splatting (2DGS). Since 3DGS (Kerbl et al., 2023) models the entire angular radiance in blob, it fails to reconstruct high-quality object surfaces. To resolve this issue, Huang et al. (2024a) proposed 2DGS (surfel-based GS) that simplifies the 3-dimensional modeling by adopting flat 2D Gaussians embedded in 3D space, which enables better alignment with thin surfaces. 3 Prepreint, work in progress Notation-wise, the 2D splat is characterized by its central point pk, two principal tangential vectors tu and tv, and scaling vector = (su, sv) that controls the variances of the 2D Gaussian. Notice that the primitive normal is defined by two orthogonal tangential vectors tw = tu tv. Thus, the 2D Gaussian is parameterized with (u, v) = pk + sutuu + svtvv = H(u, v, 1, 1)T (cid:20)RS pk (cid:21) 1 svtv 0 pk 1 (cid:20)sutu 0 where = = 0 (cid:21) (1) (2) Where parameterizes the local 2D Gaussian geometry. For the point = (u, v) in uv space, its 2D Gaussian value can then be evaluated by standard Gaussian G(u) = exp , and the center pk, scaling (su, sv), and the rotation (tu, tv) are all learnable parameters. Following 3DGS Kerbl et al. (2023), each 2D Gaussian primitive has opacity α and view-dependent appearance c, and can be rasterized via volumetric alpha blending: u2+v2 2 (cid:17) (cid:16) c(x) = (cid:88) i=1 ci αi ˆGi(u(x)) i1 (cid:89) (1 αj ˆGj(u(x))), j=1 (3) where the integration process is terminated when the accumulated opacity reaches saturation. During optimization, pruning and densification operations are iteratively applied. Flow Matching and Diffusion Model. Diffusion models create data from noise (Song et al., 2021) and are trained to invert forward paths of data towards random noise. The forward path is constructed as zt = atx0 + btϵ, where ϵ (0, I) , at and bt are hyper parameters. The choice of forward process has proven to have important implications for the backward process of data sampling (Lin et al., 2023). Recently, flow matching (Liu et al., 2023c; Albergo et al., 2023; Lipman et al., 2023) has introduced particular choice for the forward path, which has better theoretical properties and has been verified on the large-scale study (Esser et al., 2024). Given unified diffusion objective (Karras et al., 2022): Lw(x0) = 1 2 EtU (t),ϵN (0,I) (cid:2)wtλ tϵΘ(zt, t) ϵ2(cid:3) , (4) where λt := log a2 denotes its derivative. By setting wt = b2 1t with zt = (1 t)x0 + tϵ, flow matching defines the forward process as straight path between the data distribution and the Normal distribution. The network ϵΘ directly predicts the velocity vΘ. denotes signal-to-noise ratio, and λ"
        },
        {
            "title": "4 GAUSSIANANYTHING",
            "content": "This section introduces our native 3D diffusion model, which learns 3D-aware diffusion prior over the novel point-cloud structured latent space through dedicated 3D VAE. The goal of training is to learn 1. An encoder Eϕ that maps set of posed RGB-D-N images = {Ri, ..., RV }, corresponding to the given 3D object to point-cloud structured latent = [zx zh]; 2. conditional cascaded transformer denoiser ϵh Θ(zh,t, zx,0, t, c) ϵx Θ(zx,t, t, c) to denoise the noisy latent code zt given diffusion time step and condition prompt c; 3. decoder Dψ (including Transformer DT and cascaded attention-base Upsampler DU ) to map z0 to the surfel Gaussian (cid:101)G corresponding to the input object. Moreover, our attention-based decoding of dense surfel Gaussian also provides novel way for efficient Gaussian prediction Beyond the advantages shared by existing 3D LDM (Zhang et al., 2024; Lan et al., 2024), our design offers flexible point-cloud structured latent space and enables interactive 3D editing. In the following subsections, we first discuss the proposed 3D VAE with detailed framework design in Sec 4.1. Based on that, we introduce the cascaded conditional 3D diffusion stage in Sec. 4.2. The method overview is shown in Fig. 2. 4 Prepreint, work in progress Figure 2: Pipeline of the 3D VAE of GAUSSIANANYTHING. In the 3D latent space learning stage, our proposed 3D VAE Eϕ encodes views of posed RGB-D(epth)-N(ormal) renderings into point-cloud structured latent space. This is achieved by first processing the multi-view inputs into the un-structured set latent, which is further projected onto the 3D manifold through cross attention block, yielding the point-cloud structured latent code z. The structured 3D latent is further decoded by 3D-aware DiT transformer, giving the coarse Gaussian prediction. For high-quality rendering, the base Gaussian is further up-sampled by series of cascaded upsampler Dk towards dense Gaussian for high-resolution rasterization. The 3D VAE training objective is detailed in Eq. (9). 4.1 POINT-CLOUD STRUCTURED 3D VAE Unlike image and video, the 3D domain is un-uniform and represented differently for different purposes. Thus, how to encode 3D objects into the latent space for diffusion learning plays crucial role in the 3D generation performance. This challenge is two-fold: what 3D representations to encode, and what network architecture to process the input. Versatile 3D Input. Instead of using dense point cloud (Zhang et al., 2024; Li et al., 2024), we adopt multi-view posed RGB-D(epth)-N(ormal) images as input, which encode the 3D input more comprehensively and can be efficiently processed by well-established network architectures (Sajjadi et al., 2022; Wu et al., 2023) in flexible manner. Specifically, the input is set of multi-view renderings of 3D object, where each rendering within the set = (I, , N, π) contains thorough 3D attributes that depict the underlying 3D object from the given viewpoint: the rendered RGB image RHW 3, depth map RHW , normal map RHW 3, and the corresponding camera pose π. To unify these 3D attributes in the same format, we further process the camera π into Plucker coordinates (Sitzmann et al., 2021) pi = (o du,v, du,v) R6, where oi R3 is the camera origin, du,v R3 is the normalized ray direction, and denotes the cross product. Thus, the Plucker embedding of given camera π can be expressed as RHW 6. Besides, following MCC (Wu et al., 2023), we use π to unproject the depth map into their 3D positions RHW 3. The resulting information is channel-wise concatenated, giving = [IXN P] RHW (3+3+3+6=15). Transformer-based 3D Encoding. Given the 3D renderings R, encoding them into 3D latent space remains significant challenge. Independently processing each input rendering with existing network architecture (Wu et al., 2023; Dosovitskiy et al., 2021) overlooks the information from other views, leading to 3D inconsistency and content drift across views (Liu et al., 2023b). Existing multi-view generation alleviates this issue by injecting 3D attention (Shi et al., 2023b; Tang et al., 2024; Shi et al., 2023a) into the U-Net architecture. Inspired by its effectiveness, here we directly adopt Scene Representation Transformer (SRT)-like encoder (Sajjadi et al., 2022; 2023) to process the multi-view inputs, which fully adopts 3D attention transformer block for the 3D representation learning. Specifically, the encoder first down-samples the multi-view inputs via shared CNN backbone, and then processes the aggregated multi-view tokens through the transformer encoder (Dosovitskiy et al., 2021): zz = TX ϕ (E CNN ϕ ({ R)}), (5) where zz is the set latent corresponding to the 3D input. This can be seen as the full-attention version of existing 3D attention-augmented architecture. The resulting latent codes zz fully capture the intact 3D information corresponding to the input. Compared to existing work that adopts point 5 Prepreint, work in progress clouds only as input (Zhang et al., 2024; Li et al., 2024), our proposed solution supports more 3D properties as input in flexible way. Besides, the attention operations can be well optimized over modern GPU architecture (Dao et al., 2022; Dao, 2024). Point Cloud-structured Latent Space. Though zz fully captures the given 3D input, it cannot be directly used for diffusion learning due to the lack of regularized, low-rank latent space (Rombach et al., 2022). Specifically, zz has shape of (H/f ) (W/f ) C, where is the number of views used for input, H, is the input resolution and is the down-sampling factor of the CNN backbone. Given = 8, = 8, and = = 512, the resulting latent codes will have shape of 32768 C, which are unsuitable for diffusion learning. Moreover, using the set (Lee et al., 2019) as the 3D latent space (Zhang et al., 2023a; 2024; Li et al., 2024) also sacrifices an explicit, editable latent space (Mou et al., 2023a) for flexibility. Here, we resolve these issues by proposing point cloud-structured latent space. Specifically, we project the un-structured features zz onto the manifold of the input 3D shape through the cross attention layer: zh := CrossAttn(PE(zx), zz, zz), (6) where CrossAttn(Q, K, ) denotes cross attention block with query Q, key K, and value . zx R3N is sparse point cloud sampled from the surface of the 3D input with Farthest Point Sampling (FPS) (Qi et al., 2017), and PE denotes positional embedding (Tancik et al., 2020). Intuitively, we define read cross attention block (Huang et al., 2024b) that cross attends information from unstructured representation zz into the point-cloud structured feature zh RChN , with Ch C. In this way, we obtain the point-cloud structured latent code = [zx zh] R(3+Ch)N for diffusion learning. High-quality 3D Gaussian Decoding. Given the point cloud-structured latent codes, how to decode them into high-quality 3D representation for supervision remains challenging. Though dense point cloud (Huang et al., 2024b) is straightforward solution, it fails to depict high-quality 3D structure with limited point quantity. Here, we resort to surfel Gaussian (Huang et al., 2024a), an augmented point-based 3D representation that supports high-fidelity 3D surface modeling and efficient rendering. Specifically, our decoder first decodes the input through the 3D-DiT blocks (Peebles & Xie, 2023; Lan et al., 2024), which has shown superior performance against traditional transformer layer: := DT (MLP(z)), (7) where an MLP layer first projects the input latent to the corresponding dimension, and DT is the DiT transformer. Since dense Gaussians are preferred for high-quality splatting (Kerbl et al., 2023), we gradually upsample the latent features through transformer blocks. Specifically, given learnable embedding zu RfuC where fu is the up-sampling ratio, we prepend it to each token in the latent sequence. Then, layers of transformer blocks are used to model the upsampling process: z(k+1) := Dk ([zu zi]), (8) where Dk in Fig. 2, and z(k+1) R(fuN )C after up-sampling are used to predict the 13-dim attributes of surfel Gaussians. is transformer block for predicting the kth levels of details (LoD) Gaussian as shown RfuC are the upsampled set of tokens. The overall tokens z(k+1) To achieve denser Gaussians prediction, we cascade the upsampling transformer defined in Eq. (8) for times, giving the final Upsampler DU for high-quality Gaussian output. Note that our solution outputs set of Gaussians that are uniformly distributed on the 3D object surface with near 100% Gaussian utilization ratio. Existing pixel-aligned Gaussian prediction models (Tang et al., 2024; Yinghao et al., 2024; Szymanowicz et al., 2023), however, usually waste 50% Gaussians due to view overlaps and empty background color. Besides, our intermediate Gaussians output naturally serves as LoD (Takikawa et al., 2021), which can be used in different scenarios to balance the rendering speed and quality. Training. Our 3D VAE model is end-to-end optimized across both input views and randomly chosen views, minimizing image reconstruction objectives between the splatting renderings and groundtruth renderings. Besides image reconstruction loss, we also impose loss over geometry regularizations, KL constraints, and adversarial loss: L(ϕ, ψ) = Lrender + Lgeo + λklLKL + λGANLGAN, (9) 6 Prepreint, work in progress Figure 3: Diffusion training of GAUSSIANANYTHING. Based on the point-cloud structure 3D VAE, we perform cascaded 3D diffusion learning given text (a) and image (b) conditions. We adopt DiT architecture with AdaLN-single (Chen et al., 2023) and QK-Norm (Dehghani et al., 2023; Esser et al., 2021). For both condition modality, we send in the conditional feature with cross attention block, but at different positions. The 3D generation is achieved in two stages (c), where point cloud diffusion model first generates the 3D layout zx,0, and texture diffusion model further generates the corresponding point-cloud features zh,0. The generated latent code z0 is decoded into the final 3D object with the pre-trained VAE decoder. where Lrender is mixture of the L1 and VGG loss (Zhang et al., 2018), Lgeo improves geometry reconstruction (Huang et al., 2024a), LKL is the KL-reg loss (Kingma & Welling, 2013; Rombach et al., 2022) to regularize structured latent space, and LGAN improves perceptual fidelity. All loss terms except LKL are applied over randomly chosen LoD in each iteration, and the Lrender is applied to both input-view and randomly sampled novel-view images. Besides, the geometry loss Lgeo is composed of two regularization terms, including the depth distortion loss to concentrate the weight distribution along rays, inspired by Mip-NeRF (Barron et al., 2021; 2022). Given ray of pixel, the distortion loss is defined as (cid:88) Ld = ωiωjdi dj, (10) where ωi = αi ˆGi(u(x)) (cid:81)i1 j=1(1αj ˆGj(u(x))) is the blending weight of the ith intersection and di is the depth of the intersection points. Besides, as surfel Gaussians explicitly model the primitive normals, we encourage the splats normal to locally approximate the actual object surface: i,j Ln = (cid:88) ωi(1 ˆN ), (11) where ˆN is the predicted normal maps. The final geometry loss is given by Lgeo = λdLd + λnLn. 4.2 CASCADED 3D GENERATION WITH FLOW MATCHING After training the point-cloud structured 3D VAE, we get dataset of shapes paired with condition vectors (e.g., caption or images), {(zi, ci)}i[D], where the shape is represented by latent code through the 3D VAE aforementioned. Our goal is to train flow-matching generative model to learn diffusion prior on top of it. Below we present how we adapt flow-based models to our case. Cascaded Flow Matching over Symmetric Data. As detailed in Sec. 3, flow matching involves training neural network ϵΘ to predict the velocity of the noisy input zt with the straight-line trajectory. After training, ϵΘ can sample from standard Normal prior (0, I) by solving the reverse ODE/SDE (Karras et al., 2022). In our case, the training data point is the point-cloud structured Prepreint, work in progress latent code = [zx zh] R(3+Ch)N , which is symmetric and permutation invariant (Zeng et al., 2022; Nichol et al., 2022). Based on this property, we opt for diffusion transformer (Peebles & Xie, 2023) without positional encoding as the ϵΘ parameterization. Here, rather than modeling zx and zh jointly, we empirically found that cascaded framework (Ho et al., 2021; Lyu et al., 2024; 2023) leads to better performance. Specifically, conditioned sparse point cloud generative model ϵx Θ is first trained to generate the overall structure of the given object: Lx w(x0) = 1 EtU (t),ϵN (0,I) (cid:2)wFM λ tϵx Θ(zx,t, t, c) ϵ2(cid:3) , (12) and point cloud feature generative model ϵh regularized feature conditioned on the sparse point cloud: Θ is cascaded to learn the corresponding KLLh w(x0) = 1 2 EtU (t),ϵN (0,I) (cid:2)wFM λ tϵh Θ(zh,t, zx, t, c) ϵ2(cid:3) . (13) The detailed cascading process is detailed Fig. 3 (c). Our proposed design enables better geometrytexture disentanglement and facilitates 3D editing over specific shape properties. Conditioning Mechanism. Compared to LRM (Hong et al., 2024b; Tang et al., 2024) line of work which only handles image conditions, our native diffusion-based method enables more flexible 3D generation from diverse conditions. As shown in Fig. 3 (a-b), for the text-conditioned model, we adopt CLIP (Radford et al., 2021) to extract penultimate tokens as the condition embeddings; and for the image conditioned model, we use DINOv2 (Oquab et al., 2023) to extract global and patch features. All conditions are injected into the DiT architecture through pre-norm (Xiong et al., 2020) cross-attention block. All the models are trained with Classifier-free Guidance (CFG) (Ho, 2021) by randomly dropping the conditions with probability of 10%. To cascade two diffusion models, we encode the output of stage-1 model ϵx Eq. (6), and add it to the first-layer features of ϵh with the input sparse point cloud structure. Θ with PE(zx) as in Θ. This guarantees that generated features are paired"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Datasets. To train our 3D VAE, we use the renderings provided by G-Objaverse (Qiu et al., 2023; Deitke et al., 2023) and choose high-quality subset with around 176K 3D instances, where each consists of 40 random views with RGB, normal, depth map and camera pose. For text-conditioned diffusion training, we use the caption provided by Cap3D (Luo et al., 2023; 2024) and 3DTopia Hong et al. (2024a). For image-conditioned training, we randomly select an image in the dataset of the corresponding 3D instance as the condition. Implementation Details. For 3D VAE, we choose = 8 views of RGB-D-N renderings as input to guarantee thorough coverage of the 3D object. The CNN Encoder is implemented with similar architecture as LDM VAE (Rombach et al., 2022) with down-sampling factor of = 8, and the multi-view transformer has five layers as in RUST (Sajjadi et al., 2023). The sparse point cloud zx has size of 3 where = 768, and the corresponding featured point cloud zh has dimension of 10. For upsampling blocks, we employ = 3 blocks with 1 = 3, giving 73, 768 Gaussians in total. All transformer blocks follow pre-norm design (Xiong et al., 2020). During 3D VAE training, the model is supervised by randomly chosen LoD renderings, with λkl = 2e 6, λd = 1000, λn = 0.2, and λGAN = 0.1. We adopt batch size 64 with both input and random novel views for training. During the conditional diffusion training stage, we adopt batch size 256. All models are efficiently and stably trained with lr = 1e 4 on 8A100 GPUs for 1M iterations with BF16 and FlashAttention (Dao, 2024) enabled. We use CFG=4 and 250 ODE steps for all sampling results. = 4, and 1 = 8, 1 5.1 METRICS AND BASELINES Evaluating Image-to-3D Generation. We evaluate GAUSSIANANYTHING on both image and text conditioned generation. Regarding image-conditioned 3D generation methods, we compare 8 Prepreint, work in progress Figure 4: Qualitative Comparison of Image-to-3D. We showcase the novel view 3D reconstruction of all methods given single image from unseen GSO dataset. Our proposed method achieves consistently stable performance across all cases. Note that though feed-forward 3D reconstruction methods achieve sharper texture reconstruction, these method fail to yield intact 3D predictions under challenging cases (e.g., the rhino in row 2). In contrast, our proposed native 3D diffusion model achieve consistently better performance. Better zoom in. the proposed method with three lines of methods: single-image to 3D methods: OpenLRM (He & Wang, 2023; Hong et al., 2024b), Splatter Image (Szymanowicz et al., 2023), multi-view images to 3D methods: One-2-3-45 Liu et al. (2023a), CRM (Wang et al., 2024), Lara (Chen et al., 2024), LGM (Tang et al., 2024), and native 3D diffusion models: Shape-E (Jun & Nichol, 2023) and LN3Diff-image (Lan et al., 2024). Quantitatively, we benchmark rendering metrics with FID (Heusel et al., 2017), KID (Binkowski et al., 2018), and MUSIQ (Ke et al., 2021; Zhou et al., 2022). For 3D quality metrics, we adopt Point cloud FID (P-FID), Point cloud KID (P-KID), Coverage Score (COV), and Minimum Matching Distance (MMD) as the metrics. Following previous works Nichol et al. (2022); Zhang et al. (2023a); Yariv et al. (2024), we adopt the pre-trained PointNet++ provided by Point-E (Nichol et al., 2022) for calculating P-FID and K-FID. Qualitatively, GSO (Downs et al., 2022; Zheng & Vedaldi, 2023) dataset is used for visually inspecting image-conditioned generation results. Evaluating Text-to-2D Generation. Regarding text-conditioned 3D generation methods, we compare against Point-E (Nichol et al., 2022), Shape-E (Jun & Nichol, 2023), 3DTopia (Hong et al., 2024a), and LN3Diff-text (Lan et al., 2024). CLIP score (Radford et al., 2021) is reported following the previous works (Lan et al., 2024; Hong et al., 2024a). 5.2 EVALUATION Image-to-3D Generation. Our proposed framework enables 3D generation given single-view image conditions, leveraging the architecture detailed in Fig. 3 (b). Following current method (Chen et al., 2024; Tang et al., 2024), we qualitatively benchmark our method in Fig. 4 over the singleview 3D reconstruction task on the unseen images from the GSO dataset. Our proposed framework is robust to inputs with complicated structures (row 1-2), low-light (row 3), and self-occlusion (row 4), yielding consistently intact 3D reconstruction. Besides, our generative-based method shows more natural back-view reconstruction, as opposed to regression-based methods that are commonly blurry on uncertain areas (row 5). Quantitatively, we showcase the evaluation in Tab. 1. As can be seen, our proposed method achieves state-of-the-art performance over all 3D metrics, and shows competitive results over conventional 9 Prepreint, work in progress Table 1: Quantitative evaluation of image-conditioned 3D generation. Here, quality of both 2D rendering and 3D shapes is evaluated. As shown below, the proposed method demonstrates strong performance across all metrics. Although multi-view images-to-3D approaches like LGM achieves better performance on the FID/KID metrics, they fall short on more advanced image quality assessment metrics such as MUSIQ and performs significantly worse in 3D shape quality. For multiview to 3D methods, we also include the number of input views (V=#). Method OpenLRM Splatter-Image One-2-3-45 (V=12) CRM (V=6) Lara (V=4) LGM (V=4) Shape-E LN3Diff Ours FID 38.41 48.80 88.39 45.53 43.74 19. 138.53 29.08 24.21 KID(%) MUSIQ 1.87 3.65 6.34 1.93 1.95 0.55 11.95 0.89 0.76 45.46 30. 59.02 64.10 39.37 54.78 31.51 50.39 65.17 P-FID 35.74 19.72 72.40 35.21 32.37 40.17 20.98 27.17 8.72 P-KID(%) COV(%) MMD() 39.33 37. 29.08 30.69 12.60 7.03 30.83 13.19 12.44 19.45 7.41 10.02 3.22 33.33 38.83 39.33 50.83 61.33 55.17 59. 35.09 28.91 28.84 22.06 19.17 19.94 15.48 2D rendering metrics FID/KID. Note that LGM leverages pre-trained MVDream (Shi et al., 2023b) as the first-stage generation, and then maps the generated 4 views to pixel-aligned 3D Gaussians. This cascaded pipeline achieves better visual quality, but prone to yield distorted 3D geometry, as visualized in Fig. 4. Text-to-3D Generation. We demonstrate the text-to-3D generation performance in Fig. 5 and Tab. 2. The diffusion model trained on GAUSSIANANYTHINGs latent space has demonstrated high-quality text-to-3D generation of generic 3D objects, yielding superior performance in terms of object structure, textures, and surface normals. Quantitatively, our proposed method achieves better text-3D alignment against competitive baselines. 3D-aware Editing. Compared to existing methods that use unstructured tokens for 3D diffusion learning (Jun & Nichol, 2023), our proposed point-cloud structured latent space naturally facilitates geometry-texture disentanglement and allows for interactive 3D editing. As visualized in Fig. 6, given the text-conditioned generated point cloud z0 by ϵx Quantitative EvaluaTable 2: tion on Text-to-3D. The proposed method outperforms existing methods on CLIP scores over two different backbones. Method ViT-B/32 ViT-L/14 Point-E Shape-E LN3Diff 3DTopia Ours 26.35 27.84 29.12 30.10 31. 21.40 25.84 27.80 28.11 29.38 Θ, we sample the final 3D objects with ϵh Θ with Figure 5: Qualitative Comparison of Text-to-3D. We present text-conditioned 3D objects generated by GAUSSIANANYTHING, displaying two views of each sample. The top section compares our results with baseline methods, while the bottom shows additional samples from our method along with their geometry maps. Our approach consistently yields better quality in terms of geometry, texture, and text-3D alignment. Prepreint, work in progress Figure 6: 3D editing. Given two text prompts, we generate the corresponding point cloud z0,x with stage-1 diffusion model with ϵx Θ, and the corresponding point cloud features z0,h can be further generated with ϵh Θ. As can be seen, the samples from stage-2 are consistent in overall 3D structures but with diverse textures. Thanks to the proposed Point Cloud-structured Latent space, our method supports interactive 3D structure editing. This is achieved by first modifying the stage-1 point cloud z0,x 0,x, and then regenerate the 3D object with the same Gaussian noise. different random seed. As can be seen, the generated 3D objects maintain consistent structure layout while yielding diverse textures. Besides, by directly manipulating the conditioned point cloud zx,0, our proposed method enables interactive 3D editing, as in 2D models (Pan et al., 2023; Mou et al., 2023b). This functionality greatly facilitates the 3D content creation process for artists and opens up new possibilities for 3D editing with diffusion models. 5.3 ABLATION STUDY AND ANALYSIS Table 3: Ablation of 3D VAE Design. We ablate the design of our 3D VAE. Input-side, leveraging multi-view RGB-D-N renderings shows superior performance against dense point cloud. Besides, adding Gaussian up-sampling modules leads to consistent performance gain. Design Dense PCD as Input Multi-view RGB-D as Input LPIPS@100K 0.174 0.163 Table 4: Gaussian Utilization Ratio. We compare the effective Gaussians (opacity > 0.005) used during splatting here. Pixel-aligned Gaussian prediction methods waste large portion of Gaussians when representing 3D object due to white background and multi-view overlap, while our proposed Gaussian predictions yields more compact reconstruction results. Method High-opacity Gaussians (%) + Normal Map + Gaussian SR Module + 3 Gaussian SR Module 0.157 0.095 0.067 Splatter Image LGM Ours 17.14 52.63 96.84 3D VAE Design. In Tab. 3, we benchmark each component of our 3D VAE architecture over subset of Objaverse with 50K instances and record the LPIPS at 100K iterations. As shown in Tab. 3, our input design performs better against dense (16, 384) colored point cloud (Zhang et al., 2024), and the reconstruction quality consistently improves by including normal map as input and cascading more Gaussian upsampling blocks. Gaussian Utilization Ratio. Besides, we showcase high Gaussian utilization ratio of our proposed method. Specifically, we calculate the ratio of Gaussians with an opacity greater than 0.005 as effective Gaussians, as they contribute well to the final rendering. We calculate the statistics over 50K 3D instances. As shown in Tab. 4, our proposed Gaussian prediction framework achieves much higher utilization ratio. On the contrary, pixel-aligned Gaussian prediction models waste noticeable portion of Gaussians on the overlapping views and white backgrounds. Effectiveness of Cascaded 3D Diffusion. We qualitatively ablate the cascaded 3D diffusion choice in Fig. 7 (a), where single text-conditioned DiT is trained to synthesize the 3D point cloud and features jointly. Clearly, the jointly trained model has worse texture with 3D shape artifacts to our cascaded design. Besides bringing better editing capability as shown in Fig. 6, our cascaded design enables more flexible training, where the models of two stages can be trained in parallel. 11 Prepreint, work in progress Figure 7: Qualitative ablation of Cascaded diffusion and latent space editing. We first show the effectiveness of our two-stage cascaded diffusion framework in (a). Compared to Fig. 5, the singlestage 3D diffusion yields worse texture details and 3D structure intactness. In (b), we validate the latent point cloud editing yields less 3D artifacts compared to direct 3D editing on the 3D Gaussians. 3D Editing on the 3D Latent Space. Finally, we ablate the 3D editing performance in Fig. 7 (b). As can be seen, direct editing on the final Gaussians leads to 3D artifacts, while editing on our 3D latent space yields more holistic and cleaner results."
        },
        {
            "title": "6 CONCLUSION AND DISCUSSIONS",
            "content": "In this work, we present new paradigm of 3D generative model by learning the diffusion model over interactive 3D latent space. dedicated 3D variational autoencoder encodes multi-view 3D attributes renderings into point-cloud structured latent space, where multi-modal diffusion learning can be efficiently performed. Our framework achieves superior performance over both textand image-conditioned 3D generation, and potentially facilitates numerous downstream applications in 3D vision and graphics tasks. Limitations and Future Work. Our method comes with some limitations to be resolved. 3D VAE side, we observe that the reconstruction textures are sometimes blurry on 3D objects with intricate textures. potential solution is to leverage pixel-aligned features (Saito et al., 2019; Lan et al., 2023a; Melas-Kyriazi et al., 2023) to alleviate this issue. Moreover, incorporating rendering loss (Anciukeviˇcius et al., 2023) during diffusion training may further boost the generation quality. Besides, adding more real-world data such as MVImageNet Yu et al. (2023) and more control conditions Zhang et al. (2023b) is also worth exploring. 12 Prepreint, work in progress"
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Titas Anciukeviˇcius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy Mitra, and Paul Guerrero. RenderDiffusion: Image diffusion for 3D reconstruction, inpainting and generation. In CVPR, 2023. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-NeRF: multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021. Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. Mikołaj Binkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. In ICLR, 2018. Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022. Anpei Chen, Haofei Xu, Stefano Esposito, Siyu Tang, and Andreas Geiger. Lara: Efficient largebaseline radiance fields. In ECCV, 2024. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS, 2022. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In ICML, 2023. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items. In ICRA, 2022. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, and Thomas Funkhouser. Local deep implicit functions for 3D shape. In CVPR, 2020. 13 Prepreint, work in progress Markus Gross and Hanspeter Pfister. Point-based graphics. Elsevier, 2011. Zexin He and Tengfei Wang. OpenLRM: Open-source large reconstruction models. https:// github.com/3DTopia/OpenLRM, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. Jonathan Ho. Classifier-free diffusion guidance. In NeurIPS, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim arXiv preprint Salimans. Cascaded diffusion models for high fidelity image generation. arXiv:2106.15282, 2021. Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Tengfei Wang, Liang Pan, Dahua Lin, and Ziwei Liu. 3dtopia: Large text-to-3d generation model with hybrid diffusion priors. arXiv preprint arXiv:2403.02234, 2024a. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. In ICLR, 2024b. Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery, 2024a. doi: 10.1145/3641519.3657428. Zixuan Huang, Justin Johnson, Shoubhik Debnath, James Rehg, and Chao-Yuan Wu. Pointinfinity: Resolution-invariant point diffusion models. In CVPR, 2024b. Heewoo Jun and Alex Nichol. Shap-E: Generating conditional 3D implicit functions. arXiv preprint arXiv:2305.02463, 2023. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NIPS, 2022. Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In ICCV, pp. 51485157, 2021. Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):114, 2023. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. arXiv, 2013. Adam R. Kosiorek, Heiko Strathmann, Daniel Zoran, Pol Moreno, Rosalia Schneider, Sovna Mokra, and Danilo Jimenez Rezende. NeRF-VAE: geometry aware 3D scene generative model. ICML, 2021. Yushi Lan, Xuyi Meng, Shuai Yang, Chen Change Loy, and Bo Dai. E3dge: Self-supervised geometry-aware encoder for style-based 3D gan inversion. In CVPR, 2023a. Yushi Lan, Feitong Tan, Di Qiu, Qiangeng Xu, Kyle Genova, Zeng Huang, Sean Fanello, Rohit Pandey, Thomas Funkhouser, Chen Change Loy, and Yinda Zhang. Gaussian3Diff: 3D gaussian diffusion for 3D full head synthesis and editing. arXiv, 2023b. Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and Chen Change Loy. LN3Diff: Scalable latent neural fields diffusion for speedy 3D generation. In ECCV, 2024. Christoph Lassner and Michael Zollhofer. Pulsar: Efficient sphere-based neural rendering. In CVPR, 2021. 14 Prepreint, work in progress Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: framework for attention-based permutation-invariant neural networks. In ICML, 2019. Weiyu Li, Jiarui Liu, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. CraftsMan: High-fidelity mesh generation with 3D native generation and interactive geometry refiner, 2024. Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. WACV, 2023. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3D mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928, 2023a. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3D object, 2023b. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023c. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3D: Single image to 3D using cross-domain diffusion. In CVPR, 2024. Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3D captioning with pretrained models. arXiv preprint arXiv:2306.07279, 2023. Tiange Luo, Justin Johnson, and Honglak Lee. View selection for 3D captioning via diffusion ranking. arXiv preprint arXiv:2404.07984, 2024. Zhaoyang Lyu, Jinyi Wang, Yuwei An, Ya Zhang, Dahua Lin, and Bo Dai. Controllable mesh generation through sparse latent point diffusion models. In CVPR, 2023. Zhaoyang Lyu, Ben Fei, Jinyi Wang, Xudong Xu, Ya Zhang, Weidong Yang, and Bo Dai. Getmesh: controllable model for high-quality mesh generation and manipulation, 2024. Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. 2024. Luke Melas-Kyriazi, Christian Rupprecht, and Andrea Vedaldi. PC2: Projection-conditioned point cloud diffusion for single-image 3d reconstruction, 2023. URL https://arxiv.org/abs/ 2302.10668. Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Diffeditor: Boosting accuracy and flexibility on diffusion-based image editing. arXiv preprint arXiv:2402.02583, 2023a. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023b. Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with multiresolution hash encoding. TOG, 41(4):102:1102:15, July 2022. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-E: system for generating 3D point clouds from complex prompts, 2022. 15 Prepreint, work in progress Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision, 2023. Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag Your GAN: Interactive point-based manipulation on the generative image manifold. In SIGGRAPH, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. Hanspeter Pfister, Matthias Zwicker, Jeroen Van Baar, and Markus Gross. Surfels: Surface elements as rendering primitives. In PACMCGIT, 2000. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. ICLR, 2022. Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. PointNet: Deep learning on point sets for 3D classification and segmentation. arXiv, 2016. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. PointNet++: Deep hierarchical feature learning on point sets in metric space. In NeurIPS, 2017. Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, Zilong Dong, Liefeng Bo, and Xiaoguang Han. Richdreamer: generalizable normal-depth diffusion model for detail richness in text-to-3d. arXiv preprint arXiv:2311.16918, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya In ICML, Sutskever. Learning transferable visual models from natural language supervision. 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2022. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned implicit function for high-resolution clothed human digitization. In ICCV, October 2019. Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi. Scene Representation Transformer: Geometry-free novel view synthesis through set-latent scene representations. CVPR, 2022. Mehdi S. M. Sajjadi, Aravindh Mahendran, Thomas Kipf, Etienne Pot, Daniel Duckworth, Mario Luˇcic, and Klaus Greff. RUST: Latent Neural Scene Representations from Unposed Imagery. CVPR, 2023. Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. In arXiv, 2023a. Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3D generation. arXiv:2308.16512, 2023b. Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B. Tenenbaum, and Fredo DuIn rand. Light field networks: Neural scene representations with single-evaluation rendering. NeurIPS, 2021. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Prepreint, work in progress Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3D reconstruction. In arXiv, 2023. Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. Neural geometric level of detail: Real-time rendering with implicit 3D shapes. In CVPR, 2021. Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In NeurIPS, 2020. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In ECCV, 2024. Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P. Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas A. Funkhouser. IBRNet: Learning Multi-View Image-Based Rendering. In CVPR, 2021. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3D generation with variational score distillation. In NeurIPS, 2023. Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. CRM: Single image to 3D textured mesh with convolutional reconstruction model. In ECCV, 2024. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview compressive coding for 3D reconstruction. arXiv preprint arXiv:2301.08247, 2023. Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3D: Scalable image-to-3d generation via 3D latent diffusion transformer, 2024. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture, 2020. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-NeRF: Point-based neural radiance fields. In CVPR, 2022. Lior Yariv, Omri Puny, Natalia Neverova, Oran Gafni, and Yaron Lipman. Mosaic-sdf for 3d generative models. In CVPR, 2024. Wang Yifan, Felice Serena, Shihao Wu, Cengiz Oztireli, and Olga Sorkine-Hornung. Differentiable surface splatting for point-based geometry processing. ACM Transactions on Graphics (proceedings of ACM SIGGRAPH ASIA), 38(6), 2019. Xu Yinghao, Shi Zifan, Yifan Wang, Chen Hansheng, Yang Ceyuan, Peng Sida, Shen Yujun, and Wetzstein Gordon. GRM: Large gaussian reconstruction model for efficient 3d reconstruction and generation, 2024. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. PixelNeRF: Neural radiance fields from one or few images. In CVPR, 2021. Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. Pu-net: Point cloud upsampling network. In CVPR, 2018. Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang, Guanying Chen, Shuguang Cui, and Xiaoguang Han. MVImgNet: large-scale dataset of multi-view images. In CVPR, 2023. 17 Prepreint, work in progress Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3D shape generation. In NeurIPS, 2022. Biao Zhang, Jiapeng Tang, Matthias Nießner, and Peter Wonka. 3DShape2VecSet: 3d shape representation for neural fields and generative diffusion models. ACM Trans. Graph., 42(4), jul 2023a. ISSN 0730-0301. doi: 10.1145/3592442. Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. CLAY: controllable large-scale generative model for creating high-quality 3D assets. ACM Transactions on Graphics, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023b. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In CVPR, 2021. Chuanxia Zheng and Andrea Vedaldi. Free3d: Consistent novel view synthesis without 3d representation. arXiv, 2023. Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. In NeurIPS, 2022. 18 Prepreint, work in progress"
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "A.1 TRAINING DETAILS VAE Architecture. For the convolutional encoder Eϕ, we adopt lighter version of LDM Rombach et al. (2022) encoder with channel 64 and 1 residual blocks for efficiency. When training on Objaverse with = 8, we incorporate 3D-aware attention Shi et al. (2023b) in the middle layer of the convolutional encoder. The multi-view transformer architecture is similar to RUST (Sajjadi et al., 2023; 2022). For each upsampler Dk , we have 2 transformer blocks in the middle. All hyperparameters remain at their default settings. Regarding the transformer decoder DT , we employ the DiT-B/2 architecture due to VRAM constraints. Compared to LN3Diff (Lan et al., 2024), we do not adopt cross-plane attention in the transformer decoder. Diffusion Model. We mainly adopt the diffusion training pipeline implementation from SiT Ma et al. (2024), with pred-v objective, GVP schedule, and uniform sampling. ODE solver with 250 steps is used for all the results shown in the paper. For the DiT architecture with cross attention and single-adaLN-zero design, we mainly refer to PixArt Chen et al. (2023). The diffusion transformer is built with 24 layers with 16 heads and 1024 hidden dimension, which result in 458M parameters. For all the diffusion models, we further add the global token to features as part of the condition input. A.2 DATA AND BASELINE COMPARISON Training Data. For Objaverse, we use high-quality subset from the pre-processed rendering from G-buffer Objaverse Qiu et al. (2023) for experiments. Since G-buffer Objaverse splits the subset into 10 general categories, we use all the 3D instances except from Poor-quality: Human-Shape, Animals, Daily-Used, Furniture, Buildings&Outdoor, Transportations, Plants, Food and Electronics. The ground truth camera pose, rendered multi-view images, normal, depth maps, and camera poses are used for stage-1 VAE training. Details about Baselines. We use the official released code and checkpoint for all the comparisons shown in the paper. For the evaluation on the GSO dataset, we use the rendering provided by Free3D (Zheng & Vedaldi, 2023). Evaluation details. For quantitative benchmark in Tab. 1, we use 600 instances from Objaverse with ground truth 3D mesh for evaluation. To calculate the visual metrics (FID/KID/MUSIQ), we use the first rendered instance as the image condition and render 24 images with fixed elevation (+15 degrees) with uniform azimuths trajectory (24 15 degrees) with radius= 1.8. For 3D metrics, we export the extracted 3D mesh and sample 4096 points using FPS sampling on the mesh surface. The ground truth surface point cloud is processed in the same way. The pre-trained PointNet++ model from Point-E is used for P-FID and P-KID evaluation. All generated 3D models are aligned into the same canonical space before 3D metrics calculation. All intermediate results of the baselines for evaluation will be released."
        },
        {
            "title": "B MORE VISUAL RESULTS AND VIDEOS",
            "content": "Please check our supplementary video demo and attached folders for more results. Broader Social Impact. In this paper, we introduce new latent 3D diffusion model designed to produce high-quality surfel Gaussians using single model. As result, our approach has the potential to be applied to generating DeepFakes or deceptive 3D assets, facilitating the creation of falsified images or videos. This raises concerns as individuals could exploit such technology with malicious intent, aiming to spread misinformation or tarnish reputations."
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University, Singapore",
        "Shanghai Artificial Intelligence Laboratory",
        "Peking University"
    ]
}