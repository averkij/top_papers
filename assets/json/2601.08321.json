{
    "paper_title": "UM-Text: A Unified Multimodal Model for Image Understanding",
    "authors": [
        "Lichen Ma",
        "Xiaolong Fu",
        "Gaojing Zhou",
        "Zipeng Guo",
        "Ting Zhu",
        "Yichun Liu",
        "Yu Shi",
        "Jason Li",
        "Junshi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance."
        },
        {
            "title": "Start",
            "content": "UM-Text: Unified Multimodal Model for Image Understanding and Visual Text Editing Lichen Ma1*, Xiaolong Fu1*, GaojingZhou1, Zipeng Guo1,2, Ting Zhu1, Yichun Liu1, Yu Shi1, Jason Li1, Junshi Huang1 1JD.COM 2Sun Yat-sen University {malichen2020, fxlcumt, junshi.huang}@gmail.com 6 2 0 2 3 1 ] . [ 1 1 2 3 8 0 . 1 0 6 2 : r Abstract With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance."
        },
        {
            "title": "Introduction",
            "content": "Visual text editing and generation play crucial role in various applications, such as poster design, scene text editing, and the novel task of cross-language image translation. The main challenge of these tasks lies in manual design of text layout, attributes (e.g., font type, size, color), language (e.g., English, Chinese), and visual context (e.g., poster, product image), which are cumbersome and error-prone. In this paper, we propose method that enables users to perform visual text editing via natural language instructions. Given an input image and editing command, our model automatically *These authors contributed equally. Corresponding Author. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Figure 1: Illustration of traditional framework of visual text generation and three additional generation patterns of our method. Please note that the text content, layout and implicit attributes can be adaptively generated by VLM according to instruction. generates compelling text content, appropriate layout, and visually harmonious text images with implicit text attributes. Recently, with the rapid advancement of text-to-image (T2I), diffusion models have enabled the creation of highly realistic images with only instructions. For example, Stable Diffusion 3(Esser et al. 2024), FLUX.1(Labs 2024a) and FLUX.1 Kontext(Labs et al. 2025a) has gradually improved its capabilities in general image generation and visual text rendering. However, these commonly used T2I models are still deficient in generating complex characters such as handwriting or art text. Many researchers inject enhanced text generation capabilities into pre-trained diffusion models using various approaches (Tuo et al. 2023; Tuo, Geng, and Bo 2024; Chen et al. 2023b, 2024), but manual interactions are still required for text content and layout. In visual text generation, text layout plays an important role in generating an appropriate result. Some text generation methods, such as TextDiffuser2(Chen et al. 2024), UniGlyph(Wang et al. 2025a), and GlyphDraw2(Ma et al. 2025), use the task-specific large language model (LLM) to predefine text positions. DesignDiffusion(Wang et al. 2025c) even achieves the layout and visual text image in an end-to-end manner. However, there approaches are still infeasible for visual text editing task, which requires the coordinates of the target text in image. Moreover, the potential of text editing task should be further explored to generate more harmonious and aesthetic visual text images. In this paper, we propose holistic framework that inteFigure 2: Some results produced by our UM-Text, presenting its powerful effects on tasks such as image editing, image translation, and poster design. Please note that the bounding boxes of text are adaptively generated by UM-Text model. grates multimodal understanding into the process of visual text generation and editing for implicit learning of text layout and attributes. As illustrated in Fig.1, our framework can support four different text generation and editing patterns, where the reference image and instruction information is extracted as context embeddings for visual text generation/editing. Furthermore, we introduce UM-Encoder, module for multiple condition aggregation that incorporates T5 embeddings, character-level visual embeddings, and context embeddings. To improve the accuracy of text glyphs, the regional consistency loss in both latent and RGB space are proposed for better glyph supervision. We also contribute the UM-DATA-200K dataset containing 200k diverse image pairs with/without visual text for the pre-training of VLM. In summary, our contributions are as follows. We propose an innovative framework named UM-Text, which combines unified multimodal understanding and image editing model. With three-stage training strategy and region-based losses, UM-Text allows flexible visual text generation and editing by simple natural language instructions. We introduce UM-Encoder, novel module for multiple condition aggregation that integrates text embedding, character-level visual embeddings, and multimodal embeddings. With this module, the implicit attributes and layout of text are adaptively generated for visual text generation and editing. We contribute dataset called UM-DATA-200K with manual annotation for visual text generation and editing. Extensive experiments demonstrate the effectiveness of our dataset and framework."
        },
        {
            "title": "2 Related Work\nImage Generation and Understanding Diffusion mod-\nels have become the primary method for high-quality image",
            "content": "synthesis, offering powerful capabilities in terms of photorealism, fidelity, and diversity. From DDPM (Ho, Jain, and Abbeel 2020) and DDIM (Song, Meng, and Ermon 2020) to Latent Diffusion Models (LDM) (Podell et al. 2023; Rombach et al. 2022; Tian et al. 2024), these models improve generation efficiency and scalability by operating directly within the latent embedding space, enabling image synthesis with higher resolution at lower computational costs. With the introduction of architectures such as DiT (Peebles and Xie 2023; Esser et al. 2024) and FLUX (Labs 2024a), diffusion models have made significant advances in generalization and image quality, laying solid foundation for unified handling of multimodal conditions and becoming an important architecture in modern image generation (Labs et al. 2025b; Mou et al. 2024; Zhang, Rao, and Agrawala 2023). Despite these advancements, diffusion models still face significant challenges in understanding textual and visual information, highlighting the need to introduce VLM. VLM (Bai et al. 2025; Team et al. 2025; Zhu et al. 2025; Open AI. 2024; Team et al. 2023) have made significant progress in vision-language understanding tasks. Models like Gemini (Team et al. 2024), Janus-Pro (Chen et al. 2025b), Mogao (Liao et al. 2025), BAGEL (Zhang et al. 2025b) and Nexus-Gen (Zhang et al. 2025a) further unify understanding and generation. Recent works such as MetaQueries (Pan et al. 2025), BLIP3o (Chen et al. 2025a), UniWorld-V1 (Lin et al. 2025), OmniGen2 (Wu et al. 2025), and Step1X-Edit (Liu et al. 2025) integrate VLMs into image generation via multimodal conditioning, exploring control mechanisms and latent-level fusion with diffusion models. However, these methods still face limitations in text rendering: most of them only support English and struggle with editing fine-grained textual regions. Visual Text Generation and Editing In recent years, there have been substantial developments in the tasks of T2I Figure 3: The framework of UM-Text for multi-lingual visual text generation and editing. The UM-Encoder integrates multiple modality embeddings as the condition of visual text generation. The mask in input and loss function is transformed from the predicted layout of UM-Designer. Please note our single model supports diverse downstream applications based on the instructions. generation and image editing with visual text rendering. The goal of visual text generation and editing models is to produce accurate text images where the visual elements and text layout are harmoniously integrated. Text embeddings and various loss functions are employed to help the model generate more precise text. DrawText(Liu et al. 2022) demonstrates that characteraware models consistently outperform their character-blind counterparts across various text rendering tasks. GlyphByT5(Liu et al. 2024a,b) and FLUX-Text(Lan et al. 2025) introduce method utilizing box-level contrastive learning to align text features extracted from the language model with those derived from the visual encoder. In DiffUTE(Chen et al. 2023a) and GlyphDraw(Ma et al. 2023), glyph images are directly incorporated into the text embeddings. AnyText(Tuo et al. 2023), AnyText2(Tuo, Geng, and Bo 2024), and GlyphDraw2(Ma et al. 2025) render glyph line containing multiple characters into an image, encode glyph information using pretrained OCR recognition model, and inject it into the text embedding. Unfortunately, its challenging to represent multiple characters with single token, and theres lack of embedding for image content. To address these issues, we propose UM-Encoder for text embedding injection that integrates T5 embeddings, character-level visual embeddings, and VLM embeddings. This approach enables the model to generate more accurate text while achieving better stylistic consistency with the reference image. Several T2I methods employ large language models (LLMs) for layout prediction. TextDiffuser has adopted Layout Transformer that autoregressively outputs bounding boxes for keywords in an encoder-decoder manner. GlyphDraw2 and TextDiffuser2 further leverage LLMs to generate layouts. However, these methods simply learn layout information from the text modality and cannot be directly applied to visual text editing tasks. To overcome this limitation, we propose UM-Designer, VLM that can simultaneously generate layouts and text related to the reference image."
        },
        {
            "title": "3 Methodology\nIn this section, we present the details of UM-Text. We be-\ngin to introduce the construction process of UM-DATA-\n200K in Sec.3.1, which is a large-scale synthetic dataset\ndesigned to pretrain the UM-Designer with capabilities in\nlayout planning and text content generation. In Sec.3.2, we\npresent the framework of our UM-Text for visual text gen-\neration and editing tasks. Subsequently, Sec.3.3 introduces\nthe UM-Encoder, which integrates various conditions into\nunified embeddings. In Sec.3.4, we propose a region-wise\nconsistency loss to ensure that the generated text is semanti-\ncally accurate and stylistically consistent with the reference\nimage. Finally, Sec.3.5 outlines our training strategy.",
            "content": "Figure 4: The illustration of three training stages for UM-Text optimization."
        },
        {
            "title": "3.1 Dataset Construction\nRecently, many layout planning and text content genera-\ntion datasets are limited in the scale or quality of collected\ndata. To address this gap, this paper endeavors to assemble a\nlarge-scale, high-quality dataset particularly tailored for lay-\nout planning, visual text generation, and editing tasks. Gen-\nerally, we crawled 40 million product posters from online e-\ncommerce platforms. To construct our dataset, we developed\nan advanced data pipeline including image aesthetics filter-\ning, object segmentation, OCR, image erasure, and manual\nannotation.",
            "content": "Specifically, we used the PPOCRv4(Cui et al. 2025) to extract text content and bounding boxes from images and employed Aesthetic Predictor V2.5 to rate the images. We utilized OCR results and aesthetic scores to filter five million images with detailed text layouts and contents. To achieve higher-quality images, we further applied SAM2(Ravi et al. 2024) to segment the main product, filtered out inconsistent text layouts, and used FLUX-Fill (Labs 2024b) to generate clean images based on the text layout. Ultimately, we selected 200k images, including various styles of main product and poster images."
        },
        {
            "title": "3.2 The Framework of UM-Text\nAs illustrated in Fig. 3, the main components of UM-Text\ninclude the UM-Encoder, the Diffusion Transformer, and\ntraining losses for optimization. Generally, UM-Designer is\nimplemented as a VLM to capture the semantic information\nof instruction and reference image for the prediction of text\ncontent, layout, and implicit attributes. In addition to the in-\nstruction embedding from T5 and visual embedding of ren-\ndering text images, these predicted results are adaptively se-\nlected as additional conditions for downstream tasks accord-\ning to the instruction, all of which constitute the conditions\nof diffusion model, named UM-Embedding ce.",
            "content": "In the flow-matching-based diffusion model, we use VAE encoder to extract the latent representations of input image Is, binary mask image Im from UM-Designer or manually designed layout, and condition image Ic = Is Im, resulting in z0, zm and zc, respectively. Subsequently, the diffusion algorithm progressively adds random noise to z0 at each time step t, resulting in series of noisy latent variables zt. Flow-based diffusion models employ neural network Vθ to predict the velocity field at each time step, with the objective of matching the models velocity field to the ideal velocity field that transports the data distribution along the diffusion process. This is achieved by minimizing the flow matching loss: LRF = Ezt,zm,zc,ce,tN (0,1)[V (zt, t) Vθ(zt, zm, zc, ce, t)2 2]. (1) where LRF denotes the calibrated flow matching loss, and (zt, t) is the target velocity field derived from the diffusion process."
        },
        {
            "title": "3.3 UM-Encoder\nCurrently, many ControlNet-like approaches(Ma et al. 2024;\nWang et al. 2025b; Zhao and Lian 2023; Chen et al. 2023a)\ntypically inject glyph image and text conditions into the\nmodel. However, the glyph image condition is highly sus-\nceptible to the pre-defined text attributes, which often harm\nits robustness. Some methods replace the text embedding\nwith line-level OCR embedding as text condition. However,\nthese approaches have some limitations: (1) The visual em-\nbedding from OCR model only encodes the visual text infor-\nmation, missing the detailed description of generated image.\n(2) Line-level visual embedding is insufficient for the repre-\nsentation of character stroke information. (3) The layout and\nattributes of visual text are designed without considering the\ncontext information of reference image.",
            "content": "To address these issues, we propose the UM-Encoder for comprehensive condition representation on instruction, reference image, and character-level glyph image. Specifically, we use pretrained VLM, known as UM-Designer, to capture the semantic information of instruction and reference image. The side information, including text content, layout, and attribute embeddings, for visual text generation/editing task is predicted by UM-Designer. To obtain fine-grained glyph information of text, we render the text content into glyph images in character-level, and use an OCR model to extract the visual embeddings of glyph images. Meanwhile, the output tokens of instruction and reference image are used as implicit attribute embeddings. We claim that those token embeddings are effective enough for implicit representation of text attributes due to the well-designed pre-training task of UM-Designer. More details can be found in Sec 3.5. FiFigure 5: Qualitative comparison of UM-Text and state-of-the-art models in visual text editing task. nally, the character-level visual embeddings, attribute embeddings, and instruction embeddings from T5 are aligned and concatenated into UM-Embedding as the condition embeddings of diffusion model."
        },
        {
            "title": "3.4 Regional Consistency Loss\nPrevious text generation methods often face challenges in\ngenerating correct strokes for complex characters, due to the\nlack of detailed supervision in nuanced glyph shapes. To\naddress this problem, we propose a Regional Consistency\nLoss (RC Loss) to constraint the structural consistency of\nvisual text in various spaces. Specifically, RC Loss receives\nthe mask image Im from either the UM-Designer prediction\nresult or manual annotation to localize the target regions,\nand calculate the L2 distance between prediction result and\nground-truth within target regions.",
            "content": "In our implementation, we design two types of RC Loss to constraint the structural consistency in both latent and RGB spaces. In the latent space, we calculate the RC Loss in the velocity field, which is analogous to the re-weighting strategy of flow matching loss. After that, we use VAE decoder to obtain the predicted image and use Canny edge detector to extract the edge maps of predicted image and input image. Therefore, the RC Loss can be simply calculated on the localized regions of both edge images. Formally, the RC Loss on latent and RGB spaces, denoted LRCL and LRCI respectively, can be formulated as: LRC = (cid:13) (cid:13)C( ˆI Im) C(Is Im) (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 + λ (cid:104) (zt, t) zm Vθ(zt, zm, zc, ce, t) zm2 2 (2) (cid:105) where ˆI is the predicted image, and C() denotes the Canny edge operator. The overall training loss is defined as follows: = LRF + βLRC. and λ, β are the hyper-parameters for balancing different losses. This dual-space regional consistency loss effectively (3) preserves stroke integrity in complex character generation while maintaining stability in the editing process. Notably, the RC Loss in latent space mitigates the dilution effect commonly observed in mask-based editing, where the gradients outside the mask dominate the optimization direction."
        },
        {
            "title": "3.5 Training Strategy",
            "content": "Based on the model architecture, we propose progressive three-stage training strategy to learn text editing model with context-aware designing capabilities. The training process is illustrated in Fig.4, including the pre-training of UMDesigner, pre-training of diffusion model, and semantic aligment between UM-Designer and diffusion model. The details are specified as follows. Stage 1: UM-Designer Pre-training. In this stage, we initialize our UM-Designer by the weights of Qwen2.5-VL and continue the training process on UM-DATA-200K dataset. Generally, this dataset contains various tasks, including layout planning, text content generation, text detection and recognition. These tasks simulate the process of visual text generation and editing, and thus enhance the capability of UM-Designer for image-text understanding. Stage 2: Diffusion Pre-training. In this stage, we initialize the text generation model with FLUX-Fill and train all parameters on public benchmark. This process enhances the foundational text generation capabilities for subsequent learning stage. Stage 3: Semantic Alignment. In this stage, we train the connector of UM-Encoder and diffusion model to establish the connection between condition representation and application. By further introducing VLM embedding, our UMEmbedding complements the vision-language understanding in large-scale text generation task, thereby enhances the glyph consistency and aesthetics of generated image. Overall, with the structural vision-language guidance, detailed visual condition, and powerful training strategy on unified framework, our UM-Text significantly improves the Methods Task Sen.ACC NED FID LPIPS Sen.ACC NED FID LPIPS English Chinese GlyphControl AnyText AnyText-2 FLUX-Fill AnyText AnyText-2 FLUX-Text UM-Text T2I Editing 0.5262 0.7239 0.8096 0.3093 0.6843 0.7915 0.8175 0.8553 0.7529 0.8760 0.9184 0.4698 0.8588 0.9100 0.9193 0.9395 43.10 33.54 33. 33.87 21.59 29.76 12.35 10.15 - - - 0.1582 0.1106 0.1734 0.0674 0.0656 0.0454 0.6923 0.7130 0.0292 0.6476 0.7022 0.7213 0.7988 0.1017 0.8396 0. 0.0625 0.8210 0.8420 0.8555 0.8866 49.51 31.58 27.94 29.93 20.01 26.52 12.41 10.50 - - - 0.1207 0.0943 0.1444 0.0487 0.0481 Table 1: Comparison on AnyText-benchmark dataset. capability of model to generate high-fidelity and harmonious images in text editing task."
        },
        {
            "title": "4.2 Dataset and Evaluation Metrics\nWe use UMT-DATA-200K to train the UM-Designer model\nfor layout and text design, and train UM-Text for visual text\ngeneration on AnyWord-3M (Tuo et al. 2023), which com-\nbines Wukong (Gu et al. 2022), LAION (Schuhmann et al.\n2021), and OCR-specific datasets (3M images). To ensure\na fair comparison, UMT-DATA-200K is not used for visual\ntext generation training in our experiments.",
            "content": "We evaluate on several public benchmarks following prior work. AnyWord-Benchmark (Tuo et al. 2023) includes 1,000 English and 1,000 Chinese images. TextSeg (Xu et al. 2021) and LAION-OCR (Chen et al. 2023b) provide 1,024 and 9.1M real-world text images, respectively. ICDAR13 (Karatzas et al. 2013) contributes 233 test images for text detection evaluation. Following DREAMTEXT, we randomly select 100 images from the test sets of TextSeg, LAIONOCR, and ICDAR13 for evaluation. AnyWord-Benchmark includes three evaluation metrics: Sentence Accuracy (Sen.ACC), Normalized Edit Distance (NED), and Frechet Inception Distance (FID) for distribution-level style similarity. We use Learned Perceptual Image Patch Similarity (LPIPS) to assess the consistency and realism of generated images, ensuring style consistency in edited regions while preserving non-target areas. All settings are consistent with FLUX-Text. Following DREAMTEXT, we use an off-the-shelf scene text recognition (STR) model to identify the rendered text and then evaluate word-level correctness using sequence accuracy (SeqAcc) by comparing the STR result with the ground truth."
        },
        {
            "title": "4.3 Experiment Result\nQuantitative Results We comprehensively evaluate UM-\nText and state-of-the-art methods using the AnyText-\nbenchmark, UDiffText benchmark, and our self-constructed\nUMT-benchmark. As shown in Table 1, on the AnyText-\nbenchmark, our method consistently outperforms compet-\ning approaches for both Chinese and English text across all\nmetrics, including OCR accuracy (Sen.ACC, NED) and re-\nalism (FID, LPIPS). As shown in Table 2, our method out-\nperforms previous approaches in SeqAcc and FID, although\nour LPIPS score is lower than DreamText’s. This may be\nbecause our method produces colors and textures that better\nmatch the image style during text reconstruction.",
            "content": "Our UM-Designer model is capable of designing both layout and text, which motivates us to propose the UMTbenchmark to evaluate the performance of the entire pipeline. The UM-Designer model can also be integrated with other text editing models, such as AnyText and AnyText2, to generate product posters from clean product images. For fair comparison, our generative model, like previous state-of-the-art methods, is trained on the AnyWord-3M dataset without using any product-specific data. As shown in Table 3, we compare the Sen.ACC and NED metrics for both Chinese and English, and our method significantly outperforms previous approaches on both metrics. Qualitative Results We conduct qualitative comparisons with state-of-the-art methods, including AnyText, AnyText2, and FLUX-Text, on both English and Chinese multi-line text scenarios, as illustrated in Fig.5. Our method demonstrates superior performance in generating accurate, coherent, and visually harmonious text that blends seamlessly with the background, under complex conditions for both languages. In contrast, AnyText and AnyText-2 frequently produce results with blurred characters, duplicated text, or even incorrect glyphs. FLUX-Text generates text that is visually inconsistent with the background, suffers from color distortion, and also exhibits glyph errors, particularly in complex Methods Task AnyText UDiffText DreamText UM-Text AnyText UDiffText DreamText UM-Text Recon Editing ICDAR13(8ch) ICDAR13 TextSeg LAION-OCR SeqAcc 0.89 0.94 0.95 0.99 0.81 0.84 0.87 0.93 0.87 0.91 0.94 0.98 0.79 0.83 0.89 0.93 0.81 0.93 0.96 0. 0.80 0.84 0.91 0.95 0.86 0.90 0.93 0.96 0.72 0.78 0.88 0.93 FID LPIPS 22.73 15.79 12.13 6. 0.0651 0.0564 0.0328 0.0479 - - - - - - - - Table 2: Comparison on the UDiffText benchmark dataset: The Recon task involves reconstructing text from the original image, while the Editing task focuses on modifying the text within the image. Methods Flux-Kontext Step1X-Edit OmniGen2 AnyText AnyText-2 UM-Text English Chinese Sen.ACC NED Sen.ACC NED 0.325 0.358 0.371 0.518 0.693 0.790 0.502 0.524 0.541 0.643 0.723 0.866 - - - 0.557 0.720 0. - - - 0.706 0.806 0.981 Table 3: Comparison on UMT-benchmark. Please note that all methods use the layout and text by UM-Designer. Figure 6: Compare UM-Text and ChatGPT4o in multi-turn image editing using natural language instructions, specifically in poster design, image editing, and image translation. Chinese text scenarios. Notably, our method maintains precise glyph integrity and strong background consistency, even in challenging multi-line text settings. Meanwhile, we also conducte multi-turn task comparison with ChatGPT-4o, as shown in Fig 6. Our method maintains precise glyph integrity and consistency with the background, even in complex multi-line scenarios, whereas ChatGPT-4o often introduces unnecessary text modifications."
        },
        {
            "title": "4.4 Ablation Study\nWe randomly sampled 100k images from the AnyWord-3M\ndataset, including 50k Chinese and 50k English images. To\nevaluate the contribution of each module in our method,\nwe conducte ablation studies on the AnyText-benchmark\nby training for 10 epochs on this small-scale dataset, as",
            "content": "shown in Table 4. We used FLUX-Fill as the baseline, which demonstrates lack of Chinese text generation capability. In addition, we compared the effect of adding character-level visual encoder, which significantly improved the text generation ability of the baseline. We verified that the VLM embedding further enhanced the accuracy of text generation. Finally, we evaluated the impact of LRCL and LRCI , which obtained an improvement of 4.8% and 4.2% respectively. Methods Baseline +Visual +VLM +RCL Loss +RCI Loss English Chinese Sen.ACC NED Sen.ACC NED 0.309 0.759 0.782 0.799 0.824 0.469 0.887 0.901 0.915 0.925 0.029 0.676 0.698 0.725 0.746 0.062 0.839 0.848 0.856 0.863 Table 4: Ablation experiments of UM-Text conducted on subset of the AnyWord-3M dataset."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we introduce UM-Text, novel unified multimodal method designed to accomplish complex visual text editing tasks via simple natural language instructions. We explore three-stage joint training strategy that integrates VLM and diffusion models, and propose the UMDesigner module for layout and text planning. Furthermore, we present the UM-Encoder, which fuses VLM embeddings, character-level visual embeddings, and T5 embeddings to enhance the models understanding of both scene images and text glyphs, thereby enabling accurate and styleconsistent editing and generation of textual and visual content. To supervise fine-grained visual text glyph information, we propose regional consistency loss. In addition, we contribute UM-DATA-200K, large-scale and diverse dataset of layouts and texts, as well as the UMT-benchmark for evaluating instruction-based visual text editing. Extensive qualitative and quantitative results demonstrate the superiority of our approach. References Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923. Chen, H.; Xu, Z.; Gu, Z.; Li, Y.; Meng, C.; Zhu, H.; Wang, W.; et al. 2023a. Diffute: Universal text editing diffusion model. Advances in Neural Information Processing Systems, 36: 6306263074. Chen, J.; Huang, Y.; Lv, T.; Cui, L.; Chen, Q.; and Wei, F. 2023b. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36: 93539387. Chen, J.; Huang, Y.; Lv, T.; Cui, L.; Chen, Q.; and Wei, F. 2024. Textdiffuser-2: Unleashing the power of language models for text rendering. In European Conference on Computer Vision, 386402. Springer. Chen, J.; Xu, Z.; Pan, X.; Hu, Y.; Qin, C.; Goldstein, T.; Huang, L.; Zhou, T.; Xie, S.; Savarese, S.; et al. 2025a. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568. Chen, X.; Wu, Z.; Liu, X.; Pan, Z.; Liu, W.; Xie, Z.; Yu, X.; and Ruan, C. 2025b. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811. Cui, C.; Sun, T.; Lin, M.; Gao, T.; Zhang, Y.; Liu, J.; Wang, X.; Zhang, Z.; Zhou, C.; Liu, H.; Zhang, Y.; Lv, W.; Huang, K.; Zhang, Y.; Zhang, J.; Zhang, J.; Liu, Y.; Yu, D.; and Ma, Y. 2025. PaddleOCR 3.0 Technical Report. arXiv:2507.05595. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Gu, J.; Meng, X.; Lu, G.; Hou, L.; Minzhe, N.; Liang, X.; Yao, L.; Huang, R.; Zhang, W.; Jiang, X.; et al. 2022. Wukong: 100 million large-scale chinese cross-modal pretraining benchmark. Advances in Neural Information Processing Systems, 35: 2641826431. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 68406851. Karatzas, D.; Shafait, F.; Uchida, S.; Iwamura, M.; Bigorda, L. G.; Mestre, S. R.; Mas, J.; Mota, D. F.; Almazan, J. A.; and De Las Heras, L. P. 2013. ICDAR 2013 robust reading competition. In 2013 12th international conference on document analysis and recognition, 14841493. IEEE. Labs, B. F. 2024a. FLUX. https://huggingface.co/blackforest-labs/FLUX.1-dev. Labs, B. F. 2024b. flux.1-fill. https://huggingface.co/blackforest-labs/FLUX.1-Fill-dev. Labs, B. F.; Batifol, S.; Blattmann, A.; Boesel, F.; Consul, S.; Diagne, C.; Dockhorn, T.; English, J.; English, Z.; Esser, P.; et al. 2025a. FLUX. 1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space. arXiv preprint arXiv:2506.15742. Labs, B. F.; Batifol, S.; Blattmann, A.; Boesel, F.; Consul, S.; Diagne, C.; Dockhorn, T.; English, J.; English, Z.; Esser, P.; et al. 2025b. FLUX. 1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space. arXiv preprint arXiv:2506.15742. Lan, R.; Bai, Y.; Duan, X.; Li, M.; Sun, L.; and Chu, X. 2025. Flux-text: simple and advanced diffusion transarXiv preprint former baseline for scene text editing. arXiv:2505.03329. Liao, C.; Liu, L.; Wang, X.; Luo, Z.; Zhang, X.; Zhao, W.; Wu, J.; Li, L.; Tian, Z.; and Huang, W. 2025. Mogao: An omni foundation model for interleaved multi-modal generation. arXiv preprint arXiv:2505.05472. Lin, B.; Li, Z.; Cheng, X.; Niu, Y.; Ye, Y.; He, X.; Yuan, S.; Yu, W.; Wang, S.; and Ge, Y. 2025. UniWorld: HighResolution Semantic Encoders for Unified Visual Understanding and Generation. Liu, R.; Garrette, D.; Saharia, C.; Chan, W.; Roberts, A.; Narang, S.; Blok, I.; Mical, R.; Norouzi, M.; and Constant, N. 2022. Character-aware models improve visual text rendering. arXiv preprint arXiv:2212.10562. Liu, S.; Han, Y.; Xing, P.; Yin, F.; Wang, R.; Cheng, W.; Liao, J.; Wang, Y.; Fu, H.; Han, C.; Li, G.; Peng, Y.; Sun, Q.; Wu, J.; Cai, Y.; Ge, Z.; Ming, R.; Xia, L.; Zeng, X.; Zhu, Y.; Jiao, B.; Zhang, X.; Yu, G.; and Jiang, D. 2025. Step1XEdit: Practical Framework for General Image Editing. arXiv preprint arXiv:2504.17761. Liu, Z.; Liang, W.; Liang, Z.; Luo, C.; Li, J.; Huang, G.; and Yuan, Y. 2024a. Glyph-byt5: customized text encoder for accurate visual text rendering. In European Conference on Computer Vision, 361377. Springer. Liu, Z.; Liang, W.; Zhao, Y.; Chen, B.; Liang, L.; Wang, L.; Li, J.; and Yuan, Y. 2024b. Glyph-byt5-v2: strong aesthetic baseline for accurate multilingual visual text rendering. arXiv preprint arXiv:2406.10208. Ma, J.; Deng, Y.; Chen, C.; Du, N.; Lu, H.; and Yang, Z. 2025. Glyphdraw2: Automatic generation of complex glyph posters with diffusion models and large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 59555963. Ma, J.; Zhao, M.; Chen, C.; Wang, R.; Niu, D.; Lu, H.; and Lin, X. 2023. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation. arXiv preprint arXiv:2303.17870. Ma, L.; Yue, T.; Fu, P.; Zhong, Y.; Zhou, K.; Wei, X.; and Hu, J. 2024. CharGen: High Accurate Character-Level Visual Text Generation Model with MultiModal Encoder. arXiv preprint arXiv:2412.17225. Mou, C.; Wang, X.; Xie, L.; Wu, Y.; Zhang, J.; Qi, Z.; and Shan, Y. 2024. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, 42964304. Open AI. 2024. gpt40. Accessed: 2025-06-05. Pan, X.; Shukla, S. N.; Singh, A.; Zhao, Z.; Mishra, S. K.; Wang, J.; Xu, Z.; Chen, J.; Li, K.; Juefei-Xu, F.; et al. Wang, Z.; Bao, J.; Gu, S.; Chen, D.; Zhou, W.; and Li, H. 2025c. Designdiffusion: High-quality text-to-design imIn Proceedings of age generation with diffusion models. the Computer Vision and Pattern Recognition Conference, 2090620915. Wu, C.; Zheng, P.; Yan, R.; Xiao, S.; Luo, X.; Wang, Y.; Li, W.; Jiang, X.; Liu, Y.; Zhou, J.; Liu, Z.; Xia, Z.; Li, C.; Deng, H.; Wang, J.; Luo, K.; Zhang, B.; Lian, D.; Wang, X.; Wang, Z.; Huang, T.; and Liu, Z. 2025. OmniGen2: Exploration to Advanced Multimodal Generation. arXiv preprint arXiv:2506.18871. Xu, X.; Zhang, Z.; Wang, Z.; Price, B.; Wang, Z.; and Shi, H. 2021. Rethinking text segmentation: novel dataset and text-specific refinement approach. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1204512055. Zhang, H.; Duan, Z.; Wang, X.; Zhao, Y.; Lu, W.; Di, Z.; Xu, Y.; Chen, Y.; and Zhang, Y. 2025a. Nexus-gen: unified model for image understanding, generation, and editing. arXiv preprint arXiv:2504.21356. Zhang, L.; Rao, A.; and Agrawala, M. 2023. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, 38363847. Zhang, X.; Guo, J.; Zhao, S.; Fu, M.; Duan, L.; Wang, G.- H.; Chen, Q.-G.; Xu, Z.; Luo, W.; and Zhang, K. 2025b. Unified multimodal understanding and generation models: arXiv preprint Advances, challenges, and opportunities. arXiv:2505.02567. Zhao, Y.; and Lian, Z. 2023. Udifftext: unified framework for high-quality text synthesis in arbitrary images arXiv preprint via character-aware diffusion models. arXiv:2312.04884. Zhu, J.; Wang, W.; Chen, Z.; Liu, Z.; Ye, S.; Gu, L.; Tian, H.; Duan, Y.; Su, W.; Shao, J.; et al. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. URL https://arxiv. org/abs/2504.10479, 9. 2025. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256. Peebles, W.; and Xie, S. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 41954205. Podell, D.; English, Z.; Lacey, K.; Blattmann, A.; Dockhorn, T.; Muller, J.; Penna, J.; and Rombach, R. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Ravi, N.; Gabeur, V.; Hu, Y. T.; Hu, R.; Ryali, C.; Ma, T.; Khedr, H.; Rdle, R.; Rolland, C.; and Gustafson, L. 2024. SAM 2: Segment Anything in Images and Videos. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Schuhmann, C.; Vencu, R.; Beaumont, R.; Kaczmarczyk, R.; Mullis, C.; Katta, A.; Coombes, T.; Jitsev, J.; and Komatsuzaki, A. 2021. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114. Song, J.; Meng, C.; and Ermon, S. 2020. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502. Team, G.; Anil, R.; Borgeaud, S.; Alayrac, J.-B.; Yu, J.; Soricut, R.; Schalkwyk, J.; Dai, A. M.; Hauth, A.; Millican, K.; et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Team, G.; Georgiev, P.; Lei, V. I.; Burnell, R.; Bai, L.; Gulati, A.; Tanzer, G.; Vincent, D.; Pan, Z.; Wang, S.; et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Team, K.; Du, A.; Yin, B.; Xing, B.; Qu, B.; Wang, B.; Chen, C.; Zhang, C.; Du, C.; Wei, C.; et al. 2025. Kimi-vl technical report. arXiv preprint arXiv:2504.07491. Tian, K.; Jiang, Y.; Yuan, Z.; Peng, B.; and Wang, L. 2024. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37: 8483984865. Tuo, Y.; Geng, Y.; and Bo, L. 2024. AnyText2: Visual Text Generation and Editing With Customizable Attributes. arXiv preprint arXiv:2411.15245. Tuo, Y.; Xiang, W.; He, J.-Y.; Geng, Y.; and Xie, X. 2023. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054. Wang, Y.; Han, C.; Jin, Z.; Li, X.; Du, S.; Tao, W.; Yang, Y.; Yuan, C.; Lin, L.; et al. 2025a. UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis. arXiv preprint arXiv:2507.00992. Wang, Y.; Zhang, W.; Xu, H.; and Jin, C. 2025b. DreamText: High Fidelity Scene Text Synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2855528563."
        }
    ],
    "affiliations": [
        "JD.COM",
        "Sun Yat-sen University"
    ]
}