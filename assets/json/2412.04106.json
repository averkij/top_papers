{
    "paper_title": "MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities",
    "authors": [
        "Haoning Wu",
        "Ziheng Zhao",
        "Ya Zhang",
        "Weidi Xie",
        "Yanfeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates a new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate a large-scale radiology image-text dataset, MedGen-1M, comprising modality labels, attributes, region, and organ information, along with a subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose a diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 6 0 1 4 0 . 2 1 4 2 : r MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities Haoning Wu1,2,3, Ziheng Zhao1,2,3, Ya Zhang1,3, Weidi Xie1,3, Yanfeng Wang1,3 1School of Artificial Intelligence, Shanghai Jiao Tong University, China 2CMIC, Shanghai Jiao Tong University, China 3Shanghai AI Laboratory, China Figure 1. Motivations and Overview. Left: The heterogeneity of MRI modalities poses challenges to the generalization of segmentation models. Our proposed data engine, MRGen, addresses this by controllably synthesizing training data, enabling segmentation towards mask-unannotated modalities. Right: (a) Previous generative models rely on mask-annotated data and are restricted to data augmention in training modalities; (b) Image translation typically requires registered data pairs (indicated by dashed lines), limited to conversions between specific modalities; (c) MRGen introduces new paradigm, enabling controllable generation across multiple modalities without relying on registered data pairs or mask-annotated data from target modalities. Different colors represent distinct modalities."
        },
        {
            "title": "Abstract",
            "content": "Medical image segmentation has recently demonstrated impressive progress with deep neural networks, yet the heterogeneous modalities and scarcity of mask annotations limit the development of segmentation models on unannotated modalities. This paper investigates new paradigm for leveraging generative models in medical applications: controllably synthesizing data for unannotated modalities, without requiring registered data pairs. Specifically, we make the following contributions in this paper: (i) we collect and curate large-scale radiology image-text dataset, *: These authors contribute equally to this work. : Corresponding author. 1 MedGen-1M, comprising modality labels, attributes, region, and organ information, along with subset of organ mask annotations, to support research in controllable medical image generation; (ii) we propose diffusion-based data engine, termed MRGen, which enables generation conditioned on text prompts and masks, synthesizing MR images for diverse modalities lacking mask annotations, to train segmentation models on unannotated modalities; (iii) we conduct extensive experiments across various modalities, illustrating that our data engine can effectively synthesize training samples and extend MRI segmentation towards unannotated modalities. The code, model, and data will be publicly available at https://haoningwu3639. github.io/MRGen/. 1. Introduction Medical image segmentation [21, 33, 43, 55] has achieved remarkable success, becoming cornerstone of intelligent healthcare. However, challenges such as data privacy, modality complexity, and the high cost of segmentation annotations significantly restrict its development towards unannotated modalities. As depicted in Figure 1 (left), these limitations are especially acute in MRI, which exhibits significant discrepancies across modalities. Magnetic Resonance Imaging (MRI) is non-invasive imaging technique widely recognized for its high-quality and radiation-free characteristics. However, MRI scans are often expensive, time-consuming, and sensitive to motion. Additionally, there is significant variability across different MRI modalities, and even within the same modality, differences in machines and scanning parameters can lead to substantial discrepancies in images. The complexity and variability of MRI, coupled with high annotation costs, lead to scarcity of registered data pairs and segmentation annotations, posing significant challenges to the development of robust segmentation models. In this paper, we aim to leverage the progress in generative models, specifically, diffusion models [18], to facilitate the training of segmentation models on unannotated MRI modalities. Generally speaking, there are three challenges when ap- (i) plying the generative models to MRI data synthesis: paradigm: as presented in Figure 1 (right), previous models [3, 13] primarily focus on generating data for modalities with annotations. This approach limits their scalability to under-annotated or unannotated MRI modalities; (ii) data: the scarcity of MR images and fine-grained annotations makes it challenging to develop general and controllable generative models; (iii) controllability: to better benefit downstream tasks with synthetic data, generative models must enable controllable generation, guided by conditions such as text prompts or segmentation masks, to produce training samples for downstream tasks. To tackle the above challenges, we collect and curate large-scale radiology image-text dataset, MedGen-1M, featuring CT and MRI of various modalities sourced from the Internet and open-source data. Our dataset comprises 1.2 million 2D slices with modality labels, attributes, region, and organ information, with subset also containing organ mask annotations. The extensive image-text pairs across diverse modalities provide foundation for training general MRI generation model, and the available mask annotations further enable more controllable generation. Building on this foundation, this paper introduces MRGen, diffusion-based data engine for controllable MRI synthesis, particularly, for modalities with no segmentation masks ever available. At training time, we employ twostage strategy: (i) text-guided pretraining on diverse, largescale radiology image-text pair data, enabling the model to generate images across various modalities based on text descriptions; and (ii) mask-conditioned finetuning on data with mask annotations, facilitating controllable generation based on organ masks. Consequently, such strategy even allows MRGen to extend its controllable generation abilities towards modalities that do not have segmentation annotations available. During inference, MRGen takes text prompts and organ masks as inputs, producing MR images aligned with the given modality, region, and organs. With such synthetic samples, we can thus facilitate MRI segmentation tools towards modalities without manual annotations. Our contributions in this paper can be summarized as follows: (i) we establish new paradigm for generative model applications in medical analysis, i.e., synthesizing data of unannotated modalities to serve as training samples for downstream segmentation models; (ii) we curate large-scale radiology image-text dataset, MedGen-1M, which features detailed modality labels, attributes, regions, organ information, and subset with organ mask annotations, providing foundation for building general and controllable image generation models in the radiology domain; (iii) we develop MRGen, diffusion-based data engine capable of generation conditioned on text prompts and masks, synthesizing MR images of various annotationscarce modalities; (iv) we conduct extensive experiments across diverse modalities, demonstrating that MRGen can controllably synthesize high-quality MR images, boosting segmentation performance on unannotated modalities. 2. Related works Generative models have been research focus in computer vision for years, with GANs [12] and diffusion models [18, 45] leading the advancements. These models have found extensive applications across various tasks, including text-to-image generation [24, 38, 42, 48], image-to-image translation [4, 22, 58], artistic creation [30, 44, 51], and even challenging video generation [10, 19]. Notably, CycleGAN [58] employs cycle-consistency loss to facilitate image translation with unpaired data, while the Stable Diffusion series [11, 39, 42] efficiently produces high-resolution images in latent space, earning broad recognition. Medical image synthesis aims to leverage generative models to improve medical image analysis models or tackle challenges such as data scarcity and privacy concerns [28]. Prior works train generative models on X-ray [3], CT [13], and brain MRI [8, 36], while methods like DiffTumor [5] and FreeTumor [49] focus specifically on generating tumor images to improve tumor segmentation. Existing works [3, 13] primarily focus on in-domain augmentation within mask-annotated training modalities, and often struggle to generalize to unseen and unannotated modalities. To this end, this paper explores novel paradigm for medical image generation, producing samples 2 for modalities lacking mask annotations, to train MRI segmentation models towards these challenging scenarios. Medical image segmentation has been long-standing research topic, with various architectures being proposed [15, 21, 34, 43, 56]. Recently, inspired by SAM [27, 41], largescale interactive segmentation models [9, 33, 55] have been developed for medical image analysis. However, the heterogeneity of MRI challenges the generalization of current segmentation models, which struggle with varying intensity distributions among diverse modalities. Existing methods generally address this problem by learning domaininvariant content, which either requires mixing training data from multiple domains, or relies on deliberately designed data augmentations strategies [6, 20, 37, 46, 50, 57]. Despite recent efforts in annotating segmentation masks for extensive MRI data [7, 14], building datasets with wide-ranging modalities, scanners and protocols is timeIn this paper, we exconsuming and labor-intensive. plore controllable generative models to synthesize maskconditioned MR images for any segmentation model, without tedious hyperparameter selection in data augmentation. 3. Data Curation To train our proposed data engine, we first collect and curate large-scale radiology dataset, termed MedGen-1M, containing CT and MR images with rich modality information, clinical attributes, and mask annotations. In the following, we elaborate on our data processing pipeline and present the statistics of the curated dataset. Data collection. We first collect large amount of abdominal CT and MR images from the Radiopaedia1 website, licensed under CC BY-NC-SA 3.02. This portion of data comprises wide variety of modalities, serving as the largescale image-text pairs ({(I, )}), with each sample containing an image and its modality label. As listed in Table 1, we also integrate CT and MR data with modality labels and abdominal organ mask annotations from several opensource datasets, to construct data triplets ({(I, , M)}), which will be further explained in Section 4. Automatic annotations. Given the wide variability in abdominal imaging, such as differences between the upper abdominal region and the pelvic region, relying solely on modality labels may not sufficiently distinguish these images. Thus, we divide the abdomen into six regions, including: Upper Thoracic Region, Middle Thoracic Region, Lower Thoracic Region, Upper Abdominal Region, Lower Abdominal Region, and Pelvic Region. We then use off-theshelf BiomedCLIP [52] to categorize all 2D slices into these six categories to serve as region information. 1radiopaeida.org 2To obtain the data, readers must first get permission from Radiopaedia, then we will share the download link."
        },
        {
            "title": "Dataset",
            "content": "Modality #Volumes #Slices #Masks Radiopaedia-MR Radiopaedia-CT LiQA [31] PanSeg [54] PROMISE12 [29] MSD-Prostate [2] CHAOS-MRI [25] MSD-Liver [2] AMOS22CT [23]"
        },
        {
            "title": "MR\nMR\nMR\nMR\nMR\nCT\nCT",
            "content": "5,414 8,734 205,039 812,756 30 766 50 64 60 131 300 2,185 33,360 1,377 1,204 1,917 58,638 41,430 1,446 13,779 778 366 1,492 19,163 30,"
        },
        {
            "title": "Total",
            "content": "CT & MR 15,499 1,156,529 186,525 Table 1. Dataset Statistics of MedGen-1M. Moreover, considering that even medical-specific text encoders may not clearly distinguish fine-grained correlations and differences between modalities like T1 and T2 [47, 53], we employ GPT-4 [1] to map all modality labels to free-text modality attributes that describe signal intensities of fat, muscle, and water. For example, T1 modality can be represented as fat high signal, muscle intermediate signal, water low signal. This aims to help the model understand the imaging characteristics among distinct modalities. Discussion. After the data processing mentioned above, we assemble our MedGen-1M dataset. As shown in Table 1, the dataset consists of approximately 16K 3D volumes, covering CT and over hundred MR modalities, totaling nearly 1.2M 2D slices. Each sample is paired with its corresponding modality labels, modality attributes, region, and organ information, with 190K samples featuring paired mask annotations. Compared to the existing datasets, the scale, modality diversity, and fine-grained annotations of MedGen-1M provide more information for training generative models tailored for medical imaging. More statistics will be detailed in Section B.2 of the Appendix. 4. Method We first formulate the problem in Section 4.1, followed by detailed description of the proposed MRGen architecture in Section 4.2; then we elaborate on the training details of our model in Section 4.3; lastly, we present the procedure of synthesizing and filtering samples with our data engine for downstream segmentation tasks in Section 4.4. 4.1. Problem Formulation In this paper, we focus on developing diffusion-based controllable data engine for medical imaging, particularly for synthesizing data across various modalities with no mask annotations available. Specifically, our proposed MRGen (ΦMRGen) generates the radiology image (I), conditioned on text prompt (T ) and the organs mask (M), i.e., 3 Figure 2. Architecture Overview. Developing our MRGen involves three key steps: (a) Train an autoencoder on various images from dataset Du; (b) Train text-guided generative model within the latent space, using image-text pairs across diverse modalities from Du, featuring modality, attributes, region, and organs information; (c) Train mask condition controller jointly on the mask-annotated sourcedomain dataset Ds and unannotated target-domain dataset Dt, enabling controllable generation based on both text prompts and masks. = ΦMRGen(T , M; Θ, Θc), where Θ and Θc refer to the trainable parameters of the foundation generative model and the mask condition controller, respectively. As described in Section 3, our large-scale dataset, i.e., = {Du, Ds, Dt}, encompasses three types of data, supporting the development of the proposed data engine. Concretely, the training process is carried out in two phases: (i) we pretrain text-guided generative model on the imagetext data (Du = {(Iu, Tu)}), covering diverse modalities; and (ii) we then train condition controller jointly on image-text pairs with mask annotations from sourcedomain dataset (Ds = {(Is, Ts, Ms)}), and image-text pairs from the target-domain dataset (Dt = {(It, Tt, )}). The ultimate goal is to generate target-domain images (I t), conditioned on text prompts (T t), to serve as training samples for segmentation models. Relations to existing tasks. This paper makes two key advancements over existing methods: (i) conventional generative augmentation methods are confined to generating samples within mask-annotated training modalities [3, 8, 13]. Conversely, our model can synthesize data for modalities lacking mask annotations, conditioned on text prompts and masks, improving segmentation models on these challenging scenarios; (ii) unlike image translation works [22, 36, 58] that typically require registered data pairs for training or are limited to specific conversions, our model provides flexible and controllable generation with no need for registered data or mask-annotated data of the target modality. t) and masks (M 4.2. Architecture For our generation paradigm, we expect the model to leverage the large number of image-text data pairs, and the limited data with segmentation masks, to achieve controllable generation for segmentation-scarce modalities. Our proposed model (MRGen) consists of three components: (i) latent encoding; (ii) text-guided generation; and (iii) maskconditioned generation, as detailed below. Latent encoding. Given the high resolution of medical images, we adopt the idea of latent diffusion [42], for efficient generation within compressed, low-dimensional latent space. As shown in Figure 2 (a), we utilize an autoencoder architecture, where 2D MRI slice (I RHW 1) is encoded into latent code (z Rhwd) for diffusion training. The decoder then reconstructs the image (ˆI) from the latent code, as expressed by the following: ˆI = ϕDec(z) = ϕDec(ϕEnc(I)) Text-guided generation. This part follows the diffusion model paradigm, consisting of forward diffusion process and denoising process. Concretely, the forward process progressively adds noise to the latent features (z0) over steps towards white Gaussian noise zT (0, 1). At any intermediate timestep [1, ], the noisy features (zt) can be expressed as: zt = 1 αtϵ, where ϵ (0, 1) and αt represent predefined hyperparameters, which will be detailed in Section of the Appendix. αtz0 + As depicted in Figure 2 (b), the learnable denoising process typically reconstructs images from noise by estimating the noise term ˆϵ. To model this process in the compressed latent space, we adopt UNet [43]. Specifically, to generate images guided by text prompts, we design templated text prompt (T ), that consists of diverse modality labels, modality attributes, regions, and organs, for example: 4 T1 MRI; fat high signal, muscle intermediate signal, water low signal, fat bright, water dark; upper abdomen; liver, spleen, and kidney. We employ an off-the-shelf text encoder (ϕtext) to encode the text prompt into embeddings, expressed as CT = ϕtext(T ). These text embeddings are then fed into our model via cross-attention, serving as the key and value, with visual latent codes as the query. Mask-conditioned generation. Next, we incorporate mask conditions to enable more controllable generation. As shown in Figure 2 (c), we adopt the idea similar to ControlNet [51], i.e., initializing the mask encoder (ϕmask) with the weights of the diffusion UNet encoder, coupled with learnable downsampling module (ϕdown) to align the dimensions of the mask with the latent code. In the input mask (M RHW 1), we use distinct intensity values to indicate the different organs, and encode it as residual into the corresponding layers of the UNet decoder, guiding the generation process. In addition, zero convolution initialization is employed to stabilize the training process, as observed in [51]. For each layer of the diffusion UNet decoder, the output can now be formulated as follows: = F(zt) + ϕmask(zt, ϕdown(M), ϕtext(T )) 4.3. Model Training Here, we present the training procedure for our generative model, including (i) autoencoder reconstruction, (ii) textguided pretraining, and (iii) mask-conditioned finetuning. Autoencoder reconstruction. The autoencoder for compression is trained solely on raw images from Du, using combination of MSE loss and KL divergence loss as follows: LVAE = ˆI2 2 + γLKL, where LKL imposes KL-penalty towards standard normal on the learned latent, similar to VAE [26] and γ represents predefined weight. Text-guided pretraining. The diffusion-based generative model, parameterized by Θ, is trained on large number of image-text pairs in Du, covering diverse modalities. The objective function can be formulated as the MSE loss between the added Gaussian noise (ϵ) and the prediction (ˆϵ): = EDu,t[1,T ],ϵN (0,1) (cid:104) ϵ ˆϵ(zt, t, )2 2 (cid:105) This pretraining phase enables MRGen to generate radiology images across diverse modalities based on text prompts. Mask-conditioned finetuning. The mask condition controller, consisting of the mask encoder (ϕmask) and the learnable downsampling module (ϕdown), can be trained jointly on the source-domain triplet dataset, Ds = {(Is, Ts, Ms)}, and target-domain dataset comprising only image-text pairs, Dt = {(It, Tt, )}, with all other paramFigure 3. Synthetic Data Construction Pipeline. MRGen takes text prompt and mask as conditions for controllably generating radiology images and employs pretrained SAM2 model for automatic filtering to guarantee the quality of generated samples. eters frozen. The training objective Lc can be expressed as: Lc = EDs,Dt,t[1,T ],ϵN (0,1) (cid:104) ϵ ˆϵc(zt, t, , M)2 2 (cid:105) Here, incorporating target-domain data without mask annotations prevents overfitting to the source domain. (i) The large-scale image-text dataset (Du) Discussion. comprises wide range of modalities, covering those of source-domain dataset (Ds) and target-domain dataset (Dt); (ii) Mask-conditioned finetuning on mask-annotated data allows MRGen to learn correlations between images and organ masks; (iii) Such two-stage training strategy then enables MRGen to extend its controllable generation ability towards target domains lacking mask annotations. 4.4. Synthetic Data for Segmentation Training t) and organ mask (M With our data engine, we aim to generate target-domain data conditioned on the segmentation masks, to facilitate training segmentation models towards unannotated modalities. Data synethesis. At inference time, we feed the text prompt (T t) as conditions into our MRGen model (ΦMRGen) to generate the corresponding target-domain image (I t). For simplicity, when constructing the input conditions, we directly utilize region, organ information, and mask annotations from source-domain data. Data autofilter. To ensure the fidelity of generated images to the mask conditions, we design an automatic filtering pipeline using the off-the-shelf SAM2 [41] model, as depicted in Figure 3. Specifically, we feed the mask (M t) and the generated image (I t) into SAM2 to predict segmentation confidence score (sconf ), and pseudo mask annotation used to calculate the IoU score (sIoU) with t. sample is considered high-quality and aligned with the mask condition if both its IoU score and confidence score exceed the predefined thresholds; otherwise, it is discarded. 5 Source Datset Source Modality Target Dataset Target Modality Source Domain DualNorm [57] CycleGAN [58] MRGen (Ours) CM. T2-SPIR 156.98 CM. T1 T1 156.98 CM. T2-SPIR CM. ADC 305.56 MP. T2 MP. 305.56 T2 ADC MP. MP. T1 76.95 T2 PS. PS. PS. T2 PS. 76.95 T1 CM. T2-SPIR 109.46 LQ. T1 109.46 T1 CM. T2-SPIR LQ. T2 PR. ADC MP. 387.29 ADC 387.29 MP. T2 PR. CM. CT AM. 148.04 T1 CM. T2-SPIR 164.73 CT AM. CM. T2-SPIR 176.51 CT ML. 228.16 261.97 422.73 416.31 120.36 126.27 281.56 246.73 434.54 365.10 226.23 259.96 265. 157.77 188.91 112.82 190.82 237.52 89.26 182.62 260.06 221.27 140.72 248.06 268.49 307.79 44.82 60.79 99.38 123.55 34.35 58.90 88.76 106.45 116.35 88.43 42.53 65.30 96.72 Average FID 197.06 281.20 200. 78.95 Table 2. Quantitative Results (FID) on Generation. Smaller values indicate better performance. Here, CM., MP., PS., LQ., PR., AM., and ML., denote CHAOS-MRI, MSD-Prostate, PanSeg, LiQA, PROMISE12, AMOS22, and MSD-Liver, respectively. Figure 4. Qualitative Results of Controllable Generation. We present images from the source domain Ds and the target domain Dt for reference. Here, prostate, liver, right kidney, left kidney and spleen are contoured with different colors. 5. Experiments Here, we start by describing the experimental settings in Section 5.1; then, in Section 5.2 and 5.3, we present comprehensive evaluation of our method from both quantitative and qualitative perspectives; lastly, we provide results for ablation experiments in Section 5.4 to prove the effectiveness of our proposed methods. 5.1. Experimental Settings We aim to evaluate our proposed data engine from two aspects: (i) image generation, and (ii) segmentation. Concretely, to simulate MRI segmentation towards unannotated modalities, we construct 8 cross-modal dataset pairs within our MedGen-1M. Each pair comprises mask-annotated source-domain dataset and an unannotated target-domain dataset. The generative model is trained on each dataset pair to synthesize target-domain samples, which are then used to train segmentation models. We assess the quality of generated images, and the performance of the segmentation models on the target-domain test set. Baselines. For image generation, we compare maskconditioned generated images from MRGen against two other approaches: CycleGAN [58] for translating sourcedomain images to the target domain; and DualNorm [57] for aggressive augmentation of source-domain images. For segmentation, we evaluate models trained on five data sources: (i) source-domain data, as baseline; (ii) DualNorm augmented data; (iii) CycleGAN translated data; (iv) MRGen generated data; and (v) manually annotated targetdomain data, serving as strong oracle baseline. We adopt nnUNet [21] and UMamba [34] as segmentation frameworks for all settings, except for DualNorm, which uses customized UNet. Moreover, we include SAM2 [41] as reference for interactive semi-automatic segmentation, using randomly perturbed oracle boxes as prompts. Evaluation metrics. We employ different metrics for assessment: For image generation, we adopt the commonly used Frechet Inception Distance score (FID) [16] to assess the diversity and quality of the generated images. For the downstream segmentation models, we employ the widely used Dice Similarity Coefficient (DSC) [35] to compare the predicted segmentation mask and ground truth. Considering segmentation consistency, we stack slices back into 3D volumes for DSC calculation. Implementation details. All the images are resized to resolution 512 512 pixels. For training, we start by training the VAE with learning rate of 5 105 and batch size of 256 for 50K iterations. Next, the text-guided generative model and mask condition controller are trained with learning rate of 1 105, using batch sizes of 256 and 128 for 200K and 40K iterations, respectively. Moreover, we randomly drop text prompts with 10% probability to enable classifier-free guidance for better performance. All training is conducted on 8 Nvidia A100 GPUs using the AdamW [32] optimizer. The downsampling factor, latent feature dimension d, KL divergence loss weight γ, and diffusion timesteps are set to 8, 16, 1 104, and 1000, respectively. During inference, we perform 50-step sampling using DDIM [45], with the classifier-free guidance [17] weight set to = 7.0. We adopt the pretrained text encoder in BiomedCLIP [52] to encode text prompts, and SAM2 [41] for automatic filtering. For each conditional mask, we generate 20 candidates and select the best two that 6 Source Dataset Source Modality Target Dataset Target Modality DualNorm [57] nnUNet [21] UMamba [34] Ds CycleGAN MRGen Ds CycleGAN MRGen Oracle Box SAM2 nnUNet [21] Dt Same Datasets, Different Modalities CHAOS-MRI T1 CHAOS-MRI T2-SPIR CHAOS-MRI T2-SPIR CHAOS-MRI T1 MSD-Prostate T2 MSD-Prostate ADC MSD-Prostate ADC MSD-Prostate PanSeg PanSeg T1 T2 PanSeg PanSeg T2 T2 T1 14.00 12.50 1. 6.90 0.80 5.52 12.94 22.20 0. 0.11 0.68 0.30 7.58 1.38 40. 57.06 2.40 3.59 66.18 58.10 57. 4.02 0.62 0.19 61.95 11.80 9. 12.07 0.38 0.27 Different Datasets, Different Modalities LiQA CHAOS-MRI T2-SPIR CHAOS-MRI T2-SPIR LiQA MSD-Prostate ADC PROMISE12 T2 PROMISE12 T2 MSD-Prostate ADC 19.23 31. 1.43 9.84 16.20 15.80 17.19 19. 7.84 41.02 42.13 59.95 31.73 11. 57.28 10.33 35.33 23.71 56.88 21. Different Datasets, Different Modalities, CT - MRI AMOS22 AMOS22 MSD-Liver CT CT CT CHAOS-MRI T1 CHAOS-MRI T2-SPIR CHAOS-MRI T2-SPIR"
        },
        {
            "title": "Average DSC score",
            "content": "19.78 16.09 1.58 10.79 0.11 8. 3.12 8.99 6.75 52.49 10.14 22. 56.23 38.67 0.05 3.19 1.65 10. 0.22 45.06 62.00 2.13 5.08 8. 41.22 43.24 57.21 8.06 43.21 11. 67.35 57.24 52.58 64.05 45.45 53. 43.48 51.94 61.50 65.39 61.07 66. 9.34 31.69 41.11 10.29 30.94 41. 37.30 52.54 37.12 49.77 62.72 70. 56.11 67.99 50.41 53.61 61.50 65. 26.73 60.53 40.93 43.48 51.94 45. 53.12 62.72 70.86 83.90 90.60 82. 89.80 82.28 80.27 70.83 93.60 87. 82.35 90.60 83.90 70.83 25.63 43. 6.85 25.89 43.52 50.50 57.92 83. Table 3. Quantitative Results (DSC score) on Segmentation. Oracle Box denotes the DSC score compared between the perturbed oracle box prompts for SAM2, and the ground truth; Ds and Dt denote training with real and manually annotated source-domain and targetdomain data respectively. Results are organized by three different cross-modality simulations, where the best results under nnUNet and UMamba are bolded respectively, while the second best are underlined. satisfy the predefined thresholds, which are set to 0.80 and 0.90 for IoU and confidence scores, respectively. 5.2. Quantitative Results Image generation. As shown in Table 2, images from the source domain exhibit high FID values with the target domain, indicating substantial discrepancies between distinct radiology modalities. Similarly, images augmented by DualNorm and translated by CycleGAN also exhibit high FID values, confirming their limited ability to emulate targetdomain images. Conversely, our MRGen presents significantly lower FID, demonstrating its capability to accurately generate images in target modalities, thus providing foundation for training segmentation models with synthetic data. Segmentation. As presented in Table 3, we have the following four key observations: (i) The significant discrepancies among different modalities hinder the ability of nnUNet and UMamba trained exclusively on sourcedomain data to generalize to the target domain, resulting in the lowest average DSC scores. (ii) The conventional image translation approach, CycleGAN, and the augmentationbased method, DualNorm, achieve moderate improvements in average DSC scores. However, these methods still struggle in the target domain and occasionally underperform in specific scenarios. (iii) In contrast, our proposed MRGen generates high-quality images for target domains, achieving the highest DSC score in 11 out of 13 experiments and significantly outperforming CycleGAN. Notably, MRGen consistently improves performance across all settings compared to source-only training, demonstrating the adaptability and effectiveness of its synthetic data across various segmentation architectures. (iv) The interactive SAM2 model, trained on substantially larger datasets, exhibits robust zeroshot segmentation capabilities with perturbed oracle box prompts. However, as semi-automatic method, its reliance on spatial prompts and manual intervention limits its scalability and practical applicability. 5.3. Qualitative Results Generation. As depicted in Figure 4, the images of distinct modalities exhibit substantial visual discrepancies, making it challenging for DualNorm to simulate via augmentation. Additionally, CycleGAN is prone to instability during training, frequently leading to model collapse when learning such complex translations. Conversely, our proposed MRGen effectively generates images that closely resemble the target domain and align with conditioned organ masks. This capability provides solid foundation for the controlled generation of training data to extend segmentation models towards unannotated modalities. 7 Figure 5. Qualitative Results of Segmentation towards Unannotated Modalities. We present reference images from the source-domain training set Ds, images from the target-domain test set Dt, and corresponding predictions and ground truth. We visualize prostate on the middle row, and liver, right kidney, left kidney and spleen on the upper and lower row with different colors. Method AutoFilter Dt Image CHAOS-MRI [25] MSD-Prostate [2] T1 T2S. T2S. T1 T2 ADC ADC T2 nnUNet [21] nnUNet (MRGen) (cid:37) (cid:37) (cid:33) (cid:37) (cid:33) (cid:37) (cid:37) (cid:37) (cid:33) (cid:33) 6.90 16.53 22.30 30.16 66. 0.80 15.10 20.27 29.01 58.10 5.52 39.90 42.79 49.04 57.83 22.20 18.92 25.34 40.89 61. Table 4. Ablation Study on Segmentation Performance (DSC score). Here, T2S. denotes T2-SPIR. Segmentation. As shown in Figure 5, despite significant variability between the source and target domains, MRGen substantially improves segmentation accuracy across all organs with high-quality synthetic data. However, the data derived from DualNorm and CycleGAN lead to unsatisfactory segmentation results, due to notable discrepancies from the target domain or low quality. 5.4. Ablation Studies To validate the effectiveness of our modules and strategies, we conduct ablation studies on the downstream segmentation task. Specifically, we train nnUNet using data synthesized under different training and inference settings. As presented in Table 4, we have the following observations: (i) even without integrating unannotated target-domain data during training, MRGen still improves segmentation performance on the target modality. This indicates that the twostage training strategy effectively enables MRGen to extend controllable generation to modalities without mask annotations; (ii) the autofilter pipeline based on SAM2 further improves performance by selecting data pairs that align better with the mask conditions. More ablation studies on image generation will be provided in Section D.2 of the Appendix. 6. Conclusion This paper explores novel paradigm of applying generative models in medical imaging: controllably synthesizing data for modalities lacking mask annotations, to train MRI segmentation models towards these challenging scenarios. To support this, we curate large-scale radiology imagetext dataset, MedGen-1M, featuring detailed modality labels, attributes, regions, and organ information, along with subset of organ mask annotations. Built upon the dataset, our diffusion-based controllable data engine, MRGen, generates MR images conditioned on text prompts and masks, enabling data synthesis across unannotated modalities for training segmentation models. Comprehensive quantitative and qualitative evaluations across diverse modalities demonstrate that MRGen can effectively synthesize data, advancing MRI segmentation for unannotated modalities."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 13 [2] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. 3, 8, 15 [3] Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier van der Sluijs, Małgorzata Połacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis Langlotz, and Akshay Chaudhari. visionlanguage foundation model for the generation of realistic chest x-ray images. Nature Biomedical Engineering, pages 113, 2024. 2, 4, 17 [4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2 [5] Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, and Zongwei Zhou. Towards generalizable tumor synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11147 11158, 2024. 2 [6] Ziyang Chen, Yongsheng Pan, Yiwen Ye, Hengfei Cui, and Yong Xia. Treasure in distribution: domain randomization based multi-source domain generalization for 2d medIn Medical Image Computing ical image segmentation. and Computer-Assisted Intervention, pages 8999. Springer, 2023. [7] Tugba Akinci DAntonoli, Lucas Berger, Ashraya Indrakanti, Nathan Vishwanathan, Jakob Weiß, Matthias Jung, Zeynep Berkarda, Alexander Rau, Marco Reisert, Thomas Kustner, et al. Totalsegmentator mri: Sequence-independent segmentation of 59 anatomical structures in mr images. arXiv preprint arXiv:2405.19492, 2024. 3 Sodtavilan [8] Zolnamar Dorjsembe, Hsing-Kuo Odonchimed, and Furen Xiao. Conditional diffusion models for semantic 3d brain mri synthesis. IEEE Journal of Biomedical and Health Informatics, 2024. 2, 4 Pao, [9] Yuxin Du, Fan Bai, Tiejun Huang, and Bo Zhao. Segvol: Universal and interactive volumetric medical image segmentation. In Advances in Neural Information Processing Systems, 2024. 3 [10] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings of the International Conference on Computer Vision, 2023. 2 [11] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Proceedings of the International Conference on Machine Learning, 2024. 2 [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 2 [13] Pengfei Guo, Can Zhao, Dong Yang, Ziyue Xu, Vishwesh Nath, Yucheng Tang, Benjamin Simon, Mason Belue, Stephanie Harmon, Baris Turkbey, et al. Maisi: Medical ai for synthetic imaging. arXiv preprint arXiv:2409.11169, 2024. 2, 4 [14] Hartmut Hantze, Lina Xu, Felix Dorfner, Leonhard Donle, Daniel Truhn, Hugo Aerts, Mathias Prokop, Bram van Ginneken, Alessa Hering, Lisa Adams, et al. Mrsegmentator: Robust multi-modality segmentation of 40 classes in mri and ct sequences. arXiv preprint arXiv:2405.06463, 2024. 3 [15] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI brainlesion workshop, pages 272284. Springer, 2021. 3 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibIn Advances in Neural Information Processing Sysrium. tems, 2017. 6, 17 [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, 2020. 2, 13 [19] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. In Advances in Neural Information Processing Systems, 2022. 2 [20] Shishuai Hu, Zehui Liao, and Yong Xia. Devil is in channels: Contrastive single domain generalization for medical image segmentation. In Medical Image Computing and ComputerAssisted Intervention, pages 1423. Springer, 2023. 3 [21] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2):203211, 2021. 2, 3, 6, 7, 8, 15, 16 [22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Image-to-image translation with conditional adverEfros. In Proceedings of the IEEE Conference sarial networks. on Computer Vision and Pattern Recognition, pages 1125 1134, 2017. 2, 4 [23] Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhanng, Wanling Ma, Xiang Wan, et al. Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. In Advances in Neural Information Processing Systems, pages 3672236732, 2022. 3, 15 [24] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling 9 up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2 [25] Emre Kavur, Sinem Gezer, Mustafa Barıs, Sinem Aslan, Pierre-Henri Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst, Savas Ozkan, et al. Chaos challenge-combined (ct-mr) healthy abdominal organ segmentation. Medical Image Analysis, 69:101950, 2021. 3, 8, 15, 16 [26] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the International Conference on Learning Representations, 2014. 5, 13 [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anyIn Proceedings of the International Conference on thing. Computer Vision, pages 40154026, 2023. 3 [28] Lennart Koetzier, Jie Wu, Domenico Mastrodicasa, Aline Lutz, Matthew Chung, Adam Koszek, Jayanth Pratap, Akshay Chaudhari, Pranav Rajpurkar, Matthew Lungren, et al. Generating synthetic data for medical imaging. Radiology, 312(3):e232471, 2024. [29] Geert Litjens, Robert Toth, Wendy Van De Ven, Caroline Hoeks, Sjoerd Kerkstra, Bram Van Ginneken, Graham Vincent, Gwenael Guillard, Neil Birbeck, Jindang Zhang, et al. Evaluation of prostate segmentation algorithms for mri: the promise12 challenge. Medical image analysis, 18(2):359 373, 2014. 3, 15 [30] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm - open-ended visual storytelling via latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61906200, 2024. 2 [31] Yuanye Liu, Zheyao Gao, Nannan Shi, Fuping Wu, Yuxin Shi, Qingchao Chen, and Xiahai Zhuang. Merit: Multi-view evidential learning for reliable and interpretable liver fibrosis staging. arXiv preprint arXiv:2405.02918, 2024. 3, 15 [32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In Proceedings of the International Conference on Learning Representations, 2019. 6 [33] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15:19, 2024. 2, 3, 16 [34] Jun Ma, Feifei Li, and Bo Wang. U-mamba: Enhancing long-range dependency for biomedical image segmentation. arXiv preprint arXiv:2401.04722, 2024. 3, 6, 7, 15 [35] Lena Maier-Hein, Matthias Eisenmann, Annika Reinke, Sinan Onogur, Marko Stankovic, Patrick Scholz, Tal Arbel, Hrvoje Bogunovic, Andrew Bradley, Aaron Carass, et al. Why rankings of biomedical image analysis competitions should be interpreted with care. Nature communications, 9 (1):5217, 2018. [36] Xiangxi Meng, Kaicong Sun, Jun Xu, Xuming He, and Dinggang Shen. Multi-modal modality-masked diffusion network for brain mri synthesis with random modality missing. IEEE Transactions on Medical Imaging, 2024. 2, 4 [37] Cheng Ouyang, Chen Chen, Surui Li, Zeju Li, Chen Qin, Wenjia Bai, and Daniel Rueckert. Causality-inspired singlesource domain generalization for medical image segmentation. IEEE Transactions on Medical Imaging, 42(4):1095 1106, 2022. 3 [38] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the International els with transformers. Conference on Computer Vision, pages 41954205, 2023. 2 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: In Proceedings of els for high-resolution image synthesis. the International Conference on Learning Representations, 2024. 2 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn Proceedings of the International Conference on vision. Machine Learning, 2021. 17 [41] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 3, 5, 6, 15 [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synIn Proceedings of the thesis with latent diffusion models. IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2, 4, 13, [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, 2015. 2, 3, 4, 13 [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2 [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proceedings of the International Conference on Learning Representations, 2020. 2, 6 [46] Zixian Su, Kai Yao, Xi Yang, Kaizhu Huang, Qiufeng Wang, and Jie Sun. Rethinking data augmentation for single-source domain generalization in medical image segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 23662374, 2023. 3 [47] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Medklip: Medical knowledge enhanced language-image pre-training for x-ray diagnosis. In Proceedings of the International Conference on Computer Vision, pages 2137221383, 2023. 3 [48] Haoning Wu, Shaocheng Shen, Qiang Hu, Xiaoyun Zhang, Ya Zhang, and Yanfeng Wang. Megafusion: Extend diffusion models towards higher-resolution image generation 10 without further tuning. arXiv preprint arXiv:2408.11001, 2024. 2 [49] Linshan Wu, Jiaxin Zhuang, Xuefeng Ni, and Hao Chen. Freetumor: Advance tumor segmentation via large-scale tumor synthesis. arXiv preprint arXiv:2406.01264, 2024. 2 [50] Yanwu Xu, Shaoan Xie, Maxwell Reynolds, Matthew Ragoza, Mingming Gong, and Kayhan Batmanghelich. Adversarial consistency for single domain generalization in In Medical Image Computmedical image segmentation. ing and Computer-Assisted Intervention, pages 671681. Springer, 2022. 3 [51] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the International Conference on Computer Vision, 2023. 2, 5 [52] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. 3, 6, 13, [53] Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Weidi Xie, and Yanfeng Wang. Knowledge-enhanced visual-language pretraining on chest radiology images. Nature Communications, 14(1):4542, 2023. 3 [54] Zheyuan Zhang, Elif Keles, Gorkem Durak, Yavuz Taktak, Onkar Susladkar, Vandan Gorade, Debesh Jha, Asli Ormeci, Alpay Medetalibeyoglu, Lanhong Yao, et al. Largescale multi-center ct and mri segmentation of pancreas with deep learning. arXiv preprint arXiv:2405.12367, 2024. 3, 15 [55] Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. One model to rule them all: Towards universal segmentation for medical images with text prompts. arXiv preprint arXiv:2312.17183, 2023. 2, 3, 14 [56] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Xiaoguang Han, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Volumetric medical image segmentation via 3d transformer. IEEE Transactions on Image Processing, 2023. 3 [57] Ziqi Zhou, Lei Qi, Xin Yang, Dong Ni, and Yinghuan Shi. Generalizable cross-modality medical image segmentation via style augmentation and dual normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2085620865, 2022. 3, 6, 7 [58] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycleIn Proceedings of the Inconsistent adversarial networks. ternational Conference on Computer Vision, 2017. 2, 4, 6 MRGen: Diffusion-based Controllable Data Engine for MRI Segmentation towards Unannotated Modalities"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related works 3. Data Curation 4. Method 4.1. Problem Formulation . . 4.2. Architecture . . 4.3. Model Training . 4.4. Synthetic Data for Segmentation Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5. Experiments 5.1. Experimental Settings 5.2. Quantitative Results 5.3. Qualitative Results . . 5.4. Ablation Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6. Conclusion A. Preliminaries on Diffusion Models B. Details of MedGen-1M B.1. Automatic Annotations . . B.2. Dataset Statistics . . . . . . . . . . . . . C. Implementation Details C.1. Preprocessing & Augmentation . . C.2. Autofilter Pipeline . . . C.3. Baselines . . . . . . . . . . . . . . . . . D. More Experiments . D.1. In-domain Generation . D.2. Ablation Studies . . . D.3. More Qualitative Results . . . E. Limitations & Future Works . . E.1. Limitations . . E.2. Future Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 3 3 3 4 5 5 6 6 7 7 8 . . . . . . . . 13 13 . 13 . 14 15 . 15 . 15 . 16 16 . 16 . 16 . 18 . 18 . 18 . . . . . . . . . . . . . . . . . . A. Preliminaries on Diffusion Models Diffusion Models [18] are class of deep generative models that convert Gaussian noise into structured data samples through an iterative denoising process. These models typically comprise forward diffusion process and reverse denoising process. Specifically, the forward diffusion process progressively introduces Gaussian noise into an image (x0) via Markov process over steps. Let xt represent the noisy image at step t. The transition from xt1 to xt can be formulated as: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI) Here, βt (0, 1) represents pre-determined hyperparameters that control the variance at each step. By defining αt = 1 βt and αt = (cid:81)t i=1 αi, the properties of Gaussian distributions and the reparameterization trick allow for refined expression: q(xtx0) = (xt; αtx0, (1 αt)I) This insight provides concise expression for the forward process with Gaussian noise ϵ as: xt = αtx0 + 1 αtϵ. Diffusion models also encompass reverse denoising process that reconstructs images from noise. UNet-based model [43] is typically utilized to learn the reverse diffusion process pθ, represented as: Here, µθ represents the predicted mean of Gaussian distribution, derived from the estimated noise ϵθ as: pθ(xt1xt) = (xt; µθ(xt, t), Σθ(xt, t)) µθ(xt, t) = 1 αt (xt 1 αt 1 αt ϵθ(xt, t)) Building on this foundation, Latent Diffusion Models [42] adopt Variational Autoencoder (VAE [26]) to project images into learned, compressed, low-dimensional latent space. The forward diffusion and reverse denoising processes are then performed on the latent codes (z) within this latent space, significantly reducing computational cost and improving efficiency. B. Details of MedGen-1M In this section, we provide additional details about our collected and curated MedGen-1M dataset. In Section B.1, we elaborate on the implementation details of the automatic annotation pipeline; and in Section B.2, we present more comprehensive statistics about the dataset. B.1. Automatic Annotations We employ an automated annotation pipeline to annotate the data in our MedGen-1M, ensuring that the samples contain comprehensive clinically relevant annotation information. This process primarily includes two components: human body region classification and modality explanation, which will be detailed as follows. Region classification. Considering the wide range and variability of abdominal imaging, we adopt the off-the-shelf BiomedCLIP [52] image encoder to encode all 2D slices and the BiomedCLIP text encoder to encode predefined text descriptions of six abdominal regions. Based on the cosine similarity between the image and text embeddings, the 2D slices are classified into these six categories, including Upper Thoracic Region, Middle Thoracic Region, Lower Thoracic Region, Upper Abdominal Region, Lower Abdominal Region, and Pelvic Region. For text encoding, we use templated text prompt as input: This is radiology image that shows $region$ of human body, and probably contains $organ$. Here, $region$ and $organ$ represent the items in the following list: (region, organ) = [ (Upper Thoracic Region, lung, ribs and clavicles), (Middle Thoracic Region, lung, ribs and heart), (Lower Thoracic Region, lung, ribs and liver), (Upper Abdominal Region, liver, spleen, pancreas, kidney and stomach), (Lower Abdominal Region, kidney, small intestine, colon, cecum and appendix), (Pelvic Region, rectum, bladder, prostate/uterus and pelvic bones) ] Modality explanation. To capture the correlations and distinctions among various modality labels, we leverage GPT-4 [1] to generate free-text descriptions detailing the signal intensities of fat, muscle, and water for each modality label. This helps the model better understand the imaging characteristics of distinct modalities. The prompt we use is as follows: 13 As senior doctor and medical imaging researcher, please help me map radiological imaging modalities to the signal intensities of fat, muscle, and water, as well as their corresponding brightness levels. Please provide the answer in the following format: fat {} signal, muscle {} signal, water {} signal, fat {}, muscle {}, water {}. Now, tell me the attributes of $modality$. B.2. Dataset Statistics In this section, we present more detailed statistics about our curated MedGen-1M dataset, including the unannotated imagetext pairs from Radiopaedia3, as well as the mask-annotated data sourced from various open-source datasets. Data without mask annotations. For the image-text pairs from Radiopaedia-CT and Radiopaedia-MRI, which are used for training the autoencoder and text-guided generation, we allocate 1% of the data as test set to evaluate reconstruction and generation performance, maximizing the amount of data available for pretraining. As result, 1,007,616 samples are used for training, comprising 804,628 from Radiopaedia-CT and 202,988 from Radiopaedia-MRI. The test set consists of 10,179 samples, with 8,128 from Radiopaedia-CT and 2,051 from Radiopaedia-MRI. We conduct statistical analysis of the distribution of modalities in Radiopaedia-MRI, as presented in Figure 6 (a). The free-text modality labels cover approximately 300 categories, providing diverse set of MRI modalities that form crucial foundation for MRGen to learn text-guided generation and expand its mask-conditioned generation capabilities towards modalities originally lacking mask annotations. Furthermore, the distribution of images across different regions in Radiopaedia-CT and Radiopaedia-MRI is presented in Figure 6 (b). Figure 6. Data Statistics of Radiopaedia. (a) Distribution of slice counts across various modalities in Radiopaedia-MRI; (b) Proportional distribution of slices across different regions in Radiopaedia-CT and Radiopaedia-MRI. Data with mask annotations. Following the SAT [55], we split the data with mask annotations into training and test sets, as detailed in Table 5. For dataset pairs comprising different datasets, we use their shared organs as the segmentation targets. 3radiopaeida.org"
        },
        {
            "title": "Test",
            "content": "# Vol. # Slc. # Slc. w/ mask # Vol. # Slc. # Slc. w/ mask LiQA [31]"
        },
        {
            "title": "Liver",
            "content": "T1-weighted 24 1,718 PanSeg [54]"
        },
        {
            "title": "Pancreas",
            "content": "MSD-Prostate [2]"
        },
        {
            "title": "Prostate",
            "content": "PROMISE12 [29]"
        },
        {
            "title": "Prostate",
            "content": "CHAOS-MRI [25] Liver, Right Kidney, Left Kidney, Spleen MSD-Liver [2]"
        },
        {
            "title": "Liver",
            "content": "AMOS22CT [23] Liver, Right Kidney, Left Kidney, Spleen, etc.*"
        },
        {
            "title": "Total",
            "content": "/ T1-weighted T2-weighted 309 305 14,656 12,294 T2-weighted ADC T2-weighed T1-weighted T2-SPIR 26 26 40 32 16 492 492 1, 1,018 503 1,148 5,961 5,106 100 100 645 770 6 75 77 6 6 10 8 4 3,428 2,982 110 110 110 276 120 298 1,400 1, 83 83 133 230 104 CT CT / 105 46,695 15,260 26 11943 200 26,069 19,023 100 15,361 11, 1,083 105,074 48,501 218 34,907 18, Table 5. Details of Segmentation-annotated Datasets in MedGen-1M. Here, # Vol. represents the number of 3D Volumes, # Slc. denotes the number of 2D slices, and # Slc. w/ mask indicates the number of 2D slices with mask annotations. *: AMOS22CT [23] also contains images and mask annotations for the following organs: Gallbladder, Esophagus, Stomach, Aorta, Inferior Vena Cava, Pancreas, Duodenum, Urinary Bladder, and Adrenal Gland. C. Implementation Details In this section, we will provide comprehensive explanation of the implementation details discussed in the paper. Specifically, Section C.1 describes the preprocessing and augmentation strategies applied to the training data. Section C.2 elaborates on the details of the autofilter pipeline. Finally, Section C.3 outlines the implementation details of the baselines. C.1. Preprocessing & Augmentation Data preprocessing. To ensure consistency across data from various sources and modalities, we apply tailored preprocessing strategies as follows: (i) For data from Radiopaedia, the images are directly rescaled to the range [0, 1]; (ii) For CT images with mask annotations, intensities are clipped to [-300, 200] and then rescaled to [0, 1]; (iii) For MR images with mask annotations, intensities are clipped to the 0.5 and 99.5 percentiles and rescaled to [0, 1]. After normalization, all data are subsequently rescaled to [-1, 1] for training various components of MRGen, including the autoencoder, diffusion UNet, and mask condition controller. For training downstream segmentation models, images are rescaled to [0, 255] and saved in .png format, followed by the preprocessing configurations of nnUNet [21] and UMamba [34]. Data augmentation. During autoencoder training, we apply random data augmentations to the images with 20% probability. These augmentations included horizontal flipping, vertical flipping, and rotations of 90, 180, 270. In contrast, no data augmentations are applied during the training of the diffusion UNet and mask condition controller. For downstream segmentation models, we adhere to the default data augmentation strategies provided by nnUNet [21] and UMamba [34]. C.2. Autofilter Pipeline When deploying our proposed data engine, MRGen, to synthesize data for segmentation model training, we adopt the offthe-shelf SAM2 [41] model to perform automatic interactive segmentation on generated images, with the mask conditions as prompt. This pipeline automatically selects samples that are fidelity to the condition masks, thus ensuring the quality of synthetic image-mask pairs. In this section, we elaborate on more implementation details of this automatic filtering pipeline, particularly focusing on the generation of MR images that encompass multiple organs. Specifically, we begin by defining the following thresholds: confidence threshold (τconf ), IoU score threshold (τIoU), t) conf ) and pseudo mask t), it is regarded average confidence threshold (τconf ), and average IoU threshold (τIoU). Both the mask (M are fed into SAM2. For each organ mask Mi annotation, which is then used to calculate the IoU score (si t, SAM2 will output confidence score (si t) and the generated image (I t. For each generated sample (I IoU) with Mi in 15 as high-quality sample if the following conditions are satisfied: {si sconf τconf }. Otherwise, the sample will be discarded. IoU τIoU, si conf τconf i}, and {sIoU τIoU, In all experiments, the predefined thresholds are set as follows: τIoU = 0.70, τconf = 0.80, τIoU = 0.80, and τconf = 0.90. C.3. Baselines CycleGAN. We follow the official implementation and training strategies across all experimental settings. Subsequently, source-domain images are translated into the target domain and paired with the source-domain masks to create paired samples for training downstream segmentation models. SAM2. We use SAM2-Large in all the experiments. Segmentation results are derived in slice-by-slice and organ-by-organ manner: For each slice with mask annotations, we simulate box prompts for each annotated organ individually. To take into account the error introduced by manual intervention, the oracle boxes are randomly shifted at each corner, by up to 8% of the image resolution, following the protocol in MedSAM [33]. DualNorm. Following the official implementation, we apply random non-linear augmentation on each image in the source domain, to generate source-dissimilar training sample, and train the dual-normalization model. All preprocessing steps, network architectures, and training strategies follow the official recommendations, with the exception that images are rescaled to 512 512, consistent with other approaches. Note that we evaluate DualNorm on all the slices in the test set, offering more rigorous evaluation compared to the official implementation, which only considers slices with segmentation annotation. D. More Experiments In this section, we present additional experimental results to demonstrate the superiority of our proposed data engine. In Section D.1, we showcase both quantitative and qualitative results of in-domain generation. In Section D.2, we conduct extensive ablation studies to illustrate the improvements introduced by our design in radiology image generation. Finally, in Section D.3, we provide additional qualitative results to validate the accuracy and flexibility of the generated outputs. D.1. In-domain Generation Our proposed data engine not only synthesizes images for target modalities lacking mask annotations but also maintains controllable generation capabilities within the source domain. Moreover, as presented in Table 6, downstream segmentation models trained exclusively on synthetic source-domain data can achieve performance comparable to those trained on real manually-annotated source-domain data. This offers feasible solution to address concerns about medical data privacy."
        },
        {
            "title": "Target\nModality",
            "content": "Ds Dt"
        },
        {
            "title": "Ds MRGen Ds MRGen Dt",
            "content": "CHAOS-MRI [25] T1 T2-SPIR T2-SPIR 90.60 83.90 T1 88.14 82.06 4.02 0. 67.35 57.24 83.90 90.60 Table 6. More Quantitative Results (DSC score) on Segmentation. We compare the performance of nnUNet [21] trained on real data versus synthetic data generated by our MRGen in both the source domain (Ds) and target domain (Dt). Moreover, we provide visualizations of in-domain generation in Figure 7, qualitatively demonstrating that our MRGen can reliably perform controllable generation of large number of samples within the training domain with mask annotations. D.2. Ablation Studies In this section, we provide more ablation study results on image generation, to demonstrate the necessity of model and data design within our proposed data engine, focusing on autoencoder reconstruction and text-guided generation. Autoencoder reconstruction. We evaluate the reconstruction performance of various VAEs on the test set derived from our curated MedGen-1M dataset, comprising Radiopaedia-CT and Radiopaedia-MRI. The evaluated models include: (i) the VAE of pretrained Stable Diffusion (VAE-SDM), (ii) the VAE of Stable Diffusion finetuned on our dataset (Finetuned-VAE-SDM), (iii) our model trained solely on Radiopaedia-MRI data (VAE-MRGen (MRI-only)), and (iv) our VAE-MRGen trained on the complete dataset (VAE-MRGen (Ours)). The reconstruction quality is assessed using standard metrics, including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM), L1 Error, and Mean Squared Error (MSE) between the reconstructed and original images. 16 Figure 7. Qualitative Results of In-domain Generation. As illustrated in Table 7, we draw the following three key observations: (i) Finetuning on radiology data: VAEs finetuned on radiology data significantly improve the reconstruction quality compared to pretrained Stable Diffusion models, revealing the limited generalizability of natural image VAEs to radiology images. (ii) Latent code dimension: our autoencoders with larger latent code feature dimension achieve further improvements in reconstruction quality, suggesting that increasing the increased capacity of latent code enables better representations. (iii) Extra data: incorporating CT data into training further improves reconstruction quality, approaching the performance of lossless compression. To this end, unlike RoentGen [3] that relies on pretrained VAEs, we employ high-capacity autoencoders trained specifically on radiology data, to ensure that the generative models built upon this foundation can produce better image details. Text-guided generation. We evaluate the text-guided image generation performance of various models on the test set derived from our MedGen-1M dataset, encompassing Radiopaedia-CT and Radiopaedia-MRI. The evaluated models include: (i) the pretrained Stable Diffusion (SDM), (ii) the Stable Diffusion finetuned on our MedGen-1M (Finetuned-SDM), (iii) our model using only free-text modality labels as text conditions (MRGen (Modality-only)), and (iv) our MRGen, which utilizes templated text prompts incorporating modality labels, attributes, regions, and organs information (MRGen (Ours)). We employ commonly used evaluation metrics for image generation, including the Frechet Inception Distance (FID [16]) to assess the quality and diversity of generated images. Additionally, we utilize BiomedCLIP [52] to calculate two similarity metrics: image-to-image similarity (CLIP-I) between the generated images and ground truth, and image-to-text similarity (CLIP-T) between the generated images and the free-text modality descriptions. As presented in Table 8, we draw the following three observations: (i) Finetuning on radiology data: pretrained Stable Diffusion tends to generate natural images rather than ideal radiology images, resulting in suboptimal performance across all evaluation metrics. However, finetuning on our dataset significantly enhances its generative performance. (ii) Text encoder: using BiomedCLIP [52] as the text encoder, trained on biomedical data, instead of CLIP [40] in Stable Diffusion, improves MRGens ability to capture similarities and differences among radiology modalities. Additionally, our high-capacity VAE, provides further potential improvements to generation quality. (iii) Templated text prompt: incorporating templated text prompts with detailed information (including modality, attributes, regions, and organs) outperforms using modality labels alone. This enables MRGen to better grasp the relationships among distinct modalities, thereby further improving generation quality, particularly in terms of image-to-image similarity with the ground truth."
        },
        {
            "title": "Model",
            "content": "Dim PSNR SSIM L1 Error MSE"
        },
        {
            "title": "Model",
            "content": "FID CLIP-I CLIP-T VAE-SDM [42] Finetuned-VAE-SDM 4 4 VAE-MRGen (MRI-only) 16 16 VAE-MRGen (Ours) 31.32 35.65 40.56 42.62 0.989 0.996 0.998 0. 0.0294 0.0190 0.0115 0.0091 0.0037 0.0014 0.0005 0.0003 SDM [42] Finetuned-SDM 249.24 0.3151 0.6698 91.48 0.7512 MRGen (Modality-only) 41.82 0.8457 39.63 MRGen (Ours) 0.1748 0.3199 0.3765 0. Table 7. Ablation Studies on Autoencoder Reconstruction. Table 8. Ablation Studies on Text-guided Generation. D.3. More Qualitative Results In this section, we provide qualitative visualizations of more datasets, covering both image generation and segmentation. 17 Image generation. As depicted in Figure 9 and 10, we present additional visualizations of controllable generation on target modalities lacking mask annotations. These results demonstrate that the proposed data engine can effectively generate highquality samples based on masks across various datasets and modalities, facilitating the training of downstream segmentation models towards these challenging scenarios. Image segmentation. As presented in Figure 11 and 12, we provide more visualizations of segmentation models trained using synthetic data on modalities that originally lack mask annotations. This validates that the samples generated by MRGen can effectively assist in training segmentation models, achieving impressive performance in previously unannotated scenarios. E. Limitations & Future Works E.1. Limitations Our proposed data engine, MRGen, is not without its limitations. Specifically, MRGen encounters difficulties when generating conditioned on extremely small organ masks and occasionally produces false-negative samples. Extremely small organ masks. The morphology of the same organ, such as the liver or spleen, can vary significantly across different slices of 3D volume, resulting in significant variability in their corresponding masks. Furthermore, the distribution of these masks is often imbalanced, with extremely small masks being relatively rare. When generating in the latent space, these masks are further downsampled, leading to unstable generation quality, as depicted in Figure 8 (a). feasible solution to mitigate this issue is to increase the amount of data with mask annotations, thereby improving the models robustness. False-negative samples. Another challenge arises from the varying number of organs to be segmented on each slice. For instance, one slice may contain the liver, kidneys, and spleen, while another may include only the liver and spleen. This variability causes MRGen to occasionally generate additional segmentation targets not specified in the mask condition. For example, as illustrated in Figure 8 (b), kidneys are unexpectedly synthesized by MRGen, despite not being included in the mask conditions, leading to false negatives during the training of downstream segmentation networks. feasible solution is to design more comprehensive and robust data filtering pipeline to filter these false-negative samples. Alternatively, simple manual selection can serve as quick and effective method to remove samples that do not meet the requirements. Figure 8. Failure Cases Analysis. Our proposed MRGen is not without limitations: (a) it may struggle to handle extremely small organ masks; (b) it occasionally produces false-negative samples, such as the unexpected synthesis of kidneys in the given example. E.2. Future Works To address the aforementioned limitations of MRGen, we propose several directions for future improvement: (i) Constructing more comprehensive and richly annotated datasets, such as incorporating more annotated CT data or collecting more MRI data, to enhance the models ability to effectively utilize mask conditions; (ii) Designing finer-grained, accurate, and efficient generative model architectures to improve generation efficiency and accuracy, particularly for small-volume organs; and (iii) Developing more robust and comprehensive data filtering pipeline to reliably select high-quality samples that meet the requirements of downstream tasks. 18 Figure 9. More Qualitative Results of Controllable Generation. We present images from the source domain Ds and the target domain Dt for reference. Here, specific organs are contoured with distinct colors: prostate in MSD-Prostate and PROMISE12 datasets, and pancreas in PanSeg dataset, and liver, right kidney, left kidney and spleen in other datasets. 19 Figure 10. More Qualitative Results of Controllable Generation. We present images from the source domain Ds and the target domain Dt for reference. Here, specific organs are contoured with distinct colors: prostate in MSD-Prostate and PROMISE12 datasets, and pancreas in PanSeg dataset, and liver, right kidney, left kidney and spleen in other datasets. 20 Figure 11. More Qualitative Results on Segmentation towards Unannotated Modalities. We present reference images from the sourcedomain training set Ds, images from the target-domain test set Dt, and corresponding predictions and ground truth. Here, specific organs are highlighted with different colors: prostate in MSD-Prostate and PROMISE12 datasets, and pancreas in PanSeg dataset, and liver, right kidney, left kidney and spleen in other datasets. 21 Figure 12. More Qualitative Results on Segmentation towards Unannotated Modalities. We present reference images from the sourcedomain training set Ds, images from the target-domain test set Dt, and corresponding predictions and ground truth. Here, specific organs are highlighted with different colors: prostate in MSD-Prostate and PROMISE12 datasets, and pancreas in PanSeg dataset, and liver, right kidney, left kidney and spleen in other datasets."
        }
    ],
    "affiliations": [
        "CMIC, Shanghai Jiao Tong University, China",
        "School of Artificial Intelligence, Shanghai Jiao Tong University, China",
        "Shanghai AI Laboratory, China"
    ]
}