{
    "paper_title": "GenRecal: Generation after Recalibration from Large to Small Vision-Language Models",
    "authors": [
        "Byung-Kwan Lee",
        "Ryo Hachiuma",
        "Yong Man Ro",
        "Yu-Chiang Frank Wang",
        "Yueh-Hua Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. A key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token types-differing in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to a specific VLM type, we present Generation after Recalibration (GenRecal), a novel, general-purpose distillation framework for VLMs. GenRecal incorporates a Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale open- and closed-source VLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 1 8 6 5 1 . 6 0 5 2 : r GenRecal: Generation after Recalibration from Large to Small Vision-Language Models Byung-Kwan Lee NVIDIA, KAIST leebk@kaist.ac.kr Ryo Hachiuma NVIDIA rhachiuma@nvidia.com Yong Man Ro KAIST ymro@kaist.ac.kr Yu-Chiang Frank Wang NVIDIA, National Taiwan University frankwang@nvidia.com Yueh-Hua Wu NVIDIA krisw@nvidia.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in vision-language models (VLMs) have leveraged large language models (LLMs) to achieve performance on par with closed-source systems like GPT-4V. However, deploying these models in real-world scenarios, particularly on resource-constrained devices, remains challenging due to their substantial computational demands. This has spurred interest in distilling knowledge from large VLMs into smaller, more efficient counterparts. key challenge arises here from the diversity of VLM architectures, which are built on different LLMs and employ varying token typesdiffering in vocabulary size, token splits, and token index ordering. To address this challenge of limitation to specific VLM type, we present Generation after Recalibration (GenRecal), novel, general-purpose distillation framework for VLMs. GenRecal incorporates Recalibrator that aligns and adapts feature representations between heterogeneous VLMs, enabling effective knowledge transfer across different types of VLMs. Through extensive experiments on multiple challenging benchmarks, we demonstrate that GenRecal significantly improves baseline performances, eventually outperforming large-scale openand closed-source VLMs. [Project Page]"
        },
        {
            "title": "Introduction",
            "content": "Vision-language models (VLMs) have emerged as powerful tools for understanding and processing multimodal information, enabling tasks such as image captioning and visual question answering [2, 3]. Recent advancements in VLMs [46] have leveraged large-scale language models (LLMs) to enhance reasoning and generation capabilities by incorporating extensive textual knowledge [7, 8]. In pursuit of superior performance, state-of-the-art VLMs [913] now integrate scaled-up LLMs such as 72B model parameters, achieving results comparable to proprietary models like GPT-4V [14] and Claude-3.5 Sonnet [15]. Despite these advances, the growing model sizes introduce substantial computational burdens, limiting their applicability in real-world scenarios, particularly for on-device deployment. To address this challenge, distillation techniques have been explored to transfer knowledge from large VLMs to smaller, more efficient models. This process typically follows two approaches: (a) text-based distillation using crafted text answer responses and (b) feature-based distillation via layer supervision. The former has been adopted in standard VLM training by leveraging high-quality visual instruction datasets [9, 16], often curated using high-performing proprietary models like GPT-4V [14]. In Work Done during Internship. Preprint. Under review. Figure 1: (Left) Visualizing the token indices of given image and text prompt and representing the possibility of distillation among various VLM pair combinations, comparing traditional distillation with our proposed distillation framework, GenRecal. Note that, the parentheses mean each VLMs LLM tokenizer, ... indicates the placement of image features, and the number of these features varies depending on the image embedding strategy. (Right) Comparing the performance of challenging evaluation benchmark, MM-Vet [1], with [A] baseline, [B] SFT on the baseline, [C] traditional distillation and [D] GenRecal from same token types of large VLMs, and GenRecal with more powerful [E] teacher and [F] student VLMs. contrast, the latter requires access to internal model features, which limits its applicability to opensource VLMs. In detail, recent studies [1719] have focused on directly distilling intermediate layers features or logits on language heads (i.e., last layers outputs). However, existing distillation methods that attempt to transfer knowledge beyond the natural language spacesuch as logits from the language head or token spacesare largely limited by fundamental constraint: they only work when the large (teacher) and small (student) VLMs share the same vocabulary sizes, token splits, and token index ordering schemes (i.e., what we refer to as token types), as shown on the left side of Figure 1. Note that different image token splitting strategies for each VLM can also cause distillation to fail, as discussed in Appendix A. For example, the InternVL2.5 series [13] splits an image into up to 12 segments based on its actual size, whereas the Qwen2-VL series [10] does not split the imageinstead, it employs multimodal RoPE technique to dynamically handle single-shot image. These differences in token types among various VLMs naturally result in significantly different total input or output token lengths. Consequently, this restriction limits the opportunities to distill knowledge from more powerful VLMs that have different token types from those of the student VLMs. Various combination lists are more described in Appendix B. To overcome this limitation, we propose Generation after Recalibration (GenRecal), novel framework for token types-compatible VLM distillation. At its core, GenRecal introduces Recalibrator that aligns and adapts the feature representations of small VLMs with those of large VLMs. Recalibrator is trained to bridge the gap between the large and small VLMs by projecting small VLM features into one representation space compatible with the teacher VLMs features. In the end, this mechanism brings in general-purpose distillation based on the shared feature representation through Recalibrator. To prove the necessity of GenRecal, we first compare the performances of traditional distillation and GenRecal, as displayed in experiment types and at Figure 1. Although we employ same token types of VLMs (Qwen2-VL-72B and Qwen2-VL7B), GenRecal outperforms traditional distillation implemented by LLaVA-KD [17]. This suggests that matching feature representations between large and small VLMs is really crucial point to seamlessly conduct distillation with less information loss. Moreover, when we replace the teacher VLM from Qwen2-VL-72B with the more powerful InternVL2.5-78B, GenRecal achieves enhanced performance while still using the same small VLM 2 Figure 2: (Left) Comparison of the challenging benchmark performances, MMB [20], MM-Vet [1], MMMU [21], and MMMU-Pro [22] by changing teacher vision-language models (VLMs) to distill the knowledge into small VLMs. Notably, the more powerful the teacher VLMs we select, the greater the performance improvement we can achieve. (Right) Comparing the performance of the challenging benchmark: MMMU [21], with GenRecal and various vision-language models across model sizes. architecture (Qwen2-VL-7B). With this upgraded teacher, if we also replace the student VLM from Qwen2-VL-7B with InternVL2.5-8B, we obtain more powerful VLM, as summarized on the right side of Figure 1. These experiments underscore the significance of GenRecal, as traditional distillation methods cannot handle VLM pairs with different token types. Furthermore, we validate our approach across wide range of VLM architectures, spanning various token types and model sizes. Interestingly, we find that selecting stronger large VLMs consistently yields better-performing distilled VLMs, as summarized in Figure 2(a). As shown in Figure 2(b) and the subsequent sections, GenRecal enables small VLMs to surpass existing openand closed-source VLMs. Note that, all the experiments in Figure 1 and Figure 2 are conducted on the equal training dataset. We summarize our contributions as follows: New Efficient VLM Family: We introduce efficient VLM family, Generation after Recalibration (GenRecal), which consistently outperforms both openand closed-source VLMs on challenging benchmarks. Token Types-compatible Recalibration: GenRecal employs Recalibrator to align and adapt feature representations of large and small VLMs, enabling general-purpose distillation across various token types of vocabulary size, token splits, and token index ordering. Broad Applicability: GenRecal is compatible with wide range of VLM architectures across different model sizes, overcoming the challenge of selecting large VLMs that have different token types and demonstrating its practicality for real-world deployment in resourceconstrained settings."
        },
        {
            "title": "2 Related Works",
            "content": "Large-scale VLMs such as NVLM-72B [12], Qwen2-VL-72B [10], and InternVL2.5-78B [13] have closely reached the performances of GPT-4V [14] and Claude-3.5 Sonnet [15] since the evolution of VLMs (see Appendix C), but they are imposing significant computational burden in real-world applications such as on-device processing. Hence, it is necessary to develop small VLMs for embedded on the light devices and fast inference, and this demand has led to the emergence of two types of distillation approaches. First one is constructing visual instruction tuning datasets [9, 16, 23 28] from large-scale VLMs and training small VLMs with those curated datasets. This dataset includes images question-answer pairs generated from GPT-4V or human review. Second one is conducting distillation of feature representation. LLaVA-MoD [18] conducts logits-based distillation on mixture of expert (MoE) [29, 30] architectures, while using DPO [31] with positive or negative preference responses. LLaVA-KD [17] also uses logits-based distillation but presents three-stage training process where trainable parameters are different for each stage to effectively warm-up and distill the knowledge. Align-KD [19] distills both vision encoder features and decoder logits, while encouraging the student VLMs first decoder layers attention map to resemble that of the large VLM. MoVE-KD [32] employs multiple vision encoders to large VLMs and utilize mixture of LoRA [33] to small VLM. It distills visual information of large VLMs by using visual attention map. Besides, there 3 Figure 3: Overview of GenRecal architecture and its training stages. We let qs and as denote small VLMs embedded tokens (i.e., image and text tokens are included together) for question and answer in visual instruction tuning dataset. In addition, ql and al are denoted by the large VLMs embedded tokens. Note that, vision-related modules such as vision encoder and projector for image tokens are omitted in this figure. are earlier distillation studies (see Appendix C), but they do not consider either discrete token-derived or different token type distillation. Likewise, all these works are largely limited to VLMs that have same token types of vocuabulary size, token splits, and token index ordering. This is because, in order to compute the distillation loss, we are normally using KL divergence and it requires the same number of output tokens and their split token indices should correspond the same meaning. For example, if we select large and small VLMs with different tokenizers or image embedding strategy, distillation cannot be performed well. There are so many different architectures of VLMs that are trained on individual token types on numerous model sizes, thus addressing this limitation would be significant step forward. We believe it will help expand the scope of distillation research in AI."
        },
        {
            "title": "3 GenRecal: Generation after Recalibration",
            "content": "3.1 Model Architecture & Various Accommodation We illustrate the overview of model architecture and training stage briefly in Figure 3, where GenRecal is made up of three main components: large (teacher) and small (student) VLMs, and Recalibrator. For large VLMs, we choose model sizes over 72B to have competitive performances: NVLM-72B (Qwen2-72B) [12], Qwen2-VL-72B (Qwen2-72B) [10], and InternVL2-76B (Llama3-70B) [34], and InternVL2.5-78B (Qwen2.5-72B) [13]. Next, we also select various model sizes of small VLMs: Qwen2-VL-2B/7B (Qwen2-2B/7B) and InternVL2.5-1B/2B/4B/8B (InternLM2.5-1.8B/7B and Qwen2.5-0.5B/3B/72B). Note that, the parentheses mean LLM used in their VLMs. Starting from this section, we refer to four segment modules in VLM architecure as vision encoder, vision projector, VLM-body, and VLM-head (i.e., language head). We build Recalibrator that plays role in acquiring shared features on one representation space. To construct it, we employ two decoder blocks and two projectors as described in Figure 3(c). We call the decoder blocks as Rec-body and call the projectors as each Rec-proj-pre and Rec-proj-post. We borrow the structure of small VLMs decoder for constructing Rec-body, and we employ one-layer linear module for drawing up Rec-proj-pre and Rec-proj-post. 3.2 Design and Propagation of Recalibrator For the training, we input same question-answer pair to large and small VLMs both, and we then get their VLM-body features such that [zql , zal ] = VLM-bodyl([ql, al]) (l: large) and [zqs , zas ] = VLM-bodys([qs, as]) (s: small). Note that, we assume that image tokens are already included in question tokens, and we let and denote question and answer tokens from the vision encoder, vision projector, and word embedding [35] for corresponding large and small VLMs. We extract questionfeature part zqs in the output of [zqs , zas ], and we pick answer-feature part zal and concatenate them to make question-answer sequence [zqs, zal ]. This sequence will be directly into Recalibrator. However, it should be noted that we cannot guarantee that large and small VLMs hidden dimensions are the same. Therefore, we apply Rec-proj-pre to zal so that its hidden dimension can be matched 4 Figure 4: Overview of regularization simultaneously done with first training stage-alignment. to the dimension of zqs in small VLMs. Once it is applied, we can finally concatenate them for [zqs, Rec-proj-pre(zal )]. Next, it is directly into Rec-body and its output feature then meets Rec-projpost to go back to the hidden dimension of large VLMs. Through VLM-head of large VLMs, we then calculate logits, and we simply call them as Recalib-based logits. This structure design and propagation rule are all for aligning and adapting their feature representation. In other words, this is the process of making VLM-head of large VLMs simultaneously understand VLM-body features of large and small VLMs. This is because we aim to conduct general-purpose distillation for any token types, and distillation is normally conducted under logits on VLM-head. 3.3 Training Process To make general-purpose distillation accelerated, we carry out three-stage training process. The first stage is to only train Recalibrator with all the model parameters of large and small VLMs frozen, as described in Figure 3(a). In this stage, Recalib-based logits are used to compute autoregressive loss and KL divergence. The target to compute autoregressive loss is straightforwardly ground-truth labels of visual instruction tuning dataset, whereas the target to calculate KL divergence is the logits of large VLMs such that VLM-head(VLM-body([ql, al])). We interpret that this stage brings in alignment on the feature representation of large and small VLMs. Detailed procedures of calculating loss functions are described in Algorithm 1 where the purple ones represent the trainable parameters. One of the important things that we found in this stage is regularization (Figure 4) that plays vital key in preventing the deviation too much from large VLMs feature. If without this regularization, the final distillation performances will not significantly leap, because their insufficient alignment representations affect general-purpose distillation performances, as shown in the experiment section. Therefore, it eventually promotes efficient harmonization with large and small VLMs feature representation. Algorithm 2 describes the technical detail of this regularization procedures, where the propagation for Recalib-proj-pre is slightly changed because the way of dealing with hidden dimension is different, compared with the propagation in Figure 3(c). For second stage (Algorithm 3), we equally deal with the loss function of first stage and additionally use small VLMs SFT autoregressive loss together. As shown in Figure 3(b), this step conducts distillation from the knowledge of large VLMs, where we train VLM-body of small VLM to adapt the shared feature representation and learn the richer knowledge. In the last stage, we train all parameters except vision encoders while doing additional SFT step to further enhance instruction-following capabilities."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Detail To ensure successful reproducibility, we outline four key technical aspects and details of GenRecal: (a) the structure detail of Recalibrator, (b) the visual instruction tuning dataset, and (c) specific details of training and evaluation. (a) Structure Detail of Recalibrator. As described in Section 3, Recalibrator is composed of two transformer decoder blocks (Rec-body) and two projectors (Rec-proj-pre and Rec-proj-post). The decoder blocks follow all configurations of small VLMs decoder block such as causal mask, hidden 5 Table 1: Evaluation of standard model size open-source VLMs and GenRecal on several challenging vision-language benchmarks: AI2D [36], ChartQA [37], MathVista [38], MMB [20], MMBCN [20], MM-Vet [1], MMMU [21], MMMU-Pro [22], BLINK [39], SEED-2-Plus [40], and RealWorldQA (RWQA). We use the notations of XX-GenRecal (YY) where XX and YY denote the student (small) and teacher (large) VLM, respectively. VLMs Cambrian-1-8B [41] Cambrian-1-13B [41] Eagle-8B [42] Eagle-13B [42] VILA1.5-8B [43] VILA1.5-13B [43] CogVLM2-19B [44] LLaVA-OneVision-7B [9] InternVL2-8B [45] MiniCPM-V2.5-8B [46] MiniCPM-V2.6-8B [46] Qwen2-VL-7B [10] InternVL2.5-8B [13] 93.9 Qwen2-VL-7B-GenRecal (InternVL2.5-78B) InternVL2.5-8B-GenRecal (InternVL2.5-78B) 93.0 VILA1.5-3B [43] Phi-3.5-Vision-4B [47] InternVL2-4B [45] InternVL2.5-4B [45] 57.9 77.8 78.9 81.4 InternVL2.5-4B-GenRecal (InternVL2.5-78B) 90. InternVL2-2B [45] Qwen2-VL-2B [10] Aquila-VL-2B [13] InternVL2.5-2B [13] 74.1 60.2 75.0 74.9 Qwen2-VL-2B-GenRecal (InternVL2.5-78B) 89.1 InternVL2.5-2B-GenRecal (InternVL2.5-78B) 91.8 LLaVA-OneVision-0.5B [9] InternVL2-1B [45] InternVL2.5-1B [13] 57.1 64.1 69.3 InternVL2.5-1B-GenRecal (InternVL2.5-78B) 82. AI2D ChartQA MathVista MMB MMBCN MM-Vet MMMU MMMU-Pro BLINK SEED-2-Plus RWQA 73.0 73.6 76.1 74.0 58.8 69.9 73.4 81.4 83.8 78.4 82.1 77.5 84.8 73.3 73.8 80.1 77.6 - - 81.0 80.0 83.3 - - 83.0 84.8 95.3 93.6 - 81.8 81.5 84.0 91. 76.2 73.5 76.5 79.2 90.9 90.7 61.4 72.9 75.9 89.4 49.0 48.0 52.7 54.4 37.3 42.5 38.6 63.2 58.3 54.3 60.6 58.2 64.4 68.8 74. 31.6 43.9 58.6 60.5 70.4 46.3 43.0 59.0 51.3 60.5 62.5 34.8 37.7 43.2 56. 75.9 75.7 75.9 75.7 75.3 74.9 80.5 80.8 81.7 77.2 - 83.0 84.6 88.4 89.5 - 76.0 78.6 81.1 88.4 73.2 74.9 - 74.7 82.9 84. 61.6 65.4 70.7 80.1 - - - - 69.9 66.3 - - 81.2 74.2 - 80.5 82.6 87.4 88.2 - 66.1 73.9 79.3 86. 70.9 73.5 - 71.9 81.0 80.4 55.5 60.7 66.3 74.4 48.0 48.9 33.3 42.6 43.2 44.3 60.4 57.5 54.2 52.8 60.0 62.0 62.8 70.4 73. 38.8 43.2 51.0 60.6 66.1 39.5 49.5 43.8 60.8 57.3 61.2 32.2 32.7 48.8 54. 42.7 40.0 43.8 41.6 38.6 37.9 44.3 48.8 49.3 45.8 49.8 54.1 56.0 65.6 68.1 34.2 43.0 34.3 52.3 58.3 34.3 41.1 47.4 43.6 52.9 59. 31.4 36.7 40.9 45.6 - - - - - - - 24.1 29.0 - 27.2 30.5 34.3 49.6 48.8 - 19.7 32.7 32.7 45. 18.2 21.2 - 23.7 37.5 36.6 - 14.8 19.4 28.7 44.9 43.1 22.4 21.8 39.5 48.1 - 53.0 50.9 - 55.2 53.8 54.8 64.3 65. 39.7 58.3 46.1 50.8 63.5 43.8 45.2 34.1 44.0 53.4 55.5 52.1 38.6 42.0 48. 59.7 60.0 57.5 60.2 45.2 50.2 66.0 65.4 67.3 61.4 65.7 68.6 69.7 70.7 72.3 41.4 62.2 63.9 66.9 69.3 60.0 61.2 63.0 60.0 65.5 67. 45.7 54.3 59.0 65.1 60.0 58.6 63.8 62.9 43.4 53.3 62.9 69.9 64.2 63.5 65.0 68.5 70.1 79.2 81.4 53.2 53.6 60.7 64.3 75. 57.3 62.6 65.0 60.1 72.8 73.0 55.6 50.3 57.5 70.8 dimensions, number of heads, and FFN structure. In addition, the projectors include the one-layer linear module. For propagation in Recalibrator, there are some critical techniques that should be considered. New positional embedding should be assigned to input sequence for Recalib-body. This is because the input sequence is based on question features in small VLMs and answer features in large VLMs, so they naturally have different type of positional embeddings and wrongly matched position-ids as well, when we are concatenating the features from large and small VLMs. Therefore, we employ another RoPE [48] to realign their positional embeddings, and their position-ids are reassigned as well. Lastly, we apply additional layer-norm [49] into the output features of Recalibrator for stable adaptation, before meeting VLM-head of large VLM. (b) Visual Instruction Tuning Dataset. We gather 9M visual instruction tuning dataset covering wide range of vision-language capabilities such as general visual question answering, dense image captioning, chart/diagram/document understanding, common-sense knowledge, science and math understanding, and multi-dimensional reasoning. Our dataset includes LLaVA-OneVision [9], MMC [50], DenseFusion [51], Cambrian [41], GPT-4V-filtered synthetic data of SA-1B [52] and Infinity-MM [16], Finance-QA [28], Wikipedia knowledge [24], InfoSeek [25], science & mathematical reasoning (SMR) [23], document-downstream/reasoning [26], WildVision [53], SROIE [54], RLAI-F [55], M3CoT [56], LLaVAR [57], KonIQ [58], iNaturalist2018 [59]. For further analysis, we particularly categorize the visual instruction tuning dataset that we use to build GenRecal into three domains: Knowledge, Science & Math, and Chart & Document. Based on these categories, we conduct dataset composition analysis by removing each of the three domains (see Appendix F). This implies that MMMU [21] requires VLMs ability for the domain of Knowledge more than the others, while MathVista [38] requires their capabilities for Science & Math. (c) Details of Training and Evaluation. By using DeepSpeed engine with ZeRO-3 [60], we train and evaluate GenRecal on 256 NVIDIA A100 80GB GPUs. To make training more efficient in memory usage, we apply gradient checkpointing [61] and LoRA [33] to VLM-body of small VLMs, where we set LoRA rank and alpha hyper-parameter to 64 and its dropout rate to 0.05. This is because loading both large and small VLMs onto GPUs brings in high memory pressure. We use AdamW optimizer [62] and apply linearly decayed learning rate from 1e-4 to 1e-5 at each training stage. The first and second stages utilize the entire 9M dataset, which are taken approximately 5 to 7 days depending on the model sizes. The last stage is taken 4 to 6 days by using 6M dataset which is 6 Table 2: (b) Comparison of GenRecal with large-scale open-source and closed-source VLMs on challenging benchmarks: MMB [20], MM-Vet [1], MM-Vet-v2 [63], MMMU [21], MMMU-Pro [22], MMStar [64], AI2D [36], ChartQA [37], SEED-2-Plus [40], MathVista [38], BLINK [39], RealWorldQA (RWQA). VLMs NVLM-72B [12] LLaVA-OneVision-72B [9] Molmo-72B [11] Qwen2-VL-72B [10] InternVL2-76B [45] InternVL2.5-78B [9] Claude-3.5-Sonnet [15] Gemini-1.5-Pro [65] GPT-4o (0513) [14] 88.8 InternVL2.5-8B-GenRecal (NVLM-72B) 89.0 InternVL2.5-8B-GenRecal (InternVL2-76B) 89.5 InternVL2.5-8B-GenRecal (Qwen2-VL-72B) InternVL2.5-8B-GenRecal (InternVL2.5-78B) 89. MMB MM-Vet MM-Vet-v2 MMMU MMMU-Pro MMStar AI2D ChartQA SEED-2-Plus MathVista BLINK RWQA - 85.8 - 86.5 86.5 88.3 82.6 73.9 83.4 58.9 60.6 61.1 74.0 65.7 72.3 70.1 64.0 69.1 63.9 72.4 71.4 73.2 - - - 68.7 68.4 65.5 71.8 66.9 71.0 59.3 65.0 62.6 67. 59.7 56.8 54.1 64.5 62.7 70.1 68.3 62.2 69.1 60.3 64.6 65.6 68.1 - 31.0 - 46.2 40.0 48.6 51.5 46.9 51.9 34.7 42.8 43.8 48.8 63.7 65.8 63.3 68.3 67.4 69.5 65.1 59.1 64.7 65.1 65.8 66.1 67. 85.2 85.6 83.4 88.1 87.6 89.1 81.2 79.1 84.6 88.5 92.1 92.2 93.0 86.0 83.7 87.3 88.3 88.4 88.3 90.8 87.2 85.7 84.3 90.4 87.5 93.6 68.4 - - 72.3 69.7 71.3 71.7 70.8 72.0 69.3 70.9 71.3 72. 66.6 67.5 58.6 70.5 65.5 72.3 67.7 63.9 63.8 67.5 71.9 71.4 74.9 48.0 55.4 - 60.5 56.8 63.8 60.1 59.1 68.0 55.6 57.7 61.4 65.3 69.9 71.9 73.7 77.8 72.2 78.7 60.1 67.5 75.4 72.5 75.5 78.8 81. Figure 5: Distillation performance on MMMU [21] for various pairings os teacher and student VLM. Each cell indicates the resulting score when using the corresponding teacher (rows) and student (columns) model sizes. the sample collections after removing general visual question answering [9]. For stable training, we handle large batch sizes by using gradient accumulation with 16 steps. We use 4 (first and third stage) or 2 (second stage) batches per one GPU, leading to total 16K (256164) or 8K (256162) batches. For evaluation, we remove large VLMs and Recalibrator, and generate answers by using the default generation hyperparameter of small VLMs. 4.2 Validation of GenRecal Throughout its realization of GenRecal, Table 1 demonstrates that GenRecal outperforms not only previous standard model size VLMs but also smaller size VLMs, highlighting that GenRecal generalizes well across different model sizes. From this table, we can say that if we choose more high-performing small VLMs like InternVL2.5-8B [13] significantly impacts the overall distillation results as well. Furthermore, when we choose same large VLMs, selecting more smaller size VLMs makes the lower distillation performances in the end. It implies that employing more capable student VLMs is crucial for achieving higher distillation performances. Next, we examine the effect of changing teacher VLMs to determine whether the aforementioned property holds consistently across different teacher VLMs. As shown in Figure 2 and Table 3, we can see that the results remain consistent, indicating that selecting more powerful large VLMs leads to greater performance improvements. To further investigate it, we utilize InternVL2.5 series [13] and conduct experiments with various combinations of teacher and student VLM sizes, as illustrated in Figure 5. The results also consistently indicates that choosing smaller size teacher and student VLMs eventually leads to lower distillation performances. Conversely, we can conclude that selecting larger size teacher and student models yields higher distilled performance due to their stronger baseline capabilities. 4.3 Analysis of Recalibrator We shift our focus to Recalibrator in order to explore its role in general-purpose distillation. Appendix presents loss graphs for various combinations of large and small VLMs. Since the losses 7 Figure 6: An overview of our training pipeline, illustrating both the question prompt and the measurement/legend annotations (top), followed by t-SNE visualizations (bottom) of teacher and student VLM pairings at the initial and final training stages. The question prompt (upper-left) shows the format of the question, while the measurement and legend box (upper-right) shows key model components to measure. Each scatter plot in the lower panels corresponds to different combination of teacher and student VLM sizes, capturing how the learned representations evolve from early to later training iterations. for training Recalibrator (Recalib(qs, al) and Recalib(ql, al)) are minimized well compared with the perplexity of SmallVLM(qs, as) and LargeVLM(ql, al), we can infer that Recalibrator successfully aligns the feature representations of large and small VLMs. It suggests that Recalibrator generalizes well in various combinations of teacher and student VLMs. Furthermore, Figure 6 visualizes multiple feature spaces to identify whether the features of large and small VLMs after Recalibrator are really matched and whether we can consider them as shared feature representation. From this figure, the features of small VLMs before Recalibrator are initially close to the features of small VLMs after Recalibrator, but they are getting matched to the features of large VLMs after Recalibrator at the final training period. Based on this analysis, we can interpret that Recalibrator is really helpful to make the shared feature representation for large and small VLMs. From this help, during distillation, Recalibrator contributes to make the student VLMs effectively learn the teacher VLMs features that contain richer knowledge. Moreover, we conduct an ablation study that investigates how important the regularization term is for the final distillation performances. We first remove the regularization term and analyze the differences before and after its removal. To illustrate this in detail, we simply prepare 10 different question-answer pairs and input them through VLM-body of both small and large VLMs. Afterwards, we propagate their output features into Recalibrator, as described in Figure 7(a). We then compute 10 10 cosine similarity matrix between Recalibrators output features from the small and large VLMs. To ensure reliable cosine similarity values, we use all training dataset samples and compute the average of the cosine similarity values for all the samples. With the regularization term included, as shown in Figure 7(c), we observe that the diagonal values in the cosine similarity matrix are significantly higher than the off-diagonal values. This implies that the features of small and large VLMs are explicitly shared through Recalibrator, aligned with the results in Figure 6 as well. In contrast, when the regularization term is removed, as depicted in Figure 7(b), the diagonal values are comparable with the off-diagonal values. This indicates that Recalibrator fails to explicitly align large and small VLMs features, preventing feature matching and sharing. 8 Table 3: Comparing the final distillation performances from the teacher model: InternVL2.578B [13] with and without the regularization term (denoted as Reg). Note that, the student models are each InternVL2.5-8B, 4B, 2B, and 1B. MMB MathVista MM-Vet MMMU VLMs Reg Table 4: Comparing supervised finetuning (SFT), three traditional distillations, and GenRecal where we use equal training dataset (Section 4.1(b)) and choose the teacher (Qwen2-VL-72B [10]) that has the same token type of student. VLMs MMB MathVista MM-Vet MMMU InternVL2.5-8B w. GenRecal w. GenRecal InternVL2.5-4B w. GenRecal w. GenRecal InternVL2.5-2B w. GenRecal w. GenRecal InternVL2.5-1B w. GenRecal w. GenRecal - - - - 84.6 88.2 89.5 81.1 82.9 88.4 74.7 75.8 84. 70.7 72.6 80.1 64.4 69.8 74.9 60.5 61.7 70.4 51.3 53.2 62.5 43.2 45.9 56.5 62.8 63.5 73. 60.6 61.1 66.1 60.8 60.9 61.2 48.8 49.8 54.4 56.0 58.9 68.1 52.3 53.6 58.3 43.6 45.4 59. 40.9 41.1 45.6 Qwen2-VL-7B w. SFT w. MiniLLM w. DistiLLM w. LLaVA-KD w. GenRecal Qwen2-VL-2B w. SFT w. MiniLLM w. DistiLLM w. LLaVA-KD w. GenRecal 83.0 84.3 84.4 84.8 85.0 87.8 74.9 76.3 77.5 77.9 78.3 82.9 58.2 60.5 61.3 61.5 61.8 69. 43.0 45.8 46.2 46.6 46.9 58.3 62.0 64.2 65.1 65.3 65.9 67.8 49.5 51.8 52.5 53.0 53.7 57.1 54.1 56.3 57.4 57.9 58.2 64.2 41.1 44.3 45.3 46.1 46.9 51.4 Figure 7: Depicting the process of (a) computing cosine similarity, (b) the result without regularization or (c) with regularization. Because the number of the embedded tokens for small and large VLMs are naturally different due to vocabulary size, token split, and token ordering, therefore we do average the output tokens and compute cosine similarity. As result, this leads to significant drop in final distillation performances, as reported in Table 3. Insufficient alignment of large and student VLMs reduces efficient knowledge transfer during distillation. These findings underscore the crucial role of the regularization term in feature alignment and knowledge transfer for general-purpose distillation."
        },
        {
            "title": "5 Discussion and Conclusion",
            "content": "We directly compare GenRecal with traditional distillations under the same token types of teacher and student VLMs. We select same token type of Qwen2-VL series [10] as large (72B) and small (7B or 2B) VLMs. Surprisingly, although we limit GenRecal to using the same token types of VLMs, Table 4 shows that GenRecal outperforms not only SFT but also traditional distillation by large margin. Appendix deals with additional strategy of distillation. Note that, MiniLLM [66] uses reverse KLD, DistilLLM [67] uses skewed reverse KLD, and LLaVA-KD [17] uses normal KLD with systemic three stages. It suggests that even though large and small VLMs are sharing same token types, their feature alignment is required to reduce their feature disparity. Moreover, when building GenRecal, VLM-head of large VLMs is only used to make distillation into small VLMs (see Figure 3(a) and (b)), while traditional distillation uses VLM-head of small VLMs to inject the knowledge into small VLMs. It indicates that GenRecal can receive more rich knowledge than traditional distillation, because VLM-head of large VLMs fundamentally has more larger hidden dimension than that of small VLMs. We present new, efficient family of vision-language models (VLMs) called Generation after Recalibration (GenRecal). At its core, Recalibrator enables token types-compatible general-purpose distillation. Beyond the current version, we will expand intermediate layers Recalibrator to embed fine-grained and sequential knowledge, or multiple VLMs-source distillation. We envision GenRecal as pivotal framework in the field of distillation, providing versatile scheme."
        },
        {
            "title": "References",
            "content": "[1] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, Mm-vet: Evaluating large multimodal models for integrated capabilities, arXiv preprint arXiv:2308.02490, 2023. [2] H. Tan and M. Bansal, Lxmert: Learning cross-modality encoder representations from transformers, arXiv preprint arXiv:1908.07490, 2019. [3] J. Li, D. Li, C. Xiong, and S. Hoi, BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation, in Proceedings of the 39th International Conference on Machine Learning (K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, eds.), vol. 162 of Proceedings of Machine Learning Research, pp. 1288812900, PMLR, 1723 Jul 2022. [4] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: visual language model for few-shot learning, Advances in Neural Information Processing Systems, vol. 35, pp. 2371623736, 2022. [5] J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, arXiv preprint arXiv:2301.12597, 2023. [6] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, arXiv preprint arXiv:2310.03744, 2023. [7] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, arXiv e-prints, 2019. [8] L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, E. Walsh, L. Zettlemoyer, N. Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo, Dolma: an open corpus of three trillion tokens for language model pretraining research, in Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (L.-W. Ku, A. Martins, and V. Srikumar, eds.), (Bangkok, Thailand), pp. 1572515788, Association for Computational Linguistics, Aug. 2024. [9] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li, Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [10] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin, Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. [11] M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park, M. Salehi, N. Muennighoff, K. Lo, L. Soldaini, et al., Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models, arXiv preprint arXiv:2409.17146, 2024. [12] W. Dai, N. Lee, B. Wang, Z. Yang, Z. Liu, J. Barker, T. Rintamaki, M. Shoeybi, B. Catanzaro, and W. Ping, Nvlm: Open frontier-class multimodal llms, arXiv preprint, 2024. [13] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al., Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. [14] OpenAI, Gpt-4v(ision) system card, 2023. https://openai.com/research/ gpt-4v-system-card, Last accessed on 2024-02-13. [15] Anthropic, The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic. com, 2024. [16] S. Gu, J. Zhang, S. Zhou, K. Yu, Z. Xing, L. Wang, Z. Cao, J. Jia, Z. Zhang, Y. Wang, et al., Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data, arXiv preprint arXiv:2410.18558, 2024. 10 [17] Y. Cai, J. Zhang, H. He, X. He, A. Tong, Z. Gan, C. Wang, and X. Bai, Llava-kd: framework of distilling multimodal large language models, arXiv preprint arXiv:2410.16236, 2024. [18] F. Shu, Y. Liao, L. Zhuo, C. Xu, G. Zhang, H. Shi, L. Chen, T. Zhong, W. He, S. Fu, et al., Llava-mod: Making llava tiny via moe knowledge distillation, arXiv preprint arXiv:2408.15881, 2024. [19] Q. Feng, W. Li, T. Lin, and X. Chen, Align-kd: Distilling cross-modal alignment knowledge for mobile vision-language model, arXiv preprint arXiv:2412.01282, 2024. [20] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, Is your multi-modal model an all-around player?, arXiv preprint et al., Mmbench: arXiv:2307.06281, 2023. [21] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al., Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, arXiv preprint arXiv:2311.16502, 2023. [22] X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, et al., Mmmu-pro: more robust multi-discipline multimodal understanding benchmark, arXiv preprint arXiv:2409.02813, 2024. [23] Y.-F. Zhang, Q. Wen, C. Fu, X. Wang, Z. Zhang, L. Wang, and R. Jin, Beyond llava-hd: Diving into high-resolution large multimodal models, arXiv preprint arXiv:2406.08487, 2024. [24] e. a. Yunxin Li, Cognitive visual-language mapper: Advancing multimodal comprehension with enhanced visual knowledge alignment, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, 2024. [25] Y. Chen, H. Hu, Y. Luan, H. Sun, S. Changpinyo, A. Ritter, and M.-W. Chang, Can pre-trained vision and language models answer visual information-seeking questions?, arXiv preprint arXiv:2302.11713, 2023. [26] A. Hu, H. Xu, J. Ye, M. Yan, L. Zhang, B. Zhang, C. Li, J. Zhang, Q. Jin, F. Huang, et al., mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, arXiv preprint arXiv:2403.12895, 2024. [27] W. Wang, Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, J. Zhu, X. Zhu, L. Lu, Y. Qiao, et al., Enhancing the reasoning ability of multimodal large language models via mixed preference optimization, arXiv preprint arXiv:2411.10442, 2024. [28] H. R. Sujet AI, Allaa Boutaleb, Sujet-finance-qa-vision-100k: large-scale dataset for financial document vqa, 2024. [29] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, in International Conference on Learning Representations, 2017. [30] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers, and N. Houlsby, Scaling vision with sparse mixture of experts, Advances in Neural Information Processing Systems, vol. 34, pp. 85838595, 2021. [31] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, Direct preference optimization: Your language model is secretly reward model, Advances in Neural Information Processing Systems, vol. 36, 2024. [32] J. Cao, Y. Zhang, T. Huang, M. Lu, Q. Zhang, R. An, N. Ma, and S. Zhang, Move-kd: Knowledge distillation for vlms with mixture of visual encoders, arXiv preprint arXiv:2501.01709, 2025. [33] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685, 2021. 11 [34] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al., How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, arXiv preprint arXiv:2404.16821, 2024. [35] T. Mikolov, K. Chen, G. Corrado, and J. Dean, Efficient estimation of word representations in vector space, arXiv preprint arXiv:1301.3781, 2013. [36] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251, Springer, 2016. [37] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque, Chartqa: benchmark for question answering about charts with visual and logical reasoning, arXiv preprint arXiv:2203.10244, 2022. [38] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao, Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, arXiv preprint arXiv:2310.02255, 2023. [39] X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna, Blink: Multimodal large language models can see but not perceive, arXiv preprint arXiv:2404.12390, 2024. [40] B. Li, Y. Ge, Y. Chen, Y. Ge, R. Zhang, and Y. Shan, Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension, arXiv preprint arXiv:2404.16790, 2024. [41] S. Tong, E. Brown, P. Wu, S. Woo, M. Middepogu, S. C. Akula, J. Yang, S. Yang, A. Iyer, X. Pan, et al., Cambrian-1: fully open, vision-centric exploration of multimodal llms, arXiv preprint arXiv:2406.16860, 2024. [42] M. Shi, F. Liu, S. Wang, S. Liao, S. Radhakrishnan, D.-A. Huang, H. Yin, K. Sapra, Y. Yacoob, H. Shi, et al., Eagle: Exploring the design space for multimodal llms with mixture of encoders, arXiv preprint arXiv:2408.15998, 2024. [43] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han, Vila: On pre-training for visual language models, arXiv preprint arXiv:2312.07533, 2023. [44] W. Hong, W. Wang, M. Ding, W. Yu, Q. Lv, Y. Wang, Y. Cheng, S. Huang, J. Ji, Z. Xue, et al., Cogvlm2: Visual language models for image and video understanding, arXiv preprint arXiv:2408.16500, 2024. [45] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, arXiv preprint arXiv:2312.14238, 2023. [46] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He, et al., Minicpm-v: gpt-4v level mllm on your phone, arXiv preprint arXiv:2408.01800, 2024. [47] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al., Phi-3 technical report: highly capable language model locally on your phone, arXiv preprint arXiv:2404.14219, 2024. [48] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, Roformer: Enhanced transformer with rotary position embedding, Neurocomputing, vol. 568, p. 127063, 2024. [49] J. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization, 2016. [50] F. Liu, X. Wang, W. Yao, J. Chen, K. Song, S. Cho, Y. Yacoob, and D. Yu, Mmc: Advancing multimodal chart understanding with large-scale instruction tuning, arXiv preprint arXiv:2311.10774, 2023. 12 [51] X. Li, F. Zhang, H. Diao, Y. Wang, X. Wang, and L.-Y. Duan, Densefusion-1m: Merging vision experts for comprehensive multimodal perception, arXiv preprint arXiv:2407.08303, 2024. [52] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segment anything, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. [53] Y. Lu, D. Jiang, W. Chen, W. Y. Wang, Y. Choi, and B. Y. Lin, Wildvision: Evaluating visionlanguage models in the wild with human preferences, arXiv preprint arXiv:2406.11069, 2024. [54] Z. Huang, K. Chen, J. He, X. Bai, D. Karatzas, S. Lu, and C. Jawahar, Icdar2019 competition on scanned receipt ocr and information extraction, in 2019 International Conference on Document Analysis and Recognition (ICDAR), pp. 15161520, IEEE, 2019. [55] T. Yu, H. Zhang, Y. Yao, Y. Dang, D. Chen, X. Lu, G. Cui, T. He, Z. Liu, T.-S. Chua, et al., Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness, arXiv preprint arXiv:2405.17220, 2024. [56] Q. Chen, L. Qin, J. Zhang, Z. Chen, X. Xu, and W. Che, M3cot: novel benchmark for multi-domain multi-step multi-modal chain-of-thought, in Proc. of ACL, 2024. [57] Y. Zhang, R. Zhang, J. Gu, Y. Zhou, N. Lipka, D. Yang, and T. Sun, Llavar: Enhanced visual instruction tuning for text-rich image understanding, arXiv preprint arXiv:2306.17107, 2023. [58] V. Hosu, H. Lin, T. Sziranyi, and D. Saupe, Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment, IEEE Transactions on Image Processing, vol. 29, pp. 40414056, 2020. [59] G. Van Horn, O. Mac Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard, H. Adam, P. Perona, and S. Belongie, The inaturalist species classification and detection dataset, in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 87698778, 2018. [60] S. Rajbhandari, J. Rasley, O. Ruwase, and Y. He, Zero: Memory optimizations toward training trillion parameter models, in SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116, IEEE, 2020. [61] N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, and C. RÃ©, Low-memory neural network training: technical report, arXiv preprint arXiv:1904.10631, 2019. [62] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, in International Conference on Learning Representations, 2019. [63] W. Yu, Z. Yang, L. Ren, L. Li, J. Wang, K. Lin, C.-C. Lin, Z. Liu, L. Wang, and X. Wang, Mm-vet v2: challenging benchmark to evaluate large multimodal models for integrated capabilities, arXiv preprint arXiv:2408.00765, 2024. [64] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al., Are we on the right way for evaluating large vision-language models?, arXiv preprint arXiv:2403.20330, 2024. [65] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al., Gemini: family of highly capable multimodal models, arXiv preprint arXiv:2312.11805, 2023. [66] Y. Gu, L. Dong, F. Wei, and M. Huang, Minillm: Knowledge distillation of large language models, in The Twelfth International Conference on Learning Representations, 2024. [67] J. Ko, S. Kim, T. Chen, and S.-Y. Yun, Distillm: Towards streamlined distillation for large language models, arXiv preprint arXiv:2402.03898, 2024. [68] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. 13 [69] W. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, InstructBLIP: Towards general-purpose vision-language models with instruction tuning, in Thirty-seventh Conference on Neural Information Processing Systems, 2023. [70] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin, Sharegpt4v: Improving large multi-modal models with better captions, arXiv preprint arXiv:2311.12793, 2023. [71] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi, V. Chandra, Y. Xiong, and M. Elhoseiny, Minigpt-v2: large language model as unified interface for vision-language multi-task learning, arXiv preprint arXiv:2310.09478, 2023. [72] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, Minigpt-4: Enhancing vision-language understanding with advanced large language models, arXiv preprint arXiv:2304.10592, 2023. [73] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [74] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers, et al., Mm1: Methods, analysis & insights from multimodal llm pre-training, arXiv preprint arXiv:2403.09611, 2024. [75] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia, Mini-gemini: Mining the potential of multi-modality vision language models, arXiv preprint arXiv:2403.18814, 2024. [76] X. Dong, P. Zhang, Y. Zang, Y. Cao, B. Wang, L. Ouyang, S. Zhang, H. Duan, W. Zhang, Y. Li, et al., Internlm-xcomposer2-4khd: pioneering large vision-language model handling resolutions from 336 pixels to 4k hd, arXiv preprint arXiv:2404.06512, 2024. [77] B.-K. Lee, B. Park, C. W. Kim, and Y. M. Ro, Collavo: Crayon large language and vision model, arXiv preprint arXiv:2402.11248, 2024. [78] B.-K. Lee, B. Park, C. W. Kim, and Y. M. Ro, Moai: Mixture of all intelligence for large language and vision models, arXiv preprint arXiv:2403.07508, 2024. [79] Z. Zong, B. Ma, D. Shen, G. Song, H. Shao, D. Jiang, H. Li, and Y. Liu, Mova: Adapting mixture of vision experts to multimodal context, arXiv preprint arXiv:2404.13046, 2024. [80] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in Proceedings of the 38th International Conference on Machine Learning (M. Meila and T. Zhang, eds.), vol. 139 of Proceedings of Machine Learning Research, pp. 87488763, PMLR, 1824 Jul 2021. [81] S. Woo, S. Debnath, R. Hu, X. Chen, Z. Liu, I. S. Kweon, and S. Xie, Convnext v2: Codesigning and scaling convnets with masked autoencoders, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1613316142, 2023. [82] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [83] B.-K. Lee, C. W. Kim, B. Park, and Y. M. Ro, Meteor: Mamba-based traversal of rationale for large language and vision models, arXiv preprint arXiv:2405.15574, 2024. [84] B.-K. Lee, S. Chung, C. W. Kim, B. Park, and Y. M. Ro, Trol: Traversal of layers for large language and vision models, arXiv preprint arXiv:2406.12246, 2024. [85] B.-K. Lee, S. Chung, C. W. Kim, B. Park, and Y. M. Ro, Phantom of latent for large language and vision models, arXiv preprint arXiv:2409.14713, 2024. [86] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, et al., Qwen2 technical report, arXiv preprint arXiv:2407.10671, 2024. 14 [87] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, Scaling laws for neural language models, arXiv preprint arXiv:2001.08361, 2020. [88] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, et al., Scaling instruction-finetuned language models, arXiv preprint arXiv:2210.11416, 2022. [89] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in neural network, arXiv preprint arXiv:1503.02531, 2015. [90] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, Distilbert, distilled version of bert: smaller, faster, cheaper and lighter, arXiv preprint arXiv:1910.01108, 2019. [91] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova, Well-read students learn better: On the importance of pre-training compact models, arXiv preprint arXiv:1908.08962, 2019. [92] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio, Fitnets: Hints for thin deep nets, arXiv preprint arXiv:1412.6550, 2014. [93] S. Sun, Y. Cheng, Z. Gan, and J. Liu, Patient knowledge distillation for bert model compression, arXiv preprint arXiv:1908.09355, 2019. [94] P. Chen, S. Liu, H. Zhao, and J. Jia, Distilling knowledge via knowledge review, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 50085017, 2021. [95] J. Wang, Y. Chen, Z. Zheng, X. Li, M.-M. Cheng, and Q. Hou, Crosskd: Cross-head knowledge distillation for object detection, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1652016530, 2024. [96] E. Ben-Baruch, M. Karklinsky, Y. Biton, A. Ben-Cohen, H. Lawen, and N. Zamir, Its all in the head: Representation knowledge distillation through classifier sharing, arXiv preprint arXiv:2201.06945, 2022. [97] N. Passalis, M. Tzelepi, and A. Tefas, Probabilistic knowledge transfer for lightweight deep representation learning, IEEE Transactions on Neural Networks and learning systems, vol. 32, no. 5, pp. 20302039, 2020. [98] W. Park, D. Kim, Y. Lu, and M. Cho, Relational knowledge distillation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 39673976, 2019. [99] Y. Tian, D. Krishnan, and P. Isola, Contrastive representation distillation, arXiv preprint arXiv:1910.10699, 2019. [100] S. Zagoruyko and N. Komodakis, Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer, arXiv preprint arXiv:1612.03928, 2016. [101] J. Yim, D. Joo, J. Bae, and J. Kim, gift from knowledge distillation: Fast optimization, network minimization and transfer learning, in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 41334141, 2017. [102] F. Tung and G. Mori, Similarity-preserving knowledge distillation, in Proceedings of the IEEE/CVF international conference on computer vision, pp. 13651374, 2019. [103] B. Heo, J. Kim, S. Yun, H. Park, N. Kwak, and J. Y. Choi, comprehensive overhaul of feature distillation, in Proceedings of the IEEE/CVF international conference on computer vision, pp. 19211930, 2019. [104] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732, 2021. [105] R. Agarwal, N. Vieillard, Y. Zhou, P. Stanczyk, S. R. Garea, M. Geist, and O. Bachem, Onpolicy distillation of language models: Learning from self-generated mistakes, in The Twelfth International Conference on Learning Representations, 2024. [106] S. Zhang, X. Zhang, Z. Sun, Y. Chen, and J. Xu, Dual-space knowledge distillation for large language models, arXiv preprint arXiv:2406.17328, 2024. [107] N. Boizard, K. E. Haddad, C. Hudelot, and P. Colombo, Towards cross-tokenizer distillation: the universal logit distillation loss for llms, arXiv preprint arXiv:2402.12030, 2024. [108] S. Muralidharan, S. T. Sreenivas, R. Joshi, M. Chochowski, M. Patwary, M. Shoeybi, B. Catanzaro, J. Kautz, and P. Molchanov, Compact language models via pruning and knowledge distillation, arXiv preprint arXiv:2407.14679, 2024. [109] I. Timiryasov and J.-L. Tastet, Baby llama: knowledge distillation from an ensemble of teachers trained on small dataset with no performance penalty, arXiv preprint arXiv:2308.02019, 2023. [110] Y.-S. Lee, M. Sultan, Y. El-Kurdi, T. Naseem, A. Munawar, R. Florian, S. Roukos, and R. F. Astudillo, Ensemble-instruct: Instruction tuning data generation with heterogeneous mixture of lms, in Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1256112571, 2023. [111] F. Wan, L. Zhong, Z. Yang, R. Chen, and X. Quan, Fusechat: Knowledge fusion of chat models, arXiv preprint arXiv:2408.07990, 2024. [112] G.-H. Wang, Y. Ge, and J. Wu, Distilling knowledge by mimicking features, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 11, pp. 81838195, 2021. [113] K. Xu, L. Rui, Y. Li, and L. Gu, Feature normalized knowledge distillation for image classification, in European conference on computer vision, pp. 664680, Springer, 2020. [114] W. Son, J. Na, J. Choi, and W. Hwang, Densely guided knowledge distillation using multiple teacher assistants, in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 93959404, 2021. [115] T. Wu, C. Tao, J. Wang, R. Yang, Z. Zhao, and N. Wong, Rethinking kullback-leibler divergence in knowledge distillation for large language models, arXiv preprint arXiv:2404.02657, 2024. [116] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and T. Pfister, Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes, arXiv preprint arXiv:2305.02301, 2023. [117] Y. Tian, Y. Han, X. Chen, W. Wang, and N. V. Chawla, Beyond answers: Transferring reasoning capabilities to smaller llms using multi-teacher knowledge distillation, arXiv preprint arXiv:2402.04616, 2024. [118] B.-K. Lee, J. Kim, and Y. M. Ro, Masking adversarial damage: Finding adversarial saliency for robust and sparse network, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1512615136, 2022. [119] B.-K. Lee, Training encoder-attention through fully-connected crfs for efficient end-to-end lane detection model, 2020. [120] B.-K. Lee, R. Hachiuma, Y.-C. F. Wang, Y. M. Ro, and Y.-H. Wu, Vlsi: Verbalized layers-tointeractions from large to small vision language models, arXiv preprint arXiv:2412.01822, 2024."
        },
        {
            "title": "A Token Type Examples",
            "content": "17 Possible Distillation Pair for VLM Combinations: Traditional Distillation versus GenRecal Figure 8: We explore the range of distillation combinations between teacher and student VLMs using two approaches: (a) traditional distillation [17] and (b) our proposed model, GenRecal. Unlike traditional distillationwhich support only limited set of pairingsGenRecal offers the flexibility to select any model for distillation, thereby enabling more versatile and comprehensive distillation framework."
        },
        {
            "title": "C Extended Related Works",
            "content": "Evolution of Vision-Language Models. Since the emergence of visual instruction tuning [68], there surge many variations of VLMs such as LLaVA-1.5 [6], InstructBLIP [69], ShareGPT-4V [70], MiniGPT-v2 [71] [6, 6972], and they initially start with standard model size of 7B. After that period, numerous VLMs [7376] have divided an image into multiple sub-regions to focus on the images details so that visual perception is enhanced. CoLLaVO [77] and MoAI [78] utilize computer vision models directly for visual capability, and Mini-Gemini [75], MoVA [79], and Eagle [42] employ multiple vision encoders such as CLIP [80], ConvNext [81], DINO-v2 [82], and SAM [52]. In parallel, Meteor [83] explores the efficient way of learning complex reasoning abilities, and TroL [84] and Phantom [85] investigate propagation modification for how we can embed visionlanguage knowledge as much as possible despite using the same architectures. More recently, many VLMs like Molmo-72B [11], LLaVA-OneVision-72B [9], NVLM-72B [12], Qwen2-VL-72B [10] and InternVL2.5-78B [13] have employed large-scale language models such as Qwen2/Qwen2.572B [86]. Thanks to scaling laws [87, 88], they have closely reached the performances of GPT-4V [14] and Claude-3.5 Sonnet [15]. Therefore, developing VLMs with large-scale language models is getting to standard these days. Knowledge Distillation. [89] offer means to transfer the rich insights of expansive, highperforming models into smaller, efficient counterparts while preserving their effectiveness. Initially, research concentrated on aligning the output logits of teacher model with those produced by its student [90, 91]. Not long after, the FitNet framework [92] expanded this notion by guiding the student with intermediate feature representations instead of solely relying on final outputs. In this configuration, convolutional layers transform the students features to mirror those of the teacher, with any divergence measured by an L2 loss. Spurred by this single-stage, feature-based approach, subsequent techniques delved deeper into leveraging internal representations for effective knowledge transfer. For instance, investigations by [9396] revealed that incorporating multiple internal layers from the teacher significantly enhances the distillation process. One noteworthy method, PKT [97], recast the teachers knowledge as probability distribution and employed KL divergence to align it with the students outputs. In similar fashion, RKD [98] concentrated on capturing the relationships between samples, while CRD [99] combined contrastive learning objectives with traditional distillation techniques. Moreover, multi-stage and relational strategies soon emerged. For example, AT [100] made use of attention maps extracted from several teacher layers, and FSP [101] utilized matrices derived from feature maps to guide the student. Further refinements followed: SP [102] assessed the similarity among input samples rather than individual features, and OFD [103] introduced novel distance metric based on marginal ReLU activations to capture essential information. As the methodology evolved, two principal strategies became apparent. On-policy techniques [104, 67, 105, 66] dynamically sample data during training, whereas off-policy methods [67, 93, 91, 106, 107] rely on pre-existing datasets. While initial efforts primarily involved single-teacher frameworks [105, 66, 18, 108], integrating multiple teachers has gradually attracted attention despite challenges such as architectural differences, vocabulary mismatches, and task misalignment [109111]. Indeed, approaches like DKMF [112] and FNKD [113] have demonstrated the benefits of harnessing teacher feature representations, with DGKD [114] further consolidating knowledge from various teachers to boost student performance. More recent endeavors have concentrated on refining the loss functions employed during distillation. MiniLLM [66] and DistiLLM [67] have proposed adaptations of KL divergencesuch as reverse or skewed variantsto mitigate overfitting, particularly in cases involving long-tail predictions. Additionally, [115] put forward dynamic loss-balancing strategy that adaptively fuses conventional and reverse KL divergence during training. Finally, to enhance the reasoning capabilities of the distilled models, contemporary research has incorporated chain-of-thought techniques. Investigations by [116, 117] exploit detailed, step-by-step reasoning traces from larger models to enrich the training signals, as exemplified by TinyLLM [117], which integrates insights from multiple teachers to diversify the distilled knowledge and improve generalization."
        },
        {
            "title": "D Loss Graphs for Recalibrator",
            "content": "Figure 9: Illustrating the loss graphs of training the Recalibrator, where we deal with various combinations of VLMs: NVLM [12], Qwen2-VL [10], InternVL2 [45], and InternVL2.5 [13]. Note that, the parenthesis in the figure means the name of LLM used in VLMs. Recalib(qs, al) and Recalib(ql, al) represent the cross entropy loss with Recalibrator logits and VLM-head of large VLM (see Section 3.2). SmallVLM(qs, as) and LargeVLM(ql, al) means original cross entropy loss for SFT without Recalibrator. They are not used in training Recalibrator. They just represent the averaged cross entropy (perplexity) during the whole training to compare them with Recalib(qs, al) and Recalib(ql, al)."
        },
        {
            "title": "E Implementation Details of GenRecal for Algorithms",
            "content": "Algorithm 1 First stage Loss Functions 1: Input: (ql, al), (qs, as), gtl (label index) 2: [zql , zal ] VLM-bodyl([ql, al]) 3: [zqs , zas ] VLM-bodys([qs, as]) 4: [rqs , ral] Recalibrator([zqs, zal ]) 5: Lar CE(VLM-headl(ral ), gtl) 6: Lkl DKL(VLM-headl(zal) ral ) 7: Return: Lar + Lkl Algorithm 2 First stage Regularization 1: Input: (ql, al), (qs, as), gtl (label index) 2: [zql , zal ] VLM-bodyl([ql, al]) 3: [rql , ral ] Recalibrator([zql , zal ]) 4: Lar CE(VLM-headl(ral ), gtl) 5: Lkl DKL(VLM-headl(zal) ral ) 6: Return: Lar + Lkl Algorithm 3 Second stage Loss Functions 1: Input: (ql, al), (qs, as), (gtl, gts) (label index) 2: [zql , zal ] VLM-bodyl([ql, al]) 3: [zqs , zas ] VLM-bodys([qs, as]) 4: [rqs , ral] Recalibrator([zqs, zal ]) 5: Lar CE(VLM-headl(ral ), gtl) 6: Lar Lar + CE(VLM-heads(zas ), gts) 7: Lkl DKL(VLM-headl(zal) ral ) 8: Return: Lar + Lkl"
        },
        {
            "title": "F Dataset Composition Analysis for Building GenRecal",
            "content": "Figure 10: Accuracy trends across different datasets: (Left) MMMU [21], and (Right) MathVista [38] over three training stages. The performance is shown for different category of visual instruction tuning dataset: All (without any exclusion), w.o. Knowledge, w.o. Science & Math, and w.o. Chart & Document. The results indicate the impact of these exclusions on accuracy progression, highlighting how the absence of specific knowledge domains affects. Note that, Note that each data point within the same stage represents 20% of the training progress, with five data points measured per stage. Knowledge: GPT-4V filtered synthetic of SA-1B [52] and Infinity-MM [16], Wikipedia Knowledge [24], Wild-Vision, InfoSeek [25], WildVision [53], and iNaturalist2018 [59] Science & Math: LLaVA-OneVision (subset) [9], Cambrian (subset) [41], Finance-QA [28], SMR [23], M3CoT [56], and KonIQ [58] Chart & Document: LLaVA-OneVision (subset) [9], Cambrian (subset) [41], MMC [50], RLAI-F [55], document-downstream/reasoning [26], SROIE [54], and LLAVAR [57]"
        },
        {
            "title": "G Additional Distillation Comparison",
            "content": "We will experiment with different perspective, by changing the distillation strategy to only answer distillation. Normally, distillation transfers the knowledge corresponding not only question tokens (i.e., image tokens included) but also answer tokens, while the following table is conducted in the setting where only answer tokens are distilled as text response. In this setting denoted by , we can conduct distillation for InternVL2.5-78B (Qwen2.5 tokenizer) to Qwen2-VL-7B (Qwen2 tokenizer), although they have different image embedding strategies. Note that, Qwen2 tokenizer and Qwen2.5 tokenizer are equal tokenizer. Methods Large VLMs MMB MathVista MM-Vet MMMU Qwen2-VL-7B w. SFT w. MiniLLM w. MiniLLM w. DistiLLM w. DistiLLM w. LLaVA-KD w. LLaVA-KD w. GenRecal w. GenRecal - - Qwen2-VL-72B InternVL2.5-78B Qwen2-VL-72B InternVL2.5-78B Qwen2-VL-72B InternVL2.5-78B Qwen2-VL-72B InternVL2.5-78B 83.0 84.3 84.4 84.9 84.8 85.3 85.0 85.7 87.8 88.4 58.2 60.5 61.3 62.8 61.5 63.0 61.8 61.5 69.5 68.8 62.0 64.2 65.1 66.5 65.3 67.1 65.9 67.6 67.8 70.4 54.1 56.3 57.4 58.0 57.9 58.3 58.2 59.5 64.2 65."
        },
        {
            "title": "H Broader Impacts",
            "content": "Our work introduces Generation after Recalibration (GenRecal), novel framework for visionlanguage model (VLM) distillation that effectively addresses the challenge of distilling models with different token types. This is critical advancement in the field of machine learning and model compression, as existing distillation methods largely rely on models sharing the same token types, limiting their applicability across diverse architectures. By proposing Recalibrator, GenRecal enables knowledge transfer between large and small VLMs regardless of their token types, unlocking new possibilities for more flexible and scalable model distillation. From broader perspective, GenRecal has the potential to impact various real-world applications that require resource-efficient multimodal AI systems, such as on-device processing, real-time visual understanding, assistive AI for accessibility and multimodal conversational agents. By enabling effective distillation across wide range of VLM architectures, GenRecal can contribute to the development of lightweight high-performance AI systems [118] that can be deployed in low-resource environments and edge devices while maintaining strong reasoning and generation capabilities. Furthermore, this research opens avenues for future work on multi-teacher distillation, intermediatelayer knowledge transfer inspired by [119, 120], and cross-architecture alignment techniques. As AI models continue to evolve with diverse training paradigms and architectures, GenRecal provides foundational step toward generalizable knowledge distillation, fostering the development of efficient multimodal AI systems."
        }
    ],
    "affiliations": [
        "KAIST",
        "NVIDIA",
        "National Taiwan University"
    ]
}