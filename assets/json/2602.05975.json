{
    "paper_title": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents",
    "authors": [
        "Tiansheng Hu",
        "Yilun Zhao",
        "Canyu Zhang",
        "Arman Cohan",
        "Chen Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises a critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, a benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with a 200,000 paper retrieval corpus.We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose a corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively."
        },
        {
            "title": "Start",
            "content": "SAGE: Benchmarking and Improving Retrieval for Deep Research Agents Tiansheng Hu1 Yilun Zhao2 Canyu Zhang1 Arman Cohan2 Chen Zhao1,3 3 Center for Data Science, New York University 1 NYU Shanghai 2 Yale University https://github.com/HughieHu/Sage 6 2 0 2 ] . [ 1 5 7 9 5 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Deep research agents have emerged as powerful systems for addressing complex queries. Meanwhile, LLM-based retrievers have demonstrated strong capability in following instructions or reasoning. This raises critical question: can LLM-based retrievers effectively contribute to deep research agent workflows? To investigate this, we introduce SAGE, benchmark for scientific literature retrieval comprising 1,200 queries across four scientific domains, with 200,000 paper retrieval corpus. We evaluate six deep research agents and find that all systems struggle with reasoning-intensive retrieval. Using DR Tulu as backbone, we further compare BM25 and LLM-based retrievers (i.e., ReasonIR and gte-Qwen2-7B-instruct) as alternative search tools. Surprisingly, BM25 significantly outperforms LLM-based retrievers by approximately 30%, as existing agents generate keyword-oriented sub-queries. To improve performance, we propose corpus-level test-time scaling framework that uses LLMs to augment documents with metadata and keywords, making retrieval easier for off-the-shelf retrievers. This yields 8% and 2% gains on short-form and open-ended questions, respectively. Figure 1: SAGE task overview. Given complex question, the deep research agent (e.g., DR Tulu) iteratively reasons, generates keyword-based sub-queries, searches for relevant papers, and outputs final answer. We first evaluate the agents with their native web-search tool, and then modify DR Tulus MCP service to replace web search with retrievers that performs corpus search over our paper collection."
        },
        {
            "title": "Introduction",
            "content": "Like human experts, deep research agents (OpenAI, 2025b; GoogleDeepmind, 2024; Perpelexity, 2025; Shao et al., 2025a) address complex queries by iteratively searching and synthesizing information across multiple sources. With the help of recent advances in the agentic capabilities of large language models (LLMs), these systems demonstrate strong and robust performance in benchmarks across multiple domains (Agashe et al., 2025; Zheng et al., 2025; Li et al., 2025a; Wang and Yuan, 2025; Chervonyi et al., 2025; Zhao et al., 2025). At the core of deep research agents lie their retrieval stack (Zheng et al., 2025; Besrour et al., Correspondence: Chen Zhao (cz1285@nyu.edu) 2025). Recent advances in LLM-based retrievers have shown strong promise, particularly in their ability to follow instructions and support reasoningintensive retrieval (Shao et al., 2025b; Muennighoff et al., 2025a; Weller et al., 2025a). However, most existing commercial deep research agents adopt proprietary search APIs over large web corpora, which rely on surface-form matching. We thus ask the following research question: Whether LLMbased retrievers can effectively contribute to deep research agent workflows? We propose to systematically study the retrieval behaviors of deep research agents with scientific literature search task. As shown in Figure 1, queries in this task often require deep understanding of research concepts as well as the ability to reason across entire scholarly articles. Moreover, unlike open-domain web search, this task provides controllable experimental environment with fixed and well-defined corpus of scientific papers. To this end, we introduce SAGE, deepresearch benchmark for Scientific AGentic retrieval Evaluation, consisting of 1,200 queries over corpus of 200,000 papers spanning four scientific domains. SAGE includes two complementary types of questions: (1) short-form questions with verifiable answer that often require intensive reasoning, and (2) open-ended questions that reflect practical research tasks such as searching related work. We first evaluate six deep research agents, including both proprietary systems like GPT-5 (OpenAI, 2025a) and Gemini-2.5-Pro (GoogleDeepmind, 2025b) and the open-source one DR Tulu (Shao et al., 2025a). While proprietary agents perform best and DR Tulu is competitive, all systems struggle with reasoning-intensive retrieval that requires synthesizing metadata and inter-paper relationships. Using DR Tulu as the backbone agent, we further find that BM25 (Robertson et al., 1994) significantly outperforms LLM-based retrievers by about 30%. Analysis shows that the sub-queries generated by existing deep research agents are keyword-oriented. This behavior aligns well with surface-form matching, while the semantic capabilities of LLM-based retrievers falter due to mismatched query formulations. To address the reasoning-intensive retrieval challenge, we propose novel corpus-level test-time scaling framework. The key idea is to leverage LLMs to reason over each paper and enrich the corpus with additional signals that make retrieval easier for off-the-shelf retrievers. Specifically, we augment each paper with informative metadata and keywords. This approach yields substantial improvements on SAGE, achieving 8% gains on shortform questions and 2% on open-ended questions. We summarize our key contributions as follows: We introduce SAGE, reasoning intensive benchmark combining short-form queries and openended queries together with large dataset. We conduct extensive evaluation and find that LLM-based retrievers collaborate poorly with deep-research agent. on both short-form and open-ended queries."
        },
        {
            "title": "2 Related Work",
            "content": "Deep Research Agents. Deep research agents represent new paradigm of autonomous AI systems designed to tackle complex, multi-step information-seeking tasks (Huang et al., 2025). Commercial systems including OpenAIs Deep Research (OpenAI, 2025b), Googles Gemini Deep Research (GoogleDeepmind, 2025a) have demonstrated impressive performance on challenging benchmarks such as BrowseComp (Wei et al., 2025). In parallel, open-source efforts have rapidly advanced, with systems such as SearchR1, WebThinker, and Tongyi Deep Research approaching competitive performance (Li et al., 2025b; Jin et al., 2025; Li et al., 2025c; Team et al., 2025). Notably, DR Tulu (Shao et al., 2025a) is the first open model explicitly trained for open-ended, longform deep research via reinforcement learning, achieving results comparable to proprietary systems on benchmarks. Despite these advances, existing deep research agents rely primarily on web search or proprietary retrieval backends. Whether such agents can function as plug-and-play solutions when paired with LLM-based retrievers over closed-domain corpora remains largely unexplored, which we systematically investigate in this work. LLM-based Retrievers. The advent of largescale contrastive learning marked significant advancement for retrievers (Ni et al., 2022; Gao et al., 2023; Li et al., 2023; Wang et al., 2024; Chen et al., 2024). More recently, decoder-based retrievers such as LLM2Vec (BehnamGhader et al., 2024) and GritLM (Muennighoff et al., 2025a) have emerged, repurposing generative LLMs for embedding tasks. Beyond general-purpose embeddings, recent work has explored training LLMbased retrievers to enhance specific capabilities. Promptriever (Weller et al., 2025a) introduces instruction-trained retrievers that can be prompted like language models. ReasonIR (Shao et al., 2025b) presents the first retriever specifically trained for reasoning-intensive tasks such as finding similar coding problems. However, whether these retrievers can collaborate effectively with agentic search paradigms remains unexplored, and our work bridges this gap. We introduce new framework for corpus-level test-time scaling and achieve great improvements Test-time Scaling for Retrieval. Test-time scaling has emerged as an effective paradigm for enhancing model performance by allocating additional computation during inference (Snell et al., 2025; Muennighoff et al., 2025b). Within retrieval domain, Rank1 (Weller et al., 2025b) introduces the first reranking model trained to leverage testtime compute. Other approaches explore query expansion (Gao et al., 2023), query rewriting (Ma et al., 2023) and to further leverage inference-time computation. Here in our work, we investigate how corpus-level test-time scaling can adapt the corpus to better align with automatically decomposed sub-queries from deep-research agent like DR Tulu (Shao et al., 2025a) for task-specific retrieval."
        },
        {
            "title": "3 SAGE Benchmark",
            "content": "This section introduces SAGE Benchmark. We begin with motivating SAGE (3.1), then present data curation and evaluation metric for shot-form questions (3.2), followed by those for open-ended questions (3.3) and corpus construction (3.4). Problem Formulation. Unlike traditional RAG system, which given query q, retrieves documents = Retrieve(q) and generates response conditioned on in one shot, deep research agent is an agentic system composed of one or more LLMs augmented with search tools. Such agents autonomously plan multi-step research procedures, retrieve information from online sources, and synthesize evidence into comprehensive, well-cited answer. Specifically, the agent selects an action ai {think, tool, answer} at each step: reasoning internally, issuing sub-query qi to retrieve documents Di = Retrieve(qi), or producing the final answer conditioned on the accumulated evidence (cid:83) Dj. This formulation enables the agent to decompose complex questions into subqueries {q1, q2, . . . , qn}, progressively building an evidence base across multiple retrieval rounds. Property Com. Sci. Nat. Sci. Health. Human. Short-Form Questions Query Num Query Length 150 201.5 150 180.3 150 187.6 150 188.3 GT Documents DB Size 1.00 47, 1.00 50,000 1.00 50,000 1.00 39,032 Open-Ended Questions Query Num Query Length 150 99. 150 103.9 150 101.5 150 101.2 GT Documents DB Size 17.62 46,756 12.67 48, 10.83 47,745 9.94 37,506 Table 1: Our Benchmark statistics. Domains: Computer Science (Com. Sci.), Natural Science (Nat. Sci.), Healthcare (Health.), and Humanities (Human.). Query length is in tokens. GT Documents = average ground truth papers per query. corpus, limited by the use of commercial search APIs. In contrast, scientific literature search adopts collections of papers as controlled corpus for precise evaluation of different retrievers. (3) Existing Datasets Fall Short. While several datasets exist for scientific literature search, they fail to evaluate deep research agents. This is because the papers used in these datasets are outdated and often include LLMs pre-existing knowledge. However, scientific literature is rapidly evolving field, with new papers published daily. Our dataset uses up-todate papers to better study the retrieval behavior of deep research agents in dynamic environment. Based on these reasons, we construct SAGE, which includes 1,200 questions spanning shortform and open-ended types. These questions cover four critical scientific domains: Computer Science, Natural Science, Healthcare, and Humanities. For each domain, we curate corpus of 50,000 upto-date papers. The statistics of our dataset are presented in Table 1."
        },
        {
            "title": "3.2 Short-form Questions",
            "content": "Our primary goal is to study the retrieval behavior of deep research agents. To achieve this, we choose scientific literature search as our testbed for several reasons: (1) Task is Common and Impactful. Searching for relevant literature is an integral part of the research process, whether it is to verify if an idea has been explored before or to collect related work. Therefore strong agentic system could significantly accelerate the scientific discovery process. (2) Controllable Domain Specific Corpus. Existing deep research tasks rely on entire web as The first type of questions in SAGE is short-form questions. Similar to existing deep research benchmarks (Wei et al., 2025; Chen et al., 2025), shortform questions emphasize two key characteristics: (1) Intensive Reasoning. These questions require deep research agents to browse multiple papers, synthesize detailed and scattered information, and derive final answer; (2) Verifiability. The answer to each question is unique and fixed, therefore the correctness is easily verifiable. An example of short-form question can be found at Figure 2. Figure 2: Overview of short-form questions that require intensive reasoning over metadata, paper details and inter-paper relationships. Each question consists of three parts and has only one ground-truth answer. Figure 3: Overview of open-ended questions that are grounded on real-world scenarios. Each question consists of three parts and has multiple ground-truth papers weighted by their relevance. Data Curation. We construct question-answer pairs from three sources: extracted paper metadata (e.g., author count, title length), figures and tables extracted using PyMuPDF (McKie, 2025), and inter-paper relationships established via reference overlap. To establish inter-paper relationships, we compute the citation overlap between papers, which we consider two papers as related if they share at least four common references in their reference lists. Specifically, we first sample seed paper and related paper published after 2024 from major venues in each domain (e.g., ACL, ICML, NeurIPS for computer science). Next, we extract the corresponding metadata, figures, tables, and inter-paper relationships. We then prompt LLMs (GPT-5-mini (OpenAI, 2025a) in this case) to generate questions that require reasoning across these multiple sources. The answer to each question is the seed paper itself. Evaluation Metric. We use Exact Match (EM) as the metric to evaluate whether the ground truth answer is included in the output text or citations."
        },
        {
            "title": "3.3 Open-Ended Questions",
            "content": "Unlike short-form questions, which primarily aim to objectively measure and compare different deep research systems (Rodriguez and Boyd-Graber, 2021), open-ended questions are grounded in realworld scenarios. They mimic the types of questions researchers encounter when conducting literature reviews and exploring new ideas. An example of open-ended question can be found at Figure 3. Data Curation. The open-ended questions consist of two components: (1) the background context of the research topic, and (2) the shared citations between pair of papers. We construct questions through the following pipeline: First, we leverage the reference-overlap information from Section 3.2 to select paper pairs. For each selected pair, we adopt GPT-5-mini (OpenAI, 2025a) to analyze the inter-relationship between the two papers and the reasons for their shared citations. Based on this analysis, GPT-5-mini (OpenAI, 2025a) generates corresponding questions. Note that each openended question has multiple ground truth papers, so we create the ground-truth using hierarchical structure. The most relevant papers are the selected seed paper pair, followed by those cited by both papers. Evaluation Metric. Given the list of groundtruth papers for open-ended questions, we first assign discrete relevance scores {2, 1, 0}: Most Relevant (r=2) for the two seed papers; Relevant (r=1) for the intersection of the core papers references; and Not Relevant (r=0) for all others. We report Weighted Recall to capture all papers from both the output text and citation lists: Weighted Recall = (cid:80) (cid:80) dL g(rel(d)) dG g(rel(d)) , (1) where is the set of retrieved documents, is the set of all relevant documents, and g(r) = is the linear gain function."
        },
        {
            "title": "3.4 Corpus Construction",
            "content": "For each domain, we construct 50k-paper corpus using only open-access PDFs to ensure accessibility. The corpus begins with the following: (1) the ground-truth target paper and its highest-overlap partner from the computed reference-overlap information, (2) the intersection of their references, and (3) the union of their references. We then expand the corpus by sampling papers published in or after 2020 from major venues in the respective domain until the desired corpus size is reached. Due to the limited availability of papers in the humanities, this process results in approximately 40k papers, as we intentionally exclude very old literature. can match or exceed proprietary systems in precise, retrieval-heavy settings."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we first describe the experiment setup for deep research agents with web search (4.1) and report their results on SAGE (4.2). We then move to controlled setting by evaluating retriever performance within the same deep-research agent (i.e., DR Tulu) using retrieval corpus we constructed (4.3 and 4.4). At last, we presents ablation results on short-form questions (4.5)."
        },
        {
            "title": "4.1 Web-Search Experiment Setup",
            "content": "We evaluate two categories of deep research agents: (1) Proprietary deep research agents, including GPT-5 (OpenAI, 2025a), GPT-5-mini (OpenAI, 2025a), GPT-5-nano (OpenAI, 2025a), Gemini-2.5Pro (GoogleDeepmind, 2025b), and Gemini-2.5Flash (GoogleDeepmind, 2025a), by using the offical APIs; (2) Open-source deep research agents, notably AI2s recently released DR Tulu (Shao et al., 2025a), which sets new SOTA among opensource deep-research agents. For GPT series1, we set the reasoning effort to medium, and enable web search functionality. For Gemini series, we set thinkingBudget to -1 to enable dynamic thinking and give web search permission. For DR Tulu (Shao et al., 2025a), we deploy the model on server equipped with one H100 GPU and perform inference using vLLM."
        },
        {
            "title": "4.2 Web-Search Results",
            "content": "Table 2 presents the results of deep research agents with web search. We have the following findings: GPT-5 leads overall on short-form questions, while open-ended questions vary more by domain and model. On short-form questions, the GPT-5 series delivers the strongest performance across all domains, with GPT-5 achieving the best EM (71.69%). In contrast, open-ended questions induce more heterogeneous outcomes: GPT-5-nano performs best in healthcare, while Gemini-2.5-flash is competitive in computer science and humanities. Notably, DR Tulu outperforms the closed-source Gemini-2.5 series agents on short-form questions, indicating that open-source deep research agents Search quantity is not the main driver of accuracy. On short-form questions, Gemini-2.5-flash issues nearly twice as many web-search calls as GPT-5, and DR Tulu returns an exceptionally large number of references (37.32 on average), yet both trail GPT-5 by substantial margin. This pattern suggests that brute-force searching or reference accumulation is insufficient for precise retrieval. Instead, stronger models appear to benefit from more accurate query decomposition and more targeted evidence selection, achieving higher accuracy with fewer, better-aligned searches. Agents adapt search effort differently across query types. When moving from short-form to open-ended questions, DR Tulu and the Gemini series reduce the number of searches, consistent with looser constraints and potentially earlier stopping. In contrast, GPT-5 increases search activity on open-ended questions and attains the best overall results, with only modest and acceptable increase in the number of references compared with other agents. Query decomposition strategies differ across agents. As shown in Figure 7 and Figure 8 in Appendix, the proprietary models tend to decompose queries into more phrasal, semantically structured search queries, whereas DR Tulu sub-queries more often resemble less structured keyword concatenations. This difference aligns with the observed efficiency gap, where more structured decomposition corresponds to fewer but higher-yield searches and improved retrieval precision."
        },
        {
            "title": "4.3 Corpus-Search Experiment Setup",
            "content": "Motivated by our web-search results on the dataset, we next investigate how LLM-based retrievers integrate with deep research workflows. We use DR Tulu (Shao et al., 2025a) as the backbone agent for all corpus-search experiments. We modify DR Tulus MCP service so it can only use our provided retriever as the search tool. We study three retrievers, which are as follows: BM25 (Robertson et al., 1994), spase retriever; gte-Qwen-2-7Binstruct (Li et al., 2023), LLM-based retriever, and ReasonIR (Shao et al., 2025b), reasoningintensive retriever. 1We do not evaluate o3and o4-mini-deep-research (OpenAI, 2025b), as GPT-5 already surpasses these them on complex reasoning-intensive retrieval (OpenAI, 2025a). Retrieval Index Construction. Before experiment, we first download all PDFs according to Method Com. Sci. Healthcare Humanities Nat. Sci. Avg. Searches Avg. Refs Avg. Perf. WEB SEARCH Short-Form Questions (Exact Match) GPT-5 GPT-5-mini GPT-5-nano DR Tulu Gemini-2.5-pro Gemini-2.5-flash 57.3 40.0 30.7 36.0 27.7 30.3 78.7 72.0 46.7 58.0 47.8 43.7 Open-Ended Questions (Weighted Recall) GPT-5 GPT-5-mini GPT-5-nano DR Tulu Gemini-2.5-flash Gemini-2.5-pro 35.1 27.4 25.9 18.0 28.2 20.0 25.0 17.8 29.8 17.2 10.1 5. CORPUS SEARCH Short-Form Questions (Exact Match) BM25 k=10 BM25 k=5 gte-Qwen k=10 ReasonIR k=10 gte-Qwen k=5 ReasonIR k=5 63.3 56.0 44.4 28.0 32.9 25.9 88.8 79.9 69.7 57.0 52.1 42.2 Open-Ended Questions (Weighted Recall) gte-Qwen k=10 BM25 k=10 ReasonIR k=10 gte-Qwen k=5 BM25 k=5 ReasonIR k=5 28.9 20.3 16.1 22.4 18.2 11. 33.5 36.2 29.5 26.3 29.4 18.5 79.1 70.7 62.0 49.3 40.3 41.6 18.8 13.9 15.7 14.1 15.7 12.2 84.4 82.0 73.3 61.3 62.6 48.2 36.6 34.0 32.0 29.3 28.3 22.3 71.7 66.4 44.3 44.7 37.7 36. 26.2 22.4 21.1 20.2 9.8 6.7 88.4 85.3 64.4 50.7 40.0 38.4 33.0 32.3 27.4 26.2 26.3 17.0 8.78 8.15 8.92 7.35 14.02 15.64 13.69 10.07 8.59 4.25 6.50 10.77 6.42 7.54 5.88 7.51 4.82 8. 4.54 4.17 4.44 4.79 4.72 6.21 6.54 4.69 7.06 37.32 1.43 3.79 29.02 30.70 30.27 35.95 22.12 14.18 40.0 22.4 33.2 35.8 14.2 17.4 29.3 29.9 26.8 15.4 16.7 15.8 71.7 62.3 45.9 42.0 38.5 38. 26.3 20.4 20.6 17.4 16.0 11.0 81.2 75.8 63.0 49.3 46.9 38.7 33.0 30.7 26.2 26.0 25.5 17.3 Table 2: Performance comparison across two question types. Avg. Perf. denotes the average performance across all domains. Bold indicates the best result and underline indicates the second best. For Avg. Searches: dark red = highest, light red = lowest. For Avg. Refs: dark green = highest, light green = lowest. the URLs in the SAGE dataset (detailed in Section 3.4). We then convert them to markdown using PyMuPDF (McKie, 2025) for text and PDFPlumber (Singer-Vine and Jain, 2025) for tables. Next, we embed the first 32,000 tokens of each markdown file with the corresponding retriever to ensure that the vast majority of each PDFs content is retained while matching the maximum input length of gte-Qwen-2-7B-instruct. We embed each document individually, setting the batch size to 1 to avoid unnecessary padding. Both ReasonIR and gte-Qwen-2-7B-instruct embeddings are computed on single H100 GPU. We present the paper length distribution for all four domains in the Appendix Figure 9, Figure 10, Figure 11 and Figure 12. Retrieval Setup. During experiments, the DR Tulu agent is deployed on two H100 GPUs, where one running vLLM for answer generation and the other running MCP powered by the selected retriever. We set the maximum search iteration to 10 and for each retriever we evaluate two settings for the number of results returned per search, which are top-5 and top-10 (i.e. k=5 and k=10). Each retrieval step return list of paper titles together with their abstracts."
        },
        {
            "title": "4.4 Corpus-Search Results",
            "content": "Table 2 presents our results of DR Tulu using inhouse retrievers as search tools. We have the following main findings: BM25 dominates LLM-based retrievers on short-form questions, while the gap for openended questions is narrower. On short-form questions, BM25 significantly outperforms LLMbased retrievers by roughly 30%, suggesting that sparse lexical matching is better aligned with multiconstraint evidence retrieval in this setting. On open-ended questions, BM25 and gte-Qwen-2-7BMethod EM Met. Det. Rel. Web Search GPT-5 DR Tulu Gemini-2.5-Pro 71.69 -24.45 42.00 -17.92 -21.60 38.50 -11.43 -14. -3.85 -16.92 -6.88 -5.67 Corpus Search DR Tulu (BM25) 75.84 DR Tulu (ReasonIR) 38.72 -8.17 -14.94 -24.59 -9.84 -15.99 -5.33 Table 3: Ablation study on short-form questions components. EM denotes Exact Match. denotes the relative accuracy change (%) when removing each component: metadata (Met.), multimodality detail information (Det.), and relationship constraints (Rel.). Highlighted cells indicate the most impactful component for each method. LLM-based retrievers suffer from reduced diversity under long-document constraints. LLM-based retrievers also face information loss when documents approach the maximum input length, and embedding convergence can further reduce per-search diversity. We define Unique References per Search (URS) as the average number of retrieved documents returned per search call, computed as the ratio of the average number of documents to the average number of searches. Under top-5 on short-form questions, BM25 achieves URS of 2.97, whereas ReasonIR attains only URS of 1.98. This indicates that LLM-based retrievers are less effective to surface the target document under fixed search budget. Low-diversity decomposition blunts retriever differences on open-ended queries. DR Tulu exhibits relatively low diversity in its query decomposition. BM25 appears more compatible with DR Tulus decomposition and is more robust to long documents, but it does not open clear advantage on open-ended queries. plausible explanation is that DR Tulus sub-queries cover only limited portion of the evidence space, so even when retrievers behave differently, multiple ground-truth targets are only partially retrieved. Figure 4: An illustrative case where LLM-based retrieval fails due to semantic drift. The query seeks paper that uses physics-informed heuristics. ReasonIR over-emphasizes title-level keywords (highlighted in red) and thus retrieves wrong papers. The retrieved content then reinforces this focus in subsequent retrieval steps, creating feedback loop that increasingly prioritizes physics-informed in title. In contrast, BM25 remains anchored by lexical matching in similar subqueries and avoids this drift. instruct achieve comparable performance, while ReasonIR ranks last on both query types. Notably, gte-Qwen-2-7B-instruct can even slightly outperform BM25, indicating that LLM-based retrieval can be competitive when evaluation tolerates broader evidence coverage. case for BM25 beating LLM-based retrievers is presented in Figure 4. Increasing per-search top-k consistently improves performance. Across all retrievers, increasing the per-search top-k yields measurable gains, and ReasonIR benefits the most. This suggests that larger candidate set partially compensates for weaker first-page ranking, especially for LLM-based retrievers. Query-retriever mismatch limits the value of LLM-based semantics. key issue is pronounced Query-Retriever Mismatch: although LLM-based retrievers are trained on naturallanguage queries, agents often generate keywordlike sub-queries, as shown in Appendix Figure 8, which poorly match the retrievers training distribution and can underutilize semantic capabilities."
        },
        {
            "title": "4.5 Ablation",
            "content": "We conduct ablation studies using short-form questions, as their answers are easier to verify. As discussed earlier, these questions span three aspects of query information: paper metadata, multimodal details, and inter-paper relationships. Manual inspection shows that leveraging any two of these components is sufficient to locate 93.67% of the target papers. Based on this observation, we examine how deep research agents exploit different sources of query information. For each model family, we select one model and report results in Table 3. Search method strongly shapes which information matters. Different deep-research agents emphasize different components of the query, and this emphasis shifts with the search method. Under web search, DR Tulu is most sensitive to paper details, whereas under corpus-based search, inter-paper relationships become the dominant factor. Moreover, agents that share the same search method exhibit similar sensitivity patterns. For instance, both DR Tulu and Gemini-2.5-Pro rely on Google Search and are most influenced by paper details, indicating that the retrieval backend largely determines which part of query information drive performance."
        },
        {
            "title": "5 Test-Time Corpus Scaling",
            "content": "Our analysis in Section 4 reveals fundamental limitation of existing deep research agents: certain papers requiring intensive reasoning are inherently difficult to retrieve. Prior work (SU et al., 2025; Shao et al., 2025b) proposes to address this challenge through test-time scaling on the query side, augmenting queries with reasoning chains. In contrast, we propose an alternative form of test-time scaling at the document corpus side. The key intuition is that, rather than increasing query complexity, we incorporate reasoning-derived information into documents, making them easier to retrieve for off-the-shelf retrievers."
        },
        {
            "title": "5.1 Method",
            "content": "Since DR Tulu primarily issues keyword-based queries, we augment each documents Markdown by prepending salient keywords to improve retrieval effectiveness. Specifically, we first obtain key bibliographic metadata, including publication venue, year, authors, and citation In addition, we use Qwen3-Next-80Bcounts. A3B-Instruct (Qwen, 2025) to process the Markdown and extract eight topic-relevant keywords that summarize the papers core contributions. These fields are formatted as emphasized keywords and prepended to each document, so that both bibliographic signals and high-level semantic cues are surfaced for effective keyword-based retrieval.2 2We scale the corpus by augmenting documents with additional information in bag of keywords. With LLMs, future work could explore more aggressive corpus scaling strategies, Short-form Open-ended Retriever Before After Before After BM25 75.80 83.98 +8.18 gte-Qwen 46.90 47.80 +0.90 ReasonIR 38.70 40.40 +1.70 25.52 27.25 +1.73 26.03 27.82 +1.79 17.25 19.79 +2.54 Table 4: Performance before and after corpus-level testtime scaling. Short-form is evaluated by Exact Match (EM) (%) and open-ended by Weighted Recall (%). Improvements are shown with green background ."
        },
        {
            "title": "5.2 Results",
            "content": "In this experiment, we set the maximum number of search iterations to 10 and retrieve the top-5 results per search. Table 4 reports the results of DR Tulu with three different retrievers, both before and after applying test-time corpus scaling. BM25 benefits most from corpus scaling. On short-form questions, BM25 achieves absolute gain of 8.18%, LLM-based retrievers exhibit only modest improvements. This is largely because BM25 is more sensitive to keyword signals, while LLM-based retrievers, as discussed, struggle when documents approach input-length limits. Therefore, the added information makes documents only marginally easier for them. Limited improvement on open-ended questions. All three retrievers show only marginal improvements on open-ended questions. This result aligns with our earlier observation at section 4.4 that DR Tulu (and other deep research agents) generated query lacks diversity, which limits retrieval breadth and prevents corpus-level scaling from fully translating into downstream performance gains."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce SAGE, benchmark for reasoningintensive scientific literature retrieval. Through extensive evaluation, we reveal critical finding: LLM-based retrievers underperform BM25 by approximately 30% in deep research agent workflows, as existing agents generate keyword-oriented subqueries. To address this limitation, we propose corpus-level test-time scaling, which enriches papers with metadata and LLM-generated keywords, and achieves consistent improvements. Our work highlights that effective collaboration between retrievers and agents requires further adaptation. such as directly editing or rewriting each paper."
        },
        {
            "title": "Limitations and Future Work",
            "content": "We acknowledge limitations in our study. We do not perform instruction fine-tuning or alignment on the open-source deep-research agents. As result, we are unable to assess whether training agents to adapt their query generation strategies based on the underlying retriever type could improve performance. Exploring such retriever-aware agent training remains valuable direction for future work. Additionally, most of our behavioral analysis is conducted on DR Tulu, whose post-training procedures may significantly influence the observed agent behaviors. Consequently, our findings may not fully generalize to agents with different training recipes or base model architectures."
        },
        {
            "title": "Acknowledgements",
            "content": "Tiansheng Hu and Chen Zhao were supported by NYU Shanghai Center for Data Science. This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise."
        },
        {
            "title": "References",
            "content": "Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. 2025. Agent s2: compositional generalist-specialist framework for computer use agents. In Conference on Language Modeling. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. LLM2vec: Large language models are secretly powerful text encoders. In Conference on Language Modeling. Ines Besrour, Jingbo He, Tobias Schreieder, and Michael Färber. 2025. Ragenta: Multi-agent retrieval-augmented generation for attributed question answering. arXiv preprint arXiv:2506.16988. Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024. M3embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through selfIn Findings of the Associknowledge distillation. ation for Computational Linguistics. Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin. 2025. Browsecomp-plus: more fair and transparent evaluation benchmark of deep-research agent. arXiv preprint arXiv:2508.06600. Yuri Chervonyi, Trieu H. Trinh, Miroslav Olšák, Xiaomeng Yang, Hoang Nguyen, Marcelo Menegali, Junehyuk Jung, Junsu Kim, Vikas Verma, Quoc V. Le, and Thang Luong. 2025. Gold-medalist performance in solving olympiad geometry with alphageometry2. arXiv preprint arXiv:2502.03544. Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023. Precise zero-shot dense retrieval without relevance labels. In Proceedings of the Annual Meeting of the Association for Computational Linguistics. GoogleDeepmind. 2024. Gemini deep research. Official blog post introducing Gemini Deep Research. GoogleDeepmind. 2025a. Gemini 2.5 flash best for fast performance on everyday tasks. Official blog post introducing Gemini 2.5 Flash models. GoogleDeepmind. 2025b. Gemini 2.5: Our most intelligent ai model. Official blog post introducing Gemini 2.5 Pro models. Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Huichi Zhou, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, and Jun Wang. 2025. Deep research agents: systematic examination and roadmap. arXiv preprint arXiv:2506.18096. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training LLMs to reason and leverage search engines with reinforcement learning. In Conference on Language Modeling. Xiangyu Li, Yawen Zeng, Xiaofen Xing, Jin Xu, and Xiangmin Xu. 2025a. QuantAgents: Towards multiagent financial system via simulated trading. In Findings of the Conference on Empirical Methods in Natural Language Processing. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025b. Search-o1: Agentic search-enhanced large reasoning models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yongkang Wu, Ji-Rong Wen, Yutao Zhu, and Zhicheng Dou. 2025c. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023. Towards general text embeddings with multi-stage contrastive learning. arXiv preprint arXiv:2308.03281. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query rewriting in retrievalaugmented large language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Jorj X. McKie. 2025. Pymupdf: Python bindings for mupdf. Version 1.26.7. Niklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2025a. Generative representational instruction tuning. In The International Conference on Learning Representations. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candes, and Tatsunori Hashimoto. 2025b. s1: Simple test-time scaling. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large dual encoders are generalizable retrievers. In Proceedings of theConference on Empirical Methods in Natural Language Processing. OpenAI. 2025a. Introducing gpt-5. Official blog post introducing GPT-5 models. OpenAI. 2025b. Openai deep research. Official blog post introducing OpenAI Deep Research. Perpelexity. 2025. Introducing perplexity deep research. Official blog post introducing Perplexity Deep Research. Qwen. 2025. Qwen3-next: Towards ultimate training & inference efficiency. Official blog post introducing Qwen3-Next family. Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. In Proceedings of The 1994. Okapi at TREC-3. Text REtrieval Conference. Pedro Rodriguez and Jordan Boyd-Graber. 2021. EvalIn Prouation paradigms in question answering. ceedings of the Conference on Empirical Methods in Natural Language Processing. Rulin Shao, Akari Asai, Shannon Zejiang Shen, Hamish Ivison, Varsha Kishore, Jingming Zhuo, Xinran Zhao, Molly Park, Samuel G. Finlayson, David Sontag, Tyler Murray, Sewon Min, Pradeep Dasigi, Luca Soldaini, Faeze Brahman, Wen tau Yih, Tongshuang Wu, Luke Zettlemoyer, Yoon Kim, Hannaneh Hajishirzi, and Pang Wei Koh. 2025a. Dr tulu: Reinforcement learning with evolving rubrics for deep research. Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen tau Yih, Pang Wei Koh, and Luke Zettlemoyer. 2025b. ReasonIR: Training retrievers for reasoning tasks. In Conference on Language Modeling. Jeremy Singer-Vine and Samkit Jain. 2025. pdfplumber: Plumb pdf for detailed information about each character, rectangle, and lineplus text and table extraction. Version 0.11.8. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2025. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The International Conference on Learning Representations. Hongjin SU, Howard Yen, Mengzhou Xia, Weijia Shi, Niklas Muennighoff, Han yu Wang, Liu Haisu, Quan Shi, Zachary Siegel, Michael Tang, Ruoxi Sun, Jinsung Yoon, Sercan Arik, Danqi Chen, and Tao Yu. 2025. BRIGHT: realistic and challenging benchmark for reasoning-intensive retrieval. In The Thirteenth International Conference on Learning Representations. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, Kuan Li, Liangcai Su, Litu Ou, Liwen Zhang, Pengjun Xie, Rui Ye, Wenbiao Yin, Xinmiao Yu, Xinyu Wang, Xixi Wu, Xuanzhong Chen, Yida Zhao, Zhen Zhang, Zhengwei Tao, Zhongwang Zhang, Zile Qiao, Chenxi Wang, Donglei Yu, Gang Fu, Haiyang Shen, Jiayin Yang, Jun Lin, Junkai Zhang, Kui Zeng, Li Yang, Hailong Yin, Maojia Song, Ming Yan, Minpeng Liao, Peng Xia, Qian Xiao, Rui Min, Ruixue Ding, Runnan Fang, Shaowei Chen, Shen Huang, Shihang Wang, Shihao Cai, Weizhou Shen, Xiaobin Wang, Xin Guan, Xinyu Geng, Yingcheng Shi, Yuning Wu, Zhuo Chen, Zijian Li, and Yong Jiang. 2025. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: technical report. arXiv preprint arXiv:2402.05672. Ziqi Wang and Boqin Yuan. 2025. L-mars: Legal multiagent workflow with orchestrated reasoning and agentic search. arXiv preprint arXiv:2509.00761. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. 2025. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516. Orion Weller, Benjamin Van Durme, Dawn Lawrie, Ashwin Paranjape, Yuhao Zhang, and Jack Hessel. 2025a. Promptriever: Instruction-trained retrievers can be prompted like language models. In The International Conference on Learning Representations. Orion Weller, Kathryn Ricci, Eugene Yang, Andrew Yates, Dawn Lawrie, and Benjamin Van Durme. 2025b. Rank1: Test-time compute for reranking in information retrieval. In Conference on Language Modeling. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations. Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Yixin Liu, Xiangru Tang, Joseph Chee Chang, Jesse Dodge, Jonathan Bragg, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, and Arman Cohan. 2025. Sciarena: An open evaluation platform for non-verifiable scientific literatureIn The Thirty-ninth Annual Congrounded tasks. ference on Neural Information Processing Systems Datasets and Benchmarks Track. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. DeepResearcher: Scaling deep research via reinforcement learning in real-world environments. In Proceedings of the Conference on Empirical Methods in Natural Language Processing."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Query-Answer Example . . . . . A.2 Query-Decomposition Case Study A.3 Document Length Distribution . . A.4 Comparison with BrowseCompPlus: Retriever Behavior . . . . . with Experiments SearchR1-32B . . . . . . . . . . . A.6 Prompt Templates . . . . . . . . . A.5 Further 12 12 12 14 14"
        },
        {
            "title": "A Appendix",
            "content": "A.2 Query-Decomposition Case Study A.1 Query-Answer Example Query: Find me the paper that matches this requirement: Published between 2023 and 2025 (inclusive), presented at the Annual Meeting of the Association for Computational Linguistics, and building on 40 prior works. key visualization shows their method dramatically improves T5-small versus the vanilla baseline: typical JGA gains of 0.150.25, with striking +0.25 jump on Task 4 (their method T5 = 0.58 vs vanilla T5 = 0.33). This paper and APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference both cite Movement Pruning: Adaptive Sparsity by Fine-Tuning for weight-gradient salience scoring to guide which parameters to prune or tune, and Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning for sensitivity, smoothing, and uncertainty measures to allocate and remove tuning parameters. They share 7 citations in total and are specifically related by salience-driven pruning and adaptive budget allocation for parameter-efficient fine-tuning. Answer: TaSL: Continual Dialog State Tracking via Task Skill Localization and Consolidation Figure 5: Example of Short-Form question. Query: am working on multi-agent reinforcement learning in general-sum Markov games, specifically focusing on learning Nash Equilibrium, Coarse Correlated Equilibrium, and Correlated Equilibrium using oracle-based algorithmic frameworks. am considering comparing sample-efficient methods for equilibrium learning under stationary environments with general function approximation (as in MAMEX and related complexity measures) against black-box, multiscale restart approaches designed for non-stationary, timevarying settings. Please give me some background and methodological basis references for establishing theoretical foundations and performance benchmarks for regret minimization and equilibrium computation in MARL. Answer: Black-box Approach for Non-stationary Multi-agent Reinforcement Learning Sample-Efficient Multi-Agent RL: An Optimization Perspective ... Query: Find me the paper that matches this requirement: Published in the period from 2021 to 2025, presented at the Annual Meeting of the Association for Computational Linguistics conference, and written by 5 authors. Its primary figure demonstrates that the architecture explicitly leverages both source KBQA training data and target KBQA few-shot examples to influence retrieval, reranking, and generation, highlighting transfer from source domains plus few-shot adaptation to the target as core design choice. This paper and RetinaQA: Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions both cite Fewshot In-context Learning on Knowledge Base Question Answering for its few-shot in-context generatethen-ground sketching pipeline and Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases for the GrailQA dataset and s-expression logical-form formalism. They share 17 citations in total and are specifically related by emphasis on transferable sketch-based logical-form generation and evaluation on GrailQA. Decomposed Search Queries: 1. site:aclanthology.org ACL 2021 knowledge sketch answering base generation transfer few-shot question GrailQA 2. site:aclanthology.org knowledge base question answering few-shot transfer GrailQA sketch 2024.acl 3. RetinaQA arXiv PDF 2403.10849 4. Semantic Scholar RetinaQA 2403. references 5. site:semanticscholar.org 2403.10849 references GrailQA 6. site:aclanthology.org 2024.acl-long RetinaQA RetinaQA 7. Shu and Yu 2024 KBQA few-shot in-context learning TIARA retriever ACL 8. ACL 2024 KBQA transfer few-shot retriever source site:aclanthology.org generation figure shots reranking target 2024.acl-long few 9. Beyond I.I.D.: Generalization for on Knowledge Bases ACL Anthology Three Question Levels of Answering Answer: Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with InContext Learning Figure 6: Example of Open-Ended question. Figure 7: GPT-5 Query Decomposition Example. Query: Find me the paper that matches this requirement: Published in the period from 2021 to 2025, presented at the Annual Meeting of the Association for Computational Linguistics conference, and written by 5 authors. Its primary figure demonstrates that the architecture explicitly leverages both source KBQA training data and target KBQA few-shot examples to influence retrieval, reranking, and generation, highlighting transfer from source domains plus few-shot adaptation to the target as core design choice. This paper and RetinaQA: Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions both cite Fewshot In-context Learning on Knowledge Base Question Answering for its few-shot in-context generatethen-ground sketching pipeline and Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases for the GrailQA dataset and s-expression logical-form formalism. They share 17 citations in total and are specifically related by emphasis on transferable sketch-based logical-form generation and evaluation on GrailQA. Decomposed Search Queries: 1. RetinaQA knowledge base question answering ACL 2. FuSIC-KBQA ACL few-shot transfer learning KBQA figure retrieval reranking LLM 3. RetinaQA ACL 2024 GrailQA s-expression citations \"Few-shot In-context Learning Question Answering\" Knowledge authors Base on 4. RetinaQA ACL authors arXiv 2403.10849 GrailQA s-expression 5. RetinaQA ACL 2024 authors arXiv 2403.10849 GrailQA s-expression citations Answer: Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with InContext Learning Figure 8: Dr-Tulu Query Decomposition Example. Figure 9: Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Computer Science domain. Figure 10: Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Healthcare domain. Figure 11: Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Humanities domain. noise. This asymmetric coverage therefore biases the comparison toward dense retrievers. In our experiments, we allow up to 32,000 tokens per document, so dense retrievers do not benefit from short-prefix encoding. Agent strength and query decomposition modulate retriever sensitivity. BrowseComp-Plus further suggests that stronger agent models (e.g., GPT-5 and o3) are less sensitive to retriever choice, as the gap between BM25 and Qwen3-Embed8B narrows compared to weaker models (e.g., Qwen3-32B and Gemini-2.5 Flash). Moreover, BrowseComp-Plus adopts ReAct-style framework (Yao et al., 2023) that produces naturallanguage sub-queries, while our setup (GPT-5 series, Gemini-2.5 series, and DR-Tulu) uses more keyword-oriented decomposition. This difference in query formulation can shift the relative advantage between lexical and dense retrievers. A.5 Further Experiments with SearchR1-32B As supplement to our main experiments with DR Tulu, we evaluate another open-source deepresearch agent, SearchR1-32B (Jin et al., 2025). Table 5 summarizes corpus-search performance across domains. SearchR1-32B exhibits near single-shot retrieval. SearchR1-32B issues only 1.11.2 searches per question, leaving limited room for iterative query refinement. Consequently, end-toend performance is primarily determined by the initial query formulation and the base retriever. Natural-language querying does not obviate lexical matching. Although SearchR1-32B produces natural-language queries rather than keyword-style decompositions, BM25 remains markedly stronger on short-form questions. On open-ended questions, BM25 and gte-Qwen are closer in performance, while ReasonIR remains substantially worse, conImportantly, the sistent with previous findings. average number of references is similar across retrievers, suggesting that the observed differences are driven by retrieval quality rather than references counts. A.6 Prompt Templates Figure 12: Distribution of markdown length (in tokens) for 1,000 randomly sampled documents from the Natural Science domain. A.3 Document Length Distribution A.4 Comparison with BrowseComp-Plus:"
        },
        {
            "title": "Retriever Behavior",
            "content": "We observe retriever ranking that differs from BrowseComp-Plus (Chen et al., 2025). In our experiments, BM25 consistently outperforms LLM-based dense retrievers (e.g., gte-Qwen27B-Instruct), whereas BrowseComp-Plus reports stronger performance from dense retrievers such as Qwen3-Embed-8B. We attribute the discrepancy to differences in (i) task characteristics, (ii) retriever implementations, and (iii) agent model strength and query decomposition. Longer documents and weaker answer locality in our setting. BrowseComp-Plus uses substantially shorter documents on average (6733 tokens vs. 13376 in ours) and exhibits strong early-answer locality: truncating documents to the first 512 tokens still preserves the ground-truth answer in at least one gold document for 86.5% of queries. This property favors dense retrievers that primarily represent the document prefix. In contrast, our documents are longer and evidence is more dispersed, reducing the effectiveness of limited-window dense encoding. under early-answer only Asymmetric text coverage can favor dense retrievers locality. BrowseComp-Plus the first encodes 4096 tokens for Qwen3-Embed-8B, while BM25 indexes the full document. When answers are front-loaded, prefix-only dense encoding can act as an implicit denoising mechanism, whereas lexical full-text BM25 may incur additional Method Com. Sci. Healthcare Humanities Nat. Sci. Avg. Searches Avg. Refs Avg. Perf. CORPUS SEARCH (SEARCHR1-32B) Short-Form Questions (Exact Match) BM25 k=5 45.26 gte-Qwen k=5 23.44 ReasonIR k=5 20.90 19.57 4.29 3.79 Open-Ended Questions (Weighted Recall) BM25 k=5 gte-Qwen k=5 ReasonIR k=5 12.58 11.78 5. 6.80 7.57 4.65 34.13 26.23 15.70 13.52 12.73 8.19 40.91 16.18 11.28 10.20 7.48 2.93 1.21 1.24 1. 1.16 1.10 1.11 5.7 5.7 5.5 5.6 5.2 5.4 34.97 17.54 12.92 10.77 9.89 5.32 Table 5: Corpus-search results with SearchR1-32B. Academic Paper Keyword Generation Prompt Based on the following academic paper content, generate exactly 8 keywords that best represent the main topics, methods, or contributions of this paper. Content: {content[:20000]} Return ONLY the 8 keywords separated by commas, nothing else. Example format: keyword1, keyword2, keyword3, keyword4, keyword5, keyword6, keyword7, keyword8 Figure 13: Prompt of Academic Paper Keyword Generation Prompt for Analyzing Shared Reference Functions System: You are research assistant helping to analyze academic citations. Provide concise, accurate summaries. User: Shared Citation Paper: \"{shared_title}\" by {shared_authors} Paper 1: Target Paper - Citing Contexts: 1. {context_1} 2. {context_2} ... - Intents: {intents} Paper 2 (shared {shared_count} papers with Target Paper): \"{cited_paper_title}\" - Citing Contexts: 1. {context_1} 2. {context_2} ... - Intents: {intents} Task: Summarize in ONE sentence what role this shared citation paper played for both papers. Focus on the specific contributions or methods it provided to each paper. Figure 14: Prompt for Analyzing the Functional Role of Shared References Between Two Papers Prompt for Generating Comprehensive Summaries of Paper Relationships System: You are research assistant analyzing paper relationships. Write concise, specific summaries: Maximum 2 sentences Mention ONLY 2 most important shared citations by title Use This paper for Paper 1, full title for Paper 2 Be specific about what the citations are used for Avoid generic phrases and long lists User: Paper 1 (This Paper): {paper1_title} Paper 2: {cited_paper_title} These two papers share {shared_count} common citations. Analysis of shared citations: 1. {citation_title} by {citation_authors} Paper 1 uses it for: {intents} Paper 2 uses it for: {intents} How both papers use it: {summary} 2. ... (additional shared citations) Task: Write concise summary (2 sentences maximum) that: 1. Uses This paper to refer to Paper 1 2. Uses the full title when referring to Paper 2 3. Selects and mentions 2 important shared citations by title (in quotes). Avoid famous papers mentioned only as convention, e.g., Attention is All You Need 4. Briefly explains what commonality these key citations reveal 5. Mentions that they share {shared_count} citations in total 6. Be specific and concreteavoid generic statements Format: First sentence: Introduce the 2 key shared citations and their specific role Second sentence: State They share {N} citations in total and are specifically related by [brief commonality]. Additional constraints: No parentheses or brackets; keep each part simple and direct. Figure 15: Prompt for Generating Comprehensive Summaries of Shared References Relationships Between Paper Pairs Prompt for Selecting the Most Characteristic Summary System: You are helpful assistant that selects the most characteristic summary. Return only single integer. User: You are given list of paper summaries. Each summary describes the relationship between source paper and cited paper. Your task: Select exactly 1 summary that is MOST characteristic and informative. Choose the summary that provides the most specific, concrete technical details Prefer summaries that mention distinctive methods, techniques, or research approaches Avoid generic or vague descriptions Summaries: [0] {summary_0} [1] {summary_1} [2] {summary_2} ... (additional summaries) Return ONLY single integer index (e.g., 0, 1, 2, etc.) No explanation needed, just the number. Query Construction: After selecting index i, the final query is constructed as: Find me the paper that have the following characteristics: {selected_summary} Figure 16: Prompt for Selecting the Most Characteristic Summary from Multiple Paper Relationship Descriptions and Constructing Retrieval Query"
        }
    ],
    "affiliations": [
        "Center for Data Science, New York University",
        "NYU Shanghai",
        "Yale University"
    ]
}