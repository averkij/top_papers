{
    "paper_title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs",
    "authors": [
        "Xi Fang",
        "Weijie Xu",
        "Yuchong Zhang",
        "Stephanie Eckman",
        "Scott Nickleach",
        "Chandan K. Reddy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated user independent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight a key challenge for memory enhanced AI: systems designed for personalization may inadvertently reinforce social inequalities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 5 0 9 9 0 . 0 1 5 2 : r The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs Xi Fang1*, Weijie Xu1*, Yuchong Zhang1, Stephanie Eckman1, Scott Nickleach1, Chandan K. Reddy1 1Amazon"
        },
        {
            "title": "Abstract",
            "content": "When an AI assistant remembers that Sarah is single mother working two jobs, does it interpret her stress differently than if she were wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated userindependent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight key challenge for memoryenhanced AI: systems designed for personalization may inadvertently reinforce social inequalities."
        },
        {
            "title": "Introduction and Related Work",
            "content": "Large language models (LLMs) now incorporate sophisticated long-term memory that persists across conversations (Fountas et al., 2024; Zhong et al., 2023; Wang et al., 2023), while demonstrating remarkable emotional capabilities that can surpass human performance on standardized tests by over 40% (Schlegel et al., 2025). These systems promise to remember our preferences, understand our context, and respond with finely-tuned emotional intelligence (Li et al., 2023). *Equal contribution. Email:lxifan@amazon.com. 1 Yet this convergence of personalization and emotional intelligence may harbor an insidious problem: the potential for social bias to become encoded in AIs emotional reasoning. Consider how an AI assistant might interpret stress differently when it remembers that Sarah is single mother working two jobs versus wealthy executive. While researchers have studied how to personalize LLMs for user preferences and tasks (Ning et al., 2024; Doddapaneni et al., 2024), we lack critical understanding of how this personalization affects emotional reasoning across diverse user populations. This knowledge gap becomes particularly concerning in high-stakes domains like mental healthcare and educational technology, where biased emotional responses could amplify existing socioeconomic disparities and compromise service quality for marginalized populations (Weissburg et al., 2025; Schnepper et al., 2025). Drawing on Bourdieus theory of social capital (Bourdieu, 1985), we can understand how user information creates personalization trap: social position across economic, cultural, and social dimensions shapes how others interpret our actions and emotions. When AI systems incorporate user background information, they risk replicating these societal biases (Shin et al., 2024; Hida et al., 2024), potentially processing identical emotional situations differently based on who the user appears to be. We address three research questions: (RQ1) Does adding user profiles to system memory influence LLMs emotional understanding abilities? (RQ2) How do different identities (gender, age, race, ethnicity) shape LLMs emotional understanding, and what biases emerge? (RQ3) How do biases in LLMs emotional understanding translate into their emotionrelated recommendations and guidance? Figure 1: An illustration demonstrating how User profiles affect AI models Emotional comprehension. Our evaluation of 15 models on validated emotional intelligence tests reveals troubling reality: user memory systematically shapes LLMs emotional judgments, with identical scenarios producing markedly different interpretations based on user profiles. Multiple high-performing models exhibit larger shifts in emotional understanding for users with disadvantaged profiles, along with systematic demographic biases across gender, religion, and age (Table 1, Figure 3, 4), suggesting that personalization may be internalizing social hierarchies directly into the models reasoning processes."
        },
        {
            "title": "2 Methods",
            "content": "To assess how user memory affects emotional reasoning, we created diverse profiles via two methods: explicit manipulation of social capital and intersectional control of demographic variables. LLMs were then evaluated on two validated emotional intelligence tests after removing culturally variable items."
        },
        {
            "title": "2.1 User Profile Generation",
            "content": "Explicit user profile generation. We constructed user personas by sampling thirty base profiles from Persona Hub, each containing short description outlining occupation, expertise, and background(Ge et al., 2025). We then generated two versions of each persona, drawing on Bourdieus framework (Bourdieu, 1985), which posits four dimensions of social stratification: Demographics, Family background, Social connections, and Personal assets. The advantaged version of each profile featured demographic privileges, beneficial connections, and access to resources and opportunities across the four dimensions. Conversely, the disadvantaged version introduced structural barriers, limited resource access, and challenges in each dimension (Figure 2). We used DeepSeek-R1 for profile generation due to its performance in creative writing tasks (Wu et al., 2025). Intersectional persona generation. To examine how demographic identities interact to shape LLM responses, we extracted demographic information from the international PRISM dataset (Kirk et al., 2024). We constructed 81 personas by combining Figure 2: Explicit user profile generation and emotional tasks. four demographic dimensions gender (3), age (3), religion (3), and ethnicity (3) each representing unique profile. This design allows us to examine how these demographic characteristics interact to influence LLMs emotional understanding."
        },
        {
            "title": "2.2 Emotional Intelligence Assessment\nEmotional understanding (STEU). We em-\nployed the Situational Test of Emotional Under-\nstanding (STEU) (MacCann and Roberts, 2008), a\nvalidated instrument assessing how accurately in-\ndividuals recognize and reason about othersâ€™ emo-\ntions across 42 hypothetical scenarios, capturing\nboth emotional recognition and emotional rea-\nsoning.",
            "content": "Emotional guidance (Modified STEM). To assess emotion-related behavioral recommendations, we adapted the Situational Test of Emotion Management (STEM) (MacCann and Roberts, 2008; Schlegel et al., 2025), which comprises 44 vignettes depicting individuals experiencing negative emotions (anger, sadness, fear, or disgust) in personal and professional contexts. We transformed the original third-person scenarios into first-person consultative prompts (e.g., What should Alex do 2 when feeling anxious about presentation? Im feeling anxious about my upcoming presentation. What should do?), shifting the task from abstract emotional judgment to personalized emotional support and enabling assessment of how LLMs provide emotion-guided behavioral advice. Human Annotation. The STEU/STEM scales include correct answer for each item which should not vary with the persona. To identify questions where the correct answers do vary, we had 9 qualified annotators independently review all items and flag questions which might reasonably vary across demographic or cultural contexts (e.g., due to differences in social privilege or lived experience). Items flagged by 20% annotators were removed following consensus review, resulting in the exclusion of nine questions from each dataset (Appendix for detailed annotation process)."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluated emotional understanding and emotion-related suggestive behaviors across 15 language models spanning architectures and capabilities. We inject memory in the system prompt for main experiments but also explore other memory injection methods in ablation studies (Zhang et al., 2024, 2025b,a). See Appendix for implementation details and ablation study results. In Experiment 1 (RQ1), we evaluated 15 models on the STEU dataset, comparing performance with and without explicit user profiles to quantify the influence of user memory. We reported both absolute accuracy and flip rate, defined as the proportion of predictions that changed relative to the No-Memory baseline. Results revealed systematic behavioral patterns; however, the use of complex personas (e.g., full professor at Stanford University vs. an adjunct at regional university) hampered our ability to isolate demographic effects. To address this shortcoming, Experiment 2 (RQ2) employed intersectional personas to quantify how demographic variables (gender, age, religion, and ethnicity) influence model responses. Building on these findings, Experiment 3 (RQ3) used the revised STEM instrument to evaluate three models emotion-based behavioral recommendations across the same intersectional personas. For RQ2RQ3, we analyzed errors using mixedeffects models to estimate demographic effects on accuracy. The models predicted the probability of correct response, with demographic factors as fixed effects and question-level variation as random effect. The baseline was white, christian, male aged 2534; negative coefficients indicated lower accuracy relative to this group. Model No Mem. Adv. Disadv. Claude 3.7 Sonnet Claude 3.5 Sonnet DeepSeek-R1 Llama 3.2 90B Llama 3.1 405B Llama 4 Maverick Ministral-8B-Instruct-2410 Qwen3 4B Claude 3.5 Haiku DeepSeek-V3 Phi4 reasoning GPT-OSS 20B Command Qwen2.5 7B Phi-4-mini-instruct 90.91 84.85 84.85 84.85 78.79 78.55 72.73 72.73 69.05 69.70 66.67 63.64 63.64 63.64 54.55 80.10* 83.33* 81.62* 64.91* 64.42* 75.96* 72.73 74.81 67.47* 68.99* 66.97 69.92* 60.91* 58.69* 54.08* 77.37* 82.03* 76.57* 62.24* 62.31* 70.81* 73.84 75.58 67.89* 68.18* 66.26 69.49* 60.36* 65.56 54.23* Table 1: Models emotional understanding performance (STEU scores) across memory conditions. Asterisks (*) indicate significant (p-value < 0.05) difference from the No Memory condition. Dagger () indicates significant (p-value < 0.05) difference between Advantaged condition and Disadvantaged condition."
        },
        {
            "title": "4 Results and Discussion",
            "content": "Our experiments reveal three key findings: Finding 1: User memory systematically influences emotional understanding. Incorporating user profiles into model memory significantly altered performance relative to the no-memory baseline, with statistically significant differences observed in 11 of the 15 evaluated models. For nearly all affected models, performance decreased once user memory was introduced, except for GPT-OSS (Further studied in Appendix E). Interestingly, we observe significant disparities when given advantaged user profiles (wealthy, well-connected users) compared to disadvantaged profiles (users facing economic or social barriers) across multiple highperforming models. Claude 3.7 Sonnet (80.10% vs.77.37%), DeepSeek-R1 (81.62% vs.76.57%), and Llama 3.2 90B (64.91% vs.62.24%) all demonstrate substantial performance gaps favoring advantaged profiles. Disadvantaged profile also elicits higher flip rate from the No-memory baseline (Figure 4). Finding 2: Models show demographic biases in emotional understanding. Several models show different bias when profiles are Muslim, non-binary, or over 65+ (Figure 3, top panel). For instance, DeepSeek R1 performed better with Christian users than with Muslim while it performed better with older personas. In con3 Figure 3: Model performance varies by user demographics in both emotional understanding (top) and guidance tasks (bottom). Bars show performance differences compared to baseline users (white, non-religious, male, aged 25-34). Positive values mean better performance. Figure 4: Models emotional Reasoning impacted by User profile demonstrated by flip rate (the proportion of predictions that changed relative to the No-Memory baseline). ***: < 0.001, **: < 0.01, *: < 0.05 trast, Qwen 3 4B showed inferior performance with elderly users compared with middle-aged but much better performance towards Muslim and nonbinary personas. Models with thinking capabilities showed lower biases than their standard versions. Finding 3: Demographic biases persist when models give emotional advice. The biases we found in emotional understanding is also significant when models provide emotional guidance and suggestions. Most bias exists in age and gender attributes (Figure 3, bottom panel). For instance, Claude 3.7 was significantly worse at helping female and non-binary than male personas, while Qwen 3 4B Thinking continued to perform better with female and non-binary users. Error Analysis. We examined reasoning traces from large reasoning models on misclassified cases. With the exception of GPT-OSS, most models integrated persona information during inference, often overweighing it and introducing bias (Table 7-8). This tendency to personalize reasoning led to systematic performance declines when user memory cues were present. Correlation analysis (Appendix E.2) further revealed that top models shared highly similar response patterns, reflecting common bias sources, whereas correlations among other thinking models were low, indicating diversity in reasoning."
        },
        {
            "title": "5 Conclusions\nOur findings\nreveal a critical paradox in\nemotionally-aware AI: attempts to enhance empa-\nthy through personalization may inadvertently am-\nplify social inequities. Incorporating user memory\nconsistently alters emotional reasoning, often re-\nducing performance in ways that favor privileged\nover disadvantaged personas. This personalization-\nfairness tension necessitates novel approaches to\nbalance adaptive capabilities with equitable perfor-\nmance across demographic groups.",
            "content": ""
        },
        {
            "title": "6 Limitations",
            "content": "The STEU and STEM tests are validated instruments based on human expert-defined consensus answers. We further validated the influence of personas by human experts. Thus, score differences should be interpreted as correctness. Importantly, the original tests present thirdperson, hypothetical scenarios, yet the models responses varied systematically depending on the user-specific memory. This finding suggests that user memory can inappropriately influence general reasoning, even in contexts that should be userindependent. We could benefit from doing more error analysis. We did not purpose mitigation strategy. Future work should explore mechanisms to disentangle user-specific adaptation from task-general reasoning, and investigate mitigation strategies for memory-induced bias."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Pierre Bourdieu. 1985. The social space and the genesis of groups. Theory and Society, 14(6):723744. Sumanth Doddapaneni, Krishna Sayana, Ambarish Jash, Sukhdeep Sodhi, and Dima Kuzmin. 2024. User embedding model for personalized language prompting. Preprint, arXiv:2401.04858. Zafeirios Fountas, Martin Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. 2024. Humanlike episodic memory for infinite context llms. Preprint, arXiv:2407.09450. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2025. Scaling synthetic data creation with 1,000,000,000 personas. Preprint, arXiv:2406.20094. Rem Hida, Masahiro Kaneko, and Naoaki Okazaki. 2024. Social bias evaluation for large language Preprint, models requires prompt variations. arXiv:2407.03129. Hannah Rose Kirk, Alexander Whitefield, Paul RÃ¶ttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera, Max Bartolo, Adina Williams, He He, Bertie Vidgen, and Scott A. Hale. 2024. The prism alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Preprint, arXiv:2404.16019. Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, and Xing Xie. 2023. Large language models understand and can be enhanced by emotional stimuli. Preprint, arXiv:2307.11760. MacCann and RD Roberts. 2008. New paradigms for assessing emotional intelligence: theory and data. Emotion, 8(4):540551. Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn OBanion, and Jun Xie. 2024. User-llm: Efficient llm contextualization with user embeddings. Preprint, arXiv:2402.13598. Katja Schlegel, Nils Sommer, and Marcello Mortillaro. 2025. Large language models are proficient in solving and creating emotional intelligence tests. Communications Psychology, 3:80. Rebecca Schnepper, Niklas Roemmel, Rainer Schaefert, Lukas Lambrecht-Walzinger, and Gunther Meinlschmidt. 2025. Exploring biases of large language models in the field of mental health: Comparative questionnaire study of the effect of gender and sexual orientation in anorexia nervosa and bulimia nervosa case vignettes. JMIR Mental Health, 12:e57986. Jisu Shin, Hoyun Song, Huije Lee, Soyeong Jeong, and Jong Park. 2024. Ask LLMs directly, what shapes your bias?: Measuring social bias in large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1612216143, Bangkok, Thailand. Association for Computational Linguistics. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023. Augmenting language models with long-term memory. Preprint, arXiv:2306.07174. Zheng Wang, Zhongyang Li, Zeren Jiang, Dandan Tu, and Wei Shi. 2024. Crafting personalized agents through retrieval-augmented generation on editable memory graphs. Preprint, arXiv:2409.19401. Iain Weissburg, Sathvika Anand, Sharon Levy, and Haewon Jeong. 2025. Llms are biased teachers: Evaluating llm bias in personalized education. Preprint, arXiv:2410.14012. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. 2025. Writingbench: comprehensive benchmark for generative writing. Preprint, arXiv:2503.05244. Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, and Chandan K. Reddy. 2025. Quantifying fairness in llms beyond tokens: semantic and statistical perspective. Preprint, arXiv:2506.19028. 5 Kai Zhang, Yejin Kim, and Xiaozhong Liu. 2025a. Personalized llm response generation with parameterized memory injection. Preprint, arXiv:2404.03565. Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, and Xian Li. 2025b. Personaagent: When large language model agents meet personalization at test time. Preprint, arXiv:2506.06254. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and JiRong Wen. 2024. survey on the memory mechanism of large language model based agents. Preprint, arXiv:2404.13501. Zhehao Zhang, Weijie Xu, Fanyou Wu, and Chandan K. Reddy. 2025c. Falsereject: resource for improving contextual safety and mitigating overrefusals in llms via structured reasoning. Preprint, arXiv:2505.08054. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023. Memorybank: Enhancing large language models with long-term memory. Preprint, arXiv:2305.10250."
        },
        {
            "title": "A Experiment Settings",
            "content": "STEU and STEM are in English. In Experiment 2 and 3, we include demographics such as age (25-34 years old, 35-64 years old and 65 + years old), gender (male, female or non-binary), religions (nonreligious, Muslim or christian), and race (Asian, Black and White). We have tested 2520 questions in experiment 1, 3402 question in experiment 2 and 3564 in experiment 3. A.1 Memory Implementations To inject memories, we explicitly encode user information as structured text within the system prompt at the start of each interaction, following the method described in Zhang et al. (2024). This augmented prompt is concatenated with the users current input and then passed to the LLM. Ablation studies compare this direct injection method with memory retrieval-based augmentation, as discussed in Wang et al. (2024). We use titan text embedding version 2 as our embeddings with maximum length equal to 8,192 tokens. Facebook AI SImilarity Search (FAISS) to implement, and retrieve top 3 relevant sentences per question. A.2 Hyperparameters To ensure consistent and high-quality outputs across different models, we standardized the decoding hyperparameters for most model generations by setting the temperature to 0 (to promote deterministic outputs), top-p (nucleus sampling) to 0.95 (to allow for balance between diversity and relevance), and maximum token limit of 128 tokens (Anthropic, 2024; Xu et al., 2025; Zhang et al., 2025c). Recognizing the enhanced reasoning capabilities of certain models, we adjusted the configurations accordingly. For Claude 3.7 Thinking, we set the thinking budget to be 16k. For R1 and other reasoning mode, we set max new tokens to be 16k. This is to provide enough budget for reasoning models to finish thinking. A.3 Test Scoring For STEU, the number of correct responses for each persona was recorded. Models are tested with or without user memory provided. Mean score is calculated as the number of correct answers/total question answered. The modified STEM responses were scored on 4-point scale, with points (4 to 1) assigned based on expert-weighted rankings from highest to lowest. To ensure deterministic outputs and eliminate sampling variability, we set the generation temperature to 0 in all experiments. We calculate the mean scores and standard deviations across user profiles/personas for each LLM. A.4 Compute Resources We use AWS Bedrock batch inference for large models inference, including Claude 3.5 Sonnet, Claude 3.7 Sonnet, Claude 3.5 Haiku, Llama 3.2 90B, Llama 4, Llama 3.1 70B, DeepSeek R1, and Mistral Large V2. For Claude 3.7 Sonnet Reasoning and DeepSeek R1, we utilize AWS cross-region inference. Models such as Qwen2.57B-Instruct, Qwen3-4B, c4ai-command-r7b-122024, Phi-4-mini-reasoning, Phi-4-mini-instruct, Ministral-8B-Instruct-2410, DeepSeek-R1-DistillLlama-8B, and DeepSeek-R1-Distill-Qwen-7B are accessed via Hugging Face endpoints. For experiments that require accessing models hidden states and log probs. We run inference on one EC2 p4d.24xlarge (Nvidia A100 40GiB GPU) instance and one EC2 p4d.24xlarge (Nvidia A100 40GiB GPU) in Sydney(ap-southeast-2) region. We have used them for 55 to 60 hours for opensource model inference using vLLM as our inference framework. We have also attached 8000GiB disk volume with AL2023 Linux OS image. We use HuggingFace and PyTorch as the main software 6 frameworks. A.5 Models Used See Table 2 for specifications and details for all evaluated large language models."
        },
        {
            "title": "B Human Labeling",
            "content": "The correct answer of STEU/STEM may influence be correlated with user personas. In real-world settings, additional context (e.g., financial hardship, social privilege) can legitimately change emotional interpretation. Thus, we used human labeling to remove questions that could be influenced by user personas. B.1 Human Labeling Design We use Amazon Ground Truth (formerly MTurk) where annotator expertise cannot be pre-filtered, so we screen for persona sensitivity in two phases. First, each EQ question is shown without any persona and we retain only responses from annotators who answer correctly (quality gate). Second, for each retained question, we randomly sampled two personas one advantaged and one disadvantaged and asked whether two annotators with those personas, both aiming to be correct on third-person question, would give different answers. We collect answers until we have nine valid annotations per second question. We also ask each each annotator judge three persona pairs (advantaged vs. disadvantaged). Each question is judged per 9 annotators given 3 different personas pair. They judge each question 2 independently. We paid each annotator 0.96 dollar per question set and did not enable automated data labeling. We then dropped any question for which 20% of these judgments indicate different answers, and we discarded annotations completed in less than minutes (speeding filter). Applying these rules led us to remove nine questions in each dataset. We provide an example in Table 3."
        },
        {
            "title": "C Ablation Study Results",
            "content": "The ablation study compared two memory injection methods using direct injection method or memory retrieval-based augmentation. As shown in Table 4, the two methods yielded similar results where advantaged profiles received significantly higher performance compared to disadvantaged profiles. To avoid retrieving algorithms influence on results, we used the first approach for the main experiments."
        },
        {
            "title": "D Mixed Effects Model",
            "content": "We ran separate mixed effects models for each LLM and report and compare the coefficients. Each mixed effects model is specified as: = XÎ² + Zu + Îµ (1) In this hierarchical modeling framework, fixed effects represent population-level parameters that are constant across all decision questions, while random effects capture question-specific deviations that are assumed to follow normal distribution. Here, is 0/1 variable indicating whether the question was answered correctly, is the design matrix for the fixed effect predictors (with columns for intercept, age, gender, and race), Î² is the vector of coefficients for the fixed effects representing the average population-level associations, is the design matrix for the random effects (with columns for the question number and its interactions with each of the three demographic variables), is the vector of random effect coefficients representing question-specific deviations from the population averages (with (0, G)), and Îµ is the vector of error terms for each observation. Because all models use identical input data and model specifications, the resulting coefficients are directly comparable and reveal differences in how each LLM responds to demographic information. While models may differ in overall performance (captured by the overall intercept), the slope coefficients isolate demographic effects independent of baseline accuracy. We fit the models in python (statsmodels) to estimate Î², u, and 95% confidence intervals around these terms. Figure 3 reports the Î² coefficients and confidence intervals."
        },
        {
            "title": "E Error Analysis",
            "content": "E.1 Reasoning Models Error Deep Dive We classify errors into five main types. (i) Persona Distraction occurs when irrelevant persona details influence the reasoning process. (ii) Complexity Overreach involves the unnecessary exploration of irrelevant pathways, complicating the solution. (iii) Logic Inconsistency manifests as incoherent reasoning with disconnected conclusions. (iv) Context Fabrication is the generation of fictional scenarios 7 Table 2: Model cards summarizing specifications and details for all evaluated large language models. Model Name Creator Complete Model ID Release Hosting Anthropic Claude 3.5 Sonnet Anthropic Claude 3.7 Sonnet Claude 3.7 Sonnet Thinking Anthropic DeepSeek R1 Meta Llama 3.2 90B Meta Llama 4 Maverick Anthropic Claude-3.5 Haiku Meta Llama 3.1 405B Mistral AI Mistral Large V2 Microsoft Phi4 reasoning Cohere for AI Command Alibaba Qwen2.5 7B Alibaba Qwen 3 4B Alibaba Qwen 3 4B Thinking DeepSeek R1-Distill-Llama Phi-4-mini-instruct Microsoft Ministral-8B-Instruct-2410 Mistral AI DeepSeek R1-Distill-Qwen 06/20/24 AWS Bedrock anthropic.claude-3-5-sonnet-20240620-v1:0 02/24/25 AWS Bedrock anthropic.claude-3-7-sonnet-20250219-v1:0 02/24/25 AWS Bedrock anthropic.claude-3-7-sonnet-20250219-v1:0 01/20/25 AWS Bedrock deepseek.r1-v1:0 09/25/24 AWS Bedrock meta.llama3-2-90b-instruct-v1:0 2025 AWS Bedrock meta.llama4-maverick-17b-instruct-v1:0 10/22/24 AWS Bedrock anthropic.claude-3-5-haiku-20241022-v1:0 07/23/24 AWS Bedrock meta.llama3-1-405b-instruct-v1:0 07/24/24 AWS Bedrock mistral.mistral-large-2407-v1:0 04/15/25 Hugging Face microsoft/phi-4-mini-reasoning Hugging Face 2024 c4ai-command-r7b-12-2024 09/19/24 Hugging Face Qwen/Qwen2.5-7B-Instruct Hugging Face 2025 Qwen/Qwen3-4B Qwen/Qwen3-4B Hugging Face 2025 deepseek-ai/DeepSeek-R1-Distill-Llama-8B 02/01/25 Hugging Face Hugging Face 2025 microsoft/phi-4-mini-instruct Hugging Face mistral-8b-instruct-2410 2024 Hugging Face deepseek-ai/DeepSeek-R1-Distill-Qwen-7B 2025 or constraints not present in the original query. (v) Priority Misalignment describes the LLMs failure to distinguish between critical and trivial information, leading to misguided focus in problemsolving. These categories encompass the primary ways in which LLM may deviate from effective reasoning and problem-solving (See example in Figure 4). We use claude 4 sonnet and gpt oss 120B to classify all errors in emotional understanding experiments. We have shared our prompt in Table 6 If they disagree on any question, we will show both models reasoning process and final answer for human to judge. Each question with disagreement is judged by 9 annotators. We filter out any answer completed by 1 min and do maximum vote to get final error clasisfication. We share the result of disadvantaged personas in Figure 7 and advanataged personas in Figure 8. Advantaged version in general lead to all kinds of errors instead of just persona distraction. This demonstrates that LLMs overall reasoning capability decreases after injecting disadvantaged personas. Notiably, the disadvantaged version demonstrated significant error rate in priority misalignment, where LLMs struggled to effectively differentiate between critical and peripheral information within queries. This manifested as either an overemphasis on irrelevant details during reasoning processes or the oversight of crucial information. GPT OSS 120B showed greater resilience against such distractions when operating with dis8 advantaged personas and maintained more focused reasoning patterns. This explains why it performs better with memory. E.2 Model Correlations Analysis As shown in Figure 5, correlations were calculated across selected reasoning and non-reasoning models across 36 intersectional persona and 42 questions. Figure 5: Correlation Analysis of raw predicted outputs. Table 3: This is an example of question 1 and question 2. We only count the answer of question 2 whenever the annotator answer the question 1 correctly and spend 2 mins in total. Question 1 This is standard EQ test and please be as objective as possible. The question is about the 3rd person you do not know in person. Please assume there is one correct answer which is better than the rest. Answer the following question: Shona has not spoken to her nephew for months, whereas when he was younger they were very close. She rings him but he can only talk for five minutes. What action would be the most effective for Shona? A. Realize that he is growing up and might not want to spend so much time with his family any more. B. Make plans to drop by and visit him in person and have good chat. C. Understand that relationships change, but keep calling him from time to time. D. Be upset about it, but realize there is nothing she can do. Background descriptions of two annotators: Annotator 1: { demographics: [Holds an endowed chair in historical studies at an Ivy League university, with recognition as leading authority in 18th-century Atlantic history., Maintains affiliations with elite research institutes in London and Paris, leveraging global academic networks., Identifies as white male, benefiting from institutional prestige and gender/racial biases in academia.], family_background: [Born to tenured professors at Stanford, with childhood access to private tutors and rare archival collections., Attended boarding schools emphasizing classical education, later graduating debt-free from Oxfords history program., Inherited generational wealth enabling unpaid research fellowships and international fieldwork.], social_connections: [Serves on boards of major historical societies and consults for documentary filmmakers, amplifying public influence., Hosts salons connecting diplomats, Pulitzer-winning journalists, and museum directors., Regularly keynote speaker at Davos-adjacent think tanks shaping cultural heritage policies.], personal_assets: [Owns restored Georgian townhouse near campus with private library of first-edition historical texts., Sponsors annual research expeditions to Versailles-funded archives., Pursues haute cuisine tourism and collects antique maritime maps as leisure activities.] } Annotator 2: { demographics: [Works as an adjunct professor across three community colleges, juggling medieval and early modern survey courses unrelated to specialization., Identifies as Black woman facing pay disparities and tokenization in regional state schools., Resides in Rust Belt city with dwindling humanities funding.], family_background: [Raised by single mother working night shifts as hospital janitor, sharing foreclosed home with extended family., Took out predatory loans for state college degree while caregiving for disabled siblings., Lacks mentorship, with dissertation research delayed by childcare costs.], social_connections: [Excluded from conference panels due to unpaid membership fees in historical associations., Relies on fragmented online networks of overworked adjuncts sharing syllabi hacks., Avoids collaborating with senior scholars after experiencing exploitative co-authorship demands.], personal_assets: [Rents basement apartment with mold issues near highway, using public library Wi-Fi for research., Sells plasma to afford interlibrary loan fees for primary sources., Sporadic freelance fact-checking for true crime podcasts replaces sustained archival work., Leisure limited to scanning Reddit history threads during bus commutes.] } Question 2 Will Annotator 1 and Annotator 2 provide the same answer to Question 1? (Answer: Yes or No) Table 4: Comparison of model performances between advantaged and disadvantaged versions for using FAISS or inject directly as system memory (System). Values are presented as mean standard deviation. * indicates < 0.001 between advantaged and disadvantaged versions. Model Mistral Large V2 Claude 3.5 Sonnet Llama 4 Maverick Adv FAISS 66.35 2.71 78.33 2.82* 69.29 3.02* Dis FAISS 64.52 4.94 74.37 2.70* 65.63 2.77* Adv System 66.51 3.12 79.68 2.48* 68.57 3.33* Dis System 65.00 4.11 74.92 1.62* 61.90 2.50* Table 5: Summary of model performance under three memory conditions. Last 3 columns are associated p-values. Model Name No Memory Advantaged Disadvantaged Adv vs No Disadv vs No Adv vs Disadv Claude 3.5 Sonnet Claude 3.7 Sonnet R1 Llama 3.2 90B Llama 4 Maverick Claude-3.5 Haiku Llama 3.1 405B Mistral Large V2 Phi4 reasoning Command Qwen2.5 7B Qwen 3 4B R1-Distill-Llama Phi-4-mini-instruct Ministral-8B-Instruct-2410 R1-Distill-Qwen 85.71 80.95 78.57 73.81 73.81 69.05 69.05 64.29 61.90 59.52 59.52 59.52 57.14 52.38 50.00 45.24 79.68 2.48 73.41 2.73 73.10 2.18 56.65 2.26 68.57 3.33 57.86 2.27 56.60 2.68 66.51 3.12 60.55 3.99 60.79 3.47 57.93 2.01 60.79 2.85 50.24 4.30 49.05 2.22 48.81 3.05 46.98 4.63 74.92 1.62 69.92 3.67 68.89 2.99 54.73 2.95 61.90 2.50 59.13 2.94 55.30 3.60 65.00 4.11 60.16 4.72 57.06 3.45 58.57 2.97 65.00 3.38 51.82 4.32 49.60 2.87 40.88 3.37 47.30 4.71 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 0.075 0.054 <0.001 0.021 <0.001 <0.001 0.041 0.048 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 <0.001 0.352 0.052 <0.001 0.090 <0.001 <0.001 <0.001 <0.001 0.023 <0.001 <0.001 <0.001 0.007 <0.001 0.066 0.120 0.115 0.727 <0.001 0.337 <0.001 0.159 0.405 <0.001 0. 10 Table 6: The prompt for error classification of emotional understanding responses. <instruction> You are model for analyzing the errors made by chatbot in an emotional understanding question. Your task is to review the provided information, including the chatbots memory about the person, the question and the chatbots answer, the correct answer, and the chatbots reasoning. Based on this information, you will classify the error made by the chatbot according to the following potential error classifications: <error_classifications> A. Persona Distraction: The chatbot uses any information from personas [LLM_PERSONAS] during reasoning stage [LLM_REASONING]. B. Complexity Overreach: The chatbot creates unnecessary complexity by exploring multiple irrelevant pathways in [LLM_REASONING] that dont contribute to the final answer [LLM_ANSWER]. C. Logic Inconsistency: The reasoning process in [LLM_REASONING] lacks coherent progression and jumps between different answers without clear connections. D. Context Fabrication: The chatbot generates additional scenarios or assumptions in [LLM_REASONING] not present in the original question [QUESTION] or personas [LLM_PERSONAS]. E. Priority Misalignment: The chatbot fails to distinguish between critical information and useless details in the given question [QUESTION]. This error results in using useless details in reasoning [LLM_REASONING]. F. None of the above. </error_classifications> <guidelines> - If the LLM is asking for help with an emotional understanding question, the answer should not retrieve any persona information. - If [LLM_REASONING] has no reasoning, you should classify it as \"none of the above\" / <answer>F</answer>, which is extremely uncommon. - If [LLM_REASONING] is extremely short, you may classify it as \"none of the above\" / <answer>F</answer> if you believe there is not enough information to make classification. - One reasoning could have multiple errors. In that case, you should provide all applicable error choices, such as <answer>AD</answer> or <answer>BCE</answer>. - If [LLM_PERSONAS] is NA. Then, the error classifications cannot be A. </guidelines The conversation will be presented in the following format: [LLM_PERSONAS] [QUESTION] [CORRECT_ANSWER] [LLM_ANSWER] [LLM_REASONING] The answer is incorrect, which means the reasoning is incorrect. Your classification should only apply to the last message marked by [LLM_REASONING]. The prior messages are included to provide context for classifying the final message. </instruction> <output_format> Provide your classification choice in the <answer></answer> tag, as well as your confidence level from 1-5 (1 being least confident, 5 being most confident) in the <score></score> tag. </output_format 11 Table 7: Error Classification Distribution Across Different Models For Disadvantaged Personas Error Category DeepSeek-R1 Llama 4 Maverick mini-reasoning Phi-4Persona Distraction Complexity Overreach Logic Inconsistency Context Fabrication Priority Misalignment None of the above 70.70 8.20 0.39 2.34 11.72 6. 39.53 3.99 2.99 18.27 26.25 8.97 16.26 43.60 7.96 15.92 7.27 9.00 GPT OSS Qwen3 4B claude 3.7 20B 3.56 5.33 12.89 2.67 21.33 54.22 43.37 23.76 12.71 1.10 11.88 7. 29.55 0.00 0.00 0.38 4.55 65.53 Table 8: Error Classification Distribution Across Different Models For Advantaged Personas Error Category DeepSeek-R1 Llama 4 Maverick mini-reasoning Phi-4Persona Distraction Complexity Overreach Logic Inconsistency Context Fabrication Priority Misalignment None of the above 92.92 0.28 0.57 1.42 3.68 1.13 50.15 2.77 0.31 14.15 29.54 1.29 39.51 27.96 2.74 16.41 5.47 3.08 GPT OSS Qwen3 4B claude 3.7 20B 1.81 3.62 9.95 5.88 31.67 47.06 66.94 13.71 9.68 2.15 4.30 3.23 38.02 0.00 0.00 0.90 2.40 58."
        }
    ],
    "affiliations": [
        "Amazon"
    ]
}