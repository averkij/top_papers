{
    "paper_title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning",
    "authors": [
        "Jiwon Song",
        "Dongwon Jo",
        "Yulhwa Kim",
        "Jae-Joon Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 6 8 3 1 . 5 0 5 2 : r Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning Jiwon Song 1 Dongwon Jo 1 Yulhwa Kim Jae-Joon Kim 1 1 Seoul National University 2 Sungkyunkwan University {jiwon.song, dongwonjo, kimjaejoon}@snu.ac.kr {yulhwakim}@skku.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60 compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) equipped with reasoning capabilities have expanded the application of LLMs beyond simple natural language processing tasks to logical problem-solving tasks such as Science, Technology, Engineering, and Mathematics (STEM) problems and code generation. Early reasoning approaches primarily focused on guiding LLMs through explicit step-by-step logic to facilitate more interpretable and accurate outcomes [1]. Recently, advanced reasoning LLMs, such as OpenAI o1 [2], DeepSeek-R1 [3], adopted the concept of test-time compute scaling [4, 5]. This method involves generating longer, iterative reasoning outputs, which significantly enhance accuracy. Such iterative generation allows models to carefully evaluate intermediate reasoning steps, refine outputs through internal reflection, and ultimately handle tasks requiring complex reasoning. Though reasoning LLMs have been widely adopted due to their ability to handle complex tasks through complicated reasoning processes, reasoning LLMs face challenges regarding inference efficiency due to their tendency to generate long reasoning sequences. The long token sequences required for detailed reasoning processes substantially increase the KV cache overhead during inference. For example, the reasoning path of OpenAIs o3-mini-high can exceed 50K tokens [6], and Claude 3.7 Sonnet [7] supports reasoning sequences of up to 64K tokens. Such long token generation imposes critical memory and computational overhead, significantly slowing down inference. Consequently, it is crucial to develop KV cache compression techniques to mitigate these inference efficiency issues and support practical deployment of reasoning LLMs. Although there are several existing works on compressing KV caches for long sequences [8, 9, 10, 11], these works primarily focus on efficient handling of long input prompts. In contrast, the problem of Preprint. Under review. Figure 1: Example of reasoning path of reasoning LLM. Redundant reasoning steps (e.g., repeated checks and re-derivations) are visually highlighted, illustrating the semantic sparsity that motivates our compression method. The parts highlighted in the same color are semantically identical. efficiently managing the KV cache for long generated sequences has received limited attention. Unlike input prompts, whose importance can be easily assessed at prefill stage [8], generated tokens pose challenge because their future relevance is often unpredictable. As token seemingly insignificant at one point might become crucial later, naively discarding such tokens can substantially degrade model accuracy. However, as illustrated in Figure 1, we observe that sequences generated during reasoning processes exhibit distinct properties compared to sequences generated in conventional LLM decoding. Specifically, reasoning sequences frequently revisit previous cases or repeat similar logic, so they have low information density relative to their length. We refer to this phenomenon as the semantic sparsity of reasoning paths. This sparsity highlights the inefficiency of retaining all KV entries and the possibility to selectively remove KV cache associated with low-importance tokens without disrupting the overall reasoning process. Motivated by this observation, we propose Reasoning Path Compression (RPC), method for accelerating inference in reasoning LLMs by compressing the KV cache associated with explicit thinking tokens. RPC compresses KV cache periodically during decoding, significantly reducing overhead compared to previous step-wise compression techniques which compress KV cache at each decoding step. At each compression interval, it estimates token importance based on attention allocation over recent window and retains only the top-ranked entries according to fixed compression ratio. This design preserves recent context while discarding low-impact KV entries, mitigating performance degradation. By applying RPC to QwQ-32B [12], we reduce the KV cache size of generated tokens by up to 75%, and improve decoding throughput by up to 1.60, while keeping the pass@1 drop on the AIME 2024 [13] dataset within 1.2% compared to the inference with full KV cache."
        },
        {
            "title": "2 Background",
            "content": "2.1 Reasoning LLMs Reasoning LLMs solve problems by generating explicit intermediate steps, known as reasoning paths, instead of directly producing an answer [2, 3, 12, 14]. This behavior is reinforced by the way such models are trained: reasoning LLMs are typically fine-tuned with reinforcement learning objectives that reward correct answers after multi-step inference, thereby encouraging longer generations. 2 Figure 2: (a) Token generation throughput and (b) peak memory of QwQ-32B at different generation lengths. The results are evaluated on 4 H100 GPUs with batch size 16. Consequently, the lengths of generated sequences increase as training progresses [15], with reasoning LLM outputs frequently spanning 16K to 32K tokens. Such extensive token generation significantly enlarges the KV cache, increasing memory usage and reducing inference throughput. As shown in Figure 2, generating sequences of 16K to 32K tokens dramatically reduces throughput while sharply increasing peak memory usage. To mitigate the overhead of generating long reasoning paths, previous works have attempted to train reasoning LLMs to shorten reasoning sequences [16, 17, 18, 19, 20, 21]. These approaches utilize length-aware training objectives: either encouraging the generation of short sequences or introducing mechanisms to compress tokens into latent representations. However, their effectiveness typically remains limited when applied to complex reasoning benchmarks widely used to evaluate modern reasoning LLMs (e.g. LiveCodeBench [22]). For example, although LightThinker [21] achieves competitive accuracy with shortened reasoning paths on relatively simpler reasoning tasks like MMLU [23] and BBH [24], our experimental results in Section 4.2 indicate significant performance degradation when evaluated on more complex reasoning benchmarks. This discrepancy arises primarily due to the conflicting training objectives. The reasoning-oriented objectives aim to promote detailed reasoning steps, whereas the length-aware objectives encourage shorter outputs. Thus, effectively training reasoning LLMs to consistently produce shorter reasoning paths remains challenging. 2.2 KV Cache Compression The degradation of throughput and the increase in memory usage observed when processing long sequences with LLMs primarily result from growth in KV cache size. Thus, there are many attempts to directly compress the KV cache, but these works primarily focus on efficient handling of long input prompts. For example, SnapKV [8] and HeadKV [9] are specifically designed to compress KV cache associated with long input contexts. These methods do not address the compression of generated tokens produced in reasoning paths. Other techniques like H2O [10] and TOVA [11] attempt to extend KV cache compression mechanisms to support basic levels of compression during generation. These methods maintain the KV cache within predefined budget by evicting tokens whenever the cache reaches this size limit during decoding. However, their designs predominantly target scenarios involving long input sequences and relatively short outputs, they are effective when indentifying and evicting less relevant input tokens is critical for efficient output generation. Hence, H2O and TOVA struggle to preserve accuracy when applied to reasoning LLMs (see Section 4.2). Moreover, while setting fixed KV cache budget is straightforward in input-dominated scenarios, it is challenging to predifine cache budgets for reasoning LLMs, as they inherently produce long output sequences of varying lengths. Overall, there are currently no KV cache compression methods suitable for reasoning LLMs."
        },
        {
            "title": "3 Reasoning Path Compression",
            "content": "3.1 Motivation: Semantic Sparsity of Reasoning Paths Reasoning LLMs do not directly generate the final answer. Instead, they produce reasoning paths, which often contain redundant segments offering little new information, such as repeated logical 3 Figure 3: 3-gram Shannon entropy comparison between reasoning LLM and non-reasoning LLM steps or re-evaluations of previous generated reasoning. As previously presented in Figure 1, such redundancy is frequently observed in model-generated reasoning paths. Additional examples are provided in Appendix A. We refer to this phenomenon, the presence of extended spans of generated tokens that are semantically redundant, as semantic sparsity. To quantify semantic sparsity, we compute the n-gram Shannon entropy using base-2 logarithm, defined as: Hn = (cid:88) gGn p(g) log2 p(g) (1) where Gn denotes the set of all unique n-grams of length n, and p(g) is the empirical probability of each n-gram g. To analyze semantic sparsity of reasoning paths, we compare the redundancy in sequences generated by conventional LLMs and reasoning LLMs. For this comparison, we use 3-gram entropy to measure phrase-level repetition and evaluate two models with identical architecture (LLaMA-3.18B-Instruct [25]): DeepSeek-R1-Distill-Llama-8B [3], reasoning-oriented model, is tested on AIME 2024 [13], and LongWriter-8B [26], tuned for long-form writing, is tested on subset of HelloBench [27] consisting of prompts that require generating outputs exceeding 8192 tokens. As shown in Figure 3, DeepSeek-R1-Distill-Llama-8B consistently exhibits lower 3-gram entropy than LongWriter-8B across output lengths from 1024 to 8192 tokens. This indicates higher phraselevel repetition in reasoning paths compared to general long-form writing. These results provide quantitative evidence of semantic sparsity, suggesting that large portions of the reasoning trace can be compressed with minimal impact on overall coherence. 3.2 Overview of Reasoning Path Compression We introduce Reasoning Path Compression (RPC), KV cache compression framework tailored for reasoning LLMs (Figure 4). RPC leverages the semantic sparsity inherent in reasoning paths to efficiently eliminate KV entries. The key insight motivating RPC is that reasoning LLMs generate explicit reasoning steps, and many of these reasoning steps lose relevance as reasoning process progress. Exploiting this observation, RPC periodically compresses redundant KV entries during Illustration of RPC with compression interval = 4, selector window = 2, and Figure 4: compression ratio = 4. At each compression step, recent tokens are used to evaluate the importance of previously generated tokens. 4 token generation. Moreover, since recently generated tokens inherently rely on the context provided by preceding tokens, these recent tokens serve as essential indicators of contextual importance. Thus, RPC assesses the relevance of previously generated tokens by analyzing how strongly they are attended to by the most recent tokens, referred to as selector window. All compression decisions are made dynamically during inference based solely on attention-derived statistics. Hence, RPC does not require any model modification or additional training, and it is straightforward to integrate RPC into existing inference pipelines of reasoning LLMs. 3.3 Periodic KV Cache Compression Dynamics of RPC One of the unique features of RPC compared to other KV cache compression methods is its periodic approach to KV cache compression. The KV cache compression dynamics of RPC is controlled by two critical hyperparameters: the compression interval , which represents how frequently KV cache compression is triggered, and the size of the selector window R, which denotes the number of recent tokens used to assess importance. As illustrated in Figure 4, RPC waits for + tokens to be generated to start the first compression cycle. At this point, the importance of the initial set of tokens is evaluated using the selector window composed of the most recent tokens. Given target compression ratio c, RPC retains only the top tokens based on their importance scores. Subsequent compression cycles are triggered each time an additional tokens have been generated. It is important to note that during each periodic compression cycle, RPC evaluates combined set comprising both tokens retained from the previous cycle and newly generated tokens, rather than compressing only newly generated ones. By jointly reassessing all these tokens at each cycle, RPC naturally allows outdated tokens to gradually fade out as the reasoning path advances. As result, the reasoning context remains properly updated and relevant throughout the inference process, even after multiple cycles of KV cache compression. Specifically, at the second compression cycle, the selector window, now updated to include the latest tokens, evaluates the importance of the previously retained tokens and the newly generated set of tokens. Among these + tokens, RPC retains only the top 2P tokens with the highest importance scores and discards the rest. Generalizing this procedure, at the -th compression cycle, the total number of tokens evaluated with the selector window is (N 1)P + . RPC retains only the top tokens with the highest importance scores from this set. As selector tokens are always preserved, the total number of KV entries remaining after the -th compression cycle is + R. As shown in Figure 5, this periodic compression effectively regulates the size of KV cache over time. Figure 5: KV cache size with and without RPC To fully leverage the advantages of periodic compression, the compression interval must be carefully selected. small value may lead to accuracy degradation after compression, as the semantic context is too limited. On the other hand, large provides broader semantic context for effective compression, while it introduces computational inefficiency and higher peak memory usage by delaying the compression. Given the significance of the compression interval , an ablation study analyzing its impact and recommendations derived from the analysis are discussed in Section 4.4. 3.4 Important Token Selection with Selection Window Another unique feature of RPC is the concept of the selector window used for selecting important tokens. Previous KV cache compression methods employ different strategies for calculating token importance. For example, SnapKV computes attention scores relative to the final tokens in the input prompt, based on the observation that the last segment of the input shows similar attention allocation pattern to the generation stage. H2O averages attention scores across all preceding tokens, and TOVA mimics RNN operations by reusing the attention scores calculated during token generation as gating Algorithm 1: Important token selection algorithm of RPC Input: generation step t, query of step qt, KV cache CKV , selector query cache CQ Output: updated CKV , updated CQ // Cache selector queries if (t R) 0 and (t R) mod < then"
        },
        {
            "title": "Append qt to CQ",
            "content": "// Compress KV cache every steps if (t R) 0 and (t R) mod = 0 then Importance of tokens in CKV ; Ctmp KV cache with topN CKV Ctmp CKV [R :] ; CQ [] ; return CKV , CQ // Get importance score importance scores ; // Retain important KV cache // Retain KV cache of selector window // Reset selector query cache scores for the KV cache eviction. In contrast, RPC leverages the observation that recently generated tokens in reasoning paths represent logical outputs derived from preceding contexts. Therefore, attention scores relative to these recent tokens can effectively indicate the relevance of previously generated tokens. The algorithm for selecting important tokens in RPC is presented in Algorithm 1. RPC evaluates token importance using attention scores aggregated across selector window of the most recent tokens and all attention heads. Then, to promote coherent token selection and reduce token-level noise, RPC applies local average pooling. Formally, the importance of each past token at each layer is defined as: Importance(t) = 1 2w + 1 (cid:88) (cid:88) (cid:88) i=w r= h=1 Attnℓ h(qr, kt+i) (2) Here, Attnℓ h(qr, kt+i) denotes the attention weight from the r-th selector token to token generated at + i-th generation step at head of layer ℓ. The pooling window size controls the smoothing level, encouraging contiguous selection of semantically related tokens. To eliminate redundant computations and efficiently compute these importance scores, RPC caches the query vectors of selector tokens. The selector window size determines how many recent tokens RPC uses to assess the importance of previously generated tokens. smaller may lead to unstable or noisy importance estimations, as scoring can be dominated by limited number of tokens. In contrast, larger values of increase memory overhead by requiring additional caching of query vector. Thus, choosing an appropriate value for involves balancing the robustness in token scoring with computational overhead. detailed ablation study and recommendation for optimal values are provided in Section 4.4."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Models and Datasets. We evaluate RPC using two open-source reasoning LLMs with different model sizes: DeepSeek-R1-Distill-Qwen-7B with 7B parameters [3] and QwQ-32B with 32B parameters [12]. All outputs are generated using nucleus sampling with temperature = 0.6 and top-p = 0.95. For QwQ-32B, we additionally set top-k = 40 following its recommended decoding configuration. The maximum number of generated tokens is capped at 32768, following the default settings of tested models. Datasets. Our evaluation covers three reasoning-intensive benchmarks: American Invitational Mathematics Examination (AIME) 2024 for mathematical reasoning, LiveCodeBench [22] for coding tasks, and IFEval [28] for instruction following. We sample completions per instance to compute pass@1, where = 8 for AIME 2024, = 4 for LiveCodeBench, and = 1 for IFEval. 6 Table 1: Comparison between RPC (P = 4096) and baselines on AIME 2024 (pass@1). DeepSeekR1-Distill-Qwen-7B is used for evaluation. Method Full KV Cache LightThinker AIME 2024 (pass@1) Compression Ratio 55.5 1.0 6.7 1.4 H2O 45.0 4.0 TOVA 31.7 4.0 RPC 52.9 4.0 Table 2: Additional accuracy (%) evaluation with proposed RPC. Method DeepSeek-R1-Distill-Qwen-7B QwQ-32B AIME 2024 (pass@1) LiveCodeBench (pass@1) IFEval (pass@1) AIME 2024 (pass@1) LiveCodeBench (pass@1) IFEval (pass@1) Full KV Cache RPC (P = 4096) RPC (P = 1024) 55.5 52.9 50.4 37.6 35.9 33. 55.1 56.6 57.3 79.5 78.3 78.3 63.4 62.2 61.2 83.9 82.6 81.7 Implementation Details. Our implementation uses FlashAttention-2 [29] as the attention kernel for all decoding layers and is built on top of HuggingFace Transformers v4.45 [30]. Unless otherwise specified, we use the following default RPC hyperparameters: We set the selector window size to 32 and apply local pooling with window size = 3 for importance smoothing. The compression interval is set to 1024 or 4096. The target compression ratio is set to 4 by default. Baselines. We compare our proposed RPC with training-based reasoning path compression method, LightThinker [21], and previous KV cache compression techniques, H2O [10] and TOVA [11]. To ensure fair comparison with H2O and TOVA, we set their KV cache budgets to match the compression ratio (4) of RPC. Specifically, we measure the generation lengths of original reasoning LLMs with full KV caches and define the KV cache budget as 25% of these lengths. Meanwhile, LightThinker does not offer direct control over the compression ratio, so we measure its effective compression ratio after inference. 4.2 Accuracy Evaluation Comparison with Baselines. For the comparison with baselines, we evaluate accuracy of DeepSeekR1-Distill-Qwen-7B on AIME 2024 dataset. LightThinker achieves the lowest accuracy at 6.7%. Our measurement further reveals that LightThinker only attains compression ratio of 1.4, whereas RPC and other baselines consistently achieve 4 compression ratio. This demonstrates the limited effectiveness of conventional length-aware training approaches for reasoning LLMs. Although H2O and TOVA achieve relatively higher accuracies than LightThinker, with 45.0% and 31.7%, respectively, their accuracy still remains significantly lower than that of the original model with full KV cache, 55.5%. Additionally, the requirement of predefined KV cache budget restricts their applicability in real-world scenarios. In contrast, the proposed RPC achieves an accuracy comparable to the case with full KV cache. Specifically, RPC reaches an accuracy of 52.9%, without any additional training or knowledge of generation lengths ahead of inference. These experimental results clearly demonstrate that RPC effectively compresses KV cache in reasoning LLMs, whereas previous approaches suffer from significant accuracy degradation. Additional Evaluation with RPC. To verify the effectiveness of RPC across diverse benchmarks and reasoning LLMs, we conduct additional evaluations using three reasoning benchmarks (AIME 2024, LiveCodeBench, and IFEval) and two reasoning LLMs (DeepSeek-R1-Distill-Qwen-7B and QwQ-32B). As presented in Table 2, RPC with = 4096 successfully limits accuracy degradation compared to the original models with full KV cache within 2.6% for DeepSeek-R1-Distill-Qwen-7B. For the larger model QwQ-32B, accuracy degradation consistently remains within 1.3% across all evaluated benchmarks. It demonstrates the effectiveness of RPC across different models and reasoning tasks. Additionally, we observe that accuracy is notably influenced by the compression interval , with = 1024 typically resulting in lower accuracy than = 4096. Therefore, careful selection of is important, and we provide detailed ablation study examining its impact in Section 4.4. 4.3 Efficiency Evaluation We evaluate the efficiency of RPC in terms of token-generation throughput and peak memory usage. All experiments are conducted using an input prompt with 128 tokens and meausre throughput for Figure 6: (a) Throughput (tokens/s) and (b) peak memory usage (Gb) comparison between RPC (P = 4096) and full KV cache inference. generating sequences of 8192, 16384, 32768 tokens, with batch size of 16. The compression interval is set to 4096. Throughput and memory measurements for DeepSeek-R1-Distill-Qwen-7B are obtained on single NVIDIA H100 SXM GPU, while QwQ-32B evaluations are conducted on four H100 SXM GPUs. Figure 6 presents the throughput and peak memory improvements achieved by RPC relative to the original models with full KV cache. Additional analyses on the efficiency are also provided in Appendix B.1. Throughput. As shown in Figure 6(a), RPC consistently improves token generation throughput with particularly large gains observed for long generation length (e.g. 32768 tokens), scenario commonly encountered with reasoning LLMs. RPC achieves 1.68 throughput improvement when generating 32768 tokens with DeepSeek-R1-Distill-Qwen-7B, and 1.60 throughput improvement when generating 16384 tokens with QwQ-32B. Notably, QwQ-32B with full KV cache cannot handle reasoning tasks with generation lengths of 32768 tokens as it runs out of memory. However, RPC successfully enables token generation at this length. Memory Consumption. As shown in Figure 6(b), RPC effectively reduces peak memory usuage by periodically comperssing the KV cache. Since peak memory usage includes contributions from model parameters, intermediate activations, and the KV cache, the reduction in peak memory is not directly proportional to the KV cache compression ratio. Nevertheless, as the KV cache becomes the dominant factor in peak memory usage for longer generation lengths, RPC provides increasingly substantial memory savings as generation length grows. For DeepSeek-R1-Distill-Qwen-7B, RPC reduces peak memory usage from 75.7GB to 36.2GB when generating 32768 tokens, thereby RPC achieves over 50% memory reduction. Similarly, for QwQ-32B, RPC reduces the overall memory requirement over 50%, thereby resolving the out-of-memory issue for the generation of 32768 tokens. These results demonstrate that RPC effectively mitigates the memory bottleneck inherent in long-sequence generation of reasoning LLMs by compressing KV cache. 4.4 Ablation Studies To better understand the effect of key hyperparameters in RPC, we perform ablation studies on DeepSeek-R1-Distill-Qwen-7B using the AIME 2024 dataset. We analyze two critical components: the compression interval , which determines how often KV-cache compression is applied, and the selector window size R, which controls the number of recent tokens used for attention-based importance scoring. 8 Figure 7: Effect of compression interval on accuracy, throughput, and peak memory. Compression Interval. We evaluate compression interval from 4 to 16384 to examine the trade-off between compression interval, reasoning accuracy, and inference efficiency (throughput and peak memory). As shown in Figure 7, reasoning accuracy improves as increases. It indicates that overly frequent compression can disrupt the reasoning process by prematurely evicting tokens critical for subsequent reasoning steps. However, when becomes excessively large (e.g. = 8192), throughput declines and peak memory usage rises significantly, as large delays the KV cache compression. Therefore, selecting an appropriate value is essential to balance accuracy preservation and efficiency gains. Here, the configurations = 4096 and = 1024 represent practical choices that offer strong performance-efficiency balance in reasoning-intensive scenarios. Table 3: Effect of selector window size R. Metric 4096 1024 AIME 2024 (pass@1) Throughput (tokens/s) Peak Memory (GB) AIME 2024 (pass@1) Throughput (tokens/s) Peak Memory (GB) = 1 49.2 662.54 28. 45.8 746.21 24.62 8 48.3 662.84 28.72 49.2 745.08 24.62 32 52.9 671.38 28. 50.4 751.69 25.15 128 49.2 673.26 29.38 50.0 742.46 27.37 Selector Window Size. We evaluate the impact of the selector window size on the RPC algorithm by evaluating {1, 8, 32, 128}. As shown in Table 3, small values such as 1 and 8 yield relatively low accuracy (e.g. below 50% with = 4096), because small values can result in unstable selection of semantically critical tokens. This effect is more pronounced for = 1024 than = 4096, as tokens are evicted more frequently with smaller . Therefore, must be sufficiently large to ensure robust importance estimation. However, excessively large (e.g. 128) can negatively impact accuracy, as older selector tokens may not reflect the current reasoning context effectively. Because varying has only marginal effects on throughput and peak memory usage, accuracy is the primary factor when selecting an appropriate R. Based on our results, = 32 is the best choice as it provides the highest accuracy."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce Reasoning Path Compression (RPC) for compressing KV cache of reasoning LLMs. Our observation presents that reasoning paths often contain redundant segments and inherent semantic sparsity. RPC leverages this characteristic by periodically compressing the KV cache and employs an importance scoring mechanism based on selector window composed of recent queries. As RPC does not require any additional training or model modifications, it can be applied to broad range of reasoning LLMs. Experimental results demonstrate that RPC compress KV cache 4 with accuracy degradation limited to 1.2%. This aggressive KV cache compression results in up to 1.60 throughput improvement. Moreover, RPC successfully resolves the out-of-memory issue encountered with large reasoning models with 32B parameters when generating long reasoning paths of up to 32K tokens, by achieving over 50% reduction of overall memory requirement."
        },
        {
            "title": "References",
            "content": "[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [2] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [3] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [4] Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. Recursive introspection: Teaching language model agents how to self-improve. Advances in Neural Information Processing Systems, 37:5524955285, 2024. [5] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [6] Marthe Ballon, Andres Algaba, and Vincent Ginis. The relationship between reasoning and performance in large language modelso3 (mini) thinks harder, not longer. arXiv preprint arXiv:2502.15631, 2025. [7] Claude 3.7 Sonnet anthropic.com. https://www.anthropic.com/claude/sonnet. [Accessed 03-05-2025]. [8] Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. [9] Yu Fu, Zefan Cai, Abedelkadir Asi, Wayne Xiong, Yue Dong, and Wen Xiao. Not all heads matter: head-level kv cache compression method with integrated retrieval and reasoning. arXiv preprint arXiv:2410.19258, 2024. [10] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. [11] Matanel Oren, Michael Hassid, Nir Yarden, Yossi Adi, and Roy Schwartz. Transformers are multi-state rnns. arXiv preprint arXiv:2401.06104, 2024. [12] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [13] Mathematical Association of America. American invitational mathematics examination. https://maa. org/maa-invitational-competitions/. [Accessed 15-05-2025]. [14] LG AI Research. Exaone deep: Reasoning enhanced language models. arXiv preprint arXiv:2503.12524, 2025. [15] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [16] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. [17] Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. [18] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. [19] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. [20] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025. [21] Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression. arXiv preprint arXiv:2502.15589, 2025. [22] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [23] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021. [24] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [25] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [26] Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. Longwriter: Unleashing 10,000+ word generation from long context llms. arXiv preprint arXiv:2408.07055, 2024. [27] Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. [28] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. [29] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. [30] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. [31] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [32] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024."
        },
        {
            "title": "A Examples of Redundant Reasoning Paths",
            "content": "In this section, we provide additional redundant reasoning examples from the models that we used for evaluations. These examples clearly illustrate the semantic sparsity present in the generation outputs of reasoning LLMs. Ellipses ([...]) indicate omitted portions of the lengthy original output. In Example A.1, QwQ-32B [12] identifies the correct answer early but redundantly re-derives it multiple times, leading to excessive token generation. Each redundant derivation is highlighted with box to illustrate the inefficiency. The problem is sampled from AIME 2024 dataset [13]. Example A.1 Problem. There exist real numbers and y, both greater than 1, such that logx(yx) = logy(x4y) = 10. Find xy. QwQ-32B Output. <think> Okay, so have this problem here: there exist real numbers and y, both greater than 1, such that logx(yx) = 10, and logy(x4y) = 10. need to find the product xy. Hmm, logarithms with variables in the base and the argument. Let me try to unpack this step by step. First, lets recall some logarithm properties. [...] So now, have two equations: 1. = 10 2. 4y = 10 So maybe can solve for from one equation and substitute into the other? From equation 1: = 10 Plugging into equation 2: = 10 4xy = 100 xy = 25 = 10 4y 10/x = 10 4y 10 Wait, hold on, that seems too straightforward. But let me check. Wait, step by step: Equation 2 after substitution: 4y divided by (cid:0) 10 4xy (cid:1) is equal to 10. So dividing by (cid:0) 10 4xy 10 = 10 10 4y = 10 4xy = 100 xy = 25 (cid:1) is multiplying by 10 . Therefore, Hmm, so is that the answer? Wait, but let me make sure didnt skip any steps where assumptions might be wrong. Lets check if this holds. [...] Lets check the original equations. First equation: log5(55). Wait, log5(55) is 5, but the problem says it should be 10. So thats not good. Hmm. So = 5 and = 5 is not solution. Hmm, so maybe my assumption that xy = 25 is correct but and individually arent integers? Alternatively, maybe made an error in the substitution. Let me check again. [...] Let me denote Equation 1 as xv Let me multiply these two equations together: = 10 and Equation 2 as 4yu = 10. (cid:16) (cid:17) (cid:19) (cid:18) 4y = 10 10 4xy = 100 xy = Therefore, despite the complication with substitution, the product xy is indeed 25 . However, the problem states that such real numbers and exist, so even if we cant find exact values for and y, the product is 25 . Wait, but maybe we need to confirm that there actually exist such and y? [...] Therefore, even though we cant get exact values for and y, the product xy = 25 is indeed correct, and such numbers exist. Therefore, the answer is 25 . 12 Wait, but the problem says \"Find xy\", so maybe 25 is the answer, and the existence is given, so even without finding and y, through the algebra we can get that xy = 25. So maybe the answer is 25 . [...] logy(x) = 1 Cross-multiplying: 5 10 = 2y 50 = 2xy xy = 25 Oh! Wait, this is much simpler way! didnt need to go through all the substitutions and transcendental equations. Since logy(x) is logx(y) , substituting the expressions from both equations gives directly the relation. So indeed, logx(y) 5 10/x 5 2y = 2y = 10 1 5 2y = 50 = 2xy xy = 25 . So thats straightforward. The earlier complication was because tried to substitute variables and ended up in loop, but this reciprocal relationship is key here. Therefore, the answer is 25 . So think thats solid. **Final Answer** 25 </think> [...] similar pattern is observed in Example A.2, where DeepSeek-R1-Distill-Qwen-7B [3] repeatedly re-computes the correct solution despite having already reached it. Each redundant derivation is highlighted with box to illustrate the inefficiency. The problem is sampled from GSM8k dataset [31]. Example A. Problem. Alice has 20 quarters. She wants to exchange them for nickels and so she goes to the bank. After getting back from the bank, she discovers that 20% of the nickels are iron nickels worth $3 each. What is the total value of her money now? DeepSeek-R1-Distill-Qwen-7B Output. <think> Alright, so Alice has 20 quarters, and she wants to exchange them for nickels. Hmm, okay, let me think about this. First, need to figure out how much money Alice has initially. Quarters are worth 25 cents each, right? So, if she has 20 quarters, can calculate the total value by multiplying 20 by 25 cents. Let me write that down: 20 quarters * $0.25 per quarter. So, 20 * 0.25 equals... hmm, 20 divided by 4 is 5, so 20 * 0.25 is $5.00. Okay, so Alice has $5.00 in quarters. [...] So, now need to calculate the total value of her money now. That would be the value of the iron nickels plus the value of the regular nickels. First, the iron nickels: 20 nickels * $3 each. Let me compute that: 20 * 3 = $60.00. Then, the regular nickels: there are 100 total nickels, minus 20 iron nickels, so 80 regular nickels. Each regular nickel is worth $0.05, so 80 * 0.05 = $4.00. So, adding those together, $60 + $4 = $64.00 . Wait, that seems like big jump from $5.00 to $64.00. Let me double-check my calculations. She started with 20 quarters, which is 20 * $0.25 = $5.00. Exchanged them all for nickels, so 100 nickels. 20% of 100 is 20, so 20 iron nickels at $3 each is 20 * 3 = $60. The remaining 80 nickels are regular, so 80 * 0.05 = $4. So, total is $60 + $4 = $64 . That seems correct. But let me think again about the exchange. [...] So, 20% of her nickels are iron nickels worth $3 each, so she effectively has 20 coins worth $3 each and 80 coins worth $0.05 each. So, total value is indeed 203 + 800.05 = 60 + 4 = $64 . Wait, but in reality, exchanging quarters for nickels should just be different form 13 [...] Wait, maybe Im of the same money. So, why is the total value increasing? overcomplicating. The key is that she exchanges all her quarters into nickels, which would give her 100 nickels, but 20% of them are iron nickels worth $3 each, so 20 nickels at $3 and 80 at $0.05. So, the total value is 203 + 800.05 = 60 + 4 = $64 . So, thats the total value. But let me think again, is this possible? Because exchanging 20 quarters for nickels should just be different form of the same money, so why would the total value increase? [...] But in this case, 20 of them are worth $3 each, so total value is 203 + 800.05 = 60 + 4 = $64.00 . So, in effect, shes getting more money by exchanging her quarters into nickels where some are worth more. So, this seems to be the case. So, maybe the answer is $64.00 . But just to make sure, let me recast the problem. Total initial value: 20 quarters * $0.25 = $5.00. After exchanging, she has 100 nickels. But 20% of those are iron nickels worth $3. So, 20 nickels * $3 = $60, and 80 nickels * $0.05 = $4. So, total is $60 + $4 = $64 . Yes, that seems correct. Alternatively, maybe the question is trying to trick me into thinking that the total value is still $5, but no, since 20% of the nickels are worth more, so the total value increases. Therefore, the total value of her money now is $64.00 . Final Answer The total value of her money now is 64 dollars. </think> [...] In Example A.3, QwQ-32B exhibits similar pattern to Figure 1, repeating stating previously mentioned priors or revisiting earlier examined cases during the problem-solving process. The problem is sampeled from GPQA-Diamond dataset [32]. Example A.3 Problem. You perform high-throughput experiment on white lupine to find genes contributing to resistance to the fungal disease anthracnose. As result, you receive three candidate genes of unknown function G1, G2, and G3. You create three knock-out mutants, g1, g2, and g3, and set of double-mutants, g1g2, g1g3, and g2g3. You know that at least one of these genes is transcription factor acting upstream of (an)other gene(s). You start to test those mutant plants: do they have higher sensitivity to anthracnose than the wild-type because they cannot produce certain gene products? After tests with the pathogen, you receive the following results where 100% is the level of resistance to the pathogen in control; 50% is half of the controls resistance; 25% is quarter of the controls resistance; 0% all plants show signs of infection: resistance of g1: 75% of control resistance of g2: 0% from control resistance of g3: 50% from control resistance of g1g3: 10% from control resistance of g2g3: 0% from control resistance of g1g2: 0% from control Which conclusion regarding those genes interaction can you draw from this experiment? QwQ-32Bs Output. <think> Okay, so have this problem about white lupine genes and their resistance to anthracnose. Let me try to figure this out step by step. First, the setup: There are three genes, G1, G2, G3. They made knockout mutants for each (g1, g2, g3), and all the double mutants (g1g2, g1g3, g2g3). The question is about their interactions, especially since at least one is transcription factor upstream of others. The resistance levels are given for each mutant and double mutant. The resistance percentages are as follows: g1: 75% g2: 0% g3: 50% g1g3: 10% g2g3: 0% g1g2: 0% Hmm. Let me think about how these genes might interact. Since transcription factor is upstream, it probably controls the expression of another gene. If gene is transcription factor, knocking it out would prevent the downstream gene from being expressed, leading to loss of function. Looking at the single mutants first: g2 has 0% resistance. Thats the lowest, so losing G2 completely breaks resistance. g1 is 75%, so its still somewhat resistant. g3 is 50%, so halfway. Now, the double mutants: g1g2 is 0%: same as g2 alone. g2g3 is also 0%: same as g2. g1g3 is 10%: which is worse than either g1 (75%) or g3 (50%). Wait, so when you knock out G1 and G3 together, the resistance drops further to 10%, which is worse than either single knockout. That suggests that G1 and G3 might be acting in parallel pathways? Because their combined loss is worse than either alone. But then looking at g2g3 and g1g2, both are 0%, same as g2. So maybe G2 is epistatic over the others? Because when you have g2, its already 0%, so adding another mutation doesnt make it worse. That might mean that G2 is the main gene, and the others are downstream or in different pathway that doesnt matter once G2 is gone. Alternatively, maybe G2 is the transcription factor, and its required for the expression of G1 and/or G3. If G2 is transcription factor, then knocking it out would prevent expression of the downstream genes. So if G2 is upstream of G1 and G3, then in the g2 mutant, both G1 and G3 wouldnt be expressed either. But then why does the g1 mutant still have 75% resistance? That seems contradictory. Wait, maybe G2 is not the transcription factor here. Let me think again. Alternatively, maybe G3 is the transcription factor. Lets see. If G3 is transcription factor that controls G1 and/or G2. Then, in the g3 mutant, both G1 and G2 might be downregulated. But then the g3 single mutant has 50% resistance. The g2 mutant has 0 Alternatively, G1 might be transcription factor. Lets consider that. If G1 is transcription factor, and its controlling G2 or G3. Looking at the double mutants: g1g3 has 10%. Since G1 alone is 75%, and G3 alone is 50%, combining them gives 10%, which is significant drop. So maybe G1 and G3 are in parallel pathways, and their combined loss is more severe. [...] Alternatively, maybe G2 is transcription factor thats needed to activate G1 and G3. Let me think through that. If G2 is transcription factor, then in the g2 mutant, neither G1 nor G3 would be expressed. But then why does the g1 mutant still have 75% resistance? If G1 is not expressed in the g2 mutant, but the g1 mutant (which still has G2) has 75%, that suggests that G1 can function even if G2 is present. Wait, this is getting confusing. If Alternatively, perhaps G3 is transcription factor that activates G2. Lets see: G3 is transcription factor for G2, then in the g3 mutant, G2 might not be expressed. Then, the g3 single would have no G2, so resistance would be 0%, but in reality g3 is 50%, so that cant be right. Hmm. Lets try another approach. [...]"
        },
        {
            "title": "B Extra Experimental Results",
            "content": "B.1 Efficiency Evaluation We evaluate the efficiency gains of RPC by comparing decoding throughput and peak memory usage against the inference with full KV cache. Specifically, we report results for the default 4 compression setting of RPC with two compression intervals, = 1024 and = 4096. Throughput is measured in tokens per second, and peak memory reflects the maximum GPU memory consumption during generation. Measurements were conducted using two reasoning models: DeepSeek-R1-Distill-Qwen-7B and QwQ-32B. For DeepSeek-R1-Distill-Qwen-7B, evaluations were performed on single NVIDIA H100 SXM GPU, while QwQ-32B was tested using 4 NVIDIA H100 SXM GPUs in parallel. We fix the input length to 128 tokens and vary the generation length across 4096, 8192, 16384, and 32768 tokens. Batch size is varied across 8, 16, and 32 to assess scalability under different workloads. Results for DeepSeek-R1-Distill-Qwen-7B are shown in Table 4, and the corresponding results for QwQ-32B are reported in Table 5. Table 4: DeepSeek-R1-Distill-Qwen-7Bs throughput and peak memory usage by batch size and generation length. Metric Batch Size 4096 8192 16384 32768 Throughput (tokens/sec) Peak Memory (GB) Throughput (tokens/sec) Peak Memory (GB) Throughput (tokens/sec) Peak Memory (GB) Full KV Cache 8 16 32 8 16 32 401.50 669.53 1328.58 368.72 653.04 1031.51 19.20 23.09 30.86 22.95 30.60 45. 330.41 504.50 671.83 30.47 45.63 75.96 RPC (P = 1024) 8 16 32 8 16 32 448.19 848.75 1504. 428.31 794.62 1499.80 407.00 751.69 1288.51 17.08 18.86 22.40 18.02 20.74 26.16 20.27 25.15 35.00 RPC (P = 4096) 8 16 32 8 16 32 406.43 753.11 1318.33 420.62 708.95 1247.44 385.75 671.38 1064.05 19.20 23.09 30. 20.14 24.96 34.62 22.02 28.72 42.13 256.92 342.88 OOM 45.50 75.70 OOM 385.00 650.20 977.11 24.75 34.20 53. 362.75 575.21 883.43 25.77 36.24 57.16 The results show that RPC consistently improves decoding efficiency over full KV cache inference across various batch sizes and generation lengths. As the batch size increases, both the throughput gains and peak memory reductions become more pronounced. This is because larger batches amplify the memory bottleneck imposed by the growing KV cache, allowing RPCs compression to better utilize available GPU compute resources. Notably, full KV cache inference results in out-of-memory 16 Table 5: QwQ-32Bs throughput and peak memory usage by batch size and generation length. Metric Batch Size 4096 8192 16384 32768 Throughput (tokens/sec) Peak Memory (GB) Throughput (tokens/sec) Peak Memory (GB) Throughput (tokens/sec) Peak Memory (GB) Full KV Cache 8 16 32 8 16 32 128.79 213.75 351.34 83.40 101.14 136.61 RPC (P = 1024) 8 16 8 16 32 135.79 238.76 411.42 73.75 81.81 97.95 RPC (P = 4096) 8 16 32 8 16 126.59 214.27 345.02 83.40 101.14 136.61 109.80 173.99 228.59 100.58 135.50 205.33 131.97 229.22 392.04 78.42 91.24 116. 113.32 207.28 314.67 87.70 109.73 153.79 93.28 117.51 OOM 134.94 204.22 OOM 124.45 176.73 328.56 89.19 112.77 159. 115.75 187.97 279.34 96.28 126.90 188.14 64.85 OOM OOM 203.66 OOM OOM 111.84 178.06 246.81 111.84 155.81 245. 102.57 147.26 208.48 114.53 163.40 261.13 (OOM) errors for DeepSeek-R1-Distill-Qwen-7B when the batch size is 32 and the generation length reaches 32,768, and for QwQ-32B when the batch size is 16 at 32,768 tokens or 32 at 16,384 tokens or longer. In contrast, RPC enables successful generation under all of these settings. When comparing compression intervals, = 1024 achieves slightly higher throughput and lower peak memory than = 4096 across both models. While = 1024 offers stronger compression, it may come at modest accuracy cost, as shown in Section 4.2. Therefore, = 1024 and = 4096 can be considered complementary settings: the former prioritizes efficiency, and the latter provides more balanced trade-off between performance and accuracy. 17 B.2 Effect of Aggressive Compression To assess the robustness of RPC under extreme compression, we evaluate its performance with target compression ratio of 8. This setting represents highly aggressive compression scenario where only one-eighth of the generated tokens KV entries are retained over time. Table 6 shows the resulting performance across the three benchmark datasets. Table 6: Accuracy (%) of RPC 8 compared to RPC 4 and full KV cache. Method DeepSeek-R1-Distill-Qwen-7B QwQ-32B AIME 2024 (pass@1) LiveCodeBench (pass@1) IFEval (pass@1) AIME 2024 (pass@1) LiveCodeBench (pass@1) IFEval (pass@1) Full KV Cache RPC 4 Best RPC 8 (P = 4096) RPC 8 (P = 1024) 55.5 52.9 47.5 37.5 37.6 35.9 32.8 27.2 55.1 57.3 55.1 58.4 79.5 78.3 72.1 72.1 63.4 62.2 57.2 57.4 83.9 82.6 84.3 82. Both models exhibit notable performance drop on AIME 2024 and LiveCodeBench under 8 compression, compared to the default 4 setting, indicating the difficulty of preserving reasoning fidelity under extreme compression. Nevertheless, the stronger reasoning model QwQ-32B demonstrates greater robustness, maintaining pass@1 scores close to the results of RPC 4 across both benchmarks. In contrast, on IFEval, benchmark characterized by lower reasoning difficulty, the performance remains stable or even improves slightly for both models, suggesting that light-weight instruction-following tasks are less sensitive to aggressive KV cache compression."
        }
    ],
    "affiliations": [
        "Seoul National University",
        "Sungkyunkwan University"
    ]
}