{
    "paper_title": "Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics",
    "authors": [
        "Jingdi Lei",
        "Di Zhang",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 2 0 6 2 1 . 2 1 5 2 : r Error-Free Linear Attention is Free Lunch: Exact Solution from Continuous-Time Dynamics Jingdi Lei1, Di Zhang2, Soujanya Poria1 1Nanyang Technological University, 2Fudan University Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order RungeKutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides new theoretical foundation for building high-fidelity, scalable linear-time attention models. Github: https://github.com/declare-lab/EFLA # Correspondance: Di Zhang (di.zhang@ustc.edu) Date: December 16,"
        },
        {
            "title": "1 Introduction",
            "content": "As large language models (LLMs) evolve into increasingly capable agents (Yao et al., 2022; Team et al., 2025b; Google, 2025; OpenAI, 2025), the efficiency of inference computation has emerged as critical bottleneck (Dao et al., 2022; Kwon et al., 2023; Kim et al., 2024). This challenge becomes particularly acute in demanding scenarios such as long-context processing and reinforcement learning (RL) environments (Guo et al., 2025; Lai et al., 2025) where models are required to handle extended reasoning trajectories or engage in complex tool-use interactions (Lightman et al., 2023; Yao et al., 2023), the quadratic time complexity (Vaswani et al., 2017) inherent in standard attention mechanisms leads to severe inefficiencies. These inefficiencies introduce substantial computational overhead, significantly constraining model throughput, scalability to long contexts, and real-time interactivity (Liu et al., 2023; Jiang et al., 2024; Katharopoulos et al., 2020). This has led to surge of research in linear-time attention methods, aiming to approximate or reformulate the attention operation in sub-quadratic time. Prior works like Mamba-2 (Dao and Gu, 2024), DeltaNet (Schlag et al., 2021; Yang et al., 2024b) connect attention mechanisms with continuous-time systems. They bridge modern sequence modeling with the mathematical foundations of control theory and signal processing. We observe that this type of linear attention is not merely an approximation, but also suboptimal modeling of continuous-time dynamics, discretization of an ODE system, analogous to physical system with exponential decay and input injection. Under this interpretation, we formalize the classical linear attention method as continuous-time dynamical system. Thus, attention can be understood as solving this ODE via numerical integration methods such as the Euler scheme (Euler, 1792). From an analytical perspective, linear-attention formulations implicitly reduce to first-order Euler discretization, which limits their precision. While computationally simple, Euler discretization introduces truncation errors and suffers from stability issues. This explains why linear attention often exhibits instability under long sequences or large decay rates, it is numerically integrating stiff ODE with an insufficient integration scheme. Several models have tried to improve the Euler update and mitigate the error accumulation by introducing decay factors or gating 1 functions (Dao and Gu, 2024; Ma et al., 2022; Sun et al., 2023a), or adaptive forgetting coefficients (Yang et al., 2023, 2024a; Team et al., 2025a). While these methods stabilize training and improve long-term retention, they remain heuristic corrections to an inherently low-order numerical approximation. However, these low-order methods cannot eliminate the discretization error itself, they merely rescale or damp its effect. Unlike prior works that rely on approximations, we propose EFLA, principled approach to eliminate discretization errors by solving the underlying ODE exactly. This results in solution that is both analytically tractable and computationally efficient. This exact closed-form solution can be mathematically interpreted as the infinite-order RungeKutta (RK-) (Runge, 1895; Kutta, 1901) limit or the general solution of first-order ODE. In other words, it pushes the approximation order to infinity, yielding continuous-time, error-free formulation of linear attention. This exact integration not only ensures numerical stability but also establishes theoretical bridge between linear attention and modern state-space models like Mamba. By bypassing the limitations of Euler-based approximations, our computable analytic solution offers fundamental path toward high-fidelity attention mechanisms. Empirically, we demonstrate that EFLA exhibits superior robustness in noisy environments and achieves significantly accelerated convergence compared to baseline methods. Furthermore, it consistently outperforms DeltaNet across diverse set of downstream benchmarks, validating the practical efficacy and scalability of our error-free formulation. Our contributions are summarized as follows: Precise Identification of Error Sources in Existing Linear Attention: We analyze the numerical error in mainstream linear attention methods and point out that the core limitation lies in the low-order discretization of an underlying continuous-time process. These approximations introduce significant truncation errors and instability, especially in long-context scenarios. Reformulating Linear Attention as Continuous-Time Dynamical System: By treating the online-learning update as first-order ordinary differential equation (ODE), we reconstruct it from the perspective of continuous-time dynamics. Deriving an Exact Closed-Form Solution with Linear-Time Complexity: Leveraging the rank-1 property of the dynamics matrix, we theoretically derive an exact, closed-form solution to the continuous-time ODE governing linear attention. Importantly, our formulation maintains the desirable linear time complexity, while eliminating numerical integration errors."
        },
        {
            "title": "2.1 Scaled Dot-Product Attention",
            "content": "Given queries Rnd, keys Rnd, and values Rnd, the scaled dot-product attention (Vaswani et al., 2017) is defined as Attn(Q, K, V) = softmax (cid:18) QK (cid:19) + V, (1) where Rnn is the additive causal mask (zeros on and below the diagonal, and above)."
        },
        {
            "title": "2.2 Linear Attention\nLinear Attention as Online Learning. Linear attention (Katharopoulos et al., 2020) maintains a\nmatrix-valued recurrent state that accumulates key–value associations:",
            "content": "St = St1 + ktv , ot = qt. (2) From the fast-weight perspective (Schlag et al., 2021), St serves as an associative memory storing transient mappings from keys to values. This update can be viewed as performing gradient descent on an unbounded correlation objective: Lt(S) = Skt, vt, (3) 2 which continually reinforces recent keyvalue pairs without any forgetting mechanism. However, such an objective lacks criterion for erasing old memories; consequently, the accumulated state grows unbounded, leading to interference over long contexts. DeltaNet: Reconstruction Loss Perspective. DeltaNet reinterprets this recurrence as online gradient descent on reconstruction loss objective: 2 Skt vt2. Taking gradient step with learning rate βt yields the update rule: Lt(S) = St = St1 βtSLt(St1) = (I βtktk )St1 + βtktv . (4) (5) This classical delta rule treats as learnable associative memory that continually corrects itself toward the mapping kt (cid:55) vt. The rank-1 update structure, equivalent to generalized Householder transformation, facilitates hardware-efficient chunkwise parallelization (Bischof and Van Loan, 1987; Yang et al., 2024b)."
        },
        {
            "title": "2.3 Euler and Runge-Kutta Methods in ODE",
            "content": "Numerical Solutions to ODEs. Given first-order ordinary differential equation (ODE) form dS(t) numerical methods aim to approximate the solution S(t) at discrete time point t. dt = (t, S(t)), Euler Method. The Explicit Euler method (Euler, 1792) is the most fundamental numerical integration scheme. It approximates the state at the next time step using the derivative at the current position: St = St1 + βt (t 1, St1), (6) where βt represents the integration step size. While computationally efficient, Euler discretization is firstorder method with local truncation error of O(β2 ). Existing linear attention models, such as DeltaNet (Schlag et al., 2021), implicitly adopt this formulation. However, due to its low-order nature, Euler integration often suffers from numerical instability and error accumulation, particularly when integrating stiff dynamics over long sequences. Runge-Kutta Methods. To achieve higher precision, the Runge-Kutta (RK) family (Runge, 1895; Kutta, 1901; Butcher, 1996, 2016) of methods estimates the future state by aggregating multiple slope estimates (stages) within single step. For general -th order RK method, the update is given by weighted sum of intermediate derivatives: St = St1 + βt (cid:88) i=1 ciki, (7) where ki represents the slope at the i-th stage."
        },
        {
            "title": "3.1 Numerical Approximations.",
            "content": "We begin by revisiting DeltaNet (Schlag et al., 2021), which formulates linear attention as online gradient descent on reconstruction objective: Applying single gradient descent step with learning rate βt yields: Lt(S) = 1 2 Skt vt2. St = St1 + βt (cid:0)ktk St1 + ktv (cid:1) . 3 (8) (9) and the input forcing term To formalize the underlying dynamics, we define the dynamics matrix At = ktk . Since the input data arrives as discrete sequence, we model the continuous signal using the bt = ktv Zero-Order Hold (ZOH) (Iserles, 2009) assumption. Under this physically grounded assumption for digital systems, the time-varying matrices A(t) and b(t) become piecewise constant. The system evolves according to first-order ODE: dS(t) dt = AtS(t) + bt. (10) In this framework, standard DeltaNet corresponds to the first-order Explicit Euler discretization. To mitigate the discretization errors inherent in this low-order scheme, one might resort to higher-order solvers. For instance, the second-order (RK-2) and fourth-order (RK-4) Runge-Kutta updates are given by (see Appendix for details): RK-2: RK-4: (cid:18) St = βtAt + (cid:19) 1 A2 β2 (cid:18) St1 + βt (cid:19) βtAt bt. 1 2 St = (cid:32) 4 (cid:88) n= (cid:33) (βtAt)n n! St1 + βt (cid:32) 3 (cid:88) n= (βtAt)n (n + 1)! (cid:33) bt. (11) (12) Based on the same principle, by truncating the RungeKutta expansion, we obtain an -th order form (see Appendix for details): St = (cid:34) (cid:88) n=0 (cid:35) (βtAt)n n! St1 + βt (cid:34)N 1 (cid:88) n=0 (βtAt)n (n + 1)! (cid:35) bt. (13) As the order increases, the numerical approximation becomes progressively more accurate. Taking the limit as , the truncated series converges to their analytical limits: lim (cid:88) n=0 (βtAt)n n! = eβtAt, lim (cid:88) n=0 βt (βtAt)n (n + 1)! bt = (cid:90) βt 0 e(βtτ )Atbtdτ. (14) (15) Thus, the truncated polynomial expansions converge to the exact analytic form of matrix exponential and its associated integral, yielding: St = eβtAtSt1 + (cid:90) βt 0 e(βtτ )At bt dτ. (16) This expression represents the general solution of the first-order ODE as shown in Eq. 10, and corresponds to the infinite-order (error-free) limit of the RungeKutta family1. 1This analytical solution can also be directly obtained by applying the general solution method for ordinary differential equations. The detailed derivation of this closed-form expression is provided in Appendix D."
        },
        {
            "title": "3.2 The “Aha!” Moment: Efficient Computation via rank-1 Property",
            "content": "While the infinite-order solution eliminates discretization error, naively evaluating the matrix exponential eβtAt for general matrix typically requires O(d3) complexity (Gu et al., 2020). We bypass this computational bottleneck by leveraging the rank-1 structure of the dynamics matrix At = ktk , which allows the exponential to be computed in linear time. We observe that At satisfies the idempotence-like property An kt (see Appendix for the proof). This property allows us to collapse the Taylor series of the matrix exponential into computable closed form: At for 1, where λt = = λn1 eβtAt = + (cid:88) n=1 (βt)n n! An = 1 eβtλt λt At. (17) Substituting this transition operator into the integral term (cid:82) βt 0 e(βtτ )Atbt dτ yields the exact input injection: It = (cid:90) βt (cid:18) 0 1 eλt(βtτ ) λt (cid:19) At bt dτ = βtbt Atbt λt (cid:18) βt 1 eβtλt λt (cid:19) . (18) and At = ktk Crucially, since bt = ktv significant simplification of the integral term: , we have Atbt = λtbt. This algebraic relationship allows for It = βtbt βtbt + 1 eβtλt λt bt = 1 eβtλt λt bt. Combining these results, the final Error-Free Linear Attention update rule is given by: (cid:18) St = 1 eβtλt λt (cid:19) ktk St1 + 1 eβtλt λt ktv . (19) (20) This update maintains linear time complexity with respect to sequence length, i.e., O(Ld2), while capturing the exact continuous-time dynamics."
        },
        {
            "title": "4 Chunkwise Parallelism Form",
            "content": "We observe that the EFLA update rule shares an identical algebraic structure with DeltaNet. Given this structural equivalence, we can seamlessly adapt the hardware-efficient parallelization strategies originally developed for DeltaNet (Yang et al., 2024b). In this section, we derive the chunkwise parallel formulation specifically for EFLA. To derive the chunkwise parallel form, we first unroll the recurrence relation. Denoting 1eβtλt state update becomes: λt = αt, the St = (I αtktk )St1 + αtktv = (cid:88) (cid:89) (I αjkjk ) αi(kiv ). Then we can define the following variables: i=1 j=i+1 (cid:89) (I αtktk ), Pj = t=i Hj = (cid:88) t=i Pj t+1αtktv (21) (22) where Pj accumulation term to Sj from token i. The Chunkwise can be written as follows: can be considered as decay factor applied to Si to obtain Sj and Hj = when > j. Pj is an [t] = Pr Sr [t]S0 [t] + Hr [t] where we define the chunkwise variables Sr , Hr of size C. We can use induction to derive the WY representations of Pr [t] [t] = StC+r, Pr [t] = PtC+r . Here we have [t] = HtC+r tC+1 : and Hr [t] tC+1 Pr [t] = (cid:88) i=1 [t]wi ki [t] , Hr [t] = (cid:88) i= [t]ui ki [t] (cid:32) (cid:32) wr [t] = αr [t] kr [t] r1 (cid:88) (kr [t] ki [t])wi [t] (cid:33) , [t] = αr ur [t] vr [t] r1 (cid:88) (kr [t] ki [t])ui [t] (cid:33) subsequently, we can obtain the chunk-level recurrence for states and outputs: i=1 i= [t] = S0 Sr [t] (cid:32) (cid:88) ( i=1 (cid:33) [t]wi ki [t] )S0 [t] + (cid:88) i=1 [t]ui ki [t] = [t] + (cid:88) i=1 [t](ui ki [t] wi [t] [t]) [t] = Sr or [t] qr [t] = S0 [t] qr [t] + (cid:88) (ui [t] S0 [t] wi [t])(ki [t] qi [t]) letting S[t] = S0 [t] , the above can be simplified to matrix notations: i=1 S[t] = S[t] + [t] (cid:16) (cid:0)U[t] W[t]S[t] (cid:1) O[t] = Q[t]S[t] + Q[t]K [t] (cid:17) (cid:0)U[t] W[t]S[t] (cid:1) , (23) chunks (24) (25) (26) (27) (28) (29) [t] RCd for {Q, K, V, O, U, W} defines the chunkwise matrices that are formed from where [t] = 1:C stacking the qt, kt, vt, ot, ut, wt vectors and is the lower triangular causal mask. Finally, we can apply the UT transform (Joffrain et al., 2006) to simplify the recurrence calculations of ur [t] and wr [t] . (cid:16) T[i] = + StrictTril(diag(αt)K[i]K [i]) (cid:17)1 diag(αt) W[t] = T[t]K[t], U[t] = T[t]V[t] (30) (31)"
        },
        {
            "title": "5.1 Numerical Stability and Robustness Verification",
            "content": "As mentioned in Section 3, linear attention mechanisms like DeltaNet employ first-order approximation. While computationally efficient, they still suffer from error accumulation, particularly when dealing with noisy environments or high-energy inputs. In this section, we conduct the following three tests to demonstrate this limitation: Image Pixel Dropout. We apply Bernoulli dropout to input tokens with probability to simulate data corruption. This setting evaluates the models robustness against information loss and its ability to preserve long-range dependencies despite corrupted input signals. 6 Figure 1 EFLA outperforms DeltaNet in both convergence speed and robustness on sMNIST. The plots illustrate training dynamics and robustness against dropout, scale intensity, and additive noise. Notably, EFLA maintains significantly higher accuracy as interference intensity increases, particularly when trained with larger learning rate (bottom row, lr=3 103; see Appendix for detailed discussion on learning rate). OOD Intensity Scaling. We amplify input signals by factor to conduct rigorous stress test on numerical stability. The primary objective of this scaling is to simulate deployment scenarios involving unnormalized or high-variance inputs. Additive Gaussian Noise. We inject Gaussian noise with varying standard deviations. This setting aims to assess the models robustness against signal corruption and its ability to filter out random perturbations We conduct experiments on the pixel-level Sequential MNIST (LeCun et al., 2010) (sMNIST) task, flattening 28 28 images into sequences of length = 784. The model is Linear Attention Classifier with hidden dimension of = 64. We compare EFLA against the DeltaNet baseline. DeltaNet employs L2-normalized queries and keys (kt = 1), whereas EFLA utilizes unnormalized keys. This allows EFLA to leverage the key norm (kt2) as dynamic gate for the exact decay factor. Both models are trained using AdamW with batch size of 128. The performance comparisons are illustrated in Figure 1. As the input scale factor increases, DeltaNet exhibits rapid performance collapse, confirming the vulnerability of Euler approximations to high-energy inputs. In stark contrast, EFLA maintains high accuracy even at large scales, empirically confirming that its exact saturation mechanism effectively mitigates error accumulation and state explosion. Across both the additive noise and dropout benchmarks, EFLA consistently outperforms DeltaNet. Under severe interference conditions, EFLA demonstrates significantly slower rate of degradation. This indicates that by eliminating discretization errors, EFLA constructs higher-fidelity memory representation that is intrinsically more resilient to data corruption than the approximation-based methods."
        },
        {
            "title": "5.2 Language Modeling\nExperimental setup. We adapt the same model architecture with Yang et al. (2024b), see Appendix A\nfor hyperparamter settings. We evaluate on Wikitext (Merity et al., 2016) perplexity and a comprehensive\nsuite of zero-shot common sense reasoning tasks, including LAMBADA (Paperno et al., 2016), PiQA (Bisk\net al., 2020), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-easy (ARC-e),\nARC-challenge (Arc-c) (Clark et al., 2018), BoolQ (Clark et al., 2019), OpenBookQA (OBQA) (Mihaylov\net al., 2018), and SciQ (Johannes Welbl, 2017). We report the perplexity (lower is better) for Wikitext and\nLAMBADA, and accuracy (higher is better) for other downstream tasks.",
            "content": "Main Results. Our main language modeling results are shown in Table 1. With an identical training budget of 8B tokens, EFLA consistently outperforms the DeltaNet baseline across the majority of tasks. For instance, on LAMBADA, which evaluates the prediction of the final word based on broad context, EFLA 7 Perplexity () Accuracy () Model Wiki. LMB. LMB. PIQA Hella. Wino. ARC-e ARC-c BoolQ OBQA SciQ Avg. ppl ppl acc acc acc_n acc acc acc_n acc acc acc 340M Parameters DeltaNet 38.09 96.26 EFLA 37.01 81.28 23.9 61.9 22.5 60.7 30.1 31.1 51.9 51. 40.4 41.5 22.1 53.0 16.2 71.9 40. 22.5 60.4 17.2 73.3 42.6 1.3B Parameters DeltaNet 31.82 69.27 EFLA 32.1 53.3 31.48 61.83 26.1 63.8 32.4 52. 62.3 25.9 42.0 42.7 23.1 22.9 58. 17.0 73.0 43.0 60.4 17.8 77.0 43.9 Table 1 Main language modeling results compared with DeltaNet. Perplexity: Lower () is better. Accuracy: Higher () is better. Best results are bolded. achieves significantly lower perplexity of 81.28 (vs. 96.26 for DeltaNet) alongside higher accuracy of 23.9%. Notably, on the BoolQ, EFLA improves accuracy by substantial margin (+7.4% absolute). We further assessed the scalability of our approach using 1.3B parameter models. As detailed in the bottom of Table 1, even at an intermediate checkpoint of 16B tokens, where models have not yet reached full convergence2, EFLA still establishes distinct performance lead. We anticipate this performance gap to widen as training proceeds toward the full token budget. These results empirically validate that by eliminating the discretization error inherent in Euler-based methods, EFLA maintains higher fidelity of historical information over long sequences, capability critical for complex reasoning."
        },
        {
            "title": "6 Analysis of Memory Dominance",
            "content": "To understand the mechanism govering memory retention in EFLA, we analyze the spectral properties of the rank-1 dynamics matrix At = ktk . We show that the key norm kt2 act as dynamic gate, regulating the trade-off between forgetting and retention. Spectral Decomposition and Exact Decay. Since At is symmetric and rank-1, it possesses single non-zero eigenvalue λt = kt2 while remaining eigenvalues are zero. Leveraging this property, the matrix exponential in Eq. 14 admits simplified closed-form: eβtAt = 1 eβtλt λt ktk . (32) This operator induces directional decay, it contracts the component of memory state St1 aligned with kt by factor of eβtλt. λt serves as spectral gate: strong input signals (large key norms) cause rapid exponential decay along kt, effectively clearing the memory slot for new information, while weak signals result in slower, linear decay governed by βt, thereby prioritizing the retention of historical context. Asymptotic Connection to Delta Rule. exponential term can be linearized via first-order Taylor expansion: In the regime of vanishing key norms, i.e. λt 0, the (cid:0)I βtktk (cid:1) St1 + βtktv , lim λt0 (33) 2The results for the 1.3B model are preliminary due to limited computational resources. We are currently training the model and will update the paper with final results soon. 8 In this case, the update rule asymptotically recovers to the delta rule, indicating that delta-rule linear attention serves as first-order approximation of EFLA, valid only when the dynamics are non-stiff (i.e., small λt). In contrast, EFLA remains robust regardless of signal magnitude due to its exact integration."
        },
        {
            "title": "7.1 Linear Attention as Low-Order Numerical Integrators",
            "content": "Early linear-time attention mechanisms approximate softmax attention by replacing the normalized kernel with feature map that enables associative accumulation of keyvalue statistics in recurrent state. Linear Transformers (Katharopoulos et al., 2020) and Performer (Choromanski et al., 2020) rewrite causal attention as running sum of outer products, St = St1 + ϕ(kt)ϕ(vt), ot = ϕ(qt), yielding an RNN-like formulation of attention that scales linearly in sequence length. Schlag et al. (Schlag et al., 2021) interpret such mechanisms as fast-weight programmers, where the matrix state St implements dynamic associative memory updated via low-rank modifications. DeltaNet (Yang et al., 2024b) makes this perspective explicit by viewing St as the parameter of an online regression problem. At each time step, it minimizes reconstruction loss Lt(S) = 1 2 Skt vt2 via single gradient step with learning rate βt, leading to the delta-rule update St = (I βtktk )St1 + βtktv . This recurrence coincides with an explicit Euler discretization of linear continuous-time dynamical system dS dt = ktk + ktv with step size βt. From numerical-analysis viewpoint, this corresponds to first-order (RK-1) integrator with local truncation error O(β2 ). Subsequent work such as Gated DeltaNet (Yang et al., 2024a) and Kimi Delta Attention (KDA) (Team et al., 2025a) enriches the delta rule with channel-wise gates, data-dependent learning rates, and more expressive value mixing, leading to strong empirical performance on long-context language modeling. However, these methods still rely on first-order explicit Euler integration of the underlying linear dynamics: the continuous-time model is fixed, and improvements come from better parameterizations of the right-hand side rather than from more accurate time integrator. In contrast, we adopt the continuous-time viewpoint as the primary design principle and ask different question: for the same linear attention dynamics, is it possible to eliminate discretization error altogether? We show that, by exploiting the rank-1 structure of the dynamics matrix, the exact closed-form solution corresponding to the infinite-order RungeKutta limit (RK-) can be computed in linear time, yielding an error-free linear attention mechanism."
        },
        {
            "title": "7.2 State Space Models and Discretization of Continuous Dynamics",
            "content": "Structured State Space Models (SSMs) provide principled framework for modeling long-range sequence dependencies via linear time-invariant (LTI) dynamics. The S4 family (Gu et al., 2022a,b, 2020) and related models specify continuous-time state equation dx dt = Ax + Bu, = Cx, 9 and then derive efficient discrete-time implementations by discretizing (A, B, C). Common choices include the bilinear (Tustin) transform and zero-order hold (ZOH) (Gu et al., 2022a, 2020). The bilinear transform can be interpreted as trapezoidal rule or implicit second-order RungeKutta scheme applied to the underlying ODE, while ZOH yields an exact discretization for piecewise-constant inputs in time. In practice, these methods enable stable and efficient convolutional realizations, but they still approximate or indirectly parameterize the matrix exponential etA for general full-rank A. Mamba (Gu and Dao, 2024) and its successors extend this line of work by introducing selective state spaces, where the effective continuous-time parameters A, B, and depend on the input sequence. The resulting models retain the continuous-time flavor but still require discretization step to obtain practical recurrent or convolutional update. In particular, Mamba employs bilinear transform (or closely related variants), which is formally equivalent to an implicit second-order RungeKutta method applied to the linear SSM. Thus, despite their continuous-time interpretation, modern SSM architectures ultimately rely on finite-order approximations of the exact propagator etA for tractability in the general case. Classical exponential moving average (EMA) filters can also be written as simple SSMs with scalar and B, further highlighting the tight connection between memory mechanisms and linear dynamical systems. Recent EMA-based sequence models (Fu et al., 2022; Ma et al., 2022; Sun et al., 2023b) typically design forgetting factors directly in discrete time, without explicitly deriving them from an underlying continuous-time system. Our formulation is conceptually close to these SSM approaches: we also start from continuous-time linear ODE and ask how to implement it efficiently in discrete time. The key difference lies in the structure of the dynamics. Whereas S4, Mamba and related SSMs must handle general (potentially full-rank) matrices and therefore resort to finite-order approximations of etA, the attention dynamics studied in this work have rank-1 transition matrix At = ktk . We show that this special structure makes both the matrix exponential and the associated input integral analytically tractable, enabling an exact RKupdate without incurring the cubic cost typically associated with matrix exponentials."
        },
        {
            "title": "7.3 Continuous-Time Transformers and ODE-Based Attention",
            "content": "A parallel line of work applies ideas from neural ordinary differential equations (Neural ODEs) (Chen et al., 2019) to Transformer-like architectures. ODE-based Transformers reinterpret residual blocks as discrete steps of an ODE solver in depth, and explore higher-order RungeKutta schemes or adaptive step sizes to improve stability and expressivity along the layer dimension. Similarly, continuous-time Transformers for irregular time series treat token positions as continuous timestamps and parameterize the hidden dynamics by ODEs while retaining attention-like interaction structure. More recently, PDEor ODE-guided attention mechanisms have been proposed to study the evolution of attention weights in pseudo-time dimension, with the goal of obtaining smoother or more interpretable attention maps for very long sequences. These methods typically introduce auxiliary continuous variables or diffusion-like processes on top of standard attention, and then perform numerical integration over the additional dimension using standard finite-step solvers. Conceptually, these works share with us the high-level idea of viewing Transformers and attention through the lens of continuous-time dynamics. However, there are two crucial differences. First, most existing ODE-based Transformer variants operate at the level of residual blocks, hidden states, or attention weights, rather than on the fast-weight memory state of linear attention. Second, they generally depend on off-the-shelf ODE solvers (e.g., Euler, RK2, RK4, or adaptive-step methods) with finite number of stages, and do not attempt to derive an exact closed-form solution that remains efficient in sequence length. In contrast, we focus directly on the continuous-time dynamics of the linear attention state and show that, thanks to its rank-1 structure, the exact infinite-order RungeKutta limit is both analytically available and computationally efficient."
        },
        {
            "title": "7.4 Other Linear-Time Long-Context Models",
            "content": "Beyond linear attention and SSMs, several architectures achieve linear or near-linear time complexity for long-context modeling without relying on attention in its classical form. RetNet (Sun et al., 2023b) introduces retention mechanism that admits parallel, recurrent, and chunkwise implementations, providing unified 10 framework that interpolates between convolutional and recurrent behaviors. Hyena (Poli et al., 2023) and related long-convolution models exploit implicit filters and hierarchical convolutional structures to capture long-range dependencies with sub-quadratic complexity, while eschewing explicit pairwise token interactions. These models demonstrate that long-context sequence modeling does not necessarily require attention or SSMs, and that alternative primitives such as long convolutions and retention can also be highly effective. Our work is complementary to these approaches. Rather than proposing new structural building block, we revisit the numerical foundations of existing and widely used primitive linear attention, shows that its continuous-time dynamics admit an exact, error-free discretization in the rank-1 setting. This provides new theoretical lens for understanding and improving attention-like mechanisms, and may inform the design of future linear-time architectures."
        },
        {
            "title": "8 Conclusion",
            "content": "We presented Error-Free Linear Attention (EFLA), new attention paradigm that bridges the gap between computational efficiency and mathematical precision. We identified that the performance bottleneck in classical linear attention stems from the accumulation of errors in low-order discretization schemes. Instead of refining these approximations, we bypassed them entirely by deriving the exact analytical solution to the attention dynamics. This results in an update mechanism that is theoretically free from error accumulation yet remains fully parallelizable and computable in linear time. Empirically, we demonstrated that this theoretical precision translates into tangible gains: EFLA exhibits superior robustness in noisy environments, accelerated convergence, and consistent performance improvements over DeltaNet across diverse benchmarks. By proving that exact integration is attainable without sacrificing scalability, our work lays solid foundation for the next generation of stable, high-fidelity sequence models. We hope this work inspires future exploration into exact solvers for complex continuous-time attention architectures."
        },
        {
            "title": "References",
            "content": "Christian Bischof and Charles Van Loan. The wy representation for products of householder matrices. SIAM Journal on Scientific and Statistical Computing, 8(1):s2s13, 1987. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. John Charles Butcher. history of runge-kutta methods. Applied numerical mathematics, 20(3):247260, 1996. John Charles Butcher. Numerical methods for ordinary differential equations. John Wiley & Sons, 2016. Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations, 2019. https://arxiv.org/abs/1806.07366. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Leonhard Euler. Institutiones calculi integralis, volume 1. impensis Academiae imperialis scientiarum, 1792. 11 Daniel Fu, Tri Dao, Khaled Saab, Armin Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models. arXiv preprint arXiv:2212.14052, 2022. Google. Gemini deep research, 2025. https://gemini.google/overview/deep-research/. Accessed: 2025-12-11. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling, 2024. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020. https://arxiv.org/abs/2008.07669. Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with structured state spaces, 2022a. https://arxiv.org/abs/2111.00396. Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models, 2022b. https://arxiv.org/abs/2206.11893. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. Arieh Iserles. first course in the numerical analysis of differential equations. Number 44. Cambridge university press, 2009. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16581677, 2024. Thierry Joffrain, Tze Meng Low, Enrique Quintana-Ortí, Robert van de Geijn, and Field Van Zee. Accumulating householder transformations, revisited. ACM Transactions on Mathematical Software (TOMS), 32(2):169179, 2006. Matt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science questions. 2017. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael Mahoney, Kurt Keutzer, and Amir Gholami. An llm compiler for parallel function calling. In Forty-first International Conference on Machine Learning, 2024. W. Kutta. Beitrag zur näherungsweisen Integration totaler Differentialgleichungen. Teubner, 1901. https://books. google.com.hk/books?id=Zc4TAQAAIAAJ. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. Hanyu Lai, Xiao Liu, Junjie Gao, Jiale Cheng, Zehan Qi, Yifan Xu, Shuntian Yao, Dan Zhang, Jinhua Du, Zhenyu Hou, et al. survey of post-training scaling in large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27712791, 2025. Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention. arXiv preprint arXiv:2209.10655, 2022. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. 12 OpenAI. Introducing deep research, 2025. https://openai.com/index/introducing-deep-research/. Accessed: 202512-11. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th annual meeting of the association for computational linguistics (volume 1: Long papers), pages 15251534, 2016. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models, 2023. https: //arxiv.org/abs/2302.10866. Carl Runge. Über die numerische auflösung von differentialgleichungen. Mathematische Annalen, 46(2):167178, 1895. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621, 2023a. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models, 2023b. https://arxiv.org/abs/2307.08621. Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, et al. Kimi linear: An expressive, efficient attention architecture. arXiv preprint arXiv:2510.26692, 2025a. Tongyi DeepResearch Team, Baixuan Li, Bo Zhang, Dingchu Zhang, Fei Huang, Guangyu Li, Guoxin Chen, Huifeng Yin, Jialong Wu, Jingren Zhou, et al. Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701, 2025b. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024a. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. Advances in neural information processing systems, 37:115491115522, 2024b. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019."
        },
        {
            "title": "A Experimental Setting",
            "content": "We used 8 A100 GPUs for 340M and 1.3B language modeling experiments. The random seed it set to 42. Each model uses AdamW for optimization, with peak learning rate of 3 104. The 340M models are trained for 8 billion tokens with global batch size of 1M tokens, while the 1.3B models are trained for 16 billion tokens with global batch size of 4M tokens. We use cosine learning rate schedule, starting with warm-up phase of 1 billion tokens for the 340M models and 4 billion tokens for the 1.3B models (corresponding to 1024 warm-up steps). Both have configurations that initial and final learning rates set at 3 105. We apply weight decay of 0.1 and use gradient clipping at maximum of 1.0. The head dimension is set to 128, and the kernel size for convolution layers is set at 4. Discussion: Saturation Effects and Learning Rate Scaling While EFLA eliminates the discretization error inherent in Euler-based methods like DeltaNet, our empirical observations reveal distinct optimization behavior, EFLA demonstrates superior semantic capture in the early training stages but exhibits slower convergence rate in the final asymptotic regime. We attribute this phenomenon to the saturation property of the exact decay factor. The Stability-Responsiveness Trade-off. Analytically, the Euler update employed by DeltaNet implies linear response to the input magnitude, where the update step βt. In contrast, the EFLA update is governed by the soft-gating term αt = (1eβtλt ) < 1 for all > 0, the effective update magnitude of EFLA is strictly sub-linear with respect to the energy of the key λt = kt2. In the early training phase, this saturation acts as robust filter against high-variance gradients and outliers (large λt), preventing the catastrophic divergence often seen in unnormalized Euler updates. This allows EFLA to establish stable semantic representations rapidly. However, as the model approaches convergence, this same mechanism dampens the magnitude of parameter updates. Specifically, for high-confidence features where λt is large, the gradient signal is exponentially suppressed, leading to vanishing update problem that slows down fine-grained optimization. . Considering the inequality 1ex λt In this case, EFLA naturally necessitates larger global learning Implication for Hyperparameters. rate to compensate for this exponential saturation, allowing the model to maintain responsiveness in the saturation regime without sacrificing its theoretical error-free guarantees. To validate this, we performed robustness ablation study across varying learning rates, as illustrated in Figure 2. The results reveal critical sensitivity: when trained with conservative learning rate (e.g., lr = 1 104, purple curve), the model fails to learn robust features, resulting in performance degradation. This empirical evidence confirms that relatively larger learning rate is structural necessity for EFLA to counteract the dampening effect of the exponential gate and achieve its full potential. Construction and Properties of rank-1 Matrices At is rank-1 matrix, and it satisfies: A2 = ktk ktk = kt(k kt)k = λtAt (34) Where λt = Then it gives us key property: At is scaled projection matrix (i.e., A2 kt is scalar value. = λtAt). 14 Figure 2 Impact of learning rate scaling on EFLA robustness. We evaluate the test accuracy of EFLA on sMNIST under three interference settings: OOD Intensity scaling (left), Additive Gaussian Noise (middle), and Dropout (right). The curves demonstrate clear correlation between learning rate magnitude and model robustness. Notably, increasing the learning rate from 1 104 to 3 103 significantly mitigates performance degradation under high interference, empirically validating the necessity of larger step sizes to counteract the saturation effect."
        },
        {
            "title": "D General Solutions of Ordinary Differential Equations",
            "content": "We start with first-order linear matrix ODE: Which can be rewrite as: dS dt dS dt = AS + b, + AS = b, For this type of differential equation, the integrating factor is: Since is constant, the integrating factor is simply: (cid:82) dt, Multiply the entire equation by eAt: Expanding the left-hand side: eAt eAt (cid:18) dS dt (cid:19) + AS = eAtb, eAt dS dt + eAtAS = eAtb, By the product rule for matrix-vector multiplication: and since dt eAt = AeAt, we have: dt (eAtS) = (cid:19) eAt (cid:18) dt + eAt dS dt , dt (eAtS) = (AeAt)S + eAt dS dt , Notice this matches exactly the left-hand side of Eq. 40 (since and eAt commute). The equation becomes: dt (eAtS) = eAtb, 15 (35) (36) (37) (38) (39) (40) (41) (42) (43) Integrate both sides from (initial time) to + βt (final time). To avoid confusion, we use τ as the integration variable: (cid:90) t+βt dτ (eAτ S(τ )) dτ = eAτ dτ, (cid:90) t+βt (cid:2)eAτ S(τ )(cid:3)t+βt = (cid:90) t+βt eAτ dτ, Thus: eA(t+βt)S(t + βt) eAtS(t) = (cid:90) t+βt eAτ dτ, Multiply both sides by eA(t+βt): S(t + βt) eA(t+βt)eAtS(t) = eA(t+βt) (cid:90) t+βt eAτ dτ, Simplify using exponential properties: S(t + βt) eAβtS(t) = (cid:90) t+βt eA(t+βtτ )b dτ, Since eA(t+βt) is constant, it can be moved inside the integral. Let = τ t. Then: Substitute: (cid:40) = 0, = βt, τ = τ = + βt and dτ = ds, (cid:90) βt eA[t+βt(s+t)]b ds = (cid:90) βt 0 eA(βts)b ds, Rename back to τ (dummy variable) and replace constants with At, bt: (cid:90) βt 0 e(βtτ )Atbt dτ, Combining everything, the full solution is: S(t + βt) = eβtAtS(t) + (cid:90) βt e(βtτ )Atbt dτ, Detailed Derivation of Runge-Kutta Methods Given the ordinary differential equation (ODE): We advance from tn to tn+1 = tn + h. dy dt = (t, y), y(t0) = y0 k1 = (tn, yn), k2 = (cid:0)tn + yn+1 = yn + hk2 2 , yn + 2 k1 (cid:1) , (44) (45) (46) (47) (48) (49) (50) (51) (52) (53) Continuous system: Let: Then: dS dt = ktk S(t) + ktv At = ktk , bt = ktv k1 = AtSt1 + bt (cid:16) k2 = At St1 + βt 2 k1 (cid:17) + bt St = St1 + βtk2 St = St1 + βt (cid:20) (cid:16) ktk St1 + βt 2 (ktk St1 + ktv ) (cid:17) St = (cid:0)I βtktk + 1 2 β (ktk )2(cid:1) St1 + βt (cid:16) βt 2 ktk (cid:21) + ktv (cid:17) ktv General RK-4: k1 = (tn, yn), k2 = (cid:0)tn + 2 k1 k3 = (cid:0)tn + 2 k2 k4 = (tn + h, yn + hk3), 2 , yn + 2 , yn + (cid:1) , (cid:1) , yn+1 = yn + 6 (k1 + 2k2 + 2k3 + k4) St = (cid:0)I βtAt + 1 2 β A2 1 6 β3 A3 + 1 24 β A4 (cid:1) St1 + (cid:0)βtI 1 2 β2 At + 1 6 β3 t 1 24 β4 A3 (54) (55) (56) (cid:1) bt (57) Then the general RungeKutta(N) method is defined by: k1 = (tn, yn), k2 = (tn + a2h, ; yn + hb21k1) , k3 = (tn + a3h, ; yn + h(b31k1 + b32k2)) , ... kN = tn + aN h, ; yn + yn+1 = yn + (cid:88) i=1 ciki, bN jkj , 1 (cid:88) j=1 where the coefficients ai, bij, ci are specified by the Butcher tableau of the corresponding RK scheme. Then, for linear dynamical system (t, S) = AtS + bt, the RKN update can be expressed as truncated matrix power series: St = (cid:34) (cid:88) n=0 (cid:35) (βtAt)n n! St1 + βt (cid:34)N 1 (cid:88) n=0 (βtAt)n (n + 1)! (cid:35) bt. (58)"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Nanyang Technological University"
    ]
}