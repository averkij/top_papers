{
    "paper_title": "VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning",
    "authors": [
        "Yuqi Liu",
        "Tianyuan Qu",
        "Zhisheng Zhong",
        "Bohao Peng",
        "Shu Liu",
        "Bei Yu",
        "Jiaya Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, a unified framework capable of reasoning and solving multiple visual perception tasks within a shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in a unified framework. The model generates a structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as a unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 8 0 2 1 . 5 0 5 2 : r VisionReasoner: Unified Visual Perception and Reasoning via Reinforcement Learning Yuqi Liu1 Tianyuan Qu1 Zhisheng Zhong1 Bohao Peng1 Shu Liu2 Bei Yu1 Jiaya Jia2,3 CUHK1 SmartMore2 HKUST3 https://github.com/dvlab-research/VisionReasoner"
        },
        {
            "title": "Abstract",
            "content": "Large vision-language models exhibit inherent capabilities to handle diverse visual perception tasks. In this paper, we introduce VisionReasoner, unified framework capable of reasoning and solving multiple visual perception tasks within shared model. Specifically, by designing novel multi-object cognitive learning strategies and systematic task reformulation, VisionReasoner enhances its reasoning capabilities to analyze visual inputs, and addresses diverse perception tasks in unified framework. The model generates structured reasoning process before delivering the desired outputs responding to user queries. To rigorously assess unified visual perception capabilities, we evaluate VisionReasoner on ten diverse tasks spanning three critical domains: detection, segmentation, and counting. Experimental results show that VisionReasoner achieves superior performance as unified model, outperforming Qwen2.5VL by relative margins of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting)."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in large vision-language models (LVLMs) [1, 44, 7, 31] have demonstrated remarkable capabilities in visual conversations. As the field progresses, researchers are increasingly applying LVLMs to wider range of visual perception tasks, such as visual grounding [35] and reasoning segmentation [12, 24] , often incorporating task-specific modules or techniques. Inspired by the emergent test-time reasoning capabilities of large language models (LLMs) [8, 30], recent studies have explored the integration of reinforcement learning (RL) into LVLMs [43, 25, 24, 50]. Works such as VisualRFT [25] and Seg-Zero [24] demonstrate that RL can enhance reasoning in visual perception tasks. However, these approaches often employ RL in task-specific manner, relying on distinct reward functions for different tasks, which limits their scalability and generalizability. Through an analysis of diverse visual perception tasks, we observe that many can be categorized into three fundamental types: detection (e.g., object detection [17], visual grounding [48]), segmentation (e.g., referring expression segmentation [48], reasoning segmentation [12]), and counting (e.g., object counting [33]). Notably, our analysis reveals that these three task types share common structure as multi-object cognition problems, suggesting that they can be addressed through unified framework. Building on this insight, we propose VisionReasoner, unified framework that addresses diverse visual perception tasks through shared architecture. The frameworks core capabilities, which include advanced reasoning and multi-object cognition, are enabled through carefully designed reward mechanism. Format rewards, including thinking rewards that promote structured reasoning and non-repeat rewards that prevent redundant reasoning patterns. Accuracy rewards, comprising multi-object IoU rewards and L1 rewards for precise localization, strengthen multi-object cognition. Unlike previous approaches like Kosmos [35] that use cross-entropy loss, our RL framework requires *Equal contribution. Preprint. Figure 1: (a) VisionReasoner addresses diverse tasks within unified framework. It generates reasoning process and outputs the expected result corresponding to each query. (b) VisionReasoner significantly outperforms Qwen2.5VL. (c) VisionReasoner retains strong VQA capabilities. optimal prediction-to-ground-truth matching. We address this challenge by implementing an efficient matching pipeline combining the batch computing and the Hungarian algorithm, significantly improving computational efficiency while maintaining matching accuracy. To comprehensively evaluate model performance, we conduct extensive experiments with VisionReasoner across 10 diverse tasks spanning three fundamental types: detection, segmentation, and counting. Remarkably, our VisionReasoner-7B model achieves strong performance despite being trained on only 7,000 samples, demonstrating both robust test-time reasoning capabilities and effective multi-task generalization, as shown in Figure 1 (a)-(b). Experimental results show significant improvements over baseline models, with relative gains of 29.1% on COCO-val (detection), 22.1% on ReasonSeg-test (segmentation), and 15.3% on CountBench-test (counting), validating the effectiveness of our unified approach. Additionally, VisionReasoner exhibits visual question answering capabilities comparable to state-of-the-art models, as shown in Figure 1 (c). Our contributions are summarized as follows: We propose VisionReasoner, unified framework for visual perception tasks. Through carefully crafted rewards and training strategy, VisionReasoner has strong multi-task capability, addressing diverse visual perception tasks within shared model. Experimental results show that VisionReasoner achieves superior performance across ten diverse visual perception tasks within single unified framework, outperforming baseline models by significant margin. Through extensive ablation studies, we validate the effectiveness of our design and offer critical insights into the application of RL in LVLMs."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Large Vision-language Models Following LLaVAs [20] pioneering work on visual instruction tuning for large vision-language models, subsequent studies [44, 28, 31, 1, 16, 51] have adopted this paradigm for vision-language 2 conversation. Beyond visual conversation tasks, LVLMs have been extended to diverse vision applications, including visual grounding [35] and reasoning segmentation [12]. Notely, the recent GPT-4.1 [31] demonstrates state-of-the-art performance in multi-modal information processing and visual reasoning. Although these models are evaluated on specific tasks, their performance has not been systematically evaluated under unified visual perception framework. 2.2 Reinforcement Learning in Large Models In the field of large language model (LLMs), various reinforcement learning (RL) algorithms are used to enhance model performance, such as reinforcment learning from human feedback (RLHF) [32], direct preference optimization (DPO) [36] and proximal policy optimization (PPO) [40]. The recent DeepSeek R1 [8], trained using GRPO [41], demonstrates remarkable test-time scaling capabilities, significantly improving reasoning ability and overall performance. Building on these advances, researchers try to apply these RL techniques to LVLMs. Notable efforts include Visual-RFT [25], EasyR1 [50] and Seg-Zero[24], all of which exhibit strong reasoning capabilities and achieve impressive performance."
        },
        {
            "title": "3 Method",
            "content": "To develop unified visual perception model capable of solving diverse vision tasks, we first identify and analyze the representative visual perception tasks, then reformulate their inputs and outputs into set of three fundamental task categories (Section 3.1). Next, we detail the architecture of our VisionReasoner model (Section 3.2). Additionally, we present the reward functions employed for training our model (Section 3.3). Finally, we elaborate on our novel training strategy of multi-object cognition (Section 3.4). 3.1 Task Reformulation and Categorization Our analysis of vision tasks listed in Papers With Code [34] reveals that approximately 50 task types (constituting 10% of the roughly 500 cataloged vision task types) can be categorized into three fundamental task types. This suggests that single model capable of addressing these fundamental task types could potentially solve 10% of existing vision tasks. Further details are provided in the supplementary materials. Detection. Given an image and text query T, the detection task type aims to generate set of bounding boxes (Bi)N i=1 that localize objects of interest. This type requires multi-object cognition ability. This category includes tasks such as Visual Grounding [48, 11] and Object Detection [17]. Segmentation. Given an image and text query T, the segmentation task type aims to generate set of binary segmentation masks (Mi)N i=1 that identify the regions of interest. We address this type by detect-then-segment paradigm. This category includes tasks such as Referring Expression Segmentation [11, 48] and Reasoning Segmentation [12, 46]. Counting. Given an image and text query T, the counting task type aims to estimate the number of target objects specified by the query. We address this type by detect-then-count paradigm. This category includes tasks such as Object Counting [5, 33]. 3.2 VisionReasoner Model Our VisionReasoner model incorporates reasoning module, which processing image and locates targeted objects, and segmentation module that produces segmentation masks if needed. The whole architecture is shown in Figure 2 (a). We implement our model based on Seg-Zero [24], due to its superior performance in single-object segmentation tasks. To broaden its applicability, we augment it with multi-object cognition capabilities. This critical enhancement enables VisionReasoner to address three fundamental task types: detection, segmentation, counting. Specifically, given an image I, text query T, the VisionReasoner generates an interpretable reasoning process, and then produces the expected output corresponding to T. The model output is represented in structured format ({Bi, Mi})N i=1, where Bi denotes the bounding box (bbox) and Mi represents the binary mask of the targeted object. Note that Mi is produced by the segmentation module using Bi and central 3 Figure 2: Illustration of VisionReasoner. (a) For given image and text instruction T, our model generates the expected output corresponding to the instruction. (b) For each observation oi, we calculate the rewards (Section 3.3) and attain the optimal match of multi-objects (Section 3.4) points Pi as prompts. This process can be formulated as: ({Bi, Mi})N i=1 = F(I, T). (1) During inference, the user provide the input image and text prompt T, and define specified task type {detection, segmentation, counting}. The system then produces the expected outputs as follows: Output = i=1, i=1, (Bi)N (Mi)N N, if is detection, if is segmentation, if is counting. (2) In this way, our VisionReasoner can process diverse perception tasks in unified manner within shared framework. 3.3 Reward Functions Our reward functions consist of two types: format rewards and accuracy rewards. Following Seg-Zero [24], we use target object bboxes and center points to calculate the rewards rather than binary masks. These rewards jointly guide the optimization process by reinforcing both structural correctness and multi-object recognition performance. Thinking Format Reward. This reward constraint the model output thinking process between <think> and </think>tags, and output the final answer between the <answer> and </answer>tags. Answer Format Reward. We use bounding boxes (Bi)N i=1 as the answer as it has better training efficiency. So this reward restrict the model output answer in [{bbox_2d : [x1, y1, x2, y2], point_2d : [x1, y1]}, ...]. Non Repeat Reward. To avoid repeated patterns, we split the reasoning process into individual sentences and prioritize those with unique or non-repetitive thinking processes. i=1 and points (Pi)N Bboxes IoU Reward. Given set of ground-truth bounding boxes and predicted bounding boxes, this reward computes their optimal one-to-one matched Intersection-over-Union (IoU) scores. For each IoU exceeding 0.5, we increment the reward by 1 max{N, K} . Bboxes L1 Reward. Given set of ground-truth bounding boxes and predicted bounding boxes, this reward computes their one-to-one matched L1 distance. For each L1 distance below the threshold of 10 pixel, we increment the reward by 1 max{N, K} . Points L1 Reward. Given set of ground-truth points and predicted points, this reward computes their one-to-one matched L1 distance. For each L1 distance below the threshold of 30 pixel, we increment the reward by 1 . max{N, K} 4 3.4 Multi-object Cognition Multi-object Data Preparation. We derive bboxes and points directly from the original mask annotations in existing segmentation datasets (e.g., RefCOCOg [48], LISA++[46]). Specifically, for given binary mask of an object, we determine its bounding box by extracting the leftmost, topmost, rightmost, and bottommost pixel coordinates. Additionally, we compute the center point coordinates of the mask. Different from Seg-Zeros [24] single-object formulation, we process multiple objects per image by: (i) using one central point (ii) joining all textual descriptions with the conjunction and, and (iii) concatenating all associated bounding boxes and center points into list per image. Multi-object Matching. key challenge in training VisionReasoner is its multi-object matching mechanism. Our framework addresses this through batch computation and the Hungarian algorithm, which optimally solves the many-to-many matching problem for bounding boxes IoU rewards, bounding boxes L1 rewards, and points L1 rewards. As shown in Figure 2 (b), for each observation oj, which contains list of bboxes (Bpredi)K i=1, we calculate its reward scores with the ground-truth bboxes (BGTi)N i=1 by implementing batch computation. We then calculate the optimal one-to-one matching with using Hungarian algorithm. These design guarantees optimal assignment between predictions and ground truth annotations while achieving high computational efficiency. i=1 and points (Ppredi)K i=1 and points (PGTi)N"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Evaluation Benchmarks We use ten benchmarks to evaluate model performance across general vision perception tasks. Our evaluation includes three fundamental task types: detection, segmentation and counting. Specifially, we employ COCO [17] and RefCOCO(+/g) [48] for detection evaluation; RefCOCO(+/g) and ReasonSeg [12] for segmentation evaluation; PixMo-Count [5] and CountBench [33] for counting evaluation. Table 1: Statistics of evaluation benchmarks. We report the number of instances for detection and segmentation tasks. The reported numbers combine validation and test splits where applicable. Type Data # of samples Det COCO RefCOCO RefCOCO+ RefCOCOg Annotation Preparation. To ensure consistency across all evaluation tasks, we standardize the evaluation data by converting all samples into unified multi-modal conversation format and removing potential information leakage. This preprocessing involves: converting numeric class labels to textual descriptions in COCO [17]; removing explicit numerical references from text descriptions in CountBench [33]; applying consistent formatting across all datasets to maintain evaluation fairness. RefCOCO RefCOCO+ RefCOCOg ReasonSeg Pixmo-Count CountBench Count SUM Seg 36,781 5,786 5,060 7,596 1,975 1,975 5,023 979 1,064 504 66, Evaluation Metrics. For object detection on COCO, we adopt the standard AP metric computed using the COCO API [42]. For referring object grounding on RefCOCO(+/g), we use bbox AP, which measures detection accuracy at an IoU threshold of 0.5. For object segmentation on RefCOCO(+/g) and ReasonSeg, we use gIoU, computed as the mean IoU across all segmentation masks. For counting tasks, we use count accuracy as evaluation metric. Statistics and Visualization. We show the statistic data in Table 1. For detection and segmentation tasks, we report the number of valid instances. For counting tasks, we provide the total number of test samples. Our evaluation comprises total of 66,023 test samples, covering three fundamental visual perception task types and 10 specific tasks. We visualize some examples in Figure 3. 4.2 Experimental Settings Training Data. The training data is sourced from four datasets: LVIS [9], RefCOCOg [48], gRefCOCO [18], and LISA++ [46], following the strategy outlined in Section 3.4. These datasets provide diverse text annotations: LVIS employs simple class names as text, RefCOCOg contains referring 5 Figure 3: Examples from evaluation benchmarks. Zoom in for better viewing. Table 2: Performance comparison on detection tasks. Detection Method COCO RefCOCO RefCOCO+ RefCOCOg Avg. val val testA val testA val test Task-specific Models VGTR TransVG RefTR MDETR OWL-ViT YOLO-World-S GLIP-T G-DINO-T DQ-DETR - - - - 30.9 37.6 46.6 48.4 50.2 Large Vision-language Models Shikra-7B Qwen2-VL-7B Qwen2.5-VL-7B VisionReasoner-7B - 28.3 29.2 37.7 79.0 81.0 85.7 86.8 - - 50.4 74.0 88.6 87.0 80.8 88.8 88.6 82.3 82.7 88.7 89.6 - - 54.3 74.9 91.0 90.6 83.9 91.7 90.6 63.9 64.8 77.6 79.5 - - 49.5 66.8 81. 81.6 72.5 82.3 83.6 70.1 70.7 82.3 84.1 - - 52.8 69.9 86.2 87.4 76.5 88.2 87.9 65.7 68.7 79.3 81.6 - - 66.1 71.1 82.8 82.3 77.3 84.7 86.1 67.2 67.7 80.0 80.9 - - 66.9 72.1 83. 82.2 78.2 85.7 87.5 - - - - - - 55.2 68.2 80.6 - 71.1 78.6 80.3 expressions where each text corresponds to single object, gRefCOCO includes expressions that may refer to multiple objects, and LISA++ features texts requiring reasoning. Together, these datasets comprise diverse range of text types, with approximately 7,000 training samples in total. Reinforcement Learning. We train VisionReasoner using the GRPO algorithm [41]. During training, the policy model produces several response samples for each input. These samples are assessed by reward functions, and the policy model optimizes its parameters to maximize reward while maintaining proximity to the reference model via KL-divergence regularization. Implementation Details. We initialize VisionReasoner with the similar settings in Seg-Zero [24]. We employ batch size of 16 and learning rate of 1e-6. The entire training process takes 6 hours. 4.3 Main Results We compare the results with LVLMs and task-specific models on each of the three fundamental task types. It is worthy note that our VisionReasoner is capable of handling different tasks within the same model and is evaluated in zero-shot manner. Detection. We compare VisionReasoner with several state-of-the-art LVLMs, including Shikra [2], Qwen2-VL-7B [44] and Qwen2.5VL-7B [1]. For task-specific models, we evaluate against VGTR [4], TransVG [6], RefTR [15], MDETR [10], OWL-ViT [29], YOLO-World [3], GroundingDINO [22], DQ-DETR [21], GLIP [14]. Since LVLMs do not output confidence score, we approximate it using the ratio of the bounding box area to the total image area (bbox_area / image_area) to enable compatibility with COCOAPI [42]. However, this coarse approximation leads to underestimated AP scores. As shown in Table 2, VisionReasoner achieves superior performance among LVLMs. While 6 Table 3: Performance comparison on segmentation tasks and counting tasks. We use SAM2 for vision-language models if necessary in segmentation tasks. Method ReasonSeg RCO RCO+ RCOg Avg. Pixmo Count Avg. Segmentation Counting val test testA testA test val test test Task-specific Models LAVT ReLA - - 22.4 21. 75.8 76.5 44.4 36.8 Large Vision-language Models LISA-7B LLaVA-OV-7B GLaMM-7B PixelLM-7B Seg-Zero-7B Qwen2-VL-7B Qwen2.5-VL-7B VisionReasoner-7B 66.3 62.6 57.5 44.5 38.7 56.9 52.1 63.6 - - - - - - 76.5 - 58.1 76.5 80.3 68.7 79.9 78.9 68.4 71.0 67.4 - 47.1 71.7 76.2 65.7 76.8 74.9 62.1 66.0 68.5 - 55.6 70.5 72.6 63.5 72.8 71.3 - 51. 58.7 - - - 69.8 56.2 67.7 71.0 - - - - - - 55.8 53. - - - - - - 29.9 48.0 63.3 67.9 69.5 70.1 - - - - - 78.8 - - - 76.5 76.0 87. - 62.8 51.5 69.1 75.7 Table 4: Comparison on the reasoning length. Complex instructs require longer reasoning. Table 5: Comparison of multi-object matching. Our code achieves 6 1035 times speedup. Data Avg. Len (# words) Hungarian Batch Comp Time (s) COCO RefCOCOg ReasonSeg 62 65 71 > 3 1032 2 103 5 104 our model shows performance gap compared to some task-specific baselines on COCO datasets, it maintains competitive advantages due to its superior generalization capability. Segmentation. We evaluate VisionReasoner against state-of-the-art LVLMs, including LISA [12], GLaMM [37], PixelLM [39], Seg-Zero [24], Qwen2-VL [44] and Qwen2.5VL [1]. For these LVLMs, we first extract bounding box predictions and subsequently send them into SAM2[38] to generate segmentation masks. We also compare task-specific models, including LAVT [47] and ReLA [19]. For models that do not report gIoU, we report their cIoU as an alternative. As shown in Table 3, VisionReasoner achieves state-of-the-art performance, outperforming both general-purpose LVLMs and task-specific approaches. Counting. We evaluate VisionReasoner against state-of-the-art LVLMs, including LLaVA-OneVision [13], Qwen2-VL-7B [44] and Qwen2.5VL-7B [1]. We evaluate these LVLMs in first-detect-thencount manner. As shown in Table 3, VisionReasoner achieves state-of-the-art performance. 4.4 Ablation Study We perform ablation studies to assess the effectiveness of our design and validate the optimal hyperparameter selection and training recipe design for VisionReasoner. We also evaluate VisionReasoner on VQA tasks. Reasoning Length. As shown in Table 4, our analysis reveals that the models reasoning length adapts dynamically to text query complexity. Specifically, for simple class names in COCO and short phrases in RefCOCOg, the reasoning process is relatively concise. In contrast, complex reasoning-intensive queries in ReasonSeg require longer reasoning processes. Multi-object Matching. We quantitatively assess the efficiency of our two key design choices for multi-object matching: the Hungarian algorithm and batch computation. In scenario with 30 objects, Table 5 demonstrates that random permutation and simple matching require over 3 1032 seconds (i.e., 8 1024 years) to complete, while our optimized approach achieves matching in just 5 104 seconds - 6 1035 times speedup. 7 Figure 4: Ablation on non-repeat reward. (a) Consistent performance gain across different datasets using non-repeated reward. (b) Non-repeat rewards lead to shorter response lengths. Table 6: Performance comparison on different training data. Method Training Data Det Seg RefCOCOg gRefCOCO LVIS LISA++ RefCOCOg-val ReasonSeg-val VisionReasoner-7B 84.1 85.8 85.5 86.1 61.9 63.8 64.2 66.3 Avg. 73.0 74.8 74.9 76.2 Table 7: Performance comparison on VQA tasks. Method OCRBench RealworldQA MMMUvision MMMUstd ChartQA DocVQA Qwen2VL-7B Qwen2.5VL-7B VisionReasoner-7B 809 822 66.1 69.2 69.5 28.0 32.4 32.6 33.8 36.4 37.4 81.4 83.1 84.9 94.5 95.7 96.0 Non Repeat Reward. Figure 4 (a) presents the performance comparison with and without the non-repeat reward. Models are trained only on 2,000 samples from RefCOCOg. The model achieves better results when trained using the non-repeat reward. Additionally, model without non-repeat reward tends to generate longer reasoning processes, as shown in Figure 4 (b), and we observe repetitive reasoning patterns during inference. Different Training Data. We conduct an ablation study on different training datasets, with results presented in Table 6. The four datasets provide diverse text annotations: LVIS uses simple class names, RefCOCOg contains single-object referring expressions, gRefCOCO includes expressions that may refer to multiple objects, and LISA++ features texts requiring reasoning. Our experiments demonstrate that these datasets consistently improve model performance. Visual QA Ability. We also compare VisionReasoners VQA [26, 27, 23, 49, 45] ability with Qwen2VL [44] and the baseline model Qwen2.5VL[1]. As shown in Table 7, VisionReasoner achieves slight performance gain even thought we do not train on VQA data. Sampling Number. Figure 5 presents performance comparison with different sampling number. Models are trained using all 7,000 training samples. We observe an initial performance gain followed by notable decline with larger sampling number, suggesting that excessive sampling may induce overfitting to the training distribution and consequently degrade generalization capability. Reasoning. Figure 6 compares the performance of models with and without reasoning capabilities. We implement no reasoning by removing thinking reward. Models are trained using all 7,000 training samples. Our results show that both approaches outperform the baseline. And the reasoning-enhanced model demonstrates significant gain on intricate reasoning segmentation data. 4.5 Qualitative Results We visualize some results on Figure 7. Notably, VisionReasoner addresses several visual perception tasks within shared model. Our model generates comprehensive reasoning processes for all 8 Figure 5: Different sampling number. Figure 6: Reasoning vs. no reasoning Figure 7: Qualitative results on different datasets. Zoom in for better visualization. tasks while producing expected outputs. We find that VisionReasoner can effectively distinguish between similar objects, as shown in the visual grounding and referring expression segmentation. VisionReasoner also accurately localize multiple targets, as shown in object detection and counting. We also observe that the length of the reasoning process adapts dynamically: more intricate imagequery pairs elicit detailed rationales, while simpler inputs result in concise explanations."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present VisionReasoner, unified vision-language framework. With systematic task reformulation, VisionReasoner effectively addresses diverse visual perception tasks within shared model. By introducing novel multi-object cognitive learning strategies and curated reward functions, VisionReasoner demonstrates strong capabilities in analyzing visual inputs, generating structured reasoning processes and delivering task-specific outputs. Experiments across ten diverse tasksspanning detection, segmentation, and countingvalidates the robustness and versatility of our approach. Notably, VisionReasoner achieves significant improvements over existing models such as Qwen2.5VL, with relative performance gains of 29.1% on COCO (detection), 22.1% on ReasonSeg (segmentation), and 15.3% on CountBench (counting). These results highlight the potential of unified frameworks in advancing multiple visual perception tasks."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [3] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. [4] Cheng Da, Chuwei Luo, Qi Zheng, and Cong Yao. Vision grid transformer for document layout analysis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1946219472, 2023. [5] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [6] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang Zhou, and Houqiang Li. Transvg: End-to-end visual grounding with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 17691779, 2021. [7] Google. Gemini pro 2.5. https://deepmind.google/technologies/gemini/, 2025. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. [10] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17801790, 2021. [11] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [12] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [14] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1096510975, 2022. [15] Muchen Li and Leonid Sigal. Referring transformer: one-step approach to multi-task visual grounding. Advances in neural information processing systems, 34:1965219664, 2021. [16] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 10 [17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [18] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2359223601, 2023. [19] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Generalized referring expression segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2359223601, 2023. [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [21] Shilong Liu, Shijia Huang, Feng Li, Hao Zhang, Yaoyuan Liang, Hang Su, Jun Zhu, and Lei Zhang. Dq-detr: Dual query detection transformer for phrase extraction and grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 17281736, 2023. [22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [23] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [24] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Segzero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. [25] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [26] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [27] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [28] Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. https: //ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/, 2024. [29] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European conference on computer vision, pages 728755. Springer, 2022. [30] OpenAI. OpenAI o1. https://openai.com/o1/, 2024. [31] OpenAI. Introducing gpt-4.1 in the api. https://openai.com/index/gpt-4-1/, 2025. [32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [33] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31703180, 2023. [34] Papers with Code. Datasets - papers with code. https://paperswithcode.com/datasets? mod=images&page=1, 2021. [35] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, Qixiang Ye, and Furu Wei. Grounding multimodal large language models to the world. In The Twelfth International Conference on Learning Representations, 2024. [36] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [37] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. [38] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [39] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. [40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [42] COCO Team. Microsoft COCO: Common objects in context. https://github.com/ cocodataset/cocoapi, 2014. [43] R1-V Team. R1-V. https://github.com/Deep-Agent/R1-V?tab=readme-ov-file, 2025. [44] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [45] xAI. Grok-1.5 vision. https://x.ai/news/grok-1.5v, 2024. [46] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. Lisa++: An improved baseline for reasoning segmentation with large language model. arXiv preprint arXiv:2312.17240, 2023. [47] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1815518165, 2022. [48] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. [49] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 12 [50] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github. com/hiyouga/EasyR1, 2025. [51] Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, et al. Lyra: An efficient and speech-centric framework for omni-cognition. arXiv preprint arXiv:2412.09501, 2024."
        }
    ],
    "affiliations": [
        "CUHK",
        "HKUST",
        "SmartMore"
    ]
}