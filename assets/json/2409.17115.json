{
    "paper_title": "Programming Every Example: Lifting Pre-training Data Quality like Experts at Scale",
    "authors": [
        "Fan Zhou",
        "Zengzhi Wang",
        "Qian Liu",
        "Junlong Li",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (ProX), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on ProX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, ProX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by ProX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over Mistral-7B, with 14.6% for Llama-2-7B and 20.3% for CodeLlama-7B, all within 10B tokens to be comparable to models like Llemma-7B trained on 200B tokens. Further analysis highlights that ProX significantly saves training FLOPs, offering a promising path for efficient LLM pre-training.We are open-sourcing ProX with >100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. Code: https://github.com/GAIR-NLP/ProX"
        },
        {
            "title": "Start",
            "content": "PROGRAMMING EVERY EXAMPLE: LIFTING PRETRAINING DATA QUALITY LIKE EXPERTS AT SCALE Fan Zhou αµ Zengzhi Wangαµ Qian Lius Junlong Liα Pengfei Liuαµδ αShanghai Jiao Tong University δShanghai Artificial Intelligence Laboratory sSea AI Lab µGenerative AI Research Lab (GAIR) {zhoufan98,pengfei}@sjtu.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language model pre-training has traditionally relied on human experts to craft heuristics for improving the corpora quality, resulting in numerous rules developed to date. However, these rules lack the flexibility to address the unique characteristics of individual example effectively. Meanwhile, applying tailored rules to every example is impractical for human experts. In this paper, we demonstrate that even small language models, with as few as 0.3B parameters, can exhibit substantial data refining capabilities comparable to those of human experts. We introduce Programming Every Example (PROX), novel framework that treats data refinement as programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. Experimental results show that models pre-trained on PROX-curated data outperform either original data or data filtered by other selection methods by more than 2% across various downstream benchmarks. Its effectiveness spans various model sizes and pre-training corpora, including C4, RedPajama-V2, and FineWeb. Furthermore, PROX exhibits significant potential in domain-specific continual pre-training: without domain specific design, models trained on OpenWebMath refined by PROX outperform human-crafted rule-based methods, improving average accuracy by 7.6% over MISTRAL-7B, with 14.6% for LLAMA-2-7B and 20.3% for CODELLAMA-7B, all within 10B tokens to be comparable to models like LLEMMA-7B trained on 200B tokens. Further analysis highlights that PROX significantly saves training FLOPs, offering promising path for efficient LLM pre-training. We are open-sourcing PROX with 100B corpus, models, and sharing all training and implementation details for reproducible research and future innovation. HF Repo: https://huggingface.co/gair-prox Code: https://github.com/GAIR-NLP/ProX 4 2 0 2 5 2 ] . [ 1 5 1 1 7 1 . 9 0 4 2 : r Figure 1: Training FLOPs v.s. average downstream performance. Although these corpora have gone through expert-crafted rules, applying PROX still yields significant improvements over these baseline models trained with original data corpus. Moreover, with much less training FLOPs, model trained on PROX curated data show comparable performance with existing models. Equal contribution. Corresponding author."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have made significant strides in capabilities (Meta, 2024; Achiam et al., 2023; Anthropic, 2024; Reid et al., 2024), excelling in tasks such as creative writing (Yuan et al., 2022), complex reasoning (Wei et al., 2022; Kojima et al., 2022), and agentic task planning and execution (Fan et al., 2022; Park et al., 2023). Behind these, massive, high-quality pre-training corpora form the backbone of these models, equipping them with the essential knowledge and reasoning abilities crucial for wide range of downstream tasks (Together, 2023; Penedo et al., 2024a). The Internet offers vast amounts of data, but much of it is noisy and unrefined, requiring extensive cleaning and quality enhancement before being applied for pre-training. Previous works focus primarily on designing heuristic-based pipelines to lift data quality, such as document filtering (Rae et al., 2021; Penedo et al., 2024a; Soldaini et al., 2024) and perplexity-based scoring methods (Together, 2023), relying heavily on human expertise and manual adjustments (Zhang et al., 2024a). While widely adopted, these labor-intensive solutions are inherently limited by rule coverage and their inability to address every specific case. Recently, some efforts have explored leveraging LLMs for high-quality data acquisition. On the one hand, language models have been applied for data filtering or selection (Xie et al., 2023; Wettig et al., 2024; Yu et al., 2024; Dubey et al., 2024), but their role is largely limited to identifying low-quality documents without enabling fine-grained refinements (e.g., string-level). On the other hand, LLMs are also being used directly generating high-quality data, i.e., data synthesis (Gunasekar et al., 2023; Li et al., 2023; Ben Allal et al., 2024). Unlike filtering, synthesis methods actively create or refine data to produce new documents, but they require substantial computational resources, limiting scalability. Despite their success, these methods can also inherit issues like hallucination (Maini et al., 2024), and assessing their correctness and completeness in an interpretable manner remains challenge (Liu et al., 2024a). Standing at the intersection of data processing efficiency and data quality improvement, in this work, we propose PROX, model-based framework for pre-training level data refinement. PROX focuses on refining large-scale data with relatively smaller models, offering more efficient alternative. As shown in Figure 2, in practice, PROX first adapts small base language model (less than 1B) to data refining tasks via fine-tuning on seed data. This PROXs refining model then determines the appropriate operations for each example in the pre-training corpora through versatile programs, including operations such as filtering, string normalization and noisy line removal. Finally, the generated program is executed by pre-defined executor, producing refined corpus ready for pretraining. In this way, PROX is empowered with language models to autonomously refine pre-training corpora, leveraging flexible function calls to enhance data quality. Experimental results demonstrate that the proposed PROX framework consistently lifts data quality for pre-training. Specifically, PROX achieves an average improvement of 2.1% over 10 downstream benchmarks and outperforms state-of-the-art data selection methods by over 2.0%. Furthermore, PROX shows broad applicability across model sizes from 0.3B to 1.7B and shows consistent performance gains across diverse pre-training corpora of varying quality, including RedPajama-V2 (Together, 2023), C4 (Raffel et al., 2020), and FineWeb (Penedo et al., 2024a). In domain-specific continual pre-training, PROX yields an 11% gain over OpenWebMath (Paster et al., 2024) for TINYLLAMA-1.1B and 7.6% for MISTRAL-7B across 9 mathematical tasks, with similar improvements seen on LLAMA-2-7B and CODELLAMA-7B. Beyond performance gains, results also suggest that pre-training on the refined corpus significantly boosts pre-training efficiency, achieving similar downstream performance with up to 20 less computing. We believe it is worthwhile to scale up computing FLOPs for data refinement, which enables similar performance with much less training cost and offers promising path for efficient LLM pre-training."
        },
        {
            "title": "2.1 DATA REFINEMENT TASK FORMULATION",
            "content": "Given any document in the corpus D, such as an HTML extract or textbook, we define data refinement as the process of transforming into ˆd, where ˆd exhibits higher quality. While it is challenging to formally define higher quality for pre-training data, we assume it can be described 2 Figure 2: An overview of PROX framework: (1) we adapt base language model to perform data refinement; (2) PROX refining models are able to generate complex programs for each document, including document level filtering and more fine-grained chunk level refining; (3) Python executor will execute the programs with the docs, producing the refined high-quality corpora. through qualitative improvements, such as the removal of advertisements, meaningless URL links, random code gibberish, and content lacking educational value, just as shown on the left side of Figure 2. Specifically, we formulate this refining process as the generation of data processing program Z, conditioned on d. The refined document ˆd is then produced by executing program on the original document d. For instance, the string normalization can be very fine-grained process transforming noisy strings into clean ones with executor and program Znormalize : E(Znormalize, d) = (s i)d i=1, where i = normalize(si) if si needs normalization else si (1) Here, = (s1, s2, ..., sd) is the original document represented as sequence of strings, and normalize() is our normalization function that maps certain strings to their normalized versions. Moreover, the document filtering process can be regarded as special case of such refining transformation where executing on Zfilter will lead to removing the whole document, i.e., E(Zfilter, d) = . In this manner, data quality improvement operations, such as data cleaning or normalizing, can be unified into the standardized function that applies specific transformation or cleaning process to the document. These operations can be represented as various instantiations of the general executor E(Z, d), where encodes the function calling snippets or heuristics for the specific task."
        },
        {
            "title": "2.2 PROX FRAMEWORK",
            "content": "Overview As shown in Figure 2, given any document as input, the PROX framework utilizes the language model itself with parameter θ to generate the data refinement program = (θ, d). The snippet is executed within the executor E, producing the refined document ˆd = E(f (θ, d), d). We include two stages in the PROX framework, aiming to refine the data progressively, from rough to fine-grained. These two stages are referred to as document-level programming and chunk-level programming, as illustrated in Figure 2. In each stage, the PROX refining model will generate programs Zdoc and Zchunk that refine the corpora at varying levels of granularities. PROX Program Design The detailed program space design is also crucial for maximizing the capabilities of language models. We believed designing such model-based operations should consider several realistic factors when scaling to large pre-training corpora: (1) the model does not need to be very powerful or very large to handle these tasks, it only needs to recognize several patterns; (2) the solution, though requiring more computing budget compared to heuristic-rule-based pipelines, still needs to be simple and efficient. Under such consideration, we simply let the language models generate function calls without detailed implementations. These design choices aim to balance functionality with the limitations of small language models, enabling effective document manipulation while maintaining simplicity and coherence. The most fundamental operations we aim to perform on document, are deletion and replacement. We incorporate these types of operations across different programming stages aiming to refine the corpus with different granularities in PROX: (1) In the document-level programming stage, we simply define the function drop_doc()to delete document and keep_doc()to retain it. (2) In chunk-level programming, we split the lengthy documents into smaller chunks and apply fine-grained operations to these chunks. These operations include deleting specific lines remove_lines() 3 Table 1: PROX program design of document-level and chunk-level refining stage. For input, doc and chunk will be sent into the corresponding function as string-type inputs for execution. Stage Function Interface Document Level drop_doc() <None> keep_doc() <str> Description Delete the whole doc. Return the orignal doc. remove_lines(line_start, line_end) <str> line_start<int>, index of the first line to be removed line_end<int>, index of the last line to be removed Delete noisy lines from chunk; Return chunk after removal. Chunk Level normalize(source_str, target_str) <str> source_str<str>, the noisy string pattern target_str<str>, the string for replacement Replace strings with normalized ones; Return chunk after replacement. keep_chunk() <str> Return the orignal chunk. and replacing strings normalize(), providing flexibility in modifying content rather than simply dropping the whole document. Also for high-quality chunks that do not require any modifications, we use the keep_chunk() function for flagging. We present the detailed function definition in Table 1, which is also the generation space of PROXs refining models. While the individual functions may seem straightforward, their design space is flexible and capable of expressing complex rules previously developed by human experts as shown in Table 1. In fact, these rules can be projected into the program space of PROX, showcasing that our approach not only simplifies but also enhances the rule-creation process, offering more systematic and scalable refinement capabilities. PROX Execution During the execution stage, the generated program snippets will be executed by the executor to refine the document. For simplicity and flexibility, PROX integrates Pythonic grammars, wrapping all operations into different function calling with parameters and implements these function in Python for later execution. For example, in Figure 2, the document contains some noisy patterns including navigation bars, meaningless HTML links and page indexes. The refining model will then generate programs to remove the corresponding lines and patterns. In the document-level and chunk-level cleaning stage, PROX utilizes an independent refining model to generate programs with various function calls described in Table 1. We believe this sequential approach ensures structured and effective refinement, addressing the larger document noise first, and then focusing on finer-grained cleaning."
        },
        {
            "title": "2.3 MODEL ADAPTATION FOR PROX",
            "content": "Figure 3: The illustration of the model adaptation in PROX. We employ powerful LLMs (LLAMA-3) to annotate random seed documents with valid programs, and use this doc-program pairs to fine-tune small base model, obtaining the refining model suitable for fine-grained data refining tasks. It is generally difficult for base models to directly generate PROX programs. In fact, even for the most powerful post-trained LLMs, generating custom API calls is relatively challenging at the current stage (Zhuo et al., 2024). Thus, it will be necessary that we curate some seed data to adapt the model for these scenarios. Under such consideration, we employ strong LLMs to annotate these operations via zero-shot and few-shot prompting, and then adapt our base model to these tasks by supervised fine-tuning (SFT). We first use two additive scale scoring prompts (Yuan et al., 2024; Penedo et al., 2024a) to split the corpus into kept documents and dropped documents. And then we use large models to annotate fine-grained programs based on kept documents. Specifically, we leverage the LLAMA-3 series of models (Dubey et al., 2024) for data collection and annotation. In PROX, this data collection is performed only once, and all base models are adapted with the same 4 curated data. To ensure the reliability of the collected data, we also conduct necessary checks for grammar correctness and control the removal ratio threshold. The detailed procedure for program synthesis and post-processing can be found in A.1. For simplicity, we directly use small language model (e.g., 0.3B parameters) that we have trained on approximately 26B tokens of original unrefined data as the base model, which also serves as the comparison baseline in subsequent experiments. The adapted models performance is then evaluated using the F1 score on the split validation dataset, ensuring robust assessment. We select the highestperforming model checkpoint and employ the model to generate programs Z, for each document or chunk of the dataset. These programs together with the documents are then executed using the corresponding function implementation, resulting in the final processed corpus. Please see appendix for more training details ( A.2), implementation for calculating the F1 score ( A.3), and large scale inference ( A.4)."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "In this section, we first describe our experimental setup, then verify the effectiveness of each PROX stage and compare it with existing data selection methods tailored for pretraining corpus ( 3.2). We then apply PROX to various model sizes and corpora to demonstrate its broad applicability ( 3.3). Finally, we apply PROX to the mathematical domain, demonstrating its superiority and universality in domain-specific training ( 3.4)."
        },
        {
            "title": "3.1 EXPERIMENT SETUP",
            "content": "Training Corpora We utilize various corpora for both general and specific domain data in our experiments. For general domain data, we begin with RedPajama-V2 (Together, 2023), preprocessed large-scale dataset of 30 trillion tokens from diverse Internet sources, ready for pre-training. We further apply PROX on the C4 corpus (Raffel et al., 2020) with 198 billion tokens and the FineWeb dataset (Penedo et al., 2024a) containing 15 trillion tokens, noted for high data quality. For specific domain experiments, we use OpenWebMath (Paster et al., 2024), math-focused dataset with 15 billion tokens. Given the limitations in computational resources, we conduct experiments on randomly sampled subset of the entire pre-training dataset. See Table 7 ( B.2) for sampling details. Base Model Selection Our pre-training experiments are conducted using various sizes of decoderonly language models. Detailed specifications of these models and all training recipes are provided in B.3, especially in Table 8 and Table 9. 1. To verify different stages effectiveness of PROX, we employ 750M sized model sharing LLAMA2 architecture (Touvron et al., 2023a), denoted as TLM-S, used for both pre-training from scratch and refining. We also compare PROX with data selection methods using PYTHIA-410M/1Bs architecture (Biderman et al., 2023), as those employed in MATES (Yu et al., 2024). 2. For further evaluation of PROX using different refining and base model sizes, we scale the model sizes from 350M (0.5 smaller, denoted as TLM-XS) and 1.7B (2 larger, denoted as TLM-M), all based on the LLAMA-2 architecture. 3. For domain-specific continual pre-training, we select TINYLLAMA-1.1B (Zhang et al., 2024b), LLAMA-2 (Touvron et al., 2023a), CODELLAMA (Rozière et al., 2023) and MISTRAL-7B (Jiang et al., 2023) as representative base models for their adequate training and solid performance. Baselines To ensure fair comparison w.r.t. training cost, we keep all training hyperparameters, such as training steps and batch size, consistent across baselines, with only the data refining and selection pipelines differing. We compare PROX to series of baselines: 1. In 3.2, to verify PROXs effectiveness, we first compare with PROX with regular pre-training over the raw RedPajama-V2 data. We also introduce heuristic baselines used to curate the FineWeb corpora, which is the combination of three filtering strategies from C4 (Raffel et al., 2020), Gopher (Rae et al., 2021), and newly crafted rules (as FineWeb rules). Apart from rule-based baselines, we also introduce existing data selection techniques proposed in previous works, including 5 (1) importance resampling: DSIR (Xie et al., 2023); (2) model-based selection: DsDm (Engstrom et al., 2024), MATES (Yu et al., 2024), and QuRating (Wettig et al., 2024). 2. In 3.3, to test PROX on different model sizes and training corpora, we finally scale the TLM-Ms training tokens to 50B over RedPajama-V2, C4, and FineWeb. To show PROX efficiency, we then directly compare with models covering variety of pre-training approaches including (1) large-scale pre-training: TINYLLAMA-1.1B (Zhang et al., 2024b) trained on 3T tokens; (2) model pruning from existing models: (SHEADLLAMA (Xia et al., 2024) pruned from LLAMA-2 and trained on extra 50B tokens); (3) LLM synthesis (INSTRUCTIONLM-1.3B (Cheng et al., 2024) trained on MISTRAL-7B generated data and COSMO-1.8B (Ben Allal et al., 2024) trained on MISTRAL-8x7B generated data). 3. In 3.4s specific domain continual pre-training, apart from standard continual pre-training on TINYLLAMA-1.1B, LLAMA-2-7B, CODELLAMA-7B, and MISTRAL-7B, we additionally introduce with well-known and strong baselines trained on public (or partially public) data, including RHO-1 (Lin et al., 2024), INTERNLM2-MATH (Ying et al., 2024), LLEMMA (Azerbayev et al., 2024), and an internal checkpoint reported in DEEPSEEK-MATH (Shao et al., 2024). Evaluation Setup We compare the base models performance over vast of datasets for comprehensive evaluation: (1) For general pre-training, we evaluate the performance across ten selected tasks using lightevals implementation (Fourrier et al., 2023), and report the zero-shot accuracy; we have also included LM-eval-harness (Biderman et al., 2024) for fair comparison with data selection methods. (2) For domain-specific continual pre-training evaluation, i.e., mathematical related benchmarks, we use the same nine implementation and benchmarks used in RHO-1 (Lin et al., 2024) and evaluate all the base models with few-shot chain-of-thought (CoT) examples (Wei et al., 2022). The selected evaluation benchmarks, number of evaluation examples, and full details can be found in C."
        },
        {
            "title": "3.2 VERIFYING PROX’S EFFECTIVENESS",
            "content": "Verifying Effectiveness for Each PROX Operation We first conduct series of experiments to verify the effectiveness of each PROX operation. We begin by training TLM-S on the RedPajama-V2 raw data for approximately 26B tokens (or 12.5K steps) as the initial baseline. Following Wettig et al. (2024) and for convenience, we then sequentially apply the doc-level and chunk-level refining pipelines by fine-tuning the 0.7B model itself. We then perform large-scale program synthesis and execution using the refining models, resulting in DDoc and DDoc+Chunk. Such 2-stage synthesis requires approximately 192 A100-80G GPU hours for processing 60B tokens of data. The resulting zero-shot downstream performance is presented in Table 2, including base models trained on the data produced by PROX refinement methods and different rule-based filtering methods. Moreover, we visualize the dynamic benchmark performance in Figure 4, implying the consistent improvement of PROX over all baselines. See D.1 for full detailed results of all intermediate checkpoints. These results show that PROX is highly effective, outperforming the raw corpus with an average boost of 2.5%, including significant improvements such as 7.6% on ARC-E, 3.3% on HellaSwag, and 2.1% on MMLU. We believe such consistent performance is significant given that these improvements were achieved even on benchmarks that are typically prone to performance instability, such as SIQA, WinoGrande, and CSQA. By contrast, rule-based methods demonstrate relatively marginal overall improvement. For instance, Gopher rules achieve only 0.2% boost, while C4 shows modest 0.5% improvement. Furthermore, combining all three rules (as is done in constructing the official FineWeb corpus), does not lead to any larger enhancement in overall performance. Comparing with Data Selection Methods Apart from comparing with heuristic methods, we also include existing representative model-based data selection methods tailored for pertaining corpus to verify PROXs effectiveness in Table 3, where we report both 0-shot and 2-shot performance under the same settings used in MATES (Yu et al., 2024). While we merely apply document-level stage (i.e., PROX-D) which is indeed similar to data selection methods, we can see that PROX outperforms the strongest data selection method MATES, by 2.2% and 2.5% in 0-shot and 2-shot average performance for 410M model, and by 1.0% and 2.0% for 1B model. Additionally, PROX achieves the best performance on 7 out of 8 benchmarks tested, demonstrating its superiority over existing data selection methods. Full evaluation results are provided in Table 11 ( D.2). 6 Table 2: Zero-shot performance on 10 selected tasks. All models use the same TLM-S architecture and are trained on RedPajama-V2. The doc-level (PROX-D) and chunk-level (PROX-C) refining are done by fine-tuning the raw data pre-trained model as refining model. Bolded entries represent the best results. #Win represents the number of tasks where the method achieved the best performance."
        },
        {
            "title": "Method",
            "content": "ARC-C ARC-E CSQA HellaS MMLU OBQA PIQA SIQA WinoG SciQ AVG #Win"
        },
        {
            "title": "Raw",
            "content": "26.1 44.3 29.7 39.1 27.3 29. 66.9 39.0 52.0 67.4 42.1 0 / 10 Rule-based filtering: GO = Gopher rules, C4 = C4 rules, FW = FineWeb rules."
        },
        {
            "title": "25.7\nGO\nC4\n25.0\nFW\n25.2\nGO+C4+FW 25.2",
            "content": "44.0 46.0 46.8 43.9 31.3 31.0 32.6 30.0 40.2 40.5 39.6 41.9 27.3 27.1 27.2 27.5 29.0 29.2 29.0 31.0 66.3 68.5 66.5 67. 39.0 40.5 39.4 39.9 51.2 51.7 52.4 51.9 68.9 42.3 0 / 10 66.6 42.6 2 / 10 69.2 42.8 2 / 10 65.3 42.3 0 / 10 PROX (ours): = Doc-level Programming, = Chunk-level Programming. PROX-D PROX-D+C 26.6 26. 49.7 51.9 30.1 30.9 40.5 42.4 29.4 29.4 30.4 31.6 66.3 67. 39.0 40.0 51.2 52.2 71.6 43.5 2 / 10 73.5 44.6 5 / 10 Table 3: Comparison with different data selection methods on 8 benchmarks using the C4 corpus and PYTHIA architecture. #Win represents the count of best performance."
        },
        {
            "title": "Method",
            "content": "0-shot 2-shot #Win Model Architecture: PYTHIA-410M Random DSIR (Xie et al., 2023) DsDm (Engstrom et al., 2024) QuRating (Wettig et al., 2024) MATES (Yu et al., 2024) PROX (ours) 42.7 42.5 43.4 43.5 44.0 46.2 43.8 43.7 44.1 44.6 45.0 47.5 Model Architecture: PYTHIA-1B Random MATES (Yu et al., 2024) PROX (ours) 44.7 45.8 46.8 45.4 46.4 48.4 0 / 8 1 / 8 0 / 8 0 / 8 0 / 7 / 8 0 / 8 1 / 8 7 / 8 Figure 4: Downstream zero-shot performance w.r.t. different training steps: first 0.5K, then evenly from 2.5K to 12.5K. Rule: the best performing FineWeb rule in Table 2."
        },
        {
            "title": "3.3 APPLYING PROX ACROSS MODEL SIZES AND PRETRAINING CORPORA",
            "content": "In this section, we demonstrate that PROX can effectively benefit models beyond scale and across different corpora, showing potential for iterative pre-training improvements. PROX works well across different scales. We train family of models from 350M to 1.7B (i.e., TLM-XS, TLM-S, and TLM-M) on the same 26B tokens used in 3.2, and then fine-tune these models on doc-level and chunk-level tasks, obtaining refining models with different sizes. We then apply these models in doc-level refining and chunk-level refining stages, and use the curated data for from-scratch pre-training. We report in Table 4 the adaptation performance on refining tasks of different refining model sizes. According to the validation performance, adapting PROX works well across all model sizes, all achieving 80% F1 on doc-level refinement, and 75% F1 on chunk-level refinement. We further train these models of different sizes from scratch using data produced by refining models of varying sizes. In Figure 5, the results indicate that refining models of all sizes help improve performance over raw data, with consistent absolute gap of 2% over all base model sizes. While in Figure 5, TLM-XS curated data shows slightly better downstream performance, it has significantly lower token-level retention ratio (23.2% vs. 28.8%) compared to larger models as reflected in Table 4. This implies that moderately larger models suggest favorable balance between data quality and quantity. These additional tokens likely provide more knowledge during 7 Table 4: Refining models performance on valid set and token retention ratio of original corpus. Size Doc-level Chunk-level Kept Ratio TLM-XS TLM-S TLM-M 82.6 81.3 83.7 75.2 75.6 77.3 23.2% 25.6% 28.8% Figure 5: PROXs effect over different model sizes. Figure 6: Performance of original data and PROX curated data trained models across different datasets using 50B tokens and comparison with existing models trained using different techniques. Inst-LM represents INSTRUCTIONLM1.3B; Cosmo represents COSMO-1.8B; S-Llama represents SHEAREDLLAMA-1.3B. pre-training without compromising downstream benchmark performance, showcasing an effective trade-off between data refinement and information preservation. PROX works well across pre-training corpora. To assess the applicability of PROX across various pre-training corpora, we extend our experiments beyond RedPajama-V2 to include C4 (Raffel et al., 2020), and the recently released 15-trillion-token pre-training corpus, FineWeb (Penedo et al., 2024a). For consistency, we apply exactly the same PROX-xs refining models detailed in Table 4 to these corpora without constructing new SFT data for each corpus. We conducted larger-scale experiments by training our model on approximately 50 billion tokens, again achieving notable improvements. On ten downstream benchmarks, models trained on our methods curated data showed improvements of +2.0% on RedPajama-V2, +3.1% on C4, and +2.4% on FineWeb. ProX trains language models with much greater efficiency. To demonstrate the non-trivial nature of these results, we compared models trained on PROX curated data against various models trained by different approaches. These include models like TINYLLAMA-1.1B-3T (trained directly on 3 trillion tokens, about 60 of our training tokens and 40 training FLOPs), SHEADLLAMA-1.3B (denoted as S-Llama, pruned version of LLAMA-2-7B, with extra training on 50 billion tokens), and models using LLM data synthesis, such as INSTRUCTIONLM-1.3B (denoted as Inst-LM) and COSMO-1.8B. Our results, including TLM-M (PROX) and TLM-M (Raw), are presented alongside all these baselines in Figure 6. On FineWeb, which is recognized for its high-quality data, TLM-M using PROX-refined data performs comparably to pruned models like SHEADLLAMA-1.3B and TINYLLAMA-1.1B, despite their reliance on additional pruning techniques or much larger datasets. Moreover, using much less inference-time computing overhead, our model surprisingly outperforms models that rely heavily on LLM data synthesis, underscoring PROXs efficiency. Notably, models like INSTRUCT-LM-1.3B, trained on 100 billion tokens leveraging fine-tuned MISTRAL-7B synthesizer, and COSMO-1.8B, trained on 180 billion tokens (including 25 billion tokens synthesized by MISTRAL-8x7B), require significantly more computational resources than PROX."
        },
        {
            "title": "3.4 APPLYING PROX TO DOMAIN-SPECIFIC CONTIUAL PRERAINING",
            "content": "We also demonstrate the potential of PROX in the continual pre-training scenario, specifically, in the mathematical domain. We apply the very same pipeline as in general domains to the already cleaned OpenWebMath corpus (Paster et al., 2024), aiming to further refine and mine the high quality and clean data from the vast web pages crawled in it. We then adapt and apply PROX-xs series, which was initially trained on general text as described in 3.3, and further adapted on math text for the doc-level and chunk-level refining tasks. Finally, we obtain about 5.5B tokens left after the 8 Table 5: OpenWebMath Continual Pre-training (CPT) Results. All models are tested using fewshot CoT prompts. LLEMMA and INTERNLM2-MATH are continual pre-trained models from CODELLAMA and INTERNLM2 (Team, 2023) with public available data, respectively. DEEPSEEKLLM denotes an internal DeepSeek model, and the model trained on OpenWebMath introduced by Shao et al. (2024). Note that the unique tokens and training tokens in the column refer exclusively to the token numbers from math-specific corpora (calculated by corresponding tokenizers). : MQA evaluation of INTERNLM2-BASE is based on an alternative prompt due to non-prediction issues with the original prompt. The bolded entries represent the best results within the same base model. Model Size Method Uniq Toks Train Toks GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG DEEPSEEK-LLM CODELLAMA (Base) LLEMMA INTERNLM2-BASE INTERNLM2-MATH 1.3B - 1.3B - 7B - 34B - 7B - 34B - 7B - 20B - 7B - 20B - Existing Continual Pre-training for Reference - - 14B 150B - - - - 55B 200B 55B 50B - - - - 31B 125B 120B 500B 2.9 11.5 11.8 31.8 38.8 54.2 27.0 50.6 41.8 65. 3.0 8.9 5.0 10.8 17.2 23.0 6.6 18.8 14.4 30.0 - - 44.2 61.9 56.1 67.9 49.0 72.5 61.6 75.7 - - 50.7 66. 69.1 75.7 59.3 75.9 66.8 79.3 - - 62.6 83.4 82.4 90. 74.8 93.9 83.7 94.0 Applying Data Refinement Approaches TINYLLAMA (Base) 1.1B - - - TINYLLAMA (CPT) 1.1B - 1.1B RHO 1.1B Rule 1.1B PROX 15B 15B 15B 9B1 6.5B 15B 5B 15B LLAMA-2 (Base) 7B - - - LLAMA-2 (CPT) 7B - 7B PROX 15B 10B 5B 10B CODELLAMA (Base) 7B - - - CODELLAMA (CPT) 7B - 7B PROX 15B 10B 5B 10B MISTRAL (Base) 7B - - - MISTRAL (CPT) 7B - 15B 10B 7B PROX 4.7B 10B 2.8 6.2 7.1 4.5 9. 14.1 29.6 30.6 11.8 31.1 35.6 40.6 44.4 51. 3.2 4.8 5.0 2.8 5.6 3.8 13.6 16.8 5.0 14.8 17. 11.4 19.2 22.4 10.9 22.3 23.5 17.5 23.8 39.5 49.2 50. 44.2 51.4 55.8 65.4 65.2 64.9 18.0 36.2 41.2 29.4 41. 51.6 61.9 63.7 50.7 62.1 67.9 68.5 69.6 72. 20.2 47.6 53.8 39.3 56.9 63.6 78.4 79.3 62.6 81.2 82. 87.0 88.4 89.2 - - - - 30.6 14.3 51.6 23.7 48.7 41.0 57.9 49. 40.1 20.9 45.4 33.1 50.0 57.3 50.9 38.5 12.5 14.6 - 19.3 11.6 18.0 15.1 12.4 22.2 15.6 30.9 12. 36.3 31.9 37.3 40.1 30.6 14.3 33.6 30.4 41.3 38.9 52.9 32.3 46.6 43.1 49.8 53.0 19.5 29. 20.4 43.0 45.4 54.7 19.0 53.7 24.8 53.1 16.4 20.7 - 19.4 26. 32.9 40.5 43.8 20.4 40.5 42.6 50.0 50.8 54. 15.6 31.3 21.9 53.1 59.4 68.8 28.1 59.4 37.5 71.9 21. 25.0 - 25.0 31.2 34.4 43.8 53.1 21.9 43.8 62.5 56. 65.6 75.0 - - 29.1 47.3 50.9 (+21.8) 60.1 (+12.8) 36.1 55.9 48.7 (+12.6) 62.1 (+6.2) 14.7 21.5 (+8.1) - 18.4 (+3.7) 25.7 (+11.0) 31.5 42.8 (+11.3) 46.1 (+14.6) 29.1 43.2 (+14.1) 49.4 (+20.3) 51.6 54.8 (+3.2) 59.2 (+7.6) document-level cleaning stage and about 4.7B tokens left after the chunk-level refining stage. We present the final mathematical evaluation results of models trained on the refined OpenWebMath in Table 5, with full evaluation results presented in D.4. PROX boosts math continual pre-training efficiency vastly. Without any domain-specific design, Table 5 shows that pre-training on OpenWebMath refined by PROX brings 11.0% average performance improvements for base TINYLLAMA-1.1B, 14.6% for base LLAMA-2, 20.3% for base CODELLAMA, 7.6% for base MISTRAL, which clearly exceed the improvements of all baselines, including their counterparts pre-trained on the original corpus, under the same settings. It is also worth noticing that, applying the rule-based filtering method does not bring improvements; instead, it leads to 3.1% performance degradation compared to continual pre-training on the original corpus. This finding implies that there are no universal workable heuristics for all domains, highlighting the demands for automated pipelines just like PROX. Moreover, compared with some existing state-of-the-art math continual pre-training models like LLEMMA and INTERNLM2-MATH typically requiring hundreds of billions of tokens continual pre-training, our PROX demonstrates remarkable efficiency gains. more controlled comparison further highlights this efficiency: LLEMMA-7B, based on CODELLAMA-7B, was trained on 200B tokens, whereas our PROX, also starting from CODELLAMA-7B, reaches similar performance levels with just 10B tokens of training, indicating 20 times reduction in training computes. These results suggest that our approach may contribute to more efficient and accessible development of LLMs and could offer new perspective in domain-specific model adaptation, potentially enhancing how to address specialized LLM in resource-constrained settings. 1RHO-1 only counts the selected tokens that are used for training (loss calculation)."
        },
        {
            "title": "4 ANALYSIS",
            "content": "4."
        },
        {
            "title": "IMPACT ON THE ORIGINAL DATA",
            "content": "What changes occur in the corpora after applying PROX? We compare the document length distribution of the original corpus with that of the PROX-refined corpus in Figure 7. In the general domain corpora (RedPajama-V2, C4, and FineWeb), the data refined by PROX exhibits noticeable shift in the average number of tokens per document. For instance, in RedPajama-V2, we observe that documents with fewer than 100 tokens make up significant portion of the corpus. After applying the PROX, the majority of documents contain more than 200 tokens, with an average number of tokens per document increasing from 1217 to over 2000. This suggests that very short documents may be noisy and lack sufficient meaningful information to be suitable for pre-training. This shift, however, is not observed in OpenWebMath, where the average number of tokens per document is already larger. One possible reason for this outlier is that the OpenWebMath corpus is collected mostly from sources different from the general domain, e.g., online forums like Stack Exchange, and academic publisher websites such as arXiv. The noises of these sources can be quite different from general domains. Further case studies on these documents are provided in E.1. RedPajama-V2 C"
        },
        {
            "title": "OpenWebMath",
            "content": "Figure 7: Comparison of docs token length distributions between original and PROX-refined data."
        },
        {
            "title": "4.2 COMPUTING OVERHEAD ANALYSIS",
            "content": "Although PROX demonstrates promising results in downstream tasks, it is important to acknowledge that large-scale model inference still requires substantial computing budget. For example, as mentioned in 3.2, and in Table 7, the RedPajama-V2 corpus used for training TLM-S was refined from about 60B raw tokens. As calculated in E.2, if we utilize PROX-XS for both two refining stages, the additional computational overhead will amount to approximately = 5 1019 FLOPs, which is equivalent to training an additional 12B tokens on TLMS and 5B tokens on TLM-M. It is noteworthy that this overhead ratio keeps decreasing as model size increases, meaning that the relative computational cost diminishes for larger models. In Figure 8, we compare the FLOPs consumed by checkpoints with similar performance, both with and without applying PROX, across three different model sizes. As the model size increases, the proportion of inference FLOPs required for applying PROX decreases. For the 0.7B model, the total FLOPs when using PROX are already lower than without it (6.31e19 vs. 6.7 1e19). Notably, for the largest 1.7B model, we achieve performance comparable to model pretrained on the original data, but with only 58% of the total FLOPs. This demonstrates that refining methods like PROX not only enhances data quality but also becomes more computationally efficient as model sizes grow, reinforcing the value of allocating additional resources to refining pre-training data. Figure 8: FLOPs comparison for comparable downstream performance with/without PROX refining: 0.3B(Avg.Perf = 40.5), 0.7B (41.6), and 1.7B (42.9).2 2The train FLOPs for the base model (approximately 5.3 1019) used to create the refining model are excluded. This is because any pre-trained LLM can theoretically serve as the base for refinement. This also reflects PROXs flexibility."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Pre-training Data Processing Raw data collected from public sources (e.g., CommonCrawl) are noisy, and directly using these data can greatly hurt model performance; thus, it has been common practice to execute extensive pre-processing before pre-training (Touvron et al., 2023b; Together, 2023; Penedo et al., 2024a). The pipeline usually starts with document preparation, which includes URL filtering, text extraction, language-based filtering (Smith et al., 2022). The remaining document will then undergo several quality checks with heuristic rules like overall length, symbol-to-word ratio, and other criteria to determine whether it is kept, partially or fully aborted (Zhang et al., 2024a; Dou et al., 2024; Qiu et al., 2024). Finally, these documents are deduplicated using different matching methods, e.g., fuzzy match like MinHash (Broder, 1997), or exact sequences matches (Penedo et al., 2024b). In PROX, we uses the language model for further data refining, outperforming heuristic rules with acceptable computational overhead. Data Selection Methods Data selection, slightly distinct from data processing, is more commonly applied in the later stages of large-scale data pre-processing. In supervised fine-tuning (SFT), it typically involves selecting much smaller subset of samples to minimize tuning overhead while maintaining performance (Liu et al., 2024b). Recent efforts have extended these selection strategies to the pre-training stage (Engstrom et al., 2024; Xie et al., 2023; Ankner et al., 2024; Sachdeva et al., 2024; Liu et al., 2024c). For instance, Wettig et al. (2024) train rater model to score documents on four quality criteria in SlimPajama (Soboleva et al., 2023) and conduct pre-training on resampled subset based on scores. MATES (Yu et al., 2024) apply smaller model for estimating data influence during pre-training, enabling dynamic data selection schema. Moreover, as mentioned in LLAMA3 (Meta, 2024), LLAMA-2 models (Touvron et al., 2023a) was used as text-quality classifiers that underpin LLAMA-3s training data. Instead of merely selecting documents, PROX enables more fine-grained operations within documents, contributing to further performance improvements. Model-based Data Synthesizing Another branch of research focuses on editing or rephrasing existing data with models to improve the data quality. Fan et al. (2024) use ChatGPT to rephrase several instruction-tuning datasets for clear format based on massive scenario-based criteria. Yue et al. (2024) use LLMs to extract and refine 5M QA pairs from web documents, obtaining 10M instruction-response pairs. Synthesis techniques have also been applied in the pre-training phase such as the PHI series (Gunasekar et al., 2023; Li et al., 2023). Recently, Maini et al. (2024) and Cheng et al. (2024) utilize off-the-shelf instruction-tuned models to paraphrase web documents in specific styles such as QA, and mix these synthetic rephrases with real data in pre-training. Ben Allal et al. (2024) further synthesize from mere seed topics, by prompting LLMs to generate pre-training samples in cleaner format like textbooks. However, despite its success, it typically requires substantial computation to synthesize pre-training-scale corpus, and more critically, it inevitably inherits flaws from the advanced model, also suffering from hallucination issues (Liu et al., 2024a). In this work, we focus on leveraging language models to lift data quality through the synthesis of executable and interpretable programs, rather than directly generating data. We demonstrate that PROX could clearly improve data quality at scale only with acceptable extra computing. Inference Time Scaling Recent trends in language models have begun to explore the potential of allocating additional computing at inference time, complementing the extensive computations already deviated to the pre-training and post-training phases. Several studies have demonstrated the potential of this approach, showing that smaller language models equipped with additional inference-time computing can perform comparably to, or even outperform, significantly larger models, evidenced across various domains, including code generation (Hassid et al., 2024; Brown et al., 2024), and math problem-solving (Snell et al., 2024; Wu et al., 2024). The significance of this approach has been further corroborated by OpenAIs latest o1 model release (OpenAI, 2024). While these studies focus on scaling computing on test time, our work demonstrates an alternative perspective on inference computing scaling. We advocate for allocating computing to refine pre-training corpora, particularly given that Internet-based corpora have been extensively utilized in language model pre-training. Our proposed PROX demonstrates remarkable gains in pre-training efficiency by investing moderately additional compute in the corpus refinement, facilitating more efficient and accessible development of LLMs."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced PROX, framework that uses language models to refine pre-training data at scale through program generation. Our extensive experiments show that PROX curated data improves model performance by over 2% on various downstream benchmarks and is effective across different model sizes and pre-training datasets. For domain-specific continual pre-training, models trained on PROX curated tokens also yield significant improvements in 20 fewer tokens, and comparable to state-of-the-art models trained on 200B tokens. Further analysis also implies applying PROX can achieve similar results with less computing power for large-scale LLM pre-training. In summary, these results demonstrate PROXs potential for greatly improving data quality and reducing costs in language model training."
        },
        {
            "title": "IMPLICATIONS AND FUTURE DIRECTIONS",
            "content": "The strong results from PROX highlight the potential of automated data refinement to significantly improve model performance while reducing computational costs. By refining data more effectively, PROX opens new possibilities for improving training efficiency and achieving better results across range of benchmarks. Looking ahead, these results suggest several future directions. First, incorporating additional refining operations like reformatting and rephrasing could further enhance data quality. Second, improving efficiency by reducing model size and applying inference acceleration techniques is key goal. Expanding PROX to domains like code and multilingual data is also promising. Scaling up with more computational resources will allow for thorough evaluation of its potential. Finally, we believe that prioritizing data refinement before pre-training can greatly improve training efficiency, and we encourage continued exploration in this area."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We extend our profound gratitude to Shanghai AI Lab and Sea AI Lab for generously providing valuable computational resources, which were instrumental in the realization of this project. Our sincere thanks also go to Mingxuan Wang and Jiaze Chen from ByteDance for their crucial support. We are deeply thankful to Ethan Chern from Shanghai Jiao Tong University and Yuqing Yang from University of Southern California for their early discussions and insightful contributions, and equally grateful to Zhoujun Cheng from UC San Diego, Yiheng Xu and Tianbao Xie from University of Hong Kong, and Terry Yue Zhuo from Monash University for their valuable feedback, to Guilherme Penedo and Loubna Ben Allal from Hugging Face for their guidance on hyper-parameter tuning, to Zhibin Gou from Tsinghua University for providing advise on continual pre-training, to Lyumanshan Ye for helping with illustrations and color scheme design. Finally, special thanks go to Peiyuan Zhang from UC San Diego, representing the TinyLlama team, for providing great open pre-training framework and supporting series of acceleration operators. These collective wisdom and unwavering support have been pivotal to our project. This project is supported by SJTU SEIEE - ByteDance Large Language Model Joint Laboratory, Shanghai Artificial Intelligence Laboratory."
        },
        {
            "title": "REFERENCES",
            "content": "Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. ClaudeAI Anthropic. https://www-cdn.anthropic.com/ 3 Model de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. The claude 3 model family: sonnet, haiku. Opus, Card, 2024."
        },
        {
            "title": "URL",
            "content": "Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. Wordcraft: story writing with large language models. In 27th International Conference on Intelligent User Interfaces, pages 841852, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Advances in Neural Information Processing Systems, 35: 1834318362, 2022. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 122, 2023. Together. Redpajama: an open dataset for training large language models, October 2023. URL https://github.com/togethercomputer/RedPajama-Data. Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024a. Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.840. Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, et al. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327, 2024a. Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language models via importance resampling. Advances in Neural Information Processing Systems, 36: 3420134227, 2023. Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. QuRating: Selecting high-quality data for training language models. In International Conference on Machine Learning (ICML), 2024. Zichun Yu, Spandan Das, and Chenyan Xiong. Mates: Model-aware data selection for efficient pretraining with data influence models. arXiv preprint arXiv:2406.06046, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, February 2024. URL https://huggingface.co/datasets/ HuggingFaceTB/cosmopedia. Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang, and Navdeep Jaitly. Rephrasing the web: recipe for compute and data-efficient language modeling. arXiv preprint arXiv:2401.16380, 2024. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models. arXiv preprint arXiv:2404.07503, 2024a. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=jKHmjlpViu. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. arXiv preprint arXiv:2401.10020, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023a. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 23972430. PMLR, 2023. 14 Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. arXiv preprint arXiv:2401.02385, 2024b. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10. 48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. arXiv preprint arXiv:2401.12926, 2024. Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama: Accelerating language model pre-training via structured pruning. In The Twelfth International Conference on Learning Representations, 2024. Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. Instruction pre-training: Language models are supervised multitask learners. arXiv preprint arXiv:2406.14491, 2024. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4WnqRR915j. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Clémentine Fourrier, Nathan Habib, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/huggingface/ lighteval. Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, François Yvon, and Andy Zou. Lessons from the trenches on reproducible evaluation of language models, 2024. InternLM Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023b. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. 15 Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, and Min Lin. Sailor: Open language models for south-east asia. CoRR, abs/2404.03608, 2024. doi: 10.48550/ARXIV.2404. 03608. URL https://doi.org/10.48550/arXiv.2404.03608. Jiantao Qiu, Haijun Lv, Zhenjiang Jin, Rui Wang, Wenchang Ning, Jia Yu, ChaoBin Zhang, Pei Chu, Yuan Qu, Runyu Peng, et al. Wanjuan-cc: safe and high-quality open-sourced english webtext dataset. arXiv preprint arXiv:2402.19282, 2024. Andrei Broder. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No. 97TB100171), pages 2129. IEEE, 1997. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36, 2024b. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview. net/forum?id=BTKAeLqLMw. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew Leavitt, and Mansheej Paul. Perplexed by perplexity: Perplexity-based pruning with small reference models. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models, 2024. Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, Ed Chi, James Caverlee, Julian McAuley, and Derek Zhiyuan Cheng. How to train data-efficient llms. arXiv preprint arXiv:2402.09668, 2024. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. CoRR, abs/2407.01492, 2024c. doi: 10.48550/ARXIV.2407.01492. URL https://doi.org/10. 48550/arXiv.2407.01492. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama, June 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. Run-Ze Fan, Xuefeng Li, Haoyang Zou, Junlong Li, Shwai He, Ethan Chern, Jiewen Hu, and Pengfei Liu. Reformatted alignment. arXiv preprint arXiv:2402.12219, 2024. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. arXiv preprint arXiv:2405.03548, 2024. Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, and Yossi Adi. The larger the better? improved LLM code-generation via budget reallocation. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=QJvfpWSpWm. Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. CoRR, abs/2407.21787, 2024. doi: 10.48550/ARXIV.2407.21787. URL https: //doi.org/10.48550/arXiv.2407.21787. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling model parameters. CoRR, abs/2408.03314, 2024. doi: 10.48550/ ARXIV.2408.03314. URL https://doi.org/10.48550/arXiv.2408.03314. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. An empirical analysis of compute-optimal inference for problem-solving with language models. CoRR, abs/2408.00724, 2024. doi: 10.48550/ARXIV.2408.00724. URL https://doi.org/10.48550/arXiv. 2408.00724. OpenAI. Introducing openai o1-preview, 2024. URL https://openai.com/index/ introducing-openai-o1-preview. Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et al. Fingpt: Large generative models for small language. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 27102726, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372, 2024. Guilherme Penedo, Hynek Kydlíˇcek, Alessandro Cappelli, Mario Sasko, and Thomas Wolf. Datatrove: large scale data processing, 2024c. URL https://github.com/huggingface/ datatrove. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lightning AI. Litgpt. https://github.com/Lightning-AI/litgpt, 2023. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Pritam Damania, Bernard Nguyen, Geeta Chauhan, Yuchen Hao, Ajit Mathews, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded data parallel. Proc. VLDB Endow., 16(12):38483860, aug 2023. ISSN 21508097. doi: 10.14778/3611540.3611569. URL https://doi.org/10.14778/3611540. 3611569. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. Openelm: An efficient language model family with open-source training and inference framework. arXiv preprint arXiv:2404.14619, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology. org/N19-1421. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 17 Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium, OctoberNovember 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https://aclanthology.org/D18-1260. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936, 2019. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 20802094, 2021. Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su. diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 975984, 2020. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: math word problem repository. In Proceedings of the 2016 conference of the north american chapter of the association for computational linguistics: human language technologies, pages 11521157, 2016. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based In Proceedings of the 2019 Conference of the North American Chapter of the formalisms. Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23572367, 2019. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In International Conference on Learning Representations (ICLR), 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 18 20 20 25 25 27 27 27 27 29 29 30 31 31 32 33 37 43 43 43 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A PROX Implementation Details",
            "content": "A.1 Supervised Fine-tuning Data Collection . . . . A.2 Supervised Fine-tuning Details A.3 Evaluation Metrics for PROX Refining Tasks . . A.4 PROX Inference at Scale . . . . . . . . . . . . . . . . . . . Pre-training Details . B.1 Training Infrastructure . B.2 Pre-training Corpora . . . B.3 Model Configuration and Training Parameters . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Downstream Tasks Evaluation",
            "content": "C.1 General Pre-training Evaluation . . C.2 Continual Pre-training Evaluation . . . . . . . . . . ."
        },
        {
            "title": "D Full Evaluation Results",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.1 Detailed Performance on 10 Benchmarks in Sec 3.2 . . D.2 Detailed Performance on 8 Benchmarks Used in Data Selection Experiments . D.3 Detailed Performance in Sec 3.3 . . . . D.4 Evaluation Results of Continual Pre-training in Sec 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Analysis",
            "content": "E.1 Case Studies . . E.2 Computing Overhead Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A PROX IMPLEMENTATION DETAILS",
            "content": "A.1 SUPERVISED FINE-TUNING DATA COLLECTION In this section, we elaborate the detailed prompts used to generated the SFT data for model adaptation. In principle, We apply the same prompts for general domain corpora (including C4 (Raffel et al., 2020), RedPajama-V2 (Together, 2023), FineWeb (Penedo et al., 2024a)) and mathematical corpus (OpenWebMath (Paster et al., 2024)). And all seed data is randomly sampled from the raw corpora. Document-Level Programming We apply two zero-shot scoring prompts to evaluate and assign combined score to each web document before synthesizing the (doc, program) pair. One of the prompts is the same as the one used in FineWeb-Edu, which is prompt to let the model decide the educational score. Additionally in PROX, we add new format scoring prompt, focusing on the format and structure of the document. Both prompts follow the additive style proposed by Yuan et al. (2024). Given these prompts, the language models generate short critiques and assign score between 0 and 5. In FineWeb-Edu, documents are retained only if the educational score (Edu Score) is greater than 2. However, this approach is too aggressive when attempting to preserve larger portion of the tokens. For instance, FineWeb-Edu retains only 1.3 trillion tokens out of the original 15 trillion in the FineWeb corpus. To recall more documents, we relax the filtering criteria by incorporating the format score as follows: Filtering Criteria = Edu Score 3, keep document; Edu Score = 2 and Format Score 4, keep document; Edu Score < 2, drop document. (2) Finally, we use LLAMA-3-70B-INSTRUCT to annotate 51K data, splitting 5K for validation 3. The FineWeb-Edu prompt and our format scoring prompts are presented in Figure 9. Chunk-level Programming We apply chunk-level programming for more fine-grained operations. We find three very popular patterns that keep occurring in all corpus: (1) menu, navigation bars at the top of the document; (2) button, html elements, links; (3) footers. In general, LLMs work well given within 5 few-shot examples. But to generate these program snippets more accurately, we apply few-shot prompting with LLAMA-3-70B-INSTRUCT for each type of noise. We merge these programs aiming to clean different types of noises, perform some grammar checking, and make them the final data for training and validation during the chunk-level refining stage. The annotated source comes from the same seed document used in the previous document filtering stage, accumulating to about 57K data, of which 5K is split as validation. After the release of LLAMA-3.1-405B-INSTRUCT, We also try to use only one prompt aiming to remove all the noises. However, we find such practices lead to aggressive removal of the original document, often making the document less coherent. Finally, we decide to only keep the head part and tail part of the program generated by LLAMA-3.1-405B-INSTRUCT, which is previously mentioned in FinGPT (Luukkonen et al., 2023), and merge with the previous programs generated by LLAMA-3-70B-INSTRUCT. The few-shot prompts used to generate program snippets are presented in Figure 10, Figure 11 and Figure 12. 3In the earlier stage of experiments, we found that dataset of thousands of data points (i.e., 5K) is also sufficient to equip the model with the programming abilities. This generally holds true for both document-level and chunk-level programming tasks. Scaling the dataset size could enhance the models robustness across various documents. 20 Edu Scoring Prompts (Penedo et al., 2024a) Below is an extract from web page. Evaluate whether the page has high educational value and could be useful in an educational setting for teaching from primary school to grade school levels using the additive 5-point scoring system described below. Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the extract provides some basic information relevant to educational topics, even if it includes some irrelevant or non-academic content like advertisements and promotional material. - Add another point if the extract addresses certain elements pertinent to education but does not align closely with educational standards. It might mix educational content with non-educational material, offering superficial overview of potentially useful topics, or presenting information in disorganized manner and incoherent writing style. - Award third point if the extract is appropriate for educational use and introduces key concepts relevant to school curricula. It is coherent though it may not be comprehensive or could include some extraneous information. It may resemble an introductory section of textbook or basic tutorial that is suitable for learning but has notable limitations like treating concepts that are too complex for grade school students. - Grant fourth point if the extract highly relevant and beneficial for educational purposes for level not higher than grade school, exhibiting clear and consistent writing style. It could be similar to chapter from textbook or tutorial, offering substantial educational content, including exercises and solutions, with minimal irrelevant information, and the concepts arent too advanced for grade school students. The content is coherent, focused, and valuable for structured learning. - Bestow fifth point if the extract is outstanding in its educational value, perfectly suited for teaching either at primary school or grade school. It follows detailed reasoning, the writing style is easy to follow and offers profound and thorough insights into the subject matter, devoid of any non-educational or complex content. The extract: <EXAMPLE>. After examining the extract: - Briefly justify your total score, up to 100 words. - Conclude with the score using the format: Educational score: <total points>"
        },
        {
            "title": "Format Scoring Prompts",
            "content": "Evaluate the provided web content extraction sample. Points are accumulated based on the satisfaction of each criterion: 0. Start with 0 points. 1. Add 1 point if the extract contains some readable content, even if it includes significant amount of HTML tags, navigation elements, or other web page artifacts. The main content should be identifiable, albeit mixed with noise. 2. Add another point if the extract shows signs of basic cleaning. Most obvious HTML tags have been removed, though some may remain. The text structure begins to emerge, but non-content elements (e.g., footer links, button text) may still be present. The writing style may be disjointed due to remnants of page structure. 3. Award third point if the extract is largely cleaned of HTML and most non-content elements. The main body of the content is intact and coherent. Some extraneous information (e.g., isolated URLs, timestamps, image alt text) may persist, but doesnt significantly impede readability. The extract resembles rough draft of the original content. 4. Grant fourth point if the extract is highly refined, with clear paragraph structure and formatting. Almost all HTML tags and non-content elements have been eliminated. Minimal noise remains. The content flows well and reads like near-final draft, with consistent formatting and style. 5. Bestow fifth point if the extraction is flawless. The content is entirely clean, preserving the original structure (paragraphs, headings, lists) without any HTML tags or web page elements. No extraneous information is present. The extract reads as if it were professionally edited document, perfectly capturing the original content. The extract: <EXAMPLE>. After examining the extract: - Briefly justify your total score, up to 100 words. - Conclude with the score using the format: \"Extraction Quality Score: <total points>\" Figure 9: Edu scoring prompts used in FineWeb (Penedo et al., 2024a) and newly proposed format scoring prompts for PROX. Comparison with FineWeb-Edus Approach Compared with the recently released FineWeb-Edu, which also uses model-based scoring by applying BERT model to evaluate documents, we find that our relaxed design retains more tokens without compromising overall data quality. Specifically, FineWeb-Edu retains about 1.3 trillion tokens out of 15 trillion token corpus (less than 9%), while PROX curation typically keeps 23% to 28%, providing up to 3 more unique tokens for training. Moreover, we conducted preliminary study by training 0.7 billion parameter models on these data. We found that models trained on our curated data achieved similar downstream performance, as shown in Table 6. Therefore, we believe our current strategy is more suitable for large scale pre-training, as it is capable of retaining more tokens while maintaining very high data quality. Table 6: Comparing FineWeb-Edu with our strategy on TLM-S. Methods Kept Ratio ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG #Win FineWeb-Edu FineWeb-PROX 8.6% 28.0% 30.3 27. 58.7 55.7 29.0 30.4 42.0 44.2 30.4 29.5 31.8 31.0 67.7 68. 38.1 39.3 50.4 52.2 73.3 72.8 45.2 45.2 5/10 5/"
        },
        {
            "title": "Navigation Removal Prompts",
            "content": "Youre tasked with generating Python programs to clean web text strings by removing navigation bars. The web text will be presented with line numbers starting from [000]. Your task is to use the following pre-defined functions to clean the text: python def untouch_doc(): \"\"\"leave the clean doc untouched, for tagging clean and high quality doc.\"\"\" def remove_lines(start: int, end: int): \"\"\"remove noisy lines from start until end, including end.\"\"\" Your goal is to identify navigation bars or menu items at the beginning of the text and remove them using the remove_lines() function. If the text doesnt contain navigation bar or menu items, use the untouch_doc() function to indicate that no cleaning is necessary. If the line contains other text other than navigation, also call untouch_doc to escape overkilling. Here are some examples to guide you: Example 1: [doc] [000] Home Products About Us Contact [001] Welcome to our website [002] Heres our main content... [/doc] Program: python remove_lines(start=0, end=0) Example 2: [doc] 341 US 479 Hoffman v. United States 341 US 479 Hoffman v. United States 341 U.S. 479 95 L.Ed. 1118 HOFFMANv.UNITED STATES. Mr. William A. Gray, Philadelphia, Pa., for petitioner. Mr. John F. Davis, Washington, D.C., for respondent. ...... [/doc] Program: python untouch_doc() Example 3: [doc] [000]Police Search Tunbridge Wells House Over Human Remains Tip Off [001]Posted: 16/04/2012 10:44 Updated: 16/04/2012 10:44 reddit stumble [002]Crime, Body Buried In House, Buried Body, Buried Remains, Tip-Off, Uk News, Uk Police, [003]Detectives are searching the gardens of house following information that human remains may be buried there. [/doc] Program: python untouch_doc() Example 4: [doc] [000]Home > Bollywood News > Bollywood Stars clash on Indian TV Bollywood Stars clash on Indian TV [001]By Lekha Madhavan09:47 pm Betting big on the festive season, general entertainment channels (GECs) are launching celebrity-driven shows, but media buyers are concerned about the audience split that is set to happen. [002]The fourth season of Bigg Boss on Colors is almost certain to clash with the fourth season of Kaun Banega Crorepati (KBC) on Sony Entertainment Television (SET) in the second week of October. [003]Another big property, Master Chef, to be hosted by Akshay Kumar, on STAR Plus, is also expected to go on air in October. However, the channel is yet to disclose the launch date. [004]Big-budget shows like these are often loss-making propositions for channels, as the operating cost is very high and advertisement revenues do not suffice to cover the cost. [005]Source: IBNS [/doc] Program: python untouch_doc() For each given web text, analyze the content and determine if theres navigation bar or menu items at the beginning. If present, use remove_lines() or normalize() to remove them. If not, use untouch_doc() to indicate that no cleaning is needed. Example: <EXAMPLE>. After examining the web text: - Briefly describe if the web extract contains navigation bar at the begining (10 lines). - You must not mistakenly decide that title of the page is navigation bar and remove it. - When the whole line is navigation bar, call remove_lines; if the line contains other information, call normalize to remove part of it. - Give your program using the same format: python[your code] Figure 10: Few-shot navigation bar removal prompts."
        },
        {
            "title": "URL Removal Prompts",
            "content": "Youre tasked with generating Python programs to clean web text strings by removing http lines. The web text will be presented with line numbers starting from [000]. Your task is to use the following pre-defined functions to clean the text: python def untouch_doc(): \"\"\"leave the clean doc untouched, for tagging clean and high quality doc.\"\"\" def remove_lines(start: int, end: int): \"\"\"remove noisy lines from start until end, including end.\"\"\" def normalize(source_str: str, target_str: str=\"\"): \"\"\"turn noisy strings into normalized strings.\"\"\" Your goal is to identify http links from the text and remove them using the remove_lines() or normalize() function. If the text doesnt contain http lines, use the untouch_doc() function to indicate that no cleaning is necessary. Here are some examples to guide you: Example 1: [doc] [013] http://groups.google.com/group/toowoombalinuxLast [014] Breaking News: Major Event Unfolds [015] http://code.google.com/p/inxi/ [/doc] Program: python # the whole line-[013] is http, so remove the line-[013] remove_lines(start=13, end=13) # the whole line-[015] is http, so remove the line-[015] remove_lines(start=15, end=15) Example 2: [doc] [000] The Impact of Climate Change on Global Ecosystems [001] By Dr. Jane Smith [002] Climate change continues to be pressing issue... [/doc] Program: python untouch_doc() Example 3: [doc] [021]Bow-wow [022]http://groups.google.com/group/toowoombalinuxLast edited by Puppyt on Mon 06 Jun 2011, 00:23; edited 1 time in total [023]I would like to see something like Jitsi [024]http://www.jitsi.org/. Plus some others incorporated into puppy distro. [/doc] Program: python # the http link in line 22 and line 24 comes with other text, so use normalize to ONLY remove the link without touching text. normalize(source_str=\"http://groups.google.com/group/toowoombalinuxLast\", target_str=\"\") normalize(source_str=\"http://www.jitsi.org/.\", target_str=\"\") For each given web text, analyze the content and determine if theres navigation bar or menu items at the beginning. If present, use remove_lines() or normalize() to remove them. If not, use untouch_doc() to indicate that no cleaning is needed. Example: <EXAMPLE>. After examining the web text: - do not remove text together with http. - Briefly describe if the web extract contains http links; and make sure remove them will not influence the main content. - Program only contain sequences of function callings and comments, no other codes. - note line number starts with 0. make accurate annotations about line number. put the exact int line number of the given line. do not add 1 or minus 1. - Give your program using the same format: python[your code] Figure 11: Few-shot URL removal prompts."
        },
        {
            "title": "Footer Removal Prompts",
            "content": "Youre tasked with generating Python programs to clean web text strings by removing footer sections, references. The web text will be presented with line numbers starting from [000]. Your task is to use the following pre-defined functions to clean the text: python def untouch_doc(): \"\"\"leave the clean doc untouched, for tagging clean and high quality doc.\"\"\" def remove_lines(start: int, end: int): \"\"\"remove noisy lines from start until end, including end.\"\"\" def normalize(source_str: str, target_str: str=\"\"): \"\"\"turn noisy strings into normalized strings.\"\"\" Your goal is to identify footer sections from the text and remove them using the remove_lines() function. Footers and references typically appear at the end of the text and may contain information such as copyright notices, contact details, or navigation links. If the text doesnt contain footer section or any references, use the untouch_doc() function to indicate that no cleaning is necessary. Here are some examples to guide you: Example 1: [doc] [013] In conclusion, the study demonstrates significant findings. [014] 2023 Research Institute. All rights reserved. [015] Contact: info@research-institute.com [016] Follow us on social media: @ResearchInst [/doc] Program: python # Remove the footer section starting from line 14 remove_lines(start=14, end=16) Example 2: [doc] [000] The Impact of Climate Change on Global Ecosystems [001] By Dr. Jane Smith [002] Climate change continues to be pressing issue... [003] Further research is needed to fully understand its implications. [/doc] Program: python untouch_doc() Example 3: [doc] [020] Thank you for reading our newsletter. [021] Stay informed with our latest updates! [022] --- [023] Unsubscribe Privacy Policy Terms of Service [024] NewsletterCo, 123 Main St, Anytown, USA [/doc] Program: python # Remove the footer section starting from the divider remove_lines(start=22, end=24) For each given web text, analyze the content and determine if there is footer section or reference. If present, use remove_lines() to remove it. If not, use untouch_doc() to indicate that no cleaning is needed. Example: <EXAMPLE>. After examining the web text: - Briefly describe if the web extract contains footer section or references; ensure that removing it will not influence the main content. If not, simply call untouch_doc. - The program should only contain sequences of function calls and comments, no other code. - Note that line numbers start with 0. Make accurate annotations about line numbers. Put the exact int line number of the given line. Do not add 1 or subtract 1. - Give your program using the same format: python[your code] Figure 12: Few-shot footer removal prompts. 24 A.2 SUPERVISED FINE-TUNING DETAILS Training Parameters We use llama-factory (Zheng et al., 2024) as our main code base for Adaptation Stage. We apply full paraemter supervised fine-tuning on our base models: we train on the whole seed dataset for 3 to 5 epochs, with batch size as 64, and cosine learning rate schedular (lr from 1e-5 1e-6). Also, we find that base model convergent quite fast on these tasks, thus we do not apply further tuning over hyper-parameters, and keep the same training configurations for all the adaptation tasks. A.3 EVALUATION METRICS FOR PROX REFINING TASKS Document-level refining Task The document filtering task is indeed equal to binary classification problem, where documents are classified as either to be kept (1) or dropped (0). We evaluate the performance using the F1 score, calculated as follows: where: F1 = 2 Precision Recall Precision + Recall Precision = TP TP + FP , Recall = TP TP + FN (3) (4) The F1 score ranges from 0 to 1 and we assume higher F1 score indicates better classification performance. Chunk-level Refining Task This task actually contains two parts: line removal and string normalization. However, we find it is rather hard to evaluate the normalization task, so we use the line removal accuracy to reflect the refining performance. We propose line-wise F1 score metric: The F1 score is computed by comparing the predicted noisy lines with the labeled noisy lines. First, we extract the noisy line indexes from both the prediction and the label. Then, we calculate the overlap between these two sets. The true positives (TP) are the number of lines in this overlap. False positives (FP) are the predicted noisy lines that are not in the labeled set, and false negatives (FN) are the labeled noisy lines that are not in the predicted set. The calculation is actually simple: TP (True Positives) = Predicted Noisy Lines Actual Noisy Lines FP (False Positives) = Predicted Noisy Lines Actual Noisy Lines FN (False Negatives) = Actual Noisy Lines Predicted Noisy Lines (5) (6) (7) Then we use same calculation of F1 score mentioned before, i.e., F1 = 2TP 2TP+FP+FN . 25 A.4 PROX INFERENCE AT SCALE Thanks to the Datatrove project (Penedo et al., 2024c), we are able to efficiently split, and load the whole corpus to each worker (which normally equals to the number of the GPUs since small models do not require tensor parallelism). We use the vllm (Kwon et al., 2023) to perform large scale inference. For chunk-wise programming, we will split the original document into several chunks, controlling the tokens of each chunk less than the context window. In practice, we normally replace token count process as word count process for saving time, and control the window size as 1, 500. The general algorithm is implemented as below: Algorithm 1 Document Chunk Splitting Algorithm else + if = then {c} if TokenCount(c + l) then end if if TokenCount(l) then Require: Document D, context window size Ensure: Set of chunks 1: , 2: for each line in do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: if = then 18: 19: end if 20: return {FlagAsSkipped(l)} {c} end if end if else Add line to current chunk Save current chunk Start new chunk Flag long line Add the final chunk 26 PRE-TRAINING DETAILS B.1 TRAINING INFRASTRUCTURE Code Base Thanks to litgpt (AI, 2023), and TinyLlaMA (Zhang et al., 2024b), we are able to flexibly train all our base models. We inherit several fused kernels from the TinyLlaMA, which is installed from the FlashAttention (Dao, 2024) including fused rotary positional embedding (RoPE), layer normalization, and cross entropy loss to help saving memory. We mainly apply FSDP strategy (Zhao et al., 2023) to enable training larger scale models on multiple nodes. B.2 PRE-TRAINING CORPORA Due to computing constraints and fair comparison purpose, we cannot exhaustively train over the whole corpora. Thus, we apply random sampling for some of the pre-training corpora and make them as our pre-training data pools. For RedPajama-V2, We randomly download 70 file shards, obtaining total data pool consisting about 500B tokens, we evenly separate it into 8 dumps, with each containing about 62.5B tokens; due to computing constraints, we use only 1 dump for verifying effectiveness (Section 3.2) and use 2 dumps for scaling the training to 50B tokens (Section 3.3); For C4, we download the whole dataset, which contains about 198B tokens; For FineWeb, we download the official 350B sample4; For OpenWebMath, we download the whole dataset. We report the corpora details applied in each experiment in Table 7. Table 7: The detailed breakdown for pre-training corpora in all experiments. Section Experiments Source Data Description Corpora Size (B) Effective Train Tokens (B) Epoch Section 3.2 Table 2, Figure 4 RedPajama-V2 Section 3.2 Table 3 Section 3.3 Figure 5 RedPajama-V2 C4 Section 3.3 Figure RedPajama-V2 FineWeb Section 3.4 Table 5, 1.1B model OpenWebMath Section 3.4 Table 5, 7B model OpenWebMath raw data size after rule-based filtering after PROX-D after PROX-D+C random after PROX-D other baselines raw data size after PROX-D+C (using PROX-xs) after PROX-D+C (using PROX-s) after PROX-D+C (using PROX-m) raw data size after PROX-D+C (using PROX-xs) raw data size after PROX-D+C (using PROX-xs) raw data size after PROX-D+C (using PROX-xs) raw data size after rule-based filtering after PROX-D after PROX-D+C raw data size after PROX-D after PROX-D+C 62.5 31.5 19.0 16.0 - 41.5 (GPT-NeoX) - 62.5 14.5 16.0 18.0 198.0 44.5 123.5 29 79.0 18.0 15.0 6.5 5.5 4. 15.0 5.5 4.7 26.2 26.2 26.2 52.4 15. 10.5 0.42 0.83 1.38 1.64 - 0.63 - 0.42 1.80 1.64 1.46 0.53 1.18 0.42 1. 0.66 2.91 1.05 2.40 2.85 3.49 0.70 1.91 2.23 B.3 MODEL CONFIGURATION AND TRAINING PARAMETERS 4https://huggingface.co/datasets/HuggingFaceFW/fineweb/tree/main/ sample/350BT 27 Table 8: The details of the pre-training experiments model architecture. Model Hidden Size Intermediate Size Context Len Heads Layers Vocab Size # Params (w/o embed) TLM-XS TLM-S TLM-M PYTHIA-410M PYTHIA-1B TINYLLAMA-1.1B LLAMA-2-7B CODELLAMA-7B MISTRAL-7B 1,280 1,536 2,048 1,024 2,048 2,048 4,096 4,096 4,096 2,048 4,864 8,192 4,096 8,192 5,632 11,008 11,008 14, Training From Scratch 2,048 2,048 2,048 1,024 1,024 16 24 32 16 8 Continual Pre-training 2,048 4,096 4,096 4,096 32 32 32 32/8 (GQA) 24 24 24 24 16 22 32 32 32 32,000 32,000 32, 50,304 50,304 32,000 32,000 32,016 32,000 354,284,800 (313,324,800) 758,982,144 (709,830,144) 1,741,785,088 (1,676,249,088) 405,334,016 (353,822,720) 1,011,781,632 (908,759,040) 1,100,048,384 (1,034,512,384) 6,738,415,616 (6,607,343,616) 6,738,546,688 (6,607,409,152) 7,241,732,096 (7,110,660,096) Table 9: Training hyper-parameters of all base models. Model Context Length Batch Size Max Steps Warmup Steps Weight Decay Optimizer LR Scheular LR TLM-XS TLM-S TLM-M PYTHIA-410M PYTHIA-1B TINYLLAMA-1.1B LLAMA-2-7B CODELLAMA-7B MISTRAL-7B 1,024 1,024 1,024 512 512 2,048 4096 4096 4,096 2,048 2,048 2,048 1,024 1,024 1,024 256 1024 Training from Scratch 12,500 12,500 12,500/2,5000 50,200 50,200 500 500 500 2,000 2,000 Continual Pre-training 7,500 15,000 (early stop at 10,000) 3,750 (early stop at 2,500) 15,000 (early stop at 10,000) 0 0 0 0 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW cosine cosine cosine WSD WSD cosine cosine cosine cosine 5e-4 5e-5 5e-4 5e-6 3e-4 3e1e-3 6.25e-5 1e-3 6.25e-5 8e-5 8e-6 8e-5 8e-6 3e-4 3e-5 2e-5 2e-6 Model Architecture The models we used in general and continual pre-training are presented at Table 8 with detailed architecture configuration. Training Hyperparameter Choice We primarily use cosine learning rate scheduler and follow established settings used in Zhang et al. (2024b) and Lin et al. (2024). The default configurations for each experiment can be found below and we elaborate full details in Table 9. 1. For general pre-training experiments, we set the learning rate to 5e-4 for TLM-XS and TLM-S, 3e-4 for TLM-M; the maximum sequence lengths are uniformly set to 2048, and the global batch size is set to 2M tokens. 2. Additionally, we align all our hyper-parameters with those used in MATES (Yu et al., 2024) to facilitate direct comparison with their existing data selection methods, as previously shown in Table 3. In this case, we switch to the warmup-stable-decay (WSD) learning rate scheduler (Hu et al., 2024), as implemented in MATES. For fair comparison with baselines implemented in MATES, we apply the exact same WSD Schedular (Hu et al., 2024): lr(t) = η, η, 0.54(tS)/D η, if < if < if < + (8) where equals to 2000, equals to 50000, equals to 200. 3. For continual pre-training experiments, we set different hyperparameters for different base models, as shown in Table 9. We apply an early-stop mechanism mentioned in INTERNLM2-MATH (Ying et al., 2024) for 7B model experiments. We mainly refer these settings to the setup reported in Rho-1 (Lin et al., 2024) and LLEMMA (Azerbayev et al., 2024). We do not use warmup in continual pre-training experiments."
        },
        {
            "title": "C DOWNSTREAM TASKS EVALUATION",
            "content": "C.1 GENERAL PRE-TRAINING EVALUATION Lighteval Configurations We mainly borrow the evaluation benchmarks from the FineWebs nine selected early signal tasks (Penedo et al., 2024a), and use the implementation of lighteval (Fourrier et al., 2023) to test all our base models. We also introduce SciQ (Welbl et al., 2017) which is widely used in previous works and proved good testbed (Mehta et al., 2024; Wettig et al., 2024). By default, we report the normalized zero-shot accuracy. All the nine benchmarks at listed below: ARC (Clark et al., 2018): including ARC-Easy (ARC-E) and ARC-Challenge (ARC-C) CommonSense QA (Talmor et al., 2019) (CSQA) HellaSwag (Zellers et al., 2019) MMLU (Hendrycks et al., 2021) OpenBook QA (Mihaylov et al., 2018) (OBQA) PIQA (Bisk et al., 2020) SocialIQA (Sap et al., 2019) (SIQA) WinoGrande (Sakaguchi et al., 2021) (WinoG) SciQ (Welbl et al., 2017) We follow the lightevals configuration, which randomly picks 1, 000 samples for each dataset (for MMLU, it selects 1, 000 samples for each of the 57 subsets), and report the normalized accuracy. These average performance is calculated over the nine benchmarks, where ARC-C and ARC-E are considered as two separate benchmarks, and MMLU is treated as single benchmark. This approach differs slightly from the aggregation score calculation in FineWeb, as we believe MMLUs performance is relatively unstable, and we aim to give equal weight to all benchmarks, preventing MMLU from becoming dominant factor. For the original lighteval scores, please refer to the D.1, where we include dynamic result curve that clearly illustrates the fluctuations in each benchmark. We present zero shot evaluation results in Table 2, Figure 4. LM-Eval Harness Configurations We also include the lm-evel-harness (Biderman et al., 2024) for zero-shot and few-shot performance, for fair comparison with different data selection methods including DSIR (Xie et al., 2023),DsDm (Engstrom et al., 2024), Qurating (Wettig et al., 2024) MATES (Yu et al., 2024). Similar to lighteval configuration, we include: ARC: including ARC-E and ARC-C HellaSwag LogiQA (Liu et al., 2020) OpenBook QA (OBQA) PIQA WinoGrande (WinoG) SciQ We exclude the BoolQ (Clark et al., 2019) tasks from MATES (Yu et al., 2024), leaving eight tasks in total. This decision was made because we observed that the BoolQ benchmark performance exhibited severe fluctuations and showed notable declining trend in the early stages. Therefore, we decided to exclude it from our evaluation set. Such trend is also observed earlier in the OpenELM work (Mehta et al., 2024). We report both zero-shot and two-shot performance. If the metrics include normalized accuracy, we use that measure; otherwise, we use accuracy. 29 C.2 CONTINUAL PRE-TRAINING EVALUATION We evaluate all benchmarks implemented in the math-eval-harness repository,5 including: Math (MATH) (Hendrycks et al., 2021) GSM8K (Cobbe et al., 2021) SVAMP (Patel et al., 2021) ASDiv (Miao et al., 2020) MAWPS (Koncel-Kedziorski et al., 2016) MathQA (MQA) (Amini et al., 2019) TableMWP (TAB) (Lu et al., 2023) SAT MATH (Azerbayev et al., 2024) We use few-shot CoT prompting (Wei et al., 2022) when evaluating these tasks, and report the accuracy of each task. 5https://github.com/ZubinGou/math-evaluation-harness"
        },
        {
            "title": "D FULL EVALUATION RESULTS",
            "content": "D.1 DETAILED PERFORMANCE ON 10 BENCHMARKS IN SEC 3.2 We report full evaluation results of checkpoints saved at different training steps in Section 3.2. We present the results for 0.7B models trained on data curated by different methods in Table 10, including models trained on raw data, rule-based filtered data, and data curated by PROX. Table 10: Full evaluation results on TLM-S. Train Steps 2500 5000 7500 10000 12500 2500 5000 7500 10000 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 22.1 24.4 26.5 25.8 26.1 22.3 25.1 26.5 26.2 25.7 22.6 22.9 24.2 24.6 25.0 23.2 24.2 24.4 23.6 25.2 23.6 23.9 25.6 25.8 25.0 25.6 25.4 26.9 26.7 26. 24.9 24.9 25.5 26.2 26.4 39.0 41.2 43.9 43.5 44.3 39.4 41.4 43.0 44.2 44.0 40.6 41.6 44.2 44.8 46.0 39.4 42.3 44.1 46.6 46.8 39.3 40.9 42.2 43.3 43. 43.2 46.2 49.2 48.2 49.7 43.4 49.6 51.2 51.7 51.9 27.6 28.8 29.5 29.1 29.7 26.6 29.8 30.5 31.8 31.3 28.8 29.3 29.5 30.4 31.0 27.2 29.8 30.4 32.0 32. 27.6 29.0 30.7 30.8 30.0 27.7 28.4 29.1 30.5 30.1 27.3 28.8 30.8 30.8 30.9 Raw Data 25.9 26.7 27.2 27.4 27.3 Gopher 25.6 26.4 27.2 27.5 27.3 C4 26.2 26.8 27.2 27.0 27.1 FineWeb 25.6 27.0 27.2 27.0 27.2 26.6 27.0 29.0 29.8 29. 27.0 27.2 28.8 29.4 29.0 27.4 27.6 28.4 29.4 29.2 26.2 28.4 28.2 27.8 29.0 31.6 34.8 37.2 38.8 39.1 31.3 34.3 38.5 39.2 40.2 31.3 36.0 39.2 39.5 40. 31.8 36.2 37.8 39.6 39.6 Gopher + C4 + FineWeb 32.1 36.2 39.7 41.4 41.9 32.9 35.7 39.2 39.9 40.5 32.1 36.8 38.8 39.9 42.4 25.8 26.9 27.0 27.5 27. PROX-D 27.2 28.1 28.6 28.6 29.4 PROX-D+C 26.9 27.9 28.4 29.0 29.4 26.0 26.8 28.4 29.8 31.0 27.0 28.8 30.8 28.6 30. 28.2 30.6 31.2 32.6 31.6 61.2 64.9 64.8 66.9 66.9 61.1 64.5 65.7 66.6 66.3 61.7 64.7 66.2 68.7 68.5 62.6 64.3 66.1 66.3 66.5 61.7 65.3 66.0 66.9 67. 61.3 64.7 65.4 66.2 66.3 60.9 64.7 67.3 68.6 67.9 37.3 39.3 38.7 39.0 39.0 38.9 39.6 38.2 38.9 39.0 39.3 40.2 40.9 40.9 40.5 39.0 38.9 39.5 39.2 39. 39.8 39.3 40.2 39.5 39.9 39.4 39.3 38.8 39.7 39.0 38.8 38.8 40.2 39.7 40.0 48.9 50.4 50.8 51.2 52.0 51.3 52.1 53.7 51.3 51.2 51.2 50.9 51.6 51.7 51. 51.4 51.4 50.8 53.1 52.4 50.9 52.7 51.8 51.8 51.9 50.6 53.3 51.2 51.9 51.2 51.2 51.1 50.3 51.7 52.2 59.1 61.9 68.2 66.2 67.4 58.6 62.9 66.4 68.2 68. 57.1 63.6 63.8 63.9 66.6 57.1 61.4 66.2 70.5 69.2 55.4 62.4 60.9 63.1 65.3 63.0 64.2 71.7 71.2 71.6 60.8 66.9 71.7 73.7 73.5 37.9 39.9 41.6 41.8 42. 38.2 40.3 41.8 42.3 42.3 38.6 40.4 41.5 42.1 42.6 38.3 40.4 41.5 42.6 42.8 38.2 40.3 41.2 42.0 42.3 39.8 41.4 43.1 43.2 43.5 39.5 42.0 43.5 44.4 44. 31 D.2 DETAILED PERFORMANCE ON 8 BENCHMARKS USED IN DATA SELECTION"
        },
        {
            "title": "EXPERIMENTS",
            "content": "The full benchmark performance used in data-selection method comparison experiments is presented in Table 11. Table 11: Detailed evaluation results for different data selection methods. Method ARC-C ARC-E HellaSwag LogiQA OBQA PIQA WinoGrande SciQ AVG Random DSIR DsDm QuRating MATES PROX Random DSIR DsDm QuRating MATES PROX Random MATES PROX Random MATES PROX 25.6 23.8 24.7 25.4 25.0 27.2 25.3 23.6 23.6 23.6 25.3 27.0 25.6 25.9 26. 25.5 26.8 27.3 40.2 39.9 41.7 42.0 41.8 48.9 42.6 42.0 44.2 43.9 43.8 52.7 43.7 44.9 49.1 45.1 46.1 54.5 PY A-410M 0-shot 24.7 27.0 27.5 25.3 25.7 26.9 29.4 28.4 29 30.2 30.8 31.8 PY A-410M 2-shot 24.1 26.1 23.5 26.1 24.9 23.7 28.6 28.6 29.2 30.2 30.6 32.8 PY A-1B 0-shot 27.5 28.7 24.8 31.8 32.2 32.2 PY A-1B 2-shot 24.6 25.2 26.6 30.0 30.6 32.2 39.7 39.6 40.3 40.7 41.0 43. 39.9 39.8 40.1 40.4 40.6 42.6 43.8 45.3 46.6 42.9 44.8 46.2 67.1 66.8 68.1 67.5 68.7 68.4 66.9 66.1 66.5 67.4 67.1 68.2 68.9 69.5 70. 68.3 68.7 69.0 50.6 51.5 50.1 52.1 52.7 54.1 52.2 51.6 51.5 51.4 53.4 53.9 50.7 52.4 54.2 52.1 51.6 53.9 64.1 63.1 65.4 64.8 66.0 69. 70.6 71.4 74 74.1 74.1 78.9 65.8 67.3 70.9 74.6 75.7 77.4 42.7 42.5 43.4 43.5 44.0 46.2 43.8 43.7 44.1 44.6 45.0 47.5 44.7 45.8 46. 45.4 46.2 48.4 Figure 13: Visualization of dynamic performance on ten benchmarks. 32 D.3 DETAILED PERFORMANCE IN SEC 3.3 In 3.3, we test PROXs effectiveness using different sizes of refining models, and also train series of models by using these curated data. We report these detailed results in Table 12, Table 13 and Table 14. Table 12: Full evaluation results of TLM-XS trained on different PROX model curated data. Train Steps 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 22.5 23.6 23.8 23.8 22.6 24.8 23.7 24.1 25.3 25.9 23.5 24.7 25.3 25.6 26.4 22.9 25.8 26.0 26.0 26.5 38.5 39.2 42.7 41.2 41.9 43.5 44.3 46.0 46.1 47. 41.9 44.5 45.3 45.7 46.7 41.3 44.0 45.3 46.6 46.4 TLM-XS trained on Raw data 29.1 33.1 33.4 35.0 32.8 25.8 26.1 26.0 26.6 26.2 25.0 26.6 26.2 28.0 26. TLM-XS trained on PROX-xs data 30.3 33.8 35.0 35.7 36.7 26.8 27.3 27.7 28.1 28.1 26.6 28.8 30.6 29.2 30.2 TLM-XS trained on PROX-s data 30.4 33.8 34.0 35.6 37. 26.6 27.5 27.9 28.6 28.1 27.6 28.0 29.2 30.2 29.8 60.2 62.2 64.0 65.3 62.2 59.3 61.3 63.4 64.4 64.6 62.0 62.4 63.4 63.6 62.8 TLM-XS trained on PROX-m curated data 31.1 34.0 36.6 37.3 37.6 26.9 27.1 27.7 27.6 28.1 27.0 29.6 29.8 30.6 29.4 62.2 63.1 63.6 63.3 64.1 27.0 28.7 28.0 27.8 29.7 26.5 28.1 29.2 28.3 29. 24.9 27.0 27.3 27.6 27.5 26.5 27.3 28.5 28.8 29.1 38.8 39.5 39.3 40.9 39.3 38.6 38.9 38.7 38.5 38.0 37.8 38.0 37.7 37.4 37.8 37.6 38.5 39.4 38.7 38. 50.4 49.9 51.5 50.1 51.3 50.8 50.9 52.0 51.2 51.7 49.3 50.6 52.9 52.0 52.2 50.6 51.8 51.3 51.6 51.5 58.6 66.2 67.0 65.9 63.3 60.7 70.2 70.4 70.6 71. 61.4 67.0 68.7 71.1 70.1 62.4 64.9 68.5 70.3 68.0 37.6 39.5 40.2 40.5 39.6 38.8 40.7 41.7 41.7 42.3 38.5 40.3 41.2 41.7 41.9 38.9 40.6 41.7 42.1 41. Table 13: Full evaluation results of TLM-S trained on different PROX model curated data. Train Steps 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 2500 5000 7500 10000 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 22.1 24.4 26.5 25.8 26.1 23.8 26.8 26.9 26.7 26.8 24.9 24.9 25.5 26.2 26.4 25.3 26.1 27.1 26.7 27.4 39.0 41.2 43.9 43.5 44. 44.1 48.1 49.0 51.3 52.1 43.4 49.6 51.2 51.7 51.9 45.3 45.4 47.5 50.5 50.7 27.6 28.8 29.5 29.1 29.7 26.5 28.4 30.6 29.4 30.2 27.3 28.8 30.8 30.8 30. 27.5 28.6 30.6 30.7 30.6 TLM-S trained on Raw data 31.6 34.8 37.2 38.8 39.1 25.9 26.7 27.2 27.4 27.3 26.6 27.0 29.0 29.8 29.2 61.2 64.9 64.8 66.9 66. TLM-S trained on PROX-xs curated data 33.5 36.7 39.5 40.1 41.8 26.9 28.0 28.2 28.3 28.5 29.4 30.6 29.6 31.8 31.6 60.7 64.0 65.3 64.1 65.5 TLM-S trained on PROX-s curated data 32.1 36.8 38.8 39.9 42.4 26.9 27.9 28.4 29.0 29.4 28.2 30.6 31.2 32.6 31.6 60.9 64.7 67.3 68.6 67.9 TLM-S trained on PROX-m curated data 27.0 27.8 29.2 30.2 30. 62.4 65.7 66.8 67.0 67.4 32.2 37.2 41.0 41.5 42.0 26.7 27.4 28.6 28.4 28.8 33 37.3 39.3 38.7 39.0 39.0 38.9 38.6 39.6 39.3 39. 38.8 38.8 40.2 39.7 40.0 38.7 38.9 39.3 40.1 39.4 48.9 50.4 50.8 51.2 52.0 50.6 50.3 52.2 51.4 51.9 51.2 51.1 50.3 51.7 52.2 50.6 50.9 51.1 49.9 48. 59.1 61.9 68.2 66.2 67.4 62.1 65.6 69.6 69.9 70.8 60.8 66.9 71.7 73.7 73.5 60.8 65.6 69.9 70.9 70.1 37.9 39.9 41.6 41.8 42.1 39.6 41.7 43.0 43.2 43. 39.5 42.0 43.5 44.4 44.6 39.6 41.4 43.1 43.6 43.5 Table 14: Full evaluation results of TLM-M trained on different PROX model curated data. Train Steps 2500 5000 7500 10000 12500 2500 5000 7500 10000 2500 5000 7500 10000 12500 2500 5000 7500 10000 12500 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 23.5 24.0 24.3 24.8 26.3 24.9 26.7 27.5 28.4 28.8 25.3 26.1 27.7 27.2 28. 24.7 27.7 26.7 28.4 28.3 41.5 42.1 44.9 46.1 46.8 49.6 47.6 52.1 54.7 54.2 45.7 49.0 53.6 54.0 56.1 44.1 48.0 51.9 52.4 53.7 TLM-S trained on Raw data 32.9 37.6 39.3 41.4 43.2 26.4 27.6 27.8 27.9 28.3 25.2 27.2 27.6 28.4 27.8 62.1 65.0 66.4 67.5 68.2 TLM-M trained on PROX-xs curated data 34.0 39.7 41.8 45.2 46. 27.3 28.5 29.6 30.8 30.9 30.4 31.8 31.8 31.8 31.8 61.8 65.4 67.6 67.9 68.2 TLM-M trained on PROX-s curated data 34.2 40.2 44.1 45.1 45.5 27.8 29.2 29.6 30.3 30. 29.0 30.8 34.8 33.8 34.4 64.4 65.6 67.6 67.7 68.5 TLM-M trained on PROX-m curated data 34.8 40.5 42.9 45.0 45.9 27.4 28.5 29.3 29.7 30.1 27.8 30.6 31.4 32.0 33. 62.9 67.4 69.1 70.2 70.6 27.5 29.6 28.9 29.6 29.0 26.5 28.6 30.4 29.8 29.7 27.8 28.8 31.1 31.5 31.8 25.9 26.8 26.7 27.9 28.4 39.4 39.7 40.4 39.8 40. 37.9 39.5 39.6 39.7 39.9 37.5 39.0 39.4 39.7 39.4 38.9 39.4 40.3 40.0 41.1 51.5 53.2 51.3 51.9 50.7 51.3 50.2 51.7 52.0 51.3 49.3 50.5 52.5 52.9 51. 49.2 50.3 50.4 51.9 52.3 65.1 68.5 69.2 70.9 72.5 65.1 70.7 75.2 77.7 78.3 66.3 71.2 72.2 74.2 76.1 67.0 69.1 73.3 75.4 72.5 39.5 41.4 42.0 42.8 43. 40.9 42.9 44.7 45.8 46.0 40.7 43.0 45.3 45.6 46.2 40.3 42.8 44.2 45.3 45.7 We also further scale PROX to other two pre-training corpora, C4 and FineWeb. We also scale our training to about 50B tokens, and directly compare with existing well-trained models developed by different research groups. We report our detailed results in Table 15, Table 16 and Table 17. We also present other models results in Table 18. Table 15: Full evaluation results on scaling pre-training to about 50B tokens on RedPajama-V2. Train Steps 2500 5000 7500 10000 12500 15000 17500 20000 22500 25000 2500 5000 7500 10000 12500 15000 17500 20000 22500 25000 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 24.0 24.3 25.1 25.8 25.3 26.2 25.8 26.7 27.4 26.9 24.8 26.9 28.5 28.2 29.5 30.0 31.5 31.2 32.0 31.1 42.9 45.9 45.3 49.3 50.1 50.3 51.1 52.5 51.7 51. 46.8 49.3 53.1 53.5 55.3 57.1 59.6 61.2 61.7 60.7 TLM-M trained on RedPajama-V2 raw data. 26.6 26.4 28.8 31.5 30.2 31.2 30.8 31.7 32.1 32.4 33.7 37.4 40.3 42.5 43.0 44.3 44.7 47.2 47.2 47.3 25.9 27.0 27.1 28.0 28.2 28.8 29.0 28.6 29.3 29.3 26.0 27.6 29.2 28.8 30.0 28.4 29.6 30.4 30.4 32. 62.4 64.1 66.3 66.7 66.6 68.2 67.7 69.0 69.5 69.7 39.4 39.7 39.1 39.6 39.2 39.8 39.2 39.6 39.5 39.6 TLM-M trained on PROX refined RedPajama-V2 data. 28.2 30.6 33.2 31.6 32.2 33.0 33.6 35.2 34.0 33.2 61.3 66.2 66.9 68.4 68.6 69.5 69.4 70.6 70.0 70.9 38.6 39.7 39.3 39.6 40.2 39.8 39.8 40.1 39.9 39. 27.2 28.5 29.2 30.1 30.2 30.2 29.4 29.4 30.2 29.8 33.8 40.1 41.7 43.6 46.4 47.6 49.5 50.4 51.4 51.0 27.3 28.0 29.4 29.8 30.5 30.9 31.6 31.4 31.4 31.7 34 52.3 49.5 51.7 51.5 51.1 51.7 52.6 53.0 51.9 52.1 50.3 50.2 53.0 52.0 52.6 52.2 53.0 53.7 53.2 53. 64.0 66.2 66.9 74.0 74.2 76.2 75.2 78.2 78.5 79.1 65.1 70.1 73.0 75.3 76.9 77.8 78.9 79.6 79.5 79.1 39.7 40.8 42.0 43.8 43.8 44.5 44.6 45.7 45.7 46.0 40.3 43.0 44.7 45.2 46.2 46.8 47.6 48.3 48.3 48.0 Table 16: Full evaluation results on scaling pre-training to about 50B tokens on C4. Train Steps 2500 5000 7500 10000 12500 15000 17500 20000 22500 25000 2500 5000 7500 10000 12500 15000 17500 20000 22500 25000 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 22.4 23.9 25.1 25.5 25.8 26.9 26.6 26.3 25.8 25.3 24.1 27.3 28.3 30.0 29.3 29.6 30.6 30.0 30.1 31.1 39.7 42.9 44.8 46.0 48.8 48.0 48.8 50.1 50.7 48. 45.9 50.0 53.7 54.3 56.7 55.9 55.5 57.6 56.7 56.0 TLM-M trained on C4 raw data. 36.5 42.3 45.4 48.2 49.7 50.5 52.1 52.5 52.9 52.4 26.5 27.1 27.1 27.9 27.9 28.5 28.6 28.5 28.8 28.8 27.6 29.6 29.2 31.6 31.6 31.4 31.2 32.6 33.8 32.2 64.8 68.2 70.7 71.1 71.2 71.9 73.2 72.3 73.0 72. TLM-M trained on PROX refined C4 data. 37.3 42.4 47.7 50.9 52.3 53.9 53.3 54.9 55.2 55.2 27.2 28.6 29.3 30.0 30.9 30.6 31.2 31.1 31.4 31.1 29.0 33.8 35.4 33.6 33.8 35.0 34.2 37.2 37.2 36.2 66.3 68.1 71.1 71.2 72.8 72.9 73.6 74.6 73.8 74.0 26.8 27.5 28.2 32.3 30.3 28.2 30.3 29.7 31.0 30. 26.0 26.6 27.7 28.1 27.5 28.3 28.7 28.3 28.6 28.4 40.2 39.6 40.7 39.7 40.9 41.1 41.6 41.7 41.6 40.6 39.8 40.5 39.3 40.6 39.9 41.0 40.4 40.7 41.6 41.0 50.1 50.3 51.6 52.3 52.0 51.4 52.0 52.3 53.0 53.6 50.8 53.0 54.0 52.0 52.5 53.8 53.4 53.6 53.3 54.1 60.0 66.6 66.3 67.6 69.4 69.7 70.0 71.0 71.5 71. 65.9 71.9 73.1 74.2 77.5 75.8 76.7 79.4 77.7 76.8 39.5 41.8 42.9 44.2 44.8 44.8 45.4 45.7 46.2 45.5 41.2 44.2 46.0 46.5 47.3 47.7 47.8 48.7 48.6 48.4 Table 17: Full evaluation results on scaling pre-training to about 50B tokens on FineWeb. Train Steps 2500 5000 7500 10000 12500 15000 17500 20000 22500 2500 5000 7500 10000 12500 15000 17500 20000 22500 25000 ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 22.9 25.5 26.8 27.2 26.4 27.1 26.4 27.1 27.1 28.5 25.8 28.5 28.2 29.3 28.7 31.1 32.6 33.2 34.7 34.4 41.2 44.5 45.6 46.2 49.2 49.6 50.9 53.1 51.2 52.6 46.8 52.1 52.0 54.3 57.8 59.6 60.9 62.5 63.6 63. TLM-M trained on FineWeb raw data. 28.9 30.4 31.4 31.3 32.1 32.8 33.8 33.2 34.9 33.9 34.3 39.8 44.1 47.2 48.7 49.5 51.3 51.2 51.7 53.2 26.1 26.9 27.6 28.3 28.7 28.9 29.3 29.6 29.5 29.8 27.6 32.0 30.2 31.6 31.6 31.0 31.0 32.2 33.4 32.6 64.8 68.4 70.9 72.1 71.5 72.7 71.9 73.4 73.7 72. TLM-M trained on PROX refined FineWeb data. 27.4 28.8 30.6 30.6 30.7 31.9 31.9 32.5 32.9 32.6 36.1 43.5 45.9 48.5 48.1 50.4 51.5 51.6 53.3 53.0 28.8 32.6 33.0 33.2 32.6 34.4 33.8 34.6 34.8 34.4 63.9 66.4 69.3 69.7 72.0 71.9 72.3 72.4 73.1 73.1 27.7 29.3 29.9 30.8 31.1 31.8 32.2 32.4 32.9 33. 35 39.3 39.2 38.8 38.8 40.1 39.0 39.3 39.7 40.1 40.2 39.3 38.7 39.5 40.7 40.4 40.5 39.7 39.7 40.3 39.3 52.1 52.1 52.2 53.4 52.6 52.3 53.0 52.3 52.4 53.0 51.9 51.2 51.7 50.6 52.7 50.8 52.5 51.7 54.2 52.7 62.8 67.2 70.3 69.0 74.7 77.1 78.0 76.3 78.0 77. 69.1 71.3 71.8 74.4 77.4 78.0 78.9 80.7 80.5 81.5 40.0 42.6 43.8 44.5 45.6 46.0 46.5 46.8 47.2 47.4 41.7 44.2 45.2 46.2 47.2 48.0 48.6 49.1 50.0 49.8 Table 18: Detailed evaluation results of existing base models trained on different corpora and trained using different techniques. ARC-C ARC-E CSQA HellaSwag MMLU OBQA PiQA SIQA WinoG SciQ AVG 31. 59.0 35.5 57.8 32.8 33.4 72. 40.0 56.0 82.4 50.1 TINYLLAMA-1.1B (trained on 3T tokens) 31. 59.7 38.9 61.9 32.2 38.4 76. 41.5 53.9 78.8 51.3 OLMO-1B (trained on 2T tokens) PYTHIA-1.4B 28.7 56.9 34.7 51.7 31.5 36. 71.8 40.8 55.1 79.3 48.7 PYTHIA-2.8B 32.9 61.0 36.5 60.4 33.3 35. 73.5 41.1 57.0 83.1 51.4 SHEAREDLLAMA-1.3B (pruned from LLAMA-2-7B) 22.4 39.7 29.3 36.0 26.4 28. 62.6 39.9 52.0 71.4 40.8 SHEAREDLLAMA-1.3B (pruned from LLAMA-2-7B, and further trained on 50B tokens) 29.0 58.3 34.8 59.6 32.0 35. 74.6 41.0 56.3 82.3 50.3 28. 57.9 32.5 52.3 30.0 34.0 74. 39.9 56.1 86.9 49.2 INSTRUCTLM-1.3B (LLM data synthesis) COSMO-1.8B (LLM data synthesis) 33.4 57.0 31.2 55.1 32.4 35. 71.4 42.0 54.7 84.4 49.7 D.4 EVALUATION RESULTS OF CONTINUAL PRE-TRAINING IN SEC 3.4 We provide full ablation results for each base model, as shown in Table 19. We can observe that PROX-D+C consistently improves average performance over PROX-D across various base models. Although the performance gain from PROX-D+C compared to PROX-D is less pronounced than the improvement of PROX-D over continual pre-training on raw OpenWebMath, this is both understandable and expected. PROX-D+C does not significantly reduce the token count beyond the reductions achieved by PROX-D alone. Given the scale of the OpenWebMath corpus, more aggressive token removal strategy could potentially diminish the diversity of unique tokens below the threshold necessary for robust pre-training. This observation underscores the delicate balance between data refinement and maintaining sufficient linguistic variety for effective language model training, particularly when working with limited-scale corpora. Table 19: Full ablation results on OpenWebMath Continual Pre-training (CPT). All models are tested using few-shot CoT prompts. LLEMMA and INTERNLM2-MATH are continual pre-trained models from CODELLAMA (Rozière et al., 2023) and INTERNLM2 (Team, 2023) with public available data, respectively. DEEPSEEK-LLM denotes an internal DeepSeek model, and the model trained on OpenWebMath introduced by Shao et al. (2024). Note that the unique tokens and training tokens in the column refer exclusively to the token numbers from math-specific corpora (calculated by corresponding tokenizers). : MQA evaluation of INTERNLM2-BASE is based on an alternative prompt due to non-prediction issues with the original prompt. The bolded entries represent the best results within the same base model and CPT experiments. Model Size Method Uniq Toks Train Toks GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG Existing Continual Pre-training for Reference DEEPSEEK-LLM CODELLAMA (Base) LLEMMA INTERNLM2-BASE INTERNLM2-MATH 1.3B - 1.3B - 7B - 34B - 7B - 34B - 7B - 20B - 7B - 20B - - - 14B 150B - - - - 55B 200B 55B 50B - - - - 31B 125B 120B 500B 2.9 11.5 11.8 31.8 38.8 54.2 27.0 50.6 41.8 65. 3.0 8.9 5.0 10.8 17.2 23.0 6.6 18.8 14.4 30.0 - - 44.2 61.9 56.1 67.9 49.0 72.5 61.6 75.7 - - 50.7 66. 69.1 75.7 59.3 75.9 66.8 79.3 Applying Data Refinement Approaches TINYLLAMA (Base) 1.1B - - - TINYLLAMA (CPT) 15B 15B 1.1B - 15B 9B6 1.1B RHO 6.5B 15B 1.1B Rule 1.1B PROX-D 5.4B 15B 1.1B PROX-D+C 5B 15B LLAMA-2 (Base) 7B - - - LLAMA-2 (CPT) 15B 10B 7B - 7B PROX-D 5.4B 10B 7B PROX-D+C 5B 10B CODELLAMA (Base) 7B - - - CODELLAMA (CPT) 15B 10B 7B - 7B PROX-D 5.4B 10B 7B PROX-D+C 5B 10B MISTRAL (Base) 7B - - - MISTRAL (CPT) 15B 10B 7B - 7B PROX-D 5.5B 10B 7B PROX-D+C 4.7B 10B 2.8 6.2 7.1 4.5 9.3 9.0 14.1 29.6 30.3 30. 11.8 31.1 38.1 35.6 40.6 44.4 47.8 51.0 3.2 4.8 5.0 2.8 7.4 5. 3.8 13.6 16.0 16.8 5.0 14.8 17.0 17.6 11.4 19.2 24.8 22. 10.9 22.3 23.5 17.5 23.4 23.8 39.5 49.2 54.2 50.2 44.2 51.4 54.2 55. 65.4 65.2 63.5 64.9 18.0 36.2 41.2 29.4 41.9 41.9 51.6 61.9 63.8 63. 50.7 62.1 67.0 67.9 68.5 69.6 72.4 72.9 - - 62.6 83. 82.4 90.1 74.8 93.9 83.7 94.0 20.2 47.6 53.8 39.3 55.6 56.9 63. 78.4 79.5 79.3 62.6 81.2 83.1 82.7 87.0 88.4 88.9 89.2 - - - - 30.6 14.3 51.6 23.7 48.7 41.0 57.9 49.8 40.1 20.9 45.4 33.1 50.0 57.3 50.9 38.5 12.5 14. - 19.3 11.6 18.0 15.1 12.4 22.1 14.6 22.2 15.6 30.9 12.5 36.3 31.9 37.3 37.2 37.3 40.1 30.6 14.3 33.6 30.4 40.9 39.8 41.3 38. 52.9 32.3 46.6 43.1 48.3 48.2 49.8 53.0 19.5 29.6 20.4 43.0 45.4 54.7 19.0 53. 24.8 53.1 16.4 20.7 - 19.4 24.1 26.8 32.9 40.5 44.2 43.8 20. 40.5 43.7 42.6 50.0 50.8 54.1 54.2 15.6 31.3 21.9 53.1 59.4 68. 28.1 59.4 37.5 71.9 21.9 25.0 - 25.0 25.0 31.2 34.4 43.8 46.9 53. 21.9 43.8 50.0 62.5 56.2 65.6 62.5 75.0 - - 29.1 47. 50.9 (+21.8) 60.1 (+12.8) 36.1 55.9 48.7 (+12.6) 62.1 (+6.2) 14.7 21.5 (+8.1) - 18.4 (+3.7) 24.8 (+10.1) 25.7 (+11.0) 31. 42.8 (+11.3) 45.5 (+14.0) 46.1 (+14.6) 29.1 43.2 (+14.1) 48.2 (+19.1) 49.4 (+20.3) 51.6 54.8 (+3.2) 56.4 (+4.8) 59.2 (+7.6) Besides, we report the detailed dynamic evaluation results of our continual pre-training experiments on OpenWebMath: Tables 20, 21, 22, and 23 present the evaluation results for TINYLLAMA-1.1B. 6RHO-1 only counts the selected tokens that are used for training (loss calculation). 37 Tables 24, 25, and 26 present the evaluation results for LLAMA-2. Tables 27, 28, 29 present the evaluation results for CODELLAMA. Tables 30, 31, and 32 show the evaluation results for MISTRAL-7B. Table 20: Full evaluation results of TINYLLAMA-1.1B continual pre-training on OpenWebMath with raw data. Note that about 1B tokens are trained per 500 steps. Train Steps 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 2.8 1.9 3.1 2.7 4.5 4.9 4.1 4.9 4.8 5.4 5.5 4.9 6.1 6.3 6.1 6.2 3.2 3.4 2.2 3.0 3.2 3.4 5.2 3.6 4.8 4.8 4.6 5.8 4.4 3.6 4.6 4. 10.9 16.3 16.6 17.6 16.4 19.3 19.1 19.7 19.5 20.2 22.3 23.6 22.8 23.2 22.2 22.3 18 23.9 25.6 28.5 28.5 31.0 32.0 31.4 33.8 35.0 34.6 35.2 36.2 37.3 36.6 36.2 20.2 30.3 32.4 34.5 39.0 39.2 43.0 40.4 44.5 45.2 42.9 44.0 45.4 48.0 46.9 47. 12.5 13.9 12.5 13.9 15.1 16.0 15.3 18.1 16.4 17.9 16.0 20.4 17.8 19.7 19.4 19.3 14.6 10.3 12.0 8.7 10.2 12.1 9.6 11.3 10.7 12.7 10.6 11.0 12.7 10.3 12.0 11.6 16.4 14.8 16.6 14.1 16.6 18.6 16.1 19.6 19.9 21.0 21.7 21.1 21.4 21.0 21.5 20. 21.9 18.8 25.0 15.6 34.4 9.4 18.8 15.6 12.5 18.8 28.1 21.9 15.6 18.8 21.9 25.0 14.7 14.8 16.2 15.4 18.7 17.1 18.1 18.3 18.5 20.1 20.7 20.9 20.3 20.9 21.2 21.5 Table 21: Full evaluation results of TINYLLAMA-1.1B continual pre-training on OpenWebMath with data after rule-based filtering. Note that about 1B tokens are trained per 500 steps. Train Steps 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 2.8 3.4 3.0 3.6 3.5 3.3 3.5 3.2 3.5 4.1 4.2 4.1 4.3 4.2 4.0 4.5 3.2 3.6 2.8 3.2 2.4 1.6 3.0 3.4 3.6 3.8 3.6 3.8 3.6 3.2 4.0 2.8 10.9 13.6 14.1 13.6 15.0 15.0 16.4 17.2 15.6 15.6 18.6 16.3 16.0 16.4 16.2 17. 18 22.5 22.5 24.0 25.1 25.3 25.5 27.0 26.2 27.9 28.7 29.3 28.7 29.5 29.6 29.4 20.2 25.9 27.8 31.2 33.0 33.5 33.4 37.7 36.5 38.2 37.7 38.4 39.1 39.0 37.9 39.3 12.5 13.1 11.4 13.9 12.5 13.7 14.1 14.6 13.4 14.9 14.3 14.7 13.5 15.1 16.0 15. 14.6 14.2 11.0 9.2 10.6 11.1 10.2 11.2 12.1 11.6 12.7 10.8 12.8 11.7 13.8 12.4 16.4 13.5 16.4 18.0 13.9 18.1 18.4 13.3 15.9 17.1 17.5 17.5 19.5 17.9 17.8 19.4 21.9 28.1 12.5 18.8 15.6 25.0 18.8 25.0 18.8 18.8 21.9 18.8 21.9 21.9 21.9 25. 14.7 15.3 13.5 15.1 14.6 16.3 15.9 17.0 16.2 16.9 17.7 17.1 17.7 17.7 17.9 18.4 38 Table 22: Full evaluation results of TINYLLAMA-1.1B continual pre-training on OpenWebMath with data after PROX-D. Note that about 1B tokens are trained per 500 steps. Train Steps 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 2. 3.3 4.6 5.2 6.8 7.1 7.4 8.8 8.6 8.6 8.9 8.0 8.3 9.4 9.2 9.3 3.2 2.8 4.0 5.4 5.8 3.8 4.4 4.8 4.6 4.2 5.2 6.2 5.2 5.6 5.8 7.4 10.9 17.7 18.1 21.1 20.2 20.7 22.9 22.8 24.0 24.2 24.0 23.2 22.2 24.4 25.8 23.4 29.0 31.6 32.9 33.5 37.0 37.1 39.4 38.7 39.2 40.0 41.4 39.8 40.2 40.6 41.9 20.2 38.7 41.9 43.1 46.6 48.6 50.5 53.3 51.4 53.6 52.6 55.0 54.0 54.5 55.3 55.6 12.5 12.4 15.9 15.3 18.2 18.3 18.3 19.2 18.8 20.4 20.0 22.3 24.3 20.3 22.5 22.1 14. 9.5 11.9 11.1 10.7 12.0 12.3 12.0 14.8 13.5 13.6 14.3 12.6 13.0 12.5 14.6 16.4 15.7 18.2 20.4 20.3 21.4 21.2 22.8 24.4 23.9 23.9 24.9 25.1 24.9 24.5 24.1 21.9 15.6 25.0 12.5 12.5 18.8 25.0 34.4 18.8 18.8 18.8 25.0 31.2 31.2 21.9 25.0 14. 16.1 19.0 18.6 19.4 20.9 22.1 24.2 22.7 22.9 23.0 24.5 24.7 24.8 24.2 24.8 Table 23: Full evaluation results of TINYLLAMA-1.1B continual pre-training on OpenWebMath with data after PROX-D+C. Note that about 1B tokens are trained per 500 steps. Train Steps 0 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 5500 6000 6500 7000 7500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 2.8 4.3 5.5 5.2 6.3 7.8 6.4 8.5 8.2 8.3 8.5 8.7 8.3 8.6 8.9 9.0 3. 5.0 3.8 4.4 5.4 5.4 3.4 4.6 6.0 5.4 7.0 4.0 5.0 6.4 6.0 4.4 10.9 16.4 20.5 21.4 20.1 22.1 23.0 24.1 24.1 24.1 26.0 23.2 24.8 24.5 23.4 23.8 18 28.8 34.6 34.5 33.7 37.0 38.6 40.2 41.0 41.3 40.5 41.1 41.3 41.6 40.5 41.9 20. 36.4 44.6 44.7 46.2 49.5 51.1 53.8 52.4 54.4 54.9 54.8 54.3 55.1 53.4 56.4 12.5 15.3 15.3 16.1 19.4 17.9 18.5 22.1 19.8 20.6 21.7 20.5 23.2 22.2 22.0 22.2 14.6 11.4 12.1 11.2 10.5 13.3 12.6 12.5 10.2 15.2 13.9 14.4 14.0 14.4 15.8 15.6 16. 18.5 19.6 21.4 21.2 22.9 24.3 23.1 26.1 24.2 25.5 26.5 25.3 26.5 27.3 26.8 21.9 15.6 28.1 34.4 12.5 21.9 18.8 25.0 31.2 28.1 34.4 21.9 25.0 25.0 28.1 31.2 14.7 16.9 20.5 21.5 19.5 22.0 21.9 23.8 24.3 24.6 25.8 23.9 24.6 24.9 25.0 25.7 Table 24: Full evaluation results of LLAMA-2 continual pre-training on OpenWebMath with raw data. Note that about 1B tokens are trained per 1000 steps. Train Steps 0 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 14.1 17.2 19.7 19.6 21.8 22.6 24.5 23.3 29.0 26.1 29.6 3.8 3.6 6.0 8.6 8.8 10.4 10.0 10.4 12.4 12.8 13. 39.5 39.1 43.9 42.9 44.6 45.9 44.9 46.5 46.4 48.8 49.2 51.6 50.4 55.5 56.3 57.3 57.0 57.6 59.0 59.7 59.9 61.9 63.6 63.0 68.3 68.4 72.0 73.5 73.7 75.3 77.0 74.3 78. 30.9 30.2 32.9 32.2 28.9 31.5 35.5 32.9 33.1 35.0 36.3 12.5 18.9 19.0 17.4 23.6 23.9 25.8 27.7 30.2 28.3 31.9 32.9 31.8 33.0 34.6 35.8 39.0 36.1 39.0 38.8 39.2 40. 34.4 31.2 37.5 40.6 40.6 43.8 43.8 50.0 50.0 50.0 43.8 31.5 31.7 35.1 35.6 37.0 38.6 39.1 40.5 41.8 41.6 42.8 Table 25: Full evaluation results of LLAMA-2 continual pre-training on OpenWebMath with PROXD. Note that about 1B tokens are trained per 1000 steps. Train Steps 0 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 14.1 17.1 21.9 20.5 27.2 28.9 31.9 31.5 30.3 30.6 30.3 3.8 7.2 9.2 10.8 11.8 14.2 15.0 16.8 13.8 14.0 16.0 39.5 39.8 43.2 45.7 45.7 49.3 51.5 51.9 51.9 52.7 54. 51.6 51.6 57.0 58.6 58.7 60.2 62.0 63.2 63.7 62.6 63.8 63.6 68.4 72.8 76.2 76.6 77.9 79.0 77.9 80.6 78.7 79.5 30.9 31.4 33.1 35.3 35.9 38.8 39.2 36.5 38.3 37.5 37. 12.5 21.4 24.0 25.8 29.2 32.8 33.3 35.9 36.1 36.1 37.2 32.9 35.2 37.6 38.3 41.0 41.7 41.4 43.8 41.3 43.2 44.2 34.4 40.6 56.2 53.1 31.2 53.1 68.8 43.8 59.4 43.8 46. 31.5 34.7 39.4 40.5 39.7 44.1 46.9 44.6 46.2 44.4 45.5 Table 26: Full evaluation results of LLAMA-2 continual pre-training on OpenWebMath with PROXD+C. Note that about 1B tokens are trained per 1000 steps. Train Steps 0 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 14.1 18.8 23.1 23.4 25.2 24.4 29.6 29.9 30.2 34.0 30. 3.8 6.8 8.6 11.8 14.2 13.6 12.8 13.6 15.8 15.4 16.8 39.5 40.1 45.7 47.9 49.0 48.0 46.1 50.5 50.8 52.1 50.2 51.6 54.4 56.5 59.1 57.8 58.7 63.4 61.5 63.7 62.4 63. 63.6 66.1 72.7 74.6 72.7 72.1 75.6 75.2 77.1 79.3 79.3 30.9 29.7 30.7 30.4 32.8 28.9 33.7 36.4 37.7 35.9 37.3 12.5 22.9 25.1 28.2 33.1 33.0 31.6 34.5 36.3 40.2 40. 32.9 35.6 35.6 38.3 40.7 40.6 42.8 41.7 43.4 44.0 43.8 34.4 53.1 46.9 59.4 40.6 50.0 53.1 53.1 43.8 56.2 53.1 31.5 36.4 38.3 41.5 40.7 41.0 43.2 44.0 44.3 46.6 46. 40 Table 27: Full evaluation results of CODELLAMA-7B continual pre-training on OpenWebMath with raw data. Note that about 1B tokens are trained per 250 steps. Train Steps 0 250 500 750 1000 1250 1500 1750 2000 2250 2500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 11.8 16.7 18.3 20.2 24.7 24.3 26.2 25.5 28.0 27.7 31.1 5. 8.2 7.8 8.0 9.8 10.4 13.2 11.8 13.6 13.6 14.8 44.2 45.2 43.1 45.2 40.6 44.0 48.4 49.1 46.3 48.9 51.4 50.7 52.2 53.9 54.2 58.6 57.5 58.8 58.7 61.7 62.2 62.1 62. 65.3 69.0 71.9 72.7 74.8 75.4 76.6 80.0 80.3 81.2 30.6 33.9 29.3 29.9 29.3 29.2 29.4 32.4 33.8 32.5 33.6 14.3 16.0 15.3 17.1 20.7 21.4 28.1 26.7 29.4 28.9 30.4 20. 28.8 22.5 31.2 31.9 36.1 34.9 37.3 37.2 39.1 40.5 21.9 43.8 37.5 37.5 34.4 50.0 50.0 43.8 50.0 59.4 43.8 29.1 34.5 33.0 35.0 35.9 38.6 40.5 40.2 42.2 43.6 43.2 Table 28: Full evaluation results of CODELLAMA continual pre-training on OpenWebMath with PROX-D. Note that about 1B tokens are trained per 250 steps. Train Steps 0 250 500 750 1000 1250 1500 1750 2000 2250 2500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 11.8 21.1 23.7 25.1 28.4 33.0 36.0 34.7 35.7 37.2 38.1 5.0 9.2 11.6 15.4 14.2 15.2 15.0 14.6 17.6 18.8 17.0 44. 48.7 49.8 48.1 50.9 49.3 54.2 53.1 53.3 54.5 54.2 50.7 56.1 57.4 58.9 61.2 62.9 65.0 63.6 65.4 65.4 67.0 62.6 71.3 74.7 78.8 79.8 81.1 81.0 83.3 83.5 83.2 83.1 30. 33.4 32.9 36.8 36.7 33.4 39.3 40.6 42.4 41.9 40.9 14.3 22.2 28.5 29.4 27.7 32.8 34.1 35.9 37.1 41.0 39.8 20.4 34.1 35.8 37.6 37.6 41.0 42.0 43.4 42.4 44.9 43.7 21. 50.0 59.4 53.1 50.0 46.9 62.5 62.5 56.2 71.9 50.0 29.1 38.5 41.5 42.6 42.9 44.0 47.7 48.0 48.2 51.0 48.2 Table 29: Full evaluation results of CODELLAMA continual pre-training on OpenWebMath with PROX-D+C. Note that about 1B tokens are trained per 250 steps. Train Steps 250 500 750 1000 1250 1500 1750 2000 2250 2500 GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 11. 18.1 22.4 26.8 29.0 31.4 31.5 33.7 36.2 37.1 35.6 5.0 10.2 10.0 11.4 14.4 15.0 17.4 15.2 16.0 16.6 17.6 44.2 46.0 50.3 51.2 54.1 51.7 53.4 50.6 54.7 55.3 55.8 50. 54.5 59.7 61.0 62.8 63.8 64.4 64.3 65.1 65.6 67.9 62.6 71.9 76.4 78.5 80.1 81.1 80.7 81.5 83.1 82.4 82.7 30.6 33.0 31.3 34.9 36.9 37.2 39.6 39.2 39.9 41.3 41.3 14. 21.3 26.1 26.4 34.2 32.5 35.4 36.1 39.1 36.5 38.9 20.4 34.4 36.0 38.0 40.4 41.4 41.6 40.5 43.4 42.7 42.6 21.9 50.0 59.4 53.1 62.5 75.0 71.9 53.1 71.9 75.0 62.5 29. 37.7 41.3 42.4 46.0 47.7 48.4 46.0 49.9 50.3 49.4 41 Table 30: Full evaluation results of MISTRAL-7B continual pre-training on OpenWebMath with raw data. Note that about 1B tokens are trained per 1000 steps. Train Steps 0 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 40.6 31.6 32.4 33.6 35.1 33.4 38.7 39.6 44.0 43.9 44. 11.4 12.0 10.8 14.8 14.8 16.0 16.6 17.2 16.4 19.4 19.2 65.4 56.5 54.7 60.4 58.7 59.3 61.5 60.5 64.5 63.7 65.2 68.5 66.0 63.5 64.7 65.2 65.0 68.1 68.2 69.8 69.7 69. 87.0 80.1 82.6 84.5 84.4 83.8 86.1 86.2 88.7 87.6 88.4 52.9 43.9 40.8 43.5 41.2 46.7 47.4 44.4 45.5 44.9 46.6 32.3 27.1 31.6 33.1 38.5 34.6 35.3 38.5 41.3 42.9 43. 50.0 45.1 45.7 47.2 47.3 49.1 48.5 49.3 50.6 51.0 50.8 56.2 56.2 59.4 68.8 62.5 62.5 37.5 53.1 59.4 62.5 65.6 51.6 46.5 46.8 50.1 49.7 50.0 48.9 50.8 53.4 54.0 54. Table 31: Full evaluation results of MISTRAL-7B continual pre-training on OpenWebMath with PROX-D. Note that about 1B tokens are trained per 1000 steps. Train Steps 0 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 40.6 36.8 38.5 40.0 38.5 42.5 46.8 47.5 44.6 46.6 46.7 11.4 14.6 17.0 19.0 20.4 20.2 17.8 22.4 23.8 24.6 22. 65.4 57.2 57.9 59.3 59.3 63.0 62.5 64.1 63.2 61.6 63.5 68.5 66.1 69.0 68.7 66.2 70.5 72.7 71.8 70.8 72.3 72.4 87.0 83.1 86.3 87.0 85.1 86.6 88.2 89.1 87.7 86.4 88. 52.9 45.7 44.7 46.8 42.6 47.2 51.2 51.4 47.6 46.9 48.3 32.3 32.6 33.6 41.0 42.8 43.4 47.7 47.9 49.1 49.8 48.2 50.0 47.7 49.2 48.0 49.5 49.8 51.3 52.4 54.1 53.2 54. 56.2 59.4 56.2 68.8 68.8 62.5 56.2 65.6 65.6 65.6 62.5 51.6 49.2 50.3 53.2 52.6 54.0 54.9 56.9 56.3 56.3 56.4 Table 32: Full evaluation results of Mistral-7B continual pre-training on OpenWebMath with PROXD+C. Note that about 1B tokens are trained per 1000 steps. Train Steps 0 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k GSM8K MATH SVAMP ASDiv MAWPS TAB MQA MMLU STEM SAT MATH AVG 40.6 30.9 40.3 42.4 43.8 42.5 47.7 46.8 48.4 48.5 51.0 11.4 16.0 17.6 17.8 20.4 18.4 21.8 21.6 21.6 24.8 22.4 65.4 60.1 63.0 59.6 63.7 59.3 62.7 62.9 65.0 64.4 64. 68.5 64.5 66.3 69.1 69.3 69.6 71.7 72.1 72.7 72.6 72.9 87.0 85.3 86.2 85.7 88.2 87.9 89.2 88.4 89.2 88.3 89.2 52.9 40.8 48.0 50.1 46.2 44.3 47.9 50.1 51.1 50.7 49. 32.3 33.9 33.9 38.5 46.3 46.1 48.4 46.4 49.4 48.1 53.0 50.0 48.0 48.7 49.9 50.9 51.9 54.0 52.5 52.9 53.4 54.2 56.2 59.4 53.1 59.4 65.6 65.6 68.8 68.8 65.6 62.5 75. 51.6 48.8 50.8 52.5 54.9 54.0 56.9 56.6 57.3 57.0 59."
        },
        {
            "title": "E ANALYSIS",
            "content": "E.1 CASE STUDIES We provide several cases to qualitatively illustrate the refinement effect of PROX, as shown in Tables 33-34. For the general domain, using RedPajama-V2 as an example, we observe that PROX can drop low-information documents, remove meaningless content such as navigation bars, and replace URL links (see Table 33). In the mathematics domain, PROX demonstrates the ability to eliminate documents with minimal relevance to mathematical reasoning and remove less important elements like functional buttons (see Table 34). These refinements enhance the quality and relevance of the processed data across different domains. E.2 COMPUTING OVERHEAD ANALYSIS According to Kaplan et al. (2020), both training and inference computational FLOPs for Transformerbased Language Models (denoted as Ctrain and Cinference) can be approximated as the product of model parameters (non-embedding parameter) and the number of tokens D. This can be expressed as: Ctrain 6 Dtrain, Cinference 2 (Dprefill + Ddecode) . (9) (10) In PROX, we go through two data refining stages before final training, which incurs additional inference-time computational FLOPs. Suppose the refining model parameter for each stage is denoted as Nrefine, and the raw data size in tokens is Draw. For the first document-level stage, the computational cost can be approximated as: Cdoc 2 Nrefine (Draw + Doutput) 2 NrefineDraw, (suppose Doutput Draw) (11) resulting in new pool of data sized Ddoc. Similarly, for the second chunk-level stage, the computational cost is: Cchunk 2 Nr (Ddoc + Doutput) 2 NrDdoc, (suppose Doutput Ddoc) (12) which produces the final refined data size of DProX. Thus, the total computational overhead for PROX can be calculated as the sum of the two stages: CPROX = Cdoc + Cchunk 2 Ndoc_refineDraw + 2 Nchunk_refineDdoc. (13) In general, we use refining models with same sizes, so the final inference overhead can be estimated as CPROX 2 Nrefine(Draw + Ddoc). (14) Additionally, we omit the FLOPs for fine-tuning since they are negligible compared to the large-scale pre-training and inference FLOPs. 43 Table 33: Cases from RedPajama-V2 after applying PROX. Text in red indicates content to be removed or replaced. ... denotes omitted content due to limited space."
        },
        {
            "title": "TagCollegeEducationJournalismWar",
            "content": ": Michael Lewis"
        },
        {
            "title": "ContributorMichael Lewis",
            "content": "Case 1 Michael Lewis is possibly the most entertaining nonfiction writer alive. If thats not true its at least close to true. Liars Poker, Moneyball, The Blind Side, his NYT article about Jonathan Lebed (Google it): whats not to love? 504: How Got Into College Act Two: My Ames is True Writer Michael Lewis tells the story of man named Emir Kamenica, whose path to college started with fleeing the war in Bosnia and becoming refugee in the United States. Then he had stroke of luck: student teacher read an essay hed plagiarized from book hed stolen from library back in Bosnia, and was so impressed that she got him out of bad high school and into much better one."
        },
        {
            "title": "Act Three",
            "content": "Michael Lewis story continues, and he figures out why Emir Kamenica insists on remembering, and telling, the story of his life the way he does even when he finds out that some of the facts may be wrong. Output by PROX: drop_doc() Case 2 Home > Staff > Staff search > Dr Tim Overton Dr Tim Overton BSc PhD School of Chemical EngineeringSenior Lecturer Telephone (+44) (0) 121 414 5306Emailt.w.overton@bham.ac.uk AddressSchool of Chemical EngineeringUniversity of Birmingham B15 2TT Dr Tim Overton is biochemist and molecular microbiologist who is interested in applying molecular biology and singlecell techniques to understand and develop bioprocesses. He is active in microbial flow cytometry research and collaborates widely with bioprocess engineers, molecular microbiologists, cell biologists and environmental microbiologists to develop new methods of answering fundamental questions on single-cell level. His research also focuses on using bacteria to make useful products such as protein drugs and small molecules, and the bacterial responses to stress encountered in such processes. Current and recent research funding has come from the BBSRC, TSB and EU FP7. He is the director of the MSc in Biochemical Engineering. Pages: 1 3 4 ... Google scholar: http://scholar.google.co.uk/citations?user=tF_eBKEAAAAJ ... Output by PROX: keep_doc() remove_lines(line_start=0, line_end=5) normalize(source_str=\"http://scholar.google.co.uk/citations?user\", target_str=\"\") normalize(source_str=\"Pages: ... 1 3 4\", target_str=\"\") 44 Table 34: Cases from OpenWebMath after applying PROX. Text in red indicates content to be removed or replaced. ... denotes omitted content due to limited space. Case 1 ## unhybridized pi bonds sp, sp2, sp3, dsp3, d2sp Tatiana 4B Posts: 30 Joined: Fri Sep 28, 2018 12:28 am ### unhybridized pi bonds ... ### Re: unhybridized pi bonds am not too sure in my knowledge about this, but think that both have hybridized orbitals. Since hybridization is defined as the phenomenon of intermixing of the orbitals such as sp, sigma and pi bonds are just different types of covalent bonds formed depending on the way the atomic orbitals hybridize with each other. Sigma bonds are result of when the overlap of orbitals of two atoms takes place along the line joining the two orbitals, while pi bonds are when two atoms overlap due to the sideways overlap of their orbitals. Hannah Yates 1K Posts: 59 Joined: Fri Sep 28, 2018 12:27 am ### Re: unhybridized pi bonds am also not too sure on my answer, but am pretty sure that sigma bond has just hybridized orbitals, but the reason pi bond can form is because of an extra (not hybridized) orbital. This allows for double and triple bond to form. Output by PROX: drop_doc() Solution - Trigonometric Identities Case"
        },
        {
            "title": "Question",
            "content": "Prove the following trigonometric identities: sin θ 1cos θ = cosecθ + cot θ (i) Solution You need to to view the solution Is there an error in this question or solution?"
        },
        {
            "title": "Reference Material",
            "content": "Solution for concept: Trigonometric Identities. For the course CBSE Output by PROX: keep_doc() remove_lines(line_start=0, line_end=7) remove_lines(line_start=18, line_end=24)"
        }
    ],
    "affiliations": [
        "Generative AI Research Lab (GAIR)",
        "Sea AI Lab",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}