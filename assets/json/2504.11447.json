{
    "paper_title": "Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion",
    "authors": [
        "An Zhaol",
        "Shengyuan Zhang",
        "Ling Yang",
        "Zejian Li",
        "Jiale Wu",
        "Haoran Xu",
        "AnYang Wei",
        "Perry Pengyun GU Lingyun Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 4 4 1 1 . 4 0 5 2 : r Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion An Zhao1 Shengyuan Zhang1 Ling Yang2 AnYang Wei3 Zejian Li1 Perry Pengyun GU3 Jiale Wu Haoran Xu3 Lingyun Sun1 1 Zhejiang University 2 Peking University 3 Zhejiang Green Zhixing Technology co., ltd 1 {zhangshengyuan,zhaoan040113,zejianlee,ialewu2022,sunly}@zju.edu.cn 2 {yangling0818}@163.com 3 {Haoran.Xu5,weianyang,gupengyun}@geely.com Figure 1. An example demonstration of Distillation-DPO for LiDAR scene completion on SemanticKITTI dataset. (a) The input sparse LiDAR scan. (b) The corresponding ground truth scene. (c) Completion results of the existing state-of-the-art (SOTA) model, LiDiff [21]. (d) Completion results of the proposed Distillation-DPO. Compared to LiDiff, Distillation-DPO can complete scene more than 5 times faster while achieving higher completion quality (lower Chamfer Distance)."
        },
        {
            "title": "Abstract",
            "content": "The application of diffusion models in 3D LiDAR scene completion is limited due to diffusions slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github. 1 com/happyw1nd/DistillationDPO. 1. Introduction Recently, the diffusion model has gradually been utilized for LiDAR scene completion due to the outstanding performance in image super-resolution [4, 32] and video synthesis [13, 42]. However, since LiDAR point cloud completion requires high-precision set reconstruction and high-quality completion of missing points, diffusion models often need to sacrifice sampling time to achieve high-quality completion results. Thus, despite the potential of diffusion models in this domain, the slow sampling speed limits their practicality in real-world applications. As an effective distillation method for the diffusion model, the effectiveness of score distillation has been well established [16, 37, 38], which provides an effective pathway for accelerating LiDAR scene completion diffusion models. However, score distillation inevitably leads to information loss and quality decline in the completed scene during the sampling acceleration process. Reward models provide potential way to mitigate the performance degradation caused by distillation. The reward model learns human preferences to predict the rating of generated samples, while existing methods primarily enhance generation quality by maximizing the rating predicted by the reward model [33, 40]. However, the application of the reward model in score distillation of LiDAR scene completion faces following challenges. First, due to the complexity of LiDAR scenes, obtaining large-scale human-labeled data is challenging. With limited data, the reward model is easily over-optimized and faces the issue of reward hacking [1]. Second, existing methods often use differentiable rewards to optimize the model [7], but commonly evaluation metrics such as IoU [27] and EMD [8] are non-differentiable and computationally expensive, difficult to be used directly as rewards to optimize the diffusion model. Compared to reward models, Diffusion-DPO [23, 30] directly optimizes the diffusion model using preference data pairs, eliminating the need for training an additional reward model and thus avoiding the issue of reward hacking. Thus, to tackle the above challenges, we incoporate score distillation with the post training of DPO and propose novel distillation framework dubbed Distillation-DPO for LiDAR scene completion diffusion models. Distillation-DPO includes an effective distillation strategy on the preference completed scene pairs for the first time. Specifically, based on the completed scene generated by the student model, we use LiDAR scene evaluation metrics as preference to construct the win-lose preference pairs. Then, DistillationDPO optimizes the student model by computing the score function on both the student and teacher models. Compared with state-of-the-art (SOTA) LiDAR scene completion models, Distillation-DPO achieves significantly accelerated sampling for LiDAR completion diffusion models while delivering higher-quality completion results, setting new SOTA performance  (Fig. 1)  . Our contributions are summarized as follows: (1) We propose Distillation-DPO, novel distillation framework for LiDAR scene completion diffusion models, which is the first to perform distillation based on preference data pairs. (2) Compared to the existing state-of-the-art (SOTA) LiDAR scene completion models, Distillation-DPO achieves breakthroughs in both completion quality and speed. 2. Preliminary 2.1. LiDAR scene completion diffusion model The goal of the LiDAR scene completion diffusion model ϵθ is to predict noise based on the given LiDAR sparse scan P, enabling step-by-step denoising process from an initial noisy sample GT to obtain dense scene reconstruction G0. In the existing SOTA model LiDiff [21], the sampling step is often set to 50. Given input sparse scan = {p1, p2, ..., pN } and the ground truth = {p1, p2, ..., pM } (N M), the noisy point cloud Gt = {p1 , ..., pM } can be calculated in point-wise fashion [21] = pm + (cid:0) pm (cid:1) = pm + 1 αtϵt 1 αtϵt αt0 + , p2 (1) Here pm R3 is the point cloud. Such diffusion method is adopted because LiDAR data is large in scale, and directly applying traditional noise injection methods like DDPM [12] would compress the LiDAR point cloud into smaller range, leading to loss of details. Due to the local diffusion method in Eq. (1), GT can not be directly approximated by the Gaussian distribution. Given sparse LiDAR scan P, the point in is first replicated times to obtain dense scan = {p1, p2, . . . , pM }. Then, the initial noisy point cloud = {p1 } is calculated by sampling Gaussian noise for each pm based on Eq. (1). Finally, step-by-step denoising process in Eq. (2) is conducted to generate the completed scene G0. , . . . , pM , p2 Gt1 = (cid:18) Gt 1 αt 1 αt 1 αt (cid:19) (cid:0)Gt, P, t(cid:1) ϵθ + σtz (2) 2.2. Score distillation Score distillation shares the same motivation as this paper, aiming to make the few-step distribution of the student model as close as possible to the multi-step distribution of the teacher model. Let pη and pθ be the distribution of the student model and the teacher model, separately. Score Distillaiton amis to minimize the following KL divergence min η DKL (pη (x0) pθ (x0)) (3) 2 Directly solving the optimization problem in Eq. (3) is difficult. Thus, according to Theorem 1 in [31], it is equivalent to the optimization problems in different timesteps Here prompt is omitted for compactness. By approximating the reverse process pη(x1:T x0) with the forward process q(x1:T x0), with some simplification, we have: min η DKL (pη,t (xt) pθ,t (xt)) (4) Thus, the gradient of the student model can be written as ηDKL (pη,t (xt) pθ,t (xt)) = Et,ϵ [xt log pη,t (xt) xt log pθ,t (xt)] (5) xt η Then, the score xt log pθ,t (xt) can be approximated by the pre-trained diffusion model ϵθ, and the score xt log pη,t (xt) can be approximated by an teaching assistant model ϵϕ which trained on the generative samples of the student model with standard diffusion loss. Thus, the gradient in Eq. (5) can be approximated by ηDKL (pη,t (xt) pθ,t (xt)) Et,ϵ [ϵθ(xt, t) ϵϕ(xt, t)] xt η (6) During the training, the student model and the teaching assistant model ϵϕ are optimized alternately. 2.3. brief introduction of Diffusion-DPO 0 , xl This part reviews the Direct Preference Optimization in diffusion models (Diffusion-DPO) [30]. Let = {(c, xw 0} is dataset, where each data sample consists of prompt and pair of images xw 0 with human preference xw 0. The image xw 0 xl 0 are both sampled from references distribution pref . To obtain the reward on the whole diffusion path, r(c, x0) is defined as: 0 and xl 0 and xl (c, x0) = Epη(x1:T x0,c) [R (c, x0:T )] (7) Here pη is diffusion model trained to align with human preferences. Then, pη can be optimized by maximizing the following objective maxpη EcDc,x0:T pη(x0:T c) [r (c, x0)] βDKL [pη (x0;T c) pref (x0:T c)] (8) Compared to traditional DPO [23], the objective function in Eq. (8) is defined over the entire diffusion path x0:T , which amis to maximize the reward (c, x0) while ensuring that the distributions of pη and pref remain as close as possible. The objective in Eq. (8) can be further transformed into the following objective: LDPODiffusion(η) = βExw xL 1:T pη(xw 1:T pη(xl 1:T xw 0 ) 0) 1:T xl (xw 0 ,xt 0)D log σ (cid:20) log pη(xw pref (xω 0:T ) 0:T ) log pη(xl pref (xl 0:T ) 0:T ) (cid:21) (9) 3 L(η) = 0),t,xw (xw 0 ,xl (ϵw ϵη(xw (ϵl ϵη(xl ,xl , t)2 t, t)2 log σ(βT ω(λt) 2 ϵw ϵref (xw 2 ϵl ϵref (xl , t)2 2 t, t)2 2))) (10) is the signal-noise Here = αtx 0 + σtϵ, λt = α2 σ2 ratio, ω(λt) is the weighted function. 3. Method In this section, we introduce the proposed Distillation-DPO. Distillation-DPO aims to use preference-labeled data pairs to distill pre-trained teacher LiDAR scene completion diffusion model into student model, enabling the student model to achieve better completion results with fewer sampling steps. The overall structure of Distillation-DPO is shown in Fig. 2. As shown in Eq. (8), Diffusion-DPO minimizes the KL divergence between the generative distribution pθ and the reference distribution pref over the entire diffusion path x0:T . Therefore, the more sampling steps there are, the lower the efficiency of the optimization. Given spase scan = {p1, p2, ..., pN } and the completed scene G0 = {p1 0, p2 0 }, we first rewrite the optimization objective in Eq. (8) as: 0, ..., pM min η EP,G0 [DKL(pη(G0P))pθ(G0P)) ωr(G0, P)] (11) Here pθ is the pre-trained distribution of the teacher model parameterized by θ, pη is the generative distribution of Gstu parameterized by η. The completed LiDAR scene G0 is generated by Gstu with fewer inference steps based on the sparse LiDAR scan P. However, directly optimizing Eq. (11) is challenging, because the highdensity regions of pη(G0P) are sparse in high-dimensional spaces [31]. According to Theorem 1 in [31], we extend Eq. (11) into an optimization over different time steps, min η EP,Gt,ϵ[DKL(pη,t(GtP))pθ,t(GtP)) ωr(Gt, P)] (12) Here ϵ is random noise, pη,t and pθ,t are the noisy distribution of the student model and the pre-trained teacher model at timestep t, separately. ω is the weight to control preference learning. Noisy completed LiDAR scene Gt = {p1 } is obtained using the point-level noise addition method in Eq. (1). Using Eq. (12) and some algebra, the optimization problem can be written as , ..., pM , min η EP,G0,ϵ,t[log pη,t(GtP) pθ,t(GtP) ωr(Gt, P)] (13) Figure 2. The overall structure of Distillation-DPO. (1) The student model generates the completed scene with different initial noise level λ based on the sparse scan. (2) Choosing the winning sample Gw are input to ϵθ, and Gl θ and ϵl ϕ and ϵl ϵw t. (3) The sparse scan, Gw and losing samples Gl t, separately. (5) The student model is optimized by the DPO gradient. θ are optimized on Gw ϕ. (4) The model ϵw and Gl For Eq. (13), the global optimal solution η is η,t(GtP) = pθ,t(GtP) exp(ωr(Gt, P)) Z(P) Z(P) = EG0,t,ϵpθ,t(GtP) exp(ωr(Gt, P)) Then, the reward function takes the form: r(G0, P) = 1 ω log pη,t(GtP) pθ,t(GtP) + 1 ω log Z(P) (15) Hence, the objective of Distillation-DPO is obtained min η P,Gw ,Gl t,t,ϵ 1 ω [log pη,t(Gl pθ,t(Gl tP) tP) log pη,t(Gw pθ,t(Gw P) P) Similarly, Gw and Gl student model Gstu with completion quality Gw gradient of Gstu can be calculate as (16) represent the completed scenes by t. The Gl Grad(η) = P,Gw ,Gl t,t,ϵ 1 ω [(Gl log pη,t(Gl tP) Gl log pθ,t(Gl tP)) (Gw log pη,t(Gw P) Gw log pθ,t(Gw P)) Gl η Gw η ] Score Gl (17) tP) is apt P) and Gl proximated by the pre-trained teacher diffusion model the score Gw ϵθ. P) and log pη,t(Gw log pθ,t(Gw log pθ,t(Gl Differently, t log pη,t(Gw Gl tant models ϵw P) is approximated by two teaching assisϕ and ϵl ϕ. Therefore, the gradient of Gstu is (14) Grad(η) =E P,Gw ,Gl t,t,ϵ 1 ω (ϵθ(Gw , t, P) ϵϕ(Gw [(ϵθ(Gl t, t, P) ϵϕ(Gl Gw η , t, P)) ] t, t, P)) Gl η (18) To generate preference-aware completed scenes Gw 0 and Gl 0, we first introduce parameter λ when computing GT , which controls the initial noise scale, ] = pm + λ pm 1 αT ϵT (19) By default, λ = 1. To generate completed scene Gw 0 and Gl 0 separately based on the same sparse scan P, we obtain different completion results by adjusting different values of λ. We set λ > 1 to obtain different from GT , which is then used to generate 0 different from G0. Then, according to the completion quality metrics, we assign the sample with the higher quality as Gw 0 and another as Gl 0. During the training process, the student model Gstu and two teaching assistant models ϵw ϕ are optimized alternately. The teaching assistant models ϵw ϕ and ϵl ϕ are trained on the completed scene generated by Gstu with the standard diffusion objective [12] ϕ and ϵl LDM = EP,t,ϵ (cid:104)(cid:13) (cid:13)ϵ ϵi ϕ 4 (cid:0)Gi t, P, t(cid:1)(cid:13) (cid:13) 2(cid:105) {w, l} (20) 4. Experiment Model and datasets We use the existing SOTA 3D LiDAR scene completion diffusion model LiDiff [21] as the teacher and train few-step student model with Eq. (18). LiDiff can achieve complete scene with 50 sampling steps based on the sparse LiDAR scan. The student model and the teaching assistant models ϵw ϕ are initialized with the pre-trained LiDiff model, but the student model performs scene completion with fewer sampling steps. The experiments are conducted on the SemanticKITTI [2] dataset. ϕ and ϵl Baselines and metrics Except for the existing SOTA LiDAR scene completion diffusion model LiDiff [21], we also choose LMSCNet [25], LODE [14], MID [29] and PVD [41] as the baselines. We evaluate the performance of the proposed Distillation-DPO on Chamfer Distance (CD) [3], Jensen-Shannon Divergence (JSD) [19] and Earth Movers Distance (EMD) [8]. These three metrics can provide comprehensive evaluation of the completed LiDAR scene quality from different perspectives. 4.1. Evaluation on LiDAR scene completion We first compared the performance of the proposed Distillation-DPO and existing models in LiDAR scene completion on the SemanticKITTI dataset. According to different settings, Distillation-DPO can perform sampling with different inference steps. As the sampling steps decrease, the scene completion speed increases, but it inevitably sacrifices some completion quality. After balancing completion speed and quality, we chose the result with 8 sampling steps as the completion output of Distillation-DPO for comparison with existing models. In Sec. 4.2, we further compare the performance of Distillation-DPO under different sampling steps. The comparison results of Distillation-DPO are shown in Tab. 1. Distillation-DPO achieves the optimal completion quality except in EMD. Compared with the SOTA LiDAR scene completion method LiDiff [21], DistillationDPO accelerates the completion speed by over 5 times while achieving improvements of 6% and 7% in CD and JSD. As for EMD, Distillation-DPO still maintains comparable performance compared with the existing method. Although the sampling speed of Distillation-DPO is slower than LMSCNet [25], LODE [14], the sampling quality has been significantly improved. 4.2. Ablation study Model LMSCNet [25] LODE [14] MID [29] PVD [41] LiDiff [21] LiDiff (Refined) [21] Distillation-DPO Distillation-DPO (Refined) CD JSD EMD Times (s) 0.641 1.029 0.503 1.256 0.434 0.375 0.414 0.354 0.431 0.451 0.470 0.498 0.444 0.416 0.419 0.387 - - - - 22.15 23.16 23.29 23. - - - - 17.75 17.87 3.28 3.38 Table 1. The results on LiDAR scene completion of DistillationDPO with existing models. Colors denote the 1st , 2nd , and means the completion time 3rd best-performing model. is calculated based on the official implementation and released checkpoints. Here Lidiff takes 50 NFEs while ours takes 8 only. Model NFE CD JSD EMD Time (s) LiDiff [21] LiDiff (Refined) [21] LiDiff [21] LiDiff (Refined) [21] Distillation-DPO (Refined) Distillation-DPO (Refined) Distillation-DPO (Refined) Distillation-DPO (Refined) 50 50 8 8 8 4 2 1 0.434 0.375 0.447 0.411 0.354 0.429 0.475 0. 0.444 0.416 0.432 0.406 0.387 0.413 0.398 0.430 22.15 23.16 24.90 25.74 23.66 24.24 25.30 28.11 17.75 17.87 3.35 3.48 3.38 1.84 1.08 0. Table 2. Comparison results of different inference steps on the SemanticKITTI dataset. seconds to complete scene. However, the reduction in inference steps leads to decline in completion quality. The speed improvement gained from sampling steps reduction is not enough to compensate for the loss in quality. Therefore, choosing 8 steps by default is good balance of speed and efficiency. Then, we further compare the completion quality of different values of λ. In the implementation of DistillationDPO, we set λ = 1.1 by default to calculate . Here, we use different λ values to train Distillation-DPO and compare the results in Tab. 3. When decreasing or increasing λ, the completion performance deteriorates. When λ is small, the difference between Gw 0 is minimal, making the gradients of the student model in Eq. (18) small, which leads to unstable training. Conversely, when λ is large, the quality of degrades significantly, causing it to fall outside the distribution learned by the pre-trained teacher model ϵθ. This mismatch leads to inaccurate predictions from ϵθ [39], resulting in incorrect gradients for the student model and ultimately lowering the completion quality. 0 generated from 0 and Gl In this part, we first show the completion results of Distillation-DPO with different inference steps. Tab. 2 shows the results. As the number of inference steps decreases, the completion speed of Distillation-DPO is further reduced. With just one sampling step, it only takes 0. We also conducted experiments to explore the impact of different teacher model performances on the effectiveness of Distillation-DPO. Theoretically, the final performance of the student model is constrained by the teacher model. The better the performance of the teacher model, the better the"
        },
        {
            "title": "Model",
            "content": "CD JSD EMD"
        },
        {
            "title": "Model",
            "content": "CD JSD EMD λ = 1.1 (ours) λ = 1.05 λ = 1.2 λ = 1.5 λ = 2.0 0.354 0.418 0.421 0.409 0.427 0.387 0.421 0.423 0.422 0.432 23.66 23.48 23.44 23.60 23. Table 3. Comparison results of different λ value SemanticKITTI dataset. All results have been refined."
        },
        {
            "title": "Model",
            "content": "CD JSD EMD LiDiff [21] LiDiff Distillation-DPO Distillation-DPO 0.375 0.368 0.354 0.343 0.416 0.401 0.387 0.385 23.16 22.69 23.66 23. Table 4. Comparison results of using different teacher models. LiDiff represents the LiDiff model refined with Diffusion-DPO and it enjoys boosted performance. Distillation-DPO represents the Distillation-DPO trained with LiDiff. With stronger teacher, the distillated student also have better performance. All results have been refined. Model CD JSD EMD Distillation-DPO (CD) Distillation-DPO (JSD) 0.354 0. 0.387 0.445 23.66 24.82 Table 5. Comparison results of using different metrics to determine Gw 0. All results have been refined. 0 and Gl final performance of the student model. Thus, we first finetuned LiDiff [21] using DiffusionDPO [30] to enhance its performance. Then, we retrained Distillation-DPO using the fine-tuned model. Results shown in Tab. 4 display that as the performance of the teacher model improves, the performance of Distillation-DPO also improves. 0 and Gl Moreover, we conduct experiments by changing the evaluation metric for determining Gw 0 to JSD. The results in Tab. 5 show that the performance significantly deteriorates when using JSD. Since JSD measures the similarity of point cloud distributions, it requires large number of samples to estimate the probability density distribution accurately. However, when comparing and determining whether sample is Gw 0, the metric is computed using only single generated sample and its corresponding ground truth. In this case, JSD becomes inaccurate and may even lose its practical significance, leading to the performance decline. 0 and Gl Finally, we further compare Distillation-DPO with results distilled using traditional score distillation methods to validate the effectiveness of the proposed distillation framework. Tab. 6 shows that the results obtained using score"
        },
        {
            "title": "0.375\nLiDiff [21]\nScore Distillation\n0.419\nDistillation-DPO 0.354",
            "content": "0.416 0.430 0.387 23.16 24.61 23.66 Table 6. Comparison between Distillation-DPO and traditional score distillation. All results have been refined. distillation are even inferior to those of the original teacher model LiDiff [21]. This is consistent with our statement in Sec. 1 that directly employing score distillation can accelerate the sampling speed while inevitably leading to drop in performance. In contrast, the proposed Distillation-DPO distillation framework incorporates guidance from preference data, which not only accelerates sampling but also further enhances completion quality, thereby achieving efficient and high-quality scene completion. 4.3. Qualitative comparison We visualized the scene completion results of DistillationDPO and compared them with those of the SOTA model LiDiff [21], as shown in the Fig. 3. Compared to LiDiff, Distillation-DPO achieves higher scene completion quality with only 8 sampling steps, surpassing LiDiffs results even with 50 sampling steps. Moreover, Distillation-DPO provides more complete reconstructions of fine details, such as cars, road cones, and signposts. 5. Discussion 5.1. Rationality of the student model initialization As in Sec. 4, the student model is initialized from the pre-trained teacher diffusion model ϵθ. This initialization approach is feasible and commonly used in existing methods [16, 37, 38]. Although the student model and the teacher model share the same initial parameters, the student model uses fewer sampling steps than the teacher model. As result, at the beginning, the student model performs worse in few-step sampling compared to the teacher models multi-step sampling. The generated distribution of the student model differs from the pre-trained distribution of the teacher model. The objective of Distillation-DPO aligns with that of traditional score distillation: to ensure that the few-step sampling distribution of the student model closely matches the multi-step sampling distribution of the teacher model. This allows the student model to achieve comparable or even high performance with fewer sampling steps. 5.2. Similarities and differences with Diffusion-"
        },
        {
            "title": "DPO",
            "content": "Similarities Both Diffusion-DPO [30] and the proposed Distillation-DPO use preference data pairs to optimize the Figure 3. Qualitative results on SemanticKITTI. Compared to LiDiff [21], Ditillation-DPO achieves faster and higher-quality completion. model and maximize the reward. Differences First, the minimized KL divergences are different. Diffusion-DPO minimizes the joint distribution over the entire diffusion path x0:T , i.e., the KL divergence between generative distribution pη(x0:T c) and the reference distribution pref (x0:T c). Distillation-DPO minimizes the KL divergence between the student models generative distribution and the training distribution, which is reformulated as the KL divergence between the noised distributions at different timestep t. Since the training distribution is not accessible, Distillation-DPO approximates the training distribution using pre-trained diffusion model. Therefore, the optimization objective of Distillation-DPO is transformed into minimizing the KL divergence between the student models generative distribution pη(GtP) and the teacher models pre-trained distribution pθ(GtP). Second, the optimization strategies are different. Diffusion-DPO directly optimizes the generative model pη by Eq. (10). Distillation-DPO first calculates the score difference of 0 between the teaching assistant ϕ and the teacher model ϵθ, as well as the score the winning sample Gw model ϵw difference of the losing sample Gl 0 between the teaching assistant model ϵl ϕ and the teacher model ϵθ. These two components are then combined as the gradient to optimize the student model Gstu. The teaching assistant models ϵw ϕ and ϵl based on the diffusion loss. The student Gstu and the teaching assistant models are optimized alternately. ϕ are optimized separately on Gw 0 and Gw Third, the training policies are different. Diffusion-DPOs training is off-policy. Diffusion-DPO samples the preference data pair from reference distribution pref , which are predefined before training begins and remain unchanged throughout the training process. Distillation-DPOs training is on-policy. DistillationDPO generates the preference data pairs by the student model in each optimization step, which is changed with the optimization of the student model during the training. Finally, the sampling steps are different. Diffusion-DPO requires the sampling steps of the generative model to be consistent with those of the reference model during the training. Distillation-DPO does not require the student model to have the same number of sampling steps as the teacher model during training. The student model directly conducts single-step sampling during the training. 7 5.3. Similarities and differences with Score Distillation Similarities Both methods share the training objective of making the few-step distribution of the student model as close as possible to the multi-step distribution of the teacher. Differences The training strategies are different. Score Distillation training the student model by the difference between two score functions without preference data pairs. Score Distillation only has one teaching assistant model ϵϕ to approximate the generative distribution of the student model. Distillation-DPO calculates the score function differences on preference data pairs Gw 0 separately and combines two terms as the gradient to optimize the student model. Distillation-DPO has two teaching assistant ϕ to approximate the distribution of Gw model ϵw 0 and Gl ϕ and ϵl 0 separately. 0 and Gl 5.4. Differential Rewards The rationale for employing 3D LiDAR scene completion evaluation metrics as preference signals stems from three compelling arguments. First, our approach diverges from direct reward optimization frameworks by leveraging these metrics exclusively for constructing preferencebased winning-losing pairs. This methodology emulates human preference annotation paradigm, where evaluative criteria guide pairwise comparisons rather than serving as differentiable objectives. Such indirect alignment circumvents the risk of metric exploitation. Second, prior works have demonstrated the feasibility of post-training optimization or test-time metric maximization using these criteria [7, 17, 26], with reported performance gains validating their efficacy. Our methodology extends this convention through indirect utilization. Third, experiments employing Chamfer Distance and Jensen-Shannon Divergence as training signals demonstrated consistent performance improvements across other evaluation metrics. 6. Related Work 6.1. Preference Optimization for Diffusion Models To generate results that better align with human preferences, some studies have begun to train models based on preference-optimization methods [15, 22, 23]. ImageReward [33] proposes the first general human preference reward model for text-to-image generation and directly optimizes the diffusion model based on feedback during random subsequent denoising steps. Subsequent studies have further leveraged more detailed annotation methods [15] and combined multiple open-source models [40] to obtain richer human feedback datasets. Additionally, some works have optimized reward feedback learning by integrating multiple reward models [10] or improving training methodologies [40]. Since obtaining large-scale human annotations is challenging, some methods have attempted to train reward models using semi-supervised learning with unlabeled data [11] or employing hybrid annotation strategies with AI and human [18]. Additionally, Diffusion-DPO [30] is the first to extend Direct Preference Optimization [23] to diffusion models, directly optimizing the model based on image preferences to eliminate the complex reward modeling and improve training efficiency. 6.2. LiDAR Scene Completion LiDAR scene completion aims to reconstruct sparse LiDAR scans into dense and complete 3D point cloud scenes [39]. Traditional LiDAR scene completion methods recover dense depth maps from sparse point clouds [9, 34], leveraging guidance from RGB images or birds-eye view images to achieve high-quality completion [6, 36]. Some methods represent LiDAR scenes as voxels and utilize Signed Distance Fields (SDFs) to reconstruct complete point cloud scenes [14]. However, the completion quality of these methods is constrained by the voxel resolution [21]. Due to the high generative quality and strong training stability, many studies have recently leveraged diffusion models for high-quality LiDAR scene completion [5, 20, 28, 39]. Some methods focus on reconstructing sparse LiDAR scans into dense scans, such as R2DM [20], OLiDM [35], and LiDMs [24]. Other approaches attempt to directly recover complete point cloud scenes from sparse LiDAR scans, including LiDiff [21] and DiffSSC [5]. To further accelerate LiDAR scene completion speed, ScoreLiDAR introduces distillation method based on structural loss, enabling fast and efficient LiDAR point cloud completion [39]. 7. Conclusion distillation diffusion model Summary This paper proposes novel LiDAR scene framework, completion Distillation-DPO. Distillation-DPO redefines the DiffusionDPO framework by introducing the score distillation strategy, enabling effective distillation of LiDAR scene completion diffusion models using preference data pairs. Compared to existing models, Distillation-DPO achieves new SOTA completion performance while improving completion speed more than five times over existing SOTA models. To our best knowledge, we are the first to integrate distillation and post-training with preference and provide insight to preference-aligned diffusion distillation for both areas of LiDAR scene completion and visual generation. Limitation Since the official implementations and models of SOTA diffusion-based semantic scene completion (SSC) models, such as DiffSSC [5], are not yet publicly available, Distillation-DPO has not yet been evaluated on the SSC task. Future work will explore its application in the SSC task. Additionally, while Distillation-DPO improves the sampling speed of existing models by over 5 times, it still does not achieve real-time LiDAR scene completion. Future work will focus on further accelerating the completion process without compromising quality, aiming to achieve real-time high-quality scene completion."
        },
        {
            "title": "References",
            "content": "[1] Kyungryul Back, XinYu Piao, and Jong-Kook Kim. Enhancing Reinforcement Learning Finetuned Text-to-Image GenIn International erative Model Using Reward Ensemble. Conference on Intelligent Tutoring Systems, pages 213224. Springer, 2024. 2 [2] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: Dataset For Semantic Scene Understanding Of Lidar Sequences. In Proceedings of the IEEE/CVF international conference on computer vision, pages 92979307, 2019. 5 [3] Akmal Butt and Petros Maragos. Optimum Design of Chamfer Distance Transforms. IEEE Transactions on Image Processing, 7(10):14771484, 1998. 5 [4] Shengqu Cai, Eric Ryan Chan, Songyou Peng, Mohamad Shahbazi, Anton Obukhov, Luc Van Gool, and Gordon Wetzstein. DiffDreamer: Towards Consistent Unsupervised Single-View Scene Extrapolation With Conditional Diffusion Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21392150, 2023. 2 [5] Helin Cao and Sven Behnke. DiffSSC: Semantic LiDAR Scan Completion Using Denoising Diffusion Probabilistic Models. arXiv preprint arXiv:2409.18092, 2024. 8 [6] Yun Chen, Bin Yang, Ming Liang, and Raquel Urtasun. Learning Joint 2d-3d Representations for Depth Completion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1002310032, 2019. 8 [7] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. 2, [8] Haoqiang Fan, Hao Su, and Leonidas Guibas. Point Set Generation Network for 3d Object Reconstruction From In Proceedings of the IEEE conference on Single Image. computer vision and pattern recognition, pages 605613, 2017. 2, 5 [9] Chen Fu, Christoph Mertz, and John Dolan. Lidar and Monocular Camera Fusion: On-road Depth Completion For Autonomous Driving. In 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pages 273278. IEEE, 2019. 8 [10] Jianshu Guo, Wenhao Chai, Jie Deng, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, and Gaoang Wang. Versat2i: Improving Text-to-Image Models With Versatile Reward. arXiv preprint arXiv:2403.18493, 2024. 8 [11] Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, and Han Zhao. Semi-Supervised Reward Modeling Via Iterative Self-Training. arXiv preprint arXiv:2409.06903, 2024. 8 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems, pages 68406851, 2020. 2, 4 [13] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. DreamPose: Fashion In Proceedings of Video Synthesis With Stable Diffusion. the IEEE/CVF International Conference on Computer Vision, pages 2268022690, 2023. [14] Pengfei Li, Ruowen Zhao, Yongliang Shi, Hao Zhao, Jirui Yuan, Guyue Zhou, and Ya-Qin Zhang. Lode: Locally Conditioned Eikonal Implicit Scene Completion From Sparse Lidar. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 82698276. IEEE, 2023. 5, 8 [15] Youwei Liang, Junfeng He, Gang Li, Peizhao Li, Arseniy Klimovskiy, Nicholas Carolan, Jiao Sun, Jordi Pont-Tuset, Sarah Young, Feng Yang, et al. Rich Human Feedback for Text-to-Image Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1940119411, 2024. 8 [16] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models. In Advances in Neural Information Processing Systems, page 7652576546, 2023. 2, 6 [17] Nanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, YuChuan Su, Mingda Zhang, Xuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al. Inference-Time Scaling For Diffusion Models Beyond Scaling Denoising Steps. arXiv preprint arXiv:2501.09732, 2025. 8 [18] Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative Reward Models. arXiv preprint arXiv:2410.12832, 2024. 8 [19] Marıa Luisa Menendez, JA Pardo, Pardo, and MC Pardo. The Jensen-Shannon Divergence. Journal of the Franklin Institute, 334(2):307318, 1997. 5 [20] Kazuto Nakashima and Ryo Kurazume. LiDAR Data SynIn thesis With Denoising Diffusion Probabilistic Models. 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1472414731. IEEE, 2024. 8 [21] Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, and Cyrill Stachniss. Scaling Diffusion Models To Real-World 3D LiDAR Scene Completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1477014780, 2024. 1, 2, 5, 6, 7, 8 [22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training Language Models To Follow Instructions with Human Feedback. Advances in neural information processing systems, 35:27730 27744, 2022. [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct 9 [35] Tianyi Yan, Junbo Yin, Xianpeng Lang, Ruigang Yang, Cheng-Zhong Xu, and Jianbing Shen. OLiDM: ObjectAware LiDAR Diffusion Models for Autonomous Driving. arXiv preprint arXiv:2412.17226, 2024. 8 [36] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense Depth Posterior (DDP) From Single Image And Sparse In Proceedings of the IEEE/CVF Conference on Range. Computer Vision and Pattern Recognition, pages 3353 3362, 2019. 8 [37] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved Distribution Matching Distillation for Fast Image Synthesis. arXiv preprint arXiv:2405.14867, 2024. 2, 6 [38] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step Diffusion With Distribution Matching Distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 2, 6 [39] Shengyuan Zhang, An Zhao, Ling Yang, Zejian Li, Chenye Meng, Haoran Xu, Tianrun Chen, AnYang Wei, Perry Pengyun GU, and Lingyun Sun. Distilling diffusion models to efficient 3d lidar scene completion. arXiv preprint arXiv:2412.03515, 2024. 5, [40] Xinchen Zhang, Ling Yang, Guohao Li, Yaqi Cai, Jiake Xie, Yong Tang, Yujiu Yang, Mengdi Wang, and Bin Cui. Itercomp: Iterative Composition-aware Feedback Learning From Model Gallery For Text-to-Image Generation. In International Conference on Learning Representations, 2025. 2, 8 [41] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d Shape Generation And Completion Through Point-voxel Diffusion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58265835, 2021. 5 [42] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-Video: TemporalConsistent Diffusion Model For Real-World Video SuperIn Proceedings of the IEEE/CVF Conference Resolution. on Computer Vision and Pattern Recognition, pages 2535 2545, 2024. 2 Preference Optimization: Your Language Model is Secretly Reward Model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. 2, 3, 8 [24] Haoxi Ran, Vitor Guizilini, and Yue Wang. Towards Realistic Scene Generation With LiDAR Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1473814748, 2024. 8 [25] Luis Roldao, Raoul de Charette, and Anne Verroust-Blondet. Lmscnet: Lightweight Multiscale 3d Semantic Completion. In 2020 International Conference on 3D Vision (3DV), pages 111119. IEEE, 2020. 5 [26] Raghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen McKeown, and Rajesh Ranganath. general Framework For Inference-Time ScalarXiv preprint ing And Steering Of Diffusion Models. arXiv:2501.06848, 2025. [27] Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, and Thomas Funkhouser. Semantic Scene ComIn Proceedings of pletion From Single Depth Image. the IEEE/CVF conference on computer vision and pattern recognition, pages 17461754, 2017. 2 [28] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. Gecco: Geometrically-Conditioned Point Diffusion Models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21282138, 2023. 8 [29] Ignacio Vizzo, Benedikt Mersch, Rodrigo Marcuzzi, Louis Wiesmann, Jens Behley, and Cyrill Stachniss. Make It Dense: Self-Supervised Geometric Scan Completion of Sparse 3d Lidar Scans In Large Outdoor Environments. IEEE Robotics and Automation Letters, 7(3):85348541, 2022. 5 [30] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion Model In ProAlignment Using Direct Preference Optimization. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. 2, 3, 6, 8 [31] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity And Diverse Text-to-3D Generation With Variational Score Distillation. In Advances in Neural Information Processing Systems, page 84068441, 2023. 3 [32] Bin Xia, Yulun Zhang, Shiyin Wang, Yitong Wang, Xinglong Wu, Yapeng Tian, Wenming Yang, and Luc Van Gool. DiffIR: Efficient Diffusion Model For Image Restoration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1309513105, 2023. 2 [33] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning And Evaluating Human Preferences For Text-toImage Generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 2, [34] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth Completion from Sparse LiIn Proceedings dar Data With Depth-Normal Constraints. of the IEEE/CVF International Conference on Computer Vision, pages 28112820, 2019."
        }
    ],
    "affiliations": [
        "Peking University",
        "Zhejiang Green Zhixing Technology co., ltd",
        "Zhejiang University"
    ]
}