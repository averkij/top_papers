{
    "paper_title": "Generating Multi-Image Synthetic Data for Text-to-Image Customization",
    "authors": [
        "Nupur Kumari",
        "Xi Yin",
        "Jun-Yan Zhu",
        "Ishan Misra",
        "Samaneh Azadi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose a simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create a high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose a new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose a new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuning-free methods on standard customization benchmarks."
        },
        {
            "title": "Start",
            "content": "Generating Multi-Image Synthetic Data for Text-to-Image Customization Nupur Kumari1 Xi Yin2 Jun-Yan Zhu1 Ishan Misra2 Samaneh Azadi 1Carnegie Mellon University 2Meta 5 2 0 2 3 ] . [ 1 0 2 7 1 0 . 2 0 5 2 : r Figure 1. (a) We propose new pipeline for synthetic training data generation consisting of multiple images of the same object under different lighting, poses, and backgrounds. (b) Given the high-quality training data, we propose new encoder-based model customization method. During inference, our method can successfully generate new compositions of reference object using text prompts."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Customization of text-to-image models enables users to insert custom concepts and generate the concepts in unseen settings. Existing methods either rely on costly test-time optimization or train encoders on single-image training datasets without multi-image supervision, leading to worse image quality. We propose simple approach that addresses both limitations. We first leverage existing text-to-image models and 3D datasets to create high-quality Synthetic Customization Dataset (SynCD) consisting of multiple images of the same object in different lighting, backgrounds, and poses. We then propose new encoder architecture based on shared attention mechanisms that better incorporate fine-grained visual details from input images. Finally, we propose new inference technique that mitigates overexposure issues during inference by normalizing the text and image guidance vectors. Through extensive experiments, we show that our model, trained on the synthetic dataset with the proposed encoder and inference algorithm, outperforms existing tuningfree methods on standard customization benchmarks. Please find the code and data at our website. Text-to-image models are capable of generating high-fidelity and realistic images given only text prompt [19, 55, 61, 63]. Yet, text often falls short of describing rich visual details of real-world objects, such as the unique toy in Figure 1. What if the user wishes to generate images of the toy in new scenarios? This has given rise to the emerging field of model customization or personalization [10, 21, 37, 62, 78], allowing us to generate new compositions of the object via text prompts, e.g., the toy in wheat field, as shown in Figure 1. Early works [21, 37, 79] require many optimization steps on user-provided images of every single new object, process both costly and slow. To address this, several encoder-based methods [10, 39, 69, 78, 83] train the model with reference images as an additional input. During inference, these tuning-free methods can generate new compositions of the reference object in single forward pass without expensive per-object optimization. However, the lack of dataset comprising multiple images of the same object in diverse poses, backgrounds, and lighting conditions has been major bottleneck in developing these methods. Collecting such large-scale datasets from 1 the internet is difficult, as real images are often not annotated with object identity. In this work, we aim to address this data shortage challenge using new synthetic dataset generation method. This is challenging, as we need to maintain the object identity while generating multiple images with varying contexts. To achieve this, our first idea is to employ shared attention among foreground object regions in multiple generated images, ensuring visual consistency across different images. Furthermore, to ensure 3D multi-view consistency for rigid objects, we use Objaverse [14] assets as 3D prior. Specifically, we use depth guidance and cross-view correspondence between different renderings to promote object consistency further. Finally, we automatically filter out any low-quality and inconsistent object images. Given our synthetically curated dataset, SynCD, we propose new training and inference methods for tuning-free customization. We borrow the idea of shared attention from our dataset generation pipeline and incorporate it in the training method as well. This conditions the generation on fine-grained features of input reference images, improving object identity preservation. During inference with classifierfree guidance, we propose normalization technique that better incorporates both text and image conditions without overexposure issues. Extensive experiments show that our full method using the new dataset outperforms tuning-free customization baselines like JeDi [85], Emu-2 [71], and IPAdapter [83]. Compared to standard tuning-based methods, it performs on par with identity preservation while better following the input text prompt. 2. Related Works Text-to-image models. With recent advancements in training methods [16, 27, 33, 34, 44, 65, 84], model architectures [19, 32, 55, 58, 61], and datasets [66], text-conditioned generative models have excelled at photorealistic generation while adhering to text prompts. Primarily among them are diffusion [45, 61] and flow [19, 38] based models, commonly trained in an encoded latent space [18, 61]. Their impressive generalization capability has enabled diverse applications [22, 23, 25, 26, 30, 48, 49, 53, 60]. However, text as modality can often be imprecise. This has given rise to various works on improving text alignment [8, 22, 42] and increasing user-control via additional image conditions [9, 86]. Image-conditioned generation aims to enhance control of the generation process by incorporating additional inputs, such as depth and segmentation maps [4, 35, 86, 88] or layout conditions [5, 40, 56, 89] to control the spatial structure. Another use-case is to condition the generation on context images [9, 50], either retrieved from dataset or given by the users themselves. These images provide relevant context apart from the text prompt to help guide the generation. Customizing text-to-image models. particular case of image-conditioned generation is the task of model customization or personalization [21, 37, 62], which aims to precisely learn the concept shown in reference images, such as pets or personal objects, and compose it with the input text prompt. Early works in model customization fine-tune subset of model parameters [24, 28, 37, 73] or text token embeddings [2, 21, 76, 87] on the few user-provided reference images with different regularization [37, 62]. However, this fine-tuning process for every new concept is both time-consuming and computationally expensive. Thus, many works now focus on training tuning-free methods, which do not require per-object optimization during inference. Tuning-free methods for customization add an additional image condition to the diffusion model in addition to the text prompt. To achieve this, many of the methods use pre-trained feature extractors to embed reference images into visual embeddings [11, 39, 54, 69, 78, 80], which are then mapped to text token embedding space. Some recent methods have also proposed learning mapper between multimodal autoregressive models [75] and generative models to incorporate reference images as visual prompts [52, 71]. Another commonly adopted design is the decoupled text and image crossattention [47, 78, 83]. Our training method is also motivated by this, but we also propose to insert fine-grained features via shared self-attention. However, most existing methods still rely on single-image or multi-view datasets for training, with the same or limited background diversity. The reference images are encoded in compact feature space to prevent overfitting [39, 69], hurting identity preservation. To address this, we propose new methods for creating synthetic dataset containing multiple images of the same object while having background and pose diversity. Our method is motivated by recent works in consistent character [74, 91] and multi-view generation [15, 67, 68], but tailored for the model customization task. While recently Zeng et al. [85] also aims to create synthetic dataset, they rely on text prompting alone to generate collage images with similar objects. In contrast, our dataset curation method uses explicit constraints for object consistency, resulting in higher-quality training data. 3. SynCD: Synthetic Customization Dataset Training tuning-free customization models requires diverse dataset of different objects, each with multiple images in different contexts. To address the data shortage and collection challenge, we introduce an automatic data curation pipeline for synthesizing diverse, high-quality image corpora. This section outlines our data generation pipeline with two main components: (1) Creating set of prompts per object and (2) Creating coupled images with consistent object given the prompts. For the objects, we sample approximately 75, 000 rigid category assets from Objaverse [13] and 16 deformable categories mainly belonging to pet ani2 Figure 2. Dataset Generation Pipeline. Top: For deformable objects like cats, we use an object description along with set of background descriptions, both suggested by an LLM, as input to generate multiple images with common object. Bottom: For rigid objects, we use depth from multi-view rendered images of the 3D asset [13], object description from Cap3D [46], and background prompts suggested by the LLM as input to our method consisting of depth-conditioned [86] text-to-image model. We use Masked Shared Attention (MSA) and warping (in the case of rigid objects) to promote object consistency, as shown in Figure 3. mals, as personalizing on pets is common use case. We use the Instruction-tuned LLama3 [17] and the Flux model [38] for generating these coupled images with the same object, each with distinct backgrounds and pose variations. 3.1. LLM assisted prompt generation We design each prompt to have detailed description of both the object and the background, as having detailed description of the object already helps enhance consistency. In the case of Objaverse, Cap3D [46] provides detailed captions for each asset, e.g., large metal drum with blue and pink stripes. For deformable objects, we instruct the LLM to generate descriptive captions, e.g., The Russian blue cat has thick plush coat. Based on the object description, we instruct the LLM [17] to generate plausible background scene descriptions. We then combine common object description with multiple background descriptions for multi-image generation, as shown in Figure 2. We provide the instructions used to prompt the LLM in Appendix C. 3.2. Multi-image consistent-object generation Using the LLM-suggested prompts in the previous step, we aim to generate multiple images of common object. To enforce object consistency for both deformable and rigid objects, we share the latent features across the images during the denoising process via Masked Shared Attention (MSA) mechanism [67, 74]. For rigid objects, we further leverage depth and multi-view correspondence derived from Objaverse 3D assets. We explain both steps below. Masked Shared Attention (MSA). We modify the attention block of the diffusion model such that each image attends to itself as well as the foreground object regions Figure 3. Feature warping and Masked Shared Attention (MSA) for object consistency. For rigid objects, we first warp corresponding features from the first image to the other. Then, each image feature attends to itself, and the foreground object features in other images. We show an example mask, M1, used to ensure this for the first image when generating two images with the same object. of the other images. Given query, key, and value features, qi, ki, vi Rnd , of the ith image, shared attention block performs the following operation: MSA({qi, ki, vi}N (cid:110)"
        },
        {
            "title": "Softmax",
            "content": "(cid:16) qi[k1 kN ]T (cid:17) + Mi i=1) [v1 vN ] (cid:111)N i=1 (1) , where is the feature dimension and is the sequence length. Each qi attends over the features, and the mask, i.e., attention bias matrix Mi Rn(N n) ensures that the i-th image feature only attends to the object region of other images and ignores their background. Since DiT model [55] consists of joint text and image attention, the mask Mi is initialized so that text tokens of one image do not attend to other image tokens, as shown in Figure 3. MSA enables us to generate with similar visual features among all the images. However, MSA does not explicitly enforce 3D multi-view consistency, as qualitatively shown in the Appendix, Figure 16. Therefore, for rigid objects with available 3D datasets like Objaverse, we use these assets to ensure multi-view consistency, as described next. Rigid object generation with MSA and 3D consistency. Given an Objaverse asset, we render it from varying camera poses and feed the rendered depth map and captions generated in Section 3.1 to depth-conditioned Flux model. During denoising timesteps, Masked Shared Attention (MSA) is applied across all the images using the ground truth masks from the rendered depth map. Depth guidance ensures 3D shape consistency of the object across the images, while MSA encourages similar visual appearance. Given the 3D mesh and relative camera poses, we establish pixelwise cross-view correspondence for locations visible in both views. We then enhance multi-view consistency further by warping the corresponding features from the first image to the other images, as shown in Figure 3. Thus, given latent features fi R(hw)d, {1 }, the query, key, and value features input to MSA are calculated as: ˆfi(h, w) = αf1(Ci(h, w)) + (1 α)fi(h, w), ˆqi = Wq ˆfi, ˆki = Wk ˆfi, ˆvi = Wv ˆfi, (2) where for given pixel (h, w) in the ith image, Ci(h, w) denotes its corresponding location in the first image, α is the visibility mask, f1(Ci(h, w)) is the corresponding feature in the first image computed with bilinear interpolation, and Wq, Wk, Wv are query, key, value projection matrics. We only apply this warping during the initial diffusion timesteps. This increases multi-view consistency without introducing warping artifacts and allows flexibility for lighting variations. Figure 2 shows the overall framework of our dataset generation pipeline, where we use as 3. We also use aesthetic score [1] and object similarity via DINOv2 [51] feature space to remove low-quality and inconsistent images to get final dataset of 95K objects with 2-3 images per object, uniformly distributed among rigid and deformable categories. Discussion. Our key insight is that synthesizing such dataset with consistent object identities, using internal feature sharing and external 3D guidance, is far more scalable than collecting real-world data. Moreover, generating such data is also more tractable than the task of model customization with real images, where access to the internal features and the objects true 3D geometry is not easily available. Figure 4. Training Method. We condition the model on reference images via two pathways (1) Masked Shared Attention, similar to Figure 3, on fine-grained reference features extracted from the pre-trained text-to-image model with the same timestep, t, forward noise added to it as target xt 1 and (2) Decoupled image crossattention [83] on global image feature extracted from CLIP [57]. 4. Our Customization Model 4.1. Diffusion Learning Objective text-to-image diffusion model aims to learn the distribution of images given text prompt, i.e., p(xc), where is real image corresponding to the text prompt c. The model consists of forward Markov process in which the real sample is gradually transformed to random noise xT (0, I) in timesteps. During training, the model learns to denoise the input noisy image xt = 1 αtϵ given the text prompt via the following loss function: αtx + Ext,t,c,ϵN (0,I)[wtϵ ϵθ(xt, t, c)], (3) where ϵ is the input noise, is the current timestep, αt determines the noising ratio, ϵθ is the predicted noise, and wt is weighing function. Other formulations of the training objective also exist for more stable training. Specifically in this work, we use the 1 αtx prediction objective instead of ϵ as proposed by Salimans et al. [64]. αtϵ 4.2. Architecture Design and Training tuning-free image customization method aims to learn p(xc, {(xi, ci)}K i=1), i.e., the distribution of images aligned with both the input text prompt and object identity as shown in the reference images. To train such model, we use the coupled images of an object generated in the previous step and consider one of them as the target and the rest as references to condition the generation process. Reference image conditioning. Our base model is an image-conditioned text-to-image diffusion model [83], which performs image cross-attention with global feature extracted from CLIP [57] for conditioning on an image. To condition the generation on multiple reference images, motivated by our dataset generation pipeline, we propose to additionally employ Masked Shared Attention with finegrained features of the reference images. Specifically, we use 4 Figure 5. Results. We compare our method qualitatively against other leading tuning-free baselines with single reference image as input. We can successfully incorporate the text prompt while preserving the object identity similar to or higher than the baseline methods. We pick the best out of 4 images for all methods. More qualitative samples are shown in Figure 19 and 20 in the Appendix. the forward Markov process to add the same timestep noise to the reference images as the target image and extract their features from the base model. Similar to the dataset curation step in Figure 3, reference features are then concatenated with the target image features along the sequence dimension in each MSA block. The query features of the target image are subsequently updated by attending to both itself and all foreground object features within the reference images. The overall framework is as shown in Figure 4. Training details. We fine-tune two pretrained U-Net-based latent diffusion models (with 1B and 3B parameters) using noise schedule with zero-SNR at the terminal timestep [41]. We initialize our model with the pre-trained IP-Adapter Plus [83] and randomly select one of the reference images as the global image condition. We only fine-tune LoRA [28] layers in the self-attention blocks of diffusion U-Net and key, value projection matrices in image cross-attention layers of IP-Adapter. Additionally, we incorporate random text-image pairs in 10% of training steps from licensed image-text dataset and drop text, image, or both conditions with 5% probability to enable classifier-free guidance. More training hyperparameter details are provided in Appendix C.2. 4.3. Tuning-Free Inference During inference, given reference images, = {xi}K i=1, at each time step, we add forward diffusion noise to all the reference images and extract their fine-grained features for masked shared attention. One of the reference images is randomly selected for the global feature. We then combine the text and image guidance using the formulation of Brooks et al. [6]. Empirically, we observed that the image guidance vector is typically of much higher magnitude than the text guidance vector, especially at higher guidance scales. This often leads to over-exposure issues in the generated image. To mitigate this, we propose normalizing image and text guidance vectors. This helps us achieve better image alignment with the reference object while still following the text prompt. Our final inference is ϵθ(xt, I, ) + λI gI gI + λc gc gc, where gI = ϵθ(xt, I, ) ϵθ(xt, , ), gc = ϵθ(xt, I, c) ϵθ(xt, I, ), g= min(gI , gc), (4) where is the denoising timestep, ϵθ is the diffusion U-Net output, gI and gc are the guidance vectors towards image and text condition, and λI and λc represent the guidance strength for the image and text. We scale the norm of the two guidance vectors to the minimum norm, allowing only λI and λc to vary the relative strength of the image and text guidance. Note that the number of reference images during 5 Figure 6. Results with 3 input images. Here, we show qualitative samples of our method and JeDi [85], which can take multiple reference images as input. Though JeDi maintains high object identity alignment, the background and lighting can often be incoherent in the generated images. Comparatively, our method maintains higher image fidelity while following image and text conditions. We pick the best out of 4 images for both methods. Zoom in for more details. We show more qualitative samples in Figure 21 in the Appendix. inference can be different than the training time. 5. Experiments Evaluation dataset. Consistent with prior works [52, 69, 85], we use DreamBooth [62] dataset consisting of 30 objects with 4-5 images each and 25 evaluation text prompts. Baselines. We compare our method with the leading tuning-free customization baselines, which include JeDi [85], IP-Adapter [83], Emu-2 [71], Kosmos [52], BLIPDiffusion [39], and MoMA [69]. Evaluation metric. The goal of the text-conditional image customization task given one or few reference images is to follow the input prompt while maintaining object identity and image fidelity. To measure the text alignment of generated images with the input prompt, we use CLIPScore [57] and TIFA [29]. To evaluate the alignment of the object in generated images with the reference object, we compute similarity to reference images in DINOv2 [51] feature space. Following recent works [69, 85], we compute this similarity using cropped and background-masked version of the image, denoted as MDINOv2-I, where the mask is computed by pre-trained object detectors [36, 59, 90]. Given the inherent tradeoff between text and image alignment metrics, we combine the two into single metric, Geometric score [82], by taking the geometric mean of TIFA and MDINOv2-I. It is shown in [82] that this geometric mean score is aligned better with the overall human preferences. In addition, we also conduct human evaluation to compare to prior works. 5.1. Comparison to Prior Works"
        },
        {
            "title": "5.1.1 Quantitative Comparison",
            "content": "Automatic scores. Table 1 compares our method against tuning-free baselines. For MDINOv2-I metrics, we measure it on two subsets separately prompts that only change the background vs. prompts that modify the appearance, e.g., cube-shaped or wearing sunglasses, with the latter subset expected to yield lower image similarity in comparison. We evaluate our method using either 1 or 3 input reference 6 Method MDINOv2-I CLIPScore TIFA GeometricScore Background change prompt Property change prompt Kosmos [52] BLIP-Diffusion [39] MoMA [69] JeDi [85] Ours (1B) IPAdapter [83] IPAdapter Plus [83] Emu-2 [71] Ours (3B) JeDi [85] Ours (1B) Ours (3B) 1-input 3-input 0.636 0.658 0.616 0.684 0.744 0.718 0.744 0.750 0.777 0.771 0.806 0. 0.638 0.643 0.620 0.690 0.671 0.702 0.737 0.736 0.708 0.775 0.773 0.789 0.287 0.294 0.320 0.303 0.310 0.283 0.270 0.283 0.319 0.292 0.303 0.313 0.729 0.782 0.867 0.833 0.850 0.701 0.615 0.741 0.902 0.789 0.830 0.863 0.679 0.714 0.730 0.754 0.781 0.704 0.675 0.740 0.825 0.780 0.801 0.838 Table 1. Quantitative comparison. We compare our method against other tuning-free methods with similar model scales on image alignment and text alignment metrics. Our method performs better than other baselines on the combined GeometricScore metric. For reference, the all-pairwise MDINOv2 similarity between reference images themselves using ground-truth masks is 0.851. images. Our approach outperforms all the baselines in the overall Geometric Score, last column in Table 1, when compared with baselines of similar model scales. This indicates that we maintain good balance between object identity preservation and following the input prompt. We also compare our method against tuning-based approaches in Table 2. For single input image, we compare with Break-a-Scene [3], which also uses one image. With 3 reference images, we benchmark against LoRA [31, 79]. As shown in Table 2, our method achieves comparable performance in image alignment while improving text alignment, suggesting reduced overfitting to the reference images. We show qualitative samples in Appendix A. Human evaluation. For comprehensive evaluation, we also conduct pairwise user study. In each study, participants view two generated images (from our method and baseline) alongside the text prompt and 3 reference images. We ask them to select the preferred image based on three criteria: (1) Consistency with the reference object (image alignment), (2) Alignment with the text prompt (text alignment), and (3) Overall quality and photorealism (quality). They also indicate the specific criterion or criteria for their selection. Table 3 shows the results compared to two competing methods from Table 1, i.e., Emu-2 [71] and JeDi [85]. Our method is preferred over both baselines according to all evaluation criteria. To ensure valid responses, participants complete practice test, and only those with correct responses are considered. We gather approximately 300 valid responses per comparison. We provide further details in Appendix C.4."
        },
        {
            "title": "5.1.2 Qualitative Comparison",
            "content": "We show sample comparisons of our method against other tuning-free methods in Figure 5 and 6. Our method more effectively incorporates the text prompt while keeping the object identity and image fidelity, e.g., the blue house in the background in 1st row of Figure 5. In contrast, most Method MDINOv2-I TIFA Geometric Background change prompt Property change prompt 1-input 3-input Break-a-Scene [3] Ours (1B) Ours (3B) LoRA [31] Ours (1B) Ours (3B) 0.765 0.744 0.777 0.795 0.806 0.822 0.752 0.671 0.708 0.776 0.773 0.789 0.823 0.850 0.898 0.760 0.830 0. Score 0.791 0.781 0.825 0.774 0.801 0.838 Table 2. Comparison with tuning-based methods. Our method remains competitive against tuning-based methods, with better text alignment and comparable image alignment. Method Human preference (in %) Overall preference Text alignment Image alignment Quality 1-input 3-input Ours (1B) vs JeDi Ours (3B) vs Emu-2 Ours (1B) vs JeDi Ours (3B) vs JeDi 71.91 66.74 68.19 72.14 73.27 70.49 69.51 81.40 73.41 66.88 63.05 64.02 74.13 64.66 80.89 75.13 Table 3. Human preference study. Here, we compare the pairwise preference of our method against the competing methods from Table 1, i.e., Emu-2 [71] with the same model scale and JeDi [85]. The standard error for all is within 5%. baseline methods either ignore the text prompt or have low object identity preservation. With 3 reference images as input in Figure 6, although JeDi [85] achieves high identity preservation, it can result in reduced image quality, with inconsistency in lighting and background scene. 5.2. Model Ablation Study Architecture design. We evaluate the impact of key components of our model, specifically: (1) masked shared attention, (2) mask usage in shared attention during training, and (3) global feature injection. All the experiments are with our 3B model. Table 4 shows that removing global feature significantly affects performance with 1 image as input. Adding masked shared attention allows effective use of multiple reference images during inference, improving performance as we increase the number of reference images. We show 7 Method MDINOv2-I TIFA Geometric Background change prompt Property change prompt IPAdapter Plus + our inference Ours w/o mask in MSA w/o global feature w/o MSA Ours w/o mask in MSA w/o global feature 1-input 3-input 0.744 0.719 0.777 0.763 0.709 0.766 0.822 0.804 0. 0.737 0.668 0.708 0.693 0.679 0.695 0.789 0.747 0.792 0.615 0.816 0.902 0.901 0.908 0.901 0.863 0.865 0.821 Score 0.675 0.756 0.825 0.817 0.795 0.819 0.838 0.825 0.807 Table 4. Model ablation. We show the contribution of different components of our model architecture and inference method. MSA enables the effective use of multiple reference images as input, thus significantly helping with image alignment, whereas, with only one image as input, having the global feature is more crucial. Figure 7. Inference with ours and guidance rescale [41] technique. The caption is stuffed toy with blue house in the background. As we increase guidance, our inference better follows the text prompt while increasing the image similarity. qualitative samples in Figure 11 in the Appendix. Modified guidance inference. We compare our inference approach (Section 4.3) to guidance rescale [41] with our trained model. As shown in Figure 7, increasing the guidance strength in our method preserves image fidelity while incorporating the text and image conditions. Guidance rescale was also proposed to mitigate image saturation but in vanilla text-to-image generation pipeline. We also evaluate the baseline IP-Adapter Plus [83] with our modified inference. As shown in Table 4, this improves its TIFA score from 0.615 to 0.816, with only minor decrease in image alignment. 5.3. Dataset Ablation Study Dataset curation. We ablate different steps of the dataset generation to analyze their respective contributions. We compute the average intra-cluster similarity using DINOv2 features, where cluster is composed of images coupled by the same object, as well as the filtering ratio using our automatic filtering step. Table 5 shows that MSA consistently improves intra-cluster similarity and, thereby, the filtering ratio. For rigid object generation, while feature warping minimally affects DINOv2 feature similarity, we find it beneficial in promoting multi-view consistency between the object in the images, e.g., the consistent cup colors in 1st row (left 8 Figure 8. Dataset generation ablation. Top: our synthetic training images. Middle: removing warping reduces multi-view consistency, e.g., colors of the center cup. Bottom: removing both warping and MSA further hurts visual consistency. Zoom in for details. Method DINOv2-I Filtering % Rigid categories Deformable categories Ours w/o Warping w/o MSA Ours w/o MSA w/o Detailed description 0.595 0.591 0.534 0.700 0.626 0.564 15.64 15.10 14.44 39.84 36.44 27. Table 5. Dataset curation ablation. MSA consistently enhances intra-cluster DINOv2-I similarity. While warping does not impact DINOv2-I scores, its qualitative benefits are shown in Figure 8. column) of Figure 8. For deformable objects, providing descriptive prompts in addition to MSA proves crucial. Table 4 shows the effectiveness of our training data. Our model without MSA is comparable to an IP-Adapter Plus model trained on our 95K dataset. direct comparison between the 1-input samples generated by our model without MSA (row 6) and IP-Adapter Plus using our inference (row 2) highlights the improvement due to our 95K training set, while using similar models and inference protocols. In Appendix and B, we show more quantitative and qualitative results, comparisons on other evaluation datasets [37], and effect of dataset size vs. category diversity. 6. Limitations and Discussion In this work, we focus on tuning-free model customization and propose advancements to address current limitations. To overcome the lack of training data, we have created synthetic dataset by generating multiple images with consistent objects using Masked Shared Attention and 3D asset priors. We then propose an improved model architecture and inference technique. Our approach outperforms existing tuning-free methods while being on par with existing time-consuming tuning-based approaches. While promising, our method has limitations, e.g., it struggles with intricate textures and can have limited pose variability. Integrating recent advances in text-to-3D and video generative models can enhance the quality of the generated dataset, ultimately leading to improved performance in downstream applications. Acknowledgment. We thank Kangle Deng, Gaurav Parmar, and Maxwell Jones for their helpful comments and discussion and Ruihan Gao and Ava Pun for proofreading the draft. This work was partly done by Nupur Kumari during the Meta internship. The project was partly supported by the Packard Fellowship, National AI Research Lab (South Korea), NSF IIS-2239076, and NSF ISS-2403303."
        },
        {
            "title": "References",
            "content": "[1] LAION AI. Laion-aestheticspredictor. https://github. com/LAION-AI/aesthetic-predictor, 2022. 4 [2] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. neural space-time representation for text-toimage personalization. ACM Transactions on Graphics (TOG), 2023. 2 [3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel CohenOr, and Dani Lischinski. Break-a-scene: Extracting multiple In SIGGRAPH Asia 2023 concepts from single image. Conference Proceedings, 2023. 7, 13, 17 [4] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [5] Shariq Farooq Bhat, Niloy Mitra, and Peter Wonka. Loosecontrol: Lifting controlnet for generalized depth conditioning. In ACM SIGGRAPH 2024 Conference Proceedings, 2024. 2 [6] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 5, 13, 14 [7] George Cazenavette, Avneesh Sud, Thomas Leung, and Ben Usman. Fakeinversion: Learning to detect images from unseen text-to-image models by inverting stable diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 18 [8] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 2023. [9] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. In International Conference on Learning Representations (ICLR), 2022. 2 [10] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William Cohen. Subject-driven text-to-image generation via apprenticeship learning. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 1 [11] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [12] Riccardo Corvi, Davide Cozzolino, Giada Zingarini, Giovanni Poggi, Koki Nagano, and Luisa Verdoliva. On the detection of synthetic images generated by diffusion models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. 18 [13] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 16, [14] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. In Conference on Neural Information Processing Systems (NeurIPS), 2024. 2 [15] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, and Maneesh Agrawala. Flashtex: Fast relightable mesh texturing with lightcontrolnet. In European Conference on Computer Vision (ECCV), 2024. 2 [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Conference on Neural Information Processing Systems (NeurIPS), 2021. 2 [17] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3, 16 [18] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning (ICML), 2024. 1, 2, 16 [20] Pierre Fernandez, Guillaume Couairon, Herve Jegou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In IEEE International Conference on Computer Vision (ICCV), 2023. [21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations (ICLR), 2023. 1, 2 [22] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation with rich text. In IEEE International Conference on Computer Vision (ICCV), 2023. 2 [23] Jing Gu, Yilin Wang, Nanxuan Zhao, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, HyunJoon Jung, and Xin Eric Wang. Swapanything: Enabling arbitrary object swapping in personalized visual editing. In European Conference on Computer Vision (ECCV), 2024. 2 [24] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compact parameter space for diffusion fine-tuning. In IEEE International Conference on Computer Vision (ICCV), 2023. 2 [25] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. In International Conference on Learning Representations (ICLR), 2023. 2, 17 [26] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Conference on Neural Information Processing Systems (NeurIPS), 2020. 2 [28] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations (ICLR), 2022. 2, 5, 13, 17 [29] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 6, 13 [30] Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li, Chongyang Ma, Xiu Li, and Changsheng Xu. Creativesynth: Creative blending and synthesis of visual arts based on multimodal diffusion. arXiv preprint arXiv:2401.14066, 2024. 2 [31] HuggingFace. Lora-stable diffusion. https : / / github . com / huggingface / diffusers / blob / main / examples / dreambooth / train _ dreambooth_lora_sdxl.py, 2023. 7, 13, 14, 17 [32] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Conference on Neural Information Processing Systems (NeurIPS), 2022. 2, 17 [34] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [35] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In IEEE International Conference on Computer Vision (ICCV), 2023. 2 [36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In IEEE International Conference on Computer Vision (ICCV), 2023. 6, [37] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of textto-image diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 8, 13, 22, 23 [38] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 3 10 [39] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-toimage generation and editing. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 1, 2, 6, 7, 17 [40] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [41] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 5, 8, 13, 14 [42] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision (ECCV), 2022. 2 [43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision (ECCV), 2024. 17 [44] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations (ICLR), 2023. 2 [45] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Conference on Neural Information Processing Systems (NeurIPS), 2022. 2 [46] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. In Conference on Neural Information Processing Systems (NeurIPS), 2024. 3, 16, [47] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH, 2024. 2 [48] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations (ICLR), 2022. 2 [49] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [50] Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, and Filip Radenovic. Context diffusion: In-context aware image generation. In European Conference on Computer Vision (ECCV), 2024. 2 [51] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. In Transactions on Machine Learning Research (TMLR), 2023. 4, 6 [52] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context In International with multimodal large language models. Conference on Learning Representations (ICLR), 2024. 2, 6, 7, [53] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. 2 [54] Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, and Kfir Aberman. Object-level visual prompts for compositional image generation. arXiv preprint arXiv:2501.01424, 2025. 2 [55] William Peebles and Saining Xie. Scalable diffusion models In IEEE International Conference on with transformers. Computer Vision (ICCV), 2023. 1, 2, 4 [56] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention refocusing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021. 4, 6, [58] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 [59] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 6 [60] Elad Richardson, Kfir Goldberg, Yuval Alaluf, and Daniel Cohen-Or. Conceptlab: Creative generation using diffusion prior constraints. In ACM Transactions on Graphics (TOG), 2024. 2 [61] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 1, 2 [62] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 2, 6, 13, 17 [63] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. In Conference on Neural Information Processing Systems (NeurIPS), 2022. [64] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations (ICLR), 2022. 4 [65] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In International Conference on Machine Learning (ICML), 2023. 2 [66] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 2 [67] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 2, 3 [68] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In International Conference on Learning Representations (ICLR), 2024. 2 [69] Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, and Xiao Yang. Moma: Multimodal llm adapter for fast personalized image generation. In European Conference on Computer Vision (ECCV), 2024. 1, 2, 6, 7, 17 [70] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. 17 [71] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 6, 7, 13, 17, [72] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, 2024. 17 [73] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for text-to-image personalization. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. 2 [74] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation. ACM Transactions on Graphics (TOG), 2024. 2, 3 [75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2 [76] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023. 2 [77] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surIn IEEE Conference on prisingly easy to spot... for now. Computer Vision and Pattern Recognition (CVPR), 2020. 18 11 long-range image and video generation. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [78] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In IEEE International Conference on Computer Vision (ICCV), 2023. 1, 2 [79] XavierXiao. Dreambooth on stable diffusion. https:// github.com/XavierXiao/Dreambooth-StableDiffusion, 2022. 1, 7 [80] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, 2024. 2 [81] XLabs-AI. Flux depth controlnet. https://github. com/XLabs-AI/x-flux, 2024. 17 [82] Wilson Yan, Andrew Brown, Pieter Abbeel, Rohit Girdhar, and Samaneh Azadi. Motion-conditioned image animation for video editing. arXiv preprint arXiv:2311.18827, 2023. 6, [83] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1, 2, 4, 5, 6, 7, 8, 13, 14, 17, 22 [84] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. In International Conference on Machine Learning (ICML), 2022. 2 [85] Yu Zeng, Vishal Patel, Haochen Wang, Xun Huang, TingChun Wang, Ming-Yu Liu, and Yogesh Balaji. Jedi: Jointimage diffusion models for finetuning-free personalized textto-image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 6, 7, 17, 21 [86] Lvmin Zhang and Maneesh Agrawala. Adding conditional In IEEE Intercontrol to text-to-image diffusion models. national Conference on Computer Vision (ICCV), 2023. 2, 3 [87] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, and Changsheng Xu. Prospect: Prompt spectrum for attributeaware personalization of diffusion models. ACM Transactions on Graphics (TOG), 2023. 2 [88] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K. Wong. Unicontrolnet: All-in-one control to text-to-image diffusion models. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 2 [89] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [90] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European Conference on Computer Vision (ECCV), 2022. 6, 17 [91] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for"
        },
        {
            "title": "Appendix",
            "content": "In Appendix and B, we show more qualitative samples of our method, its comparison to the baselines, and more ablation studies. Then, in Appendix C, we provide implementation details related to our dataset generation, model training, and inference. Finally, in Appendix D, we discuss our works limitations and societal impact. A. Additional Comparison with Prior Works CustomConcept101 Benchmark. Though DreamBooth [62] is widely used evaluation dataset, CustomConcept101 [37] is more diverse with 101 unique concepts. Here, we also compare our model (3B) with open-source baseline models of similar scale, i.e., Emu-2 [71] and IP-Adapter [83], on this dataset. As shown in Table 6, our method performs better in identity preservation compared to both baselines while also yielding higher text alignment as indicated by CLIPScore [57] and TIFA [29] metrics. We show qualitative comparison in Figure 22 and more samples in Figure 23. Additional Qualitative Comparison. Figures 19 - 21 show more visual comparison against the tuning-free baselines with single and three reference images as input, respectively. Similarly, Figure 9 and 10 show sample comparisons of our method with tuning-based approaches Break-a-Scene [3] and LoRA [28, 31]. When compared to tuning-based approaches, our method performs on par in identity preservation while better following the text prompt. B. Ablation Study Model ablation. Here, we show qualitative comparison of our model ablation experiments reported in Section 5.2, i.e., when training w/o global features and w/o masks in MSA. As shown in Figure 11, removing global features significantly degrades performance, especially when the text prompt is ambiguous, e.g., toy instead of rubber duck. Similarly, MSA helps the model capture fine details, e.g., the specific color pattern of the shoe in the last row of Figure 11. We also perform more detailed analysis of our proposed inference technique here. Table 7 compares our technique with the default inference of Brooks et al. [6] and guidance rescale [41] on the IP-Adapter Plus [83] baseline. For all, we use the same text and image guidance scale as ours. The default inference technique of Brooks et al. [6] and adding guidance rescale to it do not affect the final performance significantly. Meanwhile, with our normalization technique, the text alignment improves significantly with comparatively minor drop in image alignment. The sample comparisons in Figure 12 also show the same trend. Dataset diversity and size. We examine the impact of training dataset size vs. category diversity on the final model performance by creating various subsets of the data by adjusting these two factors and training our model on each subset. Method MDINOv2-I CLIPScore TIFA Geometric Background change prompt Property change prompt 1-input IPAdapter Plus [83] Emu-2 [71] Ours (3B) 3-input Ours (3B) 0.618 0.604 0. 0.689 0.626 0.619 0.609 0.666 Score 0.595 0.655 0.712 0.261 0.284 0. 0.569 0.701 0.809 0.304 0.749 0.712 Table 6. Results on CustomConcept101 [37]. Our method outperforms both Emu-2 [71] and IP-Adapter [83] on the overall Geometric Score [82] metric while being on par regarding image alignment. The Geometric Score is computed by taking the geometric mean of MDINOv2-I and TIFA scores, both of which are in the 0-1 range. Figure 9. Comparison with Break-a-Scene [3] given 1-input image. Break-a-Scene can sometimes ignore the text prompt, e.g., dog in police outfit in the last row. Our method follows the text prompt better while having on-par image alignment. Method MDINOv2-I CLIPScore TIFA Geometric Background change prompt Property change prompt IPAdapter Plus + Guidance rescale [41] (0.6) + Vanilla Img + Text [6] + Our inference 0.744 0.722 0.722 0.719 0.737 0.699 0.711 0.668 0.270 0.276 0.270 0.298 0.615 0.707 0.681 0.816 Score 0.675 0.710 0.699 0. Table 7. Our inference. We compare our inference technique with vanilla image and text guidance technique [6] as well as guidance rescale [41] with the same inference hyperparameters across all. Figure 13 shows qualitative samples from models trained on progressively larger datasets, showing that increased sample size enables capturing fine object details. However, we observe that reducing the category diversity has larger impact than the sample size, as shown in Figure 14. We plot the image alignment MDINOv2-I metric for different subsets, 13 Figure 12. Qualitative comparison of our inference. Our modified inference technique helps increase text alignment while minimally affecting the object identity. In comparison, the inference technique of Brooks et al. [6] or additional guidance rescale [41] has less effect on the final outputs. Please zoom in for details. Figure 13. Dataset size. Increasing training samples from 100 to 1K, 10K, and 95K yields gradual improvements in object identity preservation, especially in finer details. As our model is initialized from IP-Adapter Plus [83], it can already generate similar-looking object with as few as 1K samples. Zoom in for details. Figure 14. Dataset ablation. We plot the MDINOv2-I metric with increased sample size and category diversity. Given the same sample size of 1K objects, increasing category diversity from 3 to 16 and 200 gradually improves image alignment. Regarding sample size, we observe that the performance plateaus, with similar performance on 10K as 95K samples based on both quantitative metrics, as shown here, and the human preference study. The above metrics are calculated with 3 images as input. Figure 10. Comparison with LoRA [31] given 3-input image. Our method works similarly to LoRA regarding image alignment while being better on text alignment, e.g., the snow texture in the first row. Please zoom in for details. Figure 11. Qualitative samples of ablation studies. 1st column: Not training on rigid object categories leads to worse shape and identity preservation on rigid category objects during inference. 2nd column: Not using the global feature hurts the performance overall, with the most visible effect on categories with ambiguous text description, e.g., rubber duck as toy in 1st row. 3rdcolumn: Not having MSA leads to fine details being missed, e.g., the specific color pattern of the shoes in 3rd row. Please zoom in for details. and given the same sample size, e.g., 1K samples, higher category diversity leads to better performance. Similarly, given the same category diversity, performance quickly plateaus with sample size, with similar performance when trained on 10K vs. 95K samples. We also perform human preference study for our final model vs. the models trained on 1K and 10K samples, with the preference for our model being 62.16% and 50.59%, respectively. 14 Figure 15. Effect of dataset category diversity on performance. As we increase the number of unique categories from 1 to 3 to 16 and 200 (with fixed sample size of 1K), performance improves with the model capturing finer details of the object, e.g., the unique pattern in front of the toy car in 1st row or the frills of the boot in 4rth row. Please zoom in for details. Figure 16. Rigid object generation w/ vs. w/o 3D Asset guidance. We compare our final rigid object generation results with that of removing 3D asset guidance and only using MSA. Removing depth and warping guidance from the dataset generation pipeline reduces multi-view and shape consistency. 15 Rigid object generation. Figure 16 here shows that only having MSA for rigid objects fails to maintain the same shape and multi-view consistency across different views. Whereas guiding the generation using 3D assets from datasets like Objaverse [13] leads to more consistent objects. We also analyze the importance of including such categories for training. Figure 11 shows qualitative samples from the model trained only on deformable object categories, consisting of pet animals, which leads to degraded performance on rigid objects, e.g., the duck toy or sunglasses and shoes. The average MDINO-v2-I metric on the DreamBooth evaluation dataset also reduces from 0.81 to 0.78. C. Implementation details C.1. Dataset Generation LLM instruction details. To get set of prompts in our dataset generation, we use Instruction-tuned LLama3 [17]. The input instruction to the LLM always consists of the prompt shown below, which is modified from Esser et al. [19]: Role: system, Content: You are language model expert in suggesting image captions for different object categories. Role: user, Content: suggest ten captions for images of [object description/ category]. The caption should provide [TASK]. DO NOT add any unnecessary adjectives or emotional words in the caption. Please keep the caption factual and terse but complete. DO NOT add any unnecessary speculation about the things that are not part of the image, such as the image is inspiring to viewers or seeing this makes you feel joy. DO NOT add things such as creates unique and entertaining visual, as these descriptions are interpretations and not part of the image itself. The description should be purely factual, with no subjective speculation. Where in the case of rigid object generation, we provide the object description from CAP3D [46] and the TASK is description of the background. We also provide two sample descriptions, as shown below: 16 Follow this guidance for the captions: 1. Generate captions of [object description] in different backgrounds and scenes. 2. Generate captions of [object description] with another object in the scene. Example captions for White plastic bottle are: 1. white plastic bottle on roadside cobblestone with stone bricks. 2. white plastic bottle is placed next to steaming cup of coffee on polished wooden table. Example captions for blue truck are: 1. blue tank in military storage facility with metal walls. 2. blue tank on desert battlefield ground, with palm trees in the background. In the case of deformable object generation, we prompt the LLM once, with the category name, e.g., cat, and TASK as detailed visual information of the category, including color and subspecies. We append the below instruction as well to the LLM: Example caption descriptions for the category cat: 1. The Siamese cat has blue almond-shaped eyes and cream-colored fur with dark chocolate points on the ears, face, paws, and tail. 2. The white fluffy Maine Coon cat with long and bushy tail spread out beside it, and its thick fur has mix of brown, black, and white stripes. 3. The Bengal cat with marbled coat features pattern of vivid orange and black spots. We prompt the LLM again with the same category name and TASK as description of the background. We append the below instruction as well to the LLM: Follow this guidance for the captions: 1. Generate captions of [category] in different backgrounds and scenes. 2. Generate captions of [category] with another object in the scene. 3. Generate captions of [category] with different stylistic representations. Example captions for the category cat are: 1. Photo of cat playing in garden. The garden is filled with wildflowers. 2. cat is sitting beside book in library. 3. Painting of cat in watercolor style. Masked Shared Attention (MSA). When performing MSA in DiT-based text-to-image models, we modify the rotational positional encoding [70] to be image resolution for generating the images of resolution. Further, during sampling, each image attends to everything in the other image at the first time step, and the mask is then used in subsequent timesteps. The final training dataset is filtered with mean intra-cluster DINOv2 similarity threshold of 0.7 and aesthetics score above 6.0 out of 10.0. More specific details related to rigid and deformable object generation are provided below. Rigid object generation. We select approximately 75K assets from the Objaverse [13], which is subset of LVIS and high-quality assets shared by Tang et al. [72]. For the depth images, we used the Cap3D dataset [46], which renders the assets from different camera poses. We select the 3 views with minimum 10% pair-wise overlap in rendered images and pre-calculate the cross-view pixel correspondence to be used in the dataset generation pipeline. We use the ground truth rendered depth images as input to depth-conditioned FLUX model [81]. During dataset generation, we also use negative prompts, such as 3d render, low resolution, blurry, cartoon. The feature warping is performed only on the first 20% of denoising timesteps. We perform sampling with 50 steps at 512 resolution, with text guidance of 3.5 and depth control strength of 0.8. Deformable object generation. In the case of deformable objects, during Masked Shared Attention (MSA), we compute the mask of the foreground object region via text crossattention [25], which is updated at every diffusion timestep. Additionally, once generated, we remove the object description from the prompt in the dataset. We perform sampling with 50 timesteps and standard text guidance of 3.5 at 1K resolution. C.2. Our Method Training. We train our model with batch size of 32, learning rate 1 105 for 20K iterations. We sample 3 images of each object during training, with 2 as references and 1 image as target. For objects with only 2 images, we horizontally flip one image as the third image in the set. Inference. During inference, we use the text-guidance scale of 7.5 and an adaptive image-guidance scale, λI in Eqn. 4, starting from 8.0 for background change prompts and 6.0 for property/shape change prompts and linearly increasing it by 5.0 during the 50 sampling steps. For the global feature, the IP-Adapter scale is always set to its default value of 0.6 during inference. For sampling, we use the Euler Discrete scheduler [33]. Our inference time for sampling an image is 19 seconds compared to 3 seconds for the base pretrained model in mixed-precision bf loat16 on H100 GPU. The overhead is because of the longer sequence length in the masked shared attention combined with making the forward call to the model twice at every step to extract reference features. C.3. Baselines Here, we mention the implementation details of baseline methods. For baselines with recommended hyperparameters, we always followed those while keeping the sampling step consistent across all to 50 and the text guidance scale of 7.5 if not mentioned. Kosmos-G [52]. We follow their open-source code to sample images on the DreamBooth evaluation dataset. BLIP Diffusion [39]. According to the recommended technique, we modify each prompt to be an (image, category name, instruction) tuple where instruction is modified from the input prompt, e.g., toy in junle in jungle or red toy make it red. Additionally, we use the negative prompts provided in their open-source code. IP-Adapter [83]. In the case of IP-Adapter [83], we use the IP-Adapter Plus with U-Net-based diffusion model of the same parameter scale as Ours (3B). We use the recommended 0.6 IP-Adapter scale. MoMA [69]. We use their open-source code with the maximum strength parameter of 1 for increased object identity preservation. JeDi [85]. We use the generated images on the DreamBooth evaluation dataset shared by the authors. Emu-2 [71] . We use their open-source code with the recommended guidance of 3. Additionally, as mentioned in their paper, we modify each prompt to be an (image, instruction) tuple where instruction is modified from the input prompt, e.g., toy in junle in jungle or red toy make it red. Break-a-Scene [3]. We use the open-source code of Breaka-Scene and learn 2 assets, one corresponding to the object and another for the background. During inference, we only use the learned asset for the object. LoRA [28, 31]. We follow the hyperparameters from the HuggingFace implementation [31] and finetune U-Netbased diffusion model of the same parameter scale as Ours (3B). Additionally, we enable class regularization with generated images to prevent overfitting, as suggested in DreamBooth [62]. C.4. Evaluation MDINOv2-I metric. To compute this, we first detect and segment the object. For detection, we use Detic [90] and Grounding DINO [43] in case Detic fails. For object detection, we modify the category names to be more descriptive, e.g., rubber duck instead of toy, white boot instead of boot, or toy car instead of toy. We then use the detected bounding box as input to SAM [36] for segmentation. Once segmented, we mask the background and crop 17 Figure 17. Sample practice test for human preference study. We show 3 practice questions to each participant that test their ability to select the images based on the three criteria that we care about, i.e., identity preservation or image alignment, text alignment, and overall quality. D. Limitations and Societal Impact Here, we provide examples to show the limitations of our model and discuss its broader societal implications. One notable limitation of our method is that it can still fail to capture fine texture details as shown in Figure 18. This can be attributed to our dataset often having simple, uniform textures. Despite this, our method improves upon current leading tuning-free customization methods by proposing advancements in dataset collection and training architecture. We hope this will empower users in their creative endeavors to generate ever-new compositions of concepts from their personal lives. However, the potential risks of generative models, such as creating deepfakes or misleading content, extend to our method as well. Possible ways to mitigate such risks are technologies for watermarking [20] and reliable detection of generated images [7, 12, 77]. Figure 18. Limitation. Our method can still struggle with detailed textures, e.g., the cartoon-like dog design on the bag and its flaps in the first row or the stickers on the red backpack in the second row. the image around the mask for both reference and generated images. Additionally, for reference images, we manually correct the predicted mask using the SAM interactive tool to be ground truth. Human preference study details. For each human preference study, we randomly sample 750 images, with one image per object-prompt combination. We use Amazon Mechanical Turk for our study. During the study, participants first complete practice test consisting of three questions that test their ability to select an obvious ground truth image based on alignment to the text prompt, reference object similarity, and image quality. sample set of practice questions is shown in Figure 17. The study has similar setup, except the two images are now from ours and baseline method. We only consider responses from participants who answered the practice questions correctly. 18 Figure 19. Results. We compare our method qualitatively against other leading tuning-free baselines with single reference image as input. We can successfully incorporate the text prompt while preserving the object identity similar to or higher than the baseline methods. We pick the best out of 4 images for all methods. In comparison, Emu-2 and JeDi often have low fidelity, and IP-Adapter Plus overfits on the input image. MoMa, though it has reasonable performance on pet animals like dogs and cats, fails on more unique objects like the shoe in 3rd row. Please zoom in for details. 19 Figure 20. Results. We compare our method qualitatively against other leading tuning-free baselines with single reference image as input. We can successfully incorporate the text prompt while preserving the object identity similar to or higher than the baseline methods. We pick the best out of 4 images for all methods. Please zoom in for details. Figure 21. Results with 3 input images. We compare our method qualitatively against JeDi [85], which can also take multiple images as input. Compared to JeDi, our method more coherently incorporates the text prompt with higher image fidelity while being similar in performance on image alignment, e.g., the missing firefighter outfit in 2nd row or low fidelity sunglasses in 4rth row. We pick the best out of 4 images for all methods. Please zoom in for details. 21 Figure 22. Qualitative comparison on CustomConcept101 [37] dataset with 1 input image. We compare our method against baselines of similar scale models, i.e., Emu-2 [71] and IP-Adapter Plus [83]. We observe that both baselines can sometimes overfit on the input image. Whereas our method can better incorporate the text prompt while respecting the objects identity. Please zoom in for details. 22 Figure 23. Samples on CustomConcept101 [37] dataset with 3 input images. We show more samples of our method given 3 reference images of the object."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Meta"
    ]
}