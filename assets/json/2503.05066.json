{
    "paper_title": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts",
    "authors": [
        "Shwai He",
        "Weilin Cai",
        "Jiayi Huang",
        "Ang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, a phenomenon we define as the \\textbf{\\textit{Straggler Effect}}. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) \\textbf{\\textit{Capacity-Aware Token Drop}}, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) \\textbf{\\textit{Capacity-Aware Token Reroute}}, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to a more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2\\% average performance increase and a 1.94$\\times$ inference speedup on Mixtral-8$\\times$7B-Instruct."
        },
        {
            "title": "Start",
            "content": "Capacity-Aware Inference: Mitigating the Straggler Effect in Mixture of Experts Shwai He1 Weilin Cai2 Jiayi Huang2 Ang Li1 1University of Maryland, College Park 2The Hong Kong University of Science and Technology (Guangzhou) shwaihe@umd.edu, angliece@umd.edu 5 2 0 2 M 7 ] . [ 1 6 6 0 5 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The Mixture of Experts (MoE) is an effective architecture for scaling large language models by leveraging sparse expert activation, optimizing the trade-off between performance and efficiency. However, under expert parallelism, MoE suffers from inference inefficiencies due to imbalanced token-to-expert assignment, where some experts are overloaded while others remain underutilized. This imbalance leads to poor resource utilization and increased latency, as the most burdened expert dictates the overall delay, phenomenon we define as the Straggler Effect. To mitigate this, we propose Capacity-Aware Inference, including two key techniques: (1) Capacity-Aware Token Drop, which discards overloaded tokens to regulate the maximum latency of MoE, and (2) Capacity-Aware Token Reroute, which reallocates overflowed tokens to underutilized experts, balancing the token distribution. These techniques collectively optimize both high-load and low-load expert utilization, leading to more efficient MoE inference pipeline. Extensive experiments demonstrate the effectiveness of our methods, showing significant improvements in inference efficiency, e.g., 0.2% average performance increase and 1.94 inference speedup on Mixtral-87B-Instruct."
        },
        {
            "title": "Introduction",
            "content": "In recent years, the rapid evolution of Large Language Models (LLMs) (OpenAI, 2024; Team, 2024a; DeepSeek-AI et al., 2024b) has driven wave of innovations, continuously expanding the frontiers of AI research and applications. Among the model architectural innovations, the Mixture of Experts (MoE) framework has emerged as pivotal technique for optimizing the cost-performance trade-off in LLMs. Specifically, MoE (Shazeer et al., 2017a; Fedus et al., 2022) enhances scalability by integrating multiple experts while activating only subset per input. This selective activation Figure 1: Illustration of the Straggler Effect in MoE Inference, where the most burdened expert (i.e., Straggler Expert) dictates the overall latency. substantially improves model performance without corresponding increase in computational cost, effectively balancing efficiency and performance. Despite the success of MoE, key efficiency challenge lies in the imbalanced token-to-expert distribution, which results in some experts being overloaded while others remain underutilized (Lepikhin et al., 2021; Zoph et al., 2022). In the context of expert parallelism, low-load experts complete their computations faster but must wait for highload experts to finish, leading to inefficient resource utilization and increased latency. As illustrated in Figure 1, this phenomenon is defined as the Straggler Effect, where the most burdened expert dictates the overall latency of the MoE layer. While auxiliary balance losses have been incorporated into the training process to alleviate imbalance (Shazeer et al., 2017a; Fedus et al., 2022; DeepSeek-AI et al., 2024b), these techniques remain ineffective in mitigating imbalance during inference. Specifically, as shown in Figure 1, our findings reveal highly uneven token distribution among experts, with the highest-load expert handling more than seven times the expected average load. Moreover, managing such an imbalance during inference often incurs additional resource costs. For instance, DeepSeek-V3 addresses this issue by duplicating high-load experts and deploying them redundantly (DeepSeek-AI et al., 2024b). This motivates us to explore efficient token-to-expert assignment through two key questions: (1) How to prevent extreme overloading in high-load experts? and (2) How can we utilize the available capacity of low-load experts? We address these challenges through CapacityAware Inference. For high-load experts, we introduce Capacity-Aware Token Drop, technique that removes excessive tokens from overloaded experts. On the one hand, eliminating these excess tokens alleviates extreme load imbalances, significantly enhancing efficiency. On the other hand, dropping these tokens minimally impacts model performance, as they constitute only small fraction of the total tokens. For low-load experts, we extend Token Drop with Capacity-Aware Token Reroute, which leverages the available capacity of underutilized experts to process overflowed tokens. Token Reroute further enhances the performance of Token Drop within the capacity-constrained inference framework. Extensive experimental results validate the effectiveness of our proposed techniques, demonstrating significant improvements in both efficiency and performance, e.g., 0.2% average performance increase and 1.94 inference speedup in Mixtral-87B-Instruct. In short, our contributions are in three folds: We identify the Straggler Effect caused by token imbalance at inference time in Mixture of Experts, highlighting the optimization potential for reducing latency. Toward token imbalance, we propose Token Drop and Token Reroute to enhance the balanced utilization of experts. Experimental results demonstrate the effectiveness of Token Drop and Token Reroute, achieving significant improvements in expert utilization and inference efficiency."
        },
        {
            "title": "2 Related Works",
            "content": "Mixture of Experts Models The Mixture of Experts (MoE) is kind of neural network architecture with an extended set of parameters (referred to as experts) controlled by router, which is first introduced in the context of conditional computation (Jacobs et al., 1991; Jordan and Jacobs, 1994). The potential of sparse activation in MoE is subsequently exploited by (Shazeer et al., 2017b) for efficient training and inference on pretrained models with special designs, opening the door for MoE in various vision (Riquelme et al., 2021) and language (Lepikhin et al., 2020; Du et al., 2022; Fedus et al., 2022) scenarios. Attributed to its exceptional efficiency, MoE has been adopted as foundational framework in the designs of large language models (Jiang et al., 2024; Dai et al., 2024; Xue et al., 2024a; Zhu et al., 2024; Team, 2024b), achieving superior scaling laws at low computational costs. Despite these advancements, MoE still faces efficiency challenges in both training and inference (Cai et al., 2024), and our work specifically focuses on enhancing inference-time efficiency. Imbalance in Mixture of Experts The imbalance in token-to-expert assignments (Zhou et al., 2022; Chen et al., 2022) poses significant challenge to the deployment of Mixture of Experts (MoE). This imbalance leads to inefficiencies in memory, computation, and communication (He et al., 2023; Song et al., 2023; Xue et al., 2024b), making it critical bottleneck for MoE scalability and deployment. To mitigate this issue, an auxiliary balance loss (Shazeer et al., 2017a) is incorporated into the training process to encourage more uniform token distribution across experts. Additionally, various training strategies have been introduced to further balance token assignments: Switch-Transformer (Fedus et al., 2022) and DeepSeek-V2 (DeepSeek-AI et al., 2024a) implement Token Drop to alleviate expert overload, while DeepSeek-V3 (DeepSeek-AI et al., 2024b) introduces an additional sequence-level auxiliary loss to prevent severe token imbalance. However, these techniques primarily focus on training and fail to ensure balanced token assignments during inference. Instead, addressing token imbalance at inference often incurs additional resource costs. For example, DeepSeek-V3 (DeepSeek-AI et al., 2024b) mitigates this issue by duplicating high-load experts and deploying them redundantly. In contrast, our approach effectively balances token assignments without introducing additional computational overhead."
        },
        {
            "title": "3.1 Extremely Imbalanced Expert Utilization",
            "content": "A Mixture of Experts (MoE) layer consists of collection of experts, {E1, E2, . . . , En} and router that dynamically selects the most relevant experts for given input x. The router computes selection scores G(x), for all experts and selects the top experts, resulting in sparse activation: = TopK(Softmax(G(x)), k). (1) The input is processed by the selected experts, and their outputs are combined into weighted sum based on the routers scores. This process is mathematically expressed as: = (cid:88) iK G(x)i Ei(x), (2) where denotes the indices of selected experts, G(x)i represents the selection score for the i-th expert, and Ei(x) is the output from the i-th expert. In transformer models, the MoE layer usually replaces the feed-forward network (FFN) and only activates subset of experts for each input. While experts in MoE can be deployed in parallel, imbalanced token-to-expert assignments lead to varying levels of expert utilization and introduce potential latency. Despite the incorporation of balancing techniques during training, the load imbalance persists during inference. To further investigate this issue, we conduct preliminary experiments to analyze expert-specific utilization patterns and assess the impact of imbalance on practical latency. To quantify expert utilization, we measure the load across different experts. Given an input batch Rbsd with batch size and sequence length s, the total number of tokens is = bs. Since each token selects out of experts, the expected token count per expert is: within single layer of OLMoE, where some experts receive an excessively large number of tokens (e.g., more than seven times of average load), which resulting in significant latency. detailed layer-by-layer analysis is provided in Appendix B."
        },
        {
            "title": "3.2 Motivation – the Straggler Effect",
            "content": "Under the expert parallelism scenario, where the number of assigned tokens dictates the processing time of each expert, high-load experts become the bottleneck for overall latency within an MoE layer. Specifically, low-load experts remain idle while waiting for high-load experts to complete, leading to synchronization delays. Therefore, the latency of an MoE layer is given by: max({Ni}n i=1), (4) where Ni represents the number of tokens assigned to the i-th expert, with the total token allocation satisfying (cid:80)n i=1 Ni = tk. According to Eq. 4, the latency follows the Straggler Effect: the most burdened expert dictates the overall latency of the MoE layer. In the worst case, all tokens are assigned to the same group of experts, underutilizing the parallel processing capability of MoE. Conversely, distributing tokens evenly across experts maximizes computational efficiency and fully leverages the parallelism of multiple experts. With the bounds of the ideal and worst cases, the range of the highest load is given by: = tk . (3) max({Ni}n i=1) [ , k ]. (5) However, due to imbalanced token assignments, some experts may receive more or fewer tokens than the expected value. Figure 2: Expert-wise load, where each load value is divided by for clarity. To ensure generality, we visualize loads across different datasets. However, existing MoE models often adopt dropless strategy during inference, which fails to address token imbalance and can lead to significantly increased latency. Given that the imbalance stems from excessively highand low-load experts, we address this issue by exploring the following questions: (1) Are there redundant tokens assigned to High-Load Experts? That is, can the MoE model maintain its performance without these excess tokens? (2) Do LowLoad Experts matter? Should these experts be removed, or should their utilization be increased to sustain or even enhance overall efficacy?"
        },
        {
            "title": "4 Methodology",
            "content": "Figure 2 illustrates the normalized peak tokens load for each expert to accommodate all tokens Token Drop Regulates the Latency of HighLoad Experts To address the question about (a) Capacity-Aware Token Drop (b) Capacity-Aware Token Reroute Figure 3: Illustrations of (a) Capacity-Aware Token Drop and (b) Capacity-Aware Token Reroute. Token Drop mitigates overload by discarding excess tokens from overloaded experts, while Token Reroute redistributes these tokens to underutilized experts, enhancing load balancing. overloaded experts, we first regulate their maximum utilization. Specifically, we introduce expert capacity to control token allocation. Given capacity factor γ, the maximum number of tokens assigned to each expert (i.e., expert capacity) is defined as: = γ . (6) higher γ allows more tokens to be retained, but experts handling excessive tokens may introduce latency. Conversely, lower γ enforces stricter capacity limits, reducing latency by discarding more tokens, but at the risk of performance degradation. With the involvement of expert capacity γ, we constrain the upper bound of latency as follows: max({Ni}n i=1) = (cid:40) γ γ < 1 within [ , γ ] γ 1 , (7) where γ is typically much smaller than . This constraint ensures that no expert exceeds the specified capacity limit, effectively mitigating severe load imbalances and reducing latency. Specifically, when capacity constraint is imposed on each expert, experts must evaluate the volume of assigned tokens before execution. For experts with load below the predefined capacity, there is no difference between capacity-constrained inference and traditional inference. However, when the load exceeds the capacity, experts must discard excess tokens to adhere to the constraint. To address this, we introduce scoring function to evaluate each token: S(x) = s11 s21 ... st1 s12 s22 ... st2 . . . . . . ... . . . s1n s2n ... stn , (8) where sij denotes the importance score of the mapping from the i-th token to the j-th expert. With this score, each overflowed expert selectively discards those with lower scores. For the j-th expert: Kj = Nj C, τj = KthValue(S[:, j], Kj), (9) (10) where denotes the number of overflowed tokens, τ represents the K-th smallest value in S[:, j], serving as threshold to filter out excess tokens: Tj {t S[t, j] τj} S[Tj, j] 0, (11) (12) where Tj denotes the indices of the rejected tokens from the j-th Expert. We mask the scores of the rejected tokens and prevent them from being sent to their corresponding overflowed experts. Regarding the specific scoring function, we explore multiple efficient metrics and summarize them as follows: Order: Discarding later tokens once earlier tokens have filled the expert capacity. This strategy was first introduced in Switch-Transformer (Fedus et al., 2022) during training, and we extend it to the inference phase. Reverse Order: Instead of discarding later tokens, this approach removes earlier tokens to comply with the expert capacity constraint. Random: Dropping Excess tokens randomly to meet the predefined expert capacity constraints. Score: Using the gating score G(x) as an importance indicator and discarding tokens. Among these metrics, Order and Reverse Order are unstable, as shuffling sequences within batch may result in different tokens being dropped (Hayes et al., 2024). Random assumes all tokens have an equal probability of being dropped. In contrast, Score is stable, unaffected by sequence order within batch. Algorithm 1 Capacity-Aware Token Reroute Require: Expert Capacity C, Rounds Ensure: Updated Scores 1: for = 1, . . . , do 2: Top-k(S, k) 3: Indicator for top-k mappings 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: Sr Mask out the non-selected for = 1, . . . , do Nj Sr[:, j]0 Kj Nj Compute the load Number of excess tokens if > 0 then τj KthValue(Sr[:, j], Kj) Threshold for filtering Tj {t Sr[t, j] τj} Indices of retained tokens S[Tj, j] 0 Discard overflowed assignments end if end for 19: 20: end for Token Reroute Enhances the Utilization of Lowload Experts Token Drop exclusively targets overloaded experts by discarding overflowed tokens that exceed expert capacity but does not address the underutilization of low-load experts. Next, we introduce Token Reroute to ensure more balanced token-to-expert allocation. For tokens rejected by overflowed experts, we reselect underutilized experts based on the updated importance scores following Token Drop. As shown in Equation 12, masking the mappings to overflowed experts prevents these tokens from being reassigned to them, thereby encouraging their redistribution to other available experts. As illustrated in Algorithm 1, the process of masking and recomputing can be repeated iteratively, gradually achieving more balanced token-to-expert assignment. Notably, the extra cost of Token Routing is minimal, as it occurs before tokens are assigned to each expert and operates solely on the lightweight importance matrix. Additionally, overflowed experts and tokens that have already selected experts are excluded from subsequent iterations, further minimizing the additional computational cost of the Top-k operation."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct experiments under capacity-aware inference for MoE, with deployment details provided in Appendix A."
        },
        {
            "title": "5.1 Token Drop for High-load Experts",
            "content": "Investigation on Token Drop Metrics To assess the effectiveness of different metrics in regulating token load to the target capacity, we compare various approaches on OLMoE by discarding excess tokens and applying range of capacity factors. As shown in Table 1, varying the dropping metrics impacts performance at different levels. With higher capacities, the model maintains comparable performance even when using naive selection methods like Random. However, as the capacity factor decreases, performance degradation becomes more pronounced, particularly for Order, Reverse Order, and Random. Notably, Score consistently outperforms other methods by large margin, demonstrating the effectiveness of leveraging gating scores as an importance measure. Consequently, we adopt \"Score\" as the default metric. Efficiency Gains from Capacity-Constrained Inference We next explore the efficiency improvements achieved by imposing expert capacity. Specifically, we employ distributed inference using eight A30 GPUs, utilizing an 8-way Data Parallelism (DP) and 8-way Expert Parallelism (EP) strategy through the Megatron-LM framework (Shoeybi et al., 2019). In Mixtral-87B-Instruct model, each GPU hosts single expert, whereas, in models like OLMoE-Instruct, multiple experts must be deployed on single GPU (e.g., eight experts per GPU) due to GPU resource constraints. As illustrated in Figure 4, imposing constraints on expert capacity considerably accelerates inference across the four tested MoE models, in comparison to the baseline model without capacity limitations. The enhanced efficiency of each MoE layer (Figure 4 (a)) contributes to faster end-to-end inference (Figure 4 (b)). For instance, when the capacity factor γ is set to 1.5, this configuration achieves 1.94 speedup for single MoE module and 1.37 speedup for the entire end-to-end model. Furthermore, it is evident that the efficiency improves as the capacity decreases. Notably, the degree of acceleration is influenced by the numerical relationship between the number of experts and GPUs in Expert Parallelism. In Mixtral-87B-Instruct, one-to-one deployment Table 1: Performance comparison across different capacity factors and selection metrics (i.e., Order, Reverse Order, Random, and Score). The baseline operates without capacity constraints, represented as +. Due to inherent randomness, we report the average performance over multiple random seeds."
        },
        {
            "title": "Order\nReverse Order\nRandom\nScore",
            "content": "γ OBQA PIQA RTE WinoGrande BoolQ ARC-C HellaSwag MMLU Avg. + 45.6 3.0 2.0 1. 1.0 43.6 44.8 44.0 45.2 42.0 41.8 41.2 44.0 38.8 40.2 39.6 43.2 36.0 36.2 34.0 40.4 80. 75.8 76.1 76.3 79.9 71.5 71.8 75.2 79.7 67.1 67.3 72.1 76.1 60.2 59.5 63.1 71.5 53.7 53.1 50.9 52.0 51. 53.1 52.7 52.7 53.8 48.7 52.7 57.8 56.3 53.4 50.5 55.2 57.0 71.2 71.2 71.0 71.3 71.7 71.2 71.0 71.0 72. 68.5 70.1 68.3 69.9 62.6 63.3 60.8 64.3 74.7 74.4 74.8 75.0 74.4 74.2 73.9 74.1 74.2 73.3 72.7 73.8 73. 69.6 69.4 70.2 71.9 54.5 50.5 52.8 53.5 55.2 49.5 49.4 50.1 54.4 46.3 45.5 45.8 52.9 38.7 39.4 40.5 45. 79.4 78.6 78.7 78.9 79.2 76.6 76.4 76.8 78.4 54.0 54.4 74.2 77.1 58.0 58.7 66.9 72.0 52. 51.7 51.7 51.8 51.9 48.4 49.2 49.4 50.4 43.7 45.2 45.2 47.5 36.9 38.7 35.7 39.7 64.0 62.4 62.6 62.9 63. 60.8 60.8 61.3 63.4 55.1 56.0 59.6 62.1 51.9 52.0 53.3 57.8 (a) Single MoE Layer (b) End-to-End Model Figure 4: Speedup achieved by capacity-aware inference compared to the baseline without capacity constraints. maximizes the effectiveness of capacity-aware inference. Conversely, in models where multiple experts share single GPU, the acceleration gains are relatively modest. This occurs because the aggregated load from multiple experts diminishes the proportion of reduced load, which is achieved by limiting the straggler expert. Additionally, we believe that further system optimizations or allocating more GPUs for expert distribution would further enhance the potential of capacity-aware inference. Mitigating the Straggler Effect with Minimal Token Discarding Given that expert capacity enforces MoE layers to discard overflowed tokens, we next establish the relationship between expert capacity and the corresponding number of dropped tokens. Specifically, for capacity factor γ, the total proportion of dropped tokens is given by: DT = (cid:80)n i=1 ReLU(Ni γ ) i=1 Ni (cid:80)n , (13) where ReLU(Ni γ ) represent the number of dropped tokens for the i-th expert. Figure 5 visualizes the number of dropped tokens across different capacity factors for various test datasets, with more detailed illustration provided in Appendix C. Although the most overloaded expert receives much more tokens than the expected number of tokens , regulating the maximum capacity has limited impact on the overall number of accommodated tokens (e.g., reducing it by less than 20% under capacity factor of 2.0), thereby maintaining competitive performance even after discarding overflow tokens. Moreover, dropping small proportion of overflowed tokens can sigin inference speed (less than 5% speedup). These findings indicate that retaining low-load experts better preserves the performance of MoE models. Effectiveness of Token Reroute We examine the effectiveness of utilizing low-load experts by rerouting overflowed tokens to them (i.e., Token Reroute) instead of simply discarding these tokens to meet the target capacity. Comparing Token Reroute with Token Drop, redistributing excess tokens to low-load experts enhances performance, yielding 0.9% improvement in the average performance of Qwen1.5-MoE-Chat. Furthermore, considering the performance degradation observed in Expert Drop, our findings highlight the crucial role of low-load experts in maintaining model effectiveness."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "Model-Specific Imbalanced Property We explore the imbalance property in various models, such as OLMoE, DeepSeek-V2-Lite and Qwen1.5MoE, which differ in both architecture (e.g., depth and width) and training strategies (e.g., training from scratch (Muennighoff et al., 2024; DeepSeekAI et al., 2024a) vs. training after upcycling (Jiang et al., 2024; Team, 2024b)). On the one hand, our findings in Appendix reveal different training strategies result in significantly varying levels of imbalance. Specifically, MoE models trained from scratch exhibit much higher degree of imbalance. For instance, OLMoE and DeepSeek-V2-Lite experience peak expertwise token allocations exceeding 5 , whereas Qwen1.5-MoE and Mixtral are upcycled from dense language models, maintain more balanced distribution, with peak expert-wise allocations staying below 3 . This is because upcycling initializes experts with the same parameters (Komatsuzaki et al., 2023), while experts are randomly initialized when training from scratch, leading to greater imbalance in token assignments (Lo et al., 2024). On the other hand, despite the widespread use of auxiliary balance loss in MoE training, it does not guarantee balanced token assignments across experts, as token distribution still varies significantly during inference on test data. This necessitates integrating expert capacity into the inference process. Capacity Factor Beyond the specific capacity values presented in Table 1, we further investigate wide range of capacity factors in Figure 6, spanning from 0.0 to 3.0. We exclude values exceeding 3.0, Figure 5: Analysis of dropped tokens with respect to capacity factors. nificantly reduce the latency caused by overloaded experts (e.g., dropping 12% of overloaded tokens promotes the inference speed by 94% in Mixtral87B-Instruct), highlighting the effectiveness of capacity-constrained inference in improving both performance and efficiency."
        },
        {
            "title": "5.2 Token Reroute to Low-load Experts",
            "content": "Besides the experts overloaded with tokens, some low-load experts receive only few tokens, raising important questions: Are these low-load experts redundant and removable, or should they be leveraged to balance token allocation? Recent works (Lu et al., 2024; He et al., 2024) remove less important experts to improve efficiency, while our proposed Token Reroute increases their utilization by redistributing tokens for more balanced assignment. Next, we investigate the significance of low-load experts and validate the effectiveness of Token Reroute. The Critical Role of Low-Load Experts To explore the impact of low-load experts, we further compare dropping tokens (i.e., Token Drop) with skipping experts (i.e., Expert Drop). For Expert Drop, we adopt conservative strategy that dynamically skips the 10% of experts with the lowest token loads. Notably, the proportion of tokens removed in Expert Drop is significantly lower than in Token Drop (2% in Expert Drop vs. 12% in Token Drop on OLMoE-Instruct). Despite this, as shown in Table 2, Expert Drop experiences significant performance degradation and is outperformed by Token Drop by large margin. Moreover, due to the small proportion of tokens assigned to low-load experts, removing these experts provides only marginal improvements Table 2: Comparison of Expert Drop, Token Drop and Token Reroute. The capacity factor γ is set to 2.0 for OLMoE and DeepSeek-V2-Lite, and 1.5 for Qwen1.5-MoE-Chat and Mixtral-87B-Instruct. In Expert Drop, we only skip one out of eight experts for Mixtral-87B-Instruct, and 10% of the lowest load experts for other models. Model Method OBQA PIQA RTE WinoGrande BoolQ ARC-C HellaSwag MMLU GSM8K Avg. OLMoE-Instruct Qwen1.5-MoE-Chat DeepSeek-V2-Lite-Chat Mixtral-87B-Instruct Baseline Expert Drop Token Drop Token Reroute Baseline Expert Drop Token Drop Token Reroute Baseline Expert Drop Token Drop Token Reroute Baseline Expert Drop Token Drop Token Reroute 47.6 44.6 47.8 47.2 42.4 41.4 40.4 43.4 45. 41.8 45.2 45.4 47.4 46.8 46.4 47.8 80.2 76.9 77.9 79.4 79. 78.7 78.8 79.1 81.4 77.6 78.3 79.4 84.8 83.2 83.3 85.0 67. 64.0 64.6 66.3 72.9 71.2 72.6 72.6 72.6 71.9 72.6 73.3 71. 70.1 71.7 71.8 69.9 67.6 69.2 70.5 70.0 68.6 69.1 69.6 75. 72.5 74.0 75.4 82.5 81.3 82.2 83.0 80.7 78.2 80.0 80.9 81. 80.6 80.9 81.1 82.9 81.6 83.2 83.2 88.5 87.6 88.3 88.6 57. 54.4 57.2 57.1 54.1 52.9 53.0 53.4 61.0 57.1 59.3 60.4 71. 67.1 71.2 71.5 80.6 77.0 79.7 80.3 80.4 79.1 80.0 80.3 81. 75.5 80.9 81.5 87.5 85.6 87.4 87.6 52.8 50.6 51.5 52.3 59. 58.1 59.3 59.3 57.3 53.3 57.3 57.2 70.2 66.2 69.1 70.2 35. 31.6 32.4 34.4 52.0 49.4 51.9 52.1 66.4 56.0 62.7 64.1 64. 62.3 64.7 64.6 63.5 60.5 62.3 63.2 65.9 64.4 65.1 65.6 69. 65.3 68.2 68.9 74.3 72.2 73.8 74.5 (a) OLMoE (b) OLMoE-Instruct (c) Qwen1.5-MoE (d) Qwen1.5-MoE-Chat Figure 6: Performance change as capacity factors decrease from 3.0 to 0.0. as their performance closely aligns with capacityagnostic scenarios. By analyzing the performance changes when decreasing the capacity factor, we find that setting γ to 2.0 is sufficient to maintain performance comparable to the original models. This means allowing each expert to handle utmost twice the average number of tokens maintains the performance of capacity-free inference. However, maintaining the performance becomes challenging under low capacity factors (e.g., 1.0), as high-load experts experience significant token drops. Rerouting Round The Token Reroute Algorithm iteratively discourages tokens from selecting overflowed experts while encouraging their rerouting to other available experts, promoting more balanced token distribution. As iterations progress, lowerscore token-to-expert mappings are increasingly considered, further refining the redistribution process. Figure 7 illustrates the results across different iterations. In the first iteration, only excess tokens Figure 7: Ablation study on the number of iterations for Token Reroute. are dropped, while token rerouting becomes more prominent in subsequent rounds. The first two rounds of token rerouting lead to significant performance improvements, after which performance begins to saturate. We attribute this saturation to the selection of additional low-score token-to-expert mappings, which have smaller impact compared to the dominant token assignments."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we first identify the imbalanced tokento-expert assignment in Mixture of Experts (MoE) and introduce the Straggler Effect in MoE inference, where the highest-load expert becomes the efficiency bottleneck, dictating overall latency. To address this issue, we propose Capacity-Aware Token Drop, which alleviates excessive loads on individual experts, and Capacity-Aware Token Reroute, which enhances the utilization of underutilized experts. Our findings and proposed methods offer valuable insights and effective strategies for improving MoE inference efficiency."
        },
        {
            "title": "Limitations",
            "content": "Despite the progress we have made, our work still has certain limitations. First, our study focuses on subset of MoE models, including OLMoE, Qwen1.5-MoE, DeepSeek-V2-Lite, and Mixtral. promising direction for future work is to extend our analysis and methods to other architectures to further validate their effectiveness across broader range of MoE models. Second, our proposed Capacity-Aware Token Drop and CapacityAware Token Reroute strategies could be integrated into the training process, potentially enhancing both performance and efficiency."
        },
        {
            "title": "References",
            "content": "2019. Winogrande: An adversarial winograd schema challenge at scale. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. Piqa: Reasoning about physical commonsense in natural language. Preprint, arXiv:1911.11641. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. survey on mixture of experts. Preprint, arXiv:2407.06204. Zixiang Chen, Yihe Deng, Yue Wu, Quanquan Gu, and Yuanzhi Li. 2022. Towards understanding the In Admixture-of-experts layer in deep learning. vances in Neural Information Processing Systems. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. Preprint, arXiv:1905.10044. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. 2024. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066. DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu, Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. 2024a. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. Preprint, arXiv:2405.04434. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. 2024b. Deepseekv3 technical report. Preprint, arXiv:2412.19437. Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 55475569. PMLR. William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. framework for few-shot language model evaluation. Jamie Hayes, Ilia Shumailov, and Itay Yona. 2024. Buffer overflow in mixture of experts. In Neurips Safe Generative AI Workshop 2024. Shwai He, Liang Ding, Daize Dong, Boan Liu, Fuqiang Yu, and Dacheng Tao. 2023. PAD-net: An efficient framework for dynamic networks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1435414366, Toronto, Canada. Association for Computational Linguistics. Shwai He, Daize Dong, Liang Ding, and Ang Li. 2024. Demystifying the compression of mixtureof-experts through unified framework. Preprint, arXiv:2406.02500. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300. Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. 1991. Adaptive mixtures of local experts. Neural computation, 3(1):7987. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Michael Jordan and Robert Jacobs. 1994. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181214. Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. 2023. Sparse upcycling: Training mixture-of-experts from dense checkpoints. In The Eleventh International Conference on Learning Representations. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. {GS}hard: Scaling giant models with conditional In Internacomputation and automatic sharding. tional Conference on Learning Representations. Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, and Jie Fu. 2024. closer look into mixture-of-experts in large language models. Preprint, arXiv:2406.18219. Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, and Hongsheng Li. 2024. Not all experts are equal: Efficient expert pruning and skipping for mixture-of-experts large language models. Preprint, arXiv:2402.14800. Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, and Yang You. 2024a. Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. Preprint, arXiv:1809.02789. Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, and Mahesh Marina. 2024b. Moe-infinity: Activation-aware expert offloading for efficient moe serving. Preprint, arXiv:2401.14361. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? Preprint, arXiv:1905.07830. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M. Dai, Zhifeng Chen, Quoc Le, and James Laudon. 2022. Mixtureof-experts with expert choice routing. In Advances in Neural Information Processing Systems. Tong Zhu, Xiaoye Qu, Daize Dong, Jiacheng Ruan, Jingqi Tong, Conghui He, and Yu Cheng. 2024. Llama-moe: Building mixture-of-experts from llama with continual pre-training. arXiv preprint arXiv:2406.16554. Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906. Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, and Hannaneh Hajishirzi. 2024. Olmoe: Open mixture-of-experts language models. Preprint, arXiv:2409.02060. OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems, 34:85838595. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Outrageously large neural netDean. 2017a. works: The sparsely-gated mixture-of-experts layer. Preprint, arXiv:1701.06538. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017b. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053. Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. Powerinfer: Fast large language model Preprint, 2023. serving with consumer-grade gpu. arXiv:2312.12456. Gemini Team. 2024a. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Qwen Team. 2024b. Qwen1.5-moe: Matching 7b model performance with 1/3 activated parameters\". Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: multi-task benchmark and analysis platform for natural language understanding. In the Proceedings of ICLR."
        },
        {
            "title": "A Implementation Details",
            "content": "Models We mainly focus on lightweight MoE models (less than 20B parameter budget). We conduct experiments on OLMoE (Muennighoff et al., 2024), Qwen1.5-MoE (Team, 2024b), DeepSeekV2-Lite (DeepSeek-AI et al., 2024a) and Mixtral (Jiang et al., 2024), due to their competitive performance and widespread adoption. Datasets To evaluate model performance, we report normalized zero-shot or few-shot accuracy on the LM-Harness benchmark. The number of shots for each task is detailed in Table 3, which includes multiple tasks: ARC-C (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al., 2021), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2019), RTE (Wang et al., 2019), WinoGrande (ai2, 2019) and GSM8K (Cobbe et al., 2021). The evaluation code is based on EleutherAIs LM Harness framework (Gao et al., 2023). Table 3: Experimental settings for evaluation tasks. Norm refers to the normalization performed with respect to the length of the input."
        },
        {
            "title": "Task",
            "content": "Number of few-shot"
        },
        {
            "title": "Metric",
            "content": "BoolQ RTE OBQA PIQA MMLU WinoGrande GSM8K HellaSwag ARC-C 0 0 0 0 5 5 5 10 25 Accuracy Accuracy Accuracy (Norm) Accuracy (Norm) Accuracy Accuracy Exact Match Accuracy (Norm) Accuracy (Norm) Layer-wise Expert Load To analyze imbalanced token assignments, we measure the expert load for each expert by tracking the peak expert load while running MoE models on various test datasets. Figure 8, 9, 10 and 11 present the full results for the normalized layer-wise expert load for OLMoE, DeepSeek-V2, Qwen1.5-MoE, and Mixtral-87B-Instruct, respectively."
        },
        {
            "title": "C Dropped tokens Calculation",
            "content": "Based on Equation 13, we calculate the total number of dropped tokens across experts in each layer under different capacity factors, as illustrated in Figures 12, 14, 13, and 15. Figure 8: Layer-wise expert load in OLMoE-Instruct. Figure 9: Layer-wise expert load in Deepseek-V2-Lite. Figure 10: Layer-wise expert load in Qwen1.5-MoE-Chat. Figure 11: Layer-wise expert load in Mixtral-87B-Instruct. Figure 12: Dropped tokens with respect to capacity factors in OLMoE-Instruct. Figure 13: Dropped tokens with respect to capacity factors in DeepSeek-V2-Chat. Figure 14: Dropped tokens with respect to capacity factors in Qwen-1.5-MoE-Chat. Figure 15: Dropped tokens with respect to capacity factors in Mixtral-87B-Instruct."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "University of Maryland, College Park"
    ]
}