{
    "paper_title": "LLaMA-Berry: Pairwise Optimization for O1-like Olympiad-Level Mathematical Reasoning",
    "authors": [
        "Di Zhang",
        "Jianbo Wu",
        "Jingdi Lei",
        "Tong Che",
        "Jiatong Li",
        "Tong Xie",
        "Xiaoshui Huang",
        "Shufei Zhang",
        "Marco Pavone",
        "Yuqiang Li",
        "Wanli Ouyang",
        "Dongzhan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents an advanced mathematical problem-solving framework, LLaMA-Berry, for enhancing the mathematical reasoning ability of Large Language Models (LLMs). The framework combines Monte Carlo Tree Search (MCTS) with iterative Self-Refine to optimize the reasoning path and utilizes a pairwise reward model to evaluate different paths globally. By leveraging the self-critic and rewriting capabilities of LLMs, Self-Refine applied to MCTS (SR-MCTS) overcomes the inefficiencies and limitations of conventional step-wise and greedy search algorithms by fostering a more efficient exploration of solution spaces. Pairwise Preference Reward Model~(PPRM), inspired by Reinforcement Learning from Human Feedback (RLHF), is then used to model pairwise preferences between solutions, utilizing an Enhanced Borda Count (EBC) method to synthesize these preferences into a global ranking score to find better answers. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior performance in terms of search efficiency and problem-solving capability compared to existing methods like ToT and rStar, particularly in complex Olympiad-level benchmarks, including GPQA, AIME24 and AMC23."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . [ 2 4 8 8 2 0 . 0 1 4 2 : r LLaMA-Berry: Pairwise Optimization for Olympiad-level Mathematical Reasoning via O1-like Monte Carlo Tree Search Di Zhang1,2*, Jianbo Wu3, Jingdi Lei2, Tong Che4 Jiatong Li5, Tong Xie6, Xiaoshui Huang7, Shufei Zhang2, Marco Pavone8 Yuqiang Li2 , Wanli Ouyang2, Dongzhan Zhou2 1Fudan University, 2Shanghai Artificial Intelligence Laboratory, 3University of California, Merced 4Independent Researcher, 5Hong Kong Polytechnic University, 6University of New South Wales 7Shanghai Jiao Tong University, 8Stanford University {liyuqiang,zhoudongzhan}@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "This paper presents an advanced mathematical reasoning framework, LLaMA-Berry, for enhancing the problem-solving ability of large language models (LLMs). The framework combines Monte Carlo Tree Search with SelfRefine (SR-MCTS) to optimize the reasoning paths and utilizes pairwise reward model to evaluate different paths globally. By leveraging the self-critique and rewriting capabilities of LLMs, our SR-MCTS overcomes the inefficiencies and limitations of conventional stepwise and greedy search algorithms by fostering more efficient exploration of solution spaces. To guide the search process, we propose Pairwise Preference Reward Model (PPRM) to predict pairwise preferences between solutions through instruction-following capabilities trained by Reinforcement Learning from Human Feedback (RLHF). Finally, the Enhanced Borda Count (EBC) method is adopted to synthesize pairwise preferences into global quantile scores for evaluations. This approach addresses the challenges of scoring variability and non-independent distributions in mathematical reasoning tasks. The framework has been tested on general and advanced benchmarks, showing superior search efficiency and performance compared to existing open-source and closed-source methods, particularly in complex Olympiad-level benchmarks, including AIME24 and AMC23."
        },
        {
            "title": "Introduction",
            "content": "Mathematical reasoning represents great challenge in artificial intelligence, with broad applications across automated theorem proving, mathematical problem solving, and scientific discovery (Ahn et al., 2024). Recently, significant strides have been made by large language models (LLMs) like GPT4 (Achiam et al., 2023) in general mathematical *These authors contributed equally. Corresponding author 1 tasks involving arithmetic and geometric problemsolving (Cobbe et al., 2021; Sun et al., 2024; Ying et al., 2024). However, complex mathematical reasoning remains challenging, especially at the Olympiad-level benchmarks such as AIME (MAA, 2024). An intuitive approach to improving problemsolving is to break solutions into step-by-step reasoning paths (Lightman et al., 2023; Luo et al., 2024a), as demonstrated in Chain-of-Thought (CoT Wei et al., 2022). While prompt-based methods can effectively facilitate the construction of such reasoning paths, they may still encounter challenges due to the lack of comprehensive feedback during the generation process, which can affect efficiency (Paul et al., 2023). In contrast to stepwise generation methods, another promising line of research treats the entire solution as an independent state, employing rewriting capabilities to refine the solutions, such as in Self-Refine (Madaan et al., 2023a) and Reflexion (Shinn et al., 2024). However, these approaches, while innovative, may occasionally face challenges like being susceptible to local optima or potentially drifting towards suboptimal solutions due to flawed feedback, which could impact their maximum potential performance. In addition to generating reasoning paths, effective solution evaluation is crucial, with models like the outcome reward model (ORM) and process reward model (PRM) (Uesato et al., 2022) serving as valuable examples. The ORM focuses on the correctness of the final answer in reasoning path, while the PRM emphasizes the correctness of each step in the process. While both methods enable reward models to assign scalar scores, obtaining reliable labeled data for training these reward models remains significant challenge. Moreover, the scoring standards for mathematical reasoning tasks can vary significantly, as each problem presents unique characteristics. This variation complicates the scaling of reward models and hinders their abilFigure 1: The main pipeline of LLaMA-Berry, where Si stand for problem-solving solutions and Ci stands for critiques. The pipeline consists of four phases detailed in Section 2.2, including selection, expansion, evaluation, and backpropagation. ity to capture local preference relations between solutions. Although trained using language models, these reward models have yet to fully leverage instruction-following capabilities, which may limit their effectiveness in handling more complex reasoning tasks (Zhang et al., 2024a). To improve the efficiency of solution search in mathematical problems, we treat complete solution as an independent state and apply Self-Refine to optimize previous solutions in order to obtain better ones. In the Self-Refine process, feedback from critiques is utilized to make the search more efficient compared to stepwise reasoning path generation. Furthermore, we incorporate Monte Carlo Tree Search (MCTS Kocsis and Szepesvári, 2006) to replace the iterative manner in naive Self-Refine, enhancing the solution search. MCTS leverages signals from the evaluation process to assess solutions and uses the Upper Confidence Bound applied to Trees (UCT) method to balance exploration and exploitation. This approach enables the search process to effectively exploit higher-quality solutions and explore those with greater potential for improvement, while avoiding getting trapped in suboptimal local minima. In evaluation process, utilizing the instructionfollowing capabilities trained by Reinforcement Learning from Human Feedback (RLHF Christiano et al., 2017), Pairwise Preference Reward Model (PPRM) transforms the absolute rewards calculation into preferences prediction between solutions to calculate rewards. The approach reduces the variability with scoring characteristics and thus leads to more robust and consistent evaluation of different solutions. To overcome the locality limitations inherent in pairwise comparisons, we employ the Enhanced Borda Count (EBC) method to aggregate local preference evaluations into global quantile scores, leading to more informed decisionmaking and, ultimately, better solutions. Combining the PPRM and EBC method not only enables the reward model to learn more robust reward signal but also captures the global characteristics of the solution space, ensuring more reliable comparisons. Our contributions are summarized as follows: (1) We propose SR-MCTS, novel Markov Decision Process (MDP) framework that treats entire solutions as states and Self-Refine as optimization action to perform advanced solution search with MCTS. (2) PPRM is developed to leverage the preference relationship between solutions to evaluate their quality, which avoids the volatility of absolute scores while providing more guided exploration of optimal paths. We adopt the EBC method to convert the local preferences into global evaluations. (3) We verify the effectiveness of LLaMA-Berry on multiple benchmarks, which outperforms baseline approaches like ToT (Yao et al., 2024) and rStar (Qi et al., 2024) in both search efficiency and accuracy. Notably, LLaMA-Berry enhances the performance of LLaMA-3.1-8B, making the 8B-level model comparable to proprietary models, including GPT-4 Turbo on Olympiad-level mathematical reasoning without additional training."
        },
        {
            "title": "2 Methodology",
            "content": "2.1 Preliminary One of the core challenges in mathematical problem-solving is to generate and optimize reasoning paths to derive high-quality solutions. We formalize this process in path-wise Markov Decision Process (MDP) framework, where each state in the state space represents complete solution to given problem, and the action space consists of all feasible rewriting actions that make transitions between states. In the framework, we aim to quantify the expected reward Q(s, a) from executing action at state s, that is, Q(s, a) = E[R(s)s = (s, a)] (1) , where (s, a) indicates the transition from to another solution via the rewriting action a. Our primary objective is to identify the optimal state that represents the best solution. We can reach by selecting actions that maximize the reward, guiding us toward the most desirable outcome, as demonstrated in Equation 2. = arg max sS Q(s) (2)"
        },
        {
            "title": "2.2 Self-Refine applied to MCTS",
            "content": "As shown in Figure 1, SR-MCTS integrates Monte Carlo Tree Search (MCTS) with the Self-Refine mechanism to continuously evaluate and optimize the solution search. This integration leverages the iterative nature of MCTS and the self-improvement capabilities of LLMs, thereby improving the search outcomes. Monte Carlo Tree Search (MCTS) is an effective method within the Markov Decision Processes (MDP) framework, employing states, actions, and value functions through sampling. The algorithm follows four key steps: selection, expansion, evaluation, and backpropagation. In the selection phase, the root node is expanded using the Upper Confidence Bound applied to Trees (UCT) algorithm, which selects node by balancing exploration and exploitation: (cid:32) = arg max aA(s) Q(s, a) + (cid:115) (cid:33) ln (s) (s, a) (3) , where (s) is the visitation count of node s, (s, a) is the action frequency, and is parameter controlling exploration. In the expansion phase, node generates subsequent states s, added as new nodes in the tree . The evaluation phase typically uses simulations or heuristics to estimate the Q-values for these nodes. Finally, during backpropagation, the estimated Q-values are updated retroactively from the leaf nodes to the root. This iterative process allows MCTS to refine decisionmaking by balancing the exploration of new paths with the exploitation of known high-value paths. Selection phase. The selection phase identifies node si from the search tree for expansion, where each node represents complete solution state. The Upper Confidence Bound applied to Trees (UCT) algorithm is employed to select the optimal node, with dynamic pruning used to avoid local optima. node si is considered fully explored when its child nodes reach predefined limit, and at least one child nodes value exceeds the value of si. Expansion phase. In the expansion phase, as shown in Figure 1, the selected answer si is expanded by generating successor answers through Self-Refine process, which includes Criticizing and Rewriting process. The Criticizing process generates critique ci = C(si) that identifies drawbacks (e.g., mathematical wrongs or logical faults) in the current chosen answer Si, and then Rewriting process generates new answer si+1 = R(si, ci). In practice, to simplify the problem, we assume this process is deterministic, ensuring that the same original state of solutions si consistently produces the same successor state of solution si+1. The new state of solution is then added to the search tree as new node. Evaluation phase. The evaluation phase calculates the value Q(s) of the newly generated node using the Pairwise Preference Reward Model (PPRM). The evaluation involves two steps: global and local value assessments. The global value Qg(s) is determined by the quantile of in win-loss preference matrix M, which reflects the win-loss relationships between nodes. The local value Ql(s) is derived from comparisons with adjacent nodes in the search tree . The total value Q(s) is then computed as weighted combination of global and local values: Q(s) = αQg(s) + (1 α)Ql(s), where α controls the relative influence of each component. Backpropagation phase. In the backpropagation phase, the value Q(s) of the new node is propagated back to its parent node si, updating value Figure 2: Preference prediction process of PPRM and global quantile score based on Enhanced Borda Count method. of si as discounted sum of its child nodes values: Q(si) = (1 γ)Q(si) + γQ(s). The discount factor γ represents the importance of future rewards. This iterative update mechanism ensures that the values of parent nodes are progressively refined, enhancing the guidance for future selections. Additionally, to control the growth of the search tree, the SR-MCTS method restricts the maximum number of rollouts Nmax . The search process terminates when the restriction is reached, imposing limits on the unbounded expansion of the tree. The overarching objective of SR-MCTS is to maximize the expected highest value of all existing nodes S, guiding us towards the most desirable outcome s, ensuring that the search process efficiently converges to high-quality solutions."
        },
        {
            "title": "2.3 Pairwise Preference Reward Model",
            "content": "Reliable evaluation of different solutions plays crucial role in mathematical problem-solving tasks as it leads to better estimation of Q-values, thereby offering better guidance. Existing reward models typically evaluate solutions by giving absolute scores, such as process reward model (PRM Lightman et al., 2023) and outcome reward model (ORM Yu et al., 2023). However, the score-based reward models may fall short in leveraging the instruction-following capability of LLMs or effectively handling the variations in scoring standards, especially when the differences between solutions are subtle. To address this issue, we propose the Pairwise Preference Reward Model (PPRM), which leverages comprehensive preference dataset incorporating substantial samples from both PRM and ORM approaches (Toshniwal et al., 2024; Lightman et al., 2023) to learn preference relationships among mathematical solutions. For two solutions (a1 and a2) to given mathematical problem, we use a1 a2 to represent the situation where a1 is preferred over a2. PPRM predicts their relative quality using pairwise partial ordering, represented by the following probability formula: (a1 a2 ϕ) = eϕ(a1) eϕ(a1) + eϕ(a2) (4) , where (a1 a2 ϕ) denotes the probability of partial ordering relation between solution a1 and a2, with ϕ representing the parameters of the reward model. In our method, a1 a2 are represented by tokens of an LLM, and (a1 a2 ϕ) is estimated using the logits value of tokens calculated by the LLM. Then, inspired by advancements in Language Interface Fine-Tuning (LIFT Dinh et al., 2022), we frame the training process of PPRM as question-answering task to leverage the instructionfollowing capabilities of LLMs. The model is tasked with answering the question, \"For Question Q, is solution a1 better than solution a2?\" as shown in Figure 2. To form robust training objective, the predicted token labels ˆy (Yes or No) are evaluated with ground truth label using the indicator function I: I(ˆy, y) = (cid:40) 1, 0, if ˆy = if ˆy = (5) Finally, pairwise preference dataset that contains millions of mathematical problem-solving solution pairs is converted into dataset suitable for question-answering task. We employ RLHF techniques to train the model to improve its performance in the partial-order prediction questionanswering task. Subsequently, the Direct Preference Optimization (DPO Rafailov et al., 2024) method is utilized to find the optimal Pϕ by maximizing the objective arg maxϕ EP [I(ˆy, y)]. Please refer to Appendix for details about training and inference of PPRM. 4 2.4 Enhanced Borda Count Method Although PPRM allows us to directly compare the quality of two solutions, we still need to convert these local preferences into cohesive global ranking to gain comprehensive evaluation for the answers. This conversion process can be formalized as the global optimal ranking aggregation (GORA) problem related to Learning to Rank (LtR) methods. Further, we propose the Enhanced Borda Count (EBC) method based on the transitivity assumption of mathematical problem solutions, which integrates the naive Borda Count algorithm with transitive closure of preferences calculated by the Floyd-Warshall (Warshall, 1962) algorithm. For formalized discussion, please refer to Appendix H. Local preference calculation. First, the PPRM generates preference matrix Rnn for all problem solutions, where Mi,j = 1 indicates that solution ai is superior to solution aj, and Mi,j = 0 otherwise. This process can be represented as: among nodes with equal Borda counts. Specifically, for two nodes vi and vj with equal Borda counts, the soft comparison rule can be denoted as vi vj (ai aj) > (aj ai). This process ensures that the ranking remains consistent and reasonable even in the presence of cycles or local ambiguities. Global quantile score of solutions. Finally, the ranking is converted into global quantile score Qg for each solution is Qg(v) = 1 rank(v)1 1 , where rank(v) is the position of in the ranking based on Borda counts, and is the total number of nodes. To reflect local advantages in the search tree structure, the local win rate Ql(v) for node is calculated in with its children nodes Childrenv as follows: Ql[v] = (cid:80) uChildren[v] C[u, v] Children[v] (7) Finally, score Q(v) for solution is weighted sum of local win rate Ql(v) and global quantile score Qg. Mi,j = (cid:40) 1, 0, if (ai aj) 0.5 if (ai aj) < 0.5 (6)"
        },
        {
            "title": "3 Evaluation",
            "content": "As shown in Figure 2, this matrix can be viewed as an adjacency matrix of directed graph = (V, E), where each solution ai corresponds to vertex vi, and an edge = (vi, vj) exists if Mi,j = 1, indicating that solution ai is preferred over aj. Transitive closure. To simplify the problem, we adopt the assumption of transitivity for mathematical solutions, that is, if vi vk and vk vj, then vi vj. Under this assumption, the transitive closure of preference matrix can be computed through Floyd-Warshall algorithm, e.g., if Mi,k = 1 and Mk,j = 1, then Mi,j = 1. Borda count-based global ranking. Next, based on the transitive closure matrix C, we apply the Enhanced Borda Count method for global ranking. The Enhanced Borda Count determines the ranking of each node by calculating its out-degree, which corresponds to the number of nodes it defeats. For each node vi, the Borda(vi) is defined as (cid:80)n j=1 Ci,j, like the ranked node listed in Figure 2. Nodes with higher Borda counts are ranked higher. However, in practice, cyclic preferences can cause efficiency issues with the naive Borda Count method. To further refine the ranking, we devise re-ranking stage, where the logits generated by the PPRM are used for soft comparison"
        },
        {
            "title": "3.1 Experiment Settings",
            "content": "Settings. To better evaluate the effectiveness of our approach, we select LLaMA-3.1-8B-Instruct model (Meta, 2024b) as the base model for SRMCTS, without any additional training. We also train Gemma2-2B-Instruct model (Google, 2024) as PPRM to provide reward signals during the search process. We develop the Berry-Tree inference framework to ensure robust and efficient inference, which supports advanced features, including fault tolerance, checkpoint recovery, multiquery concurrency, and automatic multi-server load balancing. Hyper-parameter settings are detailed in Appendix A. Grading. We evaluate algorithm-generated answers using the correctness evaluation standards as in Lightman et al. (2023), focusing on format adherence and content accuracy. The model is provided with prompt specifying the expected answer format. We score answers as consistent if they exactly match the ground truth, closely approximate it numerically, or are equivalent in symbolic form. To ensure comprehensive and rigorous evaluation, we adopt major@k (Kuncheva, 2014) and rm@k (Yang et al., 2024c), which can be unified as the solved rate of problems (Lightman et al., 2023; 5 Table 1: Performance comparison of models across benchmarks of different difficulties, as represented by GaoKao2023En (Liao et al., 2024), College Math (Tang et al., 2024), and OlympiadBench (He et al., 2024), which range from high school to Olympiad levels. Scores denoted with subscripted notations, such as maj@8, represent specific metrics, with major@8 as an example. Scores without subscripted notations reflect the models greedy performance evaluated in zero-shot CoT manner. Benchmark Model GSM8K MATH GaoKao2023En OlympiadBench College Math MMLU STEM Qwen2-7B-Instruct (Yang et al., 2024a) Meta-Llama-3.1-8B-Instruct (Meta, 2024b) Qwen2-72B-Instruct (Yang et al., 2024a) Meta-Llama-3.1-70B-Instruct (Meta, 2024a) DeepSeekMath-7B-RL (Shao et al., 2024) Internlm2-math-plus-7b (Ying et al., 2024) Mathstral-7B-v0.1 (Mistral AI, 2024) NuminaMath-7B-CoT (Beeching et al., 2024b) Qwen2-Math-7B-Instruct (Yang et al., 2024a) NuminaMath-72B-CoT (Beeching et al., 2024a) Qwen2-Math-72B-Instruct (Yang et al., 2024a) Meta-Llama-3.1-8B-Instruct (Meta, 2024b) + LLaMA-Berry (Ours)@8 +LLaMA-Berry (Ours)@16 85.7 76.6 93.2 94.1 88.2 84.0 84.9 75.4 89.9 90.8 96.7 89.8maj@8 94.9rm@8 96.1rm@16 52.9 47.2 69.0 65.7 52.4 54.4 56.6 55.2 75.1 66.7 84.0 54.8maj@8 69.4rm@8 75.3rm@16 36.4 30.1 58.7 54.0 43.6 50.1 46.0 47.5 62.1 58.4 68.3 36.4maj@8 61.6rm@8 68.6rm@16 21.3 15.4 33.2 27.7 19.0 18.8 21.5 19.9 38.2 32.6 43.0 24.8maj@8 47.2rm@8 55.1rm@ 24.5 33.8 43.2 42.5 37.5 36.2 33.7 36.9 45.9 39.7 47.9 36.4maj@8 63.7rm@8 68.9rm@16 68.2 60.5 84.4 80.4 64.8 55.2 64.0 60.8 63.8 64.5 79.9 68.3maj@8 82.9rm@8 88.3rm@16 Luo et al., 2024a). The evaluation methods and metric details are further outlined in Appendix B."
        },
        {
            "title": "3.2 Benchmarks",
            "content": "General mathematical reasoning benchmarks. We summarize the results on general mathematical reasoning benchmarks in Table 1, which indicates that our method significantly boosts the base models performance. The results consistently demonstrate improvements across various levels of difficulty. Specifically, the solved rate of problems in 16 rollouts of Meta-Llama-3.1-8BInstruct (Meta, 2024b) has been improved by more than 35% on three benchmarks. Qwen2-Math-72BInstruct (Yang et al., 2024b) exhibits the strongest mathematical reasoning capability among the competing methods, while our LLaMA-Berry, built on base model with only 8B parameters, exceeds it in the solved rate of problems on four benchmarks. In particular, LLaMA-Berry reaches 55.1% on OlympiadBench and 68.9% on College Math, surpassing it by 11.9% and 21%, respectively. Cutting-edge mathematical Olympiad benchmarks. In Table 2, we compare the performance of LLaMA-Berry with other leading models on Olympic-level benchmarks. The results demonstrate that LLaMA-Berry is highly competitive on these benchmarks, demonstrating its capability in complex reasoning. Notably, on the most challenging AIME2024 benchmark, our method boosts the base models solving rate from 2/30 to 8/30, surpassing typical open-source models and commercial closed-source models, except for the OpenAI o1 series (OpenAI, 2024). In addition to excelling in mathematical reasoning, our approach also excels across various science and engineering domains. For example, it achieves top performance on benchmarks such as MMLU STEM (Hendrycks et al., 2021a) in Table 1 and GPQA diamond (Rein et al., 2024) in Table 2. This demonstrates the methods robustness and versatility, enabling it to tackle wide range of technical challenges and highlighting its potential for broader applications in both research and practical scenarios. Table 2: Performance comparison across multiple olympiad benchmarks, including AIME24 (MAA, 2024), AMC23 (MAA, 2023), Math Odyssey (Fang et al., 2024), and GPQA Diamond (Rein et al., 2024). Model Claude 3 Opus GPT 4 Turbo GPT 4o OpenAI o1 Preview OpenAI o1 Gemini 1.5 Pro Gemini Math-Specialized 1.5 Pro 23.3 Meta-LLaMA-3.1-8B-Instruct Qwen2-Math-7B-Instruct NuminaMath-72B CoT Qwen2-Math-72B-Instruct Meta-Llama-3.1-8B-Instruct + LLaMA-Berry (Ours)@8 +LLaMA-Berry (Ours)@16 Benchmarks AIME24 AMC23 Math Odyssey GPQADiamond 6.7 3.3 13.4 56.7 83.3 6. 42.0 15.7 62.5 52.5 60.0 40.6 47.0 45.0 55.8 41.7 6.7 13.3 3.3 20.0 13.3maj@8 22.9maj@8 44.2maj@8 16.7rm@8 48.2rm@8 60.4rm@8 26.7rm@16 54.2rm@16 65.0rm@16 50.4 38.8 56.1 78.3 78.0 30.4 39.4maj@8 77.3rm@8 92.4rm@16 Comparison with other tree-based or CoT methods. We compare our algorithm with other treebased reasoning methods and CoT-based methods on GSM8K (Cobbe et al., 2021), GSMHard (Gao et al., 2022), and MATH500 (Lightman et al., 2023) which is representative and highly challenging 10% subset of MATH (Hendrycks et al., 2021b) Table 3: Performance of different tree-based methods for LLaMA-3-8B-Instruct on GSM8K, GSMHARD, and MATH500 benchmarks. Benchmark Method Zero-Shot CoT Few-shot CoT One-turn Self-Refine Self-Cons@8 Self-Cons@64 Self-Cons@128 ToT@32 RAP@32 rStar@32 LLaMA-Berry (Ours)@8 LLaMA-Berry (Ours)@ GSM8K GSMHARD MATH500 14.9 25.6 26.5 28.5 30.3 31.2 19.6 29.6 68.4 74.5 75.7 78.4 83.2 84.7 69.1 80.6 88.7maj@32 33.4maj@32 30.2maj@8 86.4maj@8 37.1rm@8 94.1rm@8 88.1maj@16 31.5maj@16 41.1rm@16 96.4rm@16 5.8 17.8 25.0 30.0 33.0 33.8 13.6 18.8 38.3maj@32 35.2maj@8 56.4rm@8 39.6maj@16 63.8rm@16 benchmark. As shown in Table 3, the performance of RAP (Hao et al., 2023) and ToT (Yao et al., 2023) tends to degrade relative to more straightforward methods like Few-shot CoT and One-turn SelfRefine when the difficulty increases from GSM8K to GSMHard. We suspect the reason could be the weak self-evaluation capability of LLMs, which may guide reasoning steps to the inefficient side. Moreover, tree-based methods can incur more computational overhead than straightforward methods. In contrast, rStar (Qi et al., 2024) and our method maintain positive output performance trend, highlighting both approaches higher search efficiency. To make fair comparison between the reported results on LLaMA-3-8B-instruct from rStar (Qi et al., 2024), self-consistency (Wang et al., 2022), and our algorithm, we also utilize the LLaMA-38B-instruct as the base model instead of the 3.1 version. We observe that our approach achieves on-par or even better performance with fewer rollouts. Specifically, our method achieves an accuracy of 88.1%, 31.5%, and 39.6% on GSM8K, GSMHARD, and MATH500 benchmarks, respectively, using the majority voting metric, which is among the same accuracy level as others while only consuming 1/2 of the rollout times of rStar and 1/8 of Self-consistency. This provides compelling validation of the efficacy of the EBC method and the aggressive exploration fostered by the dynamic pruning strategy."
        },
        {
            "title": "3.3 Ablation Study",
            "content": "As shown in Table 4, we conduct ablation experiments to evaluate the key components of LLaMA-Berry, using the solved rate of problems (Luo et al., 2024a) as metric across benchmarks of increasing difficulty: GSM8K and Table 4: Ablation study for LLaMA-Berry framework. The PPRM and MCTS components are removed to evaluate their effectiveness. We use the solved rate of problems as the metric and the base model is LLaMA-3.18B-Instruct. GSM8K 76.6 AIME24 6.7 Zero-shot CoT 16 Rollouts 6.7 Iterative Self-Refine SR-MCTS without PPRM 6.7 SR-MCTS + PPRM (Ours) 95.0 96.1 16.7 26.7 16 78.9 78.2 82.0 81. 8 6.7 6.7 8 AIME2024. Zero-shot CoT represents the base models greedy mathematical reasoning capabilities. SR-MCTS without PPRM refers to basic version of our method that uses self-evaluation as the reward instead of PPRM and EBC. When comparing iterative Self-Refine with SRMCTS, especially on the GSM8K dataset, the introduction of MCTS effectively mitigates the issue of solution degradation into suboptimal results caused by flawed critiques in iterative methods. With rollouts of 8 and 16, SR-MCTS improves the solved rate of problems by 3.1% and 3.2%, respectively. Furthermore, in comparing SR-MCTS without PPRM and SR-MCTS with PPRM, PPRM further boosts the solved rate of problems on the relatively easier GSM8K dataset. With rollouts of 8 and 16, the solved rate of problems is elevated by 13% and 14.7%, respectively. Notably, on the more challenging AIME2024 dataset, both iterative Self-Refine and SR-MCTS without PPRM demonstrate limited search efficiency. However, SR-MCTS with PPRM can significantly improve the solved rate of problems from 6.7% (2/30) to 16.7% (5/30) and 26.7% (8/30) with rollouts of 8 and 16, respectively. The results underscore the efficacy of combining the SelfRefine method with PPRM when addressing complex problems. The contrast between self-reward and PPRM underscores the importance of designing reward mechanisms that can more effectively guide the search process. PPRM provides more holistic incentive to the model, thus fostering more effective problem-solving strategies."
        },
        {
            "title": "3.4 Scaling Study",
            "content": "To explore the potentials and trends of the scaling with rollouts in inference-time, we depict the solved rate of problems with rollouts in three benchmarks with different difficulty levels. Analyzing the performance alongside Figure 3, the increment in the number of rollouts consistently enhances 7 Figure 3: Scaling of inference-time rollouts model performance across various benchmarks, and the extent of these improvements differs depending on the benchmarks complexity and the base models reasoning capability. These curves underscore that the performance of the LLaMA-Berry framework benefits from scaling up rollouts during inference, similar to the observations in OpenAI (2024). However, there are ceiling limitations, as seen in the GSM8K dataset, which suggest that the base models capabilities in both reasoning and refinement play crucial role in determining the overall performance."
        },
        {
            "title": "4 Related Works",
            "content": "Reward models for reasoning. Reliable reward models (Kang et al., 2024; Wang et al., 2023; Havrilla et al., 2024; Lightman et al., 2023; Ma et al., 2023) can effectively distinguish desirable responses from undesirable ones, which is especially important in complex reasoning. The outcome reward model (ORM) are trained with the final results of the reasoning paths. As the rewards are determined by the final answers, ORM may suffer from coarse supervision and misalignment issues. In contrast, process reward model (PRM) provides step-wise reward signals that are easier to interpret and guide the models to follow the CoTs. Therefore, PRM is generally considered to be more effective (Lightman et al., 2023). However, the success of PRM relies on extensive manually annotated data (Luo et al., 2024a; Havrilla et al., 2024), which is time-consuming and still faces the challenge of the volatility of absolute reward scores. Pairwise Preference Reward Model (PPRM) in LLaMA-Berry converts absolute scoring into preference prediction task, which then brings robust reward signals via EBC method. Tree search reasoning. Sampling diverse reasoning paths (Brown et al., 2024) has demonstrated its effectiveness in enhancing the probability of finding the correct answers. Self-Consistency (Wang et al., 2022) samples complete path each time while tree search methods like Tree-of-Thought (ToT) (Yao et al., 2023) and Monte Carlo Tree Search (MCTS) (Chen et al., 2024a,b; Luo et al., 2024b; Feng et al., 2023; Xie et al., 2024; Xu, 2023; Liu et al., 2023; Tian et al., 2024; Ding et al., 2023) extend multiple steps to optimize step answers and ultimately obtain the optimal solution. Additionally, Self-Refine (Madaan et al., 2023a) method has become recent focus. Self-verification (Gero et al., 2023; Weng et al., 2022) and rStar (Qi et al., 2024) utilize the inherent capabilities of the model to iteratively explore and refine answers. However, the performance of Self-Refine is typically constrained by the inherent capabilities of the model, especially for small language models (SLMs) with significantly weaker Self-Refine abilities (Madaan et al., 2023b). Zhang et al. (2024b) suggests that the mathematical reasoning abilities of LLMs can be enhanced by treating the refinement process as directed acyclic graph (DAG) through multi-agent collaboration. In our approach, we combine MCTS with Self-Refine to explore potential solutions and global win-loss matrix is then constructed in the form of directed graph to calculate the final quantile scores."
        },
        {
            "title": "5 Conclusion",
            "content": "This research addresses challenges in complex mathematical reasoning, particularly at the Olympiad level, by enhancing the accuracy and efficiency of the search for reasoning paths. By introducing Self-Refine applied to Monte Carlo Tree Search (SR-MCTS), the LLaMA-Berry framework significantly improves the efficiency of solution generation by LLMs. Additionally, the Pairwise Preference Reward Model (PPRM) con8 structs preferences between solutions rather than merely scoring outcomes, calculating the final global quantile score using the enhanced Borda Count (EBC) method. Evaluation results demonstrate that LLaMA-Berry outperforms baseline approaches on benchmarks like GSM8K and MATH, and achieves competitive performance compared to closed-source models on Olympiad-level benchmarks such as AIME2024."
        },
        {
            "title": "Limitation",
            "content": "The LLaMA-Berry framework has demonstrated strong performance in reasoning tasks, but there are still some challenges in practical applications. First, methods such as Monte Carlo Tree Search (MCTS) and Self-Refine have high computational costs. These techniques demand significant computational resources, which may limit the feasibility of deployment in environments with constrained computational capacity. As for summarizer of solutions, rule-based heuristics methods like self-consistency, major voting and mutual reasoning have shown constraint to the ceiling search performance of MCTS. Thus, we aim to develop learning-based summarizer as OpenAI (2024) does to further enhance the search efficiency. Furthermore, the current evaluation of the LLaMA-Berry framework has primarily focused on mathematical reasoning benchmarks, resulting in relatively narrow assessment scope. As result, its performance in broader domains, such as general knowledge, symbolic logic tasks, and multimodal applications, has not been sufficiently validated. In future work, we aim to improve the framework by evaluating it on more diverse set of tasks to enhance its applicability. Lastly, most experiments so far have utilized relatively small open-source models, with limited testing on larger or closed-source models. In future research, we plan to investigate the performance of LLaMA-Berry on larger models, particularly addressing challenges related to scaling and performance optimization."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157. Anthropic. 2024. The Claude 3 Model Family: Opus, Sonnet, Haiku. Technical report, Anthropic. Accessed: 2024-09-19. Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunhttps:// stall. 2024a. Numinamath 72b cot. huggingface.co/AI-MO/NuminaMath-72B-CoT. Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunhttps:// stall. 2024b. Numinamath 7b cot. huggingface.co/AI-MO/NuminaMath-7B-CoT. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024a. Alphamath almost zero: process supervision without process. arXiv preprint arXiv:2405.03553. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024b. Step-level value preference optimization for mathematical reasoning. arXiv preprint arXiv:2406.10858. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Minghua Ma, Wei Zhang, Si Qin, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023. Everything of thoughts: Defying the law of penrose triangle for thought generation. arXiv preprint arXiv:2311.04254. Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Michael Gira, Shashank Rajput, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. 2022. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:1176311784. Meng Fang, Xiangpeng Wan, Fei Lu, Fei Xing, and Kai Zou. 2024. Mathodyssey: Benchmarking mathematical problem-solving skills in large language models using odyssey math data. Preprint, arXiv:2406.18321. 9 Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2023. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435. Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Naumann, Michel Galley, Jianfeng Gao, and Hoifung Poon. 2023. Self-verification improves fewshot clinical information extraction. arXiv preprint arXiv:2306.00024. Google. 2024. Gemma-2b-it. https://huggingface. co/google/gemma-2b-it. Accessed: 2024-09-19. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. 2024. Glore: When, where, and how to improve llm reasoning via global and local refinements. arXiv preprint arXiv:2402.10963. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, Jianye Hao, and Jun Yao. 2024. Mindstar: Enhancing math reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265. Levente Kocsis and Csaba Szepesvári. 2006. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer. Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. 2024. Mario: Math reasoning with code interpreter output reproducible pipeline. arXiv preprint arXiv:2401.08190. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. 2023. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding. arXiv preprint arXiv:2309.15028. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024a. Improve mathematical reasoning in language models by automated process supervision. Preprint, arXiv:2406.06592. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. 2024b. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592. Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo Yuan, Pengfei Liu, Yang You, and Hongxia Yang. 2023. Lets reward step by step: Step-level reward model arXiv preprint as the navigators for reasoning. arXiv:2310.10080. MAA. 2023. American mathematics competitions. Online. MAA. 2024. American invitational mathematics examination. Online. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023a. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023b. Self-refine: Iterative refinement with self-feedback. In Advances in Neural Information Processing Systems, volume 36, pages 4653446594. Curran Associates, Inc. Meta. 2024a. Meta-llama 3.1-70b instruct. Accessed: 2024-09-03. Ludmila Kuncheva. 2014. Combining pattern classifiers: methods and algorithms. John Wiley & Sons. Meta. 2024b. Meta-llama 3.1-8b instruct. Accessed: 2024-09-03. 10 Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondˇrej ˇCertík, Sergey B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Štˇepán Rouˇcka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony Scopatz. 2017. SymPy: symbolic computing in Python. PeerJ Computer Science, 3:e103. Publisher: PeerJ Inc. Mistral AI. 2024. Mathstral. https://mistral.ai/ news/mathstral. Accessed: 2024-08-12. OpenAI. 2024. Introducing openai o1-preview. Online. 2024. OpenAI. llms. learning-to-reason-with-llms/. 2024-09-19. reason with Learning https://openai.com/index/ Accessed: to Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023. Refiner: Reasoning feedback arXiv preprint on intermediate representations. arXiv:2304.01904. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Noah Shinn, Federico Cassano, Edward Berman, Karthik Narasimhan Ashwin Gopinath, and Shunyu Reflexion: Language agents with Yao. 2024. arXiv preprint verbal reinforcement arXiv:2303.11366. learning. 11 Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. 2024. Easy-to-hard generalization: Scalable alignment beyond human supervision. arXiv preprint arXiv:2403.09472. Zhengyang Tang, Xingxing Zhang, Benyou Wan, and Furu Wei. 2024. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884. Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward selfimprovement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253. Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and Igor Gitman. 2024. Openmathinstruct-1: 1.8 million math instruction tuning dataset. arXiv preprint arXiv:2402.10176. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with process-and outcomebased feedback. arXiv preprint arXiv:2211.14275. Peiyi Wang, Lei Li, Zhihong Shao, R.X. Xu, Damai Dai, Yifei Li, Deli Chen, Y.Wu, and Zhifang Sui. 2023. Math-shepherd: Verify and reinforce llms stepby-step without human annotations. arXiv preprint arXiv:2312.08935. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171. Stephen Warshall. 1962. theorem on boolean matrices. J. ACM, 9(1):1112. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2022. Large language models are better arXiv preprint reasoners with self-verification. arXiv:2212.09561. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451. Haotian Xu. 2023. No train still gain. unleash mathematical reasoning of large language models with monte carlo tree search guided by energy function. arXiv preprint arXiv:2309.03224. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024a. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240. Yifan Zhang, Yang Yuan, and Andrew Chi-Chih Yao. 2024b. On the diagram of thought. arXiv preprint arXiv:2409.10038. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024b. Qwen2 technical report. Preprint, arXiv:2407.10671. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024c. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, and Dahua Lin. 2024. Internlmmath: Open math large language models toward verifiable reasoning. Preprint, arXiv:2402.06332. Fei Yu, Anningzhe Gao, and Benyou Wang. 2023. Outcome-supervised verifiers for planning in mathematical reasoning. arXiv preprint arXiv:2311.09724. Hyper-paramerer settings All hyper-parameter settings for Section 3 are listed in Table A1. Table A1: Hyper-parameter settings Hyper-parameter α γ max_child Description Balance coefficient between local and global Exploration coefficient in UCT function Discount factor for backpropagation Dynamic pruning hyper-parameter in search Value 0.9 1.4 0."
        },
        {
            "title": "B Details of Grading and Metrics",
            "content": "Grading. We follow the correctness evaluation method proposed in PRM800K (Lightman et al., 2023) to score the answers generated by the algorithm. For the mathematical solutions proposed by the algorithm and their corresponding ground truth, we inform the model of the expected response format in prompt. The answers formula or value is extracted by matching the response format with predefined rules. If the model fails to follow the expected format in the prompt and the rule-based extraction fails, the solution is directly judged as inconsistent with ground truth. For the extracted label, we score the answer based on the following criteria. The answer is considered consistent with the ground truth label and passes the evaluation if at least one of the criteria is met: 1. The answer label string is exactly equal to the ground truth label string in terms of literal value. 2. Both the answer label and the ground truth label can be converted to floating-point numbers, and the difference between the two values is less than 1 106. 3. Following the proposed in criterion PRM800K (Lightman et al., 2023), we use the Sympy library (Meurer et al., 2017) to simplify the difference between the expression corresponding to the answer label, denoted as a, and the expression corresponding to the ground truth label, denoted as b. If the simplification yields 0, the criterion is satisfied. Metrics. To provide comprehensive and robust evaluation metric, we adopt major@k and rm@k as the evaluation metrics. 13 rm@k represents reward model best-of-N among sampled response (Yang et al., 2024c). major@k (Kuncheva, 2014), also abbreviated as maj@k, is defined as the fraction of tasks where majority of the top samples generated return the same correct solution. This metric focuses on consistency across multiple generated answers. The majority weight is calculated on the solutions extracted labels. The solved rate of problems (Lightman et al., 2023; Luo et al., 2024a) refers to the percentage of problems who have solutions meet the evaluation criteria. For results without subscript, the score represents the greedy evaluation using the default prompt of the base model. Results with notation indicate the use of the corresponding prompt engineering technique, and those marked with self-consistency use the self-consistency aggregation method. For closed-source models, we report the scores from existing official technical reports (Anthropic, 2024; Reid et al., 2024; OpenAI, 2024) or datasetprovided results, with no modifications."
        },
        {
            "title": "C Details of PPRM Trainging",
            "content": "C.1 Overview The design goal of the PPRM is to integrate the properties of both PRM and ORM, providing more nuanced preference prediction between any two solution answers. We attempt to utilize reinforcement learning methods for training, leveraging the models instruction-following capability to predict the relative merits of pairs of problem-solving answers. This will further enable the use of the EBC method to evaluate the global quantile scores of mathematical solutions. C.2 Data Collection synthesis Our data is derived from two datasets: PRM800K (Lightman et al., 2023) and OpenMathInstruct-1 (Toshniwal et al., 2024). The PRM800K dataset, collected from the MATH dataset, comprises substantial number of stepdivided problem-solving answers, with manual quality annotations for each step. We primarily utilize this dataset to generate pairs of answers for comparative analysis based on step-wise process quality. The OpenMathInstruct-1 dataset Figure A1: Dataset Construction of PPRM. incorporates data from the GSM8K and MATH datasets, which have been manually annotated for outcome correctness. We use this dataset to synthesize pairs for comparative analysis based on outcome quality. In processing PRM800K, for given problem, we first sample steps of varying quality annotations from the step-wise dataset to construct complete reasoning path. For pairs with the same final step annotation, paths composed of higher-quality steps are considered superior to those with lower-quality steps. In cases where the final step annotations are not identical, the reasoning path with the superior final step annotation is regarded as better. During the processing of OpenMathInstruct-1, we exclusively utilize samples without Python interpreter calls. For the same problem, we sample pairs composed of outcomes with higher and lower quality annotations. Notably, we filtered out any data samples from PRM800K and OpenMathInstruct-1 that may overlap with the GSM8K and MATH test sets, especially the PRM800K. Ultimately, we formed dataset of 7,780,951 entries for training the PPRM model. C.3 Direct Preference Pair Construction For all pairs, we frame the inquiry as \"For Question Q, is solution a1 better than solution a2?\" If solution a1 is deemed superior to solution a2, we label it as Yes; otherwise, it is labeled as No. In this manner, we transform the ordinal relationship prediction task into question-answer format, employing the Direct Preference Optimization (DPO) method for model training through reinforcement learning from human feedback (RLHF). This approach aims to enhance the models capability to follow instructions in predicting the relative merits of pairs of problem-solving answers. C.4 RLHF Training (cid:16) (cid:17)(cid:105) (cid:104) log σ β log πθ(ywx) πref (ywx) β log πθ(ylx) We apply the DPO method to train the Gemma2-2B-it model using RLHF which is shown in Figure A2. The loss function for DPO is structured as LDPO(πθ; πref ) = where σ is the logistic function, β is parameter controlling the deviation from the base reference policy πref , namely the initial model πLLM . In practice, the language model policy πθ is also initialized to πLLM . For one answer, denoted as yw ylx where yw and yl denote the preferred (ˆy, y) and dispreferred completion amongst respectively. πref (ylx) , Details of Berry-Tree Inference"
        },
        {
            "title": "Framework",
            "content": "D.1 Overview Berry-Tree is an inference framework specifically designed for complex multi-model reasoning tasks, addressing the need to improve inference efficiency and accuracy in intricate tree search processes of LLMs mathematical reasoning. This framework is particularly suited for largescale reasoning tasks that involve the integration of multiple models and algorithms. By incorporating advanced tree search methods especially Monte Carlo Tree Search (MCTS), robust concurrency handling, and high-performance inference engines, Berry-Tree significantly enhances inference speed and resource utilization of LLMs mathematical reasoning process. This section provides explanation of the system architecture and key tech14 Figure A2: RLHF training of PPRM. nologies of Berry-Tree, along with preliminary performance evaluation results. flexibility and ensuring compatibility with diverse reasoning tasks. D.2 System Architecture Overview Figure A3 demonstrates the architecture of Berry-Tree which is divided into several layers, each handling different functional requirements. The Data Management Layer is responsible for the serialization and deserialization of data, ensuring efficient data read/write operations across models and systems and the ablity of recovering the search process from serialized data. The Tree Search Methods Layer incorporates MCTS (Monte Carlo Tree Search), ToT (Tree of Thoughts), and A* algorithms to optimize the inference process and explore multiple reasoning paths. Additionally, Berry-Tree includes Reliability Layer, which ensures load balancing and failover support in highly concurrent scenarios, guaranteeing the stability of inference tasks. Finally, the Inference Engines Layer integrates efficient inference engines such as vLLM, FastAPI, and HuggingFace TGI to enable parallelized and efficient task processing. D.3 Key Technical Components Data Management. Berry-Tree employs serialization and deserialization techniques, specifically using formats including JSON and CSV, to efficiently store, transfer and recover checkpoint data from tree search reasoning processes. The framework stores this data along with hash values to ensure integrity and allows for quick restoration of tree search states in memory when needed. Furthermore, Berry-Tree leverages the HuggingFace Datasets library to handle core dataset inputs for both training and inference. It supports seamless loading of benchmark datasets such as GSM8K and MATH from the HuggingFace Hub, enhancing its Tree Search Methods. Berry-Tree support multiple tree search algorithms, with MCTS being central to handling large-scale complex reasoning tasks by leveraging random simulation and statistical analysis to optimize the search space. The Tree of Thoughts (ToT) extends the exploration depth and breadth of reasoning paths, helping the system manage uncertainty. And A* can offer efficient heuristic search capabilities. Reliability Design. To ensure the stability and continuity of inference tasks, Berry-Tree incorporates load balancing and failover mechanisms. During high-concurrency operations, the load balance componet distributes workloads across different inference servers, preventing server overload. The failover mechanism ensures that tasks can seamlessly recover and transition to backup servers in case of partial server failures. Server Architecture. The frameworks server architecture is divided into two segments: one dedicated to executing Large Language Model (LLM) inference and the other designated explicitly for handling PPRM (Pairwise Preference Reward Model) with EBC (Enhanced Borda Count) method. This modular design allows the framework to allocate computational resources flexibly, improving overall efficiency. Inference Engines. Inference engines of Berry-Tree include VLLM , HuggingFace Transformers warpped by FastAPI, and HuggingFace TGI. These engines collectively enable the system to maintain high efficiency and stability while handling multi-model inference tasks, with robust support for high-concurrency demands. 15 Figure A3: Architecture design of Berry-Tree D.4 Preliminary Performance Evaluation In preliminary performance evaluation, we utilize 16 A100 GPUs to run the LLaMA3.1-8B-instruct model for large-scale inference tasks, while 4 A100 GPUs are used to run the Gemma2-2B-it model as PPRM servers. The test dataset consists of 1319 GSM8K test samples. We conduct 16 rollouts to parallelize the inference tasks of LLaMA-Berry via Berry-Tree. The results indicate that the total inference time is 1 hour and 25 minutes, with an average inference time of approximately 3.87 seconds per sample. These results demonstrate strong parallel inference capabilities of Berry-Tree under the given hardware configuration. Scaling Study on Inference-time Token"
        },
        {
            "title": "Overheads",
            "content": "Comparing LLaMA-3-8B-Instruct and LLaMA3.1-8B-Instruct across GSM8K, GSMHARD, and MATH500, LLaMA-3.1-8B-Instruct consistently consumes more tokens across all categoriesSolutions, Critiques, and Overall. This Figure A4 result reflects LLaMA-3.1-8B-Instructs tendency to generate more detailed and comprehensive outputs with increased overhead. While this likely improves solution quality, it also introduces greater resource demands and variability in token usage, highlighting trade-off between accuracy and computational resources. LLaMA-3.1-8BInstruct is thus better suited for tasks prioritizing precision over speed. As shown in Figure A5, Token overheads during inference also scale with task difficulty across different olympiad-level benchmarks with LLaMA-3.1-8B-Instruct. AIME2024 exhibits the highest token consumption with significant variability, reflecting the complexity of its solution paths. In contrast, GPQA Diamond shows lower overall overhead, while AMC2023 falls in between, with moderate token consumption and less variability than AIME2024 but still notable."
        },
        {
            "title": "F Future Work",
            "content": "LLaMA-Berry holds significant potential for further development. First, we plan to enhance its multimodal capabilities, enabling it to better handle complex problems like VQA and mathematic geometric problems that require visual, auditory, or even tactile perception. For instance, in tasks involving visual reasoning, such as geometric problems, LLaMA-Berry could be extended to integrate image recognition and analysis capabilities, thereby assisting in solving challenges related to spatial relationships and shape recognition. Furthermore, we aim 16 Figure A4: Average token consumption comparison across datasets, error bar stands for standard deviation. Figure A5: Average token consumption for LLaMA-3.1-8B-Instruct across olympiad-level datasets, error bar stands for standard deviation. to generalize its application across other scientific domains, improving its performance in disciplines such as physics, chemistry, and biology. In physics, for example, it could be utilized to address complex microand macroscopic dynamics problems; in chemistry, it may assist in molecular structure prediction and drug design; and in biology, it could potentially be used for genome analysis and disease prediction. Moreover, LLaMA-Berry can also offer technical support for other interdisciplinary fields, such as meteorology, environmental science, and materials science, by integrating multimodal data to enhance predictive accuracy. At the same time, we will focus on how LLaMA-Berry can further enhance AI safety. For instance, LLaMA-Berry could be leveraged to design more robust safety and risk assessment mechanisms. Besides, the generation of responses could be guided by safety-trained PPRM to produce outputs with higher safety standards. These advancements not only pave the way for broader scientific applications of LLaMA-Berry but also offer new possibilities for enhancing AI safety and promoting its widespread adoption."
        },
        {
            "title": "G Case Study",
            "content": "The prompts utilized in LLaMA-Berry for the LLaMA-3.1-8B-Instruct model are presented in Figure A6. Additionally, Figure A7 provides detailed breakdown of problem-solving examples derived from the GSM8K dataset."
        },
        {
            "title": "H Convergence Analysis of the Enhanced",
            "content": "Borda Count (EBC) Method In this appendix, we present formal discussion on how the quantile scores, as evaluated by the Enhanced Borda Count (EBC) method, converge to the true quantile scores of solutions within the actual quality distribution as the number of samples increases. H.1 Definitions and Assumptions Solution Set. Let = {a1, a2, . . . , an} be finite set of solutions (answers). True Quantile Scores. Each solution ai has true quality score Q(ai) R, drawn from continuous distribution PQ. True Ranking. The true ranking is induced by ordering the solutions in decreasing order of their 17 Figure A6: Prompts for LLaMA-3.1-8B-Instruct Figure A7: Problem-solving example true quantile scores Q(ai). True Pairwise Preference Probabilities. The true probability that solution ai is preferred over aj is defined as: (ai aj) = P(Q(ai) > Q(aj)) + 1 2 P(Q(ai) = Q(aj)). (A1) Given the continuity of PQ, we have P(Q(ai) = Q(aj)) = 0, so: (ai aj) = (cid:40) 1, 0, if Q(ai) > Q(aj), if Q(ai) < Q(aj). (A2) Estimated Preference Probabilities. For independent samples, the estimated preference proba18 bility is: PT (ai aj) = 1 (cid:88) t=1 (i,j) , (A3) are independent Bernoulli random where (i,j) variables with success probability (ai aj). Convergence Assumption. As , PT (ai aj) converges almost surely to (ai aj). Preference Matrix. Construct the estimated preference matrix MT and the true preference matrix as: MT [i, j] = [i, j] = (cid:40) if PT (ai aj) 0.5, 1, 0, otherwise. (cid:40) 1, 0, if (ai aj) = 1, if (ai aj) = 0. (A4) Transitive Closure. Let CT and be the transitive closures of MT and , respectively. Borda Counts. Compute the Borda count for each solution: BT (ai) = (cid:88) j=i CT [i, j], B(ai) = C[i, j]. (cid:88) j=i (A5) Estimated Ranking and Quantile Scores. Estimated ranking RT : Order solutions by decreasing BT (ai). Estimated quantile score: Qg(ai) = 1 rankT (ai) 1 1 . (A6) True quantile score: g(ai) = 1 rank(ai) 1 1 . (A7) H.2 Objective To prove that as , the estimated quantile scores Qg(ai) converge almost surely to the true quantile scores g(ai): lim Qg(ai) = g(ai), ai A. (A8) H.3 Proof By the Strong Law of Large Numbers (SLLN), are i.i.d. Bernoulli random variables since (i,j) with success probability (ai aj), we have: lim PT (ai aj) = (ai aj) almost surely. (A9) Convergence of Preference Matrix MT to . Since PT (ai aj) converges to (ai aj) and (ai aj) {0, 1}, for sufficiently large , we have: MT [i, j] = [i, j], = j, (A10) almost surely. Justification: Because PT (ai aj) converges to either 0 or 1, and PT (ai aj) = 0.5 almost surely for large . Convergence of Transitive Closure CT to C. The transitive closure is deterministic function of the preference matrix. Therefore, since MT , it follows that: CT as , (A11) almost surely. Convergence of Borda Counts BT (ai) to B(ai). Given that CT C, the Borda counts converge: BT (ai) = (cid:88) j=i CT [i, j] B(ai) = C[i, j], (cid:88) j=i (A12) almost surely. Convergence of Estimated Ranking RT to True Ranking R. Since the Borda counts BT (ai) converge to B(ai), and assuming that all Q(ai) are distinct (due to the continuity of PQ), the rankings induced by BT (ai) converge to the true rankings: RT as , (A13) almost surely. Convergence of Quantile Scores Qg(ai) to g(ai). Since rankT (ai) rank(ai), we have: Qg(ai) = 1 g(ai) = 1 rankT (ai) 1 1 rank(ai) 1 1 (A14) Convergence of Estimated Preference Probabilities. almost surely. Conclusion. 19 Therefore, we have shown that: H.5 Addressing Practical Considerations lim Qg(ai) = g(ai), ai A, (A15) which means that the EBC methods estimated quantile scores converge almost surely to the true quantile scores of the solutions. H.4 Finite Sample Analysis and Convergence Rate Lemma 1: Hoeffdings Inequality for Preference Probability Estimates. For each pair (ai, aj), PT (ai aj) is the sample mean of i.i.d. Bernoulli trials with success probability (ai aj). By Hoeffdings inequality: (PT (ai aj) (ai aj) ϵ) 2 exp(2T ϵ2). (A16) Lemma 2: Uniform Convergence over All Pairs. Apply the union bound over all = n(n 1)/ pairs: ((i, j) : PT (ai aj) (ai aj) ϵ) 2 exp(2T ϵ2). (A17) Set the right-hand side equal to δ to solve for : 1 2ϵ2 ln (cid:18) 2N δ (cid:19) . (A18) Lemma 3: Correctness of Preference Matrix with High Probability. Given that (ai aj) {0, 1}, for any 0 < ϵ < 0.5, if: PT (ai aj) (ai aj) < ϵ, (A19) then MT [i, j] = [i, j] because PT (ai aj) will be greater than 0.5 when (ai aj) = 1, and less than 0.5 when (ai aj) = 0. Lemma 4: Probability of Correct Ranking. From Lemma 2 and 3, with probability at least 1 δ, MT = , and thus CT = C, leading to RT = and Qg(ai) = Convergence Rate Analysis. g(ai). To achieve this with confidence level 1 δ, the required number of samples per pair is: 1 2ϵ2 ln (cid:18) n(n 1) δ (cid:19) . (A20) For small δ and ϵ < 0.5, scales logarithmically with the number of solutions n. In practice, the true preference probabilities may not be exactly 0 or 1 due to noise or overlapping quality scores. To accommodate this: Extended Preference Model: Assume (ai aj) is strictly increasing function of ij = Q(ai) Q(aj), such as: (ai aj) = (Q (A21) ij), where is cumulative distribution function (CDF). Margin Condition: Define margin > 0 such that for all = j: (ai aj) 0.5 m. (A22) This ensures minimum separation between preference probabilities. Modified Sample Complexity: With the margin condition, Hoeffdings inequality becomes: (MT [i, j] = [i, j]) 2 exp(2T m2). (A23) To achieve (MT = ) 1 δ, we need: 1 2m2 ln (cid:18) n(n 1) δ (cid:19) . (A24) Convergence under Noise: Even with noisy preference probabilities, as long as there is margin > 0, the convergence of Qg(ai) to g(ai) still holds with high probability for sufficiently large . H.6 Final Conclusion We have provided formal discussion showing that the estimated quantile scores Qg(ai), obtained through the EBC method, converge almost surely to the true quantile scores g(ai) as the number of samples approaches infinity. The finite sample analysis demonstrates that the convergence rate depends logarithmically on the number of solutions and is inversely proportional to the square of the margin between preference probabilities. Pseudo-code of main Algorithms We present the process of Self-Refine applied to Monte Carlo Tree Search (SR-MCTS) Method in Algorithm 1 and provide overall pseudo-code for Ehanced Borda Count (EBC) Method in Algorithm 2. 20 Algorithm 1: Self-Refine applied to Monte Carlo Tree Search (SR-MCTS) Method Input: Initial state s0, search tree , max nodes Nmax, exploration constant Output: Ranked solution list 1 Initialize search tree with root node s0; 2 while number of nodes (T ) < Nmax do 3 Selection Phase Select node si which met the dynamic pruning rule from using UCT: (cid:32) = arg max aA(s) Q(s, a) + (cid:115) (cid:33) ln (s) (s, a) 5 Expansion Phase 6 Expand si by generating successor node using the rewriting process R(si, ci), where ci = C(si) is critique of the current state; Add the new node to ; 7 8 Evaluation Phase 9 Compute the value Q(s) of the new node with Enhanced Borda Count (EBC) method: Q(s) = αQg(s) + (1 α)Ql(s) where Qg(s) is the global value from the win-loss matrix and Ql(s) is the local value from adjacent nodes in ; Backpropagation Phase Propagate Q(s) back to its parent node si, updating sis value: Q(si) = (1 γ)Q(si) + γQ(s) Check for tree growth limit if (T ) Nmax then break; 11 12 13 14 end 15 16 end 17 return Ranked solution list 21 Algorithm 2: Ehanced Borda Count (EBC) Method Data: (Binary Reward Matrix),P (Pairwise Preference Reward model) Result: (quantile rewards), (ranked node list), (ranked layers of nodes) 1 Function FillTransitiveClosure(M ): 2 4 5 6 7 8 10 11 12 all-zero matrix from size of C[i, j] 1 for all i, C[i, j] sign(M [i, j] 0.5) if [i, j] = 1 else 1 for = 0 to 1 do for = 0 to 1 do for = 0 to 1 do if C[i, k] = C[k, j] then C[i, j] C[i, k] end end end 13 end return Updated 14 15 Function BordaCount(C): outdegree of each node from argsort(D) Define layers using unique values in return , 21 22 19 20 Function Rerank(R, L, C, ): Group by Sort each group in using local comparisons computed by L, Update L, by return R, L, Updated 24 25 Function CalculateQuantileScores(R, L): 16 17 18 26 27 SL {1 max(L) : L} Q[x] SL[layer of x] for each return 31 29 30 Function EnhancedBordaCount(M , q): FillTransitiveClosure(M ) R, BordaCount(C) R, L, Rerank(R, L, C,P ) CalculateQuantileScores(R, L) return Q, R, 32 33"
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hong Kong Polytechnic University",
        "Independent Researcher",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "Stanford University",
        "University of California, Merced",
        "University of New South Wales"
    ]
}