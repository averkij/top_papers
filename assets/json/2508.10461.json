{
    "paper_title": "X-Node: Self-Explanation is All We Need",
    "authors": [
        "Prajit Sengupta",
        "Islem Rekik"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graph neural networks (GNNs) have achieved state-of-the-art results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque, limiting their trustworthiness in high-stakes clinical applications where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, a self-explaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct a structured context vector encoding interpretable cues such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. A lightweight Reasoner module maps this context into a compact explanation vector, which serves three purposes: (1) reconstructing the node's latent embedding via a decoder to enforce faithfulness, (2) generating a natural language explanation using a pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via a \"text-injection\" mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. Repository: https://github.com/basiralab/X-Node."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 1 6 4 0 1 . 8 0 5 2 : r X-Node: Self-Explanation is All We Need"
        },
        {
            "title": "Prajit Sengupta",
            "content": "and Islem Rekik BASIRA Lab, Imperial-X (I-X) and Department of Computing, Imperial College London, London, United Kingdom Abstract. Graph neural networks (GNNs) have achieved state-of-theart results in computer vision and medical image classification tasks by capturing structural dependencies across data instances. However, their decision-making remains largely opaque limiting their trustworthiness in high-stakes clinical applications, where interpretability is essential. Existing explainability techniques for GNNs are typically post-hoc and global, offering limited insight into individual node decisions or local reasoning. We introduce X-Node, selfexplaining GNN framework in which each node generates its own explanation as part of the prediction process. For every node, we construct structured context vector encoding interpretable cues, such as degree, centrality, clustering, feature saliency, and label agreement within its local topology. lightweight Reasoner module maps this context into compact explanation vector, which serves three purposes: (1) reconstructing the nodes latent embedding via Decoder to enforce faithfulness, (2) generating natural language explanation using pre-trained LLM (e.g., Grok or Gemini), and (3) guiding the GNN itself via text-injection mechanism that feeds explanations back into the message-passing pipeline. We evaluate X-Node on two graph datasets derived from MedMNIST and MorphoMNIST, integrating it with GCN, GAT, and GIN backbones. Our results show that X-Node maintains competitive classification accuracy while producing faithful, per-node explanations. https://github.com/basiralab/X-Node.1 Keywords: Graph Neural Network Graph Topological Measures Interpretable Explainable AI Natural Language Processing"
        },
        {
            "title": "Introduction",
            "content": "Graph neural networks (GNNs) have emerged as powerful tools for modeling structured medical data, such as cellular interactions in histopathology, organ topologies in medical imaging, and anatomical relationships in population-level Corresponding author: i.rekik@imperial.ac.uk, http://basira-lab.com, GitHub: https://github.com/basiralab/X-Node 1 This paper has been selected for an Oral Presentation at the GRAIL MICCAI 2025 workshop. [X-Node YouTube Video] . 2 Sengupta et al. brain graphs [4,15,1]. By learning over nodes and edges, GNNs naturally capture both local and global structure in such data, and have shown state-of-the-art performance in various diagnostic tasks [7,11]. However, interpretability reFig. 1. Two neighbouring nodes self-explaining mains key bottleneck: in high-stakes clinical environments, it is insufficient for models to be merely accuratethey must also explain their decisions in way that is faithful, transparent, and verifiable [8,16]. Despite growing interest in graph explainability [22,13,23], current approaches face several critical limitations: First, most existing explainability methods for GNNs are post-hoc and non-intrinsic. Tools such as GNNExplainer [22] or PGExplainer [14] identify subgraphs or features after training, with no guarantee that these explanations reflect the models actual reasoning process [17]. Such post-hoc explanations are prone to instability and adversarial inconsistency, especially in sparse medical graphs where subtle topological changes can shift model predictions without warning [3]. Second, existing models do not offer localized, node-level reasoning. GNNs decision for given node emerges from hidden message passing over the graph, but rarely can the node itself articulate why it received certain label. In contrast, clinical reasoning is often local and explainable: radiologists, for instance, interpret findings based on visual features and regional context. Node-level explainabilitywhere each node reasons about its own state as shown in Fig. 1is notably absent from current architectures. Finally, current GNN pipelines treat explanation as disconnected from learning. That is, even if explanations are available, they are not used to guide or constrain training. This decoupling limits the utility of explanations in improving robustness or trust. Worse, explanations can be optimized separately to look plausible, but still diverge from the actual decision pathwaya phenomenon known as rationalization over reasoning [9]. To bridge this gap between decision accuracy and faithful interpretability, we introduce X-Node, fully novel graph learning framework that equips each node with self-explainable capability. By modeling nodes as introspective agentscapable of constructing contextual explanations, reflecting on their local topology and feature space, and injecting this explainX-Node: Self-Explanation is All We Need 3 ability reasoning back into the networkX-Node offers an interpretable and performance-aligned solution to node classification as summarized below: 1. On methodological level: X-Node introduces self-explainable graph nodes by embedding explanation-aware learning directly into the GNN. Each node builds local context, generates an explanation vector, and reinjects it into the network enabling faithful, intrinsic interpretability beyond post-hoc approaches. 2. On clinical level: X-Node allows each noderepresenting patient, organ, or regionto justify its prediction using topological cues, label agreement, and optimization signals. This supports transparent, clinician-aligned decision-making. 3. On generic level: Though evaluated on MedMNIST and MorphoMNIST, X-Node can augment any GNN (e.g., GCN, GAT, GIN) with self-explaining capabilities, offering modular interpretability layer across graph learning tasks."
        },
        {
            "title": "2 Related Work",
            "content": "Understanding the landscape of explainability techniques in GNNs is essential for positioning our contribution. Fig. 2 summarizes this taxonomy, categorizing existing methods as either post-hoc or ante-hoc, and highlighting where our approach, X-Node, fits within this space. Fig. 2. Types of GNN explainability methods Graph neural networks Our work builds on the rich literature of GNNs for node classification. Kipf and Welling introduced GCNs, which perform spectral convolutions on graphs [10]. GAT adds attention weights over neighbors [18], 4 Sengupta et al. while [20] proposed GIN (Graph Isomorphism Networks) showed how certain aggregation functions can make GNNs as powerful as the Weisfeiler-Lehman test. We use these classic architectures (GCN, GAT, GIN) as baselines in our experiments. However, unlike standard GNNs, our architecture includes reasoning modules that explicitly compute explanations at every node. Explainable AI and GNN explanation Interpretability has long been recognized as essential in critical fields like healthcare [2]. Many methods seek to explain deep models post-hoc (e.g. LIME, SHAP in vision/NLP), but these do not guarantee faithful introspection [12]. In graphs, GNNExplainer [22] was among the first general, model-agnostic explainers: it finds subgraph and feature mask that maximize the mutual information with given prediction, identifying compact rationalizing substructure. Other approaches (PGExplainer, XGNN) also produce edge/feature importance masks. While useful, these methods are post-hoc and have been shown to be brittle: even small graph perturbations can drastically change their output without affecting the models prediction [12]. In contrast, X-Node embeds explainability intrinsically into the model. Our approach is inspired by the framework of self-explaining neural networks, where explanations are generated by the model itself as part of the forward pass. Self-explaining models [2] introduced SENN, class of networks where predictions come with explicit contributions and concepts. Similarly, modular or capsule networks use interpretable sub-components. Our Self-Explainable Nodes generalize this idea to graph domains: each node uses its local graph-based context to self-reason about its label. This is akin to building hybrid deep learning with symbolic reasoning at the node level. The use of language models for explanations also resonates with recent work on chain-of-thought reasoning in LLMs [19]. Table 1. Limitations of current GNN-based medical pipelines and proposed solutions. Limitations Proposed Solutions Opaque predictions: Standard GNNs yield high accuracy but provide little insight into why decision was made. In medicine this lack of interpretability reduces clinician trust [22,16,8]. Post-hoc explanations can mislead: Common explainers (e.g., Grad-CAM, LIME) are post-hoc and may not reflect the models true reasoning. Clinicians cannot rely on them in high-stakes cases [22,17,12]. Self-Explainable Nodes: Each node produces an explanation vector reflecting explicit reasoning (e.g., graph topology and 2-hop label patterns). This makes the models logic transparent. Integrated Explainability: We incorporate explanation into the training loop (self-explainable AI). Because the Reasoner is part of the model, its explanations are inherently aligned with decision-making, improving faithfulness by aligning symbolic and neural features. No feedback from explanations: Conventional pipelines do not use explanations to improve learning. The GNN and the explainer are independent, so explanatory signals do not shape the model [9,2]. Feedback Loop: The explanation embeddings are injected back into the GNN layers. This adaptive reinjection lets the model refine its representations based on its own reasoning, effectively using explanation loss as auxiliary supervision. X-Node: Self-Explanation is All We Need 5 LLM-Augmented graph explanation models Recent advancements have explored the integration of Large Language Models (LLMs) to enhance the interpretability of Graph Neural Networks (GNNs). For instance, GraphXAIN [6] employs LLMs to translate technical outputs, such as subgraphs and feature importance scores, into coherent natural language narratives, thereby improving the understandability of GNN predictions for non-expert users. Similarly, LLMExplainer [24] integrates LLMs as Bayesian inference modules within GNN explanation networks to mitigate learning biases and generate more robust explanations. Our approach diverges from these by utilizing an LLM as an explanation decoder within the GNN framework. In our model, each node constructs local context vector, which is then transformed into natural language explanation by the LLM. This explanation is subsequently reintegrated into the GNN through process we term text-injection, guiding further message passing and enhancing interpretability. Unlike methods that rely on symbolic reasoning components, our framework leverages the generative capabilities of LLMs to produce human-readable justifications. Throughout this paper, we are formulating 3 hypotheses: H1: Node-level explainability improves interpretability Each node can produce faithful, self-contained explanation by summarizing local graph structure and topology. H2: Reasoning as regularization improves learning Injecting explanation vectors into GNNs provides inductive bias, improving generalization and aligning embeddings. H3: LLMs enhance explanation pre-trained LLM can map structured context into fluent, human-readable text that captures reasoning aligned with clinical logic."
        },
        {
            "title": "3 Methodology",
            "content": "Our proposed framework, X-Node, introduces self-explainable nodes that classify and explain decisions based on graph structure, features, and topological characteristics during training. The overall pipeline is shown in Figure 3. A. Problem setup and graph construction Let = (xi, yi)N i=1 denote dataset of medical images with class labels yi Y. Each image xi is encoded into feature vector fi Rd using pre-trained CNN encoder , i.e., fi = (xi). We then build k-nearest neighbor (k-NN) graph = (V, E, X), V: Nodes representing images. = [f1, . . . , fN ] RN d: Node features. E: Edges based on cosine similarity encoded into the adjacency matrix: Aij = cos(fi, fj) if fj Top-k(fi); 0 otherwise. This graph encodes both visual and structural similarity across instances as in (Fig 3-A). B. Context vector extraction from graph topology In graph-based medical datasets, nodes local topology often encodes clinically relevant patternssuch as label homophily (e.g., organs of the same type clustering together), centrality (e.g., typical vs. atypical organs), or community structure as shown in (Fig. 3-B). To capture such interpretable patterns, we construct compact context vector 6 Sengupta et al. Fig. 3. Proposed X-Node Architecture ci Rdc for each node vi based on topological and label-aware descriptors. These serve as reasoning cues for explanation generation and auxiliary supervision. While this vector is numerical for model input, it is preserved as labeled key-value pairs (e.g. degree: 3, 2-hop agreement: 0.85) where each key is an interpretable feature. ci = Concat(di, cci, ρ(2) , eci, bci, wi, ci) (1) The following features are selected for their empirical and theoretical relevance to graph classification, especially in sparse or hierarchical domains like medical imaging: Degree (di): Node connectivity. High degree may indicate prototypical or redundant nodes. Clustering Coefficient (cci): Reflects local cohesion. High values suggest dense neighborhoods. 2-hop Label Agreement (ρ(2) ): Measures semantic coni sistency in the extended neighborhood: to other important nodes receive high scores. Betweenness Centrality (bci): Captures bridge roles. Important for detecting outliers or misclassified nodes. Average Edge Weight ( wi): Indicates confidence in neighborhood similarity: wi = 1 (i) (cid:88) wij jN (i) ρ(2) = # same-label nodes in 2-hop # 2-hop neighbors Eigenvector Centrality (eci): in global graph flow. Nodes connected Importance Community Membership (ci): Structural cluster ID, indicating coarse graph-level partitioning. C. Explanation vector generation via Reasoner To interpret nodes behavior, its context vector ci is passed through MLP Reasoner parameterized X-Node: Self-Explanation is All We Need 7 by trainable weights W1, W2 and biases b1, b2 to produce low-dimensional explanation vector ei Rde : ei = Reasoner ϕ(ci) = σ(W2 ReLU(W1 ci + b1) + b2) (2) D. Embedding reconstruction via decoder To ensure faithfulness, ei is decoded to reconstruct the nodes latent GNN embedding ˆhi Rdh: ˆhi = Decoder ψ(ei) (Fig. 3-C). This enforces alignment between ˆhi and the actual embedding hi from the GNN. E. Textual explanation via LLM To generate interpretable explanations (Fig. 3-E), each node vi uses its structured context vector ci Rdc and its predicted label ˆyi (and optionally, true label yi) as input to pre-trained large language model (LLM), such as Groks llama-4-scout-17b-16e-instruct or Googles Gemini 2.5 Pro: Ti = LLMψ (prompt(ci, ˆyi, yi)) (3) The LLM serves as natural language decoder, converting structured nodelevel statistics into faithful textual rationales Ti supporting H3. The prompt is formatted as follows:"
        },
        {
            "title": "LLM Prompt for Explanation",
            "content": "You are node in medical graph. Your topological context is: <context vector> Your predicted label: <predicted label>. True label: <true label> Explain in natural language why you predicted <predicted label>. If incorrect, describe what might have misled you based on your structure, features, and neighbors. This design allows the node to self-narrate its decision logiceither validating correct classification or introspecting its own failure. F. Explanation-guided GNN via text injection The explanation vector ei is concatenated with the GNN embedding hi for final classification, allowing reasoning signals to directly inform prediction, creating feedback loop as shown in (Fig. 3-D): zi = Concat(hi, ei), ˆyi = MLPclass(zi) (4) G. Loss function and joint training The model is trained by jointly minimizing classification, alignment, and reconstruction losses: = (cid:88) i=1 CE(ˆyi, yi) (cid:124) (cid:125) (cid:123)(cid:122) Classification +α ei ci2 (cid:124) (cid:123)(cid:122) (cid:125) Alignment +β ˆhi hi2 (cid:124) (cid:125) (cid:123)(cid:122) Reconstruction (5)"
        },
        {
            "title": "4 Results and Discussion",
            "content": "We assess X-Node on six image-derived graph datasetsfive from MedMNIST (OrganCMNIST, OrganAMNIST, OrganSMNIST, TissueMNIST, BloodMNIST) 8 Sengupta et al. [21] and one synthetic benchmark (Morpho-MNIST) [5]. Each dataset is converted into k-NN graph using pretrained image embeddings. X-Node is evaluated not only for classification accuracy but also for its ability to generate faithful per-node explanations. Results are averaged over 3-fold cross-validation with seeds 42, 43, and 44 (9 experiments). Table 2. Graph Dataset Details created from Images OrganCMNIST OrganAMNIST OrganSMNIST TissueMNIST BloodMNIST Morpho-MNIST # of Nodes # of Edges # of Features # of Labels Task Type Training Type Training Nodes Validation Nodes Test Nodes 23583 82315 512 11 Multi-class Inductive 12975 2392 8216 58830 205404 512 11 Multi-class Inductive 34561 6491 17778 25211 88951 512 11 Multi-class Inductive 13932 2452 8827 236386 826714 512 8 Multi-class Inductive 165466 23640 47280 17092 60116 512 8 Multi-class Inductive 11959 1712 280000 970699 512 4 Multi-class Inductive 216000 24000 40000 Dataset & experimentation As shown in Table 2 the datasets span range of medical domains, with node counts from 17K (BloodMNIST) to 236K (TissueMNIST), and label spaces from 4 to 11 classes. Experiments ran on an Apple M2 Air with 16GB RAM and MPS acceleration GPU, using an 80/20 trainvalidation split in 3-fold CV setup. Adding reasoner increased epoch time and memory moderately as in Table 3. Table 3. Comparison of Average Epoch Time (s) and Peak Memory (MB) across datasets. Method OrganCMNIST OrganAMNIST OrganSMNIST TissueMNIST BloodMNIST MoprhoMNIST Time Memory Time Memory Time Memory Time Memory Time Memory Time Memory 2648.61 GCN 0.48 1717.73 GCN + Reasoner 1.90 2472.47 1781. 1024.92 889.30 1234.64 1439.06 557.36 718.44 758.95 944.25 0.35 0.38 4.20 4. 2.76 4.76 0.49 1.25 1.15 1.78 Classification performance X-Node consistently improves over baseline GNNs across all datasets. For example, on OrganAMNIST, it raises F1 from 91.19% to 93.16% and sensitivity from 91.18% to 94.07% as in Table 4. Gains are especially notable in sensitivitycrucial in medical diagnosis showing up to 35% improvement. On large-scale graphs like TissueMNIST and MorphoMNIST, although overall accuracy slightly drops, X-Node improves F1 and sensitivity metrics which is critical in medical domain. Interpretability (Per-Node Explanations) Beyond classification accuracy, X-Node generates faithful, introspective justifications for each nodes prediction. For example, Node 3 from Fig. 3, which produces the following self-explanation based on its context vector ci and prediction ˆyi supporting H1. This reasoning aligns closely with graph-theoretic descriptors like neighborhood sparsity, edge strength, and label agreement: Table 4. Comparison of GCN variants on six inductive datasets (best results in green). X-Node: Self-Explanation is All We Need 9 OrganCMNIST OrganAMNIST Method ACC F1 Sensitivity ROCAUC ACC F1 Sensitivity ROCAUC 88.200.61 86.030.89 86.130.06 99.090.08 91.850.30 91.190.33 91.180.03 99.510.03 GCN GCN + Reasoner 89.220.79 87.460.87 87.930.08 99.180.07 93.640.21 93.160.19 93.360.02 99.640.03 90.310.28 88.470.34 88.510.03 99.380.05 93.690.36 93.260.28 93.360.04 99.690.02 GAT GAT+ Reasoner 90.750.54 89.110.60 89.290.55 99.370.05 94.170.20 93.850.16 94.070.13 99.690.02 87.960.59 85.610.75 85.560.75 98.860.09 91.540.71 90.450.73 90.500.69 99.410.08 GIN GIN + Reasoner 89.610.24 87.780.24 87.880.21 99.090.08 93.240.27 92.750.27 92.980.22 99.610.02 OrganSMNIST TissueMNIST Method ACC F1 Sensitivity ROCAUC ACC F1 Sensitivity ROCAUC 78.620.82 73.740.99 73.850.08 97.800.11 50.900.32 32.610.79 32.510.07 81.980.31 GCN GCN + Reasoner 79.341.36 74.811.51 75.230.14 97.940.17 51.510.36 34.300.30 34.210.15 82.660.11 81.800.68 77.220.73 77.160.07 98.390.09 51.530.35 33.100.56 33.060.07 83.110.12 GAT GAT + Reasoner 82.080.59 77.700.69 77.990.72 98.360.09 43.980.41 37.590.40 41.020.42 82.510.29 77.230.62 71.650.65 71.770.76 97.360.10 50.511.09 30.313.70 32.502.05 81.720.85 GIN GIN + Reasoner 80.290.60 75.700.74 75.990.63 98.000.10 43.260.54 36.910.46 40.530.41 82.290.19 BloodMNIST MorphoMNIST Method ACC F1 Sensitivity ROCAUC ACC F1 Sensitivity ROCAUC 80.490.66 77.460.96 77.150.09 96.560.18 90.890.05 90.820.02 90.730.02 98.880.03 GCN GCN + Reasoner 80.320.55 78.010.62 78.180.07 96.770.10 90.780.48 90.970.35 90.780.53 98.900.20 82.020.31 79.380.40 79.180.04 97.450.08 91.500.15 91.480.16 91.330.05 98.730.04 GAT GAT+ Reasoner 80.590.62 78.290.70 79.310.56 97.180.11 91.690.32 91.650.20 91.660.18 98.790.02 80.300.60 77.210.91 76.440.95 96.580.21 91.600.19 91.590.18 91.600.19 98.750.05 GIN GIN + Reasoner 80.560.37 78.190.35 79.180.32 96.890.07 91.550.85 91.640.55 91.630.26 98.740.06 Node 3 (True = Kidney-Right, Pred = Femur-Left) Explanation predicted that was representing the femur-left class, but my true label is kidney-right. Lets reflect on this. have degree of 4, meaning Im moderately connected. My clustering coefficient is 0.00, which shows my neighbors are not well connected to each other. One potential cause for the error could be my high average edge weight (0.929), indicating strong connections that might have biased my prediction. While my top feature (F[117] = 10.00) may be distinctive for kidney-right, it was likely overridden by misleading neighborhood influence. My community label and low centrality further suggest Im not centrally embedded in the kidney-right cluster. This misclassification reveals how structural signals can dominate node identity when feature signals are ambiguous. The explanation is both structured and contextualized, allowing users to trace misclassification as in this example back to interpretable topological features. This per-node transparency highlights the value of explanation-aware training. Conclusion This work introduces X-Node, self-explainable GNN architecture that integrates per-node explanation into training. Beyond accuracy, it enables interpretable decision-making through learned context vectors and LLM-based textual rationales. Grok 3 outperforms Gemini 2.5 Pro in clarity of explanation, and future work can explore the effect of prompt variations. X-Node offers Sengupta et al. transferable framework for faithful, explanation-aware learning marking step toward trustworthy graph intelligence."
        },
        {
            "title": "References",
            "content": "1. Ahmedt-Aristizabal, D., Armin, M.A., Denman, S., Fookes, C., Petersson, L.: Graph-based deep learning for medical diagnosis and analysis: Past, present and future. Sensors 21(14), 4758 (Jul 2021). https://doi.org/10.3390/s21144758 2. Alvarez Melis, D., Jaakkola, T.: Towards robust interpretability with selfIn: Bengio, S., Wallach, H., Larochelle, H., in NeuInc. https://proceedings.neurips.cc/paper_files/paper/2018/file/ explaining neural networks. Grauman, K., Cesa-Bianchi, N., Garnett, R. ral (2018), 3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf (eds.) Advances 31. Curran Associates, Information Processing Systems. vol. 3. Bordt, S., Finck, M., Raidl, E., von Luxburg, U.: Post-hoc explanations fail to achieve their purpose in adversarial contexts. In: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency. p. 891905. FAccT 22, Association for Computing Machinery, New York, NY, USA (2022). https://doi. org/10.1145/3531146.3533153, https://doi.org/10.1145/3531146.3533153 4. Bronstein, M.M., Bruna, J., LeCun, Y., Szlam, A., Vandergheynst, P.: Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine 34(4), 1842 (2017). https://doi.org/10.1109/MSP.2017. 5. Coelho de Castro, D., Tan, J., Kainz, B., Glocker, B.: Morpho-mnist: Quantitative assessment and diagnostics for representation learning. Journal of Machine Learning Research 20 (10 2019) 6. Cedro, M., Martens, D.: Graphxain: Narratives to explain graph neural networks (11 2024). https://doi.org/10.48550/arXiv.2411.02540 7. Gao, Y., Yang, H., Chen, Y., Wu, J., Zhang, P., Wang, H.: Llm4gnas: large language model based toolkit for graph neural architecture search (02 2025). https: //doi.org/10.48550/arXiv.2502.10459 8. Holzinger, A., Biemann, C., Pattichis, C., Kell, D.: What do we need to build explainable ai systems for the medical domain? (12 2017). https://doi.org/10. 48550/arXiv.1712.09923 9. Jacovi, A., Goldberg, Y.: Towards faithfully interpretable NLP systems: How should we define and evaluate faithfulness? In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 41984205. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.386, https://aclanthology.org/2020.acl-main.386/ 10. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional networks. In: International Conference on Learning Representations (2017), https: //openreview.net/forum?id=SJU4ayYgl 11. Li, X., Zhao, H., Han, L., Tong, Y., Tan, S., Yang, K.: Gated fully fusion for semantic segmentation. Proceedings of the AAAI Conference on Artificial Intelligence 34, 1141811425 (04 2020). https://doi.org/10.1609/aaai.v34i07.6805 12. Li, Z., Geisler, S., Wang, Y., Gunnemann, S., Leeuwen, M.: Explainable graph neural networks under fire (06 2024). https://doi.org/10.48550/arXiv.2406. 06417 X-Node: Self-Explanation is All We Need 11 13. Luo, D., Cheng, W., Xu, D., Yu, W., Zong, B., Chen, H., Zhang, X.: Parameterized explainer for graph neural network. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin, H. (eds.) Advances in Neural Information Processing Systems. vol. 33, pp. 1962019631. Curran Associates, Inc. (2020), https://proceedings.neurips.cc/paper_files/paper/2020/file/ e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf 14. Luo, D., Cheng, W., Xu, D., Yu, W., Zong, B., Chen, H., Zhang, X.: Parameterized explainer for graph neural network. In: Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS 20, Curran Associates Inc., Red Hook, NY, USA (2020) 15. Parisot, S., Ktena, S.I., Lee, M., Guerrero, R., Glocker, B., Rueckert, D.: Disease prediction using graph convolutional networks: Application to autism spectrum disorder and alzheimers disease. Medical Image Analysis 48 (06 2018). https: //doi.org/10.1016/j.media.2018.06.001 16. Samek, W., Wiegand, T., Muller, K.R.: Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models. ITU Journal: ICT Discoveries - Special Issue 1 - The Impact of Artificial Intelligence (AI) on Communication Networks and Services 1, 110 (10 2017). https://doi.org/10.48550/ arXiv.1708.08296 17. Slack, D., Hilgard, S., Jia, E., Singh, S., Lakkaraju, H.: Fooling lime and shap: Adversarial attacks on post hoc explanation methods. In: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. p. 180186. AIES 20, Association for Computing Machinery, New York, NY, USA (2020). https://doi. org/10.1145/3375627.3375830, https://doi.org/10.1145/3375627.3375830 18. Veliˇckovic, P., Cucurull, G., Casanova, A., Romero, A., Li`o, P., Bengio, Y.: Graph attention networks. In: International Conference on Learning Representations (2018), https://openreview.net/forum?id=rJXMpikCZ 19. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.H., Le, Q.V., Zhou, D.: Chain-of-thought prompting elicits reasoning in large language models. In: Proceedings of the 36th International Conference on Neural Information Processing Systems. NIPS 22, Curran Associates Inc., Red Hook, NY, USA (2022) 20. Xu, K., Hu, W., Leskovec, J., Jegelka, S.: How powerful are graph neural networks? In: International Conference on Learning Representations (2019), https: //openreview.net/forum?id=ryGs6iA5Km 21. Yang, J., Shi, R., Wei, D., et al.: Medmnist v2 - large-scale lightweight benchmark for 2d and 3d biomedical 41 (2023). https://doi.org/10.1038/s41597-022-01721-8, https://doi.org/10. 1038/s41597-022-01721image classification. Scientific Data 10, 22. Ying, Z., Bourgeois, D., You, J., Zitnik, M., Leskovec, J.: Gnnexplainer: Generating explanations for graph neural networks. In: Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information Processing Systems. vol. 32. Curran Associates, Inc. (2019), https://proceedings.neurips.cc/paper_files/paper/2019/file/ d80b7040b773199015de6d3b4293c8ff-Paper.pdf 23. Yuan, H., Tang, J., Hu, X., Ji, S.: Xgnn: Towards model-level explanations of graph neural networks. pp. 430438 (08 2020). https://doi.org/10.1145/3394486. 3403085 24. Zhang, J., Liu, J., Luo, D., Neville, J., Wei, H.: Llmexplainer: Large language model based bayesian inference for graph explanation generation (07 2024). https: //doi.org/10.48550/arXiv.2407."
        }
    ],
    "affiliations": [
        "BASIRA Lab, Imperial-X (I-X) and Department of Computing, Imperial College London, London, United Kingdom"
    ]
}