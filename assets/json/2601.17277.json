{
    "paper_title": "PingPong: A Natural Benchmark for Multi-Turn Code-Switching Dialogues",
    "authors": [
        "Mohammad Rifqi Farhansyah",
        "Hanif Muhammad Zhafran",
        "Farid Adilazuarda",
        "Shamsuddeen Hassan Muhammad",
        "Maryam Ibrahim Mukhtar",
        "Nedjma Ousidhoum",
        "Genta Indra Winata",
        "Ayu Purwarianti",
        "Alham Fikri Aji"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code-switching is a widespread practice among the world's multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PingPong, a benchmark for natural multi-party code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machine-generated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: Question Answering, Dialogue Summarization, and Topic Classification. Evaluations of several state-of-the-art language models on PingPong reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of real-world multilingual discourse."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 2 ] . [ 1 7 7 2 7 1 . 1 0 6 2 : r PINGPONG: Natural Benchmark for Multi-Turn Code-Switching Dialogues Mohammad Rifqi Farhansyah1,2, Hanif Muhammad Zhafran1, Farid Adilazuarda3, Shamsuddeen Hassan Muhammad6, Maryam Ibrahim Mukhtar8, Nedjma Ousidhoum7, Genta Indra Winata5, Ayu Purwarianti1, Alham Fikri Aji4 1Institut Teknologi Bandung 2Monash University Indonesia 3University of Edinburgh 4MBZUAI 5Capital One 6Imperial College London 7Cardiff University 8Bayero University Kano {mrifqifarhansyah, hanif.zhafran07}@gmail.com Main Author Senior Author"
        },
        {
            "title": "Abstract",
            "content": "Code-switching is widespread practice among the worlds multilingual majority, yet few benchmarks accurately reflect its complexity in everyday communication. We present PINGPONG, benchmark for natural multiparty code-switching dialogues covering five language-combination variations, some of which are trilingual. Our dataset consists of human-authored conversations among 2 to 4 participants covering authentic, multi-threaded structures where replies frequently reference much earlier points in the dialogue. We demonstrate that our data is significantly more natural and structurally diverse than machinegenerated alternatives, offering greater variation in message length, speaker dominance, and reply distance. Based on these dialogues, we define three downstream tasks: QUESTION ANSWERING, DIALOGUE SUMMARIZATION, and TOPIC CLASSIFICATION. Evaluations of several state-of-the-art language models on PINGPONG reveal that performance remains limited on code-switched inputs, underscoring the urgent need for more robust NLP systems capable of addressing the intricacies of realworld multilingual discourse."
        },
        {
            "title": "1 Introduction",
            "content": "Code-switching, the practice of alternating between two or more languages within single conversation, is pervasive linguistic phenomenon in contemporary multilingual societies (Poplack, 2013; Myers-Scotton, 1997; Bullock and Toribio, 2009; Auer, 2013). With the global number of multilingual speakers surpassing that of monolinguals, the frequency of code-switching in both informal and formal communication continues to rise (Tucker, 1999; Grosjean, 2010; Wei, 2018; Winata et al., 2021). Despite recent progress in multilingual language models (Xue et al., 2020; Aryabumi et al., 2024; Team et al., 2025; Yang et al., 2025), their 1 Figure 1: Illustration of dialogue samples from PINGPONG. The bottom panel highlights how each turn corresponds to previous turns, as labeled in our dataset. ability to handle code-switched dialogue remains underexplored and insufficiently evaluated (Li and Fung, 2012; Winata et al., 2018; Adilazuarda et al., 2022). Although several code-switching benchmarks have been introduced in the past, these resources are becoming outdated, and therefore, increasingly misaligned with the current era of LLMs and are less reflective of the everyday realities of"
        },
        {
            "title": "Dataset",
            "content": "Open-Source"
        },
        {
            "title": "CS Type",
            "content": "#Comp. Lang. #Regions #Gen. Task Conv. SEAME (Lyu et al., 2010) GLUECoS (Khanuja et al., 2020a) CommonDost (Parekh et al., 2020) MalayalamMixSentiment (Chakravarthi et al., 2020a) TamilMixSentiment (Chakravarthi et al., 2020b) hinglishNorm (Makhija et al., 2020) LinCE (Aguilar et al., 2020a) CSCS (Balabel et al., 2020) CanVEC (Nguyen and Bryant, 2020) ArzEn (Hamed et al., 2020) PHINC (Srivastava and Singh, 2020) MIPE (Garg et al., 2021) HinGE (Srivastava and Singh, 2021) TCS (Tarunesh et al., 2021) GupShup (Mehnaz et al., 2021) DOSA (Ravikiran and Annamalai, 2021) TweetTaglish (Herrera et al., 2022) BaSCo (Aguirre et al., 2022) MHE (Rani et al., 2022) ASCEND (Lovenia et al., 2022) L3Cube-HingCorpus (Nayak and Joshi, 2022) CroCoSum (Zhang and Eickhoff, 2024) CS-PREF (Kuwanto et al., 2024) CS-Sum (Suresh et al., 2025) CodeMixQA (Winata et al., 2026)"
        },
        {
            "title": "PINGPONG",
            "content": "Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Trilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Bilingual Trilingual 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 4 1 2 2 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 0 1 0 0 1 1 0 1 1 1 1 2 0 0 0 0 1 1 1 1 1 2 Table 1: Comparison of code-switching benchmark datasets. Open-Source indicates public availability of the dataset. CS Type denotes the code-switching setting. # Comp. Lang. counts under-studied languages beyond highresource ones (e.g., English, Hindi, Chinese). # Regions reports geographic coverage (e.g., South Asian-English, European-English). # Gen. Tasks indicates the number of generative tasks. Conv. denotes whether the dataset is conversational. code-switching (Aguilar et al., 2020b; Khanuja et al., 2020b). To address this gap, we present PINGPONG, novel benchmark for code-switching in dialogues. Our benchmark consists of conversations in which fluent multilingual speakers naturally code-switch between languages while discussing predefined topics, resulting in multiple language-combination variations through multi-party conversational interactions.. PINGPONG comprises three downstream tasks, including QUESTION ANSWERING, DIALOGUE SUMMARIZATION, and MULTI-LABEL TOPIC CLASSIFICATION. In addition, the benchmark provides broad language coverage, spanning both high-resource and low-resource language combinations, with certain language combination further distinguished by differences in writing scripts. In summary, our contributions are as follow: We introduce PINGPONG, novel benchmark for evaluating natural multi-party codeswitching in conversational settings, designed to capture the fluid dynamics of multilingual communication. PINGPONG consists of authentic interactions among 2 to 4 speakers discussing everyday topics, with conversations ranging from brief exchanges of 17 turns to extended dialogues of up to 189 turns. Reflecting real conversational behavior, speakers may produce longdistance replies by responding to earlier messages while alternating turns. PINGPONG covers five language combinations spanning high-resource, low-resource, and diverse-script languages, enabling evaluation across broad range of multilingual scenarios. Our evaluation framework spans three downstream tasksQUESTION ANSWERING, DIALOGUE SUMMARIZATION, and TOPIC CLAS2 Figure 2: Overview of PINGPONG dataset construction. We recruit annotators who speak five language combinations and group them into small teams of 2 to 4 participants. Each group engages in multi-party dialogue creation, after which downstream annotations are performed. The final output is the curated PINGPONG dataset, focusing on natural code-switched dialogues for multiple tasks. SIFICATIONproviding holistic assessment of language understanding and generation under code-switching conditions. We conduct extensive experiments with both open-source and proprietary LLMs, revealing their limitations and highlighting persistent challenges in modeling code-switched, multiparty conversations."
        },
        {
            "title": "2 Related Work",
            "content": "Research on code-switching has been studied extensively by linguists for decades (Sitaram et al., 2019). In NLP, code-switching has been explored across wide range of tasks (Winata et al., 2023), including language identification (Aguilar and Solorio, 2020; Burchell et al., 2024; Ojo et al., 2025; Das et al., 2023), named entity recognition (Winata et al., 2019a; Aguilar et al., 2018; Jain et al., 2018), part-of-speech tagging (Soto and Hirschberg, 2018; Çetinoglu and Çöltekin, 2016; Ball and Garrette, 2018), and sentiment analysis (Zhang et al., 2021; Shynkarov et al., 2025). In parallel, dataset development and shared evaluations have been actively promoted through initiatives such as CALCS (Heredia et al., 2025; Babatunde et al., 2025) and the FIRE shared tasks (Kulkarni et al., 2024; Adeyemi et al., 2023; Mandl et al., 2024). More recently, this line of work has begun to consolidate around shared benchmarks that enable systematic comparison across models, language combinations, and tasks. However, despite the growing body of work, several limitations remain across existing codeswitching datasets  (Table 1)  . From sustainability perspective, number of datasets are not publicly released (Chakravarthi et al., 2020b; Hamed et al., 2020; Garg et al., 2021). In terms of linguistic diversity, most benchmarks focus exclusively on bilingual code-switching, with only single dataset explicitly addressing trilingual scenarios (Rani et al., 2022). Moreover, both the language and regional coverage remain narrow, as code-switching phenomena in many languages are still under-explored in NLP (Chakravarthi et al., 2020a; Aguilar et al., 2020a; Balabel et al., 2020; Nguyen and Bryant, 2020; Hamed et al., 2020; Ravikiran and Annamalai, 2021; Herrera et al., 2022; Aguirre et al., 2022; Rani et al., 2022; Kuwanto et al., 2024; Suresh et al., 2025; Winata et al., 2026). In addition, most existing datasets fall short of capturing the conversational nature of real-world code-switching, which often emerges in spontaneous and interactive settings (Lyu et al., 2010; Parekh et al., 2020; Makhija et al., 2020; Tarunesh et al., 2021; Mehnaz et al., 2021; Lovenia et al., 2022). Evaluation tasks are also frequently limited to traditional benchmarks that no longer reflect contemporary NLP challenges (Khanuja et al., 2020a; Nayak and Joshi, 2022). To address these gaps, we introduce PINGPONG, large-scale open-source evaluation suite that expands language and regional coverage, incorporates diverse and realistic conversational code-switching scenarios, and emphasizes generative and semantic tasks."
        },
        {
            "title": "3 PINGPONG Dataset",
            "content": "Our dataset is constructed through manual crowdsourcing (Figure 2), with dialogues collected and downstream tasks curated by native In speakers of each language combination. PINGPONG, the covered language combinations include five combinations: IndonesianEnglish (ID-EN), SundaneseIndonesianEnglish (SU-IDEN), JavaneseIndonesianEnglish (JV-ID-EN), HausaEnglish (HA-EN), and Algerian ArabicStandard ArabicFrench (AR-DZ-FR). This design ensures both linguistic fidelity and overall resource quality. Consequently, our benchmark provides more reliable evaluation setting compared to datasets generated automatically by LLMs."
        },
        {
            "title": "3.1 Annotator Hiring",
            "content": "We initiate the annotator recruitment process by first identifying the most prevalent language combinations exhibiting code-switching phenomena. For each language combination, we recruit language championa native speaker responsible for managing data collection for that combination. The language champions then coordinate the recruitment of annotators to contribute to dataset construction."
        },
        {
            "title": "3.1.1 Recruitment Form",
            "content": "Each language champion prepares recruitment form. This form serves two primary purposes: (i) collecting demographic information from prospective annotators, and (ii) filtering respondents into the final pool of selected annotators. The form is structured into several sections, including an introduction, self-assessment, and language assessment. Further details regarding the recruitment form and collected annotator demographics are provided in Appendix and Appendix B, respectively."
        },
        {
            "title": "3.1.2 Grouping",
            "content": "For each selected annotator, we organize dialogue collection groups using the Discord1 platform. Given the limited number of recruited annotators (detailed guidelines are provided in Appendix C), each individual participates in multiple groups. In total, we establish 100 dialogue collection groups for each language combination, comprising 50 groups with two speakers, 25 groups with three speakers, and 25 groups with four speakers. To support this process, we notify group members at three key points: five minutes before the session, at the start of the session, and at its conclusion. Annotators who are not directly involved in dialogue collection are subsequently assigned to annotate the downstream task datasets."
        },
        {
            "title": "3.2.1 Conversational Dialogue Collection",
            "content": "We conduct separate session for each assigned annotation group. In each session, participants engage in 15-minute conversation on pre-assigned topic. To capture natural code-switching behavior, participants are encouraged to mix languages within the designated language combinations whenever it feels natural to do so. Code-switching is allowed at multiple levels, including individual words, sentences, paragraphs, or even within 1https://discord.com/. single utterance. To preserve anonymity, participants are asked not to refer to one another by name, whether in full, abbreviated, or nickname form. Instead, they are instructed to use Discords mention feature whenever direct reference to another participant is necessary. This procedure ensures that all participant identities remain anonymized upon data export. Additionally, participants are allowed to use Discords reply feature to respond directly to specific utterances, thereby maintaining coherent dialogue structure."
        },
        {
            "title": "3.2.2 Question Answering",
            "content": "Once the dialogues are collected, annotators write up to 10 question-answering items for each of the corresponding dialogue, these consisted of up to five answerable questions and five unanswerable questions. The answerable questions were designed to emphasize reasoning, such that answers could not be obtained directly from the dialogue. Instead, they required the use of external knowledge and structured reasoning to reach correct answer. The unanswerable questions, on the other hand, were constructed following five categoriesNegation, Antonym, EntitySwap, Mutual-Exclusion, and Impossible Conditionas defined in SQuAD 2.0 or SQuADRUn (Rajpurkar et al., 2018). All QA items were formulated as multiple-choice questions with five options, where option explicitly denoted No correct answer. Furthermore, each question was written in the designated first language (L1) of the corresponding language combination. For example, for the Javanese IndonesianEnglish dataset, Indonesian was used as the language for the QA construction."
        },
        {
            "title": "3.2.3 Dialogue Summarization",
            "content": "For each dialogue, we collect up to 3 distinct summaries, each from different annotators. Each summary is 3-5 sentences long and written in the designated L1 of the corresponding language combination. Annotators were provided with the initial topic of the dialogue and instructed to only summarize relevant information, excluding any off-topic segments. We encouraged annotators to follow the four dimensions of summarization quality defined in (Kryscinski et al., 2019): CoherenceThe logical flow and collective quality of all sentences in the summary, FluencyThe grammatical quality of each individual sentence, RelevanceThe inclusion of only important, on-topic information, and ConsistencyThe degree to which all facts in the"
        },
        {
            "title": "Lang",
            "content": "#Dialogue #Sci/Tech #Ent #Soc/Cul #Edu #Daily IDEN JVIDEN SUIDEN HAEN ARDZFR 100 100 100 100 13 4 7 4 14 21 17 18 15 21 32 22 48 25 21 15 17 9 5 24 19 40 18 51 20 Table 2: Topic distribution of the different dialogues per language combination (Lang). We show the total number of dialogues (#Dialogue) and per topicscience and technology (#Sci/Tech), entertainment (#Ent), society and culture (#Soc/Cul), education (#Edu), and daily life (#Daily). summary are supported by the source document."
        },
        {
            "title": "3.2.4 Topic Classification",
            "content": "Since the dialogue were collected based on predefined initial topic, therefore naturally we can convert our dataset into topic classification one as well. Specifically, we map the initial topics into classes as follow: Science/Technology (e.g., scientific breakthroughs, research, new technology, etc.), Entertainment (Sports, Music, Tourism, and any other form of entertainments), Social/Culture (e.g., languages, work culture, customs, traditions, etc.) Education (e.g., curriculum, schools, colleges, etc.), and Daily Life (e.g., personal experiences, love stories, daily habits). Table 2 shows the distribution of the topics in the dataset."
        },
        {
            "title": "3.3 Data Statistics",
            "content": "The statistics of dialogues in PINGPONG are summarized in Table 3. Our dataset comprises long, spontaneous conversations between 2 to 4 participants, providing representative sample of authentic human interactions. As such, it is uniquely suited for benchmarking models on long-context, natural code-switching behavior. To quantify the linguistic complexity of these dialogues, we report two standard metrics: Code-Mixing Index (CMI) and Switch Point Fraction (SPF). Code-Mixing Index (CMI). CMI (Gambäck and Das, 2016) measures the intensity of codeswitching by assessing the distribution of languages within an utterance. While the original formulation suggests treating Named Entities separately to avoid an artificial inflation of the index, robust NER tools are often unavailable for the underresourced languages in our study. Consequently, following Winata et al. (2019b), we simplify the calculation by not distinguishing NEs. Switch Point Fraction (SPF). While CMI captures the volume of mixing, SPF (Pratapa et al., 2018) focuses on the frequency of transitions between languages. It is calculated as the ratio of the number of language-switch points to the total number of possible switching positions."
        },
        {
            "title": "3.4 Natural Conversational Pattern",
            "content": "Our dataset reflects the organic characteristics of human text-based communication, particularly in multi-party settings. We observe significant participation imbalance, where certain speakers dominate the conversation while others remain less active. Furthermore, human dialogue often exhibits multithreaded structures; speakers frequently reply to refer and respond to messages from several turns prior. Utterance length is also highly variable, ranging from detailed explanations to brief oneor twoword reactions. Notably, human participants sometimes send multiple consecutive messages before turn change occurs. In contrast, machine-generated dialogues produced by GPT-4o based on the same topics appear significantly more monotonous. These conversations tend to follow rigid turn-taking structure with uniform distribution among speakers and consistent utterance lengths. Synthetic dialogues rarely feature consecutive messages from single speaker, and replies are almost always linear and immediate. comparison of these patterns is illustrated in Figure 3. To quantify these observations, we provide statistical analysis in Table 4, which shows that across most languages, human annotators consistently rate organic conversations as more natural than their machine-generated counterparts. Detailed statistics for each language combination can be found in Appendix F.3."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We evaluate diverse set of models across the three tasks. From the perspective of language coverage, these models can be categorized into Englishcentric models, multilingual models, and regionspecific models tailored to particular language combinations. In addition, we compare base models with reasoning-oriented and instruction-tuned variants. Regarding linguistic coverage, our study leverages models such as SAILOR2 (Dou et al., 2025), AYA23 (Aryabumi et al., 2024), SAHABAT AI,2 2https://huggingface.co/Sahabat-AI. 5 Lang. #Dialogue Avg. Turn Avg. Words Avg. Tokens Avg. CMI Avg. SPF IDEN JVIDEN SUIDEN HAEN ARDZFR 100 100 100 100 100 81.93 83.07 59.88 69.14 98.60 448.96 410.14 494.03 421.32 462.74 782.15 754.04 957.20 649.84 1,120.81 0.472 0.467 0.757 0.352 0. 0.300 0.318 0.467 0.190 0.253 Table 3: Dialogue data statistics. For each language combination (Lang.), we report the average number of turns (Avg. Turn), average number of words (Avg. Words), average number of tokens (Avg. Tokens), average Code-Mixing Index (Avg. CMI), and average Switch-Point Fraction (Avg SPF) per dialogue. Metric Human-written Machine-generated 2 speakers (50 dialogue) Avg length variance (tokens) Total replies Avg degree of reply distance Avg imbalance ratio of speaker turns Avg CMI Avg SPF Human preference 3 speakers (25 dialogue) Avg length variance (tokens) Total replies Avg degree of reply distance Avg imbalance ratio of speaker turns Avg CMI Avg SPF Human preference 4 speakers (25 dialogue) Avg length variance (tokens) Total replies Avg degree of reply distance Avg imbalance ratio of speaker turns Avg CMI Avg SPF Human preference 66.540 658.2 2.729 1.366 0.525 0.306 2.727 59.093 630.4 3.698 2.961 0.521 0.305 2. 81.195 869.6 4.159 3.256 0.520 0.306 2.676 35.140 2.6 0.048 1.019 0.541 0.313 2.178 30.230 19.8 0.738 1.056 0.486 0.284 2.141 29.191 45.0 1.085 1.118 0.483 0.284 2.078 Table 4: Statistics of human-written vs. machinegenerated conversational patterns (averaged across all language combinations). QWEN2.5 (Qwen et al., 2025), GEMMA2 (Team et al., 2024), GEMMA3 (Team et al., 2025), ALLAM (Bari et al., 2025), and SILMA (silma-ai, 2024). Furthermore, to assess reasoning capabilities, we include QWEN3 (Yang et al., 2025) models with 4B and 8B parameters. Detailed hyperparameter configurations for each model are provided in Appendix 7."
        },
        {
            "title": "4.1 Reasoning Behavior",
            "content": "Some of our models, namely the Qwen3 series, have reasoning capabilities. Therefore, we investigate whether thinking trace is beneficial in our dataset. For models without built-in thinking behavior, we can elicit reasoning by explicitly asking the model to generate their reasoning trace first, akin to prior work such as chain-of-thought (Wei et al., 2022). Figure 3: Comparison between human-written and machine-generated conversation texts. Text with color denotes the original conversation, while text with color represents the English translation. Segments highlighted in indicate Indonesian words, whereas segments highlighted in indicate English words."
        },
        {
            "title": "4.2 Task Setup",
            "content": "Across all downstream tasks, we adopt set of shared experimental prompt configurations, which include the construction of example dialogues, the number of shots, and the specification of output formats. These configurations are further adapted 6 QUESTION ANSWERING (ACCURACY %) DIALOGUE SUMMARIZATION (ROUGE-L) TOPIC CLASSIFICATION (ACCURACY %) IDEN JVIDEN SUIDEN HAEN ARDZFR IDEN JVIDEN SUIDEN HAEN ARDZFR IDEN JVIDEN SUIDEN HAEN ARDZFR Model Global Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-8B Aya23-8B Gemma2-9B-Instruct Gemma3-4B-Instruct Regional Sailor2-8B Sahabat-AI-Gemma SILMA-9B-Instruct ALLAM-7B-Instruct 5.86 4.64 8.89 5.65 8.28 11.92 23.43 27.68 13.74 - - 19.00 24.65 30.91 31.72 23.03 43.63 36.76 40.40 43.23 - - 87.10 90.90 89.50 89.10 84.44 91.31 89. 86.67 91.91 - - 43.93 32.83 55.55 45.60 29.80 82.83 71.21 - - - - 22.22 40.40 34.34 44.44 28.28 52.52 36.36 - - 0.0 39.39 0.205 0.238 0.225 0.225 0.203 0.250 0. 0.233 0.241 - - 0.207 0.226 0.231 0.221 0.189 0.244 0.209 0.232 0.248 - - 0.242 0.258 0.271 0.268 0.239 0.290 0.268 0.291 0.286 - - 0.089 0.187 0.045 0.025 0.054 0.243 0. - - - - 0.006 0.106 0.085 0.085 0.022 0.058 0.023 - - 0.003 0.002 46.46 51.52 45.45 53.54 48.48 64.65 49.49 22.22 68.69 - - 39.39 55.56 43.43 51.52 44.44 39.39 45. 17.17 44.44 - - 47.47 46.46 42.42 45.45 38.38 55.56 44.44 20.20 58.59 - - 30.30 55.56 38.38 47.47 47.47 53.54 52.53 - - - - 48.48 52.53 50.51 56.57 40.40 35.35 33. - - 0.00 41.41 Table 5: Experimental Results on PINGPONG. This ablation is conducted under zero-shot prompting setting, with the reasoning (thinking) mode disabled for models that support it. For the Question Answering task, we report results on the answerable subset. to the requirements of each individual task. The prompts are detailed in Appendix D. Number of Shots. For QA, we experiment with 0 and 1 shot prompting. For Dialogue Summarization, we use 0, 1, and 3-shot settings of summary examples without dialogue example. The in-context examples for each task are drawn from dialogues that are not part of the evaluation set. Output Format. To ensure reliable answer extraction, we require the models to provide their outputs in JSON format. Furthermore, we instruct specific models to include reasoning trace to facilitate more detailed analysis of their underlying logic. which suggests that the proposed benchmark represents significant challenges for current systems. Notably, we observe performance gap between general-purpose multilingual models and regionally-designed ones. The latter generally achieve better results, highlighting the clear benefits of utilizing specialized models that are tailored to specific linguistic and cultural characteristics. While there is notable variance in performance across different language groups, it is difficult to conclude that any specific language is inherently more difficult than others. This is primarily due to the non-parallel nature of our dataset; the difficulty remains anchored to the specific conversation content of each language."
        },
        {
            "title": "5.2 Effect of Reasoning",
            "content": "All models are assessed with task-specific evaluation metrics. For QUESTION ANSWERING and TOPIC CLASSIFICATION tasks, we use accuracy as the evaluation metric. For DIALOGUE SUMMARIZATION task, we use ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), CHRF++ (Popovic, 2015), and BERTScore (Zhang et al., 2020). These metrics were chosen because they have high correlation with the four dimensions of summarization quality, according to (Fabbri et al., 2021). In the main paper we report ROUGE-L, but other metrics are shown in Appendix F.2."
        },
        {
            "title": "5.1 Overall Results",
            "content": "The performance metrics for our evaluated models across all target languages are summarized in Table 5 (Full results are detailed in Appendix F). key takeaway from these results is that the majority of models exhibit poor performance, We investigated the impact of explicit reasoning on model performance by utilizing the native thinking traces available in recent models like Qwen3, and by using explicit prompting to generate reasoning steps for other models. As shown in Figure 5, we observe consistent improvement mostly in performance when reasoning is enabled. This indicates that the tasks within our dataset benefit from the additional computation and internal verification afforded by reasoning traces."
        },
        {
            "title": "5.3 Effect of Few-Shot Learning",
            "content": "In comparison, the inclusion of few-shot examples does not appear to be significant factor in improving model performance for QA and Topic Classification, but improves significantly in Dialogue Summarization. As shown in Figure 6, providing small number of in-context examples generally does not yield consistent gains across the evaluated models. 7 Figure 4: Comparison of average model performance (Acc. %) across languages for Answerable vs. Unanswerable cases, from the perspectives of N-shot prompting (Left) and reasoning performance (Right). improvements and, in some cases, even leads to performance decline. This suggests that unanswerable cases place stricter demands on structured reasoning and error detection. Furthermore, detailed analysis of shot settings indicates that few-shot prompting does not provide stable or uniform benefits across models. The effectiveness of few-shot examples appears to be model-dependent, with some models benefiting marginally while others show no improvement or even slight regressions."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced PINGPONG, multiparty code-switching benchmark capturing the authentic complexity of human multilingual discourse across five language combinations. By utilizing human-authored conversations with 2 to 4 participants, we provide dataset containing structural nuances, such as multi-threaded dynamics and varied speaker dominance, often missing in synthetic corpora. Our analysis confirms that these dialogues are more natural and structurally diverse than machine-generated alternatives, particularly in message length and reply distances. We build an evaluation pipeline that covers three downstream tasks based on the conversation: QA, Summarization, and Topic Classification to address gaps in prior benchmarks and reflect the requirements of the modern LLM era. Experimental results show that state-of-the-art models still struggle with the intricacies of natural, multi-party code-switching. This performance gap highlights the importance of PINGPONG in identifying current NLP limitations and serves as foundation for developing more robust, inclusive systems for the worlds multilingual majority. Figure 5: Reasoning vs No-Reasoning performance in Dialogue Summarization (blue), Topic Classification (green), and QA (red). Figure 6: Zero-shot vs Few-shot performance in Dialogue Summarization (blue), Topic Classification (green), and QA (red)."
        },
        {
            "title": "Unanswerable QA",
            "content": "Based on the results in Figure 4, we observe that answerable questionsspecifically those designed to require explicit reasoningexhibit consistent performance improvements when the model leverages reasoning mechanisms, whether implicitly or explicitly. In contrast, for the unanswerable subset, performance gains are observed only when explicit reasoning traces are enabled in the Qwen3 model, whereas implicit reasoning does not yield"
        },
        {
            "title": "Limitations",
            "content": "This work covers 4 complementary (under-studied) languages across 5 language combinations, spanning 3 geographic regions. While not exhaustive, this scope provides scalable foundation for building natural code-switching datasets such as PINGPONG, which can be readily extended to additional languages and regions in future work. We evaluate diverse set of global and regional LLMs that vary in linguistic coverage, model size, and reasoning capability, including both open-source and proprietary systems. As the LLM landscape continues to evolve, expanding this evaluation to broader range of models represents natural and promising direction for future research. Finally, due to the lack of reliable tools for computing advanced CMI metrics in under-studied languages, we adopt relaxed CMI formulation that preserves meaningful comparative insights. We expect that future advances in multilingual NLP resources will enable more fine-grained code-mixing analyses for these languages. We also acknowledge the use of an AI assistant (e.g. ChatGPT, Gemini, Github Copilot) to support code implementation and to polish the writing of this manuscript."
        },
        {
            "title": "Ethical Considerations",
            "content": "The annotators involved in this study were compensated above the local minimum wage. They received detailed instructions, and any demographic information collected was obtained with their informed consent (see Appendix A). Annotators were explicitly instructed not to use offensive language, and we conducted verification to the best of our ability. Nonetheless, some instances may have been missed, and we are committed to updating the benchmark if such cases are reported. While we aimed to construct realistic datasets, we do not claim that our benchmark captures all code-switching variations across the five language combinations."
        },
        {
            "title": "References",
            "content": "Muhammad Farid Adilazuarda, Samuel Cahyawijaya, Genta Indra Winata, Pascale Fung, and Ayu Purwarianti. 2022. IndoRobusta: Towards robustness against diverse code-mixed Indonesian local languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pages 2534, Online. Association for Computational Linguistics. Gustavo Aguilar, Fahad AlGhamdi, Victor Soto, Mona Diab, Julia Hirschberg, and Thamar Solorio. 2018. Named entity recognition on code-switched data: In ProOverview of the calcs 2018 shared task. ceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching, pages 138 147. Gustavo Aguilar, Sudipta Kar, and Thamar Solorio. 2020a. LinCE: centralized benchmark for linguistic code-switching evaluation. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 18031813, Marseille, France. European Language Resources Association. Gustavo Aguilar, Sudipta Kar, and Thamar Solorio. 2020b. Lince: centralized benchmark for linguistic code-switching evaluation. arXiv preprint arXiv:2005.04322. Gustavo Aguilar and Thamar Solorio. 2020. From english to code-switching: Transfer learning with strong morphological clues. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 80338044. Maia Aguirre, Laura García-Sardiña, Manex Serras, Ariane Méndez, and Jacobo López. 2022. BaSCo: An annotated Basque-Spanish code-switching corpus for natural language understanding. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 31583163, Marseille, France. European Language Resources Association. Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, and 1 others. 2024. Aya 23: Open weight releases to further multilingual progress. arXiv preprint arXiv:2405.15032. Peter Auer. 2013. Code-switching in conversation: Language, interaction and identity. Routledge. Oreoluwa Boluwatife Babatunde, Victor Tolulope Olufemi, Emmanuel Bolarinwa, Kausar Yetunde Moshood, and Chris Chinenye Emezue. 2025. Beyond monolingual limits: Fine-tuning monolingual asr for yoruba-english code-switching. In Proceedings of the 7th Workshop on Computational Approaches to Linguistic Code-Switching, pages 1825. Mofetoluwa Adeyemi, Akintunde Oladipo, Xinyu Zhang, David Alfonso-Hermelo, Mehdi Rezagholizadeh, Boxing Chen, and Jimmy Lin. 2023. Ciral at fire 2023: Cross-lingual information retrieval for african languages. In Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation, pages 46. Mohamed Balabel, Injy Hamed, Slim Abdennadher, Ngoc Thang Vu, and Özlem Çetinoglu. 2020. Cairo student code-switch (CSCS) corpus: An annotated In Proceedings Egyptian Arabic-English corpus. of the Twelfth Language Resources and Evaluation Conference, pages 39733977, Marseille, France. European Language Resources Association. 9 Kelsey Ball and Dan Garrette. 2018. Part-of-speech tagging for code-switched, transliterated texts without explicit language identification. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 30843089. Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 6572, Ann Arbor, Michigan. Association for Computational Linguistics. Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham Abdullah Alyahya, Sultan AlRashed, Faisal Abdulrahman Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Saad Amin Hassan, Dr. Majed Alrubaian, Ali Alammari, Zaki Alawami, and 7 others. 2025. ALLam: Large language models for arabic and english. In The Thirteenth International Conference on Learning Representations. Barbara Bullock and Almeida Jacqueline Toribio. 2009. The Cambridge handbook of linguistic codeswitching. Cambridge university press. Laurie Burchell, Alexandra Birch, Robert Thompson, and Kenneth Heafield. 2024. Code-switched language identification is harder than you think. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 646658. Özlem Çetinoglu and Çagrı Çöltekin. 2016. Part of speech annotation of turkish-german codeswitching corpus. In Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with ACL 2016 (LAW-X 2016), pages 120130. Bharathi Raja Chakravarthi, Navya Jose, Shardul Suryawanshi, Elizabeth Sherly, and John P. McCrae. 2020a. sentiment analysis dataset for code-mixed Malayalam-English. In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), pages 177184, Marseille, France. European Language Resources association. Bharathi Raja Chakravarthi, Vigneshwaran Muralidaran, Ruba Priyadharshini, and John P. McCrae. 2020b. Corpus creation for sentiment analysis in code-mixed Tamil-English text. In Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), pages 202210, Marseille, France. European Language Resources association. Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. 2023. Improving pretraining techniques for code-switched nlp. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11761191. Longxu Dou, Qian Liu, Fan Zhou, Changyu Chen, Zili Wang, Ziqi Jin, Zichen Liu, Tongyao Zhu, Cunxiao Du, Penghui Yang, and 1 others. 2025. Sailor2: Sailing in south-east asia with inclusive multilingual llms. arXiv preprint arXiv:2502.12982. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409. Björn Gambäck and Amitava Das. 2016. Comparing the level of code-switching in corpora. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 1850 1855. Ayush Garg, Sammed Kagi, Vivek Srivastava, and Mayank Singh. 2021. MIPE: metric independent pipeline for effective code-mixed NLG evaluation. In Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 123132, Punta Cana, Dominican Republic. Association for Computational Linguistics. François Grosjean. 2010. Bilingual: Life and reality. Harvard university press. Injy Hamed, Ngoc Thang Vu, and Slim Abdennadher. 2020. ArzEn: speech corpus for code-switched In Proceedings of the Egyptian Arabic-English. Twelfth Language Resources and Evaluation Conference, pages 42374246, Marseille, France. European Language Resources Association. Maite Heredia, Jeremy Barnes, and Aitor Soroa. 2025. Euska nolds: naturally sourced corpus for arXiv preprint basque-spanish code-switching. arXiv:2502.03188. Megan Herrera, Ankit Aich, and Natalie Parde. 2022. TweetTaglish: dataset for investigating TagalogEnglish code-switching. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 20902097, Marseille, France. European Language Resources Association. Devanshu Jain, Maria Kustikova, Mayank Darbari, Rishabh Gupta, and Stephen Mayhew. 2018. Simple features for strong performance on named entity recognition in code-switched twitter data. In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching, pages 103 109. Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and Monojit Choudhury. 2020a. GLUECoS: An evaluation benchmark for code-switched NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 35753585, Online. Association for Computational Linguistics. Simran Khanuja, Sandipan Dandapat, Anirudh Srinivasan, Sunayana Sitaram, and Monojit Choudhury. 2020b. Gluecos: An evaluation benchmark for codeswitched nlp. arXiv preprint arXiv:2004.12376. Wojciech Kryscinski, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Neural text summarization: critical evaluation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 540551, Hong Kong, China. Association for Computational Linguistics. Nikita Kulkarni, Kareena Manghani, Sanhita Kulkarni, Pranita Deshmukh, and Raviraj Joshi. 2024. L3cubemahasum: comprehensive dataset and bart models In for abstractive text summarization in marathi. Proceedings of the 16th Annual Meeting of the Forum for Information Retrieval Evaluation, pages 7679. Garry Kuwanto, Chaitanya Agarwal, Genta Indra Winata, and Derry Tanti Wijaya. 2024. Linguistics theory meets llm: Code-switched text generation via equivalence constrained large language models. arXiv preprint arXiv:2410.22660. Ying Li and Pascale Fung. 2012. Code-switch language model with inversion constraints for mixed language speech recognition. In Proceedings of COLING 2012, pages 16711680. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Holy Lovenia, Samuel Cahyawijaya, Genta Indra Winata, Peng Xu, Yan Xu, Zihan Liu, Rita Frieske, Tiezheng Yu, Wenliang Dai, Elham Barezi, and 1 others. 2022. Ascend: spontaneous chineseenglish dataset for code-switching in multi-turn conversation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 7259 7268. Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and Haizhou Li. 2010. Seame: mandarin-english codeswitching speech corpus in south-east asia. In Interspeech, volume 10, pages 19861989. Thomas Mandl, Koyel Ghosh, Nishat Raihan, Sandip Modha, Shrey Satapara, Tanishka Gaur, Yaashu Dave, Marcos Zampieri, and Sylvia Jaki. 2024. Overview of the hasoc track 2024: Hate-speech identification in english and bengali. In Proceedings of the 16th Annual Meeting of the Forum for Information Retrieval Evaluation, pages 12. Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish Acharya, and Rajiv Ratn Shah. 2021. GupShup: Summarizing open-domain code-switched conversations. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6177 6192, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Carol Myers-Scotton. 1997. Duelling languages: Grammatical structure in codeswitching. Oxford University Press. Ravindra Nayak and Raviraj Joshi. 2022. L3CubeHingCorpus and HingBERT: code mixed HindiIn English dataset and BERT language models. Proceedings of the WILDRE-6 Workshop within the 13th Language Resources and Evaluation Conference, pages 712, Marseille, France. European Language Resources Association. Li Nguyen and Christopher Bryant. 2020. CanVEC the Canberra Vietnamese-English code-switching natural speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 41214129, Marseille, France. European Language Resources Association. Jessica Ojo, Zina Kamel, and David Ifeoluwa Adelani. 2025. Divers-bench: Evaluating language identification across domain shifts and code-switching. arXiv preprint arXiv:2509.17768. Tanmay Parekh, Emily Ahn, Yulia Tsvetkov, and Alan Black. 2020. Understanding linguistic accommodation in code-switched human-machine dialogues. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 565577, Online. Association for Computational Linguistics. Shana Poplack. 2013. sometimes ill start sentence in spanish termino en español: Toward typology of code-switching. Linguistics, 51(s1):1114. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Piyush Makhija, Ankit Kumar, and Anuj Gupta. 2020. hinglishNorm - corpus of Hindi-English code mixed sentences for text normalization. In Proceedings of the 28th International Conference on Computational Linguistics: Industry Track, pages 136145, Online. International Committee on Computational Linguistics. Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan Dandapat, and Kalika Bali. 2018. Language modeling for code-mixing: The role of linguistic theory based synthetic data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15431553. 11 Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you dont know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia. Association for Computational Linguistics. Priya Rani, John P. McCrae, and Theodorus Fransen. 2022. MHE: Code-mixed corpora for similar lanIn Proceedings of the Thirguage identification. teenth Language Resources and Evaluation Conference, pages 34253433, Marseille, France. European Language Resources Association. Manikandan Ravikiran and Subbiah Annamalai. 2021. DOSA: Dravidian code-mixed offensive span identification dataset. In Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 1017, Kyiv. Association for Computational Linguistics. Yurii Shynkarov, Veronika Solopova, and Vera Schmitt. 2025. Improving sentiment analysis for ukrainian social media code-switching data. In Proceedings of the Fourth Ukrainian Natural Language Processing Workshop (UNLP 2025), pages 179193. silma-ai. 2024."
        },
        {
            "title": "Silma",
            "content": "9b instruct v1.0. https://huggingface.co/silma-ai/ SILMA-9B-Instruct-v1.0. Sunayana Sitaram, Khyathi Raghavi Chandu, Sai Krishna Rallabandi, and Alan Black. 2019. survey of code-switched speech and language processing. arXiv preprint arXiv:1904.00784. Victor Soto and Julia Hirschberg. 2018. Joint part-ofspeech and language id tagging for code-switched data. In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching, pages 110. Vivek Srivastava and Mayank Singh. 2020. PHINC: parallel Hinglish social media code-mixed corpus for machine translation. In Proceedings of the Sixth Workshop on Noisy User-generated Text (WNUT 2020), pages 4149, Online. Association for Computational Linguistics. Vivek Srivastava and Mayank Singh. 2021. HinGE: dataset for generation and evaluation of code-mixed Hinglish text. In Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 200208, Punta Cana, Dominican Republic. Association for Computational Linguistics. Sathya Krishnan Suresh, Tanmay Surana, Lim Zhi Hao, and Eng Siong Chng. 2025. Cs-sum: benchmark for code-switching dialogue summarization and the limits of large language models. arXiv preprint arXiv:2505.13559. Ishan Tarunesh, Syamantak Kumar, and Preethi Jyothi. 2021. From machine translation to code-switching: Generating high-quality code-switched text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 31543169, Online. Association for Computational Linguistics. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, and 1 others. 2025. Gemma 3 technical report. arXiv preprint arXiv:2503.19786. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, and 1 others. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Richard Tucker. 1999. global perspective on bilingualism and bilingual education. Washington, DC. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. Li Wei. 2018. Translanguaging as practical theory of language. Applied linguistics, 39(1):930. Genta Indra Winata, Alham Fikri Aji, Zheng-Xin Yong, and Thamar Solorio. 2023. The decades progress on code-switching research in nlp: systematic survey on trends and challenges. Findings of the Association for Computational Linguistics: ACL 2023, pages 29362978. Genta Indra Winata, David Anugraha, Patrick Amadeus Irawan, Anirban Das, Haneul Yoo, Paresh Dashore, Shreyas Kulkarni, Ruochen Zhang, Haruki Sakajo, Frederikus Hudi, and 1 others. 2026. Can large language models understand, reason about, and arXiv preprint generate code-switched text? arXiv:2601.07153. Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, and Pascale Fung. 2021. Are multilingual models effective in codeswitching? In Proceedings of the Fifth Workshop on Computational Approaches to Linguistic CodeSwitching, pages 142153. Genta Indra Winata, Zhaojiang Lin, and Pascale Fung. 2019a. Learning multilingual meta-embeddings for 12 In Procode-switching named entity recognition. ceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 181 186. Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Code-switching language modeling using syntax-aware multi-task learning. In Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching, pages 62 67. Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2019b. Code-switched language models using neural based synthetic data from parallel sentences. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 271280. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mt5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Zheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde, Skyler Wang, Arjun Subramonian, Holy Lovenia, Samuel Cahyawijaya, Genta Indra Winata, Lintang Sutawika, Jan Christian Blaise Cruz, Yin Lin Tan, Long Phan, Rowena Garcia, Thamar Solorio, and Alham Fikri Aji. 2023. Prompting multilingual large language models to generate code-mixed texts: The case of south East Asian languages. In Proceedings of the 6th Workshop on Computational Approaches to Linguistic Code-Switching, pages 4363, Singapore. Association for Computational Linguistics. Ruochen Zhang and Carsten Eickhoff. 2024. CroCoSum: benchmark dataset for cross-lingual codeswitched summarization. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 41134126, Torino, Italia. ELRA and ICCL. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Preprint, Evaluating text generation with bert. arXiv:1904.09675. Wenxuan Zhang, Ruidan He, Haiyun Peng, Lidong Bing, and Wai Lam. 2021. Cross-lingual aspectbased sentiment analysis with aspect term codeswitching. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 92209230."
        },
        {
            "title": "A Recruitment Form",
            "content": "The recruitment form is organized into three main sections, as described below. Introduction. This section provides an overview and general background on code-switching, along with the eligibility criteria required for respondents to participate in the crowdsourcing process. The introduction section of the recruitment form is shown in Figure 7. This also shows what kind of data that will be collected from the participant in the process. Self-Assessment. Respondents are asked to provide personal demographic information, as well as details about the languages they commonly use when communicating within their close social circles. The self-assessment section of the recruitment form is illustrated in Figures 8, Figure 9, and Figure 10. Language Assessment. The recruitment form also includes language assessment component, in which respondents are instructed to compose short paragraph on given topic using specified combination of languages. This task is designed to simulate respondents natural code-switching behavior. The language assessment section of the recruitment form is shown in Figure 11."
        },
        {
            "title": "B Annotator Demographics",
            "content": "Table 6 presents the demographic data of the annotators involved in the dataset creation. GENDER AGE (YEARS) Lang #Annotators Male Female 1825-34 35-44 45-54 55+ IDEN JVIDEN SUIDEN HAEN ARDZFR 11 30 13 11 11 10 6 6 7 0 20 7 5 4 11 26 8 4 1 0 4 5 1 9 0 0 0 4 0 0 0 0 2 0 0 0 0 1 Table 6: Annotator Demographics Data"
        },
        {
            "title": "C Annotator Guideline",
            "content": "For each annotator-related task, set of guidelines is provided to ensure that dataset construction is carried out consistently across languages. Dialogue Construction. Guidelines for dialogue construction are provided through dedicated channel on the Discord platform, which is used for collecting the dialogue data. These guidelines describe the participants tasks, explain the grouping process, and specify the rules that must be followed Figure 7: Introduction Section of the Recruitment Form. during dialogue creation. The dialogue construction guidelines are shown in Figures 12 and 13. Question Answering. Guidelines for the Question Answering task are presented on the landing page of the annotation platform. These guidelines outline the annotators responsibilities, the types of questions that must be created, and examples of appropriate questions. The Question Answering guidelines are shown in Figure 14. Dialogue Summarization. Guidelines for Dialogue Summarization are also provided on the landing page of the annotation platform. They describe the annotators tasks, as well as general criteria for producing high-quality summaries, accompanied by illustrative examples. The guidelines for this task are shown in Figure 15. Naturalness. The Naturalness guidelines are presented in the same manner as those for Question Answering and Dialogue Summarization. This section specifies the annotators tasks and details the scoring criteria, which are based on prior work by 14 Figure 8: Self-Assessment Section (Part 1) of the Recruitment Form. Figure 10: Self-Assessment Section (Part 3) of the Recruitment Form. in the Dialogue Summarization task. Similar to the QA setting, the yellow-highlighted section is included only when the few-shot prompting option is applied. Topic Classification. Figure 19 illustrates an example of the prompt used for the Topic Classification task. This prompt uses one-shot setting and instructs the model to provide an explanation for its predicted label. Machine-Generated Conversation. Figure 20 illustrates an example of the prompt template used for inference in the Machine-Generated Conversation task. This prompt is used to perform inference with the GPT-4o model, enabling it to generate dialogue text that adheres to the specifications defined in the prompt. Figure 9: Self-Assessment Section (Part 2) of the Recruitment Form. Yong et al. (2023). The Naturalness guidelines are shown in Figure 16."
        },
        {
            "title": "D Prompt Example",
            "content": "The prompts used in our experiments consist of four components, as described below. Question Answering. Figure 17 illustrates an example of the prompt template used for inference in the Question Answering (QA) task. In the figure, the section highlighted in yellow is included only when the few-shot prompting option is enabled. Dialogue Summarization. Figure 18 presents an example of the prompt template used for inference 15 Figure 12: Dialog Construction Annotator Guideline (Part 1). Figure 13: Dialog Construction Annotator Guideline (Part 2). Model Temperature Top-P Top-K Min-K Max-Tokens QUESTION ANSWERING Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Think No-Think Qwen3-8B Think No-Think Aya23-8B Gemma2-9B-Instruct Gemma3-4B-Instruct Sailor2-8B Sahabat-AI-Gemma SILMA-9B-Instruct ALLAM-7B-Instruct TEXT SUMMARIZATION All models TOPIC CLASSIFICATION All models 0.7 0.7 0.6 0. 0.6 0.7 0.3 0.1 0.2 0.3 0.1 0.3 0.4 0.7 0.7 0.8 0.8 0.95 0.8 0.95 0.8 1.0 0.9 0.9 0.9 0.9 0.9 0. 0.8 0.8 20 20 0.20 20 0.20 20 1 5 10 30 5 10 20 50 0 0 0 0 0 0 0 0 0 0 0 0 0 - - 512 512 8192 8192 8192 8192 512 512 512 512 512 512 512 2000 2000 Table 7: Hyperparameter settings used for each model across all tasks. tings, and (2) the impact of zero-shot versus oneshot prompting strategies. Figure 11: Language-Assessment Section of the Recruitment Form."
        },
        {
            "title": "E Hyperparameter Settings",
            "content": "This section documents the hyperparameter configurations adopted for each model across the three downstream tasks. complete summary of the settings is provided in Table 7."
        },
        {
            "title": "F Quantitative Results",
            "content": "F.1.1 Reasoning vs. Non-Reasoning This section provides comprehensive overview of the quantitative findings obtained under all experimental configurations. Table 8 summarizes the performance comparison between reasoning-enabled and non-reasoning configurations. F.1 Question Answering For the Question Answering task, results are analyzed from two complementary perspectives: (1) the effect of reasoning versus non-reasoning setF.1.2 0-shot vs. 1-shot The comparative results for zero-shot and one-shot prompting setups are presented in Table 9. 16 model Global Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-8B Aya23-8B Gemma2-9B-Instruct Gemma3-4B-Instruct Regional Sailor2-8B Sahabat-AI-Gemma SILMA-9B-Instruct ALLAM-7B-Instruct ID-EN JV-ID-EN SU-ID-EN HA-EN AR_DZ-FR Reasoning Non-reasoning Reasoning Non-reasoning Reasoning Non-reasoning Reasoning Non-reasoning Reasoning Non-reasoning 11.51 12.12 15.55 21.41 25.65 13.73 25.05 35.55 16.16 - - 5.85 4.64 8.08 8.68 8.28 11.91 23.43 27.67 13.73 - - 25.85 35.75 42.22 46.66 31.51 45.05 41. 37.17 46.06 - - 18.98 24.64 30.90 31.71 23.03 43.63 36.76 40.40 43.23 - - 86.66 90.50 90.70 89.89 86.46 91.71 91.31 86.66 93.13 - - 76.16 86.68 89.29 89.29 80.00 91.91 90. 86.46 93.53 - - 43.93 32.82 49.49 50.00 29.79 82.82 71.21 - - - - 43.94 43.93 55.55 45.95 28.78 82.82 69.19 - - - - 91.91 40.40 39.39 41.41 28.28 52.52 36. - - 0.0 39.39 25.25 38.37 32.32 44.44 14.14 50.50 39.39 - - 0.0 32.32 Table 8: Statistics (Acc. %) per-language combination of Reasoning vs. non-Reasoning approach on Question Answering task ID-EN JV-ID-EN SU-ID-EN HA-EN AR_DZ-FR 0-shot 1-shot 0-shot 1-shot 0-shot 1-shot 0-shot 1-shot 0-shot 1-shot model"
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-8B Aya23-8B Gemma2-9B-Instruct Gemma3-4B-Instruct"
        },
        {
            "title": "Regional",
            "content": "Sailor2-8B Sahabat-AI-Gemma SILMA-9B-Instruct ALLAM-7B-Instruct 5.85 4.64 8.88 5.65 8.28 11.91 23.43 27.67 13.73 - - 4,64 3.63 8.08 7.07 13.93 9.09 20.40 29.89 12.12 - - 18.08 24.64 30.90 31.71 23.03 43.63 36. 40.40 43.23 - - 19.19 24.24 28.08 31.31 28.08 39.19 33.93 39.19 41.01 - - 76.16 88.68 90.70 89.89 80.0 91.91 90.70 86.46 93.53 - - 72.32 87.07 89.09 89.29 84.24 91.51 90. 85.45 93.53 - - 43.93 43.93 49.49 50.00 28.78 82.82 69.19 42.42 42.92 51.01 44.44 26.76 81.81 62.62 - - - - - - - - 25.25 38.38 39.39 41.41 14.14 50.50 39. - - 0.0 32.32 25.25 33.33 38.38 37.37 13.13 49.49 38.38 - - 0.0 35.35 Table 9: Statistics (Acc. %) per-language combination of 0-shot vs. 1-shot approach on Question Answering task F.2 Text Summarization"
        },
        {
            "title": "G Additional Results",
            "content": "For the Text Summarization task, we similarly examine two major dimensions: the role of reasoning mechanisms and the effect of prompt shot size. F.2.1 Reasoning vs. Non-Reasoning Detailed results comparing reasoning and nonreasoning settings across different language combinations are reported in Table 10, Table 11, Table 12, Table 13, and Table 14. F.2.2 0-shot vs. Few-shot The overall results for the 0-shot versus few-shot setting are reported in Table 15, Table 16, Table 17, Table 18, Table 19, F.3 Human-written vs. Machine-generated Table 20 reports the overall results comparing human-written vs. machine generated conversation. G.1 Taxonomy of Unanswerable QA Category For the Question Answering (QA) task, annotators for each language were instructed to create up to five unanswerable questions per instance. This design ensures that each instance is represented across the five targeted categories: Negation, Antonym, Entity-Swap, Mutual-Exclusion, and ImpossibleCondition. By the end of the annotation process, three language combinations (IDEN, JVIDEN, and SUIDEN) successfully produced up to five unanswerable questions for each instance. Table 21 reports the model performance (Acc.%) on these three language combinations across all unanswerable categories. The results indicate that Impossible-Condition constitutes the most challenging category, whereas Negation, Antonym, and Entity-Swap are relatively easier for the models to handle. 17 model Global Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Regional Sailor2-8B Sailor2-8B Sahabat-AI-Gemma Sahabat-AI-Gemma Reasoning ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f ID-EN 0.300 0.342 0.331 0.365 0. 0.356 0.356 0.361 0.181 0.337 0. 0.376 0.339 0.369 0.339 0.374 0. 0.367 0.070 0.083 0.093 0.112 0. 0.103 0.099 0.106 0.040 0.090 0. 0.124 0.088 0.104 0.107 0.113 0. 0.123 0.023 0.028 0.036 0.045 0. 0.039 0.039 0.040 0.012 0.033 0. 0.055 0.033 0.037 0.044 0.045 0. 0.054 0.007 0.011 0.014 0.020 0. 0.015 0.015 0.017 0.004 0.013 0. 0.027 0.014 0.014 0.019 0.018 0. 0.024 0.225 0.275 0.224 0.267 0. 0.279 0.265 0.276 0.166 0.261 0. 0.292 0.266 0.293 0.250 0.288 0. 0.283 39.283 42.564 38.935 40.911 44. 43.275 42.481 42.738 33.114 42.649 41. 43.042 43.147 45.643 29.220 43.958 40. 43.135 0.727 0.733 0.751 0.758 0. 0.746 0.742 0.753 0.697 0.735 0. 0.749 0.722 0.726 0.745 0.747 0. 0.755 0.729 0.749 0.734 0.751 0. 0.746 0.748 0.749 0.703 0.740 0. 0.750 0.739 0.752 0.738 0.754 0. 0.751 0.723 0.736 0.736 0.748 0. 0.742 0.742 0.745 0.696 0.733 0. 0.743 0.725 0.735 0.737 0.746 0. 0.746 Table 10: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Indonesian-English (ID-EN) language pair, comparing reasoning and non-reasoning approaches for the Text Summarization task. model Global Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Regional Sailor2-8B Sailor2-8B Sahabat-AI-Gemma Sahabat-AI-Gemma Reasoning ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f JV-ID-EN 0.287 0.348 0.310 0.346 0. 0.363 0.329 0.349 0.168 0.301 0. 0.368 0.314 0.348 0.325 0.370 0. 0.365 0.057 0.085 0.079 0.090 0. 0.104 0.073 0.092 0.031 0.072 0. 0.112 0.066 0.083 0.083 0.100 0. 0.117 0.016 0.033 0.028 0.034 0. 0.042 0.027 0.037 0.010 0.024 0. 0.045 0.021 0.029 0.031 0.038 0. 0.048 0.004 0.012 0.009 0.012 0. 0.018 0.009 0.015 0.004 0.008 0. 0.018 0.007 0.009 0.011 0.014 0. 0.021 0.196 0.263 0.197 0.223 0. 0.264 0.228 0.241 0.139 0.226 0. 0.251 0.226 0.261 0.219 0.262 0. 0.250 36.638 43.448 36.205 38.325 43. 43.845 41.345 41.994 29.913 40.205 38. 41.980 41.986 44.694 39.385 44.016 38. 41.993 0.721 0.731 0.742 0.751 0. 0.747 0.734 0.743 0.687 0.722 0. 0.744 0.716 0.718 0.739 0.747 0. 0.758 0.713 0.737 0.717 0.728 0. 0.739 0.733 0.733 0.686 0.719 0. 0.732 0.722 0.733 0.722 0.741 0. 0.735 0.715 0.732 0.727 0.738 0. 0.742 0.732 0.736 0.685 0.719 0. 0.736 0.718 0.724 0.728 0.743 0. 0.744 Table 11: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Javanese-Indonesian-English (JV-ID-EN) language pair, comparing reasoning and non-reasoning approaches for the Text Summarization task. 18 model Global Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Regional Sailor2-8B Sailor2-8B Sahabat-AI-Gemma Sahabat-AI-Gemma Reasoning ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f SU-ID-EN 0.323 0.402 0.361 0. 0.414 0.419 0.389 0.413 0.163 0. 0.375 0.423 0.373 0.426 0.386 0. 0.386 0.421 0.085 0.121 0.116 0. 0.129 0.144 0.125 0.143 0.040 0. 0.126 0.165 0.109 0.141 0.129 0. 0.133 0.160 0.033 0.043 0.047 0. 0.049 0.060 0.051 0.062 0.013 0. 0.052 0.076 0.041 0.058 0.053 0. 0.058 0.075 0.013 0.017 0.020 0. 0.021 0.027 0.024 0.030 0.005 0. 0.024 0.038 0.018 0.027 0.026 0. 0.028 0.040 0.226 0.285 0.219 0. 0.291 0.298 0.266 0.278 0.139 0. 0.241 0.288 0.268 0.315 0.255 0. 0.259 0.287 40.676 45.336 38.246 38. 46.198 45.494 43.257 43.431 30.086 43. 39.474 42.969 44.144 47.463 42.156 46. 40.752 44.001 0.740 0.758 0.767 0. 0.763 0.776 0.762 0.776 0.705 0. 0.763 0.777 0.748 0.758 0.770 0. 0.772 0.780 0.725 0.747 0.728 0. 0.755 0.753 0.747 0.747 0.691 0. 0.733 0.747 0.745 0.757 0.737 0. 0.736 0.748 0.730 0.750 0.744 0. 0.757 0.762 0.753 0.759 0.696 0. 0.745 0.758 0.744 0.755 0.750 0. 0.750 0.760 Table 12: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Sundanese-Indonesian-English (SU-ID-EN) language pair, comparing reasoning and non-reasoning approaches for the Text Summarization task. model Global Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Reasoning ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f HA-EN 0. 0.113 0.223 0.268 0.023 0.062 0. 0.032 0.090 0.065 0.349 0.374 0. 0.284 0.012 0.013 0.033 0.043 0. 0.007 0.005 0.003 0.015 0.009 0. 0.098 0.048 0.051 0.003 0.004 0. 0.012 0.000 0.001 0.001 0.001 0. 0.003 0.032 0.034 0.011 0.014 0. 0.002 0.003 0.004 0.000 0.000 0. 0.000 0.001 0.001 0.012 0.013 0. 0.004 0.087 0.092 0.133 0.156 0. 0.051 0.044 0.036 0.088 0.069 0. 0.232 0.191 0.201 9.860 11.325 25. 28.319 2.804 6.984 4.715 3.552 9. 9.111 35.717 36.716 27.803 26.231 0. 0.623 0.683 0.689 0.548 0.585 0. 0.590 0.610 0.586 0.731 0.738 0. 0.689 0.600 0.602 0.653 0.657 0. 0.541 0.527 0.515 0.590 0.568 0. 0.717 0.697 0.699 0.606 0.609 0. 0.671 0.524 0.524 0.548 0.546 0. 0.574 0.717 0.726 0.696 0.692 Table 13: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Hausa-English (HA-EN) language pair, comparing reasoning and nonreasoning approaches for the Text Summarization task. 19 model Global Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Regional SILMA-9B-Instruct SILMA-9B-Instruct ALLAM-7B-Instruct ALLAM-7B-Instruct Reasoning ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f AR_DZ-FR 0.026 0.007 0.154 0.141 0.082 0. 0.086 0.106 0.048 0.026 0.149 0. 0.054 0.029 0.047 0.003 0.003 0. 0.005 0.000 0.025 0.027 0.009 0. 0.010 0.019 0.008 0.002 0.024 0. 0.008 0.003 0.012 0.000 0.001 0. 0.002 0.000 0.008 0.008 0.002 0. 0.002 0.006 0.002 0.001 0.007 0. 0.003 0.001 0.005 0.000 0.001 0. 0.001 0.000 0.003 0.003 0.000 0. 0.000 0.002 0.001 0.000 0.003 0. 0.001 0.000 0.002 0.000 0.001 0. 0.046 0.035 0.120 0.109 0.078 0. 0.082 0.090 0.068 0.052 0.116 0. 0.068 0.048 0.035 0.009 0.014 0. 2.915 1.544 20.624 19.137 8.375 12. 9.754 13.002 8.547 6.537 18.390 7. 6.178 2.637 6.684 0.675 0.441 0. 0.539 0.541 0.671 0.668 0.573 0. 0.586 0.606 0.549 0.535 0.647 0. 0.584 0.552 0.470 0.540 0.247 0. 0.541 0.520 0.675 0.661 0.582 0. 0.597 0.605 0.518 0.512 0.645 0. 0.596 0.564 0.434 0.517 0.232 0. 0.538 0.529 0.672 0.663 0.576 0. 0.589 0.603 0.532 0.522 0.645 0. 0.589 0.557 0.449 0.525 0.239 0. Table 14: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Algerian Arabic-France (ar_DZ-FR) language pair, comparing reasoning and non-reasoning approaches for the Text Summarization task. model"
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct"
        },
        {
            "title": "Regional",
            "content": "Sailor2-8B Sailor2-8B Sailor2-8B Sahabat-AI-Gemma Sahabat-AI-Gemma Sahabat-AI-Gemma N-shot ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f ID-EN 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0.342 0.361 0.361 0.365 0.364 0.370 0.356 0.376 0.382 0.361 0.373 0.372 0.337 0.342 0.357 0.376 0.391 0.384 0.369 0.374 0. 0.374 0.391 0.388 0.367 0.364 0.368 0.083 0.096 0.101 0.112 0.112 0.114 0.103 0.113 0.117 0.106 0.117 0.114 0.090 0.098 0.097 0.124 0.134 0.120 0.104 0.108 0.103 0.113 0.126 0.121 0.123 0.120 0.119 0.028 0.032 0.039 0.045 0.045 0.044 0.039 0.044 0.046 0.040 0.048 0.045 0.033 0.034 0.035 0.055 0.060 0.052 0.037 0.039 0.038 0.045 0.051 0.048 0.054 0.052 0.051 0.011 0.011 0.015 0.020 0.019 0.018 0.015 0.020 0.018 0.017 0.020 0.018 0.013 0.011 0.012 0.027 0.028 0.023 0.014 0.016 0. 0.018 0.020 0.021 0.024 0.022 0.023 0.275 0.281 0.285 0.267 0.251 0.264 0.279 0.293 0.299 0.276 0.285 0.285 0.261 0.254 0.266 0.292 0.294 0.284 0.293 0.305 0.291 0.288 0.293 0.303 0.283 0.275 0.278 42.564 42.394 43.404 40.911 39.030 40.908 43.275 43.357 45.431 42.738 43.246 43.404 42.649 41.700 42.941 43.042 42.974 43.031 45.643 45.300 45.439 43.958 43.510 44.948 43.135 41.297 41.850 0.733 0.741 0.741 0.758 0.763 0.761 0.746 0.752 0.750 0.753 0.755 0.755 0.735 0.741 0.740 0.749 0.761 0.759 0.726 0.730 0. 0.747 0.754 0.753 0.755 0.737 0.731 0.749 0.749 0.755 0.751 0.744 0.749 0.748 0.751 0.756 0.749 0.753 0.751 0.740 0.740 0.747 0.750 0.755 0.753 0.752 0.751 0.749 0.754 0.756 0.761 0.751 0.730 0.721 0.736 0.740 0.744 0.748 0.748 0.749 0.742 0.747 0.749 0.745 0.748 0.748 0.733 0.736 0.739 0.743 0.751 0.749 0.735 0.736 0.736 0.746 0.751 0.751 0.746 0.727 0.720 Table 15: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Indonesian-English (ID-EN) language pair, comparing 0-shot and few-shot approaches for the Text Summarization task. 20 model"
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct"
        },
        {
            "title": "Regional",
            "content": "Sailor2-8B Sailor2-8B Sailor2-8B Sahabat-AI-Gemma Sahabat-AI-Gemma Sahabat-AI-Gemma N-shot ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f JV-ID-EN 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0.348 0.366 0.374 0.346 0.353 0.378 0.363 0.393 0.396 0.349 0.371 0.392 0.301 0.329 0.360 0.368 0.384 0.398 0.348 0.352 0. 0.370 0.375 0.387 0.365 0.380 0.403 0.085 0.106 0.118 0.090 0.109 0.129 0.104 0.137 0.142 0.092 0.107 0.128 0.072 0.087 0.102 0.112 0.119 0.132 0.083 0.085 0.098 0.100 0.112 0.121 0.117 0.126 0.146 0.033 0.050 0.055 0.034 0.045 0.063 0.042 0.073 0.074 0.037 0.044 0.059 0.024 0.032 0.043 0.045 0.050 0.058 0.029 0.031 0.038 0.038 0.045 0.050 0.048 0.054 0.075 0.012 0.026 0.029 0.012 0.019 0.033 0.018 0.045 0.045 0.015 0.020 0.033 0.008 0.014 0.022 0.018 0.021 0.024 0.009 0.012 0. 0.014 0.017 0.022 0.021 0.024 0.043 0.263 0.269 0.284 0.223 0.229 0.251 0.264 0.295 0.302 0.241 0.259 0.276 0.226 0.239 0.257 0.251 0.258 0.270 0.261 0.260 0.269 0.262 0.267 0.283 0.250 0.262 0.282 43.448 43.902 44.230 38.325 38.947 39.564 43.845 45.914 46.440 41.994 43.699 43.478 40.205 42.041 43.195 41.980 41.663 42.324 44.694 44.470 44.488 44.016 30.148 32.551 41.993 42.562 43.468 0.731 0.738 0.743 0.751 0.754 0.762 0.747 0.753 0.757 0.743 0.748 0.762 0.722 0.732 0.741 0.744 0.756 0.760 0.718 0.720 0. 0.747 0.746 0.755 0.758 0.754 0.768 0.737 0.743 0.745 0.728 0.734 0.744 0.739 0.754 0.756 0.733 0.742 0.753 0.719 0.726 0.742 0.732 0.739 0.744 0.733 0.736 0.739 0.741 0.743 0.752 0.735 0.732 0.746 0.732 0.740 0.743 0.738 0.742 0.750 0.742 0.752 0.756 0.736 0.744 0.756 0.719 0.728 0.740 0.736 0.746 0.751 0.724 0.727 0.732 0.743 0.743 0.752 0.744 0.741 0.756 Table 16: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Javanese-Indonesian-English (JV-ID-EN) language pair, comparing 0-shot and few-shot approaches for the Text Summarization task. model"
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct"
        },
        {
            "title": "Regional",
            "content": "Sailor2-8B Sailor2-8B Sailor2-8B Sahabat-AI-Gemma Sahabat-AI-Gemma Sahabat-AI-Gemma N-shot ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f SU-ID-EN 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0.402 0.390 0.410 0.393 0.384 0.404 0.419 0.407 0.438 0.413 0.416 0.420 0.375 0.343 0.359 0.423 0.422 0.439 0.426 0.417 0. 0.441 0.440 0.443 0.421 0.416 0.425 0.121 0.121 0.120 0.131 0.129 0.136 0.144 0.140 0.148 0.143 0.143 0.144 0.118 0.098 0.101 0.165 0.154 0.161 0.141 0.137 0.141 0.160 0.160 0.160 0.160 0.164 0.164 0.043 0.047 0.045 0.051 0.056 0.058 0.060 0.061 0.062 0.062 0.062 0.061 0.046 0.038 0.039 0.076 0.072 0.077 0.058 0.056 0.058 0.071 0.073 0.075 0.075 0.080 0.075 0.017 0.021 0.018 0.021 0.026 0.027 0.027 0.028 0.028 0.030 0.031 0.029 0.019 0.015 0.015 0.038 0.034 0.040 0.027 0.026 0. 0.035 0.036 0.036 0.040 0.043 0.038 0.285 0.280 0.291 0.240 0.243 0.262 0.298 0.304 0.325 0.278 0.290 0.295 0.265 0.240 0.253 0.288 0.293 0.307 0.315 0.307 0.309 0.314 0.316 0.319 0.287 0.288 0.299 45.336 44.656 45.493 38.773 38.704 40.958 45.494 46.275 48.013 43.431 45.175 45.306 43.433 41.868 43.532 42.969 43.871 45.235 47.463 47.019 47.424 46.538 47.273 47.422 44.001 43.675 44.927 0.758 0.758 0.756 0.780 0.777 0.778 0.776 0.766 0.771 0.776 0.768 0.772 0.757 0.746 0.742 0.777 0.779 0.780 0.758 0.758 0. 0.779 0.771 0.776 0.780 0.783 0.782 0.747 0.745 0.748 0.738 0.739 0.746 0.753 0.753 0.761 0.747 0.752 0.754 0.739 0.732 0.734 0.747 0.751 0.755 0.757 0.754 0.757 0.756 0.755 0.760 0.748 0.748 0.754 0.750 0.748 0.749 0.755 0.754 0.759 0.762 0.757 0.764 0.759 0.758 0.760 0.745 0.737 0.735 0.758 0.761 0.764 0.755 0.753 0.756 0.765 0.760 0.765 0.760 0.762 0.765 Table 17: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Sundanese-Indonesian-English (SU-ID-EN) language pair, comparing 0-shot and few-shot approaches for the Text Summarization task. 21 model"
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct N-shot ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f HA-EN 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0.113 0.155 0.261 0.268 0.295 0.312 0.062 0.138 0.116 0.032 0.129 0.231 0.065 0.077 0.105 0.374 0.394 0.400 0.284 0.319 0.327 0.013 0.025 0.068 0.043 0.065 0.075 0.007 0.026 0.030 0.003 0.030 0.050 0.009 0.007 0.022 0.098 0.120 0.126 0.051 0.074 0. 0.004 0.006 0.037 0.012 0.024 0.029 0.001 0.009 0.014 0.001 0.012 0.024 0.003 0.001 0.007 0.034 0.048 0.052 0.014 0.026 0.025 0.002 0.002 0.026 0.004 0.011 0.012 0.000 0.004 0.008 0.000 0.005 0.013 0.001 0.000 0.001 0.013 0.019 0.022 0.004 0.009 0.008 0.092 0.130 0.209 0.156 0.169 0.182 0.051 0.116 0.107 0.036 0.098 0.157 0.069 0.075 0.093 0.228 0.242 0.248 0.201 0.226 0.226 11.325 14.350 23.147 28.310 29.287 29.869 6.984 13.009 10.055 3.552 12.017 22.503 9.111 8.098 9.857 36.716 38.164 38.178 26.231 29.031 29.676 0.623 0.636 0.669 0.689 0.702 0.701 0.585 0.622 0.619 0.590 0.624 0.677 0.586 0.598 0.622 0.738 0.747 0.745 0.689 0.701 0.701 0.602 0.627 0.686 0.657 0.671 0.676 0.541 0.604 0.610 0.515 0.587 0.667 0.568 0.573 0.609 0.717 0.722 0.724 0.699 0.706 0. 0.609 0.628 0.675 0.671 0.684 0.686 0.558 0.609 0.610 0.546 0.600 0.669 0.574 0.582 0.612 0.726 0.732 0.732 0.692 0.702 0.702 Table 18: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Hausa-English (HA-EN) language pair, comparing 0-shot and few-shot approaches for the Text Summarization task. model"
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-4B Qwen3-4B Qwen3-8B Qwen3-8B Qwen3-8B Aya23-8B Aya23-8B Aya23-8B Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma2-9B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct Gemma3-4B-Instruct"
        },
        {
            "title": "Regional",
            "content": "SILMA-9B-Instruct SILMA-9B-Instruct SILMA-9B-Instruct ALLAM-7B-Instruct ALLAM-7B-Instruct ALLAM-7B-Instruct N-shot ROUGE1 ROUGE2 ROUGE3 ROUGE4 METEOR CHRF++ BERTScore-p BERTScore-r BERTScore-f AR_DZ-FR 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0 1 3 0.007 0.078 0.145 0.141 0.174 0.220 0.119 0.154 0.165 0.106 0.155 0.168 0.026 0.055 0.067 0.070 0.182 0.214 0.029 0.139 0. 0.003 0.020 0.041 0.002 0.003 0.002 0.000 0.012 0.022 0.027 0.029 0.040 0.022 0.006 0.014 0.019 0.013 0.029 0.002 0.005 0.003 0.016 0.031 0.037 0.003 0.019 0.024 0.000 0.002 0.005 0.001 0.000 0.001 0.000 0.004 0.006 0.008 0.010 0.015 0.008 0.000 0.003 0.006 0.003 0.011 0.001 0.001 0.001 0.007 0.009 0.012 0.001 0.007 0.007 0.000 0.001 0.002 0.000 0.000 0.001 0.000 0.002 0.003 0.003 0.004 0.005 0.003 0.000 0.000 0.002 0.001 0.003 0.000 0.000 0.000 0.002 0.003 0.003 0.000 0.003 0. 0.000 0.000 0.001 0.000 0.000 0.000 0.035 0.084 0.137 0.109 0.127 0.161 0.104 0.128 0.145 0.090 0.124 0.133 0.052 0.075 0.085 0.057 0.131 0.154 0.048 0.127 0.149 0.009 0.018 0.031 0.029 0.032 0.028 1.544 9.519 14.531 19.137 22.360 22.724 12.945 16.568 15.542 13.002 16.640 14.893 6.537 10.644 12.104 7.075 21.082 21.825 2.637 15.377 15.924 0.675 2.848 4.942 0.801 0.824 0.855 0.541 0.593 0.628 0.668 0.688 0.683 0.616 0.657 0.643 0.606 0.642 0.613 0.535 0.567 0.583 0.584 0.665 0.671 0.552 0.631 0. 0.540 0.548 0.573 0.537 0.537 0.527 0.520 0.597 0.661 0.661 0.686 0.691 0.625 0.684 0.672 0.605 0.661 0.621 0.512 0.547 0.567 0.570 0.661 0.675 0.564 0.658 0.671 0.517 0.527 0.541 0.520 0.523 0.522 0.529 0.593 0.643 0.663 0.686 0.686 0.619 0.669 0.656 0.603 0.650 0.615 0.522 0.555 0.573 0.576 0.662 0.672 0.557 0.643 0.652 0.525 0.534 0.553 0.527 0.529 0.523 Table 19: Statistics for the remaining evaluation metrics (ROUGE-1, ROUGE-2, ROUGE-3, ROUGE-4, METEOR, CHRF++, and BERTScore-P/R/F) on the Algerian Arabic-French (ar_DZ-FR) language pair, comparing 0-shot and few-shot approaches for the Text Summarization task. 22 Metric Human-written Machine-gen Human-written Machine-gen Human-written Machine-gen Human-written Machine-gen Human-written Machine-gen ID-EN JV-ID-EN SU-ID-EN HA-EN AR_DZ-FR 2 speakers (50 dialogue) Avg length variance (tokens) Total replies Avg degree of reply distance Avg imbalance ratio of speaker turns Avg CMI Avg SPF Human preference 3 speakers (25 dialogue) Avg length variance (tokens) Total replies Avg degree of reply distance Avg imbalance ratio of speaker turns Avg CMI Avg SPF Human preference 4 speakers (25 dialogue) Avg length variance (tokens) Total replies Avg degree of reply distance Avg imbalance ratio of speaker turns Avg CMI Avg SPF Human preference 83.436 482 3.053 1.364 0.491 0.306 2.721 44.122 414 4.309 3.286 0.444 0.291 2.692 63.764 718 4.343 3.459 0.46 0.298 2.573 29.438 1 0.02 1.016 0.711 0.423 2. 22.732 23 0.98 1.053 0.704 0.429 2.538 32.003 43 1.263 1.1 0.667 0.393 2.533 47.58 1049 2.972 1.315 0.467 0.312 2.627 50.32 772 3.814 1.895 0.476 0.328 2.603 35.918 950 4.037 2.586 0.456 0.318 2.625 43.376 9 0.18 1.019 0.733 0.441 2. 42.88 17 0.558 1.107 0.689 0.411 2.359 32.82 39 1.099 1.209 0.729 0.44 2.083 104.202 831 2.066 1.368 0.734 0.461 2.840 98.882 941 3.222 1.832 0.77 0.469 2.893 167.263 1307 3.841 2.3 0.789 0.479 2.853 27.267 1 0.02 1.017 0.719 0.436 2. 23.811 22 0.668 1.044 0.718 0.437 2.107 24.299 55 0.998 1.098 0.719 0.452 2.200 48.252 747 2.645 1.195 0.37 0.202 2.483 48.519 761 3.188 3.324 0.348 0.187 2.467 58.806 1062 3.543 2.786 0.321 0.172 2.385 28.469 2 0.02 1.017 0.374 0.189 2. 23.069 20 0.723 1.052 0.202 0.1 2.520 15.9 43 1.005 1.086 0.191 0.096 2.538 49.231 182 2.909 1.587 0.565 0.251 2.969 54.157 270 3.921 4.347 0.565 0.248 2.942 80.132 305 5.115 4.272 0.576 0.261 2.942 47151 0 0 1.026 0.169 0.075 1. 38.657 17 0.76 1.024 0.117 0.044 1.192 40.934 45 1.097 1.097 0.108 0.041 1.192 Table 20: Full quantitative statistics per-language combination of human written VS machine generated conversational pattern ID-EN JV-ID-EN SU-ID-EN Neg. Ant. Ent. Mut. Imp. Neg. Ant. Ent. Mut. Imp. Neg. Ant. Ent. Mut. Imp."
        },
        {
            "title": "Global",
            "content": "Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen3-4B Qwen3-8B Aya23-8B Gemma2-9B-Instruct Gemma3-4B-Instruct 46.46 48.48 28.28 33.83 54.54 35.35 10.10 45.45 61.61 29.29 44.44 44.44 31.31 6.06 36.36 40.40 10.10 23.23 38.38 20.20 4.04 59.59 63.63 44.44 73.73 64.64 75.75 18.18 76.76 87.87 76.76 88.88 77.77 87.87 31. 58.58 60.60 30.30 50.50 50.50 45.45 13.13 46.46 51.51 31.31 41.41 43.43 36.36 10.10 42.42 52.52 18.18 33.33 38.38 20.20 7.07 67.67 65.65 42.42 68.68 65.65 74.74 23.23 88.88 90.90 78.78 90.90 81.81 90.90 34.34 90.90 80.80 82.82 93.93 72.72 92.92 51. 77.77 68.68 65.65 86.86 66.66 86.86 26.26 77.77 76.76 73.73 86.86 52.52 87.87 42.42 88.88 78.78 75.75 85.85 73.73 92.92 40.40 95.95 91.91 90.90 98.98 79.79 94.94 54."
        },
        {
            "title": "Regional",
            "content": "Sailor2-8B Sahabat-AI-Gemma 10.10 40.40 12.12 25.25 3.03 19.19 15.15 74.74 16.16 86. 13.13 44.44 7.07 39.39 3.03 18.18 16.16 77.77 12.12 91.91 41.41 92. 26.26 85.85 23.23 82.82 28.28 91.91 34.34 92.92 Table 21: Statistics (Acc. %) per-language combination of Unanswerable questions category on Question Answering task. (Neg. = Negation, Ant. = Antonym, Ent. = Entity-Swap, Mut. = Mutually-Exclusion, and Imp. = ImpossibleCondition) Figure 14: Question Answering Annotator Guideline. Figure 15: Dialogue Summarization Annotator Guideline. Figure 16: Human-written vs Machine-generated Naturalness Annotator Guideline."
        },
        {
            "title": "Prompt example for Question Answering",
            "content": "Answer the question provided according to the context in the dialog. Just write the answer in the form of A, B, C, D, or based on the appropriate option, without including the sentences accompanying the options. If the question does not match the context of the conversation or if none of the answer choices are correct, choose option \"<There is no correct answer>\". Below are <NUMBER OF SHOT> example QA pairs from the example dialog. ### EXAMPLE DIALOG <DIALOG EXAMPLE> ### EXAMPLE <i> QUESTION: <QUESTION EXAMPLE i> OPTIONS: A. <OPTION A> B. <OPTION B> C. <OPTION C> D. <OPTION D> E. <OPTION E> ANSWER: <ANSWER EXAMPLE i> Now answer the question for the dialog below. ### DIALOG <DIALOG> ### QUESTION <QUESTION> ### OPTIONS A. <OPTION A> B. <OPTION B> C. <OPTION C> D. <OPTION D> E. <OPTION E> ### OUTPUT FORMAT Return JSON response in the following format: <OUTPUT FORMAT> ### OUTPUT Figure 17: Prompt template for the Question Answering task (Multiple Choice subset). The highlighted middle section corresponds to an optional few-shot component and is included only in the few-shot prompting setting."
        },
        {
            "title": "Prompt example for Dialogue Summarization",
            "content": "# INSTRUCTIONS Your primary task is to write concise summary of given dialogue. The summary should be focused on the provided topic and adhere strictly to the rules and output format specified below: - The summary should be written in target-lang. Once again, write the summary in target-lang. - The summary must be single paragraph. - The paragraph must be 3 to 5 sentences. - The summary must be strictly factually correct, drawing all information directly from the dialogue. Do not introduce any external information or make assumptions not supported by the text. - Prior to composing the summary, provide brief analysis of the dialogue. This analysis should identify the key points selected for the summary and offer justification for their inclusion. # EXAMPLES ## DIALOGUE EXAMPLE Topic: <TOPIC EXAMPLE> Dialogue: <DIALOGUE EXAMPLE> ## SUMMARY EXAMPLE { \"summary\": SUMMARY } Now, please summarize the dialogue below. # DIALOGUE Topic: <TOPIC> Dialogue: <DIALOGUE> # OUTPUT FORMAT Return JSON response in the following format: <OUTPUT FORMAT> # OUTPUT Figure 18: Prompt template for the Dialogue Summarization task. The highlighted middle section corresponds to an optional few-shot component and is included only in the few-shot prompting setting."
        },
        {
            "title": "Prompt example for Topic Classification",
            "content": "# INSTRUCTIONS Your primary task is to classify the provided code-switched/code-mixed dialogue into one of the following categories: - Entertainment - Science/Technology - Social/Culture - Education - Daily Life Before classifying, please provide brief analysis of the dialogue. This analysis should identify the key points that support your classification. # DIALOGUE-TOPIC PAIR EXAMPLE Dialogue: <DIALOG EXAMPLE> { \"category\": <CATEGORY EXAMPLE> } Now, please classify the dialogue below. # DIALOGUE Dialogue: <DIALOG> # OUTPUT FORMAT Return JSON response in the following format: { \"explanation\": \"This section contains brief analysis of the dialogue that supports the classification.\", \"category\": \"This section contains exactly one category selected from the possible options. Output only the category name, without any additional text.\" } # OUTPUT Figure 19: Prompt template for the Topic Classification. 27 Prompt example for Machine GeneratedConversational Text Write dialogue about <TOPIC> that uses code-switching and fulfills the following conditions: - The dialogue involves <NUMBER OF PERSON> people. - Use <SPEAKERS LABEL> to label the speakers. - The language used in the conversation should be code-switched between <LANGUAGE>. Make sure that each language is present in the conversation. - The conversation is informal, so the use of slang, casual expressions, and relaxed punctuation, grammar, or capitalization is allowed. - The dialogue should consist of 50 to 150 utterances (about 15 minutes conversation). - Emojis may be used in the dialogue. - The dialogue takes place in an online setting (e.g., group chat, direct messages). - Replies to specific messages are allowed and can be included. - Ensure the conversation flows naturally. Use the following format for the dialogue: #1 A: insert text #2 B: insert text #3 [reply to #1]: insert text #4 A: insert text #5 D: insert text #6 [reply to #5]: insert text Write the dialogue here: Figure 20: Prompt template for the Machine GeneratedConversational Text."
        }
    ],
    "affiliations": [
        "Bayero University Kano",
        "Capital One",
        "Cardiff University",
        "Imperial College London",
        "Institut Teknologi Bandung",
        "MBZUAI",
        "Monash University Indonesia",
        "University of Edinburgh"
    ]
}