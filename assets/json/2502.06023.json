{
    "paper_title": "Dual Caption Preference Optimization for Diffusion Models",
    "authors": [
        "Amir Saeidi",
        "Yiran Luo",
        "Agneet Chatterjee",
        "Shamanthak Hegde",
        "Bimsara Pathiraja",
        "Yezhou Yang",
        "Chitta Baral"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT_Chosen, Diffusion-DPO, and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone."
        },
        {
            "title": "Start",
            "content": "Under review."
        },
        {
            "title": "DUAL CAPTION PREFERENCE OPTIMIZATION FOR\nDIFFUSION MODELS",
            "content": "Amir Saeidi1, Yiran Luo1, Agneet Chatterjee1, Shamanthak Hegde1, Bimsara Pathiraja1, Yezhou Yang1, Chitta Baral1 1Arizona State University, {ssaeidi1, yluo97, agneet, yz.yang, chitta}@asu.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to conflict distribution. Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising networks ability to accurately predict noise in preference optimization methods, known as the irrelevant prompt issue. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFTChosen, Diffusion-DPO and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone. Our code and dataset are available in Github. 5 2 0 2 9 ] . [ 1 3 2 0 6 0 . 2 0 5 2 : r Figure 1: Sample images generated by different methods on the HPSv2, Geneval, and Pickscore benchmarks. After fine-tuning SD 2.1 with SFTChosen, Diffusion-DPO, MaPO, and DCPO on Picka-Picv2 and Pick-Double Caption datasets, DCPO produces images with notably higher preference and visual appeal (See more examples in Appendix H). Equal contribution. Correspondence to ssaeidi1@asu.edu 1 Under review. Figure 2: The overview of our 3 Dual Preference Optimization (DCPO) pipelines: DCPO-c, DCPOp, and DCPO-h, all of which require duo of captioned preferred image (xw 0 , zw) and captioned 0, zl). DCPO-c (Top Left): We use captioning model to generate disless-preferred image (xl tinctive captions respectively for images xw 0 and xl 0 given the shared prompt c. DCPO-p (Bottom Left): We take prompt as the caption for image xw 0 , then we use Large Language Model (LLM) to generate semantically perturbed prompt zl 0. DCPOp for image xl (Right): hybrid method where the generated caption zl is now perturbed into zl 0. Our Pick-Double Caption Dataset discussed in Section 3.1 is constructed with the DCPO-c pipeline. given prompt as the caption for image xl"
        },
        {
            "title": "INTRODUCTION",
            "content": "Image synthesis models (Rombach et al., 2022; Esser et al., 2024) have achieved remarkable advancements in generating photo-realistic and high-quality images. Text-conditioned diffusion (Song et al., 2020a) models have led this progress due to their strong generalization abilities and proficiency in modeling high-dimensional data distributions. As result, they have found wide range of applications in image editing (Brooks et al., 2023), video generation (Wu et al., 2023a) and robotics (Carvalho et al., 2023). Consequently, efforts have focused on aligning them with human preferences, targeting specific attributes like safety (Liu et al., 2024b), style (Patel et al., 2024; Everaert et al., 2023), spatial understanding (Chatterjee et al., 2024b), and personalization (Ruiz et al., 2023), thereby improving their usability and adaptability. Similar to the alignment process of Large Language Models (LLMs), aligning diffusion models involves two main steps: 1. Pre-training and 2. Supervised Fine-Tuning (SFT). Recent fine-tuning based methods have been introduced to optimize diffusion models according to human preferences by leveraging Reinforcement Learning with Human Feedback (RLHF) (Ouyang et al., 2022), the aim of which is to maximize an explicit reward. However, challenges such as reward hacking have led to the adoption of Direct Preference Optimization (DPO) (Rafailov et al., 2024) techniques like Diffusion-DPO (Wallace et al., 2024). Intuitively, Diffusion-DPO involves maximizing the difference between preferred and less preferred image for given prompt. Although DPO-based methods are effective in comparison to SFT-based approaches, applying direct optimization in multi-modal settings presents certain challenges. Current preference optimization datasets consist of preferred (xw) and less preferred (xl) image for given prompt (c). Ideally, xw should show higher correlation with compared to xl. However, we find that in current datasets, both the images share the same distribution for the given prompt c, which we refer to as conflict distribution in the data. Additionally, irrelative information in restricts the U-Nets ability to predict noises from xl in the diffusion reverse process, which we refer to as irrelevant prompts. This entails that there is lack of sufficient distinguishing features between the two pairs (xw, c), (xl, c), thereby increasing the complexity of the optimization process. 2 Under review. Overall, we identify two key challenges: 1. conflict distribution in data, which contradicts the core idea of direct optimization, and 2. irrelevant prompts for less preferred images, which can hinder the learning process of the diffusion model during optimization. To address the aforementioned bottleneck, we propose DCPO: Dual Caption Preference Optimization, novel preference optimization technique designed to align diffusion models by utilizing two distinct captions corresponding to the preferred and less preferred image. DCPO broadly consists of two steps - text generation framework that develops better aligned captions and novel objective function that utilizes these captions as part of the training process. The text generation framework seeks to alleviate the conflict distribution present in existing datasets. We hypothesize that does not serve as the optimal signal for optimization because they do not convey the reasons why an image is preferred or dis-preferred; based on the above, we devise the following techniques to generate better aligned captions. The first method involves using captioning model Qϕ(zixi, c); which generates new prompt zi based on an image xi and the original prompt c, where (w, l). The second method introduces perturbation techniques , such that = zw, zl = (c); i.e. generating zl, to represent the less preferred image, considering the original prompt as the prompt aligned with the preferred image. We investigate multiple semantic variants of , where each variant differs in the degree of perturbation applied to the original caption c. Finally, we also explore hybrid combination of the above methods, where we combine the strong prior of the captioning model and the efficient nature of the perturbation method. All the above methods are designed to generate captions that effectively discriminate between the preferred and less preferred images. We introduce novel objective function that allows DCPO to incorporate zw and zl into its optimization process. Specifically, during optimization, the policy model pθ increases the likelihood of the preferred image xw conditioned on the prompt zw, while simultaneously decreasing the likelihood of the less preferred image xl conditioned on the prompt zl. The results in Tables 1 and 2 demonstrate that DCPO consistently outperforms other methods, with notable improvements of +0.21 in Pickscore, +0.45 in HPSv2.1, +1.8 in normalized ImageReward, +0.15 in CLIPscore, and +3% in GenEval. Additionally, DCPO achieved 58% in general preference and 66% in visual appeal compared to Diffusion-DPO on the PartiPrompts dataset, as evaluated by GPT-4o (see Figure 8). In summary, our contributions are as follows : Double Caption Generation: We introduce the Captioning and Perturbation methods to address the conflict distribution issue, as illustrated in Figure 3. In the Captioning method, we employ state-of-the-art models like LLaVA (Liu et al., 2024a) and Emu2 (Sun et al., 2024) to generate caption based on the image and prompt c. Additionally, we use DIPPER (Krishna et al., 2024), paraphrase generation model built by fine-tuning the T5XXL model to create three levels of perturbation from the prompt c. Dual Caption Preference Optimization (DCPO): We propose DCPO, modified version of Diffusion-DPO, that leverages the U-Net encoder embedding space for preference optimization. This method enhances diffusion models by aligning them more closely with human preferences, using two distinct captions for the preferred and less preferred images during optimization. Improved Model Performance: We demonstrate that our approach significantly outperforms SD 2.1, SFT, Diffusion-DPO, and MaPO across metrics such as Pickscore, HPSv2.1, GenEval, CLIPscore, normalized ImageReward, and GPT-4o (Achiam et al., 2023) evaluations."
        },
        {
            "title": "2 METHOD",
            "content": "In this section, we present the conflict distribution issue in preference datasets, where preferred and less-preferred images generated from the same prompt exhibit significant overlap. We also explain the irrelevant prompt issue found in previous direct preference optimization methods. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), method that uses distinct captions for preferred and less preferred images to improve diffusion model alignment. 3 Under review."
        },
        {
            "title": "2.1 THE CHALLENGES",
            "content": "Generally, to optimize Large Language Model (LLM) using preference algorithms, we need dataset = {c, yw, yl}, where yw and yl represent the preferred and less preferred responses to given prompt c. Ideally, the distributions of these responses should differ significantly. Similarly, in diffusion model alignment, the distributions of preferred and less preferred images should be distinct for the same prompt c. However, our analysis shows substantial overlap between these distributions, which we call conflict distribution, as illustrated in Figure 3. Another issue emerges when direct preference optimizes diffusion model. In the reverse denoising process, the U-Net model predicts noise for both preferred and less preferred images using the same prompt c. As prompt is more relevant to the preferred image, it becomes less effective for predicting the less preferred one, leading to reduced performance. We call this the irrelevant prompts problem. Figure 3: The conflict distribution issue in the Pick-a-Pic v2 dataset. µl and µw represent the average CLIPscore of preferred and less preferred images for prompt c, respectively. Also, µ shows the difference between the distributions. 2.2 DCPO: DUAL CAPTION PREFERENCE OPTIMIZATION Motivated by the conflict distribution and irrelevant prompts issues, we propose DCPO, new preference optimization method that optimizes diffusion models using two distinct captions. DCPO is refined version of Diffusion-DPO designed to address these challenges. More details are in Appendix B. 1 We start with fixed dataset = {c, xw 0}, where each entry contains prompt and pair of images generated by reference model pref . The human labels indicate preference, with xw 0 preferred over xl 0. We assume the existence of model Rϕ(zc, x), which generates caption given 0 , xl prompt and an image x. Using this model, we transform the dataset into = {zw, zl, xw 0}, where zw and zl are captions for the preferred image xw 0, respectively. Our goal is to train new model pθ, aligned with human preferences, to generate outputs that are more desirable than those produced by the reference model. 0 and the less-preferred image xl 0 , xl The objective of RLHF is to maximize the reward r(c, x0) for the reverse process pθ(x0:T z), while maintaining alignment with the original reference reverse process distribution. Building on prior work (Wallace et al., 2024), the DCPO objective is defined by direct optimization through the conditional distribution pθ(x0:T z) as follows: LDCPO(θ) = (xw 0 ,xl 0,zl,zw)D log σ(βE 0 ,zw),xl 1:T pθ(xw xw pθ(xw pref(xw 1:T xw 0:T zw) 0:T zw) [log 0zl) 1:T pθ(xl pθ(xl pref(xl 1:T ,xl 0:T zl) 0:T zl) log (1) ]) However, as noted in Diffusion-DPO (Wallace et al., 2024), the sampling process x1:T p(x1:T x0) is inefficient and intractable. To overcome this, we follow similar approach by applying Jensens inequality and utilizing the convexity of the log() function to bring the expectation outside. By approximating the reverse process pθ(x1:T x0, z) with the forward process q(x1:T x0), and through algebraic manipulation and simplification, the DCPO loss can be expressed as: LDCPO(θ) = log σ(βT w(λt)(ϵw ϵθ(xw (ϵl ϵθ(xl 0 ,xl (xw xw 0)D,tµ(0,T ),xw , zw, t)2 t, zl, t)2 0 ),xl q(xw 2 ϵw ϵref(xw 2 ϵl ϵref(xl txl tq(xl 0) , zw, t)2 2 t, zl, t)2 2)) (2) 1For additional background about diffusion and preference optimization, refer to Appendix A. 4 Under review. Figure 4: Effect of the perturbation method on semantic distributions in terms of CLIPScore. (a) shows the distributions that feature the captions zw and zl generated by the LLaVA model, while (b), (c), and (d) represent different levels of perturbation on top of the caption zl. The figure demonstrates that as the level of perturbation increases, the distance between the distributions of captions zw and zl increases. For more details on the perturbation method, refer to Appendix E. = αtx 0 + σtϵ, and ϵ (0, I) is sample drawn from q(x where represents the signal-to-noise ratio, and ω(λt) is weighting function. To optimize diffusion model using DCPO, dataset = {zw, zl, xw 0} is required, where captions are paired with the images. However, the current preference dataset only contains prompts and image pairs without captions. To address this, we propose three methods for generating captions and introduce new high-quality dataset, Pick-Double Caption, which provides specific captions for each image, based on Pick-a-Pic v2 (Kirstain et al., 2023). 0). λt = α2 /σ2 0 , xl 2.2.1 DCPO-C: CAPTIONING METHOD In this method, the captioning model Qϕ(zc, x) generates the caption based on the image and the original prompt c. As result, we obtain preferred caption zw Qϕ(zwc, xw) for the preferred image and less preferred caption zl Qϕ(zlc, xl) for the less preferred image, as illustrated in sample in Figure 2. Thus, based on the generated captions zw and zl, we can optimize diffusion model using the DCPO method. In the experiment section, we evaluate the performance of DCPO-c and demonstrate that this method effectively mitigates the conflict distribution by creating two differentiable distributions. However, the question of how much divergence is needed between the two distributions remains. To investigate this, we propose Hypothesis 1. Hyphothesis 1. Let d(z, x) represent the semantic distribution between caption and an image x, with µ being the mean of the distribution d, and µ = µ(d(zw 0)) as the difference between the two distributions. Increasing µ between the preferred and less-preferred image distributions in preference dataset beyond threshold (i.e., µ > t), can improve the performance of the model pθ. 0 )) µ(d(zl 0 , xw 0, xl Our hypothesis suggests that increasing the distance between the two distributions up to certain threshold can improve alignment performance. To examine this, we propose the perturbation method to control the distance between the two distributions, represented by µ. 2.2.2 DCPO-P: PERTURBATION METHOD While using captioning model is an effective way to address the conflict distribution, it risks deviating from the original distribution of prompt c, and the distributions of preferred and less preferred images may still remain close. To tackle these issues, we propose perturbation method. In this approach, we assume that prompt is highly relevant to the preferred image xw 0 and aim to generate less relevant caption, denoted as cp, based on prompt c. To achieve this, we use the model Wϕ(cpc), which generates perturbed version of prompt c, altering its semantic meaning. In this framework, prompt corresponds to the preferred caption zw (c = zw), while the perturbed prompt 5 Under review. cp represents the less-preferred caption zl (cp = zl). For the perturbation model Wϕ, we utilized the DIPPER model (Krishna et al., 2024) built by fine-tuning the T5-XXL (Chung et al., 2022) to produce degraded version of the prompt c. We define three levels of perturbation: 1) Weak: where prompt cp has high semantic similarity to prompt c, with minimal differences. 2) Medium: where the semantic difference between prompt cp and is more pronounced than in the weak level. 3) Strong: where the majority of the semantics in prompt cp differ significantly from prompt c. Further details can be found in Appendix E. The main advantage of DCPO-p is to reduce the captioning process cost while staying closer to the original data distribution by using prompt as the preferred caption. However, we observe that the quality of captions in DCPO-c outperforms that of the original prompt c, as shown in Table 6 in Appendix D. Based on this observation, we propose hybrid method to improve the alignment performance by combining captioning and perturbation techniques."
        },
        {
            "title": "2.2.3 DCPO-H: HYBRID METHOD",
            "content": "In this method, instead of perturbing the prompt c, we perturb the caption generated by the model Qϕ(zx, c) based on the image and prompt c. As discussed in Section 2.2.1, the goal of the perturbation method is to increase the distance between the two distributions. However, the correlation between the image x0 and prompt significantly impacts alignment performance. Therefore, we propose Hypothesis 2. Hypothesis 2. Let S(c, x) represent the correlation score between prompt and image x, and (pθ(c1, c2)) denote the performance of model pθ optimized on captions c1 and c2 with DCPO, where Wϕ is the perturbation model. If S(z, x) > S(c, x), then (pθ(zw, zw zw))) > (pθ(c, cp Wϕ(cpc))). Wϕ(zw In Section 3.3, we provide experimental evidence supporting Hypothesis 2 and investigate the pop zw) as tential of using zl originally proposed in Hypothesis 2. pzl) as the less-preferred caption zl, instead of zw Wϕ(zw Wϕ(zl"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "We fine-tuned the U-Net model of Stable Diffusion 2.1 (SD 2.1) using DCPO on the Pick-Double Caption dataset and compared it to SD 2.1 models fine-tuned with SFTChosen, Diffusion-DPO, and MaPO on Pick-a-Pic v2 across various metrics. We first describe the Pick-Double Caption dataset and compare it to Pick-a-Pic v2. Subsequently, we provide an indepth analysis of the results. More comparisons are found in Appendix C. Also, for further details on the fine-tuning, refer to Appendix F. 3.1 PICK-DOUBLE CAPTION DATASET Table 1: Results on PickScore, HSPv2.1, ImageReward (normalized), and CLIPScore. We show that DCPO significantly improves on Pickscore, HSP2.1, and ImageReward. Image Reward () CLIP Score () HPSv2.1 () () Pick Score Results from other methods SD 2.1 SFTChosen Diffusion-DPO MaPO 20.30 20.35 20.36 19.41 Results from our methods DCPO-c (LLaVA) DCPO-c (Emu2) DCPO-p DCPO-h (LLaVA) 20.46 20.46 20.28 20.57 25.17 25.09 25.10 24.47 25.10 25.06 25.42 25. 55.8 56.4 56.4 50.4 56.5 56.6 54.2 58.2 26.84 26.98 26.98 24.82 27.00 26.97 26.98 27.13 Motivated by the conflict distribution observed in previous preference datasets, we applied the captioning method described in Section 2.2.1 to generate unique captions for each image in the Pick-aPic v2 dataset. For the Pick-Double Caption dataset, we sampled 20,000 instances from Pick-a-Pic v2 and cleaned the samples as detailed in Appendix D. We then employed two state-of-the-art captioning models, LLaVa-1.6-34B and Emu2-37B, to generate captions for both the preferred and less preferred images, as illustrated in Figure 2. To generate the captions, we used two different prompting strategies: 1) Conditional prompt: where the model was explicitly instructed to generate caption for image based on the given prompt c, and 2) Non-conditional prompt: where the model provided general description of the image in one sentence without referring to specific prompt. More details are in Appendix D. 6 Under review. Table 2: Results on the GenEval Benchmark. DCPO significantly enhances model performance in generating the correct number of objects, improving image quality in terms of colors, and constructing attributes accurately. Method Overall Results from other methods SD 2.12 SFTChosen Diffusion-DPO MaPO 0.4775 0.4797 0.4857 0.4106 Results from our methods DCPO-c (LLaVA) DCPO-c (Emu2) DCPO-p DCPO-h (LLaVA) 0.4971 0.4925 0.4906 0.5100 Single object Two objects Counting Colors Position Attribute binding 0.96 1.00 0.99 0.98 1.00 1.00 1.00 0.99 0.52 0.42 0.48 0. 0.43 0.41 0.41 0.51 0.35 0.42 0.46 0.28 0.53 0.50 0.50 0.54 0.80 0.81 0.83 0.66 0.85 0.85 0.83 0.84 0.09 0.07 0.04 0. 0.02 0.04 0.03 0.05 0.15 0.14 0.11 0.09 0.14 0.15 0.17 0.14 We evaluated the captions generated by LLaVA and Emu2 using CLIPscore, which revealed several key insights. LLaVA produced captions that have more correlation with the images for both preferred and less preferred samples compared to Emu2 and the original captions, although LLaVAs captions were significantly longer (see Table 6 in Appendix D). Models fine-tuned on captions from the conditional prompt strategy outperformed those using the non-conditional approach, though the conditional prompt captions were twice as long. Interestingly, despite Emu2 generating much shorter captions, the models fine-tuned on Emu2 were comparable to those fine-tuned on the original prompts from Pick-a-Pic v2. key challenge is generating captions for the less preferred images using the captioning method. We observed that in both prompting strategies, the captions for the preferred images are more aligned with the original prompt distribution. However, the non-conditional prompt strategy often produces captions for less preferred images that are out-of-distribution (OOD) from the original prompt in most cases. We will explore this further in Section 3.3. Finally, we observe that the key advantage of the Pick-Double Caption dataset is the greater difference in CLIPscore (µ) between preferred and less preferred images compared to the original prompts. Specifically, while the original prompt has µ of 1.3, LLaVA shows much larger difference at 4.3, and Emu2 at 2.8. This increased gap reflects improved alignment performance in models fine-tuned on this dataset, indicating that the captioning method mitigates the conflict distribution. 3.2 PERFORMANCE COMPARISONS We evaluated all methods on 2,500 unique prompts from the Pick-a-Pic v2 (Kirstain et al., 2023) dataset, measuring performance using Pickscore (Kirstain et al., 2023), CLIPscore (Hessel et al., 2022), and Normalized ImageReward (Xu et al., 2023). Additionally, we generated images from 3,200 prompts in the HPSv2 (Wu et al., 2023b) benchmark and evaluated them using the HPSv2.1 model. To provide comprehensive evaluation, we also compared the methods using GenEval (Ghosh et al., 2023), focusing on how well the fine-tuned models generated images with the correct number of objects, accurate colors, and proper object positioning. We compared different versions of DCPO, including the captioning (DCPO-c), perturbation (DCPOp), and hybrid (DCPO-h) methods, with other approaches, as outlined in Section 2.2. For more information on the fine-tuning process of the models, refer to Appendix F. The results in Tables 1 and 2 show that DCPO-h significantly outperforms the best scores from other methods, with improvements of +0.21 in Pickscore, +0.45 in HPSv2.1, +1.8 in ImageReward, +0.15 in CLIPscore, and +3% in GenEval. Additionally, the results demonstrate that DCPO-c outperforms all other methods on GenEval, Pickscore, and CLIPscore. While DCPO-p performs slightly worse than DCPO-c, it still exceeds SD 2.1, SFT, Diffusion-DPO, and MaPO on GenEval. However, its scores on ImageReward and Pickscore suggest that it underperforms compared to the 2Note that we rerun all the models on same seeds to have fair comparison. Under review. Figure 5: Performance comparison of DCPO-c and DCPO-h on different perturbation levels. We plotted regression lines for the four models, showing that as µ increases, performance improves but drops after threshold (orange boundary). Table 3: Performance comparison of DCPO-h and DCPO-p across different perturbation levels. The perturbation method has strong impact on captions that are more closely correlated with images. ImageReward () CLIPscore () GenEval () Pair Caption Perturbed Level Pickscore () HPSv2.1 () Method DCPO-p DCPO-h DCPO-h DCPO-p DCPO-h DCPO-h DCPO-p DCPO-h DCPO-h (c, cp) (zw, zw ) (zw, zl p) (c, cp) (zw, zw ) (zw, zl p) (c, cp) (zw, zw ) (zw, zl p) weak weak weak medium medium medium strong strong strong 20.28 20.55 20.58 20.21 20.59 20.57 20.31 20.57 20.58 25.42 25.61 25.70 25.34 25.73 25.62 25.06 25.27 25. 54.20 57.70 58.10 53.10 58.47 58.20 54.60 57.43 57.90 26.98 27.07 27.15 26.87 27.12 27.13 27.03 27.18 27. 0.4906 0.5070 0.5060 0.4852 0.5008 0.5100 0.4868 0.5110 0.4993 other approaches. Importantly, DCPO-p shows significant improvement over the other methods on HPSv2.1, highlighting the effectiveness of the perturbation method. 3.3 ABLATION STUDIES AND ANALYSIS Support of Hypothesis 1. As described in Section 2.2.2, we defined three levels of perturbation: weak, medium, and strong. In Hypothesis 1, we proposed that increasing the distance between the distributions of preferred and less preferred images µ improves model alignment performance. To explore this, we fine-tuned SD 2.1 using the DCPO-h method with three levels of perturbation applied to the less preferred captions zl generated by LLaVA. The results in Figure 5 show that increasing the distance µ between the two distributions enhances performance. However, this distance must be controlled and kept below threshold t, hyperparameter that may vary depending on the task. These findings support our hypothesis. Support of Hypothesis 2. To illustrate the impact of the correlation between the prompt and image on the perturbation method, we perturbed both the original prompt and the less preferred caption zw, generated by the model Qϕ, where zw Wϕ(zwQϕ(zwxw, c)). At the same time, we kept the caption generated by Qϕ for the preferred image as the preferred caption, zw Q(zwxw, c). In this case, we assume Qϕ = LLaVA and Wϕ = DIPPER. The results in Table 6 in Appendix show that the caption generated by LLaVA is more correlated with the image than the original prompt c, indicating that S(z, x) > S(c, x). Based on the results in Table 3, we conclude that perturbing more correlated captions leads to better performance. Invs. Out-of Distribution. Inspired by (Verma et al., 2024), which examined the impact of out-of-distribution data in vision language models, we assessed DCPO using both in-distribution and out-of-distribution (OOD) data. As discussed in Section 3.1, the captioning model can generate OOD captions. To explore this, we fine-tuned SD 2.1 with DCPO-h using LLaVA and Emu2 captions at medium perturbation level. Figure 6 shows that in-distribution data improve alignment 8 Under review. Figure 6: Comparison of DCPO-h performance on in-distribution and out-of-distribution data. Table 4: Performance comparison of DCPO and Diffusion-DPO fine-tuned on the Pick-Double Caption dataset. While larger captions improve the performance of Diffusion-DPO, DCPO-h still significantly outperforms Diffusion-DPO. Token Length (Avg) Input Prompt ImageReward () CLIPscore () GenEval () Pickscore () HPSv2.1 () Method Diffusion-DPO Diffusion-DPO Diffusion-DPO DCPO-h (LLaVA) DCPO-h (LLaVA) prompt caption zw (LLaVA) caption zw (Emu2) Pair (zw,zl p) Pair (zw,zw ) 15.95 32.32 7.75 (32.32, 31.17) (32.32, 27.01) 20.36 20.40 20.36 20.57 20.57 25.10 25.19 25.08 25.62 25. 56.4 56.6 56.3 58.2 57.4 26.98 27.10 26.98 27.13 27.18 0.4857 0.4958 0.4960 0.5100 0. performance, while OOD results for LLaVA in GenEval, Pickscore, and CLIPscore are comparable to Diffusion-DPO. Similar behavior was observed for DCPO-c, as noted in Appendix F. Effectiveness of the DCPO. Our analysis shows that LLaVA captions are twice the length of the original prompt c, raising the question of whether DCPOs improvement is due to data quality or the optimization method. To explore this, we fine-tuned SD 2.1 with Diffusion-DPO using LLaVA and Emu2 captions instead of the original prompt. The results in Table 4 show that models finetuned on LLaVA captions outperform Diffusion-DPO with the original prompt. However, DCPOh still surpasses the new Diffusion-DPO models, demonstrating the effectiveness of the proposed optimization algorithm. In DCPO, β is key hyExplore on β. perparameter. To evaluate its impact, we fine-tuned SD 2.1 using different values of β = {500, 1000, 1500, 2500, 5000}. Interestingly, in Figure 7 we observed that β = 500 showed significant improvements on HPSv2.1 and GenEval, even surpassing DCPO-h with β = 5000, our best-reported model. Additional results for different β values can be found in Appendix F. DCPO-h vs Diffusion-DPO on GPT-4o Judgment. We evaluated DCPO-h and DiffusionDPO using GPT-4o on the PartiPrompts benchmark, consisting of 1,632 prompts. GPT-4o assessed images based on three criteria: Q1) General Preference (Which image do you prefer given the prompt?), Q2) Visual Appeal (Which image is more visually appealing?), and Q3) Prompt Alignment (Which image better fits the text description?). As shown in Figure 8, DCPO-h outperformed Diffusion-DPO in Q1 and Q2, with win rates of 58% and 66%. To see the style of the prompts, refer to Appendix G. Figure 7: DCPO-h performance comparison across various β values, evaluated on HPSv2.1 and GenEval."
        },
        {
            "title": "4 RELATED WORKS",
            "content": "Aligning Diffusion Models. Recent advances in preference alignment methods of text-to-image diffusion models have shown that RL-free methods (Wallace et al., 2024; Yang et al., 2024; Li et al., 9 Under review. Figure 8: (Left) PartiPrompts benchmark results for three evaluation questions, as voted by GPT-4o. (Right) Qualitative comparison between DCPO-h and Diffusion-DPO fine-tuned on SD 2.1. DCPOh shows better prompt adherence and realism, with outputs that align more closely with human preferences, emphasizing high contrast, vivid colors, fine detail, and well-focused composition. 2024; Yuan et al., 2024; Gambashidze et al., 2024; Park et al., 2024) outperforms RL-based approaches (Fan & Lee, 2023; Fan et al., 2023; Hao et al., 2023; Lee et al., 2023; Xu et al., 2023; Prabhudesai et al., 2024; Black et al., 2024; Clark et al., 2024) mainly because they eliminates the need for an explicit reward model (Saeidi et al., 2024a; Chatterjee et al., 2024a). Initially, Direct Preference Optimization (DPO) (Rafailov et al., 2024) and Triple Preference Optimization (Saeidi et al., 2024b) method reformulate the RLHF objective in closed-form manner and introduce it as an implicit reward model with simple classification objective. Diffusion-DPO (Wallace et al., 2024) directly adopts DPO method into text-to-image diffusion models, utilizing pairwise preference datasets consisting of text and images to guide alignment. Diffusion-KTO (Li et al., 2024) incorporates Kahneman & Tversky model of human utility to align these models, simplifying the process by using images with binary feedback signals, i.e., likes or dislikes instead of pairwise preference data. To enhance flexibility, Hong et al. (2024) introduce MaPO, an alignment technique independent of reference model previously used by other methods, enabling greater control over stylistic adaptations. However, previous methods optimize diffusion models based on single prompt for pair of images, which supports the irrelevant prompts issue explored in Section 2.1. Text-to-image Preference Datasets. Text-to-image image preference datasets commonly involve the text prompt to generate the images, and two or more images are ranked according to human preference. HPS (Wu et al., 2023c) and HPSv2 (Wu et al., 2023b) create multiple images using series of image generation models for single prompt, and the images are ranked according to realworld human preferences. Moreover, classifier is trained using the gathered preference dataset, which can be used as metric for image-aligning tasks. Also, Pick-a-Pic v2 (Kirstain et al., 2023) follows similar structure to create pairwise preference dataset along with their CLIP (Radford et al., 2021) based scoring function, Pickscore. While these datasets are carefully created, having only one prompt for both or all the images introduces conflict distribution, which will be further discussed in Section 2.1. For this reason, we modified the Pick-a-Pic v2 dataset using recaptioning and perturbation methods to improve image alignment performance."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "In this paper, we present novel preference optimization method for aligning text-to-image diffusion models called Dual Caption Preference Optimization (DCPO). We tackle two major challenges in previous preference datasets and optimization algorithms: the conflict distribution and irrelevant prompt. To overcome these issues, we introduce the Pick-Double Caption dataset, modified version of the Pick-a-Pic v2 dataset. We also identify difficulties in generating captions, particularly the risk of out-of-distribution captions for images, and propose three approaches: 1) captioning (DCPOc), 2) perturbation (DCPO-p), and 3) hybrid method (DCPO-h). Our results show that DCPO-h significantly enhances alignment performance, outperforming methods like MaPO and DiffusionDPO across multiple metrics. Limitations. Although DCPO shows strong performance across various metrics, the captioning and perturbation methods are resource-intensive. We encourage future research to explore costeffective alternatives to these methods. Additionally, the potential of using different backbones, such as Stable Diffusion XL (SDXL) (Rombach et al., 2022), has not been explored in the context of DCPO. We also invite researchers to investigate DCPOs effectiveness on other tasks, such as safety. We believe our work will have significant impact on the alignment research community. 10 Under review."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank the Research Computing (RC) at Arizona State University (ASU) and cr8dl.aifor their generous support in providing computing resources. The views and opinions of the authors expressed herein do not necessarily state or reflect those of the funding agencies and employers."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, and Aditya Grover. Comparing bad apples to good oranges: Aligning large language models via joint preference optimization. arXiv preprint arXiv:2404.00530, 2024. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=YCWjhGrJFD. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Joao Carvalho, An Le, Mark Baierl, Dorothea Koert, and Jan Peters. Motion planning diffusion: Learning and planning of robot motions with diffusion models. In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 19161923. IEEE, 2023. Agneet Chatterjee, Yiran Luo, Tejas Gokhale, Yezhou Yang, and Chitta Baral. Revision: Rendering In European Conference on Computer tools enable spatial fidelity in vision-language models. Vision, pp. 339357. Springer, 2024a. Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, Tejas Gokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, et al. Getting it right: Improving spatial consistency in text-to-image models. In European Conference on Computer Vision, pp. 204222. Springer, 2024b. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instructionfinetuned language models, 2022. URL https://arxiv.org/abs/2210.11416. Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable rewards. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=1vmSEVL19f. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Martin Nicolas Everaert, Marco Bocchio, Sami Arpa, Sabine Susstrunk, and Radhakrishna Achanta. Diffusion in style. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 22512261, October 2023. Ying Fan and Kangwook Lee. Optimizing DDPM sampling with shortcut fine-tuning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 96239639. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/fan23b.html. 11 Under review. learning for fine-tuning text-to-image diffusion models. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter DPOK: ReinAbbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. forcement In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7985879885. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2023/ 2023. file/fc65fab891d83433bd3c8d966edde311-Paper-Conference.pdf. Alexander Gambashidze, Anton Kulikov, Yuriy Sosnin, and Ilya Makarov. Aligning diffusion models with noise-conditioned perception, 2024. URL https://arxiv.org/abs/2406. 17636. Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/2310.11513. Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=BsZNWXD3a1. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. URL https://arxiv.org/ abs/2104.08718. Jiwoo Hong, Sayak Paul, Noah Lee, Kashif Rasul, James Thorne, and Jongheon Jeong. Marginaware preference optimization for aligning diffusion models without reference, 2024. URL https://arxiv.org/abs/2406.06424. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=G5RwHpBUv0. Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense. Advances in Neural Information Processing Systems, 36, 2024. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback, 2023. Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Yusuke Kato, and Kazuki Kozuka. Aligning diffusion models by optimizing human utility, 2024. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Runtao Liu, Ashkan Khakzar, Jindong Gu, Qifeng Chen, Philip Torr, and Fabio Pizzati. Latent guard: safety framework for text-to-image generation. arXiv preprint arXiv:2404.08031, 2024b. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Yong-Hyun Park, Sangdoo Yun, Jin-Hwa Kim, Junho Kim, Geonhui Jang, Yonghyun Jeong, Junghyo Jo, and Gayoung Lee. Direct unlearning optimization for robust and safe text-to-image models, 2024. URL https://arxiv.org/abs/2407.21035. Maitreya Patel, Song Wen, Dimitris Metaxas, and Yezhou Yang. Steering rectified flow models in the vector field for controlled image generation. arXiv preprint arXiv:2412.00100, 2024. 12 Under review. Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-toimage diffusion models with reward backpropagation, 2024. URL https://openreview. net/forum?id=Vaf4sIrRUC. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Sheldon Ross. Introduction to probability models. Academic press, 2014. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Amir Saeidi, Shivanshu Verma, and Chitta Baral. Insights into alignment: Evaluating dpo and its variants across multiple tasks. arXiv preprint arXiv:2404.14723, 2024a. Amir Saeidi, Shivanshu Verma, Aswin RRV, and Chitta Baral. Triple preference optimization: Achieving better alignment with less data in single step optimization. arXiv preprint arXiv:2405.16681, 2024b. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv:2010.02502, October 2020a. URL https://arxiv.org/abs/2010.02502. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in neural information processing systems, 34:1415 1428, 2021. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. Aayush Atul Verma, Amir Saeidi, Shamanthak Hegde, Ajay Therala, Fenil Denish Bardoliya, Nagaraju Machavarapu, Shri Ajay Kumar Ravindhiran, Srija Malyala, Agneet Chatterjee, Yezhou Yang, and Chitta Baral. Evaluating multimodal large language models across distribution shifts and augmentations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pp. 53145324, June 2024. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Under review. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 76237633, 2023a. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis, 2023b. URL https://arxiv.org/abs/2306.09341. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligning text-to-image models with human preference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 20962105, 2023c. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023. URL https://arxiv.org/abs/2304.05977. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Qimai Li, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model, 2024. Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation, 2024. 14 Under review."
        },
        {
            "title": "A BACKGROUND",
            "content": "A.1 DIFFUSION MODELS Based on samples from data distribution q(x0), noise scheduling function αt and σt (Rombach et al., 2022) denoising diffusion models (Song et al., 2020b) are generative models pθ(x0) that operate through discrete-time reverse process structured as Markov Decision Proces where pθ(xt1xt) = (xt1; µθ(xt), σ2 tt1 σ2 t1 σ2 I). (3) The training process involves minimizing the evidence lower bound (ELBO) associated with this model (Song et al., 2021): LDM = Ex0,ϵ,t,xt[ω(λt)ϵ ϵθ(xt, t)2 2] (4) where ϵ (0, I), U(o, ), xt q(xtx0) = (xt; αtx0, σ2 ratio (Kingma et al., 2021), ω(λt) is predefined weighting function (Song & Ermon, 2019). I).λt = α is signal-to-noise /σ2 A.2 PREFERENCE OPTIMIZATION Aligning generative model typically involves fine-tuning it to produce outputs that are more aligned with human preferences. Estimating the reward model based on human preference is generally challenging, as we do not have direct access to the reward model. However, if we assume the availability of ranked data generated under given condition c, where xw 0 representing the preferred sample and xl 0 the less-preferred sample), we can apply the Bradley-Terry theory to model these preferences. The Bradley-Terry (BT) model expresses human preferences as follows: 0c (with xw 0 xl pBT (xw 0 xl 0c) = σ(r(c, xw 0 ) r(c, xl 0)) (5) where σ denotes the sigmoid function, and r(x0, c) is derived from neural network parameterized by ϕ, which is estimated through maximum likelihood training for binary classification as follows: LBT (ϕ) = c,xw 0 ,xl 0[log σ(rϕ(c,xw 0 )rϕ(c,xl 0))] (6) where the prompt and data pairs xw 0 , xl 0 are sourced from dataset that humans have annotated. This approach to reward modeling has gained popularity in aligning large language models, particularly when combined with reinforcement learning (RL) techniques like proximal policy optimization (PPO) (Schulman et al., 2017) to fine-tune the model based on rewards learned from human preferences, known as Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022). The goal of RLHF is to optimize the conditional distribution p(x0c) (where Dc) such that the reward model r(c, x0) is maximized, while keeping the policy model within the desired distribution using KL-divergence term to ensure it remains reachable under the following objective: max pθ EcDc,x0pθ(x0c)[r(c, x0)] βDKL[pθ(x0c)pref(x0c)] (7) where β controls how far the policy model pθ can deviate from the reference model pref . It can be demonstrated that the objective in Equation 7 converges to the following policy model: θ(x0c) = pref(x0c) exp(r(c, x0)/β)/Z(c) (8) where is the partition function, the training objective for pθ, inspired by DPO, has been derived to be equivalent to Equation 8 without the need for an explicit reward model r(x, c). Instead, it learns directly from the preference data (c, xw 0 , xl 0) D. 15 Under review. LDPO(θ) = c,xw 0 ,xl 0 [log σ(β log pθ(xw pref(xw 0 c) 0 c) β log pθ(xl pref(xl 0c) 0c) )] (9) where σ represents the sigmoid function. Through this reparameterization, instead of first optimizing the reward function and then applying reinforcement learning, the method directly optimizes the conditional distribution pθ(x0c)."
        },
        {
            "title": "B FORMAL PROOFS REGARDING DCPO",
            "content": "B.1 OPTIMIZING THE DCPO LOSS IS OPTIMIZING THE DPO LOSS Inspired by Bansal et al. (2024), we can intuitively assume that the DCPO objective is to learn an aligned model pθ by weighting the joint probability of preferred images pθ(xw 0 , zw) over less pre0, zl). We set the optimization objective of DCPO is to minimize the following: ferred images pθ(xl LDCPO(θ) = (xw 0 ,xl 0,zl,zw)D log σ(βE 0 ,zw),xl 1:T pθ(xw xw pθ(xw pref(xw 1:T xw 0:T , zw) 0:T , zw) [log 0,zl) 1:T pθ(xl pθ(xl pref(xl 1:T ,xl 0:T , zl) 0:T zl) log (10) ]) Here, we highlight that reducing LDCPO(θ) is equivalently reducing LDPO(θ) when the captions are the same for the preferred and less preferred images. Lemma 1. Under the case where Ddefine = {xw for the given pair of preferred and less preferred images (xw LDCPO(θ; Ddefine; β; pref), in which DDPO = {c, xw 0 , c, xl 0 , xl 0}. 0, c}, that is, the image captions are identical 0), we have LDPO(θ; DDPO; β; pref) = 0 , xl Proof of Lemma 1. LDCPO(θ; D, β, pref) = (xw (cid:18) 0 ,xl 0,zw,zl)D (cid:18) (cid:20) log σ β log pθ(xw pref(xw 0 , zw) 0 , zw) β log (cid:19)(cid:19)(cid:21) pθ(xl pref(xl 0, zl) 0, zl) = (cid:20) (xw (cid:18) 0 ,xl 0,zw,zl)D (cid:18) log σ β log pθ(xw pref(xw 0 zw)pθ(zw) 0 zw)pref(zw) β log pθ(xl pref(xl 0zl)pθ(zl) 0zl)pref(zl) (cid:19)(cid:19)(cid:21) LDCPO(θ; Ddefine, β, pref) zw=zl=c= (cid:20) (cid:18) log σ (xw 0 ,c,xl (cid:18) pθ(c) pref(c) 0,c)Ddefine (cid:18) β log pθ(xw pref(xw 0 c) 0 c) β log pθ(xl pref(xl 0c) 0c) (cid:19)(cid:19)(cid:19)(cid:21) pθ (c) pref(c) =C = (cid:18) (cid:18) (cid:20) log σ β log (xw 0 ,xl 0,c)DDPO pθ(xw pref(xw 0 c) 0 c) β log (cid:19)(cid:19)(cid:21) pθ(xl pref(xl 0c) 0c) (11) (12) = LDPO(θ; DDPO, β, pref) In Equation 12, is constant value that equates to pθ(c) by substituting according to zw = zl = c. pref(c) . The proof above follows the Bayes rule B.2 ANALYSES OF DCPOS EFFECTIVENESS In this section, we present the formal proofs of why our DCPO leads to more optimized L(θ) of Diffusion-based model and, consequently, better performance in preference alignment tasks. 16 Under review. Increasing the difference between preferred and less-preferred improves the optimization of Proof 1. L(θ). For better clarity, the loss function L(θ) can be written as: L(θ) = (cid:2)log σ(cid:0) βT ω(λt) (cid:1)(cid:3) where σ(x) is the sigmoid function that squashes its input into the output range (0, 1), and = preferred less-preferred, i.e., the margin between the respective importance of the preferred and less preferred predictions. Characteristically, the gradient of σ(x) is at its maximum near = 0 and decreases as increases. larger margin in terms of makes it easier for the optimization to drive the sigmoid function towards its asymptotes, reducing loss. When is small (M 0): The sigmoid σ(βT ω(λt) ) is near 0.5 (its midpoint). Also, the gradient of log σ(x) is the largest near this point, meaning the model struggles to differentiate between preferred and less preferred predictions effectively. When is large (M 0): The sigmoid σ(βT ω(λt) ) moves closer to 0 or 1, depending on the sign of . For well-aligned model, if the preferred predictions are correct, > 0 and σ(βT ω(λt) ) approach 1, thus minimizing the loss. Intuitively, an ideally large represents clear distinction between the preferred image-caption versus the less preferred image-caption. Thus, by maximizing , we may push the loss L(θ) towards its minimum, leading to better soft-margin optimization. Proof 2. Replacing caption with the specifically generated caption zl for the less-preferred image xl 0 decreases less-preferred. To analyze how replacing with zl, where zl and zl Q(zlxl, c), for the less-preferred image xl 0 improves the optimization, we delve into how the loss function is affected by this substitution. The term relevant to the less-preferred image xl less-preferred = ϵl ϵθ(xl in the loss is: t, t, c)2 2 ϵl ϵref(xl t, t, c)2 2. Replacing with zl modifies the predicted noise term ϵθ(xl represents xl t, we have: t, t, c) to ϵθ(xl t, t, zl). Since zl better ϵl ϵθ(xl t, t, zl)2 2 < ϵl ϵθ(xl t, t, c)2 (13) When ϵlϵθ(xl 2 becomes smaller, the term less-preferred decreases. This leads to preferred less-preferred becoming larger, which improves the soft-margin optimization in the loss function L(θ) that we have shown in Proof 1. t, t, zl)2 We further elaborate on why Equation 13 is true. minimization, the optimal predictor of ϵl given some information is the conditional expectation: In the context of mean squared error (MSE) When conditioned on (xl t, t, c): θ(xl ϵ t, t, c) = (cid:2)ϵl xl t, t, c(cid:3) When conditioned on (xl t, t, zl): θ(xl ϵ t, t, zl) = (cid:2)ϵl xl t, t, zl(cid:3) The total variance of ϵl can be decomposed as by the Law of Total Variance (conditional variance formula) (Ross, 2014): Var (cid:0)ϵl(cid:1) = (cid:2)Var (cid:0)ϵl xl t, t, c(cid:1)(cid:3) + Var (cid:0)E (cid:2)ϵl xl t, t, c(cid:3)(cid:1) 17 Under review. Similarly, when conditioning on zl: Var (cid:0)ϵl(cid:1) = (cid:2)Var (cid:0)ϵl xl t, t, zl(cid:1)(cid:3) + Var (cid:0)E (cid:2)ϵl xl t, t, zl(cid:3)(cid:1) Since zl, the information provided by zl is richer than that of c. In probability theory, conditioning on more information does not increase the conditional variance: (14) This inequality holds because conditioning on additional information (zl) can only reduce or leave unchanged the uncertainty (variance) about ϵl. t, t, zl(cid:1) Var (cid:0)ϵl xl t, t, c(cid:1) Var (cid:0)ϵl xl The expected squared error when using the optimal predictor is equal to the conditional variance: (cid:104)(cid:13) (cid:13)ϵl ϵ θ(xl t, t, c)(cid:13) 2 (cid:13) 2 Similarly, From 14, we have: (cid:104)(cid:13) (cid:13)ϵl ϵ θ(xl t, t, zl)(cid:13) 2 (cid:13) 2 (cid:105) (cid:105) = (cid:2)Var (cid:0)ϵl xl t, t, c(cid:1)(cid:3) = (cid:2)Var (cid:0)ϵl xl t, t, zl(cid:1)(cid:3) Var (cid:0)ϵl xl t, t, zl(cid:1) Var (cid:0)ϵl xl t, t, c(cid:1) Taking expectations on both sides: (cid:2)Var (cid:0)ϵl xl t, t, zl(cid:1)(cid:3) (cid:2)Var (cid:0)ϵl xl t, t, c(cid:1)(cid:3) Therefore, (cid:104)(cid:13) (cid:13)ϵl ϵ θ(xl t, t, zl)(cid:13) 2 (cid:13) 2 (cid:105) (cid:104)(cid:13) (cid:13)ϵl ϵ θ(xl t, t, c)(cid:13) 2 (cid:13) 2 (cid:105) Assuming that the neural network ϵθ is capable of approximating the optimal predictor ϵ as training progresses and the model capacity is sufficient, we can write: θ, especially (cid:13) (cid:13)ϵl ϵθ(xl 2 (cid:13) t, t, zl)(cid:13) 2 (cid:13) (cid:13)ϵl ϵ θ(xl t, t, zl)(cid:13) 2 (cid:13)"
        },
        {
            "title": "Similarly for c",
            "content": "(cid:13) (cid:13)ϵl ϵθ(xl 2 (cid:13) t, t, c)(cid:13) 2 (cid:13) (cid:13)ϵl ϵ θ(xl t, t, c)(cid:13) 2 2 . (cid:13) Therefore, the expected squared error satisfies: (cid:104)(cid:13) (cid:13)ϵl ϵθ(xl t, t, zl)(cid:13) 2 (cid:13) 2 (cid:105) (cid:104)(cid:13) (cid:13)ϵl ϵθ(xl t, t, c)(cid:13) 2 (cid:13) 2 (cid:105) Since the term of less-preferred in the loss function involves the difference of squared errors, using zl instead of for the less preferred sample results in lower error term: less-preferred = (cid:13) (zl) (cid:13)ϵl ϵθ(xl 2 (cid:13) t, t, zl)(cid:13) 2 (cid:13) (cid:13)ϵl ϵref(xl t, t, zl)(cid:13) 2 (cid:13) 2 Comparing with the original: less-preferred = (cid:13) (c) (cid:13)ϵl ϵθ(xl t, t, c)(cid:13) 2 (cid:13) 2 (cid:13) (cid:13)ϵl ϵref(xl t, t, c)(cid:13) 2 (cid:13) 2 18 Under review. Assuming the reference model ϵref remains the same or also benefits similarly from the additional information in zl, the net effect is that the first term decreases more than the second term, leading to reduced less-preferred. Proof 3 Replacing caption with the specifically generated caption zw for the preferred image xw 0 increases preferred. To prove that replacing with zw Q(zwxw, c), where zw, for xw 0 also contributes to better optimized loss L(θ), we examine how this particular substitution affects the loss function. We let Rθ(c) = ϵw ϵθ(xw Rref(c) = ϵw ϵref(xw , t, c)2 2, , t, c)2 2. The rate of decrease in Rθ due to zw is proportional to the models ability to exploit the additional conditioning. Since ϵθ is learnable, it can more effectively leverage zw than ϵref, yielding: Rθ = Rθ(c) Rθ(zw) Rref = Rref(c) Rref(zw). We further elaborate on why the learnable models noise prediction residual (Rθ) decreases faster than the reference models residual (Rref) when is replaced by zw. The residuals for the learnable and reference models are defined as: Rθ(c) = ϵw ϵθ(xw Rref(c) = ϵw ϵref(xw , t, c)2 2, , t, c)2 2. When is replaced with zw (where zw), the residuals become: Rθ(zw) = ϵw ϵθ(xw Rref(zw) = ϵw ϵref(xw , t, zw)2 2, , t, zw)2 2. The rate of decrease for each residual is defined as: Rθ = Rθ(c) Rθ(zw), Rref = Rref(c) Rref(zw). The quality of conditioning, Q(c), represents how well the conditioning aligns with the true noise ϵw. We assume that Q(zw) > Q(c), where the improvement in conditioning quality is defined as = Q(zw) Q(c). The residual for Rθ is proportional to the misalignment between Q(c) and ϵw: Rθ(c) 1 Q(c) . Replacing with zw (higher Q) results in larger proportional reduction: Rθ(zw) 1 Q(zw) with Rθ Q. The reference models residual Rref depends weakly on Q(c), as it is fixed or less adaptable: Rref(c) 1 Qref(c) , 19 Under review. where Qref(c) is less sensitive to changes in c. Thus, the proportional improvement in Rθ due to is significantly larger than for Rref. The preferred difference term is: preferred = Rθ Rref. As Rθ decreases significantly more than Rref, the gap Rθ Rref becomes larger, increasing preferred: Rθ Rref = preferred increases. The learnable model ϵθ benefits more from the improved conditioning zw because of its adaptability and training dynamics. This results in larger reduction in Rθ compared to Rref. Mathematically, the relative rate of decrease: Relative Rate = 1, Rθ Rref which ensures that preferred also increases, hence improving the optimization process in L(θ) and helping the model distinguish predictions on preferred and less preferred image-captions more effectively."
        },
        {
            "title": "C MORE INSIGHTS ON DCPO",
            "content": "A preference alignment dataset, such as Pick-a-Pic (Kirstain et al., 2023), is defined as = {c, xw, xl}, where xw and xl represent the preferred and less preferred images for the prompt c. Diffusion-KTO (Li et al., 2024) hypothesizes the optimization of diffusion model using only single preference label based on whether an image is suitable or not for given prompt c. Diffusion-KTO uses differently formatted input dataset = {c, x}, where is generated image corresponding to the prompt c. Method GenEval () Pickscore () HPSv2.1 () ImageReward () CLIPscore () Diffusion-KTO DCPO-h 0.5008 0.5100 20.41 20.57 24.80 25.62 55.5 58.2 26.95 27.13 Table 5: Comparison of DCPO-h and Diffusion-KTO across various benchmarks. Diffusion-KTOs hypothesis is fundamentally different from our DCPOs. While Diffusion-KTO focuses on binary preferences (like/dislike) for individual image-prompt pairs, our approach involves paired preferences. We observe that using the same prompt for both preferred and less preferred images may not be ideal. To address this, we propose optimizing diffusion model using dataset in terms of = {zw, zl, xw, xl}, where zw and zl are the captions generated by static captioning model Qϕ for the preferred and less preferred images, respectively, referring to the original prompt. We nonetheless conduct comparisons between Diffusion-KTO and DCPO on various preference alignment benchmarks. The results in Table 5 show that our DCPO-h consistently outperforms Diffusion-KTO on all benchmarks, demonstrating the effectiveness of our DCPO method. PICK-DOUBLE CAPTION DATASET In this section, we provide details about the Pick-Double Caption dataset. As discussed in Section 3.1, we sampled 20,000 instances from the Pick-a-Pic v2 dataset and excluded those with equal preference scores. We plot the distribution of the original prompts, as shown in Figure 9. We observed that some prompts contained only one or two words, while others were excessively long. To ensure fair comparison, we removed prompts that were too short or too long, leaving us with approximately 17,000 instances. We then generated captions using two state-of-the-art models, LLaVA-1.6-34B, and Emu2-32B. Figure 10 provides examples from the dataset. As explained in Section 3.1, we utilized two types of prompts to generate captions: 1) Conditional prompt and 2) Non-conditional prompt. Below, we outline the specific prompts used for each captioning method. 20 Under review. Figure 9: Token distribution of original prompt."
        },
        {
            "title": "Example of Conditional Prompt",
            "content": "Using one sentence, describe the image based on the following prompt: playing chess tournament on the moon. Example of Non-Conditional Prompt Using one sentence, describe the image. Table 6 presents statistical analysis of the Pick-Double Caption dataset. With the non-conditional prompt method, we found that the average token length of captions generated by LLaVA is similar to that of the original prompts. However, captions generated by LLaVA using conditional prompts are twice as long as the original prompts. Additionally, Emu2 generated captions that, on average, are half the length of the original prompts for both methods. Table 6: Statistical information on the Pick-Double Caption dataset, including the CLIPscore of in-distribution data and average token count of captions generated by LLaVA and Emu2 for both in-distribution and out-of-distribution data. Text Token Len. (Avg-in) Token Len. (Avg-out) CLIP score (in) CLIP score (out) prompt caption zw (LLaVA) caption zl (LLaVA) caption zw (Emu2) caption zl (Emu2) 15. 32.32 32.83 7.75 7.84 15.95 17.69 17.91 8.40 8.44 (26.74, 25.41) (26.74, 25.41) 30.85 26.48 25.44 22. 29.04 28.29 25.18 24.88 Figure 10: Examples of Pick-Double Caption dataset. 21 Under review. Table 7: Examples of perturbed prompts and captions after applying different levels of perturbation. Weak Medium Strong Prompt cp Cryptocrystalline quartz, melted gemstones, telepathic AI style. Painting of cryptocrystalline quartz. Melted gems. Sacred geometry. Cryptocrystalline quartz with melted stones, in telepathic AI style. Caption zw (LLaVA) Caption zl (LLaVA) Caption zw (Emu2) Caption zl (Emu2) digital artwork featuring symmetrical, kaleidoscopic pattern with vibrant colors and central star-like motif. digital artwork featuring symmetrical, kaleidoscopic pattern with contrasting colors and central star-like motif. kaleidoscope with symmetrical and colourful patterns and central starlike motif. vivid circular stained-glass art with symmetrical star design in its center. The image is of radially symmetrical stained-glass window. colorful, round stained-glass design with symmetrical star in the center. Abstract image with glass. An abstract image of colorful stained glass. An abstract picture with glass in many colors. An abstract circular design with leaves. colourful round design with leaves. Brightly colored circular design. Original Prompt c: Painting of cryptocrystalline quartz melted gemstones sacred geometry pattern telepathic AI style"
        },
        {
            "title": "E MORE DETAILS ON PERTURBATION METHOD",
            "content": "We provide the setups for the LLM-based perturbation process involved in the DCPO-p and DCPOh pipelines. Similarly to the method of constructing paraphrasing adversarial attacks as synonymswapping perturbation by Krishna et al. (2024), we use DIPPER (Krishna et al., 2024), text generation model built by fine-tuning T5-XXL (Chung et al., 2022), to create semantically perturbed captions or prompts, as shown in Table 7. Our three levels of perturbation are achieved by only altering the setting of lexicon diversity (0 to 100) in DIPPER - we use 40 for Weak, 60 for Medium, and 80 for Strong. We also use Text perturbation for variable text-to-image prompt. to prompt the perturbation. We hereby provide code snippet to showcase the whole process to perturb sample input: 1 from transformers import T5Tokenizer, T5ForConditionalGeneration 2 class DipperParaphraser(object): 3 # As defined in https://huggingface.co/kalpeshk2011/dipperparaphraser-xxl 4 5 prompt = \"Text perturbation for variable text-to-image prompt.\" 6 input_text = \"playing chess tournament on the moon.\" 7 8 dp = DipperParaphraser() 9 10 cap_weak = dp.paraphrase(input_text, lex_diversity=40, prefix=prompt, do_sample=True, top_p=0.75, top_k=None, max_length=256) 11 cap_medium = dp.paraphrase(input_text, lex_diversity=60, prefix=prompt, do_sample=True, top_p=0.75, top_k=None, max_length=256) 12 cap_strong = dp.paraphrase(input_text, lex_diversity=80, prefix=prompt, do_sample=True, top_p=0.75, top_k=None, max_length=256)"
        },
        {
            "title": "F MORE DETAILS ABOUT TRAINING OF DIFFUSION MODELS",
            "content": "In this section, we provide detailed explanation of the fine-tuning methods used. We fine-tuned SD 2.1 with the best hyperparameters reported in the original papers for SFTChosen, Diffusion-DPO, and MaPO, using 8 A10080 GB GPUs for all models. To fine-tune SD 2.1 with Diffusion and MaPO methods, we used dataset = {c, xw, xl} where c, xw, xl represent the prompt, preferred image, and less preferred image. To optimize SD2.1 with SFTChosen we utilized dataset = {c, xw} where c, xw represent the prompt, preferred image and image. In this paper, dataset represents the sampled and cleaned version of the Pick-a-Pic v2 dataset. Additionally, we clarify the DCPO In this paper, DCPO-c and DCPO-p refer to SD 2.1 models DCPO-c, DCPO-p, and DCPO-h. models fine-tuned with the DCPO method, using LLaVA and Emu2 for captioning and perturbation methods at three distinct levels, respectively. The main results for DCPO-p in the text are based on weak perturbation applied to the original prompt. In Table 3, we also report DCPO-ps performance across other perturbation levels. 22 Under review. For DCPO-h, we applied perturbations to both the preferred and less preferred captions generated by LLaVA. The reported results for DCPO-h reflect medium level of perturbation applied to the less preferred caption. In Table 3, we present the performance of DCPO-h across various perturbation levels, including perturbations to the preferred captions. Additionally, in Table 8, we show the results for DCPO-h using captions generated by Emu2. Table 8: Results of the perturbation method applied to Emu2 captions across different levels. Method Pair Caption Perturbed Level Pickscore () HPSv2.1 () ImageReward () CLIPscore () GenEval () DCPO-h DCPO-h DCPO-h DCPO-h DCPO-h DCPO-h (zw, zw ) (zw, zl p) (zw, zw ) (zw, zl p) (zw, zw ) (zw, zl p) weak weak medium medium strong strong 20.10 20.32 20.31 20.33 20.31 20. 21.23 23.4 23.08 23.22 22.95 23.24 49.7 53.8 53.2 53.8 53.1 53. 26.87 27.06 27.01 27.09 27.11 27.08 0.5003 0.5070 0.4895 0.5009 0.4878 0. The key findings indicate that perturbation on short captions not only fails to improve performance but also produces worse outcomes compared to DCPO-c (Emu2). Additionally, we conducted more experiments on in-distribution and out-of-distribution data. For this, we generated out-of-distribution data using LLaVA and Emu2 in the captioning setup. As shown in Figure 11, in-distribution data generally outperformed out-of-distribution data. However, the most significant improvement was observed with the hybrid method, as reported in Figure 6. Figure 11: Comparison of DCPO-c performance on in-distribution and out-of-distribution data. Table 9 presents the performance details for different values of β, conducted using the medium level of DCPO-h. The results indicate that while lower values of β significantly improve GenEval and HPSv2.1 on average, the optimal value for β is 5000. We suggest that this hyperparameter may vary based on the dataset and task. Table 9: Results of DCPO-h across different β. Method β Pickscore () HPSv2.1 () ImageReward () CLIPscore () GenEval () DCPO-h DCPO-h DCPO-h DCPO-h 500 1000 5000 20.43 20.51 20.53 20.57 26.42 26.12 25.81 25.62 58.1 58.2 58. 58.2 27.02 27.10 27.02 27.13 0.5208 0.4900 0.5036 0.5100 GPT-4O AS AN EVALUATOR To obtain binary preferences from the API evaluator, we followed the approach outlined in the MaPO paper (Hong et al., 2024). Similar to Diffusion-DPO, we used three distinct questions to evaluate the images generated by the DCPO-h and Diffusion-DPO models, both utilizing SD 2.1 as the backbone. These questions were presented to the GPT-4o model to identify the preferred image. Below, we provide details of the prompts used. 23 Under review. GPT-4o Evaluation Prompt for Q1: General Preference Select the output (a) or (b) that best matches the given prompt. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). ## Prompt: {prompt} ## Output (a): The first image attached. ## Output (b): The second image attached. ## Which image do you prefer given the prompt? GPT-4o Evaluation Prompt for Q2: Visual Appeal Select the output (a) or (b) that best matches the given prompt. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). ## Prompt: {prompt} ## Output (a): The first image attached. ## Output (b): The second image attached. ## Which image is more visually appealing? GPT-4o Evaluation Prompt for Q3: Prompt Alignment Select the output (a) or (b) that best matches the given prompt. Choose your preferred output, which can be subjective. Your answer should ONLY contain: Output (a) or Output (b). ## Prompt: {prompt} ## Output (a): The first image attached. ## Output (b): The second image attached. ## Which image better fits the text description? 24 Under review."
        },
        {
            "title": "H ADDITIONAL GENERATION SAMPLES",
            "content": "We also present additional samples for qualitative comparison generated by SD 2.1, SFTChosen, Diffusion-DPO, MaPO, and DCPO-h from prompts on Pickscore, HPSv2, and GenEval benchmarks. Figure 12: Additional generated outcomes using prompts from HPSv2 benchmark. 25 Under review. Figure 13: Additional generated outcomes using prompts from Pickscore benchmark. Figure 14: Additional generated outcomes using prompts from GenEval benchmark."
        }
    ],
    "affiliations": [
        "Arizona State University"
    ]
}