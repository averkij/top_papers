{
    "paper_title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
    "authors": [
        "Bingchen Zhao",
        "Despoina Magka",
        "Minqi Jiang",
        "Xian Li",
        "Roberta Raileanu",
        "Tatiana Shavrina",
        "Jean-Christophe Gagnon-Audet",
        "Kelvin Niu",
        "Shagun Sodhani",
        "Michael Shvartsman",
        "Andrei Lupu",
        "Alisia Lupidi",
        "Edan Toledo",
        "Karen Hambardzumyan",
        "Martin Josifoski",
        "Thomas Foster",
        "Lucia Cipolina-Kun",
        "Abhishek Charnalia",
        "Derek Dunfield",
        "Alexander H. Miller",
        "Oisin Mac Aodha",
        "Jakob Foerster",
        "Yoram Bachrach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. A critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research community contributions on the NanoGPT speedrun, a competition to train a GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides a simple, non-saturated measure of an LLMs ability to automate scientific reproduction, a necessary (but not sufficient) skill for an autonomous research agent."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 1 4 2 2 . 6 0 5 2 : r The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements Bingchen Zhao,1,2, Despoina Magka,1, Minqi Jiang,1 Xian Li1, Roberta Raileanu1, Tatiana Shavrina1, Jean-Christophe Gagnon-Audet1, Kelvin Niu1, Shagun Sodhani1, Michael Shvartsman1, Andrei Lupu1, Alisia Lupidi1, Edan Toledo1, Karen Hambardzumyan1, Martin Josifoski1, Thomas Foster1, Lucia Cipolina-Kun1, Abhishek Charnalia1, Derek Dunfield1, Alexander H. Miller1, Oisin Mac Aodha2, Jakob Foerster1, Yoram Bachrach 1Meta, 2University of Edinburgh Equal contribution Rapid advancements in large language models (LLMs) have the potential to assist in scientific progress. critical capability toward this endeavor is the ability to reproduce existing work. To evaluate the ability of AI agents to reproduce results in an active research area, we introduce the Automated LLM Speedrunning Benchmark, leveraging the research communitys contributions on the NanoGPT speedrun, competition to train GPT-2 model in the shortest time. Each of the 19 speedrun tasks provides the agent with the previous records training script, optionally paired with one of three hint formats, ranging from pseudocode to paper-like descriptions of the new records improvements. Records execute quickly by design and speedrun improvements encompass diverse code-level changes, ranging from high-level algorithmic advancements to hardware-aware optimizations. These features make the benchmark both accessible and realistic for the frontier problem of improving LLM training. We find that recent reasoning LLMs combined with SoTA scaffolds struggle to reimplement already-known innovations in our benchmark, even when given detailed hints. Our benchmark thus provides simple, non-saturated measure of an LLMs ability to automate scientific reproduction, necessary (but not sufficient) skill for an autonomous research agent. Date: June 30, 2025 Code: https://github.com/facebookresearch/llm-speedrunner"
        },
        {
            "title": "1 Introduction",
            "content": "The advent of LLMs capable of succeeding in challenging math, coding, and scientific reasoning domains has led to surge of activity in applying LLM agents to the longstanding ambition of automated scientific discovery (Simon, 1995; Langley, 1987; Waltz and Buchanan, 2009; King et al., 2009; Steinruecken et al., 2019). Early results suggest LLM-based systems can improve the productivity of human researchers, from formulating hypotheses to implementing code-based experiments to testing them (Romera-Paredes et al., 2024; Castro et al., 2025; Yin, 2025; Inizan et al., 2025). Scientific progress hinges on trustworthy results, and the ultimate test of the truth behind finding is whether the experiment and its outcomes can be reproduced (Fineberg et al., 2019; Pineau et al., 2021; Henderson et al., 2018). Thus, critical component of automated science is automated reproducibility: the process of automatically reimplementing an experiment based on description of the experiment design, such that the implementation reproduces previously reported outcomes. In other words, translating the description of an experiment into its implementation (Peng, 2011; Siegel et al., 2024). Moreover, success in reimplementing Figure 1 Recent LLM agents struggle to reproduce NanoGPT Speedrun records. 1 Figure 2 The Automated LLM Speedrunning Benchmark. We create task for each consecutive pair of records Ri, to Ri. Ri+1. The performance of the agent is evaluated by comparing the relative speedup of the agent solution known study also serves as metric for assessing the reliability with which an agent can implement experiments via description, an ability that would enable researchers to quickly scale up the testing of new ideas, regardless of whether they are of human or AI origin. We study the ability of recent reasoning LLMs in combination with state-of-the-art scaffoldsprograms that iteratively make use of an LLM for finding solution to given taskon reproducing prior discoveries in the domain of LLM training. We henceforth refer to the combination of specific LLM and scaffold for the purpose of automated research as research agent, and use the more specific term AI research agent to refer to those specifically designed for automating AI research itself. While there is much speculation that AI research agents may lead to the beginnings of recursive self-improvement loop for future LLM-based research agents, we set our focus on the more modest goal of understanding whether current AI research agents can succeed at the prerequisite task of reproducing previous scientific findings on GPT-2 (Radford et al., 2019), the first model to demonstrate broad capacity for zero-shot transfer to new tasks via prompting. Towards this goal, we introduce The Automated LLM Speedrunning Benchmark, based on the series of community-driven improvements to GPT-2 training in the NanoGPT Speedrun (Jordan et al., 2024a), competition based on minimizing the wall time of training an open-source PyTorch reimplementation of GPT-2 (Karpathy, 2023) to reach target cross-entropy loss of 3.28 on the validation set of FineWeb (Penedo et al., 2024), using single 8H100 node. Since its inception in June 2024, this community effort has driven the training time of GPT-2 from 45 minutes to below 3 minutes (as of May 2025). These improvements were driven by new algorithmic enhancements, some of which have been shown to generalize beyond the scale of the 124M parameter GPT-2 model, with the most notable being the invention of the Muon optimizer (Jordan et al., 2024b), later demonstrated to show benefits for training much larger modern LLMs (Liu et al., 2025a; Shah et al., 2025). Other speedrun improvements include mixed precision training and more efficient attention variants (Dong et al., 2024). As of May 2025, the NanoGPT Speedrun includes 21 successive speedrun records. Each record is associated with its corresponding training script (train_gpt.py), measured training time, public announcement of the changes, and high-level summary of the code changes.1 The Automated LLM Speedrunning Benchmark then tasks an AI research agent with reproducing each successive speedrun record, starting from the previous record, with an optional set of hints of various formats and levels of detail. The clear code-level ground-truth targets per record alongside detailed change logs between records make this benchmark an ideal testing ground for the ability of agents to reproduce not only single experimental finding, but also series of cumulative research findingsa distinct affordance compared to prior reproducibility benchmarks. Here, all tasks share the same success metric of training time to reach the target validation loss, measured on fixed hardware configuration (a single 8xH100 node), making exact reproduction, fair comparisons, and cross-task comparisons straightforward. Lastly, perhaps the most compelling aspect of this benchmark is its focus on reproducing discoveries directly relevant to real-world LLM development. 1https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history 2 Table 1 Key motivations of our benchmark design and how it differentiates from existing ML reproducibility benchmarks. Here, Reproducibility denotes whether the tasks require replicating given technique; Sequential, whether the benchmark measures reproducibility over cumulative series of scientific results; LLM research, whether the task involves language model development; and Agent scaffold, whether baseline agent scaffold is released with the benchmark."
        },
        {
            "title": "LLM research Agent scaffold",
            "content": "MLE-bench (Chan et al., 2025) PaperBench (Starace et al., 2025) CORE-bench (Siegel et al., 2024) RE-bench (Wijk et al., 2024) MLAgentBench (Huang et al., 2024) MLGym-bench (Nathani et al., 2025) Automated LLM Speedrunning (ours)"
        },
        {
            "title": "Yes",
            "content": "Our experiments show that even when given description of the difference between two consecutive speedrun records in various formats, recent agents based on DeepSeek-R1 (DeepSeek-AI et al., 2025) and o3-mini (OpenAI, 2025) combined with state-of-the-art search scaffold, still struggle to improve ground-truth records to match the speedups of the next ground-truth record (see Figure 1). We believe the Automated LLM Speedrunning Benchmark can spur development of AI research agents that can automate reproducibility studies, paving critical step on the way towards more capable AI research agents that can realize the aspiration of accelerating the pace of scientific discovery via automated science. However, our results show that before such lofty goals can be realized, automated reproducibility remains central challenge that must be addressed."
        },
        {
            "title": "2 Related works",
            "content": "Automated reproducibility. Recent works have devised benchmarks for evaluating the ability of LLM agents to reproduce code-based experiments from published papers. CORE-Bench measures an agents ability to correctly install, execute, and interpret papers associated codebase and its outputs (Siegel et al., 2024). Other benchmarks, including PaperBench (Starace et al., 2025), Papers2Code (Seo et al., 2025), AutoP2C (Lin et al., 2025), and SciReplicate (Xiang et al., 2025) test the agents ability to convert research paper to codebase that replicates the reported findings or the agents ability to formulate and test hypotheses (Chen et al., 2025; Liu et al., 2025b). Instead of evaluating on wide set of, often, unrelated papers as in these previous works, the Automated LLM Speedrunning Benchmark focuses on single important overarching task of speeding up LLM training. This focus allows for unified success metric across diverse gradation of task complexity, defined by the natural path of innovation previously discovered by human researchers. This grounding allows for not only comparison to granular, ground-truth code-level changes, but also opens the door to evaluating an LLM agents ability to reproduce an entire research arc over multiple compounding innovations against human performance. Moreover, the benchmarks multiple hint levels allow for controlled study of how performance varies across different forms of background information. Code generation with LLMs. Code is inherently reproducible via repeated execution and requires no additional equipment to run beyond computer. Thus, many automated scientific reproducibility benchmarks, including ours, focus primarily on virtual, code-based experiments. In this domain, research agents directly benefit from and build upon the rapid progress in coding and computer-use agents, such as growing set of complex, sandboxed software-engineering agent benchmarks (Yang et al., 2024; Wang et al., 2024; Fourney et al., 2024; Mialon et al., 2023; Yoran et al., 2024; Zhou et al., 2023; Koh et al., 2024) and scaffold designs (Zhang et al., 2024), such as AIDE (Jiang et al., 2025), which we both use as baseline and extend in our experiments. LLMs for automated ML. Recent advances enabling LLMs to exploit chain-of-thought outputs during inference have led to drastic improvements in their performance on reasoning tasks in domains like math, coding, and science. These improvements have led to surge in LLM programs seeking to automate the key parts of machine learning itself, encompassing iterated hypothesis generation and testing and the writing of reports 3 detailing the findings, in the form of end-to-end agents (Lu et al., 2024; Huang et al., 2025; Yamada et al., 2025a), agents focused on hypothesis generation (Gottweis et al., 2025; ONeill et al., 2025), as well as agents that can interact with human-in-the-loop to jointly formulate and test hypotheses (Intology AI, 2025; Autoscience Institute, 2025). However, early results suggest these systems, while capable of optimizing code-level improvements, often fall short in executing on experiments that faithfully reflect their intended goals (Yamada et al., 2025b). Thus, while LLM-based reasoning models can generate, at times, novel hypotheses (Gu et al., 2024), their ability for scientific reproduction remains crucial bottleneck in automating scientific research."
        },
        {
            "title": "3 The Automated LLM Speedrunning Benchmark",
            "content": "The Automated LLM Speedrunning Benchmark seeks to evaluate an LLM agents ability to reproduce the wall-time speedup associated with each record transition from the NanoGPT Speedrun, both with and without access to hints describing the corresponding changes at varying levels of abstraction. Table 1 summarizes how our work compares to existing ML reproducibility benchmarks."
        },
        {
            "title": "3.1 Reproducibility tasks from existing records",
            "content": "For each transition from record Ri1 to record Ri for = 2, ..., 21, excluding = 7, whose speedup is purely due to upgrading PyTorch, we define the following components: Ri Training script for the i-th record in the speedrun, ti Wall-clock time (in seconds) required by Ri to reach the target validation loss, Level 1 hint: pseudocode description of code change from the previous record, Level 2 hint: natural-language description of the code change from the previous record, Level 3 hint: mini-paper summarizing the code change from the previous record. 1 2 3 All hints were first drafted by R1, manually verified, and, where necessary, edited for correctness and relevance. See Appendix for further details on our hint creation process. We provide categorized listing of all ground-truth records in Appendix and example hints in Appendix F. For convenience, we denote the set of ground-truth speedrun records (which excludes record 6) as I. We define record task as tuple Ri1, Ri, ti, m, where R1 corresponds to the initial NanoGPT training script, and where is any subset of the set of hint levels, {0, 1, 2, 3}, where level 0 corresponds to no hint. Depending on the presence of hints, we categorize the possible tasks in our benchmark into two types: Record reproduction tasks. Given hints that describe the subsequent record, i.e. = {0}, the LLM agent must reproduce record Ri+1 given Ri and the set of corresponding hints. Here the key metric of interest is the fraction of speedup recovered (FSR), defined as FSRi = ti i+1 ti ti+1 . (1) where performance is then the mean FSR over the set of all included records, I: i+1 is the training time achieved by the agent to reach the target validation loss. The full benchmark FSR = 1 (cid:88) ti i+1 ti ti+1 . (2) i+1 with minimal training time Record optimization tasks. Without any hints, i.e. = {0}, the LLM agent must produce new training script solution i+1 to reach the target validation loss, given Ri. Here we consider both the raw wall time i+1 of the solution produced, in addition to FSRi. Similar to the setting of record reproduction, we consider the mean of these metrics over all ground-truth records in the benchmark as an overall measure of performance. This allows the agent to explore its own improvements given the same SoTA starting point that humans had when each record was produced. 4 Figure 3 Overview of our flexible search scaffold. Search starts from root node containing code for the starting record Ri from which N0 initial solutions are generated. Subsequently, each search iteration debugs buggy leaf node with probability pdebug and otherwise greedily selects the best node to improve, with debug and improvement each branching solutions. At each search step, the coder submodule implements the solution, with optional access to external knowledge (e.g. hints)."
        },
        {
            "title": "3.2 Agent scaffolds",
            "content": "We provide flexible search scaffold implementation that extends AIDE (Jiang et al., 2025) into more general parameterization. In this setup, visualized in Figure 3, each node in the search tree represents solution instance contained in directory with relevant scripts, performance metrics, and an LLM-generated execution summary. For instance, in NanoGPT training, solution node consists of single train_gpt2.py script and results file describing its performance and execution outcome. The fitness of each node is evaluated based on these metricssuch as wall time to reach the target validation losswith each new search initialized using ground-truth script from the benchmark and proceeding by branching into up-to-multiple child solutions. Each search step follows three stages: implementation, execution, and analysis. During implementation, the agent generates working code from prompt that includes the task description and optionally, set of associated hints. We use Aider (Gauthier, 2025) to make diff-based edits to the initial solution, producing modified versions for execution. These solutions are then run on an 8xH100 node, and the output is summarized in natural language via the analysis stage, capturing key performance indicators and insights from standard outputs. Custom prompts guide each stage and are detailed in Appendix D. The search begins with N0 initial modifications to the root node. At each step, new node branches from either randomly chosen buggy node (with probability pdebug) or the highest-performing node. To avoid redundant debugging, we cap retries at Dmax per node. This scaffold design supports multiple search variants, outlined in Table 2, with each receiving the same budget of search steps to ensure fair comparison. Table 2 Search variants and their corresponding scaffold parameterizations. Method Tree Forest AIDE Multi-AIDE Flat (Best-of-M)"
        },
        {
            "title": "Initial branch factor Branch factor Debug probability Max debug depth",
            "content": "0 0 pdebug pdebug 0 0 Dmax Dmax"
        },
        {
            "title": "1\nN0\nN0\nN0\nM",
            "content": "N 1 5 Figure 4 Mean FSR across five search variants and four frontier models for six hint regimes: no hint (0), pseudocode (1), text (2), mini-paper (3) and combinations thereof (1 + 2, 1 + 2 + 3). Figure 5 Interquartile Mean (IQM) evaluation results. Scores are aggregated across multiple runs with the same hint level, scaffold, and model."
        },
        {
            "title": "4 Experiments and results",
            "content": "We now evaluate the performance of several baseline agents across range of scaffolds, hint formats, and model backbones for all NanoGPT Speedrun records. We report results using the normalized runtime improvement metric (FSR) from Equation 2, as well as measures of code similarity between agent and human solutions. For fair comparisons, we use training times for human records based on rerunning each ground-truth record on the same hardware configuration as agent solutions. Appendix reports the near exact reproduction of training times for human records on our cluster."
        },
        {
            "title": "4.1 Baselines",
            "content": "We compare number of LLM agents based on DeepSeek-R1, o3-mini, Gemini-2.5-Pro, and Claude-3.7-Sonnet, using instances of the search scaffolds listed in Table 2. Our choice of parameters are N0 = 3 for the initial pool of root hypotheses (forest, AIDE and multi-AIDE), = 3 for the branching factor (tree, forest and multi-AIDE), pdebug = 0.5 and Dmax = 5 for the debug probability and maximum debug depth respectively (AIDE and multi-AIDE), and search budget of = 20 nodes. Taken together, these scaffolds cover range of branching factors, search depth, and debug logic. For each pair of model and search scaffold, we assess the mean FSR across all 19 tasks for each of the following hint levels: no hint (level 0), pseudocode (level 1), text description (level 2), and mini-paper (level 3). Each solution is executed under maximum runtime of 60 minutes (i.e. maximum of 20 hours per agent run). We observe an average run time of 10 hours per agent run, across total of 6,840 agent runs (19 records 6 hint regimes 5 search variants 4 models 3 seeds), for total of 6,840 8 H100 (internal cluster) hours spent executing the generated solutions."
        },
        {
            "title": "4.2 Reproducing individual records",
            "content": "We report the mean FSR for each model, search scaffold, and hint-level combination across 3 full search runs in Figure 4, including the case of no hints. It is evident that hints are necessary for inducing greater values of FSR, with all agents failing to recover more than 20% of the speed-up achieved by human solutions on average without hints. Appendix further reports the mean FSR for each individual record transitions per agent variation across 3 runs per variation. 6 Figure 7 Correlation of FSR with L2 distance recovered for each hint level, showing modest correlation between similarity to the human solution and FSR for most hint types and models. We observe that o3-mini generally achieves equal or better results than other models in mean FSR for all hint levels, but sees slightly worse performance with no hints. Notably, flat search (i.e. best-of-M), generally matches or outperforms iterated search scaffolds across the individual hint levels (levels 13), while matching their performance in the case of no hints. Moreover, tree and forest methods, which lack debug steps, perform on par with AIDE-based search scaffolds, suggesting that explicit debug steps do not provide significant benefit on top of iterative improvement steps. Overall, the gap between the best models (o3-mini and Claude-3.7Sonnet) and the open-weights (R1) is wider for the search scaffolds incorporating branching logic (tree, search, and AIDE variants), suggesting that models like o3-mini can better iterate on their previous solutions. Figure 6 further shows how agents tend to have more difficulty in reproducing later records. Figure 6 FSR and embedding distance per record for o3-mini with text description hints (mean and std over 3 seeds). Later records tend to be harder for agents, leading to lower recovered embedding distance and speedups. Out of the various hint formats, the most useful are the pseudocode and the combinations of pseudocode with text and mini-papers hints, which enable o3-mini to recover approximately 40% and 46%, respectively, of the speed-up attained by human solutions on average. Surprisingly, R1 agents seem to worsen with the presence of the individual hints, generally achieving lower FSR compared to the no-hint setting, suggesting that attempting to implement the complex changes in these hints results in buggy code. With hints, R1 produces solutions with lower FSR than simply making no changes to the code, common outcome with no hints, as indicated by the cluster around recovered L2 embedding distance of 0.0 in Figure 7 (Section 4.6 details this similarity analysis). 7 Table 3 Performance comparison across different hint formats (mean and std over 3 runs). Color-coded values are differences relative to the best-performing individual hint in the combination. Hints L1 (pseudocode) L2 (text) L3 (mini-paper) L1+L2 L1+L2+L3 L1 (pseudocode) L2 (text) L3 (mini-paper) L1+L2 L1+L2+L3 L1 (pseudocode) L2 (text) L3 (mini-paper) L1+L2 L1+L2+L3 Model o3-mini o3-mini o3-mini o3-mini o3-mini DeepSeek-R1 DeepSeek-R1 DeepSeek-R1 DeepSeek-R1 DeepSeek-R1 Gemini-2.5-Pro Gemini-2.5-Pro Gemini-2.5-Pro Gemini-2.5-Pro Gemini-2.5-Pro Flat 0.400.02 0.220.04 0.170. Tree 0.430.02 0.160.03 0.130.03 Forest 0.400.02 0.260.04 0.150.04 AIDE 0.410.02 0.180.02 0.120. Multi-AIDE 0.430.02 0.170.03 0.250.04 0.270.03 (-0.13) 0.240.05 (-0.16) 0.380.02 (-0.05) 0.350.05 (-0.08) 0.310.04 (-0.09) 0.390.03 (-0.01) 0.340.03 (-0.07) 0.360.04 (-0.05) 0.370.03 (-0.06) 0.460.04 (+0.03) 0.130.03 0.100.01 0.130.04 0.200.00 0.070.00 0.100.03 0.070.00 0.060.00 0.090.03 0.090.02 0.060.01 0.140.02 0.160.01 0.070.00 0.200. 0.250.01 (+0.12) 0.300.04 (+0.17) 0.200.03 (+0.00) 0.240.02 (+0.04) 0.250.03 (+0.18) 0.400.04 (+0.31) 0.280.03 (+0.19) 0.360.03 (+0.22) 0.240.02 (+0.08) 0.410.02 (+0.21) 0.180.02 0.180.01 0.180. 0.160.02 0.180.03 0.180.02 0.230.04 0.190.02 0.240.02 0.130.02 0.090.01 0.150.02 0.230.03 0.160.03 0.160.03 0.180.02 (+0.00) 0.190.04 (+0.01) 0.120.03 (-0.06) 0.140.04 (-0.04) 0.240.04 (+0.01) 0.250.04 (+0.01) 0.200.04 (+0.07) 0.170.03 (+0.02) 0.190.04 (-0.04) 0.260.05 (+0.03) L1 (pseudocode) L2 (text) L3 (mini-paper) Claude-3.7-Sonnet Claude-3.7-Sonnet Claude-3.7-Sonnet 0.140.03 0.100.03 0.060. 0.130.03 0.030.01 0.220.02 0.050.01 0.060.02 0.110.01 0.140.01 0.140.02 0.340.01 0.180.04 0.140.02 0.190.03 L1+L2 L1+L2+L3 Claude-3.7-Sonnet Claude-3.7-Sonnet 0.140.03 (+0.00) 0.210.04 (+0.07) 0.110.02 (-0.11) 0.310.02 (+0.09) 0.150.02 (+0.04) 0.100.02 (-0.01) 0.300.02 (-0.04) 0.310.01 (-0.03) 0.090.01 (-0.09) 0.200.02 (+0.01)"
        },
        {
            "title": "4.3 Combining multiple hints",
            "content": "We further investigate the impact of combining hint formats, and also include these results for each agent variation in Figure 4. We observe that providing the text description or mini-paper together with the pseudocode compared to only providing the pseudocode hint can substantially degrade performance for o3-mini (see o3-mini result in Table 3), but surprisingly benefits R1. These results suggest that o3-mini may be less capable of taking advantage of longer contexts, while R1s reasoning directly benefits from longer initial prompts. On the other hand, the effect of combined hints on Gemini-2.5-Pro and Claude-3.7-Sonnet appears relatively small, suggesting they can handle longer context yet lacks the ability to leverage them for effective reasoning for reproducing code changes."
        },
        {
            "title": "4.4 Interquartile mean evaluation",
            "content": "As the agent runs could bring large variance in the experimental results, in Figure 5 we present the aggregated Interquartile Mean (IQM) results across runs with the same hint level, search scaffold, and model. The IQM metric has been shown to be robust to comparisons with small sample size, and in Figure 5 we report as 95% confidence intervals, bootstrapped from 3 seeds following Agarwal et al. (2021). On the hint level comparison, the agents reach the best performance when using all three hints combined. For individual hints, the pseudo-code hint performs the best. For search scaffold, multi-AIDE search outperforms all others. On the model side, we are surprised to find that the Gemini-2.5-Pro and Claude-3.7-Sonnet gives the lowest performance close to 0 FSR, even lagging behind the open-sourced R1 model. The results also suggest Automated LLM Speedrunning is challenging benchmark for current agents as the aggregated performances are fairly low."
        },
        {
            "title": "4.5 Analysis of search trees",
            "content": "To better understand how each agent spends its search budget, we inspect the proportion of different kinds of nodes in their search trees: buggy nodes, which crash due to runtime errors; improved nodes, which successfully improved runtime compared to their parent; and unimproved nodes, which do not improve from their parent. This breakdown of the search trees is presented in Figure 8. We observe that flat search leads to higher total proportion of buggy nodes, indicating that initially-proposed solutions are most often incorrect. We also notice that R1 agents generate more buggy nodes under AIDE and multi-AIDEthe two variants with debugging stepssuggesting that R1 may be less capable of fixing its own mistakes compared to o3-mini. Gemini-2.5-Pro tends to generate fewer buggy nodes compared to the other models, yet it lags behind on the FSR metric (see Figure 4 and Figure 1), suggesting that Gemini produces more robust code at the cost of 8 Figure 8 Fraction of node types across search trees for each model and search method. Notably, branching (i.e. non-flat) search is beneficial for reducing the proportion of buggy nodes. Further, majority of non-buggy steps produce improved nodes for all branching search methods. correctly implementing the more efficient solutions described in the hints. Surprisingly, Claude-3.7-Sonnet generates significantly more buggy nodes than the other three models, with the fraction of buggy nodes gradually overtaking the fraction of working nodes in the search tree, indicating that Claude-3.7-Sonnet struggles to improve and debug its previous solutions. The analysis of node types in the search tree provides insight into the discrepancy on the results of Claude3.7-Sonnet between the SR results in Figure 4 and the IQM results in Figure 5. While the SR results suggest that, on average, Claude-3.7-Sonnet performs comparably to o3-mini, the IQM plot indicates that Claude-3.7-Sonnet significantly lags behind o3-mini. This discrepancy can be explained by examining the distribution of node types in the search tree. Claude-3.7-Sonnet is capable of generating working solutions that substantially improve the SR. However, it also produces considerable number of buggy nodes that result in runtime errors. These errors negatively impact the overall performance as reflected in the IQM plot, despite the improvements in the averaged SR."
        },
        {
            "title": "4.6 Similarity between agent and human solutions",
            "content": "Agents may output solutions with similar performance to human ones, but may still fail to reproduce the target code changes. We thus assess code similarity between agent and human solutions by comparing code embedding distances using the SFR-Embedding-Code 2B model (Liu et al., 2024). Specifically, we normalize the embedding distance between the agents code solution and the target human solution, i.e. the next record, and divide this distance by the embedding distance between the current and the 9 next human record. Figure 7 depicts the normalized L2 embedding distance recovered with respect to the record speedups and for each type of hint. Here the distance recovered is defined as 1ei+1 i+1/ei+1 ei, where ei is the embedding for Ri, and is the embedding for the LLMs attempt at reproducing it. We observe stronger correlation between higher similarity score and FSR for richer hint formats, suggesting that distances under this embedding space can be meaningful measure of degree of successful reproduction. As an alternative measure of code similarity, we made use of R1 as judge, prompting it to assess what fraction of the ground-truth code changes between the current and next record were successfully reproduced in the agents solution, on scale of 0 to 1 with score of 1 corresponding to completely correct reimplementation. Appendix contains comparison between these judge-based similarity scores and FSR across all agent variations. We observe clear positive correlation between higher similarity scores and FSR. We provide sample outputs from R1 judge in Appendix D."
        },
        {
            "title": "4.7 Experimenting with additional knowledge",
            "content": "12 worsens when FlexAttention Table 4 FSR of docs are inserted in the models context. Certain records are particularly challenging for our baseline agents, such as R12, which achieves its speedup via FlexAttention (Dong et al., 2024), PyTorch module that enables performant implementation of custom attention variants and was released in August 2024, potentially after the knowledge cut-off of R1 and o3-mini. To determine whether the agents poor performance on R12 was due to missing in-weights knowledge of this module, we inserted content from the blog post describing FlexAttention (including usage examples) as an additional hint to the agent (across all hint levels and agent variations). Table 4 shows this additional hint actually negatively impacts performance on R12, suggesting that recent models may still struggle to correctly exploit external knowledge that was not present in their training corpus in more complex tasks. with docs without docs 0.070.01 0.090.01 0.060.01 0.100. DeepSeek-R1 o3-mini R"
        },
        {
            "title": "4.8 Cumulative speedrun experiment",
            "content": "In this section, we test the models to see if they can reproduce the record described in the hint by building on the codebase they generated when reproducing previous records. Specifically, each task is formulated as tuple of i1, Ri, ti, where the agent will be given the codebase it generated for the previous task i1 and the hint level for reproducing the next record Ri, where the performance is measured by the FSR metric. This is challenging yet realistic extension of the reproducibility task where the agent seeks to cumulatively improve from the initial codebase. We evaluate the best-performing model (o3-mini) with the best search scaffold (multiAIDE) from our previous evaluations, with access to all hint levels (L1 + L2 + L3). Results averaged across three seeds are presented in Figure 9. The agent recovers approximately 60% of the ground-truth speedup for 3 recovering only around 20% of the speed-up, compared to the 60% of speed-up recovered when starting from the ground-truth R2 (see Figure 6). By only the third record, the agents solution 4 fails to reproduce any speedup compared to R4. 2 starting from R1. Yet its performance drops significantly afterwards, with Figure 9 Cumulative Speedup from initial codebase."
        },
        {
            "title": "5 Limitations and future directions",
            "content": "Our Automated LLM Speedrunning Benchmark serves as challenging evaluation of an LLM agents ability to reproduce scientific findings specific to LLM training. However, there remain important limits in its capacity for assessing an agents true capability for scientific reproduction, and each of these limitations point the way to directions for exciting future research. 10 Scaling up external knowledge. By design, the various hint levels are succinct and easily fit within the context of the LLMs we tested. Moreover, these hints were manually defined, with the relevant hint directly provided as part of the associated task instance. more realistic setup would provide the agent with the ability to use external knowledge via some form of function calling, including the ability to store intermediate results in various kinds of memory structures, e.g. short-term scratchpad, long-term database, or neural module (Hermann et al., 2015; Weston et al., 2014). Accessing wider and potentially accumulating set of external information would also test the agents ability to manage information whose total size may exceed its context length (Sarthi et al., 2024). Memorization or generalization? As many of the ground-truth records in the NanoGPT Speedrun were published potentially before the cut-off date of the models used in our experiments (and thus, most likely of future models), there is the possibility that models may have already seen these solutions during training (Gupta and Pruthi, 2025). We find that neither R1 nor o3-mini accurately reproduce the speedups realized in the ground-truth records, but explicitly disentangling memorization from generalization may become more necessary as models begin to saturate the benchmark. More advanced techniques for measuring memorization in LLMs would allow for more nuanced evaluation (Carlini et al., 2021; Razeghi et al., 2022; Oren et al., 2023; Deng et al., 2024). Semantic diffs. Our experiment analysis focuses on FSR and numeric similarity scores between the LLMs solution and the corresponding human solution. Moving beyond similarity score toward more expressive natural-language summaries, e.g. via automatically-generated commit messages (Jiang et al., 2017), of the code diffs between LLM and human solutions would allow for more scalable identification of common mistakes or new innovations with respect to the human solutions. From LLM speedrun to ML speedrun. The skills needed for strong performance on the LLM speedrun are necessary but not sufficient for reliable research agent. To devise agents that generalize to the future series of advances in the field of machine learning as whole, we require more complex tasks for both training and evaluation. Such tasks may span entire multi-file codebases and entail optimizing for other properties of models beyond training time, such as held-out task performance or memory footprint; may involve distributed training considerations beyond single node; and may require the agent to define its own intermediate success metrics. Most importantly, our benchmark primarily tests for the ability to reproduce results rather than the ability to innovate. Should LLM solutions on our benchmark begin to outpace human speedrun records, we may surely view it as step towards automated scientific discovery. Ultimately, the real test will be in whether future agents begin to solve open frontier challenges."
        },
        {
            "title": "6 Conclusions",
            "content": "We introduced the Automated LLM Speedrunning Benchmark, challenging evaluation of an LLM agents ability to reproduce existing scientific innovations in LLM training, based on reproducing each successive record in the community-driven NanoGPT Speedrun. Unlike previous benchmarks for automated scientific reproducibility, our benchmark enables evaluations of an agents ability to reproduce not just single result, but each incremental advance across chain of research innovations. We found that even recent, leading reasoning models, like R1 and o3-mini, when combined with state-of-the-art agent scaffold, still struggle to successfully produce speedrun solutions that match the speedups attained by the corresponding human solutions. Moreover, this gap between human and agent performance persists even when these strong baseline agents are provided with detailed explanations describing the exact code changes from the previous speedrun record. Our results suggest that automated reproducibility may serve as significant obstacle in realizing reliable, autonomous research agents with current, leading models, and we expand on the potential societal impacts of our work in Appendix G. We believe the Automated LLM Speedrunning Benchmark can be an effective testbed for monitoring this crucial capability in future research agents."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:2930429320, 11 2021."
        },
        {
            "title": "Autoscience",
            "content": "Institute."
        },
        {
            "title": "Carl",
            "content": "technical report, 2025."
        },
        {
            "title": "URL",
            "content": "meet-carl-the-first-ai-system-to-produce-academically-peer-reviewed-research. https://www.autoscience.ai/blog/ Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. In 30th USENIX security symposium (USENIX Security 21), pages 26332650, 2021. Pablo Samuel Castro, Nenad Tomasev, Ankit Anand, Navodita Sharma, Rishika Mohanta, Aparna Dev, Kuba Perlin, Siddhant Jain, Kyle Levin, Noémi Éltető, et al. Discovering symbolic cognitive models from human and animal behavior. bioRxiv, pages 202502, 2025. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander Mądry. Mle-bench: Evaluating machine learning agents on machine learning engineering, 2025. URL https://arxiv.org/abs/2410.07095. Tingting Chen, Srinivas Anumasa, Beibei Lin, Vedant Shah, Anirudh Goyal, and Dianbo Liu. Auto-bench: An automated benchmark for scientific discovery in llms. arXiv preprint arXiv:2502.15224, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models, 2024. URL https://arxiv.org/abs/2311.09783. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels, 2024. URL https://arxiv.org/abs/2412.05496. Harvey Fineberg, National Academies of Sciences, and Medicine. Reproducibility and replicability in science. National Academies Press, 2019. Adam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Erkang, Zhu, Friederike Niedtner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, Peter Chang, Ricky Loynd, Robert West, Victor Dibia, Ahmed Awadallah, Ece Kamar, Rafah Hosn, and Saleema Amershi. Magentic-one: generalist multi-agent system for solving complex tasks, 2024. URL https://arxiv.org/abs/2411.04468. Paul Gauthier. Aider: Ai pair programming in your terminal. https://github.com/Aider-AI/aider, 2025. URL https://github.com/Aider-AI/aider. Version 0.82.0. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025. 12 Tianyang Gu, Jingjin Wang, Zhihao Zhang, and HaoHong Li. Llms can realize combinatorial creativity: generating creative ideas via llms for scientific research. arXiv preprint arXiv:2412.14141, 2024. Tarun Gupta and Danish Pruthi. All that glitters is not novel: Plagiarism in ai generated research, 2025. URL https://arxiv.org/abs/2502.16487. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, 2018. Karl Moritz Hermann, Laurent Orseau, Shaodi Wang, Soham Tunyasuvut, Hubert Stanczyk, and Charles Blundell. Teaching machines to read and comprehend. Advances in neural information processing systems, 28, 2015. Kexin Huang, Ying Jin, Ryan Li, Michael Li, Emmanuel Candès, and Jure Leskovec. Automated hypothesis validation with agentic sequential falsifications. arXiv preprint arXiv:2502.09858, 2025. Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation, April 2024. URL https://arxiv.org/abs/2310.03302. Theo Jaffrelot Inizan, Sherry Yang, Aaron Kaplan, Yen-hsu Lin, Jian Yin, Saber Mirzaei, Mona Abdelgaid, Ali Alawadhi, KwangHwan Cho, Zhiling Zheng, et al. System of agentic ai for the discovery of metal-organic frameworks. arXiv preprint arXiv:2504.14110, 2025. Intology AI. Zochi technical report, 2025. URL https://github.com/IntologyAI/Zochi/blob/main/Zochi_Technical_ Report.pdf. Siyuan Jiang, Ameer Armaly, and Collin McMillan. Automatically generating commit messages from diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE), pages 135146. IEEE, 2017. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138, 2025. Keller Jordan, Jeremy Bernstein, Brendan Rappazzo, @fernbear.bsky.social, Boza Vlado, You Jiacheng, Franz Cesista, Braden Koszarsky, and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024a. URL https://github.com/KellerJordan/modded-nanogpt. Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024b. URL https://kellerjordan.github.io/posts/muon/. Andrej Karpathy. nanogpt, 2023. URL https://github.com/karpathy/nanoGPT. Ross King, Jem Rowland, Stephen Oliver, Michael Young, Wayne Aubrey, Emma Byrne, Maria Liakata, Magdalena Markham, Pinar Pir, Larisa Soldatova, et al. The automation of science. Science, 324(5923):8589, 2009. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Pat Langley. Scientific discovery: Computational explorations of the creative processes. MIT press, 1987. Zijie Lin, Yiqing Shen, Qilin Cai, He Sun, Jinrui Zhou, and Mingjun Xiao. Autop2c: An llm-based agent framework for code repository generation from multimodal content in academic papers, 2025. URL https://arxiv.org/abs/ 2504.20115. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025a. Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Codexembed: generalist embedding model family for multiligual and multi-task code retrieval, 2024. URL https://arxiv.org/abs/ 2411.12644. Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, and Dongzhan Zhou. Researchbench: Benchmarking llms in scientific discovery via inspiration-based task decomposition. arXiv preprint arXiv:2503.21248, 2025b. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery, 2024. URL https://arxiv.org/abs/2408.06292. 13 Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations, 2023. Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Foerster, Yoram Bachrach, William Yang Wang, and Roberta Raileanu. Mlgym: new framework and benchmark for advancing ai research agents, 2025. URL https://arxiv.org/abs/2502.14499. Charles ONeill, Tirthankar Ghosal, Roberta Răileanu, Mike Walmsley, Thang Bui, Kevin Schawinski, and Ioana Ciucă. Sparks of science: Hypothesis generation using structured paper data. arXiv preprint arXiv:2504.12976, 2025. OpenAI. Openai o3-mini: Pushing the frontier of cost-effective reasoning, January 2025. URL https://openai.com/ index/openai-o3-mini. Accessed: 2025-05-14. Yonatan Oren, Nicole Meister, Niladri Chatterji, Faisal Ladhak, and Tatsunori B. Hashimoto. Proving test set contamination in black box language models, 2023. URL https://arxiv.org/abs/2310.17623. Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/forum?id=n6SCkn2QaG. Roger Peng. Reproducible research in computational science. Science, 334(6060):12261227, 2011. Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivière, Alina Beygelzimer, Florence dAlché Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine learning research (a report from the neurips 2019 reproducibility program). Journal of machine learning research, 22(164):120, 2021. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Yasaman Razeghi, Robert Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. In The Twelfth International Conference on Learning Representations, 2024. Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning, 2025. URL https://arxiv.org/abs/2504.17192. Ishaan Shah, Anthony Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh Shah, et al. Practical efficiency of muon for pretraining. arXiv preprint arXiv:2505.02222, 2025. Zachary Siegel, Sayash Kapoor, Nitya Nagdir, Benedikt Stroebl, and Arvind Narayanan. Core-bench: Fostering the credibility of published research through computational reproducibility agent benchmark. arXiv preprint arXiv:2409.11363, 2024. Herbert Simon. Machine discovery. Foundations of Science, 1:171200, 1995. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. Paperbench: Evaluating ais ability to replicate ai research, 2025. URL https://arxiv.org/abs/2504.01848. Christian Steinruecken, Emma Smith, David Janz, James Lloyd, and Zoubin Ghahramani. The automatic statistician. Automated machine learning: Methods, systems, challenges, pages 161173, 2019. David Waltz and Bruce Buchanan. Automating science. Science, 324(5923):4344, 2009. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for ai software developers as generalist agents, 2024. URL https://arxiv.org/abs/2407.16741. 14 Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014. Hjalmar Wijk, Tao Lin, Joel Becker, Sami Jawhar, Neev Parikh, Thomas Broadley, Lawrence Chan, Michael Chen, Josh Clymer, Jai Dhyani, et al. Re-bench: Evaluating frontier ai r&d capabilities of language model agents against human experts. arXiv preprint arXiv:2411.15114, 2024. Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, and Yulan He. Scireplicate-bench: Benchmarking llms in agent-driven algorithmic reproduction from research papers. arXiv preprint arXiv:2504.00255, 2025. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025a. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search, 2025b. URL https://arxiv.org/abs/2504.08066. John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering, May 2024. URL https://arxiv. org/abs/2405.15793. Weiguo Yin. Exact solution of the frustrated potts model with next-nearest-neighbor interactions in one dimension: An ai-aided discovery. arXiv preprint arXiv:2503.23758, 2025. Ori Yoran, Samuel Joseph Amouyal, Chaitanya Malaviya, Ben Bogin, Ofir Press, and Jonathan Berant. Assistantbench: Can web agents solve realistic and time-consuming tasks? arXiv preprint arXiv:2407.15711, 2024. Yuntong Zhang, Haifeng Ruan, Zhiyu Fan, and Abhik Roychoudhury. Autocoderover: Autonomous program improvement. In Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis, pages 15921604, 2024. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. 15 Reproducing ground-truth speedruns on our hardware Figure A.1 compares the training times reported2 with the training time of running the same code on our AWS cluster, where we report the mean and standard deviation of three runs. We can see that the two curves track closely each other and, as expected, there is no training time decrease for the R6 R7 transition which corresponds to the PyTorch upgrade (we are using the upgraded version for R1 through R6 as we were not aware which one was the previous PyTorch version). Figure A.1 Running the human speedrun records."
        },
        {
            "title": "B Additional results for reproducing individual records",
            "content": "Figures B.1, B.2, B.5, B.6 depict mean FSR of DeepSeek-R1 and o3-mini agents when aggregating by search scaffold and hint level. The metrics are reported as 95% confidence intervals bootstrapped from 3 seeds, with IQM being the interquartile mean and the optimality gap being the difference from the best possible performance. We used the rliable3 library for the evaluation of our runs across multiple search scaffolds and hint levels. 2https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history 3https://github.com/google-research/rliable 16 Figure B.1 Aggregate performance of DeepSeek-R1 agents by hint level, reported as 95% confidence intervals, bootstrapped from 3 seeds. We observe that DeepSeek-R1 agents perform better when instructed with combination of pseudocode, text and paper-like hints. Figure B.2 Aggregate performance of DeepSeek-R1 agents by search scaffold, reported as 95% confidence intervals, bootstrapped from 3 seeds. The agent maximizes speedup recovery when using the multi-AIDE scaffold Figure B.3 Aggregate performance of Gemini-2.5-Pro agents by hint level, reported as 95% confidence intervals, bootstrapped from 3 seeds. We observe that Gemini-2.5-Pro agents perform better when instructed with combination of pseudocode, text and paper-like hints. 17 Figure B.4 Aggregate performance of Gemini-2.5-Pro agents by search scaffold, reported as 95% confidence intervals, bootstrapped from 3 seeds. The agent maximizes speedup recovery when using the flat scaffold Figure B.5 Aggregate performance of o3-mini agents by hint level, reported as 95% confidence intervals, bootstrapped from 3 seeds. For o3-mini agents the pseudocode hints yield better results Figure B.6 Aggregate performance of o3-mini agents by search scaffold, reported as 95% confidence intervals, bootstrapped from 3 seeds. The agent demonstrates its best performance with the multi-AIDE scaffold. 18 Figures B.7, B.8, B.9, B.10, B.11 show FSR results for individual records for the flat, tree, forest, AIDE and multi-AIDE scaffolds, respectively. The agent encounters more difficulty in recovering speedups at later records, which is expected as minimising training time requires more complex changes later on. Figure B.7 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the flat search scaffold. 19 Figure B.8 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the tree search scaffold. 20 Figure B.9 FSR results (mean and std over 3 runs) for each record, hint format, and model when using forest search scaffold. Figure B.10 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the AIDE search scaffold. 22 Figure B.11 FSR results (mean and std over 3 runs) for each record, hint format, and model when using the multi-AIDE search scaffold."
        },
        {
            "title": "C Additional results for code similarity judge",
            "content": "Figure C.1 shows the LLM judge scores for each record and search method separately. Some records (e.g. Record 10) have low reproducibility score across all methods and different types of hints, indicating that they are inherently challenging for an AI Research agent. Figure C.1 LLM-as-judge evaluation of reproducibility. The y-axis (Reproducibility Score) measures the fraction of human expert changes which are reproduced by agent-generated code, where 1 means all human experts changes are reproduced."
        },
        {
            "title": "Judge Prompt",
            "content": "Below is baseline implementation of GPT-2 model, followed by two proposed changes (see code diffs below) to improve the training speed. The first change is from an expert human. The second change is from an AI Assistant, aiming to reproduce the improvement made by the expert human. Inspect the code diffs carefully and provide an objective evaluation of the AI Assistants solution in terms of its similarity with expert humans solution. To derive an objective evaluation, first enumerate all the key changes made by expert human which can affect training speed, and then analyze all the changes made by the AI Assistant one by one. Based on understanding of these code changes, derive percentage score (between 0 and 1) to quantify what fraction of the key changes (which has impact on training speed) made by the expert were correctly implemented in the AI Assistants solution. Return your final score in JSON object, with the key \"reproducibility_score\". # =============== Baseline Implementation =========== {human_code} # =============== Change made by Expert Human =========== {next_human_code} # =============== Change made by AI Assistant =========== {agent_code} 24 Figure C.2 How FSR (per record) correlates with LLM judge scores for o3-mini-based agents, where higher judge score means the agent solution is closer to the corresponding human speedrun record."
        },
        {
            "title": "D Prompts and formatting templates",
            "content": "In this section we present the prompts we use for the coder component (Aider) of our agent scaffold (Figures D.4, D.5), for the analyzer used by the scaffold to summarize code execution results, i.e. standard streams, (Figures D.6, D.7) and for drafting initial hints with R1 (Figures D.8, D.9, D.10)"
        },
        {
            "title": "Summary format",
            "content": "Hypothesis: {hypothesis} Results: {metrics} Has bugs? {has_bugs} Outcome summary: {outcome_summary} Figure D.1 Template for rendering results.json, which summarizes each nodes execution and evaluation results. History format (Example with single templated version history) <version_log> <info> Version: {version} Parent version: {parent_version} Hypothesis: {hypothesis} Results: {metrics} Has bugs? {has_bugs} Outcome summary: {outcome_summary} </info> ... </version_log> Figure D.2 Template rendering relevant search history in the coder prompts for debugging and improving nodes. Knowledge component (Example with two templated entries) <knowledge> <li> {knowldge_entry} </li> ... </knowledge> Figure D.3 Template for the knowledge component of the coder, where each knowledge_entry variable can be an arbitrary piece of text from an external source."
        },
        {
            "title": "Coder Prompt",
            "content": "You are machine learning scientist, with expertise in large language models and high-performance computing. Use your expertise to assist the user in their machine learning task. Study the current version of {fnames}: {code} Your goal is to implement the following ideas to improve the code so that it better achieves the task: # Task description Improve train_gpt2.py so that it achieves or goes below the target val_loss value of 3.28 in the shortest train_time possible. Make sure your code changes preserve these aspects of train_gpt2.py: - The script continues to be runnable via simply calling torchrun --nproc_per_node=8 train_gpt2.py. - Do NOT change the value of train_files, val_files, or val_token values in the Hyperparameters config used to set the training args. - Make sure the values of these hyperparameters are not changed, and keep to using the current os.environ variables. - Always keep save_checkpoint set to False in the training args. - Keep all print0 statements the same. Do not change the arguments used in the current print0 statements, so to ensure the logging format is preserved. - When possible, just change the train_gpt2.py file without making extra files. - Important: care about optimizing the performance of the implementation and do not care how organized or disorganized the code is. - Any bugs will be described in the \"outcome_summary\" value of the summary, if provided. Always focus on addressing these when present, before improving other parts of the code. If you violate any of the above constraints, the experiment run will be invalid. Your job will be run on single 8xH100 node with access to all 8 GPUs. You have access to the following knowledge, consider these when writing code: {knowledge} **Never** install or ask to install any additional packages. Assume you have access to the following packages outside of the standard python packages: {packages} If necessary, you may access pretrained model checkpoints via HuggingFace for smaller models like BERT variants or CLIP. To help with your task, here is list summarizing recent erroneous changes to the above code that you have previously tried, along with summary of the outcome of each change. {history} trust you to make good decisions, so do not ask me for permission to make any code changes. Do not ever ask to install any additional packages. The answer will be no. In your final response, include ONLY the fully-functional updated code which implements ideas in the hypothesis above. Do NOT include any other content in your final response besides the code. Figure D.4 Full prompt for the coder (Aider), conditioning on external knowledge. Here, history and knowledge template strings are first composed via the templates in Figure D.2 and D.3."
        },
        {
            "title": "Coder Prompt With No Knowledge",
            "content": "You are machine learning scientist, with expertise in large language models and high-performance computing. Use your expertise to assist the user in their machine learning task. Study the current version of {fnames}: {code} Your goal is to implement the following ideas to improve the code so that it better achieves the task: # Task description Improve train_gpt2.py so that it achieves or goes below the target val_loss value of 3.28 in the shortest train_time possible. Make sure your code changes preserve these aspects of train_gpt2.py: - The script continues to be runnable via simply calling torchrun --nproc_per_node=8 train_gpt2.py. - Do NOT change the value of train_files, val_files, or val_token values in the Hyperparameters config used to set the training args. - Make sure the values of these hyperparameters are not changed, and keep to using the current os.environ variables. - Always keep save_checkpoint set to False in the training args. - Keep all print0 statements the same. Do not change the arguments used in the current print0 statements, so to ensure the logging format is preserved. - When possible, just change the train_gpt2.py file without making extra files. - Important: care about optimizing the performance of the implementation and do not care how organized or disorganized the code is. - Any bugs will be described in the \"outcome_summary\" value of the summary, if provided. Always focus on addressing these when present, before improving other parts of the code. If you violate any of the above constraints, the experiment run will be invalid. Your job will be run on single 8xH100 node with access to all 8 GPUs. **Never** install or ask to install any additional packages. Assume you have access to the following packages outside of the standard python packages: {packages} If necessary, you may access pretrained model checkpoints via HuggingFace for smaller models like BERT variants or CLIP. To help with your task, here is list summarizing recent erroneous changes to the above code that you have previously tried, along with summary of the outcome of each change. {history} First, analyze the task and come up with plan for solving the task: 1. Consider ideas for changes and improvements needed to improve on the task. Consider both creative and practical ideas. 2. Break down the implementation into clear steps, generate pseudo codes for each step 3. Consider potential challenges and how to address them Then, implement your plan by making the necessary code changes. trust you to make good decisions, so do not ask me for permission to make any code changes. Do not ever ask to install any additional packages. The answer will be no. Respond with your plan for improving the code, followed by the fully-functional updated code implementing your plan. Figure D.5 Full prompt for the coder, without external knowledge. Here, the coder is prompted to first conceive of plan for solving the task."
        },
        {
            "title": "Log summarization prmopt",
            "content": "Task: Analyze the following output logs and extract metrics following the metrics structure and typing template provided below. # Logs {logs} # Metric dict template (showing expected type for each key) {metric_types} Respond with only the extracted metrics as JSON dict following the exact structure and type specification in the dict template below. If no metrics are successfully extracted, return the empty dict, {{}}. If any individual key: value expected in the metrics template is missing, set its value to null. Figure D.6 Prompt for extracting metrics resulting from executing solution. Here the logs are concatenation of the standard streams output by running the solution."
        },
        {
            "title": "Standard stream summarization prompt",
            "content": "Task: Produce succinct summary of the following stdout and stderr logs for job running on compute cluster. - Your summary should consider whether the logs indicate whether the goal below was achieved or not. - Keep your summary below 500 words. # Job goal {goal} # stdout logs {log_out} # stderr logs {log_err} Respond with just your summary text with no extra commentary and no extra formatting. If appropriate, include the most useful stderr logs for debugging in code blocks fenced by triple ticks. Figure D.7 Prompt for extracting standard stream summaries and metrics resulting from executing solution. 29 Level 1 hint generation prompt Given the git diff between the current and next version and the changelog, generate high-level pseudo code description of the changes made. Focus on explaining the key algorithmic changes and improvements in clear, concise way. Git diff: {diff} Changelog: {changelog} Generate pseudo code that: 1. Describes the key algorithmic changes and improvements 2. Focuses on the high-level logic and avoids implementation details 3. Explains the purpose and impact of each major change 4. Uses clear, readable pseudo code syntax Format the output as: # Pseudo Code Changes [Your pseudo code description here] Figure D.8 Prompt for generating the level 1 (pseudocode)s hints of the Automated LLM Speedrunning benchmark, where the changelog contains descriptions of the changes retrieved by the repo. Level 2 hint generation prompt Given the current code, changelog, and next code, provide detailed natural language description of the improvements made. Current code: {code} Changelog: {changelog} Next code: {next_code} Provide detailed explanation of: 1. What specific improvements were made 2. Why these changes were beneficial 3. How they contribute to the overall performance 4. Any technical challenges that were addressed Figure D.9 Prompt for generating the level 2 (text) hints of the Automated LLM Speedrunning benchmark, where the changelog contains descriptions of the changes retrieved by the repo and next_code is the full implementation of the next record. 30 Level 3 hint generation prompt Given the current code, changelog, and next code, pseudo codes and text description, generate formal paper-like summary of the improvements. Current code: {code} Changelog: {changelog} Next code: {next_code} Pseudo code: {generate_level_1(record)} Text description: {generate_level_2(record)} Use this text description and pseudocode changes to generate body of knowledge resembling scientific paper. You should tailor the generated scientific paper so that competent machine learning engineer can easily implement the suggested changes in PyTorch. Besure to include the pseudocode in the paper-like summary. Figure D.10 Prompt for generating the level 3 hints of the Automated LLM Speedrunning benchmark, where the changelog contains descriptions of the changes retrieved by the repo and next_code is the full implementation of the next record."
        },
        {
            "title": "E Record breakdown",
            "content": "Table E.1 lists each NanoGPT Speedrun record and its description from the official repository (Jordan et al., 2024a)4. Each record index is shown with its corresponding task index in Automated LLM Speedrunning, including its corresponding target next record (indexed by original record index). Table E.1 Summarized and categorized of records from (Jordan et al., 2024a) # 1 2 4 5 6 7 8 - 1 2 3 4 - 7 8 #2 #3 #3 #4 #4 #5 #5 #6 - #6 #8 #8 # ID # Transition Record time Description - 45 mins llm.c baseline #1 #2 31.4 mins Tuned learning rate & rotary embeddings 24.9 mins"
        },
        {
            "title": "Introduced the Muon optimizer",
            "content": "22.3 mins"
        },
        {
            "title": "Muon improvements",
            "content": "15.2 mins Pad embeddings, ReLU², zero-init projections, QK-norm Category"
        },
        {
            "title": "Architecture",
            "content": "13.1 mins"
        },
        {
            "title": "Parallelization",
            "content": "12.0 mins Upgraded PyTorch 2.5.0 10.8 mins"
        },
        {
            "title": "Untied embedding and head",
            "content": "8.2 mins Value and embedding skip connections, momentum warmup, logit softcap Bfloat16 activations U-net pattern skip connections & double lr 1024-ctx dense causal attention 64K-ctx FlexAttention"
        },
        {
            "title": "Attention Mechanism",
            "content": "10 11 9 10 #9 #10 #10 #11 7.8 mins 7.2 mins 12 11 #11 #12 5.03 mins 14 15 12 13 14 #12 #13 #13 #14 #14 # 4.66 mins"
        },
        {
            "title": "Attention Mechanism",
            "content": "4.41 mins"
        },
        {
            "title": "Value Embeddings",
            "content": "3.95 mins U-net pattern value embeddings, assorted code optimizations"
        },
        {
            "title": "Embeddings",
            "content": "16 15 #15 #16 3.80 mins 17 #16 #17 3.57 mins Split value embeddings, block sliding window, separate block mask"
        },
        {
            "title": "Embeddings",
            "content": "Sparsify value embeddings, improve rotary embeddings, drop an attention layer"
        },
        {
            "title": "Embeddings",
            "content": "18 19 17 18 #17 #18 #18 #19 3.4 mins Lower logit softcap from 30 to"
        },
        {
            "title": "Hyperparameter Tuning",
            "content": "3.142 mins FP8 head, offset logits, lr decay to 0.1 instead of 0."
        },
        {
            "title": "Data Type",
            "content": "20 19 #19 #20 2.992 mins Merged QKV weights, long-short attention, attention scale, lower Adam epsilon, batched Muon"
        },
        {
            "title": "Attention Mechanism",
            "content": "21 20 #20 #21 2.933 mins"
        },
        {
            "title": "Hyperparameter Tuning",
            "content": "4https://github.com/KellerJordan/modded-nanogpt"
        },
        {
            "title": "F Example hints",
            "content": "In this section, we provide example hints used for various hint levels. Level 1 hint (pseudo-code) for Record 1 # Pseudo Code Changes 1. Rotary Position Embedding Implementation # Added rotary position embeddings to attention mechanism class RotaryPositionEmbedding: def __init__(dim, base=10000): precompute inverse frequencies using base^(2i/dim) initialize cache for cos/sin values def forward(sequence_length): if sequence_length not in cache: compute angular positions calculate frequency components store cos(t), sin(t) in cache return cached cos/sin values def apply_rotary_embeddings(q, k, cos, sin): split and vectors into halves rotate components using: rotated_q = q1*cos + q2*sin rotated_k = k1*cos + k2*sin return concatenated rotated vectors 2. Modified Attention Mechanism class SelfAttention: def __init__(): # Changed from standard positional embeddings add rotary embedding module remove position embedding matrix def forward(x): split into q,k,v with same head_dim apply rotary embeddings to and use scaled_dot_product_attention with rotated q/k remove manual scaling (was /sqrt(24)) return attention output 3. Layer-Wise Attention Scaling class TransformerBlock: def __init__(): # Added depth-dependent scaling attn_scale = 1/sqrt(2 * num_layers) def forward(x): += attn_scale * attention_output += mlp_output 4. Simplified Model Architecture class GPT: def __init__(): remove position embedding matrix (wpe) keep only token embeddings (wte) remove custom embedding initialization def forward(): # Position info now handled by rotary embeddings use only token embeddings (no pos_emb addition) 5. Training Process Improvements Training Hyperparameters: batch_size: 32 64 total_batch_size: 262k 524k tokens add warmdown phase after constant LR period Optimization Changes: replace gradient clipping with: grad = grad / (norm + 1e-6) implement linear warmdown schedule add periodic model checkpoint saving Learning Rate Schedule: if step < warmup: linear increase elif step < total - warmdown: constant else: linear decrease to zero Key Impacts: - Rotary embeddings improve position awareness in attention - Layer-wise scaling stabilizes deep networks - Modified LR schedule enables better convergence - Gradient normalization replaces clipping for stability - Larger batches improve training efficiency 33 Level 2 hint (text description) for Record 1 Heres detailed breakdown of the improvements: 1. **Architectural Improvements** - **Rotary Positional Embeddings**: Replaced standard positional embeddings with rotary embeddings - Added Rotary module and apply_rotary_emb function for relative position encoding - Benefits: Better captures relative positions and attention patterns, improves model accuracy - Implementation: Applied to queries/keys in attention instead of separate positional embeddings - **Simplified Normalization** - Removed all affine parameters from RMSNorm implementation - Benefits: Reduces parameter count while maintaining effectiveness - Tradeoff: Minor performance cost offset by other optimizations 2. **Optimization Improvements** - **Learning Rate Changes**: - Increased base LR from 0.0015 to 0.0018 (3x increase as per changelog) - Changed schedule to trapezoidal (warmup constant warmdown) - Benefits: Following [2405.18392], allows more stable high-LR training - **Gradient Normalization**: - Replaced gradient clipping with per-parameter gradient norm scaling - p.grad = p.grad / (p.grad.norm() + 1e-6) - Benefits: More stable training with high LR, prevents explosion 3. **Initialization/Scaling Changes** - **Attention Scaling**: - Introduced attn_scale = 1/sqrt(2*n_layer) - Replaced ad-hoc / math.sqrt(24) with systematic layer-based scaling - Benefits: Better coordinates residual branches across layers - **Removed Positional Embeddings**: - Deleted wpe embedding layer completely - Benefits: Parameter reduction + rotary handles position information 4. **Training Process Improvements** - **Checkpointing**: - Added periodic model saving (save_every parameter) - Benefits: Fault tolerance and easier resumption - **Batch Size Optimization**: - Increased batch size from 32 to 64 tokens - Total batch size from 262K to 524K tokens - Benefits: Better hardware utilization 5. **Code Simplifications** - Removed _init_weights and special initialization flags - Eliminated position embedding mixing (tok_emb + pos_emb) - Removed unused configuration options and legacy code paths **Technical Challenges Addressed**: 1. **Stability at High Learning Rates**: Through gradient normalization and careful attention scaling 2. **Position Encoding Migration**: Non-trivial transition from absolute to relative (rotary) positioning 3. **Distributed Training Coordination**: Maintained DDP compatibility through architectural changes 4. **Learning Rate Schedule Tuning**: Required careful warmup/warmdown balancing for trapezoidal schedule **Performance Impact**: Combined these changes enable 2x faster training convergence by: 34 Level 2 hint (text description), continued - Allowing more aggressive learning rates through better normalization - Improving parameter efficiency with rotary embeddings - Increasing useful batch size through stability improvements - Reducing computational overhead from simplified operations The architectural simplifications also make the model more amenable to compiler optimizations (like torch.compile), while the training process improvements enable better utilization of distributed hardware. Level 3 hint (mini-paper) for Record # Efficient Training of GPT-style Models Through Architectural and Optimization Co-design ## Abstract We present architectural and optimization improvements enabling 2 faster training of GPT-style models while maintaining equivalent final performance. Through systematic analysis of position encoding, normalization schemes, and learning rate scheduling, we demonstrate that: 1) Rotary position embeddings (RoPE) improve position-aware attention computation; 2) trapezoidal learning rate schedule with gradient normalization enables more stable high-rate training; 3) Simplified initialization and scaled residual connections reduce parameter count while maintaining model capacity. Our modifications require minimal code changes while achieving 5B token convergence equivalent to baseline 10B token performance. ## 1. Introduction ### 1.1 Background Transformer architectures (Vaswani et al., 2017) require careful coordination of position encoding, normalization, and optimization parameters to achieve efficient training. We analyze common pain points in standard implementations: - Additive positional embeddings limit attention head flexibility - Unstable gradient flow requiring aggressive clipping - Suboptimal learning rate schedules wasting compute ### 1.2 Key Improvements Our modified architecture (Figure 1) implements four fundamental changes: 1. **Rotary Position Embeddings**: Replace additive positional encoding with rotational transformations of query/key vectors 2. **Layer-Scaled Attention**: Fixed scaling of attention outputs based on network depth 3. **Trapezoidal LR Schedule**: Three-phase schedule combining warmup, sustain, and cooldown periods 4. **Gradient Normalization**: Per-parameter gradient scaling replaces global clipping ## 2. Methodology ### 2.1 Rotary Position Encoding Traditional approaches concatenate positional embeddings to token embeddings. We implement rotary position encoding in attention computation: python class Rotary(nn.Module): def forward(self, x): = arange(seq_len) freqs = outer_product(t, inv_freq) return cos(freqs), sin(freqs) Level 3 hint (mini-paper), continued def apply_rotary_emb(q, k, cos, sin): return (q * cos + rotate(q, sin), * cos + rotate(k, sin)) This creates position-aware transformations without additional embedding parameters. The rotation operation preserves relative position information through dot product attention. ### 2.2 Trapezoidal Learning Schedule Our three-phase schedule improves upon cosine decay: Learning Rate Schedule: 1. Warmup (0 <= step < 256): lr = base * step/256 2. Sustain (256 <= step < N-2000): lr = base 3. Cooldown (N-2000 <= step <= N): lr = base * (N-step)/2000 Mathematically: $$ text{LR}(t) = begin{cases} alphafrac{t}{tau_w} & leq tau_w alpha & tau_w < leq T-tau_d alphafrac{T-t}{tau_d} & > T-tau_d end{cases} $$ Where $alpha=0.0018$, $tau_w=256$, $tau_d=2000$. ### 2.3 Gradient Normalization Replaces global gradient clipping with per-parameter scaling: python # Before: Global clipping torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # After: Per-parameter normalization for in model.parameters(): p.grad = p.grad / (p.grad.norm() + 1e-6) This prevents extreme gradient magnitudes while maintaining relative update directions. ## 3. Architectural Modifications ### 3.1 Simplified Attention Scaling Layer-dependent scaling stabilizes deep networks: python class Block(nn.Module): def __init__(self, config): self.attn_scale = 1/math.sqrt(2*config.n_layer) def forward(self, x): = + self.attn_scale * attn_output residual path accumulation in deep networks. ### 3.2 Parameter Reduction Removed components: 1. Positional embedding matrix (wpe) 2. Affine parameters in RMSNorm 3. Custom weight initialization 36 Level 3 hint (mini-paper), continued Preserves weight tying between input/output embeddings while reducing total parameters by 1.2% ## 4. Implementation Details ### 4.1 Critical Code Changes Core modifications from baseline implementation: python # Additions class Rotary(nn.Module): ... def apply_rotary_emb(...): ... # Modifications class CausalSelfAttention: def forward(): q, = apply_rotary_emb(q, k) # Rotate Q/K class Block: def __init__(): self.attn_scale = ... # Layer-dependent scaling # Removals del self.wpe # Positional embedding matrix del _init_weights # Custom initialization ### 4.2 Training Configuration Updated hyperparameters: Original Modified Parameter --------------------------------------- Batch size Total batch tokens 262k Peak LR LR schedule Warmup iterations 256 64 0.0015 0.0018 Cosine Trapezoidal 256+2000 524k ## 5. Results & Analysis ### 5.1 Performance Improvements Validation loss comparison on 5B tokens: Loss (Original) Loss (Modified) Model ----------------------------------------------- d12 (124M) 3.21 d24 (355M) 2.89 3.09 (-3.7% 2.77 (-4.1% Achieves equivalent loss to baseline in half the tokens. ### 5.2 Training Dynamics Key observations: - 18% - 2.1 fewer iterations to reach target loss - 23% ## 6. Conclusion We demonstrate that coordinated architectural and optimization changes enable 2 faster training of transformer models. Critical factors include: 1. Position encoding through rotary transformations 2. Stable gradient flow via layer-wise scaling 3. Sustained high learning rates through trapezoidal scheduling 37 Level 3 hint (mini-paper), continued The modifications require <200 lines of code changes while maintaining compatibility with standard distributed training setups. Our results suggest that careful component co-design remains crucial for efficient large model training. ## Appendix: Complete Pseudocode Algorithm 1: Modified Training Loop python Initialize model with rotary layers Load data with doubled batch size for step in total_steps: # Forward pass with rotary embeddings cos, sin = rotary(q) = * cos + rotate(q, sin) loss = model(x) # Normalized backward pass loss.backward() for in parameters: p.grad /= (norm(p.grad) + 1e-6) # Trapezoidal LR update lr = trapezoidal_schedule(step) optimizer.step(lr) This paper-style summary provides both theoretical justification and implementation-level details, enabling direct reproduction of the improvements. The combination of architectural simplification and optimized training dynamics demonstrates practical path for efficient large language model training."
        },
        {
            "title": "G Broader impact",
            "content": "AI agents that are proficient in conducting AI research autonomously can provide significant, far-reaching benefits: (1) accelerated scientific progress in healthcare, climate science, and other important domains, (2) economic growth driven by the development of novel technology, and (3) expedited safety and alignment research for models. Crucial to automated science is the ability of such agents to reproduce scientific results, which our benchmark seeks to measure. However, such innovation also requires thorough understanding of model advancements to ensure responsible deployment. We hope our benchmark can serve as useful evaluation for model autonomy. However, agents capable of executing open-ended AI research tasks can also pose risks if their capabilities outpace our ability to comprehend the consequences of their actions. Responsible deployment of such models therefore requires parallel advancements in monitoring, aligning, and controlling such models. To foster understanding, reproducibility, and further development of AI Research Agents, we open-source the full code to reproduce the experiments on the Automated LLM Speedrunning Benchmark presented in this work. We acknowledge the limitations of our benchmark and encourage the development of additional evaluations of automated AI research capabilities, particularly those tailored to the workflow of researchers training frontier models."
        }
    ],
    "affiliations": [
        "Meta",
        "University of Edinburgh"
    ]
}