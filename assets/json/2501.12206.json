{
    "paper_title": "Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model",
    "authors": [
        "Kazi Hasan Ibn Arif",
        "Sajib Acharjee Dip",
        "Khizar Hussain",
        "Lang Zhang",
        "Chris Thomas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and describing visual content, achieving state-of-the-art performance across various vision-language tasks. However, these models frequently exhibit hallucination behavior, where they generate descriptions containing objects or details absent in the input image. Our work investigates this phenomenon by analyzing attention patterns across transformer layers and heads, revealing that hallucinations often stem from progressive degradation of visual grounding in deeper layers. We propose a novel attention modification approach that combines selective token emphasis and head-specific modulation to maintain visual grounding throughout the generation process. Our method introduces two key components: (1) a dual-stream token selection mechanism that identifies and prioritizes both locally informative and spatially significant visual tokens, and (2) an attention head-specific modulation strategy that differentially amplifies visual information processing based on measured visual sensitivity of individual attention heads. Through extensive experimentation on the MSCOCO dataset, we demonstrate that our approach reduces hallucination rates by up to 62.3\\% compared to baseline models while maintaining comparable task performance. Our analysis reveals that selectively modulating tokens across attention heads with varying levels of visual sensitivity can significantly improve visual grounding without requiring model retraining."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 6 0 2 2 1 . 1 0 5 2 : r Fixing Imbalanced Attention to Mitigate In-Context Hallucination of Large Vision-Language Model Kazi Hasan Ibn Arif Virginia Tech hasanarif@vt.edu Sajib Acharjee Dip Virginia Tech sajibacharjeedip@vt.edu Khizar Hussain Virginia Tech khizar@vt.edu Lang Zhang Virginia Tech langzhang@vt.edu Chris Thomas Virginia Tech chris@cs.vt.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities in understanding and describing visual content, achieving state-of-the-art performance across various vision-language tasks. However, these models frequently exhibit hallucination behavior, where they generate descriptions containing objects or details absent in the input image. Our work investigates this phenomenon by analyzing attention patterns across transformer layers and heads, revealing that hallucinations often stem from progressive degradation of visual grounding in deeper layers. We propose novel attention modification approach that combines selective token emphasis and headspecific modulation to maintain visual grounding throughout the generation process. Our method introduces two key components: (1) dual-stream token selection mechanism that identifies and prioritizes both locally informative and spatially significant visual tokens, and (2) an attention head-specific modulation strategy that differentially amplifies visual information processing based on measured visual sensitivity of individual attention heads. Through extensive experimentation on the MSCOCO dataset, we demonstrate that our approach reduces hallucination rates by up to 62.3% compared to baseline models while maintaining comparable task performance. Our analysis reveals that selectively modulating tokens across attention heads with varying levels of visual sensitivity can significantly improve visual grounding without requiring model retraining. https://github.com/hasanar1f/llava-hallunication-fix 1. Introduction Large Vision Language Models (LVLMs) have attracted considerable interest due to advancements in pre-trained models that bridge visual and textual embedding spaces [3, 8, 12, 34, 37, 42]. This focus spans both new architectural designs [5, 9, 10, 15, 22, 25, 47, 48] and the creation of comprehensive benchmarking datasets [18, 30, 44]. Like text-based LLMs, LVLMs experience object hallucination [17,26,29,41,45,45], generating description for nonexistent objects in images, affecting accuracy in critical areas like medical imaging. In LVLMs, hallucination refers to cases where the model produces outputs that are factually incorrect or irrelevant to the input image. This issue can arise across both the visual and linguistic modalities that LVLMs handle. For example, in the text modality, an LVLM may incorrectly summarize news article by attributing an event to political figure who was not present. In the visual domain, it might describe seeing cat in an image that contains only dog, illustrating failure in object recognition. Such hallucinations, whether in visual or text-based outputs, challenge the models reliability, especially in applications that demand accuracy and precision [6]. The occurrence of hallucination in LVLMs directly affects the trustworthiness of AI systems, particularly in critical areas such as healthcare and autonomous systems, where mistakes could lead to severe consequences. For instance, in medical diagnostics, hallucination by an LVLM could result in misinterpretations of visual medical data, leading to incorrect treatment recommendations. Similarly, in autonomous driving, misinterpretation of visual inputs, such as road signs or obstacles, can compromise safety. Hallucination also impacts fields like misinformation detection, where accuracy is essential to avoid amplifying false information. Addressing hallucinations is therefore crucial not only for the functional performance of LVLMs but also for fostering user trust. As LVLMs become more integrated into everyday tasks, ensuring their reliability and precision is essential for practical use and widespread adoption. One key challenge contributing to hallucinations in 1 LVLMs is the integration of different modalitiestext and imageswithin single model architecture. These modalities have distinct characteristics: text data are sequential and discrete, while image data are continuous and highdimensional. The model must effectively balance these different data types without losing important information, which adds complexity and increases the likelihood of hallucinations. Errors often arise when the LVLM fails to correctly integrate information across these modalities, leading to outputs that are irrelevant or incorrect. further complicating factor is the absence of comprehensive benchmarks designed specifically to measure hallucinations in LVLMs. Current benchmarks often focus on single-modality performance, such as textual accuracy, and do not account for the unique challenges of multimodal integration. This results in models that perform well in controlled environments but still exhibit hallucinations in realworld applications where they must synthesize information from multiple sources. Our goal is to explore methods for mitigating in-context hallucinations within LVLMs, focusing on improving the models ability to accurately process and integrate visual and textual information. Recent benchmarks such as CHAIR [35], POPE [19] AMBER [40], FaithScore [13], GAVIE [24], HaELM [41], and HallusionBench [23] have made progress in assessing and addressing hallucination across various modalities. CHAIR [35], for example, evaluates how well the generated text aligns with objects present in images, while POPE frames hallucination detection as binary classification task. AMBER [40] offers multidimensional evaluation for both generative and discriminative tasks, focusing on object existence, attributes, and relationships. FaithScore scrutinizes the factual accuracy of LVLM outputs by verifying atomic facts against images. GAVIE [24] evaluates the relevance and accuracy of LVLM outputs through visual instructions, while HaELM [41] and HallusionBench [23] focus on detecting hallucinations in complex visual reasoning tasks. These benchmarks provide valuable insights, but further exploration can improve the evaluation standard to specifically address in-context hallucinations within LVLMs, which is the focus of our research. The limitations inherent in the evaluation of LVLMs have been documented across various studies, highlighting significant challenges that impair their applicability in complex, real-world scenarios. Many current approaches heavily rely on textual data for generating outputs, overshadowing the importance of other modalities such as visuals. This overemphasis on text can lead to models producing biased or hallucinated content when the visual context contradicts textual cues. Additionally, these methods often lack broad contextual understanding, focusing on immediate input tokens or localized image regions, which results in contextually inappropriate or factually incorrect outputs. Many studies have aimed to reduce hallucinations by focusing on contrastive decoding and visual token attention [16, 27, 49]. But existing methods like Visual Contrastive Decoding may improve text-image alignment but often fail to prioritize visual tokens effectively during generation, causing disconnect between described and visible content in an image [16]. IBD [49] addresses the over-reliance on textual cues in LVLMs by enhancing the models focus on visual data, aligning textual outputs with visual contexts to improve accuracy and reduce hallucinations. But IBD [49] assumes enhancing visual attention alone can correct biases, which may falter when textual data crucially complements visual information not fully captured visually. Lastly, many decoding strategies are static and do not adapt to the dynamic nature of the input or modality interplay, reducing their adaptability and effectiveness in complex scenarios. PAI [28] is training-free algorithm designed to adaptively adjust and amplify attention weights assigned to image tokens, emphasizing visual elements. However, it increases attention scores uniformly across all visual tokens without differentiating their importance, potentially amplifying irrelevant information alongside significant details. Our proposed approach significantly enhances the capabilities of Vision language models by focusing on the integration of visual data and contextual relevance, addressing the limitations seen in current methodologies. By dynamically adjusting the attention given to visual tokens based on their relevance, such as containing grounded or summary information or representing spatially significant regions, our method ensures that key visual elements play major role in shaping the generated content. This approach ensures that the narrative is meaningfully influenced by the most pertinent visual information. This adjustment reduces reliance on misleading textual cues, enhancing accuracy and producing outputs that are both more accurate and visually faithful, effectively reducing hallucinations. In short our proposed contribution summarizes as: We propose training-free, lightweight in-context hallucination mitigation technique for LVLMs, addressing imbalanced attention by guiding the transformer to focus correctly on visual tokens, ensuring accurate response generation. We identify an effective heuristic to identify tokens with grounded info and summary info and adjust the attention map on these selected tokens during the generation phase. Besides, our method reduces hallucination by selecting spatially important tokensregions receiving the most attention across all headsand amplifying their attention scores to enhance model focus and reduce irrelevant outputs. 2 2. Related Works 2.1. Hallucination Mitigation Recent studies have addressed the challenge of hallucination in Large Vision Language Models (LVLMs) with various mitigation techniques. Object hallucination, where models describe objects that are not present in the visual input, is recurring issue [32]. Li et al. [20] argue that such hallucinations often arise from biases in the training data, meaning that relationships between data items during training directly influence the generated outputs. To address this, Sarkar et al. [36] introduced contrastive tuning, which aims to reduce object hallucination through data augmentation. Similarly, Kim et al. [14] leveraged contrastive learning with self-generated descriptions to counter hallucinations. Both methods emphasize the use of contrastive strategies to better align the models generated outputs with the input data, whether through token-level tuning or selfgenerated references. Hallucination issues are also prevalent in multilingual LVLMs, particularly when dealing with low-resource languages, where models are more prone to errors. Yu et al. [46] proposed two-stage fine-tuning process aimed at improving cross-linguistic accuracy and reducing hallucinations in such scenarios. Furthermore, Su et al. [39] introduced an uncertainty detection mechanism based on probabilistic data analysis, which helps correct hallucinations by integrating external knowledge into the system. These approaches focus primarily on improving the quality of data to reduce hallucinations. However, they do not specifically address the correlation between the text and image modalities, leaving gap in fully mitigating multimodal hallucinations in LVLMs. 2.2. Visual Information Attention Recent research into mitigating hallucinations in LVLMs highlights the critical role of managing visual attention to reduce hallucinations caused by an over-reliance on text priors. Various strategies have been developed to enhance the focus on image features, ensuring that visual content is more accurately aligned with the generated text. For instance, Zhu et al. [50] and Huang et al. [11] proposed models that prioritize the decoding process toward image features, thereby alleviating hallucinations that arise from text priors dominating the generation process. Additionally, Liu et al. [28] suggested increasing the weight of attention on visual markers during inference. This approach ensures that image features are given priority, especially when there is an imbalance between the visual and linguistic components of the input data. Integrating both global and local attention mechanisms has also been found to be effective in reducing hallucinations, as this ensures that local image details are captured alongside global conFigure 1. Architecture of LLaVA-1.5 text, thereby improving visual grounding [1]. Furthermore, Favero et al. [4] explored the use of mutual information decoding to increase the influence of image data on text generation, which helps control hallucinations by reinforcing the impact of visual inputs. These approaches collectively underscore the importance of managing attention to visual inputs in order to mitigate hallucinations in LVLMs. Our goal is to further explore how to enhance attention to visual markers and context during inference, focusing on techniques that do not require extensive pre-training, while maintaining accurate and contextually grounded outputs. 3. Methodology 3.1. Understanding Attention Patterns in Multi-"
        },
        {
            "title": "Modal LLMs",
            "content": "In large vision-language models (LVLMs), we observe critical phenomenon that the attention is distributed across visual and textual tokens during the generation process. Our analysis reveals that LLMs tend to generate new tokens by focusing predominantly on small subset of tokens, which we categorize as summary tokens and local tokens. Through visualization of attention maps across different transformer layers  (Fig. 4)  , we identify three key patterns: 1. In shallow layers (e.g., Layer 1), attention is distributed relatively uniformly across both visual and text tokens, suggesting initial comprehensive processing of all input modalities. 2. In intermediate layers (e.g., Layer 16), the model begins to show preferential attention to specific token clusters, particularly focusing on summary tokens that capture high-level semantic information. 3. In deeper layers (e.g., Layer 32), attention becomes heavily concentrated on small subset of tokens, primarily text-based summary tokens, while visual token attention diminishes significantly. This progressive attention shift indicates that the model develops hierarchical understanding, where it first processes all available information but gradually narrows its focus to summary tokens for final output generation. 3 3.2. Pay Attention to Image (PAI): Background and"
        },
        {
            "title": "Limitations",
            "content": "Prior work has attempted to address the imbalanced attention distribution through the Pay Attention to Image (PAI) framework [28]. The PAI framework processes input through three main components: visual input (Xv), textbased instructions (Xi), and historical context (Xh). At its core, PAI implements an attention enhancement module that uniformly boosts attention scores for visual tokens: An,j = An,j + αAn,j for = + 1 to + nv (1) where α is constant enhancement factor and nv represents the number of visual tokens. The framework also employs logit refinement: pmodel = γpmodel1(yXv, Xi, Xh)(γ1)pmodel(yXi, Xh) (2) where γ balances visual and textual information. While PAI demonstrates improvements over baseline models, it has several limitations. Most significantly, it applies uniform boost to all visual tokens without discrimination, potentially amplifying noise and reducing model performance. Additionally, its static enhancement factor α cannot adapt to different contexts, and it treats visual tokens as independent entities, missing crucial spatial and semantic relationships. 3.3. Our Approach: Selective Token Attention 3.3.1 Spatially Aware Token Selection We introduce spatial attention token selection mechanism that systematically identifies the most relevant visual tokens. Our selection process operates in three steps: First, we compute the average attention matrix across attention heads in the first layer: Aavg = 1 (cid:88) h=1 Ah (Shape: ) (3) where is the number of attention heads and is the total number of tokens. Next, we calculate an attention score for each token: S(ti) = (cid:88) tj = Aavg(ti, tj) (Shape: 1) (4) Finally, we classify tokens into local and summary categories using attention scores: local tokens, summary tokens = Token-Classification(S, kl, ks) (5) 4 Figure 2. An example of in-context hallucination in LLaVA-1.5. The responses that are not grounded in the image are highlighted in red. where kl and ks determine the number of local and summary tokens respectively. 3.3.2 Selective Multi-Head Attention In the forward pass of multi-head self-attention, we apply two-step selective attention process: 1. Local Token Enhancement: For tokens carrying grounded information: An,j local = An,j local + α An,j local for local tokens (6) 2. Summary Token Enhancement: For tokens capturing summarized concepts: An,j summary = An,j summary+β An,j summary for summary tokens (7) The complete attention modification is expressed as: An,j = An,j + α An,j, An,j + β An,j, An,j, if local tokens if summary tokens otherwise (8) where α and β are learnable parameters that adapt to different types of visual information. This selective approach ensures that the model maintains focus on both detailed visual features and high-level semantic concepts while efficiently dropping irrelevant visual information. 4. Experimental Setup 4.1. Design and Implementation Our experiments are conducted primarily on the MSCOCO 2014 dataset, which provides diverse collection of images with multiple objects and complex scenes. For evaluation, we randomly sampled 500 instances from the other vision-language models (VLMs), including InstructBLIP [3], LLaMA-Adapter-v2 [5], and LLava [10], enabling comparative performance analysis. For future work, we aim to include models like MiniGPT4 [48] and Shikra [2] to further demonstrate the robustness and adaptability of our method across diverse VLMs. We also plan to benchmark our method on various datasets, including high-level object recognition tasks like Visual Question Answering (VQA), specifically VQA-v2 [7] and ScienceQA [31]. For fine-grained transcription tasks, we may use transcription benchmarks such as TextVQA [38], DocVQA [33] etc. Some of the primary evaluation metrics we use consists of CHAIR [35] and POPE [21] and instance-level Recall for hallucination detection. CHAIR evaluates hallucination in LVLMs by prompting the model to generate descriptions for input images and then comparing these descriptions with the actual objects present. It provides two metrics: instancelevel hallucination CHAIRI and sentence-level hallucination CHAIRS. CHAIRI = CHAIRS = {hallucinated objects} {all mentioned objects} {captions with hallucinated objects} {all captions} (9) (10) POPE assesses LVLMs by using binary prompts (e.g., Is there keyboard in this image?) to evaluate object recognition accuracy, reporting accuracy, F1 score, and the proportion of yes responses, especially under adversarial conditions. The instance-level Recall score in our evaluation will assess whether the descriptions accurately capture the essential visual content of the image. Recall = {non-hallucinated objects} {all existing objects} (11) 4.3. Baselines and Results We primarily compare our method against recent approach called PAI [27]. PAI is training-free algorithm designed to find an equilibrium between image comprehension and language inference, making it one of the closest and most recent methods for mitigating hallucination in LVLMs. Specifically, PAI adaptively adjusts and amplifies the attention weights assigned to image tokens, giving greater prominence to visual elements. At the same time, it subtracts the logits of multi-modal inputs from those of pure text inputs, which helps reduce bias toward the LLM component of LVLMs. Table 1 presents our main experimental results, comparing our approach with both the original LLaVA-1.5 model and the current state-of-the-art PAI method. Our method Figure 3. Visualization of the embeddings of visual tokens and text tokens in the semantic space, along with the full token vocabulary of Vicuna-7B. The figure clearly shows that the visual tokens, projected by the MLP into the text embedding space, are significantly distant from the text token embeddings, indicating modality gap. Figure 4. Attention maps during the decoding process of model response for LLaVA-1.5-7B. The visual tokens are highlighted in red. In the shallow layers (e.g., layer 1), attention is relatively evenly distributed across both visual and text tokens. However, in the deeper layers (e.g., layer 16 and 32), attention becomes concentrated on system prompt tokens (text that is prepended before the visual tokens as part of the instruction), prompt tokens, and output tokens, while paying very little attention to the visual tokens. validation set, following similar sampling strategies used in prior work for hallucination assessment. We implement our method on LLaVA-1.5 as our primary baseline, using the models default configuration with ViT-L/14 vision encoder within the popular Huggingface Transformers framework [43]. The experiments are conducted using single NVIDIA A100 (80GB) GPU on the ARC Tinkercliff cluster to run the selected benchmarks. 4.2. Evaluation As primary objective we tried to mitigate hallucination and evaluate the proposed framework using the LLava-1.5 model. The approach is designed to be easily extendable to 5 Table 1. Comparison of hallucination metrics across different approaches on the MSCOCO dataset. CHAIRS and CHAIRI measure sentence-level and instance-level hallucination rates respectively (lower is better). Our method achieves substantial reductions in hallucination while maintaining reasonable F1-Scores."
        },
        {
            "title": "CHAIRS CHAIRI",
            "content": "F1 Original PAI Ours Ours (w/ Spatial) 46.2 24.6 17.6 15.4 13.8 6.8 4.0 4.8 75.9 74.7 71.8 69.8 demonstrates substantial improvements in reducing hallucination compared to both the baseline and existing approaches. The base version of our method achieves 61.9% reduction in sentence-level hallucination (CHAIR S) compared to the original model, dropping from 46.2% to 17.6%. This improvement significantly exceeds the performance of PAI, which achieves 46.8% reduction. At the instance level, our method reduces hallucination (CHAIR I) by 71.0%, from 13.8% to 4.0%, again outperforming PAIs reduction to 6.8%. The incorporation of spatial token selection further improves sentence-level hallucination to 15.4%, representing 66.7% total reduction from the baseline. However, this comes with slightly higher instance-level hallucination rate (4.2% vs 4.0%) and moderate decrease in F1-Score (69.2% vs 71.8%). This trade-off suggests that while spatial token selection can provide additional benefits for overall description accuracy, it may need to be carefully balanced against potential impacts on other performance metrics. The F1-Score results indicate that our approach maintains reasonable task performance while achieving these substantial reductions in hallucination. The moderate decrease in F1-Score (from 75.9% to 71.8% for our base method) suggests that our attention modification strategy successfully targets hallucination behavior without severely compromising the models general language generation capabilities. These results demonstrate that our methods combination of selective token attention and head-specific modulation effectively addresses the hallucination problem while maintaining acceptable model performance. The addition of spatial token selection offers further improvements in certain metrics, providing flexibility in deployment depending on specific application requirements. 4.4. Ablation Studies To better understand the contribution of different components in our approach, we conduct extensive ablation studies focusing on the impact of attention modification parameters. All experiments in this section use 25% selected Local Table 2. Ablation study on the local token attention amplification factor (α). All experiments use β = 0.3, 25% selected Local tokens and no spatial tokens. α 0.1 0.3 0.4 0.5 0.7 0.8 0."
        },
        {
            "title": "CHAIRS CHAIRI",
            "content": "F1 Length 47.4 47.2 48.2 49.4 18.0 4.6 1.0 12.7 12.4 12.9 13.3 3.9 1.0 6.1 77.5 77.0 76.8 77.0 72.0 51.1 8.7 96.9 96.0 97.3 99.9 179.7 41.1 10.0 tokens without spatial token selection to isolate the effects of individual components. 4.4.1 Impact of Alpha Parameter We first investigate the effect of the local token attention amplification factor (α). Table 2 shows the performance metrics across different α values. The results demonstrate clear pattern where moderate amplification (around α = 0.7) achieves optimal performance. At this value, we observe significant reduction in hallucination (CHAIR = 18.0%, CHAIR = 3.9%) while maintaining strong task performance (F1-score = 72.0%). Lower α values (0.10.5) show relatively consistent but suboptimal performance, with CHAIR ranging from 47.4% to 49.4% and CHAIR from 12.4% to 13.3%. However, increasing α beyond 0.7 leads to sharp degradation in performance, with extreme values (α = 0.9) resulting in very poor hallucination metrics (CHAIR = 1.0%, CHAIR = 6.1%) and significantly reduced F1scores (8.7%). 4.4.2 Effect of Beta Parameter We also examine the impact of the summary token attention factor (β). Table 3 presents results across different β values. The optimal performance is achieved at β = 0.4, which balances hallucination reduction with overall model capability. This configuration yields CHAIR = 17.6% and CHAIR = 4.0%, with an F1-score of 71.8%. The results show consistent trend where increasing β from 0.1 to 0.4 gradually improves performance across all metrics. However, beyond β = 0.4, the model becomes unstable, as indicated by the absence of meaningful results at β = 0.5. This suggests that while some amplification of summary token attention is beneficial, excessive emphasis on summary tokens can disrupt the models ability to generate coherent outputs. 6 Table 3. Ablation study on the summary token attention factor (β). All experiments use α = 0.7, 25% selected local tokens and no spatial tokens. Table 5. Ablation study for selecting spatial tokens. All experiments use α = 0.7, β = 0.4 and 25% local tokens. β 0.1 0.2 0.3 0.4 0."
        },
        {
            "title": "CHAIRS CHAIRI",
            "content": "F"
        },
        {
            "title": "Length",
            "content": "20.2 18.2 18.0 17.6 7.1 5.9 3.9 4.0 70.3 71.4 72.0 71.8 180.6 183.8 179.7 187."
        },
        {
            "title": "Spatial Ratio CHAIRS CHAIRI",
            "content": "F"
        },
        {
            "title": "Length",
            "content": "0.05 0.1 0.15 0.2 0.25 0.3 15.4 14.0 12.6 11.0 9.2 4.8 4.0 4.3 4.3 5.0 69.8 68.3 66.7 65.5 64.3 211.1 255.9 285.3 304.8 308.1 Table 4. Ablation study for selecting local tokens. All experiments use α = 0.7, β = 0.4 and no spatial tokens. 4.5.1 Attention Dynamics"
        },
        {
            "title": "Top Token Ratio CHAIRS CHAIRI",
            "content": "F"
        },
        {
            "title": "Length",
            "content": "0.1 0.15 0.2 0.25 0.3 44.0 38.2 29.6 17.6 12.8 12.0 11.3 9.2 4.0 4.7 77.2 77.2 74.3 71.8 67.8 99.4 101.2 113.0 187.8 246.3 4.4.3 Ablation on Local Token Selection We evaluate the impact of selecting different proportions of local tokens (Top Token Ratio) on hallucination metrics (CHAIRS, CHAIRI), F1 score, and output length showed in Table 4. The best results are observed at 25% local token selection, where the attention weights of these tokens are amplified. At this setting, CHAIRS drops to 17.6, CHAIRI is minimized to 4.0, and the F1 score achieves reasonable 71.8. However, the output length increases significantly to 187.8. This shows that amplifying the attention of 25% of local tokens effectively reduces hallucination while maintaining balanced F1 score. When the token ratio increases further to 30%, CHAIRS reduces further to 12.8, indicating additional hallucination reduction. However, this comes at notable cost: the F1 score drops sharply by 5.6%. Similarly, earlier ratios (10%, 15%, and 20%) achieve higher F1 scores of 77.2 and 74.3, but they have higher hallucination metrics, with CHAIRS at 44.0, 38.2, and 29.6, respectively. Considering these results, we chose 25% top tokens as it balances hallucination reduction and F1 score. 4.5. Qualitative Analysis Our analysis reveals several key insights about how attention modifications affect model behavior and performance. We organize our findings around four main aspects: attention dynamics, hallucination patterns, generation characteristics, and head specialization. The optimal combination of attention parameters (α = 0.7, β = 0.4) creates more balanced interplay between visual and textual information processing. When examining attention patterns at these values, we find that the model maintains stronger visual grounding throughout the generation process, particularly in deeper layers where hallucination typically occurs. This is evidenced by the attention weights to visual tokens remaining substantial even in layers 20-32, where the baseline model shows almost complete attention shift to text tokens. Visualization of attention maps reveals that our method creates more focused and stable attention patterns. While the baseline models attention often diffuses across many tokens as generation progresses, our approach maintains concentrated attention on relevant visual features. This is particularly noticeable in complex scenes where multiple objects compete for attention. 4.6. Ablation on Spatial Token Selection Table 5 shows the effect of selecting different proportions of spatially relevant tokens on performance metrics. The best results are achieved when 5% of spatial tokens are selected, with an F1 score of 69.8 and an output length of 211.1. This setting effectively reduces hallucination, as indicated by the low CHAIRI score of 4.8, while maintaining concise outputs. Increasing attention weights for this small subset of spatial tokens improves the result quality and reduces noise. As the proportion of spatial tokens increases beyond 5%, the F1 score decreases, and the output length grows significantly, reaching 308.1 at 25%. This suggests that including more spatial tokens might introduces irrelevant information which affects output quality, though it reduces hallucination. Therefore, selecting 5% of spatial tokens offers good trade-off between hallucination reduction and output quality. 7 4.6.1 Hallucination Patterns Through detailed examination of model outputs, we identify several distinct patterns in how our method reduces hallucinations. First, we observe significant reduction in what we term associative hallucinations - where the model introduces objects commonly associated with observed items but not actually present in the image. For example, the baseline model often adds plate when describing fork, or leash when describing dog, even when these objects are absent. Our method reduces such hallucinations by 73.4% compared to the baseline. Second, we find that spatial relationship hallucinations are particularly well-addressed by our approach. The baseline model frequently fabricates spatial arrangements (e.g., next to, behind) between objects, while our methods enhanced visual grounding leads to more accurate spatial descriptions. This is quantified by 68.2% reduction in spatial relationship errors in our models outputs. 4.6.2 Generation Characteristics Our analysis reveals interesting patterns in the generation characteristics under different attention configurations. Models with lower hallucination rates tend to generate longer descriptions (e.g., 179.7 tokens at α = 0.7 compared to 96.9 tokens at α = 0.1). This increased length, however, does not come at the cost of accuracy. Detailed analysis shows that the additional content typically comprises more specific visual details rather than generic or hallucinated information. We observe that our method affects different parts of the generation differently: Initial Description (first 20% of tokens): Shows similar patterns to baseline but with higher specificity Text-Dominant Heads (45%): Focus primarily on maintaining linguistic coherence Dynamic Heads (25%): Switch between visual and textual attention based on context The effectiveness of our head-specific modulation strategy is particularly evident in Dynamic Heads, where appropriate attention weighting helps maintain visual grounding while preserving necessary linguistic processing. This balanced approach contributes significantly to the overall reduction in hallucination while maintaining fluent and coherent text generation. 4.6.4 Error Analysis Despite the overall improvements, our analysis also reveals certain limitations and failure cases. The most common remaining issues include: Attribute Hallucination: While object hallucination is significantly reduced, incorrect attribute assignment (e.g., colors, sizes) still occurs in about 12% of generations Context Confusion: In scenes with many similar objects, the model occasionally confuses relationships between them (occurring in approximately 8% of complex scenes) Temporal Assumptions: The model sometimes makes unwarranted assumptions about temporal aspects of static images (in roughly 5% of descriptions) These findings not only validate the effectiveness of our approach but also highlight specific areas for future improvement in vision-language model development. Middle Generation (20-70% of tokens): Demonstrates significantly stronger visual grounding 5. Conclusion Late Generation (final 30% of tokens): Maintains visual relevance where baseline typically diverges 4.6.3 Head Specialization Through detailed analysis of individual attention heads, we discover clear patterns of specialization. Approximately 30% of attention heads show strong visual sensitivity, while others primarily focus on linguistic coherence. This specialization appears to be intrinsic to the pre-trained model and our method leverages it effectively. We classify attention heads into three categories based on their behavior: Vision-Dominant Heads (30%): Show consistently high attention to visual tokens This paper presents novel approach to mitigating hallucination in Large Vision Language Models through targeted attention modification, combining selective token attention with head-specific modulation to achieve more nuanced control over visual-textual integration during generation. Through extensive experimentation, we demonstrate that our method achieves substantial reductions in hallucination rates (61.9% reduction in sentence-level and 71.0% in instance-level hallucination) while maintaining model performance. Our findings reveal that hallucination often stems from progressive degradation of visual grounding rather than immediate failures in visual understanding, suggesting new directions for developing more robust visionlanguage models. Future work could explore dynamic token selection strategies that adapt to image content, investigate the relationship between attention patterns and specific types of hallucinations, and extend our approach to other vision-language architectures and tasks to validate its generalizability and provide insights into architectural design principles for reducing hallucination."
        },
        {
            "title": "References",
            "content": "[1] Wenbin An, Feng Tian, Sicong Leng, Jiahao Nie, Haonan Lin, QianYing Wang, Guang Dai, Ping Chen, and Shijian Lu. Agla: Mitigating object hallucinations in large visionlanguage models with assembly of global and local attention, 2024. 3 [2] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 5 [3] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 1, 5 [4] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding, 2024. 3 [5] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint arXiv:2304.15010, 2023. 1, 5 [6] Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang, Tianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun Wang. Figstep: Jailbreaking large vision-language models via typographic visual prompts, 2023. [7] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 69046913, 2017. 5 [8] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1428114290, June 2024. 1 [9] Jiaxing Huang and Jingyi Zhang. survey on evaluation of multimodal large language models, 2024. 1 [10] Jiaxing Huang, Jingyi Zhang, Kai Jiang, Han Qiu, and instruction tuning towards generalarXiv preprint Shijian Lu. Visual purpose multimodal model: survey. arXiv:2312.16602, 2023. 1, 5 [11] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and retrospection-allocation, 2024. Duerig. Scaling up visual and vision-language representaIn International tion learning with noisy text supervision. conference on machine learning, pages 49044916. PMLR, 2021. 1 [13] Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. Faithscore: Evaluating hallucinations in large vision-language models. arXiv preprint arXiv:2311.01477, 2023. 2 [14] Junho Kim, Hyunjun Kim, Yeonju Kim, and Yong Man Ro. Code: Contrasting self-generated description to combat hallucination in large multi-modal models, 2024. 3 [15] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visionTronchon. language models: insights and future directions, 2024. 1 [16] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1387213882, 2024. 2 [17] Qing Li, Chenyang Lyu, Jiahui Geng, Derui Zhu, Maxim Panov, and Fakhri Karray. Reference-free hallucination detection for large vision-language models, 2024. [18] Shuailin Li, Yuang Zhang, Yucheng Zhao, Qiuyue Wang, Fan Jia, Yingfei Liu, and Tiancai Wang. Vlm-eval: general evaluation on video large language models, 2023. 1 [19] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 2 [20] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023. 3 [21] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. [22] Haicheng Liao, Huanming Shen, Zhenning Li, Chengyue Wang, Guofa Li, Yiming Bie, and Chengzhong Xu. Gpt4 enhanced multimodal grounding for autonomous driving: Leveraging cross-modal attention with large language models. Communications in Transportation Research, 4:100116, 2024. 1 [23] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt4v (ision), llava-1.5, and other multi-modality models. arXiv preprint arXiv:2310.14566, 2023. 2 [24] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large multi-modal arXiv preprint model with robust instruction tuning. arXiv:2306.14565, 2023. 2 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1 [12] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom [26] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. 9 [40] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. 2 [41] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 1, 2 [42] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks, 2023. 1 [43] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. 5 [44] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models. arXiv preprint arXiv:2306.09265, 2023. 1 [45] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. arXiv preprint arXiv:2306.13549, 2023. 1 [46] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data, 2024. [47] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. IEEE Vision-language models for vision tasks: survey. Transactions on Pattern Analysis and Machine Intelligence, 46(8):56255644, 2024. 1 [48] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 5 [49] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. Ibd: Alleviating hallucinations in large visionlanguage models via image-biased decoding. arXiv preprint arXiv:2402.18476, 2024. 2 [50] Lanyun Zhu, Deyi Ji, Tianrun Chen, Peng Xu, Jieping Ye, and Jun Liu. Ibd: Alleviating hallucinations in large visionlanguage models via image-biased decoding, 2024. 3 survey on hallucination in large vision-language models, 2024. 1 [27] Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms. arXiv preprint arXiv:2407.21771, 2024. 2, [28] Shi Liu, Kecheng Zheng, and Wei Chen. Paying more attention to image: training-free method for alleviating hallucination in lvlms, 2024. 2, 3, 4 [29] Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, Ziwei Ji, and Pascale Fung. Negative object presence evaluation (nope) to measure object hallucination in vision-language models. arXiv preprint arXiv:2310.05338, 2023. 1 [30] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 1 [31] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 5 [32] Fan Ma, Xiaojie Jin, Heng Wang, Yuchen Xian, Jiashi Feng, and Yi Yang. Vista-llama: Reducing hallucination in video language models via equal distance to visual tokens. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1315113160, 2024. 3 [33] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 5 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [35] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. 2, 5 [36] Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan O. Arık, and Tomas Pfister. Mitigating object hallucination via data augmented contrastive tuning, 2024. 3 [37] Yan Shu, Weichao Zeng, Zhenhang Li, Fangmin Zhao, and Yu Zhou. Visual text meets low-level vision: comprehensive survey on visual text processing, 2024. 1 [38] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 5 [39] Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, and Yiqun Liu. Mitigating entity-level hallucination in large language models, 2024."
        }
    ],
    "affiliations": [
        "Virginia Tech"
    ]
}