{
    "paper_title": "Test-time Computing: from System-1 Thinking to System-2 Thinking",
    "authors": [
        "Yixin Ji",
        "Juntao Li",
        "Hai Ye",
        "Kaixin Wu",
        "Jia Xu",
        "Linjian Mo",
        "Min Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The remarkable performance of the o1 model in complex reasoning demonstrates that test-time computing scaling can further unlock the model's potential, enabling powerful System-2 thinking. However, there is still a lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System-1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the model's reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out a few possible future directions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 7 9 4 2 0 . 1 0 5 2 : r Test-time Computing: from System-1 Thinking to System-2 Thinking Yixin Ji1, Juntao Li1*, Hai Ye2, Kaixin Wu3, Jia Xu3, Linjian Mo3, Min Zhang1 1School of Computer Science and Technology, Soochow University 2Department of Computer Science, National University of Singapore 3Ant Group jiyixin169@gmail.com; {ljt,minzhang}@suda.edu.cn yehai@comp.nus.edu.sg; {daniel.wkx,steve.xuj,linyi01}@antgroup.com"
        },
        {
            "title": "Abstract",
            "content": "The remarkable performance of the o1 model in complex reasoning demonstrates that testtime computing scaling can further unlock the models potential, enabling powerful System2 thinking. However, there is still lack of comprehensive surveys for test-time computing scaling. We trace the concept of test-time computing back to System-1 models. In System1 models, test-time computing addresses distribution shifts and improves robustness and generalization through parameter updating, input modification, representation editing, and output calibration. In System-2 models, it enhances the models reasoning ability to solve complex problems through repeated sampling, self-correction, and tree search. We organize this survey according to the trend of System-1 to System-2 thinking, highlighting the key role of test-time computing in the transition from System-1 models to weak System-2 models, and then to strong System-2 models. We also point out few possible future directions."
        },
        {
            "title": "Introduction",
            "content": "Over the past decades, deep learning with its scaling effects has been the driving engine behind the AI revolution. Particularly in the text modality, large language models (LLMs) represented by the GPT series (Radford et al., 2018, 2019; Brown et al., 2020; Ouyang et al., 2022; OpenAI, 2023) have demonstrated that larger models and more training data lead to better performance on downstream tasks. However, on the one hand, further scaling in the training phase becomes difficult due to the scarcity of data and computational resources (Villalobos et al., 2024); on the other hand, existing models still perform far below expectations in terms of robustness and handling complex tasks. These shortcomings are attributed to * Corresponding author. 1https://github.com/Dereck0602/ Awesome_Test_Time_LLMs. 1 the models reliance on fast, intuitive System-1 thinking, rather than slow, deep System-2 thinking (Weston and Sukhbaatar, 2023). Recently, the o1 model (OpenAI, 2024), equipped with System-2 thinking, has gained attention for its outstanding performance in complex reasoning tasks. It demonstrates test-time computing scaling effect: the greater the computational effort in the inference, the better the models performance. The concept of test-time computing emerged before the rise of LLMs and was initially applied to System-1 models (illustrated in Figure 1). These System-1 models can only perform limited perceptual tasks, relying on patterns learned during training for predictions. As result, they are constrained by the assumption that training and testing are identically distributed and lack robustness and generalization to distribution shifts (Zhuang et al., 2020). Many works (Wang et al., 2021; Ye et al., 2023) have attempted to improve model robustness with test-time computing, also known as test-time adaptation (TTA). In the era of small models, the mainstream method of TTA is to update model parameters at test-time, so that the models representation gradually adapts to the test distribution. For LLMs, due to the high cost of updating parameters, TTA leverages external information to steer model behavior, including modifying inputs, editing representations, and calibrating outputs. By introducing TTA, the System-1 model slows down its thinking process and gradually evolves into weak System-2 model. Currently, advanced LLMs with chain-ofthought (CoT) prompting have enabled language models to reach preliminary System-2 thinking, demonstrating the human-like cognitive ability to decompose problems and reason step by step (Wei et al., 2022). However, they still struggle with complex tasks like reasoning and planning (Stechly et al., 2024; Sprague et al., 2024). To achieve stronger System-2 models, researchers employ Figure 1: Illustration of test-time computing in the System-1 and System-2 model. three types of test-time computing strategies to extend model reasonings depth and accuracy: repeated sampling, self-correction, and tree search. Repeated sampling simulates the diversity of human thinking, self-correction enables LLMs to reflect, and tree search enhances reasoning depth. The remainder of this paper provides comprehensive survey of the latest research developments of test-time computing in System-1 and System-2 models. In Section 2, we present the background of System-1 and System-2 thinking. Section 3 and Section 4 detail the test-time computing methods for the System-1 and System-2 models. Then, we discuss possible future directions in Section 5."
        },
        {
            "title": "2 Background",
            "content": "System-1 and System-2 thinking are psychological concepts (Kahneman, 2011). When recognizing familiar patterns or handling simple problems, humans often respond intuitively. This automatic, fast thinking is called System-1 thinking. In contrast, when dealing with complex problems like mathematical proofs or logical reasoning, deep and deliberate thought is required, referred as System2 thinkingslow and reflective. In the field of artificial intelligence, researchers also use these terms to describe different types of models (LeCun, 2022). System-1 models respond directly based on internally encoded perceptual information and world knowledge without showing any intermediate decision-making process. In contrast, System2 models explicitly generate reasoning processes and solve tasks incrementally. Before the rise of LLMs, System-1 models were the mainstream in AI. Although many deep learning models, such as ResNet, Transformer, and BERT, achieve excellent performance in various tasks in computer vision and natural language processing, these System-1 models, similar to human intuition, lack sufficient robustness and are prone to errors. Nowadays, the strong generation and reasoning capabilities of LLMs make it possible to build System-2 models. Wei et al. (2022) propose the CoT, which allows LLMs to generate intermediate reasoning steps progressively during inference. Empirical and theoretical results show that this approach significantly outperforms methods that generate answers directly (Kojima et al., 2022; Zhou et al., 2023; Tang et al., 2024b; Feng et al., 2024a; Li et al., 2024e). However, current System-2 models represented by CoT prompting still have shortcomings. The intermediate processes generated by LLMs may contain errors, leading to cumulative mistakes and ultimately resulting in incorrect answers. Although retrieval-augmented generation (RAG) helps mitigate factual errors (Trivedi et al., 2023; Guan et al., 2024; Wang et al., 2024g; Ji et al., 2024), their impact on improving reasoning abilities remains limited. As result, CoT-enabled LLMs are still at the weak system-2 thinking stage."
        },
        {
            "title": "3.1 Updating the Model",
            "content": "Model updating utilizes test sample information to further finetune model parameters during the infer2 Parameter Updating Test-time Training TTT (Sun et al., 2020), TTT++ (Liu et al., 2021), CPT (Zhu et al., 2024); etc. Fully TTA Tent (Wang et al., 2021), SAR (Niu et al., 2023), TPT (Shu et al., 2022), OIL (Ye et al., 2022), Anti-CF (Su et al., 2023a), RLCF (Zhao et al., 2024a); etc. Test-time Adaptation (3) Input Modification Demonstration Selection EPR (Rubin et al., 2022), UDR (Li et al., 2023b), CQL (Zhang et al., 2022b), Entropy (Lu et al., 2022), MDL (Wu et al., 2023), HiAR (Wu et al., 2024a); etc. Demonstration Creation SG-ICL (Kim et al., 2022), Self-ICL (Chen et al., 2023), DAIL (Su et al., 2024), Auto-CoT (Zhang et al., 2023b), DAWN-ICL (Tang et al., 2024a); etc. ITI (Li et al., 2023a), ActAdd (Turner et al., 2024), SEA (Qiu et al., 2024b), CAA (Rimsky et al., 2024); etc. kNN-MT (Khandelwal et al., 2021), AdaNPC (Zhang et al., 2023a), Bi-kNN (You et al., 2024); etc. Representation Editing Output Calibration Feedback Modeling Score-based Verbal-based t o i - T Test-time Reasoning (4) Repeated Sampling Test-time Strategies Self-correction Tree Search Bradley and Terry (1952), ORM (Cobbe et al., 2021), PAV (Setlur et al., 2024), PRM (Lightman et al., 2024), OmegaPRM (Luo et al., 2024), Ye et al. (2024), GenRM (Zhang et al., 2024d), CriticRM (Yu et al., 2024); etc. Liu et al. (2023), LLM-as-a-Judge (Zheng et al., 2023), Auto-J (Li et al., 2024a), Prometheus (Kim et al., 2024b,c), Fennec (Liang et al., 2024c); etc. SC-CoT (Wang et al., 2023d), PROVE (Toh et al., 2024), Cobbe et al. (2021), DiVeRSe (Li et al., 2023c), PRS (Ye and Ng, 2024), Zhang et al. (2024e); etc. Improvement training: ReST (Gulcehre et al., 2023), vBoN (Amini et al., 2024), BoNBoN(Gui et al., 2024), BOND (Sessa et al., 2024), Chow et al. (2024); etc. Self-debug (Chen et al., 2024c), RIC (Kim et al., 2023), Critic (Gou et al., 2024), Shepherd (Wang et al., 2023b), MAD (Liang et al., 2024b), IoE (Li et al., 2024c), Refiner (Paul et al., 2024), Reflexion (Shinn et al., 2023), Du et al. (2023); etc. Improvement training: GLoRe (Havrilla et al., 2024), SCoRe(Kumar et al., 2024), Self-correct (Welleck et al., 2023), Qu et al. (2024), Zhang et al. (2024g); etc. ToT (Yao et al., 2023), RAP (Hao et al., 2023), rStar (Qi et al., 2024), TS-LLM (Feng et al., 2024b), AlphaMATH (Chen et al., 2024a); etc. Improvement training: ReST-MCTS* (Zhang et al., 2024a), Qin et al. (2024b), MCTS-DPO (Xie et al., 2024), Zhao et al. (2024b), Zhang et al. (2024f); etc. Generalization Jia (2024), GRM (Yang et al., 2024), DogeRM (Lin et al., 2024b), Weak-to-strong (Burns et al., 2023); etc. Future Directions (5) Multi-modal MM-CoT (Zhang et al., 2024h), VoT (Wu et al., 2024b), Lee et al. (2024), LLaVA-CoT (Xu et al., 2024); etc. Efficient Damani et al. (2024), OSCA (Zhang et al., 2024c), Wang et al. (2024a), CCoT (Cheng and Durme, 2024); etc. Scaling Law Brown et al. (2024), Snell et al. (2024), Wu et al. (2024c), Chen et al. (2024d); etc. Combination Marco-o1 (Zhao et al., 2024b), TTT (Akyürek et al., 2024), HiAR-ICL (Wu et al., 2024a); etc. Figure 2: Taxonomy of test-time computing methods and future directions. ence stage, enabling the model to adapt to the test distribution. The key lies in how to obtain information about the test samples to provide learning signals and how to select appropriate parameters and optimization algorithms to achieve efficient and stable updates. In the inference stage, Learning signal the ground-truth of test samples is unavailable. Thus many works attempt to design unsupervised or selfsupervised objectives as learning signals. Existing learning signals can be classified into two categories based on whether the training process can test-time training (TTT) and fully be modified: test-time adaptation (FTTA). TTT assumes users can modify the training process by incorporating distribution-shift-aware auxiliary tasks. During test-time adaptation, the auxiliary task loss serves as the learning signal for optimization. Many selfsupervised tasks have been shown to be effective as auxiliary tasks in image modality, such as rotation prediction (Sun et al., 2020), meta learning (Bartler et al., 2022), masked autoencoding (Gandelsman et al., 2022) and contrastive learning (Liu et al., 2021; Chen et al., 2022). Among them, contrast learning has been successfully applied to test-time adaptation for visual-language tasks due to its generalization of self-supervised learning within and across modalities (Zhu et al., 2024). In contrast, FTTA is free from accessing the training process and instead uses internal or external feedback on test samples as learning signals. Uncertainty is the most commonly learned signal, driven by the motivation that when test samples shift from the training distribution, the models confidence in its predictions is lower, resulting in higher uncertainty. Tent (Wang et al., 2021) uses the entropy of model predictions as measure of 3 uncertainty and updates the model by minimizing the entropy. MEMO (Zhang et al., 2022a) augments the data for single test sample and then minimizes its marginal entropy, which is more stable compared to Tent in the single-sample TTA setting. However, minimizing entropy also has pitfalls, as blindly reducing prediction uncertainty may cause the model to collapse and make trivial predictions (Press et al., 2024; Zhao et al., 2023; Su et al., 2023a). Some works propose new regularization terms for minimizing entropy to avoid model collapse, including Kullback-Leibler divergence (Su et al., 2023a), moment matching (Hassan et al., 2023) and entropy matching (Bar et al., 2024). For specific tasks, small amount of human feedback or external model rewards can also serve as high-quality learning signals. Gao et al. (2022) and Li et al. (2022b) utilize user feedback to adapt the QA model. Zhan et al. (2023) apply test-time adaptation to multilingual machine translation tasks by using COMET (Rei et al., 2020) for evaluating translation quality. In cross-modal tasks such as image-text retrieval and image captioning, RLCF (Zhao et al., 2024a) demonstrates its effectiveness by using CLIP scores as TTA signals. Updating parameters To advance the application of TTA in real-world scenarios, researchers must address challenges of efficiency and stability. To improve efficiency, many methods only fine-tune small subset of parameters, such as normalization layers (Schneider et al., 2020; Su et al., 2023b), soft prompt (Shu et al., 2022; Hassan et al., 2023; MA et al., 2023; Feng et al., 2023; Niu et al., 2024), low-rank adaptation module (Imam et al., 2024), adapter module (Muhtar et al., 2024; Su et al., 2023a) and cross-modality projector (Zhao et al., 2024a). Although the number of parameters to fine-tune is reduced, TTA still requires an additional backward propagation. Typically, the time cost of backward propagation is approximately twice that of forward propagation. Thus, Niu et al. (2024) propose FOA, which is free from backward propagation by adapting soft prompt through covariance matrix adaptation evolution strategy. The stability of TTA is primarily shown in two aspects. On the one hand, unsupervised or selfsupervised learning signals inevitably introduce noise into the optimization process, resulting in TTA optimizing the model in the incorrect gradient direction. To address this, Niu et al. (2023) and Gong et al. (2024) propose noise data filtering strategies and the robust sharpness-aware optimizer. On the other hand, in real-world scenarios, the distribution of test samples may continually shift, but continual TTA optimization may lead to catastrophic forgetting of the models original knowledge. Episodic TTA (Wang et al., 2021; Shu et al., 2022; Zhao et al., 2024a) is setting to avoid forgetting, which resets the model parameters to their original state after TTA on single test sample. However, Episodic TTA frequently loads the original model, leading to higher inference latency and also limiting the models incremental learning capability. To overcome the dilemma, common trick is the exponential moving average (Wortsman et al., 2022; Ye et al., 2022), which incorporates information from previous model states."
        },
        {
            "title": "3.2 Modifying the Input",
            "content": "When it comes to LLM, the large number of parameters makes model update-based TTA methods face tougher dilemma of efficiency and stability. As result, input-modification-based methods, which do not rely on parameter updates, have become the mainstream method for TTA in LLMs. The effectiveness of input-modified TTA stems from the in-context learning (ICL) capability of LLM, which can significantly improve the performance by adding some demonstrations before the test sample. ICL is highly sensitive to the selection and order of demonstrations. Therefore, the core objective of input-modification TTA is to select appropriate demonstrations for the test samples and arrange them in the optimal order to maximize the effectiveness of ICL. First, empirical studies (Liu et al., 2022) show that the closer the demonstrations are to the test sample, the better the ICL performance. Therefore, retrieval models like BM25 and SentenceBERT are used to retrieve demonstrations semantically closest to the test sample and rank them in descending order of similarity (Qin et al., 2024a; Luo et al., 2023a). To improve the accuracy of demonstration retrieval, Rubin et al. (2022) and Li et al. (2023b) specifically train the demonstration retriever by contrastive learning. Then, as researchers delve deeper into the mechanisms of ICL, ICL is considered to conduct implicit gradient descent on the demonstrations (Dai et al., 2023). Therefore, from the perspective of training data, demonstrations also need to be informative and diverse (Su et al., 2022; Li and Qiu, 2023). Wang et al. (2023c) view language models as topic models and formulate 4 the demonstration selection problem as solving Bayesian optimal classifier. Additionally, the ordering of examples is another important area for improvement. Lu et al. (2022) and Wu et al. (2023) use information theory as guide to select the examples with maximum local entropy and minimum description length for ranking, respectively. Scarlatos and Lan (2024) and Zhang et al. (2022b) consider the sequential dependency among demonstrations, and model it as sequential decision problem and optimize demonstration selection and ordering through reinforcement learning. Another line of work (Chen et al., 2023; Lyu et al., 2023; Kim et al., 2022; Zhang et al., 2023b) argues that in practice, combining limited set of externally provided examples may not always be the optimal choice. LLMs can leverage their generative and annotation capabilities to create better demonstrations. DAIL (Su et al., 2024) constructs demonstration memory, storing previous test samples and their predictions as candidate demonstrations for subsequent samples. DAWN-ICL (Tang et al., 2024a) further models the traversal order of test samples as planning task and optimizes it by the Monte Carlo tree search (MCTS)."
        },
        {
            "title": "3.3 Editing the Representation",
            "content": "For generative LLMs, some works have found that the performance bottleneck is not in encoding world knowledge, but in the large gap between the information in intermediate layers and the output. During the inference phase, editing the representation can help externalize the intermediate knowledge into the output. PPLM (Dathathri et al., 2020) performs gradient-based representation editing under the guidance of small language model to control the style of outputs. ActAdd (Turner et al., 2024) selects two semantically contrastive prompts and calculates the difference between their representations as steering vector, which is then added to the residual stream. Representation editing based on contrastive prompts has demonstrated its effectiveness in broader scenarios, including instruction following (Stolfo et al., 2024), alleviating hallucinations (Li et al., 2023a; Arditi et al., 2024), reducing toxicity (Liu et al., 2024a; Lu and Rimsky, 2024) and personality (Cao et al., 2024). SEA (Qiu et al., 2024b) projects representations onto directions with maximum covariance with positive prompts and minimum covariance with negative prompts. They also introduce nonlinear feature transformations, allowing representation editing to go beyond linearly separable representations. Scalena et al. (2024) conduct an in-depth study on the selection of steering intensity. They find that applying gradually decreasing steering intensity to each output token can improve control over the generation without compromising quality."
        },
        {
            "title": "3.4 Calibrating the Output",
            "content": "Using external information to calibrate the models output distribution is also an efficient yet effective test-time adaptation method (Khandelwal et al., 2020). AdaNPC (Zhang et al., 2023a) designs memory pool to store training data. During inference, given test sample, AdaNPC recalls samples from the memory pool and uses kNN classifier to predict the test sample. It then stores the test sample and its predicted label in the memory pool. Over time, the sample distribution in the memory pool gradually aligns with the test distribution. In NLP, the most representative application of such methods is kNN machine translation (kNN-MT). kNN-MT (Khandelwal et al., 2021) constructs datastore to store contextual representations and their corresponding target tokens. During translation inference, it retrieves the k-nearest candidate tokens from the datastore based on the decoded context and processes them into probabilities. Finally, it calibrates the translation models probability distribution by performing weighted fusion of the models probabilities and the retrieved probabilities. kNN-MT has demonstrated superior transferability and generalization compared to traditional models in cross-domain and multilingual MT tasks. Subsequent studies have focused on improving its performance and efficiency (Wang et al., 2022a; Zhu et al., 2023b; You et al., 2024) or applying its methods to other NLP tasks (Wang et al., 2022b; Bhardwaj et al., 2023)."
        },
        {
            "title": "Thinking",
            "content": "Test-time reasoning aims to spend more inference time to search for the most human-like reasoning process within the vast decoding search space. In this section, we introduce the two core components of test-time reasoning: feedback modeling and search strategies (as shown in Figure 3)."
        },
        {
            "title": "4.1 Feedback Modeling",
            "content": "Score-based Feedback Score-based feedback, also known as the verifier, aims to score generated results, evaluating their alignment with ground 5 Figure 3: Illustration of feedback modeling, search strategies and improvement training in test-time reasoning. truth or human cognitive processes. Its training process is typically similar to the reward model in RLHF, using various forms of feedback signals and modeling it as classification (Cobbe et al., 2021) or rank task (Bradley and Terry, 1952; Yuan et al., 2024a; Hosseini et al., 2024). In reasoning tasks, verifiers are mainly divided into two categories: outcome-based (ORMs) and process-based verifiers (PRMs). ORMs (Cobbe et al., 2021) use the correctness of the final CoT result as training feedback, while PRMs (Uesato et al., 2022; Lightman et al., 2024) are trained based on feedback from each reasoning step. PRM not only evaluates intermediate reasoning steps but also evaluates the entire reasoning process more accurately than ORM. However, PRM requires more human effort to annotate feedback for the intermediate steps. Math-Shepherd (Wang et al., 2024e) and OmegaPRM (Luo et al., 2024) utilize MCTS algorithm to collect high-quality process supervision data automatically. Setlur et al. (2024) argue that PRM should evaluate the advantage of each step for subsequent reasoning rather than focusing solely on its correctness. They propose process advantage verifiers (PAVs) and efficiently construct training data through Monte Carlo simulations. Score-based feedback modeling overlooks the generative capabilities of LLMs, making it difficult to detect fine-grained errors. Thus, recent works propose generative score-based verifiers (Ankner et al., 2024; Ye et al., 2024). GenRM (Zhang et al., 2024d) leverages instruction tuning to enable the verifier to answer Is the answer correct (Yes/No)? and uses the probability of generated Yes token as the score. GenRM can also incorporate CoT, allowing the verifier to generate the corresponding rationale before answering Yes or No. CriticRM (Yu et al., 2024) jointly trains the critique model and the verifier. During inference, the verifier scores according to answers and verbal-based feedback generated by the critique model. Verbal-based Feedback Although the verifier can accurately evaluate the correctness of generated answers or steps, it lacks interpretability, making it unable to locate the specific cause of errors or provide correction suggestions. Verbal-based feedback, also referred to critic, fully leverages the LLMs instruction-following ability. By designing specific instructions, it can perform pairwise comparisons, evaluate answers from multiple dimensions, and even provide suggestions for revision in natural language. Powerful closed-source LLMs, such as GPT-4 and Claude, are effective critics. They can perform detailed and controlled assessments of generated texts, such as factuality, logical errors, coherence, and alignment, with high consistency with human evaluations (Wang et al., 2023a; Luo et al., 2023b; Liu et al., 2023; Chiang and Lee, 2023). However, they still face biases such as length, position, and perplexity (Bavaresco et al., 2024; Wang et al., 2024d; Stureborg et al., 2024). LLM-as-a-Judge (Zheng et al., 2023) carefully designs system instructions to mitigate the interference of biases. To obtain cheaper verbal-based feedback, opensource LLMs can also serve as competitive alternatives through supervised fine-tuning (SFT) (Wang et al., 2024f; Zhu et al., 2023a; Liang et al., 2024c; Paul et al., 2024). Shepherd (Wang et al., 2023b) collects high-quality training data from human annotation and online communities to fine-tune an evaluation model. Auto-J (Li et al., 2024a) collects queries and responses from various scenarios and designs evaluation criteria for each scenario. GPT4 then generates critiques of the responses based on these criteria and distills its critique ability to opensource LLMs. Prometheus (Kim et al., 2024b,c) designs more fine-grained evaluation dimensions. 6 It trains single evaluation model and pairwise ranking model separately, then unifies them into one LLM by weight merging. Fennec (Liang et al., 2024c) allows GPT-4 to determine the evaluation criteria for each query, and generate corresponding verbal feedback. Compared to previous work, Fennecs evaluation criteria are more flexible, the generated evaluation data is more diverse, and it aligns better with human behavior."
        },
        {
            "title": "4.2.1 Repeated Sampling",
            "content": "Sampling strategies such as top-p and top-k are commonly used decoding algorithms in LLM inference. They introduce randomness during decoding to enhance text diversity, allowing for parallelly sampling multiple generated texts. Through repeated sampling, we have more opportunities to find the correct answer. Repeated sampling is particularly suitable for tasks that can be automatically verified, such as code generation, where we can easily identify the correct solution from multiple samples using unit tests (Li et al., 2022a; Rozière et al., 2024). For tasks that are difficult to verify, like math word problems, the key to the effectiveness of repeated sampling is the verification strategy. Verification strategy Verification strategies include two types: majority voting and best-of-N (BoN) sampling. Majority voting (Li et al., 2024b; Lin et al., 2024a) selects the most frequently occurring answer in the samples as the final answer, which is motivated by ensemble learning. Majority voting is simple yet effective. For instance, self-consistency CoT (Wang et al., 2023d) can improve accuracy by 18% over vanilla CoT in math reasoning tasks. However, the majority does not always hold the truth, as they may make similar mistakes. Therefore, some studies perform validation and filtering before voting. For example, the PROVE framework (Toh et al., 2024) converts CoT into executable programs, filtering out samples if the programs results are inconsistent with the reasoning chains outcomes. Best-of-N sampling uses verifier to score each generated result and selects the one with the highest score as the final answer (Stiennon et al., 2020; Cobbe et al., 2021; Nakano et al., 2022). Li et al. (2023c) propose voting-based BoN variant, which performs weighted voting on all answers based on the verifiers scores and selects the answer with the highest weight. In addition, some works aim to improve the efficiency of BoN. Inspired by speculative decoding, Zhang et al. (2024e); Qiu et al. (2024a); Sun et al. (2024) and Manvi et al. (2024) evaluate each reasoning step by an efficient verifier. They prune low-scoring sampled results, halting further generation for those paths, thereby significantly reducing the overall time cost. PRS (Ye and Ng, 2024) enables LLMs to self-critique and selfcorrect, guiding the model to generate expected responses with fewer sampling times. Improvement Training Repeated sampling, especially the BoN strategy, has proven to be simple yet effective method in many studies, even can surpassing models fine-tuned with RLHF (Gao et al., 2023a). However, it comes at the cost of inference times that are difficult to afford in practical applications. Therefore, many studies have attempted to train the model by BoN sampling to approximate the BoN distribution, thereby reducing the search space during inference. ReST (Gulcehre et al., 2023) samples responses with reward values above threshold from the policy model as self-training data and fine-tune the policy model by offline reinforcement learning. In each iteration, ReST samples new training data. vBoN (Amini et al., 2024), BoNBoN (Gui et al., 2024) and BOND (Sessa et al., 2024) derive the BoN distribution and minimize the difference between the policy models distribution and the BoN distribution. Chow et al. (2024) design BoN-aware loss to make the policy model more exploratory during fine-tuning."
        },
        {
            "title": "4.2.2 Self-correction",
            "content": "Self-correction is sequential test-time computation method that enables LLMs to iteratively revise and refine generated results based on external or internal feedback (Shinn et al., 2023). Feedback sources The feedback used for selfcorrection is typically presented in natural language and comes from various sources, including human evaluation, tool checking, external model evaluation, and intrinsic feedback. Human evaluation is the gold standard for feedback, but due to its high cost and limited scalability, it is mainly used in early research to explore the upper limits of selfcorrection capabilities (Tandon et al., 2021; Elgohary et al., 2021; Tandon et al., 2022). For certain domain-specific tasks, external tool checking provides accurate and efficient feedback (Gou et al., 2024; Chen et al., 2024c; Gao et al., 2023b). For example, Yasunaga and Liang (2020) propose to 7 Category sub-category Representative Methods Tasks Verifier/Critic Train-free Repeat Sampling Self-correction Majority voting Best-of-N Human feedback External tools External models CoT-SC (2023d) PROVE (2024) Cobbe et al. (2021) DiVeRSe (2023c) NL-EDIT (2021) FBNET (2022) DrRepair (2020) Self-debug (2024c) CRITIC (2024) Math, QA Math Math Math Semantic parsing Code Code Code Math, QA, Detoxifying REFINER (2024) Shepherd (2023b) Multiagent Debate (2023) Math, Reason MAD (2024b) Math, Reason QA Translation, Math self-consistency compiler ORM PRM Human Human compiler compiler text-to-text APIs critic model critic model multi-agent debate multi-agent debate Intrinsic feedback Uninformed search Self-Refine (2023) Reflexion (2023) RCI (2023) ToT (2023) Xie et al. (2023) Math, Code, Controlled generation QA Code, QA Planing, Creative writing Math self-critique self-critique self-critique self-critique self-critique Tree Search Heuristic search RAP (2023) TS-LLM (2024b) rStar (2024) ReST-MCTS* (2024a) Planing, Math, Logical Planing, Math, Logical Math, QA Math, QA self-critique ORM multi-agent consistency PRM Table 1: Overview of search strategies. obtain feedback from compilers in code repair and generation tasks. External model evaluation is an effective feedback source for general tasks, such as various verbal-based critique models described in Section 4.1. For example, Paul et al. (2024) first define multiple error types for natural language reasoning tasks and then design the corresponding feedback templates. They train an evaluation model using synthetic feedback training data, and with the critic, the reasoning model achieves substantial performance improvement. Multi-agent debate (Du et al., 2023; Xiong et al., 2023; Liang et al., 2024b; Chen et al., 2024b; Wang et al., 2024c) is another mechanism that leverages external feedback to enhance reasoning capabilities. In this approach, models do not have distinct roles as reasoners and critics. Instead, multiple models independently conduct reasoning, critique each other, and defend or refine their reasoning based on feedback. This process continues until agents reach consensus or judge model summarizes the final reasoning results. The multi-agent debate has shown its potential in factchecking (Kim et al., 2024a; Khan et al., 2024), commonsense QA (Xiong et al., 2023), faithful evaluations (Chan et al., 2024), and complex reasoning (Du et al., 2023; Cheng et al., 2024). However, multi-agent debate may be unstable, as LLMs are susceptible to adversarial information and may revise correct answers to incorrect ones in response to misleading inputs (Laban et al., 2024; Amayuelas et al., 2024). Therefore, successful multiagent debate requires that LLMs maintain their stance when faced with incorrect answers from other models while remaining open to valid suggestions (Stengel-Eskin et al., 2024). In general, the more LLMs involved in the debate, the stronger the overall reasoning performance. However, this significantly increases the number of LLM inferences required, and the length of input context, posing major challenge to LLM inference costs (Liu et al., 2024b). To reduce debate inference costs, Li et al. (2024d) investigate the impact of topological connections among multiple agents and show that sparse connections, such as ring structures, are not inferior to the fully connected topology. GroupDebate (Liu et al., 2024b) divides LLMs into groups that conduct debates internally and only share the consensus results between groups. Self-critique assumes that LLMs can selfevaluate their outputs and optimize them through intrinsic feedback (Yuan et al., 2024b). This idea stems from fundamental principle in computational complexity theory: verifying whether solution is correct is typically easier than solving the problem. Bai et al. (2022) propose self-correcting harmful responses from LLMs by prompting themselves. Self-Refine (Madaan et al., 2023) and RCI Prompting (Kim et al., 2023) iteratively prompt LLMs to self-correct their responses in tasks such 8 as arithmetic reasoning. IoE (Li et al., 2024c) observes that LLMs may over-criticize themselves during self-critique, leading to performance degradation, and designs prompt to guide LLMs in assessing confidence. et al., 2023) adopts online imitation learning, resampling new self-correction trajectories for training after each training epoch. SCoRe (Kumar et al., 2024) proposes using the multi-turn RL method to improve self-critique and self-correction capability. Arguments The effectiveness of self-correction has remained controversial. Several empirical studies on code generation (Olausson et al., 2024), commonsense QA (Huang et al., 2024), math problemsolving (Wang et al., 2024b), planning (Valmeekam et al., 2023), and graph coloring (Stechly et al., 2023) confirm that self-correction is not guaranteed solution for improving performance. Kamoi et al. (2024) think the effectiveness of selfcorrection has been overestimated. Previous successes either rely on oracle answers or weak initial answers. Only tasks that can be broken down into easily verifiable sub-tasks can truly benefit from self-correction. They suggest fine-tuning specific evaluation models to achieve better self-correction. Tyen et al. (2024) decouple the abilities of LLMs to identify and correct errors and create the corresponding evaluation datasets. The evaluation results show that LLMs do not lack the ability to correct errors during self-correction, and their main performance bottleneck lies in locating the errors. Improvement Training Most of the aforementioned self-correction methods demonstrate significant performance improvements on advanced closed-source large models or open-source LLMs with over 70B parameters. However, for mediumscale open-source models with weaker capabilities, we need to further fine-tune them to unlock their self-correction capabilities. Supervised fine-tuning optimizes the model using high-quality multi-turn correction data, either manually annotated (Saunders et al., 2022) or sampled from stronger LLMs (Paul et al., 2024; Qu et al., 2024; Gao et al., 2024b; Zhang et al., 2024g; Xi et al., 2024). GLoRe (Havrilla et al., 2024) considers that LLMs need global or local refinement for different types of errors. To address this, they construct training sets for global and local refinement, train verifiers to identify global and local errors, and develop LLMs for refinement based on different global or local feedback signals. Although SFT is effective, training data from offline-generated self-correction trajectories can only simulate limited correction patterns. This leads to the distribution mismatch with the actual self-correction behavior during model inference. Self-correct (Welleck"
        },
        {
            "title": "4.2.3 Tree Searching",
            "content": "Repeated sampling and self-correction scale testtime computation in parallel and sequentially, respectively. Human thinking is tree search that combines brainstorming in parallel with backtracking to find other paths to solutions when it encounters dead end. Search algorithms and value functions are two critical components in tree searching. Search algorithm In LLM reasoning, current search algorithms include uninformed search and heuristic search. Uninformed search does not rely on specific heuristic information but explores the search space according to fixed rule. For example, tree-of-thought (ToT) (Yao et al., 2023) adopts the BFS or DFS to search, while Xie et al. (2023) use beam search. Uninformed search is usually less efficient for problems with large search spaces, so heuristic search represented by MCTS is widely used in reasoning tasks (Hao et al., 2023; Zhang et al., 2024b; Bi et al., 2024). MCTS gradually optimizes search results through four steps: selection, expansion, simulation, and backpropagation, approaching the optimal solution. Long (2023) trains an LLM controller using reinforcement learning to guide the LLM reasoners search path. Value function The value function evaluates the value of each action and guides the tree to expand towards branches with higher values in MCTS. RAP (Hao et al., 2023) designs series of heuristic value functions, including the likelihood of the action, the confidence of the state, self-evaluation results, and task-specific reward, and combines them according to task requirements. Reliable and generalized value functions facilitate the application of MCTS to more complex problems with deeper search spaces. AlphaMath (Chen et al., 2024a) and TS-LLM (Feng et al., 2024b) replace the handcrafted value function with learned LLM value function, automatically generating reasoning process and step-level evaluation signals in MCTS. Traditional MCTS methods expand only one trajectory, while rStar (Qi et al., 2024) argues that the current value function struggles to guide the selection of the optimal path accurately. Therefore, rStar retains multiple candidate paths and performs 9 reasoning with another LLM, ultimately selecting the path where both LLMs reasoning results are consistent. Gao et al. (2024c) propose SC-MCTS inspired by contrast decoding, which utilizes multiple external reward models as value functions. Improvement Training Tree search can guide LLMs to generate long reasoning processes, and these data help train LLMs with stronger reasoning abilities. ReST-MCTS* (Zhang et al., 2024a) uses process rewards as value function to guide MCTS, collecting high-quality reasoning trajectories and the value of each step to improve the policy model and reward model. Due to the step-by-step exploration of tree search, it can obtain finer-grained step-level feedback signals. MCTS-DPO (Xie et al., 2024) collects step-level preference data through MCTS and uses DPO for preference learning. Recently, many o1-like technical reports (Qin et al., 2024b; Zhao et al., 2024b; Zhang et al., 2024f) have also confirmed the necessity of using tree search to construct high-quality long reasoning chain data for training."
        },
        {
            "title": "5.1 Generalizable System-2 Model",
            "content": "Currently, most o1-like models exhibit strong deep reasoning abilities only in specific domains such as math and code and struggle to adapt to crossdomain or general tasks. The key to addressing this issue lies in enhancing the generalization ability of verifiers or critics. Currently, some works utilize model ensemble (Lin et al., 2024b) or regularization constraints (Yang et al., 2024; Jia, 2024) to make verifiers more generalizable. Nevertheless, there is still significant room for improvement in the generalization of the verifier. Additionally, weak-to-strong generalization (Burns et al., 2023) is topic worth further exploration. People are no longer satisfied with solving mathematical problems with standard answers; they hope System-2 models can assist in scientific discovery and the proofs of mathematical conjectures. In such cases, even human experts struggle to provide accurate feedback, while weak-to-strong generalization offers promising direction to address this issue. ing (Zhao et al., 2024a). However, test-time computing methods in System-2 thinking remain limited to text modalities. Visual, speech, and other modalities are crucial for model understanding and interaction with the world. To achieve cognitive intelligence, System-2 models must be able to fully integrate multimodal information for reasoning. The exploration of multimodal CoT (Zhang et al., 2024h; Wu et al., 2024b; Mondal et al., 2024; Lee et al., 2024; Gao et al., 2024a) opens up the possibility of building multimodal System-2 models. Xu et al. (2024) are the first to apply test-time computing to visual reasoning tasks. They divide the visual reasoning process into four stages: task summary, caption, reasoning, and answer conclusion. They propose stage-level beam search method, which repeatedly samples at each stage and selects the best result to the next stage. We believe test-time computing still holds significant potential for development in multimodal reasoning. For example, incorporating more modalities like speech and video into reasoning tasks, applying successful methods such as reflection mechanisms and tree search to multimodal reasoning, or aligning the multimodal reasoning process with human cognitive processes."
        },
        {
            "title": "5.3 Efficiency and Performance Trade-off",
            "content": "The successful application of test-time computing shows that sacrificing reasoning efficiency can lead to better reasoning performance. However, researchers continue to seek balance between performance and efficiency, aiming to achieve optimal performance under fixed reasoning latency budget. This requires adaptively allocating computational resources for each sample. Damani et al. (2024) train lightweight module to predict the difficulty of question, and allocate computational resources according to its difficulty. Zhang et al. (2024c) further extend the allocation targets to more hyperparameters. There are still many open questions worth exploring, such as how to integrate inference acceleration strategies, e.g. KV cache compression, token pruning, and speculative decoding with test-time computing and how to predict problem difficulty more accurately."
        },
        {
            "title": "5.2 Multimodal Reasoning",
            "content": "In System-1 thinking, TTA has been successfully applied to multimodal LLMs, improving performance in tasks such as zero-shot image classification, image-text retrieval, and image captionUnlike training-time computation scaling, test-time computing still lacks universal scaling law. Some works have attempted to derive scaling laws for specific test-time computing strategies (Wu et al., 2024c; Chen et al., 2024d). Brown et al. (2024) 10 demonstrate that the performance has an approximately log-linear relationship with repeated sampling times. Snell et al. (2024) investigate the scaling laws of repeated sampling and self-correction, and propose the computing-optimal scaling strategy. There are two major challenges to achieving universal scaling law: first, current test-time computation strategies are various, each with different mechanisms to steer the model; thus, it lacks universal framework for describing them; second, the performance of test-time computation is affected by variety of factors, including the difficulty of samples, the accuracy of feedback signals, and decoding hyperparameters, and we need empirical studies to filter out the critical factors."
        },
        {
            "title": "5.5 Strategy Combination",
            "content": "Different test-time computing strategies are suited to various tasks and scenarios, so combining multiple strategies is one way to achieve better System2 thinking. For example, Marco-o1 (Zhao et al., 2024b) combines the MCTS and self-correction, using MCTS to plan reasoning processes, and self-correction to improve the accuracy of each step. Moreover, test-time adaptation strategies in System-1 models can also be combined with testtime reasoning strategies. Akyürek et al. (2024) combine test-time training with repeated sampling. They further optimize the language modeling loss on test samples, then generate multiple candidate answers through data augmentation, and finally determine the answer by majority voting. They demonstrate the potential of test-time training in reasoning tasks, surpassing the human average on the ARC challenge. Therefore, we think that for LLM reasoning, it is crucial to focus not only on emerging test-time strategies but also on testtime adaptation methods. By effectively combining these strategies, we can develop System-2 models that achieve or surpass o1-level performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we conduct comprehensive survey of existing works on test-time computing. We introduce various test-time computing methods in System-1 and System-2 models, and look forward to future directions for this field. We believe testtime computing can help models handle complex real-world distributions and tasks better, making it promising path for advancing LLMs toward cognitive intelligence. We hope this paper will promote further research in this area."
        },
        {
            "title": "Limitations",
            "content": "Test-time computing, especially the strategies in System-2, is evolving rapidly. While we have made efforts to provide comprehensive survey of existing research, it is challenging to cover all the latest developments. This review includes papers up to November 2024, with more recent advancements to be updated in future versions. TTA has seen many successful applications and task-specific strategies in CV tasks. Since the primary audience of our paper is researchers in NLP, we do not systematically present these works, and interested readers can refer to Liang et al. (2024a) for details."
        },
        {
            "title": "References",
            "content": "Ekin Akyürek, Mehul Damani, Linlu Qiu, Han Guo, Yoon Kim, and Jacob Andreas. 2024. The surprising effectiveness of test-time training for abstract reasoning. Preprint, arXiv:2411.07279. Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua, Liangming Pan, and William Wang. 2024. Multiagent collaboration attack: Investigating adversarial attacks in large language model collaborations via debate. Preprint, arXiv:2406.14711. Afra Amini, Tim Vieira, and Ryan Cotterell. 2024. Variational best-of-n alignment. Preprint, arXiv:2407.06057. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, and Prithviraj Ammanabrolu. 2024. Critique-out-loud reward models. Preprint, arXiv:2408.11791. Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. Refusal in language models is mediated by single direction. Preprint, arXiv:2406.11717. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback. Preprint, arXiv:2212.08073. 11 Yarin Bar, Shalev Shaer, and Yaniv Romano. 2024. Protected test-time adaptation via online entropy matching: betting approach. Preprint, arXiv:2408.07511. Alexander Bartler, Andre Bühler, Felix Wiewel, Mario Döbler, and Bin Yang. 2022. Mt3: Meta test-time training for self-supervised test-time adaption. In International Conference on Artificial Intelligence and Statistics, pages 30803090. PMLR. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya Surikuchi, Ece Takmaz, and Alberto Testoni. 2024. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks. Preprint, arXiv:2406.18403. Rishabh Bhardwaj, Yingting Li, Navonil Majumder, Bo Cheng, and Soujanya Poria. 2023. kNN-CM: non-parametric inference-phase adaptation of parametric text classifiers. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1354613557, Singapore. Association for Computational Linguistics. Zhenni Bi, Kai Han, Chuanjian Liu, Yehui Tang, and Yunhe Wang. 2024. Forest-of-thought: Scaling testtime compute for enhancing llm reasoning. Preprint, arXiv:2412.09078. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. Preprint, arXiv:2407.21787. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeff Wu. 2023. Weak-tostrong generalization: Eliciting strong capabilities with weak supervision. Preprint, arXiv:2312.09390. Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, and Jinghui Chen. 2024. Personalized steering of large language models: Versatile steering vectors through bi-directional preference optimization. Preprint, arXiv:2406.00045. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. Chateval: Towards better LLM-based evaluators through multi-agent debate. In The Twelfth International Conference on Learning Representations. Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. 2022. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295 305. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024a. Alphamath almost zero: Process supervision without process. Preprint, arXiv:2405.03553. Justin Chen, Swarnadeep Saha, and Mohit Bansal. 2024b. ReConcile: Round-table conference improves reasoning via consensus among diverse LLMs. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 70667085, Bangkok, Thailand. Association for Computational Linguistics. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen. 2023. Self-ICL: Zero-shot in-context learning with self-generated demonstrations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15651 15662, Singapore. Association for Computational Linguistics. Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2024c. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations. Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. 2024d. simple and provable scaling law for the test-time compute of large language models. Preprint, arXiv:2411.19477. Jeffrey Cheng and Benjamin Van Durme. 2024. Compressed chain of thought: Efficient reasoning through dense representations. Preprint, arXiv:2412.13171. Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. ChainLM: Empowering large language models with improved chain-of-thought prompting. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 29692983, Torino, Italia. ELRA and ICCL. Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada. Association for Computational Linguistics. Yinlam Chow, Guy Tennenholtz, Izzeddin Gur, Vincent Zhuang, Bo Dai, Sridhar Thiagarajan, Craig Boutilier, Rishabh Agarwal, Aviral Kumar, and Aleksandra 12 Faust. 2024. Inference-aware fine-tuning for bestof-n sampling in large language models. Preprint, arXiv:2412.15287. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. Preprint, arXiv:2110.14168. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 40054019, Toronto, Canada. Association for Computational Linguistics. Mehul Damani, Idan Shenfeld, Andi Peng, Andreea Bobu, and Jacob Andreas. 2024. Learning how hard to think: Input-adaptive allocation of lm computation. Preprint, arXiv:2410.04707. Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2020. Plug and play language models: simple approach to controlled text generation. In International Conference on Learning Representations. Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. Preprint, arXiv:2305.14325. Ahmed Elgohary, Christopher Meek, Matthew Richardson, Adam Fourney, Gonzalo Ramos, and Ahmed Hassan Awadallah. 2021. NL-EDIT: Correcting semantic parse errors through natural In Proceedings of the 2021 language interaction. Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 55995610, Online. Association for Computational Linguistics. Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo. 2023. Diverse data augmentation with diffusions for effective test-time prompt tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 27042714. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. 2024a. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36. Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. 2022. Test-time training with masked autoencoders. In Advances in Neural Information Processing Systems. Ge Gao, Eunsol Choi, and Yoav Artzi. 2022. Simulating bandit learning from user feedback for extractive question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5167 5179, Dublin, Ireland. Association for Computational Linguistics. Jun Gao, Yongqi Li, Ziqiang Cao, and Wenjie Interleaved-modal chain-of-thought. Li. 2024a. Preprint, arXiv:2411.19488. Kuofeng Gao, Huanqia Cai, Qingyao Shuai, Dihong Gong, and Zhifeng Li. 2024b. Embedding self-correction as an inherent ability in large language models for enhanced mathematical reasoning. Preprint, arXiv:2410.10735. Leo Gao, John Schulman, and Jacob Hilton. 2023a. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and Kelvin Guu. 2023b. RARR: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1647716508, Toronto, Canada. Association for Computational Linguistics. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. 2024c. Interpretable contrastive monte carlo tree search reasoning. Preprint, arXiv:2410.01707. Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chottananurak, and Sung-Ju Lee. 2024. Sotta: Robust test-time adaptation on noisy data streams. Advances in Neural Information Processing Systems, 36. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2024. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations. Xinyan Guan, Yanjiang Liu, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. 2024. Mitigating large language model hallucinations via autonomous knowledge graph-based retrofitting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1812618134. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. 2024b. Alphazero-like tree-search can guide large language model decoding and training. Preprint, arXiv:2309.17179. Lin Gui, Cristina Garbacea, and Victor Veitch. 2024. BoNBon alignment for large language models and the sweetness of best-of-n sampling. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando de Freitas. 2023. Reinforced self-training (rest) for language modeling. Preprint, arXiv:2308.08998. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore. Association for Computational Linguistics. Jameel Hassan, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fahad Shahbaz Khan, and Salman Khan. 2023. Align your prompts: test-time prompting with distribution alignment for zero-shot generalization. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 8039680413. Alex Havrilla, Sharath Raparthy, Christoforus Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, and Roberta Raileanu. 2024. Glore: When, where, and how to improve llm reasoning via global and local refinements. Preprint, arXiv:2402.10963. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-STar: Training verifiers for self-taught reasoners. In First Conference on Language Modeling. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2024. Large language In The models cannot self-correct reasoning yet. Twelfth International Conference on Learning Representations. Raza Imam, Hanan Gani, Muhammad Huzaifa, and Karthik Nandakumar. 2024. Test-time low rank adaptation via confidence maximization for zero-shot generalization of vision-language models. Preprint, arXiv:2407.15913. Yixin Ji, Kaixin Wu, Juntao Li, Wei Chen, Mingjie Zhong, Xu Jia, and Min Zhang. 2024. Retrieval and reasoning on KGs: Integrate knowledge graphs into large language models for complex question answerIn Findings of the Association for Computaing. tional Linguistics: EMNLP 2024, pages 75987610, Miami, Florida, USA. Association for Computational Linguistics. Chen Jia. 2024. Generalizing reward modeling for In Joint out-of-distribution preference learning. European Conference on Machine Learning and Knowledge Discovery in Databases, pages 107124. Springer. Daniel Kahneman. 2011. Thinking, fast and slow. Farrar, Straus and Giroux. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui Zhang. 2024. When can llms actually correct their own mistakes? critical survey of selfcorrection of llms. Preprint, arXiv:2406.01297. Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, and Ethan Perez. 2024. Debating with more persuasive LLMs leads to more truthful answers. In Fortyfirst International Conference on Machine Learning. Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In International Conference on Learning Representations. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations. Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. Preprint, arXiv:2303.17491. Hyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as demonstration generator. Preprint, arXiv:2206.08082. Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, and Heng Ji. 2024a. Can llms produce faithful explanations for factchecking? towards faithful explainable fact-checking via multi-agent debate. Preprint, arXiv:2402.07401. Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. 2024b. Prometheus: Inducing finegrained evaluation capability in language models. In The Twelfth International Conference on Learning Representations. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024c. Prometheus 2: An open source language model specialized in evaluating other language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, Miami, Florida, USA. Association for Computational Linguistics. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, 14 Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. 2024. Training language models to self-correct via reinforcement learning. Preprint, arXiv:2409.12917. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. 2022a. Competition-level code generation with alphacode. Science, 378(6624):10921097. Philippe Laban, Lidiya Murakhovska, Caiming Xiong, and Chien-Sheng Wu. 2024. Are you sure? challenging llms leads to performance drops in the flipflop experiment. Preprint, arXiv:2311.08596. Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski, Yeqing Li, and Eugene Ie. 2024d. Improving multi-agent debate with sparse communication topology. Preprint, arXiv:2406.11776. Yann LeCun. 2022. path towards autonomous machine intelligence. Junlin Lee, Yequan Wang, Jing Li, and Min Zhang. 2024. Multimodal reasoning with multimodal knowledge graph. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1076710782, Bangkok, Thailand. Association for Computational Linguistics. Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, hai zhao, and Pengfei Liu. 2024a. Generative judge for evaluating alignment. In The Twelfth International Conference on Learning Representations. Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye. 2024b. More agents is all you need. Preprint, arXiv:2402.05120. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023a. Inferencetime intervention: Eliciting truthful answers from language model. In Thirty-seventh Conference on Neural Information Processing Systems. Loka Li, Zhenhao Chen, Guangyi Chen, Yixuan Zhang, Yusheng Su, Eric Xing, and Kun Zhang. 2024c. Confidence matters: Revisiting intrinsic self-correction capabilities of large language models. Preprint, arXiv:2402.12563. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023b. Unified demonstration retriever for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46444668, Toronto, Canada. Association for Computational Linguistics. Xiaonan Li and Xipeng Qiu. 2023. Finding support examples for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 62196235, Singapore. Association for Computational Linguistics. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2023c. Making language models better reasoners with step-aware In Proceedings of the 61st Annual Meetverifier. ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333, Toronto, Canada. Association for Computational Linguistics. Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. 2024e. Chain of thought empowers transformers to solve inherently serial problems. In The Twelfth International Conference on Learning Representations. Zichao Li, Prakhar Sharma, Xing Han Lu, Jackie Cheung, and Siva Reddy. 2022b. Using interactive feedback to improve the accuracy and explainability of In question answering systems post-deployment. Findings of the Association for Computational Linguistics: ACL 2022, pages 926937, Dublin, Ireland. Association for Computational Linguistics. Jian Liang, Ran He, and Tieniu Tan. 2024a. comprehensive survey on test-time adaptation under distribution shifts. International Journal of Computer Vision, pages 134. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024b. Encouraging divergent thinking in large language models through multi-agent debate. Preprint, arXiv:2305.19118. Xiaobo Liang, Haoke Zhang, Helan hu, Juntao Li, Jun Xu, and Min Zhang. 2024c. Fennec: Finegrained language model evaluation and correction extended through branching and bridging. Preprint, arXiv:2405.12163. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2024. Lets verify step by step. In The Twelfth International Conference on Learning Representations. Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, and Kun Gai. 2024a. Just ask one more time! self-agreement improves reasoning of language In Findings of models in (almost) all scenarios. the Association for Computational Linguistics: ACL 2024, pages 38293852, Bangkok, Thailand. Association for Computational Linguistics. Tzu-Han Lin, Chen-An Li, Hung-yi Lee, and Yun-Nung Chen. 2024b. DogeRM: Equipping reward models with domain knowledge through model merging. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1550615524, Miami, Florida, USA. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What 15 In makes good in-context examples for GPT-3? Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100114, Dublin, Ireland and Online. Association for Computational Linguistics. Sheng Liu, Haotian Ye, Lei Xing, and James Zou. 2024a. In-context vectors: Making in context learning more effective and controllable through latent space steering. Preprint, arXiv:2311.06668. Tongxuan Liu, Xingyu Wang, Weizhe Huang, Wenjiang Xu, Yuting Zeng, Lei Jiang, Hailong Yang, and Jing Li. 2024b. Groupdebate: Enhancing the efficiency of multi-agent debate using group discussion. Preprint, arXiv:2409.14051. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Yuejiang Liu, Parth Kothari, Bastien van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. 2021. Ttt++: When does self-supervised test-time training fail or thrive? Advances in Neural Information Processing Systems, 34:2180821820. Jieyi Long. 2023. Large language model guided tree-ofthought. Preprint, arXiv:2305.08291. Dawn Lu and Nina Rimsky. 2024. Investigating bias representations in llama 2 chat via activation steering. Preprint, arXiv:2402.00402. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80868098, Dublin, Ireland. Association for Computational Linguistics. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. 2024. Improve mathematical reasoning in language models by automated process supervision. Preprint, arXiv:2406.06592. Man Luo, Xin Xu, Zhuyun Dai, Panupong Pasupat, Mehran Kazemi, Chitta Baral, Vaiva Imbrasaite, and Vincent Zhao. 2023a. Dr.icl: Demonstration-retrieved learning. Preprint, arXiv:2305.14128. in-context Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2023b. Chatgpt as factual inconsistency evaluator for text summarization. Preprint, arXiv:2303.15621. Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. Z-ICL: Zero-shot in-context learning with pseudo-demonstrations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 23042317, Toronto, Canada. Association for Computational Linguistics. XIAOSONG MA, Jie ZHANG, Song Guo, and Wenchao Xu. 2023. Swapprompt: Test-time prompt adaptation for vision-language models. In Advances in Neural Information Processing Systems, volume 36, pages 6525265264. Curran Associates, Inc. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems. Rohin Manvi, Anikait Singh, and Stefano Ermon. 2024. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. Preprint, arXiv:2410.02725. Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. 2024. Kamcot: Knowledge augmented multimodal chain-ofthoughts reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1879818806. Dilxat Muhtar, Yelong Shen, Yaming Yang, Xiaodong Liu, Yadong Lu, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Weiwei Deng, Feng Sun, Xueliang Zhang, Jianfeng Gao, Weizhu Chen, and Qi Zhang. 2024. Streamadapter: Efficient test time adaptation from contextual streams. Preprint, arXiv:2411.09289. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. Webgpt: Browserassisted question-answering with human feedback. Preprint, arXiv:2112.09332. Shuaicheng Niu, Chunyan Miao, Guohao Chen, Pengcheng Wu, and Peilin Zhao. 2024. Test-time model adaptation with only forward passes. In Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3829838315. PMLR. Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. 2023. Towards stable test-time adaptation in dyIn The Eleventh International namic wild world. Conference on Learning Representations. Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. 2024. Is self-repair silver bullet for code generation? In The Twelfth International Conference on Learning Representations. OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774. OpenAI. 2024. Learning to reason with llms. Open AI, blog. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2024. REFINER: Reasoning feedback on intermediate representations. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11001126, St. Julians, Malta. Association for Computational Linguistics. Ori Press, Ravid Shwartz-Ziv, Yann LeCun, and Matthias Bethge. 2024. The entropy enigma: Success and failure of entropy minimization. Preprint, arXiv:2405.05012. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. 2024. Mutual reasoning makes smaller llms stronger problem-solvers. Preprint, arXiv:2408.06195. Chengwei Qin, Aston Zhang, Chen Chen, Anirudh Dagar, and Wenming Ye. 2024a. In-context learning with iterative demonstration selection. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 74417455, Miami, Florida, USA. Association for Computational Linguistics. Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. 2024b. O1 replication journey: strategic progress report part 1. Preprint, arXiv:2410.18982. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, and Mengdi Wang. 2024a. Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. Preprint, arXiv:2410.16033. Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo Ponti, and Shay Cohen. 2024b. Spectral editing of activations for large language model alignment. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Yuxiao Qu, Tianjun Zhang, Naman Garg, and Aviral Kumar. 2024. Recursive introspection: Teaching language model agents how to self-improve. Preprint, arXiv:2407.18219. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Open AI, blog. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. Open AI, blog. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner. 2024. Steering llama 2 via contrastive activation addition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1550415522, Bangkok, Thailand. Association for Computational Linguistics. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2024. Code llama: Open foundation models for code. Preprint, arXiv:2308.12950. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 26552671, Seattle, United States. Association for Computational Linguistics. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. Preprint, arXiv:2206.05802. Daniel Scalena, Gabriele Sarti, and Malvina Nissim. 2024. Multi-property steering of large language models with dynamic activation composition. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 577603, Miami, Florida, US. Association for Computational Linguistics. Alexander Scarlatos and Andrew Lan. 2024. Reticl: Sequential retrieval of in-context examples with reinforcement learning. Preprint, arXiv:2305.14502. 17 Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. 2020. Improving robustness against common corruptions by covariate shift adaptation. Advances in neural information processing systems, 33:11539 11551. Pier Giuseppe Sessa, Robert Dadashi, Léonard Hussenot, Johan Ferret, Nino Vieillard, Alexandre Ramé, Bobak Shariari, Sarah Perrin, Abe Friesen, Geoffrey Cideron, Sertan Girgin, Piotr Stanczyk, Andrea Michi, Danila Sinopalnikov, Sabela Ramos, Amélie Héliou, Aliaksei Severyn, Matt Hoffman, Nikola Momchev, and Olivier Bachem. 2024. Bond: Aligning llms with best-of-n distillation. Preprint, arXiv:2407.14622. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. 2024. Rewarding progress: Scaling automated process verifiers for llm reasoning. Preprint, arXiv:2410.08146. Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. Preprint, arXiv:2303.11366. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. 2022. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems, 35:14274 14289. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Dario Amodei, and Paul Christiano. 2020. LearnIn Ading to summarize with human feedback. vances in Neural Information Processing Systems, volume 33, pages 30083021. Curran Associates, Inc. Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, and Besmira Nushi. 2024. language Improving in Preprint, models through activation steering. arXiv:2410.12877. instruction-following Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Large language models are Preprint, Suhara. 2024. inconsistent and biased evaluators. arXiv:2405.01724. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2022. Selective annotation makes language models better few-shot learners. Preprint, arXiv:2209.01975. Yi Su, Yixin Ji, Juntao Li, Hai Ye, and Min Zhang. 2023a. Beware of model collapse! fast and stable test-time adaptation for robust question answering. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1299813011, Singapore. Association for Computational Linguistics. Yi Su, Yixin Ji, Juntao Li, Hai Ye, and Min Zhang. 2023b. Test-time adaptation with perturbation consistency learning. Preprint, arXiv:2304.12764. Yi Su, Yunpeng Tai, Yixin Ji, Juntao Li, Yan Bowen, and Min Zhang. 2024. Demonstration augmentation for zero-shot in-context learning. In Findings of the Association for Computational Linguistics ACL 2024, pages 1423214244, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Zayne Sprague, Fangcong Yin, Juan Diego Rodriguez, Dongwei Jiang, Manya Wadhwa, Prasann Singhal, Xinyu Zhao, Xi Ye, Kyle Mahowald, and Greg Durrett. 2024. To cot or not to cot? chain-ofthought helps mainly on math and symbolic reasoning. Preprint, arXiv:2409.12183. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast best-of-n decoding via speculative rejection. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. 2023. Gpt-4 doesnt know its wrong: An analysis of iterative prompting for reasoning problems. Preprint, arXiv:2310.12397. Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. 2024. Chain of thoughtlessness? an analysis of cot in planning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Elias Stengel-Eskin, Peter Hase, and Mohit Bansal. 2024. Teaching models to balance resisting and accepting persuasion. Preprint, arXiv:2410.14596. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. 2020. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pages 92299248. PMLR. Niket Tandon, Aman Madaan, Peter Clark, Keisuke Sakaguchi, and Yiming Yang. 2021. Interscript: dataset for interactive learning of scripts through error feedback. Preprint, arXiv:2112.07867. Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. 2022. Learning to repair: Repairing model output errors after deployment using dynamic memory of feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 339352, Seattle, United States. Association for Computational Linguistics. 18 Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, and JiRong Wen. 2024a. Dawn-icl: Strategic planning of problem-solving trajectories for zero-shot in-context learning. arXiv preprint arXiv:2410.20215. Zecheng Tang, Keyan Zhou, Juntao Li, Yuyang Ding, Pinzheng Wang, Yan Bowen, Renjie Hua, and Min Zhang. 2024b. CMD: framework for context-aware model self-detoxification. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 19301949, Miami, Florida, USA. Association for Computational Linguistics. Vernon Y. H. Toh, Deepanway Ghosal, and Soujanya Poria. 2024. Not all votes count! programs as verifiers improve self-consistency of language models for math reasoning. Preprint, arXiv:2410.12608. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2023. Interleaving retrieval with chain-of-thought reasoning for knowledgeIn Proceedings of intensive multi-step questions. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1001410037, Toronto, Canada. Association for Computational Linguistics. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, and Steering language Monte MacDiarmid. 2024. Preprint, models with activation engineering. arXiv:2308.10248. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. 2024. LLMs cannot find reasoning errors, but can correct them given the error location. In Findings of the Association for Computational Linguistics ACL 2024, pages 1389413908, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. 2022. Solving math word problems with processand outcomebased feedback. Preprint, arXiv:2211.14275. Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023. Can large language models really improve by self-critiquing their own plans? Preprint, arXiv:2310.08118. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. 2024. Will we run out of data? limits of llm scaling based on human-generated data. Preprint, arXiv:2211.04325. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. 2021. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations. Linguistics (Volume 1: Long Papers), pages 2175 2187, Dublin, Ireland. Association for Computational Linguistics. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is ChatGPT good NLG evaluator? preliminary study. In Proceedings of the 4th New Frontiers in Summarization Workshop, pages 111, Singapore. Association for Computational Linguistics. Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. 2024a. Reasoning in token economies: Budget-aware evaluation of LLM reasoning strategies. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1991619939, Miami, Florida, USA. Association for Computational Linguistics. Junlin Wang, Siddhartha Jain, Dejiao Zhang, Baishakhi Ray, Varun Kumar, and Ben Athiwaratkun. 2024b. Reasoning in token economies: Budget-aware evaluation of llm reasoning strategies. Preprint, arXiv:2406.06461. Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and James Zou. 2024c. Mixture-of-agents enhances large language model capabilities. Preprint, arXiv:2406.04692. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. 2024d. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94409450, Bangkok, Thailand. Association for Computational Linguistics. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024e. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, Bangkok, Thailand. Association for Computational Linguistics. Shuhe Wang, Xiaoya Li, Yuxian Meng, Tianwei Zhang, Rongbin Ouyang, Jiwei Li, and Guoyin Wang. 2022b. knn-ner: Named entity recognition with nearest neighbor search. Preprint, arXiv:2203.17103. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023b. Shepherd: critic for language model generation. Preprint, arXiv:2308.04592. Dexin Wang, Kai Fan, Boxing Chen, and Deyi Xiong. 2022a. Efficient cluster-based k-nearest-neighbor machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, and William Yang Wang. 2023c. Large language models are latent variable models: Explaining and finding good demonstrations for in-context 19 learning. In Thirty-seventh Conference on Neural Information Processing Systems. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023d. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations. Yidong Wang, Zhuohao Yu, Wenjin Yao, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. 2024f. PandaLM: An automatic evaluation benchmark for LLM instruction tuning optimization. In The Twelfth International Conference on Learning Representations. Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang. 2024g. Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation. Preprint, arXiv:2403.05313. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023. Generating sequences by learning to self-correct. In The Eleventh International Conference on Learning Representations. Jason Weston and Sainbayar Sukhbaatar. 2023. System 2 attention (is something you might need too). Preprint, arXiv:2311.11829. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International conference on machine learning, pages 2396523998. PMLR. Jinyang Wu, Mingkuan Feng, Shuai Zhang, Feihu Che, Zengqi Wen, and Jianhua Tao. 2024a. Beyond examples: High-level automated reasoning paradigm in in-context learning via mcts. Preprint, arXiv:2411.18478. Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui, and Furu Wei. 2024b. Minds eye of LLMs: Visualization-of-thought elicits spatial reasoning in large language models. In The Thirtyeighth Annual Conference on Neural Information Processing Systems. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024c. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Preprint, arXiv:2408.00724. Zhiyong Wu, Yaoxiang Wang, Jiacheng Ye, and Lingpeng Kong. 2023. Self-adaptive in-context learning: An information compression perspective for incontext example selection and ordering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14231436, Toronto, Canada. Association for Computational Linguistics. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, Xiao Wang, Rui Zheng, Tao Ji, Xiaowei Shi, Yitao Zhai, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Zuxuan Wu, Qi Zhang, Xipeng Qiu, Xuanjing Huang, and YuGang Jiang. 2024. Enhancing llm reasoning via critique models with test-time and training-time supervision. Preprint, arXiv:2411.16579. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy P. Lillicrap, Kenji Kawaguchi, and Michael Shieh. 2024. Monte carlo tree search boosts reasoning via iterative preference learning. Preprint, arXiv:2405.00451. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He, and Qizhe Xie. 2023. Self-evaluation guided beam search for reasoning. In Thirty-seventh Conference on Neural Information Processing Systems. Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. Examining inter-consistency of large language models collaboration: An in-depth analysis via debate. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 75727590, Singapore. Association for Computational Linguistics. Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. 2024. Llava-cot: Let vision language models reason step-by-step. Preprint, arXiv:2411.10440. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024. Regularizing hidden states enables learning generalizable reward model for LLMs. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. In Advances in Neural Information Processing Systems, volume 36, pages 1180911822. Curran Associates, Inc. Michihiro Yasunaga and Percy Liang. 2020. Graphbased, self-supervised program repair from diagnostic feedback. In International Conference on Machine Learning (ICML). Hai Ye, Yuyang Ding, Juntao Li, and Hwee Tou Ng. 2022. Robust question answering against distribution shifts with test-time adaption: An empirical study. In Findings of the Association for Computational 20 Linguistics: EMNLP 2022, pages 61796192, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Hai Ye and Hwee Tou Ng. 2024. Preference-guided reflective sampling for aligning language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2164621668, Miami, Florida, USA. Association for Computational Linguistics. Hai Ye, Qizhe Xie, and Hwee Tou Ng. 2023. Multisource test-time adaptation as dueling bandits for extractive question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 96479660, Toronto, Canada. Association for Computational Linguistics. Zihuiwen Ye, Fraser Greenlee-Scott, Max Bartolo, Phil Blunsom, Jon Ander Campos, and Matthias Gallé. 2024. Improving reward models with synthetic critiques. Preprint, arXiv:2405.20850. WangJie You, Pei Guo, Juntao Li, Kehai Chen, and Min Zhang. 2024. Efficient domain adaptation for nonautoregressive machine translation. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1365713670, Bangkok, Thailand. Association for Computational Linguistics. Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, and Rui Hou. 2024. Self-generated critiques boost reward modeling for language models. Preprint, arXiv:2411.16646. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. 2024a. Advancing llm reasoning generalists with preference trees. Preprint, arXiv:2404.02078. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024b. Self-rewarding language models. Preprint, arXiv:2401.10020. Runzhe Zhan, Xuebo Liu, Derek F. Wong, Cuilian Zhang, Lidia S. Chao, and Min Zhang. 2023. Testtime adaptation for machine translation evaluation by uncertainty minimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 807820, Toronto, Canada. Association for Computational Linguistics. Di Zhang, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli Ouyang. 2024b. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. Preprint, arXiv:2406.07394. Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, and Lei Li. 2024c. Scaling llm inference with optimized sample compute allocation. Preprint, arXiv:2410.22480. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. 2024d. Generative verifiers: Reward modeling as next-token prediction. Preprint, arXiv:2408.15240. Marvin Zhang, Sergey Levine, and Chelsea Finn. 2022a. Memo: Test time robustness via adaptation and augmentation. Advances in neural information processing systems, 35:3862938642. Ruiqi Zhang, Momin Haider, Ming Yin, Jiahao Qiu, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024e. Accelerating best-of-n via speculative rejection. In 2nd Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ICML 2024). Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan. 2023a. Adanpc: Exploring non-parametric classifier for testtime adaptation. In International Conference on Machine Learning, pages 4164741676. PMLR. Yiming Zhang, Shi Feng, and Chenhao Tan. 2022b. Active example selection for in-context learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9134 9148, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. 2024f. o1-coder: an o1 replication for coding. Preprint, arXiv:2412.00154. Zhihan Zhang, Tao Ge, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang. 2024g. Learn beyond the answer: Training language models with reflection for mathematical reasoning. Preprint, arXiv:2406.12050. Zhuosheng Zhang, Aston Zhang, Mu Li, hai zhao, George Karypis, and Alex Smola. 2024h. Multimodal chain-of-thought reasoning in language models. Transactions on Machine Learning Research. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023b. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024a. Rest-mcts*: Llm self-training via process reward guided tree search. Preprint, arXiv:2406.03816. Hao Zhao, Yuejiang Liu, Alexandre Alahi, and Tao Lin. 2023. On pitfalls of test-time adaptation. In International Conference on Machine Learning, pages 4205842080. PMLR. 21 Shuai Zhao, Xiaohan Wang, Linchao Zhu, and Yi Yang. 2024a. Test-time adaptation with CLIP reward for zero-shot generalization in vision-language models. In The Twelfth International Conference on Learning Representations. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. 2024b. Marco-o1: Towards open reasoning models for open-ended solutions. Preprint, arXiv:2411.14405. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed H. Chi. 2023. Least-to-most prompting enables comIn The plex reasoning in large language models. Eleventh International Conference on Learning Representations. Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023a. Judgelm: Fine-tuned large language models are scalable judges. Preprint, arXiv:2310.17631. Wenhao Zhu, Shujian Huang, Yunzhe Lv, Xin Zheng, and Jiajun Chen. 2023b. What knowledge is needed? towards explainable memory for kNN-MT domain adaptation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 28242836, Toronto, Canada. Association for Computational Linguistics. Yuhan Zhu, Guozhen Zhang, Chen Xu, Haocheng Shen, Xiaoxin Chen, Gangshan Wu, and Limin Wang. 2024. Efficient test-time prompt tuning for vision-language models. Preprint, arXiv:2408.05775. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2020. comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):4376."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Department of Computer Science, National University of Singapore",
        "School of Computer Science and Technology, Soochow University"
    ]
}