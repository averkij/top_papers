{
    "paper_title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
    "authors": [
        "Md Mubtasim Ahasan",
        "Md Fahim",
        "Tasnim Mohiuddin",
        "A K M Mahbubur Rahman",
        "Aman Chadha",
        "Tariq Iqbal",
        "M Ashraful Amin",
        "Md Mofijul Islam",
        "Amin Ahsan Ali"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 7 1 0 5 1 . 0 1 4 2 : r DM-CODEC: DISTILLING MULTIMODAL REPRESENTATIONS FOR SPEECH TOKENIZATION Md Mubtasim Ahasan1, Md Fahim1, Tasnim Mohiuddin3, Mahbubur Rahman1, Aman Chadha2, Tariq Iqbal4, Ashraful Amin1, Md Mofijul Islam2,4, Amin Ahsan Ali1 1Center for Computational & Data Sciences, Independent University, Bangladesh 2Amazon GenAI, USA 3Qatar Computing Research Institute, Qatar 4University of Virginia, USA"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) language model (LM)-guided distillation method that incorporates contextual information, and (2) combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts streamlined encoder-decoder framework with Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset. The code, samples, and model checkpoints are available at https://github.com/mubtasimahasan/DM-Codec."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, the advent of Large Language Models (LLMs) has revolutionized various domains, offering unprecedented advancements across wide array of tasks (OpenAI, 2024). critical component of this success has been the tokenization of input data, enabling vast amounts of information processing (Du et al., 2024; Rust et al., 2021). Inspired by these breakthroughs, significant attention has shifted towards replicating similar successes in the realm of speech understanding and generation (Defossez et al., 2022; Hsu et al., 2021). However, tokenizing speech into discrete units presents unique challenges compared to text, as speech is inherently continuous and multidimensional, requiring various speech attributes such as acoustic properties, semantic meaning, and contextual clues (Ju et al., 2024). Traditional approaches using feature representations such as Mel-Spectrograms (Sheng et al., 2019), Mel-frequency cepstral coefficients (MFCCs) (Juvela et al., 2018), and WaveCorresponding author: mubtasimahasan@gmail.com Work does not relate to position at Amazon. Equal Supervision. 1 Figure 1: An overview of speech tokenization approaches using discrete acoustic, semantic, and contextual tokens. DM-Codec integrates these multimodal representations for robust speech tokenization, learning comprehensive speech representations. forms (Kim et al., 2021) have proven inadequate in capturing this full spectrum of information, resulting in suboptimal performance in downstream tasks such as speech synthesis (Ju et al., 2024). These limitations led researchers to explore various approaches, and one prominent direction leading to audio codecs (Borsos et al., 2023). Notable examples include SoundStream (Zeghidour et al., 2021) and EnCodec (Defossez et al., 2022), which utilize Residual Vector Quantizers (RVQ) within neural codec framework, iteratively refining quantized vectors to discretize speech into acoustic tokens. Concurrently, self-supervised speech representation learning models such as HuBERT (Hsu et al., 2021) and wav2vec 2.0 (Baevski et al., 2020) facilitated extracting speech representations as semantic tokens (Borsos et al., 2023). Efforts to unify acoustic and semantic representations have led to two notable approaches: SpeechTokenizer (Zhang et al., 2024a), which utilizes semantic distillation from HuBERT, and FACodec (Ju et al., 2024), which proposes factorized vector quantizer to disentangle speech representation into different subspaces using separate RVQs with supervision. While these approaches have shown promising results, they often overlook crucial aspect of speech representation: the integration of contextual language information. Language models (LMs) have demonstrated remarkable ability to learn contextual representations that capture the meaning of tokens based on their broader linguistic context (Devlin et al., 2019). These contextual representations can provide essential insights into speech representation, allowing for more nuanced understanding of words in varying linguistic contexts. Our empirical investigations also reveal that existing discrete speech representation models struggle to align reconstructed speech with accurate textual form, resulting in elevated Word Error Rates (WER) and Word Information Lost (WIL) scores in speech transcription tasks. This observation underscores the need for more comprehensive approach to speech tokenization that incorporates contextual language information. To address these challenges, we propose DM-Codec, novel speech tokenizer that unifies multimodal language and speech representations within comprehensive tokenizer for speech. Our approach builds on neural codec architecture incorporating RVQ with encoder, decoder, and discriminator components. Central to our innovation is the introduction of an LM-guided distillation method that effectively incorporates contextual representations into the speech tokenization process. This technique allows DM-Codec capturing the nuances of linguistic context often missed by existing models. Building upon the LM-guided approach, we further propose hybrid distillation method combining both LM and speech model (SM)-guided techniques. The distillation method only utilizes the LM and SM during training, without increasing model complexity and parameters, and eliminates the need for them during inference. To the best of our knowledge, we are the first to attempt to integrate all three essential aspects of speech representationacoustic, semantic, and contextualwithin single codec. See Figure 1 for depiction. Through extensive experimentation on the LibriSpeech benchmark dataset (Panayotov et al., 2015), we demonstrate the superiority of DM-Codec, which achieves significantly lower WER and WIL compared to state-of-the-art baseline speech tokenizers. Specifically, DM-Codec achieves WER of 4.05 and WIL of 6.61, outperforming SpeechTokenizer (4.49, 7.10), FACodec (4.68, 7.33), and 2 EnCodec (4.53, 7.17). Furthermore, DM-Codec exhibits improved speech quality, as evidenced by its Virtual Speech Quality Objective Listener (ViSQOL) score of 3.26, surpassing the performance of baseline models (EnCodec: 3.08; SpeechTokenizer: 3.09; FACodec: 3.13). Our research makes the following key contributions: We introduce DM-Codec, novel speech tokenizer that incorporates contextual representations via an LM-guided distillation method. We present novel combined LM and SM-guided representation distillation approach, uniting acoustic, semantic, and contextual representations into unified framework. Through comprehensive experiments and ablation studies, we demonstrate the effectiveness of DM-Codec in preserving increased contextual information and enhancing the retention of acoustic and speech information in reconstructed speech."
        },
        {
            "title": "2 PROPOSED METHOD",
            "content": "In this section, we present DM-Codec, novel speech tokenizer designed to encapsulate comprehensive fusion of multimodal (acoustic, semantic, and contextual) representations. As illustrated in Figure 2, we propose two distinct training approaches to incorporate these representations: (i) language model (LM)-guided distillation method, and (ii) combined LM and self-supervised speech model (SM)-guided distillation method. The first approach distills contextual representations from the LM and integrates them with learned acoustic representations. The second approach combines SM and LM to further incorporate semantic representations with contextual and acoustic representations. It ensures that DM-Codec captures the essential elements of speech by harmonizing the acoustic features with contextual and semantic information. The following subsections detail our proposed distillation methods (2.1), model details (2.2), and components (2.3). 2.1 SPEECH AND LANGUAGE MODEL GUIDED DISTILLATION Our approach first transcribes the raw speech into its corresponding text using Speech-toText (STT) model MST , such that = MST (x). For simplicity, we omit any post-processing techniques on the x. Subsequently, we pass the text through pretrained language model MLM to obtain contextual representations of x, tokenized into set of tokens, = {ti}n i=1. For each token ti, we extract its corresponding layer-wise hidden representations {hl l=1, where denotes the total number of layers in MLM . We utilize all layer representations to derive the representations for each token, as each layer of pre-trained language model captures hierarchical and contextually distinct information (Niu et al., 2022; Kovaleva et al., 2019; Hao et al., 2019). To obtain the contextual representation Si for token ti, we average the hidden representations across all layers, yielding i, where Si RD where is hidden dimension. Consequently, we obtain the Si = 1 contextual representations = {Si}n i=1 for the speech input x, which captures the contextually diverse information from MLM . l=1 hl i}L (cid:80)L Simultaneously, we process the raw speech through an Encoder E(x) to obtain the latent feature v. We then pass through Residual Vector Quantizer (RVQ) to obtain quantized features = k=1, where represents the number of quantization layers in the RVQ, and Qk RD {Qk}K where is hidden dimension of kth RVQ layer. These quantized features are subsequently used to reconstruct the audio ˆx via decoder. To align the quantized feature Qk with the LM distilled = WQk, where RDD, ensuring the features Si, we apply linear transformation dimensional consistency for the distillation process. LM Guided Distillation: In this approach, we distil the LM representations S. To calculate the LM-guided distillation loss, we adopt continuous representation distillation technique, similar to the one employed by SpeechTokenizer (Zhang et al., 2024a), which maximizes the cosine similarity at the dimension level across all time steps. In our case, we calculate the continuous representation distillation of the transformed quantized features and the LM representation features as follows: 3 Figure 2: DM-Codec framework consists of an encoder that extracts latent representations from the input speech signal. These latent vectors are subsequently quantized using Residual Vector Quantizer (RVQ). We designed two distinct distillation approaches: (i) distillation from language model, and (ii) combined distillation from both language model (LM) and speech model (SM). These approaches integrate acoustic, semantic, and contextual representations into the quantized vectors to improve speech representation for downstream tasks. LL = (cid:32) (cid:32) log σ 1 (cid:88) d= Q(:,d) Q(:,d) S(:,d) (cid:33)(cid:33) S(:,d) (1) Here, the notation (:, d) indicates vector that includes values from all time steps at the dth dimension. The function σ() represents the sigmoid activation function, commonly used to squash input values into range between 0 and 1. Combined LM and SM Guided Distillation: To further enhance the capabilities of DM-Codec, we propose hybrid approach that utilizes both audio and text modalities. To derive semantic representations from the speech model (SM), we adopt similar distillation strategy as we used for the LM. We first pass the raw speech through the pretrained speech model MSM , which generates its own set of layer-wise hidden representations {hl l=1. The semantic features are derived by j, where Aj RD. This averaging the hidden states across all layers, yielding Aj = 1 process results in the semantic representations = {Aj}n j=1 for the speech input x. The distillation loss in this case considers both the LM and SM representations, jointly optimizing for the alignment of the quantized features with the representations and derived from MSM and MLM , respectively. Finally, the distillation loss for the SM, LSM , is first computed, followed by averaging with the LM distillation loss, LL, to ensure balanced contribution from both losses. The combined distillation loss is computed as: l=1 hl j}L (cid:80)L LSM = LLS = 1 2 (cid:32) (cid:32) log σ 1 D (cid:88) d=1 (LSM + LL) Q(:,d) Q(:,d) A(:,d) (cid:33)(cid:33) A(:,d) (2) (3) This formulation ensures that DM-Codec effectively integrates both acoustic and semantic knowledge from SM, along with the contextual information provided by LM, resulting in more robust and comprehensive set of features for speech discretization."
        },
        {
            "title": "2.2 MODEL DETAILS",
            "content": "Our framework builds upon the Residual Vector Quantizer with Generative Adversarial Networks (RVQ-GAN) architecture, incorporating state-of-the-art components and novel distillation techniques. The core of our model consists of an Encoder and Decoder with an RVQ architecture, inspired by Encodec (Defossez et al., 2022) and SpeechTokenizer (Zhang et al., 2024a). Moreover, we employ multi-discriminator framework, comprising: Multi-Scale Discriminator (MSD), MultiPeriod Discriminator (MPD), and Multi-Scale Short-Time Fourier Transform (MS-STFT) Discriminator, adopted from HiFi-Codec (Yang et al., 2023) and HiFi-GAN (Kong et al., 2020). This foundation provides robust basis for speech quantization. To further enhance the quantizer with distilled multimodal representations, we use wav2vec 2.0 (wav2vec2-base-960h) as MST (Baevski et al., 2020), BERT (bert-base-uncased) as MLM (Devlin et al., 2019), and HuBERT (hubert-base-ls960) as MSM (Hsu et al., 2021). We extract the quantized output from the first layer of the RVQ (RVQ-1) for LM-guided distillation and the average of the quantized features across all eight layers (RVQ1:8) for SM-guided distillation to calculate the distillation loss. 2.3 MODEL COMPONENTS Encoder Decoder. The encoder-decoder architecture in DM-Codec is based on SEANet (Tagliasacchi et al., 2020), leveraging the successful design employed in recent speech tokenization models (Zhang et al., 2024a; Defossez et al., 2022; Zeghidour et al., 2021). The architecture is designed to efficiently process and reconstruct speech signals while maintaining high fidelity. The Encoder consists of 1D convolution layer with channels and kernel size of 7, followed by residual convolutional blocks. Each block contains strided convolutional downsampling layer with kernel size (where = 2S , and represents the stride), paired with residual unit. The residual unit comprises two convolutional layers with kernel size of 3 and skip connection, while the number of channels is doubled at each downsampling stage. This is followed by two-layer BiLSTM and final 1D convolutional layer with output channels and kernel size of 7. The Decoder mirrors the encoders structure but replaces BiLSTM with LSTM, strided convolutions with transposed convolutions, and employs reversed strides for up-sampling. The final audio output is reconstructed from D. For the experiments, we use the following configuration: = 32, = 4, and = (2, 4, 5, 8). Residual Vector Quantizers. The Residual Vector Quantizer (RVQ) plays central role in our tokenization process, quantizing the encoders outputs. Our implementation is inspired by the training procedures described in Encodec (Defossez et al., 2022) and SpeechTokenizer (Zhang et al., 2024a). The RVQ projects input vectors to the most similar entry in codebook, and the residual is calculated and processed in subsequent quantization steps, each utilizing different codebook. The codebook entries are updated using an exponential moving average (EMA) with decay rate of 0.99 for the matched item, while unmatched entries are replaced by candidates from the current batch. To ensure proper gradient flow during training, we employ straight-through estimator. commitment loss is also computed and added to the total training loss to promote stability. In our experiments, we utilize codebook size of 1024 and 8 quantization levels. Discriminators. We incorporate trio of discriminators to enhance the quality and realism of the generated speech: the Multi-Scale Discriminator (MSD), the Multi-Period Discriminator (MPD), and the Multi-Scale Short-Time Fourier Transform (MS-STFT) discriminator. The MS-STFT discriminator follows the implementation outlined in (Defossez et al., 2022), operating on the real and imaginary components of multi-scale complex-valued STFTs. It begins with 2D convolutional layer, followed by 2D convolutions with increasing dilation rates in the time dimension (1, 2, and 4) and stride of 2 across the frequency axis in each sub-network. final 2D convolution with kernel size of 3 3 and stride of (1, 1) is applied to produce the prediction. The MSD and MPD discriminators follow the architectures introduced in (Kong et al., 2020), with adjustments to the channel numbers to align the parameter count more closely with the MS-STFT discriminator. This ensemble of discriminators works in concert to provide comprehensive feedback on various aspects of the generated speech, contributing to the overall quality and naturalness of the output."
        },
        {
            "title": "2.4 TRAINING OBJECTIVE",
            "content": "Our training strategy employs GAN-guided framework, following methodologies established in recent work (Zhang et al., 2024a; Yang et al., 2023). In addition to the distillation loss described in Section 2.1, we utilize reconstruction losses, adversarial and feature matching losses, and commitment loss to guide the learning process. For the original speech and the reconstructed speech ˆx, we calculate the losses as described below. Reconstruction Loss. To ensure that the model preserves the key attributes of speech, we employ both time-domain and frequency-domain reconstruction losses. The time-domain loss Lt is computed as the L1 distance between and ˆx. For the frequency-domain loss Lf , we combine L1 and L2 losses over 64-bin Mel-spectrograms Meli, with varying window sizes of 2i, hop lengths of 2i/4, and scales = {5, . . . , 11}. Lt = ˆx1 Lf = (cid:88) ie (Meli(x) Meli(ˆx)1 + Meli(x) Meli(ˆx)2) (4) (5) Adversarial Loss. The adversarial loss promotes the generator to produce realistic and indistinguishable speech. We apply hinge loss formulation to compute the adversarial loss for both the generator Lg and the discriminator Ld. These losses are computed across all three discriminators: the multi-scale discriminator (MSD), multi-period discriminator (MPD), and the multi-scale STFT guided (MS-STFT) discriminator. Lg = Ld = 1 1 (cid:88) n=1 (cid:88) n= max(1 Rn(ˆx), 0) (max(1 Rn(x), 0) + max(1 + Rn(ˆx), 0)) (6) (7) where is the number of discriminators and Rn represents the nth discriminator. Feature Matching Loss. To prevent the generator from overfitting to the discriminators decisions, we apply feature matching loss Lf m. This loss compares features from each discriminator Rns internal layers across all dimensions, promoting stability and better generalization. Lf = 1 (cid:88) (cid:88) n=1 m= Rm (x) Rm mean(Rm (ˆx)1 (x)1) (8) RVQ Commitment Loss. To guide the encoder to produce outputs that closely match their corresponding quantized values in the residual vector quantization (RVQ) process, we introduce commitment loss Lw. For Nq quantization vectors, where qi represents the current residual and qci is the closest entry in the corresponding codebook for the ith entry, the Lw is computed as: Lw = Nq (cid:88) i=1 qi qci2 2 (9) Overall Generator Loss. The total generator loss LG is weighted sum of the individual loss components, including the distillation loss LL/LS (which is either LL or LLS depending on the chosen distillation method). We use the corresponding weighting factors λL/LS, λt, λf , λg, λf m, and λw to control the influence of each loss component on the overall training objective as: LG = λL/LSLL/LS + λtLt + λf Lf + λgLg + λf mLf + λwLw (10) This comprehensive training objective ensures DM-Codec learns acoustic speech representations while incorporating semantic and contextual representation through novel distillation approaches. 6 Table 1: Evaluation of speech reconstruction quality of DM-Codec and comparison with baselines. DM-Codec achieves the best performance in WER, WIL, and ViSQOL, highlighting its enhanced content preservation and speech quality, with competitive intelligibility results. means the results were reproduced using the official training code. means the results were obtained using official model checkpoints. indicates LM-guided Distillation method. indicates combined LM and SM-guided Distillation method. Bold highlights the best result and underline the second-best result. Tokenizer WER WIL ViSQOL STOI Groundtruth EnCodec SpeechTokenizer FACodec DM-Codec DM-Codec 3.78 4.53 4.49 4.68 4.36 4.05 6.03 7.17 7.10 7.33 7.06 6.61 - 3.08 3.09 3.13 3.18 3. - 0.920 0.923 0.949 0.935 0."
        },
        {
            "title": "3 EXPERIMENTAL SETUP",
            "content": "Dataset. We trained DM-Codec using the LibriSpeech training set of 100 hours of clean speech (Panayotov et al., 2015). This dataset was selected primarily because of its successful use for training and evaluation in various speech tokenizer and modeling tasks (Zhang et al., 2024a; Ju et al., 2024; Hsu et al., 2021). Before training, we made the data uniform by randomly cropping each sample to three seconds and ensuring consistent sample rate of 16 Hz. Training. We trained DM-Codec utilizing 2 to 4 A100 GPUs until the model converged within 100 epochs. The batch size ranged from 6 to 20, depending on GPU resource availability. We applied learning rate of 1 104 using the Adam optimizer with 0.98 learning rate decay. The embedding size was set to 1024 for RVQ and 768 for the LM and SM. For all experiments, we used random seed of 42 to ensure reproducibility. We also share our training code with the entire configuration file and docker file to reproduce the training environment, which can be accessed through the GitHub link referenced in the abstract. Baselines. We compared DM-Codec with the baseline speech tokenizers: EnCodec (Defossez et al., 2022), SpeechTokenizer (Zhang et al., 2024a), and FACodec (NaturalSpeech3) (Ju et al., 2024). We reproduced SpeechTokenizer using the official training code and used official model checkpoints of EnCodec (EnCodec 24khz 6kpbs) and FACodec as the baselines. Evaluation Dataset. To evaluate DM-Codec, we randomly selected 300 audio samples from the LibriSpeech test subset, following similar practice of sampling test data used in our baselines (Zhang et al., 2024a; Zeghidour et al., 2021) and to align the experimental setup with that of SpeechTokenizer. In our experiments, we sampled the test subset of LibriSpeech using random seed of 42. We also evaluated the baseline models with the same sampled test dataset for fair comparison. Evaluation Metrics. To evaluate DM-Codec, we employed different metrics suited to get insights into various aspects of information and quality preservation in the reconstructed speech. First, we used the Word Error Rate (WER) and Word Information Lost (WIL) metrics to evaluate context preservation by calculating the amount of word-level transcription errors and key information missing in transcription, respectively. For these metrics, we used the Whisper (whisper-medium) (Radford et al., 2023) model to extract the transcription from the reconstructed speech. To provide fairer comparison and indicate the level of transcription error by the Whisper model, we also included the Groundtruth WER and WIL scores for the Whispers transcribed text from the original speech versus the true text. Next, we assessed the acoustic and semantic information preservation using the ViSQOL (Virtual Speech Quality Objective Listener) (Hines et al., 2012) and Short-Time Objective Intelligibility (STOI) metrics, respectively. The ViSQOL metric measures the similarity between reference and test speech sample using spectro-temporal measure and produces MOS-LQO (Mean Opinion Score - Listening Quality Objective) score ranging from 1 (worst) to 5 (best). For this metric, we used the wideband model suited for speech evaluation. Lastly, the STOI metric evaluates the perceived intelligibility of speech by analyzing short-time correlations between original and reconstructed speech, with scores ranging from 0 to 1. Table 2: Significance Analysis of DM-Codec (D) compared to baselines EnCodec (E), SpeechTokenizer (S), and FACodec (F). Results reveal DM-Codec consistently achieves significantly better scores in key metrics across all individual samples. indicates that DM-Codec is significantly better, denotes dominance, and means no significant improvement over the baseline. Avg and Std mean the average and standard deviation of each score. WER WIL ViSQOL STOI DM-Codec Avg 0.053 Avg Std 0.113 0.082 Avg Std 0.157 3.258 Avg Std 0.184 0.937 Std 0.019 EnCodec Avg Std 0.061 0. Avg Std 0.090 0. Avg Std 3.078 0. Avg Std 0.920 0. SpeechTokenizer Avg 0.060 Std 0.139 F Avg 0.089 Std 0.166 Avg 3.087 Std 0.190 Avg 0.923 Std 0.021 F FACodec Avg 0.057 Std 0.123 Avg 0.086 Std 0.163 Avg 3.129 Std 0.250 Avg 0.949 Std 0."
        },
        {
            "title": "4 EXPERIMENTAL RESULTS AND DISCUSSION",
            "content": "We conducted extensive experiments to evaluate DM-Codecs reconstructed speech using WER and WIL for contextual information retention, and ViSQOL and STOI for semantic-acoustic information preservation. To demonstrate the effectiveness of our two distillation approaches, we present results for DM-Codec (LM-guided Distillation) and DM-Codec (LM and SM-guided Distillation). 4.1 COMPARISON OF SPEECH TOKENIZATION MODELS We compared the quality of DM-Codecs discrete speech representations by reconstructing speech from quantized vector features and comparing it with state-of-the-art (SOTA) speech tokenization models: EnCodec, SpeechTokenizer, and FACodec. For LM-guided distillation, we utilize quantized features from the first Residual Vector Quantizer layer (RVQ-1), and for the combined LM and SMguided distillation, we average all layers (RVQ-1:8). Results: The results in Table 1 show that DM-Codec outperformed all evaluated SOTA speech tokenization models across most metrics. Specifically, DM-Codec with only LM-guided distillation exceeds the SOTA models, achieving improved scores: WER 4.36, WIL 7.06, and ViSQOL 3.18. Furthermore, DM-Codecs with combined LM and SM-guided distillation outscore LM-guided distillation and all previous scores with 4.05 WER, 6.61 WIL, 3.26 ViSQOL, and achieved highly compatible 0.937 STOI scores compared to SOTA models. Discussion: The observed performance gains stem from the proposed LM-guided distillation, which enhances the quantized features by leveraging LMs contextual representations. This process aligns the speech with its overall context and word relation, resulting in more accurate reconstructions, as reflected in the reduced WER and WIL scores. By embedding contextual cues, the method effectively grounds isolated phonetic units within their overall context, reconstructing speech that aligns with human expectations, as demonstrated by the higher ViSQOL and STOI scores. Moreover, the integration of LM and SM-based distillation further amplifies these improvements. The addition of SM distillation contributes to enhanced semantic-acoustic fidelity, as SM models capture phonetic nuances alongside prosodic and tonal characteristics. This dual representationcontext from LM and phonetic detail from SMproduces more coherent and natural speech reconstruction, yielding superior results across all metrics."
        },
        {
            "title": "4.2 SIGNIFICANCE ANALYSIS OF SPEECH TOKENIZER PERFORMANCE",
            "content": "We conducted significance analysis at α = 0.05, following the approach of Dror et al. (2019), to measure the stochastic dominance of DM-Codec over the baselines: EnCodec, SpeechTokenizer, and FACodec. Specifically, we computed inverse cumulative distribution functions (CDFs) for all reconstructed speech samples individual WER, WIL, ViSQOL, and STOI scores. Notably, the average WER and WIL are calculated from each sentence individually, while the Table 1 scores are calculated by concatenating all sentences into one. Significance was evaluated using the ϵ value and categorized as: significantly better when 0.0 < ϵ 0.5, significantly dominant when ϵ = 0.0, and not significantly better when ϵ > 0.5. For this analysis, we selected DM-Codec, trained with combined LM and SM-guided distillation. To the best of our knowledge, we are the first to conduct significance analysis to measure the effectiveness of different speech tokenizers. Results and Discussion: The results in Table 2 show that DM-Codec significantly outperforms the baselines in WER, WIL, ViSQOL, and STOI scores. The improved average values (0.053 WER, 0.082 WIL, 3.258 ViSQOL, 0.937 STOI) and consistent standard deviations (0.113 WER, 0.157 WIL, 0.193 ViSQOL, 0.019 STOI) further demonstrate the statistical significance. Notably, DMCodecs performance in WER and WIL underscores the importance of contextual representation distillation for enhanced speech reconstruction. Additionally, its dominance in ViSQOL and STOI, especially over EnCodec, highlights the benefits of combining LM and SM distillation for retaining semantic-acoustic fidelity. While DM-Codec does not achieve significant dominance over FACodec in terms of STOI, it significantly outperforms the baselines across all other metrics. Among the baseline, however, FACodec achieves improved results over EnCodec and SpeechTokenizer, whereas SpeechTokenizer surpasses EnCodec in performance. 4.3 ABLATION STUDIES We conducted thorough analysis of DM-Codecs performance and the impact of each methodological choice in LM-guided and combined LM and SM-guided distillation. Unless otherwise stated, we use distillation for both LM and SM from the first Residual Vector Quantizer layer (RVQ-1) for comparison consistency and simplicity. 4.3.1 ABLATION STUDY: IMPACT OF COMBINED SEMANTIC DISTILLATION We conducted experiments with different weighted combinations of LM and SM distillation loss to evaluate their impact on reducing WER. The combined distillation loss from Equation 3 was updated using SM and LM weights (λSM and λLM ), ranging from 0.0 to 1.0, with the constraint λSM + λLM = 1. LLS = 1 2 (λSM LSM + λLM LL) (11) Table 3: Effects of weights on combined representation distillation: Higher LM weight enhances content preservation, leading to lower WER. λSM is the SM weight, λLM is the LM weight. Results and Discussion: The experimental results are presented in Table 3, showing the speech reconstruction results with WER scores for different weighted combinations. From the values, we notice trend showing that incorporating LM representations significantly improves WER, especially when LM distillation is dominant. The lowest WER score of 4.07 occurs with weight of λLM = 0.8 for LM, while λSM = 0.2 for SM, highlighting the strong influence of LM distillation on capturing contextual information. balanced weighting of λSM = 0.5 and λLM = 0.5 produces WER of 4.18, confirming that distillation from both LM and SM is beneficial. However, as the weighting shifts more in favor of SM (λSM > 0.7), WER deteriorates, reaching 4.83 when relying entirely on SM. This underscores that over-reliance on SM distillation compromises contextual accuracy in favor of raw speech features. Thus, an LM-dominant approach yields optimal results, while using SM alone is less effective in preserving content. λSM λLM WER 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0 4.83 4.63 4.44 4.23 4.76 4.18 4.54 4.34 4.07 4.33 4.36 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 9 Table 4: Analysis of different RVQ layers effect on speech reconstruction. LM-guided distillation on RVQ-1 layer ensures greater content preservation, while SM-guided distillation on RVQ-1:8 layer is more effective at preserving semantic representation. LM-layer and SM-layer indicate the RVQ layer used for respective distillation. indicates LM-guided Distillation. indicates combined LM and SM-guided Distillation. Bold highlights the best result and underline the second-best result. Tokenizer DM-Codec DM-Codec DM-Codec DM-Codec DM-Codec DM-Codec DM-Codec DM-Codec LM-Layer SM-Layer WER WIL ViSQOL STOI RVQ-1 RVQ-1:8 RVQ-8 RVQ-1 RVQ-1:8 RVQ-8 RVQ-1 RVQ-1 - - - RVQ-1 RVQ-1 RVQ-1 RVQ-1:8 RVQ-8 4.36 4.23 4.44 4.18 4.59 4.49 4.05 4.39 7.06 6.94 7.22 6.84 7.34 7.24 6.61 7.08 3.18 3.12 3. 3.13 3.21 3.30 3.26 3.33 0.935 0.929 0.935 0.933 0.937 0.938 0.937 0.939 4.3.2 ABLATION STUDY: IMPACT OF DISTILLATION ON DIFFERENT RVQ LAYERS We evaluated the effect of applying distillation at various Residual Vector Quantizer (RVQ) layers, including the first layer (RVQ-1), the average of eight layers (RVQ-1:8), and the last layer (RVQ-8). Table 4 shows the full results. Results and Discussion: In LM-guided distillation, RVQ-1:8 achieves the best WER and WIL scores (4.23 and 6.94), though with lower ViSQOL and STOI scores (3.12 and 0.929) compared to RVQ-8 (3.28 and 0.935). The RVQ-1 layer provides the best overall balance between content preservation and perceptual quality, with WER, WIL, ViSQOL, and STOI scores of 4.36, 7.06, 3.18, and 0.935. This demonstrates RVQ-1:8 prioritizes contextual integrity, while RVQ-8 favors perceptual quality. Thus, we select RVQ-1 for LM-guided distillation due to its balanced performance. For LM and SM-based distillation, the RVQ-1 and RVQ-1:8 combination achieves the best WER and WIL scores (4.05 and 6.61), with RVQ-1 and RVQ-1 as the second-best (4.18 and 6.84). In contrast, the RVQ-1 and RVQ-8 combination yields the highest ViSQOL and STOI scores (3.33 and 0.939), followed by RVQ-8 and RVQ-1 (3.30 and 0.938). RVQ-1 captures contextual representation more effectively due to its simpler quantized vector, while RVQ-1:8 incorporates more nuanced semantic and acoustic aspects. Overall, this ablation shows that selecting RVQ layers for LM and SM-based distillation greatly affects the balance between contextual accuracy and semantic-acoustic fidelity, allowing layer combinations to be tailored to task requirements. 4.3.3 ABLATION STUDY: IMPACT OF DIFFERENT MODELS ON DISTILLATION We experimented with different LM and SM distillations to analyze performance variations based on different model selections. In addition to our selected BERT (Devlin et al., 2019) and HuBERT (Hsu et al., 2021), we experiment with ELECTRA (electra-base-discriminator) (Clark et al., 2020) as the LM and wav2vec 2.0 (wav2vec2-base-960h) (Baevski et al., 2020) as the SM. Table 5 shows the full results. Results and Discussion: In LM-guided distillation, the ELECTRA model significantly enhances performance, achieving WER and WIL scores of 4.12 and 6.63, respectively, compared to BERTs scores of 4.36 and 7.06. This indicates the architecture of ELECTRAs effectiveness for the proposed LM-guided distillation, demonstrating its superior contextual representation. These results are consistent with ELECTRAs better performance in general natural language processing tasks. However, we select BERT for its simplicity and established performance. In LM and SM-guided distillation, the combination of BERT and wav2vec 2.0 achieves the highest overall performance, with scores of WER 4.13, WIL 6.77, ViSQOL 3.15, and STOI 0.942. However, the combination of BERT and HuBERT closely follows with second-best scores of WER 4.18, WIL 6.84, and ViSQOL 0.933. These findings demonstrate that different speech models can be effectively integrated with the BERT model."
        },
        {
            "title": "4.3.4 ABLATION STUDY: IMPACT OF DIFFERENT DISTILLATION LAYER(S)",
            "content": "We evaluated speech reconstruction using different distillation layers of the LM and SM, examining which combination of layers yields the most relevant representations of semantic and contextual information. For this ablation, we considered the average of all layer representations, the 9th layer representations, and the last layer representations. Table 6 shows the full results. Results and Discussion: In LM-guided distillation, the use of the average layer achieves superior overall performance, with WER of 4.36, WIL of 7.06, ViSQOL of 3.18, and STOI of 0.935, compared to the variants utilizing the last and 9th layers. Similarly, in LM and SM-guided distillation, the average layer yields superior results compared to the last and 9th layer variants. The results indicate that averaging all layers leads to more comprehensive representations of semantic or contextual information. In the case of LM, the averaging process provides greater contextual representation and synergizes syntactic information from earlier layers and abstract word relations from higher layers. In combined LM and SM-guided distillation, averaging all SM layers provides more nuanced understanding of the earlier layers phonetic information and the higher layers richer semantic information. Conversely, relying solely on the last layer or the 9th layer fails to capture the overall context and semantic information, yielding less relevant representation distillation."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Tokenization Techniques in Speech. Tokenization in speech processing can be broadly categorized into two main approaches: (i) speech encoder-based and (ii) language-based. In the speech encoderbased tokenization approach, pre-trained speech encoder provides audio representations. These representations are then used to guide the training model, either through an alignment network (Messica & Adi, 2024) or by optimizing specific losses (Zhang et al., 2024a; Liu et al., 2024). The language-based tokenization approach involves processing audio through speech encoder to obtain discrete representations, or using the corresponding text to feed into language model. This has been explored by (Hassid et al., 2024), (Wang et al., 2024),(Zhang et al., 2023), and (Zhang et al., 2024b). Recently, LAST (Turetzky & Adi, 2024), explored language model to tokenize speech toward improved sequential modeling, using the LLM to perform the next token prediction of quantized vectors. However, these approaches significantly differ from our language representations distillation method and do not focus on combining multimodal representations. Discrete Speech Representation. There are two well-known methods for discrete speech representation: semantic tokens and acoustic tokens. Semantic tokens are derived through self-supervised learning (SSL) techniques for speech (Baevski et al., 2019; Hsu et al., 2021; Chung et al., 2021) and capture abstract, high-level features that relate to general, symbolic aspects of speech, while omitting details related to speaker identity and acoustic characteristics. In contrast, acoustic tokens are obtained using neural audio codecs (Zeghidour et al., 2021; Defossez et al., 2022; Yang et al., 2023) and focus on delivering precise reconstructions of acoustic features. However, recent models (Turetzky & Adi, 2024; Liu et al., 2024; Shi et al., 2024) have shown that speech models based on self-supervised learning (SSL) are effective at extracting acoustic representations where LMs be Table 5: Analysis of representation distillation from different models. BERT can be effectively combined with HuBERT or wav2vec 2.0, however, ELECTRA in LM-guided distillation outperforms BERT. indicates LM-guided Distillation. indicates combined LM and SM-guided Distillation. Bold highlights the best result and underline the second-best result. SM LM BERT Tokenizer DM-Codec DM-Codec ELECTRA DM-Codec HuBERT BERT DM-Codec wav2vec 2.0 BERT DM-Codec ELECTRA wav2vec 2.0 DM-Codec ELECTRA HuBERT - - WER WIL ViSQOL STOI 4.36 4. 4.18 4.13 4.70 4.67 7.06 6.63 6.84 6.77 7.51 7.58 3.18 3.10 3.13 3.15 3.14 2.94 0.935 0. 0.933 0.942 0.933 0.932 11 Table 6: Analysis of different distillation layers representation on speech reconstruction. Average layer provides more comprehensive representations. indicates LM-guided Distillation. indicates combined LM and SM-guided Distillation. Bold highlights the best result and underline the secondbest result. Tokenizer DM-Codec DM-Codec DM-Codec DM-Codec DM-Codec DM-Codec Distillation Layer(s) WER WIL ViSQOL STOI Average Last 9th Average Last 9th 4.36 4.62 4.75 4.18 4.68 4.52 7.06 7.56 7.80 6.84 7.55 7. 3.18 2.95 2.88 3.13 3.03 3.00 0.935 0.926 0.925 0.933 0.933 0.933 employed to refine these models further, enhancing their ability to extract more nuanced semantic representations. Textual Language Models in Speech. Research on speech models, including works by (Nguyen et al., 2023), (Borsos et al., 2023), and (Kharitonov et al., 2022), has focused on utilizing raw audio to extract prosodic features, identify speaker characteristics, and generate audio without depending on textual features or supervision from textual LMs. In contrast, many newer methods have started using audio encoders to transform audio signals into discrete tokens, which can be processed by textual LMs. TWIST method introduced by (Hassid et al., 2024) initializes the weights of the SpeechLM using pre-trained text LM, showing that this combination significantly improves performance. Similarly, the SELM model developed by (Wang et al., 2024) leverages GPT (Radford, 2018; Radford et al., 2019) as its foundation due to its enhanced parallel processing capabilities and capacity. However, text-based LLMs such as GPT-3 (Brown, 2020) and Llama (Touvron et al., 2023) are essential for speech modeling. Once discrete audio representations are obtained, these large text models are trained to align with or enhance the original text embedding space, as explored in studies by (Zhang et al., 2023), (Fathullah et al., 2023), (Shu et al., 2023), and (Rubenstein et al., 2023). This trend of integrating textual LMs into speech modeling has become increasingly popular in recent research."
        },
        {
            "title": "6 LIMITATIONS AND BROADER IMPACT",
            "content": "Limitations. In this work, we present the effectiveness of our proposed method, DM-Codec, based on the LibriSpeech dataset. Future research could investigate its performance across variety of datasets and domains. Additionally, exploring the capabilities of DM-Codec in multilingual contexts would be valuable. Another limitation of our work is the absence of experiments with emerging LLMs. Currently, we focus solely on masked language models to derive representations. Further investigation into these decoder-based LLMs impact on DM-Codec can be studied and addressed. Broader Impact. The integration of language models in speech processing has traditionally focused on model-specific implementations or specific training objectives. In this work, we propose novel approach by leveraging language model during the tokenization phase through our model, DMCodec. By incorporating language-specific representations from the corresponding text, DM-Codec enhances the quality of discrete speech representations. This method bridges the gap between language and speech models, offering more unified approach to multimodal representation learning. DM-Codec provides robust framework for generating high-quality audio representations, with potential applications in various domains, including multilingual speech processing, low-resource languages, and other audio-related tasks. Our findings pave the way for more effective and contextually aware speech processing models, contributing to advancements in the broader field of speech and language technologies."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this work, we introduced speech tokenizer DM-Codec, with two novel distillation methods to leverage multimodal (acoustic, semantic, and contextual) representations from language model and speech self-supervised learning model. Our extensive experimental results and ablation studies suggest that distilling multimodal representations enables DM-Codec to introduce salient speech information in discrete speech tokens. Our significance analysis further revealed that DM-Codec with comprehensive multimodal representations consistently outperforms existing speech tokenizers. This approach highlights the potential of multimodal representations to enhance speech tokenization in various domains, including multilingual and code-switched speech processing."
        },
        {
            "title": "REFERENCES",
            "content": "Alexei Baevski, Steffen Schneider, and Michael Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. arXiv preprint arXiv:1910.05453, 2019. Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: framework for self-supervised learning of speech representations, 2020. URL https://arxiv. org/abs/2006.11477. Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31:25232533, 2023. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for selfsupervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp. 244250. IEEE, 2021. Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators, 2020. URL https://arxiv.org/ abs/2003.10555. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding, 2019. URL https://arxiv.org/ abs/1810.04805. Rotem Dror, Segev Shlomov, and Roi Reichart. Deep dominance - how to properly compare deep neural models. In Anna Korhonen, David Traum, and Lluıs M`arquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 27732785, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1266. URL https://aclanthology.org/P19-1266. Tianqi Du, Yifei Wang, and Yisen Wang. On the role of discrete tokenization in visual representation learning, 2024. URL https://arxiv.org/abs/2407.09087. Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio compression, 2022. URL https://arxiv.org/abs/2210.13438. Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Jay Mahadeokar, Ozlem Kalinli, Christian Fuegen, and Mike Seltzer. Towards general-purpose speech abilities for large language models using unpaired data. arXiv preprint arXiv:2311.06753, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Visualizing and understanding the effectiveness of bert. arXiv preprint arXiv:1908.05620, 2019. Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al. Textually pretrained speech language models. Advances in Neural Information Processing Systems, 36, 2024. Andrew Hines, Jan Skoglund, Anil C. Kokaram, and Naomi Harte. Visqol: The virtual speech In International Workshop on Acoustic Signal Enhancement, 2012. quality objective listener. URL https://api.semanticscholar.org/CorpusID:14792040. Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM transactions on audio, speech, and language processing, 29:34513460, 2021. Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, and Sheng Zhao. Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models, 2024. URL https://arxiv.org/abs/2403. 03100. Lauri Juvela, Bajibabu Bollepalli, Xin Wang, Hirokazu Kameoka, Manu Airaksinen, Junichi Yamagishi, and Paavo Alku. Speech waveform synthesis from mfcc sequences with generative adversarial networks. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 56795683, 2018. doi: 10.1109/ICASSP.2018.8461852. Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, and Wei-Ning Hsu. Text-free prosody-aware generative spoken language modeling. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 86668681, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.593. URL https://aclanthology.org/2022.acl-long.593. Jaehyeon Kim, Jungil Kong, and Juhee Son. Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech, 2021. URL https://arxiv.org/abs/2106. 06103. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: generative adversarial networks for efficient and high fidelity speech synthesis. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of bert. arXiv preprint arXiv:1908.08593, 2019. Alexander Liu, Heng-Jui Chang, Michael Auli, Wei-Ning Hsu, and Jim Glass. Dinosr: Selfdistillation and online clustering for self-supervised speech representation learning. Advances in Neural Information Processing Systems, 36, 2024. Shoval Messica and Yossi Adi. Nast: Noise aware speech tokenization for speech language models. arXiv preprint arXiv:2406.11037, 2024. Tu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden Tomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken dialogue language modeling. Transactions of the Association for Computational Linguistics, 11: 250266, 2023. Jingcheng Niu, Wenjie Lu, and Gerald Penn. Does bert rediscover classical nlp pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, pp. 31433153, 2022. OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus In 2015 IEEE International Conference on Acoustics, based on public domain audio books. Speech and Signal Processing (ICASSP), pp. 52065210, 2015. doi: 10.1109/ICASSP.2015. 7178964. 14 Alec Radford. Improving language understanding by generative pre-training. 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2849228518. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/radford23a.html. Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalan Borsos, Felix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. Phillip Rust, Jonas Pfeiffer, Ivan Vulic, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer? on the monolingual performance of multilingual language models. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 31183135, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.243. URL https: //aclanthology.org/2021.acl-long.243. Leyuan Sheng, Dong-Yan Huang, and Evgeniy N. Pavlovskiy. High-quality speech synthesis using super-resolution mel-spectrogram, 2019. URL https://arxiv.org/abs/1912.01167. Jiatong Shi, Xutai Ma, Hirofumi Inaguma, Anna Sun, and Shinji Watanabe. Mmm: Multi-layer multi-residual multi-stream discrete speech representation from self-supervised learning model. arXiv preprint arXiv:2406.09869, 2024. Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, and Yemin Shi. Llasm: Large language and speech model. arXiv preprint arXiv:2308.15930, 2023. Marco Tagliasacchi, Yunpeng Li, Karolis Misiunas, and Dominik Roblek. SEANet: Multi-Modal Speech Enhancement Network. In Proc. Interspeech 2020, pp. 11261130, 2020. doi: 10.21437/ Interspeech.2020-1563. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Arnon Turetzky and Yossi Adi. Last: Language model aware speech tokenization, 2024. URL https://arxiv.org/abs/2409.03701. Ziqian Wang, Xinfa Zhu, Zihan Zhang, YuanJun Lv, Ning Jiang, Guoqing Zhao, and Lei Xie. Selm: Speech enhancement using discrete tokens and language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1156111565. IEEE, 2024. Dongchao Yang, Songxiang Liu, Rongjie Huang, Jinchuan Tian, Chao Weng, and Yuexian Zou. Hifi-codec: Group-residual vector quantization for high fidelity audio codec, 2023. URL https: //arxiv.org/abs/2305.02765. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Trans. Audio, Speech and Lang. Proc., 30:495507, nov 2021. ISSN 2329-9290. doi: 10.1109/TASLP.2021.3129994. URL https: //doi.org/10.1109/TASLP.2021.3129994. Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. 15 Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, and Xipeng Qiu. Speechtokenizer: Unified speech tokenizer for speech large language models, 2024a. URL https://arxiv.org/abs/ 2308.16692. Ziqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, et al. Speechlm: Enhanced speech pre-training with unpaired textual data. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024b."
        }
    ],
    "affiliations": [
        "Amazon GenAI, USA",
        "Center for Computational & Data Sciences, Independent University, Bangladesh",
        "Qatar Computing Research Institute, Qatar",
        "University of Virginia, USA"
    ]
}