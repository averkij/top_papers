{
    "paper_title": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning",
    "authors": [
        "Yicheng Chen",
        "Zerun Ma",
        "Xinchen Xie",
        "Yining Li",
        "Kai Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is a primary driver of model performance. A key lever is the \\emph{data recipe}, which comprises a data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate \\emph{end-to-end data recipe generation} for LLM adaptation. Given a target benchmark and a pool of available data sources, a model is required to output a complete data recipe that adapts a base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using a proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7B-Base to the math domain, achieving 66.7 on AIME'25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems."
        },
        {
            "title": "Start",
            "content": "DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning Yicheng Chen1,2, Zerun Ma2, Xinchen Xie2, Yining Li2, Kai Chen2 1Fudan University 2Shanghai AI Laboratory Github: https://github.com/yichengchen24/DataChef 6 2 0 2 1 1 ] . [ 1 9 8 0 1 1 . 2 0 6 2 : r Figure 1: (a) Formulation. Given task instruction, evaluation protocol, and raw data sources, model is required to generate data recipe, including an executable pipeline and the resulting training dataset, for LLM adaptation. (b) Main results. DataChef matches the performance of recipes from Gemini-3-Pro across six held-out tasks. See details in Sec. 4.2."
        },
        {
            "title": "Abstract",
            "content": "In the current landscape of Large Language Models (LLMs), the curation of large-scale, high-quality training data is primary driver of model performance. key lever is the data recipe, which comprises data processing pipeline to transform raw sources into training corpora. Despite the growing use of LLMs to automate individual data processing steps, such as data synthesis and filtering, the overall design of data recipes remains largely manual and labor-intensive, requiring substantial human expertise and iteration. To bridge this gap, we formulate end-to-end data recipe generation for LLM adaptation. Given target benchmark and pool of available data sources, model is required to output complete data recipe that adapts base LLM to the target task. We present DataChef-32B, which performs online reinforcement learning using proxy reward that predicts downstream performance for candidate recipes. Across six held-out tasks, DataChef-32B produces practical recipes that Corresponding Author. reach comparable downstream performance to those curated by human experts. Notably, the recipe from DataChef-32B adapts Qwen3-1.7BBase to the math domain, achieving 66.7 on AIME25 and surpassing Qwen3-1.7B. This work sheds new light on automating LLM training and developing self-evolving AI systems."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Large Language Models (LLMs) (DeepSeek-AI et al., 2025; OpenAI, 2025) has underscored the central role of data in determining model capabilities, making shift toward data-centric AI (Jakubik et al., 2024). The composition and quality of training data emerge as decisive factors in shaping downstream performance. Practically, constructing effective training data requires well-designed multi-stage pipeline that processes heterogeneous raw data through sequence of operations, such as transformation, filtering, mixing, synthesis, and refinement, tailored to specific training goals or stages (Yang et al., 2025; Cai et al., 2025). In this work, we formalize the concept of 1 data recipe, defined as the combination of the processing pipeline and the resulting training data. In practice, constructing data recipe typically involves substantial human expertise and manual effort, with human experts orchestrating subset of data processing operations in specific order (Penedo et al., 2025; Gururajan et al., 2024). While LLMs are widely used in individual processing operations, such as data filtering (Liu et al., 2024), data selection (Zhang et al., 2025d), and data synthesis (Mitra et al., 2024; Huang et al., 2024), they still follow human-designed prompt or pattern to prepare data. Recent studies have explored automating data pipeline orchestration to reduce the reliance on manual effort. In particular, Data-Juicer Sandbox (Chen et al., 2025a) proposes Probe-Analyze-Refine workflow to identify the most impactful operators from predefined operation pool, combine effective operations, and optimize data utilization through systematic experiments in data processing, model training, and evaluation with model performance as feedback. However, the continuous scaling of data and model sizes, coupled with the increasing complexity of processing operations, renders an exhaustive exploration of the combinatorial space of data recipes infeasible. Therefore, an essential question arises: can AI systems automatically generate data recipe for training LLMs, including the orchestration of data pipelines and the implementation of each operation, in cost-efficient way? To bridge this gap, we introduce new task: endto-end data recipe generation for LLM adaptation. As shown in Fig 1(a), given target benchmark and pool of available data sources, the objective is to generate complete data recipe by specifying the precise data processing pipeline to yield training data for adapting an LLM to the target task. Producing an effective data recipe requires strong reasoning abilities, as it involves analyzing heterogeneous data sources, applying domainspecific processing operations, and generating corresponding executable code. While recent studies have demonstrated the effectiveness of reinforcement learning in enhancing LLM reasoning abilities in complex domains, such as coding (Zeng et al., 2025) and mathematical reasoning (Yeo et al., 2025), applying this paradigm to our task poses two key challenges: (1) Data absence: end-to-end data recipe generation is previously unexplored task, for which no curated datasets or standardized evaluation benchmarks exist. (2) Expensive and delayed supervision: while downstream performance naturally serves as the reward signal, it is impractical to directly incorporate downstream LLM training into an online reinforcement learning loop. To address these challenges, we curate comprehensive task pool comprising 31 widely used benchmarks across 10 distinct domains. These domains encompass reasoning-heavy fields, such as mathematics and coding, as well as knowledge-centric fields, such as finance, medicine, and the natural sciences. The pool is partitioned into 25 training tasks and 6 held-out evaluation tasks, with each task supported by 815 source training datasets. We further propose data verifier that assesses training data quality directly without performing model training, providing low-cost, instant reward signal in online RL. We systematically validate that the data verifier prediction correlates well with downstream model performance across domains. Leveraging cold-start fine-tuning and online reinforcement learning, we present DataChef-32B, an LLM specialized for generating optimal data recipes. Through extensive evaluations, DataChef-32B demonstrates matching capability to Gemini-3-Pro on data recipe generation. Fig. 1(b) shows that DataChef-32B produces effective data recipes on 6 hold-out tasks, yielding high performance on downstream benchmarks. The generated recipes outperform the best individual data source on most tasks. Notably, on the math and atmosphere domain, recipes from DataChef-32B adapt Qwen31.7B-Base to achieve 66.7 on AIME25 (AIME, 2025) and 46.3 on ClimaQA (Manivannan et al., 2025), respectively, surpassing Qwen3-1.7B with industry-level post-training on expert-curated data recipes. In summary, our contributions are as follows: We formulate new task, end-to-end data recipe generation for LLM adaptation, requiring models to automatically generate data recipes from benchmark and available data sources. We construct large-scale and diverse data pool covering 19 domains, 31 benchmarks, and 257 datasets to support this task. We propose an efficient learning framework with proxy reward that enables scalable online RL. Extensive experiments show that our DataChef-32B achieves performance comparable to that of top-tier proprietary models on the data recipe generation task."
        },
        {
            "title": "2 Related Work",
            "content": "Data Pipelines. Many existing approaches rely on human experts to design individual data processing heuristics, including data mixing (Liu et al., 2025b), data sampling (Xu et al., 2024; Chen et al., 2025d), and data synthesis (Chen et al., 2025c). General-purpose data processing frameworks (Chen et al., 2024a; Park et al., 2025) provide standardized modules and scalable pipeline construction for large-scale data processing, and are adopted to curate large-scale, high-quality training data, such as FinWeb2 (Penedo et al., 2025) for multilingual pre-training and Aloe (Gururajan et al., 2024) for medical-domain fine-tuning. However, their efficiency remains constrained by the manual pipeline design and iterative trial-and-error on downstream tasks. Data-Juicer Sandbox (Chen et al., 2025a) marking step further towards automated data pipeline construction by employing Probe-Analyze-Refine workflow to assess operator effectiveness, but still relies on feedback derived from downstream model training, which is time and computation-consuming. In contrast, our work aims to end-to-end generate data recipes from scratch. LLM Agents for Data Science. LLM-based agent systems have emerged as powerful tools for automating data science workflows, including data analysis, modeling, and visualization. Most existing approaches (Hollmann et al., 2023; Li et al., 2024b; Hong et al., 2025) rely on promptbased approaches, where complex tasks are decomposed and solved according to heuristically designed workflows. AIDE (Jiang et al., 2025) and SELA (Chi et al., 2024) further adopt iterative exploration and refinement through trial-and-error execution. Yet such prompt-driven strategies remain largely static and are constrained by the inherent knowledge limitations of LLMs. To alleviate these limitations, some studies incorporate external knowledge via search-based methods, leveraging offline repositories such as Kaggle solutions and research papers (Guo et al., 2024; Ou et al., 2025; Kulibaba et al., 2025) or online web search (Nam et al., 2025). Another line of work (Liu et al., 2025c; Zhang et al., 2025c) explores learningbased agents, where agents improve performance through interaction and experience. However, these methods are typically evaluated on well-defined Kaggle competitions (Chan et al., 2025; Zhang et al., 2025b; Jing et al., 2025) with static datasets, and even with curated initial code. In this work, we address an open-ended setting, taking arbitrary tasks and available datasets as input and directly generating data recipes for LLM training. Data Evaluation. Training and evaluating LLMs require significantly more computational resources, motivating the use of lightweight proxies to assess model performance (Chen et al., 2025a). Existing data evaluation approaches (Qin et al., 2024; Zhang et al., 2025a) can be broadly categorized into three groups. (1) Indicator-based methods (Li et al., 2024a; Friedman and Dieng, 2023) define handcrafted metrics to quantify properties such as diversity, complexity, and relevance. (2) Modelbased methods (Ge et al., 2024; Liu et al., 2024) train predictive models to estimate data quality. (3) LLM-as-a-Judge approaches (Chen et al., 2024b) prompts powerful LLMs to evaluate data according to specific protocols. However, the correlation between data assessment scores and downstream model performance remains underexplored. Prior work typically validates evaluators by comparing specific data selections against baselines, rather than through systematic correlation analysis. To bridge this gap, we conduct comprehensive study of representative assessment methods, evaluating their alignment with model performance across diverse fine-tuning tasks."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first formalize some core concepts and define the data recipe generation task in Sec 3.1. Then, we introduce the specific data pool constructed for this study in Sec 3.2. Finally, we present our learning framework in Sec 3.3."
        },
        {
            "title": "3.1 Problem Formulation",
            "content": "The goal of our method is to automatically generate data recipe given specific task. We formulate task as triplet = (I, τ, D), where is natural language instruction, including description of the task requirement, along with meta-information of data sources and evaluation protocol, denotes the set of available raw data sources, and τ is an evaluation metric that maps any model to scalar performance score τ (M) R. data recipe is formulated as = (g, d), where is data pipeline and = g(D) is the resulting training dataset. In our experiments, the data pipeline is implemented as Python scripts. Let Mθ denote language model. We use θd 3 Figure 2: Illustration of DataChef training framework. Given task, policy LLM generates data recipe, which is executed to produce training dataset. The Data Verifier then evaluates sampled subset to provide scalar reward, guiding the policy update via GRPO to optimize for data quality and executability. to present the parameters fine-tuned on dataset d. We aim to learn policy πϕ(r ) that generates data recipes to maximize the expected downstream performance of the trained model. Formally, the objective function is defined as: (ϕ) = Erπϕ(T )[τ (LMθd)] (1)"
        },
        {
            "title": "3.2 Task Pool Construction",
            "content": "Seed Task Curation. As previously unexplored task, data recipe generation lacks canonical corpus. To bridge this gap, we construct diverse task pool encompassing 19 heterogeneous domains, including reasoning, coding, and knowledge-intensive fields such as healthcare, finance, and natural science. For each domain, we select representative benchmarks (e.g., GSM8K and AIME25 for mathematics), totaling 31 benchmarks. For each benchmark, we retrieve relevant candidate datasets from Hugging Face, prioritizing those with high community engagement (downloads and likes), yielding repository of 257 distinct data sources. From this collection, we construct 25 seed tasks for training and reserve 6 heldout tasks (3 in-domain, 3 out-of-domain) for evaluation. Comprehensive details of the selected benchmarks are provided in Appx. A.1. Training Task Augmentation. To facilitate robust policy learning, we expand the 25 seed tasks into large-scale training set Ttrain. We employ probabilistic sampling strategy where benchmark τ is selected proportional to its source count D, followed by uniform sampling of subset to form new instance = (I , τ, D). After deduplication, the expansion strategy yields 5K unique task instances."
        },
        {
            "title": "3.3 End-to-end Data Recipe Generation",
            "content": "Framework Overview. As illustrated in Fig. 2, our framework optimizes the policy πϕ to generate high-quality data recipes. Given task , the policy generates data pipeline g, which consists of natural language plan for orchestrating data pipelines and its corresponding implementation as an executable code block. During training, the pipeline transforms raw data sources into training dataset d, which is then evaluated by the Data Verifier to guide policy updates via reinforcement learning. During inference, the data recipe is directly used for downstream model adaptation. Cold-start Initialization. Training the policy from scratch using RL is non-trivial due to the low executability of data recipes, leading to sparse, highvariance rewards and ineffective exploration (Shao et al., 2024; Liu et al., 2025c). To mitigate this, we employ cold-start Supervised Fine-Tuning (SFT) phase. We observe that decoupling reasoning and coding yields superior inference-time performance (as discussed in Sec. 4.4). Therefore, we construct high-quality demonstration set using decoupled generation process: strong reasoning model proposes plans, and specialized coding model implements them. We filter these pairs by execution success and data quality, retaining only valid recipes. Initializing πϕ on this curated dataset 4 equips the policy with foundational capability for code generation, significantly stabilizing the subsequent RL phase. Reward Modeling. Ideally, the reward signal would be the downstream performance τ (Mθd). However, using this as an online reward is computationally prohibitive due to the cost of repeated model training and evaluation. Instead, we design computationally efficient surrogate reward based on the quality of the generated dataset d. Inspired by rubrics-based rewards (Gunjal et al., 2025), we employ strong LLM as Data Verifier to classify each instance into one of five categories with assigned scalar scores s(x): Invalid (0): Samples with missing essential information or severe repetition. Format Error (0): Samples violating explicit output format constraints. Incorrect (0): Samples containing factual errors or wrong answers. Task Mismatch (0.4): Valid samples that are semantically irrelevant to the target task I. Pass (1.0): High-quality samples that satisfy all criteria. To ensure computational efficiency during online training, we estimate the dataset quality by randomly sampling subset ˆd d. Let s( ˆd) be the average instance score over this sampled subset. We define the final recipe reward R(r) by incorporating penalties for execution failures: R(r) = λ, λfmt, s( ˆd), if = (execution failure), if violates training format, otherwise, (2) where λ and λfmt are positive penalty coefficients. Please refer to Appx. A.2 for detailed description of the category definitions used in the prompt. Reinforcement Learning. We employ Group Relative Policy Optimization (GRPO) for policy optimization. For each task Ttrain, we sample group of candidate data recipes {ri}G i=1 from the current policy πϕold. The policy parameters are optimized by maximizing the following objective: (ϕ) = (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 (cid:16) min ρiAi, clip(ρi, 1 ϵ, 1 + ϵ) Ai (cid:17) (cid:16) β DKL πϕ πref (cid:35) (cid:17) where ρi = πϕ(riT ) πϕold (3) (riT ) is the importance ratio, σ+δ Ai = R(ri)µ is the group-relative advantage, ϵ is the clipping parameter, πref is fixed reference policy, and β controls KL regularization."
        },
        {
            "title": "4.1 Setups",
            "content": "Notably, Training. For cold-start SFT, we train Qwen332B (Yang et al., 2025) on 5K high-quality synthetic instances for 2 epochs, utilizing learning rate of 2e-5 and batch size of 32. In the RL phase, we further optimize the SFT checkpoint using GRPO (Shao et al., 2024) for 1 epoch on the same dataset, with learning rate of 5e-7. During RL, the rollout batch size is set to 128 with temperature of 1.0, and we sample 8 candidate data recipes per task. Evaluation Set. We evaluate on 6 held-out tasks: 3 in-domain tasks and 3 out-of-domain these in-domain evaluation tasks. tasks share domains with the training set but remain strictly unseen during training. The indomain benchmarks include PHYSICS (Feng et al., 2025), AIME25 (AIME, 2025), and LiveCodeBench v6 (Jain et al., 2024); the out-ofdomain benchmarks are OpenFinData (Information, 2023), ClimaQA (Manivannan et al., 2025), and CHID (Zheng et al., 2019). Metrics. Executing recipes and performing downstream fine-tuning and evaluation are computeintensive, rendering large-scale end-to-end evaluation impractical. Accordingly, for each evaluation task, we generate candidate set of = 32 independent data recipes. Based on this candidate set, we report two metrics: (1) DVSavg@32: the mean Data Verifier Score across all 32 recipes. This metric quantifies the expected quality and stability of the policy, where recipes failing to yield valid training data are assigned score of 0. (2) DBS: the Downstream Benchmark Score of model trained on single recipe, which is randomly sampled from the subset of candidates with valid execution (i.e., DVS > 0). This metric reflects the actual performance on the downstream benchmark of successfully executed recipe. Additionally, to approximate the oracle upper bound for DataChef-32B, we select the most promising recipe from the candidate set and report its downstream score. For all downstream evaluation, we fine-tune Qwen3-1.7B-Base for 3 epochs with learning rate of 2e-5 and batch size of 64. Baselines. We compare DataChef-32B against 5 Table 1: Main Results on six held-out tasks. We report the mean Data Verifier Score DVSavg@32 and the Downstream Benchmark Score DBS, where the Average column presents DBS as normalized percentage relative to SOURCEbest (100.0). Qwen3-Next Kimi-K2 denotes combination using Qwen3-Next-80B for reasoning and Kimi-K2-Instruct for coding. DataChef-32B achieves performance comparable to the closed-source Gemini-3-Pro and significantly outperforms other open-source baselines across all settings. Indomain Tasks Method SOURCEavg SOURCEbest EXPERT Qwen3-32B Kimi-K2 Qwen3-Next Kimi-K2 Gemini-3-Pro DataChef-32B DataChef-32B (Oracle) Out-of-domain Tasks Method SOURCEavg SOURCEbest EXPERT Qwen3-32B Kimi-K2 Qwen3-Next Kimi-K2 Gemini-3-Pro DataChef-32B DataChef-32B (Oracle) PHYSICS AIME LIVECODE Average DVSavg@32 DBS DVSavg@32 DBS DVSavg@32 DBS DVSavg@32 DBS - - - 11.0 19.7 48.7 69.7 61.4 - 6.1 8.5 20.8 5.9 9.0 8.9 9.2 8.7 10.4 - - - 31.5 35.4 78.3 80.7 84.7 - 23.4 39.6 33. 13.3 20.0 23.3 30.0 30.0 66.7 - - - 24.3 19.3 39.2 53.6 45.8 - 6.3 10.3 25.7 8.0 9.7 7.4 9.1 9.1 10.3 - - - 22.3 24.8 55.4 68.0 64.0 - 63.9 100.0 123.3 56.7 83.7 78.6 91.2 89.3 130.3 CLIMAQA OPENFIN CHID Average DVSavg@32 DBS DVSavg@32 DBS DVSavg@32 DBS DVSavg@32 DBS - - - 20.6 18.3 41.5 58.4 57.3 - 26.0 43.6 44.2 35.6 41.8 42.6 44.3 42.1 46. - - - 34.9 51.5 54.7 54.9 67.0 - 41.7 63.7 73.4 23.8 46.5 64.0 61.8 63.9 67.1 - - - 1.1 23.1 8.6 29.7 7.9 - 49.3 70.3 59.8 12.3 3.9 4.1 21.9 20.5 45.7 - - - 18.9 31.0 34.9 47.6 44.1 - 65.1 100.0 101.0 45.5 58.2 68.0 76.6 75.4 92. three categories of models: (1) parameter-matched model: Qwen3-32B; (2) open-source flagships: Kimi-K2-Instruct (Team et al., 2025) and Qwen3Next-80B-A3B-Thinking; (3) closed-source SOTA: Gemini-3-Pro (Google, 2025). Additionally, we incorporate the following results as reference: First, to benchmark the raw data quality, we manually format each available source and report the average (SOURCEavg) and best (SOURCEbest) downstream performance among these single-source datasets. Second, as high-standard reference, we report EXPERT: the performance of Qwen3-1.7B optimized via industry-grade post-training with expertcurated recipes. Detailed experiment setups are provided in Appx. B."
        },
        {
            "title": "4.2 Main Results",
            "content": "Main Comparison. Table 1 presents the performance of DataChef-32B against baselines across in-domain and out-of-domain tasks. DataChef32B achieves superior performance compared to strong practical baseline, Qwen3-Next Kimi-K2, which leverages open-source state-of-the-art specialized models (Qwen3-Next-80B-A3B-Thinking for reasoning and Kimi-K2-Instruct for coding). Specifically, our end-to-end model surpasses this composite system with average improvements of +8.6% and +9.2% in DVSavg@32, and +10.7% and +7.4% in DBS on in-domain and out-ofdomain tasks, respectively. Notably, DataChef-32B achieves performance comparable to the closedsource top-tier Gemini-3-Pro, demonstrating exceptional robustness and effectiveness in automated data recipe generation. Surpassing Human Baselines. By selecting the most promising recipe from 32 samples (Oracle Upper Bound), DataChef-32B outputforms SOURCEbest on most tasks, achieving an average score of 130.3 on in-domain benchmarks. This indicates that DataChef goes beyond simple dataset selection and synthesizes novel data processing pipelines, including effective selection, mixing, synthesis, and filtering, which are superior to raw manual formatting. Remarkably, it achieves 66.7 on AIME25 and 46.3 on ClimaQA, even surpassing the EXPERT baseline with industry-level post6 Figure 3: Correlation analysis of data evaluation metrics. (left) We summarize the Pearson correlation coefficients across all six evaluated tasks. (right) We detail the relationship between metric scores (X-axis) and downstream performance (Y-axis) on Language and Code tasks. The Data Verifier maintains strong, consistent positive correlation across disparate domains. Please refer to Fig. 8 in Appx. for complete results. Table 2: Ablation study on training stages and reward design. We investigate the impact of the cold-start phase and the granularity of the reward signal. Rdense denotes our proposed fine-grained Data Verifier score, while Rsparse represents constant success reward for valid execution. Model Cold Start RL Reward Performance (DVSavg@32) In-Domain Out-of-Domain MBaseline MRL MSparse DataChef-8B - Rdense Rsparse Rdense 4.1 32.9 62.7 63.2 5.5 23.9 44.1 46.8 Table 3: Analysis of collaborating with strong coding models. We compare the end-to-end paradigm against decoupled approaches where the model acts solely as planner, relying on an external coder (Kimi-K2-Instruct) for implementation. Model External Coder Performance (DVSavg@32) In-Domain Out-of-Domain Inference-Time Qwen3-32B Qwen3-32B Training Paradigm Planner-32B DataChef-32B 22.3 40.3 56.7 64.0 18.9 33.1 37.3 44.1 in Fig. 3, with supplementary plots in Appx. D, our Data Verifier exhibits strong positive correlation with downstream model performance across all six tasks, achieving an average Pearson correlation of 0.59. Crucially, the Data Verifier maintains consistent positive correlation across diverse task settings, indicating superior robustness compared to baselines."
        },
        {
            "title": "4.4 Ablation and Analysis",
            "content": "Effectiveness of RL. Fig. 4 illustrates that reward values consistently trend upward while the standard deviation decreases during training, confirming the convergence and effectiveness of RL process. Heldout evaluation reveal that RL primarily enhances Figure 4: Analysis of RL Effectiveness. (a) RL training dynamics indicate that the policy consistently converges toward high-quality data recipe generation. (b) Evaluation results show that RL yields substantial improvements on out-of-domain tasks. training on expert-curated data recipes. These results underscore the potential of fully automating data recipe generation for LLM training."
        },
        {
            "title": "4.3 Data Verifier",
            "content": "To validate the proposed Data Verifier, we analyze the Pearson correlation between the verifier scores and downstream benchmark performance. We benchmark against several widely used data evaluation metrics, including IFD (Li et al., 2024a), RewardModelScore (Liu et al., 2025a), DEITA (Liu et al., 2024), and VendiScore (Friedman and Dieng, 2023). To ensure diversity in data quality and model performance, we construct 812 datasets per task under fixed data budget using two strategies: (1) Direct sampling from available task-specific data sources. (2) Subset selection from the pool formed in (1) based on response length. As shown 7 Figure 5: Analysis of operation frequency in generated recipes. We compare the average number of function calls per recipe across different models. generalization, yielding significant gains on outof-domain tasks while preserving in-domain performance. Quantitatively, RL delivers an average DVSavg@32 improvement of 3.6% for the 8B model and 3.7% for the 32B model. Effectiveness of Cold Start. Table. 2 shows that omitting the cold start leads to significant performance degradation across all domains. To understand this behavior, we analyze the distribution of function calls in Fig. 5. The results demonstrate that the direct RL model tends to generate simplistic data pipelines, reducing the usage of complex data processing operations. We hypothesize that without the SFT warm-up, the model succumbs to reward hacking. It avoids execution penalties by generating safe, trivial scripts rather than optimizing for data quality. In contrast, DataChef-8B leverages the SFT foundation to explore and deploy sophisticated operations, such as filtering and data augmentation. Ablation on Reward Signal. To assess the effectiveness of the fine-grained data quality feedback, we conduct an ablation where the continuous verifier score s( ˆd) in Eq. 2 is replaced by constant success reward (i.e., assigning fixed value of 1.0 to any valid data recipe). Table 2 demonstrates that this quality-agnostic signal leads to noticeable performance drops. This result confirms that the model relies on the guidance from the Data Verifier to distinguish high-utility recipes from merely executable ones. Collaborating with Strong Coder. Given the proliferation of specialized coding models, natural idea is to decouple this task: use the primary model as planner (natural language orchestration) and an external coder for implementation. Table 3 shows that this paradigm enhances inference-time performance, with Qwen3-32B paired with KimiK2-Instruct yielding 18.0% and 14.5% DVSavg@32 gains on in-domain and out-of-domain tasks, re8 Figure 6: Visualization of data distribution in generated recipes. We project the source datasets and the data recipes generated by different models into 2D embedding space. spectively. However, training the model solely as planner leads to suboptimal results compared to the end-to-end approach. This suggests that integrated training of planning and coding capabilities is essential for optimal data recipe generation. Case Study. We quantitatively analyze the data recipes generated by Qwen-32B and DataChef-32B for the out-of-domain financial task in Fig 6. We categorize source datasets that yield high downstream performance as High-perf proxy, and those performing poorly as Low-Perf Sources. DataChef32B demonstrates an emergent ability to identify and prioritize high-utility datasets. Additionally, we provide detailed data processing pipelines with code examples in Appx. C, which reveals that DataChef-32B can: (1) automatically leverage LLMs to augment data into task-specific formats or to synthesize data to enhance target ability; and (2) extract the most relevant data subsets using self-generated keywords."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose novel paradigm for automated data recipe generation to streamline LLM adaptation. To facilitate this, we establish holistic dataset for both training and evaluation. Building on this foundation, we present DataChef-32B, incorporating data verifier that serves as costeffective reward function for online RL. DataChef32B demonstrates strong generalization capabilities, matching human-level expertise on specific benchmarks. Our work bridges the gap between data curation and model evolution, fostering the development of self-evolving AI. Limitations. Our reliance on an LLM-as-a-Judge for proxy rewards prioritizes generalizability but may sacrifice precision in niche tasks. Developing specialized evaluators to offer higher-resolution reward signals remains valuable direction for future research."
        },
        {
            "title": "References",
            "content": "AIME. 2025. AIME problems and solutions. Chenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, Xiaoyang Wang, Zhanping Zhong, Yun Zhu, Dahua Lin, Conghui He, and Lijun Wu. 2025. Opendataarena: fair and open arena for benchmarking post-training dataset value. arXiv preprint arXiv:2512.14051. Jun Shern Chan, Neil Chowdhury, Oliver Jaffe, James Aung, Dane Sherburn, Evan Mays, Giulio Starace, Kevin Liu, Leon Maksin, Tejal Patwardhan, Lilian Weng, and Aleksander adry. 2025. Mle-bench: Evaluating machine learning agents on machine learning engineering. In ICLR. Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, and 1 others. 2024a. Data-juicer: one-stop data processing system for large language models. In Companion of the 2024 International Conference on Management of Data. Daoyuan Chen, Haibin Wang, Yilun Huang, Ce Ge, Yaliang Li, Bolin Ding, and Jingren Zhou. 2025a. Data-juicer sandbox: feedback-driven suite for multimodal data-model co-development. In ICML. Ding Chen, Qingchen Yu, Pengyuan Wang, Mengting Hu, Wentao Zhang, Zhengren Wang, Bo Tang, Feiyu Xiong, Xinchi Li, Chao Wang, Minchuan Yang, and Zhiyu Li. 2025b. xverify: Efficient answer verifier for reasoning model evaluations. arXiv preprint arXiv:2504.10481. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2024b. Alpagasus: Training better alpaca with fewer data. In ICLR. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. Yicheng Chen, Xiangtai Li, Yining Li, Yanhong Zeng, Jianzong Wu, Xiangyu Zhao, and Kai Chen. 2025c. Auto cherry-picker: Learning from high-quality generative data driven by language. In CVPR. Yicheng Chen, Yining Li, Kai Hu, Zerun Ma, Haochen Ye, and Kai Chen. 2025d. Mig: Automatic data selection for instruction tuning by maximizing inIn Findings of formation gain in semantic space. the Association for Computational Linguistics: ACL 2025. Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, and Chenglin Wu. 2024. Sela: Tree-search enhanced llm agents for automated machine learning. arXiv preprint arXiv:2410.17238. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. DeepSeek-AI, Aixin Liu, Aoxue Mei, Bangcai Lin, Bing Xue, Bingxuan Wang, Bingzheng Xu, Bochao Wu, Bowei Zhang, Chaofan Lin, Chen Dong, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenhao Xu, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, and 245 others. 2025. Deepseek-v3.2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556. Kaiyue Feng, Yilun Zhao, Yixin Liu, Tianyu Yang, Chen Zhao, John Sous, and Arman Cohan. 2025. Physics: Benchmarking foundation models on university-level physics problem solving. In Findings of the Association for Computational Linguistics: ACL 2025. Dan Friedman and Adji Bousso Dieng. 2023. The vendi score: diversity evaluation metric for machine learning. In TMLR. Yuan Ge, Yilun Liu, Chi Hu, Weibin Meng, Shimin Tao, Xiaofeng Zhao, Mahong Xia, Zhang Li, Boxing Chen, Hao Yang, and 1 others. 2024. Clustering and ranking: Diversity-preserved instruction selection through expert-aligned quality estimation. In EMNLP. Google. 2025. Gemini 3 pro. Anisha Gunjal, Anthony Wang, Elaine Lau, Vaskar Nath, Yunzhong He, Bing Liu, and Sean Hendryx. 9 2025. Rubrics as rewards: Reinforcement learnarXiv preprint ing beyond verifiable domains. arXiv:2507.17746. Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, and Jun Wang. 2024. Ds-agent: Automated data science by empowering large language models with case-based reasoning. In ICML. Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, and 1 others. 2024. Aloe: family of fine-tuned open healthcare llms. arXiv preprint arXiv:2405.01886. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In ICLR. Noah Hollmann, Samuel Müller, and Frank Hutter. 2023. Large language models for automated data science: Introducing caafe for context-aware automated feature engineering. In NIPS. Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Robert Tang, Xiangtao Lu, and 9 others. 2025. Data interpreter: An LLM agent for data science. In Findings of the Association for Computational Linguistics: ACL 2025. Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, and Xiaodan Liang. 2024. Mustard: Mastering uniform synthesis of theorem and proof data. In ICLR. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. Ceval: multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322. East Money Information. 2023. Openfindata. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. LiveCodeBench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974. Zhengyao Jiang, Dominik Schmidt, Dhruv Srikanth, Dixing Xu, Ian Kaplan, Deniss Jacenko, and Yuxiang Wu. 2025. Aide: Ai-driven exploration in the space of code. arXiv preprint arXiv:2502.13138. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. 2025. Dsbench: How far are data science agents to becoming data science experts? In ICLR. Josué Kpodo, Parisa Kordjamshidi, and Pouyan Nejadhashemi. 2024. Agxqa: benchmark for advanced agricultural extension question answering. Computers and Electronics in Agriculture. Stepan Kulibaba, Artem Dzhalilov, Roman Pakhomov, Oleg Svidchenko, Alexander Gasnikov, and Aleksei Shpilman. 2025. Kompeteai: Accelerated autonomous multi-agent system for end-to-end pipeline generation for machine learning problems. arXiv preprint arXiv:2508.10177. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. In EMNLP. Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: largescale hallucination evaluation benchmark for large language models. arXiv preprint arXiv:2305.11747. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024a. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. In ACL. Ziming Li, Qianbo Zang, David Ma, Jiawei Guo, Tuney Zheng, Minghao Liu, Xinyao Niu, Yue Wang, Jian Yang, Jiaheng Liu, Wanjun Zhong, Wangchunshu Zhou, Wenhao Huang, and Ge Zhang. 2024b. Autokaggle: multi-agent framework for autonomous data science competitions. arXiv preprint arXiv:2410.20424. Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, and Yahui Zhou. 2025a. Skywork-reward-v2: Scaling preference data curation via human-ai synergy. arXiv preprint arXiv:2507.01352. Qian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. 2025b. Regmix: Data mixture as regression for language model pre-training. In ICLR. Johannes Jakubik, Michael Vössing, Niklas Kühl, Jannis Walk, and Gerhard Satzger. 2024. Data-centric artificial intelligence. Business & Information Systems Engineering, 66(4):507515. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In ICLR. 10 Zexi Liu, Jingyi Chai, Xinyu Zhu, Shuo Tang, Rui Ye, Bo Zhang, Lei Bai, and Siheng Chen. 2025c. Ml-agent: Reinforcing llm agents for autonomous arXiv preprint machine learning engineering. arXiv:2505.23723. Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, and Yu Li. 2024. MoleculeQA: dataset to evaluate factual accuracy in molecular comprehension. In Findings of the Association for Computational Linguistics: EMNLP 2024. Veeramakali Vignesh Manivannan, Yasaman Jafari, Srikar Eranky, Spencer Ho, Rose Yu, Duncan WatsonParris, Yian Ma, Leon Bergen, and Taylor BergKirkpatrick. 2025. Climaqa: An automated evaluation framework for climate question answering models. In ICLR. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP. Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Martiño Ríos-García, Benedict Emoekabu, Aswanth Krishnan, Tanya Gupta, Mara Schilling-Wilhelmi, Macjonathan Okereke, Anagha Aneesh, Amir Mohammad Elahi, Mehrdad Asgari, Juliane Eberhardt, Hani M. Elbeheiry, María Victoria Gil, Maximilian Greiner, Caroline T. Holick, Christina Glaubitz, Tim Hoffmann, and 16 others. 2024. Are large language models superhuman chemists? arXiv preprint arXiv: 2404.01475. Arindam Mitra, Luciano Del Corro, Guoqing Zheng, Shweti Mahajan, Dany Rouhana, Andres Codas, Yadong Lu, Wei ge Chen, Olga Vrousgos, Corby Rosset, Fillipe Silva, Hamed Khanpour, Yash Lara, and Ahmed Awadallah. 2024. Agentinstruct: Toward generative teaching with agentic flows. arXiv preprint arXiv:2407.03502. Jaehyun Nam, Jinsung Yoon, Jiefeng Chen, Jinwoo Shin, Sercan Ö Arık, and Tomas Pfister. 2025. Mlestar: Machine learning engineering agent via search and targeted refinement. In NIPS. OpenAI, :, Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul K. Arora, Yu Bai, Bowen Baker, Haiming Bao, Boaz Barak, Ally Bennett, Tyler Bertao, Nivedita Brett, Eugene Brevdo, Greg Brockman, Sebastien Bubeck, and 108 others. 2025. gpt-oss120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925. OpenAI. 2025. Introducing gpt-5. Yixin Ou, Yujie Luo, Jingsheng Zheng, Lanning Wei, Shuofei Qiao, Jintian Zhang, Da Zheng, Huajun Chen, and Ningyu Zhang. 2025. Automind: Adaptive knowledgeable agent for automated data science. arXiv preprint arXiv:2506.10974. Hyunbyung Park, Sukyung Lee, Gyoungjin Gim, Yungi Kim, Dahyun Kim, and Chanjun Park. 2025. Dataverse: Open-source etl (extract, transform, load) pipeline for large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations). Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. 2025. Fineweb2: One pipeline to scale them alladapting pre-training data processing to every language. arXiv preprint arXiv:2506.20920. Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, and Xing Sun. 2024. Unleashing the power of data tsunami: comprehensive survey on data assessment and selection for instruction tuning of language models. In TMLR. Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, and 33 others. 2025. Phybench: Holistic evaluation of physical perception and reasoning in large language models. arXiv preprint arXiv:2504.16074. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Yiqing Shen, Zan Chen, Michail Mamalakis, Luhan He, Haiyang Xia, Tianbin Li, Yanzhou Su, Junjun He, and Yu Guang Wang. 2024. fine-tuning dataset and benchmark for large language models for protein understanding. arXiv e-prints arXiv:2406.05540. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, , and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, Zhuofu Chen, Jialei Cui, Hao Ding, Mengnan Dong, Angang Du, Chenzhuang Du, Dikang Du, Yulun Du, Yu Fan, and 150 others. 2025. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534. Yuan-Sen Ting, Tuan Dung Nguyen, Tirthankar Ghosal, Rui Pan, Hardik Arora, Zechang Sun, Tijmen de Haan, Nesar Ramachandra, Azton Wells, Sandeep Madireddy, and Alberto Accomazzi. 2024. AstroMLab 1: Who Wins Astronomy Jeopardy!? arXiv e-prints arXiv:2407.11194. 11 Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and Fei Huang. 2025. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244. Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, and Yisong Yue. 2025b. Datascibench: An llm agent benchmark for data science. arXiv preprint arXiv:2502.13897. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. 2024. Demystifying clip data. In ICLR. Shaolei Zhang, Ju Fan, Meihao Fan, Guoliang Li, and Xiaoyong Du. 2025c. Deepanalyze: Agentic large language models for autonomous data science. arXiv preprint arXiv:2510.16872. Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, and Lei Bai. 2025. Earthse: benchmark for evaluating earth scientific exploration capability of llms. arXiv e-prints arXiv:2505.17139. Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Yao. 2025d. Autonomous data selection with zeroshot generative classifiers for mathematical texts. In Findings of the Association for Computational Linguistics: ACL 2025. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang. 2023. Safetybench: Evaluating the safety of large language models with multiple choice questions. arXiv preprint arXiv:2309.07045. Chujie Zheng, Minlie Huang, and Aixin Sun. 2019. ChID: large-scale Chinese IDiom dataset for cloze test. In ACL. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Yuxin Zuo, Shang Qu, Yifei Li, Zhangren Chen, Xuekai Zhu, Ermo Hua, Kaiyan Zhang, Ning Ding, and Bowen Zhou. 2025. Medxpertqa: Benchmarking expert-level medical reasoning and understanding. arXiv preprint arXiv:2501.18362. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In EMNLP. Ken Yano, Zheheng Luo, Jimin Huang, Qianqian Xie, Masaki Asada, Chenhan Yuan, Kailai Yang, Makoto Miwa, Sophia Ananiadou, and Junichi Tsujii. 2025. ELAINE-medLLM: Lightweight English Japanese Chinese trilingual large language model for biomedical domain. In Proceedings of the 31st International Conference on Computational Linguistics. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. 2025. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373. Ming Yin, Yuanhao Qu, Ling Yang, Le Cong, and Mengdi Wang. 2025. Toward scientific reasoning in llms: Training from expert discussions via reinforcement learning. Jie Ying, Zihong Chen, Zhefan Wang, Wanli Jiang, Chenyang Wang, Zhonghang Yuan, Haoyang Su, Huanjun Kong, Fan Yang, and Nanqing Dong. 2025. Seedbench: multi-task benchmark for evaluating large language models in seed science. arXiv preprint arXiv:2505.13220. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. 2025. Acecoder: Acing coder rl via automated test-case synthesis. In ACL. Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, and Dianhui Chu. 2025a. survey on data selection for llm instruction tuning. Journal of Artificial Intelligence Research."
        },
        {
            "title": "A Implementation Details of DataChef",
            "content": "A.1 Details of Task Pool Table 4: List of benchmarks used in the task pool. Domain Code Comprehension Benchmark Usage HumanEval (Chen et al., 2021) LiveCodeBench v6 (Jain et al., 2024) OpenbookQA (Mihaylov et al., 2018) RACE (Lai et al., 2017) Instruction Following IFEval (Zhou et al., 2023) Agriculture AgXQA (Kpodo et al., 2024) SeedBench (Ying et al., 2025) Astronomy Astrobench (Ting et al., 2024) Biology Genome-Bench (Yin et al., 2025) MoleculeQA (Lu et al., 2024) ProteinLMBench (Shen et al., 2024) Chemistry ChemBench (Mirza et al., 2024) Earth Science EarthSE (Xu et al., 2025) General Knowledge Medical Finance C-Eval (Huang et al., 2023) MMLU (Hendrycks et al., 2021) MedXpertQA (Zuo et al., 2025) MedQA (Yano et al., 2025) OpenFinData (Information, 2023) Atmosphere ClimaQA (Manivannan et al., 2025) Physics Long Context Math Reasoning Safety Writing Language PHYBench (Qiu et al., 2025) PHYSICS (Feng et al., 2025) L-Eval (An et al., 2023) LongBench (Bai et al., 2023) GSM8K (Cobbe et al., 2021) AIME25 (AIME, 2025) BBH (Suzgun et al., 2022) HotpotQA (Yang et al., 2018) HaluEval (Li et al., 2023) SafetyBench (Zhang et al., 2023) WritingBench (Wu et al., 2025) CHID (Zheng et al., 2019) Train Test Train Train Train Train Train Train Train Train Train Train Train Train Train Train Train Test Test Train Test Train Train Train Test Train Train Train Train Train Test A.2 Prompt Templates and Model Selection Data Verifier. We employ gpt-oss-120b (OpenAI et al., 2025) as the backbone for the Data Verifier. The detailed rubric-based prompt used for evaluation is presented below. Cold-start Models. To construct high-quality coldstart supervision, we leverage two specialized models: Qwen3-Next-80B-A3B-Thinking (Yang et al., 2025) for planning and reasoning, and Kimi-K2Instruct (Team et al., 2025) for code implementation. The specific prompts used for these roles are provided below."
        },
        {
            "title": "B Details of Experiments Setup",
            "content": "B.1 Data Evaluation Metrics Settings We use the OpenDataArena-Tool1 for data assessment, adhering to its default configurations. The specific settings for the data evaluation metrics used in our experiments are as follows: 1https://github.com/OpenDataArena/ OpenDataArena-Tool 13 IFD. We employ Qwen2.5-3B-Instruct as the backend model to calculate the InstructionFollowing Difficulty (IFD) score. Following (Li et al., 2024a), instances with an IFD score > 1 are treated as outliers. To ensure robust correlation analysis, we assign score of 0 to these anomalies. DEITA. Following (Liu et al., 2024), we define the final data score as the product of the Complexity Score and the Quality Score. These scores are computed using the checkpoints provided in the official DEITA repository. RewardModelScore. We utilize SkyworkReward-V2-Llama-3.1-8B-40M (Liu et al., 2025a) to compute the reward score, serving as proxy for response quality. VendiScore. We employ Qwen3-Embedding0.6B to compute sample embeddings and utilize Euclidean distance as the similarity metric to calculate VendiScore, measuring the diversity of the dataset. B.2 Evaluation Setup All downstream task evaluations are conducted using the OpenCompass framework2. The detailed settings for each benchmark are as follows: PHYSICS. We employ xVerify-9B-C (Chen et al., 2025b) as the evaluator and report the average accuracy across all sub-tasks. AIME25. We evaluate on the 2025 subset (covering both Part and Part II). For each question, we generate 8 responses and report the average accuracy. xVerify-9B-C is used as the evaluator. LiveCodeBench v6. We utilize the official prompt guidelines and report the pass@1 metric. The LCBCGenerationEvaluator is used for assessment. ClimaQA. We employ xVerify-9B-C as the evaluator and report the average accuracy across all sub-tasks. OpenFinData. We use the OpenFinDataKWEvaluator and report the average accuracy across all sub-tasks. CHID. We report the average accuracy on both the development and test sets."
        },
        {
            "title": "C Case Study",
            "content": "To demonstrate the capability of our model, we present complete data processing pipeline gener2https://github.com/open-compass/opencompass ated by DataChef-32B for the ClimaQA task. As shown in Fig. 7, the generated code successfully produces valid training data by: (1) automatically leveraging LLMs to augment data into task-specific formats and synthesize samples to enhance target capabilities; and (2) extracting the most relevant data subsets using self-generated keywords."
        },
        {
            "title": "Analysis",
            "content": "We provide the comprehensive correlation analysis results across all six evaluation tasks in Fig. 8. These results validate the robustness of our Data Verifier compared to baseline metrics across diverse domains. 14 Prompt for Data Recipe Generation (Natural Language) # Task Description {{ task_description }} # Benchmark ## {{ benchmark.name }} {{ benchmark.description }} # Available Hugging Face Training Datasets {% for item in datasets -%} ## {{ item.dataset_id }} {{ item.examples }} {% endfor -%} Based on the Task Description, the target Benchmark, and the Available Hugging Face Training Datasets, design feasible Data Processing Plan. This plan must include: (1) **Data Selection**: Identify the most suitable raw datasets from the available list. (2) **Data Processing Workflow**: Define the pipeline to transform selected raw data into high-quality SFT training data. The plan will serve as blueprint for code generation and data production. Ensure it is comprehensive, actionable, and free from ambiguous or vague statements. high-quality SFT dataset must exhibit the following attributes: High Quality: Accurate samples free from noise. Diversity: Coverage of varied instructions and objectives. Relevance: Strong alignment with the target vertical domain. Important Constraints & Guidelines: Source Selection: strictly select datasets provided in the context. Prioritize those with complete metadata and clear field definitions. Strictly prohibit data contamination: DO NOT use benchmark data for training. Do not hallucinate datasets, splits, or configurations not provided in the context. Grounding: Base the processing workflow solely on the actual fields and content of the selected datasets. Do not make assumptions about data fields that do not exist. Context: Ensure inputs and outputs form coherent Q&A turns. If the output relies on specific context (e.g., document or snippet) within the data, design the workflow to explicitly embed this context into the input. Format Alignment: If the benchmark requires specific formats (e.g., multiple-choice, JSON output), ensure the constructed training data aligns with these requirements. Final Output Format: The pipeline must produce data in the following standard dialogue format: {\"dialogs\": [{\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]} LLM Utilization: Flexibly leverage LLM inference capabilities for tasks such as: extracting instructions from heterogeneous documents, data augmentation/synthesis, and quality verification. Clearly specify the prompt design strategies for these steps. Output Format: Generate the plan strictly in the following format (do not include preamble or additional text): ## Training Data [ {\"dataset_id\": \"target_id\", \"split\": \"target_split\", \"name\": \"config_name\", \"sample_num\": \"int\", \"reason\": \"justification\"}, ... ] ## Data Processing Workflow [Detailed Data Processing Workflow description] 15 Prompt for Data Recipe Generation (Executable Code) # Available Hugging Face Training Datasets {% for item in datasets -%} ## {{ item.dataset_id }} {{ item.examples }} {% endfor -%} # Data Processing Plan {{ plan }} # Tool Information {{ tool_info }} Based on the Available Hugging Face Training Datasets, the Data Processing Plan, and the Tool Information, generate the executable **Data Processing Script**. To validate the correctness of the processing, also generate corresponding **Verification Script**. Important Notes & Constraints: Implementation Logic: The script must clearly implement the processing rationale and utilize tools correctly. Verification Scope: The verification script should validate that the generated data matches expectations and confirm the effectiveness of core processing steps. Target Format: The final data must follow the ShareGPT format: {dialogs: [role: user, content: ..., role: assistant, content: ...]}. Use the format_to_sharegpt utility (import from aidp). Output Directory: Save the processed data to the data/processed/ directory. Output Format: Generate exactly two Python code blocks in the following format, without any additional text: python # data-processing code block python # test code block"
        },
        {
            "title": "Prompt for Data Verifier",
            "content": "As grading expert, your task is to determine whether the candidates response matches the question and to assess the samples usefulness for the specified task. Follow these evaluation guidelines precisely: Evaluation Protocol: 1. Validity Check: Reject questions that are: INCOMPLETE (cut off), REPETITIVE (loops), or NOT_ENOUGH_INFO. Reject answers that are: INCOMPLETE, REPETITIVE, REFUSAL (e.g., \"I cannot answer...\"), or IRRELEVANT. Action: Classify as boxed{A} and specify the reason (e.g., boxed{A} - INCOMPLETE). 2. Format Check: Ensure the answer follows any explicit output-format requirements (e.g., single choice, JSON schema). If no explicit format is required, pass. Action: If violated, classify as boxed{B} - FORMAT_ERROR. 3. Correctness Check: Re-generate concise reference answer and compare it with the candidates final answer. For multi-part questions, require all parts to be correct; partial correctness Fail. Action: If mismatched, classify as boxed{C} - INCORRECT. 4. Task-Alignment Check (Training-Suitability): Evaluate if the sample (Q+A) is useful for training the specified task. Scope Fit: Targets the capabilities the task trains. I/O Contract Impact: Beneficial/Benign (ALIGN): Mild deviations that still teach the target mapping (e.g., adding brief rationale where not strictly forbidden). Fatal Mismatch (MISMATCH): Conflicts likely to cause inference failure (e.g., task requires JSON but sample invents schema; task requires single letter but sample gives long essay without choice). Action: If harmful noise or fatal I/O mismatch boxed{D} - TASK_MISMATCH. Else boxed{E} - PASS. Grading Scale: boxed{A} - INVALID: Fails validity criteria. boxed{B} - FORMAT_ERROR: Fails format check. boxed{C} - INCORRECT: Deviates from reference answer. boxed{D} - TASK_MISMATCH: Fails task-alignment check. boxed{E} - PASS: Passes all checks. Execution Steps: 1. Thoroughly evaluate validity, format, correctness, and task-alignment step-by-step. 2. If any check fails, immediately assign the corresponding grade. Input Data: <Task Description Begin> {{ task_description }} <Task Description End> <Original Question Begin> {{ question }} <Original Question End> <Original Answer Begin> {{ llm_response }} <Original Answer End> Output Format: Analysis step by step: [...] Final Judgment: boxed{GRADE} - REASON 17 1 2 # data - processing code block 3 ... 4 5 # 1. Load stackexchange_earthscience 6 stack_ds = load_remote_dataset ( 7 \" mlfoundations - dev / stackexchange_earthscience \" , name =\" default \" , split =\" train \" , shuffle = False 8 9 25 26 27 28 29 10 11 ) 12 13 # 1.1 Use LLM to convert Open QA into 4option Multiple Choice Questions 14 SYSTEM_PROMPT_STACK = \"\"\" 15 You are science tutor . 16 Convert the given open QA pair into 4option multiple - choice question . 17 Return JSON only : 18 {\" question \": \" < original question >\" , \"A\": \" < option >\" , \"B\": \" < option >\" , \"C\": \" < option >\" , \"D\": \" < option >\" , \" answer \" : \" < correct letter >\"} 19 The correct answer must be one of the four options . Distractors should be scientifically plausible but wrong . 20 \"\"\" 21 22 def stack_parser ( response : str , raw : dict ) -> list : 23 parsed = extract_json ( response ) if not parsed or \" answer \" not in parsed : return [] opts = f\"A) { parsed ['A ']} nB ) { parsed ['B ']} nC ) { parsed ['C ']} nD ) { parsed ['D ']} \" user = f\" Question : { parsed [' question ']} nOptions : n{ opts }\" assistant = parsed [\" answer \" ]. lower () return [{ \" user \": user , \" assistant \": assistant }] 30 31 stack_mc = generate_dataset_with_llm ( 32 stack_ds , system_prompt = SYSTEM_PROMPT_STACK , response_parser = stack_parser 33 34 35 ) Generated Data Recipe Code (ClimaQA Case Study Part I) 18 5 6 1 2 # 2. Load ScienceQA_text_only 3 sciqa_ds = load_remote_dataset ( 4 \" tasksource / ScienceQA_text_only \" , name =\" default \" , split =\" train \" , shuffle = False 7 8 ) 9 10 # 2.1 Filter for climate - related topics 11 def climate_filter ( sample ): 13 20 21 22 23 32 39 txt = ( sample . get (\" category \" , \"\") + \" \" + sample . get (\" topic \" , \"\")). lower () return any (k in txt for in (\" climate \" , \" weather \" , \" atmosphere \" , \" ocean \" , \" environment \")) 14 15 sciqa_climate = select_by_filter ( sciqa_ds , climate_filter ) 16 17 # 2.2 Standardize format 18 def sciqa_map ( sample ): 19 choices = sample [\" choices \"] opts = \"n\". join ([ f\"{ chr (65+ i) }) {c}\" for , in enumerate ( choices ) ]) user = f\" Question : { sample [' question ']} nOptions : n{ opts }\" assistant = chr (65 + sample [\" answer \" ]) return {\" user \": user , \" assistant \": assistant } 24 25 sciqa_mc = sciqa_climate . map ( sciqa_map , remove_columns = sciqa_climate . column_names ) 26 27 # 3. Merge & Deduplicate 28 merged = concatenate_datasets ([ stack_mc , sciqa_mc ]) 29 deduped = deduplicate_by_text_hash ( merged , text_map = lambda x: x[\" user \"], lowercase = True , ignore_non_character = True 33 34 ) 35 36 # 4. Convert to ShareGPT format 37 sharegpt_ds = format_to_sharegpt ( 38 deduped , user_map = lambda x: x[\" user \"], assistant_map = lambda x: x[\" assistant \"] 40 41 ) 42 43 # 5. Save output 44 dump_dataset ( sharegpt_ds , \" data / processed / train_climaqa_style . jsonl \") Generated Data Recipe Code (ClimaQA Case Study Part II) Figure 7: Case study of data recipe generation. 19 Figure 8: Complete results for correlation analysis."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory"
    ]
}