{
    "paper_title": "R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization",
    "authors": [
        "Yi Yang",
        "Xiaoxuan He",
        "Hongkun Pan",
        "Xiyan Jiang",
        "Yan Deng",
        "Xingtao Yang",
        "Haoyu Lu",
        "Dacheng Yin",
        "Fengyun Rao",
        "Minfeng Zhu",
        "Bo Zhang",
        "Wei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains a significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, a multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose a cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1-Onevision-Bench, a benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks."
        },
        {
            "title": "Start",
            "content": "R1-Onevision: Advancing Generalized Multimodal Reasoning through Cross-Modal Formalization Yi Yang1 Xiaoxuan He1,2 Hongkun Pan1 Xiyan Jiang1 Yan Deng1 Xingtao Yang1 Fengyun Rao2 Minfeng Zhu1 Bo Zhang1 Wei Chen1 2 WeChat Vision, Tencent Inc. 3 Renmin University of China Haoyu Lu3 Dacheng Yin2 1 Zhejiang University 5 2 0 M 3 1 ] . [ 1 5 1 6 0 1 . 3 0 5 2 : r Figure 1. Comparison of baseline models and R1-Onevision. Deepseek-R1 struggles with perception errors due to GPT-4os incomplete image description and Qwen2.5-VL lacks the necessary reasoning ability to solve the problem. In contrast, R1-Onevision accurately interprets the image, applies structured reasoning, and arrives at the correct solution."
        },
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated remarkable reasoning capability in complex textual tasks. However, multimodal reasoning, which requires integrating visual and textual information, remains significant challenge. Existing visual-language models often struggle to effectively analyze and reason visual content, resulting in suboptimal performance on complex reasoning tasks. Moreover, the absence of comprehensive benchmarks hinders the accurate assessment of multimodal reasoning capabilities. In this paper, we introduce R1-Onevision, multimodal reasoning model designed to bridge the gap between visual perception and deep reasoning. To achieve this, we propose cross-modal reasoning pipeline that transforms images into formal textural representations, enabling precise language-based reasoning. Leveraging this pipeline, we construct the R1-Onevision dataset which provides detailed, step-by-step multimodal reasoning annotations across diverse domains. We further develop the R1-Onevision model through supervised fine-tuning and reinforcement learning Equal contribution to cultivate advanced reasoning and robust generalization abilities. To comprehensively evaluate multimodal reasoning performance across different grades, we introduce R1Onevision-Bench, benchmark aligned with human educational stages, covering exams from junior high school to university and beyond. Experimental results show that R1-Onevision achieves state-of-the-art performance, outperforming models such as GPT-4o and Qwen2.5-VL on multiple challenging multimodal reasoning benchmarks. 1. Introduction Recent language reasoning models like Deekseek-R1 [9], chain-of-thought prompting [29], Cumulative Reasoning [40] have achieved remarkable progress in solving complex problems including coding [4, 8], mathematics [6, 11], and science [28]. However, multimodal reasoning remains largely underexplored challenge. Unlike textual reasoning, multimodal reasoning requires the model to iteratively extract, structure, and verify information from images. Existing visual-language models often fail to organize available information and conduct in-depth reasoning processes, leading to failures in visual reasoning tasks. Current research on visual-language models has increasingly emphasized step-by-step reasoning. Some approaches, such as LLava-CoT [33] and Llama-V-o1 [25], employ predefined thinking structure to constrain the models reasoning process, limiting its robustness and creative potential. While such structured templates improve consistency, they often lead to shallow reasoning with limited comprehension. The others, like MAmmoTH-VL [10], rely on direct imitation of curated ground-truth answers, causing models to generate responses directly without trial-and-error manner. Consequently, these models may fail to generalize beyond their training distributions. An example of these limitations is shown in Fig. 1. DeepseekR1 suffers from perception errors due to incomplete image descriptions from GPT-4o [20], while Qwen2.5-VL [3], despite its strong multimodal capabilities, lacks deep reasoning ability and ultimately fails to solve the problem. These challenges highlight the core limitations of current multimodal reasoning approaches, which either impose rigid frameworks that restrict generalization or fail to provide human-like thinking behavior to accurately process visual information. In addition, there is lack of comprehensive benchmarks for evaluating multimodal reasoning capability. Existing benchmarks used by opencompass [7] mainly focus on mathematical problems, such as MathVision [26], MathVista [18], and WeMath [23]. While some more challenging datasets, like HumanEval-V [38], Humanitys Last Exam [21], and ZeroBench [24], aim to assess complex diagram understanding or broader knowledge proficiency. However, these benchmarks remain specialized, covering only limited aspects of reasoning. In this work, we address these challenges by introducing cross-modal reasoning pipeline to construct multimodal reasoning dataset, post-training strategy to empower reasoning capabilities, and comprehensive multimodal reasoning benchmark. First, we propose crossmodal reasoning pipeline that transforms images into visual formal representations and allows language models to process and reason over images precisely. We construct the R1-Onevision dataset to provide detailed stepby-step multimodal reasoning process over diverse domains, including natural scenes, charts, mathematical expressions, and science. Second, we employ two-stage post-training strategy to train our R1-Onevision model with advanced reasoning abilities. The supervised finetuning (SFT) stage cultivates coherent thinking patterns and output structures using our R1-Onevision dataset. The reinforcement learning (RL) stage strengthen both the models reasoning performance and its generalization across diverse tasks. Third, to evaluate multimodal reasoning model, we introduce R1-Onevision-Bench, comprehensive benchmark explicitly designed to evaluate grade-level reasoning performance across scientific domains in the human educational system: mathematics, physics, chemistry, biology, and logical deduction. Finally, extensive experiments on several benchmarks, including MathVision [26], MathVerse [39], MathVista [18], WeMath [23] and our R1-Onevision-Bench, demonstrate the superior multimodal reasoning performance of our R1-Onevision model. Our main contributions are summarized as follows: We present cross-modal reasoning pipeline to construct R1-Onevision Dataset which encompasses broad spectrum of images, questions and their reasoning process. We introduce R1-Onevision, visual-language reasoning model designed for complex problem-solving. We build R1-Onevision-Bench to assess multi-modal reasoning performance across different educational levels and subject areas in course-aware manner. Extensive experiments show the superiority of R1Onevision compared to the baseline Qwen2.5-VL model and closed-source models like GPT-4o. 2. Related Work Multimodal Large Language Models. MLLMs have shown significant potential in wide range of multimodal tasks. [3, 5, 19, 27] achieve excellent visual understanding ability by integrating both visual and textual data. Recently, Multimodal Large Language Models [10, 32] demonstrates the advanced reasoning abilities when interpreting visual tasks. In addition, some studies [25, 32] introduce planbased CoT prompting to guide models to generate intermediate information for predicting final answers. LLaVACoT [32] introduces novel VLM designed to conduct visual reasoning, demonstrating exceptional performance on tasks that require structured thinking and reasoning. Based on LLaVA-CoT, LlamaV-o1 [25] introduces multi-step curriculum learning approach, where tasks are progressively organized to facilitate incremental skill acquisition and problem-solving. CoMCTS [34] introduces the concept of collective learning into tree search for effective and efficient reasoning-path searching and learning. In this paper, we propose R1-Onevision, which uses supervised finetuning and reinforcement learning during the post-training phase to generate reasoning ability to different tasks. Large Language Model Reasoning. The development of robust reasoning capabilities in Large Language Models (LLMs) has been focal point of research [2, 12, 16, 22]. Some LLMs understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. For example, Chain-of-Thought (CoT) prompting, where complex question is decomposed into intermediate reasoning steps, have shown promise in guiding LLMs to structured solutions [30, 35]. Since certain visual tasks, such as solving Sudoku puzzles, urgently reFigure 2. Overview of our R1-Onevision framework. The cross-modal reasoning pipeline begins with data preparation, integrating visual formal descriptions to enhance reasoning. We employ role-playing strategies to generate the R1-Onevision dataset with high-quality reasoning data. Post-training consists of supervised fine-Tuning to learn reasoning manner, followed by rule-based Reinforcement Learning to improve generalization across multimodal tasks. quire multimodal large language models (MLLMs) to provide step-by-step reasoning capabilities, it is crucial to develop appropriate methods to enhance the performance of multimodal foundational models. Visual Reasoning Benchmarks. With the advancement of model capabilities in visual reasoning, an increasing number of studies have proposed various benchmarks to evaluate the reasoning abilities of these models [13, 15, 37]. MATH-Vision (MATH-V) [26] dataset collects 3,040 highquality mathematical problems with visual contexts sourced from real math competitions. DynaMATH [41] provides dynamic visual math benchmark specifically designed to evaluate the mathematical reasoning robustness of VLMs. In this paper, we introduce comprehensive benchmark designed to evaluate grade-level reasoning performance across scientific domains in the educational system. 3. Method Multimodal reasoning requires comprehensive understanding of both visual and textual modalities, yet existing models often fails to understand structured visual content and struggle with high-level reasoning. To bridge this gap, we introduce cross-modal reasoning pipeline that transfers reasoning capabilities of language models to visual modal through visual formal representations. Further, we employ post-training strategy to stabilize reasoning processes and improve generalization across diverse multimodal tasks. 3.1. Cross-Modal Reasoning Pipeline Our cross-modal reasoning pipeline is designed to bridge the gap between language reasoning models and vision models by integrating visual formal representations. Formal language is structured system with strict syntactic and semantic constraints that eliminate ambiguity, ensuring logical consistency. With formal description of visual contents, language reasoning models are able to see and reason over image elements. We use DeepSeek R1 [9] to generate reasoning processes on LLaVA-OneVision [17] and collect these data into the R1-Onevision dataset. Figure 2 provides the process of cross-modal reasoning. Examples of the generated data are provided in the supplementary. Data Curation and Filtering. For visual reasoning, we aggregate diverse multimodal datasets covering natural images, OCR-based text extraction, charts, mathematical expressions, and scientific reasoning problems, selecting only those that support structured reasoning. The final dataset incorporates components from the LLaVA-OneVision dataset [17], augmented with domain-specific datasets tailored for 3 complex inference tasks. Image Formal Description. key feature of our pipeline lies in its use of formal language-based annotations. To achieve this, we leveraged combination of GPT-4o, Grounding DINO, and EasyOCR to translate visual image content into textural formal description. We outline the annotation process below and provide details of the prompt design in the supplementary. Charts & Diagrams: We prompt GPT-4o to generate structured representations, such as SPICE for circuit schematics, PlantUML or Mermaid.js for flowcharts, HTML for UI layouts, CSV/JSON for tables, and Matplotlib for annotated charts. Natural Scenes: We enhance images with fine-grained spatial description by leveraging Grounding DINO to extract bounding box annotations of key elements and GPT4o to generate descriptive captions. Text-only Images: When dealing with images containing printed or handwritten text, we employ EasyOCR to extract text with positions and use GPT-4o to reconstruct the original document. Images with Visual and Textual Content: We integrate GPT-4o-generated captions, Grounding DINO bounding boxes, and EasyOCR-extracted text to ensure both textual and visual elements are accurately captured. Mathematical Images: For images containing mathematical content, we employ GPT-4o to propose reasoning strategies to guide the inference process. Reasoning Process Generation. Given an image, we prompt language reasoning model with its dense captions and questions to construct cross-modal Chain-of-Thought (CoT) data. While the original Chain-of-Thought (CoT) approach offered structured reasoning path based on textual captions, it inherently lacked the essential visual componentthe ability to directly see and interpret the image. To address this limitation, we propose Role-Playing strategy that emulates human-like visual comprehension. This method involves iteratively revisiting the image, refining its understanding, and enhancing the fidelity of the reasoning process. This process improves multimodal coherence and ensures contextually rich reasoning process. Quality Assurance. To ensure the reliability of generated reasoning process, we employ GPT-4o to remove inaccurate, irrelevant, or inconsistent CoT steps. This step guarantees high-quality dataset for multimodal reasoning. R1-Onevision Dataset. Finally, as shown in the Figure 3, the R1-Onevision dataset is carefully crafted tool designed to push the boundaries of multimodal reasoning. R1-Onevision encompasses wide range of domains, including science, mathematics, chart data, and general realworld scenarios, totaling over 155k carefully curated samples. R1-Onevision serves as rich resource for developing Figure 3. The data distribution of R1-Onevision dataset. R1OneVision consists of 155k data, including Science, Math, General and Doc/Chart/Screen. visual reasoning model. 3.2. Post-Training Strategy To enhance multimodal reasoning capabilities, we introduce two-stage post-training strategy consisting of Supervised Fine-Tuning (SFT) and rule-based Reinforcement Learning (RL). SFT stabilizes the models reasoning process and standardizes its output format, while RL further improves generalization across diverse multimodal tasks. 3.2.1. Supervised Finetuning Leveraging the R1-Onevision dataset, we enhance the reasoning capabilities of visual language models through SFT. SFT stabilizes the model output format and cultivates more sophisticated reasoning process in the large-scale model. This approach not only standardizes the output but also lays solid foundation, enabling subsequent RL to achieve significant performance gains. 3.2.2. Reinforcement Learning on the SFT Model Building upon the SFT-trained model, we employ rulebased reinforcement learning (RL) to optimize structured reasoning and ensure output validity. Specifically, we define two reward rules inspired by R1 and update the model using Group Relative Policy Optimization (GRPO). The RL stage further encourages the model to generate reliable outputs and enhances the generalizability of the model. Rule-Based Rewards. We define two reward rules that evaluate the generated answers from two perspectives: Accuracy Reward: The accuracy reward rule evaluates the correctness of the final answer by extracting final answer via regular expressions and verifying them against the ground truth. For deterministic tasks such as math problems, the final answer must be provided in specified format (e.g., within box) to enable reliable rulebased verification. In cases like object detection, the reward is determined by the Intersection over Union (IoU) 4 score with the ground truth. Format Reward: In order to ensure the existence of the reasoning process, the format reward rule requires that the response must follow strict format where the models reasoning is enclosed between <think> and </think>. regular expression ensures the presence and correct ordering of these reasoning markers. Group Relative Policy Optimization. We employ GRPO to achieve balanced integration of consistent policy updates and robust reward signals in controlled manner. For each token in the generated answer, GRPO first compute the log probabilities under both the new policy (π(θ)) and reference policy. GRPO then calculate ratio of these probabilities and clip it to the range [1 ϵ, 1 + ϵ] to prevent overly large updates. The normalized reward (serving as the advantage) is used in PPO-style loss: (cid:104) Lclip = (cid:16) min ratiot Advt, clipped ratiot Advt (cid:17)(cid:105) . Here, Advt denotes the advantage function, capturing how much better (or worse) particular action is compared to baseline policy value. To further maintain closeness to the reference distribution, KL divergence penalty (weighted by β) is added, yielding the overall loss: (cid:104) LGRPO(θ) = (cid:16) min ratiot Advt, clipped ratiot Advt (cid:17) (cid:16) β KL πθ(y x), πref(y x) (cid:17)(cid:105) . Compared to other methods, the GRPO clipping mechanism prevents extreme policy shifts, while the KL regularization keeps the updated policy aligned with the baseline. This combination ensures that our model integrates rule-based rewards efficiently without compromising training stability. 4. Multimodal Reasoning Benchmark To assess the reasoning capabilities of our R1-Onevision model, we introduce dedicated multimodal reasoning benchmarkR1-Onevision-Bench. This benchmark is designed to comprehensively evaluate the models performance across wide spectrum of reasoning tasks, spanning multiple domains such as mathematics, physics, chemistry, biology, and logical deduction. Inspired by human educational progression, R1Onevision-Bench is structured to reflect graded levels of complexity. It includes real-world reasoning tests at the middle school, high school, and university levels, alongside social test. This design not only mirrors the stages of cognitive development but also ensures that the evaluation covers both academic and practical reasoning skills."
        },
        {
            "title": "Total questions",
            "content": "- Multiple-choice questions - Free-form questions - Newly collected questions Grades/Categories/Subcategories"
        },
        {
            "title": "Number",
            "content": "942 783 (83.1%) 159 (16.9%) 942 (100.0%) 4/5/38 938 (99.6%) 942 (100.0%) 152 (16.1%) 390 54 127.2 2.1 Table 1. Key statistics of R1-Onevision-Bench. Figure 4. Overview of R1-Onevision-Bench. R1-OnevisionBench comprises 38 subcategories organized into 5 major domains, including Math, Biology, Chemistry, Physics, Deducation. Additionally, the tasks are categorized into five levels of difficulty, ranging from Junior High School to Social Test challenges, ensuring comprehensive evaluation of model capabilities across varying complexities. By integrating diverse problem types and difficulty levels, R1-Onevision-Bench provides rigorous framework for benchmarking multimodal reasoning. This enables us to better evaluate the grade at which multimodal language models exhibit reasoning capabilities and identify which aspects of knowledge and experience require supplementation to further enhance their performance. Moreover, our benchmark further subdivides each cate5 Figure 5. Samples across each category from R1-Onevision-Bench. R1-Onevision-Bench comprises multi-modal questions and answers in math, physics, chemistry, biology and deduction across multiple educational grades that require deep thinking and reasoning. gory into specific subcategories. Figure 5 presents the detailed distribution of tasks across various categories, subcategories, and grade levels, along with examples of data from multiple domains. 5. Experiment In this section, we first introduce the Experimental Setup of R1-Onevision in Section 5.1. Then, we present the main results in Section 5.2, demonstrating the effectiveness of R1-Onevision. Section 5.3 conduct detailed evaluation of various models on the benchmark, including open-source and closed-source models, thoroughly assessing their performance across different grades and categories. Additionally, Section 5.3 provides an in-depth analysis of the areas where these models exhibit weaknesses, identifying specific challenges and limitations that hinder their effectiveness. Finally, Section 5.4 conducts systematic analysis to evaluate the importance of various components during the training process. 5.1. Experimental Setup We evaluate R1-Onevision on several multimodal reasoning benchmarks, including MathVista [18], MathVision [26], MathVerse [39] and WeMath [23]. (1) MathVista: math benchmark designed to combine challenges from diverse mathematical and visual tasks. We use the Test Mini split, around 1000 samples. (2) MathVision: meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. We use the Test Mini split, consisting of 304 samples. (3) MathVerse: an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We apply the full dataset and also report the Vision-Only result which unveils the great challenge by rendering the whole question within the diagram. (4) WeMath: benchmark designed to explore the problem-solving principles beyond the end-to-end performance. We adopt the Test Mini split of WeMath, around 1740 samples, we report the average accuracy as the main metric. The performance metrics of all baseline models are derived from VLMEvalKits testing results [7]. We adopt Qwen2.5-VL series as baseline models, and conduct experiments on baselines 3B and 7B to examine the effectiveness of our method. The SFT experiments are conducted with batch size of 128, learning rate of 1e-5, and training over 1 epochs. Then, we perform RL on the Clever dataset [14]. We experiment with training subsets of 10k for single epoch each. More details about the dataset construction, the training process and the benchmark can be found in the supplementary. 5.2. Main Results Table 2 summarizes the quantitative results, showcasing that R1-OneVision consistently outperforms state-of-the-art (SoTA) multimodal methods across the majority of benchmarks. Notably, significant accuracy improvements are observed on MathVerse and MathVision, indicating that fine-grained visual-textual alignment and coherent chainof-thought reasoning play critical role in enhancing performance. These results validate the effectiveness of our proposed approach in addressing the diverse challenges of multimodal reasoning tasks. Additionally, R1-Onevision achieves 29.9% accuracy in MathVision, which is comIn Mathparable to the closed-source model GPT-4o. Verse and MathVista, R1-Onevision surpasses the GPT-4o 5.2% (MathVerse ALL), 5.5% (MathVerse Vision Only) and 4.1% (MathVista), further demonstrating its competitiveness in improving reasoning ability. 5.3. Benchmark Analysis We evaluate the performance of two sets of models in the R1-Onevision benchmark, which categorize into four levels of difficulty (ranging from Junior High School to Social Test) and five distinct academic disciplines (Math, 6 Model MathVision Base Model Qwen2.5-VL-7B [3] Our Models R1-Onevision-7B Other Models GPT-4o [20] InternVL2.5-8B [5] Qwen2-VL-7B [27] LLaVA-CoT-11B [33] 25.4 29.9 30.6 17.1 21.7 - ALL 43.6 46.4 41.2 35.6 30.2 - MathVerse Vision Only MathVista WeMath 38.2 40.0 34.5 22.8 25.4 22.6 63.7 64. 60.0 64.5 61.6 52.5 61.0 61.8 69.0 53.8 50.5 - Table 2. Experimental results on mathematical reasoning benchmarks. Except for LLaVA-CoT-11B, all reported results are based on our reproduced experiments. The highest accuracy is marked in blue . Model Closed-source GPT-4o Gemini-2.0-Flash Claude-3.5 Open-source MiniCPM-o-2.6 [36] InternVL2.5-8B [5] InternVL2.5-8B-MPO [5] Qwen2-VL-7B [27] Qwen2.5-VL-7B [3] DeepSeek-VL2 [31] R1-Onevision-7B Qwen2.5-VL-72B [3] Avg 49.6 59.1 52.1 30.4 29.5 32.5 30.0 32.1 29.8 36.2 52.0 Grade Category Junior High School High School College Social Test Math Physics Chemistry Biology Deduction 51.3 56.0 56.0 33.4 33.1 37.4 35.4 33.8 34.4 40.1 54.3 56.2 65.9 55. 31.7 30.6 33.6 28.5 37.1 30.9 39.5 56.7 45.3 61.2 49.4 21.2 21.8 24.7 25.9 25.3 18.8 27.6 54.1 26.5 39.8 30. 31.6 27.6 26.5 26.5 19.4 30.6 26.5 23.5 41.3 52.3 46.5 24.2 26.3 28.7 26.3 31.5 23.5 33.0 48.9 52.5 64.4 54. 31.7 24.8 29.9 28.1 27.3 28.4 30.2 55.8 71.4 74.3 66.7 30.5 32.4 41.0 30.5 39.0 29.5 49.5 63.8 63.4 67.2 65. 41.8 46.3 44.8 45.5 47.0 47.8 53.0 63.4 26.5 39.8 30.6 31.6 27.6 26.5 26.5 19.4 30.6 26.5 23.5 Table 3. Evaluation results of closed-source and open-source multimodal models on R1-Onevision benchmark. We divide the benchmark into four grades (Junior Hight School, High School, College, Social Test) and five categories (Math, Physics, Chemistry, Biology, Deduction). The highest accuracy for closed-source and open-source LMMs are marked in blue and green , respectively. Physics, Chemistry, Biology, Deduction). The fist group consisted of SOTA closed-source VLMs, such as GPT-4o, Gemini-2.0-Flash and Claude-3.5. The second group comprised SOTA open-source VLMs, including MiniCPM-o2.6, InternVL2.5-8B, InternVL2.5-8B-MPO, Qwen2-VL7B, Qwen2.5-VL-7B, Qwen2.5-VL-72B, DeepSeek-VL2 and R1-Onevision. We deploy VLMEvalkit package to infer in various models. Additionally, we evaluate the impact of model parameters on benchmark performance by testing the Qwen2.5-VL series, specifically comparing the 7B and 72B models. This analysis provides insights into how scaling model parameters influences performance across the benchmark, offering valuable guidance for future model development and optimization. As for scoring in R1-OneVision benchmark, following MathVision and MathVerse, we use GPT-4o-mini to extract the answer and score. The prompt of extract and score process is shown in the supplementary. Overall Results on Average Accuracy. Table 3 illustrates the average performance of variety of closed-source and open-source models. Closed-source models, such as GPT4o, Gemini-2.0-Flash and Claude-3.5 have demonstrated excellent performance on the leaderboard, with their average scores surpassing 50% those of other models. Notably, Gemini-2.0-Flash demonstrates robust performance across questions of varying difficulty levels, ranging from Junior High school to College-level tasks. It also excels across multiple categories, achieving scores above 50% in math, physics, chemistry and biology. Furthermore, Gemini-2.0Flash surpasses the closed-source model GPT-4o about 10% average accuray, demonstrating its effectiveness in reasoning ablity. However, both closed-source and open-source models exhibit stronger proficiency in tackling questions designed for middle and high school students, while their performance tends to decline on university-level and professional certification exams. Across various academic disci7 plines, all models struggle with Deduction questions, with none of the models exceeding 40% accuracy. Finally, while open-source models typically underperform compared to closed-source counterparts, recent advancements in models like Qwen2.5-VL-72B has significantly narrowed the gap, with Qwen2.5-VL-72B achieves 52% average accuracy. Qwen2.5-VL-72B is on par with close-source Claude3.5, ranking just behind Gemini-2.0-Flash. As for the 410B models, notably, R1-Onevision, developed based on Qwen2.5-VL, has delivered outstanding results, ranking just behind the top closed-source models. 5.4. Ablation Study 5.4.1. Analysis of Training Strategy To evaluate the effectiveness of our training data, we compare the models performance under two distinct training strategies: (1) applying Supervised Fine-Tuning (SFT) on our dataset, and (2) further optimizing the SFT-trained model with Reinforcement Learning (RL). As illustrated in Table 4, the application of SFT on our dataset significantly enhances the models performance on both the MathVision and MathVerse (Vision Only) benchmarks. Notably, the model achieves comparable results on the MathVision benchmark, demonstrating its ability to construct systematic thinking habits through our dataset. These findings underscore the value of our dataset in improving models reasoning capabilities, particularly in tasks requiring structured and logical problem-solving. Furthermore, as shown in Table 5, the subsequent application of RL after SFT yields additional performance gains. This step pushes the model toward deeper and more deductive thinking, enabling it to tackle more complex and nuanced problems. The incremental improvements observed highlight the complementary nature of SFT and RL: while SFT establishes robust foundation by aligning the model with high-quality reasoning patterns, RL refines and amplifies these capabilities by encouraging more advanced cognitive processes. This study demonstrates that our training data plays pivotal role in enhancing model performance, and the combination of SFT and RL provides powerful and effective strategy for maximizing reasoning and thinking performance. These results validate the quality and utility of our reasoning dataset. 5.4.2. Ablation Study on Model Parameters Variants To demonstrate the effectiveness of our approach across models of different parameter sizes, we conducted series of ablation experiments using Qwen2.5-VL-3B as smaller base model. Specifically, we applied both Supervised FineTuning (SFT) and Reinforcement Learning (RL) to the Qwen2.5VL-3B model and evaluated its performance. The experimental results, as summarized in Table 6, reveal significant improvements in reasoning and task performance comparing with the base Qwen2.5-VL-3B. R1-OneVisionModel MathVision MathVerse MathVerse (Vision Only) Qwen2.5-VL-7B SFT SFT+RL 25.4 26.3 29.9 43.6 43.4 46.4 38.2 39.7 40.0 Table 4. Ablation study on training strategies. The best results are marked in blue . Benchmark MathVision SFT+RL 29.9 RL 28.0 Table 5. SFT+RL and RL. The best results are marked in blue ."
        },
        {
            "title": "Comparison of MathVision performance between",
            "content": "Model MathVision MathVerse MathVerse (Vision Only) Qwen2.5-VL-3B R1-Onevision-3B 21.7 23.7 34.7 38. 31.2 35.5 Table 6. Evaluation of our method using Qwen2.5-VL-3B base model on MathVision, MathVerse and MathVerse (Vision Only). The best results are marked in blue . 3B achieves 23.6% accuray in MathVision, 38.6% in MathVerse (ALL). This performance significantly surpasses that of our base model, demonstrating remarkable improvement in reasoning capabilities. These findings highlight the scalability and adaptability of our method, showing that it remains effective across different model sizes. 6. Conclusion In this paper, we introduce comprehensive framework for multimodal reasoning, built upon cross-modal formalization approach that unifies data construction, model training, and evaluation. Our framework is designed to address the inherent challenges of integrating visual and textual modalities, enabling models to reason effectively across these domains. The cross-modal reasoning pipeline at the core of this framework bridges the gap between vision and language by leveraging fine-grained alignment This mechanisms and structured reasoning pathways. framework has led to the creation of the R1-Onevision dataset, rich resource featuring detailed step-by-step reasoning annotations designed to enhance model training and evaluation. The R1-Onevision model, trained using this framework, demonstrates strong multimodal reasoning capabilities and exhibits robust generalization across diverse range of tasks, from visual question answering to complex problem-solving scenarios. To further support the evaluation of multimodal reasoning, we introduce R1-Onevision-Bench, comprehensive benchmark that rigorously assesses model performance across various Our extensive experiments dimensions of reasoning. showing validate the effectiveness of our approach, significant improvements over state-of-the-art open-source models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 12 [2] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. 2 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 7 [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 1 [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 7 [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1 [7] OpenCompass Contributors. Opencompass: universal evaluation platform for foundation models, 2023. 2, 6 [8] Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando SolarLezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. In International Conference on Machine Learning, 2024. 1 [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, [10] Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, and Xiang Yue. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. 2 [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, 2024. 1 [12] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. 2 [13] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 3 [14] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning, 2016. 6 [15] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. In International Conference on Learning Representations Workshop Track, 2018. [16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 2 [17] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. 3 [18] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. 2, 6, 12 [19] AI Meta. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI Blog. Retrieved December, 20:2024, 2024. 2 [20] OpenAI. Gpt-4o system card, 2024. 2, 7 [21] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. 2 [22] Aske Plaat, Annie Wong, Suzan Verberne, Joost Broekens, Niki van Stein, and Thomas Back. Reasoning with large language models, survey. arXiv preprint arXiv:2407.11511, 2024. [23] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma GongQue, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. 2, 6 [24] Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. 2 [25] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed 9 [37] Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and SongChun Zhu. Raven: dataset for relational and analogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5317 5327, 2019. 3 [38] Fengji Zhang, Linquan Wu, Huiyu Bai, Guancheng Lin, Xiao Li, Xiao Yu, Yue Wang, Bei Chen, and Jacky Keung. Humaneval-v: Evaluating visual understanding and reasoning abilities of large multimodal models through coding tasks. arXiv preprint arXiv:2410.12381, 2024. [39] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, Peng Gao, and Hongsheng Li. Mathverse: Does your multi-modal llm truly see the diagrams in visual In European Conference on Computer math problems? Vision, page 169186, 2024. 2, 6 [40] Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew ChiChih Yao. Cumulative reasoning with large language models. arXiv preprint arXiv:2308.04371, 2023. 1 [41] Chengke Zou, Xingang Guo, Rui Yang, Junyu Zhang, Bin Hu, and Huan Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. 3 Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamavo1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 2 [26] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. 2, 3, 6, 12 [27] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, [28] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Advances in Neural Information Processing Systems, 2024. 1 [29] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1 [30] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022. 2 [31] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding, 2024. 7 [32] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason stepby-step. arXiv preprint arXiv:2411.10440, 2024. 2 [33] Guowei Xu, Peng Jin, Hao Li, Yibing Song, Lichao Sun, and Li Yuan. Llava-cot: Let vision language models reason stepby-step, 2025. 2, [34] Huanjin Yao, Jiaxing Huang, Wenhao Wu, Jingyi Zhang, Yibo Wang, Shunyu Liu, Yingjie Wang, Yuxin Song, Haocheng Feng, Li Shen, et al. Mulberry: Empowering mllm with o1-like reasoning and reflection via collective monte carlo tree search. arXiv preprint arXiv:2412.18319, 2024. 2 [35] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. 2 [36] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024."
        },
        {
            "title": "Roadmap of Appendix",
            "content": "The structure of the appendix is delineated as follows: Descriptions of the relevant experimental details are provided in the Section A. Subsequently, Section encompasses presentation of supplementary visualization results. A. More Implementation Details A.1. Data Details Our cross-modal reasoning pipeline consists of: Data Curation and Filtering, Image Formal Description, Reasoning Process Generation and Quality Assurance. We adapt the following prompt in Reasoning Process Generation. Answer the question and provide your reasoning process, including the following: 1. Simulate image reasoning: Treat the image caption as an image. Simulate reasoning by imagining you are looking at the image, and act as if you can see it. However, avoid visualization as step in the reasoning process. 2. Direct visual language: Frame observations as if you are directly viewing the image (e.g., The image shows...). Avoid reasoning through image caption or description. 3. Forbidden phrases: Avoid phrases like based on the caption, based on the description, visualizing the image. Question: {question} Image Content: {caption}. Then, we introduce role play to bridge the gap in in real image understanding and then filter the data. The prompts are as follows: 11 Revise the provided Chain of Thought (CoT) to follow these guidelines: 1. Style Shift: Convert all references to image description-based reasoning into direct imagebased reasoning. For example: Replace phrases like based on the description based on the caption with the image shows or as seen in the image. 2. Remove image visualization step: If the CoT contains an inference step for image visualization, remove it and rewrite the CoT to reflect reasoning directly on the image itself, rather than reasoning after visualization from the image description. Apply these changes rigorously to ensure that the final CoT reflects direct image interpretation, uninfluenced by description, caption, image visualization. CoT: {cot} Give your assistants response. This response is the reasoning steps for the assistant to solve the problem. Please follow the following rules to evaluate whether the assistants response is valid. Rules for judging as valid: 1. The assistants response has correct reasoning steps. 2. The assistants response has the final reasoning answer, and the final reasoning answer is consistent with the meaning of the standard answer. 3. reasoning process of the image, not description or caption. 4. There are no steps in the assistants response that are irrelevant to the reasoning, and each reasoning step is closely related. Standard answer: {gt} Assistants response: {augmented answer} Output: The assistants response is based on the the image A.2. Model Details For model training, we utilized the llama-factory and adopted full fine-tuning startegy to optimize the models Following this, we further refined the performance. model through Reinforcement Learning (RL). During the inference phase, the RL-tuned model was evaluated using fixed prompt to ensure consistent and reliable results. The prompt is defined as: First output the thinking process in <think> </think> tags and then output the final answer in <answer> </answer> tags.. A.3. Evaluation Details To evaluate the reasoning capabilities of the models, we adopted unified evaluation framework for both public benchmarks and our benchmark R1-OneVision bench. Following [18, 26], we utilized GPT-4o-mini [1] to assess model performance. For extracting answers from Chainof-Thought (CoT) responses, we employed the prompt Below is thought process and an answer that includes the final choices. Please extract only the final choices (A, B, C, D, etc.) from the text and do not return any other words. If the final choice is not explicitly stated in the text, output NONE. No reasoning is required; just extract the answer. for multiple choice questions and The following is thought process and free-form answer. Please extract the numerical value or text of the final answer, excluding any explanation. If the final answer cannot be extracted from the given text, output NONE. No reasoning is required; just extract the answer. for free form questions. Then we score the extracted answer with the groundtruth using Compare If final answer final answer with groundtruth. matches groundtruth, output YES; otherwise, output NO. Do not return any extra words. For numerical answers with units, if final answer contains the unit but its numeric value matches groundtruth, consider it match. B. Visualization B.1. Image Formal Description In this section, we demonstrate some examples of image formal description. Figure 7. Formal Description of Music Sheet. Figure 8. Formal Description of Table. Figure 6. Formal Description of Circuts. Figure 9. Formal Description of Natural Scene. 12 Figure 10. Formal Description of print, handwrite. Figure 13. The model displays an adeptness in processing math. Figure 11. Formal Description of Math. Figure 12. Formal Description of MEME. B.2. Model Reasoning Capabilities and Qualitative"
        },
        {
            "title": "Examples",
            "content": "In this section, we present some practical examples of our R1-OneVision. 13 Figure 14. The model displays an adeptness in processing physcis. Figure 15. The model displays an adeptness in processing chemistry. 14 Figure 16. The model displays an adeptness in processing math. 15 Figure 17. The model displays an adeptness in processing logic. 16 Figure 18. The model displays an adeptness in processing physical mechanics. 17 Figure 19. The model displays an adeptness in processing physical optics. 18 Figure 20. The model displays an adeptness in processing chemical reaction. 19 Figure 21. The model displays an adeptness in processing math geometry. 20 Figure 22. The model displays an adeptness in processing biology. 21 Figure 23. The model displays an adeptness in processing physical mechanics. 22 Figure 24. The model displays an adeptness in processing math. 23 Figure 25. The model displays an adeptness in processing math. 24 Figure 26. The model displays an adeptness in processing math. 25 Figure 27. The model displays an adeptness in processing math geometry. 26 Figure 28. The model displays an adeptness in processing math geometry."
        }
    ],
    "affiliations": [
        "Renmin University of China",
        "WeChat Vision, Tencent Inc.",
        "Zhejiang University"
    ]
}