{
    "paper_title": "Rethinking Video Generation Model for the Embodied World",
    "authors": [
        "Yufan Deng",
        "Zilin Pan",
        "Hongyu Zhang",
        "Xiaojie Li",
        "Ruoqing Hu",
        "Yufei Ding",
        "Yiming Zou",
        "Yan Zeng",
        "Daquan Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence."
        },
        {
            "title": "Start",
            "content": "Yufan Deng1,2 Zilin Pan1 Hongyu Zhang1 Xiaojie Li2 Ruoqing Hu2 Yufei Ding1 Yiming Zou1 Yan Zeng2 Daquan Zhou1 1Peking University, 2ByteDance Seed"
        },
        {
            "title": "Abstract",
            "content": "Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of standardized benchmark limits fair comparisons and progress. To address this gap, we introduce comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence. Project Page: https://dagroup-pku.github.io/ReVidgen.github.io/ GitHub Repo: https://github.com/DAGroup-PKU/ReVidgen/ HuggingFace Dataset: https://huggingface.co/datasets/DAGroup-PKU/RoVid-X/ 6 2 0 2 J 1 2 ] . [ 1 2 8 2 5 1 . 1 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in diffusion models [44, 79, 84] and video generation [35, 80, 83, 92, 98] have led to significant breakthroughs, enabling applications like video editing, multi-subject generation, and motion control [20, 21, 51, 53, 96]. These models have been extended to areas such as 3D scenes [57, 82], autonomous driving [28, 104], and world modeling [5, 54], showing strong generalization across tasks. recent study [97] suggests that, similar to LLMs in natural language processing, video models are evolving into unified foundation models for machine vision. Additionally, video models are being increasingly used in robot learning and action prediction [37, 38, 45, 61, 111, 113], as well as controllable simulators for synthesizing robotic video trajectories, addressing the lack of large-scale human teleoperation data [8, 47, 86]. These advancements 1 Figure 1 Overview of the comprehensive robotics benchmark and dataset for video generation. Top: We present RBench that includes the embodiment-based evaluation set and automated evaluation metrics. Our evaluation results on 25 video models show high level of agreement with subjective human assessments. Bottom: We introduce large-scale high-quality robotic dataset (RoVid-X) specifically designed for training video generation models, with data sourced from internet videos and open-source embodied videos. highlight the growing potential of video models in the perception-reasoning-action loop of embodied agents, paving the way for more generalizable intelligent systems in the physical world. Despite these strides, systematic evaluation for robotic video generation remains underdeveloped. Current practices rely mostly on perceptual metrics, focusing on visual quality [42, 46, 66], while existing physics-based benchmarks often lack task-specific datasets and criteria [36, 72]. Consequently, evaluations frequently overlook critical aspects such as task completion, action-goal alignment, and physical feasibility. This leads to overly optimistic conclusions, where high scores are assigned even to videos containing unnatural movements or incomplete tasks. The core challenge lies in rigorously assessing whether generated videos faithfully reproduce robotic behaviors. This necessitates evaluation protocols that transcend perceptual metrics, incorporating both the physical plausibility of actions and their alignment with instructions to ensure discriminative and reproducible assessments. To address this challenge, we propose RBench, benchmark designed to evaluate the fidelity and utility of video-based world models in robotic video generation. To the best of our knowledge, it is the first comprehensive benchmark with fine-grained metrics for robotic video generation, consisting of 650 imagetext pairs across five task categories and four robot types. Evaluations are based on two dimensions: task completion and visual quality, incorporating sub-metrics like structural consistency, physical plausibility, and execution completeness. Based on RBench, we conduct qualitative and quantitative assessments of 25 representative models. The results highlight that general video foundation models still have significant room for improvement in physical robot video generation, revealing persistent gap between these models and the requirements of embodied robotic tasks. This underscores the need for systematic advancements in both robotic video data and training 2 methodologies. Advancing general robotic video generation with human-like capabilities and adaptability requires diverse, scalable, and comprehensive training data [12, 78]. However, unlike computer vision and natural language processing, which can leverage vast web-scale datasets, robotic interaction data has long been constrained by both scale and diversity [10, 26, 110]. Even the largest existing collections are smaller and less varied than those for vision or language. More critically, many datasets have narrow distributions along key axes such as environment, object set, task spectrum, and robot morphology [95, 105], often confined to specific robot types, low-resolution recordings, or limited task ranges, which hampers the generalization of video foundation models. To address these gaps, we integrate over 20 open-source datasets and multi-source video platforms, creating four-stage end-to-end data pipeline. The stages include robot video collection, video quality filtering, task segmentation and captioning, and physical property annotation, resulting in RoVid-X, large-scale, high-quality embodied robotic video dataset (see Table 1). To our knowledge, RoVid-X is currently the largest dataset specifically designed for embodied video generation models, covering broad range of robot morphologies and task types. It aims to enhance video foundation models with physical interaction priors and task semantic diversity, driving further advancements in the field. Overall, the main contributions are summarized as follows: systematic benchmark tailored for robotic video generation. We propose RBench, which comprehensively evaluates the performance of video foundation models across five robotic tasks and four robot types with 650 meticulously curated evaluation samples, while introducing reproducible automated evaluation metrics. Key insights into robotic video generation for embodied research. We conduct systematic evaluation of 25 representative video models, including open-source, commercial, and robotics-specific ones, revealing the limitations of current video foundation models and potential directions for improvement, offering new perspectives for researchers exploring the embodied domain using video world models. large-scale, high-quality robotic video dataset. We construct RoVid-X, dataset containing approximately 4 million curated robotic videos with standardized task descriptions and physical property annotations, providing essential support for the training and evaluation of embodied video models."
        },
        {
            "title": "2.1 Video World Modeling for Robotics",
            "content": "The latest breakthroughs in video generation technology have led to the development of powerful models capable of generating high-quality videos from text or image prompts [59, 75, 80, 83, 92]. With the advancement of these technologies, an increasing number of studies have begun applying them to the field of embodied intelligence [2, 11, 25, 91]. Video provides rich source of information for robot training [14]. On one hand, video generation models can be used to synthesize robot trajectories [7, 8, 47], serving as an alternative to the time-consuming and labor-intensive process of human teleoperation data. And executable actions can be extracted through inverse dynamics models (IDM) [4, 22, 89, 112] or latent action models [88, 107]. On the other hand, recent studies have explored using video to simulate task dynamics and predict future states, thereby assisting in policy learning. Specifically, video generation models are used to initialize robot policies for training [50, 63, 100] or to simultaneously train policies and inverse dynamics models, enabling co-training with robot data [37, 60, 113]. These efforts demonstrate the significant potential of video generation models in the field of embodied robotics, highlighting their important value in improving training efficiency and task performance."
        },
        {
            "title": "2.2 Datasets for Robot Learning",
            "content": "A core challenge in robot learning is the lack of large-scale, diverse datasets that facilitate the training of general-purpose robots with physical interaction capabilities [8, 78]. Currently, datasets used in the embodied intelligence community for robot learning can be broadly classified into three categories: real-world robot data [10, 26, 71, 78, 110], human video data [18, 31, 32], and synthetic robot data [52, 70, 74, 90, 95, 105]. As key element in training physical AI models, most existing real-world robot datasets are collected through 3 Figure 2 Qualitative illustration of failure modes captured by RBench. Unlike conventional metrics that focus primarily on pixel-level fidelity, RBench provides granular evaluation across multiple dimensions, including physical plausibility and task-level consistency. These results highlight persistent challenges in robotic video generation, such as structural distortion, floating components, and key action omission, which are accurately identified by our proposed sub-metrics. More cases are shown in the Appendix B. robotic teleoperation [1, 27, 102] or by teams of human operators [9, 12, 68], which leads to high collection costs and limited data scale. Furthermore, these datasets predominantly focus on similar types of robots, resulting in issues of limited diversity and restricted environments [23, 56]. Additionally, inconsistent data collection and storage methods across different datasets make it difficult to enable effective cross-dataset co-training. Our focus is on collecting robot data for video generation that spans various robot morphologies and entities, and providing unified set of physical attributes for all data sources, thereby advancing cross-entity research in robot learning."
        },
        {
            "title": "2.3 Benchmarks for Video Generation",
            "content": "Establishing robust evaluation frameworks is essential for measuring the progress of video generation models. Currently, evaluation methodologies can be categorized into three primary streams: visual fidelity and semantics, which assess basic clarity and text-video alignment [67, 85, 108]; temporal dynamics, focusing on motion consistency and long-range narrative coherence [13, 48, 62]; and physical plausibility, which examines adherence to fundamental laws such as inertia and collision dynamics [6, 72, 73, 93]. While these benchmarks provide valuable insights into general video quality, they are largely decoupled from the specific requirements of embodied AI. Specifically, existing frameworks often rely on isolated physical constraints or local visual metrics, failing to capture the complex interplay between robotic actions and environmental responses. Furthermore, there is distinct lack of systematic evaluation for task-level correctness and spatial constraints in multi-embodiment scenarios. To bridge this gap, we propose comprehensive benchmark 4 Figure 3 Statistics in RBench. The benchmark covers diverse tasks, object categories, and environments, demonstrating the high quality and comprehensiveness of the evaluation set, highlighting its high applicability to wide range of robotic video generation scenarios. specifically tailored for robotic video generation, introducing reproducible metrics that unify physical realism with task-oriented action completeness."
        },
        {
            "title": "3 RBench",
            "content": "Existing video generation benchmarks primarily focus on evaluating model performance in general scenes [42, 46], while other benchmarks specifically designed for physical scenarios mainly assess models capabilities in physical reasoning [36, 72]. In this paper, we design benchmark tailored for robotic physical scenarios, aimed at comprehensively evaluating the performance of video generation models in robotic tasks. This benchmark differs from existing general scene benchmarks by focusing on evaluating video generation models capabilities in robotic physical environments. As shown in Figure 2, our benchmark highlights common failure modes in robotic video generation, including issues such as robot shape distortion, object attribute drift, non-contact attachment, and others. Section 3.1 outlines the process of benchmark construction, while Section 3.2 discusses the automatic metrics used for evaluation."
        },
        {
            "title": "3.1 Benchmark Construction",
            "content": "To comprehensively evaluate the capabilities of video generation models in robotic scenarios, the designed evaluation dimensions must cover wide range of task scenarios and embodiment types, ensuring that these scenarios reflect realistic robotic action semantics. To this end, we design diversified benchmark from two aspects: task categories and embodiment types, containing total of 650 evaluation cases. The task-oriented categories include five representative tasks: Common Manipulation, Long-horizon Planning, Multi-entity Collaboration, Spatial Relationship, and Visual Reasoning, with total of 250 image-text pairs, 50 samples for each task. The embodiment-specific categories cover four mainstream embodiment types: Dual-arm robots, Humanoid robots, Single-arm robots, and Quadruped robots, with total of 400 image-text pairs, 100 samples for each embodiment type. The benchmark includes variety of text prompts and high-quality robot reference images. Each sample image is keyframe extracted from high-quality videos sourced from public datasets or online sources, and each image is manually verified to ensure its accuracy. To avoid overlap with the training data, we ensure that the selected videos in the evaluation set do not appear in the subsequent training database, and we redesign new task prompts for each reference image, effectively avoiding the risk of content overlap. All samples are verified and filtered by human annotators to ensure that the generated task prompts align with realistic logic. Figure 3 illustrates the high aesthetic quality of the reference images (a), the broad range of testing scenarios including various objects, tasks, and action skills (b, c), and the diversity of environments in our evaluation set (d). Additionally, we record the metadata for each sample, such as the manipulated object, embodiment type, and camera viewpoint (first-person/third-person), to support more detailed analysis. See more details in the Appendix A."
        },
        {
            "title": "3.2 Automatic Metrics",
            "content": "Existing video generation evaluation protocols, such as the representative VBench [46], primarily focus on perceptual quality, assessing aspects like frame clarity, texture fidelity, and motion smoothness. However, they lack task-specific criteria tailored to robotic scenarios. Recently, several studies [33, 85, 94] have utilized multimodal large language models (MLLMs) as zero-shot evaluators for generated videos. Building upon this, we extend this approach to the domain of robotic video evaluation and propose set of automatic evaluation metrics, incorporating manually designed indicators to assess both the visual realism and task-level validity of generated robotic videos. Following previous practices, we select the open-source Qwen3-VL [3] and the closed-source GPT-5 [76] as our MLLM evaluation models. In the following sections, we introduce the evaluation methods for task completion and visual quality, respectively. Further details on metrics design and mathematical definitions are provided in the Appendix B."
        },
        {
            "title": "3.2.1 Task Completion",
            "content": "Physical-Semantic Plausibility. This metric targets everyday physical and semantic plausibility violations that standard perception scores often miss. As shown in Figure 1, we evaluate temporal grids of uniformly sampled frames with VQA-style protocol using MLLM. Beyond assessing physical-semantic plausibility, we place special emphasis on the following frequent failure modes: (i) Floating/Penetration: parts of the robot or objects are not grounded or interpenetrate with solid objects; (ii) Spontaneous emergence: entities appear/disappear without causal motion; (iii) Non-contact attachment/Incorrect grasp: objects move with the robot without visible contact or with improper gripper closure. Task-Adherence Consistency. This metric evaluates whether the video aligns with the intent and sequence defined by the prompt. Typical deviations include missing actions (e.g., approach without grasping or placing), incorrect order (e.g., placing before grasping), semantic drift (e.g., \"wiping\" becomes \"touching\"), and non-responsiveness. We construct temporal grids and apply an MLLM-based VQA checklist, which covers: (i) Task responsiveness, ensuring the goal state is reached without premature interruption; (ii) Key actions, verifying that required actions (e.g., grasp, place, open/close) occur and align with the prompt. 3.2.2 Visual Quality Motion Amplitude This metric measures the motion amplitude of the robotic subject while discounting apparent movement caused by camera motion, thereby penalizing videos that appear smooth but lack meaningful subject activity. Following VMBench [64], active subjects are localized with GroundingDINO [65], temporally stable masks are produced by GroundedSAM [81], and salient points are tracked via CoTracker [55]. Let Dt be the mean displacement of tracked points on the subject at frame t. The Motion Amplitude Score (MAS) is t=1 where lower MAS indicates insufficient subject motion and complements smoothness by revealing smoothbut-inactive failure modes. MAS = 1 (cid:88) min(cid:0) Dt, 1(cid:1), (1) Robot-Subject Stability. This metric assesses the stability of robot morphology and target object attributes over time. Typical failures include gripper/hand shape drifting into non-mechanical forms, extra/missing manipulators, link-length/topology changes, joint inversion, object misidentification or attribute drift (class, color, position), and impossible deformation of rigid items. We adopt contrastive VQA setup based on MLLM, which compares reference frame and generated frame and assigns consistency score targeting the above failures. Motion Smoothness This metric quantifies temporal continuity and natural dynamics, targeting artifacts from low-level aliasing to high-level jitter/blur. Following VMBench [64], we measure frame-to-frame quality stability with the Q-Align aesthetic score [99]. For frames {ft}T and per-frame score Q(ft), define: t=1 Qt = Q(ft1) Q(ft). (2) 6 Figure 4 Overview of RoVid-X Construction and Descriptive Statistics. (a) shows the four-stage pipeline for constructing the RoVid-X. (b) presents descriptive statistics, covering frame intervals, skill distribution, and common objects, highlighting the datasets diversity and suitability for robotic task training and video generation. temporal anomaly is flagged when Qt exceeds an adaptive threshold τs(t) determined by the robotic subjects motion. The Motion Smoothness Score (MSS) is MSS = 1 1 (cid:88) t=2 I(cid:0)Qt > τs(t)(cid:1), (3) where I() is the indicator function. higher MSS indicates smoother motion."
        },
        {
            "title": "4 RoVid-X",
            "content": "In this section, we introduce the construction of high-quality robotic video dataset, resulting in RoVid-X. The dataset is developed through refined four-stage pipeline, as shown in Figure 4 (a). The dataset primarily comes from internet-sourced robotic videos that are public domain or non-copyrighted, as well as open-source embodied video datasets, all of which are licensed for use. We then introduce the construction process of the dataset and provide statistical information."
        },
        {
            "title": "4.1 Dataset Construction",
            "content": "Our data processing workflow consists of four distinct stages, each designed to ensure the quality, diversity, and relevance of the collected data. These stages are outlined as follows: In the first stage, we collect raw robotic videos from large-scale internet video Robot Video Collection. platforms and over 20 open-source embodied video datasets. These datasets cover variety of robot types and task scenarios, ensuring the breadth and diversity of the data. To improve dataset relevance and quality, we employ the GPT-5 model [76] to automatically filter the content of each video and remove low-quality or irrelevant video clips that do not align with the research objectives. During the filtering process, GPT-5 identifies videos related to robotic tasks and actions based on visual content and subtitles, ensuring that all collected videos effectively support the training and evaluation of robotic tasks. After this filtering process, we identify approximately 3 million raw robotic video clips, covering different actions, tasks, and robot types. In this stage, we perform rigorous filtering procedure on the collected videos to Video Quality Filtering. remove low-quality and irrelevant video clips that do not align with the research objectives. First, we apply scene segmentation detection to remove all video data unrelated to robots. Then, we use video quality scoring system to assess the videos from multiple dimensions, including clarity, dynamic effects, aesthetic performance, and optical character recognition (OCR), among other metrics. Each video clip is assigned 7 Table 1 Comparison of representative robotic video datasets. Dataset Year #Videos #Skills Resolution Optical Flow Diverse Robotic Forms Diverse Captions RoboTurk [69] RoboNet [19] BridgeData [23] RH20T [24] DROID [56] Open X-Embodiment [78] RoboMIND [101] RoboCOIN [103] Galaxea [49] InternData-A1 [90] Fourier ActionNet [26] Humanoid Everyday [110] Agibot World [12] RoVid-X (Ours) 2018 2019 2021 2023 2024 2024 2024 2025 2025 2025 2025 2025 2025 2.1k 162k 7.2k 13k 76k 1.4M 107k 180k 100k 630k 13k 10.3k 1M 4M 2 N/A 4 33 86 217 38 36 58 18 16 221 87 480P 240P 480P 720P 720P 64P720P 480P 480P 720P 480P 800P 320P720P 480P 1300+ 720P quality score based on these criteria, ensuring that the videos retained in the final dataset meet high-quality standards. In this stage, we use video understanding model [34] and specially Task Segmentation and Captioning. designed prompt template to automatically analyze the robot actions within the videos. The system segments the videos into different task segments based on timestamps, generating short subtitles for each task segment that accurately describe the robots actions and operational details in that task. The action recognition and description process for each task segment follows these steps: First, the system identifies all dynamic actions within the video and excludes static scenes or irrelevant actions (e.g., waiting or remaining still). The time range for each action (start and end times) is precisely labeled to ensure accuracy. Next, using the MLLM model [34], textual descriptions of each action are automatically generated, including the action subject (e.g., \"right arm\" or \"left gripper\"), the object being manipulated (e.g., \"nameplate\" or \"box\"), and the specific operation details (e.g., \"grasp and move\" or \"remove from the table\"). Finally, the subtitles for each task segment are output in standardized format, ensuring that the action descriptions for each video clip are clear, concise, and aligned with the task requirements. Physical Property Annotation. To ensure consistency and realism of robot actions within physical space, we apply physical attribute enhancement to the videos. Specifically, we use FlashVSR [114] to improve the video resolution, making the images clearer and enhancing the details of the actions. Then, using the AllTracker tool [43], we annotate unified optical flow for the subjects in the videos, ensuring consistency in tracking and recording robot actions across different scenes. Additionally, using Video Depth Anything [16], we generate relative depth maps to accurately describe the spatial relationships and depth information of objects in the scene. The goal of these physical attribute annotations is to provide researchers with more precise reference data, aiding in the training and evaluation of robot video generation models and offering richer physical data support for future research."
        },
        {
            "title": "4.2 Dataset Analysis",
            "content": "RoVid-X is the first open-source large-scale robotic video dataset specifically designed for training video generation models, containing 4 million robotic video clips. This dataset is designed to address the physical challenges that video generation models face when generating robotic videos, providing high-quality data for both training and evaluation. RoVid-X aims to bridge the gap between traditional video generation tasks and the unique demands of embodied robot learning, where physical interaction, spatial relationships, and real-world dynamics play crucial role. The dataset includes diverse range of robotic actions, tasks, and robot types, ensuring its applicability across different robotic domains. By incorporating videos from various robot types and scenarios, RoVid-X provides comprehensive coverage of the physical properties and task requirements needed for robot training. As shown in Figure 4 (b), detailed statistics of the dataset are provided, illustrating the variety in terms of action skills, 8 Table 2 RBench quantitative results. Evaluations across task-oriented and embodiment-specific dimensions for 25 models from open-source, commercial, and robotics-specific families. The \"Avg.\" column shows the mean score across nine indicators, with task performance in the left block and embodiment performance in the right block. In the table, \"#\" next to the Sora2 model in the top right corner indicates review limitations from the official Sora API, where approximately 50 out of 650 videos could not be generated. The scores derived from sub-metrics are reported in the Appendix G. Models Rank Avg. Tasks Embodiments Manipulation Spatial Multi-entity Long-horizon Reasoning Single arm Dual arm Quadruped Humanoid Open-source Wan2.2_A14B [92] HunyuanVideo 1.5 [98] LongCat-Video [87] Wan2.1_14B [92] LTX-2 [40] Wan2.2_5B [92] SkyReels [15] LTX-Video [39] FramePack [109] HunyuanVideo [59] CogVideoX_5B [106] Commercial Wan 2.6 [92] Seedance 1.5 pro [17] Wan 2.5 [92] Hailuo v2 [41] Veo 3 [30] Seedance 1.0 [29] Kling 2.6 pro [58] Sora v2 Pro# [77] Sora v1 [75] Robotics-specific Cosmos 2.5 [2] DreamGen(gr1) [47] DreamGen(droid) [47] Vidar [25] UnifoLM-WMA-0 [91] 8 10 11 14 15 16 18 19 20 21 23 1 2 3 4 5 6 7 17 22 9 12 13 24 25 0.507 0.460 0.437 0.399 0.381 0.380 0.361 0.344 0.339 0.303 0. 0.607 0.584 0.570 0.565 0.563 0.551 0.534 0.362 0.266 0.464 0.420 0.405 0.206 0.123 0.381 0.442 0.372 0.344 0.284 0.331 0.203 0.302 0.206 0.177 0.116 0.546 0.577 0.527 0.560 0.521 0.542 0.529 0.208 0.151 0.358 0.312 0.358 0.073 0.036 0.454 0.316 0.310 0.268 0.304 0.313 0.276 0.176 0.258 0.180 0. 0.656 0.495 0.576 0.637 0.508 0.425 0.598 0.268 0.223 0.338 0.372 0.348 0.106 0.040 0.373 0.312 0.220 0.282 0.233 0.142 0.203 0.210 0.173 0.108 0.098 0.479 0.484 0.402 0.386 0.430 0.448 0.364 0.186 0.111 0.201 0.297 0.214 0.050 0.018 0.501 0.438 0.384 0.335 0.386 0.318 0.254 0.280 0.169 0.147 0. 0.514 0.570 0.496 0.545 0.530 0.454 0.530 0.255 0.166 0.496 0.334 0.316 0.054 0.062 0.330 0.364 0.186 0.205 0.164 0.234 0.234 0.241 0.170 0.035 0.079 0.531 0.470 0.437 0.474 0.504 0.442 0.358 0.115 0.139 0.399 0.215 0.339 0.050 0.000 0.608 0.513 0.586 0.464 0.453 0.436 0.507 0.440 0.440 0.454 0. 0.666 0.648 0.680 0.594 0.634 0.622 0.570 0.476 0.314 0.544 0.564 0.499 0.382 0.268 0.582 0.526 0.576 0.497 0.424 0.448 0.477 0.456 0.464 0.480 0.385 0.681 0.641 0.634 0.611 0.610 0.641 0.605 0.513 0.324 0.560 0.532 0.476 0.410 0.194 0.690 0.634 0.681 0.595 0.622 0.590 0.586 0.526 0.626 0.625 0. 0.723 0.680 0.726 0.640 0.689 0.698 0.637 0.664 0.544 0.658 0.579 0.542 0.374 0.293 0.648 0.595 0.621 0.599 0.555 0.607 0.509 0.464 0.548 0.524 0.496 0.667 0.692 0.654 0.635 0.637 0.686 0.613 0.561 0.419 0.626 0.575 0.556 0.357 0.200 task types, and interaction objects. The wide-ranging data distribution of RoVid-X is critical for supporting the development of robust video generation models that can simulate realistic robot behaviors in dynamic environments."
        },
        {
            "title": "5.1 Evaluation Setups\nEvaluation Models. We evaluate 25 state-of-the-art video generation models, grouped into three types.\nSpecifically, the closed-source models include Hailuo [41], Wan [92], Veo 3 [30], Sora [75, 77], Kling [58],\nSeedance [17, 29], and others, while the open-source models include several representative models such as\nHunyuanVideo [59, 98], LTX [39, 40] and CogVideoX [106]. Additionally, we assess models specifically designed\nfor robotic tasks, such as DreamGen [47], Vidar [25], and Cosmos 2.5 [2]. The evaluations of these models\ncover various types of embodiments and multiple tasks, providing a comprehensive perspective on model\nperformance.",
            "content": "Implementation Details. To ensure fair comparison, all open-source models generate videos using their official default configurations to ensure consistency with the models preset settings. For closed-source video models, we use their official APIs, strictly following the methods recommended by the developers for invoking and using the models. In the benchmark testing, we generate the videos for each image-text pair. To minimize errors, we generate three videos for each model sample and take the average as the final score for that sample. These generated videos are evaluated using the automated evaluation metrics that we propose, which are designed to comprehensively assess multiple aspects of the generated videos, including task completion, action consistency, physical plausibility, and more. Further details on the model setup and configuration parameters 9 Figure 5 Qualitative comparison across representative tasks. We visualize the generated results for three representative tasks: Visual Reasoning, Long-horizon Planning, and Spatial Relationship, across six models. Each row displays temporally sampled frames from the same generated video, with captions below indicating the corresponding task instruction. More cases are shown in the Appendix. are provided in the Appendix C."
        },
        {
            "title": "5.2 Main Analysis",
            "content": "5.2.1 Quantitative Results Table 2 presents comprehensive quantitative evaluation across varying model architectures, tasks, and embodiments. Beyond standard performance metrics, the results reveal pivotal paradigm shift in the video generation landscape. From Visual Fidelity to Physical Intelligence. The most significant trend observed is the transition of video generation models from pursuing high-fidelity visualization to addressing the complex dynamics of the physical world. While traditional metrics prioritize pixel-level quality, our benchmark highlights that top-tier commercial models (e.g., Wan 2.6, Seedance 1.5 Pro) are beginning to emerge as effective World Simulators.\" This indicates that the field is moving towards new stage: Physical AI, where models must understand and simulate interaction-rich, physically challenging real-world scenarios rather than merely generating aesthetically pleasing videos. Iterative Scaling Unlocks Physical Capabilities. Analyzing model evolution reveals strong correlation between model iteration and physical reasoning capabilities. For instance, the Wan series exhibits dramatic performance leap: from Wan 2.1 (Rank 14, 0.399) to Wan 2.6 (Rank 1, 0.607). Similarly, Seedance evolves from 1.0 to 1.5 Pro, climbing from Rank 6 to Rank 2. These substantial gains suggest that scaling laws and iterative optimization are not just improving visual quality but are actively refining the models understanding of physics, distinct motion patterns, and control logic. The Media-Simulation Gap in Consumer Models. Surprisingly, widely recognized consumer-oriented models like the Sora series perform sub-optimally on this benchmark (Sora v2 Pro at Rank 17, Avg 0.362). This counter-intuitive result highlights critical \"domain gap\": models optimized for media consumption prioritize visual smoothness and cinematic transitions, often at the expense of physical fidelity and precise motion control. This discrepancy suggests that proficiency in creative video generation does not naturally transfer to Embodied AI tasks, underlining the necessity for physically-grounded training data. Table 3 Comparison between human preference scores and RBench scores. This table demonstrates high correlation between the two sets of scores, as reflected in the similar ranking orders. Table 4 RoVid-X effectiveness validation experiment. The experimental results using different models for finetuning show stable improvements across various dimensions, validating the effectiveness of the dataset. Model Human RBench Wan 2.5 Veo 3 Hailuo v2 Seedance 1.0 Cosmos 2.5 DreamGen LongCat-Video Wan2.1-14B CogVideoX-5B LTX-Video 0.573 0.540 0.513 0.505 0.500 0.482 0.480 0.378 0.333 0.246 0.570 0.563 0.565 0.551 0.464 0.420 0.437 0.399 0.256 0.344 rh 1 2 3 4 5 6 7 8 9 10 rb Model Manip. Long. Multi. Spatial. Reason. 1 3 2 4 5 7 6 8 10 9 0 1 -1 0 0 1 -1 0 1 -1 Wan2.1_14B Wan2.1_14B+Ours Wan2.2_5B Wan2.2_5B+Ours 0.344 0.376 0. 0.373 0.335 0.282 0.389 0.295 0.142 0.318 0.268 0.314 0. 0.387 0.221 0.403 0.205 0.298 0.234 0.284 Model Single Dual Quad. Humanoid Total 0.464 Wan2.1_14B 0.595 Wan2.1_14B + Ours 0.526 0.546 0.639 Wan2.2_5B 0.590 Wan2.2_5B + Ours 0.514 0.503 0.628 0.436 0. 0.497 0.599 0.628 0.607 0.641 0.399 0.446 0. 0.439 Closed-source Models Lead in Performance. Commercial closed-source models occupy the top 7 positions in our benchmark, demonstrating clear and consistent advantage over open-source counterparts. The significant performance margin between the state-of-the-art commercial model (Wan 2.6) and the leading open-source model (Wan 2.2) highlights substantial capability gap. This disparity underscores critical urgency for the open-source community: to democratize high-capability foundation models, more concerted efforts are needed in scaling physical training data and optimizing architectures for embodied video tasks. The Dilemma of Specialization: Domain Data vs. World Knowledge. While General Foundation Models lead the leaderboard, the robotics-specific model Cosmos 2.5 demonstrates remarkable resilience. Despite trailing top-tier commercial models, it outperforms significantly larger open-source video models, confirming that training with physical data yields stable gains in robotic tasks. Conversely, models fine-tuned on specific robot entities (e.g., Vidar, UnifoLM) struggle significantly, ranking at the bottom of the benchmark. This contrast highlights critical trade-off: while domain-specific data is valuable for control precision, it cannot fully compensate for the deficit in \"World Knowledge\" provided by large-scale pretraining. Balancing proprietary robot data with generalizable representations remains pivotal challenge for future research. Cognitive and Fine-grained Control Bottlenecks. consistent trend across all model families is that tasks requiring high-level logic or precise interaction represent the most significant performance bottlenecks. First, regarding cognitive capabilities, we observe substantial \"Cognitive Gap\": while top-tier models like Wan 2.6 excel in execution-oriented tasks, their performance drops sharply in Visual Reasoning (0.531). Furthermore, analyzing specific embodiments reveals \"Manipulation Gap\": models consistently score higher on coarsegrained locomotion tasks (Quadruped, Humanoid) than on fine-grained manipulation. This implies that for current video generators, mastering the fine-grained contact dynamics required for object interaction is physically more challenging than generating the rhythmic patterns of legged locomotion. 5.2.2 Qualitative Results We conduct qualitative analysis of representative tasks, and the partial results are shown in Figure 5. For the visual reasoning task, Seedance 1.0 [29] and Hailuo [41] correctly identify the blue clothing and the hollow basket, while Wan 2.5 [92] mistakenly identifies the woven basket as the hollow basket. In the long-horizon planning task, Wan 2.5 successfully completes all actions in the correct sequence, while Hailuo lacks the \"turn-on\" action, leading to violation of physical logic. In the spatial relationship task, Hailuo correctly places the bok choy to the left of the pan, whereas other models mistakenly place it inside the pan. Notably, LongCat-Video introduces an unrealistic human arm intervention, disrupting physical plausibility. More detailed analysis and qualitative results can be found in the Appendix F. These models each have their strengths, but there is still significant room for improvement in their overall performance. This further highlights the necessity of designing such benchmark to advance video generation 11 models in robotic tasks."
        },
        {
            "title": "5.3 Human Preference Study",
            "content": "We conduct human preference study to assess how well automatic metrics align with human perception. Thirty participants are invited to participate. For each comparison, two model outputs for the same prompt and video instance are presented side-by-side, and annotators choose from three options: is better, is better, or Tie. Votes are aggregated into per-model scores: win contributes 5, loss contributes 1, and tie contributes 3 to both models. We then compare these model-level human scores with the corresponding RBench benchmark scores. On the ten-model subset used in the study, the Spearman rank correlation between human scores and RBench scores is ρ = 0.96 (two-sided < 103). Table 3 presents the human scores, RBench scores, and ranks for the ten selected models, where the column denotes the rank difference (rb rh). Overall, models that rank highest under the benchmark largely match human judgments, while the remaining small discrepancies highlight opportunities to further refine the metric for improved human alignment. The high degree of consistency further demonstrates the validity and effectiveness of our automated metrics in evaluating video generation models, indicating that the metrics accurately reflect human perception and thus provide reliable evaluation standard for robotic video generation tasks. Please refer to the Appendix for more details."
        },
        {
            "title": "5.4 Validation of RoVid-X",
            "content": "To assess the effectiveness and robustness of RoVid-X, we finetune models initialized with Wan2.1 14B and Wan2.2 5B weights, using MSE loss exclusively. Due to computational constraints, we randomly sample 200k instances from the original RoVid-X dataset. The results, shown in Table 4, highlight that our dataset significantly enhances performance across five task domains and four distinct embodiments. These improvements validate both the proposed dataset and our data collection pipeline."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we rethink video generation models for the embodied world and introduce RBench, new benchmark designed to fill critical gap in evaluating robot-oriented video generation models. Unlike previous methods that primarily rely on perceptual metrics, RBench incorporates both task-level accuracy and visual fidelity, using comprehensive evaluation suite with detailed sub-metrics such as structural consistency and physical plausibility. The evaluation of 25 models highlights that current video generation models still require significant improvements to generate physically realistic robot behaviors. The strong correlation between RBench scores and human evaluations further validates the benchmarks effectiveness. Additionally, RoVid-X overcomes the limitations of existing robotic datasets by offering large-scale and diverse resource for video generation tasks. Together, RBench and RoVid-X provide robust foundation for advancing video generation models in robotics. Our findings highlight the shortcomings of current video foundation models and suggest possible avenues for improvement, providing researchers with fresh perspectives for exploring the embodied domain through video world models. Future Work. We aim to bridge the gap between video generation and actionable robot policy. We plan to employ Inverse Dynamics Models (IDM) to recover executable actions from generated videos, enabling closed-loop control experiments in both simulation environments and on real-world hardware. Furthermore, we intend to develop more automated and physically grounded evaluation metrics to rigorously assess the kinematic and dynamic feasibility of generated behaviors. Additionally, we will focus on training video generation models with improved physical capabilities, enabling the generation of robot videos that perform high-fidelity actions. Ultimately, these efforts will accelerate the development of comprehensive solution for video-driven embodied intelligence."
        },
        {
            "title": "References",
            "content": "[1] Jorge Aldaco, Travis Armstrong, Robert Baruch, Jeff Bingham, Sanky Chan, Kenneth Draper, Debidatta Dwibedi, Chelsea Finn, Pete Florence, Spencer Goodrich, et al. Aloha 2: An enhanced low-cost hardware for bimanual teleoperation. arXiv preprint arXiv:2405.02292, 2024. [2] Arslan Ali, Junjie Bai, Maciej Bala, Yogesh Balaji, Aaron Blakeman, Tiffany Cai, Jiaxin Cao, Tianshi Cao, Elizabeth Cha, Yu-Wei Chao, et al. World simulation with video foundation models for physical ai. arXiv preprint arXiv:2511.00062, 2025. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, and Tang. Qwen3-vl technical report. arXiv preprint arXiv, 2025. [4] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. [5] Philip J. Ball, Jakob Bauer, Frank Belletti, Bethanie Brownfield, Ariel Ephrat, Shlomi Fruchter, Agrim Gupta, Kristian Holsheimer, Aleksander Holynski, Jiri Hron, Christos Kaplanis, Marjorie Limont, Matt McGill, Yanko Oliveira, Jack Parker-Holder, and Frank Perbet. Genie 3: new frontier for world models. arXiv preprint arXiv, 2025. [6] Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, and Aditya Grover. Videophy: Evaluating physical commonsense for video generation. arXiv preprint arXiv:2406.03520, 2024. [7] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. arXiv preprint arXiv:2409.16283, 2024. [8] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [9] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π: vision-language-action flow model for general robot control. CoRR, 2024. [10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [11] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. [12] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [13] Emanuele Bugliarello, Hernan Moraldo, Ruben Villegas, Mohammad Babaeizadeh, Mohammad Taghi Saffar, Han Zhang, Dumitru Erhan, Vittorio Ferrari, Pieter-Jan Kindermans, and Paul Voigtlaender. Storybench: multifaceted benchmark for continuous story visualization. Advances in Neural Information Processing Systems, 36:7809578125, 2023. [14] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, et al. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [15] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, Weiming Xiong, Wei Wang, Nuo Pang, Kang Kang, Zhiheng Xu, Yuzhe Jin, Yupeng Liang, Yubing Song, Peng Zhao, Boyuan Xu, Di Qiu, Debang Li, Zhengcong Fei, Yang Li, and Yahui Zhou. Skyreels-v2: Infinite-length film generative model, 2025. URL https://arxiv.org/abs/2504.13074. 13 [16] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. arXiv:2501.12375, 2025. [17] Siyan Chen, Yanfei Chen, Ying Chen, Zhuo Chen, Feng Cheng, Xuyan Chi, Jian Cong, Qinpeng Cui, Qide Dong, Junliang Fan, et al. Seedance 1.5 pro: native audio-visual joint generation foundation model. arXiv preprint arXiv:2512.13507, 2025. [18] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision (ECCV), pages 720736, 2018. [19] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019. [20] Yufan Deng, Xun Guo, Yizhi Wang, Jacob Zhiyuan Fang, Angtian Wang, Shenghai Yuan, Yiding Yang, Bo Liu, Haibin Huang, and Chongyang Ma. Cinema: Coherent multi-subject video generation via mllm-based guidance. arXiv preprint arXiv:2503.10391, 2025. [21] Yufan Deng, Xun Guo, Yuanyang Yin, Jacob Zhiyuan Fang, Yiding Yang, Yizhi Wang, Shenghai Yuan, Angtian Wang, Bo Liu, Haibin Huang, et al. Magref: Masked guidance for any-reference video generation. arXiv preprint arXiv:2505.23742, 2025. [22] Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. Advances in neural information processing systems, 36:91569172, 2023. [23] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. [24] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu. Rh20t: comprehensive robotic dataset for learning diverse skills in one-shot. arXiv preprint arXiv:2307.00595, 2023. [25] Yao Feng, Hengkai Tan, Xinyi Mao, Chendong Xiang, Guodong Liu, Shuhe Huang, Hang Su, and Jun Zhu. Vidar: Embodied video diffusion model for generalist manipulation. arXiv preprint arXiv:2507.12898, 2025. [26] Yao Mu Fourier ActionNet Team. Actionnet: dataset for dexterous bimanual manipulation. arXiv preprint arXiv, 2025. [27] Zipeng Fu, Tony Zhao, and Chelsea Finn. Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation. arXiv preprint arXiv:2401.02117, 2024. [28] Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhenguo Li, and Qiang Xu. Magicdrive-v2: High-resolution long video generation for autonomous driving with adaptive control. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2813528144, 2025. [29] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [30] Google DeepMind. Veo-3 technical report. Technical report, Google DeepMind, May 2025. URL https: //storage.googleapis.com/deepmind-media/veo/Veo-3-Tech-Report.pdf. [31] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The\" something something\" video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pages 58425850, 2017. [32] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. 14 [33] Jing Gu, Xian Liu, Yu Zeng, Ashwin Nagarajan, Fangrui Zhu, Daniel Hong, Yue Fan, Qianqi Yan, Kaiwen Zhou, Ming-Yu Liu, et al. \" phyworldbench\": comprehensive evaluation of physical realism in text-to-video models. arXiv preprint arXiv:2507.13428, 2025. [34] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [35] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, and Chongyang Ma. I2V-adapter: general image-to-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [36] Xuyang Guo, Jiayan Huo, Zhenmei Shi, Zhao Song, Jiahao Zhang, and Jiale Zhao. T2vphysbench: firstprinciples benchmark for physical consistency in text-to-video generation. arXiv preprint arXiv:2505.00337, 2025. [37] Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. Advances in Neural Information Processing Systems, 37:112386112410, 2024. [38] Yanjiang Guo, Lucy Xiaoyang Shi, Jianyu Chen, and Chelsea Finn. Ctrl-world: controllable generative world model for robot manipulation. arXiv preprint arXiv:2510.10125, 2025. [39] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [40] Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, et al. Ltx-2: Efficient joint audio-visual foundation model. arXiv preprint arXiv:2601.03233, 2026. [41] Hailuo. Hailuo. Hailuo Lab, 2025. URL https://hailuoai.video/. [42] Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Yufan Deng, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, et al. Video-bench: Human-aligned video generation benchmark. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1885818868, 2025. [43] Adam W. Harley, Yang You, Xinglong Sun, Yang Zheng, Nikhil Raghuraman, Yunqi Gu, Sheldon Liang, WenHsuan Chu, Achal Dave, Pavel Tokmakov, Suya You, Rares Ambrus, Katerina Fragkiadaki, and Leonidas J. Guibas. AllTracker: Efficient dense point tracking at high resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. [44] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [45] Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. [46] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [47] Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, et al. Dreamgen: Unlocking generalization in robot learning through video world models. arXiv preprint arXiv:2505.12705, 2025. [48] Pengliang Ji, Chuyang Xiao, Huilin Tai, and Mingxiao Huo. T2vbench: Benchmarking temporal dynamics for text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 53255335, June 2024. [49] Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. Galaxea open-world dataset and g0 dual-system vla model. arXiv preprint arXiv:2509.00576, 2025. [50] Yuming Jiang, Siteng Huang, Shengke Xue, Yaxi Zhao, Jun Cen, Sicong Leng, Kehan Li, Jiayan Guo, Kexiang Wang, Mingxiu Chen, et al. Rynnvla-001: Using human demonstrations to improve robot manipulation. arXiv preprint arXiv:2509.15212, 2025. [51] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [52] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Jim Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual dexterous manipulation via imitation learning. In 2025 IEEE International Conference on Robotics and Automation (ICRA), pages 1692316930. IEEE, 2025. [53] Xuan Ju, Tianyu Wang, Yuqian Zhou, He Zhang, Qing Liu, Nanxuan Zhao, Zhifei Zhang, Yijun Li, Yuanhao Cai, Shaoteng Liu, et al. Editverse: Unifying image and video editing and generation with in-context learning. arXiv preprint arXiv:2509.20360, 2025. [54] Bingyi Kang, Yang Yue, Rui Lu, Zhijie Lin, Yang Zhao, Kaixin Wang, Gao Huang, and Jiashi Feng. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. [55] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudo-labelling real videos. In Proc. arXiv:2410.11831, 2024. [56] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [57] Geonung Kim, Janghyeok Han, and Sunghyun Cho. Videofrom3d: 3d scene video generation via complementary image and video diffusion models. arXiv preprint arXiv:2509.17985, 2025. [58] Kling. Image to video elements feature, 2025. URL https://klingai.com/global/. [59] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [60] Shuang Li, Yihuai Gao, Dorsa Sadigh, and Shuran Song. Unified video action model. arXiv preprint arXiv:2503.00200, 2025. [61] Junbang Liang, Pavel Tokmakov, Ruoshi Liu, Sruthi Sudhakar, Paarth Shah, Rares Ambrus, and Carl Vondrick. Video generators are robot policies. arXiv preprint arXiv:2508.00795, 2025. [62] Mingxiang Liao, Qixiang Ye, Wangmeng Zuo, Fang Wan, Tianyu Wang, Yuzhong Zhao, Jingdong Wang, Xinyu Zhang, et al. Evaluation of text-to-video generation models: dynamics perspective. Advances in Neural Information Processing Systems, 37:109790109816, 2024. [63] Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, et al. Genie envisioner: unified world foundation platform for robotic manipulation. arXiv preprint arXiv:2508.05635, 2025. [64] Xinran Ling, Chen Zhu, Meiqi Wu, Hangyu Li, Xiaokun Feng, Cundian Yang, Aiming Hao, Jiashu Zhu, Jiahong Wu, and Xiangxiang Chu. Vmbench: benchmark for perception-aligned video motion generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1308713098, 2025. [65] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [66] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. [67] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. 16 [68] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. Interactive language: Talking to robots in real time. IEEE Robotics and Automation Letters, 2023. [69] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, et al. Roboturk: crowdsourcing platform for robotic skill learning through imitation. In Conference on Robot Learning, pages 879893. PMLR, 2018. [70] Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, and Dieter Fox. Mimicgen: data generation system for scalable robot learning using human demonstrations. arXiv preprint arXiv:2310.17596, 2023. [71] Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, and Yue Wang. Learning from massive human videos for universal humanoid pose control. arXiv preprint arXiv:2412.14172, 2024. [72] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. [73] Fanqing Meng, Wenqi Shao, Lixin Luo, Yahong Wang, Yiran Chen, Quanfeng Lu, Yue Yang, Tianshuo Yang, Kaipeng Zhang, Yu Qiao, et al. Phybench: physical commonsense benchmark for evaluating text-to-image models. arXiv preprint arXiv:2406.11802, 2024. [74] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. [75] OpenAI. Sora, 2024. URL https://openai.com/sora/. Accessed: 2025-02-26. [76] OpenAI. Introducing gpt-5. https://openai.com/index/introducing-gpt-5/, 2025. [77] OpenAI. Sora2, 2025. URL https://openai.com/zh-Hans-CN/index/sora-2/. [78] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [79] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [80] Pika. Pika art 2.0s scene ingredients: Redefining personalized video creation, 2025. URL https://pikartai. com/scene-ingredients/. Accessed: 2025-02-26. [81] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. [82] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 61216132, 2025. [83] Runway. Runway, 2025. URL https://runwayml.com/. Accessed: 2025-02-26. [84] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [85] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84068416, 2025. 17 [86] GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, et al. Gigabrain-0: world model-powered vision-language-action model. arXiv preprint arXiv:2510.19430, 2025. [87] Meituan LongCat Team, Xunliang Cai, Qilong Huang, Zhuoliang Kang, Hongyu Li, Shijun Liang, Liya Ma, Siyu Ren, Xiaoming Wei, Rixu Xie, and Tong Zhang. Longcat-video technical report, 2025. URL https: //arxiv.org/abs/2510.22200. [88] Bahey Tharwat, Yara Nasser, Ali Abouzeid, and Ian Reid. Latent action pretraining through world modeling. arXiv preprint arXiv:2509.18428, 2025. [89] Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In The Thirteenth International Conference on Learning Representations, 2024. [90] Yang Tian, Yuyin Yang, Yiman Xie, Zetao Cai, Xu Shi, Ning Gao, Hangxu Liu, Xuekun Jiang, Zherui Qiu, Feng Yuan, et al. Interndata-a1: Pioneering high-fidelity synthetic data for pre-training generalist policy. arXiv preprint arXiv:2511.16651, 2025. [91] Unitree. Unifolm-wma-0: world-model-action (wma) framework under unifolm family, 2025. [92] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [93] Jing Wang, Ao Ma, Ke Cao, Jun Zheng, Zhanjie Zhang, Jiasong Feng, Shanyuan Liu, Yuhang Ma, Bo Cheng, Dawei Leng, et al. Wisa: World simulator assistant for physics-aware text-to-video generation. arXiv preprint arXiv:2503.08153, 2025. [94] Yiping Wang, Xuehai He, Kuan Wang, Luyao Ma, Jianwei Yang, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Is your world simulator good story presenter? consecutive events-based benchmark for future long video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1362913638, 2025. [95] Yufei Wang, Zhou Xian, Feng Chen, Tsun-Hsuan Wang, Yian Wang, Katerina Fragkiadaki, Zackory Erickson, David Held, and Chuang Gan. Robogen: Towards unleashing infinite data for automated robot learning via generative simulation. arXiv preprint arXiv:2311.01455, 2023. [96] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [97] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. [98] Bing Wu, Chang Zou, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Jack Peng, Jianbing Wu, Jiangfeng Xiong, Jie Jiang, et al. Hunyuanvideo 1.5 technical report. arXiv preprint arXiv:2511.18870, 2025. [99] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Chunyi Li, Liang Liao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtai Zhai, and Weisi Lin. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. Equal Contribution by Wu, Haoning and Zhang, Zicheng. Project Lead by Wu, Haoning. Corresponding Authors: Zhai, Guangtai and Lin, Weisi. [100] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In The Twelfth International Conference on Learning Representations, 2025. [101] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. [102] Philipp Wu, Yide Shentu, Zhongke Yi, Xingyu Lin, and Pieter Abbeel. Gello: general, low-cost, and intuitive teleoperation framework for robot manipulators. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1215612163. IEEE, 2024. 18 [103] Shihan Wu, Xuecheng Liu, Shaoxuan Xie, Pengwei Wang, Xinghang Li, Bowen Yang, Zhe Li, Kai Zhu, Hongyu Wu, Yiheng Liu, et al. Robocoin: An open-sourced bimanual robotic data collection for integrated manipulation. arXiv preprint arXiv:2511.17441, 2025. [104] Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, and Jianbing Shen. Rlgf: Reinforcement learning with geometric feedback for autonomous driving video generation. arXiv preprint arXiv:2509.16500, 2025. [105] Lujie Yang, HJ Suh, Tong Zhao, Bernhard Paus Graesdal, Tarik Kelestemur, Jiuguang Wang, Tao Pang, and Russ Tedrake. Physics-driven data generation for contact-rich manipulation via trajectory optimization. arXiv preprint arXiv:2502.20382, 2025. [106] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [107] Seonghyeon Ye, Joel Jang, Byeongguk Jeon, Se June Joo, Jianwei Yang, Baolin Peng, Ajay Mandlekar, Reuben Tan, Yu-Wei Chao, Bill Yuchen Lin, et al. Latent action pretraining from videos. In The Thirteenth International Conference on Learning Representations, 2025. [108] Shenghai Yuan, Xianyi He, Yufan Deng, Yang Ye, Jinfa Huang, Bin Lin, Jiebo Luo, and Li Yuan. Opens2v-nexus: detailed benchmark and million-scale dataset for subject-to-video generation. arXiv preprint arXiv:2505.20292, 2025. [109] Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for video generation. Arxiv, 2025. [110] Zhenyu Zhao, Hongyi Jing, Xiawei Liu, Jiageng Mao, Abha Jha, Hanwen Yang, Rong Xue, Sergey Zakharor, Vitor Guizilini, and Yue Wang. Humanoid everyday: comprehensive robotic dataset for open-world humanoid manipulation. arXiv preprint arXiv:2510.08807, 2025. [111] Haoyu Zhen, Qiao Sun, Hongxin Zhang, Junyan Li, Siyuan Zhou, Yilun Du, and Chuang Gan. Tesseract: learning 4d embodied world models. arXiv preprint arXiv:2504.20995, 2025. [112] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung, and Chuang Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024. [113] Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, and Abhishek Gupta. Unified world models: Coupling video and action diffusion for pretraining on large robotic datasets. arXiv preprint arXiv:2504.02792, 2025. [114] Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, and Tianfan Xue. Flashvsr: Towards real-time diffusion-based streaming video super-resolution, 2025. URL https://arxiv.org/abs/2510.12747."
        },
        {
            "title": "Appendix",
            "content": "A Evaluation Set Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 Task-Oriented Evaluation Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 Common Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.2 Long-Horizon Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.3 Multi-Entity Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.4 Spatial Relationship . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.5 Visual Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Embodiment-Specific Evaluation Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Automatic Metrics Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1 Physical-Semantic Plausibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Task-Adherence Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Robot-Subject Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Motion Amplitude . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Motion Smoothness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.6 Score Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Model Descriptions and Implementation Setups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 Commercial Models C.2 Open-source Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Robotics-specific Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Human Preference Study Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Prompt Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Qualitative Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Comprehensive Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2 2 2 2 3 4 4 6 7 9 12 13 13 14 15 17"
        },
        {
            "title": "A Evaluation Set Details",
            "content": "A.1 Task-Oriented Evaluation Set To systematically evaluate the multi-dimensional task execution capabilities of video generation models in robotic scenarios, RBench constructs task-oriented evaluation set with five core task dimensions: Common Manipulation, Long-Horizon Planning, Multi-Entity Collaboration, Spatial Relationship, and Visual Reasoning. For each task, we collect 50 images as initial frames from open-source datasets or public web sources. Human 1 annotators then create and verify corresponding text prompts to ensure both correctness and diversity of language descriptions. Together, these image-text pairs define diverse evaluation corpus that covers wide range of everyday manipulation, complex planning, multi-entity interaction, spatial reasoning, and visual-semantic reasoning scenarios. A.1.1 Common Manipulation This task evaluates the ability of video generation models to produce diverse manipulation behaviors in basic object interaction scenarios. The scenes cover single-arm, dual-arm, and humanoid robots performing typical manipulation actions such as grasping, placing, pushing, rotating, and pressing. The dataset focuses on whether the model can generate physically plausible, temporally coherent, and natural manipulation behaviors that achieve the specified goals for everyday object handling. A.1.2 Long-Horizon Planning This task evaluates the capability of video generation models to understand and generate long-horizon robotic behaviors that involve multi-stage action planning. Each instance in the evaluation set is composed of multiple sequential sub-actions, including: Object Manipulation Sequences: e.g., The robot opens the refrigerator door, takes the green box out of the refrigerator, and then closes the refrigerator door, which requires clearly delineated action stages and physically reasonable transitions. Multi-Step Spatial Planning: e.g., The humanoid robot picks up the bag, turns around, climbs up the stairs, and walks across the wooden plank, emphasizing continuous spatial transitions and modeling of multi-step action chaining. Physical Motion and Body Coordination: e.g., The quadruped robot performs front flip, lands steadily, then leans forward and balances upside down on its front legs, which assesses temporal coherence and physical plausibility in complex motion and body control. Overall, the dataset spans broad range of tasks from everyday interactions to dynamic control, focusing on models capability in action decomposition, stage transitions, and cross-time reasoning for comprehensive long-horizon planning. A.1.3 Multi-Entity Collaboration This task focuses on evaluating the capability of video generation models to depict collaborative and interactive behaviors in multi-entity robotic scenarios. Each scene contains Primary Entity and Secondary Entity. The Primary Entity can be single-arm robot, dual-arm robot, humanoid robot, or quadruped robot, while the Secondary Entity can be human, an animal, or another robot. The task covers diverse interaction types such as object handover and usage, dressing assistance, collaborative task completion, following, and guidance. The dataset is designed to assess whether the model can generate natural, temporally coherent, and task-consistent multi-entity collaboration behaviors at semantic, temporal, and physical levels. A.1.4 Spatial Relationship This task evaluates the ability of video generation models to understand and express spatial relationships in generated videos. We construct scenes where humanoid robots, single-arm robots, and quadruped robots interact with clearly defined objects while satisfying various spatial relations, such as above/below, left/right, and front/behind. The dataset requires models to correctly present relative positions, orientations, and motion trajectories between entities, revealing their competence in spatial understanding and geometric reasoning. Consistent spatial layouts and motion patterns across time are essential to correctly reflect the described spatial relations. 2 A.1.5 Visual Reasoning This task aims to evaluate the visual-semantic reasoning capabilities of video generation models in complex scenes. The evaluation set includes wide range of visual concepts and multi-level semantic logic, such as: Color recognition (e.g., pick up the sky-blue book); Numerical and ordering reasoning (e.g., the robot places the apple, water bottle, and Rubiks cube into the bag in that order); Attribute and category matching (e.g., the robot gripper places the white bottle of baby powder onto the shelf, aligning it with other identical bottles in the same column); Geometric and object-property understanding (e.g., the robot picks up the tallest orange object and places it into the basket); Text and semantic understanding (e.g., the left manipulator places the cup under the white dispenser labeled Jasmine Tea, and the right manipulator opens the dispenser to pour jasmine tea into the cup); Visual feature understanding (e.g., the robot grasps the book with portrait of person on its cover). This dataset is designed to emphasize fine-grained visual grounding, logical consistency, and the ability to align robot actions with high-level visual-semantic reasoning requirements. A.2 Embodiment-Specific Evaluation Set Different types of robots exhibit substantial variations in morphology, degrees of freedom, control modes, and task objectives. These factors directly influence the modeling complexity and generalization challenges faced by video generation models in robotics contexts. To more systematically analyze model performance across heterogeneous robot embodiments, RBench constructs embodiment-specific evaluation subsets that encompass four representative robot categories: dual-arm robots, humanoid robots, single-arm robots, and quadruped robots. For each embodiment, 100 initial-frame images are sourced from open-access or publicly available datasets, and human annotators create and verify the corresponding prompts for accuracy and linguistic diversity. Each subset includes diverse range of robot models, action types, manipulated objects, scene environments, and both first-person and third-person perspectives. This design introduces embodiment-specific challenges: dual-arm robots emphasize coordinated bimanual manipulation, humanoid robots prioritize tool use and natural full-body postures, single-arm robots focus on precise object interactions, and quadruped robots predominantly test terrain adaptation and motion continuity. Along with the task-oriented splits, these embodiment-specific subsets provide comprehensive and structured dataset for benchmarking video generation models in the embodied world. Evaluating models across these four robot categories further uncovers the current biases and capability preferences of image-to-video generation models. For instance, due to extensive pre-training on large-scale human activity datasets, many models tend to exhibit higher task completion rates and better visual quality in humanoid-robot scenarios, while they often struggle with fine-grained single-arm manipulation. By systematically comparing performance across different embodiments, RBench makes such imbalances explicit and offers principled framework to identify where current models excel or fail. More broadly, embodiment-aware datasets like RBench are pivotal for advancing video foundation models in robotics. They promote the development of architectures and training strategies capable of generalizing beyond human-centric motion priors, enabling models to learn and adapt to broader distribution of robot-specific motion patterns, rather than relying solely on human demonstrations. Furthermore, they facilitate fair and transparent comparisons between models, allowing evaluation results to clearly identify which models perform best for specific robot embodiments. Finally, such datasets help bridge the gap between generic video generation and physically grounded embodied intelligence, fostering the transition from visually appealing but brittle outputs toward robust, controllable, and deployment-ready generative models for real-world robotic systems."
        },
        {
            "title": "B Automatic Metrics Details",
            "content": "To quantitatively assess the core capabilities of different models in robot video generation, we design five fine-grained metrics: Physical-Semantic Plausibility, Task-Adherence Consistency, Motion Amplitude, RobotSubject Stability, and Motion Smoothness. These metrics are evaluated using an MLLM-based, VQA-style protocol applied to grid images composed of key frames sampled from each generated video. Additionally, the evaluation is supplemented by low-level, non-MLLM computational indicators that capture pixel-level motion statistics and temporal dynamics. Together, these two layers of evaluation provide comprehensive assessment of both task completion and visual quality in robotic video generation. B.1 Physical-Semantic Plausibility Figure 6 Visualization of robot and subject floating. In robotics video generation, models often produce physically implausible or commonsense-violating artifacts, such as grippers passing through objects, floating objects, or the sudden appearance of irrelevant entities. These errors are typically undetectable by standard visual perception metrics, yet they directly highlight limitations in models understanding of physical laws and semantic causality. To capture these issues, we introduce the Physical-Semantic Plausibility metric, implemented via VQA-style evaluation pipeline. The MLLM receives grid image composed of key frames from the generated video and is prompted to detect the following types of violations: Floating and unsupported entities. As illustrated in Figure 6, the metallic spoon and the single-arm robots manipulator are suspended in mid-air without any physically plausible support. Interpenetration. As shown in Figure 7, the humanoid robot hand unrealistically penetrates the box, indicating severe violation of rigid-body constraints. Sudden appearance, disappearance, or duplication. As demonstrated in Figure 8, (a) the robotic arm suddenly disappears in later frames, (b) human hands and new notebook suddenly appear, and (c) the number of instant noodle packs is spuriously duplicated. 4 Figure 7 Visualization of robot interpenetration. Figure 8 Visualization of robot/subject sudden appearance, disappearance, or duplication. 5 Non-contact attachment and incorrect grasping. For example, in Figure 6(a), the metallic spoon moves rigidly with the gripper even though there is no clear contact or gripper closure, resulting in an unrealistic sticking effect. These anomalies are treated as severe physical violations that significantly reduce the credibility of the generated video. Beyond local error detection, the evaluator is also required to assess whether the overall action sequence and causal progression are reasonable, thereby characterizing the extent to which the model produces videos that are consistent with basic physical laws and human common sense. B.2 Task-Adherence Consistency Figure 9 Visualization of task responsiveness and key actions completeness. Robotics video generation models often exhibit task-level deviations, such as ignoring the specified objective or omitting critical action stages. To measure this behavior, we design the Task-Adherence Consistency metric, using VQA-style evaluation protocol. The MLLM inspects the grid of key frames and assesses the following: Task responsiveness. As shown in Figure 9(a), the failure case illustrates robot gripper that does not respond to the instruction to grasp the mouse; the gripper remains static, and the intended task is never initiated or completed. Key action completeness. As illustrated in Figure 9(b), the failure case omits crucial actions such as turn on and fill : the faucet is never visibly operated, yet water still flows from the tap, disrupting the causal chain between actions and outcomes. These phenomena reflect shortcomings in semantic understanding, action planning, and execution consistency with respect to the prompt. Importantly, they are also difficult to capture with conventional low-level perception metrics, highlighting the necessity of explicit task-adherence evaluation in robotics contexts. Concretely, Task-Adherence Consistency is instantiated with task-specific criteria for the five task families introduced in Section A.1, with focus on the following: Common Manipulation. Task adherence is primarily assessed through: (i) Task Completion, which checks whether the robot successfully accomplishes the manipulation objective described in the prompt while exhibiting reasonable intermediate phases (e.g., approach grasp move place); and (ii) Action Effectiveness, which evaluates the physical plausibility and dynamic coherence of the manipulation, including natural gripper closure, appropriate contact locations, and smooth trajectories. Attempts with obviously discontinuous, incomplete, or physically implausible actions are regarded as failures. Multi-Entity Collaboration. For collaborative scenes involving Primary and Secondary Entity, task adherence is assessed through two aspects: (i) Task Completion, requiring that both entities execute their respective roles and complete all required interaction steps in temporally coherent and logically consistent manner; and (ii) Action Effectiveness, which measures the completeness and coordination of interaction behaviors. For contact-based interactions, full sequence of approach contact release/transfer is expected; for non-contact interactions (e.g., following, joint motion), coherent process of initiation alignment sustained coordination is required. Missing stages, asynchronous responses, or logically inconsistent behaviors are treated as unsuccessful. Spatial Relationship. In spatial reasoning scenarios, task adherence is assessed through: (i) Spatial Relation Accuracy, which checks whether the spatial relations between entities (e.g., above/below, left/right, front/behind) match the textual description with consistent orientation, scale, and viewpoint; and (ii) Manipulation Feasibility, which examines whether the direction, trajectory, and intent of the robots motion are compatible with the described spatial relations (e.g., moving leftward when instructed to move to the left of). Trajectories that contradict the described direction or result in physically unreasonable motions are considered incorrect. Visual Reasoning. In visually and semantically complex scenes, task adherence is assessed through two aspects: (i) Visual Reasoning Accuracy, evaluated via an automatic Question Chain mechanism: given the prompt, MLLM first generates set of stepwise verification questions covering the trigger-feedbackoutcome logic. The same MLLM then answers these questions based on the generated video, and score is computed as follows: Score = 5 completed questions total questions , (4) where missing or incorrect events are treated as unfulfilled steps. This encourages the video to satisfy both the visual and logical requirements of the task. Additionally, (ii) Action Effectiveness measures the physical plausibility and dynamic coherence of the robots motions, penalizing clearly discontinuous, incomplete, or physically implausible actions even if some high-level reasoning appears correct. Long-Horizon Planning. For long-horizon tasks composed of multiple ordered sub-events, task adherence is assessed through: (i) Event Completion Rate. For each sample, an event list is defined as an ordered set of events. This list is reformulated into numbered sequence, e.g., 1. open the refrigerator door; 2. take out the green box; 3. close the door, and the final score is computed as follows: Score = 5 completed events total events , (5) this measures how completely the required event sequence is executed. Complementarily, (ii) Action Effectiveness again assesses the physical plausibility and temporal coherence of the underlying motions (e.g., natural body coordination, stable landing, and smooth transitions between stages), ensuring that partially correct high-level event ordering without valid execution is not over-rewarded. B.3 Robot-Subject Stability In robotics video generation, maintaining stable structure and appearance for both the robot and the manipulated objects is essential for assessing generation quality. In practice, models often exhibit abnormal changes in robot morphology or severe distortions of subject attributes. To systematically evaluate these Figure 10 Visualization of robot structural stability. 8 Figure 11 Visualization of subject appearance stability. issues, we propose the Robot-Subject Stability metric, which separately measures the visual and semantic consistency of the robot and the target subject throughout the generation process. We adopt comparative VQA mechanism: the system simultaneously observes two frames, with the left frame as reference image and the right frame as generated frame, and focuses on specified entity (e.g., the robotic gripper or the target subject). The MLLM is prompted to judge how well the entitys appearance, structure, and semantics are preserved between the two frames. Specifically, the evaluator identifies: Robot structural stability. As shown in Figure 10, (a) humanoid robot degenerates into single-arm robot, (b) quadruped robot morphs into small humanoid robot, (c) single-arm robot transforms into humanoid robot, and (d) parallel gripper deforms into dexterous robotic hand. These cases reveal structural drift and inconsistency in robot embodiment over time. Subject appearance stability. As illustrated in Figure 11, (a) rectangular knitted sleeve transforms into long-sleeve sweater, and (b) green plastic cup on the table becomes round yellow mug, indicating loss of identity-preserving appearance. Beyond these explicit examples, we also observe range of additional anomalies: changes in the number of robot links or arms during task execution, the spontaneous generation of extra manipulators, and unnatural variations in arm length, connectivity, or joint bending direction over time. Target objects may also undergo unrealistic material changes, such as rigid object bending like deformable one. The Robot-Subject Stability metric is designed to capture such inconsistencies, providing focused measure of whether the model can preserve both robot morphology and object identity across the video sequence. B.4 Motion Amplitude Motivation. common failure mode in robotic video generation is that the robot remains nearly static while the generated frames appear visually smooth, as illustrated in Figure 12(a)(b). This makes pure smoothnessbased metrics insufficient. Following the perceptual motion estimation idea introduced in VMBench [64], 9 Figure 12 Visualization of robot motion amplitude. Motion Amplitude Score (MAS) is used to measure the perceptible dynamic behavior of the robot while explicitly compensating for camera motion. Robot Localization and Tracking. The robot is first localized using GroundingDINO, and temporally stable segmentation masks are obtained via SAM2. CoTracker is then used to track dense grid of keypoints inside the robot mask, ensuring that the estimated motion truly reflects robot articulation rather than background drift or mask leakage. Frame-Level Motion. Let pt,k denote the 2D location of the k-th tracked point at frame t. The raw frame-to-frame displacement is computed as: To ensure consistency across resolutions, the motion is normalized by the video diagonal: Dt = 1 (cid:88) k=1 (cid:13) (cid:13)pt,k pt1,k (cid:13) (cid:13)2. Dt = Dt 2 + . (6) (7) Camera-Motion Compensation. To estimate camera-induced movement, the robot mask is inverted and the same tracking procedure is applied to the background region. Let Dbg denote the normalized background motion. soft-zero strategy is adopted: if the robot motion does not exceed the background motion, the small residual value is retained: (cid:40) Dt Dbg ˆDt = Dt, , Dt > Dbg Dt Dbg , . (8) This behavior matches our implementation and improves robustness against tracking noise or partial occlusion, while effectively treating the robot as static. 10 Final Score. Finally, following VMBench, the compensated displacement is clipped to stabilize extreme values: MAS ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 min(cid:0) ˆDt, 1(cid:1). (9) Discussion. MAS captures whether the robot exhibits meaningful articulation rather than merely inheriting background or camera movement. By incorporating localization, mask-based tracking, background compensation, and soft-zero strategy, MAS remains stable across scenes, tracking configurations, and robotic embodiments. B.5 Motion Smoothness Figure 13 Visualization of robot motion smoothness. This metric evaluates the temporal continuity and naturalness of motion, aiming to detect frame-level discontinuities such as low-level temporal artifacts and high-level motion blur. As illustrated in Figure 13, various robot embodiments, including quadruped robots, humanoids, and single-arm manipulators, exhibit different degrees of motion-induced distortion that substantially degrade perceived video quality. 11 The assessment is based on the motion-smoothness principle introduced in VMBench [64], with temporal consistency estimated using Q-Align aesthetic quality scores. For each video, frames are processed with sliding window of size (default = 3). Each window is fed into Q-Align to obtain per-frame quality score sequence {Qt}T . Temporal quality fluctuation is then measured by the magnitude of adjacent-frame differences: t=1 Qt = Qt Qt1, = 2, . . . , T. (10) To ensure comparability across videos with different motion intensities, the threshold for detecting abnormal temporal variations is determined by the Motion Amplitude value defined in Section B.4. piecewise adaptive threshold function is used: τs(m) = 0.01, < 0.1, 0.015, 0.1 < 0.3, 0.025, 0.3 < 0.5, 0.03, 0.5. (11) Lower-motion videos therefore adopt stricter threshold for detecting subtle temporal inconsistencies, while higher-motion videos receive relaxed threshold to avoid penalizing naturally rapid movements. The function is determined through grid search on validation split to ensure reproducibility. frame is marked as temporally abnormal if its score fluctuation exceeds the adaptive threshold: It = I[Qt > τs(m)], (12) where I[] denotes the indicator function. To robustly capture abrupt artifacts such as frame drops or transient distortions, adjacent frames of each abnormal index are also flagged. The final Motion Smoothness Score (MSS) is computed as the proportion of normal frames in the entire sequence: higher MSS indicates smoother and more temporally coherent motion, whereas videos with frequent jitter, abrupt discontinuities, or artifact-heavy transitions yield lower MSS values. MSS = 1 1 (cid:88) t= It. (13) B.6 Score Aggregation We consolidate five fine-grained evaluation signals into two final indicators, Task Completion and Visual Quality. Notation. We denote the normalized values of the five fine-grained metrics as follows: (1) PSS: PhysicalSemantic Plausibility, (2) TAC: Task-Adherence Consistency, (3) RSS: Robot-Subject Stability, (4) MS: Motion Smoothness, (5) MA: Motion Amplitude. Normalization. Given raw metric value defined over range [smin, smax], its normalized value is clip[0,1] (cid:18) smin smax smin (cid:19) . (14) Penalty terms. Two penalty terms are used to down-weight videos with insufficient subject motion or unstable visual composition. Motion-amplitude penalty. Let MA denote the normalized motion amplitude. soft penalty is applied when MA falls below the threshold t: (t MA) + δ, MA < tlow, PMA(MA) = tlow MA < t, MA t, (15) MA, 0, 12 with = 0.1, tlow = 0.05, and δ = 0.1. Stability-consistency penalty. Robot and object level stability grades are mapped to penalty magnitudes: p(g) {0.2, 0.4, 0.6, 0.8} for grades {B, C, D, E}, while grade incurs zero penalty. Let gr and go denote robotand object-related stability grades: PRSS = p(gr) + p(go) 2 , p(gr), 0, if both exist, if only gr exists, otherwise. (16) (17) Final indicators. Task Completion (TC). Task correctness is computed from Physical-Semantic Plausibility (PSS) and TaskAdherence Consistency (TAC): TC = PSS + TAC 2 . (18) Visual Quality (VQ). Visual realism and temporal coherence are expressed as weighted combination of RSS and MS, penalized by low motion amplitude and visual instability: (cid:16) VQ = max 0, 0.8 RSS + 0.2 MS PMA(MA) PRSS (cid:17) . (19) Model-level aggregation. For each model, TC and VQ are computed for all evaluation samples. The final model score corresponds to the mean values of TC and VQ across samples, which are used for quantitative comparison and ranking in our benchmark."
        },
        {
            "title": "C Model Descriptions and Implementation Setups",
            "content": "C.1 Commercial Models Wan 2.6. Wan is comprehensive family of open-source foundational video generative models built on the Diffusion Transformer architecture. It supports multiple downstream tasks including T2V, I2V, editing, inpainting, and video-to-audio. We use the official API to generate 5-second 720P videos at 30 fps. Wan 2.5. Wan is comprehensive family of open-source foundational video generative models built on the Diffusion Transformer architecture. It supports multiple downstream tasks including T2V, I2V, editing, inpainting, and video-to-audio. We use the official API to generate 5-second 720P videos at 24 fps. Hailuo. Hailuo provides multimodal models for T2V, I2V, and T2A tasks, supporting resolutions up to 1080p and long-duration outputs with high temporal coherence. We generate 6 second videos at 1364 768 and 24 fps using the official Hailuo API. Veo3. Veo 3 is Googles latest foundational video generation model supporting high-resolution (1080p), longduration (up to 60 seconds), and audio-integrated video synthesis using large-scale Diffusion Transformer. We use the official Veo 3 API to generate videos up to 720p, 8 seconds, and 24 fps. Kling 2.6 pro. We use the official Kling 2.6 pro model with default parameters to generate 5-second video at 1920 1080 resolution and 24 fps. Seedance 1.0. Seedance 1.0 is large-scale video generation model from ByteDance, supporting text-to-video and image-to-video generation with high aesthetic quality and temporal stability. It integrates 3D causal VAE with 41616 compression ratio. We use the Seedance 1.0 model to generate 5-second videos at 1280 720 resolution and 24 fps. 13 Seedance 1.5. Seedance 1.5 is large-scale video generation model from ByteDance, supporting text-to-video and image-to-video generation with high aesthetic quality and temporal stability. We use the Seedance 1.5 model to generate 5-second videos at 1280 720 resolution and 24 fps. Sora v1. We use the official Sora v1 model with default parameters to generate 5-second video at 1280 720 resolution and 30 fps. Sora v2 Pro. We use the official Sora v2 Pro model with default parameters to generate 4-second video at 1280 720 resolution and 30 fps. C.2 Open-source Models Wan2.2_A14B. Wan2.2_A14B is an open-source large-scale video generation model that incorporates Mixture-of-Experts (MoE) architecture. This architecture dynamically allocates specialized expert networks to enhance the models capacity and temporal understanding. It supports multimodal inputs (text and images) for generating open-domain videos at 1280 720 resolution (720P), with typical duration of 5 seconds (120 frames) at 24 fps. Compared to its predecessors, Wan2.2_A14B demonstrates superior performance in generalizing across diverse scenes, modeling complex motions, and achieving fine-grained aesthetic control. The technical report and model weights are publicly available. We utilize the official Wan2.2_A14B model with its default parameters to generate 5-second video (81 frames) at spatial resolution of 1280 720 and frame rate of 16 fps. The model supports both text and image inputs. LongCat-Video. LongCat-Video is an open-source foundational video generation model with 13.6B parameters, developed by the Meituan LongCat Team. It unifies text-to-video (T2V), image-to-video (I2V), and video continuation (VC) tasks within single Diffusion Transformer (DiT) architecture, supporting efficient minutelong video generation without quality degradation. The model employs multi-reward RLHF optimization (Group Relative Policy Optimization) to enhance visual quality, motion coherence, and text alignment. We employ the official LongCat-Video model with default configurations, generating 1280 704 resolution videos at 15 fps using coarse-to-fine generation strategy. Wan2.2_5B. Wan2.2_5B is medium-scale model in the Wan2.2 series with 5 billion parameters, utilizing Transformer backbone architecture. It supports multi-modal video generation from text and images, generating 5-second (121 frames) open-domain videos at 1280 720 and 24 fps. Model weights and documentation are fully open-sourced. We use the official Wan2.2_5B model with default settings to generate 5-second video (120 frames) at resolution of 1248 704 and 24 fps. Wan2.1_14B. Wan2.1_14B is an early large-parameter video generation model in the Wan series with 14 billion parameters, based on multi-modal diffusion architecture. It generates 5-second (120-frame) open-domain videos at 1280 720 and 24 fps, emphasizing complex scene modeling and object motion understanding. We use the official Wan2.1_14B model with default parameters to generate 5-second video (81 frames) at 832 480 resolution and 16 fps. HunyuanVideo. We use the official HunyuanVideo model with default parameters to generate 5-second video at 1248 704 resolution and 24 fps. HunyuanVideo 1.5. We use the official HunyuanVideo 1.5 model with default parameters to generate 5-second video at 848 480 resolution and 24 fps. SkyReels-V2. SkyReels-V2 is an open-source infinite-length film generative model developed by Skywork AI. It supports text-to-video, image-to-video, and video continuation tasks, with modules including SkyCaptioner-V1, multi-stage pretraining, RL for motion quality, and diffusion forcing framework for long video synthesis. We use the official SkyReels-V2 model with default settings to generate 960 544 resolution videos at 24 fps for approximately 4 seconds. LTX-Video. LTX-Video is an open-source transformer-based latent diffusion model developed by Lightricks. It integrates Video-VAE and denoising transformer with 1:192 compression ratio using spatiotemporal downscaling, enabling efficient latent-space processing. It supports both text-to-video and image-to-video generation. We use the official LTX-Video model with default parameters to generate 5-second video at 832 480 resolution and 16 fps. 14 LTX-2. LTX-2 is an open-source transformer-based latent diffusion model. We use the official LTX-2 model with default parameters to generate 5-second video at 1536 1024 resolution and 24 fps. FramePack. FramePack is neural structure for next-frame prediction designed to avoid forgetting and drifting in video generation. It compresses input frames by importance to maintain fixed transformer context, enabling long video synthesis with low computational cost. It supports T2V and I2V and can be integrated with models such as HunyuanVideo or Wan. We employ FramePack with base model to generate 5-second videos at 832 480 resolution. CogVideoX. CogVideoX is an open-source text-to-video generation model employing diffusion transformer and 3D VAE. It introduces expert-adaptive LayerNorm and multi-resolution frame packing to enhance motion consistency. It produces up to 10-second videos at 720 1280 and 16 fps. We use the official CogVideoX weights and inference scripts with default settings to generate 6-second videos (160 frames) at 720 480 and 8 fps. C.3 Robotics-specific Models Cosmos 2.5. Cosmos-Predict2.5 and Cosmos-Transfer2.5 are NVIDIAs world simulation foundation models for physical AI, supporting Text2World, Image2World, and Video2World simulations with flow-based architectures. We generate videos at 1280 720 resolution and 16 fps using official NVIDIA codebase. DreamGen (GR). GRL is DreamGens generalizable robot learner model integrating synthetic neural trajectories and real teleoperation data for policy learning. We generate all trajectory videos at 768 432 resolution and 16 fps. DreamGen (DROID). DROID is dynamics-aware imitation learning model using video diffusion with inverse dynamics modeling for realistic physical trajectories. We generate all videos at up to 768 432 resolution and 16 fps. UnifoLM-WMA-0. UnifoLM-WMA-0 integrates video world modeling and action policy learning for generalizable robotic reasoning and control. We generate videos up to 320 512 resolution, 30 fps. Vidar. Vidar is an embodied video diffusion model designed for robotic manipulation, integrating strong video diffusion prior and Masked Inverse Dynamics Model (MIDM). Vidar generates 4-second multi-view interactive videos at 704 480 resolution and 15 fps with automatically annotated action trajectories."
        },
        {
            "title": "D Human Preference Study Details",
            "content": "To complement the main papers human preference analysis, this section provides additional statistical examinations that further characterize the agreement between human judgments and our automatic benchmark. The questionnaire used in our human preference study is shown in Figure 14. As described in the main paper, thirty participants compared pairs of generated videos for the same prompt and selected the better one (or \"Tie\"). These votes were converted into per-model human scores using the 5/3/1 win-tie-loss scheme, and the resulting model-level ranking exhibited strong correlation with RBench (ρ = 0.96). While rank correlation quantifies the consistency of ordering, it does not measure whether the two scoring methods agree in an absolute sense (i.e., whether they assign comparable magnitudes). To assess this complementary notion of agreement, we conduct Bland-Altman analysis between human scores and benchmark scores. Because the two score scales may differ by systematic offset or slope, we additionally apply leave-one-out (LOO) linear calibration to correct for potential scale mismatch without overfitting. The following subsections detail the calibration procedure, Bland-Altman computation, accompanying figures, and per-model statistics. BlandAltman basics. BlandAltman analysis assesses agreement between two measurement methods by , where and denote plotting, for each item i, the difference di = Bi Hi against the mean mi = Bi+Hi human and automatic scores, respectively. As shown in Fig. 15, the horizontal solid line represents the bias di (average difference). The dashed lines denote the 95% limits of agreement (LoA): 1.96 sd, = 1 (cid:80) 2 Figure 14 Visualization of the Questionnaire for User Study where sd is the standard deviation of the differences. Narrower LoA and small bias indicate stronger agreement beyond mere correlation. Linear LOO calibration. Human and automatic scores can exhibit scale mismatch. To mitigate such systematic differences, we apply leave-one-out (LOO) linear calibration to the benchmark. Mathematical formulation. Let {(Bi, Hi)}n be the automatic benchmark and human scores for models. For each {1, . . . , n}, define the training index set Si = {j : = i} and estimate the OLS (Ordinary Least Squares) calibration parameters by i=1 (ˆαi, ˆβi) = arg min α,β (cid:88) (cid:0)Bj α βHj (cid:1)2 . jSi The calibrated (benchmark-aligned) score for model is = ˆαi + ˆβi Bi. (20) (21) Bland--Altman with calibrated scores. We then perform BlandAltman analysis on (B, ) by computing, for each i, di = Bi , mi = Bi+H 2 . (22) The bias and the 95% LoA 1.96 sd are computed from {di}n Figure 15 shows the Bland-Altman plot after linear LOO calibration, with the bias and LoA reported in the legend. Table 5 lists, for each model, the mean mi, the difference di, and the corresponding LOO calibration parameters ˆαi and ˆβi used to obtain . As complement to the correlation analysis (Spearman ρ = 0.96 as in the basics above. i=1 16 Figure 15 BlandAltman plot after linear leave-one-out calibration (H ). Points are models; x-axis mi = Bi+H , . The solid line indicates the bias ( d); dashed lines show the 95% limits of agreement (LoA). In y-axis di = Bi our study the legend reports Bias = 0.002 and LoA = [0.108, 0.112]. 2 between human scores and the automatic benchmark), the Bland-Altman view quantifies absolute agreement: the small bias and tight LoA indicate that the calibrated benchmark scores are in close agreement with human judgments."
        },
        {
            "title": "E Prompt Template",
            "content": "This section introduces the prompt design used for the Visual Reasoning task in our MLLM-based evaluation pipeline. Among all tasks, Visual Reasoning involves the most structured form of reasoning and thus provides representative example for illustrating our prompt design. We provide the MLLM with essential contextual information recorded during dataset construction, including the video viewpoint, high-level content description, and the identities of the robotic manipulator and manipulated object. This background knowledge serves as the foundation for the models subsequent reasoning and scoring process. Unlike other tasks, Visual Reasoning requires explicit verification of logical dependencies between robot actions. Therefore, it adopts two-part structure consisting of question-chain constructor and video assessment prompt. The full templates are shown below. Question-chain construction. The first component converts the original text instruction into sequence of binary verification questions (Figure 16). The model analyzes the semantics of the instruction and generates short chain of stepwise questions that reflect the intended causal and temporal structure of the robots behavior. This question chain acts as an explicit reasoning scaffold and is subsequently fed into the main evaluation prompt. Video assessment prompt. The second component integrates the generated question chain with 32 grid of chronologically ordered frames extracted from the video (Figures 17 and 18). It also incorporates the contextual background information provided at the beginningnamely, the view perspective, video content 17 Model mean(B, ) diff(B ) αLOO βLOO LTX-Video Hailuo Seedance 1.0 Veo 3 Wan2.1 14b Wan2.5 Cosmos2.5 LongCat-Video Dreamgen Cogvideox-5B 0.295 0.539 0.528 0.551 0.388 0.571 0.482 0.458 0.451 0.295 0.098 0.052 0.046 0.024 0.021 -0.003 -0.036 -0.043 -0.062 -0. 0.141 0.205 0.203 0.204 0.193 0.199 0.200 0.202 0.203 0.242 0.613 0.496 0.500 0.503 0.524 0.517 0.523 0.520 0.522 0.446 Table 5 BlandAltman statistics after linear LOO calibration. mi = Bi+H intercept and slope learned from all other models when calibrating 2 , di = Bi . αLOO and βLOO are the . All values are rounded to three decimals. summary, robotic manipulator type, and manipulated object identity. With these inputs, the prompt instructs the model to determine whether each reasoning step has been successfully completed and to evaluate the stability, consistency, and physical plausibility of the robot and objects throughout the sequence. strict scoring protocol and structured JSON output format ensure reproducible and interpretable evaluation across models."
        },
        {
            "title": "F Additional Qualitative Comparisons",
            "content": "This section provides supplementary qualitative results that extend the \"Qualitative Comparison Across Representative Tasks\" analysis presented in the main paper. For each of the five task categories in RBench, namely Common Manipulation, Long-Horizon Planning, Multi-Entity Collaboration, Spatial Relationship, and Visual Reasoning, we select two representative cases and visualize the generated videos from ten state-of-the-art image-to-video (I2V) models. As shown in Figures 19, 20, 21, 22, 23, these examples offer more detailed inspection of model behaviors under diverse embodied scenarios, complementing the quantitative results discussed in the main text."
        },
        {
            "title": "G Comprehensive Quantitative Results",
            "content": "This section presents the complete quantitative evaluation results obtained from both GPT-based and Qwen-based evaluators. We report detailed per-model scores across all five tasks (Common Manipulation, Long-Horizon Planning, Multi-Entity Collaboration, Spatial Relationship, and Visual Reasoning) and four robot embodiments (Dual Arm, Humanoid, Single Arm, and Quadruped). To ensure clarity and readability, we summarize below the abbreviations used in all tables. For task-level metrics, each abbreviation corresponds to specific VQA-style prompt used in our MLLM-based evaluation; the construction of these prompts and the computation of the total score (TS) are illustrated with representative example in Section E. The robot-level metrics are defined in Section B. Task-level Metrics (per-task evaluation): AES: Action Execution Score TCS: Task Completion Score OCS: Object Consistency Score RCS: Robot Consistency Score PSS: Physical Semantic Score ECR: Event Completion Ratio ECS: Entity Consistency Score 18 Figure 16 Question-chain construction for Visual Reasoning. This component analyzes the original instruction and transforms it into sequence of binary verification questions that define causal and temporal dependencies in the robots intended actions. ACS: Action Coordination Score SRS: Spatial Relation Score MFS: Manipulation Feasibility Score VRS: Visual Reasoning Score TS: Total Score Robot-level Metrics (per-robot embodiment evaluation): PSS: Physical Semantic Score TAC: Task-Adherence Consistency RSS: RobotSubject Stability MSS: Motion Smoothness Score MAS: Motion Amplitude Score TC: Task Completion VQ: Visual Quality TS: Total Score Figure 17 Visual Reasoning evaluation prompt (Part I). The first part of the evaluation prompt integrates the structured reasoning chain with contextual information extracted during dataset construction. The following tables report the complete results for all models evaluated using both GPT and Qwen. As shown in Table 6, Table 7, Table 8, Table 9, and Table 10, the GPT-based evaluator provides assessments across five tasks. Similarly, the Qwen-based results are presented in Table 15, Table 16, Table 17, Table 18, and Table 19. For completeness, we also provide per-embodiment results. The detailed evaluation tables for dual-arm, humanoid, single-arm, and quadruped robots under GPT are shown in Table 11, Table 12, Table 13, and Table 14. The corresponding Qwen-based tables are provided in Table 20, Table 21, Table 22, and Table 23. 20 Figure 18 Visual Reasoning evaluation prompt (Part II). The second part specifies the structured scoring protocol and the JSON output format used to ensure consistent and interpretable evaluation results. 21 Figure 19 Visualization of Common Manipulation. The first row contains the reference image, and the accompanying text shows the input prompt. 22 Figure 20 Visualization of Long-Horizon Planning. The first row contains the reference image, and the accompanying text shows the input prompt. 23 Figure 21 Visualization of Multi-Entity Collaboration. The first row contains the reference image, and the accompanying text shows the input prompt. 24 Figure 22 Visualization of Spatial Relationship. The first row contains the reference image, and the accompanying text shows the input prompt. 25 Figure 23 Visualization of Visual Reasoning. The first row contains the reference image, and the accompanying text shows the input prompt. 26 Table 6 Results on Common Manipulation with GPT Table 7 Results on Long-Horizon Planning with GPT Model AES TCS OCS RCS PSS TS Model AES ECR OCS RCS PSS TS Open-source 0.479 0.410 0.635 0.765 0.520 0.381 Wan2.2_A14B HunyuanVideo 1.5 0.505 0.480 0.575 0.695 0.490 0.442 0.469 0.408 0.591 0.739 0.510 0.371 LongCat-Video 0.446 0.375 0.552 0.692 0.510 0.344 Wan2.1_14B 0.340 0.330 0.515 0.620 0.380 0.284 LTX-2 0.416 0.395 0.540 0.670 0.495 0.331 Wan2.2_5B 0.348 0.230 0.545 0.740 0.465 0.202 Skyreels 0.414 0.307 0.552 0.718 0.484 0.302 LTX-Video 0.346 0.188 0.637 0.739 0.556 0.205 FramePack 0.307 0.190 0.660 0.755 0.545 0.177 HunyuanVideo CogVideoX-5B 0.245 0.140 0.490 0.635 0.335 0.115 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.495 0.370 0.585 0.705 0.505 0.358 Cosmos 2.5 0.420 0.341 0.566 0.683 0.469 0.311 DreamGen(gr1) DreamGen(droid) 0.420 0.369 0.604 0.640 0.421 0.358 Vidar 0.180 0.090 0.430 0.470 0.310 0.073 UnifoLM-WMA-0 0.105 0.045 0.400 0.300 0.220 0. 0.581 0.596 0.637 0.750 0.668 0.545 0.654 0.642 0.591 0.750 0.556 0.576 0.565 0.600 0.635 0.770 0.605 0.527 0.576 0.625 0.625 0.745 0.595 0.559 0.572 0.602 0.607 0.729 0.540 0.520 0.591 0.590 0.640 0.730 0.620 0.542 0.561 0.565 0.610 0.760 0.590 0.528 0.354 0.229 0.637 0.719 0.561 0.207 0.280 0.170 0.445 0.605 0.360 0.151 Open-source 0.569 0.626 0.715 0.722 0.590 0.500 Wan2.2_A14B HunyuanVideo 1.5 0.537 0.527 0.606 0.725 0.537 0.437 0.507 0.485 0.598 0.742 0.598 0.384 LongCat-Video 0.482 0.404 0.589 0.687 0.553 0.335 Wan2.1_14B 0.485 0.446 0.566 0.676 0.529 0.386 LTX-2 0.444 0.517 0.601 0.666 0.425 0.317 Wan2.2_5B 0.394 0.311 0.673 0.740 0.500 0.253 Skyreels 0.433 0.347 0.566 0.633 0.458 0.279 LTX-Video 0.301 0.145 0.655 0.732 0.560 0.168 FramePack 0.241 0.101 0.651 0.714 0.553 0.147 HunyuanVideo CogVideoX-5B 0.301 0.223 0.543 0.629 0.448 0.212 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.568 0.556 0.706 0.656 0.593 0.495 Cosmos 2.5 0.444 0.284 0.750 0.731 0.611 0.333 DreamGen(gr1) DreamGen(droid) 0.491 0.505 0.655 0.637 0.517 0.316 Vidar 0.108 -0.023 0.550 0.491 0.375 0.054 UnifoLM-WMA-0 0.037 0.026 0.398 0.175 0.120 0.061 0.640 0.545 0.701 0.743 0.634 0.514 0.638 0.710 0.677 0.763 0.625 0.569 0.603 0.519 0.743 0.737 0.615 0.495 0.600 0.677 0.725 0.706 0.637 0.544 0.608 0.681 0.709 0.729 0.641 0.530 0.606 0.603 0.712 0.727 0.628 0.454 0.618 0.685 0.710 0.750 0.697 0.530 0.422 0.296 0.646 0.715 0.603 0.255 0.250 0.133 0.629 0.689 0.543 0.166 Mean 0.427 0.371 0.572 0.685 0.492 0.338 Mean 0.453 0.413 0.643 0.677 0.544 0.349 Table 8 Results on Multi-Entity Collaboration with GPT Model ACS TCS ECS OCS PSS TS Open-source Wan2.2_A14B 0.351 0.441 0.712 0.632 0.654 0.373 HunyuanVideo 1.5 0.301 0.375 0.687 0.625 0.585 0.311 0.190 0.244 0.696 0.642 0.654 0.220 LongCat-Video 0.261 0.318 0.693 0.545 0.551 0.282 Wan2.1_14B 0.243 0.289 0.651 0.572 0.578 0.233 LTX-2 0.125 0.181 0.687 0.568 0.537 0.141 Wan2.2_5B 0.189 0.195 0.695 0.664 0.621 0.203 Skyreels 0.207 0.255 0.686 0.574 0.563 0.209 LTX-Video 0.186 0.186 0.709 0.598 0.529 0.173 FramePack 0.100 0.100 0.672 0.683 0.650 0.107 HunyuanVideo CogVideoX-5B 0.128 0.113 0.630 0.482 0.440 0.098 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.203 0.244 0.727 0.659 0.670 0.201 Cosmos 2.5 DreamGen(gr1) 0.262 0.347 0.707 0.664 0.628 0.296 DreamGen(droid) 0.211 0.260 0.657 0.548 0.548 0.214 Vidar 0.056 0.062 0.454 0.431 0.329 0.049 UnifoLM-WMA-0 0.017 0.034 0.392 0.301 0.295 0.018 0.443 0.541 0.738 0.708 0.654 0.478 0.456 0.591 0.743 0.689 0.689 0.483 0.392 0.453 0.750 0.633 0.662 0.401 0.378 0.422 0.744 0.678 0.619 0.385 0.450 0.432 0.731 0.737 0.621 0.430 0.422 0.488 0.750 0.702 0.696 0.447 0.357 0.392 0.738 0.698 0.676 0.364 0.155 0.191 0.733 0.558 0.591 0.186 0.128 0.107 0.595 0.494 0.523 0. Mean 0.248 0.290 0.679 0.603 0.583 0.256 Table 9 Results on Spatial Relationship with GPT Model SRS MFS OCS RCS PSS TS Open-source Wan2.2_A14B 0.604 0.596 0.709 0.758 0.669 0.454 HunyuanVideo 1.5 0.370 0.344 0.629 0.750 0.534 0.315 0.390 0.375 0.679 0.734 0.648 0.310 LongCat-Video 0.316 0.338 0.669 0.713 0.610 0.267 Wan2.1_14B 0.401 0.383 0.616 0.687 0.562 0.304 LTX-2 0.441 0.389 0.602 0.705 0.485 0.312 Wan2.2_5B 0.388 0.416 0.601 0.685 0.574 0.276 Skyreels 0.224 0.215 0.612 0.681 0.491 0.176 LTX-Video 0.364 0.321 0.628 0.742 0.557 0.257 FramePack 0.163 0.192 0.605 0.740 0.586 0.179 HunyuanVideo CogVideoX-5B 0.193 0.241 0.451 0.653 0.370 0.111 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.419 0.395 0.661 0.693 0.620 0.338 Cosmos 2.5 DreamGen(gr1) 0.467 0.411 0.677 0.725 0.677 0.371 DreamGen(droid) 0.400 0.433 0.625 0.691 0.591 0.348 Vidar 0.163 0.250 0.517 0.500 0.405 0.105 UnifoLM-WMA-0 0.065 0.141 0.445 0.532 0.315 0. 0.787 0.704 0.719 0.734 0.734 0.655 0.675 0.608 0.700 0.716 0.666 0.494 0.750 0.598 0.757 0.750 0.674 0.576 0.764 0.654 0.742 0.750 0.720 0.636 0.601 0.553 0.726 0.767 0.684 0.508 0.484 0.445 0.710 0.742 0.679 0.425 0.757 0.681 0.727 0.757 0.696 0.598 0.392 0.392 0.654 0.714 0.571 0.267 0.351 0.305 0.546 0.583 0.481 0.223 Mean 0.437 0.415 0.640 0.700 0.584 0.342 27 Table 10 Results on Visual Reasoning with GPT Table 11 Results on Dual Arm with GPT Model AES VRS OCS RCS PSS TS Model PSS TAC RSS MS MA TC VQ TS Open-source 0.424 0.401 0.651 0.709 0.552 0.330 Wan2.2_A14B HunyuanVideo 1.5 0.453 0.456 0.604 0.709 0.447 0.364 0.271 0.211 0.559 0.722 0.516 0.186 LongCat-Video 0.250 0.263 0.565 0.646 0.429 0.204 Wan2.1_14B 0.267 0.224 0.500 0.610 0.360 0.163 LTX-2 0.283 0.304 0.583 0.700 0.438 0.233 Wan2.2_5B 0.290 0.267 0.587 0.668 0.447 0.233 Skyreels 0.283 0.287 0.644 0.688 0.516 0.241 LTX-Video 0.243 0.203 0.570 0.743 0.397 0.169 FramePack 0.096 0.058 0.647 0.744 0.522 0.035 HunyuanVideo CogVideoX-5B 0.136 0.120 0.428 0.577 0.261 0.079 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.482 0.493 0.664 0.744 0.590 0.399 Cosmos 2.5 0.317 0.262 0.652 0.743 0.585 0.215 DreamGen(gr1) DreamGen(droid) 0.412 0.401 0.616 0.738 0.529 0.338 Vidar 0.090 0.050 0.556 0.659 0.431 0.050 UnifoLM-WMA-0 0.017 -0.056 0.062 0.306 0.176 0.000 0.544 0.622 0.700 0.733 0.605 0.530 0.494 0.570 0.635 0.701 0.570 0.470 0.482 0.488 0.693 0.732 0.539 0.437 0.511 0.541 0.666 0.727 0.566 0.473 0.511 0.610 0.633 0.711 0.577 0.504 0.505 0.505 0.705 0.733 0.644 0.441 0.477 0.410 0.627 0.733 0.555 0.357 0.193 0.159 0.642 0.738 0.556 0.115 0.219 0.158 0.475 0.646 0.250 0. Open-source 0.638 0.570 0.764 0.915 0.204 0.604 0.561 0.582 Wan2.2_A14B HunyuanVideo 1.5 0.612 0.622 0.649 0.951 0.370 0.618 0.434 0.526 0.620 0.540 0.771 0.937 0.244 0.580 0.572 0.576 LongCat-Video 0.600 0.540 0.650 0.850 0.261 0.570 0.424 0.497 Wan2.1_14B 0.488 0.415 0.637 0.848 0.378 0.451 0.396 0.423 LTX-2 0.575 0.498 0.606 0.940 0.269 0.536 0.360 0.448 Wan2.2_5B 0.598 0.498 0.658 0.884 0.252 0.548 0.406 0.477 Skyreels 0.530 0.442 0.698 0.812 0.143 0.486 0.425 0.455 LTX-Video 0.550 0.395 0.712 0.885 0.103 0.472 0.457 0.464 FramePack 0.510 0.280 0.794 0.959 0.107 0.395 0.564 0.479 HunyuanVideo CogVideoX-5B 0.480 0.358 0.638 0.752 0.143 0.419 0.352 0.385 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.665 0.575 0.746 0.930 0.127 0.620 0.500 0.560 Cosmos 2.5 0.630 0.492 0.779 0.939 0.123 0.561 0.503 0.532 DreamGen(gr1) DreamGen(droid) 0.610 0.542 0.668 0.863 0.201 0.576 0.375 0.475 Vidar 0.450 0.238 0.781 0.933 0.025 0.344 0.475 0.409 UnifoLM-WMA-0 0.348 0.252 0.348 0.497 0.120 0.300 0.089 0.194 0.655 0.708 0.819 0.984 0.333 0.681 0.680 0.680 0.668 0.800 0.721 0.960 0.399 0.734 0.547 0.640 0.670 0.740 0.758 0.970 0.347 0.705 0.563 0.633 0.658 0.720 0.751 0.983 0.312 0.689 0.534 0.611 0.665 0.682 0.711 0.973 0.262 0.674 0.546 0.610 0.668 0.648 0.801 0.972 0.294 0.658 0.623 0.640 0.672 0.615 0.770 0.965 0.210 0.644 0.567 0.605 0.565 0.364 0.776 0.950 0.272 0.465 0.560 0.512 0.422 0.312 0.531 0.880 0.210 0.368 0.279 0.323 Mean 0.330 0.320 0.587 0.686 0.482 0.268 Mean 0.581 0.513 0.701 0.901 0.228 0.547 0.471 0. Table 12 Results on Humanoid with GPT Table 13 Results on Single Arm with GPT Model PSS TAC RSS MS MA TC VQ TS Model PSS TAC RSS MS MA TC VQ TS Open-source Wan2.2_A14B 0.678 0.748 0.787 0.966 0.105 0.712 0.584 0.647 HunyuanVideo 1.5 0.660 0.800 0.703 0.922 0.206 0.730 0.460 0.595 0.652 0.668 0.801 0.969 0.089 0.660 0.583 0.621 LongCat-Video 0.635 0.660 0.765 0.938 0.167 0.648 0.550 0.599 Wan2.1_14B 0.622 0.608 0.736 0.880 0.231 0.615 0.495 0.554 LTX-2 0.668 0.695 0.755 0.963 0.152 0.681 0.533 0.607 Wan2.2_5B 0.610 0.598 0.725 0.808 0.086 0.604 0.414 0.509 Skyreels 0.602 0.630 0.671 0.628 0.050 0.616 0.312 0.463 LTX-Video 0.598 0.572 0.776 0.864 0.069 0.585 0.511 0.548 FramePack 0.595 0.465 0.763 0.933 0.096 0.530 0.517 0.523 HunyuanVideo CogVideoX-5B 0.570 0.565 0.698 0.647 0.127 0.568 0.424 0.496 Closed-source Wan 2.6 0.665 0.818 0.794 0.980 0.132 0.741 0.593 0.667 Seedance 1.5 pro 0.682 0.838 0.793 0.955 0.158 0.760 0.623 0.691 0.698 0.782 0.779 0.981 0.122 0.740 0.568 0.653 Wan 2.5 0.682 0.815 0.741 0.970 0.133 0.749 0.521 0.635 Hailuo v2 0.655 0.785 0.776 0.968 0.132 0.720 0.554 0.637 Veo 3 0.675 0.752 0.833 0.964 0.166 0.714 0.658 0.686 Seedance 1.0 0.672 0.762 0.750 0.959 0.113 0.718 0.508 0.613 Kling 2.6 pro 0.638 0.565 0.774 0.936 0.085 0.602 0.520 0.561 Sora v2 Pro 0.542 0.450 0.597 0.900 0.216 0.496 0.342 0.419 Sora v1 Robotics-specific 0.650 0.720 0.797 0.925 0.071 0.685 0.566 0.625 Cosmos 2.5 DreamGen(gr1) 0.652 0.595 0.781 0.885 0.079 0.624 0.526 0.575 DreamGen(droid) 0.620 0.700 0.704 0.843 0.137 0.660 0.453 0.556 Vidar 0.445 0.298 0.694 0.855 0.025 0.371 0.343 0.357 UnifoLM-WMA-0 0.270 0.282 0.386 0.512 0.112 0.276 0.125 0.200 Open-source Wan2.2_A14B 0.638 0.582 0.783 0.942 0.263 0.610 0.607 0.608 HunyuanVideo 1.5 0.510 0.622 0.651 0.949 0.355 0.566 0.460 0.513 0.562 0.530 0.807 0.945 0.298 0.546 0.625 0.585 LongCat-Video 0.542 0.472 0.677 0.849 0.282 0.507 0.422 0.464 Wan2.1_14B 0.490 0.410 0.681 0.919 0.464 0.450 0.456 0.453 LTX-2 0.518 0.480 0.619 0.943 0.313 0.499 0.372 0.435 Wan2.2_5B 0.525 0.495 0.712 0.911 0.286 0.510 0.504 0.507 Skyreels 0.492 0.408 0.686 0.804 0.158 0.450 0.431 0.440 LTX-Video 0.445 0.318 0.760 0.888 0.104 0.381 0.498 0.439 FramePack 0.445 0.265 0.809 0.963 0.118 0.355 0.552 0.453 HunyuanVideo CogVideoX-5B 0.405 0.335 0.582 0.815 0.256 0.370 0.307 0.338 Closed-source Wan 2.6 Seedance 1.5 pro Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Kling 2.6 pro Sora v2 Pro Sora v1 Robotics-specific 0.632 0.592 0.737 0.888 0.206 0.612 0.475 0.543 Cosmos 2.5 DreamGen(gr1) 0.618 0.620 0.716 0.932 0.263 0.619 0.508 0.563 DreamGen(droid) 0.568 0.568 0.691 0.930 0.214 0.568 0.430 0.499 Vidar 0.415 0.272 0.726 0.929 0.068 0.344 0.420 0.382 UnifoLM-WMA-0 0.428 0.315 0.437 0.709 0.389 0.371 0.164 0.267 0.652 0.710 0.796 0.983 0.392 0.681 0.651 0.666 0.635 0.832 0.752 0.960 0.419 0.734 0.561 0.647 0.668 0.802 0.787 0.969 0.412 0.735 0.624 0.679 0.665 0.752 0.680 0.989 0.396 0.709 0.479 0.594 0.642 0.755 0.750 0.977 0.362 0.699 0.568 0.633 0.658 0.655 0.769 0.967 0.306 0.656 0.589 0.622 0.622 0.640 0.714 0.958 0.305 0.631 0.508 0.569 0.490 0.310 0.784 0.962 0.324 0.400 0.552 0.476 0.350 0.320 0.532 0.884 0.247 0.335 0.293 0.314 Mean 0.617 0.646 0.735 0.886 0.122 0.632 0.491 0.561 Mean 0.544 0.522 0.705 0.918 0.288 0.533 0.482 0.507 28 Table 14 Results on Quadruped Robot with GPT Model PSS TAC RSS MS MA TC VQ TS Open-source Wan2.2_A14B 0.760 0.712 0.800 0.888 0.196 0.736 0.643 0.689 HunyuanVideo 1.5 0.715 0.738 0.713 0.940 0.403 0.726 0.542 0.634 0.742 0.628 0.827 0.923 0.179 0.685 0.676 0.680 LongCat-Video 0.722 0.670 0.706 0.850 0.303 0.696 0.495 0.595 Wan2.1_14B 0.715 0.670 0.758 0.845 0.287 0.692 0.552 0.622 LTX-2 0.712 0.678 0.688 0.921 0.273 0.695 0.486 0.590 Wan2.2_5B 0.732 0.605 0.722 0.853 0.163 0.669 0.503 0.586 Skyreels 0.698 0.678 0.668 0.676 0.122 0.688 0.364 0.526 LTX-Video 0.720 0.575 0.827 0.876 0.069 0.648 0.605 0.626 FramePack 0.730 0.602 0.788 0.953 0.127 0.666 0.584 0.625 HunyuanVideo 0.655 0.560 0.624 0.618 0.220 0.608 0.322 0.464 CogVideoX-5B Closed-source Wan 2.6 0.755 0.792 0.813 0.970 0.316 0.774 0.672 0.723 Seedance 1.5 pro 0.748 0.820 0.746 0.884 0.407 0.784 0.577 0.680 0.785 0.792 0.809 0.948 0.322 0.789 0.664 0.726 Wan 2.5 0.748 0.738 0.711 0.961 0.354 0.742 0.538 0.640 Hailuo v2 0.745 0.695 0.798 0.961 0.214 0.720 0.658 0.689 Veo 3 0.768 0.735 0.791 0.945 0.334 0.751 0.645 0.698 Seedance 1.0 0.740 0.738 0.736 0.861 0.258 0.739 0.535 0.637 Kling 2.6 pro 0.731 0.670 0.789 0.922 0.239 0.701 0.626 0.663 Sora v2 Pro Sora v1 0.700 0.672 0.620 0.863 0.282 0.686 0.401 0.543 Robotics-specific 0.752 0.622 0.808 0.892 0.137 0.688 0.629 0.658 Cosmos 2.5 DreamGen(gr1) 0.712 0.655 0.706 0.788 0.164 0.684 0.474 0.579 DreamGen(droid) 0.705 0.568 0.687 0.854 0.160 0.636 0.448 0.542 Vidar 0.528 0.472 0.552 0.749 0.074 0.500 0.247 0.373 UnifoLM-WMA-0 0.475 0.390 0.410 0.497 0.132 0.432 0.154 0. Table 15 Results on Common Manipulation with Qwen Model AES TCS OCS RCS PSS TS 0.804 0.815 0.934 0.913 0.923 0.708 0.681 0.704 0.840 0.920 0.897 0.677 0.620 0.630 0.940 0.890 0.860 0.597 0.663 0.653 0.903 0.932 0.807 0.687 0.547 0.559 0.928 0.916 0.952 0.546 0.630 0.666 0.845 0.904 0.880 0.450 0.336 0.347 0.913 0.956 0.934 0.455 0.352 0.340 0.625 0.704 0.647 0.289 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.687 0.708 0.895 0.916 0.864 0.687 Cosmos 2.5 DreamGen(gr1) 0.625 0.656 0.906 0.937 0.906 0.507 DreamGen(droid) 0.630 0.690 0.785 0.845 0.773 0.465 UnifoLM-WMA-0 0.043 0.043 0.532 0.478 0.369 0.028 0.142 0.166 0.833 0.833 0.761 0.117 Vidar 0.928 0.946 0.946 0.964 0.973 0.887 0.916 0.952 0.940 0.940 0.976 0.843 0.953 0.962 0.962 0.981 0.990 0.896 0.946 0.928 0.955 0.946 0.964 0. Mean 0.597 0.611 0.867 0.858 0.835 0.559 Mean 0.711 0.659 0.723 0.857 0.229 0.685 0.521 0.603 Table 16 Results on Long-Horizon Planning with Qwen Table 17 Results on Multi-Entity Collaboration with Qwen Model AES ECS OCS RCS PSS TS Model ACS TCS ECS OCS PSS TS 0.836 0.883 0.942 0.971 0.951 0.680 0.650 0.706 0.925 0.950 0.912 0.489 0.520 0.594 0.947 0.937 0.895 0.449 0.681 0.669 0.931 0.931 0.875 0.465 0.416 0.511 0.944 0.958 0.861 0.324 0.352 0.414 0.779 0.808 0.705 0.357 0.212 0.252 0.862 0.987 0.937 0.196 0.276 0.292 0.855 0.763 0.684 0.096 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.731 0.810 0.953 0.981 0.935 0.596 Cosmos 2.5 DreamGen(gr1) 0.475 0.587 0.950 0.937 0.887 0.353 DreamGen(droid) 0.602 0.717 0.882 0.882 0.852 0.301 UnifoLM-WMA-0 0.000 -0.041 0.645 0.500 0.500 0.000 0.050 0.058 0.800 0.866 0.666 0.019 Vidar 0.714 0.836 0.964 0.964 0.955 0.719 0.808 0.903 0.950 0.991 0.908 0.705 0.812 0.903 0.984 0.968 0.945 0.854 0.824 0.889 0.990 0.990 0.925 0.715 0.941 0.941 0.985 0.992 1.000 0.920 0.887 0.912 0.968 0.962 0.968 0.814 0.763 0.796 0.993 0.960 0.967 0.721 0.750 0.786 0.975 0.945 0.957 0.702 0.743 0.736 1.000 1.000 0.993 0.686 0.806 0.806 0.975 0.975 0.993 0.734 0.710 0.703 0.980 0.993 0.947 0.630 0.522 0.536 0.933 0.882 0.889 0.426 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.864 0.878 0.993 1.000 0.972 0.768 Cosmos 2.5 DreamGen(gr1) 0.878 0.878 0.957 0.963 0.969 0.848 DreamGen(droid) 0.763 0.819 0.958 0.951 0.930 0.591 UnifoLM-WMA-0 0.044 0.044 0.507 0.477 0.301 0.025 0.083 0.097 0.743 0.736 0.583 0.082 Vidar 0.896 0.908 0.987 0.975 1.000 0.915 0.986 0.993 0.952 0.972 0.986 0.892 0.914 0.914 0.993 0.987 1.000 0.924 0.960 0.967 0.973 0.960 1.000 0.898 Mean 0.505 0.565 0.893 0.886 0.833 0. Mean 0.714 0.724 0.933 0.928 0.910 0.657 29 Table 18 Results on Spatial Relationship with Qwen Table 19 Results on Visual Reasoning with Qwen Model SRS MFS OCS RCS PSS TS Model AES VRS OCS RCS PSS TS 0.833 0.833 1.000 1.000 1.000 0.660 0.636 0.636 1.000 1.000 1.000 0.465 0.600 0.625 1.000 0.900 0.900 0.402 0.636 0.636 1.000 1.000 1.000 0.400 0.750 0.750 1.000 0.875 0.875 0.400 0.750 0.750 1.000 1.000 1.000 0.382 0.400 0.400 1.000 1.000 1.000 0.240 0.500 0.535 1.000 0.857 0.714 0.240 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.875 0.875 1.000 1.000 1.000 0.512 Cosmos 2.5 DreamGen(gr1) 0.642 0.642 1.000 1.000 1.000 0.500 DreamGen(droid) 0.545 0.545 1.000 1.000 1.000 0.505 UnifoLM-WMA-0 0.200 0.250 0.625 0.575 0.500 0.065 0.367 0.382 0.691 0.647 0.573 0.140 Vidar 0.916 0.916 1.000 0.979 1.000 0.825 1.000 1.000 1.000 1.000 1.000 0.840 0.933 0.933 1.000 1.000 1.000 0.740 0.666 0.666 1.000 1.000 1.000 0.665 0.727 0.701 1.000 1.000 1.000 0.550 0.361 0.358 0.944 0.958 0.847 0.354 0.291 0.218 0.906 0.906 0.906 0.420 0.397 0.401 0.886 0.897 0.784 0.269 0.420 0.401 0.960 0.930 0.830 0.357 0.390 0.410 0.859 0.937 0.859 0.285 0.350 0.343 0.980 0.990 0.950 0.345 0.073 0.044 0.779 0.632 0.544 0.030 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.593 0.632 0.984 1.000 1.000 0.506 Cosmos 2.5 DreamGen(gr1) 0.437 0.425 0.937 0.958 0.822 0.404 DreamGen(droid) 0.600 0.575 0.800 0.825 0.787 0.386 UnifoLM-WMA-0 0.000 -0.098 0.177 0.348 0.289 0.000 0.000 -0.062 0.947 0.937 0.906 0.029 Vidar 0.809 0.770 0.988 1.000 0.952 0.737 0.790 0.882 1.000 1.000 0.900 0.820 0.847 0.853 0.945 1.000 0.989 0.750 0.927 0.945 0.979 0.989 1.000 0.789 Mean 0.666 0.674 0.962 0.935 0.920 0. Mean 0.452 0.438 0.890 0.882 0.825 0.395 Table 20 Results on Dual Arm Robot with Qwen Table 21 Results on Humanoid Robot with Qwen Model PSS TAC RSS MS MA TC VQ TS Model PSS TAC RSS MS MA TC VQ TS 0.852 0.760 0.767 0.915 0.204 0.806 0.550 0.678 0.738 0.638 0.741 0.937 0.244 0.688 0.517 0.602 0.688 0.645 0.658 0.940 0.269 0.666 0.402 0.534 0.790 0.730 0.639 0.850 0.261 0.760 0.364 0.562 0.755 0.700 0.675 0.884 0.252 0.728 0.419 0.573 0.615 0.535 0.692 0.812 0.143 0.575 0.399 0.487 0.612 0.495 0.719 0.885 0.103 0.554 0.445 0.499 0.505 0.540 0.613 0.752 0.143 0.522 0.323 0.422 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.792 0.708 0.791 0.930 0.127 0.750 0.543 0.646 Cosmos 2.5 DreamGen(gr1) 0.780 0.638 0.801 0.939 0.123 0.709 0.555 0.632 DreamGen(droid) 0.722 0.678 0.711 0.863 0.201 0.700 0.441 0.570 UnifoLM-WMA-0 0.110 0.222 0.266 0.497 0.120 0.166 0.046 0.106 0.295 0.240 0.804 0.933 0.025 0.268 0.511 0.389 Vidar 0.920 0.880 0.761 0.970 0.347 0.900 0.588 0.744 0.908 0.848 0.744 0.983 0.312 0.878 0.534 0.706 0.870 0.802 0.777 0.973 0.262 0.836 0.581 0.708 0.895 0.810 0.801 0.972 0.294 0.852 0.608 0.730 0.935 0.800 0.806 0.966 0.105 0.898 0.557 0.727 0.918 0.765 0.826 0.969 0.089 0.841 0.579 0.710 0.880 0.758 0.791 0.963 0.152 0.819 0.544 0.681 0.895 0.712 0.785 0.938 0.167 0.804 0.524 0.664 0.842 0.670 0.803 0.808 0.086 0.756 0.500 0.628 0.852 0.670 0.808 0.628 0.050 0.761 0.445 0.603 0.815 0.622 0.838 0.864 0.069 0.719 0.550 0.634 0.712 0.602 0.710 0.647 0.127 0.658 0.390 0. Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.868 0.730 0.841 0.925 0.071 0.799 0.578 0.688 Cosmos 2.5 DreamGen(gr1) 0.858 0.682 0.823 0.885 0.079 0.770 0.542 0.656 DreamGen(droid) 0.842 0.736 0.739 0.843 0.137 0.788 0.478 0.633 UnifoLM-WMA-0 0.168 0.285 0.349 0.512 0.112 0.226 0.115 0.170 0.440 0.305 0.700 0.855 0.025 0.372 0.357 0.364 Vidar 0.935 0.835 0.826 0.981 0.122 0.885 0.600 0.742 0.952 0.848 0.796 0.970 0.133 0.891 0.548 0.719 0.955 0.802 0.831 0.968 0.132 0.829 0.604 0.716 0.936 0.814 0.829 0.964 0.166 0.904 0.614 0.759 Mean 0.685 0.622 0.694 0.880 0.205 0.651 0.449 0.550 Mean 0.806 0.674 0.770 0.862 0.113 0.740 0.501 0. 30 Table 22 Results on Single Arm Robot with Qwen Model PSS TAC RSS MS MA TC VQ TS Table 23 Results on Quadruped Robot with Qwen Model PSS TAC RSS MS MA TC VQ TS 0.815 0.802 0.755 0.942 0.263 0.809 0.568 0.688 0.750 0.712 0.812 0.945 0.298 0.736 0.660 0.698 0.605 0.650 0.626 0.943 0.313 0.628 0.394 0.511 0.700 0.616 0.638 0.849 0.282 0.659 0.379 0.519 0.760 0.699 0.731 0.911 0.286 0.732 0.492 0.612 0.535 0.525 0.710 0.804 0.158 0.530 0.443 0.486 0.400 0.500 0.674 0.888 0.104 0.394 0.413 0.403 0.460 0.460 0.553 0.815 0.256 0.460 0.288 0.374 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.795 0.765 0.751 0.888 0.206 0.776 0.516 0.646 Cosmos 2.5 DreamGen(gr1) 0.812 0.728 0.782 0.932 0.263 0.770 0.550 0.660 DreamGen(droid) 0.745 0.675 0.721 0.930 0.214 0.710 0.468 0.589 UnifoLM-WMA-0 0.382 0.495 0.391 0.709 0.389 0.439 0.140 0.289 0.220 0.200 0.736 0.929 0.068 0.210 0.479 0.344 Vidar 0.895 0.918 0.747 0.969 0.412 0.895 0.589 0.742 0.910 0.906 0.705 0.989 0.396 0.902 0.489 0.695 0.898 0.891 0.756 0.977 0.362 0.890 0.594 0.742 0.852 0.818 0.747 0.967 0.306 0.835 0.580 0.707 0.860 0.698 0.746 0.888 0.196 0.779 0.561 0.670 0.870 0.685 0.747 0.923 0.179 0.778 0.554 0.666 0.858 0.635 0.721 0.921 0.273 0.746 0.529 0.637 0.845 0.665 0.674 0.850 0.303 0.755 0.453 0.604 0.862 0.652 0.749 0.853 0.163 0.758 0.550 0.654 0.818 0.690 0.709 0.676 0.122 0.754 0.423 0.588 0.765 0.540 0.881 0.876 0.069 0.652 0.687 0.669 0.705 0.575 0.639 0.618 0.220 0.640 0.349 0.494 Open-source Wan2.2_A14B LongCat-Video Wan2.2_5B Wan2.1_14B Skyreels LTX-Video FramePack CogVideoX-5B Closed-source Wan 2.5 Hailuo v2 Veo 3 Seedance 1.0 Robotics-specific 0.850 0.612 0.755 0.892 0.137 0.731 0.547 0.639 Cosmos 2.5 DreamGen(gr1) 0.852 0.672 0.701 0.788 0.164 0.762 0.460 0.611 DreamGen(droid) 0.745 0.592 0.724 0.854 0.160 0.669 0.500 0.584 UnifoLM-WMA-0 0.310 0.300 0.399 0.497 0.132 0.305 0.197 0.251 0.495 0.445 0.575 0.749 0.074 0.470 0.290 0.380 Vidar 0.902 0.740 0.769 0.948 0.322 0.821 0.601 0.711 0.888 0.722 0.654 0.961 0.354 0.805 0.458 0.631 0.880 0.722 0.793 0.961 0.214 0.801 0.652 0.726 0.865 0.710 0.728 0.945 0.334 0.788 0.560 0.674 Mean 0.658 0.648 0.679 0.898 0.263 0.649 0.454 0.551 Mean 0.782 0.612 0.705 0.825 0.198 0.701 0.494 0."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Peking University"
    ]
}