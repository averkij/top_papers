{
    "paper_title": "VideoMind: A Chain-of-LoRA Agent for Long Video Reasoning",
    "authors": [
        "Ye Liu",
        "Kevin Qinghong Lin",
        "Chang Wen Chen",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning - especially for videos - remains unexplored. In this work, we introduce VideoMind, a novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop a role-based agentic workflow, including a planner for coordinating different roles, a grounder for temporal localization, a verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose a novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning."
        },
        {
            "title": "Start",
            "content": "VideoMind: Chain-of-LoRA Agent for Long Video Reasoning Ye Liu1 Kevin Qinghong Lin2 Chang Wen Chen1 (cid:66) Mike Zheng Shou2 (cid:66) 1 The Hong Kong Polytechnic University 2 Show Lab, National University of Singapore https://videomind.github.io/ 5 2 0 2 7 1 ] . [ 1 4 4 4 3 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Videos, with their unique temporal dimension, demand precise grounded understanding, where answers are directly linked to visual, interpretable evidence. Despite significant breakthroughs in reasoning capabilities within Large Language Models, multi-modal reasoning especially for videos remains unexplored. In this work, we introduce VideoMind, novel video-language agent designed for temporal-grounded video understanding. VideoMind incorporates two key innovations: (i) We identify essential capabilities for video temporal reasoning and develop rolebased agentic workflow, including planner for coordinating different roles, grounder for temporal localization, verifier to assess temporal interval accuracy, and an answerer for question-answering. (ii) To efficiently integrate these diverse roles, we propose novel Chain-of-LoRA strategy, enabling seamless role-switching via lightweight LoRA adaptors while avoiding the overhead of multiple models, thus balancing efficiency and flexibility. Extensive experiments on 14 public benchmarks demonstrate that our agent achieves state-of-the-art performance on diverse video understanding tasks, including 3 on grounded video question-answering, 6 on video temporal grounding, and 5 on general video question-answering, underscoring its effectiveness in advancing video agent and long-form temporal reasoning. 1. Introduction Recent advancements in Large Language Models (LLMs) have demonstrated remarkable effectiveness in reasoning tasks, such as Chain-of-Thought (CoT) [65, 85, 95], significantly improving both accuracy and interpretability in complex problem-solving scenarios [96]. Inspired by these achievements, researchers are now working to extend these reasoning capabilities to multi-modal domains [64, 69, 94] such as visual math understanding [73, 89, 106]. Among multi-modal inputs, videos present unique Equal contribution. (cid:66) Corresponding authors. Figure 1. An illustration of VideoMinds Chain-of-LoRA reasoning strategy applied to complex question for 50-min long video. The problem is decomposed by Planner and distributed to Grounder, Verifier, and Answerer to systematically localize, verify, and interpret the relevant video moments. Such role-based pipeline enables more human-like video reasoning compared with the pure textual CoT process. challenge due to their temporal dimension, which introduces complexities not found in static images or text. Effective video reasoning requires not only recognizing appearance but also understanding their dynamic interactions over time (i.e., temporal-grounded understanding) [6, 7, 88]. While recent visual CoT methods [73, 89, 106] excel at generating detailed thoughts for static visual inputs, they struggle with videos because they cannot explicitly localize 1 or revisit earlier parts of the sequence, as demonstrated in Fig. 1. Humans, by contrast, approach video understanding with ease: they break down complex problems, identify relevant moments, revisit them to confirm details, and synthesize their observations into coherent answers. This natural proficiency inspires the development of an AI assistant capable of replicating this process, adeptly managing multiple abilities to achieve advanced video reasoning. In this work, we introduce VideoMind, novel videolanguage agent with enhanced video temporal reasoning. (i) To meet the demands of diverse tasks, we first identify several key roles essential for understanding long videos: Grounder for precise moment retrieval, Verifier for validating information accuracy, an Answerer for generating query-aware responses, and Planner to flexibly coordinate these roles. Each role has been carefully designed to deliver strong performance, e.g., Grounder is equipped with timestamp-decoder to ensure strong temporal grounding ability. (ii) To enable efficient collaboration among these roles, we propose novel Chain-of-LoRA strategy, built upon single base MLLM (i.e., Qwen2-VL [77]). This approach embodies minimalist yet flexible design philosophy, facilitating seamless transitions and interactions between roles without the computational burden of multiple full models. Therefore, VideoMind achieves both efficiency and adaptability, offering practical and flexible solution for diverse video tasks. To evaluate the effectiveness of VideoMind, we conducted extensive experiments across 14 benchmarks, including Grounded Video Question-Answering (CGBench [6], ReXTime [8], NExT-GQA [88]), Video Temporal Grounding (Charades-STA [17], ActivityNet-Captions [27], QVHighlights [30], TACoS [60], Ego4D-NLQ [18], ActivityNet-RTL [23]), and General Video QuestionAnswering (Video-MME [13], MLVU [108], LVBench [79], MVBench [34], LongVideoBench [86]). VideoMind demonstrates its capability to progressively process complex reasoning tasks by jointly providing temporalgrounded evidence and delivering accurate answers. Notably, our model demonstrates significant performance improvements even at the smaller 2B size, achieving substantial increase in accuracy on long videos (27 minutes) in CG-Bench, even surpassing top-performing models such as GPT-4o [56]. Moreover, it demonstrates strong performance on sub-settings including video temporal grounding and general video question-answering. Ablation studies reveal the essential contributions of our design choices, particularly the Chain-of-LoRA mechanism, which enhances performance while achieving remarkable computational efficiency compared to traditional fine-tuning methods. Overall, our contributions are threefold: 1. We propose VideoMind, multi-modal agent framework that enhances video reasoning by emulating human-like processes, such as breaking down tasks, localizing and verifying moments, and synthesizing answers. This approach addresses the unique challenges of temporalgrounded reasoning in progressive strategy. 2. We introduce Chain-of-LoRA, minimalist strategy built on one vision-language model, allowing seamless collaboration among multiple roles. This method ensures VideoMind adapts flexibly to diverse tasks without the overhead of multiple models. 3. VideoMind achieves state-of-the-art results across three settings: Grounded VideoQA, Video Temporal Grounding, and General VideoQA. Notably, on the long video benchmarks [6, 79, 108], our 2B model outperforms GPT-4o on grounded QA tasks. Extensive ablations further confirm its effectiveness and efficiency. 2. Related Work 2.1. Temporal-grounded Video Understanding Significant advances in video understanding have propelled tasks such as video captioning [41, 107], video question answering [87, 99], and video-text retrieval [39, 53], which emphasize instance-level understanding, yet these models often lack visual-grounded correspondence and interpretability, particularly for long-form video streams. The task of Video Temporal Grounding [17, 27] tackles this by requiring precise temporal localization for diverse queries, though regression-based models [46, 47] excel at localization but fall short in providing textual interpretability. Recent benchmarks (such as Grounded Question Answering) [6, 88] intensify this challenge, demanding both reasoning for complex questions and fine-grained temporal correspondence. Previous baselines for these tasks typically rely on multi-task objectives or modular agents composed of distinct components [11, 82, 97, 99], often yielding suboptimal performance (e.g., LLM-based approaches for temporal grounding) or overly complex systems, which constrain their efficiency and flexibility. In this work, our proposed VideoMind introduces an agentic workflow built upon unified backbone, seamlessly integrating multiple functionalities while enhancing localization and interpretability, thus surpassing the limitations of prior methods. 2.2. Multi-modal Reasoning Large Multimodal Models [44], trained with supervised instruction-tuning (SFT), exhibit generalized capabilities such as free-form dialogue and question answering; however, they fall short in addressing complex challenges that often require the reasoning abilities of LLMs [85]. (i) One approach to overcome this is to develop agent-based interfaces [26, 99], which integrates textual outputs from multiple visual tools to enable language reasoning via LLMs. Advanced methods [16, 69, 94] leverage strategies like 2 Figure 2. The overall workflow of VideoMind. Given video and query, VideoMind adaptively activates different roles (Planner Grounder Verifier Answerer in this case) and perform step-by-step reasoning by calling individual modules. Codex or ReAct [96] to invoke visual APIs (e.g., detectors, captioners) through progressive execution and reasoning. (ii) Alternatively, pure text-based reasoning [2, 19] has been dominant paradigm in LLMs [85, 99], exemplified by training with long CoT processes using Reinforcement Learning, which provides detailed, step-by-step readable reasoning, with some works [89, 106] extending this to the visual domain for complex mathematical or scientific problems. Despite these advances, extending reasoning to videos across temporal dimensions remains an open challenge. Given the long-context nature of informative videos, we think that video-centric CoT should incorporate human-like re-watch strategy and self-validation of intermediate observations, leading us to introduce novel Chain-of-LoRA framework for video reasoning. 2.3. Inference-time Searching Inference-time searching has emerged as critical technique for tackling complex reasoning and planning challenges in domains like robotics [20, 80], games [67], and navigation [72], distinct from training-time strategies as it optimizes model behavior during inference rather than model parameters during training. The advent of OpenAIo1 has advanced these inference-time techniques within LLMs by integrating sampling strategies such as controlled decoding [5, 91], Best-of-N sampling [32, 36], and Monte Carlo Tree Search (MCTS) [74, 76, 81, 100], allowing LLMs to iteratively refine outputs and achieve superior performance without altering their underlying weights. Howthe potential of inference-time searching remains ever, largely untapped in video understanding, where temporal reasoning introduces unique challenges. In our framework, we explore how MCTS can be tailored for video temporal reasoning, observing that models are highly sensitive to the selection of temporal segments, often producing unreliable predictions when segment choices are suboptimal. To address this, we propose video moment-level MCTS approach where Grounder generates multiple segment candidates, followed by Verifier that evaluates and determines the correct correspondence, validating that this strategy significantly enhances temporal localization accuracy and robustness across diverse video contexts. 3. VideoMind Overview. Fig. 2 provides an overview of VideoMind. Our model derives from the widely adopted Qwen2-VL [77] architecture, which consists of an LLM backbone and ViTbased visual encoder that natively supports dynamic resolution inputs. Given video input and text query Q, the model adaptively activates different roles and performs step-by-step reasoning by calling individual modules: (i) Planner: Dynamically coordinates the following roles based on the query. (ii) Grounder: Identifies and localizes relevant video moments. (iii) Verifier: Evaluates the validity of the moments identified by Grounder, refining them through zoom-in process with boolean outputs. (iv) Answer: Generates the final response in natural language. This mechanism enables the models to revisit the videos several times (with varying temporal segments & spatial resolutions) to derive the final response. Chain-of-LoRA. To meet the diverse demands of different roles, we introduce novel Chain-of-LoRA strategy. The model dynamically activates role-specific LoRA adapters [21] during inference via self-calling, ensuring both efficiency and adaptability. Next, we describe how we optimize each design and detail the curation of their SFT data. 3.1. Planner In practice, an agent should be flexible enough to meet various demands and determine efficiently which function to call next. To this end, we introduce the Planner role, which dynamically coordinates all other roles for each query. The key idea is that the planner should decide the sequence of function calls based on the visual context. We formulate each function call as JSON-style object {\"type\":role, \"value\":argument}. Thus, sequence of roles can be succinctly represented as list of such objects. We define three plans illustrated in Fig. 3. Figure 3. The Planner coordinates other roles based on the query, providing three modes and rephrasing tailored for different needs. (i) Grounding & Answering: This plan requires the agent to generate both response and temporal moment. Such as What is the boy doing when the baby is crying? in grounded question-answering tasks [87], the agent must not only answer the question but also pinpoint the relevant moment. We enable this planning capability by repurposing the temporal questions from NExT-QA [87]. (ii) Grounding Only: Designed for tasks such as moment retrieval like When does the woman go downstairs?, this plan focuses solely on grounding, as the grounded timestamps are already the answer. We apply templates to sampled queries from QVHighlights [30] to train this setting. (iii) Answering Only: When the question is straightforward (e.g., Summarize this video), the model may not need to localize moments. Instead, it can watch the entire video and answer the question directly, thereby avoiding an extra grounding call. We utilize the causal and descriptive questions in NExT-QA [87] for training this plan. Query Rephrasing: Moreover, when the user query lacks sufficient detail for accurate localization, the planner is allowed to rephrase the question into more descriptive version. For instance, the original query What is the person sitting on the bed doing as the baby plays? might confuse the grounder as it contains multiple instances (person and baby) in query. It can be rephrased to the baby is playing for clear query. We train this rephrasing ability by using GPT-4o mini [56] to generate question-query pairs. 3.2. Grounder The purpose of the grounder is to localize relevant moments (i.e., start and end timestamps) based on text queries, thereby supporting the reasoning process by identifying visual cues. This requirement calls for the development of an LMM with robust temporal grounding capabilities. Timestamp Decoder. Rather than directly predicting textual timestamps through language modeling [63] or special tokens [22, 48], we develop timestamp decoder head built upon the LMM. We introduce special token <REG> to facilitate the timestamp decoding process. When this token is generated, the last-layer hidden states of <REG> and all the visual tokens will be sent into the decoder for timestamp prediction, obtaining an array (i.e., [tstart, tend]) representing the normalized start and end timestamps. As shown in Fig.4, the decoder accepts hidden states of the visual tokens hv R(T HW )DL and the <REG> token hr R1DL as inputs, where , H, , DL are the down-sampled number of frames, width, height, and hidden dimensions of the LLM, respectively. We apply 1D average pooling with kernel size and stride equal to to compress the visual tokens to one token per frame. = AvgPool(hv) RT DL (1) Then, Er to reduce the hidden dimension to D. and hr are projected by two linear layers Ev and ev = Ev(h v) RT D, er = Er(hr) R1D (2) The resulting ev and er serve as consolidated representations of the video frames and the query1, respectively. To effectively integrate their information, we add them with learnable modality embeddings, concatenate them along the sequence dimension, and encode them with transformer. [e v; r] = Transformer([ev + mv + ep; hr + mr]) (3) Here, mv R1D and mr R1D are randomly initialized learnable embeddings as modality indicators. Notably, mv is expanded to before being added to ev. ep is normalized sinusoidal positional encoding [75] for preserving temporal awareness. The output sequence is split into and r, indicating the contextualized frame and query embeddings, respectively. Temporal Feature Pyramid. To enhance adaptability to varying lengths of videos and moments, we map into four-level temporal feature pyramid with multiple temporal resolutions. This is achieved by applying varying number of Conv1D LayerNorm SiLU blocks for each pyramid level, where the Conv1D employs kernel size and stride of 2 (down-sampling the sequence by 1/2 along the temporal dimension). In practice, the four levels retain 1, 1/2, 1/4, and 1/8 of the original sequence length, rev R(T /n2)D where spectively. They can be denoted as pn 1We use the term query to denote the features of <REG> token, aligning with common notations in temporal grounding literature. Role #Samples Pretraining Datasets Planner 39K NeXT-QA-Plan (34K), QVHighlights-Plan (5K) Grounder 210K QVHighlights (5K), DiDeMo (33K), TACoS (9K), QuerYD (19K), HiRESTmr (8K), HiRESTstep (4K), CosMo-Cap (87K), InternVid-VTime (54K) Verifier 232K DiDeMo-Verify (165K), TACoS-Verify (43K) QVHighlights-Verify (24K) Table 1. Supervised fine-tuning datasets for VideoMind. The planning dataset is repurposed from NExT-QA [87] and QVHighlights [30]. Verify datasets are generated from the pre-trained Grounders predictions. mr and step denote the moment retrieval and step localization subsets of HiREST [98], respectively. pairs (denoted as {si}L i=0), then sample positive frame (falling within the ground truth boundary) and apply the following optimization objective: Figure 4. Detailed architecture of the timestamp decoder. This module accepts hidden states of both the frame tokens and the <REG> token, decoding them into the start and end timestamps. Lcon = λcon log exp(sp/τ ) exp(sp/τ ) + (cid:80) iΘ exp(si/τ ) (6) [1, 4] is the index of the pyramid level. To accelerate the prediction process, we concatenate the sequences from all pyramid levels along the temporal dimension into pv with length = + /2 + /4 + /8, such that the prediction can be made in parallel. Training Objectives. To model the continuous timestamps, we adopt two dense prediction heads [40, 47]: (i) classification head is adopted for frame-level foregroundbackground classification. This is instantiated by twolayer Conv1D module (kernel size=3, padding=1) followed by Sigmoid activation. The outputs are framelevel confidence scores {ˆci}L i=0 indicating whether each frame falls inside the desired moment. binary focal loss [42] is leveraged to optimize this process. Lcls = λclsα(1 ˆci)γ log(ˆci) (4) Here, Θ contains the frame indices with sp > si, and τ = 0.07 is temperature. The final loss for the timestamp decoder is the sum of these losses at all layers with λcls = 5.0, λreg = 1.0, and λcon = 0.05. We pre-train the grounder on large variety of temporal grounding datasets as shown in Tab. 1. During training, we optimize the timestamp decoder and introduce LoRA adapters on the LLM. 3.3. Verifier key moment is crucial for providing visual cues, yet it may be imprecise due to its sensitivity. Thus, further confirmation is necessary. We let the grounder generate top-N predictions (N = 5), which are then verified by the verifier to select the most reliable one, illustrated in Fig. 5. Here, α = 0.9 and γ = 2.0 are hyperparameters of the focal loss, and λcls is term balancing different losses. (ii) boundary regression head is utilized to predict the 1doffset with the temporal boundaries {[ˆbs i=0 for each frame. This is two-layer convolution block, with an output dimension of 2 and an exponential function as the final activation. The outputs corresponding to different pyramid levels are further scaled by different learnable scalar factors. This head is supervised by an L1 loss. , ˆbe ]}L Lreg = λreg(bs ˆbs + be ˆbe ) (5) In order to realize better alignment between ev and er, we incorporate an additional contrastive loss to encourage learning more discriminative representations. Specifically, we calculate the cosine similarities among all frame-query Figure 5. The grounder generates multiple candidate moments, which are then refined by applying zoom-in strategy and evaluated by Verifier to select the best moment. Recap by Zoom-in: For each candidate moment, we apply zoom-in strategy by expanding the boundaries by 50% on both sides, cropping, and enlarging the resolution. The resulting video segment, together with the original text query, is then sent to the verifier to assess whether Method Size long-acc. mIoU R@IoU A@IoU Method Size FT R@0.3 R@0.5 mIoU Acc Acc@IoU GPT-4o [56] GPT-4o-mini [56] Gemini-1.5-Pro [61] Gemini-1.5-Flash [61] Claude-3.5-Sonnet Video-LLaVA [37] VideoLLaMA [102] Videochat2 [34] Qwen-VL-Chat [4] ST-LLM [45] ShareGPT4Video [9] Chat-UniVi-v1.5 [25] ViLA [38] MiniCPM-v2.6 [71] LongVA [104] LLaVA-OV [31] Video-CCAM [12] Kangaroo [43] VITA [14] Qwen2-VL [78] InternVL2 [10] VideoMind (Ours) VideoMind (Ours) 7B 7B 7B 7B 7B 16B 13B 8B 8B 7B 7B 14B 8B 87B 72B 78B 2B 7B 45.2 33.4 37.2 32.3 40.5 16.2 18.4 19.3 21.6 23.8 26.7 25.9 28.7 30.1 28.7 31.1 29.7 30.2 33.3 41.3 42.2 31.0 38.4 5.62 3.75 3.95 3.67 3.99 1.13 1.21 1.28 0.89 2.23 1.85 2.07 1.56 2.35 2.94 1.63 2.63 2.56 3.06 3.58 3. 5.94 7.10 8.30 5.18 5.81 5.44 5.67 1.96 1.87 1.98 1.19 2.86 2.65 2.53 2.89 2.61 3.86 1.78 3.48 2.81 3.53 5.32 5.05 8.50 9.93 4.38 2.21 2.53 2.45 2.79 0.59 0.84 0.94 0.42 1.13 1.01 1.21 1.35 1.04 1.78 1.08 1.83 1.94 2.06 3.31 2. 4.02 4.67 R@IoU rec.@IoU, A@IoU acc.@IoU Table 2. Grounded VideoQA on CG-Bench [6] Despite its smaller size, VideoMind surpasses GPT-4o and open-source baselines on this challenging long video benchmark (avg. dur: 27 min). the queried event occurs within the segment. To enhance boundary awareness, we introduce two special tokens, <SEG START> and <SEG END>, to explicitly mark the beginning and end of the moment. These tokens are inserted among the visual tokens at the corresponding frames, effectively guiding the model in recognizing boundaries. Boolean Judgement: The verifiers responses are designed to be binary either Yes or No. To train the verifier, we sample predictions from the grounder on its training datasets and assign binary labels based on an IoU threshold of 0.5. The model is then fine-tuned via SFT to predict these labels. During inference, for each candidate moment, we employ teacher forcing to obtain the likelihoods of the <Yes> and <No> tokens, denoted as Ly and Ln, respectively. The confidence score for moment is defined as Sigmoid(Ly Ln). Finally, we re-rank the moments based on their confidence scores to yield the best one. 3.4. Answerer VTimeLLM [22] TimeChat [62] LITA [23] VTimeLLM [22] TimeChat [62] 7.61 7B 28.84 17.41 20.14 36.16 7B 14.42 11.65 40.04 13B 29.49 16.29 21.49 34.44 7B 43.69 26.13 29.92 57.58 7B 40.13 21.42 26.29 49.46 VideoMind (Ours) 2B VideoMind (Ours) 7B 34.31 22.69 24.83 69.06 38.22 25.52 27.61 74. 17.13 10.92 17.26 20.20 Table 3. Grounded VideoQA on ReXTime [8]. FT indicates whether fine-tuned on the downstream training set. VideoMind demonstrates strong generalization; its zero-shot scores outperforms all zero-shot baseline and surpasses fine-tuned variants. swerer) are built on top of the same backbone LMM and augmented with additional LoRA adapters and lightweight timestamp decoder (for Grounder). Different LoRA adapters are switched for each role, allowing us to maximize role-specific capabilities while minimizing modifications to the model architecture. 4. Experiments We conduct extensive experiments across various benchmarks to evaluate our VideoMind. Specifically, we study the following research questions. Q1. Whether VideoMind is flexible and effective on different video temporal reasoning tasks compared to baselines with task-specific designs? Q2. What effects does each individual role contribute? Q3. Compared with training single agent on multiple tasks, what advantages does Chain-of-LoRA offer? Experiments on more datasets and further ablation studies can be found in the supplementary. 4.1. Benchmarks and Settings We extensively design experiments across 14 diverse benchmarks. The tasks include (i) Grounded Video QuestionAnswering, (ii) Video Temporal Grounding, and (iii) General Video Question-Answering. We present the results on CG-Bench [6], ReXTime [8], NExT-GQA [88], Charades-STA [17], ActivityNet-Captions [27], VideoMME [13], MLVU [108], and LVBench [79] here. More experiments and details are in the supplementary. 4.2. Q1: Comparison with State-of-the-Arts The answerer is responsible for answering the given question based on either the cropped video segment (when the grounder is used) or the entire video (if the planner opts for direct answer). Since the objective of this role is strictly aligned with existing LMMs (i.e., Qwen2-VL [77] in our case), we employ the pre-trained model directly without any fine-tuning or architectural modifications. All our modules (Planner, Grounder, Verifier, and AnGrounded Video Question-Answering. In Tab. 2, we report results on CG-Bench [6], challenging benchmark with an average duration of 27 minutes. In grounding metrics, our lightweight 2B model outperforms all compared models (including InternVL2-78B [10] and most closedsource models such as Claude-3.5-Sonnet), with the exception of GPT-4o [56], while our 7B model surpasses it and achieves competitive overall performance. 6 Method Size IoU IoP R@0.3 R@0.5 mIoU R@0.3 R@0.5 mIoP FrozenBiLM NG+ [93] 890M 13.5 4.3 VIOLETv2 [15] SeViLA [97] 29.2 LangRepo [26] VideoStreaming [58] LLoVi [99] HawkEye [83] 37.0 41.2 VideoChat-TPO [92] 4B 87B 8.3B 1.8T 7B 7B VideoMind (Ours) VideoMind (Ours) 2B 7B 45.2 50.2 6.1 1.3 13.8 12.2 13.3 15.3 19.5 23.4 23.2 25.8 9.6 3.1 21.7 18.5 19.3 20.0 25.7 27.7 28.6 31.4 28.5 25.1 34.7 47. 51.3 56.0 23.7 23.3 22.9 28.7 31.0 36.9 32.8 32.6 35.3 24.2 23.6 29.5 31.3 32.2 37.3 35.6 36.4 39.0 Acc@ GQA 17.5 12.8 16.6 17.1 17.8 24.3 25.5 25.2 28.2 Method Size Video-MME MLVU LVBench All Long M-Avg Overall Gemini-1.5-Pro [70] GPT-4o [56] Video-LLaVA [37] TimeChat [63] MovieChat [68] PLLaVA [90] VideoChat-TPO [92] LongVA [103] 75.0 71.9 41.1 7B 34.3 7B 7B 38.2 34B 40.0 48.8 7B 52.6 7B VideoMind (Ours) VideoMind (Ours) 2B 7B 53.6 58. 67.4 65.3 37.8 32.1 33.4 34.7 41.0 46.2 45.4 49.2 54.5 29.3 30.9 25.8 53.6 54.7 56.3 58.7 64.4 33.1 30.8 22.3 22.5 26.1 35.4 40. Table 4. Grounded VideoQA on NExT-GQA [88]. VideoMind beats both agent-based solutions [99] and end-to-end methods [92] trained with largescale data. Notably, our 2B model is comparable with 7B counterparts. Table 5. VideoQA on Video-MME [13] (15min), MLVU [108] (15min), and LVBench [79] (1.1h). VideoMind shows superior performance on long videos. Method Size FT R@0.3 R@0.5 R@0.7 mIoU Method Size FT R@0.3 R@0.5 R@0.7 mIoU Moment-DETR [30] UniVTG [40] R2-Tuning [47] VTimeLLM [22] TimeChat [63] Momentor [57] HawkEye [83] ChatVTG [59] VideoChat-TPO [92] E.T. Chat [48] VideoMind (Ours) VideoMind (Ours) 13B 7B 7B 7B 7B 7B 4B 2B 7B 65.8 70.8 70.9 55.3 51.5 42.6 50.6 52.7 58.3 65.7 67.6 73.5 52.1 58.1 59.8 34.3 32.2 26.6 31.4 33.0 40.2 45. 51.1 59.1 30.6 35.6 37.0 14.7 13.4 11.6 14.5 15.9 18.4 20.0 26.0 31.2 45.5 50.1 50.9 34.6 28.5 33.7 34.9 38.1 42. 45.2 50.2 2D-TAN [105] MMN [84] VDI [49] VideoChat [33] Video-LLaMA [102] Video-ChatGPT [51] Valley [50] ChatVTG [59] Momentor [57] E.T. Chat [48] VideoMind (Ours) VideoMind (Ours) 7B 7B 7B 7B 7B 7B 4B 2B 7B 60.4 64.5 8.8 6.9 26.4 30.6 40.7 42.9 24.1 44.0 48.4 43.4 48.2 48. 3.7 2.1 13.6 13.7 22.5 23.0 12.8 26.5 30.3 25.0 29.4 28.8 1.5 0.8 6.1 8.1 9.4 12.4 6.1 12.6 15.7 42.5 46.6 7.2 6.5 18.9 21.9 27.2 29.3 18.9 30.1 33.3 Table 6. Zero-shot Video Temporal Grounding on CharadesSTA [17]. VideoMind significantly surpasses counterparts. Table 7. Zero-shot Video Temporal Grounding on ActivityNetCaptions [27]. VideoMind outperforms LLM-based methods. In Tab. 3, we show the results on ReXTime [8]. Despite the challenge posed by the temporal and causal event relationships, our model successfully identifies the correct event and zooms in on the relevant moment. Notably, our zero-shot model outperforms all zero-shot baselines by significant margin and yield comparable performance to several fine-tuned variants in grounding, while also achieving higher accuracy. This demonstrates the strong generalization capability of our method. We further present results on NExT-GQA [88] in Tab. 4. Compared to text-rich, agent-based solutions such as LLoVi [99] and LangRepo [26] which leverage additional tools and chain-of-thought, and SeViLA [97] self-chained video agent with similar design, our 2B model matches the performance of state-of-the-art 7B models across both agent-based and end-to-end approaches. Moreover, our 7B model significantly outperforms all other models. Video Temporal Grounding. Since the performance of Grounder and Verifier is essential for VideoMind, we evaluate these modules on temporal grounding datasets. In Tab. 6 and Tab. 7, we validate the zero-shot grounding capabilities of VideoMind. Benefiting from (i) the timestamp decoder design of Grounder, and (ii) verifier that refines the results by focusing on critical segments, our model achieves significant zero-shot performance surpassing all LLM-based temporal grounding methods and yielding competitive results compared to fine-tuned temporal grounding experts. General Video Question-Answering. We are also interested in whether our temporal-augmented design can imIn Tab. 5, we evaluate prove general video QA tasks. our model on three widely used benchmarks to determine if the Chain-of-LoRA design generalizes to common settings. Notably, Video-MME (Long) [13], MLVU [108], and LVBench [79] features long videos, where grounding and verifying are critical. Our designs effectively help the model localize cue segments before answering the question. 4.3. Ablation Studies Q2: Effect of Individual Roles. To study the contribution of each individual role, we conduct ablation studies (Tab. 8). Figure 6. Visualization of the VideoMind workflow. The Planner first determines the need for function calls and generates several candidate moments using the Grounder. It then applies the Verifier to select the most relevant video segment (highlighted in yellow). After zooming in, the segment is passed to the Answerer. By chaining the Grounder, Verifier, and Answerer roles, VideoMind accurately localizes the critical moment and selects the correct answer, thereby avoiding confusion from an incorrect segment (red box). VideoMind Roles ReXTime Charades-STA Ans. Gnd. Ver. Pla. G% mIoU Acc R@0.5 R@0.7 mIoU 0% 100% 24.5 100% 24.8 100% 24.7 40% 26. 68.0 68.8 69.1 69.2 70.0 47.2 51.1 21.7 26.0 42.0 45.2 Ans. Answerer, Gnd. Grounder, Ver. Verifier, Pla. Planner Table 8. Key ablations to study the effects of individual roles. The top half evaluates the full system on Grounded VideoQA, while the bottom half details the impact of the Grounder and Verifier on Video Temporal Grounding. G% indicates the percentage of samples processed with the Grounder module. Our observations are as follows: (i) Grounder: By identifying visual cues, the grounder can slightly improve QA acc., indicating that the grounder is especially effective on long videos. (ii) Verifier: Selecting the best candidate with the verifier improves grounded moments, yielding consistent gain of 3.2 mIoU on the pure grounding metrics for Charades-STA. (iii) Planner: Coordinating roles via the Planner even when performing grounding on only 40% samples (with the remaining 60% are processed by watching the whole video) boosts the accuracy from 69.2 to 70.0. This highlights the dynamic benefits of Planner under different visual and textual context. Q3: Effect of the Chain-of-LoRA strategy. Tab. 9 investigates the impact of integrating different modules. First, naive CoT does not improve the base model, highlighting the need for visual-centric, test-time search strategy. Second, while multi-task co-training enhances the baseline overall, it fails to optimize individual capabilities (e.g., grounding performance remains poor at around 3% mIoU on Charades-STA). Although the all-distributed approach achieves the best performance, it requires multiple copies Method Mem. NExT-GQA Charades-STA Video-MME mIoU Acc R@0.5 mIoU All Long Qwen2-VL-2B + CoT [85] 4.1G 4.1G 4.2G 28.0 + All-in-One + All-Distributed 16.6G 28.6 + Chain-of-LoRA 4.2G 28.6 69.6 69.7 70.5 71.4 71. 47.8 51.1 51.1 42.1 45.2 45.2 49.7 49.6 52.8 53.6 53. 43.1 43.2 44.6 45.4 45.4 Table 9. Key ablations to study the test-time strategy with different role integrations, including the base model, version using textual CoT, and three implementations that integrate multiple roles. Mem. indicates GPU memory usage. Notably, Chain-ofLoRA achieves the best performance with minimal memory cost. (4) of the model weights. maintains top performance in the most efficient manner. In contrast, Chain-of-LoRA 4.4. Qualitative Analysis In Fig. 6, we illustrate how VideoMind applies all roles to progressively derive the correct answer while avoiding potential mistakes or confusion. 5. Conclusion and Limitations We introduced VideoMind, novel video-language agent designed for temporal grounded video reasoning. Our approach employs an agentic workflow consisting of Planner, Grounder, Verifier, and Answerer, along with Chain-of-LoRA strategy to efficiently switch among these roles. Extensive experiments in grounded video questionanswering, video temporal grounding, and general video question-answering demonstrate the effectiveness and significance of VideoMind, particularly in long-form video reasoning tasks by providing precise, evidence-based answers. We acknowledge that our model requires huge optimization of individual designs and preparing training data. We hope this work inspires future advancements in multimodal video agents and reasoning."
        },
        {
            "title": "Appendix",
            "content": "In this document, we provide more descriptions of the model, implementation details, and benchmarks to complement the main paper. Additional experiments and prompt templates are also incorporated. A. Model Details A.1. Inference Pipeline The formulation of VideoMinds inference pipeline is illustrated in Algorithm 1. Given video and question, Planner dynamically calls different roles on demand to analyze the problem and generate the answer. Algorithm 1 VideoMinds Chain-of-LoRA Pipeline 1: Input: video with query 2: Output: an answer with temporal moment = [ts, te] 3: Plan Planner(V, Q) 4: if Grounder then 5: 6: 7: e]}i Grounder(V, Q) {[ti s, ti for all do Vi ZoomIn(cid:0)V, [ti Scorei Verifier e](cid:1) s, ti (cid:17) (cid:16) Vi, 8: end for arg max si(Scorei) 9: 10: 11: end if 12: if Answerer then 13: 14: end if 15: return (A, ) Answerer (cid:16) Vi, (cid:17) A.2. Implementation Details We leverage the 2B and 7B versions of Qwen2-VL [77] as our base model, and apply LoRA adapters with rank 64 and alpha 64 to Planner, Grounder, and Verifier. The hidden size of the timestamp decoder in Grounder is 256. The maximum number of tokens per frame and maximum number of frames for Planner, Grounder, Verifier, and Answerer are set as [64, 100], [64, 150], [64, 64], and [256, 32], respectively. The video frames are sampled as 2 FPS except Grounder, which uses 1 FPS since more frames are used. We train different roles separately on different datasets, and load them together by setting different names for LoRA adapters, such that the model can efficiently switch roles via actively setting different LoRAs. During training, we set the global batch size as 32, and utilize AdamW optimizer with learning rate 2e-5, 1e-4, and 5e-5 for Planner, Grounder, and Verifier, respectively. All the roles were trained for 1 epoch on their specific datasets, with linear warmup in the first 3% steps. During inference, we apply NMS with IoU threshold 0.75 to reduce duplicated moments from Grounder. 9 Dataset Duration Domain Main Metrics Grounded Video Question-Answering (Grounding + QA) CG-Bench [6] ReXTime [8] NExT-GQA [88] Diverse 1624.4s rec.@IoU, acc.@IoU 141.1s Mixed* mIoU, Acc (IoU 0.5) 39.5s Reasoning mIoP, Acc@GQA Video Temporal Grounding (Grounding only) Charades-STA [66] ANet-Captions [27] QVHighlights [30] TACoS [60] Ego4D-NLQ [18] ANet-RTL [23] 30.1s Indoor 111.4s Activity 150s Mixed* R@{0.3 0.7}, mIoU R@{0.3 0.7}, mIoU R@{0.5, 0.7}, mAP 358.2s Cooking R@{0.3 0.7}, mIoU 379.0s Egocentric R@{0.3 0.7}, mIoU 111.4s Reasoning P@0.5, mIoU General Video Question-Answering (QA only) Video-MME [13] MLVU [108] LVBench [79] MVBench [34] LongVideoBench [86] 1017.9s 930s 4101s 15s 473s Diverse Diverse Diverse Diverse Diverse * Vlog, News, and Activity Acc (w/o subs) Acc Acc Acc Acc Table 10. Statistics of Evaluation Datasets. The datasets encompass three representative tasks Grounded VideoQA, Video Temporal Grounding, and General VideoQA, with video durations ranging from less than 1 minute to more than 1 hour. Method Size Non-LLM-based Specialists R1 mAP @0.5 @0.7 @0.5 @0.75 Avg. XML [29] XML+ [30] Moment-DETR [30] UMT [46] MomentDiff [35] QD-DETR [54] UniVTG [40] R2-Tuning [47] LLM-based Models 41.83 46.69 59.78 60.83 58.21 62.40 65.43 68.03 30.35 33.46 40.33 43.26 41.48 44.98 50.06 49.35 44.63 47.89 60.51 57.33 54.57 62.52 64.06 69.04 31.73 34.67 35.36 39.12 37.21 39.88 45.02 47.56 32.14 34.90 36.14 38.08 36.84 39.86 43.63 46.17 VideoMind (Ours) 2B 74.38 55.77 73.38 54.59 51.38 Table 11. Fine-tuned Video Temporal Grounding on QVHighlights [30] test split. VideoMind achieves SOTA performance. B. Experimental Details B.1. Benchmarks The statistics of evaluation benchmarks we used are listed in Tab. 10. We mainly evaluate VideoMind on grounded VideoQA and video temporal grounding scenarios, and also study its generalizability on general long VideoQA benchmarks. The information about major benchmarks are introduced below. is designed for CG-Bench [6] long video grounded question-answering, featuring diverse domain and various evaluation metrics. It includes 1.2K manually curated VideoMind (Ours) VideoMind (Ours) 2B 7B 38.6 49.5 26.9 36.2 15.5 21.4 27.4 34.4 #Pyramid Levels Method Size FT R@0.3 R@0.5 R@0.7 mIoU Non-LLM-based Specialists 2D-TAN [105] VSLNet [101] Moment-DETR [30] UniVTG [40] R2-Tuning [47] LLM-based Models 40.0 35.5 38.0 51.4 49.7 28.0 23.5 24.7 35.0 38.7 12.9 13.1 12.0 17.4 25.1 27.2 25.0 25.5 33.6 35.9 Table 12. Video Temporal Grounding on TACoS [60]. VideoMinds pretraining dataset already contains TACoS, and we did not apply any further fine-tuning on it. The performance is comparable to task-specific experts trained for multiple epochs. Method Size FT R@0.3 R@0.5 R@0.7 mIoU Non-LLM-based Specialists 2D-TAN [105] VSLNet [101] Moment-DETR [30] UniVTG [40] R2-Tuning [47] UniVTG [40] LLM-based Models VideoMind (Ours) VideoMind (Ours) 2B 7B 4.3 4.5 4.3 7.3 7.2 6.5 5.9 7.2 1.8 2.4 1.8 4.0 4.5 3.5 2.9 3. 0.6 1.0 0.7 1.3 2.1 1.2 1.2 1.7 3.4 3.5 3.5 4.9 4.9 4.6 4.7 5.4 Table 13. Zero-shot Video Temporal Grounding on Ego4DNLQ [18]. VideoMind is comparable with fine-tuned experts. Method LITA [23] LITA [23] VideoMind (Ours) VideoMind (Ours) Size FT P@0.5 mIoU 7B 13B 2B 7B 21.2 25.9 20.1 28.0 24.1 28.6 22.7 31.3 Table 14. Zero-shot Reasoning Temporal Localization on ActivityNet-RTL [23]. Our zero-shot method even performs better than fine-tuned baselines. videos, ranging from 10 to 80 minutes, with total of 12K QA pairs. The dataset is categorized into perception, reasoning, and hallucination question types, and introduces clue-based evaluation methods like white box and black box assessments to ensure models provide answers based on accurate video reasoning. ReXTime [8] tests models on complex temporal reasoning, using an automated pipeline for QA pair generation, significantly reducing manual effort. It includes 921 validation and 2,1K test samples, each manually curated for accuracy, and highlights 14.3% accuracy gap between SoTA models and human performance. This benchmark is crucial for Method Charades-STA R@0.3 R@0. R@0.7 mIoU VideoMind-Grounder w/o ADL 63.55 62.52 47.23 46.90 21.69 20.88 42.02 40. Table 15. Effect of the auxiliary decoding loss (ADL). Charades-STA R@0.3 R@0.5 R@0.7 mIoU 60.55 61.51 62.62 63. 44.57 46.90 47.02 47.23 15.82 19.36 20.08 21.69 38.13 40.43 41.27 42.02 1 2 3 4 Table 16. Effect of the temporal feature pyramid. Verifier Type Direct Expand Textual Special Token Charades-STA R@0.3 R@0.5 R@0.7 mIoU 60.42 65.10 65.24 67. 45.28 48.70 49.33 51.05 19.32 23.15 23.89 25.99 39.84 43.57 44.01 45.22 Table 17. Effect of different Verifier styles. evaluating models on cause-and-effect relationships across video segments, driving advancements in video understanding research. NExT-GQA [88] aims to challenge models to reason about causal and temporal actions, supporting both multi-choice and open-ended tasks. It is an extension of NExT-QA [87] and comprises 10.5K manually videos QA pairs with temporal segments. The samples in this benchmark are categorized into causal and temporal questions, while the descriptive questions from NExT-QA are discarded. Charades-STA [17] contains 10K in-door videos, averaging 30.1 seconds each, with 16K temporal annotations spanning daily activity, alongside free-text descriptions. These rich annotations make Charades-STA particularly suitable for evaluating temporal grounding models under indoor environments. ActivityNet-Captions [27] is large-scale benchmark with 20K untrimmed YouTube videos totaling 849 hours, covering diverse activities from personal care to sports. This dataset contains high-quality dense video captioning annotations (3.65 temporally localized sentences per video), which we use as queries for video temporal grounding. Each query has an average length of 13.48 words. B.2. More Comparisons with State-of-the-Arts More Video Temporal Grounding benchmarks. We additionally compare VideoMind with representative meth10 Model Size AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg. GPT-4V [1] 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.0 59.0 11.0 43.5 Video-ChatGPT [51] Video-LLaMA [102] VideoChat [33] Video-LLaVA [37] TimeChat [63] PLLaVA [90] ShareGPT4Video [9] ST-LLM [45] VideoGPT+ [52] VideoChat2 [34] 7B 23.5 26.0 62.0 22.5 26.5 54.0 28.0 40.0 23.0 20.0 31.0 30.5 25.5 39.5 48.5 29.0 33.0 29.5 26.0 35.5 32.7 7B 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5 7B 46.0 42.5 56.5 39.0 53.5 53.0 48.0 41.0 29.0 31.5 82.5 45.0 26.0 53.0 41.5 33.5 41.5 27.5 38.5 31.5 43.0 7B 40.5 36.0 61.0 32.5 53.0 53.5 41.5 29.0 19.5 26.5 66.5 34.0 20.0 43.5 42.0 36.5 36.0 29.0 35.0 35.0 38.5 7B 58.0 49.0 55.5 41.0 61.0 56.0 61.0 36.0 23.5 26.0 82.0 39.5 42.0 52.0 45.0 42.0 53.5 30.5 48.0 31.0 46.6 7B 49.5 39.5 79.5 40.0 54.5 82.5 54.5 32.5 50.5 41.5 84.5 35.5 62.5 75.0 51.0 25.5 46.5 28.5 39.0 51.5 51.2 7B 66.0 53.5 84.0 44.0 58.5 80.5 73.5 38.5 42.5 31.0 86.5 36.5 56.5 78.5 43.0 44.5 46.5 34.5 41.5 58.5 54.9 3.8B 69.0 60.0 83.0 48.5 66.5 85.5 75.5 36.0 44.0 34.0 89.5 39.5 71.0 90.5 45.0 53.0 50.0 29.5 44.0 60.0 58.7 7B 75.5 58.0 83.5 50.5 60.5 87.5 74.5 45.0 47.5 44.0 82.5 37.0 64.5 87.5 51.0 66.5 47.0 35.0 37.0 72.5 60.4 VideoMind (Ours) VideoMind (Ours) 2B 77.0 78.0 77.0 46.5 70.5 87.0 71.5 33.0 48.0 39.5 91.0 53.0 78.0 89.0 43.5 53.5 61.5 37.5 49.5 53.0 61.9 7B 74.0 71.5 81.0 50.0 77.0 93.0 75.0 38.0 48.5 46.0 91.0 39.0 80.0 94.5 49.5 55.5 70.0 40.5 57.0 61.0 64.6 Table 18. General Video Question-Answering on MVBench [34]. VideoMind can also achieve comparable performance on short (average duration is around 15s) video understanding, demonstrating strong generalization abilities. Method Size Acc GPT-4o [56] GPT-4 Turbo [55] Gemini-1.5-Pro [70] Gemini-1.5-Flash [70] Idefics2 [28] Phi-3-Vision [3] Mantis-Idefics2 [24] Mantis-BakLLaVA [24] VideoMind (Ours) VideoMind (Ours) 8B 4B 8B 7B 2B 7B 66.7 59.0 64.0 61.6 49.7 49.6 47.0 43.7 48.8 56.3 Acc @ Duration Groups C 71.4 65.2 67.4 68.3 59.8 59.3 56.6 53.4 59.3 67.7 76.7 68.2 75.1 76.2 65.7 61.6 55.8 57.6 59.3 67.4 69.1 62.4 65.3 62.6 47.8 46.8 45.6 40. 49.3 56.8 60.9 50.5 58.6 54.0 42.7 44.7 42.2 38.7 41.7 48.6 can successfully generalize its strong zero-shot grounding capability from referring to reasoning scenarios. More General VideoQA benchmarks. Tab. 18 presents the evaluation results of VideoMind on MVBench [34] which is VideoQA benchmark with very short videos. Although this is out of our main scope, our model can still achieve good performance on such short video scenarios. For long VideoQA, we also provide evaluations on LongVideoBench [86] in Tab. 19, which further verifies the effectiveness of VideoMind on long videos. A: (8, 15] B: (15, 60] C: (180, 600] D: (900, 3600] B.3. Further Ablation Studies Table 19. Video Question-Answering on LongVideoBench [86] val split. VideoMind is superior on all duration groups. ods on the challenging QVHighlights [30], TACoS [60] and Ego4D-NLQ [18] datasets in Tab. 11, Tab. 12 and Tab. 13, respectively. To our best knowledge, VideoMind is the first LLM-based grounding model that supports multi-moment outputs, thereby being able to evaluated on QVHighlights. Compared with task-specific experts, our 2B VideoMind significantly performs better under all metrics. Our model also performs better than UniVTG [40] on TACoS but slightly worse on Ego4D-NLQ because UniVTG was originally pre-trained on egocentric videos. Even without egocentric pre-training, VideoMind can still produce comparable results on Ego4D-NLQ. Results on Reasoning Temporal Localization. We also evaluate the generalization ability of Grounder and Verifier on the more challenging reasoning temporal localization [23] task, which is similar to video temporal grounding but the queries are not directly describing the moment. The models are required to infer the actual event using its world knowledge. The results in Tab. 14 show that VideoMind Effect of auxiliary decoding loss. Tab. 15 compares the difference in performance whether the auxiliary decoding loss is added or not. When removing this auxiliary loss, the performance of VideoMind-Grounder degrades by about 1.5 mIoU, suggesting the effectiveness of this loss term. Effect of the temporal feature pyramid. Tab. 16 studies the effectiveness of the temporal feature pyramid. Our baseline model directly makes predictions on the last-layer transformer outputs. When adding more pyramid levels, the performance of video temporal grounding consistently improves under all metrics on the Charades-STA [17] dataset under zero-shot setting, suggesting the effectiveness of the improving robustness of the model when facing moments with different lengths. Design choices of Verifier. In Table 17, we examine various design choices for the Verifier. The term Direct refers to the method where the grounded moment is directly sent into the model without any expansion. Expand denotes expanding the temporal boundaries by 50%, while Textual involves adding supplementary textual information to indicate the length of the target event. Special Token represents our final approach, utilizing special tokens to denote the grounded start and end timestamps. The comparison re11 sults demonstrate that expanding the temporal boundaries effectively broadens the Verifiers perceptual range, and the use of special tokens enhances the models understanding of the precise moment boundaries. C. Prompt Templates We present the prompts used in this work, including the input prompts for each role of VideoMind and the prompt for GPT-4o mini [56] for data annotation. Prompt for Planner: You are acting as the planner now. Given question about the video, your task is to analyze the question and identify the best way to answer this question. You have access to the following tools: Grounder: Accepts text query and localizes video segment according to the query. Verifier: tool supporting grounder by verifying the reliability of its outputs. Answerer: Answer given question directly based on the whole video or cropped video segment. the relevant Your response must be list in JSON format. valid plan for reasoning could be grounder, verifier, answer, grounder, verifier, or answerer, depending on the given question. Please see an example of the format below. [{type: grounder, value: text query}, {type: verifier}, {type: answerer}] Note that only the grounder can accept an argument called value, which is the text query used for grounding. Now give you the question: {question}. Please think carefully and respond with your plan in JSON directly. You are given video with {duration} seconds long. Subtitles: {subtitles} {question} Options: (A) {option 1} (B) {option 2} (C) {option 3} (D) {option 4} Please only give the best option. Prompt for query rephrasing data generation: You are an expert in rewriting questions into queries. will give you question that requires to be answered based on specific moment in video. Your task is to analyze the question and rewrite it into declarative sentence, which could be used as text query to search for the relevant video moment. The query should be concise, describing the key event or key scene that the question asks for. Here are some examples: Question: How does the male cyclist react when he sees the steep path? Query: The male cyclist sees the steep path. Question: What did the girl do at the end of the video? Query: The end of the video. Question: What did the lady do as she was cycling off? Query: The lady is cycling off. Question: What is the person with red shirt doing on the yacht? Query: The person with red shirt stays on the yacht. Now give you the question: fully and respond with the query directly. {question}. Please think carePrompt for Grounder:"
        },
        {
            "title": "References",
            "content": "You are acting as the grounder now. Given video and text query, your goal is to temporally localize the video moment described by the query. If the query is directly describing moment, simply localize it according to its content. Otherwise, if the moment is described as before/after pivotal event, you need to determine the actual event it refers to. The localized moment should only cover the target event. Now give you the query: {query}. Please think carefully and provide your response. Prompt for Verifier: You are acting as the verifier now. You will be presented text query describing moment that potentialy happens in the given video. Your task is to identify whether the video segment between <SEG START> and <SEG END> perfectly covers the moment. If the described moment can be seen in the video, please focus on verifying whether the moment starts at <SEG START> and ends at <SEG END>. Respond with Yes if you think the moment boundaries are correct, otherwise No. If the described moment cannot be seen in the video, respond with No directly. Now give you the query: {query}. Please think carefully and respond with Yes or No directly. Prompt for Answerer: When subtitles are considered, we use only the first 100 lines as context. [1] Chatgpt can now see, hear, and speak. https : / / openai . com / blog / chatgpt - can - now - see - hear-and-speak, 2023. [2] Learning to reason with llms. https://openai.com/ index / learning - to - reason - with - llms/, 2024. 3 [3] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 11 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. 2023. 6 [5] Souradip Chakraborty, Soumya Suvra Ghosal, Ming Yin, Dinesh Manocha, Mengdi Wang, Amrit Singh Bedi, and Furong Huang. Transfer star: Principled decoding for llm alignment. arXiv preprint arXiv:2405.20495, 2024. 3 [6] Guo Chen, Yicheng Liu, Yifei Huang, Yuping He, Baoqi 12 Pei, Jilan Xu, Yali Wang, Tong Lu, and Limin Wang. Cgbench: Clue-grounded question answering benchmark for long video understanding, 2024. 1, 2, 6, [7] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online In Provideo large language model for streaming video. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1840718418, 2024. 1 [8] Jr-Jen Chen, Yu-Chien Liao, Hsi-Che Lin, Yu-Chu Yu, YenChun Chen, and Yu-Chiang Frank Wang. Rextime: benchmark suite for reasoning-across-time in videos, 2024. 2, 6, 7, 9, 10 [9] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37:1947219495, 2024. 6, 11 [10] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with opensource suites. Science China Information Sciences, 67(12): 220101, 2024. 6 [11] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. arXiv preprint arXiv:2403.11481, 2024. 2 [12] Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-language understanding with causal cross-attention masks for short and long videos. arXiv preprint arXiv:2408.14023, 2024. 6 [13] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 6, 7, 9 [14] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024. [15] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked viIn Proceedings of the IEEE/CVF Confersual modeling. ence on Computer Vision and Pattern Recognition, pages 2289822909, 2023. 7 [16] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. Assistgpt: general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint arXiv:2306.08640, 2023. 2 [17] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, pages 52675275, 2017. 2, 6, 7, 10, 11 [18] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 2, 9, 10, 11 [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3 [20] Nicklas Hansen, Xiaolong Wang, and Hao Su. Temporal difference learning for model predictive control, 2022. [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 4 [22] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. arXiv preprint arXiv:2311.18445, 2(3):9, 2023. 4, 6, 7 [23] De-An Huang, Shijia Liao, Subhashree Radhakrishnan, Hongxu Yin, Pavlo Molchanov, Zhiding Yu, and Jan Kautz. Lita: Language instructed temporal-localization assistant. arXiv preprint arXiv:2403.19046, 2024. 2, 6, 9, 10, 11 [24] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multiimage instruction tuning. arXiv preprint arXiv:2405.01483, 2024. 11 [25] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. 6 [26] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael Ryoo. Language repository for long arXiv preprint arXiv:2403.14622, video understanding. 2024. 2, 7 [27] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 2, 6, 7, 9, 10 [28] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2024. [29] Jie Lei, Licheng Yu, Tamara Berg, and Mohit Bansal. Tvr: large-scale dataset for video-subtitle moment retrieval. In ECCV, pages 447463, 2020. 9 [30] Jie Lei, Tamara Berg, and Mohit Bansal. Qvhighlights: Detecting moments and highlights in videos via natural language queries. In NeurIPS, 2021. 2, 4, 5, 7, 9, 10, 11 [31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6 13 [32] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities, 2024. 3 [33] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 7, 11 [34] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video underarXiv preprint arXiv:2311.17005, standing benchmark. 2023. 2, 6, 9, [35] Pandeng Li, Chen-Wei Xie, Hongtao Xie, Liming Zhao, Lei Zhang, Yun Zheng, Deli Zhao, and Yongdong Zhang. Momentdiff: Generative video moment retrieval from random to real. Advances in neural information processing systems, 36:6594865966, 2023. 9 [36] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. 3 [37] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 6, 7, 11 [38] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for viIn Proceedings of the IEEE/CVF sual language models. conference on computer vision and pattern recognition, pages 2668926699, 2024. 6 [39] Kevin Qinghong Lin, Jinpeng Wang, Mattia Soldan, Michael Wray, Rui Yan, Eric Xu, Difei Gao, Rong-Cheng Tu, Wenzhe Zhao, Weijie Kong, et al. Egocentric videolanguage pretraining. Advances in Neural Information Processing Systems, 35:75757586, 2022. [40] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified videothe language temporal grounding. IEEE/CVF International Conference on Computer Vision, pages 27942804, 2023. 5, 7, 9, 10,"
        },
        {
            "title": "In Proceedings of",
            "content": "[41] Kevin Qinghong Lin, Pengchuan Zhang, Difei Gao, Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, and Mike Zheng Shou. Learning video context as interleaved multimodal sequences. In European Conference on Computer Vision, pages 375396. Springer, 2024. 2 [42] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In ICCV, pages 29802988, 2017. 5 [43] Fangcheng Liu, Yehui Tang, Zhenhua Liu, Yunsheng Ni, Duyu Tang, Kai Han, and Yunhe Wang. Kangaroo: Lossless self-speculative decoding for accelerating llms via double early exiting. Advances in Neural Information Processing Systems, 37:1194611965, 2024. 6 [44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae arXiv preprint instruction tuning. Lee. Visual arXiv:2304.08485, 2023. 2 [45] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2024. 6, [46] Ye Liu, Siyuan Li, Yang Wu, Chang Wen Chen, Ying Shan, and Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval and highlight detection. In CVPR, pages 30423051, 2022. 2, 9 [47] Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, and Chang Wen Chen. r2-tuning: Efficient image-to-video transfer learning for video temporal grounding. arXiv preprint arXiv:2404.00801, 2024. 2, 5, 7, 9, 10 [48] Ye Liu, Zongyang Ma, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Et bench: Towards open-ended event-level video-language understanding. arXiv preprint arXiv:2409.18111, 2024. 4, 7 [49] Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, and Yang Liu. Towards generalisable video moment retrieval: Visual-dynamic injection to image-text pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2304523055, 2023. 7 [50] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023. 7 [51] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 7, [52] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Khan. Videogpt+: Integrating image and video encoders for enhanced video understanding. arXiv preprint arXiv:2406.09418, 2024. 11 [53] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, and Josef Sivic. Ivan Laptev, Howto100m: Learning text-video embedding by watchIn Proceedings ing hundred million narrated video clips. of the IEEE/CVF international conference on computer vision, pages 26302640, 2019. 2 [54] WonJun Moon, Sangeek Hyun, SangUk Park, Dongchan Park, and Jae-Pil Heo. Query-dependent video representation for moment retrieval and highlight detection. In CVPR, pages 2302323033, 2023. 9 [55] OpenAI. Gpt-4 technical report, 2023. 11 [56] OpenAI. Hello gpt-4o, 2024. 2, 4, 6, 7, 11, 12 [57] Long Qian, Juncheng Li, Yu Wu, Yaobo Ye, Hao Fei, TatSeng Chua, Yueting Zhuang, and Siliang Tang. Momentor: Advancing video large language model with fine-grained arXiv preprint arXiv:2402.11435, temporal reasoning. 2024. 7 [58] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models. Advances in Neural Information Processing Systems, 37:119336 119360, 2024. 7 [59] Mengxue Qu, Xiaodong Chen, Wu Liu, Alicia Li, and Yao Zhao. Chatvtg: Video temporal grounding via chat with video dialogue large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18471856, 2024. 7 [60] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt Schiele, and Manfred Pinkal. Grounding action descriptions in videos. Transactions of the Association for Computational Linguistics, 1:2536, 2013. 2, 9, 10, 11 [61] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6 [62] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. arXiv preprint arXiv:2312.02051, 2023. 6 [63] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1431314323, 2024. 4, 7, 11 [64] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36: 3815438180, 2023. 1 [65] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. URL https://arxiv. org/abs/2303.11366, 2023. 1 [66] Gunnar Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: largescale dataset of paired third and first person videos. arXiv preprint arXiv:1804.09626, 2018. [67] David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. 3 [68] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 7 [69] Dıdac Surıs, Sachit Menon, and Carl Vondrick. Vipergpt: reasoning. inference via python execution for Visual arXiv:2303.08128, 2023. 1, 2 [70] Google Gemini Team. Gemini: family of highly capable multimodal models, 2023. 7, [71] MiniCPM-V Team. Minicpm-llama3-v 2.5: gpt-4v level multimodal llm on your phone, 2024. 6 Fenghua Zhu, and Long Chen. Motion planning for autonomous driving: The state of the art and future perspectives. IEEE Transactions on Intelligent Vehicles, 8(6): 36923711, 2023. 3 [73] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. 1 [74] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. Toward self-improvement of llms via imagination, searching, and criticizing. arXiv preprint arXiv:2404.12253, 2024. 3 [75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 59986008, 2017. 4 [76] Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Dian Yu, Haitao Mi, Jinsong Su, and Dong Yu. Litesearch: Efficacious tree search for llm. arXiv preprint arXiv:2407.00320, 2024. [77] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 2, 3, 6, 9 [78] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6 [79] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu, Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv preprint arXiv:2406.08035, 2024. 2, 6, 7, 9 [80] Xiyao Wang, Ruijie Zheng, Yanchao Sun, Ruonan Jia, Wichayaporn Wongkamjan, Huazhe Xu, and Furong Huang. Coplanner: Plan to roll out conservatively but to explore optimistically for model-based rl. arXiv preprint arXiv:2310.07220, 2023. 3 [81] Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, and Dong Yu. Towards selfimprovement of llms via mcts: Leveraging stepwise knowledge with curriculum preference learning. arXiv preprint arXiv:2410.06508, 2024. 3 [82] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. arXiv preprint arXiv:2403.10517, 2024. [83] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024. 7 [72] Siyu Teng, Xuemin Hu, Peng Deng, Bai Li, Yuchen Li, Yunfeng Ai, Dongsheng Yang, Lingxi Li, Zhe Xuanyuan, [84] Zhenzhi Wang, Limin Wang, Tao Wu, Tianhao Li, and Gangshan Wu. Negative sample matters: renaissance of 15 [97] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. In NeurIPS, 2023. 2, 7 [98] Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. Hierarchical In Proceedvideo-moment retrieval and step-captioning. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2305623065, 2023. [99] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023. 2, 3, 7 [100] Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Rest-mcts*: Llm self-training arXiv preprint via process reward guided tree search. arXiv:2406.03816, 2024. 3 [101] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. Span-based localizing network for natural language video localization. arXiv preprint arXiv:2004.13931, 2020. 10 [102] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 6, 7, 11 [103] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 7 [104] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [105] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. Learning 2d temporal adjacent networks for moment localization with natural language. In AAAI, pages 12870 12877, 2020. 7, 10 [106] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. 1, 3 [107] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large lanIn Proceedings of the IEEE/CVF Conferguage models. ence on Computer Vision and Pattern Recognition, pages 65866597, 2023. 2 [108] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 6, 7, 9 metric learning for temporal grounding. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2613 2623, 2022. 7 [85] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1, 2, 3, [86] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. 2, 9, 11 [87] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa:next phase of question-answering to explaining temporal actions, 2021. 2, 4, 5, 10 [88] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13204 13214, 2024. 1, 2, 6, 7, 9, 10 [89] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. 1, 3 [90] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 7, 11 [91] Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, and Sumitra Ganesh. Genarm: Reward guided generation with autoregressive reward model for test-time alignment. arXiv preprint arXiv:2410.08193, 2024. 3 [92] Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zilei Wang, Yali Wang, Yu Qiao, et al. Task preference optimization: Improving multimodal large language models with vision task alignment. arXiv preprint arXiv:2412.19326, 2024. [93] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. Advances in Neural Information Processing Systems, 35:124141, 2022. 7 [94] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Mm-react: Liu, Michael Zeng, and Lijuan Wang. Prompting chatgpt for multimodal reasoning and action. arXiv:2303.11381, 2023. 1, 2 [95] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. 1 [96] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv. org/abs/2210.03629, 2023. 1,"
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore",
        "The Hong Kong Polytechnic University"
    ]
}