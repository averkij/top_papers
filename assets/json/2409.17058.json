{
    "paper_title": "Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors",
    "authors": [
        "Aiping Zhang",
        "Zongsheng Yue",
        "Renjing Pei",
        "Wenqi Ren",
        "Xiaochun Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based image super-resolution (SR) methods have achieved remarkable success by leveraging large pre-trained text-to-image diffusion models as priors. However, these methods still face two challenges: the requirement for dozens of sampling steps to achieve satisfactory results, which limits efficiency in real scenarios, and the neglect of degradation models, which are critical auxiliary information in solving the SR problem. In this work, we introduced a novel one-step SR model, which significantly addresses the efficiency issue of diffusion-based SR methods. Unlike existing fine-tuning strategies, we designed a degradation-guided Low-Rank Adaptation (LoRA) module specifically for SR, which corrects the model parameters based on the pre-estimated degradation information from low-resolution images. This module not only facilitates a powerful data-dependent or degradation-dependent SR model but also preserves the generative prior of the pre-trained diffusion model as much as possible. Furthermore, we tailor a novel training pipeline by introducing an online negative sample generation strategy. Combined with the classifier-free guidance strategy during inference, it largely improves the perceptual quality of the super-resolution results. Extensive experiments have demonstrated the superior efficiency and effectiveness of the proposed model compared to recent state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "Noname manuscript No. (will be inserted by the editor) Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors Aiping Zhang* Zongsheng Yue* Renjing Pei Wenqi Ren Xiaochun Cao 4 2 0 2 5 2 ] . [ 1 8 5 0 7 1 . 9 0 4 2 : r Received: date / Accepted: date Abstract Diffusion-based image super-resolution (SR) methods have achieved remarkable success by leveraging large pre-trained text-to-image diffusion models as priors. However, these methods still face two challenges: the requirement for dozens of sampling steps to achieve satisfactory results, which limits efficiency in real scenarios, and the neglect of degradation models, which are critical auxiliary information in solving the SR problem. In this work, we introduced novel onestep SR model, which significantly addresses the efficiency issue of diffusion-based SR methods. Unlike existing fine-tuning strategies, we designed degradationguided Low-Rank Adaptation (LoRA) module specifically for SR, which corrects the model parameters based on the pre-estimated degradation information from lowresolution images. This module not only facilitates powerful data-dependent or degradation-dependent SR Aiping Zhang School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China E-mail: zhangaip7@mail2.sysu.edu.cn Zongsheng Yue S-Lab, Nanyang Technological University, Singapore E-mail: zsyzam@gmail.com Renjing Pei Huawei Noahs Ark Lab E-mail: peirenjing@huawei.com Wenqi Ren School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China E-mail: renwq3@mail.sysu.edu.cn Xiaochun Cao School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China E-mail: caoxiaochun@mail.sysu.edu.cn * Equal Contribution. model but also preserves the generative prior of the pre-trained diffusion model as much as possible. Furthermore, we tailor novel training pipeline by introducing an online negative sample generation strategy. Combined with the classifier-free guidance strategy during inference, it largely improves the perceptual quality of the super-resolution results. Extensive experiments have demonstrated the superior efficiency and effectiveness of the proposed model compared to recent state-of-the-art methods. Code is available at https: //github.com/ArcticHare105/S3Diff Keywords Super-resolution, Diffusion prior, Degradation awareness, One step"
        },
        {
            "title": "1 Introduction",
            "content": "Image super-resolution (SR) is long-standing and challenging problem in computer vision, aiming to restore high-resolution (HR) image from its low-resolution (LR) counterpart. The LR images usually suffer from various complex degradations, such as blurring, downsampling, noise corruption, etc. Even worse, the degradation process is often unknown in real-world scenarios. This inherent ambiguity in the degradation model further heightens the complexity of the SR problem, driving substantial research efforts over the past years. Diffusion models have emerged as formidable class of generative models, particularly excelling in image generation tasks. Building on the foundational work (Sohl-Dickstein et al., 2015), these models have significantly advanced, resulting in highly effective frameworks (Ho et al., 2020; Song et al., 2021). The field of SR has particularly benefited from the diffusion models due to their ability to capture fine-grained details 2 Aiping Zhang* et al. imans and Ho, 2022; Sauer et al., 2023; Yin et al., 2024; Sauer et al., 2024), efficient samplers (Song et al., 2022a; Lu et al., 2022a,b) and straight forward path (Lipman et al., 2023; Liu et al., 2022) effectively reduce inference steps and achieve promising generation quality. However, directly applying these approaches for efficient SR could be problematic. Unlike text-to-image generation, super-resolution relies on an LR input image to create an HR image. The LR image provides more detailed content for the target image than textual descriptions. Moreover, understanding the process of degradation is essential in generating high-resolution images, and when used effectively, it can positively influence the SR process. Considering these observations, this work follows the second research line, focusing on efficiently leveraging LR inputs and degradation guidance to better harness the T2I prior for effective and efficient superresolution. Specifically, we propose Single-Step Superresolution Diffusion network (S3Diff ) for addressing the problem of real-world SR. Benefiting from advances in accelerating diffusion models, we take advantage of the T2I prior of SD-Turbo (Sauer et al., 2023) due to its efficient few-step inference and powerful generative capabilities. Inspired by recent methods like DifFace (Yue and Loy, 2024) and Diff-SR (Li et al., 2023a), which suggest that LR images provide robust and effective starting point for the diffusion reverse process, we use the LR image with slight or no noise perturbation as input to maximize the retention of semantic content. To achieve high-quality super-resolution, we integrate the T2I prior with Low-Rank Adaptation (LoRA) (Hu et al., 2022), transforming it into one-step SR model that maintains its generative capabilities. In comparison to previous fine-tuning approaches (Wang et al., 2024a; Lin et al., 2023; Yu et al., 2024), the application of LoRA offers more lightweight and rapidly adaptable solution for the SR task. In addition to the straightforward fine-tuning strategy based on the naive LoRA, we advance our approach specifically for SR. Considering the pivotal role of the degradation model in addressing the SR problem (Zhang et al., 2018a; Mou et al., 2022; Yue et al., 2024b), we design degradation-guided LoRA module that effectively leverages degraded information from LR images. This module draws on the core principles of LoRA, which involves modification of the targeted parameter Rdn via residual decomposition, namely Wnew = +AB, where = [a1, , ar] Rdr and Rrn are low-rank matrices. From the perspective of mathematics, the update for is implemented in low-dimensional space spanned by {a1, , ar}, determined by A, with controlling the update directions. Fig. 1: Comparison of performance and complexity among DM-based SR methods on the DIV2K-Val dataset (Agustsson and Timofte, 2017). Metrics like LPIPS, DISTS, NIQE, FID, and inference time, where smaller scores indicate better image quality, are inverted. All metrics are normalized for better visualization. S3Diff attains top-tier performance in both image quality and complexity with just single forward pass. and generate high-fidelity images. Current diffusionbased SR approaches can be broadly classified into two categories. The first category involves the specific redesign of diffusion models for SR, including SR3 (Saharia et al., 2022b), SRDiff (Li et al., 2022), ResShift (Yue et al., 2023, 2024a), and others (Luo et al., 2023; Delbracio and Milanfar, 2023; Xia et al., 2023; Wang et al., 2024b). Motivated by the huge success of large text-to-image (T2I) models, the second category harnesses large pre-trained T2I model, like Stable Diffusion (SD) (Rombach et al., 2022), as prior to facilitate the SR task. Following the pioneering work of StableSR (Wang et al., 2024a), several relevant studies (Lin et al., 2023; Yang et al., 2023; Yu et al., 2024; Wu et al., 2023) have recently emerged. These methods, which are built upon T2I models trained with hundreds to thousands of diffusion steps, typically require dozens of sampling steps even after acceleration, limiting their inference efficiency. While some methods (Yue et al., 2023; Wang et al., 2024b) in the first category can significantly reduce sampling steps by designing shorter diffusion trajectory, they necessitate training the model from scratch and cannot capitalize on the extensive knowledge embedded in large pre-trained T2I models. Recently, the acceleration of diffusion models has attracted much attention. Using distillation strategies (SalDegradation-Guided One-Step Image Super-Resolution with Diffusion Priors 3 LR images or provided by the user, enhancing the SR process with user-interactive interface. We develop an innovative training pipeline with online negative sample generation, aligning low-quality concepts with negative prompts to enable classifierfree guidance, significantly improving visual effects in generated HR images."
        },
        {
            "title": "2 Related Work",
            "content": "Image super-resolution has garnered increasing attention, evolving from traditional Maximum Posteriori (MAP)-based methods to advanced deep learning techniques. MAP-based approaches emphasize the manual design of image priors to guide restoration, focusing on non-local similarity (Dong et al., 2012; Zhang et al., 2012), low-rankness (Dong et al., 2013; Gu et al., 2015), and sparsity (Yang et al., 2010; Kim and Kwon, 2010), among others (Sun et al., 2008; Huang et al., 2015). These methods rely on mathematical models to impose constraints that help reconstruct high-resolution images from low-resolution inputs. Deep learning-based methods, on the other hand, leverage large datasets to train neural networks that can directly map lowresolution images to high-resolution counterparts. Since the introduction of SRCNN (Dong et al., 2014), various approaches have emerged, focusing on aspects such as network architectures (Shi et al., 2016; Zhang et al., 2017, 2018a; Liang et al., 2021; Chen et al., 2023b), which improve feature extraction and representation, and loss functions (Johnson et al., 2016; Zhang et al., 2018b; Wang et al., 2018b; Yue et al., 2024b), which enhance visual quality by prioritizing perceptual fidelity. Additionally, degradation models (Wang et al., 2021; Zhang et al., 2021) and image prior integration (Pan et al., 2021; Chen et al., 2022) further refine the SR process by combining traditional strengths with deep learning. Recently, diffusion-based SR techniques have gained prominence, categorized into model-driven and prior-driven methods: model-driven approaches leverage specific architectures and training processes incorporating diffusion mechanisms, while prior-driven methods utilize statistical properties of natural images to guide diffusion, producing realistic and detailed highresolution outputs. Given that our work falls within the diffusion-based methods, we provide brief overview of these approaches."
        },
        {
            "title": "2.1 Model-driven methods",
            "content": "Model-driven approaches focus on tailoring diffusion model specifically for super-resolution. direct method Fig. 2: Qualitative comparisons on one typical realworld example of the proposed method and the most recent state-of-the-arts, including SinSR (Wang et al., 2024b) and OSEDiff (Wu et al., 2024). (Zoom in for details) To better adapt LoRA to SR, we pre-estimate the degraded information using the degradation estimation model from (Mou et al., 2022) and then use this information to modulate B, thereby refining the update directions of . This degradation-guided LoRA module is appealing in two aspects. On the one hand, it facilitates data-dependent model wherein parameters are adaptively modified based on the specific degraded information from the LR image. On the other hand, during the testing phase, the degraded information can either be predicted by the degradation estimated model or manually set by users, enabling an interactive interface between the SR model and the user. To further enhance perceptual quality, we develop novel training pipeline by introducing an online negative sample generation strategy. This approach makes full use of the LR image to align poor-quality concepts with negative prompts, enabling classifier-free guidance (Ho and Salimans, 2022) during inference to further improve visual effects. As shown in Figure 1 and Figure 2, S3Diff can produce high-quality HR images with enhanced fidelity and perceptual quality in single forward pass, while significantly reducing inference time and requiring only few trainable parameters. In summary, our contributions are as follows: We propose Single-Step Super-resolution Diffusion model (S3Diff ), which leverages the T2I prior from SD-Turbo (Sauer et al., 2023), achieving highquality super-resolution with significantly reduced inference time and minimal trainable parameters. We introduce novel degradation-guided LoRA module that adaptively modifies model parameters based on specific degraded information, extracted from the Aiping Zhang* et al. involves modifying the inverse sampling process to include low-resolution images as conditions, followed by retraining the model from scratch. This is exemplified in works, such as SR3 (Saharia et al., 2022b) and SRDiff (Li et al., 2022). SR3 (Saharia et al., 2022b) introduces conditional diffusion model that incorporates LR images to guide the generation of HR outputs. It emphasizes training with large dataset to enhance the models ability to produce finer details in the HR images. Building on this, SRDiff (Saharia et al., 2022b) mainly focuses on refining the noise scheduling process, allowing for more precise control over the denoising steps. IDM (Gao et al., 2023) introduces an implicit neural representation into the diffusion model framework to tackle continuous SR tasks. This method allows the model to adaptively learn representations that capture the nuances of various resolutions, improving its flexibility and performance on diverse datasets. Additionally, ResShift (Yue et al., 2023, 2024a), which builds up shorter Markov chain between the LR image and its corresponding HR image within the discrete framework of DDPM (Ho et al., 2020), significantly reducing the sampling steps during inference. Concurrently, IR-SDE (Luo et al., 2023) and InDI (Delbracio and Milanfar, 2023) apply similar concepts within the framework of Stochastic Differential Equations (SDEs). These methods harness continuous noise processes to enhance the stability and efficiency of the diffusion models, making them robust against variations in input resolution. Furthermore, SinSR (Wang et al., 2024b) proposes one-step diffusion model by distilling the ResShift method. This innovation simplifies the diffusion process to single step, greatly improving computational efficiency while retaining high fidelity in the generated HR images. Despite their innovations, these methods are generally trained on small-scale SR datasets. This limitation means they cannot fully exploit the rich prior knowledge found in large pre-trained T2I models."
        },
        {
            "title": "2.2 Prior-driven methods",
            "content": "Motivated by the powerful image generative capabilities of large T2I models, researchers have explored the potential of leveraging these pre-trained models as priors to facilitate the SR task. Rombach et al. (Rombach et al., 2022) propose Latent Diffusion Models (LDM), which are further used to create an upscaler for superresolution. By operating in compressed latent space, this method reduces computational overhead while maintaining high-quality results. Wang et al. (Wang et al., 2024a) pioneered this approach with StableSR, which generates HR images by modulating the features of the T2I model through observed LR images. It adjusts the internal representations of the model to enhance details and sharpness. Different from StableSR, DiffBIR (Lin et al., 2023) offers an innovative strategy by incorporating conditional information from LR images through residual addition, inspired by DifFace (Yue and Loy, 2024) and ControlNet (Zhang et al., 2023), enhancing the models ability to handle complex textures and fine details. Moreover, SUPIR (Yu et al., 2024) fine-tunes large SR model derived from SDXL on an extensive dataset. It improves performance by adapting the model to diverse high-resolution data, allowing it to generalize better across various image types and text conditions. Other explorations in this domain include PASD (Yang et al., 2023), CoSeR (Sun et al., 2023), and SeeSR (Wu et al., 2023), also make some significant exploration along this research line. Unlike this fine-tuning strategy, some works suggest correcting the generated intermediate results of pre-trained diffusion model using degradation models, such as CCDF (Chung et al., 2022c), DDRM (Kawar et al., 2022), DDNM (Wang et al., 2023b), DPS (Chung et al., 2022a), and so on (Chung et al., 2022b; Song et al., 2022b). Recently, OSEDiff (Wu et al., 2024) proposes to adapt pre-trained T2I models into one-step SR model by employing distribution matching distillation (Yin et al., 2024) to maintain the fidelity of generated HR images. Despite the promising results, these methods rely on well-defined degradation models, limiting their applicability in blind SR tasks under real-world conditions."
        },
        {
            "title": "3 Method",
            "content": "Given large-scale pre-trained T2I diffusion model capable of generating realistic images, our goal is to develop an efficient yet powerful SR model based on the T2I model. To achieve this, we need to address two key questions: i) under the iterative sampling framework of diffusion models, is it possible to derive one-step SR model that extremely meets the efficiency requirement? ii) how can we effectively harness the generative prior encapsulated in the given T2I model to facilitate the SR task while minimizing the training cost? In this paper, we propose novel Single-Step Super-resolution Diffusion network (S3Diff ), which is presented in Figure 3, to answer these questions in detail. In this section, we first provide our solution to adapt pre-trained T2I model to one-step SR (Section 3.1), wherein parameters are adaptively modified with our degradation-guided LoRA (Section 3.2.1). We also introduce new training strategy called online negative prompting (Section 3.2.2), which helps the model avoid generating low-quality images. The adopted losses are finally introduced in Section 3.3. Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors 5 Fig. 3: Overview of S3Diff . We enhance pre-trained diffusion model for one-step SR by injecting LoRA layers into the VAE encoder and UNet. Additionally, we employ pre-trained Degradation Estimation Network to assess image degradation that is used to guide the LoRAs with the introduced block ID embeddings. We tailor new training pipeline that includes an online negative prompting, reusing generated LR images with negative text prompts. The network is trained with combination of reconstruction loss and GAN loss. els, particularly the SDXL variant (Podell et al., 2023), likely due to its relatively limited number of parameters. Both Imagen and IF adopt hierarchical generative framework, which poses challenges for adaptation to SR. Conversely, the direct generation mechanism employed by SD models is more friendly to SR. We thus focus on SD models. In this paper, we consider SD-Turbo (Sauer et al., 2023), distilled variant of the SD model designed to enhance sampling efficiency. SD-Turbo performs distillation on four specific steps from the original 1000step diffusion process. Thus, SD-Turbo indeed acts as robust denoiser at distinct noise levels corresponding to these four steps. Figure 4 demonstrates this by showing the one-step prediction results of SD-Turbo using appropriate text prompts (e.g., frog in the figure) on the four distilled steps. This finding encourages us to adapt SD-Turbo into single-step SR model. However, in practical scenarios, we typically do not have access to the image description. Some methods (Wu et al., 2023; Yu et al., 2024; Yang et al., 2023; Wu et al., 2024) use pre-trained models to generate image tags or descriptions but face issues with inaccuracy. Notably, the LR image provides more detailed content for the target image than textual descriptions. Figure 4 shows that even without textual guidance, inputs with reduced noise produce outputs with consistent content. Furthermore, recent methods like DifFace (Yue and Loy, 2024) and Diff-SR (Li et al., 2023a) demonstrate that LR images are effective starting points for the diffusion reverse process. Building on these findings, we directly use the LR image with little or no noise as input to maximize the retention of semantic content. Fig. 4: We demonstrate images generated from various steps using the pre-trained SD-Turbo, both with and without text prompts."
        },
        {
            "title": "3.1 One-step Solution",
            "content": "Our objective is to facilitate one-step super-resolution method using pre-trained diffusion models. This allows us to shift our focus toward leveraging high-level generative knowledge and diffusion priors embedded within the pre-trained models, rather than on the iterative denoising process. By doing so, we can enhance the efficiency and efficacy of the pre-trained diffusion model to produce high-quality images from LR inputs. We start with the selection of the pre-trained T2I base model from several prominent candidates, including PixArt (Chen et al., 2023a), Imagen (Saharia et al., 2022a), IF (inference framework, 2024), and SD models (Rombach et al., 2022). The performance of the current PixArt model does not match that of the SD mod6 Aiping Zhang* et al. Remark. Notably, SinSR (Wang et al., 2024b) is also diffusion-based SR model enabling one-step prediction. However, it is distilled from ResShift (Yue et al., 2023, 2024a), relatively small SR model trained from scratch, and therefore cannot harness the rich prior knowledge embedded in large pre-trained T2I models. This work takes step forward, aiming to develop one-step SR model based on the powerful generative prior of large T2I models. In addition, different from OSEDiff Wu et al. (2024), we aim to fully harness the rich content information in LR images, instead of relying on off-the-shelf methods to extract text prompts."
        },
        {
            "title": "3.2 Adaption Solution",
            "content": "In this section, we concentrate on the fine-tuning strategies for the proposed one-step SR model, with the goal of enhancing its awareness of degradation and the LR input. In general, we choose to fine-tune the VAE encoder and diffusion UNet. Fine-tuning the VAE encoder serves as pre-cleaning function, aiming to better align with the original training process of T2I that was implemented on clean images. For the VAE decoder, recent approaches (Wang et al., 2024a; Parmar et al., 2024) attempt to fine-tune it alongside the additional skip connections to maintain content consistency. However, our empirical findings suggest that freezing the VAE decoder ensures higher perceptual quality without compromising consistency, as demonstrated in Sec. 4.3. Besides, unlike previous works (Wang et al., 2024a; Sun et al., 2023; Lin et al., 2023; Yu et al., 2024) based on SFT (Wang et al., 2018a) or ControlNet (Zhang et al., 2023), we opt to inject LoRA (Hu et al., 2022) layers into the pre-trained T2I model for efficient finetuning. This LoRA-based strategy not only preserves the prior knowledge embedded in the T2I model as much as possible but also significantly reduces the learnable parameters owning to the low-rank assumption. Considering the challenge of SR, which mainly arises from the complexity and unknown nature of the degradation model, incorporating auxiliary degraded information has proven beneficial (Zhang et al., 2018a; Mou et al., 2022; Yue et al., 2024b). Building on this insight, we design degradation-guided LoRA module to incorporate the estimated degraded information, as detailed in the following presentation."
        },
        {
            "title": "3.2.1 Degradation-guided LoRA",
            "content": "Without loss of generality, we consider the fine-tuning for specific network parameter Rdn in the base model. LoRA introduces residual low-rank decomposition, namely Wnew = + AB, where Rdr and Rrn are low-rank matrices, where and n. By viewing as [a1, , ar] and as [bT ]T , we can rewrite the decomposition as: 1 , , bT Wnew = + AB = + (cid:88) i=1 aibT . (1) This formulation indicates that LoRA updates in low-dimensional subspace spanned by {a1, , ar}, with controlling the update directions. To enhance LoRAs adaptability for SR, we propose making it aware of degradation. This means LoRA is tailored specifically for each degraded image. Specifically, we first incorporate degradation estimation model proposed by Mou et al. (Mou et al., 2022). This model unifies the image degradation into 2-dimensional vector = {dn, db} [0, 1]2, which quantifies the extent of noise and blur. The estimated degradation vector is transformed by Gaussian Fourier embedding layer (Tancik et al., 2020) to enhance the models ability to learn complex functions over continuous inputs, which can be formulated as: ), cos(2πdWT de = concat[sin(2πdWT where We Rm is random-initialized matrix and de R22m, de is then fed into an MLP to generate correction matrix = [c1, c2, , cr] Rrr, which refines the update direction as follows: )], (2) Wnew = + (CB) = + (cid:88) i=1 ai(cT B). (3) This refinement ensures that the update direction of is degradation-aware. Such degradation-guided LoRA module offers dual benefits. Firstly, it allows for dynamic, data-dependent adjustment of model parameters in response to the specific degradation from the LR image. Secondly, it provides flexible way to handle the degraded information during testing, which can be automatically predicted by the degradation estimation model or manually configured by the user. However, using shared for all LoRA layers limits the flexibility of the fine-tuned model. Conversely, using separate for each LoRA layer necessitates distinct MLP for each layer, significantly increasing the overall number of learnable parameters. To facilitate flexible parameterization of degradation-guided LoRA within pre-trained diffusion model consisting of blocks, we introduce set of block ID embeddings = {li}L i=1. The matrix Ci for the i-th block can be generated as: Ci = MLP(FC(d), li). (4) Here, we initially project the degradation estimation de into higher dimension to prevent it from being overwhelmed by block ID embeddings. We then feed the Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors 7 concatenation of {FC(de), li} to shared MLP. This approach generates unique degradation-guided LoRA for each block in the pre-trained diffusion model. Moreover, the embeddings for block ID are learned through back-propagation, facilitating end-to-end training of the entire SR smodel."
        },
        {
            "title": "3.2.2 Online Negative Prompting",
            "content": "Adapting the T2I model to SR presents another challenge beyond input differences: the absence of text prompt. As shown in Fig. 4, the pre-trained diffusion models ability to generate high-quality outputs depends significantly on appropriate text prompts. However, we only use LR images as input, leading to gap when adapting the T2I model to SR. Some methods (Wu et al., 2023, 2024; Yu et al., 2024) try to address this issue by introducing degradation-robust prompt extractor to extract tags or employing Multi-modal Large Language Model (MLLM) to obtain dense captions from degraded images. However, text descriptions extracted from degraded images can be inaccurate, often leading diffusion models to produce inconsistent restoration results. Moreover, relying on MLLM introduces significant extra overhead over the SR model. Actually, the input LR image already provides rich information about the images semantic content. Therefore, beyond guiding the SR model on the images elements, we should also focus on prompting it regarding what defines good and poor perceptual quality. Notably, the recent method, SUPIR (Yu et al., 2024), incorporates negative samples during training. However, it depends on SDXL (Podell et al., 2023) to generate low-quality images offline, leading to gap regarding the concept of poor quality between the real-world degraded images and those generated artificially, and further introducing additional training overhead. To help the SR model more efficiently understand the concept of poor quality, we propose an online negative prompting strategy. Specifically, during training, in each mini-batch, we randomly replace the target HR image with its synthesized LR image, using sampling probability pn, to constitute the negative target. Therefore, the target images IT GT for supervising the model are constructed by combining the mixed LR and HR images, as shown in Figure 3. Negative targets are associated with negative prompts like oil painting, cartoon, blur, dirty, messy, low quality, deformation, low resolution, oversmooth, as used in (Yu et al., 2024). Meanwhile, we use general positive prompt high-resolution image full of vivid details, showcasing rich blend of colors and clear textures for positive targets. In the training phase, we forward the positive/negative text prompts into the UNet, whose outputs are supervised by the corresponding positive/negative targets, facilitating the SR model being awareness of image quality. During inference, we use Classifier-Free Guidance (CFG) (Ho and Salimans, 2022) to ensure the model avoids producing low-quality images. Specifically, the SR model makes two predictions using positive prompts tpos and negative prompts tneg, then fuses these results for the final output as follows: zpos = ϵθ(Eθ(ILR), tpos), zneg = ϵθ(Eθ(ILR), tneg), zout = zneg + λcfg(zpos zneg), (5) where Eθ and ϵθ stand for the VAE encoder and the UNet denoiser, λcfg is the guidance scale. Different from SUPIR (Yu et al., 2024), which generates negative samples offline using SDXL, we reuse synthesized LR images during training, adding no extra overhead to the training pipeline."
        },
        {
            "title": "3.3 Loss Functions",
            "content": "To train the model, we adopt reconstruction loss LRec, including L2 loss L2 and LPIPS loss LLPIPS. Inspired by ADD (Sauer et al., 2023), which leverages an adversarial distillation strategy, we also incorporate GAN loss to minimize the distribution gap between generated images and real HR images. Since our target is to achieve one-step SR model instead of that with four steps in (Sauer et al., 2023), we can simplify ADD by removing the teacher model which supervises the intermediate diffusion results. Thus, the full learning objective can be expressed as follows: min Gθ λL2L2 + λLPIPSLLPIPS + λGANLGAN, (6) where G() represents the generator, namely our model, λL2, λLPIPS and λGAN are balancing weights. The GAN loss is defined as: LGAN = EIgtPGT [log Dϕ(Igt)] + EIlqPLR [log(1 Dϕ(Gθ(Ilq)))] , (7) where Dϕ denotes the discriminator with parameters ϕ. We follow (Kumari et al., 2022) by using pre-trained DINO (Caron et al., 2021) model as fixed backbone for the discriminator and introduce multiple independent classifiers, each corresponding to distinct level feature of the backbone model. Notably, when using the proposed online negative prompting, we apply the GAN loss exclusively to the generated HR images that correspond to positive targets. 8 Aiping Zhang* et al. Table 1: Quantitative comparison with state-of-the-art methods on both synthetic and real-world benchmarks. The best and second best results are highlighted in red and blue, respectively. We report the results using publicly available codes and checkpoints of the compared methods."
        },
        {
            "title": "Methods",
            "content": "PSNR SSIM LPIPS DISTS FID NIQE MANIQA MUSIQ CLIPIQA BSRGAN 0.3351 24.58 Real-ESRGAN 24.29 0.6371 0.3112 0.6269 44.23 4.7527 37.65 4.6797 0.3560 0.3822 DIV2K-Val LDM-100 StableSR-200 PASD-20 DiffBIR-50 SeeSR-50 ResShift-15 SinSR-1 OSEDiff-1 S3Diff-1 (Ours) 23.49 23.28 24.32 23.64 23.67 0.5762 0.5732 0.6218 0.5647 0.6042 0.3119 0.3111 0.3763 0.3524 0.3194 0.2275 0. 0.2727 0.2043 0.2184 0.2128 0.1968 41.37 24.31 30.17 30.72 25.89 5.0249 4.7570 5.2946 4.7042 4.8158 6.4732 0.2245 24.71 0.6234 5.9996 0.2068 0.6017 24.41 5.4050 0.2129 0.5970 23.30 0.5953 0.2571 0.1730 19.35 4.7391 23.40 0.3402 0.3244 0.3046 42.01 35.22 26. BSRGAN Real-ESRGAN 25.65 26.38 0.7655 0.2656 0.2124 0.2065 0.7603 0."
        },
        {
            "title": "RealSR",
            "content": "LDM-100 StableSR-200 PASD-20 DiffBIR-50 SeeSR-50 0.6986 26.33 24.60 0.7047 26.56 0.7636 0.6650 24.24 0.7210 25.14 0.4148 0.3068 0.2838 0.3469 0.3007 0.2454 0.2163 0.1999 0.2300 0.2224 141.28 5.6356 136.33 5.8554 143.35 6.3368 132.20 5.7848 120.94 5.8052 134.65 5.4909 125.44 5. ResShift-15 SinSR-1 OSEDiff-1 S3Diff-1 (Ours) 26.38 26.16 24.43 25.03 0.7567 0.7368 0.7153 0.7321 149.66 6.8703 0.3159 136.47 6.0054 0.3075 126.13 6.3821 0.3173 0.2699 0.1996 108.88 5.3311 0.2433 0.2332 0.2363 BSRGAN 155.63 6.5296 Real-ESRGAN 28.61 0.8052 0.2819 0.2089 147.62 6. 0.8028 0.2858 0.2144 28."
        },
        {
            "title": "DrealSR",
            "content": "LDM-100 StableSR-200 PASD-20 DiffBIR-50 SeeSR-50 0.7409 28.70 28.24 0.7596 29.07 0.7921 0.6526 25.93 0.7684 28.07 ResShift-15 SinSR-1 OSEDiff-1 S3Diff-1 (Ours) 28.69 28.37 27.65 26.89 0.7875 0.7486 0.7743 0.7469 0.4849 0.3149 0.3146 0.4518 0. 0.3525 0.3684 0.3177 0.3122 0.2889 0.2234 0.2179 0.2762 0.2315 164.80 8.0084 149.18 6.6920 138.47 7.4215 177.41 6.2261 147.41 6.3807 176.57 7.8754 0.2542 172.77 7.0558 0.2475 0.2366 141.95 7.2915 0.2120 119.86 6.1647 61.19 61.06 62.27 65.81 61.19 65.81 68. 60.87 62.73 65.56 68.21 63.29 60.45 55.82 65.71 59.89 64.25 69.81 60.21 60.95 67.53 67.89 57.17 54.27 54.35 57.42 50.34 63.47 65. 52.37 54.97 63.55 64.19 0.5243 0.5278 0.6245 0.6753 0.5676 0.6704 0.6932 0.5933 0.6468 0.6584 0.7007 0.5114 0.4518 0.5060 0.6298 0.4924 0.6543 0. 0.5488 0.6304 0.6733 0.6722 0.5094 0.4520 0.6047 0.6062 0.5112 0.6859 0.6903 0.5402 0.6333 0.7056 0.7122 0.5127 0.4200 0.4022 0.4768 0.5041 0.3985 0.4239 0.4458 0. 0.3799 0.3765 0.3841 0.4336 0.3887 0.4853 0.5429 0.3970 0.4035 0.4878 0.4563 0.3435 0.3449 0.3240 0.3697 0.3637 0.4922 0.5145 0.3505 0.3829 0.4845 0."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Training Details. Following the recent work SeeSR (Wu et al., 2023), we train our model on the LSDIR (Li et al., 2023b) dataset and subset of 10k face images from FFHQ (Karras et al., 2019). To synthesize the LR-HR pairs for training, we employ the degradation pipeline proposed in Real-ESRGAN (Wang et al., 2021). During training, the synthesized LR images are upscaled to match the HR resolution of 512 512 before feeding into our SR model. The training process takes over 30k iterations, with batch size of 64 and learning rate of 2e5. We adopted SD-turbo (Sauer et al., 2023) as the base T2I model and fine-tuned it using the proposed degradation-guided LoRA. The rank parameter in LoRA is set as 16 for the VAE encoder and 32 for the diffusion UNet, respectively. The hyper-parameters of λL2, λLPIPS, and λGAN are set to be 2.0, 5.0, 0.5, respecDegradation-Guided One-Step Image Super-Resolution with Diffusion Priors 9 Table 2: Quantitative comparison with state-of-the-art methods on RealSet65. The best and second-best results are highlighted in red and blue, respectively."
        },
        {
            "title": "BSRGAN",
            "content": "RealESRGAN StableSR-200 DiffBIR-50 SeeSR-50 ResShift-15 SinSR-1 OSEDiff-1 S3Diff-1 NIQE MANIQA MUSIQ CLIPIQA 4.5801 0.3898 65.58 0.6159 4.3487 0.3934 64.12 0.6074 5.0371 0.4442 62.79 0. 4.2559 0.4984 69.81 0.7468 4.8959 0.4982 68.88 0.6805 6.0893 0.4106 60.90 0.6522 5.9221 0.4341 62.85 0.7172 5.2142 0.4666 68.48 0.7061 4.2467 0.4685 68.92 0. tively. The probability pn for online negative prompting is set as 0.05. Testing Details. We mainly follow the testing configurations of StableSR (Wang et al., 2024a) to comprehensively evaluate the performance of the proposed S3Diff . We adopt the degradation pipeline of RealESRGAN (Wang et al., 2021) to create 3,000 LR-HR pairs from the DIV2K validation set (Agustsson and Timofte, 2017). The resolution of LR images is 128128, and the corresponding HR images 512512. For realworld datasets, RealSR (Cai et al., 2019) and DRealSR, we follow the standard practices of StableSR (Wang et al., 2024a) and SeeSR (Wu et al., 2023) by cropping the LR image to 128128. Besides, we test our method on the real-world dataset, RealSet65 (Yue et al., 2023), to comprehensively evaluate the generative ability of our method. The guidance scale λcfg for Classifier-Free Guidance is set to 1.1. Evaluation Metrics. To thoroughly evaluate the performance of various methods, we consider both reference and non-reference metrics. PSNR and SSIM (Wang et al., 2004) are reference-based fidelity measures, which are calculated on the channel of the YCbCr space. LPIPS (Zhang et al., 2018b) and DISTS (Ding et al., 2020) serve as reference-based perceptual quality measures. FID (Heusel et al., 2017) assesses the distribution distance between ground truth and restored images. Additionally, we use NIQE (Zhang et al., 2015), MANIQA (Yang et al., 2022), MUSIQ (Ke et al., 2021), and CLIPIQA (Wang et al., 2023a) as non-reference metrics to assess image quality. Compared Methods. We compare our method with various cutting-edge real SR methods, which we have grouped into three categories. The first category includes GAN-based methods, including BSRGAN (Zhang et al., 2021) and Real-ESRGAN (Wang et al., 2021). The second category features diffusion-based methods like LDM (Rombach et al., 2022), StableSR (Wang et al., 2024a), PASD (Yang et al., 2023), DiffBIR (Lin et al., 2023), and SeeSR (Wu et al., 2023). The third category comprises state-of-the-art diffusion-based methods with few inference steps, including ResShift (Yue et al., 2023, 2024a), SinSR (Wang et al., 2024b) and OSEDiff (Wu et al., 2024). For fair comparison, we use their publicly available codes and checkpoints to generate HR images on the same testing sets and report the corresponding performance comparisons."
        },
        {
            "title": "4.2 Comparisons with State-of-the-Arts",
            "content": "Quantitative Comparisons We first show the quantitative comparison results on three synthetic and realworld datasets in Table 1. We can obtain the following observations. (1) our approach consistently achieves promising reference metrics. Compared to the accelerated diffusion-based methods, i.e., ResShift, SinSR, and OSEDiff, our S3Diff obtains comparable or better PSNR and SSIM scores. Note that, ResShift and SinSR show better PSNR and SSIM scores. This is primarily because they learn the diffusion process on the residual of LR-HR pairs, requiring training the diffusion model from scratch, rather than utilizing pre-trained text-to-image model. (2) the proposed S3Diff excels in perceptual quality, achieving top-tier LPIPS, DISTS, and FID scores across all datasets. For instance, on the DIV2K dataset, we achieve scores of 0.2571 for LPIPS, 0.1730 for DISTS, and 19.35 for FID, which are significantly lower than those of competing methods. This demonstrates our ability to produce visually appealing results that align closely with human perception. (3) our method consistently outperforms various datasets in non-reference image quality metrics, such as NIQE, MANIQA, MUSIQ, and CLIPIQA. Notably, compared to the state-of-the-art method, SeeSR (Wu et al., 2023), our approach achieves superior performance on nonreference metrics while requiring significantly less inference time. (4) the robust performance across different datasets shows the adaptability and generalization capability of our method, proving its effectiveness in both synthetic and real-world scenarios. Overall, our method surpasses other diffusion-based methods by achieving significantly better scores on both reference and nonreference metrics, requiring only one-step inference. 10 Aiping Zhang* et al. Table 3: Comparison of efficiency across various methods. The inference time of each method is calculated using the average inference time of 3000 input images of size 128 128, upscaled by factor of 4, on an A100 GPU. StableSR DiffBIR SeeSR ResShift SinSR OSEDiff S3Diff Inference Step Inference Time (s) # Trainable Param (M) 200 17.75 150.0 50 16.06 380.0 50 5.64 749.9 15 1.65 118. 1 0.42 118.6 1 0.60 8.5 1 0.62 34.5 Fig. 5: Qualitative comparisons of different methods on the synthesis dataset, DIV2K-Val (Agustsson and Timofte, 2017). (Zoom in for details) Table 2 presents the comparison on RealSet65 (Yue et al., 2023). Since RealSet65 has no ground-truth HR images, we only report non-reference metrics. We nearly achieve the best performance among efficient DM-based SR models, notably attaining NIQE score of 4.2467, which significantly surpasses all other methods. originally developed from SeeSR (Wu et al., 2023). In contrast, our method employs lightweight degradation estimation model with only 2.36M parameters. The feature of not requiring image content descriptions makes our method extremely efficient while achieving promising performance. Complexity Analysis Table 3 compares the number of parameters of different DM-based SR models and their inference time. The inference time of each method is calculated using an average inference time of 3000 input images of size 128 128, upscaled by factor of 4, on an A100 GPU. By utilizing single-step forward pipeline, S3Diff significantly outperforms multistep methods in inference time. Specifically, S3Diff is approximately 30 times faster than StableSR, 9 times faster than SeeSR, and 3 times faster than ResShift. While our method is slightly slower than SinSR and OSEDiff, it achieves superior SR quality. Regarding the number of parameters, our method is only larger than OSEDiff. However, OSEDiff utilizes large pre-trained degradation-robust tag model with 1407M parameters, Qualitative Comparisons Figures 5 and 6 present visual comparisons of synthetic and real-world images, respectively. In Figure 5, GAN-based methods like BSRGAN and Real-ESRGAN struggle to preserve fine details, resulting in loss of texture and clarity. DMbased methods, such as StableSR, DiffBIR, and SeeSR, while enhancing detail, often produce outputs inconsistent with the original low-resolution input, leading to unnatural appearances, particularly noticeable in the flowers and leaves. Inference-efficient approaches like ResShift and SinSR, trained from scratch, tend to introduce artifacts that disrupt image smoothness and quality. OSEDiff, in its attempt to enhance details, often causes distortion, resulting in unnatural colors and features. In contrast, our method excels at accurately reDegradation-Guided One-Step Image Super-Resolution with Diffusion Priors 11 Fig. 6: Qualitative comparisons of different methods on the real-world dataset. (Zoom in for details) constructing image features while maintaining semantic integrity, delivering high levels of detail, such as the leaf veins and chapped lips, without introducing artifacts. This underscores our approachs robustness in handling severe degradation and producing reconstructions that are both precise and true to the original content. In Figure 6, similar observations can be made for real-world images. GAN-based methods always produce images with distorted structures. Diffusion-based methods like StableSR can produce realistic textures but often struggle to preserve accurate semantic details. Although SeeSR is capable of generating tags to describe image content, sometimes results in inaccurate descriptions, leading to smooth edges and unclear semantic details. While ResShift and SinSR demonstrate better consistency between low-resolution and high-resolution images, they still fall short in capturing fine details. In contrast, our method delivers superior visual results, offering sharp and semantically accurate details, as evidenced by the clear edges and consistent structures throughout the images."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We first discuss the effectiveness of the proposed strategy of model adaption. Then, we discuss the effectiveness of the proposed degradation-guided LoRA, including its degradation-aware ability and the roles of block ID embeddings. Finally, we investigate the effect of the proposed online negative prompting and ablate on the used losses. Unless stated otherwise, we mainly conduct experiments on the DIV2K-Val (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) datasets. Effectiveness of Adaption Solution. In Table 4, we present four experiments to validate the effectiveness of our adaptation solution. We use the same loss functions as in our default setting. To avoid interference from other modules, we do not use degradation-guided LoRA, online negative prompting, and CFG during inference. (a) We only inject LoRA layers into the UNet. (b) Based on (a), we additionally add several skip connections between the encoder and decoder, which is proposed in (Parmar et al., 2024) to improve inputoutput structural consistency. (c) We inject LoRA layers into the VAE decoder. (d) Based on (c), we further inject LoRA layers into the VAE encoder. As we can see, injecting LoRAs into the UNet alone can already provide acceptable SR performance, especially perceptual quality. Adding skip connections between the encoder and decoder enhances the information flow, leading to improved structural consistency. However, this setup slightly improves the reference metrics but significantly worsens the non-reference metrics, like the MUSIQ score. This indicates trade-off between struc12 Aiping Zhang* et al. Table 4: Quantitative results of different settings for model adaption of our method on DIV2K-Val (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) benchmarks. Module Adaption Methods DIV2K-Val RealSR PSNR LPIPS MUSIQ PSNR LPIPS MUSIQ UNet-Lora UNet-Lora + Skip Connection (Parmar et al., 2024) UNet-Lora + Decoder-Lora UNet-Lora + Encoder-Lora + Decoder-Lora 24.10 24.57 24.25 24.65 0.2746 0.2546 0.2587 0.2543 63.23 59.44 60.68 53.85 24.56 25.45 25.65 25. 0.2770 0.2577 0.2645 0.2530 62.42 57.54 54.47 54.73 UNet-Lora + Encoder-Lora (Ours) 24.12 0.2549 65. 24.71 0.2751 63.78 Table 5: Quantitative results of different settings for degradation-guided LoRA on DIV2K-Val (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) benchmarks. Methods DIV2K-Val PSNR LPIPS MUSIQ PSNR RealSR LPIPS MUSIQ Cross-Attention Injection 24. 0.2627 Shared Ours (Unshared C, block ID embeddings) 24.06 24.13 0.2613 0.2563 64.68 65.52 66. 25.72 0.2675 25.66 25.55 0.2626 0.2573 63.67 64.23 65. tural similarity and perceptual quality, aligning with findings from StableSR (Wang et al., 2024a). Similarly, adding LoRA layers to the VAE decoder results in better structural similarity but reduced perceptual quality, possibly disrupting the well-constructed compressed latent space. Finally, we try to inject LoRA layers into all modules of the diffusion model, including the VAE encoder, decoder and UNet, leading to the best PSNR score but an unacceptable MUSIQ score. Building on these findings, our method opts to inject LoRA layers only into the VAE encoder and UNet. The encoder LoRA layers help initially recover the LR image, while the LoRA layers of UNet unleash its generative power to polish image details and textures. In the following experiments, we default inject LoRA layers in the VAE encoder and the UNet. In Effectiveness of Degradation-Guided LoRA. Table 5, we conduct three experiments to demonstrate the effectiveness of degradation-guided LoRA. To prevent interference, we avoid using online negative prompting. In our method, we introduce the auxiliary information of image degradation into the model by modulating the LoRAs weights. There are some other ways of incorporating image degradation, including the use of cross-attention (Chen et al., 2023c; Gandikota and Chandramouli, 2024) in the way of textual prompt injection. Therefore, we first explore this method of injecting image degradation. We initialize two degradation prompts using the text embeddings of noise and blur from the CLIP text encoder. These prompts are then incorporated into the template: This image contains [n] and [b] degradation, where [n] and [b] serve as placeholders for the degradation prompt, which are modulated by the estimated noise and blur scores. As shown in Table 5, our solution outperforms the crossattention-based solution, indicating the effectiveness of incorporating degradation into LoRAs. We then investigate different implementations of the degradation-guided LoRA. Compared with the vanilla LoRA (ours in Table 4), we find improvements with the degradation-guided LoRA, even when we use the shared modulation matrix C. Incorporating the block ID embeddings can improve the flexibility of the model to handle the complexity of image degradation, further enhancing the performance. Effectiveness of Online Negative Prompting. To validate the effectiveness of the proposed online negative prompting, we present four experiments in Table 6, where we also employ the degradation-guided LoRA. Notably, we discovered that S3Diff can respond to negative prompts even without training, likely due to the inherent capability of the diffusion prior. As shown in Table 6, when trained only with positive samples and tested using CFG, the perceptual quality of generated images slightly improves compared to not using CFG. After incorporating negative samples, both structural similarity and perceptual quality are improved, indicating the SR model recognizes what constitutes good and poor image quality through the proposed online negative prompting. In this context, using CFG during inference significantly boosts non-reference metric scores, making the slight reduction in reference metric scores worthwhile. As shown in Figure 7, the guidance scale Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors Table 6: Quantitative results of different settings for the proposed online negative prompting on DIV2KVal (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) benchmarks. CFG refers to Classifier-Free Guidance, with guidance scales set to 1.1 for all experiments. Methods DIV2K-Val PSNR LPIPS MUSIQ PSNR RealSR LPIPS MUSIQ w/o Online Negative Prompting Online Negative Prompting w/o CFG CFG w/o CFG CFG 24.13 23. 24.10 23.40 0.2563 0.2595 0.2521 0.2571 66.55 67.14 66.94 68.21 25.55 25. 25.76 25.03 0.2573 0.2610 0.2522 0.2699 65.78 66.25 66.42 67.88 Table 7: Quantitative results of different LoRA ranks on DIV2K-Val (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) benchmarks. 8/16 indicates setting of 8 for the VAE encoder and 16 for the UNet. DIV2K-Val"
        },
        {
            "title": "RealSR",
            "content": "LoRA Rank PSNR LPIPS MUSIQ PSNR LPIPS MUSIQ Fig. 7: Qualitative comparisons of different guidance scales using CFG. (Zoom in for details) greatly affects the final outcome, which may be because SD-Turbo (Sauer et al., 2023) does not support CFG during testing. Therefore, we set it to small value of 1.1 to balance fidelity and perceptual quality. Impact of LoRA Rank. LoRA plays crucial role in our approach, with the LoRA rank being key hyperparameter. In Table 7, we assess the impact of various LoRA ranks on super-resolution performance. As we can see, low rank primarily affects reference metrics like PSNR and LPIPS. Using low LoRA rank, such as 4 or 8, results in unstable training and poor structural similarity. This may be due to the domain gap between SR and T2I, which requires enough trainable parameters to bridge effectively. In contrast, non-reference metrics, like MUSIQ, are only slightly affected by low ranks, possibly due to the intrinsic generative ability of the T2I diffusion model. However, higher rank, such as 32, might reach saturation point where results no longer improve. We find that rank of 32 for the UNet and 16 for the VAE encoder strikes good balance between model complexity and super-resolution performance. Impact of Starting Step. From Figure 4, we can find that SD-Turbo can recover the LR image at different noise levels. Here, we investigate the impact of the starting step on the SR results. Specifically, we add noise to the LR image at different levels based on the starting step. As shown in Table 8, directly using LR 0.2720 22.55 4/4 0.2723 22.61 8/8 0.2690 8/16 23.08 0.2640 16/16 23.10 16/32 23.40 0.2571 32/32 23.45 0. 68.28 68.41 68.11 67.69 68.21 68.35 68.10 0.2742 23.99 68.28 0.2663 24.16 67.97 0.2602 24.22 68.11 0.2705 24.31 25.03 0.2699 67.88 24.98 0.2687 67.90 Fig. 8: Qualitative comparisons of injecting noise of different levels into LR images as input. (Zoom in for details) images or adding noise at 250 steps yields comparable results. This is because SD-Turbo mainly adds details in the last 250 steps, which does not significantly affect the image structure. In contrast, starting at 500 steps decreases the PSNR score but improves the MUSIQ score. This is reasonable because the image structure of the LR image is significantly degraded at this point. Additionally, the generation capability at 500 steps is much stronger, allowing for the creation of more diverse images. Visual examples in Figure 8 further demonstrate this finding. To maintain trade-off between non14 Aiping Zhang* et al. Fig. 9: Comparisons of SR performance (LPIPS and MANIQA scores) and convergence speed between different diffusion priors. Visualization results on the RealSR dataset demonstrate the advantages of using diffusion priors. (Zoom in for details) Table 8: Quantitative results of injecting noise of different levels into LR images on DIV2K-Val (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) benchmarks. Table 9: Quantitative results of integrating different text prompts on DIV2K-Val (Agustsson and Timofte, 2017) and RealSR (Cai et al., 2019) benchmarks. DIV2K-Val RealSR Method DIV2K-Val RealSR PSNR LPIPS MUSIQ PSNR LPIPS MUSIQ PSNR LPIPS MUSIQ PSNR LPIPS MUSIQ 22.50 23.12 0.2662 0. 68.90 68.05 23.39 0.2877 24.70 0.2673 67.95 67.52 23.40 0.2571 68.21 Ours + Tag 68.41 0.2653 23.15 69.57 0.2610 + Long text 23.28 25.03 0.2699 67.88 68.02 0.2745 24.86 68.57 0.2772 24.88 Start Step 500 250 w/o noise 23.40 0.2571 68.21 25.03 0.2699 67.88 GPU memory. Additionally, we use Stable Diffusion 1.5 and 2.1 as diffusion priors for comparison. The architecture is kept consistent with S3Diff to ensure fairness. As illustrated in Figure 9, utilizing diffusion priors significantly improves both convergence speed and SR performance compared to training from scratch. Moreover, we observe that training from scratch requires 23 times more GPU memory on average compared to S3Diff . Leveraging the distilled diffusion prior, SDTurbo slightly enhances convergence speed. Moreover, as demonstrated in Figure 9, all three SD models achieve similar SR performance after convergence. Integrating with Text Prompts. Even though the default S3Diff does not utilize text prompts like previous methods (Lin et al., 2023; Wu et al., 2023; Yu et al., 2024; Wu et al., 2024), which help enhance image detail recovery, S3Diff can still be seamlessly combined with textual descriptions, such as tags (Wu et al., 2023, 2024) or long texts (Lin et al., 2023; Yu et al., 2024), by substituting the used general positive prompt. Table 9 shows the experimental results on three options. The first experiment is based on our default setting, i.e., we utilize general prompt: high-resolution image full of vivid details, showcasing rich blend of colors Fig. 10: Qualitative comparisons of using different text prompts. (Zoom in for details) reference and reference metrics, we directly use the LR image as input for simplicity. Impact of Diffusion Prior. The default S3Diff utilizes SD-Turbo (Sauer et al., 2023), capable of generating images in just few sampling steps, to quickly adapt the T2I prior to super-resolution. We first evaluate the effectiveness of using proper diffusion prior by training baseline from scratch without loading pre-trained model. Notably, since the UNet comprises the majority of trainable parameters, we only train the UNet from scratch and freeze the VAE encoder to conserve Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors and clear textures. The second experiment employs the DAPE, which is proposed by SeeSR (Wu et al., 2023) to extract degradation-robust tag-style prompts. The third experiment uses long text descriptions extracted by LLaVA-v1.5 (Liu et al., 2024), following SUPIR (Yu et al., 2024). As we can see, using text prompts can improve non-reference metrics but may decrease reference metrics. By employing DAPE and LLaVA to extract text prompts, the generation capability of the pretrained T2I model is activated, leading to richer synthesized details, though this can reduce structural consistency. visual example is shown in Figure 10. Although DAPE and LLaVA provide semantic information from the low-resolution image, their visual details are similar to ours. Additionally, they introduce large models with huge number of parameters for extracting image descriptions. For example, DAPE and LLaVA have 1.4B and 7B parameters, which demand significant hardware resources for inference. More seriously, LLaVA takes 4 to 5 seconds to generate text prompt per image, complicating efficient real-time super-resolution. Given the comparable performance and to ensure efficient inference, we do not use text prompts in our default setting."
        },
        {
            "title": "4.4 Degradation Control",
            "content": "We observe that degradation information significantly impacts SR performance. Here, we aim to figure out whether the model utilizes degradation information. To do this, we manually adjust the predicted noise and blur scores to predefined values and use our model to generate corresponding super-resolution images. Figure 11 illustrates an example where the blur and noise scores are estimated at 0.85 and 0.33, respectively. As shown, when the input noise score increases, the generated images become smoother. This may reduce perceptual quality but can sometimes enhance the consistency between LR and HR images by eliminating incorrect predictions. Moreover, adjusting the blur score primarily affects the richness of image details. These experiments show our methods capability to utilize degradation information to tailor the output according to users demands. Compared to images generated with preset scores, the image generated using estimated scores achieves nearly the best metric results, highlighting the effectiveness of our approach in utilizing degradation information."
        },
        {
            "title": "5 Limitations",
            "content": "While S3Diff gains advantages from the diffusion prior, it also faces some limitations. Specifically, S3Diff strugFig. 11: Visual comparison on different degradation inputs. We replace the noise score or the blur score of estimated degradation with 0, 0.5, or 1.0 in our experiments. (Zoom in for details) gles with handling small scene texts, human faces, etc. Even if these challenges are common in existing superresolution methods, we believe that utilizing more advanced diffusion prior, like SDXL (Podell et al., 2023) and SD3 (Esser et al., 2024), and training on higherquality data could offer improvements. We plan to explore these in future work."
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduced novel one-step SR model, finetuned from pre-trained T2I model, effectively addressing the efficiency limitations of diffusion-based SR methods. We developed degradation-guided LoRA module that enhances SR performance by integrating estimated degradation information from LR images while preserving the robust generative priors of the pre-trained diffusion model. This dual focus on efficiency and degradation modeling results in powerful, data-dependent SR model that significantly outperforms recent state-ofthe-art methods. Our approach also includes an online 16 Aiping Zhang* et al. negative prompting strategy during the training phase and classifier-free guidance during inference, greatly improving perceptual quality. Extensive experimental results demonstrate the superior efficiency and effectiveness of our method, achieving high-quality image superresolution with one sampling step."
        },
        {
            "title": "Data Availability Statement",
            "content": "All experiments are conducted on publicly available datasets. Refer to the references cited."
        },
        {
            "title": "References",
            "content": "Agustsson E, Timofte (2017) Ntire 2017 challenge on single image super-resolution: Dataset and study. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pp 126 135 Cai J, Zeng H, Yong H, Cao Z, Zhang (2019) Toward real-world single image super-resolution: new benchmark and new model. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 30863095 Caron M, Touvron H, Misra I, Jegou H, Mairal J, Bojanowski P, Joulin (2021) Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 96509660 Chen C, Shi X, Qin Y, Li X, Han X, Yang T, Guo (2022) Real-world blind super-resolution via feature matching with implicit high-resolution priors. In: Proceedings of the ACM International Conference on Multimedia (ACM MM) Chen J, Yu J, Ge C, Yao L, Xie E, Wu Y, Wang Z, Kwok J, Luo P, Lu H, et al. (2023a) Pixart-α: Fast training of diffusion transformer for photorealistic text-toimage synthesis. arXiv preprint arXiv:231000426 Chen X, Wang X, Zhou J, Qiao Y, Dong (2023b) Activating more pixels in image super-resolution transformer. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 2236722377 Chen Z, Zhang Y, Gu J, Yuan X, Kong L, Chen G, Yang (2023c) Image super-resolution with text prompt diffusion. arXiv preprint arXiv: Chung H, Kim J, Mccann MT, Klasky ML, Ye JC (2022a) Diffusion posterior sampling for general noisy inverse problems. In: Proceedings of International Conference on Learning Representations (ICLR) Chung H, Sim B, Ryu D, Ye JC (2022b) Improving diffusion models for inverse problems using manifold constraints. In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS) Chung H, Sim B, Ye JC (2022c) Come-closer-diffusefaster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Delbracio M, Milanfar (2023) Inversion by direct iteration: An alternative to denoising diffusion for image restoration. Transactions on Machine Learning Research (TMLR) Ding K, Ma K, Wang S, Simoncelli EP (2020) Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 44(5):25672581 Dong C, Loy CC, He K, Tang (2014) Learning deep convolutional network for image super-resolution. In: Proceedings of the European Conference on Computer Vision (ECCV), Springer, pp 184199 Dong W, Zhang L, Shi G, Li (2012) Nonlocally centralized sparse representation for image restoration. IEEE Transactions on Image Processing (TIP) Dong W, Shi G, Li (2013) Nonlocal image restoration with bilateral variance estimation: low-rank approach. IEEE Transactions on Image Processing (TIP) Esser P, Kulal S, Blattmann A, Entezari R, Muller J, Saini H, Levi Y, Lorenz D, Sauer A, Boesel F, et al. (2024) Scaling rectified flow transformers for highresolution image synthesis. In: International Conference on Machine Learning (ICML) inference framework (2024) URL https://www. deepfloyd.ai/deepfloyd-if, accessed: 20, 05, 2024 Gandikota KV, Chandramouli (2024) Text-guided explorable image super-resolution. arXiv preprint arXiv:240301124 Gao S, Liu X, Zeng B, Xu S, Li Y, Luo X, Liu J, Zhen X, Zhang (2023) Implicit diffusion models for continuous super-resolution. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Gu S, Zuo W, Xie Q, Meng D, Feng X, Zhang (2015) Convolutional sparse coding for image superresolution. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter (2017) Gans trained by two time-scale update rule converge to local nash equilibrium. Proceedings of Advances in Neural Information Processing Systems (NeurIPS) 30 Ho J, Salimans (2022) Classifier-free diffusion guidance. arXiv preprint arXiv:220712598 Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors 17 Ho J, Jain A, Abbeel (2020) Denoising diffusion probabilistic models. In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS) Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, Chen (2022) Lora: Low-rank adaptation of large language models. In: Proceedings of International Conference on Learning Representations (ICLR) Huang JB, Singh A, Ahuja (2015) Single image superresolution from transformed self-exemplars. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Johnson J, Alahi A, Fei-Fei (2016) Perceptual losses for real-time style transfer and super-resolution. In: Proceedings of the European Conference on Computer Vision (ECCV) Karras T, Laine S, Aila (2019) style-based generator architecture for generative adversarial networks. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Kawar B, Elad M, Ermon S, Song (2022) Denoising diffusion restoration models. In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS) Ke J, Wang Q, Wang Y, Milanfar P, Yang (2021) Musiq: Multi-scale image quality transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Kim KI, Kwon (2010) Single-image super-resolution using sparse regression and natural image prior. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) Kumari N, Zhang R, Shechtman E, Zhu JY (2022) Ensembling off-the-shelf models for gan training. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Li H, Yang Y, Chang M, Chen S, Feng H, Xu Z, Li Q, Chen (2022) SRDiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing Li R, Zhou Q, Guo S, Zhang J, Guo J, Jiang X, Shen Y, Han (2023a) Dissecting arbitrary-scale superresolution capability from pre-trained diffusion generative models. arXiv preprint arXiv: Li Y, Zhang K, Liang J, Cao J, Liu C, Gong R, Zhang Y, Tang H, Liu Y, Demandolx D, et al. (2023b) Lsdir: large scale dataset for image restoration. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 17751787 Liang J, Cao J, Sun G, Zhang K, Van Gool L, Timofte (2021) Swinir: Image restoration using swin transformer. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Lin X, He J, Chen Z, Lyu Z, Fei B, Dai B, Ouyang W, Qiao Y, Dong (2023) Diffbir: Towards blind image restoration with generative diffusion prior. arXiv preprint arXiv:230815070 Lipman Y, Chen RT, Ben-Hamu H, Nickel M, Le (2023) Flow matching for generative modeling. In: Proceedings of International Conference on Learning Representations (ICLR) Liu H, Li C, Wu Q, Lee YJ (2024) Visual instruction tuning. Proceedings of Advances in Neural Information Processing Systems (NeurIPS) 36 Liu X, Gong C, Liu (2022) Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv: Lu C, Zhou Y, Bao F, Chen J, Li C, Zhu (2022a) Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Proceedings of Advances in Neural Information Processing Systems (NeurIPS) 35:57755787 Lu C, Zhou Y, Bao F, Chen J, Li C, Zhu (2022b) Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:221101095 Luo Z, Gustafsson FK, Zhao Z, Sjolund J, Schon TB (2023) Image restoration with mean-reverting stochastic differential equations. In: International Conference on Machine Learning (ICML) Mou C, Wu Y, Wang X, Dong C, Zhang J, Shan (2022) Metric learning based interactive modulation for real-world super-resolution. In: Proceedings of the European Conference on Computer Vision (ECCV) Pan X, Zhan X, Dai B, Lin D, Loy CC, Luo (2021) Exploiting deep generative prior for versatile image restoration and manipulation. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 44(11):74747489 Parmar G, Park T, Narasimhan S, Zhu JY (2024) Onestep image translation with text-to-image models. arXiv preprint arXiv:240312036 Podell D, English Z, Lacey K, Blattmann A, Dockhorn T, Muller J, Penna J, Rombach (2023) SDXL: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:230701952 Rombach R, Blattmann A, Lorenz D, Esser P, Ommer (2022) High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Saharia C, Chan W, Saxena S, Li L, Whang J, Denton EL, Ghasemipour K, Gontijo Lopes R, Karagol Ayan B, Salimans T, et al. (2022a) Photorealistic text-toimage diffusion models with deep language understanding. Proceedings of Advances in Neural Infor18 Aiping Zhang* et al. mation Processing Systems (NeurIPS) Saharia C, Ho J, Chan W, Salimans T, Fleet DJ, Norouzi (2022b) Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) Salimans T, Ho (2022) Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv: Sauer A, Lorenz D, Blattmann A, Rombach (2023) Adversarial diffusion distillation. arXiv preprint arXiv:231117042 Sauer A, Boesel F, Dockhorn T, Blattmann A, Esser P, Rombach (2024) Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:240312015 Shi W, Caballero J, Huszar F, Totz J, Aitken AP, Bishop R, Rueckert D, Wang (2016) Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Sohl-Dickstein J, Weiss E, Maheswaranathan N, Ganguli (2015) Deep unsupervised learning using nonequilibrium thermodynamics. In: International Conference on Machine Learning (ICML) Song J, Meng C, Ermon (2022a) Denoising diffusion implicit models. In: Proceedings of International Conference on Learning Representations (ICLR) Song J, Vahdat A, Mardani M, Kautz (2022b) Pseudoinverse-guided diffusion models for inverse problems. In: Proceedings of International Conference on Learning Representations (ICLR) Song Y, Sohl-Dickstein J, Kingma DP, Kumar A, Ermon S, Poole (2021) Score-based generative modeling through stochastic differential equations. In: Proceedings of International Conference on Learning Representations (ICLR) Sun H, Li W, Liu J, Chen H, Pei R, Zou X, Yan Y, Yang (2023) Coser: Bridging image and language for cognitive super-resolution. arXiv preprint arXiv:231116512 Sun J, Xu Z, Shum HY (2008) Image super-resolution using gradient profile prior. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Tancik M, Srinivasan P, Mildenhall B, Fridovich-Keil S, Raghavan N, Singhal U, Ramamoorthi R, Barron J, Ng (2020) Fourier features let networks learn high frequency functions in low dimensional domains. Proceedings of Advances in Neural Information Processing Systems (NeurIPS) 33:75377547 Wang J, Chan KC, Loy CC (2023a) Exploring clip for assessing the look and feel of images. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol 37, pp 25552563 Wang J, Yue Z, Zhou S, Chan KC, Loy CC (2024a) Exploiting diffusion prior for real-world image superresolution. International Journal of Computer Vision (IJCV) pp 1 Wang X, Yu K, Dong C, Loy CC (2018a) Recovering realistic texture in image super-resolution by deep spatial feature transform. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Wang X, Yu K, Wu S, Gu J, Liu Y, Dong C, Qiao Y, Change Loy (2018b) Esrgan: Enhanced superresolution generative adversarial networks. In: Proceedings of the European Conference on Computer Vision Workshops (ECCV-W) Wang X, Xie L, Dong C, Shan (2021) Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) image restoration using denoising diffusion null-space model. In: Proceedings of International Conference on Learning Representations (ICLR) Wang Y, Yu J, Zhang (2023b) Zero-shot Wang Y, Yang W, Chen X, Wang Y, Guo L, Chau LP, Liu Z, Qiao Y, Kot AC, Wen (2024b) Sinsr: Diffusion-based image super-resolution in single step. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Wang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004) Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing (TIP) 13(4):600 Wu R, Yang T, Sun L, Zhang Z, Li S, Zhang (2023) SeeSR: Towards semantics-aware real-world image super-resolution. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Wu R, Sun L, Ma Z, Zhang (2024) One-step effective diffusion network for real-world image superresolution. arXiv preprint arXiv:240608177 Xia B, Zhang Y, Wang S, Wang Y, Wu X, Tian Y, Yang W, Van Gool (2023) Diffir: Efficient diffusion model for image restoration. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Yang J, Wright J, Huang TS, Ma (2010) Image superresolution via sparse representation. IEEE Transactions on Image Processing (TIP) Yang S, Wu T, Shi S, Lao S, Gong Y, Cao M, Wang J, Yang (2022) Maniqa: Multi-dimension attention network for no-reference image quality assessment. Degradation-Guided One-Step Image Super-Resolution with Diffusion Priors 19 Zhang L, Zhang L, Bovik AC (2015) feature-enriched completely blind image quality evaluator. IEEE Transactions on Image Processing (TIP) 24(8):2579 2591 Zhang L, Rao A, Agrawala (2023) Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp 38363847 Zhang R, Isola P, Efros AA, Shechtman E, Wang (2018b) The unreasonable effectiveness of deep features as perceptual metric. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 586595 In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 11911200 Yang T, Ren P, Xie X, Zhang (2023) Pixelaware stable diffusion for realistic image superresolution and personalized stylization. arXiv preprint arXiv:230814469 Yin T, Gharbi M, Zhang R, Shechtman E, Durand F, Freeman WT, Park (2024) One-step diffusion with distribution matching distillation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp 6613 Yu F, Gu J, Li Z, Hu J, Kong X, Wang X, He J, Qiao Y, Dong (2024) Scaling up to excellence: Practicing model scaling for photo-realistic image restoration in the wild. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Yue Z, Loy CC (2024) Difface: Blind face restoration with diffused error contraction. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) Yue Z, Wang J, Loy CC (2023) Resshift: Efficient diffusion model for image super-resolution by residual shifting. In: Proceedings of Advances in Neural Information Processing Systems (NeurIPS) Yue Z, Wang J, Loy CC (2024a) Efficient diffusion for image restoration by residual shifting. model IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) Yue Z, Yong H, Zhao Q, Zhang L, Meng D, Wong KYK (2024b) Deep variational network toward blind image restoration. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) Zhang K, Gao X, Tao D, Li (2012) Single image super-resolution with non-local means and steering kernel regression. IEEE Transactions on Image Processing (TIP) Zhang K, Zuo W, Chen Y, Meng D, Zhang (2017) Beyond gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE Transactions on Image Processing (TIP) 26(7):31423155 Zhang K, Zuo W, Zhang (2018a) Learning single convolutional super-resolution network for multiple degradations. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Zhang K, Liang J, Van Gool L, Timofte (2021) Designing practical degradation model for deep blind image super-resolution. In: Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp"
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "S-Lab, Nanyang Technological University, Singapore",
        "School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen, China"
    ]
}