{
    "paper_title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
    "authors": [
        "Zhili Cheng",
        "Yuge Tu",
        "Ran Li",
        "Shiqi Dai",
        "Jinyi Hu",
        "Shengding Hu",
        "Jiahao Li",
        "Yang Shi",
        "Tianyu Yu",
        "Weize Chen",
        "Lei Shi",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval."
        },
        {
            "title": "Start",
            "content": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents Zhili Cheng* Yuge Tu Ran Li Shiqi Dai Jinyi Hu Shengding Hu Jiahao Li Yang Shi Tianyu Yu Weize Chen Lei Shi Maosong Sun Tsinghua University {chengzl22, hu-jy21}@mails.tsinghua.edu.cn 5 2 0 2 J 1 2 ] . [ 1 8 5 8 1 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have shown significant advancements, providing promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EMBODIEDEVAL, comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EMBODIEDEVAL features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EMBODIEDEVAL and found that they have significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval. 1. Introduction In recent years, Multimodal Large Language Models (MLLMs) [53, 67, 82] have demonstrated strong understanding and reasoning capabilities on visual and language tasks. With the rapid development of MLLMs, rich set of benchmarks for image understanding [25, 45, 56, 91] and video analysis [27, 63] has emerged. However, these benchmarks are non-interactive and insufficient in evalu- *Core contributors, Project Lead, Corresponding Author Figure 1. Examples of the five task categories in EMBODIEDEVAL. On the left are the task text and part of the action space. On the right are observations from specific steps, along with the actions taken in the expert demonstration at those moments. ating MLLMs ability to handle tasks in real-world scenarios. Beyond these basic tasks which focus on noninteractive visual scenes, researchers are actively trying to expand MLLMs as embodied agents in interactive environments, which require the model to interpret multimodal inputs into actions [1, 10, 22, 62, 66, 84, 99]. To accomplish this, MLLMs are expected to integrate multitude of capabilities that enable them to interact effectively with the environment, including ego-centric perception [15], visual 1 grounding [4, 96], spatial reasoning [8], episodic memory [63], among others. However, the comprehensive evaluation of MLLMs in embodied tasks remains largely unexplored. Existing benchmarks for embodied AI greatly lack diversity in tasks and scenes, which limits their evaluation to specific aspects. EQA [18] dataset is restricted to only 9 template questions such as queries about object color and existence, while ALFRED [76] contains only 7 template tasks such as pick and then place within 4 room types. Moreover, many of embodied AI benchmarks require specific forms of input and output, making the evaluation of mainstream MLLMs inefficient or even infeasible. For example, R2R [4] uses 3D points as both input and output, REVERIE [71] requires predicting the bounding box of target object, while ALFRED requires an object segmentation mask for interaction. To address this gap, we present the first comprehensive evaluation benchmark for evaluating MLLMs embodied capabilities in interactive environments, distinguishing it from the existing three types of evaluation benchmarks: static MLLMs benchmarks, which are non-interactive; embodied AI datasets, which are not comprehensive or diverse enough; and LLM agent benchmarks [54, 77], which heavily rely on textual environment states and have overly abstract actions (e.g., go to bathroom), thus downplaying or overlooking the critical embodied agent capabilities such as visual grounding and spatial reasoning. The key features of EMBODIEDEVAL are as follows: Diverse Interactions. EMBODIEDEVAL provides simulation framework that supports variety of interactions, such as locomotion, question-answering, and interactions with objects and humans in realistic 3D scenes. Agents interact with the environment to gather new information or change its state to complete the task. Diverse Tasks. In contrast to previous work that used task templates or tasks with minimal variation, our tasks have been systematically generated and carefully selected for high quality and diversity. EMBODIEDEVAL featuring large number of novel tasks that involve wider range of abilities to assess the comprehensive capabilities of the model as shown in Figure 1, which are categorized into five major categories. Diverse Scenes. Unlike previous work, which was limited to household scenes, our scenes feature significant diversity in terms of objects and spaces, covering small rooms, large residences, and public spaces such as gym, store, office, among others. This approach can reduce the impact of scene types on the evaluation of the models generalization, making the assessment more comprehensive. 2. Related Works Multimodal Large Language Models. By connecting vision modules with LLMs, LLaVA [53] pioneers research Benchmark Scene. Task. Disc. Ego. Nav. Obj. Video-MME [26] EgoPlan etc. [11, 14] OpenEQA [63] EQA etc. [18, 81, 88] ALFRED [76] BEHAVIOR[79] EQA-MX [35] EMBODIEDEVAL - - - So. Ans. Table 1. Comparison of EMBODIEDEVAL with previous benchmarks. The abbreviations in the table headers, from left to right, represent: Scene diversity (beyond household scenes), Task diversity (beyond task templates), Discrete action space (for MLLMs evaluation), Egocentric vision, Navigation involved, Object interaction involved, Social interaction involved, and Answering questions involved. in MLLMs through visual instruction tuning, and obtains impressive multimodal chat capabilities. Many work further improves the MLLMs from various aspects, including detailed captioning [9], trustworthy response [89, 90], multilingual multimodal capabilities [32, 80] and visual grounding [70, 87]. Beyond single-image understanding, some work explores more complicated tasks. For example, KOSMOS-1 [33] and VILA [49] focus on image-text interleaved understanding, while Video-LLaVA [48] and VideoChat [47] focus on video understanding. Evaluation for MLLMs. Mainstream benchmarks for MLLMs mainly focus on perception and cognitive evaluation, such as MME [25], MMB [56] and MMMU [91]. As existing MLLMs excel in these benchmarks, some benchmarks propose more challenging tasks, such as mathematical reasoning [31, 60, 93], OCR capability [57, 64, 78] and scientific knowledge [59, 91]. However, these benchmarks lack evaluation on egocentric vision, which is essential for broader applications of MLLMs. To address this gap, EgoVQA [24], EgoPlan-Bench [11], EgoThink [14], and OpenEQA [63] propose to evaluate the reasoning and planning capabilities of MLLMs given the first-person perspective images or videos. However, these benchmarks still use static question-answering pairs without interacting with environments. Benchmarks for Embodied Agents. The datasets for embodied agents cover areas such as navigation, interaction, and question answering: (1) Navigation The R2R [4] dataset was the first to evaluate an agents navigation ability under natural language instructions, followed by R4R [36] and RxR [44], which improve the fine-grained evaluation of the navigation process. In navigation tasks, object navigation is crucial task because it serves as prerequisite step for an embodied agent to interact with any object. There are many object navigation datasets with different fo2 cus and features, including SOON [98], REVERIE [72], (2) Interaction ALDOZE [61] and GOAT-Bench [40]. FRED [76] is the most representative interaction dataset, requiring the agent to follow instructions to complete tasks involving interactions such as picking up and placing objects. Additionally, there are datasets focusing on moving objects [28, 86], rearrangement [6, 83], tidying up room [38], and household activities [42, 46, 65, 79]. (3) Question Answering EQA [18] first proposes the navigatethen-answer mechanism. Subsequently, more diverse EQA datasets have emerged, including those with questions involving multiple objects [88], requiring knowledge integration [81], set in realistic scenes [73], and handling situational queries [21]. Additionally, IQA [30] requires interaction with the environment to gather more observations in order to answer questions. EQA-MX [34] requires understanding non-verbal human expressions, such as body movements. However, existing benchmarks are limited in task variety, lacking comprehensive assessments of navigation, object interaction, and question-answering. They rely on highly repetitive task templates, failing to adequately capture the wide spectrum of embodied capabilities. Additionally, the task-specific observation spaces and continuous action spaces in many benchmarks are inadequate for effectively evaluating MLLMs. We summarize the comparison between EMBODIEDEVAL and other representative benchmarks in Table 1. 3. EmbodiedEval EMBODIEDEVAL consists of rich scenes and tasks to comprehensively evaluate the capabilities of embodied agents. We have implemented unified simulation and evaluation framework and conducted meticulous data annotation to create the final dataset. First, we introduce the task categories involved in EMBODIEDEVAL in Section 3.1, followed by detailed explanation of the simulation and evaluation framework in Section 3.2. After that, we describe the data collection and annotation process for this framework in Section 3.3. Finally, we show the data statistics of EMBODIEDEVAL in Section 3.4. 3.1. Task Categories To comprehensively assess the capabilities of MLLMs as embodied agents, we synthesized and expanded upon existing embodied tasks while incorporating novel additions, broadly categorizing the tasks into five categories. Each category encompasses diverse set of tasks and involves various capabilities. The task categories and sample tasks are shown in Figure 1. Navigation. The navigation task involves coarse-grained and fine-grained natural language instructions, requiring the agent to navigate from its initial position to target location and find specific object if the task demands it. Object Interaction. In object interaction tasks, agents must change the state of the environment through direct interaction with objects, such as moving objects, opening/- closing doors and drawers, and operating electrical devices. These tasks possibly involve multiple objects and require multi-step actions, such as using tool to operate another object and rearranging items to meet certain requirements. Social Interaction. Social interaction tasks encomincluding item delivery, pass human-agent perspective-taking capabilities, human feedback interpretation, and non-verbal communication comprehension such as gestural cues. interactions, Attribute Question Answering (AttrQA). AttrQA tasks necessitate exploring the environment to address questions regarding object and scene attributes. While incorporating traditional EQA tasks, AttrQA significantly broadens the scope of inquiry. It encompasses comprehensive attribute queries of objects and scenes, including but not limited to category, shape, material, color, function, state, location, existence, quantity, comparative analysis and reasoning across multiple attributes. Spatial Question Answering (SpatialQA). Spatial understanding [8, 13, 23, 37, 51, 75] is fundamental capability for embodied agents. SpatialQA requires agents to answer spatial-related questions through actions and observations, such as queries about size, height, position, distance, area, path, layout, spatial relationships, and more. 3.2. Evaluation Framework We implemented unified simulation framework based on LEGENT [17] platform. The framework aims to holisticly evaluate embodied abilities through diverse tasks, rather than focusing on specific tasks with particular input-output requirements. The observation space consists of task descriptions and egocentric vision. The action space consists of movement space, interaction space and answering space, which varies in each task instance. The action space is reasonably discretized and semantically understandable, while preserving the core evaluation objectives."
        },
        {
            "title": "3.2.1 Action Space",
            "content": "Figure 2. comparison of navigation graphs between R2R [4] dataset (left) and EMBODIEDEVAL (right). 3 Movement Space We use navigation graph as the movement space where the agent can rotate its view at point or move between adjacent navigation points. Compared to continuous movement, it discretizes motion without imposing great restrictions on the high-level tasks in practice [4]. Different from grid-world movement, this approach is more natural and adaptable to all kinds of scenes. Through sampling algorithms and manual adjustment, we constructed navigation graphs for each scene. To ensure realism, the navigation points are always walkable locations with no obstacles among them. Due to the greater diversity of our scenes and tasks compared to previous work, the density of navigation points varies based on the size of the scene and the task, ensuring that the number of steps required for tasks remains reasonable. For example, in complex interaction tasks within large scenes, the navigation points are more sparse and critical. In contrast to previous datasets, our navigation points are better organized as shown in Figure 2, and the connections between these points indicate clear semantics. MLLMs are not required to make choices from set of 3D positions, but only need to make directional decisions among navigation points. Specifically, the action space consists of three types of actions: move forward (moving to the facing navigation point), turn left/right (rotating to face new navigation point), and look up/down (adjusting the vertical view). Interaction Space We follows the discrete interaction space of previous embodied AI tasks that involves object interaction such as IQA [30], CHAI [65], RoomR [83] and OVMM[86] rather than continous space [76, 79]. This choice is based on two main considerations: (1) In continuous spaces, interactions are tightly related to specific methods and types of embodiment, which contradicts the goal of generality in evaluations and goes beyond the core issues of our research. (2) Due to the high complexity of continuous space, MLLMs cannot output reasonable values without being trained on specialized numerical trajectory, leading to infeasible evaluations. In EMBODIEDEVAL, we use an open vocabulary for the actions and objects in interactions to make them as rich as possible. Each interaction action has brief action text, operable objects, and conditions for successful interaction. For example, the pick up action requires the target object to be within sight and very close, the wash action requires the agent and the target object to be next to sink, and the hand over action requires the agent to hold an object and be next to person. In given test case, several interaction actions will be involved, including those necessary to complete the task and other distracting actions. Answering Space Unlike EQA and IQA, which require outputs of specific categories or use very limited vocabulary, our answer space is very open. Responses are written by annotators and rigorously verified, encompassing variety of possible replies to the question. For QA tasks, before exceeding the maximum allowed steps, there is no specified step at which the agent must respond; it can continue to explore until it believes it can select an answer. Once an answer is chosen, the task is immediately judged as correct or incorrect. In this way, the action space is discretized and each action has semantic text. EMBODIEDEVAL directly input the list of actions into the MLLM to decide the next action."
        },
        {
            "title": "3.2.2 Success Criteria",
            "content": "We automatically and accurately evaluate task completion through predicate functions. Each predicate maps the state of the simulation environment to boolean value indicating success. For example, the agent at predicate requires designated navigation point as parameter and returns true when the agent reaches this location at the end of the episode. Beyond evaluating only the final state, EMBODIEDEVAL also includes predicates that assess the entire process, similar to R4R [36]. For example, the agent pass predicate becomes true once the agent passes specified navigation point. task is considered successful when all predicates evaluate to true at the end. Consider the task Please go to the kitchen, then come back and tell me if there are any extra cups. This task involves three predicates: agent pass, agent at, and choose. These predicates verify that the agent passes through the kitchen doorway, returns to the initial position in front of the person, and selects the correct answer, respectively."
        },
        {
            "title": "3.2.3 Evaluation Process",
            "content": "The process of an evaluation episode is as follows: (1) The simulator initializes the 3D scene and navigation graph. The agent is positioned at designated starting point, and the initial first-person view image is saved into the observation history. (2) In each subsequent step, the agent chooses an action from given list of options, including movement, interaction and answering, based on the observation history. The environment executes the action, changes the state accordingly, returns new observations, along with feedback indicating whether the action was successful. The observation, action, and feedback are then appended to the observation history. This process continues until all success criteria are met resulting in task success, or the task fails due to an incorrect answer or exceeding the maximum allowed steps. For evaluating MLLMs designed for video understanding, image sequences are no longer used in the observation history. Instead, at each step, the simulator will output an egocentric video from the start to the current moment and use it as input for the MLLM. 4 Figure 3. The dataset construction pipeline of EMBODIEDEVAL. 3.3. Dataset Construction"
        },
        {
            "title": "3.3.2 Task Collection",
            "content": "The dataset construction process of EMBODIEDEVAL consists of three parts: scene collection, task collection, and task annotation. Each sample in the dataset requires substantial effort and undergoes rigorous annotation. Figure 3 illustrates our dataset construction pipeline."
        },
        {
            "title": "3.3.1 Scene Collection",
            "content": "The diversity of scenes allows for more accurate assessment of the agents generalization capabilities. Our scenes are more varied in source and type than previous benchmarks, curated from four different sources: (1) Objaverse Synthetic. To expand our object variety, we utilize the Objaverse [20] dataset as our object database, which contains vast collection of objects. Using procedural generation methods [19], we created numerous scenes from these objects. From these procedurally generated scenes, we manually selected and refined dozens of high-quality environments. (2) AI2THOR. We extracted and structured scenes from AI2THOR [43], focusing on indoor rooms with highly interactive objects. (3) HSSD. We included scenes from the Habitat Synthetic Scenes Dataset (HSSD) [39], which provides high-quality, realistic and complex scenes featuring extensive and diverse navigable spaces, including houses, villas, yards, and more. (4) Sketchfab. We enhanced our scene collection with selected free 3D scenes from Sketchfab1. These scenes are highly realistic and diverse, ranging from classrooms and supermarkets to offices and exhibitions, enriching the scope of our evaluation. These collected scenes serve as the basis for subsequent task annotation. 1https://sketchfab.com For task collection, we first gather seed tasks from over 30 existing datasets and benchmarks for embodied agents. Using these tasks as seeds, we prompted several advanced large language models to generate diverse task examples. To enhance task complexity, we designed some tasks to incorporate various capabilities, including complex grounding, episodic memory, spatial reasoning, quantitative reasoning, common sense reasoning, and planning, which resulted in many novel tasks. From this extensive task pool, experts selected over 300 distinct candidate tasks. During task annotation, each task text could only be selected from this candidate set and used once. We chose this approach rather than allowing annotators to write tasks for given scenes, as it ensures task diversity, prevents repetition, and reduces dependency on individual annotators creativity or preferences."
        },
        {
            "title": "3.3.3 Task Annotation",
            "content": "Annoation Content Task sample annotation begins with an annotator selecting task from the candidate pool and matching it to an appropriate scene. The annotator configures the action space detailed in Section 3.2.1, which includes movement space (via navigation graph adjustment), interaction space (via action options), and answering space (via answer options). Finally, the annotator defines success criteria as outlined in Section 3.2.2 through predicate function instantiation. To maintain the validity and quality of the dataset, all task annotations must satisfy the following criteria. (1) All tasks must be unambiguous within the given scene. (2) Question-answering tasks must require scene observation, 5 with each task providing eight answer options that vary in difficulty and include misleading options to reduce the chance of guessing the correct answer. (3) Once task is correctly annotated, the tasks must be executable in the simulator with well-designed navigation graph and accurate action options. Annotators must verify task feasibility using the same observational constraints as agents. Annotation System To ensure both efficiency and precision in the complex annotation process, we developed an annotation system based on Unity2. The system provides comprehensively function, which encompassing scene and task import/export, flexible content viewing, visualized action space, and guided annotation workflow that adheres to predefined guidelines (See Section 3.3.3). Annoation Process Eight expert annotators are recruited to perform the annotations. Before starting the annotation process, we conducted systematic training on annotation requirements and system usage. To ensure the datasets high quality, each annotated task was independently evaluated for correctness and quality by at least three reviewers. Furthermore, we validated task feasibility by providing expert demonstrations for each task and testing human performance with non-expert participants. 3.4. Dataset Statistics (a) Number of tasks by category for each scene source. (b) Visualization of top vocabulary by POS and frequency. EMBODIEDEVAL consists of 328 human-annotated tasks divided into 5 categories, distributed across 125 unique scenes and demonstrating high level of visual diversity and complexity. Figure 4a shows the distribution of the task across 5 task categories and 4 scene sources. Overall, the dataset comprises 575 predicate instances and 1,533 action options excluding movement space, including 1,213 answers and 320 interactions, with each episode averaging 10.72 steps based on expert demonstrations. Task descriptions average 16.09 words in length, while options average 5.72 words. To showcase the lexical diversity in EMBODIEDEVAL, Figure 4b presents visualization of the most frequent words categorized by grammatical type. 2https://unity.com/ 4. Experiments 4.1. Evaluation Settings We evaluate the models using the evaluation process described in Section 3.2.3. For multi-image MLLMs, we input the task prompt, history information (egocentric image observation history, action history, feedback history), and the action space (movement, interaction, answering) into the model, which then outputs the corresponding action option to be executed in the environment. For video MLLMs, we replace the image observation history with egocentric video. We set the maximum number of attempt steps for each task to 24, as all tasks are completed within this number of steps in the expert demonstration. We use an image resolution of 448448. We evaluate 22 MLLMs on EMBODIEDEVAL including closed-source MLLMs, open-source multi-image MLLMs and open-source video MLLMs demonstrating strong performance on MLLM benchmarks like MMMU [92] and Video-MME [12]. For reference, we also introduce two special agents: (1) Random agent uniformly samples actions from the available option set at each timestep. (2) Human agent involves non-expert humans unfamiliar with the tasks, performing actions in the simulators user interface using the same observation and action space as the models. 4.2. Evaluation Metrics We evaluate agent performance using three metrics. (1) Success Rate (Succ.) [2, 43, 55, 74, 97] is the primary metric we use to measure the percentage of tasks that the agent fully completes. (2) For multi-goal tasks, Goal-condition Success (GcS) [41, 76] measures partial success by calculating the proportion of goal conditions achieved, as specified by predicate functions. (3) Success weighted by Path Length (SPL) [3, 7] is path-weighted metric to evaluate task execution efficiency in navigation and interaction tasks. SPL assesses an agents performance by considering both task success and the path efficiency relative to the expert demonstration. 4.3. Main Results EMBODIEDEVAL Reveals Limitations of Current MLLMs on Embodied Tasks. As shown in Table 2, Success rates across various models on EMBODIEDEVAL remain notably low. The best-performing model, GPT-4o, achieves only 25.00% overall success rate and 32.42% This is in stark contrast with non-expert GcS score. humans, who reach nearly perfect success rate of 97.26%, highlighting the difficulty these models face in executing embodied tasks that humans find trivial. This performance deficit is further underscored by significantly lower SPL scores, indicating that the models struggle to find optimal solutions. This shows the challenges for deploying MLLMs Model Random Human GPT-4o [68] GPT-4o-Mini [68] Gemini-Pro [29] Gemini-Flash [29] Qwen-VL-Max [5] Qwen-VL-Plus [5] InternVL2-40B [69] InternVL2-8B [69] InternVL2-Llama3-76B [69] LLaVA-NEXT-72B [12] LLaVA-OneVision-72B [52] LLaVA-OneVision-7B [52] VILA-40B [50] VILA-8B [50] LLaVA-NeXT-Video-32B-Qwen [94] LLaVA-Video-72B-Qwen2 [95] LLaVA-Video-7B-Qwen2 [95] Oryx-34B [58] VideoLLaMA2-72B [16] VideoLLaMA2-7B [16] Succ. 11.58 98.95 35.79 31. 27.37 26.32 37.89 10.53 14.74 13. 21.05 23.16 26.32 16.84 17.89 15. 21.05 27.37 20.00 18.95 27.37 21. Attr. QA Spatial QA Navigation Object Interaction Social Interaction Overall Succ. Succ. GcS SPL Succ. GcS SPL Succ. 3.45 8.76 3.45 0.00 6. 0.00 2.94 GcS 8.33 SPL Succ. GcS 2.94 5.49 8.66 7.69 92. 32.69 15.38 9.62 13.46 17.31 11. 5.77 13.46 13.46 5.77 19.23 17. 7.69 9.62 7.69 9.62 19.23 3. 9.62 9.62 96.55 97.84 82.28 97. 99.44 90.73 100.00 100.00 89.96 97. 97.94 Closed-Source Multi-Image MLLMs 31.03 42.53 22.23 10. 27.59 39.51 15.34 17.24 25.86 5. 17.10 9.78 3.51 24.14 30.03 16. 3.45 10.49 3.45 2.25 4.49 2. 7.87 0.00 24.25 17.42 12.36 7. 24.91 2.43 Open-Source Multi-Image MLLMs 6.90 8.62 3. 12.93 18.25 9.48 12.07 22.99 10. 23.28 5.17 0.00 1.72 9.05 5. 8.91 3.06 4.04 2.18 7.83 7. 3.28 0.00 0.96 0.00 0.00 0. 3.37 1.12 1.12 0.00 0.00 Open-Source Video MLLMs 6.90 14.08 15.52 24.28 3.45 5. 4.89 13.07 12.07 18.68 6.90 17. 5.34 9.62 1.88 4.89 6.35 4. 0.00 1.12 1.12 1.12 2.25 0. 7.68 7.43 9.08 9.74 7.81 8. 3.93 3.46 8.61 8.05 8.80 7. 7.49 1.63 5.94 1.50 3.00 0. 5.62 0.00 0.00 0.00 0.00 2. 1.12 0.80 0.00 0.00 0.00 0. 0.27 1.00 1.38 0.00 11.76 5. 5.88 2.94 8.82 2.94 5.88 5. 2.94 0.00 0.00 2.94 0.00 2. 2.94 0.00 0.00 0.00 5.88 2. 26.72 22.06 18.14 12.50 22.06 8. 19.12 18.63 13.73 12.25 12.75 9. 8.58 6.37 12.01 9.80 5.15 8. 10.78 7.35 6.74 2.98 3.44 1. 6.86 1.68 2.16 2.45 1.14 0. 0.00 1.68 0.00 1.68 0.98 0. 0.00 0.00 2.39 1.38 25.00 32. 17.68 25.58 14.33 19.26 11.59 16. 21.04 28.07 5.79 8.31 7.01 8. 9.15 11.54 13.27 13.79 10.67 15. 12.80 18.23 9.14 6.40 6.71 12. 9.53 9.27 8.84 13.39 12.50 16. 9.76 7.32 12.63 11.33 12.81 15. 9.20 11.99 Table 2. Results of different models on EMBODIEDEVAL (%). The best-performing model in each category is bolded. in embodied scenarios. Performance Gap between Best Proprietary and Open-source Models. Proprietary models demonstrate consistent advantage across all tasks and metrics. GPT4o leads in overall performance, with the highest success rate on four out of five tasks. Qwen-VL-Max ranks second, achieving the highest success rate in Attribute QA. In contrast, open-source models show substantial performance gap. The top performing multi-image MLLM, LLaVAOneVision-72B, achieves an overall success rate of 12.80%, barely competitive with proprietary models. The other multi-image MLLMs have accuracy rates of around 10% at best. Among the video MLLMs, VideoLLaMA2-72B achieves the highest success rate (12.81%), while LLaVAVideo-72B-Qwen2 achieves the highest GcS (16.95%). For open-source models, larger models often exhibit improved performance, but this trend is not universally observed. Model Performance across Different Task Types. The results highlight significant variation in model performance across different task types. GPT-4o demonstrates relatively strong results in QA and Navigation tasks, but its performance drops notably for interaction tasks. This disparity is even more pronounced among other commercial models. For instance, most models perform reasonably well in Attribute QA but see sharp decline in Spatial QA that requires spatial reasoning, often halving their success rates. The Navigation task shows substantial performance variability across models, such as Gemini-Flash, which achieves 26.32% in Attribute QA but drops to only 5.17% in Navigation. Overall, the scores for Object Interaction and Social Interaction are consistently lower across all models, underscoring the challenge these models face in scenarios that require deeper understanding of affordance or social cues. Challenges in Long-Horizon Tasks. The performance of MLLMs shows significant decline as the number of steps required for the task and the subgoals increase. As illustrated in Figure 5, models maintain relatively high success counts at lower required steps, but their performance drops fluctuatingly as tasks require longer sequences of actions. This drop in performance can be attributed to two primary factors: (1) the increased complex7 ity and reasoning demands associated with longer tasks and multiple subgoals, which challenge the models planning and decision-making capabilities, and (2) the limitations in retaining and processing the long temporal context necessary for successful execution in extended tasks. These challenges highlight the need for further model improvements to enhance MLLM capabilities in handling complex, multistep objectives over long horizons. planning. This results in random or repetitive actions, such as aimless circling or repeatedly picking up objects. They also struggle to understand the outcomes of the action and adapt after failed attempts. Figure 6 provides illustrative examples of these errors. Figure 5. Success rate vs. number of steps required for the task. Studies on Temperature. We find that all models perform slightly better at temperature = 1 compared to temperature = 0. Through observing cases, we believe this is because embodied tasks require certain level of exploration, and when the temperature is set to 0, the determinism of the output causes the model to easily get stuck in repetitive errors. However, in this paper, we propose using temperature of 0 as the evaluation standard, as this removes randomness from the evaluation, improving efficiency and better reflect the models true capabilities, including its ability to recognize and escape from erroneous trajectories. 4.4. Error Analysis We summarize four primary error categories in MLLMbased embodied agents: (1) Hallucination in Grounding: Models misperceive the environment, hallucinating nonexistent objects or overlooking present ones. For example, models may confidently describe absent items or fail to locate small objects like laptops or keys, impacting both QA (e.g., providing answers based on imagined objects) and non-QA tasks (e.g., failing to navigate to or interact with target objects). (2) Insufficient Exploration: Agents employ suboptimal exploration strategies, hindering information gathering and goal finding due to incomplete environment coverage. They are often trapped in local areas, or answer before fully exploring the environment due to overconfidence. (3) Lack of Spatial Reasoning: Models struggle with understanding spatial relationships. They misinterpret directional instructions (e.g.,to my left) and face difficulties navigating between locations, even for simple tasks such as moving to or around furniture. (4) Wrong Planning: Agents demonstrate poor state estimation and action 8 In HalluciFigure 6. Case study of common error categories. nation in Grounding, the agent mistakenly identified single blue sofa as two. In Insufficient Exploration, the agent failed to look for additional items. In Lack in Spatial Reasoning, the agent misestimated the distance between objects. In Wrong Planning, the agent did not organize the picking up and putting down of the vases in the proper order and at the correct positions. 5. Conclusion In this paper, we propose EMBODIEDEVAL, the first interactive benchmark designed for MLLMs with comprehensive embodied tasks. We provide an efficient framework to interactively evaluate the capabilities of MLLMs on embodied tasks. To ensure the accuracy, diversity, and quality of the dataset, extensive efforts are devoted to the annotation process for each task sample. Through experiments, we found that current MLLMs perform poorly on embodied tasks. However, we believe there will be more attention to improving the embodied capabilities of MLLMs upon the general capabilities learned from universal multimodal data. We hope EMBODIEDEVAL can help and guide the development of MLLMs to realize their potential in embodied intelligence."
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. 1 [2] Peter Anderson and et al. Vln-ce: visually-grounded language navigation dataset for following compositional instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 84708480, 2020. 6 [3] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 6 [4] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real In Proceedings of the IEEE conference on environments. computer vision and pattern recognition, pages 36743683, 2018. 2, 3, 4 [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. 7 [6] Dhruv Batra, Angel Chang, Sonia Chernova, Andrew Davison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra Malik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement: challenge for embodied ai. arXiv preprint arXiv:2011.01975, 2020. [7] Dhruv Batra, Aaron Gokaslan, Aniruddha Kembhavi, Oleksandr Maksymets, Roozbeh Mottaghi, Manolis Savva, Alexander Toshev, and Erik Wijmans. Objectnav revisited: On evaluation of embodied agents navigating to objects. arXiv preprint arXiv:2006.13171, 2020. 6 [8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1445514465, 2024. 2, 3 [9] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2 [10] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, and Baobao Chang. Towards end-to-end embodied decision making via multi-modal large language model: Explorations with gpt4vision and beyond. arXiv preprint arXiv:2310.02071, 2023. 1 [11] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplanbench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2023. [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 6, 7 [13] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv preprint arXiv:2406.01584, 2024. 3 [14] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Can vision-language models think from first-person perspective? arXiv preprint arXiv:2311.15596, 2023. 2 [15] Sijie Cheng, Zhicheng Guo, Jingwen Wu, Kechen Fang, Peng Li, Huaping Liu, and Yang Liu. Egothink: Evaluating first-person perspective thinking capability of visionIn Proceedings of the IEEE/CVF Conlanguage models. ference on Computer Vision and Pattern Recognition, pages 1429114302, 2024. 1 [16] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 7 [17] Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, and Maosong Sun. Legent: Open platform for embodied agents. arXiv preprint arXiv:2404.18243, 2024. [18] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, pages 110, 2018. 2, 3 [19] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35:5982 5994, 2022. 5 [20] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 5, 1 [21] Vishnu Sashank Dorbala, Prasoon Goyal, Robinson Piramuthu, Michael Johnston, Reza Ghanadhan, and Dinesh Manocha. S-eqa: Tackling situational queries in embodied question answering. arXiv preprint arXiv:2405.04732, 2024. 3 [22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palme: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 1 [23] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. arXiv preprint arXiv:2406.05756, 2024. 3 modal expression. In The Twelfth International Conference on Learning Representations, 2023. 3 [24] Chenyou Fan. Egovqa-an egocentric video question answerIn Proceedings of the IEEE/CVF ing benchmark dataset. International Conference on Computer Vision Workshops, pages 00, 2019. 2 [25] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 1, 2 [26] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The firstever comprehensive evaluation of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2 [27] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 1 [28] Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, Daniel LK Yamins, James DiCarlo, Josh McDermott, Antonio Torralba, et al. The threedworld transport challenge: visually guided taskand-motion planning benchmark towards physically realistic embodied ai. In 2022 International conference on robotics and automation (ICRA), pages 88478854. IEEE, 2022. 3 [29] Google. Our next-generation model: Gemini 1.5, 2024. 7 [30] Daniel Gordon, Aniruddha Kembhavi, Mohammad RasteIqa: gari, Joseph Redmon, Dieter Fox, and Ali Farhadi. Visual question answering in interactive environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 40894098, 2018. 3, [31] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level arXiv preprint bilingual multimodal scientific problems. arXiv:2402.14008, 2024. 2 [32] Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, Yue Zhao, Haoye Zhang, et al. Large multilingual models pivot zeroshot multimodal learning across languages. arXiv preprint arXiv:2308.12038, 2023. 2 [33] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 2 [35] Md Mofijul Islam, Alexi Gladstone, Riashat Islam, and Tariq Iqbal. EQA-MX: Embodied question answering using multimodal expression. In The Twelfth International Conference on Learning Representations, 2024. 2 [36] Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay on the path: Instruction fidelity in vision-and-language navigation. arXiv preprint arXiv:1905.12255, 2019. 2, 4 [37] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats up with vision-language models? investigating their struggle with spatial reasoning. arXiv preprint arXiv:2310.19785, 2023. [38] Yash Kant, Arun Ramachandran, Sriram Yenamandra, Igor Gilitschenski, Dhruv Batra, Andrew Szot, and Harsh Agrawal. Housekeep: Tidying virtual households using commonsense reasoning. In European Conference on Computer Vision, pages 355373. Springer, 2022. 3 [39] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1638416393, 2024. 5 [40] Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra, Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv Batra, and Roozbeh Mottaghi. Goat-bench: benchmark for multi-modal lifelong In Proceedings of the IEEE/CVF Conference navigation. on Computer Vision and Pattern Recognition, pages 16373 16383, 2024. 3 [41] Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, and Jonghyun Choi. Context-aware planning and environment-aware memory for instruction following emIn Proceedings of the IEEE/CVF Internabodied agents. tional Conference on Computer Vision, pages 1093610946, 2023. 6 [42] Taewoong Kim, Cheolhong Min, Byeonghwi Kim, Jinyeon Kim, Wonje Jeung, and Jonghyun Choi. Realfred: An embodied instruction following benchmark in photo-realistic environments. arXiv preprint arXiv:2407.18550, 2024. 3 [43] Eric Kolve and et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. 5, 6 [44] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal grounding. arXiv preprint arXiv:2010.07954, 2020. [45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 1 [34] Md Mofijul Islam, Alexi Gladstone, Riashat Islam, and Tariq Iqbal. Eqa-mx: Embodied question answering using multi- [46] Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martın-Martın, Chen Wang, 10 Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 8093. PMLR, 2023. 3 [47] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [48] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 2 [49] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 2 [50] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 7 [51] Fangyu Liu, Guy Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the Association for Computational Linguistics, 11:635651, 2023. 3 [52] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 7 [53] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2 [54] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [55] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating LLMs as agents. In The Twelfth International Conference on Learning Representations, 2024. 6 [56] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 1, 2 [57] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. 2 [58] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 7 [59] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning In The via thought chains for science question answering. 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 2 [60] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. [61] Ji Ma, Hongming Dai, Yao Mu, Pengying Wu, Hao Wang, Xiaowei Chi, Yang Fei, Shanghang Zhang, and Chang Liu. Doze: dataset for open-vocabulary zero-shot object navigation in dynamic environments. arXiv preprint arXiv:2402.19007, 2024. 3 [62] Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, and Irwin King. survey on vision-language-action models for embodied ai. arXiv preprint arXiv:2405.14093, 2024. 1 [63] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16488 16498, 2024. 1, 2 [64] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947 952. IEEE, 2019. 2 [65] Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi. Mapping instructions to actions in 3d environments with visual goal prediction. arXiv preprint arXiv:1809.00786, 2018. 3, 4 [66] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. Advances in Neural Information Processing Systems, 36, 2024. 1 [67] OpenAI. Gpt-4v(ision) system card, 2023. 1 [68] OpenAI. Hello gpt4-o, 2024. 7 [69] OpenGVLab. Internvl2: Better than the bestexpanding performance boundaries of open-source multimodal models with the progressive scaling strategy, 2024. 7 [70] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2 [71] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring exIn Proceedings of pression in real indoor environments. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99829991, 2020. 2 [72] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den 11 Hengel. Reverie: Remote embodied visual referring expression in real indoor environments, 2020. [73] Allen Ren, Jaden Clark, Anushri Dixit, Masha Itkina, Anirudha Majumdar, and Dorsa Sadigh. Explore until confident: Efficient exploration for embodied question answering. arXiv preprint arXiv:2403.15941, 2024. 3 [74] Manolis Savva and et al. Habitat challenge: photorealIn Proceedings of the IEEE istic embodied ai benchmark. International Conference on Computer Vision (ICCV), pages 93389346. IEEE, 2019. 6 [75] Manasi Sharma. Exploring and improving the spatial reasoning abilities of large language models. In Cant Believe Its Not Better Workshop: Failure Modes in the Age of Foundation Models, 2023. 3 [76] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. 2, 3, 4, 6 [77] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cˆote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020. 2 [78] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. [79] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martın-Martın, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on robot learning, pages 477490. PMLR, 2022. 2, 3, 4 [80] Hai-Long Sun, Da-Wei Zhou, Yang Li, Shiyin Lu, Chao Yi, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, DeChuan Zhan, et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539, 2024. 2 [81] Sinan Tan, Mengmeng Ge, Di Guo, Huaping Liu, and Fuchun Sun. Knowledge-based embodied question answerIEEE Transactions on Pattern Analysis and Machine ing. Intelligence, 45(10):1194811960, 2023. 2, 3 [82] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 [83] Luca Weihs, Matt Deitke, Aniruddha Kembhavi, and Roozbeh Mottaghi. Visual room rearrangement. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59225931, 2021. 3, 4 [84] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. arXiv preprint arXiv:2310.08588, 2023. 1 [85] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, Chris Callison-Burch, Mark Yatskar, Aniruddha Kembhavi, and Christopher Clark. Holodeck: Language guided generation of 3d embodied ai environments, 2024. [86] Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, et al. Homerobot: Open-vocabulary mobile manipulation. arXiv preprint arXiv:2306.11565, 2023. 3, 4 [87] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 2 [88] Licheng Yu, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara Berg, and Dhruv Batra. Multi-target embodied question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63096318, 2019. 2, 3 [89] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13807 13816, 2024. 2 [90] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 2 [91] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 1, [92] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 6 [93] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024. 2 [94] Yuanhan Zhang, Bo Li, Haotian Liu, Yong Jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llavanext: strong zero-shot video understanding model, 2024. 7 12 [95] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 7 [96] Zhuofan Zhang, Ziyu Zhu, Pengxiang Li, Tengyu Liu, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Siyuan Huang, and Qing Li. Task-oriented sequential grounding in 3d scenes. arXiv preprint arXiv:2408.04034, 2024. [97] Kaizhi Zheng, Xiaotong Chen, Odest Chadwicke Jenkins, and Xin Wang. Vlmbench: compositional benchmark for vision-and-language manipulation. Advances in Neural Information Processing Systems, 35:665678, 2022. 6 [98] Fengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiaojun Chang, and Xiaodan Liang. Soon: Scenario oriented object navigation with graph-based exploration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1268912699, 2021. 3 [99] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. 1 13 Object Selection. We curated subset of indoor assets out of Holodecks [85] annotated realistic and diverse objects choosen from the Objaverse asset library [20]. To ensure quality, we employed GPT-3.5 to filter unsuitable outdoor objects and manually reviewed frontal renderings to remove low-quality assets. This process resulted in database of about 15,000 objects spanning over 500 categories (see examples in Figure 9). Scene Generation. We leveraged GPT-3.5 to annotate object categories with their typical room occurrences (e.g., inLivingRoom, inKitchen), positions (e.g., onWall, onFloor, onEdge), and functions (e.g., receptacle, pickup). Gemini-1.5-Flash was used to annotate large objects orientations. Subsequently, procedural approach was employed to randomly place architectural elements such as walls, doors, and windows. Large objects were then arranged on the floor either against the walls or in the center of the rooms, and smaller items were finally placed on surfaces of large receptacles. Hundreds of scenes were generated randomly, from which we selected 15 living rooms, 15 bedrooms, 10 two-room, 5 three-room, and 5 four-room for further editing. Figure 7. Interactive scene editor: adjust object position (left) and angle (right). Scene Editing. To make the scene more organized and to avoid errors caused by automatic generation, we also edited the generated scene by developing runtime scene editor. Users can view the type and description of objects, and adjust their position and orientation (see Fig 7). Once editing is complete, the scene can be saved as JSON file, which can be imported to reproduce the environment. 6. Appendix 6.1. Task Samples We selected some representative examples to illustrate the diversity of the task set in Table 3. 6.2. Details of Evaluation Framework"
        },
        {
            "title": "6.2.1 Evaluation Formulation",
            "content": "We formalize the evaluation process mentioned in Section 3.2.3 as follows. The prompt we used for the MLLMs during the evaluation is shown in Figure 8. Algorithm 1 EMBODIEDEVAL Evaluation Process Input: Multimodal LLM π, an evaluation task including scene x, task description g, an option list = a0, a1, ..., an, and predicate list P. Output: boolean indicating whether the task was successful success. 1: o, E.reset(x) is the simulator, is the observed image, is the world state observation history action history 2: Ho {o} 3: Ha 4: for 0 to max steps do 5: 6: 7: π.predict(g, C, Ho, Ha) o, E.step(a) Ho.append(o) Ha.append(a) done P.judge(s) if done then return true else if is answer action then return false wrong answer end if 14: 15: end for 16: return false reach the max steps"
        },
        {
            "title": "6.2.2 Interaction Actions",
            "content": "We provided more examples of the interaction space mentioned in Section 3.2.1 in Table 5."
        },
        {
            "title": "6.2.3 Predicates",
            "content": "All the predicate functions described in Section 3.2.2 are listed in Table 4. 6.3. Creation of Objaverse Synthetic We use wide variety of objects from Objaverse to procedurally generate diverse scenes and further refine them through interactive scene editing. 1 8: 9: 10: 11: 12: 13:"
        },
        {
            "title": "Task",
            "content": "Please go to the kitchen, then come back and tell me if there are any extra cups. Imagine the house is rotated 90 degrees counterclockwise. How would this affect the natural light distribution in the room? Open black locked drawer with key found on the desk. Pick up the kettle and the box labeled BREAD from the kitchen counter and place them on the table with the coffee machine. Optimize the display of artworks on the shelves as follows: place two items on each shelf, with one shelf featuring two items of the same shape. Complete the requirements in as few steps as possible."
        },
        {
            "title": "Characteristics",
            "content": "scene memory spatial imagination tool use optical character recognition reasoning and planning Grab the object that is cylindrical and silver on the table next to the washing machine. multiple attribute reference Estimate the percentage of floor space occupied by furniture in the room youre currently in. area estimation Estimate the straight-line distance from the front door to the TV. Note that each step you take forward is approximately two meters. distance estimation Which is closer to the drink on the round table, the ginger or the ice cream? distance comparison If we were to host birthday party, which area of the house could accommodate the most people while ensuring clear pathways to exits? logic, space, and common sense Describe the path from the kitchen to the living room. If you were to draw straight line from the desk with turned-on laptop to the bookshelf, which pieces of furniture would it intersect? path description spatial reasoning What is the object am pointing at? Pick up the watermelon on my right. pointing comprehension perspective-taking comprehension My red glasses are missing. Please help me look for them in the room. Once you find them, bring them to me. object searching and delivering Get close to the lady in white and ask if she needs help. Wake up my dad. He is sleeping in the bedroom. The bedroom is the second room on your right as you walk forward. Enter the dining area and see if there is more than one door in the entire house. social navigation finding someone object counting Calculate the ratio of seating options to the number of rooms in the house. counting and calculation Tell me which objects have handle in the kitchen. Evaluate whether the painting above the living room sofa is more colorful than the carpet. How many rooms are there in total? Confirm if garbage can is located on the floor in the living room. Which room has more seating options, the kitchen or the living room? Im hungry. Find all objects that can be used as ingredients. on the table in this room. attribute grounding attribute comparison room counting object existence quantity comparison object functionality Count the maximum number of identical clocks among all the rooms. counting and attribute memory What do you think the owner of this room probably studies? Is there an egg inside the fridge? Open the drawer of the side table in the study room. If there is something inside, leave it open and put all similar items from the room into it. If there is nothing inside, close it. common sense interaction and answering logical execution Table 3. Examples of the diverse tasks in EMBODIEDEVAL."
        },
        {
            "title": "Success Conditions",
            "content": "choose agent at agent pass at grab once grab special action success The right answer. navigation point. navigation point. An object and specific point. An object. An object. An interaction action. When the agent selects the correct answer. When the agent finally arrives at this point. When the agent has passed through this point at least once. When the object is at this point. When the agent has picked up this object at least once. When the agent picks up the object. When this interaction action has been successful. Table 4. The predicates involved in EMBODIEDEVAL."
        },
        {
            "title": "Execution Requirements",
            "content": "wash hand over sit down unlock greet ask mix wipe off the table check the results of the program When the agent is holding the target object and stand next to the sink. When the agent is holding the target object and stand next to the person. When the agent is next to the target chair. When the agent is holding the target key and standing next to the drawer When the agent is near the person. When the agent is near the person. When several target beverages are on the table next to the agent. When the agent is holding an object for cleaning and standing next to the table. When the agent is next to the computer. Table 5. Some cases of the interaction actions involved in EMBODIEDEVAL. Prompt for Multi-image MLLMs You are an intelligent vision-language embodied agent skilled at solving tasks and answering questions in 3D environment. Your job is to efficiently complete specified task by choosing the optimal action at each timestep from set of available actions. You are given series of ego-centric images, and history of previous actions with optional feedback (success/failure or human response). Each image shows what you see at particular step in the action history, along with an extra image showing your current view. Current task: {} Action history (action -> feedback): {} Visual history: {} Current view: {} For the current step, your available options are listed as \"[Option Number]. Content\" as follows : {} Choose your action from the above options by replying with \"Thought: Your reasoning.nChoice: [ Option Number] (e.g. [1])\". Note: - If the task needs more information of the scene, navigate wisely to the required targets ( objects, places, or people). - Avoid repeated actions like useless forward motion and circling. - You can only interact with objects or humans (e.g. pick/place/open/close/handover) if they are within your view and very close to you. - You can only hold one object at time. Put down any held object before picking up another. - Tasks containing \"I\" or \"me\" are requested by person in the scene. - Reflect on why previous actions fail to avoid repeating mistakes and ajdust your current action. - You have limited number of {} steps to complete the task. Figure 8. Prompt for Multi-image MLLMs. 3 Figure 9. Examples of selected Objaverse assets and views of generated scenes. 6.4. Success Cases We present successful cases accomplished by closed-source MLLMs to gain deeper insights into their current capabilities. As discussed in Section 4.3, the models generally scored low and successfully completed only limited number of tasks. closer examination of these successful tasks reveals that they are typically simpler, involve fewer steps and require interaction with fewer objects. To better illustrate these findings, we present representative cases from five task types. These examples highlight the underlying behavioral patterns and reasoning processes of the models during task execution."
        },
        {
            "title": "6.4.1 Attribute QA",
            "content": "Figure 10 4 Figure 11 Figure"
        },
        {
            "title": "6.4.2 Spatial QA",
            "content": "Figure 13 Figure"
        },
        {
            "title": "6.4.3 Navigation",
            "content": "Figure 15 Figure"
        },
        {
            "title": "6.4.4 Object Interaction",
            "content": "Figure 17 Figure"
        },
        {
            "title": "6.4.5 Social Interaction",
            "content": "Figure 19 Figure 20 9 6.5. Error Cases"
        },
        {
            "title": "6.5.1 Attribute QA",
            "content": "Figure 21 Figure"
        },
        {
            "title": "6.5.3 Navigation",
            "content": "Figure 23 Figure"
        },
        {
            "title": "6.5.4 Object Interaction",
            "content": "Figure"
        },
        {
            "title": "6.5.5 Social Interaction",
            "content": "Figure"
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}