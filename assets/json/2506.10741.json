{
    "paper_title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a Unified Framework",
    "authors": [
        "SiXiang Chen",
        "Jianyu Lai",
        "Jialin Gao",
        "Tian Ye",
        "Haoyu Chen",
        "Hengyu Shi",
        "Shitong Shao",
        "Yunlong Lin",
        "Song Fei",
        "Zhaohu Xing",
        "Yeying Jin",
        "Junfeng Luo",
        "Xiaoming Wei",
        "Lei Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, a unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs a carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised fine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via best-of-n preference optimization; and (iv) joint vision-language feedback refinement. Each stage is supported by a fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appeal-approaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft"
        },
        {
            "title": "Start",
            "content": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework SIXIANG CHEN1,2,, JIANYU LAI1,, JIALIN GAO2,, TIAN YE1, HAOYU CHEN1, HENGYU SHI2, SHITONG SHAO1, YUNLONG LIN3, SONG FEI1, ZHAOHU XING1, YEYING JIN4, JUNFENG LUO2, XIAOMING WEI2, LEI ZHU1,5, 1 The Hong Kong University of Science and Technology (Guangzhou), 2 Meituan, 3 Xiamen University, 4 National University of Singapore, 5 The Hong Kong University of Science and Technology Equal Contribution; Corresponding Author 5 2 0 2 2 1 ] . [ 1 1 4 7 0 1 . 6 0 5 2 : r Fig. 1. Aesthetic posters generated by PosterCraft demonstrate that backgrounds, layouts, and typographic designs are produced directly from textual input without modular designs, highlighting its ability to employ unified framework to generate posters with visual consistency and compelling aesthetic appeal. Generating aesthetic posters is more challenging than simple design images: it requires not only precise text rendering but also the seamless integration of abstract artistic content, striking layouts, and overall stylistic harmony. To address this, we propose PosterCraft, unified framework that abandons prior modular pipelines and rigid, predefined layouts, allowing the model to freely explore coherent, visually compelling compositions. PosterCraft employs carefully designed, cascaded workflow to optimize the generation of high-aesthetic posters: (i) large-scale text-rendering optimization on our newly introduced Text-Render-2M dataset; (ii) region-aware supervised finetuning on HQ-Poster-100K; (iii) aesthetic-text reinforcement learning via best-of-n preference optimization; and (iv) joint visionlanguage feedback refinement. Each stage is supported by fully automated data-construction pipeline tailored to its specific needs, enabling robust training without complex architectural modifications. Evaluated on multiple experiments, PosterCraft significantly outperforms open-source baselines in rendering accuracy, layout coherence, and overall visual appealapproaching the quality of SOTA commercial systems. Our code, models, and datasets can be found in the Project page: https://ephemeral182.github.io/PosterCraft/ Authors Contact Information: Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, Xiaoming Wei2, Lei Zhu1,5,, 1 The Hong Kong University of Science and Technology (Guangzhou), 2 Meituan, 3 Xiamen University, 4 National University of Singapore, 5 The Hong Kong University of Science and Technology Equal Contribution; Corresponding Author. Additional Key Words and Phrases: Aesthetic Poster Generation, Unified Framework, Specific Large-scale Data"
        },
        {
            "title": "1\nDespite recent advances in automated visual design, aesthetic poster\ngeneration remains a formidable challenge and is still relatively un-\nderexplored. Existing generative approaches primarily focus on\nfoundational tasks such as text rendering [3, 4, 18, 31] or the cre-\nation of specific product-oriented posters [8, 22, 34], offering limited\ncapacity to produce high-quality, aesthetically compelling outputs.\nThese methods often fall short of addressing the multifaceted de-\nmands of aesthetic poster design, which requires not only (i) ac-\ncurate and stylistically coherent text, but also (ii) the creation of\nabstract and visually appealing artistic content and (iii) the striking\nlayouts and holistic stylistic consistency. Therefore, aesthetic poster\ngeneration demands a more comprehensive synthesis of content,\nform, and communicative intent.",
            "content": "Recent approaches to aesthetic poster generation [2, 25, 28, 40] have primarily followed modular design paradigm. Typically, fine-tuned vision-language model (VLM) acts as layout planner, suggesting text content and positioning. The suggestions are then overlaid onto separately generated background, or used as hard constraints for the generative model to follow. However, this design strategy presents several limitations. (i.) Lack of aesthetic Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 2 Xiaoming Wei2, Lei Zhu1,5, consistency: it undermines the visual and stylistic coherence essential for aesthetic poster creation. (ii.) Limited visual quality: it constrains the upper bound of visual quality due to the decoupled design process and heavy reliance on the VLMs accuracy and robustness. In contrast, existing end-to-end design-centered generation approaches [3, 8, 11, 34] remain limited to relatively simple tasks, such as greeting cards or product compositions, which lack the visual and structural complexity of high-quality aesthetic posters. Additionally, while powerful foundation models [1, 6, 9] have demonstrated impressive capabilities in generating complex natural images, they still fall short of meeting all the specific requirements of aesthetic posters. (e.g. precise text rendering, abstract artistic content, and holistic stylistic coherence). For this, we classify it as (iii.) Simplified use cases. More importantly, the absence of large-scale, versatile datasets tailored specifically for aesthetic poster generation has further constrained the development of fully generative solutions(iv.) Absence of targeted datasets. To move beyond the limitations of current modular and simply scoped generative paradigms, we leverage the capabilities of foundation models to explore unified generation for aesthetic posters, aiming to produce visually coherent and artistically compelling results. In this work, we argue that incremental, component-level improvements alone are insufficient to achieve significant aesthetic gains. Instead, we propose unified framework, PosterCraft, which includes comprehensive workflow to systematically perform four critical stages: (i.) scalable text rendering optimization, (ii.) highquality poster fine-tuning, (iii.) aesthetic-text reinforcement learning, and (iv.) vision-language feedback refinement. To support this workflow, we construct suite of specialized datasets for each stage through automated pipelines, enabling robust training and facilitating future research in aesthetic poster generation. This framework empowers the trained model to generate high-quality posters in the end-to-end pass. Experiments demonstrate that our approach significantly outperforms existing baselines and achieves competitive results compared to several closed-source models. Overall, our contributions can be summarized as follows: unified framework for aesthetic poster generation: We revisit aesthetic poster generation through an end-toend approach tailored for high-quality, visually coherent posters, surpassing prior modular pipelines and methods focused on simpler or product-centric designs. cascade workflow for high-quality poster optimization: We propose unified training pipeline with four stages: (i) scalable text rendering optimization, (ii) high-quality poster fine-tuning, (iii) aesthetic-text reinforcement learning, and (iv) vision-language feedback refinement. Each stage targets key challenge in aesthetic poster generation, enabling the model to produce artistically compelling results at inference time. Stage-specific, fully automated dataset construction: We construct specialized datasets for each workflow stage using automated collection and filtering, tailored to the unique demands of aesthetic poster generation. These datasets overcome the limitations of resources and support more robust, transferable training. Superior performance over existing baselines: Extensive experiments show that our method significantly outperforms open-source baselines in terms of both aesthetic quality and layout structure, and achieves competitive performance compared to commercial systems."
        },
        {
            "title": "3 Unified Workflow and Specific Dataset\nIn this work, we rethink aesthetic poster generation: a high-capacity\nmodel optimized through a unified workflow can directly produce\nhigh-quality, fully rendered posters without modular design. This\nenables holistic integration of textual content, visual elements, and\nlayout considerations, while leveraging vision-language feedback\nduring inference to achieve greater coherence and heightened aes-\nthetic appeal. Unlike prior methods that impose explicit layout\nembeddings [3, 8, 41] or depend on external VLM-based design-\ners [2, 25, 40] (i.e. inherently restrict a modelâ€™s expressive freedom),",
            "content": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 3 Fig. 2. The four datasets of PosterCraft across its four stages: (1) Text-Render-2M for text rendering optimization in the initial phase, (2) HQ-Poster-100K, comprising over 100K high-quality posters with masks and captions, (3) Poster-Preference-100K, yielding 6K high-quality preference pairs from 100K generated samples, and (4) Poster-Reflect-120K, constructing 64K feedback pairs from 120K generated posters. we unlock the potential of standard diffusion backbone via comprehensive workflow optimization rather than intricate architectural modifications. Therefore, our paradigm remains fully compatible with existing techniques and offers flexible foundation for future advances. Fig.4 illustrates our unified optimization workflow, and Fig.2 details the dataset construction pipeline supporting each stage in our unified framework."
        },
        {
            "title": "3.1 Scalable Text Rendering Optimization\nIn the first stage of our workflow, we target the challenge of ac-\ncurate text rendering, a persistent bottleneck in poster generation.\nProgress is hindered by two factors: (i) the scarcity of large-scale,",
            "content": "high-quality datasets with perfectly rendered text, and (ii) most available text datasets feature plain or low-quality backgrounds, which easily make the model lose the ability to represent common backgrounds. To overcome these issues, we construct Text-Render2M via an automated pipeline, producing 2 million samples with diverse text (varying in content, size, count, placement, and rotation) rendered crisply onto high-quality backgrounds. Each text instance is paired with precise captions merged seamlessly with existing image captions. This dataset ensures both 100% text rendering accuracy and rich background diversity, enhancing fidelity and robustness in real-world scenarios. Fig.2 illustrates Text-Render-2M, with construction details and examples provided in the supplementary. We Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 4 Xiaoming Wei2, Lei Zhu1,5, Fig. 3. Comparison of text rendering on poster typography, plain-text scenes, and long-form text posters. Each pair shows the Flux.1 dev baseline (left), exhibiting missing, repeated, or error text, alongside the optimized output (right) after our scalable text rendering optimization, demonstrating marked gains in text fidelity, alignment, and accuracy. then fine-tune foundation models on paired Text-Render-2M using the flow matching loss [6, 9] to enhance text rendering: Ltext flow (ğœ™) = Eğ‘¡ (0,1), ğ‘¥0, ğœ€ 2 (cid:13) ğ‘£ğœ™ (ğ‘¥ğ‘¡ , ğ‘¡) (cid:164)ğ‘¥ğ‘¡ (cid:13) (cid:13) 2, (cid:13) (1) where ğ‘¥ğ‘¡ = ğ›¼ğ‘¡ ğ‘¥0 + ğœğ‘¡ ğœ€ follows the forward noising schedule, (cid:164)ğ‘¥ğ‘¡ is its time derivative, and ğ‘£ğœ™ predicts the velocity field. Discretized over timesteps, this loss encourages the model to match the true data flow and yields markedly improved text rendering. As shown in Fig.3, the model augmented with our text-rendering optimization achieves significant gains in both rendering accuracy and text alignment across poster typography, plain-text scenes, and long-text posters."
        },
        {
            "title": "3.2 High-quality Poster Fine-tuning\nHQ-Poster-100K. To develop a high-quality dataset for supervised\nfine-tuning, we introduce HQ-Poster-100K, a meticulously filtered\nposter dataset. Our filtering pipeline begins with MD5 hash calcula-\ntions to eliminate exact duplicates. To avoid including posters with\nextensive credit/billing blocks, we implement an MLLM-based scor-\ning system. This system presents a binary choice question regarding\nthe presence of credit/billing blocks, with the model outputting\nlogits that are transformed through Softmax to obtain probabilistic\nscores. We calculate the score based on the following formula, where\nscores closer to 1 indicate better alignment with our criteria:",
            "content": "ğ‘ğ‘Ÿğ‘œğ‘ğ‘¥ = ğ‘’ğ‘™ğ‘¥ (cid:205)ğ‘¥ ğ¿ ğ‘’ğ‘™ğ‘¥ , ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ = ğ‘¥ ğ¿ ğ‘ğ‘Ÿğ‘œğ‘ğ‘¥ ğ‘¤ğ‘¥ (2) Where set ğ¿ contains all the option letters, ğ‘™ğ‘¥ represents the logit for option ğ‘¥. Here we only consider two-choice questions, that is, the case where there are only ğ´ and ğµ in ğ¿; the weight ğ‘¤ğ´ is set to 0 and ğ‘¤ğµ is set to 1. In our filtering pipeline, we utilize InternVL2.58B-MPO [12] for logits computation with threshold of 0.98, allowing precise control over filtering stringency. We further refine the dataset using perceptual hashing to remove visually similar posters. The remaining posters are captioned using Gemini2.5-Flash and subjected to HPS scoring, with posters scoring below 0.25 being filtered out to ensure aesthetic quality alignment with human preferences. To support Region-aware Calibration, HQ-Poster-100K also provides precise text region masks for each poster. Due to the limitations of traditional OCR in recognizing artistic typography, we use Gemini2.5-Flash to extract text region coordinates. Based on their proportional size relative to the poster, masks are classified as Major or Minor, denoting large and small text areas respectively, as shown in Figure2. More details can be found in our supplementary material. Region-aware Calibration. In poster design, harmony between text and background is crucial. Since our first stage has already improved the text rendering capability of the model, this fine-tuning PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 5 Fig. 4. The pipeline of PosterCraft, which has four stages: (1) Text Rendering Optimization to improve text accuracy and fidelity; (2) High-Quality Poster Fine-Tuning with region-aware calibration to poster styling across text and non-text regions; (3) Aesthetic-Text Reinforcement Learning to instill detailed aesthetic and content preferences; and (4) Joint VisionLanguage Feedback, integrating multimodal reflections for refined outputs. At inference, the fine-tuned model generates high-quality aesthetic posters end-to-end from single prompt, with an optional VLM-driven critique loop. phase shifts the focus to overall poster style. Therefore, we propose Region-aware Calibration to achieve this purpose. Specifically, essential text carries the core message and is assigned moderate weight to ensure clarity and integration with the background; by contrast, small textoccupying minimal space and prone to rendering errorsis downweighted to prevent distracting collapse. Nontext regions, which define the visual style of the poster, receive full emphasis to guarantee smooth transition from high-quality imagery to unified aesthetic layout. This balanced weighting strategy allows the fine-tuned model to preserve text accuracy while strengthening the artistic integrity of the aesthetic poster. We implement this via per-pixel weight map ğ‘¤ (ğ‘): ğ‘¤ (ğ‘) = if ğ‘ LargeTextMask, if ğ‘ SmallTextMask, 0.6 0.2 1.0 otherwise, (3) and define our weighted loss as: (cid:0)ğ‘£ğœ™ (ğ‘¥ğ‘¡ , ğ‘¡) (cid:164)ğ‘¥ğ‘¡ (cid:1) ğ‘¤ poster flow = Eğ‘¡ (0,1), ğ‘¥0, ğœ€ (cid:13) (cid:13) (cid:13) where \"\" denotes point-wise multiplication by the weight map ğ‘¤. Different from previous scalable text rendering optimization, here we multiply the scaling factor ğ‘¤, which encourages the model to learn both crisp text information and cohesive aesthetic style. (4) , 2 (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "3.3 Aesthetic-Text Reinforcement Learning\nPoster-Preference-100K. To further enhance poster generation\naesthetics and text rendering capabilities, we develop the Poster-\nPreference-100K dataset. We use about 20K prompts and generate 5",
            "content": "images for each prompt using the model after Region-aware calibration, for total of 100K poster images, which is the basic data source for us to construct preference pairs. Using HPSv2 [36], we evaluate 5 generated posters per group for human preference scoring, selecting the highest-scoring poster as the preferred sample and the lowest-scoring one as the rejected sample to form preference pairs. Since HPSv2 only evaluates image content and aesthetics, we employ Gemini2.5-Flash to verify text rendering accuracy and style consistency with captions in the preferred posters, filtering out inconsistent pairs. This process yields 6K preference pairs meeting two criteria: 1) an HPSv2 score difference exceeding 0.025, and 2) complete text accuracy in preferred posters. AestheticText Preference Optimization. While the preceding stages guarantee pixel-level text fidelity and precisely calibrated poster styles, they fail to capture the higher-order trade-offs that render poster genuinely compelling. In particular, (i) detailed preferences, such as subtle layout balance, color harmony, and typographic cohesion, which require global evaluation beyond per-pixel accuracy; (ii) even after achieving crisp text rendering, further corrective tuning is necessary to alleviate residual errors and seamlessly integrate text with the holistic aesthetic. To address these gaps, we frame poster generation as reinforcement learning problem in this stage: the model must not only denoise accurately but also preferentially generate outputs that satisfy holistic aesthetic criteria. Concretely, for each prompt, we sample ğ‘› poster variants {ğ‘¥ (ğ‘– ) }ğ‘› ğ‘–=1 under the current diffusion policy and collapse them into single winning and losing pair via best-of-ğ‘› selection under the combined aesthetictext reward ğ‘…(ğ‘¥): ğ‘¥ + = arg max ğ‘– ğ‘… (cid:0)ğ‘¥ (ğ‘– ) (cid:1), ğ‘¥ = arg min ğ‘– ğ‘… (cid:0)ğ‘¥ (ğ‘– ) (cid:1). (5) Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 6 Xiaoming Wei2, Lei Zhu1,5, We then optimize the Direct Preference Optimization (DPO) [23] objective: LRL (ğœƒ ) = Eğ‘ (cid:16) (cid:104)log ğœ ğ›½ (cid:16)log ğ‘ğœƒ (ğ‘¥ + ğ‘) ğ‘ref (ğ‘¥ + ğ‘) log ğ‘ğœƒ (ğ‘¥ ğ‘) ğ‘ref (ğ‘¥ ğ‘) (cid:17)(cid:17)(cid:105) , (6) where ğ‘ref denotes the fixed reference distribution, and ğ‘ğœƒ is learned diffusion policy parameterized by ğœƒ . Because the marginal ğ‘ğœƒ (ğ‘¥0 ğ‘) is intractable, we employ the ELBO over the full diffusion chain to evaluate these log-ratio rewards, following prior work [33, 34]. In this way, best-of-ğ‘› AestheticText Preference Optimization directly injects unified preference signal into the diffusion training process."
        },
        {
            "title": "3.4 Vision-language Feedback Refinement.\nPoster-Reflect-120K. To address potential deficiencies in content\nand aesthetic quality of initially generated posters, we implement\nreflection optimization to enhance accuracy and aesthetic value. We\nintroduce the Poster-Reflect-120K dataset, generating six posters\nfor each prompt using our preference-learned model, totaling 120K\ngenerated poster images. Subsequently, we employ Gemini2.5-Flash\nto select the optimal poster from each set of six as the target image\nfor feedback learning. This target image is required to have optimal\nprompt alignment, superior aesthetic value, and fully correct text\nrendering. This process yields 5 reflection-pairs from each set of 6\ngenerated images.",
            "content": "During the feedback collection phase, we gather suggestions in two key areas: Poster Content Suggestions and Aesthetic Style Optimization Suggestions, with Gemini2.5-Flash analyzing both the target poster and the poster requiring optimization to provide comprehensive feedback. To optimize our prompting strategy for both feedback and VLM fine-tuning, we implement specific guidelines: the model is instructed to perform internal comparisons without explicitly referencing the second reference poster, and feedback is structured as concrete editing instructions. Reflect VLM fine-tuning. To obtain optimization feedback during inference, we construct VQA samples by embedding the original caption in the prompt alongside the generated poster requiring optimization, and using Gemini2.5-Flash-generated feedback as supervision. Specifically, this input configuration maintains consistency between training and inference phases, excluding reference target posters in both cases and using the original prompt as the baseline for suggestions. Additionally, when generating feedback with Gemini2.5-Flash, we deliberately utilize only target posters as references, omitting original captions to preserve model creativity. Joint VisionLanguage Conditioning. For the poster construction, iterative critiquecombining visual inspection with targeted verbal feedbackis essential for refining both aesthetic content and background harmony. Inspired by this, we introduce joint visionlanguage feedback loop for multimodal corrections in unified workflow. For each generatedground-truth pair, Gemini produces two textual reflections, ğ‘“ğ‘ (Poster Content Suggestions) and ğ‘“ğ‘  (Aesthetic Style Suggestions). Rather than appending these strings to the original promptwhich would exceed the encoders length and degrade performancewe jointly encode them via text encoder ğ¸ğ‘¡ , yielding ğ‘’ğ‘,ğ‘  = ğ¸ğ‘¡ (ğ‘“ğ‘, ğ‘“ğ‘  ), and then concatenate this with the original prompt embedding ğ‘’ğ‘ (with positional encodings to preserve order Fig. 5. User study comparisons between PosterCraft and both SOTA opensource and closed-source models. PosterCraft consistently outperforms all open-source baselines and several proprietary systems cross multiple dimensions, with performance marginally below that of the leading closedsource model, Gemini2.0-Flash-Gen. and semantics). Additionally, drawing on OmniControl [27], we inject the image-level feedback signal ğ‘£img (encoded by VAE) directly into the conditioning branch. The resulting multimodal context is: ğ‘ = [ ğ‘’ğ‘ ; ğ‘’ğ‘,ğ‘  ; ğ‘£img ] (7) where ğ‘ serves as the conditioning input. Finally, we fine-tune the model under the conditional flow matching objective: flow (ğœƒ ) = Eğ‘¡,ğ‘¥0,ğœ€ ğ‘£ğœƒ (ğ‘¥ğ‘¡ , ğ‘¡ ğ‘) (cid:164)ğ‘¥ğ‘¡ 2 Lğ‘‰ ğ¿ 2, which enables the model to iteratively refine its outputs in response to structured textual reflections and semantically enriched visual feedback. (8)"
        },
        {
            "title": "4 Experiments\n4.1\nFor PosterCraft, we initialize from the Flux-dev [9] backbone and\ntrain exclusively in mixed precision. In text rendering optimization,\nwe perform 300 K full-parameter iterations on Text-Render-2M\nusing Adafactor [26] with a small learning rate of 2e-6. Stage 2\nconsists of 6,000 full-parameter fine-tuning steps on HQ-Poster-\n100Kâ€”again with Adafactor at lr = 1e-5 â€”during which we apply\nper-pixel flow-matching weights of 0.6, 0.2, 1.0 in different regions.\nIn reinforcement learning, for each prompt we sample n=5 candi-\ndates and optimize our reinforcement learning objective through\nAdamW [19] (lr = 1e-4) in 1500 steps, fine-tuning only LoRA adapters\nwith rank 64. For vision-language feedback refinement, we encode",
            "content": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 7 Fig. 6. Gemini serves as an authoritative evaluator for human preference comparisons across our method and other baselines. PosterCraft outperforms most state-of-the-art generative models in aesthetic coherence, prompt alignment, text rendering, and overall preference. It achieves performance nearly on par with the leading commercial model Gemini2.0-Flash-Gen in text rendering, while showing only slight gap in other aspects. Fig. 7. Visual comparison of different model outputs. Red boxes highlight misspelled or distorted text, while yellow boxes indicate redundant or missing text elements. Within the prompts, orange text denotes content and style requirements, and red text indicates textual elements. From the visual comparison, it is evident that our method achieves superior aesthetic appeal compared to existing state-of-the-art approaches. In terms of text presentation, our rendered fonts blend more naturally with the scene content, and text rendering errors are nearly eliminated. Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 8 Xiaoming Wei2, Lei Zhu1,5, Fig. 8. Visual comparison of different model outputs. Red boxes highlight misspelled or distorted text, while yellow boxes indicate redundant or missing text elements. Within the prompts, orange text denotes content and style requirements, and red text indicates textual elements. It indicates that our method significantly outperforms existing SOTA approaches in generating high-quality posters under long-prompt conditions, with notably improved prompt alignment. In terms of text rendering, our model produces fonts that align closely with the visual context of the scene, with minimal rendering errors. the dual-language reflections ğ‘“ğ‘ and ğ‘“ğ‘  via T5 [24]. Then we fine-tune the LoRA adapters (rank 128) under the conditional flow-matching loss for 6000 steps using AdamW (lr = 1e-4). We employ Internvl38B [13] as our feedback generation model, conducting fine-tuning for 2 epochs and setting the temperature to 0 during inference."
        },
        {
            "title": "4.2 Quantitative Results and Comparisons\nWe conduct a quantitative comparison of our PosterCraft against\nseven leading open-source advanced closed-source commercial mod-\nels. To assemble our test prompts, we use Gemini2.0-Flash-Gen [30]\nto randomly generate 100 aesthetic poster promptsâ€”balanced across\nshort, medium, and long lengthsâ€”and for each prompt we sam-\nple three output per model, yielding 300 test images. We gener-\nate posters with OpenCOLE [11], Playground-v2.5 [17], SD3.5 [1],\nFlux1.dev [9], Ideogram-v2 [32], BAGEL [5], and Gemini2.0-Flash-\nGen [30], apply the OCR engine of the SOTA VLM [30] to each\nimage, and report three precision-oriented metricsâ€”text recall, text\nF-score, and text accuracyâ€”averaged across all 300 samples.",
            "content": "As shown in Tab.1, PosterCraft achieves substantially higher recall and F1 scores, capturing more complete character set, while its accuracy surpasses leading open-source methods (Flux1.dev, SD3.5) and competitive commercial solutions (Ideogram-v2). PosterCrafts performance closely approaches that of mature SOTA commercial systems such as Gemini2.0-Flash-Gen, demonstrating that its textrendering not only advances academic benchmarks but also offers near production-level competitiveness. Besides, we conduct the user study and Gemini-based evaluation to assess model quality. Twenty experienced poster designers evaluate outputs across multiple dimensions, while Gemini2.0-Flash serves as strict evaluator via carefully designed prompts, as shown in Fig.5 and Fig.6. Both evaluations show that our method significantly enhances aesthetics and text rendering over the base model Flux1.dev, and outperforms all state-of-the-art open-source and several closed-source models across all evaluation metrics. Additionally, PosterCraft performs only slightly below Gemini2.0-Flash-Gen, validating the effectiveness of our unified workflow in fully unlocking the generative potential of powerful baseline model."
        },
        {
            "title": "4.3 Qualitative Results and Comparisons\nTo further evaluate the superiority of vision, we conduct a com-\nprehensive visual comparison between PosterCraft and six other\nmodelsâ€”four open-source and two proprietary commercial systems,\nas shown in Fig.7, Fig.8 and Fig.9. Playground-v2.5 exhibits complete\nfailure in text rendering, while SD3.5 [1] achieves only partial title\nrendering, with both models showing deficiencies in aesthetic qual-\nity and prompt adherence, particularly in representing animated\nstyles and semantics. Although Flux1.dev[9] and BAGEL [5] demon-\nstrate improved title rendering capabilities, they still contain textual\nerrors and fall short in aesthetic quality and prompt alignment, fail-\ning to incorporate specific details such as \"One golden mechanical\nowl perches on her shoulder.\" Among proprietary models, Ideogram-\nv2 [32] and Gemini2.0-Flash-Gen [30] show minor text errors only",
            "content": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 9 Fig. 9. Visual comparison of different model outputs. Red boxes highlight misspelled or distorted text, while yellow boxes indicate redundant or missing text elements. Within the prompts, orange text denotes content and style requirements, and red text indicates textual elements. Compared to other methods, our approach produces cleaner layouts, better theme alignment, and more accurate text rendering under short prompts. Fig. 10. Aesthetic-text reinforcement learning (top row) and vision-language feedback (bottom row) qualitative comparisons. orange boxes denote the text biases. Different color fonts represent key feedback information from VLM. The top examples demonstrate that our reinforcement learning stage effectively improves overall aesthetic quality and partially corrects text rendering errors after supervised fine-tuning. The bottom examples illustrate the impact of vision-language reflection, where feedback from VLM is integrated into the generation loop. This results in noticeable enhancements to both visual aesthetics and semantic coherence in the final poster outputs. Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 10 Xiaoming Wei2, Lei Zhu1,5, Table 1. text quality comparison with SOTA poster generation models, demonstrates that PosterCraft achieves superior performance across recall, F-score, and accuracy, while only slightly below the recent closed-source Gemini2.0-Flash-Gen. We highlight the best and second metrics. Open and Close denote open-source and closed-source. Method OpenCOLE [11] (Open) Playground-v2.5 [40] (Open) SD3.5 [1] (Open) Flux1.dev [9] (Open) Ideogram-v2 [32] (Close) BAGEL [5] (Open) Gemini2.0-Flash-Gen [30] (Close) PosterCraft (ours) Text Recall Text F-score Text Accuracy 0.082 0.157 0.565 0.723 0.711 0.543 0.798 0. 0.076 0.146 0.542 0.707 0.685 0.536 0.786 0.774 0.061 0.132 0.497 0.667 0.680 0.463 0.746 0.735 in smaller text elements and achieve superior aesthetic results. However, both demonstrate prompt adherence issues: Ideogram-v2 fails to accurately depict mountain formations, Gemini lacks photorealism, and neither successfully generates silhouette effects in the third image. In the short-prompt setting of Fig.9, PosterCraft maintains strong balance between visual appeal and accurate text rendering. It consistently integrates title and scene elements, for example, seamlessly embedding PixelPlay Retro Game Console or International Animation Festival in stylized compositions. Competing models often display legibility issues (e.g., SD3.5, Flux1.dev), omit key poster elements (e.g., BAGEL, Ideogram-v2), or suffer from text-scene disconnection (e.g., Playground-v2.5). While Gemini2.0-Flash-Gen performs well in text rendering, it still suffers from aesthetic limitations, often producing visually monotonous outputs with missing or underdeveloped design elements. Our results stand out with their vibrant layout, theme adherence, and natural visual-text coherence, even under minimal input conditions."
        },
        {
            "title": "5 Ablation Study\nIn this section, we validate the efficacy of our proposed workflow by\nindependently assessing the contribution of each stage. To ensure a\nfair comparison, all parameter settings and experimental conditions\nare held identical to those used in the preceding experiments.\nText Rendering Optimization is critical for both accuracy\nand perception. Text Rendering Optimization ensures clarity and\nfidelity. Its removal leads to drops in both OCR accuracy and hu-\nman preference, confirming that large-scale, high-quality text data\nsignificantly improves text rendering. The diverse and realistic\nbackgrounds also preserve visual quality, while models without\nthis optimization often fail to maintain legibility and accuracy, as\nshown in Fig.3. Region-aware Calibration improves poster con-\nsistency. It helps the model adapt to spatial context and balance\ntext-background. Without it, all regions are treated equally, weaken-\ning stylistic coherence in visually complex posters while suffering\ntext bias. Aesthetic-Text Reinforcement Learning boosts vi-\nsual appeal. RL strongly impacts human preference, showing its\nimportance in learning higher-order aesthetic cues like layout bal-\nance, color harmony, and typographic cohesion. It makes outputs\nmore visually compelling. The top-row results in Fig.10 indicate that\nreinforcement learning strongly impacts holistic preference, high-\nlighting its effectiveness in capturing higher-order aesthetic cues\nand text rendering. This leads to more visually compelling outputs,",
            "content": "Fig. 11. Ablation experiments on the core components of our workflow: removing Text Rendering Optimization, Region-aware Calibration, Aesthetic-Text RL, or Reflection leads to sustained declines in OCR accuracy (purple) and human preference (peach), demonstrating the effectiveness of our overall design, targeted optimizations and validating our motivation. with clear improvements evident in the poster compositions across the top row. Reflection refines outputs with vision-language feedback. Multi-modal reflection is beneficial for high-level semantic correction. Reflection enables the model to assess and iteratively improve generation quality, particularly in terms of stylistic integration. As shown in the bottom row of Fig.10, vision-language reflection can effectively refine the overall aesthetic appeal and layout consistency. These results validate the necessity of our workflow components and show that unified, targeted optimization can unlock the full potential of powerful foundation models for end-to-end aesthetic poster generation.usion models for end-to-end aesthetic poster generation."
        },
        {
            "title": "6 Conclusion\nPosterCraft presents a unified, cascaded workflow that integrates\nscalable text-rendering optimization, poster fine-tuning, RL-driven\naesthetic enhancement, and joint visionâ€“language feedback, demon-\nstrating powerful foundation model can directly produce posters\nwith both striking visuals and precise text. Our fully automated\ndataset pipelines support scalable, task-specific training, and we\nachieve substantial gains over open-source baselines, approaching\nleading commercial quality.",
            "content": "References [1] Stability AI. 2024. Stable Diffusion 3.5 Large. https://huggingface.co/stabilityai/ stable-diffusion-3.5-large. [3] [4] [2] Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, YingCong Chen, Lei Zhu, and Xinchao Wang. 2025. Posta: go-to framework for customized artistic poster generation. arXiv preprint arXiv:2503.14908 (2025). Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. 2023. Textdiffuser-2: Unleashing the power of language models for text rendering. arXiv preprint arXiv:2311.16465 (2023). Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. 2023. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems 36 (2023), 93539387. [5] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. 2025. Emerging Properties in Unified Multimodal Pretraining. arXiv:2505.14683 [cs.CV] https://arxiv.org/abs/2505. [6] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 11 [30] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [31] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. 2023. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054 (2023). Ideogram AI. Ideogram v2. https://ideogram.ai/launch. 2024. [32] [33] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. 2024. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 82288238. [34] Zhendong Wang, Jianmin Bao, Shuyang Gu, Dong Chen, Wengang Zhou, and Houqiang Li. 2025. DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models. arXiv preprint arXiv:2503.01645 (2025). [35] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. 2024. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848 (2024). [36] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. 2023. Human Preference Score v2: Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis. arXiv preprint arXiv:2306.09341 (2023). Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. 2024. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528 (2024). [38] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [39] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. 2024. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning. [40] Tao Yang, Yingmin Luo, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. 2024. PosterLLaVa: Constructing Unified Multi-modal Layout Generator with LLM. arXiv preprint arXiv:2406.02884 (2024). [41] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. 2023. Layoutdiffusion: Controllable diffusion model for layout-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2249022499. [42] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. 2024. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039 (2024). [37] Scaling rectified flow transformers for high-resolution image synthesis. In Fortyfirst International Conference on Machine Learning. [7] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2023. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems 36 (2023), 1822518250. [8] Yifan Gao, Zihang Lin, Chuanbin Liu, Min Zhou, Tiezheng Ge, Bo Zheng, and Hongtao Xie. 2025. PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering. arXiv preprint arXiv:2504.06632 (2025). [9] Black Forest Labs. FLUX. https://github.com/black-forest labs/flux. 2024. [10] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. 2023. Layoutdm: Discrete diffusion model for controllable layout generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1016710176. [12] [11] Naoto Inoue, Kento Masui, Wataru Shimoda, and Kota Yamaguchi. 2024. OpenCOLE: Towards Reproducible Automatic Graphic Design Generation. arXiv preprint arXiv:2406.08232 (2024). InternVL Team. 2024. InternVL2.5: Expanding Performance Boundaries of OpenSource Multimodal Models with Model, Data, and Test-Time Scaling. https: //internvl.github.io/blog/2024-12-05-InternVL-2.5. InternVL Team. 2025. InternVL3: Advancing Open-Source Multimodal Models with Native Multimodal Pretraining. https://internvl.github.io/blog/2025-04-11InternVL-3.0. [13] [16] [14] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Arsh Koneru, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. 2025. Reflect-DiT: Inference-Time Scaling for Text-to-Image Diffusion Transformers via In-Context Reflection. arXiv preprint arXiv:2503.12271 (2025). [15] Shuangqi Li, Hieu Le, Jingyi Xu, and Mathieu Salzmann. 2024. Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds. arXiv preprint arXiv:2411.18810 (2024). Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang Yang, Jian-Guang Lou, and Dongmei Zhang. 2023. Layoutprompter: awaken the design ability of large language models. Advances in Neural Information Processing Systems 36 (2023), 4385243879. [17] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. 2024. Playground v3: Improving text-to-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695 (2024). [18] Zeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Ji Li, and Yuhui Yuan. 2024. Glyph-ByT5-v2: Strong Aesthetic Baseline for Accurate Multilingual Visual Text Rendering. arXiv preprint arXiv:2406.10208 (2024). Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 (2017). [20] Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, and Cong Yao. 2024. LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1563015640. [21] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. 2024. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975 (2024). [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas MÃ¼ller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023). [19] [23] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems 36 (2023), 5372853741. [24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. Jaejung Seol, Seojun Kim, and Jaejun Yoo. 2024. Posterllama: Bridging design ability of langauge model to contents-aware layout generation. arXiv preprint arXiv:2404.00995 (2024). [26] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning. PMLR, 45964604. [25] [27] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. 2024. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098 (2024). [28] Zecheng Tang, Chenfei Wu, Juntao Li, and Nan Duan. 2023. Layoutnuwa: Revealing the hidden layout expertise of large language models. arXiv preprint arXiv:2309.09506 (2023). [29] Chameleon Team. 2024. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818 (2024). Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 12 Xiaoming Wei2, Lei Zhu1,5, This is supplementary material for PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework. We present the following materials in this supplementary material: Sec.7 Detailed dataset construction and comparison on TextRender-2M. Sec.8 More information about the automatic processing pipeline for HQ-Poster-100K. Sec.9 More examples and explanations of the Poster-Preference100K. Sec.10 Detailed illustration of the Poster-Reflect-120K. Sec.11 Gemini for OCR calculation and preference evaluator. Sec.12 Additional visual results generated by our PosterCraft. Sec.13 Limitations. Sec.14 Future work."
        },
        {
            "title": "7 Text-Render-2M Construction Pipeline\nIn this section, we provide a detailed overview of the automated con-\nstruction process behind Text-Render-2M, a large-scale synthetic\ndataset designed to improve text rendering quality in the baseline\nmodel. This dataset plays a critical role in the text rendering op-\ntimization stage of our workflow. By overlaying diverse textual\nelements onto high-resolution background images, it allows the\nmodel to learn accurate text generation while preserving its ability\nto represent rich visual content.\nMulti-Instance Text Rendering Each image contains a variable num-\nber of independently placed text instances, typically ranging from\none to three. This multi-instance setup introduces compositional\ncomplexity and better reflects natural layouts found in aesthetic\nposters.\nText Content Generation Text content is synthesized using a hybrid\nstrategy:",
            "content": "majority of the samples are generated using templatebased grammars, with phrases constructed from predefined structures filled with curated vocabulary lists. smaller portion uses random alphanumeric strings to simulate noisy or unstructured textual inputs. The generator supports rich variations in punctuation, casing , and structure (e.g., both single-word and short-phrase constructions), ensuring linguistic diversity. Font Selection and Style Variability Fonts are randomly drawn from categorized library containing both standard and artistic typefaces. When multiple styles are available, roughly even split is enforced between classic and stylized fonts. The system filters fonts that do not support lowercase letters to avoid invalid renderings. This selection mechanism ensures visual variability while maintaining text legibility. Layout and Placement Strategy Text is positioned using 33 gridbased partitioning scheme (e.g., top-left, center, bottom-right). Before final placement, bounding box collision checks are performed. If an overlap is detected, the system will retry placement with different positions or reduced font sizes, often within 35 attempts per instance. This strategy enables dense yet legible arrangements while minimizing visual clutter. Orientation and Alignment Orientation options include: Horizontal, Vertically rotated (rotated 90 degrees), Vertically stacked (one character per line). In the horizontal mode, small proportion of texts receive random rotation, and subset of longer text fragments are automatically wrapped across multiple lines. Alignment is randomly selected among left, center, or right justification. Prompt Generation Each image is paired with structured natural language prompt. Prompts include: The text content, Position (e.g., bottom right), Orientation (e.g., vertically stacked), Color category, and Optionally (included in 50% of cases), the font style. When multiple texts are rendered, their prompts are numbered and concatenated. If no text is successfully rendered, fallback prompt indicating the absence of text is generated. We provide number of samples to view in Fig.12."
        },
        {
            "title": "8 Automatic Processing Pipeline for HQ-Poster-100K\nFor collecting high-quality poster images, we design an automated\nimage filtering and annotation pipeline for HQ-Poster-100K utilizing\nthree MLLMs:",
            "content": "(1) MLLM Scorer: We employ Internvl2.5-8B-MPO with binary classification task to filter out posters containing extensive Credit Blocks, Billing Blocks, or \"4K ultrahd\" cover texts. The complete prompt is shown in Fig.13. (2) Gemini Caption Generation: We utilize Gemini2.5-Flash-Preview04-17 for automated poster caption annotation, systematically describing the poster content, visual style, and textual elements. The complete prompt structure is illustrated in Fig.14 (3) Gemini Mask Generation: The final step in poster data collection involves generating text region masks. We employ Gemini2.5Flash-Preview-04-17s robust OCR capabilities to extract text masks from posters, which are subsequently used for Region-aware Calibration. The prompt is shown in Fig.16 In addition, we provide more masks, images, prompts triplets in Fig.15 to demonstrate the advantages of our dataset."
        },
        {
            "title": "9 Explanations of the Poster-Preference-100K\nIn constructing the Poster-Preference-100K dataset, we first utilize\nHPSv2 to filter out poster pairs that meet aesthetic requirements\nand exhibit sufficient diversity. Subsequently, we employ Gemini2.5-\nFlash-Preview-04-17 to evaluate the alignment between the Best of\n5 posters and their corresponding prompts. The complete prompt",
            "content": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 13 Fig. 12. More samples are shown, which are high-quality paired samples in Text-Render-2M. Prompt 8.1 (MLLM Scorer Prompt) Does this poster contain large Billing Block or Credit Block at the bottom or \"4K ultrahd\" text at the top? Based on your judgment, use the closest option to answer, and only return the label: A. Yes. There is large Billing Block or Credit Block at the bottom or \"4K ultrahd\" text at the top. B. No. There is no Billing Block or Credit Block at the bottom and no \"4K ultrahd\" text at the top. Fig. 13. Prompt for MLLM Scorer in HQ-Poster-100K. Prompt 8.2 (Gemini Caption Generation) Please write structured and detailed caption in single paragraph for this poster, covering the following five aspects in order: Poster ContentDescribe what is visually depicted. Poster StyleDescribe the visual or artistic tone, such as cinematic, surreal, minimalist, or other distinct aesthetics. Poster TextProvide the exact words shown in the image (title, subtitle, slogan, etc.) and their overall communicative intent. Text Style and PositionDescribe the typography in detail, including font style, size, texture, and how it visually blends or contrasts with the background (e.g., carved into surface, embedded in light, wrapped by natural objects, etc.); also specify where each piece of text is positioned and its orientation angle in the frame. LayoutDescribe how the all elements are arranged to guide the viewers focus. Be specific, descriptive, and cohesive. Keep the response between 200 and 300 words, written as single paragraph. Avoid listing or enumeration. Do not mention any design tools or generation methods. Write as if for professional design catalog, highlighting how visual and typographic design choices form unified and compelling narrative. Fig. 14. Prompt for Gemini Caption Generation in HQ-Poster-100K. is shown in Fig.17 Our criteria require that text in the Best of 5 posters must be completely accurate while maximizing alignment with both the content and aesthetic style requirements specified in the prompt. We implement binary classification system where the model assigns 0 to unqualified samples and 1 to samples meeting all requirements. Fig.18 presents additional preference pairs, demonstrating that our pipeline built upon HPSv2 and Gemini effectively constructs preference data. Illustration of the Poster-Reflect-120K"
        },
        {
            "title": "10\nIn constructing the Poster-Reflect-120K dataset, we first employ\na preference-optimized model to generate 6 posters per prompt,\ntotaling 120K posters. Subsequently, our feedback collection pipeline\nutilizes Gemini2.5-Flash-Preview-04-17 in two phases: first to select\nthe Best of 6 posters as optimization targets, and second to gather\ndirect feedback.",
            "content": "(1) Best of 6 Selection: We sequentially input six images to Gemini, which selects the optimal image based on predetermined priorities, Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 14 Xiaoming Wei2, Lei Zhu1,5, Fig. 15. More pictures are shown, which are high-quality paired samples with masks in HQ-Poster-100K. returning the index number of the best image. The complete prompt is shown in Fig.19. (2) Feedback Collection: We input two images sequentially - the first being the image requiring improvement, and the second being the optimization target selected in phase (1). Feedback is collected across two dimensions: \"Poster Content Suggestions\" and \"Aesthetic Style Optimization Suggestions\". Each image pair receives 10 pieces of feedback through 5 Gemini requests. The complete prompt is shown in Fig.20."
        },
        {
            "title": "11 Gemini for OCR calculation and preference evaluator\nDuring the experimental phase, we employ Gemini2.5-Flash-Preview-\n05-20 as both an OCR metric calculator and a multi-dimensional\npreference evaluator. The robust perceptual and comprehension\ncapabilities of Gemini2.5-Flash establish a solid foundation for our\nexperimental accuracy.",
            "content": "(1) OCR Evaluator: Despite the significant challenges that artistic fonts in posters pose to traditional OCR algorithms, Gemini2.5-FlashPreview-05-20, as state-of-the-art MLLM model, accurately extracts textual information from images. We further utilize Gemini2.5Flashs reasoning and mathematical capabilities to directly compute and output OCR metrics. The specific prompt is shown in Fig.21. (2) Multi-Dimensional Preference Evaluator: Gemini2.5-Flash demonstrates superior assessment capabilities with its reasoning abilities. We present two images side by side and instruct Gemini2.5Flash to select from \"L\", \"R\", or \"None\", representing left image superior, right image superior, or indeterminate (either both excellent or both inadequate). The specific prompt is shown in Fig.22 and Fig.23."
        },
        {
            "title": "13 Limitations\nIn this work, we propose PosterCraft to explore how unified work-\nflow design can unlock the aesthetic design potential of powerful\nfoundation models. Our results demonstrate that with carefully\ncrafted design strategies, the modelâ€™s capabilities can be signifi-\ncantly enhancedâ€”making it competitive with leading proprietary\ncommercial systems. This validates the soundness of our motivation.\nHowever, our approach is not without limitations. Specifically, our\nmodel is fundamentally built upon the current flux1.dev baseline.\nAs such, if the pre-trained flux model has never encountered certain\ntypes of samples or contains significant flaws, our method may not",
            "content": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 15 Prompt 8.3 (Gemini Mask Generation) Detect all text regions in the image. For each text region, provide its bounding box in box_2d format [ymin, xmin, ymax, xmax]. The coordinates for each bounding box must be list of four integers [ymin, xmin, ymax, xmax], normalized to the range [0, 1000]. Ensure the box completely covers the text area. MANDATORY GUIDELINES: The box_2d coordinates [ymin, xmin, ymax, xmax] should be integers normalized to 0-1000. If no text is found in the image, the \"text_regions\" list in the JSON output should be empty. STRICT CONSTRAINTS: Adhere strictly to the JSON output format specified below. Do not include any explanations, apologies, or conversational text outside of the JSON structure. Ensure the provided normalized coordinates are accurate. RESPONSE FORMAT: Respond with single JSON object. Do NOT use markdown (e.g., ```json ... ```). The JSON object must have single key \"text_regions\". The value of \"text_regions\" must be list of bounding_boxes. Each bounding_box must be list of four integer coordinates [ymin, xmin, ymax, xmax], normalized to [0, 1000]. Example of the required JSON structure for \"text_regions\" containing two bounding boxes: [ ] [ ymin 1, xmin 1, ymax 1, xmax 1], [ ymin2 , xmin2 , ymax2 , xmax2 ] The complete JSON object should look like this: { } \" text_regions \": [ // List of bounding_boxes as shown above // e.g., [[20 , 10 , 50 , 100] , [70 , 150 , 100 , 250]] ] If no text is found, the output should be: {\"text_regions\": []}. Provide ONLY this JSON object. Now, based SOLELY on your comprehensive image analysis, provide ONLY the JSON object detailing all detected text regions and their normalized box_2d coordinates [ymin, xmin, ymax, xmax] as specified. Fig. 16. Prompt for Gemini Mask Generation in HQ-Poster-100K. challenges in typography and layout due to increased character complexity and spatial density. Collectively, these efforts will further demonstrate the scalability, adaptability, and cross-cultural applicability of our proposed workflow. Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009 be able to fully correct these shortcomings. That said, our workflow is highly unified and readily transferable to stronger baselines, ensuring full compatibility with other models in the community."
        },
        {
            "title": "14 Future work\nIn future work, we plan to enhance our unified workflow in three\nkey directions. First, we will explore integration with more advanced\nbackbone models that offer stronger pretraining and broader data\ncoverage, aiming to mitigate reliance on specific model biases and\nimprove robustness when handling rare or previously unseen poster\nstyles. Second, we intend to scale up training with larger and more\ndiverse datasets to better generalize across a wide range of visual\nand textual domains. Third, we aim to extend our framework to\nsupport multilingual poster generation. This introduces additional",
            "content": "Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 16 Xiaoming Wei2, Lei Zhu1,5, Prompt 9.1 (Prompt Alignment Evaluation) You are an expert in evaluating image content and font style against given text prompt. You will be given an image and an original text prompt that was intended to generate an image similar to the one provided. Your task is to assess whether the image is substantially consistent with the original text prompt based on the criteria below. Original Text Prompt: \"{original_prompt_text}\" Evaluation Criteria (Prioritized): (1) Text Accuracy: Thoroughly analyze all text visible in the image. Check for any inaccuracies such as typos, missing characters/words, or extra characters/words when compared to the \"Original Text Prompt\". This is the MOST CRITICAL factor. If ANY such error is found, the decision MUST be \"0\". (2) Text Style and Positioning: If text is present, does its style (font, color, decoration) and positioning (layout, orientation) in the image reasonably align with what is described or implied in the \"Original Text Prompt\"? (3) Overall Content, Artistic Style, and Visual Appeal: Does the overall image content (subjects, scene, objects) and artistic style align well with the \"Original Text Prompt\"? Is the image generally clear, well-composed, and visually appealing in the context of the prompt? Output Format: Based on your assessment, output ONLY JSON object in the following format: {{\"final_decision\": \"1\"}} if the image is substantially consistent with the original prompt across the prioritized criteria (especially if no text errors are found when text is intended) and should be kept. {{\"final_decision\": \"0\"}} if there are ANY discrepancies in Text Accuracy (typos, missing/extra characters/words), or significant issues in other critical criteria, or overall poor alignment, meaning the image should be discarded. Strict constraints: Only output the JSON object. Do NOT include any additional text, explanation, or markdown. Use exactly \"0\" or \"1\" as the value for \"final_decision\". Fig. 17. Prompt for Prompt Alignment Evaluation in Poster-Preference-100K Fig. 18. Additional Preference Pairs in Poster-Preference-100K. The images on the left are Rejected Samples, while those on the right are Preferred Samples. Orange text indicates textual content, and red text corresponds to content, style, or layout requirements. PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 17 Prompt 10.1 (Best-of-6 Selection) You are professional Poster Designer. Your task is to evaluate six generated posters based on design brief (\"Original Prompt\") and select the single best poster, or indicate if none are suitable. Evaluation Process: (1) Textual Accuracy (Paramount Importance): First, assess all posters for textual accuracy against the \"Original Prompt\". Text (if any is specified or implied by the brief) MUST be perfectly accurate: No typographical errors. No missing or extra characters/words. poster with any textual flaw cannot be chosen as the best IF an alternative poster with perfect text exists. (2) Content Alignment and Aesthetic Value: This criterion is used to select among posters that have passed the textual accuracy check. The chosen poster should: Provide content as close as possible to the \"Original Prompt\". Demonstrate the highest possible aesthetic value (considering composition, color palette, typography, imagery, and overall visual impact). Selection Logic: Ideal Case: If one or more posters have perfect textual accuracy, select from THIS group the single poster that best meets Criterion 2 (Content Alignment and Aesthetic Value). Special Case (All Posters Have Textual Flaws): If ALL six posters have some textual inaccuracies, then no poster meets the primary standard for \"best.\" In this situation, you MUST output \"none\". Fallback Case (This should ideally not be reached if \"Special Case\" is handled correctly): If the logic leads here unexpectedly after \"Special Case\" consideration, and no poster has perfect text, but selection is still forced, choose the poster that, despite its textual flaws, is superior when evaluated SOLELY on Criterion 2 (Content Alignment and Aesthetic Value across all six flawed images). However, prioritize outputting \"none\" if all have text flaws. Original Prompt (Design Brief): \"`original_prompt`\" Select the image (\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", or \"none\") that best meets these requirements. Respond ONLY with JSON object in ONE of these exact formats: {{\"best_image\": \"1\"}} OR {{\"best_image\": \"none\"}} Strict constraints: Only output the JSON object. Do NOT include any additional text or markdown. Use exactly \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", or \"none\" to refer to your selection. Fig. 19. Prompt for Best-of-6 Selection in Poster-Reflect-120K. Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 18 Xiaoming Wei2, Lei Zhu1,5, Prompt 10.2 (Feedback Collection) Internally compare the first poster against the second poster, focusing strictly on visual content layout and overall aesthetic style. Based on this internal comparison, provide detailed and specific suggestions in two aspects: 1. Poster Content Suggestions 2. Aesthetic Style Optimization Suggestions. Act as professional poster designer. Deliver highly detailed, specific, and actionable feedback in the form of standardized image editing instructions. MANDATORY GUIDELINES: The second poster must be fully followed as the standard. Identify and correct all visual layout and style discrepancies based on this reference. Focus exclusively on content and visual/aesthetic design. Completely ignore any issues related to text, typography, wording, spelling, rendering, or text styling. STRICT CONSTRAINTS: NEVER mention the second poster, reference, or target. NEVER use comparative phrases such as \"similar to the second poster\" or \"make it like the second poster\". ONLY describe the editing instructions for Poster 1, framed as standalone improvement tasks. RESPONSE FORMAT: Response should be formatted as clearly structured json schema: {`Poster Content Suggestions`: str, `Aesthetic style optimization suggestions`: str} Return ONLY the JSON object itself, without any introductory text or markdown formatting. Fig. 20. Prompt for Feedback Collection in Poster-Reflect-120K. PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 19 Prompt 11.1 (OCR Evaluation) You are an OCR evaluation assistant. Follow these steps exactly on the attached image: (1) Ground-Truth Extraction (from the design prompt only): Do NOT read text from the image for GT. Parse ONLY the following design prompt and extract ALL text strings that should appear on the poster (titles, subtitles, dates, slogans, venue, etc.), preserving spaces and punctuation exactly: `original_prompt_text` Order them in spatial sequence (topbottom, leftright) and concatenate into raw GT text. (2) OCR Extraction (from the attached image): Run OCR on the provided image and extract ALL rendered text exactly as it appears. Preserve visual reading order (top-leftbottom-right). This is your raw OCR text. (3) Text Normalization (apply to BOTH raw GT and raw OCR before comparison): Convert all letters to lowercase. Remove ALL punctuation characters: .,;:!?\"-()[]{}...` Collapse any sequence of whitespace/newlines into single space. Trim leading and trailing spaces. (4) Character-Level Alignment & Error Counting: Align the normalized GT text and OCR text character by character. Count four categories: Insertion (I): extra character in OCR not in GT (\"more\"). Deletion (D): GT character missing in OCR (\"less\"). Substitution (S): OCR character differs from GT character (\"render error\"). Correct match (C): identical characters. (5) Metrics Calculation: Let = total normalized GT characters = + + S. Let = total normalized OCR characters = + + S. Let = total compared characters = + + + S. Character Accuracy = / T. Text Precision = / (C + + S). Text Recall = / (C + + S). Text F-score = 2 * Precision * Recall / (Precision + Recall). (6) Final JSON Output (strictly this format, no extra keys or commentary): { \"GT_text\": \"<normalized GT text>\", \"insertions\": I, \"total_GT_chars\": N, \"OCR_text\": \"<normalized OCR text>\", \"deletions\": D, \"substitutions\": S, \"accuracy\": \"XX.XX%\", \"precision\": \"YY.YY%\", \"recall\": \"ZZ.ZZ%\", \"f_score\": \"WW.WW%\" } \"correct_chars\": C, Fig. 21. Prompt for OCR Evaluation. Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 20 Xiaoming Wei2, Lei Zhu1,5, Prompt 11.2 (Preference Evaluation) Your task is to evaluate single input image containing two sub-images side-by-side (Left: L, Right: R), both generated from the \"Original Prompt\". Compare them on Aesthetic Value, Prompt Alignment, Text Accuracy, and Overall Preference. General Evaluation Protocol: For each of the four categories: (1) Provide brief textual analysis justifying your choice. (2) Make definitive choice: \"L\" (Left is superior), \"R\" (Right is superior), or \"none\". When to Choose \"none\": You must select \"none\" for category if: a) and are tied or indistinguishable in quality for that category. b) The category is not applicable. c) After careful review, you cannot definitively determine superior side. d) Crucially: and exhibit clear, offsetting strengths and weaknesses within that specific category. If excels in one aspect of the category while excels in another, and these trade-offs make declaring an overall winner for that category difficult or misleading, choose \"none\". Do not attempt to weigh these distinct, offsetting pros and cons to force preference. Your careful judgment is vital. Original Prompt: \"`original_prompt`\" Please provide your evaluation in the JSON format specified below. (1) Aesthetic Value: Evaluate visual appeal: harmony and consistency of background style, text style (if present), thematic consistency between background/text, overall content/text layout, and how the artistic style (background, content, text) aligns with the \"Original Prompt\". `aesthetic_value_explanation`: Your brief analysis. `aesthetic_value`: Choose \"L\", \"R\", or \"none\" (if L/R are equally pleasing/coherent, choice is impossible, or they exhibit offsetting aesthetic strengths/weaknesses as per the \"When to Choose none\" protocol). Respond with: {{\"aesthetic_value\": \"L/R/none\", \"aesthetic_value_explanation\": \"Your analysis...\"}} (2) Prompt Alignment (excluding text elements and artistic style): Evaluate how well non-textual elements (subjects, objects, scene) in and match the \"Original Prompt\". `prompt_alignment_explanation`: Your brief analysis. `prompt_alignment`: Choose \"L\", \"R\", or \"none\" (if L/R align equally well/poorly, its too close to call, or they exhibit offsetting strengths in alignment as per the \"When to Choose none\" protocol). Respond with: {{\"prompt_alignment\": \"L/R/none\", \"prompt_alignment_explanation\": \"Your analysis...\"}} (3) Text Accuracy (if applicable): Evaluate text in and based only on textual content specified/implied in the \"Original Prompt\". Focus only on: Accuracy: All prompt-specified words/characters present, no typos/misspellings/alterations? Recall: All intended textual elements from prompt included? Any missing words/phrases? Ignore text style, font, visual appeal, legibility (unless it prevents determining accuracy/recall), and placement. `text_accuracy_explanation`: Your brief analysis. `text_accuracy`: First, determine if \"none\" is appropriate (as per the general \"When to Choose none\" protocol, especially if is better on Accuracy but on Recall, or vice-versa; or if performance is identical/text N/A). If \"none\" is not chosen, select \"L\" if is demonstrably superior overall in combined text accuracy and recall, or \"R\" if is. Respond with: {{\"text_accuracy\": \"L/R/none\", \"text_accuracy_explanation\": \"Your analysis...\"}} (4) Overall Preference: Considering all above aspects (aesthetics, alignment, text accuracy) and any other factors relevant to the \"Original Prompt\". `overall_preference_explanation`: Your brief analysis. `overall_preference`: Choose \"L\", \"R\", or \"none\" (if L/R are equally preferred, choice is impossible, or they present compelling but different and offsetting strengths across categories making neither holistically superior, as per the \"When to Choose none\" protocol). Respond with: {{\"overall_preference\": \"L/R/none\", \"overall_preference_explanation\": \"Your analysis...\"}} Fig. 22. Prompt for Preference Evaluation. PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 21 Prompt 11.3 (Preference Evaluation) Respond ONLY with single JSON object in the following format: { \"aesthetic_value\": \"your_choice_for_aesthetic\", \"aesthetic_value_explanation\": \"Your brief analysis for aesthetics...\", \"prompt_alignment\": \"your_choice_for_alignment\", \"prompt_alignment_explanation\": \"Your brief analysis for prompt alignment...\", \"text_accuracy\": \"your_choice_for_text\", \"text_accuracy_explanation\": \"Your brief analysis for text accuracy...\", \"overall_preference\": \"your_choice_for_overall\", \"overall_preference_explanation\": \"Your brief analysis for overall preference...\" } Replace placeholders with your choices (\"L\", \"R\", \"none\") and analyses. Strict constraints: Only output the JSON object. No additional text or markdown. Each choice value (e.g., \"aesthetic_value\") must be \"L\", \"R\", or \"none\". Explanation fields must contain your textual analysis. Fig. 23. Prompt for Preference Evaluation. Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 22 Xiaoming Wei2, Lei Zhu1,5, Prompt 14.1 (Final Retouching Instructions) Character You are professional image retouching artist tasked with finalizing single retouching approach based on the users preferences and previous proposals. Your expertise ensures that the final approach integrates key aspects from different suggestions or follows single selected approach in full. Background The user has reviewed previous retouching approaches and provided feedback or specific instructions for final retouching plan that aligns with their creative goals. Ambition Your goal is to either choose one of the previously proposed approaches that best matches the users vision or create new, cohesive retouching approach by combining elements from different suggestions. Ensure that the final approach fully respects the users instructions and creative intent. User Instruction User says: \"{user_instruction}\" Task (1) Review the provided retouching approaches and the users feedback or instructions. (2) Decide whether to: Select single approach that fits the users description. Create new approach that integrates relevant aspects from different suggestions. (3) Final Approach: Describe the adjustments to Light (exposure, contrast, highlights, shadows, blacks, and whites) and Color (temperature, tint, vibrance, and saturation). For each adjustment, specify which objects or areas of the image are most affected and describe the specific details (e.g., the intricate carvings on the roof are highlighted by gentle increase in exposure). Explain the expected visual effect on these objects, such as the water reflections appear richer and more defined or the sky becomes softer and more inviting. For each individual HSL adjustment (Red, Orange, Yellow, Green, Cyan, Blue, Purple, Magenta), explain why it is necessary and describe the expected visual change for specific objects (e.g., the red tones in the window frames become more vivid to emphasize their ornate design). Organize the description as step-by-step plan, indicating the sequence of adjustments. Guidelines for Description: Avoid providing exact numerical valuesfocus on explaining how the adjustments affect the images visual presentation. Mention specific objects, areas, and their corresponding changes to help visualize the effect. Ensure the approach remains detailed, logical, and cohesive, and does not exceed 100 words. Fig. 24. Prompt for Final Retouching Instructions. PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in Unified Framework 23 Fig. 25. Examples generated by our PosterCraft demonstrating high diversity and aesthetic quality across themes including education, entertainment, and science fiction. All generation results showcase genre-specific fidelity, text rendering, and layout aesthetic. Sixiang Chen1,2,, Jianyu Lai1,, Jialin Gao2,, Tian Ye1, Haoyu Chen1, Hengyu Shi2, Shitong Shao1, Yunlong Lin3, Song Fei1, Zhaohu Xing1, Yeying Jin4, Junfeng Luo2, 24 Xiaoming Wei2, Lei Zhu1,5, , Fig. 26. Examples generated by our PosterCraft demonstrating high diversity and aesthetic quality across themes including movies, product, and virtual reality. All generation results showcase genre-specific fidelity, text rendering, and layout aesthetic."
        }
    ],
    "affiliations": [
        "Meituan",
        "National University of Singapore",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Xiamen University"
    ]
}