{
    "paper_title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
    "authors": [
        "Chenyu Wang",
        "Paria Rashidinejad",
        "DiJia Su",
        "Song Jiang",
        "Sid Wang",
        "Siyan Zhao",
        "Cai Zhou",
        "Shannon Zejiang Shen",
        "Feiyu Chen",
        "Tommi Jaakkola",
        "Yuandong Tian",
        "Bo Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 2 1 4 5 9 0 . 0 1 5 2 : r SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models Chenyu Wang1,2,,, Paria Rashidinejad1,3,, DiJia Su1, Song Jiang1, Sid Wang1, Siyan Zhao1,4,, Cai Zhou2, Shannon Zejiang Shen1,2,, Feiyu Chen1, Tommi Jaakkola2, Yuandong Tian1, Bo Liu1, 1Meta Superintelligence Labs, 2MIT, 3USC, 4UCLA Work done at Meta, Core contribution Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku. Date: October 14, 2025 Correspondence: Chenyu Wang at wangchy@mit.edu, Bo Liu at lbo@meta.com Code: https://github.com/facebookresearch/SPG Figure 1 Test accuracy of SPG and baseline methods on four mathematical and logical reasoning benchmarks. All methods are evaluated with generation length of 256 in 128 denoising steps. Full results are provided in Table 1."
        },
        {
            "title": "1 Introduction",
            "content": "Diffusion models, originally pioneered for high-fidelity image generation (Song et al., 2020; Ho et al., 2020), have recently emerged as powerful and efficient paradigm for text generation (Austin et al., 2021; Campbell et al., 2022; Sun et al., 2022; Lou et al., 2023; Sahoo et al., 2024; Shi et al., 2024). These models operate in discrete space but share architectural similarities with their continuous counterparts (Peebles and Xie, 2023). They employ fixed noising process that progressively corrupts text data, while neural network is trained to learn the reverse, denoising process. For instance, Masked Diffusion Language Model (MDLM) (Sahoo et al., 2024) uses random masking as its forward noising process and optimizes an Evidence Lower Bound (ELBO) of the log-likelihood. This ELBO-based objective has been widely adopted by subsequent large-scale diffusion language models (dLLMs), including LLaDA (Nie et al., 2025) and DREAM (Gong et al., 2024). key advantage of dLLMs over their autoregressive (AR) counterparts is their ability to decode multiple tokens in parallel. This parallelism can significantly reduce inference latency, making it an attractive alternative for scalable language modeling (Wang et al., 2025a; Labs et al., 2025). 1 Figure 2 The training process of SPG for MDLM. Left: From prompt c, we generate responses {xj}g . We then maximize lower bound on the likelihood πθ(xj c) for high-reward responses while minimizing an upper bound for low-reward ones. Right: The upper/lower bound of likelihood is estimated via Monte Carlo using block-wise masking strategy, where random block is selected for masking, with earlier blocks kept clean and later blocks fully masked. The example shows sequence of length 9 with block size of 3, where the current generation block is highlighted in yellow. j=1 Aligning large language models with human preferences (Ouyang et al., 2022) or task-specific rewards (e.g., inducing reasoning behavior) (Shao et al., 2024; Guo et al., 2025) typically requires post-training stage of reinforcement learning (RL). However, applying RL to dLLMs remains underexplored. principal challenge is the computationally intractable log-likelihood of dLLMs, which is essential for accurate policy gradient estimation. To circumvent this, recent works (Zhao et al., 2025; Yang et al., 2025; Zhu et al., 2025; Tang et al., 2025) adapt standard RL and preference optimization algorithms, such as GRPO (Shao et al., 2024) and DPO (Rafailov et al., 2023), by using the ELBO or one-step estimation as surrogate for the true likelihood. While straightforward, this approximation leads to misaligned policy gradients, and potential suboptimal performance. To address these limitations, we propose Sandwiched Policy Gradient (SPG), novel reinforcement learning algorithm for diffusion language models that computes more robust and less biased policy gradient. As illustrated in Figure 2, our core idea is to sandwich the intractable log-likelihood of generated sequence: we maximize tractable lower bound for positive-reward sequences while minimizing an upper bound for negative-reward ones. To ensure stable estimation of these bounds, we also propose block-wise masking strategy that better aligns data distributions during policy rollout and optimization. SPG achieves state-of-the-art performance on four mathematical and logical reasoning benchmarks, improving accuracy by up to 3.6% on GSM8K, 2.6% on MATH500, 18.4% on Countdown, and 27.0% on Sudoku compared to the state-of-the-art RL algorithms for diffusion language models. In summary, our main contributions are: new policy gradient algorithm, SPG, which reduces bias by optimizing sandwiched variational bounds based on reward. block-wise masking technique that improves the stability of the training objectives estimation. State-of-the-art results among RL algorithms for diffusion language models on four reasoning benchmarks, demonstrating the effectiveness of our approach."
        },
        {
            "title": "2 Background",
            "content": "In this section, we provide brief overview of the masked diffusion language model (MDLM) and reinforcement learning for text diffusion models. Notation. We denote scalars by lowercase letters (x), vectors by bold lowercase (x), and sequences by x1:n. [k] represents {1, . . . , k}. Cat(x p) is the categorical distribution over with probabilities p, and U[a, b] denotes the uniform distribution in [a, b]. Throughout the paper, we use [n] for position of the token, [g] for sequence in group of rollouts, and for the diffusion timestep. For discrete time processes, [T ], while for continuous-time Markov chains, [0, 1]."
        },
        {
            "title": "2.1 Masked Diffusion Language Models",
            "content": "Diffusion models for language learn to generate text by reversing gradual noising process. Specifically, Masked Diffusion Language Models (MDLMs) (Sahoo et al., 2024) start with clean text x1:n and corrupt it into zt zt,1:n over continuous timestep [0, 1] by progressively replacing tokens with special [mask] token. At = 0, the data is original (z0 = x), while at = 1, the sequence is fully masked (z1 is all [mask] tokens). Each token is corrupted independently according to the forward transition kernel: qt0(zt,i xi) = Cat(cid:0)zt,i αtxi + (1 αt)m(cid:1), where is the one-hot representation of the [mask] token. The noise schedule, αt [0, 1], is strictly decreasing function, such as the linear schedule αt = 1 t, with α0 = 1 and α1 = 0. In the reverse process, neural network, which we denote as the policy πθ, is then trained to perform the reverse process: predicting the original tokens from corrupted version zt. The transition from zt to zs (s < t) is parameterized with πθ as follows: (1) pθ(zs zt) = (zs zt, = πθ( zt)) = (cid:40)Cat(zs; zt), (cid:16) Cat zs; (1αs)m+(αsαt)πθ (zt) 1αt (cid:17) zt = m, , zt = m. The policy is achieved by maximizing the Evidence Lower Bound (ELBO) of the log-likelihood of each clean sequence pdata, which simplifies to the following objective: LELBO(x; θ) = Et,zt (cid:20) (cid:88) (cid:21) w(t) 1(zt,i = m) log πθ(xi zt) , (2) i=1 where w(t) = α t/(αt 1) is time-dependent loss weight, and the expectation is over random timestep U[0, 1] and the corrupted sequence zt qt0( x). In essence, this objective trains the model to fill in the blanks by predicting the original tokens at masked positions. For more comprehensive overview of MDLM, please refer to Section and Sahoo et al. (2024)."
        },
        {
            "title": "2.2 Reinforcement Learning for Diffusion Language Models\nReinforcement Learning (RL) aligns a language model with desired objectives by treating it as a policy πθ that generates\na response x to a prompt c. A reward function R(c, x) provides a scalar score for the response, and the training goal is\nto update θ to maximize the expected reward: J (θ) := Ex∼πθ (·|c)[R(c, x)]. This objective is commonly optimized\nusing policy gradient methods, which rely on the following gradient estimator.",
            "content": "θJ (θ) = Exπθ (c) (cid:21) (cid:20) R(c, x)θ log πθ(x c) . (3) The Intractability Challenge. central challenge in applying RL to diffusion models is that the policys log-likelihood, log πθ(x c), is intractable and cannot be computed directly. To overcome this, prior work (Zhu et al., 2025; Yang et al., 2025) approximates this term using its ELBO, effectively replacing log πθ(x c) with score derived from the pre-training objective in Equation (2). However, this popular workaround introduces critical flaw. The ELBO is only lower bound on the true log-likelihood (ELBO log πθ). Consequently, the RL objective is only valid lower bound on the true expected reward if all rewards R(c, x) are non-negative. This constraint prevents the model from effectively learning from negative feedback (i.e., penalizing bad outputs) and is incompatible with advanced RL algorithms that use relative or negative rewards (Shao et al., 2024), biasing the final policy. Our work aims to resolve this limitation."
        },
        {
            "title": "3 Sandwiched Policy Gradient with Evidence Bounds",
            "content": "We introduce SPG, novel policy gradient algorithm designed for masked diffusion language models (Algorithm 1). Our method aims to address critical issue in applying reinforcement learning to dLLMs by creating valid optimization objective based on tractable bounds of the models evidence. 3 4: 5: 6: 7: 8: 9: Algorithm 1 SPG: Sandwiched Policy Gradient for Masked dLLMs Require: prompt distribution D, number of completions per prompt g, number of inner updates µ, forward process q, number of Monte Carlo samples m, initial policy π0, learning rate ϵ. 1: Initialize πθ π0 2: while not converged do 3: Sample prompt D, then completions {xj πθ( c)}g [g], compute reward R(c, xj) and advantage Aj(xj, c) for gradient update iterations {1, . . . , µ} do [g], generate perturbed samples {zj Compute the sandwiched policy gradient JSPG(θ) where: tτ }m j= τ =1 q( xj) via block-wise masking (Section 3.3). JSPG(θ) = (cid:20) 1 (cid:88) (cid:16) j= 1Aj 0 AjLELBO(xj c; θ) + 1Aj <0 Aj LEUBO(xj c; θ) (cid:17) (cid:21) , and LELBO, LEUBO are estimated from {zj tτ }m τ =1 Perform gradient update: θ θ + ϵJSPG(θ) , using Equation 2 and 7. 10: return πθ"
        },
        {
            "title": "3.1 A Lower Bound Objective for Policy Optimization",
            "content": "Our approach is based on group relative policy optimization (Shao et al., 2024; Liu et al., 2025b). For given prompt c, we generate group of responses {xj}g from the policy πθ. We then compute the advantage (cid:80)g ȷ=1 R(c, xȷ). Moreover, we transform the conventional policy optimization objective as Aj(c, xj) := R(c, xj) 1 an advantage-weighted log-likelihood objective, for reasons that will be clear later: j=1 group(θ) = Ec,{xj }πsg[θ] (cid:20) 1 (cid:88) j= Aj(xj, c) log πθ(xj c) (cid:21) , (4) where sg[θ] indicates that gradients are not computed for the policy that generates the samples. This objective encourages generations with positive advantages (Aj > 0) and discourages those with negative advantages (Aj < 0). For dLLMs, the log-likelihood log πθ is intractable. common surrogate is the evidence lower bound (ELBO). While maximizing the ELBO is valid way to increase the true log-likelihood, minimizing the ELBO for negatively-rewarded samples does not guarantee reduction in the true log-likelihood. To address this, we propose sandwiched objective. For samples with positive advantages, we maximize the ELBO. For samples with negative advantages, we instead minimize tractable evidence upper bound (EUBO), LEUBO. This creates true lower bound for the original objective: JSPG(θ) = (cid:20) 1 g (cid:88) j=1 (cid:0)1Aj 0 AjLELBO(xj c; θ) + 1Aj <0 AjLEUBO(xj c; θ)(cid:1) (cid:21) , (5) where the expectation is take with respect to c, {xj} πsg[θ]. Since LELBO log πθ LEUBO, it follows that JSPG(θ) group(θ). Maximizing this tractable bound therefore serves as valid proxy for optimizing the true objective."
        },
        {
            "title": "3.2 A Tractable Evidence Upper Bound",
            "content": "To effectively penalize negatively-rewarded samples by minimizing their log-likelihood, we require tractable EUBO, which we derive in the following theorem based on the Rényi variational bound. Theorem 1 (Evidence Upper Bound for Masked Diffusion). Assume the forward denoising process has steps with monotonic schedule αt. For any β 1 and sequence x1:n, we have: LEUBO(x1:n; θ) = 1 β (cid:88) log 1 (cid:88) i=1 t=1 (cid:34) Ezt+1 αt αt+1 1 αt+ 1(zt+1,i = m) πβ (cid:35) θ (xi zt+1) + C(T ), (6) 4 where C(T ) := 1(β < n) β log Ez1:T q(x) (cid:104) q(z1:T x)n(cid:105) is constant independent of θ. Here, β 1 is hyperparameter that controls the tightness of the bound, with values closer to 1 yielding tighter bound. The expectation is taken over the timestep U[0, 1] and the noised latent zt qt0( x). Corollary 1. Taking the limit of , we have: θLEUBO(x1:n; θ) = θ (cid:16) LEUBO(x1:n; θ) + C(T ) (cid:17) = θ LEUBO(x1:n; θ), where LEUBO(x1:n; θ) = 1 β (cid:88) i=1 (cid:104) log Et,zt w(t) 1(zt,i = m) πβ (cid:105) θ (xi zt) . (7) In practice, we estimate LEUBO using Monte Carlo sampling and plug it in Equation 5 in place of LEUBO. The proof and theoretical analysis are provided in Appendix B. Remark. key structural difference from LELBO is that the logarithm in LEUBO (Equation (6)) appears outside the expectation. Therefore, in practice, due to Jensens inequality, applying the concave logarithm to Monte Carlo estimate of the expectations argument yields biased estimate of the true EUBO. While it is possible to derive looser but unbiased bound using inequalities like log(x) 1, we found this approach empirically worse by widening the gap to the true log-likelihood, as shown in Table 10. We therefore retain the tighter, albeit slightly biased, formulation."
        },
        {
            "title": "3.3 Practical Considerations",
            "content": "In practice, we approximate LELBO and LEUBO in Block-Wise Masking Strategy for Monte Carlo Estimation. and generate the Equation (5) via Monte Carlo sampling: for each xj, we randomly sample timesteps {tτ }m corresponding partially masked samples {zj τ =1 q( xj). One straightforward approach as used in Yang et al. (2025) would be to apply random masking to clean sequences. However, recent dLLMs like LLaDA (Nie et al., 2025) employ block-wise semi-autoregressive unmasking strategy during generation and achieve state-of-the-art performance over random unmasking. As result, the policy rollout process actually encounters much narrower and more structured set of partially masked sequences than with fully random masking. tτ }m τ =1 To better align data distributions during policy rollout and optimization, we adopt block-wise masking strategy rather than random masking. As depicted in Figure 2, the sequence is divided into several blocks, and random block is selected, with all preceding blocks left clean and all following blocks fully masked. Within the chosen block, tokens are randomly masked. Additionally, following D1 (Zhao et al., 2025), we lightly perturb the prompt and clean blocks by randomly masking tokens with small probability pmask = 0.15 to enhance stability and generalization. Altogether, our block-wise masking strategy improves the stability of the objectives estimation and the efficiency of policy optimization. While similar block-wise masking approaches have been explored in concurrent work for supervised fine-tuning or block diffusion models (Sun et al., 2025; Wang et al., 2025b), our focus is on RL for full-attention masked dLLMs. As shown in Figure 6, our models trained with block-wise masking generalize well to various inference strategies. Mixture of Upper and Lower Bound for Negative Advantage Traces. Monte Carlo estimation of Equation (6) leads to biased estimation to LEUBO and potentially requires substantial number of samples to get reliable approximations, resulting in high computational costs and instability during training. To address these challenges, we use mixture of LEUBO and LELBO as more practical log-likelihood approximation for negative advantage traces: LMix(x c; θ) := ω LEUBO(x c; θ) + (1 ω) LELBO(x c; θ) (8) where 0 ω 1 is blend coefficient. Intuitively, the upper bound LEUBO sharpens the model decisions by applying β-power adjustment to the original model output, acting as strong correction signal for negative advantage traces. In contrast, the lower bound LELBO is easier and more stable to estimate with small number of Monte Carlo samples, but it tends to introduce larger, systematic bias relative to the true log-likelihood. In particular, as conservative approximation, LELBO alone is insufficient for effectively penalizing negative advantage traces, thus limiting its efficacy. Therefore, combining them allows us to harness the strengths of each, resulting in more effective log-likelihood estimation in 5 practice. In the following proposition, we formalize the advantages of using the mixture by deriving the gradient of the mixture loss and analyzing the variance of the gradient. Proposition 1 (Optimal Mixture Strictly Reduces Variance). Fix coordinate and let ρβ := w(t, zt)πβ θ (xi zt, c)/E (cid:104) w(t, zt)πβ (cid:105) , θ (xi zt, c) where w(t, zt) := w(t)1(zt = m). Then, the gradient of mixture objective (8) is given by gω,k = ((1 ω)w(t, zt) + ωρβ) θk log πθ(x zt, c). (9) If Var((ρβ w(t, zt))θk log πθ(x zt, c)) > 0, then Var[gω,k] is strictly convex quadratic in ω and thus admits unique minimizer ω k. Moreover, Var[gω k,k] < min(cid:8) Var[g0,k], Var[g1,k](cid:9), proof for the above proposition is provided in Section C.1. few remarks are in order: Confidence-aware weighting: The mixture gradient in Equation (9) realizes confidence-aware weighting: uncertain tokens with small πβ θ (xi zt, c), indicating low recovery chance, have smaller weight, while confident tokens with large πβ θ (xi zt, c) are upweighted. The sharpness is controlled by parameter β and the blend by ω. Furthermore, the convex interpolation of the confidence-aware coefficient of the upper bound with the lower bound ensures clipping tiny gradients to minimum value and thus prevents vanishing gradients. Lower variance and more stable training: According to Proposition 1, the gradient of the optimal mixture, i.e., gω k,k, has strictly smaller coordinate-wise variance than the gradient of either the lower bound (g0,k) or the upper bound (g1,k)1. In our experiments, we fix β and ω as hyperparameters for simplicity. These values can also be adaptively adjusted during training to better match the evolving training dynamics and data distribution. Thus, the mixture approach offers theoretical advantages over using either the upper or lower bound alone, as supported by our experimental results in Section 4. Further discussions of the mixture approach and empirical evidence of reduced gradient variance are provided in Appendix C.2 and Figure 7, and Appendix C.3 presents toy example illustrating the distinct behaviors of the lower and upper bounds."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present experimental results highlighting the superior performance of SPG across various benchmarks. Further, we provide detailed analysis and ablations of SPG to assess the contribution of each component, examine the influence of key hyperparameters, and evaluate the robustness of our approach under different inference strategies."
        },
        {
            "title": "4.1 Experimental Setup and Main Results\nExperimental Setup. We conduct RL fine-tuning with SPG following the experimental settings in D1 (Zhao et al.,\n2025) and WD1 (Tang et al., 2025). We employ LLaDA-8B-Instruct (Nie et al., 2025), a state-of-the-art open-sourced\ndLLM without post-training, as the base model, and experiment on four benchmarks: two for mathematical reasoning\n(GSM8K (Cobbe et al., 2021) and MATH500 (Lightman et al., 2023)) and two for logical reasoning (Countdown (Pan\net al., 2025) and Sudoku (Arel, 2025)). We follow the same train-test splitting, reward functions, and evaluation protocol\nas D1 and WD1, except for Sudoku. For Sudoku, to avoid train-test leakage, we take the training set from D1 and split\nthe data by Sudoku answers, ensuring that the test set contains entirely new puzzle solutions. This guarantees that the\nmodel cannot solve test puzzles merely by memorizing possible answers. All experiments are conducted in the zero-shot\nsetting, except for Sudoku, where 3-shot generation is used for both training and evaluation2. For all models, we employ\nLow-Rank Adaptation (LoRA) with a rank of r = 128 and scaling factor α = 64. For SPG, we report results using both",
            "content": "1Proposition 1 extends directly to single, coordinate-independent optimizer ω obtained by minimizing the sum of coordinate-wise variances. 2We use 3-shot generation for Sudoku because zero-shot is too difficult for this task, resulting in very few meaningful RL rollouts. Few-shot examples used in our experiments are provided in Section D.3. 6 Table 1 Model performance on four reasoning benchmarks. The best results are bolded and the second best are underlined. SPG consistently outperforms all other methods. We denote the absolute gain of test accuracy to the previous state-of-the-art in green. GSM8K (0-shot) MATH500 (0-shot) Countdown (0-shot) Sudoku (3-shot) Model / Seq Len LLaDA-8B-Inst. LLaDA-1.5 D1 WD1 UniGRPO SPG w/ EUBO SPG w/ Mixture 128 69.5 70.4 72.2 74.6 74.9 77. 256 77.2 80.5 80.6 81.5 82.5 83.8 512 79.8 81.9 81.3 83.0 82.7 83.9 128 28.2 26.8 31.4 31.0 32.4 33. 256 32.4 32.2 36.0 37.4 37.4 37.6 512 34.6 35.8 39.4 39.0 39.4 39.4 128 18.8 21.9 30.9 48.8 44.5 68. 78.5+3.6 86.1+3.6 84.5+1.5 33.4+1.0 40.0+2.6 41.8+2. 68.8+20 256 16.8 21.1 30.9 52.3 43.0 71.5 70.7+18 512 16.8 21.5 34.4 50.8 57.0 68.0 128 5.7 7.4 7.2 33.1 59. 81.2 256 27.7 26.9 32.5 32.1 67.0 87.1 512 26.2 29.0 29.3 22.5 62.9 89.9 70.3+ 82.9+24 94.0+27 93.1+30 Figure 3 Reward dynamics of SPG w/ Mixture during RL training, compared with D1, WD1, and UniGRPO. SPG consistently leads to faster convergence and higher reward level. We report mean and standard deviation over rolling window of 50 steps. LEUBO (i.e., SPG w/ EUBO) and LMix (i.e., SPG w/ Mixture) for negative advantage traces. We select the value of β in the EUBO from {1.0, 1.5, 2.0} based on the best average test accuracy across all generation lengths, and fix the mixture coefficient ω at 0.5. Further experimental details are in Section D.1 and Section D.2. Baselines. We compare our method with several recent RL algorithms for dLLMs, including D1 (Zhao et al., 2025), WD1 (Tang et al., 2025), and UniGRPO (Yang et al., 2025). For D1 and WD1, we reproduce results using the official codebases and instructions, and for fair comparison, we omit the additional SFT stage in D1 across all models. For UniGRPO, since the code is not publicly available and the original work focuses on vision-language multimodal models, we reimplement the algorithm within our setup. For consistency, we set the number of inner gradient updates µ to 4 for all models, following GRPO (Shao et al., 2024). We also evaluate LLaDA-1.5 (Zhu et al., 2025) under our settings, which fine-tune LLaDA-8B-Instruct using VRPO, preference optimization approach on 350K preference pairs. For both RL rollouts and evaluation, we use the semi-autoregressive confidenceGeneration and Evaluation Setup. based decoding strategy, following LLaDA, D1 and WD1. We apply the same generation setup as D1, with the denoising timestep set to half the total sequence length. The sequence is divided into blocks of 32 tokens, and in diffusion step, we unmask the 2 tokens with the highest confidence (measured by the probability of the sampled token) within the current incomplete block. During RL rollout, to encourage diverse outputs, we use generation length of 256 and sampling temperature of 0.9 across all benchmarks, except for sudoku, where the temperature is set to 0.3 as in D1. During evaluation, the sampling temperature is set to 0.0. We evaluate the models every 100 steps, reporting results from the checkpoint that achieves the highest average test accuracy across generation lengths of 128, 256, and 512. Results. We provide the performance of SPG on each benchmark in comparison to the base model and other baselines in Table 1. Both SPG w/ EUBO and SPG w/ Mixture consistently achieve significant improvements over the baselines across all tasks and generation lengths, with the Mixture approach that combines ELBO and EUBO for negative advantage traces yielding the best performance. In particular, at generation length of 256, SPG w/ Mixture improves the test accuracy over the previous state-of-the-art by 3.6% on GSM8K, 2.6% on MATH500, 18% on Countdown, and 27% on Sudoku, showcasing the effectiveness of SPG to conduct RL for dLLMs. Reward dynamics throughout training are illustrated in Figure 3, where SPG shows rapid and steady increase in reward over the optimization steps, further demonstrating its efficiency and robustness. We provide additional results and comparisons to the baselines in Table 4 and Section E.1. 7 Table 2 Ablations on log-likelihood estimation methods for negative advantage traces. The best results are bolded and the second best underlined. We denote the absolute gain of test accuracy to SPG w/ ELBO in green. SPG w/ Mixture consistently outperforms other likelihood estimation methods. Table 3 Ablations on the masking strategies in Monte Carlo estimation. We denote the absolute gain of test accuracy to random masking for each model in green. Our block-wise masking strategy leads to consistent improvement to random masking on both benchmarks. Model SPG wo/ neg SPG w/ ELBO SPG w/ EUBO SPG w/ Mixture GSM8K MATH500 Countdown 77.4 80.9 81.6 32.7 37.4 36.7 45.5 67.1 69.3 Sudoku 68.8 82.4 86.1 Model SPG w/ EUBO SPG w/ Mixture 83.1+2.2 38.4+1.0 69.9+2.8 90.0+7.6 Masking random block-wise random block-wise MATH500 Countdown 36.7 36.7+0.0 36.9 38.4+1.5 45.4 69.3+23.9 62.8 69.9+7."
        },
        {
            "title": "4.2 Ablations and Further Analysis",
            "content": "We conduct series of ablation studies to gain deeper insights from the following aspects: The contribution of each individual component, including log-likelihood estimation methods for negative advantage traces  (Table 2)  and the masking strategy in Monte Carlo estimation  (Table 3)  . The effect of key hyperparameters, including β that controls the tightness of the upper bound and the mixture coefficient ω (Figure 5). The robustness of our approach under various inference strategies (Figure 6). Due to computational constraints, some ablation experiments are conducted on representative mathematical reasoning benchmark (MATH500) and logical reasoning benchmark (Countdown). Unless otherwise noted, we report average test accuracy across generation lengths 128, 256, and 512 for the ablation studies, with detailed results for each generation length provided in Section E.2. In Section E.2, we also investigate alternative log-likelihood estimation methods for positive advantage traces in place of ELBO, as detailed in Table 11, and study the diversity of model generations by evaluating the pass@K performance of each model in Table 12. Ablations on Algorithm Components. We first study the impact of different log-likelihood estimation methods for negative advantage traces in Table 2. Specifically, we compare our approach using LEUBO or LMix with those using LELBO (SPG w/ ELBO) or omitting the negative advantage loss entirely (SPG wo/ neg). Removing the negative advantage loss results in substantial performance drop, highlighting the importance of negative advantage penalties to RL. Additionally, both Mixture and EUBO methods outperform ELBO (except for EUBO in MATH500), showcasing the benefits of evidence upper bound regularization for negative rewards. We provide complete results for each generation length in Table 6. The effect of log-likelihood estimation methods is further illustrated by the reward dynamics of each model in Figure 4, taking Sudoku as an example. SPG w/ ELBO converges rapidly during training but plateaus early, as minimizing the lower bound does not necessarily minimize the true log-likelihood for negative advantage traces. In contrast, SPG w/ EUBO achieves higher final rewards but converges more slowly and less stably. Combining both, SPG w/ Mixture attains fast, stable convergence and high rewards, leading to an effective balance. This aligns with our discussions in Section 3.3. Figure 4 Reward dynamics of different loglikelihood estimation methods for negative advantage traces on Sudoku. SPG w/ Mixture leads to both fast convergence and high rewards. We also conduct ablations on the masking strategies in Monte Carlo estimation of LELBO, LEUBO, and LMix. As shown in Table 3, the block-wise masking strategy outperforms random masking, demonstrating the importance of aligning input distributions between policy rollout and optimization. We provide complete results for each generation length in Table 7. Ablations on Key Hyperparameters β and ω. We first examine the effect of β, crucial hyperparameter in evidence upper bound estimation, in panels (a)-(d) of Figure 5. In general, relatively small value of β (i.e., close to 1.0) leads to tighter bound and thus better performance. Nevertheless, SPG consistently performs well across range of β values 8 Figure 5 (a)-(d): ablations on the effect of β in the upper bound; (e)-(f): ablations on the mixture coefficient ω. The best performed β 1 and ω [0, 1] are marked by triangle in each setting. Figure 6 Ablations on inference strategies, including different combinations of decoding orders (i.e., semi-autoregressive (semi-AR) decoding with varying block sizes and full sequence decoding) and unmasking approaches (i.e., confidence-based and random unmasking). We set generation length to 256 and report the average accuracy across four benchmarks. SPG consistently outperforms all baselines by large margin across different inference strategies. on most tasks, indicating its robustness. For our main results in Table 1, we fix ω = 0.5 and select the optimal β 1, resulting in β = 1.0 for Sudoku and β = 1.5 for the other three benchmarks, except for Countdown with SPG w/ EUBO where β = 2.0. Besides, since the ELBO corresponds to the case of β = 0 theoretically and EUBO corresponds to β 1, we also investigate intermediate values 0 < β < 1, which may serve as an implicit mixture of lower and upper bounds. However, it is unstable in Sudoku and underperform SPG w/ Mixture on most benchmarks. We also experiment on the effect of the mixture coefficient ω, keeping β fixed at its optimal value determined for ω = 0.5 as mentioned before. As illustrated in panels (e)-(f) of Figure 5, combining lower and upper bounds with ω (0, 1) leads to better performance than leveraging either bound solely, resulting in an inverted U-shaped curve. This observation is consistent with our analysis in Proposition 1 and Section 3.3. We provide complete ablation results of β and ω for each generation length in Table 8 and Table 9. In the above experiments, we adopt consistent state-of-the-art inference setup Ablations on Inference Strategies. during both RL rollout and evaluation, i.e., confidence-based, block-wise semi-autoregressive generation with block size of 32. The same configuration and block size are also used in our block-wise masking strategy. This raises the question of whether our approach generalizes well to alternative inference strategies. To assess this, we evaluate the base model and all RL fine-tuned models using various inference strategies, as shown in Figure 6. Despite being trained under confidence-based semi-AR decoding, SPG consistently outperforms all baselines by substantial margin across all inference strategies, demonstrating its robustness and strong generalizability. Complete results for each benchmark individually are provided in Table 13."
        },
        {
            "title": "5 Related Work",
            "content": "Building on the remarkable success of diffusion models for image generation in Diffusion Language Models. continuous domains (Song et al., 2020; Ho et al., 2020), researchers have explored their extension to discrete data such as text. Initial attempts focused on training continuous diffusion models in the text embedding space (Li et al., 2022; Gong et al., 2022; Han et al., 2022; Sahoo et al., 2025a), while they face challenges in optimization and generalization due to the discrete nature of text data. Masked diffusion models (Lou et al., 2023; Zheng et al., 2023; Campbell et al., 2024; Sahoo et al., 2024; Shi et al., 2024) address this by defining the diffusion process directly in the discrete token space, using random masking as the forward process, and have achieved strong empirical results. Block Diffusion (Arriola 9 et al., 2025) further advances this direction by combining the strengths of autoregressive models, such as the capability to generate variable-length outputs and using KV cache to accelerate inference, with the benefits of diffusion language models like parallel decoding and flexible, any-order generation within blocks. Recently, large-scale diffusion language models trained with masked diffusion objectives have demonstrated performance competitive with similarly sized autoregressive models (Nie et al., 2025; Gong et al., 2024). More recent works (Wu et al., 2025; Ma et al., 2025; Liu et al., 2025a; Sahoo et al., 2025a,b) have introduced caching and parallel decoding algorithms that greatly enhance the inference efficiency of dLLMs. The seminal works apply reinforcement learning to large Reinforcement Learning for LLMs and Reasoning. language models (LLMs) to align them with human preferences via reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022). More recently, reinforcement learning has proven highly effective at enhancing the reasoning abilities of LLMs during the post-training stage, where rewards can be provided by process reward model (Lightman et al., 2023) or verifiable reward signals. Algorithms such as Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO) constrain policy updates to trust region, reducing variance and promoting stable learning by preventing excessive shifts from the reference policy (Schulman et al., 2015, 2017). Group Relative Policy Optimization (GRPO) (Shao et al., 2024) introduces group-relative rewards, enabling efficient training without the need for an additional value (critic) model. GRPO and its variants have demonstrated strong empirical performance in state-of-the-art models such as DeepSeek-R1 (Guo et al., 2025), particularly on mathematical reasoning tasks, where incorporating long reasoning traces with self-reflection and verification steps yields significant improvements. Recent works (Liu et al., 2025b; Zheng et al., 2025; Team et al., 2025; Cohen et al., 2025) further improve RL algorithms for LLMs by reducing the bias introduced by the GRPO objective, enhancing sample efficiency, and introducing additional regularization. Reinforcement Learning for Diffusion Language Models. Numerous studies have explored RL-based fine-tuning algorithms for diffusion models with continuous objectives (Fan et al., 2023; Black et al., 2023; Clark et al., 2023). While RL algorithms have achieved notable success to LLMs and continuous diffusion models, their applications to diffusion language models in the discrete space remain underexplored. DRAKES (Wang et al., 2024) leverages reward backpropagation along the denoising trajectory, but is computationally intensive for large scale models as the gradients are propagated through each denoising step. Alternatively, methods like D1 (Zhao et al., 2025) and UniGRPO Yang et al. (2025) utilize the GRPO framework, approximating the log-likelihood through either one-step unmasking (as in D1) or Monte Carlo estimation using the ELBO (as in UniGRPO). VRPO (Zhu et al., 2025) adapts DPO (Rafailov et al., 2023) to fine-tune dLLMs by applying MC estimation of the ELBO. WD1 (Tang et al., 2025) starts from the GRPO formulation and the same log-likelihood estimation as in D1, while avoiding direct estimation of the old and reference policy log-likelihoods by integrating them into weighted policy optimization objective. Despite these advances, principled analysis of RL algorithms for dLLMs, especially the challenging log-likelihood estimation, is missing. This results in substantial bias in the optimization objective and suboptimal performance."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose SPG, novel reinforcement learning algorithm for diffusion large language models. SPG addresses the intractable log-likelihood in dLLMs by maximizing tractable lower bound on positive reward sequences and minimizing an upper bound on negative ones, resulting in more robust and less biased policy gradient. Additionally, we propose block-wise masking strategy for Monte Carlo estimation to enhance optimization stability and efficiency. Extensive experiments on four mathematical and logical reasoning benchmarks demonstrate the superior performance of SPG, achieving significant improvement over baselines and the state-of-the-art performance."
        },
        {
            "title": "References",
            "content": "Arel. Arels sudoku generator. https://www.ocf.berkeley.edu/ arel/sudoku/main.html, 2025. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Taco Cohen, David Zhang, Kunhao Zheng, Yunhao Tang, Remi Munos, and Gabriel Synnaeve. Soft policy optimization: Online off-policy rl for sequence models. arXiv preprint arXiv:2503.05453, 2025. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023. Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control. arXiv preprint arXiv:2210.17432, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. github, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 11 Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. Tinyzero. https://github.com/Jiayi-Pan/TinyZero, 2025. Accessed: 2025-01-24. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Alfréd Rényi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pages 547562. University of California Press, 1961. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37: 130136130184, 2024. Subham Sekhar Sahoo, Justin Deschenaux, Aaron Gokaslan, Guanghan Wang, Justin Chiu, and Volodymyr Kuleshov. The diffusion duality. arXiv preprint arXiv:2506.10892, 2025a. Subham Sekhar Sahoo, Zhihan Yang, Yash Akhauri, Johnna Liu, Deepansha Singh, Zhoujun Cheng, Zhengzhong Liu, Eric Xing, John Thickstun, and Arash Vahdat. Esoteric language models. arXiv preprint arXiv:2506.01928, 2025b. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 18891897. PMLR, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37:103131103167, 2024. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Bowen Sun, Yujun Cai, Ming-Hsuan Yang, and Yiwei Wang. Blockwise sft for diffusion language models: Reconciling bidirectional attention and autoregressive decoding. arXiv preprint arXiv:2508.19529, 2025. Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discrete diffusion models. arXiv preprint arXiv:2211.16750, 2022. Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. wd1: Weighted policy optimization for reasoning in diffusion language models. arXiv preprint arXiv:2507.08838, 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Tim Van Erven and Peter Harremos. Rényi divergence and kullback-leibler divergence. IEEE Transactions on Information Theory, 60 (7):37973820, 2014. Chenyu Wang, Masatoshi Uehara, Yichun He, Amy Wang, Tommaso Biancalani, Avantika Lal, Tommi Jaakkola, Sergey Levine, Hanchen Wang, and Aviv Regev. Fine-tuning discrete diffusion models via reward optimization with applications to dna and protein design. arXiv preprint arXiv:2410.13643, 2024. 12 Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025a. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. Revolutionizing reinforcement learning framework for diffusion large language models. arXiv preprint arXiv:2509.06949, 2025b. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Lin Zheng, Jianbo Yuan, Lei Yu, and Lingpeng Kong. reparameterized discrete diffusion model for text generation. arXiv preprint arXiv:2302.05737, 2023. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        },
        {
            "title": "A Basics of dLLMs",
            "content": "In this section, we provide more self-contained overview of masked dLLMs. Please also refer to Sahoo et al. (2024) for more details. Notation. We denote scalars by lowercase letters (x), vectors by bold lowercase (x), and sequences by x1:n. superscript (e.g., xj) denotes an items index within group. We define the set of the first integers as [k] := {1, . . . , k} and the k-dimensional probability simplex as k1. Distributions include the categorical Cat( p) and the uniform U[a, b]. Throughout the paper, we use the following primary indices: [n] for position, [g] for sequence in group, and [0, 1] for the continuous diffusion timestep. We start from discrete time version of the diffusion models with finite [T ]. Assume one-hot categorical variable {e1, . . . , ek} k1. Further assume we gradually corrupt into an absorbing state (i.e., e[mask] ) with transition matrix Qt at time t. Then: q(zt x) = Cat(zt Qtx) = Cat(zt (cid:89) τ = Qτ x). Here, zt is also one-hot categorical random variable in k1. In practice, one could choose Qt such that: q(zt x) = Cat(zt αtx + (1 αt)m). < 0. Here, α1 = 1, αT = 0, α Normally, the goal is to construct the lower bound of the evidence (ELBO) and maximize it. For this particular case, consider the discretized Markov chain with latent variables z1, z2, . . . , zT , where zT = and z1 = x. We use the shorthand = z1:T and write LELBO(x; θ) = Ezq(x) (cid:20) log (cid:21) pθ(x, z) q(z x) (cid:20) =Ezq(x) log pθ(x z1) (cid:124) (cid:125) (cid:123)(cid:122) =0 + 1 (cid:88) t=1 log pθ(zt zt+1) q(zt zt+1, x) (cid:21) + log (cid:124) pθ(zT ) q(zT x) (cid:125) (cid:123)(cid:122) =0 (10) = = 1 (cid:88) t=1 1 (cid:88) t= Ezt,zt+1q (cid:20) log (cid:21) pθ(zt zt+1) q(zt zt+1, x) Ezt+1q(x)Eztq(zt+1,x) (cid:20) log pθ(zt zt+1) q(zt zt+1, x) (cid:21) . Here, log pθ(x, z1) = 0 because we assume z1 = x, and pθ(zT ) = q(zT x) because we assume zT = m. common method to parameterize pθ is via predicting with model πθ in q: pθ(zt zt+1) = (zt zt+1, = πθ( zt+1)) . Now, given that zt+1 is either or (assuming = x). Then the KL term in equation 10 decomposes into the following. log pθ(zt zt+1) q(zt zt+1, x) = zt = zt+1 = x, 0 0 zt = m, zt+1 = x, log πθ(x zt+1) zt = x, zt+1 = m, 0 zt = zt+1 = m. (Impossible) (11) 14 Moreover, q(zt = zt+1 = m, x) = αtαt+1 1αt+1 , and note that πθ(x zt) = 1 when zt = x, so we have: LELBO(x; θ) = = 1 (cid:88) t=1 1 (cid:88) t=1 Ezt+1q(x) Ezt+1q(x) (cid:20) αt αt+1 1 αt+1 (cid:20) αt αt+1 1 αt+1 Taking the above limit as , we have: (cid:21) log πθ(x zt+1)1(zt+1 = m) (cid:21) . log πθ(x zt+1) (If zt+1 = x, then log πθ(x zt+1) = 0) LELBO(x; θ) = (cid:90) 1 t=0 Eztq(x) (cid:20) α αt log πθ(x zt) (cid:21) . (12) (13) Generalization to Sequence becomes sequence of categorical variables x1:n. Then we write The above is for single categorical variable x. In practice as in language modeling, it LELBO(x1:n; θ) = Ez1:nq(x1:n) (cid:20) log (cid:21) pθ(x1:n, z1:n) q(z1:n x1:n) = E{ziq(xi)}n i=1 (cid:20) (cid:88) i=1 log pθ(xi, z1:n) q(zi xi) (cid:21) (Independence of q( xi)) E{zi q(xi )}n i=1 (cid:20) log (cid:21) pθ(xi, z1:n) q(zi xi) LELBO(xi; θ). = = (cid:88) i=1 (cid:88) i=1 (14) The key distinction from the single-token formulation (mentioned beforehand) is that the reverse process pθ is conditioned on all z1:n instead of single tokens zi."
        },
        {
            "title": "B Evidence Upper Bound for dLLMs",
            "content": "In this section, we provide the derivation of the evidence upper bound. Following the above section, we start from the discrete time version of the diffusion models. Lemma 1 (Rényi Variational Bound; Rényi (1961); Van Erven and Harremos (2014)). Fix an observation x. Let q( x) be any distribution on such that p( x) q( x), denoting that p( x) is absolutely continuous with respect to q( z). Then, the following holds for any β 1: Ezq(x) (cid:20) log (cid:21) p(x, z) q(z x) log p(x) 1 β log Ezq(x) (cid:34)(cid:18) p(x, z) q(z x) (cid:19)β(cid:35) . (15) In view of the above lemma, we derive an evidence upper bound for masked diffusion models in the following theorem. Theorem 1 (Evidence Upper Bound for Masked Diffusion). Assume the forward denoising process has steps with monotonic schedule αt. For any β 1 and sequence of categorical variables x1:n, we have: log πθ(x1:n) LEUBO(x1:n; θ), (16) where LEUBO(x1:n; θ) := 1 β (cid:88) log 1 (cid:88) i=1 t=1 (cid:34) Ezt+1 αt αt+1 1 αt+1 1(zt+1,i = m) πβ (cid:35) θ (xi zt+1) + C(T ), (17) and C(T ) = (cid:40) 1 β log Ez1:T q(x) 0, (cid:104) q(z1:T x)n(cid:105) , is constant independent of θ. 1 β < β Proof. We first consider the case with single categorical variable x. On the account of Lemma 1 and following similar argument as in equation 10, for any β 1, we can write log πθ(x) 1 β = log Ezq(x) (cid:19)β(cid:35) (cid:34)(cid:18) pθ(x, z) q(z x) 1 β log Ez1:T q(x) (cid:20) 1 (cid:89) t=1 (cid:18) pθ(zt zt+1) q(zt zt+1, x) (cid:19)β(cid:21) Note that the sequence z1:T has form {x, . . . , x, m, . . . , m}. Define the transition event: Then, by the law of total expectations, equation 18 can be expressed as: At := {zt = x, zt+1 = m} log Ez1:T q(x) (cid:20) 1 (cid:89) t=1 P(At)Ezq(x) (cid:19)β(cid:21) (cid:18) pθ(zt zt+1) q(zt zt+1, x) (cid:34)T 1 (cid:89) (cid:18) pθ(zs zs+1) q(zs zs+1, x) (cid:35) At (cid:19)β(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) s=1 1 β 1 β 1 β 1 β = = = log log log 1 (cid:88) t=1 1 (cid:88) t=1 1 (cid:88) t=1 (cid:20) (cid:20) Ezt+1q(x) Ezt+1q(x) 1(zt+1 = m)q(zt = zt+1 = m, x) 1(zt+1 = m) αt αt+1 1 αt+1 (cid:21) πβ θ (x zt+1) (cid:18) pθ(zt = zt+1 = m) q(zt = zt+1 = m, x) (cid:19)β(cid:21) (18) (19) (20) The penultimate line is due to the fact that conditioned on the event At, the ratio pθ (zszs+1) is equal to one for any q(zszs+1,x) = t. The last line uses the formula for q. The indicator 1(zt = m) appears in the final expression because the terms in the bound are only non-trivial when the model must make prediction from corrupted state. Now we generalize the above to sequence of categorical variables = x1:n. Similar as Equation (18), we have log πθ(x1:n) 1 β log Ez1:T q(x) (cid:20) 1 (cid:89) (cid:89) t=1 i= (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) (cid:19)β(cid:21) Denote C(T ) := 1 β log Ez1:T q(x) (cid:104) q(z1:T x)n(cid:105) . The upper bound in the RHS can be further derived as 1 β 1 β 1 β 1 β 1 β 1 β = = = = log Ez1:T q(x) (cid:20) 1 (cid:89) (cid:89) t=1 i=1 (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) (cid:19)β(cid:21) log Ez1:T q(x) (cid:20) q(z1:T x)n (cid:89) (cid:88) q(yi 1:T x)1(yi 1:T = z1:T ) log Ez1:T q(x) (cid:20) q(z1:T x)n i=1 yi 1:T (cid:89) (cid:88) i=1 yi 1:T q(yi 1:T x) 1 (cid:89) t=1 (cid:32) log Ez1:T q(x) q(z1:T x)n(cid:105) (cid:104) (cid:18) (cid:89) (cid:88) q(yi 1:T x) Ez1:T q(x) (cid:20) 1 (cid:89) i=1 yi (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) 1:T (cid:19)β(cid:21) (cid:89) i=1 log (cid:88) i=1 log Ez1:T q(x) t=1 (cid:20) 1 (cid:89) t=1 (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) (cid:19)β(cid:21) + C(T ) 1 (cid:89) t=1 t,i yi (cid:18) pθ(yi q(yi t,i yi t+1 t+1, x) t,i yi (cid:18) pθ(yi q(yi t,i yi t+1 t+1, x) (cid:19)β(cid:21) 1 (cid:89) t=1 t,i yi (cid:18) pθ(yi q(yi t,i yi t+1 t+1, x) (cid:19)β(cid:19)(cid:33) + 1 β log Ez1:T q(x) q(z1:T x)n(cid:105) (cid:104) (cid:19)β(cid:21) (21) Here, yi 1:T = z1:T ). C(T ) is constant independent of θ, and the first term in Equation (21) can be derived similar to the single variable case in Equation (20): are copies of z1:T enforced to agree with z1:T using the indicator 1(yi 1:T (cid:88) i=1 (cid:88) 1 β 1 β = log Ez1:T q(x) 1 (cid:88) log Ezt+1q(x) i=1 t=1 (cid:20) 1 (cid:89) t=1 (cid:34) (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) (cid:19)β(cid:21) αt αt+1 1 αt+1 1(zt+1,i = m) πβ (cid:35) θ (xi zt+1) Additionally, for β n, we can obtain tighter bound using Hölders inequality: 1 β log Ez1:T q(x) (cid:20) 1 (cid:89) (cid:89) (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) (cid:19)β(cid:21) t= i=1 (cid:20) 1 (cid:89) log Ez1:T q(x) 1 (cid:88) log t=1 (cid:34) Ezt+1q(x) (cid:88) i=1 (cid:88) = = 1 nβ 1 nβ i=1 t=1 (cid:18) pθ(zt,i zt+1) q(zt,i zt+1, x) (cid:19)nβ(cid:21) αt αt+1 1 αt+ 1(zt+1,i = m) πnβ θ (xi zt+1) (22) (cid:35) Denote β := nβ. Then the upper bound log πθ(x1:n) LEUBO(x1:n; θ) holds with the constant term C(T ) being zero and the hyperparameter β n. Combining these two cases, we have C(T ) = (cid:40) C(T ), 0, 1 β < β . Furthermore, we can derive the continuous time version by omitting the constant term that does not affect the gradient with respect to θ, and taking the limit of similar as the derivations for LELBO, as shown in Corollary 1: Corollary 1. Taking the limit of , we have: θLEUBO(x1:n; θ) = θ (cid:16) LEUBO(x1:n; θ) + C(T ) (cid:17) = θ LEUBO(x1:n; θ), where LEUBO(x1:n; θ) = 1 β (cid:88) i=1 (cid:104) log Et,zt w(t) 1(zt,i = m) πβ (cid:105) θ (xi zt) . (23) One caveat of the above LEUBO is that the log is outside of the expectation, which in general makes Monte Carlo sample estimates biased. One could certainly further loosen the bound using the inequality log 1: LEUBO(x) 1 β (cid:88) i= EtU [0,1],ztq (cid:34) w(t) 1(zt,i = m) πβ θ (xi zt) (cid:35) β (24) But in practice we found this results in much worse performance, as demonstrated in Table 10, potentially due to the much larger gap between EUBO and likelihood."
        },
        {
            "title": "C Additional Analysis on Upper and Lower Bounds",
            "content": "C.1 Proof of Proposition 1 Proposition 1 (Optimal Mixture Strictly Reduces Variance). Fix coordinate and let ρβ := w(t, zt)πβ θ (xi zt, c)/E (cid:104) w(t, zt)πβ (cid:105) , θ (xi zt, c) 17 where w(t, zt) := w(t)1(zt = m). Then, the gradient of mixture objective (8) is given by gω,k = ((1 ω)w(t, zt) + ωρβ) θk log πθ(x zt, c). (25) If Var((ρβ w(t, zt))θk log πθ(x zt, c)) > 0, then Var[gω,k] is strictly convex quadratic in ω and thus admits unique minimizer ω k. Moreover, Var[gω k,k] < min(cid:8) Var[g0,k], Var[g1,k](cid:9), Proof. We first derive the formulas for the gradient of each objective. Consider specific example xi. The gradient of the LELBO and LELBO are given by: θLELBO = [w(t, zt) log πθ(xi zt, c)] (cid:105) (cid:104) w(t, zt)πβ θ (xi zt, c) log πθ(xi zt, c) (cid:104) w(t, zt)πβ (cid:105) θ (xi zt, c) θ LEUBO = Then the gradient of the mixture objective LMix is given by: θ LMix = (cid:104)(cid:16) (1 ω)w(t, zt) + ωρβ (cid:17) (cid:105) θ log πθ(xi zt, c) (26) (27) (28) We further compute the per-parameter (per-dimension) variance of the gradient of LMix and consider the optimal mixture coefficient ω to minimize the variance. For simplicity, we use the following short-hand notation: We denote the k-th coordinate of the gradient θ LMix by gω,k. Then, the coordinate-wise variance of the gradient is given by sk := θk log πθ(xi zt, c) Var(cid:2)gω,k (cid:3) = (cid:104)(cid:0)(1 ω) + ω ρβ (cid:1) (cid:16) (cid:105) s2 E(cid:2)(cid:0)(1 ω) + ω ρβ (cid:1) sk (cid:3)(cid:17)2 = Var(wsk) + 2ω Cov(wsk, (ρβ w)sk) + ω2 Var((ρβ w)sk) where we used the shorthand w(t, zt). The above expression is quadratic in ω and we find the optimal ω by setting the derivative of variance to zero: ω Var(cid:2)gω,k (cid:3) = 2 Cov(cid:0)w sk, (ρβ w) sk (cid:1) + 2ω Var(cid:0)(ρβ w) sk (cid:1) = 0 ω = Cov(cid:0)w sk, (ρβ w) sk Var(cid:0)(ρβ w) sk (cid:1) (cid:1) . The above yields per-coordinate optimal ω . Equivalently, we can write ω Var(cid:0)w sk (cid:1) Cov(cid:0)w sk, ρβ sk (cid:1) + Var(cid:0)ρβ sk (cid:1) 2 Cov(cid:0)w sk, ρβ sk (cid:1) as follows: (cid:1) ω = Var(cid:0)w sk is minimizer of coordinate-wise variance in the non-degenerative case with Var(cid:0)(ρβ w) sk Furthermore, ω as the variance is strongly convex in ω. The coordinate-wise variance of gradients in LELBO (ω = 0) and LELBO (ω = 1), and the optimal mixture coefficient ω are then given by (cid:1) > 0, LELBO : Var(cid:2)g0,k LELBO : Var(cid:2)g1,k (cid:3) = Var(cid:2)w sk (cid:3) = Var(cid:2)w sk Optimal: Var(cid:2)gω k,k (cid:3) = Var(cid:2)w sk (cid:3), (cid:3) + 2 Cov(cid:0)w sk, (ρβ w) sk Cov(cid:0)w sk, (ρβ w) sk (cid:1) Var(cid:0)(ρβ w) sk (cid:3) (cid:16) (cid:1) + Var(cid:0)(ρβ w) sk (cid:1)(cid:17)2 (cid:1), , 18 Figure 7 Dynamics of the gradient norm of models trained with different log-likelihood estimation methods. SPG w/ Mixture achieves lower gradient norm and more stable optimization. We report mean and standard deviation over rolling window of 50 steps. The difference between the variance of LELBO and LELBO with the optimal mixture coefficient can then be derived as follows: Var(cid:2)w sk (cid:3) Var(cid:2)gω k,k (cid:3) = Var(cid:2)ρβ sk (cid:3) Var(cid:2)gω k,k (cid:3) = (cid:16) (cid:16) Cov(cid:0)w sk, (ρβ w) sk Var(cid:0)(ρβ w) sk (cid:1) Cov(cid:0)w sk, (ρβ w) sk (cid:1)(cid:17)2 0 (cid:1) + Var(cid:0)(ρβ w) sk (cid:1)(cid:17)2 Var(cid:0)(ρβ w) sk (cid:1) 0 C.2 Additional Comparison Between the Mixture Loss and the Lower and Upper Bounds Consider the ratio of the coefficient of score function θ log πθ(xi zt, c) Comparing Mixture with the Lower Bound. in the gradient in the case of the mixture objective (i.e., θ LMix in Equation (28)) over using only the lower bound (i.e., θLELBO in Equation (26)): wMix wELBO = (1 ω)w(t, zt) + ωρβ w(t, zt) = (1 ω) + ω πβ θ (xi zt, c) w(t, zt)πβ (cid:105) θ (xi zt, c) (cid:104) (cid:105) θ (xi zt, c) Treating the expectation over all samples (cid:104) w(t, zt)πβ as constant (since it is averaged), the second term θ (xi zt, c). This realizes confidence-aware weighting: uncertain tokens θ (xi zt, c), i.e., those with low recovery chance, have smaller weight, while confident tokens with in the above ratio is strictly increasing in πβ with small πβ large πβ Comparing Mixture with the Upper Bound. We compute the ratio of coefficient of score function in the gradient of upper bound (i.e., θ LEUBO in Equation (27)) over the mixture gradient: θ (xi zt, c) are upweighted, with sharpness being controlled by parameter β and the blend by ω. wEUBO wMix = ωρβ (1 ω)w(t, zt) + ωρβ θ (xi zt, c) is very small, the coefficient of score function in θ LEUBO, wEUBO, Considering the above ratio, when πβ becomes very small, preventing updates to the parameters. However, the mixing approach maintains per-sample weights by preventing that from collapsing to (near) zero. In other words, for each sample, the mixture coefficient computes convex interpolation that simultaneously floors very small EUBO weights to minimum value and applies an uncertainty-aware capping to large EUBO weights. Empirical Evidence of Reduced Gradient Variance. As practical indicator of gradient variance, we plot the gradient norm of each model trained with different log-likelihood estimation methods for negative advantage traces in Figure 7. When using the mixture objective, the model has consistently smaller and more stable gradient norm throughout training, aligning well with our theoretical analysis. 19 C.3 Toy Example for Upper and Lower Bounds. In this section, we provide toy example highlighting the contrasting behaviors and landscapes of the upper and lower bounds, further demonstrating the necessity to select the appropriate bound for optimization based on the optimization direction. Consider simple case where the sequence length is 2 and the vocabulary size is 2, i.e., = [x1, x2] and = {A, B}. Then, We can calculate LELBO and LEUBO in closed form: LELBO(x = AA) = (cid:104) 1 2 log πθ(x1 = MA) + log πθ(x1 = MM) + log πθ(x2 = AM) + log πθ(x2 = MM) θ (x1 = MA) + πβ θ (x1 = MM) (cid:16) πβ LEUBO(x = AA) = log 1 β + 1 β log (cid:16) πβ θ (x2 = AM) + πβ θ (x2 = MM) 2 (cid:105) (cid:17) (cid:17) (29) (30) (31) (32) For simplicity, denote := πθ(x1 = MA) and := πθ(x1 = MM), and consider the of the likelihood of the first token x1. We have LELBO(x1) = LEUBO(x1) = 1 2 1 β (log + log b) log (cid:16) aβ + bβ 2 (cid:17) Take the partial gradient with respect to and respectively, LELBO(x1) LEUBO(x1) = = 1 2a ; = 1 LELBO(x1) 2b LEUBO(x1) aβ1 aβ + bβ ; = bβ1 aβ + bβ (33) (34) (35) (36) Therefore, for LEUBO, the gradient direction is dominated by the larger one between and b, while for LELBO, the gradient direction is dominated by the smaller one. Such property is illustrated in the landscapes of LELBO and LEUBO for a, (0, 1) in Figure 8. When = AA has negative advantage, the corresponding LELBO and LEUBO are minimized. For LELBO, the model benefits more from further decreasing the smaller one between probabilities and b. In the extreme case, LELBO = when either or equals to zero, leaving the other term not sufficiently decreased. Instead, when using LEUBO for negative advantage traces, the larger one between and is preferentially minimized, leading to more balanced optimization that stably decreases the log-likelihood. Similarly, when = AA has positive advantage, the corresponding LELBO and LEUBO are maximized. Using LELBO enables effectively increasing the smaller likelihood, while LEUBO focuses on the larger one, leading to less efficient optimization."
        },
        {
            "title": "D Additional Experimental Details",
            "content": "D.1 Datasets and Reward Functions We follow the setting in D1 (Zhao et al., 2025) and WD1 (Tang et al., 2025), using the same reward functions and train-test splitting, except for Sudoku. The rewards are designed to encourage both correctness and proper formatting, with varying levels of granularity tailored for each task. For completeness, we provide details as follows. 20 Figure 8 Landscapes of LELBO and LEUBO for 0 < a, < 1. LEUBO is flatter among low value regions while sharper among high value regions, making it more suitable for log-likelihood minimization; vice versa for LELBO. GSM8K. We utilize the train split of the GSM8K dataset3 for RL training, and evaluate model performance on the test split. We follow the Unsloth reward setup4, utilizing five equally-weighted additive components: XML Structure Reward: +0.125 per correct formatting tag; small penalties for extra contents after the closing tag. Soft Format Reward: +0.5 for outputs matching the pattern: <reasoning>...</reasoning><answer>...</answer> Strict Format Reward: +0.5 for exact formatting with correct line breaks. Integer Answer Reward: +0.5 if the answer is valid integer. Correctness Reward: +2.0 of the answer matches the ground truth. MATH500. We utilize the train split of the MATH dataset5 for RL training, and evaluate model performance on the test split. We use format reward and correctness reward: Format Reward: We award 1.00 if <answer></answer> tags are present with boxed inside them; 0.75 if answer tags are present without boxed; 0.50 if answer tags are not present but boxed is present; 0.25 if neither the answer tags nor boxed is present. Correctness Reward: We award 2.00 if the answer in boxed{} matches the ground truth. Countdown. We utilize the train split of the Countdown dataset6 for RL training, restricting to instances that use only three numbers. We evaluate on the same set of 256 synthetically generated countdown questions with 3 numbers as in D1 (Zhao et al., 2025). The reward covers three cases: +1.0 if the expression reaches the target using the exact numbers; +0.1 if the numbers are correct but does not reach the target; +0.0 otherwise. Sudoku. We experiment on the 44 Sudoku dataset7 generated by Arel (2025). The original training split contains 1M unique Sudoku puzzles covering all 288 44 Soduku solutions. To avoid train-test leakage and potential cheating by memorizing all the solutions, we randomly select 200 solutions and include all puzzles corresponding to these solutions into the new training set, resulting in 694,006 training puzzles. We then randomly select 2 or 3 puzzles corresponding to the left 88 solutions to construct the test set, which has 256 Soduku puzzles in total. 3https://huggingface.co/datasets/openai/gsm8k 4https://unsloth.ai/blog/r1-reasoning 5https://huggingface.co/datasets/ankner/math-500 6https://huggingface.co/datasets/Jiayi-Pan/Countdown-Tasks-3to4 7https://github.com/Black-Phoenix/4x4-Sudoku-Dataset 21 We observe that the zero-shot setting is too difficult for the base LLaDA-8B-Instruct model, which has test accuracy below 7% with generation length of 256 and struggles to correctly interpret the questions, leading to very few meaningful RL rollouts. Therefore, we instead use 3-shot for all the Sudoku experiments. We ensure that the solutions presented in the 3-shot samples do not appear in test set solutions, and the puzzles do not appear in both train and test set. The detailed few-shot samples are provided in Section D.3. D.2 Hyperparameter Settings and Implementation Details We follow D1 (Zhao et al., 2025) for most hyperparameter settings. We employ Low-Rank Adaptation (LoRA) with rank of = 128 and scaling factor α = 64. The training was conducted on 8 NVIDIA A100-80G or NVIDIA H100-80G GPU, with the following hyperparameters: batch size of 6 per GPU, and gradient accumulation steps of 2. We set the number of inner gradient update µ as 4 for all models. We use the AdamW optimizer (Loshchilov and Hutter, 2017), with β1 = 0.9, β2 = 0.99, weight decay of 0.1, learning rate of 3 106, and gradient clipping at 0.2. We utilize Flash Attention 2 (Dao, 2023)and 4-bit quantization. For RL rollout, we use sequence length of 256 tokens, and 128 diffusion steps. We employ confidence-based semiautoregressive generation with block size 32, and set the temperature as 0.9 (except for Sudoku where temperature is set as 0.3 following D1). We set number of completions per prompt as 6, and number of Monte Carlo estimation samples as 2 due to computational constraint. Since the rollout stage dominates the training time, the average time per gradient update step for SPG is similar to that of the other baselines. We train 6000 steps (i.e., number of gradient updates) for GSM8K and Countdown, 4000 steps for MATH500, and 2500 steps for Sudoku. For all RL models, we run evaluation every 100 steps with generation sequence length 128, 256, 512, and report the result of the checkpoint with the highest average accuracy over the three generation lengths, except for the ablations on inference strategies in Table 13, where we only evaluate on generation length 256. D.3 Few-Shot Samples for Sudoku Sample 1: Question: Solve the following Sudoku puzzle: 3014002020004130 Answer: <reasoning> Interpret puzzle as 4 rows of 4: R1: 3 0 1 4 R2: 0 0 2 0 R3: 2 0 0 0 R4: 4 1 3 0 Fill easy singles: R1 missing 2 R1C2=2. R4 missing 2 R4C4=2. Box (R3-4,C3-4) then needs 1,4; column4 can only accept 1 R3C4=1, R3C3=4. R3 now missing 3 R3C2=3. Column1 missing 1 R2C1=1. Column2 missing 4 R2C2=4. Last cell R2C4=3. Final grid: R1: 3 2 1 4 R2: 1 4 2 3 R3: 2 3 4 1 R4: 4 1 3 2 </reasoning> <answer> 22 3214142323414132 </answer> Sample 2: Question: Solve the following Sudoku puzzle: 0000100420013142 Answer: <reasoning> Interpret puzzle as 4 rows of 4: R1: 0 0 0 0 R2: 1 0 0 4 R3: 2 0 0 1 R4: 3 1 4 2 Fill easy singles: Col1 missing 4 R1C1=4. Col4 missing 3 R1C4=3. Box (R1-2,C1-2) missing 2,3 and R1 now needs 1,2 R1C2=2, R2C2=3. R1C3=1. R2 now missing 2 R2C3=2. Col2 missing 4 R3C2=4, then R3C3=3. Final grid: R1: 4 2 1 3 R2: 1 3 2 4 R3: 2 4 3 1 R4: 3 1 4 2 </reasoning> <answer> 4213132424313142 </answer> Sample 3: Question: Solve the following Sudoku puzzle: 2001403002001420 Answer: <reasoning> Interpret puzzle as 4 rows of 4: R1: 2 0 0 1 R2: 4 0 3 0 R3: 0 2 0 0 R4: 1 4 2 0 Fill easy singles: R1 missing 3,4; Col2 cant be 1 so R1C2=3 R1C3=4. R4 missing 3 R4C4=3. Col4 missing 2,4; R2 must take 2 R2C4=2 R2C2=1. Col1 missing 3 R3C1=3. Col3 missing 1 R3C3=1 R3C4=4. Final grid: R1: 2 3 4 1 R2: 4 1 3 2 R3: 3 2 1 4 23 R4: 1 4 2 3 </reasoning> <answer> 2341413232141423 </answer>"
        },
        {
            "title": "E Additional Results",
            "content": "E.1 Additional Evaluations to the Main Results Complete evaluation results. We provide the complete evaluation results, along with those reported in D1 (Zhao et al., 2025) and WD1 (Tang et al., 2025), in Table 4. Our reproduced numbers closely match the reported results. d1-LLaDA (Zhao et al., 2025) denotes the model that conducts first SFT and then RL (using D1). All other models are trained solely with RL. In D1 and d1-LLaDA, the best result for each generation length is reported separately, whereas we select single checkpoint with the highest average accuracy across all three generation lengths, leading to slightly worse results than the reported numbers. The reported results in WD1 are based on evaluations on fewer checkpoints, so they are generally bit lower than our reproduced values. Table 4 Complete model performance on four reasoning benchmarks compared with baselines. We provide both the reported and the reproduced results for D1 and WD1. The best results are bolded and the second best are underlined. SPG consistently outperforms all other models. GSM8K (0-shot) MATH500 (0-shot) Countdown (0-shot) Sudoku (3-shot) Model / Seq Len LLaDA-8B-Instruct LLaDA-1.5 D1 (reported) D1 (reproduced) d1-LLaDA (reported) WD1 (reported) WD1 (reproduced) UniGRPO SPG w/ EUBO (ours) SPG w/ mixture (ours) 128 69.5 70.4 72.6 72.2 73.2 - 74.6 74.9 77.1 256 77.2 80.5 79.8 80.6 81.1 80.8 81.5 82.5 83.8 512 79.8 81.9 81.9 81.3 82.1 82.3 83.0 82. 83.9 78.5 86.1 84.5 128 28.2 26.8 33.2 31.4 33.8 - 31.0 32. 33.2 33.4 256 32.4 32.2 37.2 36.0 38.6 34.4 37.4 37.4 37.6 512 34.6 35.8 39.2 39.4 40.2 39.0 39.0 39.4 39.4 128 18.8 21.9 33.2 30.9 34.8 - 48.8 44. 68.4 40.0 41.8 68.8 256 16.8 21.1 31.3 30.9 32.0 51.2 52.3 43.0 71.5 70.7 512 16.8 21.5 37.1 34.4 42.2 46.1 50.8 57. 68.0 70.3 128 5.7 7.4 - 7.2 - - 33.1 59.0 81.2 256 27.7 26.9 - 32.5 - - 32.1 67.0 87. 512 26.2 29.0 - 29.3 - - 22.5 62.9 89.9 82.9 94.0 93.1 Dynamics of Completion Length. We provide the dynamics of the effective sequence length of SPG during RL training in Figure 9. We also report the effective length of the best checkpoint in Table 5. SPG leads to effective usage of the total given length and good adaptation to task difficulties. Figure 9 Dynamics of the effective generation length of SPG during RL training, compared with D1, WD1, and UniGRPO. SPG leads to concise solutions with better token efficiency. We report mean and standard deviation over rolling window of 50 steps. 24 Table 5 Effective sequence length of each model at the best checkpoint corresponding to Table 1 on four reasoning benchmarks. GSM8K (0-shot) MATH500 (0-shot) Countdown (0-shot) Sudoku (3-shot) Model / Seq Len LLaDA-8B-Instruct LLaDA-1.5 D1 WD1 UniGRPO SPG w/ EUBO SPG w/ mixture 128 114 115 115 115 114 110 108 256 212 214 209 225 211 196 176 512 257 265 261 312 257 227 128 123 123 123 123 123 120 121 256 235 237 234 231 235 228 229 512 402 407 399 378 400 382 128 111 114 107 83 100 68 75 256 213 215 211 84 207 70 78 512 407 411 397 90 374 78 128 111 112 111 105 113 89 115 256 232 232 231 227 230 137 239 512 448 419 449 473 472 249 E.2 Additional Ablation Results In this section, we provide the complete results for each generation length and task in supplement to Section 4.2. We also include additional ablation studies on the looser upper bound, different log-likelihood estimation methods for positive advantage traces, and Pass@K performance. Ablations on Algorithm Components. We provide the complete results for ablations on log-likelihood estimation methods in Table 6 and for ablations on masking strategies in Table 7. Table 6 Ablations on log-likelihood estimation methods for negative advantage traces. The best results are bolded and the second best are underlined. SPG w/ Mixture consistently outperforms other likelihood estimation methods. GSM8K (0-shot) MATH500 (0-shot) Countdown (0-shot) Sudoku (3-shot) Model SPG wo/ neg SPG w/ ELBO SPG w/ EUBO SPG w/ Mixture 128 72.0 75.6 77.1 78.5 256 79.0 82.8 83.8 86.1 512 81.3 84.4 83.9 84. Avg. 128 28.2 77.4 35.8 80.9 81.6 33.2 83.0 33.4 256 32.2 37.6 37.6 40.0 512 37.8 38.8 39.4 41.8 Avg. 32.7 37.4 36.7 38.4 128 43.8 66.8 68.4 68. 256 48.1 66.0 71.5 70.7 512 44.5 68.4 68.0 70.3 Avg. 45.5 67.1 69.3 69.9 128 55.0 73.8 81.2 82.9 256 82.9 89.4 87.1 94. 512 68.4 84.1 89.9 93.1 Avg. 68.8 82.4 86.1 90.0 Table 7 Ablations on the masking strategies in Monte Carlo estimation. Our block-wise masking strategy leads to consistent improvement to random masking on both benchmarks. MATH500 (0-shot) Countdown (0-shot) Model SPG w/ EUBO SPG w/ Mixture Masking random block-wise random block-wise 128 33.4 33.2 33.8 33. 256 35.4 37.6 38.2 40.0 512 41.4 39.4 38.8 41.8 Avg. 36.7 36.7 36. 38.4 128 42.6 68.4 52.3 68.8 256 41.0 71.5 64.5 70.7 512 52.7 68. 71.5 70.3 Avg. 45.4 69.3 62.8 69.9 Ablations on Key Hyperparameters β and ω. We provide the complete results for ablations on β in Table 8 and for ablations on ω in Table 9. Ablations on Inference Strategies. We provide complete results for ablations on different inference strategies in Table 13. Note that the reported numbers of each method for Semi-AR, Block=32, Confidence is in general slightly higher than the results in Table 1 under the same inference setting. This is because in Table 13, we select best checkpoint specifically for generation length 256 to maintain consistency with other inference settings, while in Table 1, we choose the checkpoint with the highest average accuracy across generation lengths 128, 256, and 512. Ablations on the Looser Upper Bound. As mentioned in Section 3.2 and Section B, looser but unbiased bound can be derived using inequalities like log(x) 1, i.e., LLoose (Equation (24)). However, as shown in Table 10, this looser bound performs worse empirically than the tighter upper bound LEUBO we used, possibly due to larger discrepancy from the true log-likelihood. Instead of always using LELBO for positive Ablations on Log-Likelihood Estimations for Positive Advantage Traces. advantage traces, we experiment on MATH500 and Countdown benchmarks using both LEUBO and LMix for positive 25 Table 8 Ablations on the value of β in the upper bound. GSM8K (0-shot) MATH500 (0-shot) Countdown (0-shot) Sudoku (3-shot) Model SPG w/ EUBO SPG w/ Mixture β 0.50 0. 1.00 1.50 2.00 1.00 1.50 2.00 128 77.7 77.2 76.5 77.1 76.5 78.8 78.5 78.8 256 83.2 83. 83.9 83.8 83.9 85.6 86.1 85.7 512 84.5 84.5 83.6 83.9 83.2 84.9 84.5 84.7 Avg. 81.8 81. 81.3 81.6 81.2 83.1 83.1 83.1 128 32.8 31.0 31.0 33.2 32.4 34.0 33.4 32.4 256 36.4 36. 37.4 37.6 36.8 40.2 40.0 38.8 512 41.2 40.0 38.8 39.4 38.2 39.2 41.8 39.8 Avg. 36.8 35. 35.7 36.7 35.8 37.8 38.4 37.0 128 71.1 70.7 66.0 69.5 68.4 69.9 68.8 70.3 256 68.8 70. 66.8 64.5 71.5 69.5 70.7 69.1 512 74.6 70.7 66.4 66.4 68.0 70.3 70.3 69.5 Avg. 71.5 70. 66.4 66.8 69.3 69.9 69.9 69.6 128 64.7 63.4 81.2 32.7 28.1 82.9 83.2 44.3 256 53.4 65. 87.1 40.5 31.9 94.0 86.0 60.5 512 57.4 45.4 89.9 39.9 28.0 93.1 84.6 60.7 Avg. 58.5 58. 86.1 37.7 29.3 90.0 84.6 55.2 Table 9 Ablations on the mixture coefficient ω on MATH500 and Countdown. SPG w/ Mixture MATH500 (0-shot) Countdown (0-shot) ω 0.00 0.25 0.50 0.75 1.00 128 35.8 34.6 33.4 34.2 33.2 256 37.6 37.6 40.0 38.6 37.6 512 38.8 42.2 41.8 41.2 39.4 Avg. 37.4 38.1 38.4 38.0 36. 128 66.8 71.5 68.8 69.5 69.5 256 66.0 68.0 70.7 69.1 64.5 512 68.4 67.2 70.3 74.2 66.4 Avg. 67.1 68.9 69.9 70.9 66.8 Table 10 Ablations on the looser upper bound. The loose bound performs worse than the tighter upper bound we used, indicating inferior performance due to larger discrepancy from the true log-likelihood. SPG w/ EUBO MATH500 (0-shot) Countdown (0-shot) β 1.0 1.5 Upper Bound LLoose LEUBO LLoose LEUBO 128 29.4 31.0 29.8 33.2 256 35.4 37.4 31.8 37. 512 39.4 38.8 38.8 39.4 Avg. 34.7 35. 33.5 36.7 128 43.8 66.0 46.9 69.5 65.2 66.8 54.7 64.5 512 64.8 66.4 57.0 66.4 Avg. 57.9 66.4 52.9 66.8 Table 11 Ablations on log-likelihood estimation for positive advantage traces. Using the upper bound for log-likelihood estimation of positive advantage traces perform worse than using the lower bound. Positive traces MATH500 (0-shot) Countdown (0-shot) Model SPG w/ EUBO SPG w/ Mixture likelihood estimation LEUBO (β = 1.0) LELBO LMix (β = 1.0, ω = 0.5) LELBO 128 34.4 33.2 35.4 33.4 256 36.2 37.6 38.4 40. 512 39.2 39.4 39.0 41.8 Avg. 36.6 36. 37.6 38.4 128 48.1 68.4 69.1 68.8 46.7 71.5 68.4 70.7 512 50.8 68.0 70.3 70.3 Avg. 48.5 69.3 69.3 69.9 advantage traces. Correspondingly, we use ω = 0.5 and the best performed β as previously discussed for negative advantage traces. For the positive advantage traces, we always use the tightest β = 1.0 for both LEUBO and LMix. The results are shown in Table 11, indicating that using the upper bound for likelihood estimation of positive advantage traces performs worse than using LELBO. This aligns well with our theoretical insights that the lower bound is better objective for log-likelihood maximization. In all previous experiments, we apply greedy sampling by setting temperature as Ablations on Pass@K Performance. 0.0 following D1 and LLaDA. However, beyond accuracy, it is essential for models to generate diverse set of outputs that can cover the correct solution and allow for explorations. In this section, we investigate the models ability to generate diverse outputs using higher temperature, and evaluate their Pass@K performance on MATH500 and Countdown, as shown in Table 12. Specifically, we set temperature to 0.9 and generation length to 256, conduct evaluations every 26 100 steps, and report results from the checkpoint with the highest accuracy. For comparison, we also include results from greedy sampling, denoted as Pass@1Greedy. As expected, increasing the temperature leads to decrease in Pass@1 performance across all models, aligning with observations from previous work. For K>1, the Pass@K scores improve for all models as increases from 1 to 4. SPG achieves the best performance across all settings, with SPG w/ Mixture reaching 55.6% Pass@4 accuracy on MATH500 and 76.6% on Countdown, demonstrating the ability of SPG to generate diverse outputs that can recover the correct solution. Table 12 Pass@K performance of each model on MATH500 and Countdown. We set temperature as 0.9 and report results of the best checkpoint of each case at generation length of 256. For comparison, we also include the greedy sampling performance, i.e., Pass@1Greedy. The best results are bolded and the second best are underlined. MATH500 (0-shot) Countdown (0-shot) Model LLaDA-8B-Instruct LLaDA-1.5 D1 WD1 UniGRPO SPG w/ EUBO SPG w/ mixture Pass@1Greedy Pass@1 Pass@2 Pass@3 Pass@4 Pass@1Greedy Pass@1 Pass@2 Pass@3 Pass@4 32.4 32.2 37.8 38.6 38.4 38.0 31.5 32.6 34.3 36.0 34.7 34. 40.9 42.2 43.1 44.9 43.9 44.3 40.0 36.5 46.0 45.7 47.4 48.0 49.9 49. 49.9 51.2 48.8 50.4 52.0 53.6 53.2 54.0 55.6 16.8 21.1 32.4 54.7 44. 71.5 71.1 15.8 18.2 24.5 44.3 36.8 68.2 67.5 28.1 32.1 40.4 60.6 55.2 71.9 72. 37.7 42.5 51.4 68.0 65.0 73.9 75.1 45.3 50.0 60.6 73.1 72.3 76.6 76.6 Table 13 Ablations on the inference strategy. SPG leads to consistently superior performance to baselines with different inference strategies. The best results are bolded and the second best are underlined for each setting. We report results for generation length 256. Inference Strategy Model LLaDA-8B-Instruct LLaDA-1.5 D1 Semi-AR, Block=16, Confidence WD1 UniGRPO SPG w/ EUBO SPG w/ Mixture LLaDA-8B-Instruct LLaDA-1.5 Semi-AR, Block=32, Confidence WD1 UniGRPO SPG w/ EUBO SPG w/ Mixture LLaDA-8B-Instruct LLaDA-1.5 D1 Semi-AR, Block=64, Confidence WD1 Semi-AR, Block=32, Random Full Sequence, Confidence Full Sequence, Random UniGRPO SPG w/ EUBO SPG w/ Mixture LLaDA-8B-Instruct LLaDA-1.5 D1 WD1 UniGRPO SPG w/ EUBO SPG w/ Mixture LLaDA-8B-Instruct LLaDA-1.5 D1 WD1 UniGRPO SPG w/ EUBO SPG w/ Mixture LLaDA-8B-Instruct LLaDA-1.5 D1 WD1 UniGRPO SPG w/ EUBO SPG w/ Mixture Sudoku 26.2 23.0 31.4 36.8 63.4 82. 96.2 27.7 26.9 32.8 35.7 67.0 88.5 95.6 32.6 36.4 39.8 41.8 82.9 88.8 93.8 24.4 27.5 29.9 29.9 60.1 72.6 86.9 68.3 67.9 72.3 68.9 95.6 88.9 96.5 27.1 28.0 31.8 30.0 57.3 71.3 72. Avg. 37.5 37.8 43.8 52.6 57.3 68.7 73.5 38.5 40.2 45.9 58.1 58.2 70.7 73.2 42.9 43.3 49.2 53.5 64.0 70.0 72.7 28.8 32.6 36.4 43.1 51.0 55.2 65.6 27.5 32.4 38.1 40.2 44.5 57.5 61.2 27.7 28.8 34.0 37.9 46.3 52. 55.7 GSM8K MATH500 Countdown 31.4 33.4 37.2 37.4 36.8 37.4 40.8 32.4 32.2 37.8 38.6 38.4 38.0 40.0 33.2 35.4 37.6 37.4 37.4 37.4 41.4 21.0 24.8 27.4 30.8 29. 31.4 31.0 17.8 20.4 22.6 25.0 24.2 23.4 24.0 17.8 19.8 22.6 23.4 22.6 25.2 25.2 13.7 16.0 27.0 53.9 46.5 70. 70.7 16.8 21.1 32.4 54.7 44.9 71.5 71.1 27.3 20.3 38.3 52.3 53.5 69.5 69.9 6.3 10.9 18.4 37.5 41.4 42.6 66.0 0.0 0.0 0.0 10.2 8.2 63.3 65.6 13.3 13.7 28.5 43.4 47.7 56. 64.1 78.7 78.8 79.7 82.3 82.5 84.7 86.4 77.2 80.5 80.6 81.7 82.6 84.8 86.2 78.6 81.0 80.9 82.5 82.3 84.3 85.5 63.5 67.1 69.7 74.1 72.8 74.1 78.4 23.9 41.4 57.5 56.7 50.0 54. 58.8 52.7 53.5 53.0 54.6 57.6 58.5 61.2 28 E.3 Qualitative Examples on GSM8K We present qualitative examples for all models on GSM8K, using generation length of 256 and 128 diffusion steps. Incorrect reasoning steps or answers are highlighted in red and the correct answers are highlighted in green. SPG leads to more concise and accurate reasoning compared to other models. Note that the models may sometimes achieve the correct answer through flawed reasoning, while such artifacts are expected to diminish with stronger base model. Model: LLaDA-8B-Instruct Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Model: LLaDA-8B-Instruct Answer: To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost pattern of the glass. The first glass costs $5, and every second glass costs 60% of the price. Therefore, the cost of every second glass is 0.6 5 = 3 dollars. We can break down the cost as follows: The first glass costs $5. The second glass costs $3. The third glass costs $5. The fourth glass costs $3. And so on... We can see that the cost alternates between $5 and $3. Since Kylar wants to buy 16 glasses, we need pair them up. The first of each pair costs $5, and the second of each pair costs $3. Therefore, the cost of each pair is 5 + 3 = 8 dollars. Since there are 8 pairs of glasses (16 glasses divided into pairs), the total cost is 8 8 = 64 dollars. </reasoning> <answer> 64 </answer> Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many eyeshadow colors does she have left? Model: LLaDA-8B-Instruct Answer: First, lets determine the total number of eyeshadow colors Amy initially has. She has two eyeshadow palettes with four colors each, 2 4 = 8. She also has three makeup sets with six colors each, 3 6 = 18. So, the total number of eyeshadow colors Amy initially has is: 8 + 18 = 26. Next, her sister steals one of the palettes, which she loses completely. Therefore, the number of colors left from the remaining palettes is: 8 4 = 4. Amy uses up half of the colors from one makeup set. Each makeup set has six colors, so half of that is: 6 2 = 3. Thus, the number of colors left from the one makeup set is: 6 3 = 3. Adding, the total number of eyes left is: 4 + 3 = 11. </reasoning> <answer> 11 </answer> Model: D1 Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Model: D1 Answer: To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost pattern of the glass. The first glass costs $5, and every second glass costs 60% of the price. Therefore, the cost of every second glass is 5 0.6 = 3 dollars. We can break down the cost into two parts: 1. The cost of the first 8 glasses, each costing $5. 29 2. The cost of the next 8 glasses, each costing $3. First, lets calculate the cost of the first 8 glasses: 8 5 = 40 dollars. Next, lets calculate the cost of the next 8 glasses: 8 3 = 24 dollars. Now, we add these two amounts together: 40 + 24 = 64 dollars. Therefore, Kylar needs to pay $64 for them. </reasoning> <answer> 64 </answer> Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many eyeshadow colors does she have left? Model: D1 Answer: Amy initially has two eyeshadow palettes with four colors each, so she has 2 4 = 8 colors from the palettes. She also has three makeup sets with six colors each, so she has 3 6 = 18 colors from the makeup sets. Hers sister steals one of the palettes, so she now has 8 1 = 7 colors from the palettes. Amy uses up half of the colors from one makeup set, 2 = 3 colors from that set. Therefore, she has 18 3 = 15 colors left from the makeup sets. Adding these so she uses 6 to the remaining palettes palettes, she has total of 7 + 15 = 22 eyeshadow colors left. </reasoning> <answer> 22 </answer> Model: WD1 Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Model: WD1 Answer: To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost of both the full price and the discounted price glasses. First, lets calculate the cost of the first 8 glasses, which are sold at the full price of $5 each: 8 5 = 40 dollars. Next, lets calculate the cost of the next 8 glasses, which are sold at 60% of the full price. Since 60% of $5 is: 0.6 5 = 3 dollars. So, the cost for the next 8 glasses is: 8 3 = 24 dollars. Now, we add the cost of the first 8 glasses and the cost of the next 8 glasses: 40 + 24 = 64 dollars. Therefore, Kylar needs to pay 64 . </reasoning> <answer> 64 </answer> Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many eyeshadow colors does she have left? Model: WD1 Answer: First, lets determine the total number of eyeshadow colors Amy initially has. She has two eyeshadow palettes with four colors each: 2 4 = 8. Additionally, she has three makeup sets with six colors each: 3 6 = 18. So, together, she has: 8 + 18 = 26. Next, her sister steals one of the palettes, which she loses 4 colors: 26 4 = 22. Then, Amy uses up half of the colors from one makeup set. Each makeup set has six colors, so half of that is: 6 2 = 3. Thus, she uses 3 colors from one makeup set, leaving: 6 3 = 3 colors in that makeup set. Finally, we add the remaining colors from the makeup set to the remaining colors: 22 + 3 = 25. </reasoning> <answer> 30 25 </answer> Model: UniGRPO Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Model: UniGRPO Answer: To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost of both the regular glasses and the discounted glasses. regular glass costs $5, and every second glass costs 60% of that price, which is 5 0.6 = 3. Since Kylar will buy 16 glasses, he will have 8 regular glasses and 8 discounted glasses. The cost of 8 regular glasses is 8 5 = 40 dollars. The cost of 8 discounted glasses is 8 3 = 24 dollars. Therefore, the total cost for 16 glasses is 40 + 24 = 64 dollars. </reasoning> <answer> 64 </answer> Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many eyeshadow colors does she have left? Model: UniGRPO Answer: First, lets determine the total number of eyeshadow colors Amy initially has. She has two eyeshadow palettes with four colors each, so she has 2 4 = 8 colors from the palettes. She also has three makeup sets with six colors each, so she has 3 6 = 18 colors from the makeup sets. Adding these together gives her initially 8 + 18 = 26 eyeshadow colors. Next, her sister steals one of the palettes, so she now has 8 4 = 14 colors from the palettes. She then uses up half of the colors from one makeup set. Since she used one makeup set, containing six colors, she used 6 2 = 3 colors from that set. Therefore, she has 18 3 = 15 colors left from the makeup sets. Finally, adding the remaining colors from the palettes and the makeup sets, Amy has 14 + 15 = 29 eyeshadow colors left. </reasoning> <answer> 29 </answer> Model: SPG w/ EUBO Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Model: SPG w/ EUBO Answer: To determine how much Kylar needs to pay for 16 glasses, we need to consider the cost structure of the glasses. The first glass costs $5, and every second glass costs 60% of the price of the first glass. Therefore, the cost of each second glass is 5.0 0.6 = 3.0 dollars. Kylar wants to buy 16 glasses, which means he will have 8 pairs of glasses. Each pair consists of one first glass and one second glass. The cost of one pair is: 5.0 + 3.0 = 8.0 dollars. Since there are 8 pairs, the total cost is: 8 8.0 = 64.0 dollars. Thus, Kylar needs to pay 64 dollars for 16 glasses. </reasoning> <answer> 64 </answer> 31 Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many eyeshadow colors does she have left? Model: SPG w/ EUBO Answer: First, lets calculate the total number of eyeshadow colors Amy initially has: She has two eyeshadow palettes with four colors each: 2 4 = 8 colors. She has three makeup sets with six colors each: 3 6 = 18 colors. Adding these together gives: 8 + 18 = 2 Next, we account for the changes: Her sister steals one palette, so she loses 4 colors: 26 4 = 22 Amy uses up half of the colors from one makeup set, which is 6 2 = 3 colors: 22 3 = 19 Therefore, Amy has 19 eyeshadow colors left. </reasoning> <answer> 19 </answer> Model: SPG w/ Mixture Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them? Model: SPG w/ Mixture Answer: 1. The first glass costs $5. 2. Every second glass costs 60% of $5, which is 0.6 5 = 3 dollars. 3. Kylar wants to buy 16 glasses, so he will have 8 full-priced glasses and 8 discounted glasses. 4. The cost for 8 full-priced glasses is 8 5 = 40 dollars. 5. The cost for 8 discounted glasses is 8 3 = 24 dollars. 6. Adding these the total cost is 40 + 24 = 64 dollars. </reasoning> <answer> 64 </answer> Question: Amy had two eyeshadow palettes with four colors each and three makeup sets that came with six eyeshadow colors each. Her sister steals one of the palettes. Amy uses up half of the colors from one makeup set. How many eyeshadow colors does she have left? Model: SPG w/ Mixture Answer: 1. Amy initially has two eyeshadow palettes with four colors each: 2 4 = 8 colors 2. She also has three makeup sets with six colors each: 3 6 = 18 colors 3. Adding these together gives gives: 8 + 18 = 26 colors 4. Her sister steals one palette, so she loses: 4 colors 5. Amy uses up half of the colors from one makeup set: 6 6. Subtracting these lost and used colors: 26 4 3 = 19 colors 2 = 3 colors </reasoning> <answer> 19 </answer>"
        }
    ],
    "affiliations": [
        "MIT",
        "Meta Superintelligence Labs",
        "UCLA",
        "USC"
    ]
}