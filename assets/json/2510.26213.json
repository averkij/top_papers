{
    "paper_title": "OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation",
    "authors": [
        "Hengrui Kang",
        "Zhuangcheng Gu",
        "Zhiyuan Zhao",
        "Zichen Wen",
        "Bin Wang",
        "Weijia Li",
        "Conghui He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. A major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattan-style structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to a specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 3 1 2 6 2 . 0 1 5 2 : r OMNILAYOUT: ENABLING COARSE-TO-FINE LEARNING WITH LLMS FOR UNIVERSAL DOCUMENT LAYOUT GENERATION Hengrui Kang1,2, Zhuangcheng Gu2*, Zhiyuan Zhao2, Zichen Wen1,2, Bin Wang2, Weijia Li2,3, Conghui He2 1 Shanghai Jiao Tong University, 2 Shanghai AI Laboratory, 3 Sun Yat-sen University Figure 1: Overview of OmniLayout. (Top & Middle) show the curation and examples of OmniLayout-1M. (Bottom) illustrates unconditional layouts generated by our OmniLayout-LLM via coarse-to-fine learning."
        },
        {
            "title": "ABSTRACT",
            "content": "Document AI has advanced rapidly and is attracting increasing attention. Yet, while most efforts have focused on document layout analysis (DLA), its generative counterpart, document layout generation, remains underexplored. major obstacle lies in the scarcity of diverse layouts: academic papers with Manhattanstyle structures dominate existing studies, while open-world genres such as newspapers and magazines remain severely underrepresented. To address this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse document layouts, covering six common document types and comprising contemporary layouts collected from multiple sources. Moreover, since existing methods struggle in complex domains and often fail to arrange long sequences coherently, we introduce OmniLayout-LLM, 0.5B model with designed two-stage Coarse-to-Fine learning paradigm: 1) learning universal layout principles from OmniLayout-1M with coarse category definitions, and 2) transferring the knowledge to specific domain with fine-grained annotations. Extensive experiments demonstrate that our approach achieves strong performance on multiple domains in M6Doc dataset, substantially surpassing both existing layout generation experts and several latest general-purpose LLMs. Our code, models, and dataset will be publicly released. These authors contributed equally to this work. Corresponding author(s). E-mail(s): liweij29@mail.sysu.edu.cn, heconghui@pjlab.org.cn"
        },
        {
            "title": "INTRODUCTION",
            "content": "Document AI has attracted growing attention across academia and industry recently, as it plays critical role in enabling machines to understand, process, and generate documents. On the one hand, increasing efforts have been devoted to document parsing (Wang et al., 2024; Li et al., 2025; Cui et al., 2025), which aims to extract structural and semantic information from massive amounts of pages through layout analysis and optical character recognition (OCR), On the other hand, however, its generative counterpartdocument layout generation (Gupta et al., 2021; Kong et al., 2022), has not yet been fully explored. This task focuses on producing well-organized layouts by arranging visual elements like text blocks, tables, and figures in coherent manner, with great potential for applications including content-driven layout design and document image generation. few studies have started investigating document layout generation with different generative models, ranging from early GAN-based approaches (Kikuchi et al., 2021) to more recent diffusion or flow-based methods (Guerreiro et al., 2024). Subsequently, the rise of large language models (LLMs) has opened new possibilities for conditional layout generation, drawing on their extensive prior knowledge and long-context understanding abilities. However, thorough review of previous studies and datasets reveals notable limitations: (i) Data Scarcity of Diverse Document Layouts. The domain bias in existing public datasets poses critical obstacle to the development of general Document AI. Widely-used datasets such as PubLayNet (Zhong et al., 2019) and DocBank (Li et al., 2020) offer massive annotations but primarily focus on single domainalways academic articles with relatively simple Manhattan layouts. Although datasets like DocLayNet (Pfitzmann et al., 2022) and D4LA (Da et al., 2023) include variety of document types, many of these (e.g., letter) are no longer commonly seen in modern real-world scenarios, and their data sources are often outdated. Among existing resources, M6Doc (Cheng et al., 2023) and OmniDocBench (Ouyang et al., 2025) stand out as the most valuable datasets to date, as they cover broader spectrum of contemporary document types, even including highly complex layouts such as newspapers. Unfortunately, they contain only limited number of samples, making them insufficient to support large-scale training. Overall, the landscape of publicly accessible document data exhibits severe long-tail distribution: academic articles are overrepresented, while complicated, non-Manhattan layouts such as textbooks remain drastically underrepresented. (ii) Challenges in Complex and Long-Sequence Scenarios. Due to the lack of diverse layout data, most existing methods are restricted to simple, homogeneous academic layouts, where progress has plateaued. In contrast, most real-world document layouts are more complex, with finer-grained element categories and larger number of bounding boxes. Our experiments (Table xx) show that these methods struggle in such settings, especially with long-sequence modeling. Diffusion-based layout generation models like LayoutDM (Inoue et al., 2023) and LACE (Chen et al., 2024) are particularly data-hungry and require extensive training to converge in complex domains. While recent LLM-based conditional layout generation approaches, such as RAG (Wu et al., 2025), CoT (Shi et al., 2025), and in-context learning (Lin et al., 2023), offer promise, direct fine-tuning on complex domains increases learning difficulty and leads to frequent failures. Domain-agnostic models like LayoutNUWA (Tang et al., 2023) and LGGPT (Zhang et al., 2025) represent early progress, but are only tested on limited document types and require substantial computational resources. To this end, we introduce OmniLayout-1M, the first million-scale dataset for diverse document layout generation. ❶ It contains twice as many samples as DocBank, ❷ covers six common document types from real-world scenarios, and ❸ adopts fully automated annotation pipeline, providing powerful foundation for training layout generation models. Moreover, to enable diverse document layout generation under limited fine-grained annotated data, we propose unified framework that formulates the task as two-stage Coarse-to-Fine learning paradigm. Specifically, we first let an LLM learn basic layout principles such as alignment and spatial organization on OmniLayout-1M across sufficiently diverse document types with coarse-grained labels. Then, with only small amount of fine-grained annotated data, we perform fine-grained adaptation on specific domain, enabling controllable and adaptable layout generation with minimal supervision and parameter footprint. Our contribution is summarized as follows: We introduce OmniLayout-1M, the first million-scale document layout dataset, comprising six commonly used document types and annotations for ten block-level element categories. 2 We propose OmniLayout-LLM, trained with Coarse-to-Fine learning paradigm, where aesthetic rules are first acquired from diverse layouts and subsequently adapted to specific document domain with fine-grained labels. To the best of our knowledge, we are the first to extend document layout generation to complex and challenging domains such as newspapers. Extensive experiments across multiple domains demonstrate that our method achieves state-ofthe-art (SOTA) performance consistently. In addition, our visualization examples demonstrate alignment with both aesthetic principles and user expectations."
        },
        {
            "title": "2.1 MOTIVATION",
            "content": "Despite the rapid advances in document parsing that have given rise to variety of document layout datasets in recent years, we observe that existing resources still suffer from several notable limitations. (i) Limited Diversity. Early document layout datasets, such as PubLayNet (Zhong et al., 2019) and DocBank (Li et al., 2020), are largely derived from large-scale academic paper repositories (e.g., PubMed, arXiv) and thus consist of single-domain pages with relatively simple Manhattan layouts. (ii) Deficient Volume. Generative tasks typically require more data than detection tasks, particularly for diffusion-based models. However, existing diverse datasets like M6Doc (Cheng et al., 2023) and OmniDocBench (Ouyang et al., 2025), contain samples on the order of 102 103 per document type, making them inadequate for training layout generation models. (iii) Outdated Source. As document layouts evolve toward improved aesthetics , the timeliness of data sources is critical. Although D4LA (Da et al., 2023) covers 12 document types, its images are sourced from RVL-CDIP (Harley et al., 2015), which contains obsolete formats (e.g., handwritten letters) and consists largely of noisy or skewed scans, substantially degrading the quality of the layout. (iv) Inefficient Annotation. Recent datasets like DocLayNet (Pfitzmann et al., 2022), often rely on labor-intensive manual annotation, which hinders scalability. With the rapid advancement of document parsing tools (e.g., MinerU (Wang et al., 2024)), returning to fully automated pipelines for accurate layout annotation has become feasible and convenient."
        },
        {
            "title": "2.2 DATASET CONSTRUCTION",
            "content": "To address the limitations outlined in Section 2.1, we present OmniLayout-1M, the first millionscale document layout dataset, featuring diverse common document types, up-to-date data collected from multiple databases and websites, and fully automated annotation and filtering pipeline. Preprocessing. To ensure the diversity of OmniLayout-1M, we collect documents from massive sources on the Internet. During the preprocessing stage, we use format standardization techniques to handle different document formats including PDF, DocX, Markdown, etc. Meanwhile, methods such as deduplication and document quality analysis are employed to filter out noisy documents and ensure the high quality of OmniLayout-1M. Finally, we collect data from 36 sources in total, including Academic Databases (13 sources), Publishers (7 sources), and Document-sharing Platforms (16 sources), covering various fields, such as academia, education, news, economics and etc."
        },
        {
            "title": "Dataset",
            "content": "the document"
        },
        {
            "title": "To accurately convert",
            "content": "Annotation. image into corresponding element sequence, we employ MinerU (Wang et al., 2024), powerful open-source toolkit, to automatically annotate the samples. Furthermore, MinerU outputs the element sequence more aligned with the natural reading order, property essential for reliable and coherent layout generation. In particular, for newspapers whose layouts are quite complicated and that MinerU cannot handle well, we manually annotate 1,000 in-domain newspaper pages and fine-tune DocLayoutYOLO (Zhao et al., 2024) to produce better performance, especially in capturing dense and irregular layouts. DSSE-200 Prima-LAD PubLayNet DocBank DocLayNet D4LA M6Doc OmniLayout-1M (Ours) Element Number 2,546 200 7,453 478 3.3M 360K 500K 6.7M 80.9K 1.1M 11.1K 294K 9.1K 237K 1M 48.0M"
        },
        {
            "title": "Annotation\nMethod\nAutomatic\nAutomatic\nAutomatic\nAutomatic\nManual\nManual\nManual\nAutomatic",
            "content": "Source Count Unknown Unknown 1 1 Unknown 1 3 36 Layout Type 2 5 1 1 6 12"
        },
        {
            "title": "Volume",
            "content": "Table 1: Comparison with Existing Layout Datasets."
        },
        {
            "title": "2.3 DATASET STATISTICAL ANALYSIS",
            "content": "Comparisons with existing datasets. Table 1 highlights OmniLayout-1Ms advantages over existing datasets. In terms of diversity, OmniLayout-1M significantly surpasses existing datasets in the number of document elements (about 48M), layout types (6 types), data volume (1M), and data sources diversity. The comprehensive diversity of OmniLayout-1M effectively meets the demand for synthesizing realistic and various document layouts. (a) Document types distribution. (b) Hand-crafted feature distribution (c) Co-occurrence heatmap Figure 2: Statistical Analysis of OmniLayout-1M. Document type distribution. The distribution of documents across different layout types is shown in Fig. 2a. OmniLayout-1M encompasses six layout types: textbook, newspaper, magazine, exam, academic, and slide. Data is balanced across all layout types to ensure robust performance. Hand-crafted feature distribution. OmniLayout-1M exhibits significantly more layout diversity than simple and homogeneous distribution of academic papers in PubLayNet, as evidenced by an UMAP visualization as shown in Fig. 2b. Hand-crafted features such as number of elements, average area, element centroids are used for visualization. Element co-occurrence analysis. To validate the plausibility of OmniLayout-1M, Fig. 2c visualizes element co-occurrence patterns. The distributions align with expectations: text and title are most frequent across all document types, followed by image and table, while formula is prominent in academic content such as textbook, paper, and exucational slide. These observations confirm the OmniLayout-1Ms adherence to real-world principles."
        },
        {
            "title": "3.1 PROBLEM FORMULATION",
            "content": "Following previous work (Inoue et al., 2023; Guerreiro et al., 2024; Zhang et al., 2025), document layout is represented as set of 5-tuples: = { ei = (c, x, y, w, h) = 1, . . . , }, (1) where denotes the element category, x, are the horizontal and vertical coordinates of the bounding box (either the top-left corner or the center point), and w, are its width and height. As stated in our contributions, unlike all prior approaches, we formulate complex layout generation task as two-stage Coarse-to-Fine learning paradigm, which enables enable the model to learn complex layout logic from easy to hard. Let Dcoar = {D(m) m=1 be diverse collection of document types with coarse-grained label set Ccoar, and let Dfine be the specific complex domain with fine-grained label set Cfine. We first perform Stage 1 on the diverse data of OmniLayout-1M to acquire basic layout abilities in spatial organization, and then conduct Stage 2 on fine-grained annotated dataset (e.g., M6Doc) to adapt to the target domain with complex element categories as shown in Fig. 3. coar}M (cid:0)Dcoar, Ccoar (cid:123)(cid:122) (cid:124) Stage 1: Coarse Learning [EASY] (Diverse Domains, Coarse-grained Labels) (cid:1) (cid:125) ====== Transfer 4 (cid:1) (cid:0)Dfine, Cfine (cid:123)(cid:122) (cid:124) Stage 2: Fine Adaptation [HARD] (Specific Domain, Fine-grained Labels) (cid:125) (2) Figure 3: Overview of Our Layout Generation Framework. Unified layout generation prompt (base metadata + task-specific conditions for U-Cond, CS+P, C+SP, Completion, Refinement) and Coarse-to-Fine mapping ϕ that transfers priors from diverse coarse labels to domain-specific fine categories."
        },
        {
            "title": "3.2 LAYOUT GENERATION MODELING",
            "content": "We cast layout generation as conditional sequence modeling over unified token space that encodes both semantic categories and normalized bounding boxes. Given layout serialized into sequence of discrete tokens = (t1, t2, . . . , tK), the model is trained to maximize the conditional log-likelihood of this sequence. Layout Generation Tasks. We follow the task setting introduced in (Zhang et al., 2025), and use five conditioning regimes that factor layout generation into category (C), size (S), and position (P ), enabling controllable synthesis, constrained placement, completion, and editing: (1) U-Cond: Unconditional generation without external constraints. (2) CS+P: Given the category of each element, the model predicts both its size and position. (3) C+SP: The position of each element is masked; the model infers it from the provided category and size. (4) Completion: subset (020%) of elements is retained on the page; the model completes the remaining layout to form coherent structure. (5) Refinement: Geometric attributes are perturbed by Gaussian noise (0, 102); the model recovers the original layout. Layout Representation & Generation Prompt. Each element ei = (c, x, y, w, h) is serialized with prefix-aware encoding <cat start> <cat end><box start> 0x 1y 2w 3h <box end>, where coordinates x, y, w, are normalized and uniformly quantized to [0, 999]. This unified serialization enables partial tuples for conditioning. The page-level layout generation prompt concatenates (i) base header (document type, canvas size, bbox count, valid categories) and (ii) task-specific condition list that supplies none / categories only / categories+wh / partial or perturbed tuples for, respectively, U-Cond, CS+P, C+SP, Completion, and Refinement, as shown in Fig. 3."
        },
        {
            "title": "3.3 COARSE-TO-FINE LEARNING",
            "content": "The above formulation defines layout representation and conditioning regimes. key challenge is how to train models that generalize across diverse document types and complex element categories. Directly learning fine-grained structures from limited data risks overfitting and poor transfer. We therefore adopt Coarse-to-Fine learning paradigm, where the model first acquires robust spatial priors and structural regularities from diverse domains with coarse-grained labels, and then adapts to specific domains with fine-grained supervision. This staged strategy allows the model to progress from easy to hard, aligning training objectives with increasing complexity. Coarse-grained Learning. The coarse-grained pre-training stage aims to establish strong foundation for layout generation by harnessing the diversity of pre-training domains Dcoar. At this stage, the model is exposed to wide range of document types, enabling it to acquire broad understanding of document structures and the spatial relationships among various layout elements. Central to our approach is the unified representation of layout elements and the harmonization of label spaces. To 5 Task Method Textbook Magazine FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU Newspaper Academic Exam LayoutDM 180.25 0. U-Cond LACE LayoutPrompter LGGPT Ours LayoutDM LACE 251.41 - 197.81 40.28 178.24 187.31 CS+P LayoutPrompter 44.67 LGGPT Ours 177.91 18.38 0.001 - 0.980 0.219 0.520 0.005 0.512 0.990 0. C+SP LayoutDM 174.82 0.471 LACE LayoutPrompter LGGPT Ours 28.79 42.38 181.61 16.92 0.001 0.224 0.940 0. LayoutDM 172.35 0.012 Compl. LACE LayoutPrompter LGGPT Ours 268.36 46.76 192.32 31.58 0.185 0.491 1.180 0.235 LayoutDM 124. 0.521 LACE 143.95 0.174 Refin. LayoutPrompter LGGPT Ours Test Data 11.82 217.05 4.51 - 0.149 1.031 0.317 0. 0.310 3.206 - 1.049 0.102 0.246 2.345 0.191 0.463 0. 0.452 6.345 0.469 0.587 0.122 0.429 0.158 0.244 0.892 0. 0.269 0.736 0.511 1.282 0.145 0.010 - - - 0.000 0.288 0.041 0.025 0.166 0.064 0.154 281.56 0.008 423.21 - 154.20 39.73 288.98 308.24 0.001 - 2.591 0.015 0.010 0.009 124. 0.312 167.39 10.71 2.731 0.014 0.093 285.43 0. 0.015 256.08 0.005 0.199 0.000 0.219 126.78 185.72 6.13 0.219 2. 0.021 0.000 270.15 0.007 0.000 0.169 0.158 0. 432.76 86.99 160.25 22.48 0.123 264.69 0.165 291. 0.672 0.010 0.681 - 113.84 220.33 10.60 - 0.034 0.357 2.696 0. 0.010 0.013 0.154 3.675 0.017 0.012 0.628 4.982 - 0.350 0.084 0.582 2.873 0.899 0.444 0.086 0.679 4. 1.387 0.402 0.188 0.704 2.865 0.481 0.335 0.098 0.624 2. 1.216 0.695 0.064 0.051 - - - 0.000 0. 0.091 0.000 0.160 0.000 0.185 281.91 0.233 325.67 - 162.94 41.82 271.52 220.00 65.24 172.45 21.08 0.001 - 3.190 0. 0.401 0.001 0.899 3.161 0.092 0.135 172.01 0. 0.006 196.78 0.002 0.156 0.000 0.240 41.52 169.95 20.74 0.245 3.297 0. 0.000 260.15 0.113 0.000 0.000 0.000 0.000 316.32 39. 153.43 38.56 0.043 0.676 3.511 0.098 0.142 203.91 0. 0.234 216.23 0.141 0.647 0.000 0.732 - 10.26 145.76 4. - 0.919 3.665 0.113 0.083 0.462 6.789 - 0.813 0. 0.453 0.862 0.362 1.133 0.138 0.537 6.015 0.442 1.102 0. 0.557 0.342 0.234 0.743 0.153 0.431 0.693 0.430 0.662 0. 0.054 - - - 0.038 0.266 0.081 0.069 0.224 0.026 0. 287.58 0.131 325.45 - 157.11 40.32 296.86 276.12 46. 186.38 8.68 0.002 - 1.324 0.072 0.171 0.010 0.262 1.392 0. 0.136 144.29 0.162 0.048 160.28 0.235 0.038 0. 14.58 180.76 5.42 0.008 0.042 1.441 0.083 0.000 255. 0.073 0.000 0.342 0.052 0.288 332.15 32.83 153.79 25.92 0. 228.59 0.256 214.19 0.689 0.024 0.752 - 10.87 215.24 6. - 0.071 0.066 1.461 0.068 0.138 0.048 0.072 2.420 0. 0.062 0.382 3.602 - 0.135 0.182 0.365 0.442 0. 0.229 0.241 0.468 6.327 0.341 0.189 0.235 0.459 0.218 0. 0.167 0.203 0.410 0.921 0.422 0.578 0.295 0.266 - - - 0.047 0.236 0.043 0.009 0.098 0.032 0.121 0. 0.050 0.138 0.026 0.200 153.66 0.440 276.05 - 236.72 36. 141.28 212.41 20.89 244.44 16.84 76.72 99.86 14.58 244.67 9. 0.001 - 0.533 0.089 1.458 0.006 0.221 0.710 0.084 1. 0.008 0.203 0.561 0.162 0.000 134.51 0.370 0.000 0. 0.000 0.310 0.090 0.187 0.502 0.000 0.641 - 284.16 32. 242.17 30.56 63.15 42.89 9.23 246.62 8.25 - 0.107 0. 0.975 0.106 1.212 0.219 0.177 0.408 0.132 0.097 0. 9.980 - 0.100 0.062 0.422 2.645 0.264 3.182 0.070 0. 1.402 0.332 3.151 0.085 0.418 0.768 0.287 2.812 0.070 0. 0.621 0.437 3.342 0.138 0.126 - - - 0.000 0. 0.068 0.088 0.216 0.000 0.246 0.118 0.097 0.286 0.000 0. 0.000 0.000 0.642 0.000 0.620 0.132 0.254 0.667 0.010 0. Table 2: Comparison with Layout Generation Experts across Five Document Types in M6Doc. For metrics, Ali. and Ove. denote Alignment and Overlap, means closer to ground truth is better. For tasks, Compl. and Refin. denote Completion and Refinement, respectively. - indicates not applicable. promote generalization across domains, we employ coarse-grained label set Ccoar that covers essential document components, such as text, table, image, and title, as well as associated classes like caption and footnote. This unified labeling strategy ensures the model learns transferable structural priors applicable to diverse document layouts. Fine-grained Adaptation. Given target domain Dfine with fine-grained labels Cfine, we adapt the foundation model under supervised sequence-modeling objective. The adaptation relies on label mapping ϕ : Ccoar Cfine, where each coarse class is expanded into its fine-grained descendants (e.g., text (cid:55) {paragraph, lead, ordered list}). We fine-tune models for heterogeneous targets containing multiple document types (e.g., NEWSPAPER, EXAM, ACADEMIC), yielding sharper, type-aware categories while preserving the general structural priors and cross-type generalization acquired during coarse-grained pretraining."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "Datasets. We first conduct coarse learning on OmniLayout-1M to acquire general layout patterns, and then perform fine-grained adaptation on five document types from M6Doc (Cheng et al., 2023) datase, which includes: (1) Textbook: 2,080 samples spanning three grade levels and nine subjects, annotated with 42 element categories. (2) Newspaper: 1,000 samples from Peoples Daily1 and The Wall Street Journal2, with 42 element categories. (3) Magazine: 2,000 samples from globally recognized publishers such as Time USA, annotated with 26 categories. (4) Exam: 2,000 exam paper samples covering the same nine subjects as textbooks, with 31 categories. (5) Academic: 1,000 samples sourced from the arXiv, with 25 categories. We follow its original 6:1:3 split for training, validation, and testing. Evaluation Metrics. Following previous work, we conduct our experiments using four standard (1) Similarity Assessment: Frechet Inception Distance metrics, grouped into two categories. (FID) (Heusel et al., 2017) measures the similarity between generated and ground truth layouts by comparing their feature distributions in the embedding space of deep neural network trained on corresponding images; Maximum Intersection over Union (mIoU) (Kikuchi et al., 2021) evaluates spatial alignment by optimally matching generated layouts with their ground truth counterparts to maximize the average IoU. (2) Aesthetic Consistency: We also adopt the Alignment (scaled by 100 for better visualization) and Overlap metrics from LayoutGAN++ (Kikuchi et al., 2021) to evaluate generation quality from the perspective of aesthetic principles. 1https://en.people.cn/ 2https://www.wsj.com/ 6 Task Method U-Cond CS+P C+SP Compl. Refin. GPT-4o Gemini-2.5* Claude-3.7* Ours GPT-4o Gemini-2.5* Claude-3.7* Ours GPT-4o Gemini-2.5* Claude-3.7* Ours GPT-4o Gemini-2.5* Claude-3.7* Ours GPT-4o Gemini-2.5* Claude-3.7* Ours Test Data Textbook Magazine FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU Newspaper Academic Exam 135.32 147.88 96.23 40.28 0.017 0.264 0.102 0. 103.15 0.084 54.74 42.99 18.38 64.67 139.01 26.86 16.92 61.20 108. 61.14 31.58 12.71 23.88 15.02 4.51 - 0.175 0.117 0.228 0. 1.103 0.147 0.366 0.240 0.511 0.135 0.235 0.371 0.394 0. 0.317 0.289 0.007 6.490 0.145 0.102 0.072 0. 0.068 0.121 0.363 0.751 0.103 0.122 0. 7.337 0.054 0.123 0.162 0.171 0.176 0.145 0. 0.060 193.13 0.020 0.154 0.236 0.288 194.77 171.01 39.73 0.078 0.079 0. 0.119 202.84 0.002 0.078 0.127 0.154 69.40 53.62 10.71 0.569 0.001 0. 0.091 106.97 0.043 0.057 0.136 0.219 0.522 0. 0.275 0.478 0.616 0.631 0.603 0.681 - 117.93 30.80 6. 97.60 95.02 90.96 22.48 67.25 8.92 3.86 10.60 - 0.034 0.002 0. 0.227 0.165 0.025 0.013 0.040 0.034 0.028 0.017 0.012 0. 0.098 0.031 0.084 0.112 0.666 0.520 0.086 4.759 6.159 0.300 0. 0.057 0.252 0.072 0.098 0.172 0.186 0.118 0. 0.051 0.000 236.11 0.015 0.000 0.000 0. 118.78 165.76 41.82 0.041 0.030 0.089 0.028 165.36 0.055 0.070 0.079 0. 53.32 87.00 21.08 0.065 0.071 0.092 0.052 112.38 0.332 0.039 0.127 0. 0.000 0.000 0.000 0.000 0.628 0.627 0.635 0.732 - 110.78 39.05 20.74 155.36 111.59 118.13 38.56 7.76 20.76 17.93 4.73 - 0.259 0.086 0.130 0.115 0.209 0.062 0.098 0.198 0.206 0.116 0. 0.083 0.040 0.213 0.182 0.151 0.164 0.104 0. 0.138 0.765 0.969 0.247 0.174 0.072 0.463 0. 0.153 0.108 0.111 0.304 0.072 0.054 0.089 163. 0.226 0.000 0.266 140.48 114.90 40.32 0.097 107.17 0.101 0.126 0.221 0. 0.085 0.160 0.256 0.075 0.355 0.195 0.288 0.654 0.661 0.635 0. - 42.25 27.96 8.68 61.67 43.38 12.69 5.42 116.18 91.24 63.31 25. 5.88 10.59 6.08 6.66 - 0.020 0. 0.014 0.072 0.040 0.053 0.041 0.074 0. 0.138 0.054 0.083 0.124 0.225 0.063 0.068 0.121 0.125 0.095 0. 0.062 0.010 0.313 0.038 0.182 0.049 0.063 0.080 0. 0.905 0.937 0.170 0.235 0.068 0.929 0.042 0.203 0.278 0. 0.375 0.295 0.266 0.000 135.60 0. 0.000 0.000 0.236 0.082 0.036 0.087 0.121 0.049 0.050 0.096 0.200 0.000 0. 0.000 0.310 0.577 0.585 0.584 0.641 - 57.36 106.98 36.48 90. 31.79 66.22 16.84 58.49 62.75 26.47 9.02 93.49 52.29 77.85 30.56 3.27 5.78 1. 8.25 - 0.347 0.030 0.089 0.224 0.351 0.138 0.084 0. 0.994 0.236 0.162 0.068 0.254 0.067 0.106 0.178 0.203 0.136 0.132 0. 0.006 0.089 0.101 0.062 0.027 0.051 0.075 0. 0.852 0.788 0.116 0.085 0.063 0.778 0.053 0.070 0.127 0.167 0. 0.138 0.126 0.000 0.000 0.000 0.415 0.123 0.084 0.139 0. 0.075 0.063 0.161 0.360 0.000 0.284 0.000 0.620 0.618 0.624 0.651 0. Table 3: Comparison with Powerful General-purpose LLMs in 0-shot Setting across Five Document Types in M6Doc. For models, Gemini-2.5* and Claude-3.7* denote Gemini-2.5-Flash and Claude-3.7-Sonnet. Implementation Details. We choose Qwen2.5-0.5B-Instruct3 as our base model. In coarse-grained learning stage, we constructed 9M samples from OmniLayout-1M across five tasks with ratio of 1:1:1:3:3. Our model was then trained for 1 epoch on 40 NVIDIA A100 GPUs with batch size of 16 per device and an initial learning rate of 1e-4, which took about 20 hours. For subsequent finegrained adaptation stage, we adopted the same data construction strategy and trained for 5 epochs on different categories, respectively. In general, this process was conducted using 8 NVIDIA A100 GPUs, taking about 2 hours, with batch size of 16 per device and an initial learning rate of 5e-5."
        },
        {
            "title": "4.2 COMPARISON WITH LAYOUT EXPERTS",
            "content": "Baselines. For layout generation experts, we compare our approach against four representative methods spanning two major categories: (1) Diffusion-based Models: LayoutDM (Inoue et al., 2023) and LACE (Chen et al., 2024). (2) LLM-based Methods: LayoutPrompter (Lin et al., 2023) and LGGPT (Zhang et al., 2025). Several work are excluded from comparison for the following reasons: (1) Early Vintage. Earlier research such as LayoutGAN++ (Kikuchi et al., 2021) and LayoutFormer++ (Jiang et al., 2023) are no longer suitable as fair baselines against modern models. (2) Unavailable or Buggy Implementation. The latest work like LayoutCoT (Shi et al., 2025) or LayoutRAG (Wu et al., 2025) lack publicly available code repositories, and the released implementation of LayoutNUWA (Tang et al., 2023) is hard to reproduce. (3) Poor Convergence. We trained on LayoutFlow (Guerreiro et al., 2024) for more than 100K epochs but failed to converge to satisfactory result. Analysis. We adopt unified domain-specific training strategy, selecting the checkpoint with the lowest validation loss for fair comparison. The detailed results are shown in Table 2. We observe that: (1) For the two diffusion-based models, the overall performance on FID is unsatisfactory. This can be attributed to the intrinsic nature of diffusion models, which require substantially more training data and longer convergence time to accurately learn probability distributions. As result, they fail in low-resource and complex domains. Although LACE achieves significant improvement in element alignment through post-processing, it still struggles to control overlap. (2) For the three LLM-based models, thanks to the autoregressive formulation and strong long-context modeling capability, they can naturally follow aesthetic layout rules without the need for specially designed post-processing. An exception is LGGPT, where the underlying GPT2-XL (Radford et al., 2019) often produces incoherent or nonsensical outputs when handling long prompt sequences, problem not observed on shorter-sequence datasets like PubLayNet. Compared to these baselines, our model achieves consistently superior results across all metrics, with particularly notable gains on mIoU."
        },
        {
            "title": "4.3 COMPARISON WITH GENERAL-PURPOSE LLMS",
            "content": "Baselines. For general-purpose LLMs, we select three powerful LLMs: GPT-4o (OpenAI, 2024), Gemini-2.5-Flash (Google, 2025a), and Claude-3.7-Sonnet (Anthropic, 2025), chosen for their 3https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct 7 Figure 4: Visualization Examples of Various Methods with U-Cond Task. For general-purpose LLMs, we adopt the strongest 5-shot setting. strong long-context capability, with the latter two also offering advanced reasoning abilities relevant to complex layout generation. Analysis. We conduct evaluation under the classic 0/1/5-shot settings to assess whether existing LLMs can achieve competitive performance on conditional document layout generation solely through in-context learning. Owing to the page limit, Table 3 reports zero-shot results, while fewshot results are provided in Appendix Table 5. We observe that: (1) Under zero-shot setting, all general-purpose LLMs achieve reasonably good alignment and overlap, but exhibit high stochasticity, sometimes leading to extreme outliers. For example, Gemini-2.5-Flash attains an unexpectedly high overlap score on the textbook while performing unconditional task. Moreover, complex layouts such as Newspaper remain the most challenging, as reflected by the highest average FID, whereas performance on Academic is relatively better, likely because such formats are more prevalent in pre-training corpora. Among the three models, Claude-3.7-Sonnet delivers the best results. (2) Under few-shot settings, all LLMs improve as the number of shots increases, confirming that incontext learning indeed enables better layout generation. Nevertheless, although additional shots yield better performance, the improvement tends to converge to an intrinsic upper bound, and comes at the cost of longer input sequences, higher API expenses, and slower inference. The visualization results of different methods on the U-Cond task are shown in Fig. 4, and more examples of our OmniLayout-LLM can be found in Appendix C."
        },
        {
            "title": "Task",
            "content": "U-Cond U-Cond C+SP CS+P CS+P Stage FID Ali. Ove.mIoU Task Param FID Ali. Ove.mIoU 35.81 0.009 0.052 0.000 3B 1.5B 36.58 0.013 0.105 0.000 0.5B 39.73 0.015 0.084 0.000 3B 26.88 0.007 0.110 0.000 1.5B 10.63 0.012 0.097 0.179 0.5B 10.71 0.014 0.086 0.185 17.12 0.020 0.181 0.320 3B 1.5B 5.65 0.022 0.205 0.238 0.5B 6.13 0.021 0.188 0.240 3B 27.08 0.008 0.125 0.000 1.5B 26.86 0.011 0.093 0.000 0.5B 22.48 0.013 0.098 0.000 3B 67.24 0.017 0.062 0.725 1.5B 6.98 0.017 0.061 0.730 0.5B 10.60 0.017 0.064 0.732 In this section, we perform ablation studies on the number of parameters and the two stages of the Coarse-to-Fine framework, conducted in the most challenging newspaper domain. The results are shown in Table 4. We observe that: (1) For model size, the overall differences are marginal. The 3B model achieves slightly lower Alignment scores, but its FID increases in most tasks. in the C+SP For instance, task the 3B model yields an FID that is 2.79 higher than Table 4: Ablation on Model Sizes and Learning Stages. F. and C. denote that of the 0.5B model. This Fine-grained Adaptation and Coarse-grained Learning only, respectively. phenomenon is likely due to the inherent volatility of FID when evaluated on limited test samples. (2) For Coarse-to-Fine learning paradigm, coarse-grained learning brings substantial gains in orga42.98 0.017 8.308 0.000 249.1 0.016 0.388 0.000 39.73 0.015 0.084 0.000 14.88 0.024 0.493 0.164 246.4 0.016 0.357 0.000 10.71 0.014 0.086 0.185 11.24 0.023 0.476 0.220 235.8 0.021 1.162 0.000 6.13 0.021 0.188 0.240 36.99 0.015 6.627 0.000 248.79 0.015 0.480 0.000 22.48 0.013 0.098 0.000 22.07 0.023 1.452 0.618 254.98 0.018 0.386 0.000 10.60 0.017 0.064 0.732 0.012 0.051 F. C. Both F. C. Both F. C. Both F. C. Both F. C. Both 0.012 0."
        },
        {
            "title": "Test Data",
            "content": "C+SP Compl. Compl. Refin. Refin. - - - - 8 nizing and perceiving the overall layout, as evidenced by sharp reduction in Overlap. Fine-grained adaptation further enhances the models ability to output diverse and detailed element labels across document types, enabling more sophisticated layout generation. Notably, multiple zero scores in mIoU are attributed to its definition: it requires exact label-level matches, which are often absent in complex multi-element layout generation, particularly in U-Cond and Completion tasks."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Document Layout Dataset. Existing layout datasets largely stem from parsing tasks and initially focused on academic articles. PubLayNet (Zhong et al., 2019) auto-annotates papers from PubMed Central via XML matching, while DocBank (Li et al., 2020) uses weak supervision on arXiv. Recent efforts broaden document types but remain small and often require manual annotation. Although DocLayNet (Pfitzmann et al., 2022) and D4LA (Da et al., 2023) cover six and twelve types, respectively, they still contain only on the order of 104 pages. M6Doc (Cheng et al., 2023) and OmniDocBench (Ouyang et al., 2025) reflect real-world formats (e.g., textbooks, newspapers) but are even smaller, and the latter serves purely as benchmark without training split. Layout Generation. Document layout generation has gained traction. Early methods used GANs or transformers: LayoutGAN++ (Kikuchi et al., 2021) enhances the GAN framework with transformer blocks and optimizes latent codes to achieve constrained layout generation. LayoutTransformer (Gupta et al., 2021) leverages self-attention to learn contextual relationships among layout elements, BLT (Kong et al., 2022) adopts non-autoregressive bidirectional transformer that iteratively refines layouts by masking and predicting low-confidence attributes through hierarchical sampling strategy. LayoutFormer++ (Jiang et al., 2023) employs constraint tokenization and restricted decoding space to strike balance between user constraint satisfaction and overall layout quality. More recently, diffusion-based methods have gained attention. LayoutDM (Inoue et al., 2023) and LACE (Chen et al., 2024)) denoise element coordinates and labels in discrete/continuous spaces, respectively, and attempt to inject hard/soft constraints. Flow-based LayoutFlow (Guerreiro et al., 2024) frames the task as flow matching, speeding training and inference. Large Language Model. With the remarkable success of LLMs in sequence generation tasks (OpenAI, 2024; Anthropic, 2025; Google, 2025b), autoregressive generation has become the mainstream paradigm for document layout generation in recent years. LayoutPrompter (Lin et al., 2023) casts layouts into unified HTML representations and employs adaptive exemplar retrieval to enable incontext learning. LayoutCoT (Shi et al., 2025) leverages the deep reasoning capabilities of generalpurpose LLMs through chain-of-thought prompting, substantially improving the performance and practicality of training-free layout generation. LayoutRAG (Wu et al., 2025) retrieves optimal reference layouts from layout database and introduces condition-modulated attention module to selectively incorporate prior knowledge. While effective, these approaches remain largely domain-specific and rely heavily on prompt engineering or retrieval heuristics. In contrast, LayoutNUWA Tang et al. (2023) and LGGPT (Zhang et al., 2025) pioneer domain-agnostic paradigms that fully exploit the generalization strength of LLMs: the former formulates the task as HTML code completion, whereas the latter demonstrates that pure string-based inputoutput reduces redundant tokens and yields better efficiency."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we move beyond the domain limitations of previous studies and explore complex document layout generation with LLMs. To address the scarcity of diverse training data, we introduce OmniLayout-1M, the first million-scale dataset for document layouts, covering six common types such as newspapers and textbooks. Moreover, leveraging the strong capability of LLMs in long-sequence generation, we propose Coarse-to-Fine learning paradigm: first acquiring fundamental aesthetic layout rules from comprehensive document types, and then performing fine-grained adaptation on specific complex domain. Our approach significantly surpasses both existing layout generation experts and powerful general-purpose LLMs. However, our experiments also reveal challenges, such as the inadequacy of current metrics when evaluating complex layouts under limited samples. We will continue to investigate these issues to further advance the field of Document AI."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "The development of OmniLayout-1M and the associated model was guided by commitment to ethical research practice. We acknowledge that our dataset, despite its scale and diversity, may contain inherent biases from its sources, which could be reflected in the models trained on it. We encourage users to be aware of these potential biases. Furthermore, we recognize that layout generation models could be misused to create misleading or fraudulent documents. Our research is intended for positive applications, such as enhancing document synthesis, streamlining content creation workflows, and improving document understanding. We disavow any malicious use of our work and hope that by making our methods and dataset public, we can foster further research into responsible and ethical generative AI for documents."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We are committed to the full reproducibility of our work. The core concepts and methodology of our Coarse-to-Fine learning paradigm are detailed in Section 3. Our experimental setup, including the datasets used, evaluation protocols, and baseline models, are described in Section 4. Further details on the OmniLayout-1M dataset, including more data statistics and analysis, are provided in Appendix D. To facilitate the reproduction of our results and to encourage further research, we will release our codebase, the OmniLayout-1M dataset, and the pretrained model weights."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude-3.7-sonnet. claude-3-7-sonnet, 2025. https://www.anthropic.com/news/ Anthropic. Claude sonnet 4, 2025. URL https://www.anthropic.com/claude/sonnet. Accessed: 2025-05-23. Jian Chen, Ruiyi Zhang, Yufan Zhou, Rajiv Jain, Zhiqiang Xu, Ryan Rossi, and Changyou Chen. Towards aligned layout generation via diffusion model with aesthetic constraints. arXiv preprint arXiv:2402.04754, 2024. Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding, and Lianwen Jin. M6doc: large-scale multi-format, multi-type, multi-layout, multilanguage, multi-annotation category dataset for modern document layout analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1513815147, 2023. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, Yue Zhang, Wenyu Lv, Kui Huang, Yichao Zhang, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. Paddleocr 3.0 technical report, 2025. URL https://arxiv.org/abs/2507.05595. Cheng Da, Chuwei Luo, Qi Zheng, and Cong Yao. Vision grid transformer for document layout analysis. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1946219472, 2023. Google. gemini-2.5-flash. https://deepmind.google/models/gemini/flash/, 2025a. Google. gemini-2.5-pro. https://deepmind.google/models/gemini/pro/, 2025b. Julian Jorge Andrade Guerreiro, Naoto Inoue, Kento Masui, Mayu Otani, and Hideki Nakayama. Layoutflow: flow matching for layout generation. In European Conference on Computer Vision, pp. 5672. Springer, 2024. Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layouttransformer: Layout generation and completion with self-attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10041014, 2021. Adam Harley, Alex Ufkes, and Konstantinos Derpanis. Evaluation of deep convolutional nets for document image classification and retrieval. In 2015 13th international conference on document analysis and recognition (ICDAR), pp. 991995. IEEE, 2015. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Layoutdm: In Proceedings of the IEEE/CVF Discrete diffusion model for controllable layout generation. Conference on Computer Vision and Pattern Recognition, pp. 1016710176, 2023. Zhaoyun Jiang, Jiaqi Guo, Shizhao Sun, Huayu Deng, Zhongkai Wu, Vuksan Mijovic, Zijiang James Yang, Jian-Guang Lou, and Dongmei Zhang. Layoutformer++: Conditional graphic layout generation via constraint serialization and decoding space restriction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1840318412, 2023. Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Constrained graphic layout generation via latent optimization. In Proceedings of the 29th ACM International Conference on Multimedia, pp. 8896, 2021. Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, and Irfan Essa. Blt: Bidirectional layout transformer for controllable layout generation. In European Conference on Computer Vision, pp. 474490. Springer, 2022. Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and Ming Zhou. Docbank: benchmark dataset for document layout analysis. arXiv preprint arXiv:2006.01038, 2020. Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structure-recognition-relation triplet paradigm, 2025. URL https://arxiv.org/abs/2506.05218. Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang Yang, Jian-Guang Lou, and Dongmei Zhang. Layoutprompter: Awaken the design ability of large language models. Advances in Neural Information Processing Systems, 36:4385243879, 2023. OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024. Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document In Proceedings of the Computer Vision and Pattern parsing with comprehensive annotations. Recognition Conference, pp. 2483824848, 2025. Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. Doclaynet: In Proceedings of the 28th large human-annotated dataset for document-layout segmentation. ACM SIGKDD conference on knowledge discovery and data mining, pp. 37433751, 2022. Alec Radford, Jeffrey Wu, and et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Hengyu Shi, Junhao Su, Huansheng Ning, Xiaoming Wei, and Jialin Gao. Layoutcot: Unleashing the deep reasoning potential of large language models for layout generation. arXiv preprint arXiv:2504.10829, 2025. Zecheng Tang, Chenfei Wu, Juntao Li, and Nan Duan. Layoutnuwa: Revealing the hidden layout expertise of large language models. arXiv preprint arXiv:2309.09506, 2023. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. Mineru: An open-source solution for precise document content extraction, 2024. URL https://arxiv.org/abs/2409.18839. 11 Yuxuan Wu, Le Wang, Sanping Zhou, Mengnan Liu, Gang Hua, and Haoxiang Li. Layoutrag: Retrieval-augmented model for content-agnostic conditional layout generation. arXiv preprint arXiv:2506.02697, 2025. Peirong Zhang, Jiaxin Zhang, Jiahuan Cao, Hongliang Li, and Lianwen Jin. Smaller but better: Unifying layout generation with smaller large language models. International Journal of Computer Vision, 133(7):38913917, 2025. Zhiyuan Zhao, Hengrui Kang, Bin Wang, and Conghui He. Doclayout-yolo: Enhancing document layout analysis through diverse synthetic data and global-to-local adaptive perception. arXiv preprint arXiv:2410.12628, 2024. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. Publaynet: largest dataset ever for document layout analysis. In 2019 International conference on document analysis and recognition (ICDAR), pp. 10151022. IEEE, 2019."
        },
        {
            "title": "A LLM Usage Statement",
            "content": "B Few-shot Performance of General-purpose LLMs Qualitative Results of OmniLayout-LLM across Diverse Domains OmniLayout-1M Dataset D.1 Element-wise Statistical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 More Visualization Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Layout Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 14 17 17"
        },
        {
            "title": "A LLM USAGE STATEMENT",
            "content": "We used AI tools (Large Language Models) at minimal level and only for linguistic polishing (grammar, spelling, punctuation, and minor word-choice/style edits). The tools did not change the original meaning, nor did they introduce any new ideas, claims, content, data, code, figures, analyses, or results. All technical contributions and writing decisions were authored and verified by the authors, and no confidential or proprietary data were provided to AI services. Task Method Setting GPT-4o U-Cond Gemini-2.5* Claude-3.7* Ours GPT-4o CS+P Gemini-2.5* Claude-3.7* Ours GPT-4o C+SP Gemini-2.5* Claude-3.7* Ours GPT-4o Compl. Gemini-2.5* Claude-3.7* Ours GPT-4o Refin. Gemini-2.5* Claude-3.7* Ours Test Data 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot - 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot - 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot - 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot - 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot 0-shot 1-shot 5-shot - Exam Textbook Academic Newspaper Magazine FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU FID Ali. Ove. mIoU 0.000 135.32 0.000 97.34 0.460 71.56 0.000 147.88 0.000 84.16 0.322 57.30 0.000 96.23 0.000 87.87 59.73 0.064 40.28 0.415 0.123 103.15 0.125 72.34 51.50 0.142 0.084 54.74 42.26 0.118 0.148 35.33 0.139 42.99 0.144 33.61 0.175 18.86 0.246 18.38 0.075 64.67 0.150 52.66 0.176 46.00 0.063 139.01 0.154 94.61 0.177 73.67 0.161 26.86 0.186 20.04 0.222 21.47 0.360 16.92 0.000 61.20 0.000 44.68 0.438 33.44 0.284 108.60 0.272 86.44 0.440 65.85 0.000 61.14 1.000 54.28 29.74 0.329 0.620 31.58 0.618 12.71 6.75 0.640 0.658 10.25 0.624 23.88 0.646 9.72 0.661 8.18 0.651 15.02 0.650 18.97 0.631 11.23 4.51 0.708 - 236.11 177.61 143.70 118.78 88.91 66.30 165.76 141.53 84.23 41.82 165.36 134.33 104.85 53.32 53.72 43.72 87.00 55.11 27.34 21.08 112.38 69.15 63.56 110.78 88.37 66.91 39.05 33.47 34.77 20.74 155.36 130.47 86.84 111.59 89.80 68.69 118.13 100.98 47.29 38.56 7.76 7.63 8.92 20.76 8.62 6.87 17.93 11.01 3.33 4.73 - 163.94 111.87 76.48 140.48 90.10 70.05 114.90 72.29 50.83 40.32 107.17 58.18 31.96 42.25 33.10 21.22 27.96 16.32 17.34 8.68 61.67 19.53 13.33 43.38 15.94 13.06 12.69 9.75 7.64 5.42 116.18 78.13 39.22 91.24 50.65 36.62 63.31 45.10 25.88 25.92 5.88 4.32 4.89 10.59 5.21 1.02 6.08 16.28 5.84 6.66 - 135.60 100.70 90.93 57.36 82.93 51.41 106.98 100.70 75.50 36.48 90.67 64.51 41.54 31.79 28.47 18.49 66.22 44.09 17.49 16.84 58.49 32.53 26.73 62.75 28.41 33.06 26.47 22.69 14.14 9.02 93.49 70.53 46.80 52.29 35.75 28.30 77.85 68.76 36.78 30.56 3.27 2.10 1.38 5.78 3.79 1.32 1.67 4.22 3.72 8.25 - 193.13 100.18 54.34 194.77 74.14 30.55 171.01 73.78 25.57 39.73 202.84 152.61 91.87 69.40 37.62 17.09 53.62 36.33 13.14 10.71 106.97 52.16 30.81 117.93 65.27 35.17 30.80 17.91 18.75 6.13 97.60 84.59 50.11 95.02 81.28 45.02 90.96 53.40 18.54 22.48 67.25 23.67 31.24 8.92 1.05 1.27 3.86 3.12 3.85 10.60 - 0.040 0.215 0.198 0.213 0.152 0.136 0.182 0.251 0.093 0.151 0.164 0.178 0.193 0.104 0.240 0.326 0.109 0.204 0.112 0.138 0.765 0.292 0.298 0.969 0.600 0.426 0.247 0.226 0.331 0.174 0.072 0.083 0.099 0.463 0.227 0.223 0.103 0.209 0.148 0.153 0.108 0.104 0.116 0.111 0.105 0.104 0.304 0.624 0.173 0.072 0.054 0.015 0.038 0.051 0.041 0.063 0.190 0.030 0.023 0.044 0.089 0.055 0.108 0.112 0.065 0.057 0.071 0.071 0.067 0.061 0.092 0.332 0.242 0.259 0.259 0.264 0.305 0.086 0.078 0.061 0.130 0.115 0.125 0.143 0.209 0.135 0.127 0.062 0.042 0.072 0.098 0.198 0.190 0.194 0.206 0.196 0.200 0.116 0.099 0.175 0.113 0.083 0.006 0.011 0.024 0.089 0.054 0.061 0.101 0.112 0.098 0.062 0.027 0.040 0.030 0.051 0.036 0.036 0.075 0.109 0.105 0.070 0.852 0.115 0.101 0.788 0.379 0.319 0.116 0.143 0.084 0.085 0.063 0.068 0.060 0.778 0.300 0.073 0.053 0.082 0.128 0.070 0.127 0.134 0.134 0.167 0.159 0.163 0.127 0.410 0.325 0.138 0. 0.006 0.023 0.060 0.347 0.107 0.074 0.030 0.049 0.042 0.089 0.224 0.166 0.169 0.351 0.201 0.103 0.138 0.109 0.062 0.084 0.743 0.669 0.632 0.994 0.333 0.391 0.236 0.142 0.257 0.162 0.068 0.112 0.135 0.254 0.247 0.164 0.067 0.070 0.112 0.106 0.178 0.162 0.198 0.203 0.188 0.189 0.136 0.111 0.158 0.132 0.097 0.020 0.043 0.049 0.071 0.037 0.038 0.014 0.025 0.043 0.072 0.040 0.030 0.040 0.053 0.059 0.069 0.041 0.021 0.025 0.074 0.187 0.163 0.169 0.138 0.088 0.116 0.054 0.028 0.031 0.083 0.124 0.168 0.184 0.225 0.243 0.289 0.063 0.061 0.071 0.068 0.121 0.125 0.124 0.125 0.124 0.124 0.095 0.073 0.093 0.079 0.062 0.010 0.049 0.085 0.313 0.299 0.086 0.038 0.120 0.131 0.182 0.049 0.054 0.087 0.063 0.169 0.136 0.080 0.227 0.130 0.241 0.905 0.085 0.121 0.937 0.546 0.398 0.170 0.274 0.390 0.235 0.068 0.060 0.087 0.929 0.578 0.220 0.042 0.110 0.139 0.203 0.278 0.286 0.291 0.350 0.328 0.321 0.375 0.729 0.545 0.295 0.266 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.236 0.082 0.087 0.091 0.036 0.047 0.060 0.087 0.095 0.100 0.121 0.049 0.102 0.116 0.050 0.082 0.094 0.096 0.113 0.122 0.200 0.000 0.320 0.290 0.210 0.402 0.025 0.000 0.000 0.089 0.310 0.577 0.599 0.601 0.585 0.597 0.606 0.584 0.608 0.596 0.641 - 0.089 0.158 0.174 0.226 0.214 0.289 0.000 0.000 0.165 0.266 0.097 0.102 0.110 0.101 0.110 0.128 0.126 0.128 0.139 0.221 0.076 0.143 0.146 0.085 0.124 0.134 0.160 0.171 0.178 0.256 0.075 0.139 0.169 0.355 0.168 0.180 0.195 0.232 0.172 0.288 0.654 0.670 0.672 0.661 0.665 0.669 0.635 0.663 0.676 0.752 - 0.007 0.052 0.074 0.098 0.098 0.094 0.031 0.512 0.245 0.084 0.112 0.230 0.312 0.666 0.750 0.353 0.520 0.416 0.332 0.086 4.759 0.396 0.516 6.159 1.470 0.700 0.300 0.381 0.409 0.188 0.057 0.058 0.118 0.252 5.176 0.166 0.072 0.173 0.167 0.098 0.172 0.175 0.170 0.186 0.182 0.183 0.118 0.102 0.165 0.064 0. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.028 0.043 0.059 0.070 0.073 0.095 0.079 0.096 0.000 0.185 0.052 0.100 0.108 0.039 0.089 0.127 0.127 0.147 0.159 0.240 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.628 0.639 0.639 0.627 0.638 0.637 0.635 0.643 0.638 0.732 - 0.007 0.099 0.083 6.490 0.552 0.219 0.145 0.164 0.114 0.102 0.072 0.059 0.074 0.060 0.114 0.106 0.068 0.083 0.102 0.121 0.363 0.094 0.113 0.751 0.398 0.316 0.103 0.130 0.082 0.122 0.051 0.045 0.052 7.337 0.699 0.360 0.054 0.190 0.111 0.123 0.162 0.157 0.180 0.171 0.165 0.158 0.176 0.543 0.378 0.145 0.010 0.020 0.031 0.032 0.078 0.015 0.014 0.079 0.003 0.007 0.015 0.002 0.006 0.012 0.569 0.005 0.010 0.001 0.003 0.004 0.014 0.043 0.027 0.034 0.034 0.015 0.035 0.002 0.005 0.006 0.021 0.227 0.144 0.090 0.165 0.184 0.049 0.025 0.008 0.012 0.013 0.040 0.042 0.040 0.034 0.036 0.034 0.028 0.028 0.027 0.017 0.012 0.017 0.105 0.149 0.264 0.159 0.173 0.102 0.096 0.093 0.219 0.084 0.111 0.146 0.175 0.217 0.152 0.117 0.111 0.103 0.228 0.448 0.447 0.514 1.103 0.480 0.534 0.147 0.136 0.159 0.366 0.240 0.309 0.340 0.511 0.553 0.394 0.135 0.103 0.225 0.235 0.371 0.392 0.397 0.394 0.386 0.392 0.272 0.231 0.284 0.317 0.289 0.060 0.061 0.177 0.154 0.221 0.202 0.236 0.171 0.091 0.288 0.119 0.115 0.118 0.078 0.089 0.098 0.127 0.127 0.122 0.154 0.091 0.132 0.136 0.057 0.103 0.117 0.136 0.146 0.156 0.219 0.522 0.131 0.268 0.219 0.302 0.310 0.275 0.259 0.331 0.478 0.616 0.646 0.650 0.631 0.656 0.657 0.603 0.638 0.642 0.681 - Table 5: Comparison with Powerful General-purpose LLMs in 0/1/5-shot Setting across Five Document Types in M6Doc. 13 FEW-SHOT PERFORMANCE OF GENERAL-PURPOSE LLMS Due to space limitations, the complete 0/1/5-shot comparison results are reported in Table 5 of the appendix. In particular, evaluation of complex document layouts requires significantly longer inference time and more than 10,000 USD in API costs owing to excessive sequence length. QUALITATIVE RESULTS OF OMNILAYOUT-LLM ACROSS DIVERSE"
        },
        {
            "title": "DOMAINS",
            "content": "In this section, we demonstrate the qualitative results generated by OmniLayout-LLM: Fig. 5 for textbook and newspaper, Fig. 6 for magazine and exam, and Fig. 7 for academic. The visualization results demonstrate that OmniLayout-LLM can generate reasonable and aesthetically pleasing layouts for wide variety of document types. Furthermore, it effectively adheres to the requirements of different generation tasks and adapts well to various constraints, showcasing its ability to perform under diverse conditions and tasks. 14 Figure 5: Visualization of Layouts Generated by OmniLayout-LLM (Textbook and Newspaper). 15 Figure 6: Visualization of Layouts Generated by OmniLayout-LLM (Magazine and Exam). Figure 7: Visualization of Layouts Generated by OmniLayout-LLM (Academic). OMNILAYOUT-1M DATASET D.1 ELEMENT-WISE STATISTICAL ANALYSIS First, we analyze the diversity of OmniLayout-1M from the perspective of element distribution. Specifically, we examine element diversity in three aspects: the number of elements per page, the proportion of the layout area occupied by all elements on page, and the aspect ratios of the elements. The data distribution is illustrated in Fig. 8. As can be observed, OmniLayout-1M exhibits significantly greater diversity in elements compared to PubLayNet and DocBank. This ensures the robustness of the pre-trained model, enabling our proposed method to adapt to various element types (with different aspect ratios and categories) and diverse layout attributes (with varying densities and numbers of elements) in downstream tasks. (a) Element numbers per page. (b) Area ratio per page. (c) Element aspect ratio. Figure 8: Element Statistical Analysis of OmniLayout-1M. D.2 MORE VISUALIZATION EXAMPLES In this section we present more visualization examples from our OmniLayout-1M dataset, accompanied by high-quality annotations extracted with MinerU (Wang et al., 2024). Visualization of 17 6 layout types: textbook  (Fig. 9)  , newspaper  (Fig. 10)  , magazine  (Fig. 11)  , exam  (Fig. 12)  , academic  (Fig. 13)  , slide  (Fig. 14)  are shown. D.3 LAYOUT DIVERSITY Next, we visualize and compare the document layout diversity of PubLayNet, DocBank, and OmniLayout-1M as shown in Fig. 15 and Fig. 16. indicates number of documents used for visualization. Compared with two-column format and Manhattan layout typical of academic papers in PubLayNet or DocBank, document layout in OmiLayout-1M significant variation and diversity. 18 Figure 9: Visualization of Textbook Layout Data in OmniLayout-1M. 19 Figure 10: Visualization of Newspaper Layout Data in OmniLayout-1M. 20 Figure 11: Visualization of Magazine Layout Data in OmniLayout-1M. 21 Figure 12: Visualization of Exam Layout Data in OmniLayout-1M. 22 Figure 13: Visualization of Academic Layout Data in OmniLayout-1M. 23 Figure 14: Visualization of Slide Layout Data in OmniLayout-1M. 24 (a) Document layout distribution of PubLayNet. (b) Document layout distribution of DocBank. (c) Document layout distribution of textbook in OmniLayout-1M. (d) Document Layout distribution of newspaper in OmniLayout-1M. Figure 15: Document Layout Distribution of (a) PubLayNet, (b) DocBank, (c) Textbook in OmniLayout-1M, and (d) Newspaper in OmniLayout-1M. 25 (a) Document layout distribution of magazine in OmniLayout-1M. (b) Document layout distribution of exam in OmniLayout-1M. (c) Document layout distribution of academic in OmniLayout-1M. (d) Document layout distribution of slide in OmniLayout-1M. Figure 16: Document Layout Distribution of (a) Magazine, (b) Exam, (c) Academic, and (d) Slide in OmniLayout-1M."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Sun Yat-sen University"
    ]
}