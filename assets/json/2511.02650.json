{
    "paper_title": "Can Visual Input Be Compressed? A Visual Token Compression Benchmark for Large Multimodal Models",
    "authors": [
        "Tianfan Peng",
        "Yuntao Du",
        "Pengzhou Ji",
        "Shijie Dong",
        "Kailin Jiang",
        "Mingchuan Ma",
        "Yijun Tian",
        "Jinhe Bi",
        "Qian Li",
        "Wei Du",
        "Feng Xiao",
        "Lizhen Cui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, a unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates system-level metrics such as runtime and prefilling latency to provide a holistic view. Our experiments uncover several key findings: (1) random pruning is a surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as a reliable foundation for future research on efficient multimodal modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 2 0 5 6 2 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "CAN VISUAL INPUT BE COMPRESSED? VISUAL INPUT TOKEN COMPRESSION BENCHMARK FOR LARGE MULTIMODAL MODELS Pengzhou Ji3, Tianfan Peng1,2, Yuntao Du1, Mingchuan Ma5, Yijun Tian6, 1Shandong University 3Tongji University 4University of Science and Technology of China 5Sichuan University 6University of Notre Dame 7Ludwig Maximilian University of Munich 8EB Tech Co., Ltd. 2Shenzhen University of Information Technology Jinhe Bi7, Qian Li1, Wei Du1, Feng Xiao8, Lizhen Cui Shijie Dong1, Kailin Jiang4, Project Page: https://uniprunebench-lmm.github.io/"
        },
        {
            "title": "ABSTRACT",
            "content": "Large multimodal models (LMMs) often suffer from severe inference inefficiency due to the large number of visual tokens introduced by image encoders. While recent token compression methods, such as pruning and merging, have shown promise in reducing redundancy, their evaluation remains fragmented and inconsistent. In this work, we present UniPruneBench, unified and extensible benchmark for visual token pruning in multimodal LLMs. UniPruneBench provides standardized protocols across six ability dimensions and ten datasets, covering ten representative compression algorithms and three families of LMMs (LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL). Beyond task accuracy, it incorporates systemlevel metrics such as runtime and prefilling latency to provide holistic view. Our experiments uncover several key findings: (1) random pruning is surprisingly strong baseline, (2) no single method consistently outperforms others across scenarios, (3) pruning sensitivity varies significantly across tasks, with OCR being most vulnerable, and (4) pruning ratio is the dominant factor governing performance degradation. We believe UniPruneBench will serve as reliable foundation for future research on efficient multimodal modeling."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recently, large multimodal models (LMMs) have achieved remarkable progress across wide range of multimodal tasks. These models are typically built upon pre-trained large language models (LLMs) by integrating visual encoders (e.g., CLIP (Radford et al., 2021), CoCa (Yu et al., 2022)) and lightweight adapter modules. The visual encoders transform images into sequences of visual tokens, while the adapters bridge these visual representations with the textual space, enabling seamless multimodal understanding and reasoning. Representative LMMs follow two main adapter paradigms: BLIP-style models that rely on cross-modal attention and LLaVA-style models that concatenate ViT patch tokens into the LLM context with MLPl layers. These approaches, exemplified by LLaVA (Liu et al., 2023a), Qwen-VL (Yang et al., 2024), and Intern-VL (Zhu et al., 2025), have achieved strong performance in visual question answering (VQA), grounding, and multimodal reasoning. However, incorporating visual inputs into LMMs inevitably introduces large number of visual tokens, leading to substantial redundancy and creating strong demand for faster inference (Vasu et al., 2025; Wen et al., 2025a). Unlike text tokens, which are semantically dense, visual tokens are often redundant and highly correlated(Bi et al., 2025b). Directly appending hundreds of tokens per image leads to steep increases in computation, memory usage, and inference latency, posing severe * Equal Contribution. Corresponding Author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Overview of UniPruneBench, along with experimental results for representative pruning methods across various data scenarios. bottlenecks for real-time and large-scale deployment. Moreover, for many vision-language tasks such as VQA, it is unnecessary to process the entire image. Only subset of task-relevant regions needs to be considered, further highlighting the inefficiency of uniform dense tokenization. To address this, visual token compression has emerged as promising direction. Recent work explores token pruning and merging to reduce redundancy while preserving essential semantics. For example, FastV (Vasu et al., 2025) prunes tokens with low attention scores, PyramidDrop (Xing et al., 2024) shrinks sequences in layer-wise schedule. Such methods reduce computation and memory and are compatible with existing LMMs due to their ability to process variable-length inputs. Despite these advances, existing token compression methods lack fair and systematic evaluation: 1) Limited coverage of methods, model series, and downstream tasks: Most prior work evaluates only small subset of compression algorithms on narrow set of datasets, preventing comprehensive understanding across different downstream task scenarios. 2) Absence of standardized evaluation protocols: Current studies adopt heterogeneous frameworks, such as LLaVA native eval, LMMS-Eval (Zhang et al., 2024), and VLMEvalKit (Duan et al., 2024b), with inconsistent prompt templates, scoring metrics, and token retention ratios. These inconsistencies make it difficult to reliably compare methods and reproduce reported results. 3) Neglect of system-level metrics and low modularity: Evaluations typically focus on task accuracy while ignoring important metrics such as runtime and end-to-end latency. In addition, many pruning implementations are tightly coupled to specific architectures, limiting their flexibility and making it challenging to extend them to emerging multimodal models. These limitations highlight the need for unified, extensible, and user-friendly benchmark to enable fair and reproducible evaluation of token compression methods for multimodal LLMs. in this paper, we introduce the Unified Visual Token Pruning Benchmark To achieve this, (UniPruneBench), benchmark designed to systematically evaluate plug-and-play visual token compression algorithms. As shown in Fig 1, UniPruneBench is standardized evaluation benchmark that offers fair and unified platform for comparing visual token pruning techniques. In addition, it provides modular and user-friendly interface that decouples pruning logic from model architecture, enabling seamless integration with various LMMs. Specifically, UniPruneBench provides (1) diverse and challenging benchmark spanning six ability dimensions (e.g., comprehensive understanding, OCR, mathematical reasoning, and hallucination) across ten datasets. (2) It categorizes existing plug-and-play token compression methods into three types: ViT-only, LLM-only, and hybrid, based on where token pruning is applied, and offers comprehensive evaluations of ten representative algorithms. Besides, (3) it conducts experiments on three series of large multimodal models: LLaVA-v1.5, Intern-VL3, and Qwen2.5-VL. In addition to measuring performance drop, (4) the benchmark also reports system-level metrics, including total running time, prefilling time, providing holistic view of both accuracy and efficiency. Through extensive experiments, UniPruneBench reveals key observations: (1). Random pruning is surprisingly strong baseline. Despite its simplicity, random pruning often surpasses existing designed strategies, highlighting the need for stronger baselines; (2). No single method dominates across scenarios. Different methods excel under different models, pruning ratios, and datasets;"
        },
        {
            "title": "Preprint",
            "content": "(3). Task sensitivity varies significantly. Instruction-following tasks remain robust, while OCR benchmarks suffer the most severe degradation; (4). Pruning ratio drives accuracy-efficiency trade-offs. Light pruning incurs only moderate drops, whereas aggressive pruning sharply degrades performance; (5). These trends are consistent across all three models, indicating generality."
        },
        {
            "title": "2.1 LARGE MULTIMODAL MODEL",
            "content": "Large Multimodal Models (LMMs) have brought significant breakthroughs in integrating vision and language, enabling models to perform complex cross-modal understanding and reasoning tasks. typical end-to-end LMM consists of three major components: language encoder, vision encoder, and cross-modal interaction module (Caffagni et al., 2024). The language encoder is usually adapted from large language models like LLaMA (Grattafiori et al., 2024; Touvron et al., 2023) and Qwen (Yang et al., 2024), while the vision encoder often adopts architectures such as Vision Transformer (ViT) (Dosovitskiy et al., 2020). The cross-modality module connects the two modalities, allowing language models to process visual inputs effectively. Based on this architecture, various LMMs have been developed with different design choices and training strategies. For instance, Qwen2.5-VL (Bai et al., 2025) introduces visual receptor and follows structured multi-stage training process. Intern-VL3 (Chen et al., 2024b) adopts joint multimodal pretraining across large-scale datasets, while LLaVA (Liu et al., 2023a) and its successor LLaVA-OneVision (Li et al., 2025) focus on enhancing visual grounding and reasoning through taskaligned training with MLP layer. These approaches collectively push the limits of vision-language alignment, leading to strong performance across variety of multimodal benchmarks (Kil et al., 2024; Huang & Zhang, 2024). In addition, recent work explores the use of LLM agents equipped with visual tools (Gao et al., 2024; Fan et al., 2024; Gupta & Kembhavi, 2023; Bi et al., 2025c) to handle more dynamic and interactive multimodal tasks. However, such agent-based methods go beyond the scope of this paper, which focuses on the architectural and training advances of the agent framework."
        },
        {
            "title": "2.2 VISUAL TOKEN COMPRESSION BENCHMARK.",
            "content": "Few benchmarks have been proposed for this task. The most relevant prior analysis work proposed in (Wen et al., 2025a), which evaluates only four token-pruning baselines, namely FastV, SparseVLM, random, and pooling. And it does not provide source code or reproducible scripts. In contrast, our work implements 10 state-of-the-art pruning algorithms and will release all implementation details publicly. Moreover, we evaluate these methods across wider range of downstream tasks and metrics, providing more comprehensive and reproducible benchmark."
        },
        {
            "title": "3 UNIPRUNEBENCH",
            "content": "The UniPruneBench comprises six evaluation dimensions spanning ten datasets, each released under permissive licenses that permit research use. In addition, it benchmarks ten representative methods across three categories and covers five representative LMMs from three different model families."
        },
        {
            "title": "3.1 VISUAL TOKEN COMPRESSION METHODS",
            "content": "LMM pruning aims to reduce redundant tokens while preserving model performance. Especially, The number of visual tokens is usually tens to hundreds of times that of language tokens, and visual signals are inherently more sparse, thus needing to be pruned. As shown in Fig 2, Existing plug-andplay methods can be divided into three categories according to where to prune: ViT-only method (Bolya et al., 2023; Jiang et al., 2025), LLM-only method (Wen et al., 2025b; Ye et al., 2025) , and Hybrid based method (Zhang et al., 2025; Liu et al., 2024a). Besides, one very basic method, i.e., random token pruning, is also adopted as strong baseline. ViT-only methods: Token pruning in ViTs is achieved through two paradigms: token selection and token merging. For token selection, DivPrune (Alvar et al., 2025) formulates pruning as subset selection problem that maximizes diversity, thus reducing redundancy while preserving representative information. G-Prune (Jiang et al., 2025) iteratively updates importance scores via information"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Taxonomy of visual token pruning Methods, including ViT-only, LLM-only, and hybrid. propagation, retaining the most representative tokens from both foreground and background regions. LLaVA-PruMerge (Shang et al., 2024) further optimizes token processing in the CLIP image encoder through an adaptive token merging strategy. LLM-only methods: These approaches prune visual tokens within the LLM to reduce computation while maintaining performance. FastV (Chen et al., 2024a) makes an early attempt by discarding tokens after the second layer of LMMs. VTW (Lin et al., 2025) argues that tokens can be entirely removed once the model reaches sufficient depth. DART (Wen et al., 2025b) selects small set of pivot tokens and removes others with high redundancy. Hybrid methods: Multi-stage hybrid pruning combines strategies across different components of LMMs. SparseVLM uses rank-based strategy to set sparsification ratios adaptively and recycles pruned tokens into compact representations (Zhang et al., 2025). MustDrop evaluates token importance across visual encoding, prefill, and decode, applying stage-specific strategies to remove redundancy and reduce computation (Liu et al., 2024a)."
        },
        {
            "title": "3.2 DATASETS",
            "content": "To systematically assess the impact of pruning techniques on LMMs, we conduct experiments on ten benchmark datasets covering six capability dimensions: (1) Comprehensive Evaluation: MME (Fu et al., 2023), and MMBench (Liu et al., 2024b); (2) Mathematical Reasoning: MathVista (Lu et al., 2024), and Math-Vision (Wang et al., 2025); (3) Optical Character Recognition: SEEDBench-2-Plus (Li et al., 2024), and OCRBench (Liu et al., 2023b); (4) Instruction Following: MIA-Bench (Qian et al., 2024); (5) Multidisciplinary Knowledge: ScienceQA (Lu et al., 2022); (6) Hallucination: POPE (Li et al., 2023), and HallusionBench (Guan et al., 2024). Our dataset selection follows the capability dimensions identified in MME-Survey (Fu et al., 2024), ensuring coverage of core competencies such as visuallanguage understanding, cross-modal reasoning, and instruction following, skills central to current LMM research and applications."
        },
        {
            "title": "3.3 BASE MODEL",
            "content": "Following prior work (Zhang et al., 2025; Liu et al., 2024a; Bi et al., 2025a), we evaluate five representative open-source LMMs from three model families, namely LLaVA-v1.5-7B, InternVL31B, InternVL3-8B, Qwen2.5-VL-3B and Qwen2.5-VL-7B. These models align vision and language by integrating advanced visual and textual components. Typically, visual encoder (e.g., CLIP) processes image inputs, while large language model (e.g., Qwen) handles textual inputs. The extracted features are then fused via Multilayer Perceptron (MLP) connectors, enabling effective multimodal reasoning and alignment. Previous studies typically conduct experiments on only one or two model families, whereas we evaluate models across all three families."
        },
        {
            "title": "3.4 EVALUATION METRICS",
            "content": "We consider multiple metrics in this benchmark. Accuracy serves as basic evaluation metric, complemented by performance drop measured before and after token compression. To reflect"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison across different methods and benchmarks on LLaVA-v1.5-7B. Methods LLaVA-v1.5-7B Vanilla Comprehensive OCR Multidisciplinary Hallucination MME MMB-cn MMB-en SEED OCR-B Science QA POPE Avg. 48.1 43. 63.6 38.8 30.2 68.2 80.1 53. Upper Bound: 576 Tokens (100%) Retain Averaged 192 Tokens ( 66.7%) Random VTW PruMerge FastV DivPrune VisPrune DART MustDrop SparseVLM Random VTW PruMerge FastV DivPrune VisPrune DART MustDrop SparseVLM Random VTW PruMerge FastV DivPrune VisPrune DART MustDrop SparseVLM 46. 3.0% 23.3 51.6% 44.0 8.6% 44.9 6.7% 49.2 2.2% 47.3 1.7% 46. 2.7% 47.8 0.8% 47.7 1.0% 45.4 5.7% 24.7 48.7% 41. 13.8% 41.9 13.0% 48.9 1.7% 47.9 0.5% 46.1 4.3% 47.4 1.5% 48. 1.5% 42.3 12.1% 25.0 48.0% 41.2 14.3% 32.2 33.0% 48. 0.0% 47.8 0.7% 42.8 11.1% 43.9 8.7% 45.5 5.5% 41. 4.4% 0.8 98.2% 6.0 86.3% 23.1 46.9% 12.3 71.8% 18.8 56.9% 25. 40.7% 41.3 5.2% 44.0 1.2% 39.1 10.1% 1.1 97.4% 4. 89.1% 20.7 52.5% 9.0 79.3% 14.5 66.8% 22.4 48.5% 41.7 4.2% 48. 10.7% 4.9 88.7% 4.4 89.9% 4.5 89.7% 12.7 70.8% 6. 85.9% 7.5 82.7% 17.4 60.0% 13.2 69.8% 15.7 63.8% 60. 5.3% 21.0 66.9% 57.1 10.2% 60.5 4.8% 60.4 5.0% 61.6 3.1% 61. 3.0% 61.0 4.1% 61.7 2.8% 40.6 4.5% 36.9 4.9% 38. 1.3% 37.2 4.2% 38.5 0.9% 38.3 1.5% 37.8 2.7% 37.9 2.2% 39. 2.3% 24.8 17.9% 0.9 97.0% 23.4 22.5% 27.0 10.6% 28. 6.0% 29.1 3.6% 28.3 6.3% 28.9 4.3% 28.1 7.0% Retain Averaged 128 Tokens ( 77.8%) 57.8 9.1% 23.5 63.1% 55.4 12.8% 57.8 9.1% 59.5 6.4% 60. 5.2% 61.0 4.1% 61.1 3.8% 62.4 1.8% 39.9 2.7% 36. 4.9% 38.6 0.6% 36.9 5.0% 39.1 0.8% 38.3 1.5% 37.6 3.0% 39. 1.9% 39.9 2.8% 22.2 26.5% 1.0 96.7% 23.7 21.5% 25. 15.9% 27.9 7.6% 29.4 2.6% 26.7 11.6% 29.6 2.0% 24.9 17.5% Retain Averaged 64 Tokens ( 88.9%) 52.8 17.0% 50.0 21.4% 53.1 16.5% 45.8 27.9% 57. 8.9% 58.2 8.4% 57.4 9.7% 57.4 9.7% 58.8 7.5% 37. 3.4% 38.3 1.5% 38.8 0.1% 36.2 6.8% 39.0 0.3% 38.6 0.7% 37. 3.0% 38.0 2.1% 38.1 1.9% 19.1 36.8% 1.4 95.4% 22. 24.5% 16.8 44.4% 26.9 10.9% 28.1 7.0% 23.4 22.5% 24.8 17.9% 16. 44.7% 68.2 0.0% 63.1 7.5% 66.5 2.5% 68.1 0.2% 67. 0.9% 68.0 0.2% 66.3 2.8% 67.6 0.8% 67.4 1.1% 67. 1.7% 64.5 5.4% 67.6 0.9% 68.3 0.2% 67.5 1.0% 67.8 0.6% 67. 1.6% 67.5 0.9% 67.4 1.2% 66.2 2.9% 65.7 3.6% 67. 0.5% 67.2 1.5% 65.9 3.3% 67.5 1.0% 67.9 0.4% 68.5 0.4% 67. 0.6% 84.2 5.1% 4.9 93.9% 74.5 6.9% 74.8 6.6% 86. 7.9% 85.2 6.4% 83.2 4.0% 80.1 0.1% 80.6 0.6% 81. 2.0% 0.0 100.0% 69.8 12.9% 67.5 15.6% 86.4 7.9% 83.9 4.8% 79. 0.2% 78.9 1.5% 83.1 3.7% 75.1 6.2% 9.2 88.6% 65. 18.7% 51.3 35.9% 85.3 6.5% 80.7 0.8% 71.0 11.3% 67.2 16.1% 76. 4.0% 52.3 21.6 44.3 47.9 49. 49.8 50.0 52.1 52.7 50.4 21. 43.1 45.5 48.3 48.9 48.7 52. 53.5 42.6 27.7 41.9 37.5 47. 46.9 45.4 44.7 45.6 practical considerations, we also record total running time, the time required for the entire process, and compression strategy time, the time spent solely on token pruning, and prefilling-phase time, the forward pass that processes image and text tokens before the first decoding step."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Performance comparison across different methods and benchmarks on InternVL3-8B."
        },
        {
            "title": "Methods",
            "content": "InternVL3-8B Vanilla Random-Pre Random-Intra"
        },
        {
            "title": "GPrune",
            "content": "Random-Pre Random-Intra"
        },
        {
            "title": "DivPrune",
            "content": "Random-Pre Random-Intra"
        },
        {
            "title": "Instruction",
            "content": "MME MMB-en SEED"
        },
        {
            "title": "Science QA",
            "content": "POPE Hallus Math-V MathVista"
        },
        {
            "title": "MIA",
            "content": "Avg. 86.44 85.80 69.52 77.21 10.7% 81. 6.2% 80.60 6.8% 81.76 5.4% 82.62 4.4% 77.30 10.6% 78. 9.6% 79.40 8.1% 78.97 8.6% 80.25 7.2% 73.11 15.4% 72. 15.6% 76.49 11.5% 70.82 18.1% 75.79 12.3% 78.79 8.2% 82. 4.4% 83.00 3.3% 84.00 2.1% 83.00 3.3% 77.00 10.2% 79. 7.9% 83.00 3.3% 79.00 7.9% 83.00 3.3% 72.00 16.1% 72. 16.1% 80.00 6.8% 71.00 17.3% 81.00 5.6% 56.65 18.5% 60. 13.7% 58.00 16.6% 63.00 9.4% 62.00 10.8% 55.00 20.9% 57. 18.0% 56.00 19.4% 60.00 13.7% 60.00 13.7% 52.00 25.2% 52. 25.2% 54.00 22.3% 55.00 20.9% 56.00 19.4% Upper Bound, 100% Tokens (100%) 69.70 98.07 90. 49.80 28.29 72.22 70.02 Retain Averaged 33.3% Tokens ( 66.7%) 90. 8.2% 94.00 4.2% 92.00 6.2% 95.00 3.1% 96.00 2.1% 88. 1.9% 89.70 0.7% 90.21 0.1% 90.09 0.3% 90.12 37.25 25.2% 41. 17.5% 42.95 13.8% 44.08 11.5% 46.91 0.2% 5.8% 50. 28.3% 53.90 22.7% 49.80 28.6% 60.50 13.2% 64.60 7.3% Retain Averaged 22.2% Tokens ( 77.8%) 88.00 10.2% 91.00 7.1% 91.00 7.1% 94.00 4.1% 93.00 5.1% 88.08 2.5% 88.93 1.6% 89.37 1.1% 89.33 1.1% 90.18 35. 28.7% 38.24 23.2% 38.29 23.1% 43.70 12.3% 42.64 0.2% 14.4% 47.60 31.7% 48.10 31.0% 49.70 28.7% 55.50 20.4% 56.00 19.7% Retain Averaged 11.1% Tokens ( 88.9%) 85.00 13.3% 86.00 12.2% 89.00 9.2% 88.00 10.2% 90. 8.2% 86.32 4.4% 86.61 4.1% 88.00 2.6% 85.35 5.5% 88. 34.72 30.3% 34.47 30.8% 34.88 30.0% 36.68 26.3% 38.13 1.5% 23.4% 44.00 36.9% 44.90 35.6% 47.00 32.6% 47.90 31.2% 49. 29.4% 22.70 19.8% 27.63 2.3% 24.34 14.0% 26.97 4.7% 26. 4.7% 21.38 24.4% 22.04 22.1% 26.32 7.0% 24.01 15.1% 23. 17.4% 21.38 24.4% 24.67 12.8% 22.04 22.1% 26.32 7.0% 26. 7.0% 80.93 12.1% 74.50 3.2% 74.74 3.5% 79.66 10.3% 81. 13.0% 77.34 7.1% 72.69 0.6% 72.20 0.01% 74.70 3.4% 79. 10.5% 77.93 7.8% 71.97 0.3% 68.97 4.5% 88.71 22.8% 70. 1.8% 64.68 67.10 66.18 69.45 70. 62.91 63.90 65.03 66.58 67.58 60. 60.73 62.26 63.31 64."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "We implemented UniPruneBench in PyTorch and conducted all experiments on NVIDIA A100 GPUs. For benchmark execution across baselines and models, we employed the open-source toolkit VLMEvalKit (Duan et al., 2024a). Unless otherwise specified, all results are reported with batch size of 1. We evaluate performance under different pruning ratios, ensuring that pruned models maintain sufficiently high accuracy for meaningful comparison with baselines. To enable fair comparisons across benchmarks of varying scales, we report both average accuracy and relative performance. To ensure fair comparison across all Intra-LLM pruning strategies, we fix the pruning location inside the large model at layer = 2 for every method. For clarity, Random-Pre denotes uniform random dropping applied to visual tokens before they enter the LLM (Pre-LLM stage), whereas Random-Intra performs the same stochastic removal inside the LLM (Intra-LLM stage); both retain exactly the target sparsity yet introduce no learned importance bias. For task performance metrics, we have normalized to 0-100 for MME to align with other datasets, with higher values indicating better results, while for runtime measurements, lower values are preferable. To simplify presentation, we adopt the following dataset abbreviations when reporting results: MMBench as MMB, Math-Vision as Math-V, SEEDBench-2-Plus as SEED, OCRBench as OCR-B, MIA-Bench as MIA, and HallusionBench as Hallus."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance comparison across different methods and benchmarks on Qwen2.5-VL-7B. Methods Qwen2.5-VL-7B Vanilla Comprehensive OCR Multidisciplinary Hallucination Mathematical Instruction MME MMB-en SEED OCR-B Science QA POPE Hallus Math-V MathVista MIA Avg. 82.5 79.8 69. 78.3 89.0 87.5 47.5 24.3 63. 70.2 69.3 Upper Bound: 100% Token (100%) Retain Averaged 33.3% Tokens ( 66.7%) FastV 69. 74.1 59.0 50.5 15.9% 7.1% 15.2% 35.5% Random-Intra 67. 74.7 58.0 53.3 GPrune 17.9% 6.4% 16.7% 31.9% 68. 71.2 57.0 54.9 16.8% 10.8% 18.1% 29.9% Random-Pre 71. 71.2 57.0 54.9 DART DivPrune GPrune FastV DART DivPrune FastV GPrune 12.8% 10.8% 18.1% 29.9% 71.8 76.5 54.0 58.2 13.0% 4.1% 22.4% 25.7% 72. 73.7 12.4% 7.6% 63.0 9.5% 16.5% 65.4 59.8 65.1 52. 38.1 27.5% 18.4% 25.3% 51.3% 66.4 70.6 55.0 36. 19.5% 11.5% 21.0% 53.5% 17.7% 13.2% 22.4% 41.8% 68.4 74.5 50.0 50. 17.1% 6.6% 28.2% 35.3% 70.0 73.0 59.0 57.5 15.2% 8.5% 15.2% 26.6% 51.4 53.2 47.0 18.9 37.7% 33.3% 32.5% 75.9% 49. 53.7 46.0 16.4 39.5% 32.7% 33.9% 79.1% Random-Intra 64. 71.0 53.0 45.6 21.6% 11.0% 23.9% 41.8% Random-Pre 67. 69.3 54.0 45.6 Random-Intra 62.5 68. 47.0 31.1 24.2% 14.4% 32.5% 60.3% Random-Pre 64.1 64. 48.0 31.7 DART DivPrune 22.3% 18.8% 31.0% 59.5% 62. 70.7 46.0 37.7 24.6% 11.4% 33.9% 51.9% 64.6 68. 51.0 40.8 21.7% 14.0% 26.7% 47.9%"
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "Retain Averaged 22.2% Tokens ( 77.8%) Retain Averaged 11.1% Tokens ( 88.9%) 80.0 10.1% 79.0 11.2% 80.0 10.1% 80.0 10.1% 80.0 10.1% 79.0 11.2% 9.8 37.8 41. 38.6 37.9 40.2 9.34 9.51 83.7 4.3% 20.2% 61.6% 37.1% 81.9 6.4% 18.7% 60.9% 35.8% 84.4 3.5% 20.4% 59.7% 25.2% 84.4 3.5% 20.4% 61.3% 28.5% 81.9 6.4% 12.4% 62.6% 32.2% 84.3 3.7% 12.4% 60.6% 24.4% 9.57 9.41 43.3 41.6 37.8 47. 45.7 48.3 41.6 9.1 78.0 12.4% 78.0 12.4% 78.0 12.4% 78.0 12.4% 80.0 10.1% 79.0 11.2% 9. 31.5 47.5 8.55 35.6 9.21 38. 36.2 80.6 7.9% 33.7% 59.7% 25.7% 80.7 7.8% 25.1% 62.1% 40.4% 80.1 8.5% 23.8% 64.8% 40.5% 79.9 8.7% 28.0% 60.2% 28.8% 79.9 8.7% 15.8% 60.5% 37.2% 82.8 5.4% 21.9% 59.7% 25.4% 40.1 38.0 47.7 37. 34.2 40.0 45.5 9.67 9.6 9. 74.0 16.9% 71.0 20.2% 75.0 15.7% 73.0 18.0% 79.0 11.2% 76.0 14.6% 69.2 29.3 8.98 30.3 21.0% 38.3% 63.0% 52.6% 70.0 24.4 9.24 47.4 20.0% 48.6% 62.0% 25.8% 74. 32.4 8.09 31.9 14.9% 31.8% 66.7% 50.1% 73.4 27. 9.80 45.3 16.1% 41.7% 59.7% 29.1% 73.2 34.8 9. 33.9 16.3% 26.7% 59.3% 46.9% 78.3 30.8 8.95 47. 10.5% 35.2% 63.2% 25.0% 60.9 13.2% 61.9 11.8% 60.10 14.4% 62.9 10.4% 61.69 12.1% 61.5 12.4% 60.71 13.5% 60.3 14.1% 62.3 11.3% 63.0 10.3% 60.83 13.4% 61.3 12.7% 58.2 17.1% 60.3 14.1% 61.3 12.7% 62.7 10.7% 62.6 10.8% 62.22 11.4% 56.5 56. 57.2 57.5 57.8 59.8 52.3 53. 53.7 54.7 55.4 57.7 44.0 44. 49.2 50.1 51.0 52.9 The comparison results of different methods are shown in Table 1, Table 2, and Table 3. Based on these results, several key findings emerge: 1. Random pruning remains surprisingly strong baseline. Random pruning consistently outperforms several well-designed methods, such as GPrune, VTW, and PruMerge. On LLaVA-v1.57B, six out of eight perform worse than random pruning at 66.7% and 77.8% pruning ratios. This unexpected result highlights the limitation of current designs and suggests that more effective pruning strategies are needed beyond naive baselines. 2. No single method achieves universal superiority. No approach dominates across all models and pruning ratios. DivPrune achieves the best results on both Qwen2.5-VL-7B and InternVL3-8B under all ratios. However, on LLaVA-v1.5-7B, SparseVLM surpasses DivPrune under light pruning ratios, while DivPrune regains superiority under more aggressive pruning. This indicates that performance strongly depends on both the model architecture and the pruning level. 3. Hybrid-based methods demonstrate strong overall performance. Among the three categories of methods, hybrid-based approaches achieve the best results on LLaVA-v1.5-7B at the 77.8% and 66.7% pruning ratios, though they perform worse at the 88.9% ratio. On InternVL3-8B and Qwen2.5-VL-7B, ViT-only methods (e.g., DivPrune) consistently outperform LLM-only methods (e.g., FastV), suggesting that vision-side pruning is more effective than language-side pruning."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Performance Comparison of different model sizes for InternVL3 at 88.9% pruning rate. 4. Task-level sensitivity varies: instruction following improves, while OCR degrades severely. Most benchmarks show accuracy degradation as pruning intensifies. However, instruction-following tasks (e.g., MIA) exhibit improvements in some cases. For example, on InternVL3-8B, DivPrune raises accuracy from 72.22% to 79.82%. We hypothesize that pruning increases the relative weight of textual inputs, thereby enhancing instruction adherence. In contrast, OCR tasks are highly sensitive to pruning: as more visual tokens are removed, crucial details are lost, leading to rapid performance decline. 5. Higher pruning ratios induce sharper performance loss. Light pruning leads to moderate degradation, while aggressive pruning causes substantial drops. For example, on Qwen2.5-VL-7B, the average accuracy decreases from 57.5% at 33% tokens to 50.1% at 11% tokens under random pruning. Similarly, on InternVL3-8B, DivPrune maintains 67.58% at 22% tokens but falls to 64.04% at 11% tokens. Notably, DivPrune consistently achieves the best results under the highest pruning ratio (88.9%), showing stronger robustness in extreme scenarios. 6. Consistent cross-model trends. Despite architectural differences, all three models exhibit similar behaviors: random pruning is unexpectedly competitive, OCR tasks are highly fragile, instructionfollowing tasks remain robust, and no single method dominates universally."
        },
        {
            "title": "4.3 ANALYSIS AND DISCUSSION",
            "content": "Influence of model size To investigate the sensitivity of token compression techniques to model scale, we evaluate three representative methods, DivPrune, GPrune, and FastV, across two variants of InternVL: InternVL3-1B (small) and InternVL3-8B (large). As shown in Fig 3, scaling up the base model consistently yields significant accuracy gains across nearly all benchmarks under all compression methods, confirming that larger models retain more semantic capacity even after token reduction. The results indicate that larger architectures provide greater robustness to token reduction, suggesting that compression strategies should be evaluated across scales rather than in isolation. Running time Considering real-world scenarios, we also evaluate the running time of different pruning methods. We profile three nested intervals: Total time, the elapsed time to finish the entire dataset; Prefill time, the single encoder forward pass that computes keys and values for all visual and textual tokens before any decoding starts, phase that is compute-bound for the large model; and Method time, the GPU milliseconds spent only on the compression subroutine (token scoring, selection and tensor re-layout). All measurements were collected on an NVIDIA A100-40 GB GPU with batch size = 1 and three independent runs. All reported methods correspond to uniform pruning rate of 88.9% on the MME benchmark. The results in Table 4 show that the last component never exceeds 0.5s, less than 0.12 % of the corresponding total. So the cost of importance estimation is negligible. Pruning therefore exerts its effect entirely within the prefill: DivPrune and GPrune shorten it from 320 to 185 and 167 s, delivering 1.731.92 encoder acceleration and an overall 1.621.68 end-to-end speed-up versus the vanilla model. Combination of different pruning strategies To examine whether the same overall sparsity should be applied in one step or decomposed, we fix the global pruning ratio at 88.9% and realize it through two design choices: Single-stage, single 88.9% drop executed either before the LLM (Pre-LLM) or inside the LLM (Intra-LLM). Two-stage, 66.7% Pre-LLM pruning followed by 66.7% Intra-LLM pruning, giving the same compound retention. Contrary to the more-is-better intuition, Table 5 reveals that simply chaining two existing pruning stages underperforms the stronger single-stage baseline: the best solitary Pre-LLM method (64.04%) still surpasses every 66.7% 66.7% hybrid method, despite the latter retaining the same number of tokens. This outcome indicates that naÄ±vely"
        },
        {
            "title": "Preprint",
            "content": "Table 4: The running time comparison of different methods on InternVL3-8B"
        },
        {
            "title": "Methods",
            "content": "Total time (sec) Prefill time (sec) Method time (sec) Vallania Random-pre Random-intra Fastv DivPrune GPrune 761.00 491.00 481.00 497.00 469.00 454.00 320.00 201.00 209.00 212.00 185.00 167.00 0.00 0.11 0.12 0.33 0.32 0.47 Table 5: Performance comparison for the combination of different pruning strategies on InternVL3-8B. All methods achieve 88.9% pruning rate. Mixed methods use two-stage pruning. Pre-LLM Intra-LLM"
        },
        {
            "title": "Science QA",
            "content": "POPE Hallus Math-V MathVista"
        },
        {
            "title": "MIA",
            "content": "Avg. InternVL3-8B Vanilla 86.44 85.80 69.52 Upper Bound: 100% Tokens (100%) 98. 49.80 90.33 69.70 28.29 72.22 70."
        },
        {
            "title": "FastV",
            "content": "44.90 34.47 72.97 52.00 72.00 15.6% 16.1% 25.2% 73.11 52.00 72.00 15.4% 16.1% 25.2% 76.49 54.00 80.00 11.5% 6.8% 22.3% 70.82 55.00 71.00 18.1% 17.3% 20.9% 56.00 81.00 75.79 12.3% 5.6% 19.4% Retain Averaged 11.1% Tokens ( 88.9%) 86.00 12.2% 85.00 13.3% 89.00 9.2% 88.00 10.2% 90.00 8.2% 86.61 24.67 4.1% 30.8% 35.6% 12.8% 86.32 21.38 4.4% 30.3% 36.9% 24.4% 88.00 22.04 2.6% 30.0% 32.6% 22.1% 26.32 85.35 47.90 36.68 7.0% 5.5% 26.3% 31.2% 26.32 88.95 38.13 49.20 7.0% 1.5% 23.4% 29.4% 47. 34.88 34.72 44.00 53.0 53.4 51. 73.72 72.0 35.4% 16.1% 25.4% 74.24 73.5 33.0% 14.3% 23.7% 74.6 77.57 32.0% 13.1% 23.1% 77.89 77.7 30.8% 9.4% 22.2% 76.97 77.4 31.6% 9.8% 22.5% 77.97 80.1 31.5% 6.6% 22.0% 54.2 53.9 54.1 84.6 13.7% 87.3 11.0% 89.4 8.8% 88.0 10.2% 88.2 10.0% 89.7 8.5% 34. 37.1 37.2 45.6 50.3 47.4 86.4 10.29 4.3% 30.7% 34.6% 63.6% 87.5 10.68 3.1% 25.5% 32.0% 62.2% 10.03 87.6 3.0% 25.3% 27.8% 64.5% 87.7 10.49 2.9% 27.5% 30.0% 62.9% 87.6 3.0% 24.9% 31.1% 64.8% 10.66 88.5 2.0% 28.3% 30.3% 62.3% 50.3 37.4 9.96 48.0 37.4 48. 36.1 71.97 0.3% 77.93 7.8% 68.97 4.5% 88.71 22.8% 70.97 1.8% 69.8 3.4% 71.0 1.7% 68.7 4.9% 70.3 2.6% 71.7 0.7% 72.6 1.8% 60.62 60.72 62. 63.31 64.04 58.76 60.19 60.98 61. 61.24 62.38 concatenating off-the-shelf pruning criteria does not guarantee additive gains. Instead, an effective combination requires deliberate design that respects the complementary nature of each stage as well as the downstream scenario. Without such targeted orchestration, it may be that the second stage often re-discards already informative tokens, leading to sub-optimal performance even though the aggregate sparsity is unchanged."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced UniPruneBench, unified benchmark for evaluating visual token pruning methods in large multimodal models. By systematically covering diverse datasets, model families, pruning algorithms, and system-level efficiency metrics, UniPruneBench addresses the limitations of prior fragmented and non-standardized evaluations. Our results reveal surprising trends, including the competitiveness of random pruning, the lack of universally superior method, and the task-specific vulnerabilities of pruning strategies. These insights highlight both the challenges and opportunities for designing more effective token compression methods. We hope UniPruneBench not only facilitates fair comparison and reproducibility but also inspires future advances in efficient multimodal learning and deployment."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "Our UniPruneBench benchmark is designed to provide standardized, fair, and reproducible evaluation of visual token pruning methods in large multimodal models. The benchmark itself does not generate content or make decisions, and thus poses minimal direct ethical risk. However, potential considerations include: 1. Data sources and bias: UniPruneBench relies on existing public datasets, which may contain inherent biases in terms of demographics, languages, or visual concepts. Users should be aware that these biases may affect model evaluation outcomes. 2. Misuse of results: While the benchmark aims to improve efficiency in multimodal models, it could indirectly enable faster deployment of models in sensitive applications. We encourage responsible use and adherence to ethical AI guidelines. 3. User queries and prompts: Models evaluated on UniPruneBench could still produce harmful or inappropriate outputs in response to malicious or unsafe queries. The benchmark does not mitigate such risks, and appropriate safeguards should be implemented by users. Overall, UniPruneBench aims to advance research in efficient multimodal modeling in safe and responsible manner, providing transparency and reproducibility while minimizing ethical concerns."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide full details to ensure that all experiments in this paper are reproducible. The code of UniPruneBench is shown in the appendix, including standardized evaluation scripts and token pruning implementations. All datasets used are publicly available, and we specify dataset preprocessing, prompt templates, token retention ratios, and system-level measurement protocols. Additionally, we include instructions for reproducing results across different model families (LLaVA, Intern-VL, and Qwen-VL) and pruning methods. By releasing the benchmark and evaluation pipeline, we aim to enable fair comparison, facilitate further research, and ensure transparency in reported findings."
        },
        {
            "title": "REFERENCES",
            "content": "Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversitybased visual token pruning for large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 93929401, 2025. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Jinhe Bi, Yifan Wang, Danqi Yan, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. Prism: Self-pruning intrinsic selection method for training-free multimodal data selection. arXiv preprint arXiv:2502.12119, 2025a. Jinhe Bi, Yujun Wang, Haokun Chen, Xun Xiao, Artur Hecker, Volker Tresp, and Yunpu Ma. LLaVA steering: Visual instruction tuning with 500x fewer parameters through modality linear representation-steering. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1523015250, Vienna, Austria, July ISBN 979-8-89176-251-0. URL https: 2025b. Association for Computational Linguistics. //aclanthology.org/2025.acl-long.739/. Jinhe Bi, Danqi Yan, Yifan Wang, Wenke Huang, Haokun Chen, Guancheng Wan, Mang Ye, Xun Xiao, Hinrich Schuetze, Volker Tresp, et al. Cot-kinetics: theoretical modeling assessing lrm reasoning process. arXiv preprint arXiv:2505.13408, 2025c. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. In ICLR, 2023."
        },
        {
            "title": "Preprint",
            "content": "Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey. ACL, 2024. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models. In European Conference on Computer Vision, pp. 1935. Springer, 2024a. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2020. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pp. 1119811201, 2024a. Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pp. 1119811201, 2024b. Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding. In European Conference on Computer Vision, pp. 7592. Springer, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv:2306.13394, 2023. Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296, 2024. Zhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing Li. Clova: closed-loop visual assistant with tool usage and update. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1325813268, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for In Proentangled language hallucination and visual illusion in large vision-language models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14375 14385, 2024. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1495314962, 2023. Jiaxing Huang and Jingyi Zhang. survey on evaluation of multimodal large language models. arXiv preprint arXiv:2408.15769, 2024. Yutao Jiang, Qiong Wu, Wenhao Lin, Wei Yu, and Yiyi Zhou. What kind of visual tokens do we need? training-free visual token pruning for multi-modal large language models from the perspective of graph. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 40754083, 2025."
        },
        {
            "title": "Preprint",
            "content": "Jihyung Kil, Zheda Mai, Justin Lee, Arpita Chowdhury, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, and Wei-Lun Harry Chao. Mllm-compbench: comparative reasoning benchmark for multimodal llms. Advances in Neural Information Processing Systems, 37:2879828827, 2024. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. Transactions on Machine Learning Research, 2025. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 292305, 2023. Zhihang Lin, Mingbao Lin, Luxi Lin, and Rongrong Ji. Boosting multimodal large language models with visual tokens withdrawal for rapid inference. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 53345342, 2025. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023a. Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model, 2024a. URL https: //arxiv.org/abs/2411.10803. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pp. 216233. Springer, 2024b. Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023b. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Miabench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
        },
        {
            "title": "Preprint",
            "content": "Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel, and Hadi Pouransari. Fastvlm: Efficient vision encoding for vision language models, 2025. URL https://arxiv.org/ abs/2412.13303. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, and Linfeng Zhang. Token pruning in multimodal large language models: Are we solving the right problem? arXiv preprint arXiv:2502.11501, 2025a. Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more, 2025b. URL https://arxiv.org/abs/2502.11494. Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. arXiv preprint arXiv:2410.17247, 2024. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv:2412.15115, 2024. Weihao Ye, Qiong Wu, Wenhao Lin, and Yiyi Zhou. Fit and prune: Fast and training-free visual token pruning for multi-modal large language models. Proceedings of the AAAI Conference on Artificial Intelligence, 39(21):2212822136, Apr. 2025. doi: 10.1609/aaai.v39i21.34366. URL https://ojs.aaai.org/index.php/AAAI/article/view/34366. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui arXiv preprint Coca: Contrastive captioners are image-text foundation models. Wu. arXiv:2205.01917, 2022. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024. Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, and Shanghang Zhang. SparseVLM: Visual token sparsification for efficient vision-language model inference. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=80faIPZ67S. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A THE USE OF LARGE LANGUAGE MODELS",
            "content": "Large language models (LLMs) have increasingly become valuable tools for academic writing and manuscript preparation. In this work, we leverage LLMs primarily for text refinement, language polishing, and structural editing of our paper. This includes improving clarity, correcting grammatical errors, rephrasing sentences for conciseness, and ensuring logical flow across sections. Importantly, LLMs are used only as assistive tools. All scientific content, experiments, and analyses are independently designed, implemented, and verified by the authors. We emphasize that LLMs do not contribute to the experimental results, numerical analyses, or core intellectual content of this work. This responsible usage ensures the integrity and reproducibility of our research while benefiting from advanced language capabilities to improve presentation quality."
        },
        {
            "title": "B MORE IMPLEMENTATION DETAILS",
            "content": "System Configuration Our codebase was implemented in Python 3.12 with PyTorch 2.5.1, Transformers 4.54.0 and CUDA 12.4. All experiments were conducted on NVIDIA A100-40 GB GPUs. Model Configuration We evaluate three representative LMM families: LLaVA-v1.5 (7B), Qwen2.5-VL (3B & 7B), and InternVL-3 (1B & 8B), all in their official HuggingFace checkpoints without fine-tuning or weight modification. For Qwen2.5-VL, we adopt the image pre-processing setting min pixels=2562828 and max pixels=12802828, These settings help the model maintain image quality while controlling computational cost and resource consumption. Method Implementation Pre-LLM pruning is inserted immediately after the ViT forward: visual features are collected, scored by the selected algorithm (Random-Pre, GPrune, DivPrune, etc. ), and the kept indices are used to rebuild shorter multimodal embedding tensor before the LLM sees any tokens. Intra-LLM pruning is implemented as per-layer hook that activates at layer = 2; hidden states are split into system, visual and instruction segments, the visual subset is pruned, and position ids, position embeddings, and the causal mask are truncated to match the reduced token count; the subsequent layer thus computes keys/values only for the kept subset. Attention weights required by attention-based methods are obtained with single eager-mode forward pass, saved to temporary file, and loaded by the prune routine, keeping the modification orthogonal to FlashAttention or SDPA code paths. All pruning decisions are executed after vision encoding and before KV-cache construction, ensuring that generation length and memory footprint shrink proportionally. Metric Calculation Task scores are produced by the official VLMEvalKit evaluation scripts. For MME, we normalise the original counts to 0100 scale to align with other datasets; higher values indicate better performance. We further compute relative performance, allowing direct comparison of accuracy retention across tasks and pruning strengths. Time Measurement Wall-clock latency is decomposed into three nested intervals: (1) totalendto-end elapsed time for completing the entire benchmark; (2) prefillthe compute-bound encoder forward pass that processes all visual and textual tokens before the first decode step; (3) methodthe GPU milliseconds consumed inside prefill by the pruning subroutine (token scoring, selection and tensor re-layout). Intervals are recorded with on an A100-40 GB, batch size = 1, and averaged over three runs."
        },
        {
            "title": "C MORE RESULTS",
            "content": "We benchmark the pruning performance of existing methods on InternVL3-1B and Qwen2.5-VL3B, with results summarized in Table 6 and Table 7, respectively. Across both model families, we observe consistent trends that align with the broader empirical patterns reported in Section Experiments. These findings reinforce the generality of our benchmark conclusions, indicating that the relative strengths and limitations of current pruning techniques are largely preserved across architectures of varying scales and designs. Such consistency underscores the reliability of our evaluation protocol and highlights the transferable insights that can be drawn from the benchmark results."
        },
        {
            "title": "Preprint",
            "content": "Table 6: Performance comparison across different methods and benchmarks on InternVL3-1B."
        },
        {
            "title": "Methods",
            "content": "InternVL3-1B Vanilla Random-Pre Random-Intra"
        },
        {
            "title": "DivPrune",
            "content": "Random-Pre Random-Intra"
        },
        {
            "title": "DivPrune",
            "content": "Random-Pre Random-Intra"
        },
        {
            "title": "Instruction",
            "content": "MME MMB-en SEED"
        },
        {
            "title": "Science QA",
            "content": "POPE Hallus Math-V MathVista"
        },
        {
            "title": "MIA",
            "content": "Avg. 68.41 73.25 58.41 65.06 4.9% 62. 8.4% 66.52 2.8% 65.78 3.8% 66.71 2.5% 55.84 18.4% 59. 13.2% 64.55 5.6% 56.09 18.0% 56.93 16.8% 52.63 23.1% 55. 19.5% 59.33 13.3% 55.18 19.4% 55.41 19.0% 65.45 10.6% 35. 51.9% 34.20 53.3% 71.77 2.0% 71.26 2.7% 63.20 13.7% 27. 62.5% 24.24 66.9% 71.08 3.0% 70.13 4.2% 58.70 19.8% 17. 76.7% 19.74 73.0% 66.32 9.5% 65.80 10.2% 47.30 19.0% 52. 10.7% 45.19 22.6% 49.56 15.1% 52.61 9.9% 44.93 23.1% 43. 25.1% 42.15 27.8% 46.60 20.2% 49.28 15.6% 41.46 29.0% 39. 31.7% 38.46 34.1% 42.95 26.4% 44.53 23.8% Upper Bound, 100% Tokens (100%) 46.20 91.57 36. 89.57 18.75 63.48 60.64 Retain Averaged 33.3% Tokens ( 66.7%) 83. 9.2% 87.12 4.9% 88.65 3.2% 89.74 2.0% 88.00 3.9% 87. 2.4% 87.09 2.8% 88.67 1.0% 88.27 1.4% 88.36 1.3% 32. 11.1% 35.33 2.2% 34.04 5.8% 33.93 6.1% 33.13 8.3% 37. 19.0% 37.00 19.9% 30.50 34.0% 40.80 11.7% 42.20 8.7% Retain Averaged 22.2% Tokens ( 77.8%) 81.61 10.9% 82.42 10.0% 85.73 6.4% 88.35 3.5% 86.66 5.4% 85.60 4.4% 86.12 3.8% 87.55 2.2% 85.43 4.6% 87.63 26. 26.4% 33.55 7.1% 34.40 4.8% 28.54 21.0% 28.77 2.1% 20.4% 35.70 22.7% 32.40 29.9% 31.50 31.8% 38.10 17.5% 37.90 17.9% Retain Averaged 11.1% Tokens ( 88.9%) 79.52 13.1% 80.97 11.6% 84.42 7.8% 83.74 8.5% 83. 8.9% 82.51 7.8% 80.65 9.9% 82.84 7.5% 75.56 25. 28.6% 29.11 19.4% 31.90 11.7% 27.09 15.6% 25.0% 84.58 26.72 5.6% 26.0% 33.20 28.1% 29.30 36.6% 31.60 31.6% 33.70 27.1% 36.30 21.4% 14.14 24.6% 16.78 10.5% 13.49 28.1% 18.42 1.8% 14.14 24.6% 14.14 24.6% 16.78 10.5% 13.49 28.1% 13.16 29.8% 14.80 21.1% 15.46 17.5% 17.76 5.3% 18.09 3.5% 11.51 38.6% 11.84 36.8% 59.85 5.7% 41.88 34.0% 38.98 38.6% 62.71 1.2% 61.38 3.3% 60.70 4.4% 46.67 26.5% 41.21 35.1% 61.34 3.4% 63.68 0.3% 61.12 3.7% 42.61 32.8% 39.26 38.1% 61.84 2.5% 60.68 4.4% 54.65 50.58 48.91 57.89 57.53 52. 47.62 47.20 54.30 55.09 50.04 43. 45.07 50.88 52."
        },
        {
            "title": "Preprint",
            "content": "Table 7: Performance comparison across different methods and benchmarks on Qwen2.5-VL-3B. Comprehensive OCR Multidisciplinary Hallucination Mathematical Instruction MME MMB-en SEED OCR-B Science QA POPE Hallus Math-V MathVista MIA Avg. 77.0 79.2 68.0 82.3 81. 86.7 45.3 9.9 38.5 73.5 64. Upper Bound: 100% Token (100%) Retain Averaged 33.3% Tokens ( 66.7%) Methods Qwen2.5-VL-3B Vanilla GPrune Random-Intra Random-Pre DART FastV DivPrune GPrune 70.6 8.3% 70.3 8.7% 72.3 6.1% 72.0 6.5% 70.7 8.2% 72.6 5.7% 55.5 54.6 55.0 59.0 72.1 9.0% 13.2% 33.7% 73.5 56.0 7.2% 17.6% 32.6% 71.7 57.0 9.5% 16.2% 33.2% 53.0 75.6 4.5% 22.1% 32.0% 75.3 57.0 4.9% 16.2% 38.4% 75.4 60.0 4.8% 11.8% 21.1% 64.9 56. 50.7 80.0 1.2% 80.0 1.2% 79.0 2.5% 82.0 1.2% 80.0 1.2% 80.0 1.2% 9.0 9.6 9.4 37.8 83.7 3.4% 16.6% 5.1% 84.3 36.7 2.8% 19.0% 9.1% 83.6 35.6 3.6% 21.4% 3.0% 38.1 80.0 7.7% 15.9% 9.1% 85.4 40.3 1.5% 11.0% 2.0% 86.4 39.7 0.4% 12.4% 8.1% 9.7 9.0 9.1 Retain Averaged 22.2% Tokens ( 77.8%) 41.9 64. 54.0 38.5 45.6% 18.3% 20.6% 53.2% Random-Intra 65.2 70. 53.0 43.7 15.3% 11.2% 22.1% 46.9% Random-Pre 69.1 70. 53.0 44.3 10.3% 11.4% 22.1% 46.2% 68.9 74.4 50. 48.3 10.5% 6.1% 26.5% 41.3% 68.2 71.5 53.0 36. 11.4% 9.7% 22.1% 55.9% 69.3 73.9 57.0 57.5 10.0% 6.7% 16.2% 30.1% DART FastV DivPrune GPrune 20.6 52. 49.0 14.5 73.2% 33.6% 27.9% 82.4% Random-Intra 60.3 65. 48.0 30.8 21.7% 17.3% 29.4% 62.6% Random-Pre 62.6 65. 48.0 30.5 DART FastV DivPrune 18.7% 17.8% 29.4% 62.9% 62.0 69.9 47.0 29.5 19.5% 11.7% 30.9% 64.2% 20. 56.6 49.0 18.6 73.2% 28.5% 27.9% 77.4% 63.3 70. 52.0 43.1 17.8% 11.4% 23.5% 47.6% Retain Averaged 11.1% Tokens ( 88.9%) 78.0 3.7% 80.0 1.2% 79.0 2.5% 82.0 1.2% 80.0 1.2% 80.0 1.2% 9. 9.0 9.6 79.1 33.8 8.7% 25.4% 1.0% 82.3 36.2 5.0% 20.1% 9.1% 33.2 81.7 5.8% 26.7% 3.0% 37.2 76.1 12.2% 17.9% 9.1% 82.6 37.8 4.7% 16.6% 5.1% 85.3 36.9 1.6% 18.5% 7.1% 9.2 9.0 9. 68.6 27.9 8.9 27.0 20.8% 38.4% 10.1% 29.9% 9. 8.8 68.0 76.0 77.2 31.2 10.9% 31.1% 11.1% 29.0 12.3% 36.0% 6.1% 33.1 21.5% 26.9% 2.0% 33.2 16.8% 26.7% 9.1% 81.7 34.8 5.8% 23.2% 9.1% 72. 9.0 9.0 9.7 34.8 9.6% 35.5 7.8% 33.1 14.0% 31.2 19.0% 36.9 4.2% 75.0 7.4% 77.0 4.9% 77.0 4.9% 81.0 0.0% 79.0 2.5% 78.0 3.7% 34.9 9.4% 37.5 2.6% 37.5 2.6% 38.5 0.0% 38.6 0.3% 39.7 3.1% 33.8 12.2% 36.2 6.0% 36.9 4.2% 38.0 1.3% 37.8 1.8% 39.1 1.6% 73.0 0.7% 72.9 0.8% 72.2 1.8% 73.5 0.0% 70.4 4.2% 72.5 1.4% 73.3 0.3% 72.9 0.8% 72.3 1.6% 72.6 1.2% 71.7 2.4% 73.9 0.5% 68.3 7.1% 72.0 2.0% 72.2 1.8% 73.1 0.5% 69.0 6.1% 75.0 2.0% 57. 57.6 57.4 57.8 57.8 60.0 50. 54.9 54.9 55.7 54.8 58.2 41. 50.6 50.5 50.6 43.8 54."
        }
    ],
    "affiliations": [
        "EB Tech Co., Ltd.",
        "Ludwig Maximilian University of Munich",
        "Shandong University",
        "Shenzhen University of Information Technology",
        "Sichuan University",
        "Tongji University",
        "University of Notre Dame",
        "University of Science and Technology of China"
    ]
}