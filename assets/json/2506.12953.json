{
    "paper_title": "Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition",
    "authors": [
        "Mayank Bumb",
        "Anshul Vemulapalli",
        "Sri Harsha Vardhan Prasad Jella",
        "Anish Gupta",
        "An La",
        "Ryan A. Rossi",
        "Hongjie Chen",
        "Franck Dernoncourt",
        "Nesreen K. Ahmed",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible prompt-based strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of a complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 3 5 9 2 1 . 6 0 5 2 : r Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition Mayank Bumb1, Anshul Vemulapalli1, Sri Harsha Jella1, Anish Gupta1, An La1, Ryan Rossi2, Hongjie Chen3, Franck Dernoncourt2, Nesreen Ahmed4, Yu Wang5 1University of Massachusetts Amherst, 2Adobe, 3Dolby Labs, 4Intel, 5University of Oregon"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Models (LLMs) have demonstrated new possibilities for accurate and efficient time series analysis, but prior work often required heavy fine-tuning and/or ignored inter-series correlations. In this work, we explore simple and flexible promptbased strategies that enable LLMs to perform time series forecasting without extensive retraining or the use of complex external architecture. Through the exploration of specialized prompting methods that leverage time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation, we find that it is possible to enhance LLM forecasting quality while maintaining simplicity and requiring minimal preprocessing of data. To this end, we propose our own method, PatchInstruct, which enables LLMs to make precise and effective predictions."
        },
        {
            "title": "Introduction",
            "content": "Time-series forecasting (TSF) has broad range of applications in agriculture, business, epidemiology, finance, etc. Many of these applications require robust predictions of time series, and accurately modeling the dependencies between variables remains to be challenge (Shao et al., 2020). Traditional forecasting models such as ARIMA, LSTMs, and even Transformer/Graph-based architectures have displayed strong performance on these tasks (Zhou et al., 2024). More recently, Large Language Models (LLMs) have shown promising future in modeling time series, with accurate predictions that rival state of the art (SOTA) methods, due to their strengths in pattern recognition, sequence modeling, and generalization across tasks. However, current LLM-based methods often rely on complex architectures or require heavy fine-tuning, limiting their scalability to real-world applications. One prominent approach, S2IP-LLM (Pan et al., 2024), embeds time series into semantic space to enhance forecasting performance. While effective, it introduces two key limitations. First, it incurs high computational cost during inference due to its reliance on complex decomposition and patching pipelines. Second, it does not explicitly model dependencies across related time series, which can be critical in domains such as traffic and energy forecasting where inter-series relationships play significant role. We aim to develop method (see Figure 1) that maintains the predictive strength of LLM-based models while addressing the above limitations of inference speed and generalization. Therefore we guide our experimentation around the idea of whether we can create general-purpose prompts that guide LLMs to forecast time series both accurately and efficiently, without requiring model fine-tuning or architectural changes. To this end, we introduce PatchInstruct, prompt-based framework that tokenizes time series data into meaningful patches that encapsulate temporally relevant patterns and guides the LLM via structured natural language instructions to output precise predictions. Unlike prior work, PatchInstruct requires no model retraining or architecture modification and also significantly reduces inference time (in comparison to the baseline and complex architectures) alongside token usage while preserving or improving accuracy. We compare PatchInstruct with several other prompting strategiesincluding Zero-shot, Neighbors, and PatchInstruct + Neighborsand evaluate them on diverse, real-world datasets (Weather and Traffic), primarily using GPT-4 and GPT-4o as the LLM backbones. PatchInstruct based methods consistently outperform baselines across small forecasting horizons and datasets. Notably, PatchInstruct almost always achieves top forecasting accuracy on small values of horizons (at most 12), in regards to MSE/MAE, while reducing the inference overhead by 10x100x Figure 1: LLM-based Time-Series Forecasting Pipeline compared to S2IP-LLM while maintaining strong accuracy. These results support our hypothesis that prompt engineering can effectively replace some degree of architectural complexity and will help with scalable and domain-adaptable time series forecasting using LLMs."
        },
        {
            "title": "2.1 Time Series Foundation Models",
            "content": "Foundation Models (FMs), otherwise known as large pre-trained models, are known to have helped achieve state-of-the-art performance with large language models (LLMs) in natural language processing (NLP) and advanced models in computer vision (CV) (Shi et al., 2024). These models consume large amounts of data and serve to provide general purpose representation which may be modified to perform on diverse set of tasks. As of late, the ideas behind FMs have been extended towards the time series domain allowing for the creation of many different time series foundation models (TSFMs). comprehensive taxonomy provided by Yuxuan Liangs survey on time series allows us to distinguish and classify TSFMs through the use of four hierarchical levels: data category, model architecture, pre-training techniques, and application domain (Liang et al., 2024). Data category is separated into 3 subsections in which time series are classified as either standard, spatial, or other (trajectory/event data). The defining feature of TSFM, the architecture, is similarly divided into Transformer-based, non-Transformer based, and Diffusion-based models. Most popular and well received models for time series forecasting seem to utilize transformer based backbone as they are able to handle sequential data efficiently (Miller et al., 2024). There exist some models that aim to bridge the gap between different architectures by combining multiple different backbones such as TimeDiT, novel diffusion transformer-based foundation model for time series analysis (Cao et al., 2024b). Lastly, the majority of TSFMs can be categorized as using either self-supervised or fullysupervised pre-training techniques. Many papers have proposed models satisfying the above taxonomy, but some notable TSFMs include Lag-Llama and TimeGPT. Both models train on large amounts of diverse data and demonstrate that large-scale pretraining is able to improve forecasting accuracy and adaptability (Rasul et al., 2023)."
        },
        {
            "title": "2.2 LLMs for Time Series Forecasting",
            "content": "Large Language Models (LLMs) have recently begun to demonstrate their prowess in time series analysis and forecasting, stemming from their ability to handle complex numerical sequences across wide domains. key innovation introduced by LLMs is their ability to bridge the modality gap between textual and numerical data, which allows for time series to be reinterpreted as language modeling tasks. Early efforts like PatchTST (Nie et al., 2022) demonstrated that breaking time series down into smaller segments, localized patch tokens, significantly helps to enhance the forecasting accuracy while reducing computational overhead. These ideas have been used as foundation and extended into new models like TST (Zerveas et al., 2020), which work by embedding patches into latent vectors to use as form of textual representations for numerical inputs. Many recent methods involve converting realvalued time series into some form of discrete tokens. Chronos (Ansari et al., 2024) is an example where the model applies quantization and scaling to transform observations into fixed vocabulary useful for zero-shot and transfer learning. Similarly, digit-level tokenization (Gruver et al., 2024) is another method in which each digit of number is treated as separate token, allowing LLMs to leverage their advantages in next-token prediction abilities for forecasting tasks. Building even further upon this idea, PromptCast (Xue and Salim, 2023) reformulates forecasting as sentence-to-sentence generation using specialized prompt engineering. Models like GPT4TS (Zhou et al., 2023) combine tasks such as anomaly detection, classification, and forecasting through textual prompting alone. Although there are numerous benefits presented by the usage of LLMs in TSF, they also seem to face challenges when applied to diverse time series data. This is especially noticeable in data with missing values and irregular patterns. To address concerns such as these, LLM4TS (Chang et al., 2024) introduces two-stage training pipeline. This model first works by aligning pretrained LLMs to standardized time series structure and then begins the fine-tuning for forecasting. Other approaches like TEMPO (Cao et al., 2024a) explicitly decompose time series into trend, seasonal, and residual components, enhancing model interpretability and accuracy. Furthermore, some architectures have merged different methodologies into hybrid form in an attempt to draw out more of the LLMs capabilities. TPLLM (Ren et al., 2024) incorporates CNNs and GCNs alongside LLMs to capture spatial-temporal dependencies in traffic prediction. GenTKG (Liao et al., 2024) leverages retrieval-augmented generation and parameter-efficient tuning for temporal knowledge graphs. Despite the previous works done in adapting LLMs for time series, many existing and recent methods seem to require heavy fine-tuning and/or complex architecture that is usually associated with high costs. In this work, we propose PatchInstruct, simple and straightforward prompting framework that divides time series data into smaller segments called patches, and directly prompts LLMs without the need for any additional training. This approach minimizes token usage while maintaining strong predictive performance on low horizons across diverse domains."
        },
        {
            "title": "3 Methodology",
            "content": "We propose an approach that leverages Large Language Models (LLMs) for time series forecasting through specialized prompt engineering techniques that eliminates the need for model fine-tuning or architectural modifications. Our approach begins with zero-shot baseline, inspired by TimeLLM (Gruver et al., 2024), where the model is prompted with raw historical time series values and tasked with predicting future values. While this baseline offers simplicity and generality, it lacks the inductive bias necessary to capture local temporal dynamics, leading to suboptimal performance in complex forecasting settings. Our second baseline S2IP-LLM introduces significant inference-time overhead due to its reliance on complex decomposition pipelines and fine-tuning, limiting its scalability in real-world deployments. To address these limitations, we introduce PatchInstruct (see Figure 2), prompting strategy that encode temporal structure through patch-based representations, and provide pretrained LLMs more context on the dataset. The core idea is to decompose time series into fixed-length overlapping patches and provide them to the LLM in structured format, along with instructions to predict future values. Additionally, we experimented the models forecasting ability by supplementing the target time series with small set of similar time series referred to as Neighbors (Neighs). Specifically, we select the five most similar time series from the datasets past seen data, referred to as neighbors. The motivation behind this approach is to provide the LLM with additional contextual signals and recurring patterns that may not be fully observable in the target series alone. We finally also tested combination of these the Patch-Instruct and Neighbors strategy, enriching the prompt with structurally decomposed information from the target series (via patching), while augmenting it with relevant patterns from similar series (via nearest neighbors). In the following subsections, we detail the construction of each approach, describe the datasets and evaluation metrics used, and present comparative analysis of their forecasting performance. We evaluate our method on two time series datasets: Weather and Traffic. These datasets comprised of continuous measurements sampled at regular intervals. 96-timestep input window is used to forecast future horizons of 1,2,3,4,5,6 and 12 steps. Figure 2: PatchInstruct Forecasting Pipeline 3.1 Overview of Framework 3.2 Prompt Design Our framework is designed to adapt large language models (LLMs) for time series forecasting without any fine-tuning, using carefully structured prompts that condition the model with temporal data and forecasting instructions. The pipeline is modular and supports multiple prompting strategies, including PatchInstruct, Neighbors, and PatchInstruct + Neighbors, and Zeroshot by modifying the structure of the system prompt and the input representation. At inference time, raw time series is converted into sequence of string-formatted numerical values. Depending on the method, additional transformations are appliedfor example, decomposing the sequence into overlapping fixed-length patches (in PatchInstruct), or retrieving similar time series (in Neighs). These inputs are concatenated with forecasting instructions (e.g., \"Predict the next values\") and passed to the LLM. The output is parsed into numerical forecast and compared with the ground truth using standard forecasting metrics. In addition to forecasting, PatchInstruct also prompts the model to output reconstructed patches from the input, enabling an optional interpretability step. These predicted patches can be compared with the actual ones to assess whether the LLM is learning meaningful temporal structure and capturing local dynamics, thus providing deeper insight into the models understanding of the task. We now delve deeper into the specific construction of each prompting strategy. This section provides detailed formulations illustrating how time series data, patching instructions, and neighboring trends are encoded within the input to the LLM. The prompts were designed through rigorous empirical testing to ensure clarity and effectiveness. Each prompt consists of system prompt, which defines the forecasting method and describes how we construct the series, and user prompt, which contains the actual time series data provided to the LLM for prediction. PatchInstruct is built upon the zeroshot prompt inspired by LLMTime, this method decomposes the time series into patches and the LLM uses them to form predictions. Below, we outline the structure of the best Patch-Instruct prompt. Additional experiments exploring alternative patching strategies are presented in the Appendix A. The following prompt is user prompt used for most methods to specify the horizon, and give the context window to the LLM. Horizon Prompt (Input Time Series) Continue the following sequence without Sequence: producing any additional text. <x1, x2, x3, ..., x96>. Predict the next 3 values. PatchInstruct System Prompt PatchInstruct + Neighs System Prompt You are forecasting assistant that sees time series data. The sequence represents the total regional humidity measured every 10 minutes. Task: (1) Split the series into overlapping patches with window size 3 and stride 1. (2) Generate the patches in natural order, then reverse the list so the most recent patch appears first. (3) Use these patch tokens to forecast the next 3 values. Output format: Patches: [[latest_patch], ..., [oldest_patch]] Prediction: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032). For the System prompt, we included both the patching instructions and dataset-specific information, such as the name of the series. Among the variants of prompts tested, we found reverse patching to perform the best. Further details and comparisions of patching strategies are provided in (Section A; see appendix for Table 5). Neighs is built upon our zero-shot prompt. This method adds closest neighboring series in terms of euclidean distance over all the past windows of data and construct composite prompt by giving all the 5 neighboring prompts as additional context. We outline the structure of the Neighs prompt used for the weather dataset in the Neighs System Prompt box. We specified the number of neighbors that, and gave the model additional instructions. PatchInstruct+Neighs integrates the strengths of both PatchInstruct and Neighs approaches. We combined the two methods using the system prompt in the PatchInstruct+Neighs system prompt box."
        },
        {
            "title": "Neighs System Prompt",
            "content": "You are forecasting assistant that sees time series data. The sequence represents the total regional humidity measured every 10 minutes. You will also be given 5 neighbor time-series similar to the one to forecast. Use it to understand the trends. Output format: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032). You are forecasting assistant that sees time series data. The sequence represents the total regional humidity measured every 10 minutes. You will also be given 5 neighbor time-series similar to the one to forecast. Use it to understand the trends. (1) Split the series into overlapping Task: patches with window size 3 and stride 1. (2) Generate the patches in natural order, then reverse the list so the most recent patch appears first. (3) Use these patch tokens to forecast the next 3 values. Output format: Patches: [[latest_patch], ..., [oldest_patch]] Prediction: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032). In summary, these prompt-based variants allow us to systematically assess the impact of explicit instructions for time series decomposition, patching, and neighbor augmentation within LLM-based forecasting frameworks. The results, presented in Table 4, provide comparative analysis of these prompting strategies."
        },
        {
            "title": "3.3 Evaluation",
            "content": "We evaluate forecasting performance using Mean Squared Error (MSE), Mean Absolute Error (MAE). Runtime Efficiency and Input/Output token usage."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we design experiments to investigate our proposed patch instruct framework."
        },
        {
            "title": "4.1 Datasets",
            "content": "We evaluate our approach on two real-world datasets: Weather and Traffic. Weather captures fast-changing environmental conditions, while Traffic reflects urban flow patterns with spatial and temporal dependencies The Weather dataset is collected from meteorological station at the Max Planck Institute for Biogeochemistry (Jena, Germany). It contains 14 meteorological features, including temperature, humidity, and atmospheric pressure measurements. The high-frequency recordings capture intricate weather dynamics critical for testing short-term forecasting precision. The Traffic dataset consists of sensor network data from Los Angeles, collected between March and June 2012. It records traffic flow rates and congestion patterns across urban arteries. The spatialtemporal correlations in this dataset test the models ability to capture complex topological dependencies in transportation systems. We summarize key statistics in Table 1. This selection provides systematic coverage of (1) different sampling frequencies, (2) variable sequence lengths, and (3) heterogeneous feature interactionsthree critical axes for stress-testing tokenization strategies in temporal learning tasks. The datasets public availability ensures reproducibility, while their domain diversity demonstrates our methods generalizability beyond narrow application contexts. Dataset Features Frequency Time Span Samples Value Range WEATHER TRAFFIC 14 181 10 minutes Hourly 3 years 4 months 157,680 34, 0.518.13 2.570 WEATHER Table 1: Summary of datasets used in our experiments."
        },
        {
            "title": "4.2 Main Results\nFor our experiments, we adopt S2IP-LLM as the\nprimary baseline, a method that aligns time se-\nries embeddings with the semantic space of a pre-\ntrained LLM through a tokenization framework.\nWhile effective, S2IP-LLM suffers from signifi-\ncant computational overhead, requiring extensive\ntraining and inference time due to its fine-tuning of\nLLM components. All methods are evaluated in a\nconsistent zero-shot setting without model retrain-\ning to isolate the impact of prompting strategies.",
            "content": "Using various prompting strategies, we instructed pre-trained LLM to consider time-series patches and utilize them for forecasting without any additional fine-tuning or retraining. Our experiments demonstrate that such patch-based prompting methods can significantly improve forecasting performance across multiple datasets and over shorter horizons. In contrast to models like S2IPLLM, which rely on explicit decomposition, semantic alignment, and parameter tuning, our approach leverages instruction-tuned LLMs. Among all the strategies evaluated the PatchInstruct technique consistently delivered the best results. This suggests that prompting pre-trained LLMs with thoughtfully structured temporal context can match or even surpass models trained from scratch, offering lightweight yet effective alternative for time-series forecasting. Table 3 presents performance comparison between S2IPLLM (the baseline) and our bestperforming Patch Instruct method across multiple time series datasets and forecast horizons. The results clearly indicate that Patch Instruct consistently outperforms the baseline in terms of both MSE and MAE. Finally, Table 3 compares the two methods in terms of input/output token counts and computation time. The analysis reveals that Patch Instruct not only improves forecasting accuracy but also significantly reduces computational overhead. Overall, this comparison highlights the efficiency and effectiveness of the Patch Instruct method. Dataset Horizon S2IP-LLM Zeroshot PatchInstruct MSE MAE MSE MAE MSE MAE 0.0095 0.056 0.0028 0.043 0.0014 0.029 0.077 0.0085 0.072 0.0076 0.067 0.017 0.0238 0.0875 0.0106 0.068 0.0110 0.085 0.0326 0.1051 0.0115 0.085 0.0236 0.113 0.0159 0.094 0.0371 0.1120 0.0277 0.0439 0.1228 0.0204 0.11 0.0101 0.083 0.0904 0.1823 0.1098 0.221 0.0436 0.137 0.1 21.0814 2.4067 43.49 2.76 1.89 24.0935 2.4919 23.47 1.78 29.9573 2.7849 22.38 1.89 29.8382 2.6147 27.50 1.88 36.0289 2.7971 34.27 2.75 42.3193 3.0444 29.66 68.7149 3.8473 296.20 7.72 235.75 5.89 20.05 9.38 6.47 11.15 8.46 25. 3.18 2.53 2.54 2.87 3.20 3.07 1 2 3 4 5 6 12 1 2 3 4 5 6 12 TRAFFIC Table 2: Results comparing our approach to baselines. Dataset Horizon S2IP-LLM Zeroshot PatchInstruct Time (s) IT OT Time (s) IT OT Time (s) IT OT 535.42 518.45 533.73 518.37 537.85 522.43 558.67 50.94 52.88 55.34 51.00 49.96 50.11 52.66 1 7 2 7 3 7 4 7 5 7 7 6 7 12 1 7 2 7 3 7 4 7 5 7 7 6 7 12 1.36 1.06 1.20 1.01 1.14 1.35 1.44 2.59 2.04 1.17 2.13 1.14 1.38 1. 7370 80 7370 120 7370 160 7370 200 7370 240 7370 280 7370 520 7360 80 7360 116 7360 160 7360 200 7360 240 7360 268 7360 520 1.24 1.20 1.01 1.16 1.29 2.05 1.51 1.31 1.05 1.11 1.17 1.12 1.23 1.36 8500 80 8500 120 8500 160 8500 196 8500 216 8500 280 8500 499 7950 86 7950 134 7950 185 7950 223 7950 239 7950 336 7950 1 2 3 4 5 6 12 1 2 3 4 5 6 12 WEATHER TRAFFIC Table 3: Token and Time Comparison for Forecasting."
        },
        {
            "title": "4.3 Cost vs. Performance Analysis",
            "content": "Our instruction-based forecasts deliberately spend more input tokens than the baseline S2IP-LLM. Across the prompt variants, single prediction consumes about 800 - 1000 input tokens. By contrast, S2IP-LLM needs only the horizon-length of output tokens once its patch encoder has been trained. The extra prompt length therefore represents about 100 times increase in in front-loaded cost. Because our method relies on an already-trained LLM and does no task-specific fine-tuning, the endto-end latency of producing forecast collapses from minutes to just seconds. For example, on the Weather dataset at horizon = 1, S2IP-LLM requires 535 s, whereas Reverse Patch returns the prediction in 0.86 (see Table 3). Thus, even after accounting for the larger prompt, our approach is two to three orders of magnitude faster in real-time settings. The additional 800900 input tokens result in substantial improvement in short-range forecasting accuracy. On Weather (H=1), mean squared error (MSE) drops from 1.15 102 to 2.6 104 (a 97.7% reduction), and mean absolute error (MAE) decreases from 6.52 102 to 1.4 102. Similar improvements are observed on Traffic, where the MSE at the same horizon is reduced by 85%, indicating that the gains generalize across domains. Given (i) the low marginal price of LLM tokens relative to GPU training hours, and (ii) the consistent short-horizon error reductions that are operationally most valuable, the accuracy and latency benefits comfortably offset the larger prompt size. Hence trading cheap tokens for immediate, higher-quality forecasts yields more favorable costperformance envelope than the current state of the art, especially when rapid deployment and low engineering overhead are priorities."
        },
        {
            "title": "4.4 Neighbor Results",
            "content": "In order to understand whether incorporating neighboring time-series into our PatchInstruct approach leads to better performance we compare our results for PatchInstruct and Neighs across multiple datasets  (Table 4)  . Both the Neighs and PatchInstruct+Neighs prompting strategies demonstrate clear improvements over the S2IP-LLM baseline across datasets. As seen in Table 4, using Neighs alone often improves performance over PatchInstruct, particularly in the Weather dataset. For example, at horizon 2, Neighs reduces the MSE from 0.0076 (PatchInstruct) to 0.0039 and MAE from 0.067 to 0.051. At horizon 4, Neighs again performs better with an MSE of 0.0172 compared to 0.0236. These gains suggest that incorporating neighboring series can help the model infer more accurate trends by providing contextual information beyond the target sequence itself. However, this is not universally true. In some cases, Neighs and PatchInstruct+Neighs underperform compared to PatchInstruct. For instance, in the Traffic dataset at horizon 5, Neighs shows significant degradation, increasing the MSE from 8.46 (PatchInstruct) to 43.50, and PatchInstruct+Neighs to 36.43. This indicates that when neighbor series are less correlated, they can introduce confusion rather than useful context. these Despite exceptions, the PatchInstruct+Neighs strategy still achieves the best overall performance in many cases aswell. But these results also highlight the importance of carefully selecting relevant neighbors to avoid negative transfer and ensure consistent forecasting improvements. These results underline the strength of combining temporal structuring (via patches) with spatial context (via neighbors), enabling the model to learn more holistic representations and deliver significantly more accurate forecasts than our baseline. Dataset Horizon PatchInstruct Neighs PatchInstruct+Neighs MSE MAE MSE MAE MSE MAE WEATHER TRAFFIC 1 2 3 4 5 6 12 1 2 3 4 5 6 0.0014 0.029 0.0024 0.042 0.0032 0.0076 0.067 0.0039 0.051 0.0056 0.0110 0.085 0.0083 0.065 0.0138 0.0236 0.113 0.0172 0.091 0.0114 0.0159 0.094 0.0105 0.077 0.0124 0.0101 0.083 0.0116 0.084 0.0338 0.0436 0.137 0.0371 0.144 0.0393 2.76 1.89 1.78 1.89 1.88 2.75 20.05 35.15 22.09 9.38 18.85 15.40 6.47 26.67 13.61 11.15 21.41 13.22 8.46 43.50 36.43 14.94 40.42 25.59 235.75 5.89 285.82 7.71 269.68 3.05 2.50 2.74 2.57 3.39 3.35 0.046 0.056 0.087 0.075 0.088 0.108 0.141 2.72 2.19 2.10 2.15 3.25 2.13 6. Table 4: Forecasting Comparison: PatchInstruct vs Neighs vs PatchInstruct+Neighs."
        },
        {
            "title": "5 Analysis",
            "content": "This analysis compares the performance of S2IPLLM (baseline) against our approach across different datasets and forecast horizons. The comparison focuses on error metrics (MSE and MAE) where lower values indicate better performance. 5.1 Performance Overview Across Models The S2IP-LLM baseline consistently shows higher error rates compared to our methods across datasets. Our methods shows remarkable improvements, with percentage reductions in MSE ranging from approximately 13% to 85% depending on the dataset and method. For the Weather dataset, all our methods achieve over 80% MSE improvement. Our method achieve orders-of-magnitude faster runtimes than S2IP-LLM with modest token usage growth, making them highly efficient for inference, especially when balanced with patch or neighbor-based prompts. 5.2 Method-Specific Performance Our approach exhibits distinct strengths across datasets and forecasting horizons. The PatchInstruct framework demonstrates the most balanced performance, delivering substantial improvements on both the Traffic and Weather datasetsachieving up to 83% and 85% improvement over the baseline, respectively. The Neighs variant, which augments prompts with the closest neighboring time series, performs particularly well on the Weather dataset. Meanwhile, the combined PatchInstruct+Neighs strategy outperforms other methods on the Traffic dataset at longer horizons, highlighting the benefit of incorporating both local structure and external context in more challenging settings. These results suggest that method selection can be guided by the characteristics of the dataset and the specific forecasting task, with PatchInstruct offering robust default across most conditions."
        },
        {
            "title": "5.3 Dataset-Specific Analysis",
            "content": "For Weather forecasting, all methods substantially outperform the baseline. Overall MSE values are reduced from 0.0090.0904 (baseline) to as low as 0.00140.043 (our methods). PatchInstruct deliver substantial improvements, reducing MSE from 2168 (baseline) to 6.47 20.05. However, Neighs and PatchInstruct+Neighs perform poorly in this scenario. Across both approaches, forecast accuracy generally decreases as the horizon increases, but this pattern varies by dataset and method. For the Weather dataset, the performance degradation with longer horizons is less pronounced, especially for PatchInstruct+Neighs in multivariate settings. 5.4 Key Insights and Implications The optimal forecasting method varies notably depending on the characteristics of the dataset and the forecasting horizon. For the Weather dataset, the PatchInstruct and Neighs strategy yields the most accurate results, effectively capturing the contextual signals from related series. In contrast, for the Traffic dataset, our main approach PatchInstruct performs best, suggesting that more complex augmentation may not always be beneficial in settings with high variability or less correlated neighbors."
        },
        {
            "title": "6 Conclusion",
            "content": "The analysis demonstrates that prompt-based methods generally outperform the S2IP-LLM baseline across most forecasting scenarios, especially at shorter horizons. The optimal method depends significantly on the specific dataset and forecast horizon, with PatchInstruct dominating in the majority of cases. This suggests that while promptbased strategies offer lightweight and effective alternative for time series forecasting."
        },
        {
            "title": "7 Limitations",
            "content": "While our method achieves competitive accuracy compared to the S2IP-LLM baseline, several limitations warrant consideration. First, the evaluation is limited to two benchmark datasets, which, though diverse, may not fully represent the diversity of real-world time series scenarios, such as irregular sampling or high-frequency patterns. Second, the framework remains heavily contingent upon carefully engineered prompts, introducing laborintensive design process that risks overfitting to specific tasks or datasets without systematic adaptation strategies. Future research should prioritize expanding dataset coverage, and developing adaptive prompting mechanisms."
        },
        {
            "title": "References",
            "content": "Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, et al. 2024. Chronos: Learning the language of time series. arXiv preprint arXiv:2403.07815. Defu Cao, Furong Jia, Sercan Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. 2024a. Tempo: Prompt-based generative pre-trained transformer for time series forecasting. Defu Cao, Wen Ye, Yizhou Zhang, and Yan Liu. 2024b. Timedit: General-purpose diffusion transformers for time series foundation model. arXiv preprint arXiv:2409.02322. Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen. 2024. Llm4ts: Aligning pre-trained llms as data-efficient time-series forecasters. Hao Xue and Flora D. Salim. 2023. Promptcast: new prompt-based learning paradigm for time series forecasting. Zerveas, Jayaraman, Patel, Bhamidipaty, and Eickhoff. 2020. transformer-based framework for multivariate time series representation learning. arxiv. arXiv preprint arXiv:2010.02803. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Wilson. 2024. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 36. Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. 2023. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36:4332243355. Yuxuan Liang, Yue Wu, Sheng Wang, Xiaoyi Zhou, Wang Yang, Rong Xu, Wen Ye, Weizhi Lin, Zhiguo He, Zongyan Li, et al. 2024. Foundation models for time series analysis: tutorial and survey. arXiv preprint arXiv:2403.14735. Xinyu Zhou, Zhengyuan Ding, Shuo Ren, Yutao Chen, Xinhui Huang, Jianhao Shi, and Wayne Xin Zhao. 2024. Ditto: survey on fine-grained alignments of large language models. arXiv preprint arXiv:2411.05793. Ruotong Liao, Xu Jia, Yangzhe Li, Yunpu Ma, and Volker Tresp. 2024. Gentkg: Generative forecasting on temporal knowledge graph with large language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 43034317. John Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, Budak Arpinar, and Ninghao Liu. 2024. survey of deep learning and foundation models for time series forecasting. arXiv preprint arXiv:2401.13912. Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. 2022. time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730. Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, and Dongjin Song. 2024. S2ip-llm: Semantic space informed prompt learning with llm for time series forecasting. Kashif Rasul, Arjun Ashok, Andrew Robert Williams, Hena Ghonia, Rishika Bhagwatkar, Arian Khorasani, Mohammad Javad Darvishi Bayazi, George Adamopoulos, Roland Riachi, Nadhir Hassen, Marin Biloš, Sahil Garg, Anderson Schneider, Nicolas Chapados, Alexandre Drouin, Valentina Zantedeschi, Yuriy Nevmyvaka, and Irina Rish. 2023. Lag-Llama: Towards foundation models for probabilistic time series forecasting. arXiv preprint arXiv:2310.08278. Yilong Ren, Yue Chen, Shuai Liu, Boyue Wang, Haiyang Yu, and Zhiyong Cui. 2024. Tpllm: traffic prediction framework based on pretrained large language models. Xiaofeng Shao, Soumya Ghosh, and Suhasini Subba Rao. 2020. Time-series analysis and its applications in scientific disciplines. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 378(2174):20200209. Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, and Ming Jin. 2024. Time-moe: Billion-scale time series foundation models with mixture of experts. arXiv preprint arXiv:2409.16040."
        },
        {
            "title": "Across different datasets",
            "content": "Table 5 represents evaluating five prompting strategiesBasic, Non-Overlapping, STR Decompose, Reverse Patches, and Meta Patchesacross the Weather and Traffic datasets, for horizons of 1, 3, and 6. While STR Decompose occasionally shows the lowest error for short-term predictions, Reverse Patch Instruct consistently delivers strong performance across all horizons and datasets. Notably, in long-range forecasts (H=6), where prediction becomes more challenging, Reverse Patch Instruct achieves the lowest or near-lowest MAE and MSE in both datasets, highlighting its stability and generalizability. Although it incurs slightly higher inference time than the most lightweight methods, the trade-off is minimal when weighed against the accuracy benefits. Overall, the results suggest that Reverse Patch Instruct is the most effective and reliable strategy, outperforming other variants in terms of both robustness and predictive accuracy. A.1 Basic PatchInstruct The Basic PatchInstruct method employs overlapping sliding windows (size=3, stride=1) to capture local temporal patterns, followed by strategic sequence reversal to prioritize recent context. Unlike conventional approaches that process time series chronologically, this method reverses the generated patches such that the most recent window [xt-2, xt-1, xt] appears first in the token sequence. This architectural innovation forces the model to attend to immediate temporal patterns before historical context, combining the local sensitivity of patchbased methods (Nie et al., 2022) with explicit recency prioritization. The approach demonstrates Table 5: Ablation Study: Comparing Variants of Patch-Based Prompting Strategies Across Datasets. Dataset Horizon Basic Non-Overlapping STR Decompose Reverse Patches Meta Patches WEATHER TRAFFIC 1 3 6 1 3 6 MAE MSE Time MAE MSE Time MAE MSE Time MAE MSE Time MAE MSE Time 0.014 0.0003 0.66 0.012 0.0002 1.6151 0.009 0.0001 1.2553 0.015 0.0005 1.2290 0.020 0.0007 0.7471 0.050 0.0045 0.989 0.055 0.0064 1.3580 0.045 0.0030 1.0737 0.053 0.0045 0.9732 0.067 0.0073 0.9269 0.078 0.0116 1.146 0.063 0.0079 3.9016 0.100 0.0230 1.1523 0.056 0.0070 1.5120 0.060 0.0074 0.9760 1.43 1.07 1.14 6.27 3.36 4. 0.699 1.35 0.940 1.47 0.910 1.14 5.60 7.17 3.60 1.3404 1.34 1.1910 0.99 1.3010 1.79 4.17 2.53 8.71 1.2677 1.15 1.1801 0.89 1.5667 0.89 3.69 1.83 2. 1.1221 1.32 1.2018 0.94 1.1137 1.03 5.35 2.38 2.95 1.3846 1.1584 1.1650 full sequence reversal of overlapping windows. The architectural innovation forces models to process the final patch [x94, x95, x96] first, implementing \"recency-first\" attention mechanism. This structural bias proves particularly effective for 10-minute interval forecasting where immediate consumption patterns (last 30 minutes) contain stronger signals than older data. The approach maintains patch-based efficiency while adding temporal prioritization through simple sequence manipulation. A.5 Meta Tokens Patches This advanced variant enriches temporal representation through explicit time slot encoding. Each value vt pairs with its absolute position in the daily cycle (0-143 slots) as (vt; slotid), creating hybrid tokens like (8.35;63). These metatokens are windowed into overlapping patches: [(v1;slot1), (v2;slot2), (v3;slot3)] [(v2;slot2), (v3;slot3), (v4;slot4)] . . . [(v94;slot94),(v95;slot95),(v96;slot96)] enabling joint learning of consumption patterns and their absolute temporal positions. The fixed slot indices provide crucial circadian context, helping disambiguate similar patterns occurring at different times (e.g., morning vs. evening peaks). This method adapts positional encoding strategies from language models to time series, grounding predictions in both value sequences and absolute time references. particular efficacy in high-frequency electricity demand forecasting where near-term consumption patterns strongly influence subsequent values. A.2 Non-Overlapping PatchInstruct This variant utilizes non-overlapping windows where both window size and stride equal the prediction horizon (typically 3). The method partitions the series into discrete blocks like [8.35, 8.36, 8.32] followed by [8.45, 8.35, 8.25], eliminating redundant data coverage while maintaining temporal progression. The design trades off some contextual granularity for computational efficiency, making it suitable for scenarios with pronounced periodic patterns. By processing patches in natural order without sequence reversal, the method preserves strict temporal causality, particularly effective when historical seasonal trends dominate the forecasting signal. A.3 STR Decompose PatchInstruct Integrating seasonal-trend-residual decomposition, this method first separates raw values into trend (trend_t) and residual (residual_t = series_t - trend_t) components. Each time step becomes composite token [Tt, Rt], enabling joint modeling of long-term trajectories and short-term fluctuations. These dual-aspect tokens are organized into overlapping windows: [[T1,R1], [T2,R2], [T3,R3]] preserving both local context and decomposition characteristics. The architecture explicitly captures multi-scale temporal dynamics, particularly beneficial for electricity demand series containing both gradual load changes and sudden consumption spikes. A.4 Reverse Ordered Patches Building on basic patch inversion, this method systematically prioritizes recent context through Basic PatchInstruct STR Decompose PatchInstruct You are forecasting assistant that receives STL-decomposed tokens. Input: - \"series\": 96 raw numbers (Humidity demand) - \"horizon\" : 3 (fixed) Task: 1. Decompose the series into trendt and residualt = seriest trendt 2. For each time-step create pair token: (trendt , residualt). 3. Split the 96 composite tokens into overlapping patches (window = 3, stride = 1). 4. Use those patches to forecast the next 3 raw values. Output exactly [[T1,R1], [T2,R2], [T3,R3]] [[T2,R2], [T3,R3], [T4,R4]] . . . [[T94,R94], [T95,R95], [T96,R96]] Prediction: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032). You are forecasting assistant that sees time series data. The sequence represents the total regional humidity measured every 10 minutes. Task: (1) Split the series into overlapping patches with window size 3 and stride 1. (2) Generate the patches in natural order, then reverse the list so the most recent patch appears first. (3) Use these patch tokens to forecast the next 3 values. Output format: Patches: [[latest_patch], ..., [oldest_patch]] Prediction: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032). Non-Overlapping PatchInstruct Tokenize the given time-series data into non-overlapping patches where patch is contiguous subsequence of the time-series. Ensure to use fixed window size equal to the Horizon size (e.g., 3) and the stride is equal to the window size. This means that each patch starts exactly where the previous one ends and there will be no overlap. Output patches as list, in order, using square brackets. Each patch becomes token used to represent local temporal patterns. Use the sequence of patches to predict the next value(s). Below are few shot examples of non-overlapping patching: Time series data: 8.35, 8.36, 8.32, 8.45, 8.35, 8.25, 8.20, 8.09, 8.13, 8.00, 7.94, 7.86 Patches generated based on Horizon (3), stride = 3: [8.35, 8.36, 8.32] [8.45, 8.35, 8.25] [8.20, 8.09, 8.13] [8.00, 7.94, 7.86] Prediction: [7.89, 7.97, 7.94] Reverse Ordered Patches PatchInstruct Meta tokens Patches PatchInstruct You are forecasting assistant that sees time series data. The sequence represents the total regional humidity measured every 10 minutes. Input: - \"series\": 96 raw numbers (Humidity, 10-min cadence) - \"horizon\" : 3 (fixed) Task: 1. Split the series into overlapping patches (window = 3, stride = 1). 2. Generate them in natural order, then reverse the list so the most recent patch appears first. 3. Use those patch tokens to forecast the next 3 normalised values. Output format: Patches: [[latest_patch], ... , [oldest_patch]] Prediction: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032). You are forecasting assistant that sees time series data, where each datapoint is paired with its 10-minute slot index within the day. The sequence represents the total regional humidity measured every 10 minutes. Input: - \"series\": 96 raw numbers (Humidity, 10-min cadence) - \"horizon\" : 3 (fixed) Time-slot index - day is divided into 144 slots (0 143). - slot = floor((60*HH + MM)/10). Example: 10:30 63 (because 10*60 + 30 = 630; 630/10 = 63). Token format (value ; slotid) slotid corresponds to the measurements clock time Task: 1. Convert the 96-point series into 96 twoelement tokens as above. 2. Split the token stream into overlapping patches (window = 3, stride = 1). 3. Use those patches to forecast the next 3 raw demand values. Output format: [(v1;slot1), (v2;slot2), (v3;slot3)] [(v2;slot2), (v3;slot3), (v4;slot4)] . . . [(v94;slot94), (v96;slot96)] (v95;slot95), Prediction: [y1, y2, y3] No headings or extra words. Decimals 4 places; keep leading zeros (e.g., 0.8032)."
        }
    ],
    "affiliations": [
        "Adobe",
        "Dolby Labs",
        "Intel",
        "University of Massachusetts Amherst",
        "University of Oregon"
    ]
}