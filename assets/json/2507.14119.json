{
    "paper_title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining",
    "authors": [
        "Maksim Kuprashevich",
        "Grigorii Alekseenko",
        "Irina Tolstykh",
        "Georgii Fedorov",
        "Bulat Suleimanov",
        "Vladimir Dokholyan",
        "Aleksandr Gordeev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets: original image, instruction, edited image. Yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated edit-quality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines high-fidelity triplets across domains, resolutions, instruction complexities, and styles. Built on public generative models and running without human intervention, our system uses a task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by approximately 2.2x, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows a new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit: an open dataset of 358k high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an open-source fine-tuned Bagel model, which achieves state-of-the-art metrics in our experiments."
        },
        {
            "title": "Start",
            "content": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, Aleksandr Gordeev Layer Team, SALUTEDEV https://riko0.github.io/No-Humans-Required/ 5 2 0 2 8 1 ] . [ 1 9 1 1 4 1 . 7 0 5 2 : r Figure 1. High-quality samples from our NHR-Edit dataset."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in generative modeling enable image editing assistants that follow natural language instructions without additional user input. Their supervised training requires millions of triplets original image, instruction, edited image, yet mining pixel-accurate examples is hard. Each edit must affect only prompt-specified regions, preserve stylistic coherence, respect physical plausibility, and retain visual appeal. The lack of robust automated editquality metrics hinders reliable automation at scale. We present an automated, modular pipeline that mines highinstruction fidelity triplets across domains, resolutions, complexities, and styles. Built on public generative models and running without human intervention, our system uses task-tuned Gemini validator to score instruction adherence and aesthetics directly, removing any need for segmentation or grounding models. Inversion and compositional bootstrapping enlarge the mined set by 2.2, enabling large-scale high-fidelity training data. By automating the most repetitive annotation steps, the approach allows new scale of training without human labeling effort. To democratize research in this resource-intensive area, we release NHR-Edit, an open dataset of 358 high-quality triplets. In the largest cross-dataset evaluation, it surpasses all public alternatives. We also release Bagel-NHR-Edit, an opensource fine-tuned Bagel model, which achieves state-of-theart metrics in our experiments. 1 1. Introduction In recent years, we have witnessed an unprecedented acceleration in generative modeling, which has facilitated imageediting assistants that follow purely natural language instructions. Creating such expert editors typically involves It begins with foundational prea multi-stage process. training on large, diverse, but often noisy datasets (e.g., [9, 17, 24, 43, 52, 53, 60]). This crucial stage adapts base text-to-image model for instruction-guided editing, teaching it to process an input image, execute wide array of user-specified edits, and, most importantly, preserve the unedited regions across diverse visual domains. Following this, second stage of initial SFT on smaller (from thousands to hundreds of thousands examples), meticulously curated datasets can elevate performance on specific tasks. ObjectDrop [10] and OmniPaint [36] have shown that as few as 2500-3300 high-quality examples can teach model to master task like object removal with remarkable precision. The third stage is continual supervised fine-tuning (SFT) and preference optimization [33, 42], intended to make the model able to handle more complex edits and to iteratively improve its overall quality. This stage, however, presents critical data bottleneck, as it requires scalable method for generating continuous stream of high-fidelity training examples. Scaling this stage remains constrained by reliance on human annotators who must review millions of routine, pixel-level candidate edits to verify instruction fidelity and context preservation. Such reviews are not the best uses of expert attention; human expertise matters most on subtle, ambiguous, or creative cases. Existing large-scale data collection methods, however, have fundamental drawbacks: Complex synthetic pipelines and data poisoning: Methods using cascade of external tools (e.g., for grounding [27], segmentation [25], and inpainting [37]) not only create visual artifacts but can lead to more tricky problem. The model could learn spurious correlation when an imperfect \"remove\" edit with inpainting artifacts is inverted to an \"add\" operation. Instead of understanding the instructions true semantic meaning, it learns to use the artifacts as spatial cue, correctly guessing where to place the object. This effectively poisons the training data. Manual and semi-manual methods: Approaches like 3D rendering [14] lack realism and scalability, while video frame extraction [28] depends on complex and error-prone auxiliary models. The validation bottleneck: the lack of reliable validation metric sensitive enough to detect subtle defects. While the image editing field has recently shifted towards using Multimodal Large Language Models (MLLMs) as evaluators [43, 45, 52], we found that even top-ranked proprietary models like Gemini 2.5 Pro [19] are insufficient for this task. To overcome this, we developed specialized validator by fine-tuning Gemini-2.0-flash [20] on human scoring data (3.2. We posit that the latent potential of model that has undergone initial SFT remains under-exploited. By utilizing the models new abilities and sensitivity to stochastic initialisation, the editor itself can be transformed into virtually unlimited source of high-quality synthetic data. To realize this potential, we introduce novel, end-to-end tripletmining pipeline. Our pipeline begins with source image, which can be real-world photo or one we generate synthetically. The framework performs multiple editing iterations on the image for each instruction, producing diverse set of candidate outputs.. These candidates first go through coarse pre-filtering step to eliminate obvious failures. The ones that pass are then carefully judged by our fine-tuned Gemini-2.0-flash validator [20]. For each instruction, the validator picks only the single best edit, and only if it meets our strict quality standards (detailed in Algorithm 3). This two-stage filtering process is crucial for ensuring the final datasets quality. This self-contained framework unlocks several unique capabilities for continual learning: Direct complexity measurement for curricula: The difficulty of any instruction for the current model can be quantified by counting the attempts required for successful edit. This provides direct, empirical signal to implement an easy-to-hard learning curriculum. Targeted weakness correction: Rare successes can be mined. Even if model succeeds at complex task only once in while, it can be run repeatedly to harvest targeted dataset that fixes that specific weakness. Compositional edit synthesis: Complex training data can be created by combining multiple instructions of any type: simple or compositional, forward or inverted. For example, it is possible to form single, multi-part instruction that executes two additions, one deletion, and global style change in single pass. Flexible input sourcing: The framework can operate on both real-world user images and synthetically generated inputs, each offering distinct advantages. Real images provide authentic user scenarios with natural lighting conditions and artifacts that the base editing model must learn to handle while preserving unedited regions. Synthetic images enable exploration of the long-tail distribution, including scenarios that are impossible or impractical to photograph (e.g., corgi in spacesuit standing on rocket flying through space), ensuring comprehensive coverage of potential editing requests. Unparalleled simplicity and flexibility: The framework is model-agnostic and requires no external specialist models for segmentation, depth estimation, or grounding. 2 To demonstrate effectiveness, NOHUMANSREQUIRED DATASET (NHR-Edit) is released as publicly available dataset of 358k rigorously validated triplets. Building on this data, we release BAGEL-NHR-EDIT, LoRA-tuned BAGEL[15] variant trained on NHR-Edit that surpasses the base model on two benchmarks, indicating that targeted fine-tuning with NHR-Edit improves instruction-guided image editing capability. The primary contribution is an endto-end pipeline that serves as powerful engine for advancing research in self-improving generative models [12, 59]. 2. Related Work Our research builds upon two main pillars of generative modeling: methodologies for creating instruction-based editing data and the paradigm of model self-improvement through preference optimization. 2.1. Methodologies for Editing Data Generation Creating high-quality editing triplets is foundational challenge. Existing large-scale approaches can be categorized by their data generation strategy, each presenting unique set of trade-offs in complexity, fidelity, and scope. Pipelines on Real-World Data. The most common strategy involves cascade of models to edit real images. Methods like AnyEdit [53] and ImgEdit [52] typically use pipeline for open-vocabulary object detection (e.g., GroundingDINO [27], YOLO-World [13]), segmentation (SAM [18, 25]), and inpainting (e.g., [5, 37]). While grounded in realistic photos, each stage in this cascade can introduce errors that propagate downstream, and global edits using methods like ControlNet [56] can struggle to preserve fine details. Sourcing pairs from video, as in Step1XEdit [28], introduces further complexity: their pipeline uses BiRefNet [61] and RAFT [39] filter out dynamic backgrounds and estimate foreground motion, which is then described by an MLLM to generate the edit instruction. Furthermore, this approach can suffer from dataset bias; for instance, ImgEdit builds on LAION Aesthetic [35], which may not reflect typical user photos. Fully Synthetic Generation. To gain more control, other methods generate data synthetically. AURORA [14] uses 3D rendering, which offers high control but is laborintensive, struggles with photorealism, and is ill-suited for global style edits. UltraEdit [60] uses diffusion inversion to derive noise latent from real photo, but the inversion process itself can introduce artifacts. HQ-Edit [24] generates diptychs with DALL-E 3 [3], which tends to alter fine-grained details. Seed-Data-Edit [17] also employs synthetic pipeline using techniques like Plug-and-Play [41], which, while flexible, generates data that may not align with real-world distributions. Specialist Models. distinct approach, taken by OmniEdit [43], is to train separate, specialized diffusion models for each editing task and integrate them into the pipelines similar to described above. For example, their inpainting specialist uses mask from GroundingDINO [27] + SAM [25] detection and segmentation steps, while their attribute modification pipeline consists of GroundingDINO + SAM followed by Prompt-to-Prompt [21]. While this approach can ensure higher quality for simple, predefined operations, it inherits the same cascade complexity and error propagation issues as other pipeline-based methods. Furthermore, it cannot generate complex, compositional instructions that combine multiple editing operations. In summary, existing methods navigate complex landscape of trade-offs. Our work offers different approach: by using the editor model itself as the data source, our simple, general-purpose framework bypasses the need for external specialist models for segmentation, inpainting, 3D assets, or complex video analysis. 2.2. The Metric Gap in Image Editing key challenge in our field is evaluation. Traditional metrics are doubly flawed for this task: they have been shown to correlate poorly with human preferences for editing, and furthermore, their reliance on ground-truth references makes them unsuitable for our generative framework. This includes image-based metrics that require target image, such as LPIPS [57], DINO (as proposed in [34], using [11]), and CLIP-I (as proposed in [16], using [32]), as well as textbased metrics like CLIPScore [22], which requires target caption. In Image Quality Assessment (IQA), models like VisualQuality-R1 [46] and Q-Align [44] teach MLLMs to assess image quality. In Text-to-Image (T2I) generation, diverse toolkit has emerged, including MLLMs like VisionReward [50], BLIP-based model like ImageReward [49], and CLIP-based models like HPSv2 [47] and PickScore [26]. Even the Text-to-Video (T2V) domain has specialized metrics like VQ-Insight [58]. The use of MLLMs as reference-free evaluators for image editing was pioneered by VIEScore [45], which showed that GPT-4o judgments correlate with human preferences better than CLIP-I, DINO and LPIPS. Subsequent work built on this approach: OmniEdit [43] distilled these judgments into smaller model, and ImgEdit [52] fine-tuned Qwen-2.5-VL [31] for the same purpose. However, the precision required to curate data for high-quality SFT demands higher standard. Our findings indicate that even top-performing foundation models are insufficient out-of-the-box. For instance, Gemini 2.5 Pro [19], despite leading public vision leaderboards [29], fails to reliably detect subtle yet critical editing flaws. We therefore developed specialized validator by fine-tuning Gemini-2.0flash [20] on human preference data to achieve the required sensitivity. 2.3. Self-improvement and Iterative Learning The concept of model generating its own training data to refine its capabilities has proven highly effective. In NLP, this was demonstrated through methods like rejection sampling in Llama 2 [40] and preference optimization algorithms like IPO [33]. More recently, this paradigm has been extended to generative computer vision, with self-play finetuning for text-to-image models [2, 54] and preference optimization for text-to-video [51]. Our framework serves as direct, automated engine for applying these state-of-the-art preference alignment techniques to the more complex domain of image editing. Modern algorithms like DPO [42] and KTO [4] depend on scalable source of preferencelabeled data, which our pipeline provides automatically: For KTO, which requires labeled \"good\" and \"bad\" examples, our frameworks output can be used directly. The pointwise scores from our validator provide clear signal, where edits above certain quality threshold are labeled as \"successes\" and those below are labeled as \"failures\". For DPO, which requires preference pairs, our frameworks \"one-to-many\" generation process is natural fit. For any given source image and instruction, we can sample two edited candidates from the multiple attempts. The one with the higher validator score is designated the \"winner\", and the one with the lower score becomes the \"loser\", creating correctly formatted preference pair yw, yl. By solving the data generation and labeling bottleneck, our work makes it practical to explore these powerful selfimprovement techniques for the nuanced task of instructionbased image editing. 3. Methodology This section details the autonomous triplet-mining pipeline. The framework is composed of four primary modules: (i) prompt engineering module that generates mutually consistent text-to-image (T2I) and image-to-image (I2I) instructions; (ii) high-fidelity T2I generator; (iii) an instructionguided image editor; and (iv) multi-stage validation stack to ensure triplet quality. 3.1. Automated Mining Pipeline Figure 6 and Algorithm 3 provide schematic overview of the pipeline. The process begins with set of initial constraints or descriptions (Algorithm 1), such as topic, domain, style, edit category, or any other conditions. These conditions are supplied manually in this work, but an adaptive agent designed to guide an iterative self-improvement loop can also be used. The prompt engineering module uses these constraints to produce T2I prompt (pt2i) and corresponding set of coherent edit instructions ({pe}k), as demonstrated in the example in Listing 1. T2I prompt \"prompt\": \"A living room with large window: small cactus on the windowsill, half-eaten bowl of cereal on the coffee table, remote control, crocheted blanket, and dog toy on the rug.\", I2I prompts for editing \"edits\": [ \"Get rid of that cactus.\", \"Remove the cereal bowl.\", \"No remote control, thanks.\", \"Lose the crocheted blanket.\", \"Eliminate the dog toy.\", \"Remove the cactus, cereal, remote, blanket, and toy\" ] Listing 1. corresponding edit instructions. Example of generated T2I prompt and its For each T2I prompt, the pipeline generates candidate source images (I0) using different random seeds (Algorithm 2). Each source image undergoes edit attempts for every instruction pe. This process yields large pool of candidate triplets I0, pe, Ie. These candidates are subjected to coarse pre-filtering step before final validation (see 3.2). In the final stage, for each unique source image and edit prompt pair I0, pe, the highest-quality edited image is selected based on scoring prompt (see Algorithm 3). This image is added to the final dataset only if its scores exceed predefined quality thresholds. Algorithm 1 SAMPLESPROMPTSDESIGN Input: Task description PA.1 Output: Set = (cid:8)(pt2i, {pe}k)(cid:9) 1: OpenAI o3(cid:0)PA.1 2: return (cid:1) 3.2. Validation Framework key challenge in automated triplet mining is the robust validation of edited images. proficient validator must therefore combine strong visual reasoning with specialization for the editing task. To meet this requirement, two-stage validation process was designed. First, pre-filter using Qwen-VL 72B1 discards obvious failures. This reduces the number of calls to the more powerful and computationally expensive final validator, but it is acknowledged that an open-source level model cannot filter out all the noise in such broad 1https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct 4 Algorithm 2 TripletMining Input: T2I prompt pt2i, edits {pe}k, parameters N, , global GPU-hour budget Budget Output: Candidate pool 1: , Jobs 2: for 1 to do 3: 4: 5: 6: seedi Random(i) I0 FLUX.1-schnell(pt2i, seedi) if not Qwen7B continue (cid:0)I0, PA.5 (cid:1) then for all pe {pe}k do for 1 to do 7: 8: Jobs Jobs {(I0, pe, Random(j))} 9: 10: while Jobs = and GPU_hours < Budget do sample (I0, pe, s) Uniform(Jobs) 11: Jobs Jobs {(I0, pe, s)} 12: Ie I2I Dit (internal)(I0, pe, s) 13: (saes, sadh) Qwen72B if saes Taes and sadh Tadh then (cid:0)I0, pe, Ie, PA.2 (cid:1) checkp Qwen72B(I0, pe, Ie, PA.3, PA.4) checkl LowLevelCheck(I0, Ie) if checkp and checkl then {I0, pe, Ie} return 14: 15: 16: 17: 18: 19: Algorithm 3 Autonomous Triplet-Mining Pipeline Input: Task description in PA.1, parameters N, , 3.5 Taes, Tadh, Tinv,aes, Tinv,adh Output: Final dataset 1: , Pool 2: SAMPLESPROMPTSDESIGN(PA.1) 3: for all (pt2i, {pe}k) do 4: 5: for all distinct I0, pe in Pool do {Ie I0, pe, Ie Pool} 6: saes(Ie), sadh(Ie) Gemini(cid:0)I0, pe, Ie, PA.2 7: Pool PoolTRIPLETMINING(pt2i, {pe}k, N, ) every Ie 8: 9: 10: {Ie saes Taes sadh Tadh} if = then (cid:112)saes(Ie) sadh(Ie) arg max IeS {I0, pe, 11: 12: DAPPLYINVERSIONS(D) 13: DAPPLYBOOTSTRAPS(D) 14: return } (cid:1) for 3.6 3.6 and generic task. The second stage employs specialized Gemini 2.0 Flash model, fine-tuned on curated corpus of instruction-editing examples, to assign final scores for aesthetic quality and instruction adherence. Low-level check. The absolute-difference image = Ie I0 is thresholded (>40) and analysed with 5 ConnectedComponents (OpenCV [8], Spaghetti backend [7]) using 4-connectivity and 32-bit labels; triplet is discarded if the largest connected component covers <0.5 % of all pixels flagged as changed. This purely heuristic, optional filter empirically outperforms raw image-difference threshold, removing silent failures where the output appears plausible yet ignores the instruction, though about 3% of such cases still bypass the soft pre-filter validation  (Table 3)  . 3.3. Gemini Validator Modern editing pipelines often evaluate generated edits with large proprietary visionlanguage models such as GPT-4o [24, 43, 45]. While convenient, these generalpurpose assessors are not optimised for fine-grained pixellevel changes (see Fig. 5). To obtain reliable quality estimates, we fine-tune Gemini-2.0-flash [38] model is fine-tuned on dedicated human-annotated corpus. This corpus was meticulously constructed to cover wide spectrum of edit qualities. The triplets were generated using combination of in-house trained DiT editor, and leading proprietary models, including Grok [48] and Gemini [38], operating on both real-world photographs and synthetically generated images. This diverse sourcing strategy ensured that the assessor was trained on broad distribution of potential successes and failures, preventing overfitting to the idiosyncrasies of single generator. Following HQ-Edit [24], OmniEdit [43] and AnyEdit [53], each edited image is rated on two five-point scales: (i) Instruction score, measuring semantic compliance with the textual edit; (ii) Aesthetics score, reflecting overall visual appeal. The collected set contains 2 998 training and 827 validation examples; every example is judged by two to four independent raters. Inter-rater reliability, estimated as the mean pair-wise Spearman correlation, equals ρ = 0.410.09 for Aesthetics and ρ = 0.640.05 for Instruction, corresponding to moderate and substantial agreement in the LandisKoch nomenclature. The higher consistency on the instruction axis is expected semantic correctness is less subjective than perceived beauty while the aesthetic agreement is on par with other public datasets, where visual preference typically shows larger variance. To aggregate scores, each rating is first normalized by subtracting the annotators mean deviation from the global average, which mitigates individual rater bias. The final score for each triplet was then computed as the mean of these normalized ratings. This de-biasing process consists of two steps. First, the bias bj for each rater is calculated as the difference between their personal average score and the average of the Table 1. Quality metrics of the Assessor model on validation data Model Instr. MAE Instr. ρ Aesth. MAE Aesth. ρ Qwen 2.5 72B Gemini-2.5-pro Gemini-2.0-flash Gemini-2.0-flash (finetune) 0.961 0.869 1.241 0.503 0.551 0.609 0.359 0.815 0.839 0.915 1.063 0. 0.361 0.523 0.245 0.631 mean scores for the exact set of triplets they rated: bj = 1 Nj (cid:88) si,j 1 Nj (cid:88) iNj (cid:32) 1 Ri (cid:88) kRi (cid:33) si,k iNj (cid:123)(cid:122) Rater js mean score (cid:124) (cid:125) (cid:124) (cid:123)(cid:122) Mean score of triplets rated by (cid:125) (1) where Nj is the set of triplets rated by rater j, and Ri is the set of all raters for triplet i."
        },
        {
            "title": "The final score Si for a triplet is then the mean of the",
            "content": "bias-corrected scores from each rater: Si = 1 Ri (cid:88) jRi (si,j bj) (2) Using this annotated validation set, we benchmarked our task-specific, fine-tuned Gemini 2.0-flash model its original version (Gemini against several baselines: 2.0-flash), the larger Gemini 2.5-pro [38], and Qwen 2.5 72B (our pipelines pre-validator, see Section 3.2). Table 1 compares the mean absolute error (MAE) and Spearman ρ on the validation set. Vanilla checkpoints suffer from noticeable calibration error, whereas finetuning halves the MAE and boosts rank correlation on the instruction axis from 0.36 to 0.82 outperforming even the larger 2.5-pro model. Notably, the fine-tuned model provides these high-quality scores directly, without costly chain-of-thought reasoning step, confirming that specialized assessor is more efficient paradigm for large-scale automated filtering. To further validate our assessors robustness, we benchmarked it against the publicly available ImgEdit validator [52] on per-category basis. As shown in Table 2 the Spearman correlation between each assessors scores and humanannotated ground-truth ratings across various edit types. Our fine-tuned Gemini validator demonstrates significantly higher correlation across all categories, with particularly large gains in complex operations like Replace (+0.58ρ), Action (+0.25ρ), and Compose (+0.36ρ). This confirms the superior generalization capability of our specialized assessor, highlighting its reliability for diverse range of editing tasks. 3.4. Image Editing Backbone For our data mining framework it is important to have an instruction guided image-to-image (I2I) model that accepts source image I0 and natural-language edit prompt pe, and returns an edited image ˆIe. Throughout the paper, we instantiate this role with an internal diffusion-based editor that we previously trained ourselves. The exact architecture and weights are proprietary, but in terms of the current method, no component of the pipeline depends on the models internals. The editing model is invoked as black-box function, meaning our framework interacts with it solely through its defined inputs and outputs. This design choice ensures that no other component of our pipeline is dependent on the models internal workings, allowing for easy swapping with alternative I2I models, whether they are open-source, commercially available, or different in-house solutions, as long as they adhere to the same input/output specifications. All quality control and validation are performed by an external validation stack, as described previously, further reinforcing this modular approach. 3.5. Implementation Details Component specification. Our pipeline is fully modular; each block can be replaced by any compatible alternative. Unless otherwise noted, we use the following defaults: Prompt engineer. We query the reasoning-centric OpenAI o3 model [30] with the template in A.1 to jointly emit text-to-image (T2I) prompt and set of logically consistent edit instructions. T2I generator. Source images are synthesised with FLUX.1-schnell [6] at random resolution (long side [860, 2200] px; aspect ratio bounded by 1:6 AR 6:1) using 4 DDIM steps. Plausibility gate. We retain only sample seeds whose captions pass plausibility check by Qwen2.5-VL-7B [31] using A.5. Instruction-guided editor. By default we employ our internal I2I DiT model with 18-28 diffusion steps. Soft pre-validation filter. Candidate edits first pass coarse screen with Qwen2.5-VL-72B [31] using A.2, A.3, A.4. Hard validation filter. The fine-tuned Gemini validator (Sec. 3.2) runs at temperature 0.0 with A.2. All HuggingFace transformers default configuration with temperature 106. Qwen-VL calls use the Configuration. Optimal counts for the T2I seeds (N ) and the edit retries (M ) are determined by prompt difficulty, instruction complexity, and the specific models in the loop. smaller is preferable because new prompt injects fresh diversity, whereas additional random seeds for the same prompt yield diminishing returns. Harder samples benefit from larger , which trades additional compute for higher chance of success. Here, =10 and =5 are selected as reasonable compromise to generate the dataset. 6 Table 2. Per-category Spearman correlation (ρ) comparing our Gemini validator to the ImgEdit assessor [52] against unified human ground-truth score. For our model, this ground truth is the geometric mean of the human-annotated Instruction and Aesthetics scores. Score aggregation for the ImgEdit-Judge assessor follows the method described in [52]. Model Overall Remove Replace Style Adjust Background Add Extract Action Compose Gemini-2.0-flash (finetune) ImgEdit-Judge 0.79 0.41 0.75 0.46 0.89 0.31 0.55 0.30 0.79 0. 0.70 0.53 0.72 0.59 0.38 0.16 0.83 0.58 0.43 0.07 No dedicated ablation is presented, since the expected utility of increasing either parameter is monotonic and saturates quickly, making the choice mainly budget question. Validation thresholds remain fixed at Taes = Tadh = 4.7. Budget-aware random scheduler. Up to seedinstruction pairs are enumerated, filtered by the Qwen-7B plausibility test, placed in queue, and drawn uniformly without replacement until the global GPU-hour budget is exhausted. In our case, the queue almost never runs out; realized compute, dataset yield, and quality are dictated by the budget, not by the nominal (N, ) values, and larger values simply leave more jobs unsampled. In future iterations, may be set per instruction category, or retries may continue until the first pre-filter success, removing as fixed hyperparameter. The same queue can support adaptive sampling methods, such as giving higher priority to instruction categories where the model performs worse. 3.6. Data Augmentation The dataset is further refined and expanded through several post-processing and augmentation strategies. Semantic Inversion. Obviously, any edit operation can be inverted. The only requirement is to rewrite the original edit instruction into its logical inverse. This is done with Gemini 2.5 Flash and Prompt A.6. Crucially, because the model has access to the original T2I prompt, the process preserves positional and descriptive details to create high-quality learning signal. For the example in Listing 1, the inverse of the composite deletion instruction is not simply Add cactus, bowl, remote control, crocheted blanket, and dog toy but rather the fully specified prompt: Add small cactus on the windowsill, half-eaten bowl of cereal on the coffee table, remote control, crocheted blanket, and dog toy on the rug. novel compositional triplet Ie1, Figure 2). e2, Ie2 (demonstrated in Backward consistency filter. The semantic inversion process enables an optional backward consistency filter, which is particularly useful for complex T2I prompts where the generation model is prone to error. For instance, the T2I model may occasionally fail to generate requested content from complex prompt, producing semantically invalid scene (e.g., the prompt specifies cat on the sofa, but the image contains no cat), and the subsequent T2I validator may also fail to detect this mistake. In given example, forward edit, such as remove the cat could then receive high score because the hard filter model interprets the instruction as correctly performed and, by design, is not trained to judge the validity of the instruction itself. To filter out such cases, every inverse instruction is re-scored after augmentation. If the backward edit (e.g., add the cat on the sofa) receives low score, both the inversion and its forward counterpart are removed. This filter is not crucial for the frameworks performance. Its necessity depends heavily on the T2I model, the validator, and the complexity of the prompts, and it requires re-running the data through the hard filter. Nevertheless, in scenarios where it is considered necessary, it serves as an additional layer of quality assurance. Figure 2. Solid arrows represent forward instructions, and dashed arrows represent their semantic inversions. Instructions for compositional triplets are aggregated from both forward instructions and inversions. Bootstrap Composition. Since each source image I0 can be successfully edited into multiple distinct images (Ie1, Ie2, etc.), new triplets can be constructed from these existing results. Given two successful edits, new instruction e2 can be formulated to transform Ie1 into Ie2, yielding 3.7. NoHumansRequired Dataset The final pipeline yields dataset of 358 463 high-quality triplets. detailed breakdown of the data volume changes at each stage is provided in Table 3. The initial generation and editing phases have survival rates of 56% and 57%, respectively, with subsequent filtering and quality control steps further refining the set. Augmentation through inversion and compositional generation increases the dataset size by 99.4% and 17.67%, respectively. (a) General category group distribution. (b) Miscellaneous operations distribution. (c) Composite operations distribution, logarithmic scale. Figure 3. Editing operation categories distribution, split into 3 charts for clarity. As shown in Figure 3, the primary focus is on object removal instructions (Figure 3a), as these relatively simple operations, upon successful inversion, provide high-quality and challenging object addition examples, which are crucial for improving modern instruction-based image editors. The dataset also includes variety of other miscellaneous and compositional operations (Figures 3b and 3c), obtained during the stage described in Section 3.6. Furthermore, the data spans wide range of different styles and perspectives, from standard photographic images to cartoon renderings, ensuring real-world diversity (Figure 4). This diversity ex8 Figure 4. Image style distribution, standard stands for images with no explicit style. tends to image aspect ratios, which are detailed in Table 8 of the Appendix. The framework enables the scalable generation of highquality triplets. It also supports weakness-targeted mining: if an editor exhibits high performance on one class of simpler edits but performs poorly on another class of more complex edits, the pipeline can focus on simpler operations with subsequent reversals. Therefore, the pipeline can function as self-correcting loop, continuously targeting and addressing model weaknesses without any human intervention. 3.8. Cross-dataset comparison. To situate the quality of the dataset within the broader landscape, fine-tuned assessor is leveraged to perform largescale comparison, scoring 5 000 randomly sampled edits from each public benchmark. Table 4 reports the mean Instruction and Aesthetics scores for each dataset. Following OmniEdit [43], the geometric mean of these two scores is additionally computed for every edit and its average is reported as an aggregate measure of overall quality. The results presented in Table 4 unequivocally demonstrate the superior quality of the automatically mined dataset. With geometric mean score of 4.53, NHR-Edit not only establishes new state-of-the-art for instructionbased editing datasets but also significantly outperforms the next-best-in-class. This result validates that the fully automated methodology can produce corpus whose quality is not just competitive with, but superior to, existing benchmarks, including those with significant manual curation. 4. Experiments In this section, we conduct experiment to address the following research question: Can NoHumansRequired Dataset improve the performance of an existing edit method? Table 3. Each stage statistics for 63 292 prompts. Taking 2 317 764 generation attempts, the survival rate can be estimated as 20.3%, excluding the squeezing step. All rates depend strongly on instruction complexity. Method / Model Delta (%) Remaining Data Volume"
        },
        {
            "title": "Initial Generation\nGeneration Filtering\nEditing Generation\nEditing Filtering\nLow Level Check\nQuality Scoring\nFinal Selection",
            "content": "FLUX.1-schnell 56.00 Qwen-7B +495.90 In-house DiT 57.00 Qwen-72B (Pre-Filter) Connected Component Analysis 3.00 63.21 Gemini Validator (Hard Filter) 64.04 ArgMax Selection Inversion Composition Backward Consistency Filtering Gemini Validator (Hard Filter) Gemini 2.5 Flash Bootstrap & Concatenation +99.40 +17.67 9.89 1 171 773 515 584 3 072 385 1 321 126 1 281 492 471 523 169 538 338 065 397 804 358 Table 4. Quality metrics across editing datasets, sorted in ascending order by geometric mean. The Type column indicates the generation method: for Automatic and for Manual. The asterisk (*) denotes highly curated automatic dataset. Dataset Type Instr. Aesth. Geom. UltraEdit[60] Seed Part 2[17] Seed Unsplash[17] InstructPix2Pix[9] MagicBrush[55] AnyEdit[53] HQ-Edit[24] ImgEdit[52] Seed OpenImages[17] Seed Part 3[17] A* OmniEdit[43] NHR-Edit 2.67 3.20 3.01 3.17 3.62 3.39 2.90 3.26 3.42 4.06 4.21 4.56 3.30 3.03 3.84 3.58 3.27 3.64 4.21 3.91 3.86 4.37 4.35 4.52 2.92 3.09 3.28 3.30 3.38 3.44 3.45 3.49 3.50 4.13 4. 4.53 image-generation expert exchange information through shared self-attention in single transformer stack. In this work, we performed parameter-efficient adaptation only to the generation experts attention and feed-forward projection layers using LoRA[23] (rank = 16, alpha = 16, dropout = 0.05, bias = \"none\"). We refer to this fine-tuned variant as BAGEL-NHR-EDIT. All other BAGEL components remain frozen to preserve the models pretrained multimodal reasoning and VQA capabilities. We choose LoRA because it offers greater training stability than full-parameter updates and substantially lower computational and memory cost. 4.2. Benchmarks and Evaluation Metrics. We evaluate BAGEL-NHR-EDIT and the baseline BAGEL model on two public image-editing benchmarks: GEditBench[28], which targets instruction-driven guided edits across varied edit types, and ImgEdit-Bench[52], which (a) Change the soapstone carving to jade carving. (b) Remove the sandwich and the headphones. Illustration of poor performance by vanilla MLLMs. Figure 5. Scores for instruction adherence and image aesthetics are provided below: a) gpt-4o-2024-08-06: 5.0, 4.8; Gemini 2.5 Pro: 5.0, 5.0. b) gpt-4o-2024-08-06: 5.0, 4.9; Gemini 2.5 Pro: 5.0, 4.5. 4.1. Experimental Setup BAGEL[15] is 14B-parameter open-source multimodal foundation model with Mixture-of-Transformer-Experts ara multimodal-understanding expert and an chitecture: 9 Figure 6. Proposed NoHumansRequired framework scheme. spans singleand multi-turn region-targeted edits with diverse difficulty and preservation requirements. tail Preservation (fidelity of regions meant to remain unchanged). follow the GEdit-Bench metrics. We authors Semantic ConsisVIEScore[45] automated protocol: tency (SQ, 010) for instruction adherence; Perceptual Quality (PQ, 010) for realism and artifact absence; and an Overall (O) composite derived from SQ and PQ, with scoring produced by GPT-4o[1]. ImgEdit-Bench metrics. We follow the ImgEdit-Bench authors evaluation: GPT-4o[1] scores each edited result on three 15 dimensions Instruction Adherence (prompt comprehension and conceptual alignment), Edit Quality (precision of changes in the target region), and De4.3. Results Table 5 reports performance on ImgEdit-Bench and Table 6 reports performance on GEdit-Bench. Across both, BAGEL-NHR-EDIT model fine-tuned on NoHumansRequired Dataset consistently outperforms the baseline, indicating that additional training on our dataset yields measurable improvements in instruction adherence, edit fidelity, and perceptual quality for image editing. 5. Conclusion and Discussion We introduced an end-to-end, iterative pipeline for mining high-quality triplets for instructionfully automated, 10 Table 5. Comparison results on ImgEdit-Bench [52]. Overall is calculated by averaging all scores across tasks."
        },
        {
            "title": "Add Adjust Extract Replace Remove Background",
            "content": "Style Compose Action Overall BAGEL [15] BAGEL-NHR-EDIT 3.98 4.19 3.55 3.55 1.53 1.62 3.5 3. 3.04 3.18 3.3 3.42 4.22 4.3 3.0 2.94 4.07 3.95 3.3 3. Table 6. Quantitative comparison on GEdit-Bench-EN [28] with categories. SC (Semantic Consistency) evaluates instruction following, and PQ (Perceptual Quality) assesses image naturalness and artifacts. Higher scores are better for all metrics. Alteration Category background_change color_alter material_alter motion_change ps_human style_change subject-add subject-remove subject-replace text_change tone_transfer Bagel-7B-MoT [15] BAGEL-NHR-EDIT SC PQ SC PQ 8.625 8.525 7.625 8.175 6.171 8.133 9.183 8.561 8.683 7.253 6. 6.750 6.350 5.975 7.000 6.457 4.817 7.633 6.912 6.400 8.020 5.950 7.356 6.985 6.202 7.298 5.885 6.020 8.178 7.513 7.252 7.218 6.224 8.825 9.000 7.800 7.175 6.614 7.933 9.333 8.649 8.650 7.354 7.400 6.825 6.800 6.125 7.800 6.714 4.667 7.750 7.211 7.017 8.333 6.450 7.511 7.480 6.441 6.835 6.253 5.847 8.354 7.622 7.631 7.503 6.785 Average 7.983 6.570 6.921 8.067 6.881 7. guided image editing. Unlike traditional dataset construction methods tied to narrow domains, our approach uses pretrained editor to generate new training data. By sampling multiple edit attempts and filtering only successful ones, we capture both simple and hard cases where the model succeeds stochastically. Strong quality requirements enforced during filtering, along with instruction inversion and compositional editing, allow us to build semantically rich, diverse triplets suitable for generative models tuning. The use of T2I model further enables domain and stylistic diversity beyond real-world data, helping mitigate the overfitting risks associated with narrow training domain. Moreover, the ability to jointly use both real and synthetic domains provides fine-grained control over diversity, quality, and bias in the dataset, ensuring robust generalization without degradation on real-world data. Crucially, the pipeline may be cyclic and self-improving: as the editor improves, it generates higher-quality triplets for more edit types, creating feedback loop where the model not only learns from data but also curates it. We release BAGEL-NHR-EDIT, LoRA-tuned BAGEL variant that achieves higher scores than the base model on ImgEditBench and GEdit-Bench, demonstrating that fine-tuning on NHR-Edit improves the underlying editing model. 5.1. Limitations The framework is bounded by the base editor: operations it cannot perform yield no valid triplets, and inversion/composition help only where edits are logically reversible. Multiseed sampling mitigates but does not remove this ceiling. Additionally, the capabilities and limitations of the text-toimage generator and the LLM used for producing generation prompts and editing instructions directly affect the resulting dataset. Certain visual concepts may lie beyond the generative capacity of the T2I model, while others may not be expressible or interpretable by the instruction-generating LLM. Generating large candidate pools and running multistage visionlanguage validation is computeand costintensive, particularly when proprietary APIs are involved. Prompt specification can introduce bias from both human templates and model priors. Without careful design, reusing narrow templates limits variation, and the data may repeat similar concepts. T2I models can further skew the distribution by generating some styles or subject types more frequently than others. Finally, LLM-generated edit prompts may be differ from real user queries. We mitigate this with paraphrasing and diverse prompting strategies. 5.2. Future Work Future directions include extending our fully synthetic pipeline, which currently contains no real user images or instructions, to settings that incorporate real data. In such deployments, user-provided images and edit requests substitute for the synthetic T2I stage, simplifying the pipeline while allowing the existing editing, validation, and mining loop to apply with appropriate consent and privacy safeguards. The same architecture could support agentic selfsupervised learning by iteratively retraining on mined hard cases. 5.3. Ethical and Legal Considerations The released NHR-Edit dataset is produced with FLUX.1schnell(text-to-image generation) and ChatGPT o3 (prompt generation). Despite automated moderation, residual adult or otherwise objectionable content may persist. We disclaim liability for any such material. Furthermore, we disclaim responsibility for models 11 trained on our data that may be used for misinformation, identity spoofing, or content manipulation. Responsible use and transparency remain with the community. 5.4. Broader Impact By releasing NHR-Edit, we aim to support research on textbased editing and provide strong benchmark for future training and evaluation pipelines."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 10 [2] Sina Alemohammad, Ahmed Imtiaz Humayun, Shruti AgarSelfwal, John Collomosse, and Richard Baraniuk. improving diffusion models with synthetic data, 2024. 4 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Lilian Weng, and Aditya Ramesh. Improving image generation with better captions. Technical report, OpenAI, 2023. 3 [4] Megha Bhardwaj and Anant Hans. Aligning text-to-image arXiv diffusion models with k-fold tamer preference. preprint arXiv:2404.04465, 2024. 4 [5] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-Fill-dev, 2024. 3 FLUX.1-Fill-dev. [6] Black-Forest-Labs. https : / / huggingface . co / black - forest - labs / FLUX . 1-schnell, 2024. 6 FLUX.1-schnell. [7] Federico Bolelli, Stefano Allegretti, Lorenzo Baraldi, and Costantino Grana. Spaghetti labeling: Directed acyclic graphs for block-based connected components labeling. IEEE Transactions on Image Processing, 29:19992012, 2020. [8] Gary Bradski. The opencv library. Dr. Dobbs Journal of Software Tools, 2000. 5 [9] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions, 2023. 2, 9 [10] Jacopo Burroni, Federico Boin, Federico Amato Galatolo, Oussama Es-sounayni, Marco De Nadai, Federico Becattini, Nicu Sebe, Claudio Baecchi, and Alberto Del Bimbo. Objectdrop: Bootstrapping counterfactuals for photorearXiv preprint alistic object arXiv:2403.18818, 2024. 2 removal and insertion. [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 96509660, 2021. 3 [12] Yongcen Chen, Chen Wang, Yichun Zhao, Jerry Wang, Jialu Han, Yihua Zhu, Ceyuan Zhou, Yujun He, Kewei Wu, Yong-jin Li, Tiezheng Wang, and Yu-gang Wang. Self-play fine-tuning of diffusion models for text-to-image generation. arXiv preprint arXiv:2402.10210, 2024. [13] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time openvocabulary object detection, 2024. 3 [14] Xueting Cheng, Teli Wang, Zheyuan Liu, Wen-gang Li, Hong-gang Li, Yu-cheng Wang, and Li Wang. Aurora: system for composing and editing images with rich styles and semantics. arXiv preprint arXiv:2407.03471, 2024. 2, 3 13 [15] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3, 9, 11 [16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion, 2022. 3 [17] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 2, 3, 9 [18] Ross Girshick, Georgia Gkioxari, and Piotr Dollar. Segment anything 2. arXiv preprint arXiv:2408.00714, 2024. 3 [19] Google. Gemini 2.5 Pro Preview Model Card. Model card, https:/ /storage.googleapis. Google, 2024. com / model - cards / documents / gemini - 2 . 5 - pro-preview.pdf. 2, [20] Google. Gemini 2.0 Flash Model Card. https://cloud. google.com/vertex-ai/generative-ai/docs/ models/gemini/2-0-flash, 2024. 2, 3 [21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022. 3 [22] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning, 2022. 3 [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 9 [24] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 2, 3, 5, 9 [25] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 2, [26] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-Pic: An open dataset of user preferences for text-to-image generation, 2023. 3 [27] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 2, 3 [28] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2, 3, 9, 11 [29] LMSYS Org. Chatbot Arena Vision (V) Leaderboard. https : / / lmarena . ai / leaderboard / vision, 2024. Accessed on 2024-07-26. 3 [30] OpenAI. OpenAI o3 and o4-mini System Card. https: / / openai . com / index / o3 - o4 - mini - system - card/, 2025. System Card, accessed 18 July 2025. 6 [31] Qwen Team. https : / / huggingface . co / Qwen / Qwen2 . 5 - VL - 7B - Instruct, 2024. 3, 6 Qwen2.5-VL-7B-Instruct. [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 3 [33] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. Ipo: An identity-preserving-optimization method for aligning lms. arXiv preprint arXiv:2402.02088, 2024. 2, 4 [34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023. 3 [35] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. [36] Sang-Hyeon Shin, Jae-Ha Yang, Dong-Hyeok Han, YoungWoon Kim, and Kwang-Hyun Lee. Omnipaint: Mastering object-oriented editing via disentangled insertion-removal inpainting. arXiv preprint arXiv:2503.08677, 2025. 2 [37] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, and Victor Lempitsky. Resolutionrobust large mask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161, 2021. 2, 3 [38] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 5, 6 [39] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow, 2020. 3 [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 4 [41] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation, 2022. [42] Bram Wallace, Rafael Rafailov, Kevin Fein, Dorsa Ilas, Stefano Ermon, Christopher Ré, and Nikhil Naik. Diffusiondpo: Aligning text-to-image models with human preferences. arXiv preprint arXiv:2311.12908, 2023. 2, 4 [43] Cong Wei, Zheyang Xiong, Weiming Ren, Xinrun Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. arXiv preprint arXiv:2411.07199, 2024. 2, 3, 5, 8, 9 [44] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, 14 Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtao Zhai, and Weisi Lin. Q-Align: Teaching lmms for visual scoring via discrete text-defined levels, 2023. 3 [45] Quanzeng Wu, Jian-hao Wang, Jiachen Wang, Zexin Lin, Jiacheng Gao, Jing Zhang, and Jin Lu. VIEScore: Towards Explainable and Controllable Image-to-Text Evaluation. arXiv preprint arXiv:2312.14867, 2023. 2, 3, 5, [46] Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, and Kede Ma. VisualQuality-R1: Reasoning-induced image quality assessment via reinforcement learning to rank, 2025. 3 [47] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human Preference Score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. 3 [48] xAI. Grok. https://x.ai/blog/grok, 2023. Accessed: 2025-07-10. 5 [49] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and evaluating human preferences for textto-image generation, 2023. 3 [50] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. VisionReward: Fine-grained multi-dimensional human preference learning for image and video generation, 2025. [51] Xiaomeng Yang, Zhiyu Tan, and Hao Li. IPO: Iterative preference optimization for text-to-video generation, 2025. 4 [52] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 2, 3, 6, 7, 9, 11 [53] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 2, 3, 5, 9 [54] Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation, 2024. 4 [55] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 9 [56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 3 [57] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [58] Xuanyu Zhang, Weiqi Li, Shijie Zhao, Junlin Li, Li Zhang, and Jian Zhang. VQ-Insight: Teaching vlms for ai-generated video quality understanding via progressive visual reinforcement learning, 2025. 3 [59] Zekun Zhang, Zheyuan Huang, Yushi Li, Hong Zhou, and Hongsheng Li. Self-improving diffusion models with synthetic data. arXiv preprint arXiv:2408.16333, 2024. 3 [60] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2025. 2, 3, 9 [61] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation, 2024. 3 15 A. Prompts Prompt A.1: Samples Design Prompt"
        },
        {
            "title": "Appendix",
            "content": "[WARN] ABSOLUTE BAN: The model must never run Python, or any other executable code, while thinking. It must compose prompts with its own knowledge only. -------------------------------------------------------------------------------- 1. HIGH-LEVEL PRINCIPLES -------------------------------------------------------------------------------- 1. Natural-language first - Full phrases beat comma-separated keyword lists. 2. Specificity over brevity - Vague prompts yield \"average\" images; be precise. 3. One coherent vision - Avoid conflicting or scatter-shot modifiers. 4. Layered thinking - Describe foreground -> mid-ground -> background in order. 5. Active, sensory wording - \"Swirls\", \"emerges\", \"diffused glow\" enrich texture & motion. 6. Iterate, dont dilute - Refine the core idea; no kitchen-sink modifier piles. -------------------------------------------------------------------------------- 2. CORE PROMPT TEMPLATE -------------------------------------------------------------------------------- [TECH / STYLE TAG]: [SUBJECT + ACTION], [ENVIRONMENT / CONTEXT], [COMPOSITION & CAMERA], [LIGHTING], [COLOUR & MOOD]. (use as prose; brackets describe purpose) (Optional) [TEXT ELEMENTS]. Example DSLR photograph on Nikon Z8 with 85 mm f/1.4: red fox pauses atop snow-dusted log in quiet boreal forest, captured at eye-level; shallow depth-of-field isolates the fox. Soft overcast light yields gentle shadows; muted winter palette of whites, greys and russets conveys tranquillity. -------------------------------------------------------------------------------- 3. DETAILED COMPONENT GUIDE -------------------------------------------------------------------------------- - Subject & focal point - species, character, or object with defining traits - Action / interaction - dynamic verb or relationship - Environment / setting - location, era, weather, cultural cues - Composition / lens - shot type, framing, spatial layout, focal length - Lighting - source, quality, direction, time-of-day - Colour palette - dominant hues, contrasts, transitions - Mood / atmosphere - emotional tone, sensory adjectives - Art / render style - medium, artist, movement - Technical descriptors - camera body, film stock, HDR, focus stacking, 8-K and related specs - Text integration - exact wording, font, placement, effect -------------------------------------------------------------------------------- 4. LAYERED & SPATIAL CONTROL -------------------------------------------------------------------------------- Describe layers in order (foreground -> mid -> background) or label them explicitly. Use spatial cues (\"above\", \"to the left\", \"half-submerged\") so FLUX can reason about position. 16 -------------------------------------------------------------------------------- 5. ADVANCED TECHNIQUES -------------------------------------------------------------------------------- - Contrast / dual aesthetics - Define clear borders & transitions (day/night split, joy/sorrow). - See-through materials - Clarify front/behind & distortion (\"rain-soaked glass distorts neon...\"). - Spotlighting - Bracket clause or write \"strong emphasis on ...\" for key elements. - Text-rich posters & UI - Specify font family, size, orientation; keep text short and unique. -------------------------------------------------------------------------------- 6. DOS & DONTS -------------------------------------------------------------------------------- [OK] Use grammatical sentences; always give some background; <= 7 focal subjects. [OK] Reference known artists or genres to cue style; describe lighting every time. [OK] Mix gear-specific tags *sometimes* (e.g. \"DSLR photograph on Canon EOS R5 with 35 mm f/1.8\"); at other times say \"Realistic photo, 4K\" - but always be explicit. [NO] Dump raw keywords or weight syntax; leave background implicit; issue contradictory fixes in one prompt; over-use \"white background\" (causes blur in dev builds). -------------------------------------------------------------------------------- 7. PROMPT-DRAFTING WORKFLOW -------------------------------------------------------------------------------- 1) Gather intent (subject, style, mood, use-case, text, resolution). 2) Fill the template, omitting only truly irrelevant slots. 3) Check consistency-no style or light contradictions; max 7 focal subjects. 4) Add layer/spatial cues for multi-element scenes. 5) Return the final prompt (plus an optional short troubleshooting tip if helpful). -------------------------------------------------------------------------------- 8. TROUBLESHOOTING CHECKLIST -------------------------------------------------------------------------------- Blurry or flat Wrong era/style Missing background -> add explicit environment sentence. Unwanted objects Illegible text Overcrowded -> issue deletion edits (next section). -> shorten phrase or specify font. -> split ideas into separate images. -> specify sharper lens/aperture or refine light source. -> state artist or medium earlier. (OPERATION = \"DELETE\" ONLY) -------------------------------------------------------------------------------- 9. OBJECT-REMOVAL EXTENSION -------------------------------------------------------------------------------- GENERAL RULES - Each prompt must name **1 - 5** clearly visible, dramatic objects. - Supply **exactly the same number** of deletion edits-one per object. - Edits may be casual, slangy or profane (\"yeet the kite\") but must target their object unambiguously. Include spatial clues; make them *sometimes* tricky so the receiving model must reason about the scene, but not so tricky that mistakes are likely. - Deletion-only-no recolours, swaps, resizes. - Edits are independent; never reference other edits or prior context. - Mix everyday, exotic and fantasy objects; vary scales (colossi foreground -> tiny background). - **Prefer descriptive spatial cues** (\"the far-right lantern above the tea stall\", \"the upper-left hotspot near the chimney vent\") **over ordinal placeholders** (\"lantern three\", \"hotspot two\"). Ordinals presume an invisible ordering and leave the downstream model guessing which target to erase; explicit visual references keep deletions predictable and robust. COMPOSITE EDIT RULE - If prompt names **2 or more objects**, the **last** edit line **must** be composite deletion that lists *all* objects again, for example: \"Remove the bench, the cat and the payphone.\" SCENE VARIETY & STYLE - Constantly shuffle viewpoints: macro, fisheye HDR, overhead drone, thermal, infrared, ultraviolet, night-vision, aerial panoramic, underwater focus-stacked macro, 360-degree VR stitch. - Rotate visual aesthetics across the batch: photoreal, anime cell-shade, ukiyo-e woodblock, glitch poster, pop-art halftone, doodle sketch, steampunk schematic, cyberpunk panorama, impressionist oil, linocut, caricature, Western cartoon. - Maintain single coherent style inside the realistic, every-day life. - Use DSLR gear tags only intermittently, as noted in Section 6. -------------------------------------------------------------------------------- 10. BATCH REQUIREMENTS -------------------------------------------------------------------------------- - Generate exactly 50 prompt + edit pairs themed around realistic, every-day life. - Spread object counts roughly evenly: about 10 prompts each with 1, 2, 3, 4, 5 objects. -------------------------------------------------------------------------------- 11. OUTPUT JSON FORMAT -------------------------------------------------------------------------------- Return **valid JSON**: an array where each item is an object { \"prompt\": \"<detailed scene prompt>\", \"edits\": [ \"<delete instruction 1>\", \"<delete instruction 2>\" ] } Constraints - Array length = 50. - \"edits\" length = number of named objects (1 - 5). - For prompts with 2+ objects, the final edit line is always the composite deletion listing all objects. Prompt A.2: Image Evaluation Prompt You are an expert evaluator of image editing quality. Your task is to judge how well an edited image matches given editing instruction when compared to the original image. You will receive: 1. The **original image** 2. The **edited image** 18 3. The **instruction** - text describing the desired change(s) **Important**: You must perform your reasoning internally, without revealing your chain-of-thought. Then, you will provide only two scores - in clearly parseable technical format - corresponding to: 1. **Instruction Adherence Score** (from 1.0 to 5.0, floats allowed) 2. **Image Aesthetic Score** (from 1.0 to 5.0, floats allowed) These two scores must always be provided, even if you suspect policy violations or if you are uncertain. No matter what the images contain, you must output: - single structured response with exactly two numerical scores. - No additional explanations or justifications beyond these scores. **Guidelines**: 1. **Instruction Adherence** - The instruction must be followed completely. - Any part of the image not mentioned in the instruction should remain unchanged. - If the original image is realistic or photorealistic, ensure the edit is also realistic, unless told otherwise. - If the original image is stylized (cartoon, digital art, painting, etc.), the edit must preserve that style unless the instruction specifies different style. - Global style changes in the instruction (e.g. draw this image in an anime style) override the original style. 2. **Aesthetic / Coherence** - The edited image should remain coherent and visually pleasing (aesthetic). - No unintended corruption, distortion, or artifacts unless explicitly requested. - If an instruction demands glitch or distortion, follow it - otherwise keep the image looking appealing relative to its starting style. 3. **Separate Scores** - Instruction Adherence: Range from 1.0 to 5.0 - Image Aesthetic: Range from 1.0 to 5.0 **Editing Instruction**: {} Your final output must be only the two scores in JSON format. Do not include your reasoning or any text beyond these scores. Example: { \"InstructionAdherence\": 4.3, \"ImageAesthetic\": 2.8 } No matter the circumstances, produce two numeric scores every time. Prompt A.3: Unwanted Modifications Check Prompt You are provided with two images: - ORIGINAL: the source image. - EDITED: the image after editing. The edited image was created according to the following instruction: 19 \"{instruction}\" Examine the EDITED image carefully. Consider this guideline: - If the edited image perfectly matches the given instruction without any additional or unwanted modifications, respond with yes. - If it does not, respond with no. - If the instruction is vague, abstract, unfeasible, or lacks deterministic outcome, then respond with no. Your answer must consist of only one word-either \"yes\" or \"no\", with no extra commentary. Prompt A.4: Visual Aesthetics Check Prompt You are an expert in visual aesthetics. Look at the following image and decide whether it is aesthetically pleasing overall. Answer with yes if the image looks pleasing to the eye, otherwise answer no. Respond with only that single word. Prompt A.5: T2I check prompt Does this image accurately depict the prompt: {} and does it look realistic and plausible? Answer Yes or No. Prompt A.6: Inverse Instruction Prompt You are an expert in crafting image-editing instructions. You will be given two inputs Original description: \"{}\" Editing instruction: \"{}\" Write **one concise inverse instruction** that, when applied to the edited image, reverses exactly the stated change. Constraints - Output only the inverse instruction - no commentary. - Refer only to the object(s) that changed; ignore everything else. - Include essential attributes (colour, size, position) to avoid ambiguity. - Do not use the words revert, undo, restore, or back. - Keep the instruction short and natural. Examples Original: \"A picture of man and woman with an artistic black mustache.\" Edit: \"Remove the mustache.\" Inverse: \"Add an artistic black mustache to the woman.\" Original: \"A wooden table with single red apple at its center.\" 20 Edit: \"Remove the apple.\" Inverse: \"Place red apple at the center of the wooden table.\" B. Assessor Details In this section, we provide additional details on the corpus used to train our Gemini validator and more granular analysis of its performance. B.1. Fine-Tuning Corpus Analysis As mentioned in Section 3.3, dedicated dataset was collected to fine-tune the assessor. Figure 7 shows the distribution of Instruction and Aesthetics scores for both the training and validation splits. The distributions are similar across splits, ensuring consistent evaluation. The bimodal distribution of the Instruction scores is by design: we deliberately included clear successes and obvious failures to train the model to distinguish between them with high confidence. Figure 8 shows the composition of this fine-tuning dataset by the source model used for generating the edits. The majority of examples were generated using our internal Image-2-image model, which allowed us to create large and diverse set of editing scenarios. To ensure robustness and prevent overfitting to single generators idiosyncrasies, we also supplemented the corpus with data from leading proprietary and open-source models (Gemini, Grok, SD3), as detailed in Section 3.3. Figure 7. Score distributions for the training and validation splits of the assessor fine-tuning dataset. B.2. Detailed Error Analysis While the overall MAE reported in Table 1 provides general performance summary, more detailed analysis reveals important nuances. MAE by Score Bucket. Figure 9 plots the MAE calculated for examples grouped by their ground-truth score bucket. This analysis reveals that the assessors error is not uniform. The highest error (MAE > 0.6) occurs for mid-quality examples (scores between 2.0 and 4.0). Crucially, for high-quality examples (scores 4.5-5.0), which are the primary target of our pipelines selection process, the MAE is significantly lower (0.25-0.35). This indicates that our assessor is most accurate Figure 8. Composition of the Gemini Assessor Fine-Tuning Corpus by Source Model. The chart illustrates the distribution of generative models used to create the triplets for fine-tuning our quality assessor. in the exact region where precision is critical for curating the final dataset. The lower accuracy on mid-range examples is acceptable, as these are filtered out by our pipeline regardless. Figure 9. Assessor MAE as function of the ground-truth score bucket. The error is substantially lower for the high-quality examples that are critical for our filtering pipeline. Confusion Matrices. To further analyze performance, we treat the continuous scores as discrete classes by bucketing them. Figure 10 presents the confusion matrices where both predicted and ground-truth scores are grouped into ranges. The strong diagonal in all three heatmaps indicates that the assessor correctly classifies most examples into their corresponding quality tier. For instance, examples with ground-truth score in the [4.7-5.0] range are almost never misclassified as \"poor\" (below 4.0). Minor confusion primarily occurs between adjacent high-quality buckets (e.g., [4.5-4.7] vs. [4.7-5.0]), which is an expected and non-critical behavior for this task. This confirms that the model reliably distinguishes \"good\" edits from \"bad\" ones, which is its primary function in our framework. 22 Figure 10. Confusion matrices for Aesthetics and Instruction. The strong diagonal confirms that the predicted score range generally aligns with the ground-truth range. B.3. Threshold Selection and Classification Analysis While our Gemini validator is trained as regression model, its performance can also be analyzed from binary classification perspective. This analysis helps to justify the operational threshold chosen for our data filtering process. For this analysis, we define \"successful\" triplet (the positive class) as one with human-annotated Instruction and Aesthetics scores both above baseline of 4.0. Table 7 presents the classification metrics obtained when applying our operational prediction threshold of 4.7 (as specified in Section 3.5) to the models outputs. The table also includes results for several other base models to provide comparative context. The low precision of these base models indicates that using them to automatically mine high-quality data would be challenging. Table 7. Comparative classification performance of different validator models. Metrics are calculated on the validation set using an operational threshold of 4.7 for both Instruction and Aesthetics scores."
        },
        {
            "title": "Recall",
            "content": "F1-Score"
        },
        {
            "title": "Accuracy",
            "content": "Qwen 2.5 72B Gemini 2.0-flash (base) Gemini 2.5-pro Gemini 2.0-flash (finetune) 0.571 0.473 0.649 0.834 0.483 0.931 0.591 0. 0.523 0.628 0.619 0.581 0.628 0.531 0.692 0.727 The choice of specific threshold determines the trade-off between precision and recall. As specified in Section 3.5, our main pipeline uses threshold of 4.7. As illustrated in Figure 11, this threshold strikes good balance: it maintains high precision to ensure the quality of selected triplets while keeping recall at an acceptable level, thus avoiding the rejection of an excessive number of successful candidates. Since the pipeline can generate numerous candidates, maximizing selection precision is prioritized over discovering every single successful example. Therefore, the 4.7 threshold represents balanced solution for our goal of building high-fidelity dataset. 23 Figure 11. Precision and Recall as function of the score threshold applied to both Instruction and Aesthetics predictions. Our operational threshold of 4.7 is chosen to balance high precision with acceptable recall. C. Aspect Ratio Distribution"
        },
        {
            "title": "Aspect ratio",
            "content": "#Edits"
        },
        {
            "title": "Sample Aspect ratio",
            "content": "#Edits"
        },
        {
            "title": "Sample",
            "content": "640 1600 640 1536 704 1472 704 1408 369 5752 7857 1024 960 22186 1088 960 1088 896 19934 1152 896 18021 768 1344 1152 832 19070 768 1280 15087 1216 832 832 1216 21838 1280 768 16941 832 1152 1344 768 10564 896 1152 18807 896 1088 960 1088 21256 960 1024 20951 1024 1024 1344 704 1408 704 1472 704 1536 640 1600 640 3706 5872 3103 411 Table 8. Distribution of image aspect ratios. D. Representative Examples 25 26 27 28"
        },
        {
            "title": "Initial image\nshortcomings",
            "content": "The pipeline filters may occasionally miss problems in the original images, for example in scenes with dynamic human poses. Inclusions found (random subset of 300)"
        },
        {
            "title": "Examples",
            "content": "Handling complex shadows, reflections, and lighting Although the system usually removes or adds these effects correctly, some sophisticated (especially lighting-related) cases remain challenging."
        },
        {
            "title": "Incorrectly\nidentifying\nthe target\nregion",
            "content": "In some cases, the edit may over-affect or under-affect the image (e.g., failing to remove heavily occluded portions of the target object)."
        },
        {
            "title": "Other\nshortcomings",
            "content": "There are also occasional errors of other kinds, such as imperfect inpainting after object removal. 13 10 5 Remove the flickering lantern Remove the sports car in the background Remove the red folding bike Table 9. Ablation study. Remove the combine harvester"
        }
    ],
    "affiliations": [
        "Layer Team, SALUTEDEV"
    ]
}