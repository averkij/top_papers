{
    "paper_title": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking",
    "authors": [
        "Greta Warren",
        "Jingyi Sun",
        "Irina Shklovski",
        "Isabelle Augenstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as fact-checking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is a key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice."
        },
        {
            "title": "Start",
            "content": "Show me the evidence: Evaluating the role of evidence and natural language explanations in AI-supported fact-checking 6 2 0 2 6 1 ] . [ 1 7 8 3 1 1 . 1 0 6 2 : r Greta Warren grwa@di.ku.dk Department of Computer Science, University of Copenhagen Copenhagen, Denmark Irina Shklovski ias@di.ku.dk University of Copenhagen Copenhagen, Denmark Link√∂ping University Link√∂ping, Sweden Abstract Although much research has focused on AI explanations to support decisions in complex information-seeking tasks such as factchecking, the role of evidence is surprisingly under-researched. In our study, we systematically varied explanation type, AI prediction certainty, and correctness of AI system advice for non-expert participants, who evaluated the veracity of claims and AI system predictions. Participants were provided the option of easily inspecting the underlying evidence. We found that participants consistently relied on evidence to validate AI claims across all experimental conditions. When participants were presented with natural language explanations, evidence was used less frequently although they relied on it when these explanations seemed insufficient or flawed. Qualitative data suggests that participants attempted to infer evidence source reliability, despite source identities being deliberately omitted. Our results demonstrate that evidence is key ingredient in how people evaluate the reliability of information presented by an AI system and, in combination with natural language explanations, offers valuable support for decision-making. Further research is urgently needed to understand how evidence ought to be presented and how people engage with it in practice. CCS Concepts Human-centered computing Empirical studies in HCI; Empirical studies in collaborative and social computing; Computing methodologies Natural language processing. Keywords explainable AI, fact-checking, explanation, natural language processing, misinformation"
        },
        {
            "title": "1 Introduction\nLarge Language Models (LLMs) are increasingly popular tools\nfor decision support for high-stakes tasks that require retrieving\nand reasoning over complex information, such as fact-checking\n[13, 19, 56]. However, these artificial intelligence (AI) models suf-\nfer from issues with factuality [4, 12, 27] and bias [20, 44] in their\noutputs, which means that relying on LLMs for decision-making is",
            "content": "Jingyi Sun jisu@di.ku.dk Department of Computer Science, University of Copenhagen Copenhagen, Denmark Isabelle Augenstein augenstein@di.ku.dk Department of Computer Science, University of Copenhagen Copenhagen, Denmark risky. At the same time, the linguistic fluency and persuasive qualities of LLM-generated text [52] can convince people to erroneously accept their outputs [6]. As result, overreliance, that is, accepting or following incorrect AI recommendations, is an increasingly recognised problem for human-AI interaction [5], particularly in information-seeking and decision-support tasks [10, 32]. Common mitigation approaches for overreliance include encouraging people to evaluate AI input before coming to final decision [8, 48] and providing explanations for AI decisions [59]. recent critique of explainable AI efforts highlights lack of consideration for the importance of evidence in how people can integrate AI support into practice [48]. As commercial search systems integrate LLM output, these services increasingly include both explanations and links to retrieved evidence to help people decide whether to rely on AI-generated search summaries or investigate further [32, 42]. Although sourceor evidence-seeking behaviour is considered important to understanding and calibrating reliance on AI advice, surprisingly little research has addressed this issue directly [26]. Some LLM-assisted information-seeking studies have shown that including explanations and links to evidence sources can help mitigate AI overreliance, although few people actively inspect the underlying evidence provided by the sources in the course of completing their tasks [32, 33]. Moreover, there is scant research on what may be more or less effective ways of presenting evidence for different types of information search and decision support activities. In particular, little prior work explores providing evidence as part of the experimental setup. Making the evidence readily accessible alongside the AI system output decreases friction inherent in clicking through links to external information, and may enable people to more easily inspect the validity of the systems predictions, allowing for greater oversight and reliance calibration. second stream of overreliance mitigation approaches is the provision of explanations of AI system decisions, with the goal of supporting critical engagement with AI output, especially in decision-support tasks [47, 59, 70]. However, research on the relationship between explanations and reliance on AI system advice has produced conflicting findings. In some cases, explanations appear to enable more efficient use of AI support [70], although different types of explanations may also increase overreliance [5]. Communicating model uncertainty in AI outputs has been found to help people identify when advice may be unreliable [11, 41], induce more deliberative thinking [55], and decrease overreliance [31]. While most have used numerical scores [34, 51, 71], or verbal hedges (e.g., Im not sure but... [31]), to indicate model uncertainty, recent work suggests that people may find natural language explanations of uncertainty helpful and compelling [23, 66, 67]. Explanations of uncertainty may be an effective way to highlight inconsistencies in AI output, which can help mitigate overreliance [32, 64]. Building on this work, we sought to explore the roles of evidence and explanations in an AI-assisted information-seeking and decision-making task. We examined to what extent people used AI system explanations and the evidence underlying the AI system decision when evaluating an LLM-based AI system for factchecking. We conducted mixed methods 3x2x2 controlled experiment (N=208) with crowdworkers to examine the effects of providing explanations and evidence in fact-checking scenario and collected qualitative responses about how participants made their decisions. We compared two types of natural language explanations: standard verdict-focused explanations and explanations of AI uncertainty, along with control condition consisting of only the typical numerical model certainty. We included the option to access the evidence documents used by the AI system in its output as part of the experimental setup across all conditions. We measured which sources of information (e.g., evidence, explanations, own knowledge) participants found useful, their reliance on the AI systems advice, decision accuracy, subjective measures of confidence, usefulness and trust and qualitative evaluations of how they used the information available to them. We found that participants overwhelmingly identified the evidence as the most useful information source, regardless of the correctness of AI advice, level of AI certainty, and whether they saw an explanation or not. When participants were provided with natural language explanations, these were also viewed as helpful, particularly when the AI systems advice was correct or highly confident. We also discovered an association between participants self-reported trust in the AI system and the information they relied upon: those who placed more trust in the AI system tended to use AI explanations more, while those with lower levels of trust tended to rely more on the underlying evidence. Together, our results highlight that making the underlying evidence for AI system outputs accessible has powerful effects for effective decision-support in information-seeking tasks."
        },
        {
            "title": "2 Related Work\n2.1 Mitigating overreliance in decision-support",
            "content": "systems Overreliance is typically defined as the acceptance of AI system output when it is incorrect, or delegation of decisions to an AI system when it is inappropriate to do so [26], e.g., if doctor were to wrongly accept an AI diagnosis without critically evaluating whether it aligned with the patients symptoms. Excessive deference to AI advice can lead to errors and severe harms, particularly in high-stakes scenarios such as clinical decision-support [29, 37]. 2 The growing use of LLMs for decision-making and informationseeking in everyday life has also raised concerns about risks of professional deskilling [9], diminished educational outcomes [72] and declines in general problem-solving and critical thinking abilities [16]. Approaches to mitigating overreliance on AI systems primarily fall into two categories: those that encourage users of AI systems to consider the evidence for themselves (e.g., cognitive forcing [8] and evaluative AI paradigms [48]), and those that encourage users to develop better understanding of the AI system and its reliability, through providing explanations (e.g., [70]) and/or indications of model certainty (which may be numerical [55] or verbal [31]). In the following subsections, we examine prior work on both of these approaches. Reliance is also related to trust, although they are distinct constructs [14, 25]; trust is typically construed as willingness or intent to rely on third party or system [28], while overreliance may be the behavioural outcome of placing too much trust in the system (although this may also stem from error or desire to eschew responsibility for decision or outcome). In this study, we measured the extent to which participants chose to use the AI systems predictions, to evaluate the impacts of presenting evidence and explanations for AI system outputs on participants reliance. We also measured participants subjective trust in the system to examine the relationship between trust and reliance on AI advice, explanations and evidence."
        },
        {
            "title": "2.2 The role of evidence in AI reliance\nA promising approach to reducing overreliance is including the\nevidence underpinning the AI system‚Äôs prediction alongside its\noutput. Prior work has explored how decision-support paradigms\ncan be used to promote cognitive engagement with the available\nevidence and encourage system users to come to their own conclu-\nsions. Cognitive forcing is one such technique, in which the user of\na decision-support system is presented with the AI system input\nand must make their own prediction as to what the outcome will be,\nbefore being shown the AI system‚Äôs output [8]. The Evaluative AI\nparadigm is an alternative, which proposes that decision-support\nsystems should provide evidence for and against each hypothetical\noutcome, rather than recommending a single prediction [48]. Recent\nwork on reliance on LLMs for information-seeking and decision-\nsupport has focused on providing evidence in the form of clickable\nlinks to external information (e.g., news articles or government\nadvice). In a question-answering task, LLM responses that listed\nsources were associated with lower overreliance, and higher user\nconfidence in their own answers [32]. However, the explanations\nprovided by the AI system did not explain how or why the sources\ncited support the AI system‚Äôs prediction. Moreover, participants\nonly clicked on one or more of these sources 22-28% of the time,\nsuggesting that the positive impacts of sources may be more due\nto the credibility of the source linked (which were vetted by the\nauthors to contain high-quality, correct information) than the infor-\nmation contained in the links themselves. While source credibility\nis a critical component of fact-checker decision-making [43], it is\nnotoriously difficult to integrate into automated decision-support\nsystems because it tends to be highly variable and subjective [2].\nIn this study, we include evidence alongside all explanations, but\nomit source credibility information to limit impacts of familiarity",
            "content": "and subjective bias. We hypothesised that evidence would be more helpful when explanations are not provided and could help mitigate overreliance."
        },
        {
            "title": "2.4 AI decision-support for fact-checking\nWe designed our experiment as a fact-checking task due to its rele-\nvance to both information-seeking and decision-support contexts,\nwith potentially high stakes consequences, given that AI systems\nsuch as LLMs are increasingly used as sources of information and\nnews [65]. Recent work with fact-checking practitioners suggests\nthat understanding how the AI system arrives at its prediction is\nkey to effective use [71]. The two most common approaches to ex-\nplainability in this space are attribution-based explanations, which\nhighlight the key tokens or words that contributed to the AI sys-\ntem‚Äôs output [53], and natural language explanations in the form\nof free-text justifications for the AI system‚Äôs decision [3, 35]. While\nthere is evidence that explanations can enhance understanding of\nAI systems by increasing model transparency [15], research shows\nthat they also can lead to problematic overreliance on AI advice\n[5, 50, 54, 73]. For instance, example-based and attribution-based\nexplanations seem to have little effect on people‚Äôs ability to cor-\nrectly assess the veracity of claims [40], but can lead to reliance on\nAI system advice even where it can be demonstrably wrong [39]. In",
            "content": "this study, we designed task in which participants are presented with the input (claim and evidence) and outputs of an AI system (predicted verdict, model uncertainty, AI explanation) and asked to decide whether to use the AI systems prediction or not, and to identify which information they used to make their own decision."
        },
        {
            "title": "3.1 Experimental setup and design\n3.1.1 Generating study materials. The claims and evidence docu-\nments that formed the basis for the materials were drawn from the\nDRUID [22] dataset, which consists of fact-checking claims drawn\nfrom professional fact-checking websites, such as Snopes (https:\n//www.snopes.com/), and Full Fact (https://fullfact.org/), along with\nevidence documents retrieved from various online sources. For\neach claim and pair of evidence documents, we used Qwen2.5-14B-\nInstruct1, an open-weights, instruction-tuned language model with\n14B parameters, to generate a predicted verdict and explanation\nfor the given claim. We chose this model as a representative mid-\nsized open model that can be run locally (supporting reproducibility\nwithout relying on proprietary APIs), and that provides access to\ntoken-level logits required for our entropy-based uncertainty esti-\nmate. The AI system‚Äôs uncertainty was quantified by the entropy of\nits logits for generating the verdict, i.e., True or False (see Appendix\nE.1 for the estimation method).",
            "content": "3.1.2 AI explanations. We compared three types of explanations (see Table 1 for examples). The Uncertainty explanations provided reasons for the model certainty score. For example, conflicting evidence provided to the model may decrease the models certainty that the predicted verdict is correct, while corroborating evidence may increase the models certainty. To generate the uncertainty explanations, we followed the CLUE method [67]. Given claim and some evidence documents as input, this method first identifies key conflicting and concordant spans of text between the claim and evidence pieces that influence model certainty, and then instructs the model to describe how these conflicting or concordant span 1https://huggingface.co/Qwen/Qwen2.5-14B-Instruct 3 Figure 1: Example of task interface in Uncertainty Explanation condition interactions, such as disagreements between two pieces of evidence, affect the certainty of the verdict prediction (see Appendix E.3 for the instruction prompt). The Verdict-focused explanations were generated using few-shot prompting method in which the model was instructed to explain why the verdict is predicted by referencing or summarising the provided evidence, and provided with three complete examples within the prompt (see Appendix E.2). The explanations in the No explanation condition were designed as control condition and merely restated the AI systems verdict prediction and certainty score in text. We assessed all generated explanations for quality, making minor edits to ensure consistency in structure (e.g., line-breaks and 2-3 numbered paragraphs for readability) and equivalent length in the explanations for the same claim. Where explanations did not explicitly explain the extracted text spans and the resulting AI system certainty percentage as instructed, we made minor edits to include this information to ensure that all explanations met our intended aims. We ensured that all Uncertainty explanations began with The parts of the evidence with the most impact on the AI systems certainty in the prediction are: and that all Verdict explanations began with The parts of the evidence with the most impact on the AI systems predicted verdict are: to ensure the target of each explanation type was clear. Interface design. Following previous studies examining AI3.1.3 supported fact-checking (e.g., [40, 49, 61]), we designed the interface to mimic news dashboard. Participants were presented with factchecking claim, the AI systems predicted verdict, certainty, and an explanation for the AI systems output. Participants could access the evidence by clicking on the evidence boxes in the interface to reveal them. We chose to present the evidence within the experiment, rather than as links to external sources (as in e.g., [30, 32]) for several reasons: (i) to mitigate the risk that evidence could be edited or removed, given that the AI outputs were pre-generated, (ii) to maintain participants focus on the experimental task, rather than becoming distracted by side-research, and hence (iii) to control the content, relevance, and length of information participants were 4 provided, (iv) to allow us to measure the extent to which participants chose to view or hide the evidence, and (v) to study the approach of providing evidence documents in-window used by number of LLM-based chatbots designed specifically for knowledge intensive information-seeking tasks such as Consensus AI (https://consensus. app/) and Google Geminis Double-check response feature (https: //gemini.google.com/app)."
        },
        {
            "title": "3.2 Pre-testing experimental setup through",
            "content": "think-aloud study We pre-tested our initial experimental setup with think-aloud study to refine our design and materials and to observe in-situ how people reacted to the different types of explanations. We recruited five participants with diverse expertise from our institution (T1T3) and wider community (T4-T5) (see App. for demographics). Sessions lasted approximately 1 hour, and were audio recorded and transcribed with participants permission (see App. B). Each participant saw six claims, two with each type of explanation, in random order. We conducted semi-structured interviews upon task completion. We made several key design choices based on the observations and insights gathered during these sessions. 3.2.1 Task Design. Our initial study design asked participants to make decision about whether the claim was true or false and gave no time limit for task completion. After three think-aloud sessions, we observed that people often focused on reading the evidence, to the exclusion of the AI system decisions and explanations. primary reason for this was that the decision about whether the claim was true or false did not necessarily require people to engage with the AI system, as the evidence was readily available and people could spend as much time as desired reading the evidence documents to form their decision. Therefore, to encourage participants to engage with the AI system prediction, certainty and explanations, we made two main adaptations to the task. Firstly, we altered the main decision participants made, from True vs False to Use AI Prediction vs Do more research. In the experiment instructions (see Appendix F), participants were told to imagine that their goal was to release assessments about whether claim is true or false, and that their task was to decide whether to rely on the AI system prediction, or that more research was needed before releasing the assessment. The purpose of this change was to shift participants focus towards evaluating the AI system and its usefulness, rather than merely evaluating the claim and the evidence. Secondly, we introduced time limit of 5 minutes per instance, to create more realistic situation for decision-making, engender greater sense of urgency and discourage participants from conducting side-research. When the time limit elapsed, evidence and explanations disappeared from the screen, and participants were prompted to make final decision and move on to the next instance. After introducing these changes, we conducted further think-aloud sessions with two additional participants, and observed that the changes that we made shifted participants focus more towards the AI system. about the claims, and replaced claims that were too vague, too familiar, or too difficult to understand (e.g., dont like the claim [...] its very vague and feel like it should just throw it out T1). 3.2.3 Evidence sources. Participants frequently commented on the importance of knowing the identity of sources: including the source of the evidence... It will allow me to know how trustworthy the evidence is, who made the claim (T2). While we wanted to avoid including specific names of sources to control for potential biases of participants [2, 38], we modified the task instructions to advise them that all evidence provided should be considered reasonably reliable."
        },
        {
            "title": "3.3 Controlled Experiment\n3.3.1 Design and Measurements. The final experimental design\nhad a 3 (Explanation type: Uncertainty explanation vs Verdict ex-\nplanation vs No explanation) x 2 (AI advice: correct vs incorrect) x 2\n(AI certainty: high vs low) design to investigate the effects of expla-\nnations and evidence across different levels of AI correctness and\nAI certainty. Explanation type was a between-participants factor,\nwhile AI correctness and AI uncertainty were within-participants\nfactors. In the main task (see Figure 1), participants were presented\nwith a fact-checking claim, the AI system‚Äôs predicted verdict, cer-\ntainty, and an explanation for the AI system‚Äôs output. Participants\ncould access the evidence by clicking on the evidence boxes in the\ninterface to reveal them. For each claim, participants were asked to\ndecide whether to (a) use the AI system‚Äôs prediction, or (b) do more\nresearch. If a participant chose to ‚Äôdo more research‚Äô, a follow-up\nquestion asked them to explain their decision given the options\n‚ÄòThe AI is incorrect‚Äô, ‚ÄòMore information is needed to make a de-\ncision‚Äô, or ‚ÄòOther‚Äô. For each decision, participants were asked to\nindicate what sources of information (i.e., AI verdict, AI certainty,\nAI explanation, evidence, their own knowledge, and other) their\nfinal decision was based on. They were also asked to indicate (i)\nhow confident they were in their own decision, and (ii) how use-\nful the explanation was to their decision on a scale of 1-7. There\nwas a time-limit of 5 minutes for each claim. If this elapsed, the\nevidence and AI explanations disappeared and participants were\nforced to make a decision and move on. A post-task questionnaire\ncollected subjective evaluations and free-text responses, see Table\n2 and Appendix C for details.",
            "content": "3.3.2 Participants. power analysis [17] indicated that 207 participants were required for 90% power for medium effect (alpha<.05) for two-tailed tests. Participants (N=208) were recruited using Prolific (https://www.prolific.com/), and randomly assigned to one of three conditions: Uncertainty explanation (n=70), Verdict-based explanation (n=69) and No explanation (n=69). Participant demographics are reported in Table 3. Participants were pre-screened to be native English speakers from Ireland, the United Kingdom, the United States, Australia, Canada and New Zealand, who had not participated in previous related studies. Twenty-two participants who failed more than one attention or memory check were excluded prior to data analysis. 3.2.2 Claims. The think-aloud pre-testing also helped to identify potential issues with claims presented to participants. After each think-aloud session, we reviewed the comments participants made 3.3.3 Materials. Eight unique claims (and corresponding evidence documents) were selected from the DRUID [22] dataset to balance (i) true and false verdicts, (ii) correct and incorrect AI predictions, 5 Claim: windmill could spin until it falls apart and never generate as much energy as was invested in building it. Predicted Verdict: False Certainty: 31% Uncertainty Explanation Verdict Explanation The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, No, professor didnt say windmill will never generate the energy invested in building it, directly contradicts the claims assertion that windmills will never generate sufficient energy. This contradiction increases the AI systems certainty that the claim is False. 2. The evidence in Evidence 1, will never generate the energy invested, aligns with the part of the claim spin until it falls apart and never generate as much energy as was invested, reducing the AI systems certainty that the claim is False. 3. The evidence in Evidence 1, expended to, aligns with the phrase invested in building in the claim, indicating agreement on the energy expenditure aspect, reducing the certainty that the claim is False. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 explains that the claim comes from misrepresented quote. The meme states that windmills will never generate as much energy as was expended to build the structures, but the original quote clarifies that while poorly placed windmills may never generate enough energy payback, good wind site could generate it in three years or less. This shows the claim exaggerates and omits important context. 2. The same evidence highlights that well-sited windmills can generate enough energy to offset their construction cost quickly, making the claim that all windmills never generate enough energy inaccurate. The phrase good wind site could generate it in three years or less directly contradicts the claim. 3. Evidence 2 reinforces this, stating that analyzing the energy used in the production of wind turbines does not show that its greater than the electricity produced over the turbines working lifetime. This confirms that, over time, turbines produce more energy than was invested in building them. No Explanation The AI system has judged this claim to be False, with 31% certainty. Table 1: Example of claim, corresponding AI prediction, certainty, and explanations Task Measurement Response Main task AI advice use Use AI prediction vs. Do more research Post-task Questionnaire Information usage Decision confidence Explanation usefulness Multiple choice: The AI systems verdict / AI certainty / AI explanation / Evidence / Own knowledge / Other 7-point scale: 1=Not at all to 7=Extremely 7-point scale: 1=Not at all to 7=Extremely Information usage (qualitative) Open text response to How did you use the information provided by the AI system to make your decisions? Explanation utility (qualitative) Open text response to Please explain why the AI system explanations were/werent useful for determining how reliable the system was. Explanation helpfulness (App. C.1) 5-point scale: 1=Not at all to 5=Extremely Trust Belief & Intention [45] (App. C.2) 5-point scale: 1=Strongly disagree to 5=Strongly agree Claim familiarity (App. C.3) 4 options: No knowledge / Limited knowledge / Some knowledge / Full knowledge Table 2: Overview of tasks, measurements, and response formats. 6 Age Gender = 41.44 Woman SD = 13.34 Man Non-binary No answer % Education level 45.67 Doctorate degree 51.44 Masters degree 1.92 .96 Bachelors degree Some third-level High school graduate Less than high school No answer % 5.28 13.94 35.58 24.52 17.79 1.92 0.96 Table 3: Participant demographic details. (iii) high (>50%) and low (<50%) AI certainty. Explanations were consistent with the AI prediction, regardless of whether it was correct or incorrect. For all claims, evidence and explanations see Appendix G. 3.3.4 Procedure. Participants received link to the study hosted on Gorilla2 (see Appendix for task instructions). Participants read detailed instructions and completed practice example. They then progressed through the main task, providing responses for the eight claims in random order. They then responded to post-task questionnaire and optionally provided demographic information. Participants were paid 4.50 for taking part. The average completion time was 29m 54s, corresponding to payment rate of 9 per hour. 3.3.5 Data analysis. Quantitative analysis was performed using [57]. Alongside traditional quantitative analysis, we analysed themes in our qualitative data from think-aloud interviews and free-text response questions. Two authors open-coded all of the data independently, periodically comparing codes and reconciling differences until achieving full agreement. Open codes were then combined into thematic categories to aid analysis [7]."
        },
        {
            "title": "4.1 The role of explanations in claim",
            "content": "assessment Our initial hypothesis was that uncertainty explanations in natural language format would be more effective in supporting participants to evaluate whether to agree or disagree with the AI system. We 2https://gorilla.sc/ 7 analysed the extent to which participants used the AI systems prediction, depending on the explanation condition, correctness of the advice, and level of certainty of the system (see Table 4). Predictably, participants were more likely to follow the AI systems advice when it was correct vs. incorrect, F(1, 207)=208.779, <.001, ùúÇ2 ùëù =.50, and when AI certainty was high vs. low, F(1, 207)=55.55, <.001, ùúÇ2 ùëù =.21. Yet there was no difference in reliance based on the explanation condition F(2, 205)=1.825, p=.164, nor did this factor interact with advice correctness, F(2, 205)=0.1914, p=.9087, or certainty, F(2, 205)=0.5771, p=.749. We did not observe any effect of explanation condition on confidence or trust judgments, nor did prior familiarity with claims affect results (see App. for additional details). However, we observed main effect of explanation condition on how often people used the explanations (see Table 5), F(2, 205)=24.668, p<.001, ùúÇ2 ùëù =.19. Post hoc Tukey HSD tests showed that participants in the two natural language explanation groups reported using the explanations in their decisions significantly more than the No Explanation group (Uncertainty: p<.001, d=.99, Verdict: p<.001, d=1.07). People judged that both Uncertainty (p=.003, d=.5) and Verdict (p=.013, d=.46) Explanations were more helpful than No Explanation and that the Verdict Explanations were more useful than No Explanation, p=.01, d=.34. Further, participants reported using the AI explanations more frequently when the AI system was correct (M=55.5%) than when it was incorrect (M=48.1%), F(1, 207)=12.125, p<.001, ùúÇ2 ùëù =.055. No differences emerged between the two natural language explanation groups (p>.05 for all comparisons). There was main effect of explanation condition on how useful people judged the explanations, F(2, 205)=4.412, p=.013, ùúÇ2 ùëù =.02. Bonferroni-corrected post hoc tests indicated that the Verdict Explanation group rated the explanations as more useful than No Explanation, p=.01, d=.34. Participants also judged the explanations to be more useful when the AIs advice was correct than when it was incorrect, F(1, 207)=81.53, <.001, ùúÇ2 ùëù =.28, and when the AI systems certainty was high rather than low, F(1, 207)=66.868, p<.001, ùúÇ2 ùëù =.24. In the qualitative responses, participants complained about the lack of information in the No Explanation condition, although several mentioned using numerical uncertainty as cue to be more sceptical. Participants in the natural language explanation conditions often found the explanations useful as summaries of evidence when they aligned with evidence. When explanations and evidence were misaligned, however, participants commented that this helped them understand the AI systems reasoning, aligning with prior research [64]: Sometimes by reading the explanations could spot errors in the AIs logic. Qualitative data also suggested reasons for why the natural language explanations did not differ along expected dimensions. Nearly all qualitative responses in the No Explanation condition mentioned using evidence to assess the claim, suggesting that having access to evidence documents compensated for differences in explanation types. Participants in the Uncertainty condition had mixed opinions on the utility of these explanations. Some found them difficult to understand: on some of the choices the AI was very helpful but in others found it very contradictory and confused. This suggests Measure All conditions Uncertainty Explanation Verdict Explanation No Explanation 54.56 73.44 35.70 62.62 46.51 Use of AI prediction (%) Overall Correct AI advice Incorrect AI advice High AI certainty Low AI certainty Explanation usefulness (17) Overall Correct AI advice Incorrect AI advice High AI certainty Low AI certainty Explanation helpfulness (15) Overall 4.93 5.30 4.56 5.22 4. 3.67 51.79 71.43 32.14 57.86 45.71 4.98 5.28 4.68 5.15 4.81 3.80 57.79 76.09 39.49 65.58 50.00 5.12 5.47 4.76 5.43 4. 3.88 54.17 72.83 35.51 64.49 43.84 4.70 5.16 4.24 5.07 4.33 3.32 Table 4: Mean (i) use of AI prediction responses, (ii) explanation usefulness, and (iii) explanation helpfulness judgments by participants in each condition. that more research on how to effectively explain uncertainty in natural language is necessary."
        },
        {
            "title": "4.2 The role of evidence in claim assessment\nParticipants reported using the evidence most frequently (67.8%\noverall), regardless of explanation condition, correctness of AI ad-\nvice, or level of AI certainty. A one-way repeated measures ANOVA\nindicated differences in how often participants used each infor-\nmation source in their decisions, F(5, 1035)=200.6, p<.001, ùúÇ2\nùëù =.49.\nRegarding the proportion of evidence documents opened, partic-\nipants in the No Explanation group were more likely to open all\nevidence documents (83%), compared to the explanation groups\n(55% Uncertainty, p=.001, d =.62; 57% Verdict, p=.003, d=.58). This\nsuggests that natural language explanations may often have offered\nsufficient information to make a judgment based on the explanation\nalone or a single evidence document. Mirroring the overall trend,\nparticipants in the Verdict-focused explanation condition relied\nmore on the evidence than all other information sources, and relied\non the explanations second most often, more so than the remaining\ninformation sources, p<.001 for all comparisons. Participants in\nthe Uncertainty explanation condition, however, reported using\nthe evidence and explanations equally as often, p=.241. They re-\nlied on these two information sources significantly more than the\nothers, p<.001 for all comparisons. Participants in the No Expla-\nnation condition used evidence significantly more than any other\ninformation source, p <.001 for all comparisons. The ‚Äòexplanations‚Äô\n(restatements of AI verdict and certainty) were no more useful here\nthan the AI verdict (p=.408), AI certainty (p =.911), or their own\nknowledge (p=.028; not significant on the Bonferroni-corrected\nalpha).",
            "content": "The qualitative data revealed interesting dynamics with respect to evidence. While evidence generally dominated as the most important information for evaluating claims, common criticism was the absence of source information for the evidence documents. People commented that without source information they found it difficult to fully trust the evidence. In some cases, people tried to reverse 8 engineer sources by paying attention to format, language, and, most often, the use of statistics. In fact, the use of statistics was at times perceived as more credible: if the evidence had actual numbers and data from respectable sources then usually would side with the AIs verdict. Given that there was no source information in the evidence documents on whether the sources were respectable the credibility was clearly gained through the presence of numbers alone. This has implications for what kinds of evidence AI systems might present and suggests caution in how statistical evidence might be interpreted."
        },
        {
            "title": "4.3 The role of subjective trust in reliance on AI",
            "content": "advice, explanations, and evidence We investigated the relationships between participants subjective trust judgments and their propensity to rely on (i) AI advice and use (ii) AI explanations and (iii) the underlying evidence in their decisions. We found moderate correlation between peoples subjective trust in the AI system and their tendency to rely on the AIs recommendation (r=.425, p<.001), and trust predicted higher agreement with AI advice, ùõΩ=0.91, SE=.135, p<.001, ùëÖ2=.181. Participants with higher trust were more likely to follow AI recommendations. When we examined between group differences, we found that all explanation conditions exhibited negative correlation between trust and AI agreement (Uncertainty: r=-0.35, p=0.003; Verdict: r=-0.17, p=0.176; No explanation: r=-0.35, p=0.003). Participants trust in the AI system was also correlated with their tendency to use the AI explanations and underlying evidence in their decisions, although the effects were relatively small. Our analysis showed small positive correlation between participants subjective trust judgments and the frequency with which they used the AI explanation to make their final decision (r=.26, p<.001), and trust predicted higher usage of AI explanations, ùõΩ=0.838, SE=0.214, p<.001, ùëÖ2=.069. On the other hand, trust in the AI system was negatively correlated with usage of evidence (r=-.29, p<.001), with trust predicting lower reliance on evidence, ùõΩ=-1.015, SE=0.234, p<.001, ùëÖ2=.084. In other words, usage of evidence was associated Measure No Uncertainty Explanation Explanation 37.50 40.00 34.42 59.46 34.78 27.14 67.75 64.82 25.54 19.64 1.63 3.21 Table 5: Mean percentages of each source of information used. Verdict Explanation 45.83 61.41 30.98 71.01 15.76 3. All conditions 41.10 51.80 31.00 67.80 20.30 2.88 AI Verdict AI Explanation AI Certainty Evidence Own knowledge Other with lower trust in the AI system, while usage of explanations was associated with higher trust. Our qualitative findings corroborated the association between low trust in the AI system and reliance on evidence: didnt trust it so had to check what it claimed. Participants also provided insight to how trust in the AI was shaped by how accurately the outcome reflected the evidence: considered whether the AIs decision was based off the evidence, if their decision contradicted the evidence, this makes me feel that the AI was less trustworthy as it had not acknowledged the evidence properly. People in the Explanation conditions also discussed using logical fallacies in explanations as cues to how trustworthy the systems decision was. Some participants in the Uncertainty condition reported using the sources of uncertainty to judge when to trust the system less. This suggests that, when evidence is provided, explanations can play an important role in helping to determine whether the system has interpreted it correctly."
        },
        {
            "title": "5.1 Availability is key for evidence engagement\nPrevious work has shown that when AI systems provide people with\nlinks to external sources, relatively few (10-28%) actually click the\nlinks [31, 32], and it remains unclear to what extent people meaning-\nfully engage with the content of these sources. In contrast, approxi-\nmately two-thirds of participants in the present study opened all of\nthe available evidence documents and almost all opened at least one\nfor each claim. Several factors may account for this difference. First,\nthe evidence in our study was embedded directly within the task\ninterface. Participants could click and read the evidence alongside\nthe claim, AI prediction, and explanation, instead of following a\nlink to an external webpage (e.g., [32]), which likely reduced fric-\ntion and effort associated with accessing the evidence. Second, the",
            "content": "9 explanations explicitly referenced the evidence, which may have encouraged participants to use it to verify the AI systems interpretation, or made it easier to identify the key parts of the evidence more efficiently. However, we observed that people reported using the evidence in their decisions regardless of whether explanations were provided. It may also be the case that the fact-checking context of our task may have been perceived as more high-stakes than the question-answering setups in previous studies [31, 32], motivating participants to consider the information provided to them more carefully, though we note that click rates were extremely low (7%) even for potentially high-stakes health stimuli [31]. These differences indicate that more research is needed to understand what kind of evidence is needed, in what context, and what are the best approaches to make it available to people using AI systems. Participants were also sensitive to inconsistencies between the verdict and the natural language explanation, but even where the explanation and the verdict aligned, inconsistencies between explanation and evidence alerted them to critically consider AI system output. As suggested by Kim et al. [32], the provision of sources themselves may also lead to more deliberative reasoning."
        },
        {
            "title": "5.2 Evidence-focused explanations may",
            "content": "mitigate overreliance On one hand, our findings suggest more optimistic state of affairs about peoples reliance on and critical engagement with AI system explanations. Prior studies in AI-supported fact-checking have highlighted that natural language explanations in the form of high-level abstractive summaries, designed to simulate how humans reason about claim veracity, can persuade and mislead people to incorrect conclusions [61, 64]. However, the natural language explanations in our study, which extracted relevant parts of the evidence and explained how they contributed to the AI systems predicted verdict or certainty, did not lead participants to over-rely on incorrect advice in comparison to the No Explanation group. This may indicate that explanations that focus on the evidence make it easier for people to identify inconsistencies, flawed argumentation or other cues that the explanation may be misleading. However, we also highlight potential for misuse of these explanations: cherry-picking evidence or otherwise using evidence out of context to falsely push certain narrative could be highly persuasive. Although our qualitative data suggested that our participants were relatively alert to when this occurred, this may be less feasible when evidence documents are longer or more complex. Future work in HCI is needed to examine how we can improve systems and explanations that allow people to detect when plausible-sounding explanations provide incorrect or misleading reasoning. Further research should also examine evidence and explanation evaluation in more naturalistic, less structured task contexts and with domain experts who may interact differently with explanations than crowdworkers [61, 68]."
        },
        {
            "title": "5.3 Design implications for",
            "content": "information-seeking & decision-support systems The role of evidence in AI-assisted decision-making has received relatively little attention in the literature to date. There exists some theoretical research on the use of evidence in explanations, such as the evaluative AI framework [48], which builds on an earlier interpretability approach that utilised the information theory concept of weight of evidence [46]. However, few studies have considered the role of evidence empirically or developed methods that use it along with AI predictions and advice. Many commercial LLM chatbots already provide people with sources and evidence documents alongside AI output; some automatically, while others require further inquiry about sources. However, there is currently little consensus on best practices. When current LLM implementations do give sources, LLMs struggle with factuality, i.e., reliably producing factual information [4], and often hallucinate references to external sources [1], highlighting key gaps for further development of NLP and information retrieval tools to support AI-assisted fact-checking and other complex decision-making tasks. Research in natural language explanations has largely focused on the production of fluent explanations (e.g., [3, 36]) and how positively users evaluate them [60]. Our findings call for efforts in NLP research to focus not only on how to produce explanations, but also consider the role of evidence in explaining model behaviour to the people who use them. Concurrently, HCI research is needed to gain fine-grained understanding of how evidence should be presented, and how evidence is evaluated and used by people in different AI-supported decision-making contexts."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was co-funded by the European Union (ERC, ExplainYourself, 101077481), and supported by the Pioneer 10 Centre for AI, DNRF grant number P1. Views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. References [1] Hussam Alkaissi and Samy McFarlane. 2023. Artificial hallucinations in ChatGPT: implications in scientific writing. Cureus 15, 2 (2023). doi:10.7759/cureus. 35179 [2] Emily Andrews, Nathan Walter, and Erga Atad. 2025. Should We Retire the Concept of Source Credibility? An Experimental Exploration of When Credibility Is (and Is Not) Useful. Science Communication (2025), 10755470251334094. doi:10. 1177/10755470251334094 [4] [3] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating Fact Checking Explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online, 73527364. doi:10.18653/v1/2020.acl-main.656 Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni Zagni. 2024. Factuality challenges in the era of large language models and opportunities for fact-checking. Nature Machine Intelligence (2024), 112. doi:10.1038/s42256-02400881-z [5] Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi, Ece Kamar, Marco Tulio Ribeiro, and Daniel Weld. 2021. Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI 21). Association for Computing Machinery, New York, NY, USA, Article 81, 16 pages. doi:10.1145/3411764.3445717 [6] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual Event, Canada) (FAccT 21). Association for Computing Machinery, New York, NY, USA, 610623. doi:10.1145/3442188.3445922 [7] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology. Qualitative research in psychology 3, 2 (2006), 77101. [8] Zana Bu√ßinca, Maja Barbara Malaya, and Krzysztof Z. Gajos. 2021. To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AIassisted Decision-making. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 188 (April 2021), 21 pages. doi:10.1145/ [9] Zana Bu√ßinca, Siddharth Swaroop, Amanda E. Paluch, Finale Doshi-Velez, and Krzysztof Z. Gajos. 2025. Contrastive Explanations That Anticipate Human Misconceptions Can Improve Human Decision-Making Skills. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 1024, 25 pages. doi:10.1145/3706598.3713229 [10] Krzysztof Budzy≈Ñ, Marcin Roma≈Ñczyk, Diana Kitala, Pawe≈Ç Ko≈Çodziej, Marek Bugajski, Hans O. Adami, Johannes Blom, Marek Buszkiewicz, Natalie Halvorsen, Cesare Hassan, et al. 2025. Endoscopist deskilling risk after exposure to artificial intelligence in colonoscopy: multicentre, observational study. The Lancet Gastroenterology & Hepatology (2025). doi:10.1016/S2468-1253(25)00133-5 [11] Teodor Chiaburu, Frank Hau√üer, and Felix Bie√ümann. 2024. Uncertainty in xai: Human perception and modeling approaches. Machine Learning and Knowledge Extraction 6, 2 (2024), 11701192. doi:10.3390/make6020055 [12] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel Ho. 2024. Large legal fictions: Profiling legal hallucinations in large language models. Journal of Legal Analysis 16, 1 (2024), 6493. doi:10.1093/jla/laae003 [13] Laurence Dierickx, Arjen Van Dalen, Andreas L. Opdahl, and Carl-Gustav Lind√©n. 2024. Striking the balance in using LLMs for fact-checking: narrative literature review. In Multidisciplinary International Symposium on Disinformation in Open Online Media. Springer, 115. doi:10.1007/978-3-031-71210-4_1 [14] Mary T. Dzindolet, Scott A. Peterson, Regina A. Pomranky, Linda G. Pierce, and Hall P. Beck. 2003. The role of trust in automation reliance. International journal of human-computer studies 58, 6 (2003), 697718. doi:10.1016/S1071-5819(03)00038-7 [15] Motahhare Eslami, Kristen Vaccaro, Min Kyung Lee, Amit Elazari Bar On, Eric Gilbert, and Karrie Karahalios. 2019. User Attitudes towards Algorithmic Opacity and Transparency in Online Reviewing Platforms. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI 19). Association for Computing Machinery, New York, NY, USA, 114. doi:10.1145/3290605.3300724 [16] Harry Barton Essel, Dimitrios Vlachopoulos, Albert Benjamin Essuman, and John Opuni Amankwa. 2024. ChatGPT effects on cognitive skills of undergraduate students: Receiving instant responses from AI-based conversational large language models (LLMs). Computers and Education: Artificial Intelligence 6 (2024), 100198. doi:10.1016/j.caeai.2023.100198 [17] Franz Faul, Edgar Erdfelder, Axel Buchner, and Albert-Georg Lang. 2009. Statistical power analyses using G* Power 3.1: Tests for correlation and regression analyses. Behavior research methods 41, 4 (2009), 11491160. doi:10.3758/BRM.41.4.1149 [18] Andrea Ferrario and Michele Loi. 2022. How Explainability Contributes to Trust in AI. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT 22). Association for Computing Machinery, New York, NY, USA, 14571466. doi:10.1145/3531146.3533202 [19] Shannon Gallagher, Jasmine Ratchford, Tyler Brooks, Bryan Brown, Eric Heim, William Nichols, Scott Mcmillan, Swati Rallapalli, Carol Smith, Nathan VanHoudnos, et al. 2024. Assessing llms for high stakes applications. In Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice. 103105. doi:10.1145/3639477.3639720 Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. 2024. Bias and Fairness in Large Language Models: Survey. Computational Linguistics 50, 3 (Sept. 2024), 10971179. doi:10.1162/coli_a_00524 [20] [21] Zhijiang Guo, Michael Schlichtkrull, and Andreas Vlachos. 2022. Survey on Automated Fact-Checking. Transactions of the Association for Computational Linguistics 10 (2022), 178206. doi:10.1162/tacl_a_00454 [22] Lovisa Hagstr√∂m, Sara Vera Marjanovic, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, and Isabelle Augenstein. 2025. Reality Check on Context Utilisation for Retrieval-Augmented Generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (Eds.). Association for Computational Linguistics, Vienna, Austria, 1969119730. doi:10.18653/v1/2025.acl-long.968 [23] Sree Harsha Tanneru, Chirag Agarwal, and Himabindu Lakkaraju. 2024. Quantifying Uncertainty in Natural Language Explanations of Large Language Models. In Proceedings of The 27th International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research, Vol. 238), Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li (Eds.). PMLR, 10721080. https://proceedings.mlr.press/v238/harsha-tanneru24a.html [24] Gaole He, Nilay Aishwarya, and Ujwal Gadiraju. 2025. Is Conversational XAI All You Need? Human-AI Decision Making With Conversational XAI Assistant. In Proceedings of the 30th International Conference on Intelligent User Interfaces (IUI 25). Association for Computing Machinery, New York, NY, USA, 907924. doi:10.1145/3708359.3712133 [25] Yoyo Tsung-Yu Hou and Malte F. Jung. 2021. Who is the Expert? Reconciling Algorithm Aversion and Algorithm Appreciation in AI-Supported Decision Making. Proc. ACM Hum.-Comput. Interact. 5, CSCW2, Article 477 (Oct. 2021), 25 pages. doi:10.1145/3479864 [26] Lujain Ibrahim, Katherine M. Collins, Sunnie S. Y. Kim, Anka Reuel, Max Lamparth, Kevin Feng, Lama Ahmad, Prajna Soni, Alia El Kattan, Merlin Stein, Siddharth Swaroop, Ilia Sucholutsky, Andrew Strait, Q. Vera Liao, and Umang Bhatt. 2025. Measuring and mitigating overreliance is necessary for building human-compatible AI. arXiv:2509.08010 [cs.CY] https://arxiv.org/abs/2509.08010 [27] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, Article 248 (March 2023), 38 pages. doi:10.1145/3571730 [28] A.D. Kaplan, T.T. Kessler, and P.A. Hancock. 2020. How Trust is Defined and its use in Human-Human and Human-Machine Interaction. Proceedings of the Human Factors and Ergonomics Society Annual Meeting 64, 1 (2020), 11501154. doi:10.1177/1071181320641275 [29] Charalampia Xaroula Kerasidou, Angeliki Kerasidou, Monika Buscher, and Stephen Wilkinson. 2022. Before and beyond trust: reliance in medical AI. Journal of medical ethics 48, 11 (2022), 852856. doi:10.1136/medethics-2020-107095 [30] Kyungha Kim, Sangyun Lee, Kung-Hsiang Huang, Hou Pong Chan, Manling Li, and Heng Ji. 2024. Can LLMs Produce Faithful Explanations For Factchecking? Towards Faithful Explainable Fact-Checking via Multi-Agent Debate. arXiv:2402.07401 [cs.CL] https://arxiv.org/abs/2402. [31] Sunnie S. Y. Kim, Q. Vera Liao, Mihaela Vorvoreanu, Stephanie Ballard, and Jennifer Wortman Vaughan. 2024. \"Im Not Sure, But...\": Examining the Impact of Large Language Models Uncertainty Expression on User Reliance and Trust. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (Rio de Janeiro, Brazil) (FAccT 24). Association for Computing Machinery, New York, NY, USA, 822835. doi:10.1145/3630106.3658941 [32] Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo, and Olga Russakovsky. 2025. Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 420, 19 pages. doi:10.1145/3706598.3714020 [33] Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, and Juho Kim. 2024. Understanding Users Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level. In Proceedings of the 29th International Conference on Intelligent User Interfaces (Greenville, SC, USA) (IUI 24). Association for Computing Machinery, New York, NY, USA, 385404. doi:10.1145/3640543.3645148 [34] Emir Konuk, Robert Welch, Filip Christiansen, Elisabeth Epstein, and Kevin Smith. 2024. framework for assessing joint human-AI systems based on uncertainty estimation . In proceedings of Medical Image Computing and Computer Assisted Intervention MICCAI 2024, Vol. LNCS 15010. Springer Nature Switzerland. doi:10.1007/978-3-031-72117-5_1 [35] Neema Kotonya and Francesca Toni. 2020. Explainable Automated Fact-Checking for Public Health Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Online, 77407754. doi:10.18653/v1/2020.emnlp-main.623 [36] Neema Kotonya and Francesca Toni. 2020. Explainable Automated Fact-Checking for Public Health Claims. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 77407754. doi:10.18653/v1/2020.emnlp-main.623 [37] Min Hun Lee and Chong Jun Chew. 2023. Understanding the Effect of Counterfactual Explanations on Trust and Reliance on AI for Human-AI Collaborative Clinical Decision Making. Proc. ACM Hum.-Comput. Interact. 7, CSCW2, Article 369 (Oct. 2023), 22 pages. doi:10.1145/ [38] Q. Vera Liao and Wai-Tat Fu. 2014. Expert voices in echo chambers: effects of source expertise indicators on exposure to diverse opinions. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Toronto, Ontario, Canada) (CHI 14). Association for Computing Machinery, New York, NY, USA, 27452754. doi:10.1145/2556288.2557240 [39] Gionnieve Lim and Simon T. Perrault. 2024. XAI in Automated Fact-Checking? The Benefits Are Modest and Theres No One-Explanation-Fits-All. In Proceedings of the 35th Australian Computer-Human Interaction Conference (Wellington, New Zealand) (OzCHI 23). Association for Computing Machinery, New York, NY, USA, 624638. doi:10.1145/3638380.3638388 [40] Rhema Linder, Sina Mohseni, Fan Yang, Shiva Pentyala, Eric Ragan, and Xia Ben Hu. 2021. How level of explanation detail affects human performance in interpretable intelligent systems: study on explainable fact checking. Applied AI Letters 2, 4 (2021), e49. doi:10.1002/ail2.49 [41] Bingjie Liu. 2021. In AI we trust? Effects of agency locus and transparency on uncertainty reduction in humanAI interaction. Journal of computer-mediated communication 26, 6 (2021), 384402. doi:10.1093/jcmc/zmab013 [42] Nelson Liu, Tianyi Zhang, and Percy Liang. 2023. Evaluating Verifiability in Generative Search Engines. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 70017025. doi:10.18653/ v1/2023.findings-emnlp.467 [43] Xingyu Liu, Li Qi, Laurent Wang, and Miriam Metzger. 2023. Checking the Fact-Checkers: The Role of Source Type, Perceived Credibility, and Individual Differences in Fact-Checking Effectiveness. Communication Research (2023), 00936502231206419. doi:10.1177/ [44] Marta Marchiori Manerba, Karolina Stanczak, Riccardo Guidotti, and Isabelle Augenstein. 2024. Social Bias Probing: Fairness Benchmarking for Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 1465314671. doi:10.18653/v1/2024.emnlp-main.812 [45] D. Harrison McKnight, Vivek Choudhury, and Charles Kacmar. 2002. The impact of initial consumer trust on intentions to transact with web site: trust building model. The journal of strategic information systems 11, 3-4 (2002), 297323. doi:10.1016/S0963-8687(02)00020-3 [46] David Alvarez Melis, Harmanpreet Kaur, Hal Daum√© III, Hanna Wallach, and Jennifer Wortman Vaughan. 2021. From human explanation to model interpretability: framework based on weight of evidence. In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, Vol. 9. 3547. doi:10.1609/hcomp.v9i1.18938 [47] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence 267 (2019), 138. doi:10.1016/j.artint.2018.07.007 [48] Tim Miller. 2023. Explainable AI is Dead, Long Live Explainable AI! Hypothesisdriven Decision Support using Evaluative AI. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT 23). Association for Computing Machinery, New York, NY, USA, 333342. doi:10.1145/3593013.3594001 [49] Sebasti√£o Miranda, David Nogueira, Afonso Mendes, Andreas Vlachos, Andrew Secker, Rebecca Garrett, Jeff Mitchel, and Zita Marinho. 2019. Automated Fact Checking in the News Room. In The World Wide Web Conference (San Francisco, CA, USA) (WWW 19). Association for Computing Machinery, New York, NY, USA, 35793583. doi:10.1145/3308558.3314135 [51] [50] Marvin Pafla, Kate Larson, and Mark Hancock. 2024. Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of Human and Machine Explanations for Large Language Models. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI 24). Association for Computing Machinery, New York, NY, USA, Article 839, 20 pages. doi:10.1145/ 3613904.3642934 Ioannis Papantonis and Vaishak Belle. 2025. Why not both? Complementing explanations with uncertainty, and self-confidence in human-AI collaboration. Frontiers in Computer Science 7 (2025), 1560448. doi:10.3389/fcomp.2025.1560448 [52] Amalie Brogaard Pauli, Isabelle Augenstein, and Ira Assent. 2025. Measuring and Benchmarking Large Language Models Capabilities to Generate Persuasive Language. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), Luis Chiruzzo, Alan Ritter, and Lu Wang (Eds.). Association for Computational Linguistics, Albuquerque, New Mexico, 1005610075. doi:10.18653/v1/2025.naacl-long.506 [53] Kashyap Popat, Subhabrata Mukherjee, Andrew Yates, and Gerhard Weikum. 2018. DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (Eds.). Association for Computational Linguistics, Brussels, Belgium, 2232. doi:10.18653/v1/D18-1003 [54] Forough Poursabzi-Sangdeh, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman Vaughan, and Hanna Wallach. 2021. Manipulating and Measuring Model Interpretability. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. ACM, New York, NY, USA, 167. doi:10.1145/ 3411764.3445315 [55] Snehal Prabhudesai, Leyao Yang, Sumit Asthana, Xun Huan, Q. Vera Liao, and Nikola Banovic. 2023. Understanding Uncertainty: How Lay Decision-makers Perceive and Interpret Uncertainty in Human-AI Decision Making. In Proceedings of the 28th International Conference on Intelligent User Interfaces (Sydney, NSW, Australia) (IUI 23). Association for Computing Machinery, New York, NY, USA, 379396. doi:10.1145/3581641.3584033 [56] Dorian Quelle and Alexandre Bovet. 2024. The perils and promises of factchecking with large language models. Frontiers in Artificial Intelligence 7 (2024), 1341697. doi:10.3389/frai.2024. [57] Core Team. 2021. R: Language and Environment for Statistical Computing. Foundation for Statistical Computing, Vienna, Austria. https://www.R-project. org/ [58] Vincent Robbemond, Oana Inel, and Ujwal Gadiraju. 2022. Understanding the Role of Explanation Modality in AI-assisted Decision-making. In Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization (Barcelona, Spain) (UMAP 22). Association for Computing Machinery, New York, NY, USA, 223233. doi:10.1145/3503252.3531311 [59] Max Schemmer, Niklas Kuehl, Carina Benz, Andrea Bartos, and Gerhard Satzger. 2023. Appropriate Reliance on AI Advice: Conceptualization and the Effect of Explanations. In Proceedings of the 28th International Conference on Intelligent User Interfaces (Sydney, NSW, Australia) (IUI 23). Association for Computing Machinery, New York, NY, USA, 410422. doi:10.1145/3581641.3584066 [60] Michael Schlichtkrull, Nedjma Ousidhoum, and Andreas Vlachos. 2023. The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 86188642. doi:10.18653/v1/2023.findings-emnlp.577 [61] Vera Schmitt, Luis-Felipe Villa-Arenas, Nils Feldhus, Joachim Meyer, Robert P. Spang, and Sebastian M√∂ller. 2024. The Role of Explainability in Collaborative Human-AI Disinformation Detection. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (Rio de Janeiro, Brazil) (FAccT 24). Association for Computing Machinery, New York, NY, USA, 21572174. doi:10.1145/3630106.3659031 [62] Claude E. Shannon. 1948. Mathematical Theory of Communication. Bell System Technical Journal 27, 34 (1948), 379423, 623656. doi:10.1002/j.15387305.1948.tb01338.x [63] Donghee Shin. 2021. The effects of explainability and causability on perception, trust, and acceptance: Implications for explainable AI. International journal of human-computer studies 146 (2021), 102551. doi:10.1016/j.ijhcs.2020.102551 [64] Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng, Hal Daum√© III, and Jordan L. Boyd-Graber. 2024. Large Language Models Help Humans Verify Truthfulness Except When They Are Convincingly Wrong. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL): Long Papers. Association for Computational Linguistics, Mexico City, Mexico, 14591474. doi:10.18653/v1/2024.naacl-long.81 [65] Felix M. Simon, Rasmus Kleis Nielsen, and Richard Fletcher. 2025. Generative AI and News Report 2025: How People Think About AIs Role in Journalism and Society. Technical Report. Reuters Institute for the Study of Journalism, University of Oxford. doi:10.60625/risj-5bjv-yt69 [66] Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer Karny, Xinyue Hu, Lukas Mayer, and Padhraic Smyth. 2025. What large language [67] models know and what people think they know. Nature Machine Intelligence (2025), 111. doi:10.1038/s42256-024-00976-7 Jingyi Sun, Greta Warren, Irina Shklovski, and Isabelle Augenstein. 2025. Explaining Sources of Uncertainty in Automated Fact-Checking (CLUE). arXiv preprint (2025). arXiv:2505.17855 https://arxiv.org/abs/2505.17855 [68] Maxwell Szymanski, Martijn Millecamp, and Katrien Verbert. 2021. Visual, textual or hybrid: the effect of user expertise on different explanations. In Proceedings of the 26th International Conference on Intelligent User Interfaces (College Station, TX, USA) (IUI 21). Association for Computing Machinery, New York, NY, USA, 109119. doi:10.1145/3397481.3450662 [69] Dennis Ulmer, Jes Frellsen, and Christian Hardmeier. 2022. Exploring Predictive Uncertainty and Calibration in NLP: Study on the Impact of Method & Data Scarcity. In Findings of the Association for Computational Linguistics: EMNLP 2022, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 27072735. doi:10.18653/v1/2022.findings-emnlp.198 [70] Helena Vasconcelos, Matthew J√∂rke, Madeleine Grunde-McLaughlin, Tobias Gerstenberg, Michael S. Bernstein, and Ranjay Krishna. 2023. Explanations Can Reduce Overreliance on AI Systems During Decision-Making. Proceedings of the ACM on Human-Computer Interaction 7, CSCW1 (April 2023), 138. doi:10.1145/ 3579605 [71] Greta Warren, Irina Shklovski, and Isabelle Augenstein. 2025. Show Me the Work: Fact-Checkers Requirements for Explainable Automated Fact-Checking. In Proceedings of the CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI 25). Association for Computing Machinery, New York, NY, USA, Article 197, 21 pages. doi:10.48550/arXiv.2502. [72] Chunpeng Zhai, Santoso Wibowo, and Lily Li. 2024. The effects of over-reliance on AI dialogue systems on students cognitive abilities: systematic review. Smart Learning Environments 11, 1 (2024), 28. doi:10.1186/s40561-024-00316-7 [73] Yunfeng Zhang, Q. Vera Liao, and Rachel K. E. Bellamy. 2020. Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* 20). Association for Computing Machinery, New York, NY, USA, 295305. doi:10.1145/3351095.3372852 Think-Aloud Study Participants ID Age Gender Background TI T2 T3 T4 T5 Physics International Relations Computer Science Translation Sustainability Female Female Male Female Male 24 58 33 66 Table 6: Demographics of participants in think-aloud study Think-Aloud Study Protocol Background knowledge How familiar are you with artificial intelligence or AI? Have you used any AI systems or LLMs like ChatGPT? How familiar are you with fact-checking? Introduction to think-aloud study In this experiment, youll be testing an AI system designed to help with fact-checking. Youll see some examples of claims that the AI system has been given, and its prediction about whether the claim is true or false. Youll see some more detailed instructions once you begin the study, and wed like you to describe what youre thinking as you read through the information you see, for example if something is unclear or confusing. Then, during the main study, as youre evaluating the AI system and making decisions about the claims, wed like 12 you to also describe your impressions and thought processes, and talk through your decisions as you make them. At some points, may ask you to explain little more about why you did something. Then, at the end of the study Ill ask you some questions about how you found the study overall. Think-aloud prompts (to be asked if participant doesnt verbalise their thoughts) What was your first impression when you saw this screen? Why did you check [the evidence/explanation/AI certainty]? How did you decide whether the claim was true or false? What did you base your decision on? Why were you confident (or not) in your decision? Why did you find the explanation (not) useful? What was (not) useful about it? Did the AIs certainty score impact your decision in any way? Post-study interview How did you find the task overall? Was anything unclear or confusing about what you were asked to do? Did you find anything difficult about the task? You saw different styles of explanation in the study. Were they helpful? Why/why not? Subjective Evaluation Scales C.1 Explanation Helpfulness To what extent were the AI system explanations helpful in determining how reliable the system was? (1 = Not at all helpful, 2 = Somewhat helpful, 3 = Neither, 4 = Somewhat helpful, 5 = Extremely helpful) C.2 Trust Belief and Intention Scales On this page you are asked to provide your opinion on the AI system you used in the experiment. Please select one answer for each statement (1 = Strongly disagree, 2 = Somewhat disagree, 3 = Neither, 4 = Somewhat agree, 5 = Strongly agree) *Reverse-scored items (1) The AI system is competent and effective in fact-checking claims (2) Overall, the AI system is capable and proficient information provider (3) would characterise the AI system as honest (4) The AI system is NOT truthful in providing information to me* (5) believe that the AI system was developed to act in my (or the users) best interest (6) The AI system was developed with good intentions to do its best to help me (or the users) (7) When an important claim arises, would NOT feel comfortable depending on the information provided by the AI system* (8) can always rely on the AI system to find information about claims (9) would feel comfortable acting on the information given to me by the AI system (10) would not hesitate to use the information the AI system supplied to me C.3 Claim familiarity In the study, you saw 8 different real life claims, some of which you may have come across some of these claims before in your day-to-day life. Please indicate whether you had read or come across any of these claims **before this experiment**, and if so, how much you knew about them. No knowledge = had never come across this claim before Limited knowledge = had not come across this specific claim before but had previously come across something similar Some knowledge = had come across this claim before but could not be sure that knew whether it was true/false Full knowledge = had come across this claim and was confident that knew it was true/false Additional analyses We collected data on how familiar participants were with the claims prior to the experiment, and on participants confidence and trust judgements during the experiment. These variables did not appear to impact the main findings of the study, but we report them here for completeness and transparency. D.1 Claim familiarity On average, people were unfamiliar with the claims prior to the study 58% of the time, had some knowledge 14% of the time, had limited knowledge 24.94% of the time, and had full knowledge of the claims 2.5% of the time. People relied more on their own knowledge when they had some knowledge of the claim than when they did not, t(384)=6.87, p<.001, otherwise, no differences emerged in how people used the information available to them (p>.05 for all comparisons). Verdict explanations were rated as more useful than no explanations when people had no familiarity with the claims, ùúí 2(2)= 8.22, p=.016. Otherwise, claim familiarity did not impact in how people judged the explanations, or how much they agreed with the AI system (p>.05 for all comparisons, see Table 7). D.2 Confidence and Subjective trust D.2.1 Confidence. There was no impact of explanation condition on how confident people were in their decisions, F(2, 205)=0.428, p=.652 (see Table 8). Participants were more confident in their decisions when the AIs advice was correct than when it was incorrect, F(1, 207)=6.122, p=.014, ùúÇ2 ùëù =.03, and when the AI systems certainty was high rather than low, F(1, 207)=27.886, p<.001, ùúÇ2 ùëù =.12. D.2.2 Trust. The explanation groups did not differ in their judgments for the Trust scales, F(2, 205)=.091, p=0.40. 13 Condition No Explanation Any knowledge Familiarity Verdict Uncertainty No knowledge Any knowledge No knowledge Any knowledge No knowledge (#responses) Agreement (%) Confidence (1-7) Usefulness (1-7) 230 322 206 338 258 302 53.5 55.7 55.3 59.2 50.8 52.6 5.78 5.79 5.62 5.73 5.86 5.71 4.83 4.60 5.04 5.24 4.96 4. Table 7: Participant AI Agreement, confidence, and usefulness ratings for claims that people had either (a) any prior knowledge of or (b) no prior knowledge of, across explanation conditions. Measure Uncertainty Explanation Verdict Explanation No Explanation Confidence (17) Overall Correct AI advice Incorrect AI advice High AI certainty Low AI certainty Subjective trust (1050) Overall 5.78 5.80 5.76 5.87 5.68 5.70 5.81 5.59 5.88 5.52 5.78 5.86 5.70 5.88 5.68 32.0 Table 8: Mean confidence and trust judgments by participants in each condition. 33.9 33."
        },
        {
            "title": "E Uncertainty Estimation and Explanation",
            "content": "Generation Method E.1 Uncertainty Estimation By curating the instruction, we guided the model to generate the verdict along with the explanations, see Appendix E.2 and E.3 for the prompts. Based on the models verdict prediction, we derived its uncertainty as follows. To quantify model uncertainty for verdict label on input ùëã (one claim plus two evidence passages), we compute the predictive entropy [62, 69] of the softmax distribution over logits, which requires only single forward pass and is widely used. For candidate labels = {True, False, NEI} with logits ‚Ñì (ùë¶ùëñ ), the probability of label ùë¶ùëñ is ùëÉ (ùë¶ùëñ ùëã ) = exp(cid:0)‚Ñì (ùë¶ùëñ )(cid:1) ùëó=1 exp(cid:0)‚Ñì (ùë¶ ùëó )(cid:1) (cid:205) . (1) The uncertainty score for ùëã is the entropy of this distribution, ùë¢ (ùëã ) = ùë¶ùëñ ùëÉ (ùë¶ùëñ ùëã ) log ùëÉ (ùë¶ùëñ ùëã ), (2) E.2 Few-shot Prompt for Verdict Explanation"
        },
        {
            "title": "Generation",
            "content": "E.3 Few-shot Prompt for Uncertainty"
        },
        {
            "title": "Explanation Generation",
            "content": "For the uncertainty explanation generation, we used three-shot prompt to instruct the model to generate the verdict, along with the reasons to forward its verdict prediction certainty by referring to the key span interactions extracted in the previous step, following [67], see the prompts in Table 10. Task Instructions Screen 1 Welcome to the experiment! What is this study about? This study is about AI systems and fact-checking. Fact-checking is the process of verifying the truthfulness of claims, statements, or stories by comparing them against credible evidence and sources. In this study, you will see examples of an AI system designed to help people with fact-checking. Your task will be to carefully read and evaluate the information provided by the AI system. You will see and evaluate 8 different claims in total. After you have evaluated these claims, you will answer some questions about your experiences of and opinions on the AI system and demographics. You will have limited time to evaluate each claim. The main study will take 25-30 minutes to complete, while the questionnaire will take 5-10 minutes. Before starting, make sure your browser window is set to full screen. Please make sure you are sitting somewhere comfortable and quiet so that you can complete the experiment in one sitting! Click Next to read the instructions for the task. Screen 2 What do have to do? Imagine your job is to release assessments about whether claim is true or false. You will see claims, an AI systems prediction about whether this claim is true or false, how certain the system is about its label, and an explanation of the AI systems prediction. These explanations are intended to help you decide how to interpret the AI prediction. You have the option to see the evidence that the AI system used to make its prediction. Your task is to decide whether you can rely on the AI system prediction or need to do more research before releasing the assessment of the claim. For the verdict explanation generation, we leveraged three-shot prompt for instructing the model to generate the verdict along with the reasons towards this verdict, see Table 9. Important: Please only consider the provided information claim, verdict, uncertainty, explanations & evidence documents (if applicable) when evaluating explanations. Sometimes you will be Task Description: You are helpful assistant. Here are your tasks: 1. Supply the verdict for the claim based on the relationship between the claim and the two evidence passages. 2. Explain your reasoning for the verdict prediction by identifying the three most influential span interactions from Claim-Evidence 1, Claim-Evidence 2, and Evidence 1-Evidence 2, and describing how each interaction (agree, disagree, or unrelated) affects your overall prediction. Return format: [ Verdict Prediction ] [ Explanation ] SHOT 1 Input Claim: Winston Churchill once stated, pessimist sees the difficulty in every opportunity; an optimist sees the opportunity in every difficulty. Evidence 1: . . . pessimist, says the Rev. F. W. Cole, president of the Cardiff Free Church Council, . . . Evidence 2: Winston Churchills quote,A pessimist sees the difficulty in every opportunity; an optimist sees the opportunity in every difficulty, . . . Verdict Prediction: False Explanation: The parts of the evidence with the most impact on my predicted verdict are: The evidence in Evidence 1, says the Rev. F. W. Cole, conflicts with the part of the claim Winston Churchill once stated; the evidence in Evidence 2 that Winston Churchills quote, pessimist sees the difficulty in every opportunity . . . aligns with the claims attribution; taken together, the attribution in Evidence 1 (Rev. F. W. Cole) versus Evidence 2 (Winston Churchill) is inconsistent, which weighs against the claim. Claim: During the last hundred years the temperature is increased about 0.1C because of carbon dioxide. The human contribution was about 0.01C. Evidence 1: The likely range of total human-caused global surface temperature increase . . . is 0.8C to 1.3C (best estimate 1.07C [2.01 F]). Over this period, well-mixed greenhouse gases (GHGs) contributed warming of 1.0C to 2.0C, and other human drivers (principally aerosols) contributed cooling of 0.0C to 0.8C, natural (solar and volcanic) drivers changed global surface temperature by 0.1C to +0.1C, and internal variability changed it by 0.2C to +0.2C. Evidence 2: After using bunch of nonsense calculations to say that 0.1 warming is from CO2, they say that 90% of the change in CO2 is caused by the oceans. . . . the oceans are actually absorbing CO2 . . . their conclusions cannot be supported. Verdict Prediction: False Explanation: The parts of the evidence with the most impact on my predicted verdict are: Evidence 1 reports human-caused warming near 1C overall and 1.02.0C from well-mixed greenhouse gases, contradicting the claims about 0.1C because of carbon dioxide and about 0.01C human contribution; Evidence 2 challenges the assertion that 0.1 warming is from CO2, which also goes against the claim; the segment in Evidence 1 about aerosol cooling and natural drivers (0.1C to +0.1C) is unrelated to Evidence 2s point about oceans absorbing CO2. Claim: Vitamin appears increase COVID-19 mortality rates Evidence 1: If vitamin does in fact reduce the severity of COVID-19 in regard to pneumonia/ARDS, inflammation, inflammatory cytokines and thrombosis, . . . supplements would offer relatively easy option to decrease the impact of the pandemic. Evidence 2: Vitamin deficiency that is not sufficiently treated is associated with COVID-19 risk. Verdict Prediction: False Explanation: The parts of the evidence with the most impact on my predicted verdict are: Evidence 1 indicates that vitamin reduces COVID-19 severity, which opposes the claim that vitamin increases mortality; Evidence 2 links untreated vitamin deficiency with higher COVID-19 risk, which also runs against the claim; Evidence 1s indication of benefit from adequate vitamin is consistent with Evidence 2s association between deficiency and risk, jointly weighing against the claim. Claim: { CLAIM } Evidence 1: { E1 } Evidence 2: { E2 } Your answer: Table 9: Verdict explanation generation prompt SHOT 1 Output SHOT 2 Input SHOT 2 Output SHOT 3 Input SHOT 3 Output NEW INSTANCE familiar with the claim, but we ask you to approach each claim as new, whether or not you have seen it before. It doesnt matter whether you personally agree or disagree with the claim or evidence. We are asking you to evaluate what the AI produces: if you were to see this claim for the first time, would you find the 15 explanation provided by the AI useful? Would you be willing to rely on the AI verdict to make statement about the claim? Task Description: You are helpful assistant. Here are your tasks : 1. Supply the verdict for the claim based on the relationship between the claim and the two evidence passages. 2. Explain your certainty for your verdict prediction by referring to the three span interactions provided below ( Claim-Evidence 1 , Claim-Evidence 2 , Evidence 1-Evidence 2) and describing how each interactions relation (agree, disagree, or unrelated) affects your overall certainty . Return format: [ Verdict Prediction ] [ Explanation ] SHOT 1 Input Claim: Winston Churchill once stated, pessimist sees the difficulty in every opportunity; an optimist sees the opportunity in every difficulty. Evidence 1: . . . pessimist, says the Rev. F. W. Cole, president of the Cardiff Free Church Council, . . . Evidence 2: Winston Churchills quote, pessimist sees the difficulty in every opportunity; an optimist sees the opportunity in every difficulty, . . . Span interactions (Pre-filled): (1) Winston Churchill once stated - says the Rev. F. W. Cole (C - E1) relation: [disagree]; (2) Winston Churchill once stated - Winston Churchills quote, pessimist sees the difficulty in every opportunity (C - E2) relation: [agree]; (3) pessimist, says the Rev. F. W. Cole, president of the Cardiff Free Church Council - Winston Churchills quote, pessimist sees the difficulty in every opportunity (E1 - E2) relation: [disagree] Verdict Prediction: False Uncertainty Explanation: The parts of the evidence with the most impact on my certainty in the prediction are: the claims phrase Winston Churchill once stated conflicts with Evidence 1s attribution to the Rev. F. W. Cole, which decreases my certainty in the claim; the same claim wording matches Evidence 2, which introduces the line as Winston Churchills quote, increasing my certainty; since one passage attributes the quote to Rev. Cole and the other to Churchill, the evidences disagree with each other, and that inconsistency lowers my overall certainty and supports predicting False. Claim: During the last hundred years the temperature is increased about 0.1C because of carbon dioxide. The human contribution was about 0.01C. Evidence 1: . . . well-mixed greenhouse gases (GHGs) contributed warming of 1.0C to 2.0C . . . Evidence 2: . . . 0.1 warming is from CO2 . . . cannot be supported. . . . Span interactions (Pre-filled): (1) temperature is increased about 0.1C because of carbon dioxide - well-mixed greenhouse gases (GHGs) contributed warming of 1.0C to 2.0C (C - E1) relation: [disagree]; (2) 0.1C warming is from CO2 - 0.1 warming is from CO2 . . . cannot be supported (C - E2) relation: [disagree]; (3) well-mixed greenhouse gases . . . 1.0C to 2.0C - 0.1 warming is from CO2 . . . cannot be supported (E1 - E2) relation: [agree] Verdict Prediction: False Uncertainty Explanation: The parts of the evidence with the most impact on my certainty in the prediction are: the claims about 0.1C because of carbon dioxide contrasts with Evidence 1s 1.0C to 2.0C contribution from well-mixed greenhouse gases, which pushes my certainty toward False; Evidence 2 explicitly states that 0.1 warming is from CO2 ... cannot be supported, further moving my certainty toward False; together, the larger range in Evidence 1 and the rejection of the 0.1 figure in Evidence 2 reinforce each other, increasing my certainty in the False prediction. SHOT 1 Output SHOT 2 Input SHOT 2 Output SHOT 3 omitted for space (structure identical to SHOT 1 and SHOT 2). NEW INSTANCE Claim: { CLAIM } Evidence 1: { E1 } Evidence 2: { E2 } Span interactions (Pre-filled): (1) { SPAN1 - } - { SPAN1 - } (C - E1) relation: { REL1 }; (2) { SPAN2 - } - { SPAN2 - } (C - E2) relation: { REL2 }; (3) { SPAN3 - } - { SPAN3 - } (E1 - E2) relation: { REL3 } Your answer: Table 10: Uncertainty explanation generation prompt Screen 3 About the AI System The AI System you will evaluate in this study is based on Large Language Model (LLM). This AI System has been designed to help people with fact-checking. It has been trained to predict whether the claim is true or false based on the evidence it has been provided. 16 The AI System also provides explanations for its outputs, which you will evaluate in this study. What information will see? (Definitions) claim is some statement about the world. It may be true, false, or somewhere in between. Evidence is additional information is typically necessary to verify the truthfulness of claim. An evidence document consists of one or several sentences extracted from an external source for the particular claim. In this study, you will have the option of reading two evidence document excerpts that have been retrieved for claim. These evidence document excerpts may or may not agree with each other. Please assume that all evidence documents are reasonably reliable. verdict is reached based on the available evidence, regarding whether claim is true or false. certainty score indicates how confident an AI system about the correctness of its prediction. For example, Certainty score of 99% indicates very high confidence that the predicted verdict is correct, but score of 1% indicates very low confidence that the predicted verdict is correct. An explanation for the AI systems prediction explains the sources of uncertainty regarding the verdict. In other words, the explanation highlights parts of the evidence that increase or reduce its certainty that its verdict is correct. Your TASK is to decide whether you can rely on the verdict about the claim produced by the AI system, based on the information available, and how confident you are in your judgment, and decide how helpful the explanation is in making your decision. On the next page, you will see an example of the task. Screen 4 (1) First, read the claim that the AI system has been given. (2) Next, read the verdict output by the AI system, along with its certainty that the output is correct. (3) Next, read the explanation for the AI systems output. Here, the explanation highlights parts of the evidence that increase or reduce its certainty that its verdict is correct. (4) If you want to, you can check the evidence that the AI system used to make its prediction, by clicking on Evidence 1 and/or Evidence 2. (5) Based on the information provided, decide whether you can rely on the verdict produced by the AI system. Try to disregard any prior knowledge or opinions you may have and focus only on the information presented here. Based on the information provided, decide whether you can rely on the verdict produced by the AI system. Try to disregard any prior knowledge or opinions you may have and focus only on the information presented here. (6) Indicate how confident you are about your decision whether to rely on the AI system verdict or not. (7) Rate how useful the AI system explanation above was for making your decision. (8) Select the sources of information that you based you final decision on. There is no limit to how many you can select. If you used other sources not listed here, you can type them in Other. (9) When you have completed all these steps, press Next to move on to the next claim. You have 300 seconds (5 minutes) to complete your decision for each claim. If the timer runs out before you finish, your response will not be counted. Screen 5 Thank you for completing the practice example! Now, youre ready to start the study. You will see 8 different claims in total. When youre ready, click Next to begin the study. Study Materials G.1 Claim 1 (True, Correct, High Certainty) Claim: 86% of Americans and 82% of gun owners support requiring all gun buyers to pass background check. AI Prediction: True AI Certainty: 76% Evidence 1. Statistic: 86% of Americans and 82% of gun owners support requiring all gun buyers to pass background check, no matter where they buy the gun and no matter who they buy it from. [...] One of the surveys questions was: Do you support or oppose requiring background check on all gun buyers? 86 percent of respondents said support, 7 percent said oppose, and 8 percent said unsure, according to the Giffords report on the poll. [...] March 2019 poll conducted by Quinnipiac University found that 87 percent of national gun owners support requiring background checks for all gun owners. The poll surveyed 1,120 national voters and had margin of error of plus or minus 3.4 percentage points. Evidence 2. Iowa voters will decide in 2022 whether to add state Constitution amendment affirming the right to bear arms. Support for the amendment has fallen along party lines. Opposing Democrats refer to poll that says an overwhelming number of even gun owners support background checks for gun buyers. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, 86% of Americans and 82% of gun owners support requiring all gun buyers to pass background check, aligns perfectly with the claim 86% of Americans and 18% of gun owners support requiring all gun buyers to pass background check. This consistency increases the certainty that the claim is true. 2. The agreement between the phrase gun buyers in the claim and gun in Evidence 1 indicates that the focus on background checks applies uniformly across gun-related transactions and increases the certainty in the prediction. 3. Lastly, the phrase support in the claim and owners in Evidence 1, while seemingly unrelated in terms of direct content, contribute to the overall coherence of the argument. The emphasis on the level of support among gun owners corroborates the claims specific statistic, and increases certainty about the alignment between the claim and the provided evidence. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 directly supports the claim, stating that 86% of Americans and 82% of gun owners support requiring all gun buyers to pass background check, no matter where they buy the gun and no matter who they buy it from. The poll details show that when asked, Do you support or oppose requiring background check on all gun buyers?, 86% responded support. This aligns exactly with the percentages in the claim. 2. Evidence 1 cites Quinnipiac University poll reporting that 87 percent of national gun owners support requiring background checks for all gun owners, which reinforces the claims assertion of broad support among both the general public and gun owners. 3. Evidence 2 does not give exact percentages but echoes the same conclusion. It states that an overwhelming number of even gun owners support background checks for gun buyers, which is consistent with the data presented in Evidence 1. While it is less specific, it corroborates the general finding of widespread support. No Explanation. The AI system has judged this claim to be True, with 76% certainty. G.2 Claim 2 (False, Correct, High Certainty) Claim: satanic-themed hotel is opening in Texas. AI Prediction: False AI Certainty: 67% Evidence 1. Baphomet bedside buddies, upside down crosses, spewing Satan sinks and devilish welcome, one Facebook user claimed satanic hotel would soon be opening in Plano, Texas. But dont pull out the rosaries and holy water yet, the hotel is not actually real. On Feb. 23, Facebook user claimed that satanic hotel was supposedly opening in an abandoned office building in the heart of the Downtown Plano Art District. [...] But the Baphomet-themed hotel wont be coming anywhere near Plano. The images were created using an AI art platform. According to The Buzz, an Instagram account posted photos showing how AI art can be made using specific algorithm. In this case, satanically-spooky hotel. Evidence 2. video shared on Facebook allegedly shows images of satanic-themed hotel that is set to open in Plano, Texas. [...] 18 Facebook video allegedly shows photos of Satanic-themed hotel that is set to open in Plano on June 6. The post shares photo of red bedroom with goat-like figure. This is satanic hotel located in Plano, Texas the caption reads. This brand new hotel will open June 6 at 6pm (666). The claim is fabricated. Check Your Fact found no credible news reports about satanic-themed hotel opening in Plano, Texas. The photo originates from Ink Poisoning, an apparel brand. The company posted the images on Facebook with disclaimer stating the images are AI concept art created by us. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, Baphomet bedside buddies, upside down crosses, spewing Satan sinks and devilish welcome, one Facebook user claimed satanic hotel would soon be opening in Plano, Texas, is somewhat related to the claim satanic-themed hotel is opening in Texas. This reduces the AI systems certainty that the claim is false. 2. The evidence in Evidence 1, the hotel is not actually real, is in direct contradiction to the claim satanic-themed hotel is opening in Texas. This statement increases the certainty that the claim is false by clarifying that the images and descriptions of the hotel are merely AI-generated concepts and not an actual establishment. 3. The agreement between the evidence in Evidence 1, that the images were created using an AI art platform, and the phrase supposedly opening increase AI system uncertainty that the claim is false. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 makes clear that the idea of satanic-themed hotel opening in Plano, Texas, comes from social media post and is not real. It explains that Facebook user claimed satanic hotel would soon be opening in Plano, Texas, but the article quickly clarifies that the hotel is not actually real. The supposed images of the Baphomet bedside buddies, upside down crosses, spewing Satan sinks and devilish welcome were not photos of real hotel but instead created using an AI art platform. 2. The Buzz further explained that the Instagram account that posted them did so to show how AI art can be made using specific algorithm. This confirms the images were fictional concept art, not evidence of an actual hotel. 3. Evidence 2 supports this conclusion. It describes how Facebook video claimed satanic-themed hotel. . . is set to open in Plano on June 6 and even added details like open June 6 at 6pm (666). However, the evidence directly states that the claim is fabricated and that no credible news reports support the idea of such hotel opening. It also identifies the true origin of the images: they came from Ink Poisoning, an apparel brand, which shared them with disclaimer noting they were AI concept art created by us. No Explanation. The AI system has judged this claim to be False, with 67% certainty. G.3 Claim 3 (True, Correct, Low Certainty) Claim: scone can equal third of ones recommended daily calories. AI Prediction: True AI Certainty: 35% Evidence 1. An average scone equals fifth of recommended daily calories for females, and sixth for males. The number of calories tends to be proportionate to the size of scone rather than the luxuriousness of its filling. Evidence 2. On 11 February 2019, headline of the Irish News and the Belfast Telegraph stated that One large scone can equal third of recommended daily calories. Both articles used Press Association report that referred to survey. [...] The report states that the highest calorie scone, without the addition of spread or jam, provides over third (38%) of the recommended daily calorie intake. [...] For people aged 19 to 64, the Government Dietary Recommendations suggest daily intake of 2,000kcal for females and 2,500kcal for males. The largest scone of the survey, then, indeed equals third of recommended calories; taking up 37.8% of daily female diet, and 30.2% of daily male diet. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, An average scone equals fifth of recommended daily calories for females, and sixth for males, aligns with the claim scone can equal third of ones recommended daily calories. Although the specific fractions differ, the core message that scone can be substantial contributor to daily calorie consumption is consistent, increasing the certainty that the claim is true. 2. The evidence in Evidence 2, without the addition of spread or jam, provides over third (38%) of the recommended daily calorie intake, corroborates the claim by providing concrete example where larger scone meets or exceeds the claims assertion. This specific instance strengthens the certainty that the claim is true. 3. The evidence in Evidence 2, the largest scone of the survey, then, indeed equals third of recommended calories; taking up 37.8% of daily female diet, and 30.2% of daily male diet, further reinforces the certainty that the claim is true by offering precise figures that closely match the claims statement. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 states that an average scone provides fifth of recommended daily calories for females, and sixth for males. This shows that even regular-sized scone contains significant share of daily energy needs. 2. Evidence 1 also emphasizes that calorie content is more closely tied to the size of the scone than to its filling, suggesting that larger scones will contribute an even higher proportion of daily calories. 3. Evidence 2 confirms that in some cases single scone can equal about third of recommended calories. It explains that the highest calorie scone, without the addition of spread or jam, provides over third (38%) of the recommended daily calorie intake. This is further broken down to 37.8% of daily female diet, and 30.2% of daily male diet, based on standard intake levels of 2,000 kcal for women and 2,500 kcal for men. No Explanation. The AI system has judged this claim to be True, with 35% certainty. 19 G.4 Claim 4 (False, Correct, Low Certainty) Claim: windmill could spin until it falls apart and never generate as much energy as was invested in building it. AI Prediction: False AI Certainty: 31% Evidence 1. No, professor didnt say windmill will never generate the energy invested in building it [...] Now, we have meme roaming social media that claims Canadian author and professor Thomas Homer-Dixon said that windmills will never generate as much energy as was expended to build the structures. [...] The meme does get part of the quote right, but it strategically leaves out large swaths of information in order to give people the impression that all windmills never expend the amount of energy that was invested in it, which is not accurate. The memes omitted section explains that, while poorly placed windmills may never generate enough energy payback, good wind site could generate it in three years or less. Evidence 2. Factually Inaccurate: Analyzing the energy used in the production of wind turbines does not show that its greater than the electricity produced over the turbines working lifetime. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, No, professor didnt say windmill will never generate the energy invested in building it, directly contradicts the claims assertion that windmills will never generate sufficient energy. This contradiction increases the AI systems certainty that the claim is False. 2. The evidence in Evidence 1, will never generate the energy invested, aligns with the part of the claim spin until it falls apart and never generate as much energy as was invested, reducing the AI systems certainty that the claim is False. 3. The evidence in Evidence 1, expended to, aligns with the phrase invested in building in the claim, indicating agreement on the energy expenditure aspect, reducing the certainty that the claim is False. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 explains that the claim comes from misrepresented quote. The meme states that windmills will never generate as much energy as was expended to build the structures, but the original quote clarifies that while poorly placed windmills may never generate enough energy payback, good wind site could generate it in three years or less. This shows the claim exaggerates and omits important context. 2. The same evidence highlights that well-sited windmills can generate enough energy to offset their construction cost quickly, making the claim that all windmills never generate enough energy inaccurate. The phrase good wind site could generate it in three years or less directly contradicts the claim. 3. Evidence 2 reinforces this, stating that analyzing the energy used in the production of wind turbines does not show that its greater than the electricity produced over the turbines working lifetime. This confirms that, over time, turbines produce more energy than was invested in building them. No Explanation. The AI system has judged this claim to be False, with 31% certainty. G.5 Claim 5 (True, Incorrect, High Certainty) Claim: All tea bags contain harmful microplastics. AI Prediction: True AI Certainty: 73% Evidence 1. At ORGANIC INDIA, we believe in plastic-free tea bags to protect your body and the earth; and preserve the pure, organic tea experience that you deserve. Unfortunately, the industry norm is to include plastic in the mesh tea bag, which releases billions of microplastics and nanoplastics into your infusion. Consuming microplastics, as you can imagine, can have negative impacts on your health and on the earth. [...] Yes, but not all tea bags. The vast majority of brands on the shelves have mesh tea bags that are composed of 20-30% plastic. According to studies, single standard tea bag releases 11.6 billion microplastics and 3.1 billion nanoplastics into every cup of tea. Consumers are increasingly becoming aware of this fact and opting either for loose teas, or selecting tea brands that have no plastic in the teabag. ORGANIC INDIA tea bags do not contain plastic, and are made instead of unbleached filter paper that is verified home compostable, biodegradable, Non-GMO, allergen-free and chemical-free. [...] In short, yes. Microplastics in tea bags can be harmful not only for the environment, but for human health as well. Microplastics may enter the human system through ingestion, inhalation or dermal absorption. Studies have shown that microplastics have potential to cause damage to the cells in the quantities currently found in food and beverages like tea. Evidence 2. Therefore, the use of plastic in tea bags has come under scrutiny due to the potential release of microplastics when exposed to heat. As we mentioned earlier, many tea bag manufacturers use nylon or polyethylene terephthalate (PET) in their tea bags, which are not biodegradable and can release harmful microplastics when exposed to heat. These microplastics can end up in your drink, and studies have shown that they may pose risk to human health. [...] Another study published in the journal Environmental Science & Technology in 2020 found that plastic tea bags could release up to 16 micrograms of microplastics per liter of tea when brewed at temperature of 95C. These studies highlight the importance of choosing plastic-free tea bags to minimize exposure to potentially harmful plastics. [...] Yes, some tea bags can contain microplastics, particularly those made with plastic materials like nylon or polypropylene. However, The Tea Spots sachets are made from sugar cane fibers, which are biodegradable, compostable, and completely free from petroleum-based plastic. Therefore, you can rest assured that when you brew our tea, youre not consuming any harmful plastics or microplastics. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The statement in Evidence 2, Therefore, the use of plastic in tea bags has come under scrutiny due to the potential release of microplastics when exposed to heat, does not support the blanket statement that All tea Bags contain harmful microplastics. This disagreement reduces the AI system certainty that the claim is True. 20 2. The statement in Evidence 1, Yes, but not all tea bags. The vast majority of brands on the shelves have mesh tea bags that are composed of 10-30% plastic, contradicts the claim by indicating that not all tea bags contain harmful microplastics, reducing certainty that the claim is True. 3. The statement in Evidence 1, ORGANIC INDIA tea bags do not contain plastic, and are made instead of unbleached filter paper that is verified home compostable, biodegradable, Non-GMO, allergen-free and chemical-free, undermines the claim by providing specific example of brand that produces tea bags without harmful microplastics, further reducing certainty that the claim is True. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 states that the vast majority of brands on the shelves have mesh tea bags that are composed of 2030% plastic. This shows that most commercially available tea bags contain plastic, making the exposure to microplastics nearly universal for consumers using standard tea bags. 2. Evidence 1 notes that single standard tea bag releases 11.6 billion microplastics and 3.1 billion nanoplastics into every cup of tea, and Evidence 2 confirms that materials like nylon or PET can release harmful microplastics when exposed to heat. Both sources directly link the presence of plastic in tea bags to microplastic contamination in brewed tea. 3. Evidence 1 explains that microplastics can be harmful not only for the environment, but for human health as well and may cause damage to the cells in the quantities currently found in food and beverages like tea. Evidence 2 echoes this, stating that microplastics may pose risk to human health. No Explanation. The AI system has judged this claim to be True, with 73% certainty. G.6 Claim 6 (False, Incorrect, High Certainty) Claim: Lagan Valley is one of the least wooded areas in Northern Ireland, particularly with regards to ancient woodland. AI Prediction: False AI Certainty: 78% Evidence 1. The Lagan Valley (Irish: Cluain an Lag√°in, Ulster Scots: Glen Lagan) is an area of Northern Ireland between Belfast and Lisburn. The Lagan is famous river that flows into Belfast Lough. For section, the river forms part of the border between the counties of Antrim and Down. It has number of interesting features including towpath which runs alongside the River Lagan. The towpath is popular with walkers, runners, cyclists, dog owners etc. It is very scenic and peaceful area and is ideal for walking, cycling etc. The towpath begins in the Stranmillis area of south Belfast and runs all the way to Lisburn. The cycle route forms part of National Cycle Route 9. There are number of off route mountain bike trails along the towpath. Lagan Valley is an Area of Outstanding Natural Beauty (AONB). The AONB was established in 1965 and the greater part of it lies within the Greater Belfast area. [...] Belvoir Park Forest Education Centre Evidence 2. Therefore, it is fair to say that Lagan Valley is one of the least wooded areas in NI. [...] Lisburn & Castlereagh is the least densely wooded council (3.7%) in Northern Ireland, followed by Armagh, Banbridge & Craigavon (4.0%) and Antrim & Newtownabbey (4.4%). [...] Based on all this, Ms Eastwoods claim that Lagan Valley is one of the least wooded areas in NI is backed by evidence although the precise figures are available broken down by council areas, rather than Westminster/Assembly constituencies. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, Lagan Valley is an Area of Outstanding Natural Beauty (AONB), does not directly address the claim about the level of woodlands in the area, reducing the certainty of the AI systems prediction. 2. The evidence in Evidence 2, Therefore, it is fair to say that Lagan Valley is one of the least wooded areas in NI, directly supports the claim that Lagan Valley is one of the least wooded areas in Northern Ireland, particularly with regards to ancient woodland. This agreement reduces the certainty that the claim is False. 3. The evidence in Evidence 2 that Lisburn & Castlereagh is the least densely wooded council (3.7%) in Northern Ireland aligns with the evidence in Evidence 1 that the Lagan Valley is an area of Northern Ireland between Belfast and Lisburn. This reduces the certainty that the claim is False. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 describes Lagan Valley as an Area of Outstanding Natural Beauty (AONB) and mentions that it has number of interesting features including towpath which runs alongside the River Lagan. It also emphasizes that the area is very scenic and peaceful and ideal for walking, cycling etc., suggesting the presence of natural green spaces rather than sparsely wooded landscape. 2. Evidence 2 notes that Lisburn & Castlereagh is the least densely wooded council (3.7%) in Northern Ireland, followed by Armagh, Banbridge & Craigavon (4.0%) and Antrim & Newtownabbey (4.4%). It adds that Ms Eastwoods claim that Lagan Valley is one of the least wooded areas in NI is backed by evidence although the precise figures are available broken down by council areas, rather than Westminster/Assembly constituencies, showing that the claim is not directly supported for Lagan Valley itself. 3. Neither piece of evidence provides specific data about ancient woodland in Lagan Valley. The lack of precise figures for the area means the claim about ancient woodland cannot be verified. NoExplanation. The AI system has judged this claim to be False, with 78% certainty. G.7 Claim 7 (True, Incorrect, Low Certainty) Claim: Cristiano Ronaldo was the only European soccer captain who did not wear rainbow armband at Euro 2020. AI Prediction: True AI Certainty: 29% Evidence 1. Cristiano Ronaldo is refusing to wear the One Love armband, which advocates for the LGBTQ+ community, during the 2022 World Cup. The world-renowned soccer player, Cristiano 21 Ronaldo, revealed in press conference that he will not be participating in wearing the One Love armband. The armband consists of heart with rainbow inside of it and the number one. This One Love band is being used to advocate inclusivity, being that the 2022 World Cup is being held in Qatar, country where samesex relationships are illegal. [...] Players who wear the armband are threatened with receiving yellow card, according to FIFAs regulations. This yellow card can result in suspension from the games if the player were to receive another yellow card during the game. Obviously, any mention of social issues is discouraged on the field. Due to the consequences, none of the captains from any of the other national teams have worn the armband, but none have faced backlash like Ronaldo has. Evidence 2. There is evidence that three team captains from Germany, England, and Denmark wore rainbow or pro-LGBTQ+ armbands during this tournament, which included 24 teams. Germanys captain, Manuel Neuer, was briefly investigated by the Union of European Football Associations for his rainbow armband. Cristiano Ronaldo was among many other captains who wore normal armbands. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, team captains from Germany, England, and Denmark wore rainbow or pro-LGBTQ+ armbands during this tournament, is not consistent with the part of the claim Cristiano Ronaldo was the only European soccer captain who did not wear rainbow armband at Euro 2020. This indicates that there were multiple captains who did not wear the armband, not just Ronaldo and reduces AI system certainty that the claim is True. 2. The evidence in Evidence 1, Cristiano Ronaldo was among many other captains who wore normal armbands, is not consistent with the part of the claim Cristiano Ronaldo was the only European soccer captain., implying he was not the only captain without rainbow armband, reducing certainty that the claim is True. 3. The evidence in Evidence 1, during this tournament, is not consistent with the part of the claim Euro 2020, as it refers to different event. This discrepancy highlights that the evidence discusses different tournament, possibly the 2022 World Cup, rather than Euro 2020 and reduces certainty that the claim is True. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 notes that Ronaldo is refusing to wear the One Love armband, which advocates for the LGBTQ+ community, and describes how he revealed in press conference that he will not be participating in wearing the One Love armband. The evidence emphasizes that while players who wear the armband are threatened with receiving yellow card, Ronaldos decision not to wear it drew notable attention, suggesting he stood out among team captains. The passage also clarifies that none of the captains from any of the other national teams have worn the armband at other tournaments, highlighting Ronaldos high-profile refusal. 2. Evidence 2 provides additional support, stating that three team captains from Germany, England, and Denmark wore rainbow or pro-LGBTQ+ armbands during this tournament. It also specifically mentions that Cristiano Ronaldo was among many other captains who wore normal armbands, directly confirming that he did not wear the rainbow armband while other European captains did. Students are worried about finding jobs, their future finances, and the growing cost of everything, reducing AI system certainty that the claim is False. Verdict Explanation. The parts of the evidence with the most impact on the AI systems predicted verdict are: 1. Evidence 1 reports that, according to polling by Young Americas Foundation, the economy was the top issue for college students who participated in the groups poll. While this appears to support the claim, the evidence also raises the question: Is Walker right that The top issue for college students is the economy? suggesting uncertainty about whether this finding is representative of all college students. 2. Evidence 2 provides more detailed data, showing that college students are concerned about multiple issues. It states that the economy and abortion rights are the top issues of concern for college students, with 37% ranking the economy and 33% ranking abortion rights as their primary concern. The evidence further breaks this down by gender: college men chose the economy as their top issue (44%), whereas abortion rights were the most important issue for college women (43%), with women of color expressing the deepest concern. This demonstrates that the economy is not universally the top issue across all college students. No Explanation. The AI system has judged this claim to be False, with 49% certainty. No Explanation. The AI system has judged this claim to be True, with 29% certainty. G.8 Claim 8 (False, Incorrect, Low Certainty) Claim: The top issue for college students is the economy. AI Prediction: True AI Certainty: 49% Evidence 1. The everyday concerns of Americans are the economy even young people that we work with at Young Americas Foundation, our polling shows nationwide, their top issue for college students is the economy, Walker said. [...] Is Walker right that The top issue for college students is the economy? [...] The economy was the top issue for college students who participated in the groups poll. Wisconsin pollster Charles Franklin said the polling company is reputable and the questions were overall evenhanded. Evidence 2. The midterm election is right around the corner and the economy and abortion rights are the top issues of concern for college students according to new survey by BestColleges.com. These issues are also ranked as the most important for Americans overall. BestColleges.com surveyed over 1000 students about their political beliefs and plans to vote. Thirty-seven percent (37%) of those surveyed noted the economy, inflation, and employment as the most important issues in this election. Students are worried about finding jobs, their future finances, and the growing cost of everything from housing to peanut butter. White college men, in particular, are overwhelmingly concerned about the economy, employment, and inflation, with almost half (49%) selecting these three issues as their top political issues significantly higher percentage than other demographic groups surveyed. Abortion rights, which have been the subject of heated national and local political campaigns, were ranked as top concern by college students with 33% of those surveyed ranking these rights as top issue. College women were particularly concerned about legislation having an impact on their bodies. While college men chose the economy as their top issue (44%), abortion rights were the most important issue for college women (43%), with women of color expressing the deepest concern. Only 21% of college men considered abortion rights an important issue. For college men, the economy, gun policy, healthcare, racial and ethnic inequality, and climate change all came out ahead of abortion rights as leading issues. Uncertainty Explanation. The parts of the evidence with the most impact on the AI systems certainty in the prediction are: 1. The evidence in Evidence 1, The economy was the top issue for college students who participated in the groups poll, is consistent with the claim The top issue for college students is the economy, reducing AI system certainty that the claim is False. 2. The evidence in Evidence 2, abortion rights were the most important issue for college women (43%) contrasts with the claim, increasing certainty that the claim is False. 3. The statement in Evidence 1, The everyday concerns of Americans are the economy, is consistent with the part of Evidence 2,"
        }
    ],
    "affiliations": [
        "Link√∂ping University",
        "University of Copenhagen"
    ]
}