{
    "paper_title": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments",
    "authors": [
        "Xuweiyi Chen",
        "Wentao Zhou",
        "Zezhou Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass."
        },
        {
            "title": "Start",
            "content": "WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments Xuweiyi Chen, Wentao Zhou, Zezhou Cheng University of Virginia https://wild-rayzer.cs.virginia.edu/ 6 2 0 2 5 ] . [ 1 6 1 7 0 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present WildRayZer, self-supervised framework for novel view synthesis (NVS) in dynamic environments, where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, causing ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with single feed-forward pass. 1. Introduction Self-supervised learning from large-scale unlabeled data with minimal handcrafted inductive bias has driven major advances in LLMs [31, 57] and visual understanding [18, 49] and generation [53]. similar trend is only beginning to emerge in 3D vision. While recent 3D models achieve strong performance [20, 80, 83], they often rely on accurate 3D annotations such as point maps and camera poses, or on explicit 3D inductive biases in representations such as NeRF [46] and 3D Gaussian Splatting [32]. These requirements limit scalability, robustness in dynamic environments, and out-of-distribution generalization. Motivated by this, recent work has explored reducing or removing explicit 3D supervision or inductive bias. For example, LVSM [29] proposes transformer-based view synthesis model with minimal 3D inductive bias. NoPoSplat [92] eliminates the need for camera pose at inference; Figure 1. Our self-supervised WildRayZer learns to render static novel views from dynamic images without any 3D or GT mask supervision. It extends the state-of-the-art self-supervised large view synthesis model RayZer to dynamic environments by adding learned motion mask estimator and masked 3D scene encoder. SPFSplat [22] and Splatt3R [71] removes pose supervision during training. Building on LVSM, RayZer [27] introduces self-supervised framework for sparse-view novel view synthesis that does not require any 3D supervision. Despite recent progress [47, 98], prior work still relies on fundamental assumption: the 3D scene is static, as illustrated in Figure 1(a). These models require static inputs during both training and inference, yet real-world 3D environments are inherently dynamic. As result, existing methods rely on static scene datasets such as RealEstate10K [100], which restricts scalability and prevents full use of abundant in the wild videos with natural dynamic content. In this work, we propose WildRayZer, self-supervised learning framework for novel view synthesis in dynamic environments from sparse, unposed views. Similar to prior efforts that adapt NeRF and 3DGS to in-the-wild settings [33, 45, 64, 89, 99], WildRayZer extends RayZer to dynamic scenes while removing dynamic objects in the final renderings. As in RayZer, WildRayZer takes unposed multiview images as input, reconstructs the 3D scene implicitly, and renders novel views (See Figure 1(a)); Differently, our inputs include both camera motion and object dynamics (See Figure 1(b)). Achieving this capability requires 1 solving two key challenges. First, how can we localize dynamic objects without any groundtruth dynamic mask supervision? WildRayZer adopts an analysis-by-synthesis strategy: static renderer predicts what rigid scene should look like, and deviations from this prediction identify dynamic regions. Specifically, we begin with pretrained RayZer and derive pseudo motion masks from its rendering error, computed using combination of DINOv3 [70] features and SSIM [86]. motion mask head is then distilled from these masks. Inspired by prior work in instance segmentation [16, 84], we further improve the robustness of this module using simple copy paste augmentation strategy to enrich dynamic mask supervision with synthetic examples. Second, how can we train and evaluate our model on this task when existing large-scale 3D datasets capture mainly static scenes? Common real-world datasets for novel view synthesis  (Table 1)  contain only static scenes and therefore cannot support systematic training or evaluation of methods that separate static structure from dynamic objects. To enable controlled studies of this setting, we construct Dynamic RealEstate-10K, as natural extension of RealEstate-10K, which contains over 15K casually captured indoor sequences with moving cameras and moving objects such as humans and pets. We also collect benchmark with paired transient and clean views of the same scenes for evaluation. Experiments and qualitative results on both existing datasets and our newly collected benchmarks show that WildRayZer outperforms prior baselines in novel view synthesis and dynamic motion prediction in large margin. Our main contributions are summarized as follows: We propose WildRayZer, self-supervised learning framework for novel view synthesis in dynamic environments without any supervision on camera poses or dynamic region masks, with sparse unposed views as input. We collect Dynamic RealEstate-10K, large-scale video dataset captured in dynamic scenes, as natural extension of RealEstate-10K, complementary to the commonly used real-world video datasets for 3D vision tasks. Our experiments show the superior performance of the WildRayZer in both NVS from sparse dynamic inputs and the motion segmentation tasks. 2. Related Work Optimization-based Novel View Synthesis. NeRF [46] introduced neural volumetric 3D representation with differentiable volume rendering, enabling neural scene reconstruction by minimizing photometric rendering loss and achieving remarkable novel view synthesis quality. However, classical NeRF pipelines critically rely on clean camera poses and static scenes, which severely limits their applicability in realistic, in-the-wild settings. This has motivated line of work on robust NVS that relaxes these assumptions and seeks to make NeRF viable in the wild [10, 45, 62, 66, 68, 76]. Beyond these, explicit warping pipelines [93] and early pose-free NVS [40] offer alternatives, but can be brittle under in-the-wild dynamics settings. More recently, 3D Gaussian Splatting further improves rendering quality and speed by representing scenes with explicit 3D Gaussians and fast rasterization, but inherits the same reliance on accurate poses and static scenes. Another line of work trains Gaussian splatting in the wild by modeling appearance variation with learned appearance embeddings, per-primitive color adaptation, or spatial appearance fields [12, 85, 89, 95], typically down-weighting high-error pixels following the robust NeRF line of work [45]. In contrast, we perform NVS with large transformer-based renderer without specific 3D representation such as NeRF or 3DGS, and explicitly target dynamic, in-the-wild scenarios. More Generalizable Novel View Synthesis. Generalizable methods enable fast NVS by training neural networks across many scenes to predict novel views or an underlying 3D representation in single forward pass. Early work such as PixelNeRF [94], MVSNeRF [9], and IBRNet [82] predicts volumetric 3D features from input views, leveraging strong 3D inductive biases like epipolar geometry and plane-sweep cost volumes. Subsequent methods improve robustness under sparse views [25, 26, 30, 41, 74], while others extend these ideas to 3DGS-based scene representations [8, 11, 75, 77]. recently, reconstruction models (LRMs) [20, 43, 81, 88, 96, 102] adopt scalable transformer architectures trained on large-scale data to learn generic 3D priors. LVSM [29] goes step further by largely removing hand-crafted 3D inductive biases and learning powerful token-space renderer, leading to improved fidelity and scalabilitybut still assumes known poses and static imagery. RayZer [27] follows similar transformer-based direction and introduces self-supervised, pose-free framework for sparse-view NVS, yet remains restricted to static scenes. Our method builds on this line of work and explicitly lifts the static-scene assumption, targeting dynamic, in-the-wild inputs where both cameras and objects move. 3D large Moving Object Segmentation. Most moving-object segmentation approaches are explicitly video-based and fall into three families. (a) Flow-based methods [6, 50, 69, 87] detect motion by grouping optical-flow cues; (b) Trajectorybased approaches [2, 19, 21, 23, 28, 48, 90] reason over multi-frame geometric or point trajectories; (c) Unsupervised Video Object Segmentation (UVOS) [24, 39, 54, 55, 63, 84, 91, 101] typically segments salient objects, and thus can capture static entities rather than true movers. Instead of operating on explicit video sequences and point trajectories, we derive motion evidence from what static multi-view renderer cannot explain. This learned mask predictions become reliable under sparse-view unposed setting. 2 3. Dynamic RealEstate10K Dataset #Seq. Real? Dynamic? Large? Scene Type We target novel view synthesis from dynamic, in-the-wild videos where both the camera and scene undergo motion. Rather than relying on controlled captures, we mine diverse handheld footage from public sources. key observation is that user-generated real-estate walkthroughs naturally provide abundant multi-view coverage of cluttered indoor scenes. Unlike prior static NVS datasets, which aggressively filter out motion, we intentionally retain clips containing humans, pets, and object interactions to expose transient dynamics. As shown in Table 1, static NVS datasets are large, but existing dynamic datasets remain extremely limited in size, often containing less then ten sequences due to their optimization-heavy pipelines. To close this gap, we collect large-scale dynamic scene video dataset, namely Dynamic RealEstate10K (D-RE10K), which consists of 15K real indoor sequences. Following standard practice for web-mined datasets (e.g., WebVid-2M [4], RealEstate10K [100]), we will release annotations, metadata, and video links under CC license and we have anonymized faces using EgoBlur [58]. 3.1. Data Curation Our curation pipeline proceeds in three stages: (1) Source identification. We start by querying YouTube channels that primarily publish real-estate walkthroughs and indoor pet-interaction videos. These videos provide smooth handheld motion, stable lighting, and cluttered but mostly rigid backgrounds, which is ideal for multi-view reconstruction. We filter based on metadata to exclude overly short or low-quality uploads. (2) Image-level filtering. From each candidate clip, we uniformly sample frames and assess visual quality using image quality assessment [73], discarding low-quality or heavily compressed videos. We further apply an OCR-based text detector [3] to remove videos containing intrusive overlays (e.g., subtitles, watermarks, real-estate banners). (3) Sequence extraction. NVS requires geometric coherence within localized region. We apply TransNet V2 [72] to detect scene cuts and discard transitions. Within each shot, we estimate camera trajectories using DPVO [79] and subdivide clips based on translation magnitude to ensure sufficient parallax and consistent motion. The resulting sequences exhibit stable camera motion and meaningful dynamic content. 3.2. Benchmark Construction We build two evaluation splits: (1) the D-RE10K motionmask benchmark, providing motion annotations for Internet videos, and (2) D-RE10K-iPhone, real-world paired transient/clean dataset for sparse-view transient-aware NVS. D-RE10K Motion Mask. To evaluate transient-region estimation performance on Internet videos, we derive per-frame DL3DV [38] RealEstate10K [100] CO3D [61] Matterport3D [7] Objaverse-XL [13] Plenoptic Video [34] D-NeRF [56] NSFF [35] Nerfies [51] HyperNeRF [52] DyCheck [15] RobustNeRF [66] NeRF-On-the-Go [64] WildGS-SLAM [99] T-3DGS [44] 10K 80K 19K 10K 10M 6 8 8 9 17 14 5 12 10 5 D-RE10K (Ours) 15K Mixed Mixed Indoor Object Indoor Object Indoor Object Mixed Object Object Object Object Mixed Indoor Indoor Indoor Table 1. Common datasets for novel view synthesis. Large? marks datasets with 10K sequences. Static NVS datasets are large, but dynamic ones are typically tiny. D-RE10K closes this gap with 15K real, in-the-wild and dynamic indoor sequences featuring diverse transient objects (people, pets, clutter), enabling training for transient-aware NVS at scale. motion masks by fusing coarse cues from MegaSAM [36] with image segmentation from SAM2 [60]. The fused masks are human-verified for accuracy. We curate 25 sequences for validation and 74 for testing. On the test split, we use ground-truth to analysis mask quality. Moreover, we report masked-PSNR, masked-SSIM, and maskedLPIPS [97] restricted to static regions in order to ensure fair comparison of NVS quality. D-RE10K-iPhone. Although dynamic NVS datasets exist, such as NeRF-On-the-Go [64] and WildGS-SLAM [99], their train and test splits are often captured under distinct camera trajectories and viewpoint. In our sparse-view regime with only 24 input views, reliable evaluation requires substantial spatial overlap between inputs and targets; otherwise, reconstruction errors become entangled with coverage gaps rather than modeling quality. Moreover, datasets like RobustNeRF [66] contain only two highly controlled tabletop scenes, making it difficult to draw conclusions about performance in real, in-the-wild environments. To address this, we construct D-RE10K-iPhone, 50sequence benchmark for real-world, sparse-view transientaware NVS (i.e., handling transient objects). Using tripod-mounted iPhone, we reposition the camera between views to ensure geometric diversity and, for each viewpoint, capture pair of images: one containing transient object or multiple transient objects (e.g., person, vehicle) and one without, triggered via Bluetooth to minimize pose drift. The resulting matched pairs span kitchens, studies, dining rooms, and living spaces with paintings and decor, enabling fine-grained evaluation of transient-region reconstruction and full-frame fidelity under realistic conditions. Additional details are provided in the appendix. 3 Figure 2. WildRayZer self-supervised learning framework. (a) Training. WildRayZer takes unposed, uncalibrated multi-view dynamic images and predicts per-view camera parameters (intrinsics and relative poses), which are converted into pixel-aligned Plucker ray maps R. camera-only static renderer explains the rigid background; residuals between renderings ˆIB and targets IB highlight dynamic regions, which are sharpened by our pseudo-motion mask constructor (see Section 4.2 and Figure 3). We distill motion estimator from these pseudo-masks and use it to gate dynamic image tokens before scene encoding; the same pseudo-masks also gate dynamic pixels in the photometric rendering loss. (b) Inference. Given dynamic input views I, the model predicts camera parameters, motion masks, and static scene representation in single feed-forward pass. The motion estimator operates on the input views to mask dynamic tokens, and the renderer synthesizes transient-free novel views given the inferred scene representation and target camera. 4. Methodology In this section we detail the WildRayZer training pipeline. We first review RayZer in Section 4.1, then present the details of the WildRayZer architecture and learning scheme in Section 4.2. 4.1. Preliminaries RayZer [27] is self-supervised, transformer-based tokenspace renderer for novel view synthesis in static scenes, trained without any 3D supervision (no ground-truth geometry or camera poses). The input is set of static, unposed, uncalibrated images = {Ii RHW 3 = 1, . . . , K}, where is the image resolution and is the number of input views. Each Ii is split into non-overlapping ss patches (patch size s), and linearly embedded following ViT [14] into tokens fi Rhwd, where = H/s, = W/s denote the patch-grid height and width, and is the token embedding dimension. RayZer uses the sum of sinusoidal spatial and shared image-index positional encoding [5, 14]. camera estimator Ecam predicts per-view rigid poses Pi SE(3) and shared intrinsics K. RayZer splits into two disjoint subsets, IA (inputs) and IB (targets). The scene reconstructor Eencode encodes IA into scene representation RLd with scene tokens, conditioned on PA = {(Pi, K) Ii IA}. rendering decoder Drender then synthesizes the held-out views using transformer decoder [29]. Given and tokenized Plucker rays derived from PB = {(Pi, K) Ii IB}, the decoder produces rendered targets ˆIB. The self-supervised objective compares rendered and ground-truth targets: = 1 IB (cid:88) (cid:16) ˆI ˆIB MSE(I, ˆI) + λ Percep(I, ˆI) (cid:17) , (1) where each ˆI is paired with its ground-truth IB, and Percep(, ) is perceptual loss. 4.2. WildRayZer Overview. RayZer recovers cameras and scene representation from unposed, uncalibrated inputs but assumes static imagery. WildRayZer lifts this restriction: given dynamic, in-the-wild inputs, it aims to render the underlying static 3D scene by disentangling transient object motion from camera motion in fully self-supervised manner. Concretely, we augment RayZer with motion estimator Emot placed alongside the camera estimator Ecam, scene encoder Escene, and renderer Drender. To prevent dynamic content from contaminating the static scene tokens, we adopt an alternating optimization schedule: first learn motion masks while freezing the renderer stack, and then learn masked renderer while freezing the motion head. Once the masks are sufficiently reliable, we jointly fine-tune all components. Throughout training, there is no supervision from ground-truth poses, depth, or semantic labels; all pseudo-labels are derived from discrepancies between rendered and observed images, and external backbones (e.g., DINOv3) remain frozen. 4 To illustrate the pipeline, we show the learning framework in Figure 2. When training the motion estimator, Ecam, Escene, and Drender are fixed. For each held-out view IB, the static renderer predicts target ˆI, and we construct soft pseudo motion mask (I) [0, 1]HW by treating appearance and feature-space residuals between and ˆI as evidence of dynamics. The motion head Emot predicts per-pixel logits S(I) RHW , which we train to match (I) using standard BCE-with-logits objective. In the complementary phase, when training the renderer, the motion estimator is frozen. For each input view IA, we convert S(I) to probabilities, downsample to the patch grid, threshold to obtain binary patch mask Π {0, 1}hw, and zero out dynamic token positions in the fused token map before encoding with Escene. This yields static scene representation that is explicitly purged of transient content, and we then optimize the reconstruction loss in Eq. (1) on the held-out targets IB. Finally, to improve robustness to open-set distractors, we apply simple copypaste augmentation [16, 84]: COCO [37] objects with ground-truth masks are pasted into training views and their masks are treated as additional (I), providing precise synthetic transients without altering cameras or the rendering architecture. At this stage, we train the model end to end with augmented data and dynamic data, as illustrated in Figure 2. Motion Estimator. The motion estimator Emot predicts, for each image I, per-pixel logit map S(I) RHW . To obtain robust motion probability map under noisy pseudo labels, we fuse three complementary signals that are all aligned on the token grid: (a) DINOv3 [70] patch features, (b) RayZer image tokens, and (c) Pluckerray tokens derived from (P, K). For each token position (i, j), we take the corresponding DINOv3 feature, image token, and ray token, apply LayerNorm and learned linear projection to shared width d, and concatenate them. small fusion MLP then produces unified token representation that serves as input to shallow transformer stack. Tokens from all input views are concatenated along the sequence dimension to enable cross-view reasoning, and DPT-style [59] decoder upsamples the fused tokens back to HW to produce S(I) for each image. We find that including DINOv3 features accelerates convergence and yields sharper, more semantically aligned motion masks. Importantly, for the image tokens we use the features before the camera estimator Ecam so that the motion head never relies on target views or test-time-only signals: at inference, it sees exactly the same type of inputs as during training. The motion estimator is then trained with standard BCE-withlogits loss to match the pseudo labels (I). Figure 3. Pseudo Motion Mask Pipeline. We fuse SSIMand DINO-based dissimilarity into saliency map, cluster DINO patch features to vote for dynamic patches, then refine the coarse patch mask to pixel resolution via morphological smoothing, smallcomponent removal, and GrabCut [65]. fusing semantic and appearance cues. As observed in Figure 3 and prior work [33], raw MSE is not very informative when renderings are imperfect; instead, we aim for less noisy pseudo-mask than the coarse error maps used in optimization-based pipelines. We first extract DINOv3 patch embeddings and define semantic dissimilarity map DDINO(p) = 1 Φp(I), Φp( ˆI), where Φp() are L2-normalized patch features and , is the inner product. In parallel, we compute pixel-level appearance dissimilarity via SSIM, DSSIM(x) = 1 SSIM(I, ˆI)(x). For sharp motion segmentation, we operate at patch resolution: DSSIM is downsampled to the patch grid by area pooling, and both maps are z-score normalized, Z(D) = (D µD)/σD. We then fuse the two dissimilarities with adaptive weights that depend on the current rendering fidelity. Let wDINO, wSSIM [0, 1] with wDINO + wSSIM = 1. When renderings are still coarse, we upweight the more stable semantic cue from DINOv3 (larger wDINO); as training progresses and photometric quality improves, we gradually increase wSSIM to exploit fine-grained appearance differences. The fused saliency is Dbin(p) = wDINO Z(DDINO(p)) + wSSIM Z(DSSIM(p)). Pseudo-label Construction. Given target image and its static rendering ˆI, we construct motion pseudo-labels by We cluster all patch embeddings {Φp(I)} across batch of frames from the same scene using K-means to group 5 semantically similar regions. cluster is marked as foreground (motion) if its mean saliency sk = Epk[Dbin(p)] lies in the top 5% and it is salient (above the 75th percentile of Dbin) in at least 4 frames, enforcing cross-frame consistency. Selected clusters are upsampled to pixel resolution via nearest-neighbor interpolation and refined with (a) morphological smoothing, (b) small connected-component removal, and (c) GrabCut [65] boundary refinement seeded by eroded foreground masks. This yields clean binary mask Mbin {0, 1}HW with sharp boundaries. All clustering and fusion are performed at patch resolution, reducing computation by roughly 100 while preserving motion boundaries. This is crucial in our learning-based setting, where we cannot afford to precompute and store DINOv3 features for the entire dataset. Masking Dynamic Input Tokens. To prevent transient content from entering the scene representation, we mask input tokens that correspond to predicted dynamic regions. For each input view, the motion estimator produces probability map, which we downsample to the token grid to obtain patch-level motion score. Patches whose score exceeds threshold are marked as dynamic, and their fused tokens are simply zeroed out before being fed into the scene encoder Escene. Only static tokens participate in scene reconstruction, ensuring that moving objects do not leak into the static scene tokens. CopyPaste Augmentation. To further improve the robustness of motion estimation and masking, we inject synthetic transients using simple copypaste strategy [16, 84]. We randomly sample objects from COCO [37], apply their provided segmentation masks, and paste them onto training images at random scales and positions. These pasted regions are treated as ground-truth transient masks and override the models predicted motion scores for those pixels. This augmentation exposes the system to broader distribution of moving objects, strengthens open-set generalization, and provides clean supervision for token removal without altering the masking mechanism itself. See out-of-domain visualizations in Figure 5. We provide details in appendix. 5. Experiments We now describe our experimental setup and present quantitative and qualitative results. WildRayZer uses 28 transformer layers: 4 for the motion estimator and 8 each for the camera estimator, scene encoder, and rendering decoder. Given our 4H100 compute budget, we train RayZer with 768 scene tokens using learning rate of 4 104, cosine schedule for 100k iterations, and batch size of 64. The perceptual-loss weight is λ = 0.2. For motion-mask training, we use learning rate of 2 104, and 1 104 for training the masked renderer. All models operate at 2562 resolution with patch size 16. Additional implementation details are provided in the Appendix. 5.1. Experiment Setup We describe the datasets, evaluation protocol, metrics, and baseline methods used in our experiments. Datasets. We evaluate on two challenging dynamic-scene benchmarks. D-RE10K-Mask contains 74 Internet-curated indoor sequences with moving humans, pets, and vehicles; each frame is paired with human-verified motion masks, enabling evaluation restricted to transient regions. D-RE10KiPhone consists of 50 real-world sequences captured using tripod-mounted iPhone. Evaluation Protocol and Metrics. We adopt sparseview NVS protocol: two, three or four input images are used for pose and scene estimation, and six target views are used for evaluation. We report static-region metrics for DRE10K-Mask and report full-image metrics for D-RE10KiPhone. Image quality is evaluated using PSNR [17], SSIM [86], and LPIPS [97]. Furthermore, when evaluating motion mask, motion-mask accuracy is measured using mIoU and Recall against human verified annotations. We provide details about evaluation protocol in appendix. Baselines. We compare against state-of-the-art dynamicincluding NeRF On-the-go [64], scene NVS methods, 3DGS [32], T-3DGS [44], WildGaussians [33], and Spotless-Splats [67]. All baselines are re-evaluated under the same sparse-view setting. WildGS-SLAM [99] is excluded because its renderer is not public and its SLAM assumptions are incompatible with our few-view regime. We further implement three RayZer-based variants using off-the-shelf motion estimators: (a) training-free cosegmentation [1], (b) Segment Any Motion (SAV) [21], and (c) MegaSAM [36]. Their predicted masks are downsampled to the token grid and used to drop corresponding image tokens before scene aggregation. All variants are tuned on the D-RE10K validation set and evaluated without modification on the iPhone benchmark. 5.2. Main Results As shown in Table 2 and Figure 4, WildRayZer consistently outperforms all baselines. Optimization-based pipelines struggle to suppress transients and reconstruct 3D scene structure from only 24 views. WildGaussians is performing better, but its rendering fidelity drops significantly In contrast, WildRayZer is feedunder sparse inputs. forward, pose-free at test time, and produces sharper background reconstructions with reliable transient removal. On D-RE10K-iPhone, our model also best recovers occluded background revealed across views, yielding the strongest full-image results. detailed static vs. transient breakdown is provided in Section 5.3. 6 Views = 2 D-RE10K Views = 3 Views = 4 Views = Views = 3 Views = 4 D-RE10K-iPhone Method PSNRs SSIMs LPIPSs PSNRs SSIMs LPIPSs PSNRs SSIMs LPIPSs PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS NeRF On-the-go [64] 3DGS [32] T-3DGS [44] Spotless-Splats [67] WildGaussians [33] 15.90 13.49 15.90 16.45 16.12 RayZer + Co-Seg [1] RayZer + MegaSAM [36] RayZer + SAV [21] WildRayZer (ours) 16.76 19.01 21.78 0.518 0.442 0.518 0.548 0. 0.547 0.628 0.734 0.582 0.605 0.582 0.468 0.624 0.516 0.397 0.308 18.45 14.92 18.45 17.77 17.76 17.53 20.13 20.35 21.98 0.624 0.514 0.624 0.600 0. 0.565 0.688 0.696 0.754 0.446 0.531 0.446 0.394 0.588 0.580 0.336 0.332 0.314 Optimization-Based Methods 19.52 16.28 18.70 18.05 18.11 0.620 0.552 0.613 0.608 0. 0.443 0.490 0.454 0.390 0.574 Feed-forward Methods 18.67 20.66 20.73 22.38 0.636 0.702 0.711 0.773 0.481 0.310 0.308 0.290 17.04 13.80 14.07 17.73 18. 16.06 19.57 20.89 0.412 0.411 0.380 0.569 0.514 0.446 0.558 0.611 0.636 0.600 0.632 0.424 0.643 0.645 0.428 0.364 17.53 15.94 15.61 18.65 19. 17.09 18.52 19.94 20.91 0.484 0.517 0.428 0.606 0.542 0.522 0.516 0.588 0.633 0.581 0.516 0.573 0.385 0.637 0.561 0.502 0.406 0.354 17.04 16.30 16.95 18.86 20. 17.96 19.98 20.01 20.98 0.470 0.529 0.525 0.615 0.571 0.685 0.718 0.704 0.734 0.616 0.503 0.487 0.382 0.620 0.473 0.372 0.355 0.298 Table 2. Main Results on Novel View Synthesis. We report mean performance for 2, 3, 4 input views on D-RE10K (left, static regions only) and D-RE10K-iPhone (right, full-image fidelity). Metrics are PSNR , SSIM , and LPIPS . Cells highlighted in red, orange, and yellow denote the best, second, and third results respectively. SAV denotes Segment Any Motion in Videos [21]. Figure 4. Qualitative Comparisons. Qualitative results on DRE10K-Mask (top two rows) and DRE10K-iPhone (bottom row). Compared to baselines, our method (1) more cleanly removes transient objects, (2) better handles cross-view completion (compare with RayZer + SAV baseline), and (3) better preserves global scene geometry (e.g., kitchens) and fine details (e.g., plants). SLS denotes Splotless-Splats [67]. Among RayZer-based variants, co-segmentation [1] often over-masks under sparse viewpoints, leading to severe artifacts; we tune its hyper-parameters on the D-RE10K validation set. MegaSAM [36] provides useful cues, but its boundaries are diffuse and predictions remain noisy with limited inputs. SAV [21] relies on pretrained tracker to select objects and can lock onto the wrong target under sparseview settings. Finally, even with accurate off-the-shelf masks, naively masking tokens yields blurry artifacts (akin to local interpolation), as visualized by the RayZer+SAV baseline in Figure 4. This suggests that cross-view completion requires learning, rather than masking alone. 5.3. Analysis Analysis of Rendering. To better analyze differences between methods, we separately evaluate static and transient regions on D-RE10K-iPhone with two input views (v=2; see Table 3). Because transient objects typically cover only small image areas, full-image PSNR can be misleading. Optimization-based methods show large gaps between static and transient metrics, while WildGaussians [33] is the only baseline that achieves relatively balanced performance in the v=2 setting, but with lower overall image quality. Feed-forward baselines without learned masked renderer exhibit strong occlusion artifacts, indicating that cross-view 7 Figure 5. Qualitative Results. (1) First row: D-RE10K (no ground-truth novel views). (2) Second row: D-RE10K-iPhone. (3) Third and fourth rows: additional NVS results on DAVIS [54], where ground truth is also unavailable, demonstrating that WildRayZer generalizes to outdoor scenes and can mask unseen transient objects. Method PSNRs SSIMs LPIPSs PSNRt SSIMt LPIPSt Optimization-Based Methods NeRF On-the-go [64] 3DGS [32] T-3DGS [44] Spotless-Splats [67] WildGaussians [33] 17.11 13.85 14.42 17.91 18.47 0.419 0.417 0.387 0.575 0. 0.633 0.596 0.630 0.420 0.642 Feed-forward Methods RayZer + Co-Seg [1] RayZer + SAV [21] WildRayZer (ours) 20.05 20.22 21.00 0.565 0.579 0.612 0.469 0.406 0. 16.98 14.63 13.98 17.42 18.46 15.74 17.77 20.99 0.323 0.337 0.292 0.494 0.449 0.363 0.437 0.575 0.667 0.615 0.662 0.477 0.652 0.621 0.536 0. Table 3. Novel View Synthesis on D-RE10K-iPhone (Views=2). Metrics are split into static (s) and transient (t) regions: PSNRs , SSIMs , LPIPSs , and PSNRt , SSIMt , LPIPSt . completion needs to be learned rather than purely inferred from static geometry. WildRayZer improves performance in both static and transient regions by explicitly modeling motion masks and maintaining robust pose estimation under sparse, dynamic inputs. Analysis of Motion Masks. We further evaluate motionmask quality across methods. Co-segmentation with DINOv2 [49] over-segments, hurting NVS quality. MegaSAM is sensitive to thresholding and becomes noisy in sparseview setting. SAV [84] often chooses irrelevant objects under sparse views. All baseline models improve with more views. As an oracle check, we run Segment-Any-Motion on 25-frame segments and observe strong agreement with our GT annotations (mIoU > 65). Additional examples are shown in the appendix. Method mIoU Recall mIoU Recall mIoU Recall n= n=3 n=8 Supervised Methods MegaSAM [36] Segment Any Motion [21] 31.9 47. 12.7 41.2 Self-supervised Methods Co-segmentation [1] WildRayZer 9.6 53.9 45.0 85.1 13.7 52. 37.1 57.1 53.2 84.3 35.4 50.9 16.3 54.2 60.7 70.8 45.5 87. Table 4. Motion-mask quality. Comparison of supervised and self-supervised motion segmentation methods under sparse-view settings. WildRayZer achieves higher mIoU and recall across different numbers of input views. Training Recipe D-RE10K D-RE10K-iPhone CopyPaste Only PseudoMask Only CopyPaste + PseudoMask 18.2 53.9 53.9 11.1 45.3 49. Table 5. CopyPaste Ablation. Copypaste alone does not transfer to real videos but improves out-of-domain generalization when combined with pseudo-masks. 5.4. Ablation Study Effects of CopyPaste Augmentation. We isolate copypaste augmentation by freezing the renderer and training only the motion estimator. Copypaste alone does not produce meaningful masks, but when added on top of D-RE10K pretraining, it improves cross-dataset generalization. The motion estimator transfers to DAVIS [54], where copypaste improves mIoU from 3.4 to 31.0 on subset of eight sequences with clear motion and sufficient parallax. We provide visualizations on DAVIS [54] in Figure 5. 8 Effects of Motion Estimator Input Modalities. Using both image tokens and camera-ray tokens allows gradients to flow into the pose estimator, leading to more stable camera estimation in dynamic scenes. Incorporating DINOv3 features substantially accelerates mask emergence and improves their quality: reaching mIoU=30 takes about 20k steps without DINOv3 but only 1.5k with it. Final performance also increases (39.4 vs. 29.4 mIoU on DRE10K), which is particularly important given the cost of each pseudo-mask iteration. 6. Conclusions In this work, we introduced WildRayZer, feed-forward SSL framework for novel view synthesis in dynamic environments that disentangles object motion from static 3D structure. Across sparse-view, transient-aware NVS and motion segmentation benchmarks, WildRayZer consistently outperforms each baseline. We further curated DRE10K and D-RE10K-iPhone, providing large-scale training data and benchmark for dynamic NVS under sparse views. Our results indicate that disentangling camera motion and object motion is feasible in fully self-supervised setting; limitations and future directions are discussed in the supplementary material. 7. Acknowledgment Services & Support The authors acknowledge the Adobe Research Gift, the University of Virginia Research Computing and Data Analytics Center, Advanced Micro Devices AI and HPC Cluster Program, Advanced Cyberinfrastructure Coordination Ecosystem: (ACCESS) program, and National Artificial Intelligence Research Resource (NAIRR) Pilot resources, (National Science including the Anvil supercomputer Foundation award OAC 2005632) at Purdue University and the Delta and DeltaAI advanced computing (National Science Foundation award OAC resources The authors thank Wen Ying, Zhenyu Li 2005572). and Jin Yao for constructing D-RE10K-iPhone, and thank Jiayun Wang, Hanwen Jiang, and Hao Tan for proofreading. for computational"
        },
        {
            "title": "References",
            "content": "[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2(3):4, 2021. 6, 7, 8, 3 [2] Federica Arrigoni, Luca Magri, and Tomas Pajdla. On the usage of the trifocal tensor in motion segmentation. In European Conference on Computer Vision, pages 514530. Springer, 2020. 2 [3] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text deIn Proceedings of the IEEE/CVF conference on tection. 9 computer vision and pattern recognition, pages 93659374, 2019. 3 [4] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021. [5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In ICML, page 4, 2021. 4 [6] Pia Bideau and Erik Learned-Miller. Its moving! probabilistic model for causal motion segmentation in moving camera videos. In European Conference on Computer Vision, pages 433449. Springer, 2016. 2 [7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. 3 [8] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. 2 [9] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1412414133, 2021. 2 [10] Jiahao Chen, Yipeng Qin, Lingjie Liu, Jiangbo Lu, and Guanbin Li. Nerf-hugs: Improved neural radiance fields in non-static scenes using heuristics-guided segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1943619446, 2024. 2 [11] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting In European Conference from sparse multi-view images. on Computer Vision, pages 370386. Springer, 2024. 2 [12] Hiba Dahmani, Moussab Bennehar, Nathan Piasco, Luis Roldao, and Dzmitry Tsishkou. Swag: Splatting in the wild images with appearance-conditioned gaussians. In European Conference on Computer Vision, pages 325340. Springer, 2024. [13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36:3579935813, 2023. 3 [14] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4 [15] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. Advances in Neural Information Processing Systems, 35:3376833780, 2022. 3 [16] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin Cubuk, Quoc Le, and Barret Zoph. Simple copy-paste is strong data augmentation method for In Proceedings of the IEEE/CVF instance segmentation. conference on computer vision and pattern recognition, pages 29182928, 2021. 2, 5, 6, 1 [17] Rafael Gonzalez. Digital image processing. Pearson education india, 2009. 6, [18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. 1 [19] Christian Homeyer and Christoph Schnorr. On moving object segmentation from monocular video with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 880891, 2023. 2 [20] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. In The Twelfth International Conference on Learning Representations, 2024. 1, 2 [21] Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, and Qianqian Wang. Segment any motion in videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 34063416, 2025. 2, 6, 7, 8, 3 [22] Ranran Huang and Krystian Mikolajczyk. No pose at all: Self-supervised pose-free 3d gaussian splatting from sparse views. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2794727957, 2025. 1 [23] Yuxiang Huang, Yuhao Chen, and John Zelek. Zero-shot monocular motion segmentation in the wild by combining deep learning with geometric motion model fusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27332743, 2024. 2 [24] Ge-Peng Ji, Keren Fu, Zhe Wu, Deng-Ping Fan, Jianbing Shen, and Ling Shao. Full-duplex strategy for video object segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 49224933, 2021. 2 [25] Hanwen Jiang, Zhenyu Jiang, Yue Zhao, and Qixing Huang. Leap: Liberate sparse-view 3d modeling from camera poses. arXiv preprint arXiv:2310.01410, 2023. 2 [26] Hanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke Zhu. Few-view object reconstruction with unknown categories and camera poses. In 2024 International Conference on 3D Vision (3DV), pages 3141. IEEE, 2024. [27] Hanwen Jiang, Hao Tan, Peng Wang, Haian Jin, Yue Zhao, Sai Bi, Kai Zhang, Fujun Luan, Kalyan Sunkavalli, Qixing Huang, et al. Rayzer: self-supervised large view synthesis model. arXiv preprint arXiv:2505.00702, 2025. 1, 2, 4 [28] Yangbangyan Jiang, Qianqian Xu, Ke Ma, Zhiyong Yang, Xiaochun Cao, and Qingming Huang. What to select: Pursuing consistent motion segmentation from multiple geometric models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 17081716, 2021. 2 [29] Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, and Zexiang Xu. Lvsm: large view synthesis model with minimal 3d inductive bias. arXiv preprint arXiv:2410.17242, 2024. 1, 2, 4 [30] Mohammad Mahdi Johari, Yann Lepoittevin, and Francois Fleuret. Geonerf: Generalizing nerf with geometry priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1836518375, 2022. 2 [31] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 1 [32] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 6, 7, [33] Jonas Kulhanek, Songyou Peng, Zuzana Kukelova, Marc Pollefeys, and Torsten Sattler. Wildgaussians: 3d gaussian splatting in the wild. arXiv preprint arXiv:2407.08447, 2024. 1, 5, 6, 7, 8 [34] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, et al. Neural 3d video synthesis from multi-view video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55215531, 2022. 3 [35] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang. Neural scene flow fields for space-time view synthesis of dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 64986508, 2021. 3 [36] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. Megasam: Accurate, fast and robust structure and motion from casual dynamic videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1048610496, 2025. 3, 6, 7, 8 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 5, 6 [38] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. 3 [39] Runtao Liu, Zhirong Wu, Stella Yu, and Stephen Lin. The emergence of objectness: Learning zero-shot segmentation from videos. Advances in Neural Information Processing Systems, 34:1313713152, 2021. [40] Xiaofeng Liu, Tong Che, Yiqun Lu, Chao Yang, Site Li, and Jane You. Auto3d: Novel view synthesis through unsu10 pervisely learned variational viewpoint and global 3d representation. In European Conference on Computer Vision, pages 5271. Springer, 2020. 2 [41] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang. Neural rays for occlusion-aware image-based renIn Proceedings of the IEEE/CVF Conference on dering. Computer Vision and Pattern Recognition, pages 7824 7833, 2022. 2 [42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [43] Ziqiao Ma, Xuweiyi Chen, Shoubin Yu, Sai Bi, Kai Zhang, Chen Ziwen, Sihan Xu, Jianing Yang, Zexiang Xu, Kalyan Sunkavalli, et al. 4d-lrm: Large space-time reconstruction model from and to any view at any time. arXiv preprint arXiv:2506.18890, 2025. 2 [44] Alexander Markin, Vadim Pryadilshchikov, Artem Komarichev, Ruslan Rakhimov, Peter Wonka, and Evgeny Burnaev. T-3dgs: Removing transient objects for 3d scene reconstruction. arXiv preprint arXiv:2412.00155, 2024. 3, 6, 7, 8 [45] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 72107219, 2021. 1, 2 [46] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020. 1, 2 [47] Thomas Mitchel, Hyunwoo Ryu, and Vincent Sitzmann. True self-supervised novel view synthesis is transferable. arXiv preprint arXiv:2510.13063, 2025. 1 [48] Michal Neoral, Jan Sochman, and Jirı Matas. Monocular In arbitrary moving object discovery and segmentation. BMVC, page 319, 2021. [49] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 8 [50] Anestis Papazoglou and Vittorio Ferrari. Fast object segIn Proceedings of the mentation in unconstrained video. IEEE international conference on computer vision, pages 17771784, 2013. 2 [51] Keunhong Park, Utkarsh Sinha, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Steven Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58655874, 2021. 3 [52] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan Barron, Sofien Bouaziz, Dan Goldman, Ricardo MartinBrualla, and Steven Seitz. Hypernerf: higherdimensional representation for topologically varying neural radiance fields. arXiv preprint arXiv:2106.13228, 2021. 3 [53] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the IEEE/CVF els with transformers. international conference on computer vision, pages 4195 4205, 2023. 1 [54] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video In Proceedings of the IEEE conferobject segmentation. ence on computer vision and pattern recognition, pages 724732, 2016. 2, [55] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 2, 1, 3, 5 [56] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1031810327, 2021. 3 [57] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 1 [58] Nikhil Raina, Guruprasad Somasundaram, Kang Zheng, Sagar Miglani, Steve Saarinen, Jeff Meissner, Mark Schwesinger, Luis Pesqueira, Ishita Prasad, Edward Miller, Prince Gupta, Mingfei Yan, Richard Newcombe, Carl Ren, and Omkar Parkhi. Egoblur: Responsible innovation in aria, 2023. 3 [59] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020. 5 [60] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [61] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 3 [62] Konstantinos Rematas, Andrew Liu, Pratul Srinivasan, Jonathan Barron, Andrea Tagliasacchi, Thomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1293212942, 2022. 2 [63] Sucheng Ren, Wenxi Liu, Yongtuo Liu, Haoxin Chen, Guoqiang Han, and Shengfeng He. Reciprocal transformations In Proceedfor unsupervised video object segmentation. ings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1545515464, 2021. 2 [64] Weining Ren, Zihan Zhu, Boyang Sun, Jiaqi Chen, Marc Pollefeys, and Songyou Peng. Nerf on-the-go: Exploiting 11 uncertainty for distractor-free nerfs in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 89318940, 2024. 1, 3, 6, 7, 8 [65] Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. grabcut interactive foreground extraction using iterated graph cuts. ACM transactions on graphics (TOG), 23(3): 309314, 2004. 5, [66] Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David Fleet, and Andrea Tagliasacchi. Robustnerf: IgIn Proceedings of noring distractors with robust losses. the IEEE/CVF conference on computer vision and pattern recognition, pages 2062620636, 2023. 2, 3 [67] Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David Fleet, and Andrea Tagliasacchi. Spotlesssplats: Ignoring distractors in 3d gaussian splatting. ACM Transactions on Graphics, 44(2):111, 2025. 6, 7, 8 Jan-Michael Frahm. and In Proceedings of Structure-from-motion revisited. the IEEE conference on computer vision and pattern recognition, pages 41044113, 2016. 2 [68] Johannes Schonberger [69] Hicham Sekkati and Amar Mitiche. variational method for the recovery of dense 3d structure from motion. Robotics and Autonomous Systems, 55(7):597607, 2007. 2 [70] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 2, 5, 1 [71] Brandon Smart, Chuanxia Zheng, Iro Laina, and VicSplatt3r: Zero-shot gaussian tor Adrian Prisacariu. splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. [72] Tomas Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1121811221, 2024. 3 [73] Shaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by self-adaptive hyper network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36673676, 2020. 3 [74] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconIn Proceedings of the IEEE/CVF conference struction. on computer vision and pattern recognition, pages 10208 10217, 2024. 2 [75] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. In 2025 International Conference on 3D Vision (3DV), pages 670681. IEEE, 2025. 2 [76] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul Srinivasan, Jonathan Barron, and Henrik Kretzschmar. Block-nerf: Scalable In Proceedings of large scene neural view synthesis. the IEEE/CVF conference on computer vision and pattern recognition, pages 82488258, 2022. [77] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multiview gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pages 118, 2024. 2 [78] Meng Tang, Lena Gorelick, Olga Veksler, and Yuri Boykov. In Proceedings of the IEEE internaGrabcut in one cut. tional conference on computer vision, pages 17691776, 2013. 1 [79] Zachary Teed, Lahav Lipson, and Jia Deng. Deep patch visual odometry. Advances in Neural Information Processing Systems, 36:3903339051, 2023. 3 [80] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 1 [81] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for In The Twelfth Internajoint pose and shape prediction. tional Conference on Learning Representations, 2024. 2 [82] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46904699, 2021. [83] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. 1 [84] Xudong Wang, Ishan Misra, Ziyun Zeng, Rohit Girdhar, and Trevor Darrell. Videocutler: Surprisingly simple unsupervised video instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2275522764, 2024. 2, 5, 6, 8, 3 [85] Yuze Wang, Junyi Wang, and Yue Qi. We-gs: An in-thewild efficient 3d gaussian representation for unconstrained photo collections. arXiv preprint arXiv:2406.02407, 2024. 2 [86] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility IEEE transactions on image proto structural similarity. cessing, 13(4):600612, 2004. 2, 6, 1 [87] Andreas Wedel, Annemarie Meißner, Clemens Rabe, Uwe Franke, and Daniel Cremers. Detection and segmentation of independently moving objects from dense scene flow. In International Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition, pages 1427. Springer, 2009. 2 [88] Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Soren Pirk, Arie Kaufman, Xin Sun, and Hao Tan. [101] Tianfei Zhou, Jianwu Li, Shunzhou Wang, Ran Tao, and Jianbing Shen. Matnet: Motion-attentive transition network for zero-shot video object segmentation. IEEE transactions on image processing, 29:83268338, 2020. 2 [102] Chen Ziwen, Hao Tan, Kai Zhang, Sai Bi, Fujun Luan, Yicong Hong, Li Fuxin, and Zexiang Xu. Long-lrm: Longsequence large reconstruction model for wide-coverage gaussian splats. arXiv preprint arXiv:2410.12781, 2024. 2 Lrm-zero: Training large reconstruction models with synthesized data. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [89] Jiacong Xu, Yiqun Mei, and Vishal Patel. Wild-gs: Realtime novel view synthesis from unconstrained photo collections. Advances in Neural Information Processing Systems, 37:103334103355, 2024. 1, 2 [90] Xun Xu, Loong Fah Cheong, and Zhuwen Li. Motion segmentation by exploiting complementary geometric models. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28592867, 2018. 2 [91] Shu Yang, Lu Zhang, Jinqing Qi, Huchuan Lu, Shuo Wang, and Xiaoxing Zhang. Learning motion-appearance co-attention for zero-shot video object segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 15641573, 2021. 2 [92] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. [93] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park, and Jan Kautz. Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53365345, 2020. 2 [94] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578 4587, 2021. 2 [95] Dongbin Zhang, Chuming Wang, Weitao Wang, Peihao Li, Minghan Qin, and Haoqian Wang. Gaussian in the wild: 3d gaussian splatting for unconstrained image collections. In European Conference on Computer Vision, pages 341359. Springer, 2024. 2 [96] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large In Euroreconstruction model for 3d gaussian splatting. pean Conference on Computer Vision, pages 119, 2024. 2 [97] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 3, 6, 2 [98] Qitao Zhao, Hao Tan, Qianqian Wang, Sai Bi, Kai Zhang, Kalyan Sunkavalli, Shubham Tulsiani, and Hanwen Jiang. E-rayzer: Self-supervised 3d reconstruction as spatial visual pre-training. arXiv preprint arXiv:2512.10950, 2025. 1 [99] Jianhao Zheng, Zihan Zhu, Valentin Bieri, Marc Pollefeys, Songyou Peng, and Iro Armeni. Wildgs-slam: Monocular gaussian splatting slam in dynamic environments. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1146111471, 2025. 1, 3, [100] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Stereo magnification: Learning and Noah Snavely. view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018. 1, 3 13 WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments"
        },
        {
            "title": "Supplementary Material",
            "content": "Section details the full WildRayZer training pipeline, including RayZer pretraining, motion-mask learning, masked reconstruction, and joint training with copypaste augmentation. Section details the D-RE10K-iPhone benchmark and clarifies evaluation metrics. Section presents extended qualitative visualizations, including motion-mask comparisons, failure analyses, and additional results on D-RE10K, D-RE10K-iPhone, and DAVIS [55]. A. Training Details RayZer Pretraining. We first train RayZer [27] using official release code with scene and pose latent representations. The encoder consists of 12 transformer layers with 8 additional geometry-specific layers, while the decoder uses 12 layers. We use hidden dimension of = 768 with attention heads of size 64. Both encoder and decoder emImages are ploy QK normalization for training stability. tokenized into 16 16 patches from 256 256 input resolution. The scene latent code has length 768, and camera poses are represented using 6D rotation representation. Importantly, we find smaller scene latent code yields better rendering quality compared to scene latent code length of 3084 when training with smaller batch size. Additionally, instead of relying solely on absolute positional embeddings, we introduce dedicated embedding to distinguish input and target views. We also mix cases with 2, 3, and 4 input views during training so that RayZer can handle variable-length inputs without performance degradation. We train on the RealEstate10K [100] dataset with 2 input views and 6 target views per scene. The model is trained for 100K steps with batch size of 8 per GPU and gradient accumulation over 2 steps. We use the AdamW [42] optimizer with β1 = 0.9, β2 = 0.95, learning rate of 4 104, and weight decay of 0.05. The learning rate is warmed up over 6K steps. Gradient clipping is applied with maximum norm of 1.0. We use mixed precision training (bfloat16) with TF32 enabled on H100. The training objective combines L2 reconstruction loss (weight 1.0) and perceptual loss (weight 0.2) computed using VGG features. Input views are sampled with frame distances between 25 and 192 frames, with scene scale factor of 1.35 applied during data augmentation. Motion Mask Training Stage. In this stage, we train the motion mask predictor while keeping the pre-trained rendering model frozen. The goal is to learn to distinguish dynamic regions from static using self-supervised pseudolabels derived from DINOv3 [70] features and SSIM [86]. We generate motion pseudo-labels by computing ground-truth and renderings differences semantically and structurally. For each pair of ground truth and rendering, we extract dense feature maps and compute cosine similarity between corresponding spatial locations. Then we derive fused error map from SSIM dissimilarity map and DINO dissimilarity map, viewing as saliency map. Then we cluster DINO feature at patch level across input images and assign average saliency score based on the saliency map. Regions with low feature consistency across frames (similarity below learned threshold) are labeled as dynamic, while consistent regions are labeled as static. We apply morphological operations (kernel size 3, 1 iteration) to expand the mask and filter out small components (area < 0.25% of image) to ensure coherent motion segments. Lastly, we use grabcut [78] to refine the boundary. The motion mask predictor is trained on D-RE10K using binary cross-entropy loss between predicted masks and pseudo motion labels. We use learning rate of 104 with AdamW optimizer (β1 = 0.9, β2 = 0.95, weight decay 0.01) and batch size 32. Critically, we apply PSNRbased sample filtering strategy: only samples with rendering PSNR > 17 dB are used for mask supervision. This prevents our model to learn from noisy pseudo-labels on challenging dynamic scenes where the frozen renderer struggles. We show detailed process of pseudo motion mask construction in Fig. 3. Masked Latent Scene Reconstruction Stage. In this stage, we freeze the learned motion mask predictor and train the rendering model to reconstruct static scene content while explicitly providing masks on input images. This stage teaches the model to perform cross-view completion over masked areas. We implement mask token strategy during training by masking 10% token, similar to standard MAE training but tailored for novel view synthesis. More specifically, we find clustered mask performances better compare to random token masking. The reconstruction loss is computed on the entire image. Joint training with Copy-Paste Augmentation. In the final stage, we jointly train the motion-mask predictor and the rendering model with synthetic motion augmentation. Specifically, we adopt copypaste augmentation [16] to insert controlled transient objects into static RE10K scenes, providing explicit supervision for the motion-mask predictor while simultaneously training the renderer to remain robust to these diverse transients. In addition, we mix D-RE10K sequences by masking the predicted motion regions, iteratively refining pseudo motion labels and excluding dynamic areas from the reconstruction loss. 1 clean frame without it. An iPhone mounted on tripod with Bluetooth shutter is used to minimize pose differences between paired images. Each sequence require roughly 30 minutes to curate. To reduce illumination-induced artifacts, we compare static background regions across each pair and discard pairs with noticeable lighting changes (e.g., sudden occlusions or reappearance of direct sunlight). For the v=2, 3, 4 sparseview settings, input views are selected to span the full range of available camera angles in each sequence, ensuring that we evaluate genuine novel view synthesis rather than nearview interpolation. B.2. Masked Image Quality Metrics We compute standard image quality metrics (PSNR [17], SSIM [86], LPIPS [97]) over spatial regions specified by real-valued masks [0, 1]HW , enabling separate evaluation of transient and static scene components. Masked PSNR. Given ground-truth image I, prediction ˆI, and mask , we compute masked mean squared error: MSEM = (cid:80) i,j(Iij ˆIij)2 Mij i,j Mij (cid:80) PSNRM = 10 log10(MSEM ). , (3) (4) This yields localized measure of pixel-level fidelity within the masked region. Masked SSIM. We first compute standard SSIM map RH using an 11 11 Gaussian window (σ = 1.5) with stability constants C1 = (0.01)2 and C2 = (0.03)2 for images in [0, 1]. Let µ and σ denote local means and variances. The per-pixel SSIM is Sij = (2µij ˆµij + C1)(2σij + C2) ij + ˆµ2 ij + C1)(σ2 ij + ˆσ2 ij + C2) (µ . (5) The mask is downsampled to the SSIM resolution via area pooling (M ), and the masked SSIM is SSIMM = (cid:80) i,j Sij ij (cid:80) i,j ij . (6) Masked LPIPS. To obtain perceptually weighted masked errors, we use LPIPS in spatial mode, producing per-layer feature difference maps D(ℓ) RHℓWℓ. For each feature level ℓ, we downsample the mask via area pooling to (ℓ) and compute LPIPSM = (cid:88) ℓ (cid:80) ij (ℓ) ij i,j D(ℓ) (cid:80) i,j (ℓ) ij . (7) This applies perceptual masking consistently across all LPIPS feature scales. 2 Figure 6. Examples of Copypaste mask augmentation. We inject synthetic transient objects (e.g., animals, household items, vehicles) into static RE10K scenes to simulate dynamic elements in otherwise static environments. We detail the copy-paste augmentation here and provide examples in Fig. 6. We randomly paste 1-2 COCO objects (animals, vehicles, people) into 50% of training scenes occupying 25-35% of image size. Objects are blended using Gaussian smoothing (σ = 3) and positioned with 15% margin from image borders. With 80% probability, we use perview random objects; otherwise, we paste the same object across all views in sequence. This generates binary overlay masks Mpaste indicating pasted regions. In this stage, the motion mask predictor receives supervision from two sources: (1) DINOv3 pseudo-labels MDINO for real dynamic content in D-RE10K, and (2) ground-truth paste masks Mpaste for synthetic objects in augmented Static RE10K. We use binary cross-entropy loss with PSNR filtering (threshold 17 dB) to ensure supervision quality. The rendering loss combines masked reconstruction on both static regions and pasted regions: = Lmasked + λmask BCE(Mpred, Mtarget), (2) where Mtarget = MDINO for dynamic scenes and Mtarget = Mpaste for augmented static scenes, and λmask = 1.0. If we train only in the final stage, we observe many rendering artifacts due to inaccurate motion-mask predictions during early iterations. Therefore, adopting progressive, multi-stage training strategy becomes essential for stability and reliable convergence. B. Benchmarking Details In this section, we provide additional details of the proposed D-RE10K-iPhone benchmark in Section B.1 and describe our masked image quality metrics in Section B.2. We will release our implementation as reference. B.1. D-RE10K-iPhone Benchmark Details D-RE10K-iPhone comprises 50 real-world sequences featuring humans and vehicles. Each sequence contains 18 paired images captured from the same viewpoint: one transient frame with moving object (e.g., person, car) and one Figure 7. Motion Masks Comparisons. We present motion masks comparisons on D-RE10K (row 1) and D-RE10K-iPhone (row 2) across Co-segmentation [1], MegaSAM [36], Segment Any Video [21], VideoCutler [84] and WildRayzers motion mask predictions. Implementation Details. All metrics are evaluated per image. For D-RE10K-iPhones analysis, we report PSNR, SSIM, and LPIPS separately on transient regions (using Mtransient) and on static regions (using Mstatic = 1 Mtransient). We notice different implementation can lead to different evaluation results, therefore we will release our implementations for these metrics as reference. C. Additional Visualizations In this section, we provide additional visualization: (1) motion mask comparisions with state-of-art methods in Section C.1; (2) present WildRayzers failure cases in Section C.2; (3) demonstrates additional WildRayzers results on D-RE10K, D-RE10K-iPhone and DAVIS [55] benchmarks in Section C.3. C.1. Motion Mask Visualization tuning, We compare several popular motion-mask generators: cosegmentation based on DINO features [1], MegaSAM [36], Segment Any Video (SAV) [21], and the self-supervised Even instance segmentation model VideoCutler [84]. after careful hyperparameter training-free cosegmentation tends to either over-mask or under-mask large portions of the scene, which severely degrades rendering quality. MegaSAM provides useful cues, but under sparseview inputs its masks are often noisy and inconsistent across views. SAV can be highly accurate when it selects the correct actor, but under sparse views it frequently segments the wrong object. VideoCutler, by design, returns salient instances rather than true movers, and thus does not distinguish dynamic from static objects; we visualize its first predicted mask for comparison. In contrast, our learned motion masks are the most faithful to true motion boundaries, despite being trained in fully self-supervised manner without ground-truth motion labels. We demonstrate motion masks with one example from D-RE10K (top row) and D-RE10KiPhone (bottom row) in Fig. 7. C.2. Failure Cases We illustrate common failure cases in Fig. 8. Because our pseudo motion masks do not enforce instance-level segmentation, they may capture only the moving parts of an object when other parts remain static across the input views. While this behavior can be consistent with the motion cue, it often degrades rendering quality and we treat it as failure mode. We also observe under-segmentation, where the predicted motion mask is smaller than the true moving region (row 2). Finally, mask quality drops when the moving object occupies large fraction of the input images (rows 34). C.3. Additional Visualizations We provide 12 additional qualitative examples to further illustrate the behavior of WildRayZer beyond the main paper results in Fig. 9. The first three rows depict challenging dynamic indoor scenes from D-RE10K; in these cases, WildRayZer successfully removes transients while plausibly complete occluded structure across input views. The next two rows demonstrate cross-dataset generalization on unseen DAVIS sequences [55], showing that the learned motion masks and masked rendering transfer to videos with different content and capture conditions. The final row presents real-world examples from D-RE10K-iPhone under casual handheld capture, highlighting that our model maintains sharp static geometry and clean background reconstruction. 3 Figure 8. Failure Cases. Each row contains one failure case of WildRayzer. Motion mask only highlights moving parts dispite highlight television, it only highlights part of human body in row 1. Moreover, motion mask may miss small places such as feet in row 2. Row 3 and 4 show failure case when transient object is too big and predicted masks are smaller. Figure 9. Additional qualitative results. We show 12 extra examples to illustrate WildRayZers behavior across datasets. The first three rows are from D-RE10K, the next two rows demonstrate generalization to the unseen DAVIS dataset [55], and the last row shows additional real-world results on D-RE10K-iPhone."
        }
    ],
    "affiliations": [
        "University of Virginia"
    ]
}