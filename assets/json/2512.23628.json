{
    "paper_title": "Memorization in 3D Shape Generation: An Empirical Study",
    "authors": [
        "Shu Pu",
        "Boya Zeng",
        "Kaichen Zhou",
        "Mengyu Wang",
        "Zhuang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem."
        },
        {
            "title": "Start",
            "content": "Memorization in 3D Shape Generation: An Empirical Study Shu Pu1 Boya Zeng1 Kaichen Zhou2 Mengyu Wang2 2Harvard University 1Princeton University Zhuang Liu1 5 2 0 2 9 ] . [ 1 8 2 6 3 2 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at github.com/zlab-princeton/3d mem. 1. Introduction 3D shape creation is crucial for various industries, including digital twins, robotics, and AR/VR. Since manual modeling is slow and labor-intensive, the community has developed variety of AI-powered models for 3D shape generation [1, 21, 22, 32, 39, 57]. Recently, this field has benefited from feed-forward diffusion models trained directly on 3D shape datasets [10, 24, 30, 53, 54, 64, 66] and compact 3D latent representations [7, 9, 61], which together enable fast and scalable synthesis of high-fidelity 3D assets. Despite this rapid progress, the impressive performance of these models calls for closer examination. In particular, if these models achieve high fidelity by memorizing training shapes, they may risk training data leakage and fail to generalize to unseen geometries. Prior empirical studies of memorization in generative models have largely focused on image generation [16, 27, 44]. These studies investigate Figure 1. Examples of generated 3D shapes that illustrate memorization vs. generalization relative to the training shapes. In this paper, we propose framework to evaluate memorization in 3D shape generation, use it to quantify memorization in existing methods, and conduct controlled experiments to study how data and modeling designs impact memorization. how memorization scales with factors such as dataset size and model capacity. However, memorization in 3D generative models has not yet been systematically studied. Given the complex geometric data and novel modeling factors in the 3D modality (e.g., 3D representations and rotation augmentations), it is natural testbed for studying the impact of data and modeling designs that prior work has not explored. Currently, in 3D shape generation, there is no standard metric to quantify the novelty of generated shapes relative to the training set. Existing evaluation metrics, such as Uni3DScore [34] and Frechet Point Cloud Distance (FPD) [42], evaluate only shape fidelity and distributional similarity. In this work, we introduce an evaluation framework to quantify memorization in 3D generative models. Concretely, we define object-level memorization as generating shapes that are visually near-identical to training shapes. See Figure 1 for an example. By benchmarking metrics commonly used for 3D object retrieval (3DOR) against human judgments, we find that Light Field Distance (LFD) [6] is the most accurate among the evaluated metrics for identifying replicas of training shapes. We then define model-level memorization as the generated set being closer to the training set than held-out test set is. Building on LFD, we propose an evaluation framework that employs the z-score ZU from statistical test [35, 36] to quantify this memorization in practice, alongside Frechet Distance (FD) [19] to decouple it from generation quality. Applying this framework to existing 3D generators, we find that models trained on smaller datasets (e.g., single ShapeNet category) [23, 43, 68] show clear memorization, producing near-exact replicas, whereas recent large-scale conditional generative models [54, 61, 66] exhibit limited memorization and an enhanced ability to generalize. To understand what drives memorization and to promote generalization, we conduct controlled experiments using the vector-set (Vecset) representation [61], latent shape representation widely used in recent state-of-the-art 3D generative models [24, 29, 30, 52, 64]. Our results highlight multiple factors that affect models memorization. From the data perspective, rendered images are more likely to be memorized than 3D shapes in our setup; higher semantic diversity and finer-grained conditioning result in stronger memorization. From the modeling perspective, memorization is highest at moderate classifier-free guidance scale, while increasing the latent Vecset length and applying simple rotation augmentation can effectively reduce it. In summary, we introduce an evaluation framework to assess memorization in 3D generative models. Using this framework, we evaluate memorization in existing methods and design controlled experiments to study the influence of various data and modeling designs on memorization. Our findings suggest that specific strategies, such as applying rotation augmentation and increasing latent Vecset length, can help mitigate memorization while maintaining generation quality. We hope these insights offer valuable guidance for future research into generalizable 3D synthesis. 2. Related Work Shape generation in 3D vision. 3D shape data have different representations, diverse structures, and conditional generation paradigms that condition on various modalities. Recently, 3DShape2VecSet [61] proposes Vecset, which enables training 3D generative models in compact latent vector-set space and makes diffusion models easier to train and scale. As result, many current state-of-the-art models are built on this representation [24, 29, 30, 52, 64]. Memorization in generative models. Motivated largely by copyright infringement concerns, growing line of work studies data replication and memorization in generative models [16, 44, 58]. These works typically analyze how factors such as dataset scale, model capacity, and training dynamics correlate with models tendency to generate samples nearly identical to its training data. More recent studies [4, 27, 46] take more mechanistic view of diffusion models, arguing that their ability to generate novel samples arises from structured inductive biases and from how they selectively fit or approximate the target score. 3D Object Retrieval (3DOR). 3DOR is fundamental in AR/VR [14, 48], the gaming industry [71], and medical applications [28]. Traditional 3D distance metrics, such as Chamfer Distance (CD) and LFD [6], compute distances based on raw 3D geometry or renderings. Modern 3DOR methods are built on point cloud encoders [40, 55, 69] or multi-view encoders [17, 47], with the latter often trained on relatively small rendered-image datasets. Recently, 3DOR has been explored in various settings [49, 50]. 3. Methodology To date, methods for quantifying and analyzing memorization in generative models of 3D shapes remain underexplored. To study this memorization, we first identify an accurate retrieval method that can reliably find the closest training shape for each generated shape. Using this retrieval method, we employ the Mann-Whitney z-score ZU [35, 36] to quantify memorization in generative models. Finally, we combine the z-score with generation quality measure to establish an evaluation framework capable of distinguishing true generalization from low-quality generation. 3.1. Distance Metrics We construct benchmark to identify the most effective distance metric for memorization detection. This benchmark contains 133 3D shapes generated in roughly equal numbers from four ShapeNet categories (chair, car, airplane, and table). We generate these shapes using models [23, 43] that, upon manual inspection, exhibit limited generalization. For each generated shape and each distance metric, we retrieve the nearest neighbor from the training set, and we mark the retrieval as correct if the generated shape is visually near-identical to its retrieved shape according to manual inspection. Generated shapes for which none of the metrics retrieves visually similar result are excluded from the final accuracy computation. We evaluate seven widely used point cloud-based and view-based metrics on this benchmark. Implementation details are in Appendix A.1. Point cloud-based metrics mainly assess geometric similarity by either directly computing distances between point clouds (e.g., Chamfer Distance) or comparing features produced by point cloud encoders (e.g., classical encoders like PointNet++ [40] and image-text-aligned encoders like ULIP-2 [55] and Uni3D [69]). Across all settings, we consistently sample 4096 points for each shape. View-based metrics mainly assess perceptual similarity by comparing images of 3D objects from multiple viewpoints, using either handcrafted image descriptors (e.g., LFD [6]) or image encoders (e.g., DinoV2 [37] and SSCD [38]). LFD renders object silhouettes from 10 canonical viewpoints. For each silhouette, it extracts feature vector using handcrafted image descriptors [63]. The distance between two shapes is then defined as the L1 distance between their feature vectors. For image encoders, we use SSCD and DinoV2 as suggested in [44]. Following prior work [54, 67], we render 12 views per shape and obtain the final embedding by average-pooling the [CLS] tokens from each view. Human evaluation results. The results in Table 1 show that, among all seven retrieval metrics, LFD achieves the highest accuracy. Recent point cloud encoders such as Uni3D also deliver competitive performance, whereas image encoders trained on image datasets (i.e., DinoV2 and SSCD) still struggle to perform well in the 3D setting. Therefore, we adopt LFD as our primary retrieval metric. metric LFD Uni3D ULIP-2 CD DinoV2 PointNet++ SSCD acc. (%) 78.4 46.8 66.2 18.7 74.8 20. 7.2 Table 1. Top-1 retrieval accuracy (%) of distance metrics on 133 generated shapes from four ShapeNet categories (chair, car, airplane, and table). LFD achieves the highest accuracy. 3.2. Memorization Metric With LFD as our most accurate object-level distance metric, we use it to define model-level memorization score based on the Mann-Whitney test [35]. Following prior work [36], this test takes the distances from generated and held-out samples to their nearest training shapes and outputs single value that quantifies memorization. Formally, let be the training set, Ptest the held-out test set, and the generated set. Let Ptest be held-out test sample, and let be generated sample. We define dT ( ) = min tT dist( , t), and construct two sets of distance values Dtest = {dT (x)}xPtest, Dgen = {dT (y)}yQ. Following [36], we use the standardized z-score statistic ZU (Dtest, Dgen) (ZU for short) as our memorization score. detailed explanation of ZU calculation is in Appendix A. In short, ZU quantifies memorization by ranking generated samples and held-out test samples based on their distances to their nearest training samples. When ZU < 0, the generated set is, on average, closer to than Ptest is to . Thus, we treat ZU < 0 as evidence of memorization, with the strength of memorization increasing as ZU decreases. Since ZU is based on the ranks of nearest-neighbor distances, it is directly comparable across models as long as Ptest and are fixed. However, ZU 0 can come either from genuine generalization or from low-quality samples that are far from all training shapes. We therefore design an evaluation framework that controls for generation quality and makes memorization levels comparable across models. 3.3. Evaluation Framework We use Frechet Distance (FD) [19], widely used metric for generative models, as our quality indicator to disentangle memorization from generation quality. In Section 3.1, we find that Uni3D is the strongest point cloud encoder among the encoders we evaluate, so we use it to compute FD. Specifically, given generated set and reference set R, we extract Uni3D features and fit Gaussian distributions to the features with means µR, µQ and covariance matrices ΣR, ΣQ. The FD between these two distributions is: (cid:16) (cid:1)1/2(cid:17) 2+Tr ΣQ Σ1/2 ΣR+ΣQ2(cid:0)Σ1/2 FD = µRµQ2 . (1) We consider two variants of this metric: training FD and test FD. For training FD, we generate shapes from training prompts and compute FD with the training set; for test FD, we generate shapes from test prompts and compute FD with the test set. While training FD is more widely used [54, 67], test FD may be fairer quality indicator, since training FD can be impacted by memorization: exactly reproducing the training data can drive training FD to zero. With these components, we build an evaluation framework to decouple true generalization from low generation quality. We use LFD to compute two sets of nearestneighbor distances from generated shapes and test shapes to the training set, and use ZU to compare the two distance sets. To maintain concise notation, we hereafter refer to the memorization score calculated using LFD as ZU (formally LFD ), unless different distance metric is specified. We then interpret ZU only among models with similar test FD values, so that differences in ZU mainly reflect changes in memorization rather than changes in sample quality. 4. Evaluating Existing Models We evaluate the memorization behavior of several representative models with fixed Ptest = = 100. The selected models span earlier models trained on single ShapeNet category [23, 43], models trained on the full ShapeNet dataset [61, 66], and Trellis [54] trained on 500K samples. We assume that these models already generate high-quality shapes, so we only evaluate their memorization level. We first sample chair shapes from multiple models whose training data include the chair category in ShapeNet, and compute ZU with respect to ShapeNets chair training and test sets. As shown in Table 2 and Figure 25 in Appendix B, some models, such as LAS-Diffusion and Wavelet Generation, produce replicas of training shapes, whereas others, such as 3DShape2VecSet and Michelangelo, generate geometrically novel shapes. Models that method LAS-Diffusion (uncond.) LAS-Diffusion (class) Wavelet Generation 3DShape2VecSet Michelangelo -7.02 -4.93 -1.35 4.56 9. ZU gen & retrieved Table 2. Evaluating memorization on the chair category in ShapeNet. We report ZU for each model with respect to the chair subset of ShapeNets training and test sets. The last row shows, for each model, generated shapes at the 60th percentile ranked by LFD distance to their nearest training shapes. As ZU increases, generated samples transition from near-identical replicas of training shapes to clearly novel shapes. For the text-conditional model Michelangelo, we use the prompt 3D model of chair to sample, following its training recipe. method LAS-Diffusion (class) 3DShape2VecSet Michelangelo Trellis-small Trellis-large Trellis-xlarge split ZU IM-NET [8] 0.46 3DILG [60] 1. 3DILG -0.33 Trellis500K [54] -0.67 Trellis500K -1.57 Trellis500K -2.19 Table 3. Evaluating memorization on the entire training sets. We report ZU for each model with respect to its entire training set and test set. For the text-conditional models (i.e., Michelangelo and Trellis), we use training prompts for generation. For Trellis, we randomly sample 100 shapes from the Trellis500K dataset as the test set in the ZU calculation. qualitatively show stronger memorization also have lower ZU , indicating that ZU correctly captures memorization. We then evaluate recent state-of-the-art models using their full reported training and test datasets. For Trellis, which does not provide an official split, we construct test set by randomly sampling and excluding data from the training set and calculate ZU accordingly. As shown in Table 3, recent 3D generative models have ZU values clustered around zero. Text-conditional models Michelangelo and Trellis have slightly negative ZU values. Consistent with the qualitative examples in Appendix B, we find that Michelangelo and Trellis already demonstrate reasonable generalization capabilities. In Appendix B, we also provide additional qualitative results for recent models whose training and test sets are not available for computing ZU . In summary, we find that earlier models exhibit strong memorization, whereas recent models demonstrate effective generalization. This shift may result from various evolving design factors in the field, including the increasing data diversity and dataset size in 3D datasets, as well as modeling choices such as parameter scale and 3D representations. 5. Impact of Data on Memorization To understand how each design factor affects memorization in 3D models, we conduct controlled experiments based on our evaluation framework. In this section, we first investigate the impact of training data properties, including data modality, diversity, and conditioning granularity. 5.1. Experimental Setup Dataset. We conduct controlled experiments on customized subset of Objaverse-XL [11]. We obtain captions for shapes from Cap3D [33]. To assign class la10th (629) 20th (1031) 30th (1302) 40th (1753) 50th (2146) 60th (2528) 70th (2936) 80th (3417) 90th (4225) Figure 2. Generated samples from the baseline model at each decile, ranked by LFD to the nearest training shape. bels to shapes, we first use Qwen3 Embedding [65] to select the most relevant class for each caption from the 100 most frequent categories in Objaverse-LVIS [12]. Then, we use Qwen3 [56] to remove mismatched caption-label pairs. This process results in dataset of about 140K 3D objects with captions and labels. In our experiments, we split the dataset into 120K training set and 20K test set. Model. While 3D representations are diverse, we use the vector-set (Vecset) representation in our controlled experisampling prompt training data 3D gen. top-1 NN (SSCD) top-1 NN (LFD) image gen. top-1 NN (SSCD) plush toy character with round head and small limbs Buddha statue sitting on mat Table 4. Qualitative comparison between 3D and image models for memorization. For each prompt, we show the training example, the generated 3D shape and its top-1 nearest training shapes under SSCD and LFD, together with the generated image and its top-1 nearest training image under SSCD. Image generation tends to replicate training samples, whereas the 3D generator produces more novel shapes. ments, as it is widely used in 3D generation [24, 30, 52, 66]. The autoencoder encodes the sampled point cloud into compact, unordered latent Vecset and decodes it by predicting occupancy or signed distance function (SDF) values at randomly sampled spatial query points. In our experiments, we use the pre-trained 1024 32 VecSetX autoencoder [61] and adopt Hunyuan3D 2.1 [24] as the diffusion backbone. Memorization evaluation. We follow standard practice in image diffusion models [44] by evaluating memorization in samples generated from training prompts. Throughout the experiments, we use our evaluation framework in Section 3.3 and fix Ptest = = 2500. Ptest is downsampled from the full test set while preserving the class distribution. Details of the data curation pipeline, autoencoder, and diffusion model backbone are provided in Appendix C.1. around 200K steps, and further training results in higher memorization, as indicated by ZU . Meanwhile, the strong correlation between training FD and ZU suggests that training FD may indeed be influenced by memorization and, therefore, is not reliable indicator of generation quality. In Appendix C.2, we sanity-check our setup by confirming that increasing dataset size reduces memorization and that larger models memorize more. These observations are consistent with prior work [5, 16, 44, 58]. 5.3. Data Modality modality DinoV2 8.58 15. SSCD -8.71 2.49 LFD - -5.60 image 3D Images are more prone to memorization than 3D Table 5. and SSCD shapes. We compute DinoV2 for image generation models, and extend these metrics to 3D shapes as in Section 3.1. Images have lower ZU , indicating stronger memorization. U Figure 3. Training dynamics of the baseline model. As training progresses, ZU and training FD simultaneously decrease, whereas test FD decreases initially before plateauing at around 200K steps. 5.2. Baseline Model As the basis of our controlled experiments, we design baseline with reasonable generation quality and low training cost. We focus on the 16 most frequent classes and limit the dataset to 50K objects to improve stability and avoid model collapse. We then train 323M-parameter text-conditional 3D diffusion model on this subset for 200K steps. Figure 2 confirms that most shapes generated by the baseline model are visually plausible. We show the trends of training FD, test FD, and ZU in Figure 3. We observe that generation quality on test prompts (i.e., test FD) plateaus Both image and 3D shape generation models operate in latent space rather than ambient space, so it is unclear whether data modality still affects memorization. We test this by comparing the two modalities in controlled setting. Setup. We render single fixed-view image for each object in the training dataset to obtain 50K-image dataset at 256 256 resolution, in one-to-one correspondence with the 3D assets. We train two models, one for images and one for shapes, using the same text prompts and the same Hunyuan3D backbone. The 3D model operates on the unordered Vecset representation, for which we omit positional embeddings, whereas the image model uses 2D latent grid, to which we add 2D positional embeddings before flattening it into sequence. Empirically, adding positional embeddings to Vecset or removing them from the image latent grid significantly degrades generation quality, so this discrepancy is necessary. For images, we use Flux VAE [3]. It also provides high reconstruction quality and matches VecSetXs latent sequence length (1024), enabling controlled comparison. Details are provided in Appendix C.3. (a) Class-conditional models. (b) Text-conditional models. Figure 4. Fine-grained conditioning increases memorization. For both class-conditional (left) and text-conditional (right) models, as the conditioning becomes finer-grained, the 3D generative models exhibit stronger memorization. and SSCD Memorization evaluation. To compare memorization across modalities, we follow prior work [44] and use SSCD and DinoV2 as copy-detection models for images, and we extend them to 3D as described in Section 3.1. Table 5 shows that, under both DinoV2 , the image generation model exhibits stronger memorization than the 3D generation model. As discussed in Section 3.1, SSCD and DinoV2 are not reliable retrieval metrics for 3D shapes. However, even if we compare ZU for shapes and images based on the most accurate distance metrics for each respective modality (i.e., LFD for shapes and SSCD for images), the ZU results still indicate that the image generation model memorizes more than the 3D generation model. This aligns with the qualitative examples in Table 4, where generated images are replications of training images, while generated shapes exhibit novel features (e.g., toy with floating wings and taller, sharper-headed Buddha). Discussion. Since each training image is single view of the underlying 3D object, which collapses spatial structure and geometric detail into near-grayscale projection, our image set naturally has less information. Thus, the image modality being more prone to memorization than the 3D modality could potentially be explained by the observation that diffusion models tend to memorize simpler data [45]. Under matched configurations, diffusion models are more prone to memorizing images than 3D shapes. 5.4. Data Diversity Setup. We study how data diversity impacts memorization by creating sub-sampled datasets of equal size from the top 16, 32, 64, and 100 most frequent classes, while preserving the class distribution of the corresponding full datasets. Memorization evaluation. In Figure 5, we compare the four settings. Higher data diversity increases memorization (i.e., leads to lower ZU ) without reducing quality, as indicated by the nearly flat test FD for 32, 64, and 100 classes. Discussion. This observation stands in contrast to prior findings on image diffusion models [16], which report that increasing dataset diversity improves generalization on Figure 5. Higher data diversity increases memorization. With fixed training set size, increasing the number of classes from 16 to 100 leads to moderate increase in memorization. CIFAR-10. However, CIFAR-10 is fundamentally limited in scale and semantic diversity, and the experiments in [16] only use up to 2000 images. In comparison, our setting is more realistic: our data are sub-sampled from ObjaverseXL, one of the largest and most diverse 3D object datasets. With fixed data budget, higher semantic diversity leads 3D diffusion models to memorize more. 5.5. Conditioning Granularity We conduct experiments with both class-conditional and text-conditional models to study how conditioning granularity affects memorization in diffusion models. Setup. For class conditioning, in addition to the original 16 fine-grained LVIS labels, we use Qwen3-VL [2] to assign each training shape to one of six coarse categories from GObjaverse [72] (animals, daily-use, electronics, furniture, human-shape, and transportation). For text conditioning, we use Qwen3-VL to generate training prompts at three increasing levels of granularity: phrase, sentence, and paragraph. Captioning prompts, implementation details, and text-length distributions are provided in Appendix C.5. Memorization evaluation. Figure 4 shows that memorization consistently increases as we move from coarse-grained conditions (i.e., broad classes or phrases) to fine-grained conditions (i.e., specific classes or paragraphs). (a) Base model (b) Large model Figure 6. Moderate guidance scales lead to the strongest memorization. For both the base model (left) and the large model (right), unconditional generation (w = 0) or generation with no guidance (w = 1) yields low memorization but poor generation quality. moderate guidance scale (w = 3) leads to the strongest memorization; further increasing the guidance scale (w > 3) reduces memorization. Discussion. As the conditions become finer-grained, it is natural that fewer training samples correspond to each condition. This reduced diversity makes it easier for the generative model to associate the specific conditions with their exact corresponding training samples [45, 51]. Fine-grained conditions have higher controllability, while coarser ones reduce memorization. Mixing prompts with different granularities may be good way to balance the two. Higher granularity in conditions increases memorization in 3D shape generation models. 6. Impact of Modeling on Memorization In the previous section, we explore how characteristics of training data can impact memorization. However, modifying training data may be unrealistic in practical settings. Thus, to identify effective ways to mitigate memorization, we further investigate the impact of modeling designs. 6.1. Guidance Scale The guidance scale is core factor in diffusion models that apply classifier-free guidance (CFG). It represents how strongly the condition is applied, following the formula: ϵ(z, c) = ϵ(z, c) + (1 w) ϵ(z) (2) where is the latent Vecset, is the conditioning embedding, and ϵ is the noise prediction network. Here, is the guidance scale; = 0 yields unconditional generation, and = 1 yields conditional generation with no guidance. Setup. We study how guidance scale impacts memorization by generating samples from the text-conditional baseline model with different guidance scales (w {0, 1, 3, 5, 7, 10}). We then repeat this study on larger (1.5B-parameter) text-conditional model (details in Appendix C.2) and class-conditional model (details in Appendix C.6) to confirm that the same trend holds. Memorization evaluation. Figure 6 shows nonmonotonic trend between guidance scale and memorization: when generating unconditionally or with no guidance, the gen top match 1 3 5 7 10 caption (top match) stylized tommy gun with drum magazine and cartoonish eyes LFD 5748 vase 3482 table lamp with conical lampshade and cylindrical base slender cylinder with circular base slender cylinder with circular base table lamp with shade 1076 4202 4134 Table 6. Guidance scale case study for the prompt table lamp with conical lampshade and cylindrical base. Memorization is strongest at guidance scale = 3, where the model most closely reproduces the training lamp. In contrast, samples at = 0 or = 1 either ignore the prompt or only weakly follow it, and samples at larger emphasize sub-phrases such as the base or the shade, rather than reproducing the full shape. memorization level is low; increasing the guidance scale to 3 significantly increases memorization, yet further increasing the guidance scale turns out to reduce memorization. Discussion. In line with previous work [16, 26], we find that generating unconditionally or with no guidance leads to reduced memorization. However, this is accompanied by poor generation quality (as shown by the high training and test FD). Moreover, studies on guided diffusion models [13, 20] show that larger guidance scales place more weight on the conditioning signal and trade diversity for higher fidelity, which has led to the common belief that, for training prompts, stronger guidance encourages memorization. Nevertheless, when the guidance scale exceeds 3, our results no longer align with this common belief. To better understand this, we identify prompts for which = 3 yields low retrieval distance, while higher values of do not, and show an example in Table 6 (two more cases in Appendix C.6). The model generates irrelevant shapes (e.g., machine gun) when = 0, generates only similar cylindrical base when = 1, faithfully generates shape highly similar to the training data when = 3, and produces shapes partially aligned with the prompts for even higher (e.g., cylindrical base, table lamp with shade). Moderate guidance scales lead to the strongest memorization, whereas larger guidance scales may drive models to focus on parts of the prompt and thereby reduce copying. 6.2. Latent Space aligned with the prompt. The model with the shortest Vecset length generates shapes that closely resemble the training shape. As the latent length increases, the model still generates high-quality shapes aligned with the text prompt, but it no longer fully reproduces specific training example. Discussion. These observations indicate that increasing the Vecset length helps models generalize better. Using the same training recipe and autoencoder, models with longer Vecsets generate shapes that are better aligned with the prompts and less overfitted to specific training examples."
        },
        {
            "title": "Longer Vecsets mitigate memorization while maintaining",
            "content": "high shape fidelity and prompt alignment. 6.3. Rotation Augmentation Figure 7. Increasing Vecset sequence length helps generalization. Generation quality improves, and memorization decreases. Setup. We study the effect of latent Vecset sequence length on memorization by training diffusion models with sequence lengths of 768, 1024 (baseline), and 1280. This does not require retraining the autoencoder [64]. In Appendix C.7, we verify that changing sequence length does not degrade the autoencoders reconstruction performance. train 1024 1280 Figure 8. Shapes generated from models trained with different Vecset lengths for the prompt blocky stylized rabbit figure. Models trained with longer Vecset lengths (1024 and 1280) generate high-quality shapes that are well aligned with the prompt, while exhibiting novel features not present in the training shape. Memorization evaluation. As shown in Figure 7, increasing the latent sequence length improves generation quality and reduces memorization. To further inspect this, we identify prompts for which the 768-Vecset model achieves low retrieval distance, while the longer-Vecset models do not. As illustrated in Figure 8 (additional examples in Appendix C.7), all models produce shapes that are visually Figure 9. Training dynamics with yaw rotation augmentation. Compared to the baseline, test FD converges more slowly, yet after convergence, memorization (as reflected by ZU ) is much lower. Data augmentation is widely used to improve generalization in deep learning [18, 62]. In 3D, however, augmentation is less straightforward. Common choices [25, 70], such as jittering or downsampling point clouds, can distort geometry and harm quality. Among shape-preserving augmentations, rotation is natural choice. Setup. We study how rotation affects memorization in 3D generative models. For simplicity, we only consider yaw rotations at four discrete angles: 0, 90, 180, and 270 (details are in Appendix C.8). Each training shape is encoded only once, after applying single random yaw rotation chosen from these angles. For each generated shape, we calculate the LFD for all four poses and take the minimum to prevent rotation from affecting retrieval performance. Memorization evaluation. Figure 9 shows the training dynamics under rotation augmentation. Compared to the baseline curves in Figure 3, the generation quality (i.e., test FD) requires substantially more training steps to converge with augmentation (350K vs. 200K). Nonetheless, once both runs converge and reach similar test FD, the model trained with rotation augmentation memorizes less, as indicated by ZU closer to 0 (e.g., -1.66 at 350K vs. -3.12 at 200K). Discussion. Rotation augmentation slows the convergence of generation quality, which can be drawback under tight training budgets. However, at similar generation quality (i.e., similar test FD), the rotation-augmented model memorizes less than the baseline. This suggests that rotation improves generalization without sacrificing performance. In 3D, where data is limited and expensive, rotation offers simple and practical way to mitigate memorization. Rotation augmentation slows convergence of generation quality but reduces memorization at similar quality levels. 7. Limitations Since our controlled experiments rely on the retrieval metric, LFD, our findings may be affected by its inaccuracy. In addition, using training prompts for text-conditional generation may bias ZU downward even for models that do generalize. Using test prompts could potentially yield more accurate ZU estimates for high-performance 3D generators. Our study is based on Vecset diffusion models, which we treat as representative of current 3D generative models. Although we expect most of our findings to generalize across architectures, some experiments, such as varying Vecset length, remain specific to this model family. Further, due to computational constraints, we conduct our controlled experiments on relatively small-scale model. Therefore, while the generation quality of our model is acceptable, it is not fully comparable with cutting-edge models. 8. Conclusion In this work, we take an initial step toward understanding memorization in 3D generative models. We design memorization evaluation framework for 3D generative models, and quantitatively evaluate the memorization level of existing 3D generators. Furthermore, through carefully controlled experiments, we find that generative models tend to memorize image data more than 3D data. Increasing data diversity or conditioning granularity, or using moderate guidance scale, results in more memorization. We propose several empirical recommendations to reduce memorization without degrading performance, such as using longer latent Vecset and applying simple rotations during training. We hope our study can provide insights for designing future 3D generative models with improved generalization. Acknowledgments. We thank Koven Yu and Yinbo Chen for helpful discussions and feedback. We are pleased to acknowledge that our experiments are primarily conducted using the Princeton Research Computing resources at Princeton University, which are managed by consortium of groups led by the Princeton Institute for Computational Science and Engineering (PICSciE) and Research Computing. We also gratefully acknowledge the support of Lambda, Inc. for providing additional computing resources."
        },
        {
            "title": "References",
            "content": "[1] Genesis Authors. Genesis: generative and universal physics engine for robotics and beyond, 2024. [2] Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025. [3] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. [4] Quentin Bertrand, Anne Gagneux, Mathurin Massias, and Remi Emonet. On the closed-form of flow matching: Generalization does not arise from target stochasticity. arXiv preprint arXiv:2506.03719, 2025. [5] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In USENIX, 2023. [6] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming Ouhyoung. On visual similarity based 3d model retrieval. In Computer graphics forum, 2003. [7] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. In CVPR, 2025. [8] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, 2019. [9] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. In CVPR, 2025. [10] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liang-Yan Gui. SDFusion: Multimodal In 3d shape completion, reconstruction, and generation. CVPR, 2023. [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. In NeurIPS, 2023. [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. large-scale dataset of 3d medical shapes for computer vision. arXiv preprint arXiv:2308.16139, 2023. [13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. [14] Daniele Giunchi, Stuart James, and Anthony Steed. Model retrieval by 3d sketching in immersive virtual reality. In 2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), 2018. [15] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. arXiv preprint arXiv:2002.10099, 2020. [16] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On memorization in diffusion models. arXiv preprint arXiv:2310.02664, 2023. [17] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem. Mvtn: Multi-view transformation network for 3d shape recognition. In ICCV, 2021. [18] Alex Hernandez-Garcıa and Peter Konig. Data augmenarXiv preprint tation instead of explicit regularization. arXiv:1806.03852, 2018. [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [21] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [22] Zehuan Huang, Yuan-Chen Guo, Xingqiao An, Yunhan Yang, Yangguang Li, Zi-Xin Zou, Ding Liang, Xihui Liu, Yan-Pei Cao, and Lu Sheng. Midi: Multi-instance diffusion for single image to 3d scene generation. In CVPR, 2025. [23] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation. In SIGGRAPH Asia, 2022. [24] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to highfidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. [25] Anastasia Ioannidou, Elisavet Chatzilari, Spiros Nikolopoulos, and Ioannis Kompatsiaris. Deep learning advances in computer vision with 3d data: survey. ACM computing surveys (CSUR), 2017. [26] Anubhav Jain, Yuya Kobayashi, Takashi Shibuya, Yuhta Takida, Nasir Memon, Julian Togelius, and Yuki Mitsufuji. Classifier-free guidance inside the attraction basin may cause memorization. In CVPR, 2025. [27] Zahra Kadkhodaie, Florentin Guth, Eero Simoncelli, and Stephane Mallat. Generalization in diffusion models arises arXiv from geometry-adaptive harmonic representations. preprint arXiv:2310.02557, 2023. [29] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman3d: High-fidelity mesh generation with 3d native genarXiv preprint eration and interactive geometry refiner. arXiv:2405.14979, 2024. [30] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. [31] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [32] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. [33] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin JohnIn son. Scalable 3d captioning with pretrained models. NeurIPS, 2023. [34] Baorui Ma, Haoge Deng, Junsheng Zhou, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Geodream: Disentangling 2d and geometric priors for high-fidelity and consistent 3d generation. arXiv preprint arXiv:2311.17971, 2023. [35] Henry Mann and Donald Whitney. On test of whether one of two random variables is stochastically larger than the other. The annals of mathematical statistics, 1947. [36] Casey Meehan, Kamalika Chaudhuri, and Sanjoy Dasgupta. non-parametric test to detect data-copying in generative models. In AISTATS, 2020. [37] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [38] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. self-supervised descriptor for image copy detection. In CVPR, 2022. [39] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. [40] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. In NeurIPS, 2017. [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [28] Jianning Li, Zongwei Zhou, Jiancheng Yang, Antonio Pepe, Christina Gsaxner, Gijs Luijten, Chongyu Qu, Tiezheng Zhang, Xiaoxi Chen, Wenxuan Li, et al. Medshapeneta [42] Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions. In ICCV, 2019. [43] Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field generation using triplane diffusion. In CVPR, 2023. [58] Boya Zeng, Yida Yin, Zhiqiu Xu, and Zhuang Liu. Generative modeling of weights: Generalization or memorization? arXiv preprint arXiv:2506.07998, 2025. [44] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In CVPR, 2023. [45] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. In NeurIPS, 2023. [46] Kiwhan Song, Jaeyeon Kim, Sitan Chen, Yilun Du, Sham Kakade, and Vincent Sitzmann. Selective underfitting in diffusion models. arXiv preprint arXiv:2510.01378, 2025. [47] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition. In ICCV, 2015. [48] Yi-Zhen Tsai, James Luo, Yunshu Wang, and Jiasi Chen. The world is too big to download: 3d model retrieval for worldscale augmented reality. In Proceedings of the 14th Conference on ACM Multimedia Systems, 2023. [49] Jarne Van den Herrewegen, Tom Tourwe, Maks Ovsjanikov, et al. Fine-tuning 3d foundation models for geometric object retrieval. Computers & Graphics, 2024. [50] Zhichuan Wang, Yang Zhou, Zhe Liu, Rui Yu, Song Bai, Yulong Wang, Xinwei He, and Xiang Bai. Describe, adapt and combine: Empowering clip encoders for open-set 3d object retrieval. In ICCV, 2025. [51] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and mitigating memorization in diffusion models. In ICLR, 2024. [52] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. In NeurIPS, 2024. [53] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation arXiv preprint made easy with spatial sparse attention. arXiv:2505.17412, 2025. [54] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In CVPR, 2025. [55] Le Xue, Ning Yu, Shu Zhang, Artemis Panagopoulou, Junnan Li, Roberto Martın-Martın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, et al. Ulip-2: Towards scalable multimodal pre-training for 3d understanding. In CVPR, 2024. [56] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [57] Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025. [59] Biao Zhang and Peter Wonka. Lagem: large geometry model for 3d representation learning and diffusion. arXiv preprint arXiv:2410.01295, 2024. [60] Biao Zhang, Matthias Nießner, and Peter Wonka. 3dilg: Irregular latent grids for 3d generative modeling. In NeurIPS, 2022. [61] Biao Zhang, Jiapeng Tang, Matthias Nießner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. In SIGGRAPH, 2023. [62] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Understanding deep learnarXiv preprint Recht, and Oriol Vinyals. ing requires rethinking generalization. arXiv:1611.03530, 2016. [63] Dengsheng Zhang and Guojun Lu. An integrated approach to shape based image retrieval. In ACCV, 2002. [64] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. In SIGGRAPH, 2024. [65] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025. [66] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In NeurIPS, 2023. [67] Xinyang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong. Sdf-stylegan: implicit sdf-based stylegan for 3d shape generation. In Computer Graphics Forum, 2022. [68] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional sdf diffusion for controllable 3d shape generation. ACM Transactions on Graphics (ToG), 2023. [69] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. In ICLR, 2024. [70] Qinfeng Zhu, Lei Fan, and Ningxin Weng. Advancements in point cloud data augmentation for deep learning: survey. Pattern recognition, 2024. [71] Chaoshun Zuo, Chao Wang, and Zhiqiang Lin. peek into the metaverse: detecting 3d model clones in mobile games. In USENIX, 2023. [72] Qi Zuo, Xiaodong Gu, Yuan Dong, Zhengyi Zhao, Weihao Yuan, Lingteng Qiu, Liefeng Bo, and Zilong Dong. Highfidelity 3d textured shapes generation by sparse encoding and adversarial decoding. In ECCV, 2024. Memorization in 3D Shape Generation: An Empirical Study"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Metrics Definition and Implementation A.2. Memorization Metrics In this section, we detail the definitions and implementation of the metrics and the evaluation framework from Section 3. A.1. Distance Metrics Chamfer distance (CD) measures the average squared nearest-neighbor distance between two point clouds. We randomly sample 4096 points to compute CD. Given two point clouds S1, S2 R3, their CD is defined as: dCD(S1, S2) = min yS2 y2 2 (cid:88) 1 S1 xS1 1 S2 (cid:88) yS2 + (1) min xS1 x2 2 Point cloud encoders encode point cloud into an embedding (S) RD. We consider the following encoders and embedding dimensions: DPointNet++ = 512, DUni3D = 1024, DULIP-2 = 1280 Given two point clouds S1 and S2, we define their L2normalized embedding distance as: demb(S1, S2) = 1 (cid:10)f (S1), (S2)(cid:11). (2) Light Field Distance (LFD) is computed by first rendering 256 256-resolution silhouettes of an object from 10 canonical viewpoints. Then, feature vector is extracted from each silhouette by concatenating its Zernike moments and Fourier descriptors. The LFD between two shapes is computed by summing the L1 distances between the corresponding view descriptors across the 10 viewpoints. We use the original implementation [6] to compute LFD. Image encoders. For each mesh , we render 12 RGB images {Ii(M )}12 i=1 at resolution 256 256 using the same pipeline in Appendix D. We encode each view with an image encoder and use the [CLS] token as the per-view representation. The final embedding (M ) RD for the mesh is obtained by averaging the [CLS] tokens across the 12 views and L2-normalizing the result. Given two meshes M1 and M2, the embedding distance induced by an image encoder is defined analogously to the point cloud encoders as: demb(M1, M2) = 1 (cid:10)f (M1), (M2)(cid:11). (3) Mann-Whitney Test is well-established nonparametric test. Following [36], we use it to detect datacopying behavior, and we only consider its global version. Let denote the underlying data distribution. The training set and the held-out test set Ptest are both drawn i.i.d. from . Let denote the set generated by generative model. Let d(x, ) be the distance from point to the training set (e.g., the distance to its nearest neighbor in under chosen metric). For each Ptest, define = d(x, ), and for each Q, define = d(y, ). When the model generalizes, the distance distributions induced by Ptest and should match. The null hypothesis, therefore, states that generated distance is larger than test distance with probability 1/2: H0 : (Ptest, Q) = 1 2 , (4) where (5) In practice, let = Ptest and = Q, and collect the (Ptest, Q) = Pr(B > A). sets of distances: {Ai}n i=1 = {d(xi, )}n i=1, {Bj}m j=1 = {d(yj, )}m j=1. The Mann-Whitney statistic for is: UQ = (cid:88) (cid:88) j=1 i=1 I(cid:2)Bj > Ai (cid:3), (6) which is an unbiased estimator of mn (Ptest, Q). Equivalently, if we rank all + values {Ai}n j=1 from smallest to largest, and let R(Bj) denote the rank of Bj, then: i=1 {Bj}m RQ = (cid:88) j= R(Bj), UQ = RQ m(m + 1) 2 . (7) Mann-Whitney z-score. Under the null hypothesis H0 and for m, 20, the statistic UQ is approximately normally distributed with mean µU and standard deviation σU : µU = mn 2 , (cid:114) σU = mn(m + + 1) . (8) We therefore define the normalized Mann-Whitney z-score as: ZU (Ptest, Q; ) = . (9) UQ µU σU Intuitively, ZU 0 indicates data-copying (generated samples are systematically closer to the training set than test samples). ZU 0 indicates generalization. A.3. Evaluation Framework (a) Memorization (b) Generalization Figure 10. Toy example illustrating the relationship between ZU and training FD. Grey crosses, gold stars, and red circles denote training, test, and generated data, respectively. Training and test data are sampled from 2D Gaussian distribution. In the left plot, the generated samples memorize the training data, whereas in the right plot, the generated samples generalize. ZU (-3.74 vs. -0.04) captures the difference in memorization between the two scenarios, while training FD does not (0.11 vs. 0.10). Encoder. We employ training FD and test FD as quality metrics. Based on Section 3.1, we select Uni3D as the best semantic encoder, and we use it in our FD calculation. Relation between ZU and training FD. While training FD often correlates with memorization (e.g., decrease in ZU generally accompanies decrease in training FD), here, we use toy example in Figure 10 to show that low training FD may not always indicate memorization. In Figure 10a, we draw training, test, and generated samples from 2D Gaussian distribution, where the generated samples are created by sampling near the training points (i.e., memorization). In Figure 10b, the generated samples follow the same distribution, but are not particularly close to the training data (i.e., generalization). ZU (-3.74 vs. -0.04) captures the difference in memorization between the two scenarios, whereas training FD is almost identical. Figure 11. Baseline model loss curves. The training and validation losses of the baseline model throughout the training process. Alternative metrics. We observe that although the baseline model progresses from low-quality to high-quality generation throughout training (see Figure 3), the training and validation losses fail to reflect this evolution. In Figure 11, both losses converge at an early stage (approximately 10K steps) and then oscillate around 0.6. Consequently, we do not consider loss to be reliable metric for quality evaluation. B. Existing Models Retrieval Visualization In this section, we visualize retrieval results for several existing models. Table 7 summarizes the conditioning type, training data, and data split for all models considered. model cond. training dataset split NFD Wavelet Generation LAS-Diffusion LAS-Diffusion 3DShape2VecSet Michelangelo 3DTopia-XL Trellis Uncond Uncond Uncond Class-cond Class-cond Text-cond Text-cond Text-cond ShapeNet chair ShapeNet chair ShapeNet chair Five ShapeNet classes ShapeNet ShapeNet 256K Objaverse subset Trellis500K - IM-NET IM-NET IM-NET 3DILG 3DILG - - Table 7. Conditioning type, training dataset, and dataset split for all evaluated models. dash (-) indicates that the official split is not publicly released. Figure 25 shows retrieval results for six models on the chair category. Figure 26 shows retrieval results for five models on their respective full training sets. The trends are consistent with the quantitative results in Section 4: models trained on smaller datasets (NFD, LASDiffusion, and Wavelet Generation) exhibit strong memorization; even among generated chairs in the 60th80th percentiles (ranked by distance to the nearest training shape), many closely match training shapes. In contrast, models trained on larger and more diverse datasets show strong generalization, with even generated shapes at lower percentiles being noticeably more diverse and exhibiting novel features. C. Detailed Experimental Setups This section details the model architectures, datasets, and training configurations employed in the controlled experiments of Section 5 and 6. Furthermore, we include supplementary experiments and qualitative visualizations to support the findings presented in the main text. C.1. Dataset and Model Setup Dataset pipeline. Our dataset is sourced from ObjaverseXL [11]. We rank classes by frequency within the Objaverse-LVIS subset and select the 100 most common single-object classes. Rather than re-captioning the entire Objaverse-XL, we use the existing Cap3D captions [33]. Caption Template: Given an object description, classify it into one of the fine-grained object classes. The description is: {description} Label Template: \"class label: {class}\" Figure 12. Instruction template used to encode Cap3D captions and object labels with Qwen3-8B Embedding. To ensure data quality, we filter caption-label pairs for semantic alignment using Qwen3-8B Embedding [65]. We compute caption and class label embeddings using the instruction templates shown in Figure 12 and measure their cosine similarity. We retain approximately 229K captionlabel pairs by applying similarity threshold of τ = 0.7. You are strict data screener for single-object 3D captions. TASK: Given (label, caption), decide if the caption predominantly and concretely describes ONE instance of the object class == label. STRICT RUBRIC (all must be true for KEEP=YES): 1) SINGLE OBJECT: Caption is about single object instance, not set, row, collection, scene, room, or environment. 2) LABEL DOMINATES: The main described entity is the label; the label isnt just small part of something else. 3) NOT PART-ONLY: The description is not mainly part-of-object (e.g., table leg, door handle), unless the label itself is such part class. 4) WHEN UNSURE, SAY NO. Favor precision over recall. SIMPLIFY CAPTION (if KEEP=YES): Remove color and material words. Keep geometry/parts/structure words that help identify the object shape. OUTPUT FORMAT (MUST be valid one-line JSON): { \"keep\": \"simple caption\": if no>\" } \"<string or empty \"yes\"\"no\", Figure 13. Prompt - Filtering out misaligned caption-label pairs. Then, we apply Qwen3-30B [56] to aggressively remove mislabeled pairs. As shown in Figure 13, we keep only pairs for which the LLM is confident and discard low-quality captions. To reduce class imbalance, we cap the most frequent object class at 10K examples. As result, we obtain around 140K caption-label pairs, which we randomly split Figure 14. Category distribution of our customized 100-category subset of Objaverse-XL. into 120K for training and 20K for testing. The class distribution of our customized dataset is shown in Figure 14. Vecset autoencoder. We use the latest pre-trained Vecset autoencoder, VecSetX, released by 3DShape2VecSet [61]. The model takes sub-sampled input point embeddings as queries and is trained with an SDF regression objective [30] and an Eikonal regularizer [15]. The bottleneck follows the normalized bottleneck autoencoder design [59]. The network consists of 24 layers of self-attention blocks with hidden dimension of 1024, and the resulting latent code has shape 1024 32 by default. Formally, let D(x, ) denote the predicted SDF value at any query spatial location given latent code , and let s(x) denote the ground-truth signed distance. The autoencoder is trained using combination of an SDF regression loss and an Eikonal loss: = LSDF + λeik Leik. The SDF regression term is: LSDF = Ex (cid:2)D(x, ) s(x)(cid:3), and the Eikonal regularization term is: Leik = Ex (cid:104)(cid:0)xD(x, )2 1(cid:1)2(cid:105) . (10) (11) (12) Diffusion backbone. We build upon the Hunyuan3D 2.1 codebase [24]. The architecture is flow-based diffusion model composed of multiple Hunyuan-DiT blocks [31]. It uses cross-attention layers to inject conditioning signals and replaces the feed-forward layers in the last few Transformer blocks with mixture-of-experts (MoE) layers. C.2. Baseline and Sanity Check In this section, we introduce three model sizes and describe controlled experiments on dataset size and model size. Baseline model configuration. Our baseline model comprises approximately 323M parameters, consists of 12 Transformer blocks with hidden dimension of 1024, and 16 attention heads. Following the official Hunyuan3D 2.1 implementation, we disable positional embeddings and attention pooling. The model is text-conditional: we use CLIP-B/16 [41] as the text encoder, giving conditioning dimension of 512. We use three MoE layers in the backbone, each with four experts and top-2 gating. model #params blocks hidden dim attn heads small baseline large 80M 323M 1.5B 12 12 512 1024 2048 8 16 16 Table 8. Hyperparameter specifications for model variants. All models share the same text encoder and MoE settings. Model scaling variants. To investigate the impact of model capacity, we introduce lightweight small model and scaled-up large model. These variants retain the same architectural components as the baseline, including the CLIPB/16 text encoder, MoE configuration, and the lack of positional embeddings, but vary in network depth and width. We summarize the specific hyperparameter settings for all three model configurations in Table 8. Dataset size sanity check. We train our baseline model with several class numbers and dataset size settings. Figure 15 shows that as the dataset size scales up, the memorization behavior is substantially reduced. Since the 16class subset contains only about 58K training shapes, for the 16-class experiments, we only use up to 50K samples. Figure 15. Larger datasets reduce memorization. With the same diffusion model, increasing the dataset size decreases ZU . Model size sanity check. We also evaluate how model size affects memorization. We train the small, baseline, and large models on 16-class subset of 50K shapes for 200K steps. Figure 16 shows that larger models exhibit stronger memorization. It is worth noting that the small model fails to generate high-quality shapes and has high FD on both the training and test sets. Figure 16. Larger models strengthen memorization. With the same training dataset, increasing the model size decreases ZU . Our experimental results are consistent with the existing literature [5, 44, 58] on the impact of dataset size and model size on memorization. C.3. Data Modality In this section, we provide additional details on the implementation and training of the image generation model, along with supplementary experimental results. Dataset. Instead of using the baseline models 16-class training data, we use the 100-class dataset. We ensure this dataset is identical to the 50K dataset used for the 100-class model presented in Figure 15. Rendering pipeline. We render the 256256 image dataset following the procedures detailed in Appendix D. To constrain the dataset size to 50K, we exclusively use the sixth view from the 12 renderings available for each object. Image autoencoder. We use the Flux VAE to encode input images of resolution 3256256 into latent grids of shape 16 32 32. After injecting 2D positional embeddings, we flatten the spatial dimensions to yield latent sequence of size 1024 16, which serves as the input to the diffusion model. We select the Flux VAE not only for its superior reconstruction capabilities but also to ensure the latent sequence length aligns with our Vecset configuration. modality DinoV2 (%) SSCD (%) LFD (%) image 3D 52.71 22.04 68.05 32.56 - 46.44 Table 9. Source data retrieval rate using the same 2500 training prompts for both image and 3D generative models. Higher retrieval rate means higher probability of reproducing the source data. Images show stronger tendency to replicate than 3D shapes. Source data retrieval rate measures how often the nearest training sample to generated sample is exactly the ground truth sample for the input prompt. While previous work [44] on large-scale text-to-image models reports that training prompts rarely regenerate their exact source images, Table 9 shows that our image model has higher probability of retrieving training data than the 3D generative model when conditioned on training captions. This further demonstrates that images are more susceptible to memorization. C.4. Data Diversity Our experiments use the 16, 32, 64, and 100 most frequent classes in the original dataset. For each class subset, we sample 50K examples by taking samples from each class in proportion to its frequency within the selected class subset. C.5. Conditioning Granularity In this section, we provide further information on how we construct class and text conditions of different granularities. Coarse-grained class labels. We use the same dataset as the baseline but introduce an additional set of coarsegrained class labels. We adopt the GObjaverse taxonomy, which originally comprises 10 categories: animals, dailyuse, electronics, furniture, human-shape, transportation, buildings&outdoor, plants, food, and poor-quality. However, since the distribution of our top-16 classes (Figure 14) shows minimal representation of buildings&outdoor, plants, and food, we exclude these categories from our experiments. Additionally, we remove the poor-quality category and re-classify any abstract shape descriptions as dailyuse. Consequently, each shape in our dataset is assigned to one of the remaining 6 coarse-grained categories (animals, daily-use, electronics, furniture, human-shape, and transportation) using the prompt shown in Figure 17. The coarse-grained category distribution is shown in Figure 18. You are precise data labeler. TASK: Given label and caption, choose exactly ONE super-category from the provided CLOSED SET. ALLOWED CATEGORIES: animals, daily-use, electronics, furniture, humanshape, transportation GUIDANCE: Use the caption semantics to determine the supercategory. Semantic Rule: If the description suggests toy, LEGO, or miniature but depicts real-world concept (e.g., toy car), choose the semantic category (e.g., Transportation). Closed Set: Never invent categories; pick ONLY from the allowed list. Fallback: If classification is truly impossible, select daily-use. OUTPUT FORMAT (Strict one-line JSON): { \"category\": \"<Category Name>\" } Figure 17. Prompt - GObjaverse category classification. Figure 18. Coarse-grained category distribution. We use six categories (animals, daily-use, electronics, furniture, humanshape, and transportation) from the GObjaverse taxonomy. Multi-granularity captioning. We generate captions using Qwen3-VL [2] across three distinct levels of granularity: phrase, sentence, and paragraph. To prompt Qwen3-VL, we randomly select four views from the 12 rendered images detailed in Appendix D. We show the lengths of different text granularities in Figure 19. The prompt used for the multi-granularity captioning is shown in Figure 20. Figure 19. Caption length distribution across granularities. The distribution of caption length for phrase, sentence, and paragraph captions demonstrates distinct levels of detail. Class-conditional model configuration. Our classconditional model shares nearly the same architecture as our baseline model; the only difference lies in the conditioning embedder. Instead of using text encoder, we construct trainable 512-dimensional embedding matrix. For each class label, we learn corresponding class embedding vector, which is then injected into the model via cross-attention. C.6. Guidance Scale In Section 6.1, we evaluate six distinct guidance scales, {0, 1, 3, 5, 7, 10}, across both our baseline and large models (details in Appendix C.2). Here, we provide an additional guidance scale experiment in Figure 21. Specifically, we run the same evaluation on class-conditional checkpoint (details in Appendix C.5). This additional result is consistent with the conclusion in Section 6.1: memorization is strongest at moderate guidance scales, while stronger guidance reduces memorization. You are given multiple views of the SAME object captured from different angles. TASK: Combine evidence across views and describe the object at three distinct granularities. GRANULARITY DEFINITIONS: Phrase: concise 3-10 word noun phrase (no punctuation at the end). Sentence: 1-2 sentences summarizing main parts, colors, and overall shape. Paragraph: 3-6 sentences covering fine details like materials, textures, distinctive features, geometry, and any context seen in the background. CONSTRAINTS: Return ONLY compact JSON object. No extra text. No markdown. No newlines. Avoid speculation; if something is unknown, say unknown. OUTPUT FORMAT (Must use these exact keys): { \"phrase\": \"...\", \"paragraph\": \"...\", \"sentence\": \"...\" } gen top match 1 3 5 10 caption (top match) geometric sculpture with trapezoidal base and rectangular prism suspended above it by rods LFD 2427 Same as = 1 Same as = 1 2467 round base, vertical stem, domed shade 7555 Same as = 7 Table 10. Guidance scale case study for the prompt geometric sculpture with trapezoidal base and rectangular prism suspended above it by rods. The model reproduces the training shape at lower scales (w {1, 3, 5}), and emphasizes sub-phrases such as trapezoidal base and rods, but fails to generate the rectangular prism at larger w. Figure 20. Prompt - Multi-granularity caption generation. gen top match caption (top match) 1 3 5 7 10 cube blocky humanoid figure standing on base chair with high curved backrest and rectangular base abstract sculpture with rectangular body, square cap, smaller square base, recessed trapezoidal shape on vertical face LFD 1396 1991 2805 Same as = 7 2188 Figure 21. Moderate guidance scales lead to the strongest memorization (class-conditional checkpoint), while further increasing the guidance scale reduces memorization, consistent with Section 6.1. Weak guidance also yields low memorization. Interestingly, in Figure 21, as the guidance scale increases from 3 to 7, training FD moves in the opposite direction of ZU , despite the two metrics being generally correlated. One possible explanation is that, in this case, increasing the guidance scale leads to lower diversity and higher similarity to training shapes in the generated outputs. Because FD measures similarity at the distribution level, reduced diversity among generated samples results in higher training FD. In contrast, ZU is computed based on objectlevel similarity and is therefore less impacted by diversity. Table 11. Guidance scale case study for the prompt blocky humanoid figure standing on base. The model generates similar but lower-quality blocky figure with no guidance, reproduces the training shape when = 3, and emphasizes sub-phrases such as figure standing on base, but fails to generate the blocky head with larger w. plete prompt. Although these scales yield novel shapes that are distinct from the training set, we argue that while large guidance scales mitigate memorization, they do so at the cost of reduced prompt alignment. C.7. Latent Space Supplementary case studies. Tables 10 and 11 present two additional case studies involving different guidance scales. These results are consistent with the discussion in Section 6.1. We observe that generated shapes produced with higher guidance scales often fail to fully align with the comVecset is produced by applying cross-attention from set of latent queries to point cloud positional embeddings. Following 3DShape2VecSet [61], the queries can be learnable vectors or sub-sampled point queries. Previous studies [7, 64] apply different sub-sampling rates to obtain dif- (y), and roll (z) axes. If metric is rotation invariant, the distance between the original and the rotated shapes should remain near zero. As illustrated in Figure 24, LFD is sensitive to rotation, with high distances between original and rotated objects for all axes. In contrast, objects rotated along the yaw axis have near-zero distance to the original objects under Uni3D. Rotation implementation. To guarantee the accuracy of ZU and FD, we limit our experiments to yaw rotation. In our dataset, the y-axis corresponds to the yaw axis. Consequently, we restrict our rotations to four discrete angles around the y-axis (0, 90, 180, and 270) to ensure Uni3D remains consistent. For LFD, we compute distances across these four distinct poses for each generated sample and report the minimum retrieval distance. Figure 24. Robustness of distance metrics to rotation. Mean and standard deviation of LFD (left) and Uni3D (right) distances between original and randomly rotated objects. LFD exhibits significant variation across all rotation axes, indicating that it is not rotation-invariant. Uni3D demonstrates strong robustness (i.e., low distances) specifically under yaw rotation. D. Rendering Details We use Blender to render = 12 views for each object at 256 256 resolution. Prior to rendering, the geometry is normalized to fit within unit cube centered at the origin. Camera poses are sampled uniformly from spherical shell with radii [1.5, 2.2], oriented to face the object center. Lighting is similarly randomized for each view using an area light with varying energy and position. To ensure alignment across the dataset, we use fixed random seed, resulting in identical camera trajectories and lighting conditions for every object. ferent Vecset lengths. We adopt similar point query design and compare three sequence lengths: 768, 1024, and 1280. latent shape dataset CD (103) F-Score (t = 0.01) (768 32) Objaverse (1024 32) Objaverse (1280 32) Objaverse 12.50 12.27 12.09 0.86 0.86 0. Table 12. Reconstruction performance of VecSetX trained on the entire Objaverse. Different Vecset lengths result in similar performance in terms of both CD and F-Score. Vecset autoencoder performance. In Table 12, we evaluate the autoencoders reconstruction performance. Varying the Vecset length does not significantly affect reconstruction quality: all three lengths yield comparable performance. Supplementary case studies. Figures 22 and 23 present two additional case studies with different Vecset lengths. The results are consistent with the discussion in Section 6.2: shapes produced with longer Vecsets maintain high fidelity while showing reduced memorization. train 768 1280 Figure 22. Shapes generated with different Vecset lengths for the prompt an assault rifle with stock, foregrip, and pistol grip; body; and long barrel. Vecsets of longer sequence lengths (1024 and 1280) generate high-quality shapes aligned with the training prompt, while exhibiting novel features (e.g., stocks and foregrips) that are different from the training shape. train 768 1024 Figure 23. Shapes generated with different Vecset lengths for the prompt wing chair. Vecsets of longer sequence lengths (1024 and 1280) generate high-quality shapes that are well aligned with the training prompt, while exhibiting novel features (e.g., arms and wings) that are different from the training shape. C.8. Rotation Augmentation Rotation invariance of distance metrics. We evaluate the robustness of LFD and Uni3D by measuring the distance between an objects original pose and its rotated poses under each metric. Specifically, we apply random rotations sampled from [0, 360] independently along the pitch (x), yaw Figure 25. Qualitative retrieval results on ShapeNets chair category. We generate 100 chairs for each model and visualize the nearest training shapes for generated samples at the 1st, 20th, 40th, 60th, 80th, and 90th percentiles of the LFD distance distribution for each model. NFD, unconditional LAS-Diffusion, and Wavelet Generation exhibit strong memorization: even generated shapes at the 60th-80th percentiles remain very close to their nearest training shapes. In contrast, conditional LAS-Diffusion, 3DShape2VecSet, and Michelangelo show novel geometric features even for generated samples with lower nearest-neighbor distances, indicating stronger generalization. Figure 26. Qualitative retrieval results on the entire training sets. We generate 100 samples for each model and visualize the nearest training shapes for generated samples at the 1st, 20th, 40th, 60th, 80th, and 90th percentiles of the nearest-neighbor LFD distance distribution for each model trained on large datasets. Across all models, the retrieved training shapes already become novel at moderate percentiles (e.g., 20th-60th). Although LFD is category-sensitive and may not always retrieve visually near-identical shapes, the overall trends suggest that these models trained on large datasets primarily generalize rather than copy individual training examples. We note that 3DTopia-XL does not generalize well and often degenerates, producing low-quality shapes even when using training prompts."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Princeton University"
    ]
}