{
    "paper_title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings",
    "authors": [
        "Yixuan Tang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 4 4 8 0 1 . 9 0 5 2 : r GAPRUNE: GRADIENT-ALIGNMENT PRUNING FOR DOMAIN-AWARE EMBEDDINGS Yixuan Tang The Hong Kong University of Science and Technology ytangch@connect.ust.hk, imyiyang@ust.hk Yi Yang"
        },
        {
            "title": "ABSTRACT",
            "content": "Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resourceconstrained environments. Model compression through pruning offers promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in oneshot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with new approach for development1."
        },
        {
            "title": "INTRODUCTION",
            "content": "The deployment of large language models in specialized domains has revealed critical challenge: while general-purpose models excel at broad language understanding, they often fail to capture domain-specific semantics crucial for real-world applications (Gu et al., 2021; Yao et al., 2024). This semantic gap is evident for embedding models, where precise representation of domain-specific concepts directly impacts downstream task performance. Consider the financial domain: liability inherently carries negative sentiment due to its association with obligations and risks, contrasting with its neutral denotation of legal responsibility in general usage. Balyasny Asset Management, leading quantitative investment firm, has reported that domain-specific embeddings demonstrate significantly higher sensitivity to such financial concepts compared to general-purpose models (Anderson et al., 2024). Similarly, in biochemistry, understanding that binding refers to molecular interactions rather than document binding can be crucial for drug discovery pipelines (Kasmaee et al., 2025). This raises the need for domain adaptation. Most existing approaches for better domain adaptation follow straightforward scaling paradigm: fine-tune increasingly larger pre-trained language models to capture domain-specific knowledge with designed training data. For example, BMEmbed (Wei et al., 2025) synthesizes specialized training data for domain-specific retriever. CodeXEmbed (Liu et al., 2024) develops series of embedding models for code retrieval ranging from 400M to 7B parameters using code-related data, 1https://github.com/yixuantt/GAPrune 1 with performance consistently improving as model size increases. This trend aligns with established scaling laws that predict better performance with more parameters (Kaplan et al., 2020). However, this scaling-centric approach creates deployment paradox in real-world applications: while larger models deliver superior results, computational efficiency may drive real-world adoption. Current usage patterns illustrate this efficiency-performance trade-off. At the time of writing, Qwen3-Embedding compact 0.6B model (Zhang et al., 2025) has garnered 3.37M downloads versus only 382K for the higher-performing 8B variant. This is nearly 9-fold difference that highlights how computational constraints drive adoption decisions over raw performance. This introduces critical research question: How can we perform domain adaptation for embedding models while achieving better deployment efficiency? Model compression through pruning offers promising solution, potentially reducing model size by 30-50% while maintaining acceptable performance (Frantar & Alistarh, 2023; Zhang et al., 2024). Yet existing pruning methods face mismatch when applied to pruning models for domain adaptation. Traditional approaches, whether using magnitude-based pruning (Han et al., 2015) or layer pruning with retraining (Zhang et al., 2024), evaluate parameter importance through uniform lens, treating all parameters equally regardless of their role in domain adaptation. This uniform treatment creates two critical failure modes. First, parameters encoding crucial domain-specific knowledge might appear unimportant from general perspective and be incorrectly removed during pruning (Bhattacharyya & Kim, 2025; Zhang et al., 2024). Conversely, calculating importance scores solely based on domain samples may cause models to lose essential general linguistic capabilities, ultimately degrading overall performance (Williams et al., 2025; Zhang et al., 2024). The result is pruned models that either lose their specialized representation or compromise their fundamental abilities. To address these limitations, we propose Gradient-Alignment Pruning (GAPrune), novel framework that explicitly balances domain-specific importance with general representation capabilities. Unlike existing methods that evaluate parameters through single lens, GAPrune measures each parameter across two critical dimensions: (1) its importance for domain-specific performance, and (2) its alignment between general and domain-specific objectives. Our method leverages Fisher Information (Theis et al., 2018) to quantify parameter importance and introduces cross-domain gradient analysis to assess objective alignment. By combining these signals using our Domain Alignment Importance (DAI) scoring, GAPrune can identify parameters that are both important in the domain and well-aligned with the general objective. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on FinMTEB and ChemTEB domains show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our work provides both practical tools for deploying efficient domainspecific models and theoretical insights into the nature of domain knowledge encoding."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work builds on three research areas: LLM-based embedding models, domain-specific embedding adaptation, and model compression for embedding models. LLM-based Embedding Models. Modern embedding models have evolved beyond traditional architectures to incorporate instruction-following capabilities. Models such as E5-MistralInstruct (Wang et al., 2024) and Qwen3-Embedding (Zhang et al., 2025) can process task-specific instructions like Given financial question, retrieve relevant documents and generate embeddings tailored to the specific task. Recent work has further extended this capability, with models like bge-en-icl Li et al. (2025a) leveraging in-context learning to enhance downstream performance. Unlike earlier approaches that relied on smaller models such as BERT (Reimers & Gurevych, 2019), these instruction-tuned models leverage extensive pre-training on diverse text corpora to achieve multi-task capabilities such as STS and retrieval. However, this flexibility comes at computational cost, as these models typically contain billions of parameters, making efficient deployment critical challenge for practical applications. 2 Figure 1: GAPrune framework overview. We compute Fisher Information for domain-specific importance and cross-domain gradient alignment. The Domain Alignment Importance (DAI) score combines these signals to identify parameters that 1) are important for domain performance and 2) well-aligned for the general and domain-specific objective. Pruning removes parameters with low DAI scores. Domain-Specific Embedding Models. The demand for domain-specific embedding models has grown significantly as there are more applications that require specialized semantic understanding, such as code agents (Liu et al., 2024) and high-stakes domains such as finance and healthcare (Anderson et al., 2024; Khodadad et al., 2025). Recent benchmarks like CoIR (Li et al., 2025b), FinMTEB (Tang & Yang, 2025) and ChemTEB (Kasmaee et al., 2025) also demonstrate that domain-specific embeddings significantly outperform general-purpose models on specialized tasks. To achieve such specialization, various adaptation training strategies have been developed such as BMEmbed (Wei et al., 2025). However, deploying these large domain-specific embedding models in resource-constrained environments presents significant challenges. Model Compression for Embedding Models. The field of model pruning has progressed significantly from its early magnitude-based foundations (Han et al., 2015). LeCun et al. (1989) pioneered the concept of optimal brain damage, using second-order derivatives to identify less critical parameters, while Li et al. (2017) showed that removing entire computational units through structured pruning could achieve better hardware efficiency than unstructured approaches. Modern large language model pruning methods like SparseGPT (Frantar & Alistarh, 2023) and Wanda (Sun et al., 2024) have pushed the boundaries further, with SparseGPT achieving 50% sparsity through Hessian-based one-shot pruning and Wanda incorporating both weight magnitudes and activation patterns. However, these methods are designed for generative LLMs and face unique challenges when applied to embedding models. Unlike generative models evaluated on perplexity and generation quality, embedding models are assessed using task-specific metrics such as nDCG@10 for retrieval and accuracy for classification (Muennighoff et al., 2023). This difference in evaluation creates unique parameter sensitivity patterns, where embedding models exhibit higher sensitivity to attention head removal as different heads capture distinct semantic relationships essential for representation quality (Voita et al., 2019). Furthermore, existing pruning methods largely treat all parameters uniformly, failing to distinguish between those essential for general linguistic understanding and those specific to particular domains. Recent work by Zhang et al. (2024) and Williams et al. (2025) has demonstrated that domain-aware pruning strategies can better preserve critical domain knowledge, but domain-aware pruning strategies specifically designed for LLM-based embedding models remain underexplored, representing critical gap in current compression literature."
        },
        {
            "title": "3 GAPRUNE: GRADIENT-ALIGNMENT PRUNING",
            "content": "In this section, we propose GAPrune, pruning framework that characterizes parameters along two dimensions: their importance for domain-specific performance and their alignment between general and domain-specific objectives. By understanding these patterns, GAPrune makes pruning decisions that preserve domain-specific knowledge while achieving substantial compression."
        },
        {
            "title": "3.1 PROBLEM FORMULATION: DOMAIN PRUNING",
            "content": "Consider pre-trained embedding model with parameters θ Rd. Traditional pruning approaches typically rely on magnitude-based criteria or single-objective importance scores, implicitly assuming that parameter importance is universal across tasks. However, this assumption may break down in domain adaptation scenarios where parameters exhibit domain-dependent behavior: some parameters encode general semantic representations crucial for semantic foundations, while others capture domain-specific patterns. We formulate domain-specific pruning as constrained optimization problem that seeks to minimize performance degradation on the target domain while maintaining desired sparsity level. Let {0, 1}d be binary pruning mask. Our objective is: min s.t. Ldom(θ m) Ldom(θ) m0 k, = (1 s) (1) (2) where [0, 1] is the target sparsity ratio, is the number of parameters to retain, is the total number of parameters, and denotes element-wise multiplication. The constraint m0 ensures that at most parameters are retained, achieving the desired compression ratio. 3.2 METHOD GAPrune operates through three sequential stages. First, we sample representative subsets from both general and domain-specific datasets to enable efficient gradient computation. Second, we characterize each parameter using Fisher Information for importance estimation and gradient cosine similarity for cross-domain alignment analysis. Finally, we combine these signals into unified importance score guided by Information Bottleneck principles, enabling principled trade-offs between compression and domain expertise preservation. Algorithm 1 presents the complete GAPrune procedure. 3.2.1 DATA PREPARATION AND SAMPLING STRATEGY Data Sources and Format. Our approach requires two datasets: general dataset to capture universal linguistic patterns and domain-specific dataset to capture specialized knowledge. The general dataset contains contrastive triplets constructed from diverse text sources, including news articles, encyclopedic entries, and conversational text, ensuring coverage of different linguistic patterns. The domain-specific dataset contains triplets tailored to the target application, such as financial triplets built from financial reports, or biomedical triplets from clinical notes. Each data sample is structured as contrastive triplet (q, p, n), where is query text serving as the anchor, is positive document semantically similar to the query (discussing the same topic or concept), and is negative document semantically dissimilar to the query. This triplet format enables us to compute gradients using InfoNCE Loss (Oord et al., 2018). Representative Sampling. For computational efficiency, we distill the essential statistical properties of both datasets into small, representative subsets that preserve the gradient patterns necessary for parameter analysis. Our sampling strategy employs k-means clustering on the embedding space to select 5,000 representative samples from each dataset while ensuring diverse coverage of the semantic space. Specifically, we use Qwen3-Embedding-0.6B (Zhang et al., 2025) to generate embeddings for the query in each triplet. The k-means clustering with = 5000 centroids and 20 iterations. For each centroid, we identify the nearest data point in the embedding space, ensuring that the selected samples are distributed across different semantic regions. This approach guarantees that our representative subset captures the full diversity of linguistic patterns present in the original datasets. These carefully constructed triplets provide the foundation for the following stages."
        },
        {
            "title": "3.2.2 CHARACTERIZING PARAMETER BEHAVIOR",
            "content": "To understand how each parameter contributes to model performance, we analyze it from two complementary perspectives. First, we measure its importance for maintaining performance using Fisher Information, which quantifies how sensitive the models predictions are to changes in that parameter. Second, we examine how the parameter affects the relationship between general and domain-specific objectives through gradient alignment analysis. Importance Estimation via Fisher Information. key challenge in domain-specific pruning is determining which parameters are truly essential for downstream performance. We employ Fisher Information (Theis et al., 2018) to quantify parameter importance. Fisher Information measures the expected curvature of the loss landscape around each parameter, providing insight into how much the models performance would degrade if that parameter were removed or significantly altered. Intuitively, parameters with high Fisher Information are those where small changes lead to large changes in the models output, making them critical for maintaining performance. For each parameter θj, we approximate the diagonal Fisher Information as: ˆFjj = (cid:19)2 1 N (cid:88) i=1 (cid:18) Li θj (3) where is the number of calibration data samples and Li is the InfoNCE (Oord et al., 2018) loss for triplet i. We compute this separately for general data (F gen ). ) and domain-specific data (F dom jj jj Gradient Alignment for Cross-Domain Analysis. While Fisher Information reveals parameter importance, it does not capture how parameters interact across different domains. We introduce gradient alignment analysis to understand whether parameter contributes positively or negatively to both general and domain-specific goals. Our approach computes gradients using the InfoNCE loss on contrastive triplets from both general and domain-specific datasets. For each parameter θj, we calculate gradients ggen by accumulating gradients over multiple batches and computing their average. Specifically, we backpropagate through the InfoNCE loss (Oord et al., 2018) computed on batches from the general and domain-specific datasets, then average the resulting gradients across all processed batches. This averaging is used for obtaining stable and representative gradient estimates. Individual batch gradients can be noisy and may not accurately reflect the true gradient direction for the entire dataset. By averaging gradients across multiple batches, we obtain more robust estimates that better represent the overall optimization landscape for each domain. and gdom We then measure gradient alignment using cosine similarity between these averaged gradient vectors: sj = , gdom ggen ggen gdom + ε (4) and gdom represent averaged gradients computed on general and domain-specific data, where ggen respectively, and ε prevents division by zero. The alignment score sj [1, 1] reveals valuable insight about parameter behavior in domainadaptive pruning. When sj > 0, the parameter exhibits consistent behavior across domains, suggesting it encodes shared knowledge that benefits both general linguistic understanding and domainspecific tasks. Such parameters represent the core semantic foundations that should be preserved to maintain model versatility. When sj 0, the parameter serves distinct roles in different contexts, indicating specialized functionality that requires careful evaluation of its domain-specific importance before pruning decisions. When sj < 0, the parameter demonstrates conflicting contributions to general and domain objectives, suggesting that this parameter contains knowledge that is beneficial for one domain but potentially harmful for the other, making it candidate for removal when prioritizing domain-specific performance."
        },
        {
            "title": "3.2.3 DOMAIN-AWARE PRUNING STRATEGY",
            "content": "The core insight of our pruning strategy is to identify parameters that are both important for domainspecific performance and well-aligned with general linguistic objectives, while removing those that create conflicts between these two goals. This dual-criteria approach ensures that the pruned model maintains both specialized domain representation and general semantic capabilities. With each parameter characterized by its domain-specific importance and its cross-domain alignment, we now introduce pruning strategy that synthesizes these signals. Our approach is guided by the Information Bottleneck (IB) principle (Tishby et al., 2000), which posits that an optimal representation should be maximally informative about target variable while being minimally complex with respect to the original input. In our context, this translates to finding parameter sub-network that maximizes fidelity to the domain-specific task while dropping information that creates conflicts between general and domain-specific objectives. To operationalize this principle, we formulate the Domain-Alignment Importance (DAI) score, which directly embodies the IB trade-off. The score evaluates each parameter θj by balancing its utility for the domain task against the representational cost of retaining its general-domain knowledge, while considering the nature of its interaction and parameter magnitude: DAIj = (cid:18) (F dom jj β gen jj ) θj + γ (cid:113) (cid:19) θj (1 + α sj g) (5) jj jj β gen The first term, (F dom jj ) θj, forms the core of our IB-inspired importance metric. It prioritizes parameters with high domain-specific Fisher Information (F dom ) while penalizing those that are primarily important for the general domain (F gen jj ), weighted by their magnitude θj. The hyperparameter β 0 controls the strength of this penalty, thus steering the trade-off between domain specialization and the preservation of general capabilities. This term effectively quantifies the net value of parameter for the target domain, scaled by its magnitude to reflect both importance and representational capacity. The second term, γ (cid:112)θj, provides magnitude-based regularization that encourages the retention of parameters with substantial representational capacity, even when their Fisher Information scores are moderate. This helps maintain model expressiveness while still allowing for domain-specific optimization. The third term, (1 + α sj g), serves as crucial modulator that refines the score based on crossdomain gradient alignment. It rewards parameters that serve multiple objectives; when parameter contributes to both domains (sj > 0), its importance is increased, as it encodes knowledge beneficial for both general understanding and the specific task. Conversely, when parameters influence conflicts with the domain objective (sj < 0), its score is reduced. This allows the model to selectively prune parameters that introduce counterproductive interference, thereby resolving optimization conflicts. The hyperparameter α > 0 dictates the sensitivity to this alignment signal. In our experiments, β is set to 1.0, α is set to 0.2, and γ is set to 0.5, providing balanced influence of importance signals, magnitude information, and alignment information. We apply one-shot pruning mask by retaining the top-k parameters with the highest DAI scores. This approach creates compressed model that is both sparse and domain-specialized, keeping parameters that work well for both general and domain-specific tasks while removing those that create conflicts. We provide detailed computational efficiency analysis in Appendix D."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we test our GAPrune approach across different settings to demonstrate its performance in domain-specific compression. 4.1 EXPERIMENTAL SETUP Domains and Datasets. We test our method on two domains: finance and chemical. These domains present different challenges for embedding model compression and require domain-specific repre6 sentations to achieve good performance. The finance domain contains specialized terminology like liquidity ratios and market capitalization that require domain expertise, along with regulatory language and quantitative expressions uncommon in general text. The chemical domain presents highly technical vocabulary with systematic nomenclature, molecular formulas, and complex entity relationships that create semantic patterns distinct from general language. For computing DAI scores as described in our method, we use three datasets. Our general dataset consists of contrastive triplets from Tang & Yang (2024), sampled from publicly available datasets such as MSMARCO (Bajaj et al., 2016) and SQuAD (Rajpurkar et al., 2016). This provides broad coverage of semantic relationships across diverse text types. For the finance domain, we use the synthesized embedding training dataset from (Tang & Yang, 2025). For the chemical domain, we construct our dataset from the chemistry subset of peS2o (Soldaini & Lo, 2023). We use GPT-4omini (OpenAI, 2025) to generate queries based on the corpus, then use these queries as anchors, the original corpus as positive documents, and employ hard negative sampling with all-MiniLM-L6v2 (Reimers & Gurevych, 2019) to generate negative documents. All datasets follow the contrastive triplet format (q, p, n) where is query, is positive document, and is negative document. We sample 5,000 examples from each dataset and ensure no overlap with the evaluation set. Models and Compression Ratio. Our experiments use two embedding models with different architectures: Qwen3-Embedding-4B (Zhang et al., 2025) and e5-mistral-instruct (Wang et al., 2024). These models represent recent LLM-based multi-task embedding models with instruction-following capabilities. We test two compression ratios: 30% and 50% sparsity on the MLP layers to examine the trade-off between compression and performance preservation. Baselines. We compare GAPrune against several pruning baselines: Dense: Corresponding dense model without pruning. Random Pruning: Randomly selects parameters for removal. Magnitude-based Pruning (Han et al., 2015): Remove parameters with the smallest absolute values, assuming that weights with lower magnitudes contribute less to model performance. The method calculates importance scores as and prunes parameters below threshold. Fisher Pruning (Theis et al., 2018): Uses Fisher Information weighted by parameter magnitude for importance scoring. We test two variants: Domain Fisher Pruning computed from domain datasets and General Fisher Pruning computed from general datasets. L3 Prune (Thennal et al., 2025): layer-wise LLM-based embedding model pruning method based on the models initial loss that removes entire layers. Following the original paper, we only compare this method in the re-training evaluation. We use the small variant of L3 Prune as it prunes similar parameters as our method. Evaluation Benchmarks. The evaluation benchmarks are FinMTEB (Tang & Yang, 2025) and ChemTEB (Kasmaee et al., 2025). For FinMTEB, we evaluate 8 classification, 2 semantic textual similarity (STS), and 8 retrieval tasks. We select these subtasks because they are the most challenging and representative of domain-specific capabilities. ChemTEB contains 17 classification tasks and 2 retrieval tasks. The detailed evaluation instructions are provided in the Appendix B. All the scores reported in experiments are the main score for each task and are detailed in the Appendix Evaluation Protocol. We conduct two types of evaluation: One-shot Pruning Evaluation and Pruneand-Retrain Evaluation. In one-shot pruning, we apply the pruning mask directly to the pre-trained model and evaluate performance without any additional training. This setting tests whether our DAI scoring can identify parameters that maintain performance immediately after pruning. In the prune-and-retrain evaluation, we first apply the pruning mask, then retrain the pruned model using the corresponding domain dataset with InfoNCE loss (Oord et al., 2018) for 100 steps. This setting examines whether our pruning strategy provides good foundation for post-pruning optimization and domain adaptation. 7 Table 1: One-shot Pruning Results on FinMTEB and ChemTEB Datasets. The Dense rows show the unpruned baseline models. The % column shows the percentage change relative to the dense model. Bold values indicate best performance. Model Method Sparsity Qwen3-Embedding-4B E5-mistral-7B-Instruct Dense Random Magnitude General Fisher Domain Fisher GAPrune (ours) Random Magnitude General Fisher Domain Fisher GAPrune (ours) Dense Random Magnitude General Fisher Domain Fisher GAPrune (ours) Random Magnitude General Fisher Domain Fisher GAPrune (ours) 30% 30% 30% 30% 30% 50% 50% 50% 50% 50% 30% 30% 30% 30% 30% 50% 50% 50% 50% 50% FinMTEB ChemTEB Retr. Cla. STS Avg. % Retr. Cla. Avg. % 0.6378 0.0111 0.6263 0.5850 0.6245 0.6278 0.0094 0.5722 0.1471 0.5528 0.5763 0.6168 0.0433 0.6148 0.5998 0.6145 0.6162 0.0095 0.6013 0.5487 0.5995 0.6096 0.6100 0.3752 0.6247 0.6017 0.6150 0.6259 0.4028 0.5984 0.5732 0.5911 0. 0.6450 0.5109 0.6459 0.5965 0.6406 0.6457 0.4212 0.6316 0.5965 0.6290 0.6387 0.3580 0.2354 0.3736 0.3807 0.3311 0.3739 0.2373 0.3807 0.3665 0.3224 0.3840 0.3801 0.3125 0.3791 0.3775 0.3694 0.3790 0.2484 0.3808 0.3780 0.3704 0. 0.5353 0.2072 0.5415 0.5225 0.5235 0.5425 0.2165 0.5171 0.3623 0.4887 0.5224 0.5473 0.2889 0.5466 0.5246 0.5415 0.5470 0.2264 0.5379 0.5077 0.5330 0.5446 0.6858 -61.29% 0.0000 +1.16% 0.6851 -2.39% 0.6327 -2.20% 0.6843 +1.35% 0. -59.55% 0.0175 -3.40% 0.6310 -32.32% 0.4826 -8.70% 0.5852 -2.41% 0.6564 0.3677 -47.21% 0.0322 -0.13% 0.3750 -4.15% 0.3640 -1.07% 0.3789 -0.06% 0.3800 -58.64% 0.0139 -1.72% 0.4085 -7.23% 0.3383 -2.61% 0.3770 -0.50% 0.4102 0.8419 0.4350 0.8426 0.8300 0.8387 0.8437 0.4714 0.8289 0.8096 0.8268 0. 0.8276 0.6263 0.8272 0.8210 0.8310 0.8314 0.4923 0.8294 0.8192 0.8287 0.8309 0.7639 0.2175 0.7639 0.7313 0.7615 0.7641 0.2445 0.7299 0.6461 0.7060 0.7462 0.5976 0.3292 0.6011 0.5925 0.6050 0.6057 0.2531 0.6189 0.5787 0.6029 0. -71.52% 0.00% -4.26% -0.30% +0.04% -68.00% -4.44% -15.42% -7.57% -2.31% -44.91% +0.58% -0.86% +1.22% +1.34% -57.65% +3.56% -3.16% +0.88% +3.84% 4.2 ONE-SHOT PRUNING EVALUATION Our one-shot pruning experiments reveal several key insights about how different pruning strategies affect embedding model performance. At 30% sparsity, GAPrune outperforms all baselines on both benchmarks, achieving +1.35% improvement on FinMTEB and +0.04% on ChemTEB for Qwen3Embedding-4B. This suggests that our DAI scoring successfully identifies parameters critical for domain-specific performance while removing those that create conflicts between general and domain objectives. The performance differences become clearer at 50% sparsity. Random pruning causes severe degradation (40-60% drop), while GAPrune maintains performance within 2.5% of the dense model. More importantly, Fisher-based methods show larger drops than our approach, particularly General Fisher pruning, which degrades by over 30% on FinMTEB. This indicates that gradient alignment provides crucial information beyond what Fisher information alone can capture. However, one-shot pruning only tells part of the story. In practice, pruned models are often retrained to recover performance, which raises the question of whether our pruning strategy provides good foundation for post-pruning optimization. 4.3 PRUNE-AND-RETRAIN EVALUATION The retraining experiments show that GAPrune not only recovers from pruning but often exceeds the original dense model performance. Qwen3-Embedding-4B improves by +4.51% on FinMTEB and +1.73% on ChemTEB after retraining, suggesting that our pruning removes redundant parameters while preserving the models learning capacity for domain-specific tasks. The comparison with L3 Prune reveals an important distinction. While L3 Prune also shows improvements after retraining, GAPrune consistently achieves higher performance across all metrics. This difference likely stems from our methods ability to preserve domain-specific knowledge during pruning, providing better starting point for retraining. The gradient alignment component appears particularly crucial here, as it helps maintain the models ability to capture specialized semantic patterns. These results hold across different architectures. E5-mistral-7B-Instruct shows similar patterns, with GAPrune achieving the best performance on both datasets. The consistency suggests that our DAI-based approach captures fundamental principles of parameter importance that generalize across model architectures and domains. 8 Table 2: Prune-and-Retrain Results on FinMTEB and ChemTEB Datasets (50% sparsity). The % column shows the percentage change relative to the dense model. Bold values indicate best performance. Model Method FinMTEB ChemTEB Retr. Cla. STS Avg. % Retr. Cla. Avg. % Qwen3-Embedding-4B E5-mistral-7B-Instruct Dense Magnitude General Fisher Domain Fisher L3 Prune GAPrune (ours) Dense Magnitude General Fisher Domain Fisher L3 Prune GAPrune (ours) 0.6378 0.6367 0.6350 0.6372 0.6361 0. 0.6168 0.5941 0.5864 0.5832 0.6083 0.6153 0.6100 0.6382 0.6317 0.6353 0.6328 0.6402 0.6450 0.6704 0.6713 0.6718 0.6710 0.6721 0.3580 0.3836 0.3632 0.3877 0.3577 0.3980 0.3801 0.3842 0.3762 0.3793 0.3426 0.3842 0.5353 0.5528 0.5433 0.5534 0.5422 0. 0.5473 0.5496 0.5446 0.5448 0.5406 0.5572 0.6858 +3.28% 0.6713 +1.49% 0.6858 +3.39% 0.6855 +1.29% 0.6866 +4.51% 0.7119 0.3677 +0.41% 0.4745 -0.49% 0.4578 -0.46% 0.4725 -1.23% 0.5071 +1.81% 0.5219 0.8419 0.8317 0.8345 0.8347 0.8418 0. 0.8276 0.8267 0.8319 0.8295 0.8302 0.8327 0.7639 0.7515 0.7601 0.7601 0.7642 0.7770 0.5976 0.6506 0.6448 0.6510 0.6687 0.6773 -1.62% -0.49% -0.49% +0.05% +1.73% +8.86% +7.89% +8.93% +11.88% +13.33% 4.4 GEOMETRIC AND ADDITIONAL ANALYSIS In this section, we further analyze the relationship between GAPrune and Fisher-based methods, demonstrating that our approach captures different parameter importance patterns and produces sparse embedding models with superior geometric properties. 4.4.1 LAYER CORRELATION ANALYSIS (a) Average importance scores per layer (b) Layer-wise performance analysis Figure 2: Layer-wise analysis of pruning methods. (a) Average importance scores per layer indicate that GAPrune assigns higher importance to parameters in critical later layers compared to Fisher-based methods. (b) Performance across different layers shows that retrieval tasks benefit significantly from later layers. To understand how different pruning methods prioritize parameters across model layers, we analyze the correlation between GAPrune and Fisher-based methods. As illustrated in the Appendix E, GAPrune shows minimal correlation with both Domain Fisher (-0.406) and General Fisher (-0.459) methods, indicating that our dual-criteria evaluation (domain importance and gradient alignment) identifies different parameters as critical compared to single-objective Fisher-based approaches. In contrast, Domain Fisher and General Fisher show high correlation (0.978) despite being computed on different datasets, suggesting that Fisher Information alone is relatively domain-agnostic and fails to distinguish between parameters important for general linguistic understanding versus those critical for domain-specific tasks. Layer-wise analysis further demonstrates GAPrunes better understanding of parameter importance in Figure 2. By extracting hidden states from different layers of Qwen3-Embedding-4B and evaluating on FinMTEB tasks, we observe that retrieval performance improves significantly in the later layers (around layer 24), where high-level semantic representations are formed. However, Fisherbased methods prune more aggressively in these critical layers, removing parameters essential for 9 retrieval performance. In contrast, GAPrunes gradient alignment component helps identify parameters that maintain both general semantic foundations and domain-specific patterns, leading to better retention of parameters in layers crucial for embedding quality."
        },
        {
            "title": "4.4.2 GEOMETRIC HYPERSPHERE ANALYSIS",
            "content": "Table 3: Embedding Analysis Results on Qwen3-Embedding-4B (Sample Size: 1000). Lower values are better for Uniformity Loss and Alignment Loss (). Higher values are better for Mutual Info., Cosine Sim., and Effective Dim. (). Bold values indicate best performance. Model Dense Magnitude General Fisher Domain Fisher GAPrune (ours) Uniformity Loss Alignment Loss Cross-Dim. Corr. Cosine Sim. Effective Dim. (out of 2560) -3.30 -3.30 -1.93 -2.82 -2.41 0.79 0.79 0.59 0.75 0.51 0.54 0.44 0.45 0.51 0.52 0.16 0.11 0.14 0. 2560 1713 1715 1605 1820 We investigate how pruning affects embedding geometry by analyzing key quality metrics. We evaluate uniformity loss (Wang & Isola, 2020) to measure how uniformly distributed embeddings are in the embedding space, alignment loss (Wang & Isola, 2020) to assess how well query embeddings align with their positive samples, cross-dimensional correlation to quantify the dimensional relationships between query and positive embeddings, effective dimensionality (Roy & Vetterli, 2007) to measure how many dimensions are actually utilized in the embedding space, and cosine similarity to measure how well the pruned model preserves the original embedding structure. These metrics are computed on sample of 1000 triplets from the finance domain dataset using Qwen3-Embedding-4B with 50% sparsity. As shown in Table 3, GAPrune achieves the best alignment between queries and positive samples (0.51 alignment loss) while maintaining high cross-dimensional correlation (0.52) and cosine similarity (0.22), indicating better preservation of semantic relationships. Magnitude pruning matches the dense models uniformity (-3.30) but suffers from poor semantic alignment, with cross-dimensional correlation dropping to 0.44 and cosine similarity to 0.16. GAPrune uses 1820 out of 2560 dimensions compared to 1605 for domain Fisher pruning, suggesting that domain-only approaches may prune too aggressively and remove parameters important for general knowledge. These results confirm that GAPrunes balanced approach maintains both statistical distribution and semantic structure."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we present GAPrune, gradient-alignment pruning framework for domain-specific embedding models. Our key insight is that effective pruning requires understanding both parameter importance and cross-domain alignment, rather than treating all parameters uniformly. By combining Fisher Information with gradient alignment, GAPrune identifies parameters that are both critical for domain performance and well-aligned with general objectives. Experiments show that GAPrune maintains performance within 2.5% of dense models at 50% sparsity in one-shot pruning, while achieving performance improvements with retraining in 100 steps. These results hold across different model architectures and domains, demonstrating that principled domain-aware compression can achieve both efficiency and performance for specialized embedding models. We hope this work will serve as valuable tool for practitioners and the community."
        },
        {
            "title": "REFERENCES",
            "content": "Peter Anderson, Mano Vikash Janardhanan, Jason He, Wei Cheng, and Charlie Flanagan. Greenback bears and fiscal hawks: Finance is jungle and text embeddings must adapt. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pp. 362370. Association for Computational Linguistics, November 2024. 10 Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Chaitali Bhattacharyya and Yeseong Kim. Finescope: Precision pruning for domain-specialized large language models using sae-guided self-data cultivation. arXiv preprint arXiv:2505.00624, 2025. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pp. 1032310337. PMLR, 2023. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1): 123, 2021. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Ali Shiraee Kasmaee, Mohammad Khodadad, Mehdi Astaraki, Mohammad Arshi Saloot, Nicholas Sherck, Hamidreza Mahyar, and Soheila Samiee. Chembed: Enhancing chemical literature search through domain-specific text embeddings. arXiv preprint arXiv:2508.01643, 2025. Mohammad Khodadad, Ali Shiraee, Mahdi Astaraki, and Hamidreza Mahyar. Towards domain specification of embedding models in medicine. arXiv preprint arXiv:2507.19407, 2025. Yann LeCun, John Denker, and Sara Solla. Optimal brain damage. Advances in neural information processing systems, 2, 1989. Chaofan Li, Minghao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Defu Lian, Yingxia Shao, and Zheng Liu. Making text embedders few-shot learners. In The Thirteenth International Conference on Learning Representations, 2025a. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In International Conference on Learning Representations, 2017. Xiangyang Li, Kuicai Dong, Yi Quan Lee, Wei Xia, Hao Zhang, Xinyi Dai, Yasheng Wang, and Ruiming Tang. CoIR: comprehensive benchmark for code information retrieval models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, pp. 2207422091. Association for Computational Linguistics, July 2025b. ISBN 979-8-89176-2510. Ye Liu, Rui Meng, Shafiq Joty, Silvio Savarese, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Codexembed: generalist embedding model family for multiligual and multi-task code retrieval. arXiv preprint arXiv:2411.12644, 2024. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. Mteb: Massive text emIn Proceedings of the 17th Conference of the European Chapter of the bedding benchmark. Association for Computational Linguistics, pp. 20142037, 2023. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. OpenAI. Gpt-4o mini: more efficient multimodal model, 2025. URL https://openai.com/ blog/gpt-4o-mini. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. 11 Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bertnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 11 2019. Olivier Roy and Martin Vetterli. The effective rank: measure of effective dimensionality. In 2007 15th European signal processing conference, pp. 606610. IEEE, 2007. Luca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. Yixuan Tang and Yi Yang. Pooling and attention: What are effective designs for llm-based embedding models? arXiv preprint arXiv:2409.02727, 2024. Yixuan Tang and Yi Yang. FinMTEB: Finance massive text embedding benchmark. In The Conference on Empirical Methods in Natural Language Processing, 2025. Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Huszar. Faster gaze prediction with dense networks and fisher pruning. arXiv preprint arXiv:1801.05787, 2018. DK Thennal, Tim Fischer, and Chris Biemann. Large language models are overparameterized text encoders. In Proceedings of the 10th Workshop on Representation Learning for NLP (RepL4NLP2025), pp. 170184, 2025. Naftali Tishby, Fernando Pereira, and William Bialek. The information bottleneck method. arXiv preprint physics/0004057, 2000. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head In Proceedings self-attention: Specialized heads do the heavy lifting, the rest can be pruned. of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 57975808. Association for Computational Linguistics, July 2019. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1189711916, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International conference on machine learning, pp. 99299939. PMLR, 2020. Yubai Wei, Jiale Han, and Yi Yang. Adapting general-purpose embedding models to private datasets In Findings of the Association for Computational Linguistics: using keyword-based retrieval. ACL 2025, pp. 68566870. Association for Computational Linguistics, July 2025. ISBN 979-889176-256-5. Miles Williams, George Chrysostomou, Vitor Jeronymo, and Nikolaos Aletras. Compressing language models for specialized domains. arXiv preprint arXiv:2502.18424, 2025. Shunyu Yao, Qingqing Ke, Qiwei Wang, Kangtong Li, and Jie Hu. Lawyer gpt: legal large In Proceedings language model with enhanced domain knowledge and reasoning capabilities. of the 2024 3rd International Symposium on Robotics, Artificial Intelligence and Information Engineering, pp. 108112, 2024. Nan Zhang, Yanchi Liu, Xujiang Zhao, Wei Cheng, Runxue Bao, Rui Zhang, Prasenjit Mitra, and Haifeng Chen. Pruning as domain-specific llm extractor. In Findings of the Association for Computational Linguistics: NAACL 2024, pp. 14171428, 2024. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025."
        },
        {
            "title": "A ALGORITHM SUMMARY",
            "content": "Algorithm 1 presents the complete GAPrune procedure, which operates through three sequential stages: (1) representative sampling to distill essential statistical properties from both general and domain-specific corpora, (2) comprehensive parameter analysis that computes Fisher information and gradient alignment scores for each parameter, and (3) unified importance scoring and pruning based on our proposed Domain-Aware Importance (DAI) metric. Algorithm 1 Gradient-Alignment Pruning (GAPrune) Require: Pre-trained embedding model with parameters θ Rd, general corpus Dgen, domain corpus Ddom, target sparsity (0, 1), trade-off parameters α, β, γ 0 Ensure: Pruned model Mpruned with sparsity 1: Stage 1: Representative Sampling 2: Sgen KMeansSample(Dgen, = 5000) {Sample representative general data} 3: Sdom KMeansSample(Ddom, = 5000) {Sample representative domain data} 4: Stage 2: Parameter Analysis 5: for each parameter θj θ do 6: Compute gradients: ggen Compute gradients: gdom Estimate Fisher info: gen Estimate Fisher info: dom θj L(M, Sgen) θj L(M, Sdom) (cid:80) i(ggen j,i )2 (cid:80) j,i )2 i(gdom +ε jj 1 Sgen jj 1 Sdom ggen ggen ,gdom gdom Compute alignment: sg,j 7: 8: 9: 10: 11: end for 12: Stage 3: DAI Scoring 13: for each parameter θj θ do 14: Compute magnitude: θj θj (cid:16) Compute DAI score: DAIj (F dom jj β gen (cid:17) jj ) θj + γ (cid:112)θj 15: 16: end for 17: Sort parameters by DAI scores: {θ(1), θ(2), . . . , θ(d)} 18: Retain top (1 s) parameters: Mpruned Mask(M, {θ(1), . . . , θ((1s)d)}) 19: return Mpruned (1 + α sg,j) Computational Complexity The computation costs of GAPrune mainly come from the parameter analysis stage. Computing gradients and Fisher information requires one forward and backward pass over samples in each calibration set, resulting in O(θN ) complexity. The remaining operations, gradient alignment and DAI score computation, are simple element-wise vector operations with O(θ) cost. This single-shot analysis makes GAPrune practical for large-scale models without iterative procedures or retraining."
        },
        {
            "title": "B EVALUATION INSTRUCTIONS",
            "content": "This section provides the evaluation instructions for all tasks used in our experiments. Table 4 shows the evaluation instructions for the 20 FinMTEB tasks used in our experiments. Table 5 shows the evaluation instructions for the 19 ChemTEB tasks used in our experiments."
        },
        {
            "title": "C BENCHMARK METRICS",
            "content": "This section provides the detailed benchmark metrics for all tasks used in our experiments. As the domain benchmarks follow the MTEB evaluation protocol (Muennighoff et al., 2023), we use the following main metrics for each task type: Classification Tasks. For classification tasks, we use accuracy as the primary metric, which measures the percentage of correctly classified samples. 13 Table 4: Evaluation instructions for FinMTEB tasks Instruction Task Retrieval Tasks (8) FiQA2018Retrieval FinanceBenchRetrieval HC3Retrieval Apple10KRetrieval TATQARetrieval FinQARetrieval USNewsRetrieval TheGoldmanEnRetrieval Classification Tasks (8) FinancialPhraseBankClassification FinSentClassification FiQAClassification SemEva2017Classification FLSClassification ESGClassification FOMCClassification FinancialFraudClassification STS Tasks (2) FinSTS FINAL Given financial question, retrieve user replies that best answer the question. Given financial question, retrieve the related context. Given financial question, retrieve relevant passages that answer the query. Given financial question, retrieve the related context. Given financial question, retrieve user replies that best answer the question. Given financial question, retrieve user replies that best answer the question. Given financial question, retrieve documents that can help answer the question. Given financial term, retrieve the related context. Classify the sentiment of given finance text as either positive, negative, or neutral. Classify the sentiment of given finance text as either positive, negative, or neutral. Perform aspect based financial sentiment classification. Classify the sentiment of given finance text as either positive, negative, or neutral. Classify the sentence into not-fls, specific fls, or nonspecific fls class. Classify the following sentence into one of the environmental,social, governance, non-esg classes. Classify the following sentence from FOMC into hawkish, dovish, or neutral class. Detecting financial fraud from the given text. Detecting Subtle Semantic Shifts in Financial Narratives. Retrieve semantically similar finance text. Retrieval Tasks. For retrieval tasks, we use nDCG@10 (Normalized Discounted Cumulative Gain at rank 10) as the primary metric, which measures the quality of ranking by considering both relevance and position of retrieved documents. Semantic Textual Similarity (STS) Tasks. For STS tasks, we use Spearman correlation as the primary metric, which measures the rank correlation between predicted similarity scores and ground truth similarity scores."
        },
        {
            "title": "D RUNTIME ANALYSIS",
            "content": "To evaluate the computational efficiency of GAPrune, we compare its floating-point operations (FLOPs) against the dense Qwen3-Embedding-4B model on 43090 GPUs. As shown in Table 6, GAPrune achieves 33.4% reduction in computational requirements. We also measure the actual runtime performance on the FiQARetrieval task, with each GPU consuming 10.0 GB of memory. The results are presented in Table 7."
        },
        {
            "title": "E METHOD CORRELATION ANALYSIS",
            "content": "This section analyzes the correlation between different pruning methods to understand how they rank parameters differently. 14 Table 5: Evaluation instructions for ChemTEB tasks"
        },
        {
            "title": "Instruction",
            "content": "Classification Tasks (17)"
        },
        {
            "title": "WikipediaChemistryTopicsClassification",
            "content": "Classify whether eye protection is required when handling the chemical based on its safety data sheet. Classify whether gloves are required when handling the chemical based on its safety data sheet. Classify chemistry texts into biometallurgical chemistry or other chemistry fields. Classify chemistry texts into bioluminescence/neurochemistry or other chemistry fields. Classify chemistry texts into chemical engineering specialties or other chemistry fields. Classify chemistry texts into different main chemistry fields. Classify chemistry texts into specific chemistry topics or general chemistry. WikipediaCompChemSpectroscopyClassification Classify chemistry texts into computational chemistry/spectroscopy or other chemistry fields. Classify chemistry texts into cryobiology/separation processes or other chemistry fields."
        },
        {
            "title": "WikipediaCryobiologySeparationClassification",
            "content": "WikipediaCrystallographyAnalyticalClassification Classify chemistry texts into crystallography/analytical chemistry or other chemistry fields. into texts chemistry"
        },
        {
            "title": "WikipediaOrganicInorganicClassification",
            "content": "WikipediaGreenhouseEnantiopureClassification Classify chemistry texts into greenhouse chemistry/enantiopure compounds or other chemistry fields. isoClassify topes/nuclear fission or other chemistry fields. Classify chemistry texts into luminescence chemistry or other chemistry fields. Classify chemistry texts into organic chemistry or inorganic chemistry. Classify into salts/semiconductors chemistry or other chemistry fields. Classify solidstate/colloidal chemistry or other chemistry fields. Classify chemistry texts into theoretical chemistry or applied chemistry."
        },
        {
            "title": "WikipediaTheoreticalAppliedClassification",
            "content": "chemistry chemistry texts texts into Retrieval Tasks (2)"
        },
        {
            "title": "ChemNQRetrieval",
            "content": "Given chemical question, retrieve the related context. Given chemical question, retrieve the related context. 15 Table 6: Computational Comparison. Table 7: FiQARetrieval Test Runtime Model FLOPs Reduction Model Time (hours) Dense GAPrune 8.24T 5.48T 33.4% Dense GAPrune 1.89 1.17 For each method, we first normalize the importance scores to rank scores (ranging from 0 to 1) based on the parameters relative importance within that method. We then compute Pearson correlation coefficients between the normalized rank scores of different methods across all common parameters. Table 8: Correlation matrix between different pruning methods based on normalized rank scores. Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation). Method Correlation with GAPrune Domain Fisher General Fisher GAPrune Domain Fisher General Fisher 1.000 -0.406 -0.459 -0.406 1.000 0.978 -0.459 0.978 1.000 The correlation analysis reveals important insights about the relationship between different pruning approaches. GAPrune shows negative correlations with both Fisher-based methods (-0.406 with Domain Fisher and -0.459 with General Fisher). This suggests that GAPrune provides complementary information beyond what Fisher Information alone can capture. The high positive correlation between Domain Fisher and General Fisher (0.978) indicates that these methods rank parameters very similarly, despite being computed on different datasets. This also suggests that Fisher Information may be relatively domain-agnostic, which could explain why both methods struggle to preserve domain-specific knowledge during pruning."
        },
        {
            "title": "F GEOMETRIC METRICS COMPUTATION",
            "content": "This section details the calculation process for each metric: uniformity loss (Wang & Isola, 2020), alignment loss (Wang & Isola, 2020), cross-dimensional correlation, and effective dimensionality (Roy & Vetterli, 2007). Uniformity Loss. The uniformity loss (Wang & Isola, 2020) measures how uniformly distributed the embeddings are in the embedding space. For set of embeddings {zi}n i=1, we compute: Luniformity = log Ei,j (cid:2)exp(tzi zj2 2)(cid:3) (6) where is temperature parameter (set to 2.0) and the expectation is taken over all pairs (i, j) with = j. Lower values indicate more uniform distribution. Alignment Loss. The alignment loss (Wang & Isola, 2020) measures how well aligned query embeddings are with their corresponding positive samples. For query embeddings {qi}n i=1 and positive embeddings {pi}n i=1, we compute: Lalignment = Ei [qi piα 2 ] (7) where α is power parameter (set to 2.0). Lower values indicate better alignment between queries and positives. Cross-Dimensional Correlation. We compute the cross-dimensional correlation between query and positive embeddings by measuring the average absolute correlation across all dimensions: 16 CDC = 1 (cid:88) k=1 corr(qk, pk) (8) where is the embedding dimension, qk and pk are the k-th dimensions of all query and positive embeddings respectively, and corr(, ) is the Pearson correlation coefficient. This metric quantifies how well the pruned model preserves the dimensional relationships between queries and their positive samples. Effective Dimensions. The effective dimensionality (Roy & Vetterli, 2007) measures how many dimensions are actually needed to capture the essential information in the embedding space. For embeddings {zi}n i=1 with dimension d, we compute: Eff. Dim. = min (cid:40) : (cid:80)k (cid:80)d i=1 σ2 (i) i=1 σ2 (i) (cid:41) 0.95 (9) where σ2 Higher values indicate better utilization of the embedding space. (d) are the sorted variances of each dimension, with σ (2) . . . σ2 (1) σ2 = Var(zk). Cosine Similarity. We compute the average cosine similarity between embeddings from the pruned model and the dense model to measure how well the pruned model preserves the original embedding structure. For pruned embeddings {zpruned }n i=1, we compute: i=1 and dense embeddings {zdense }n i Cosine Sim. = 1 (cid:88) i=1 zpruned zpruned zdense 2zdense 2 (10) Higher values indicate that the pruned model better preserves the original embedding structure."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology"
    ]
}