{
    "paper_title": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion",
    "authors": [
        "Qizhi Pei",
        "Lijun Wu",
        "Zhuoshi Pan",
        "Yu Li",
        "Honglin Lin",
        "Chenlin Ming",
        "Xin Gao",
        "Conghui He",
        "Rui Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modifications-such as rephrasing or generating syntactic variations-which fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, a novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate a new dataset, \\textbf{MathFusionQA}, followed by fine-tuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing a substantial improvement over traditional single-instruction approaches. Our datasets, models, and code are publicly available at https://github.com/QizhiPei/mathfusion."
        },
        {
            "title": "Start",
            "content": "MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion Qizhi Pei1, Lijun Wu2, Zhuoshi Pan3, Yu Li2, Honglin Lin2, Chenlin Ming4, Xin Gao2, Conghui He2, Rui Yan1,5* 1Gaoling School of Artificial Intelligence, Renmin University of China 2Shanghai AI Laboratory 3Tsinghua University 4Shanghai Jiao Tong University 5School of Computer Science, Wuhan University {qizhipei,ruiyan}@ruc.edu.cn {wulijun,heconghui}@pjlab.org.cn 5 2 0 2 0 2 ] . [ 1 2 1 2 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have shown impressive progress in mathematical reasoning. While data augmentation is promising to enhance mathematical problem-solving ability, current approaches are predominantly limited to instance-level modificationssuch as rephrasing or generating syntactic variationswhich fail to capture and leverage the intrinsic relational structures inherent in mathematical knowledge. Inspired by human learning processes, where mathematical proficiency develops through systematic exposure to interconnected concepts, we introduce MathFusion, novel framework that enhances mathematical reasoning through cross-problem instruction synthesis. MathFusion implements this through three fusion strategies: (1) sequential fusion, which chains related problems to model solution dependencies; (2) parallel fusion, which combines analogous problems to reinforce conceptual understanding; and (3) conditional fusion, which creates context-aware selective problems to enhance reasoning flexibility. By applying these strategies, we generate new dataset, MathFusionQA, followed by finetuning models (DeepSeekMath-7B, Mistral-7B, Llama3-8B) on it. Experimental results demonstrate that MathFusion achieves substantial improvements in mathematical reasoning while maintaining high data efficiency, boosting performance by 18.0 points in accuracy across diverse benchmarks while requiring only 45K additional synthetic instructions, representing substantial improvement over traditional singleinstruction approaches. Our datasets, models, and code are publicly available at https: //github.com/QizhiPei/mathfusion. * Corresponding authors: Lijun Wu (wulijun@pjlab. org.cn), Conghui He (heconghui@pjlab.org.cn), and Rui Yan (ruiyan@ruc.edu.cn). 1 Figure 1: Average performance across six benchmarks of mathematical LLMs built on Llama3-8B, along with the respective # SFT samples. MathFusion yields superior performance with fewer synthetic instructions."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning tasks (Wei et al., 2022; Huang and Chang, 2023), with mathematical problem-solving emerging as critical domain for assessing their cognitive abilities (Ahn et al., 2024). Specialized mathematical LLMs have emerged to address the unique challenges of solving complex mathematical problems (Yang et al., 2024; Shao et al., 2024; Ying et al., 2024; team, 2024). Current approaches to enhance mathematical reasoning primarily focus on four paradigms: continued pre-training with math corpora (Yang et al., 2024; Shao et al., 2024), reinforcement learning (RL) from human or automated feedback (Luo et al., 2023; Lu et al., 2024), testtime compute scaling (Wang et al., 2024a; Kang et al., 2024; Guan et al., 2025; Xi et al., 2024), and supervised fine-tuning (SFT) using problemsolution pairs (Tang et al., 2024; Tong et al., 2024). Among these, SFT is the most widely adopted paradigm (Setlur et al., 2024) due to its simplicity. However, its effectiveness is often limited by the complexity and diversity of the mathematical training data (Luo et al., 2023) during SFT. To this end, data augmentation and synthesis have emerged as promising directions to enhance mathematical reasoning. For example, approaches such as MetaMath (Yu et al., 2024) and WizardMath (Luo et al., 2023) emphasize enhancing individual problems through rephrasing and difficulty variation. While instance-level modifications have shown potential, they do not resolve the fundamental challenge: the inability of LLMs to effectively capture and leverage the intrinsic relational structures that characterize mathematical knowledge (ChuCarroll et al., 2024; Srivatsa and Kochmar, 2024). This limitation becomes particularly apparent in real-world scenarios, where complex mathematical problems are often composed of interdependent sub-problems that form intricate dependency graphs (Bagherzadeh et al., 2019; Prabawa et al., 2023). For instance, solving system of equations requires the sequential solution of individual equations, followed by the reconciliation of constraints. Motivated by the way human learners develop proficiency through systematic exposure to interconnected ideas (Komarudin et al., 2021), we propose MathFusion, novel framework that enhances mathematical reasoning by fusing different mathematical problems. The key insight behind MathFusion is that the strategic combination of complementary mathematical instructions can unlock deeper reasoning capabilities. Specifically, by combining two existing problems, MathFusion synthesizes new math problem that encapsulates the relational and compositional aspects of the original two problems. To achieve this, we introduce three distinct fusion strategies: (1) sequential fusion, which links related problems by chaining them together through shared variables to model solution dependencies; (2) parallel fusion, which integrates analogy problems to enhance conceptual comprehension and generate novel problem that encapsulates their shared mathematical essence; and (3) conditional fusion, which generates selective problems based on specific context to promote flexible reasoning. Starting from existing datasets, we first identify pairs of problems that are suitable for fusion. Then we generate new problems by applying these fusion strategies to pairs of mathematical problems that share similar types and contexts. After that, we use strong LLMs to generate corresponding solutions. The resulting dataset, MathFusionQA, is then used to fine-tune LLMs including DeepSeekMath7B, Mistral-7B, and Llama3-8B. Experimental results demonstrate that MathFusion enables LLMs to effectively capture the underlying relational structures of mathematical thereby enhancing their capacity to retasks, solve complex, multi-step problems. Moreover, MathFusion yields considerable improvements in mathematical reasoning accuracy across both indomain and challenging out-of-domain benchmarks, outperforming traditional single-instruction fine-tuning by 18.0 points in accuracy on average while incorporating only 45K additional synthetic instructions. Further integration with the state-ofthe-art (SOTA) data augmentation method DARTMath (Tong et al., 2024) leads to additional improvements, surpassing it by 1.4 points in accuracy on average while utilizing less than one-third of the data employed by DART-Math. This highlights the complementary and orthogonal nature of our approach with existing data augmentation techniques."
        },
        {
            "title": "2 Related Work",
            "content": "2."
        },
        {
            "title": "Individual Data Augmentation for Math",
            "content": "Existing mathematical data augmentation methods primarily focus on two aspects: enhancing existing data and generating new data. Enhancing existing data typically involves modifying the problem or solution. For the problem, strategies include altering the level of complexity/difficulty (Luo et al., 2023), rephrasing the wording (Yu et al., 2024; Li et al., 2024b), and employing backward reasoning (Yu et al., 2024). For the solution, methods such as generating diverse and high-quality mathematical reasoning paths through multiple calls (Yu et al., 2024; Li et al., 2024b; Zhang et al., 2024; Tong et al., 2024), and incorporating reflection (Zhang et al., 2024) are commonly used. Generating new data typically involves creating new mathematical problems based on key mathematical concepts (Tang et al., 2024), seed datasets (Ding et al., 2024), specific example (Li et al., 2024a), and then using strong mathematical models (OpenAI et al., 2023; Shao et al., 2024) to generate corresponding solutions. These methods, however, focus primarily on individual mathematical problems, overlooking the underlying relationships between different mathematical problems. 2 Figure 2: The overview of MathFusion. Given two mathematical problems PA and PB from the original mathematical dataset, MathFusion synthesizes new mathematical problem PF by fusing these two problems through three fusion strategies: sequential fusion, parallel fusion, and conditional fusion."
        },
        {
            "title": "2.2 Compositional Data Augmentation",
            "content": "Most data augmentation methods focus on eninstances, while few conhancing individual sider the relationships between different instances. mixup (Zhang et al., 2018) is an augmentation technique that addresses this gap by generating synthetic training samples through linear interpolations between pairs of input data points and their corresponding labels, which has been shown to be effective across various tasks (Cao et al., 2025; Jin et al., 2024), such as image classification (Zhang et al., 2018; Thulasidasan et al., 2019), text classification (Guo et al., 2019; Zhang et al., 2020), and neural machine translation (Guo et al., 2020; Wu et al., 2021). Mosaic-IT (Li et al., 2024c) is model-free data augmentation method that concatenates instruction data and trains LLMs with meta-instructions, thereby enhancing performance and reducing training costs. Some works also consider the composition of multiple skills or keypoints. Instruct-SkillMix (Kaur et al.) extracts core skills for instruction-following and generates new instructions by randomly combining pairs of skills. KPMath (Huang et al., 2024) shares the same idea with Instruct-SkillMix, but focuses on mathematical problems by extracting topics and key points from the problem and generates new problems by combining them. In contrast to existing works, our approach primarily focuses on fusing mathematical problems and places particular emphasis on the logical coherence of the fusion."
        },
        {
            "title": "3 MathFusion",
            "content": "example for PA and PB is shown in Example 3.1, and we show the corresponding PF for three fusion strategies in the following sections. More cases are shown in the Appendix F. Example 3.1: Original Questions PA: During one day, there are 4 boat trips through the lake. The boat can take up to 12 people during one trip. How many people can the boat transport in 2 days? PB: The school is organizing trip to the museum. 4 buses were hired to take the children and teachers to their destination. The second bus has twice the number of people on it as the first bus. The third bus has 6 fewer people than the second bus. The fourth bus has 9 more people than the first bus. If the first bus has 12 people, how many people are going to the museum in total? In the following sections, we will first introduce the problem pair construction in Section 3.1, and then introduce the three fusion strategies: sequential fusion in Section 3.2, parallel fusion in Section 3.3, and conditional fusion in Section 3.4. Based on the augmented problem sets generated by these fusion strategies, we present the MathFusionQA dataset in Section 3.5."
        },
        {
            "title": "3.1 Problem Pair Construction",
            "content": "To construct problem pairs for fusion, for each problem PA, we need to identify suitable problem PB . straightforward approach is to select problem PB that shares the same type and similar context with PA. Formally, the problem pair set Dp pair is defined as: Dp pair = (PA, PB ) PA Dp train, PB = arg max train {PA } SIM(PA, ) , The overview of MathFusion is shown in Figure 2. Given two mathematical problems PA and PB from the original mathematical training set, MathFusion synthesizes new mathematical problem PF by fusing these two problems. simple where Dp train is set of problems from the original training set, and SIM(PA, PB) is the inner product of the embeddings of PA and PB using OpenAI embedding API text-embedding-3-large (OpenAI et al., 2023)."
        },
        {
            "title": "3.2 Sequential Fusion",
            "content": "In mathematical problem-solving, sequential reasoning is common pattern where the solution of the whole problem is the sequential combination of the solutions of the sub-problems. Sequential fusion constructs new mathematical problem seq by establishing solution dependencies between two original problems PA and PB through shared variables, where the answer of PA becomes prerequisite for solving PB. Formally, the sequential fusion process and the resulting augmented problem set are defined as: is provided in Example 3.3. The total number of people transported by boat and buses over 2 days is asked to be calculated, and the input of A(the number of trips made by the boat in one day) is different from that of PA. Example 3.3: Parallel Fusion para : school organizes field trip to museum and hires 4 buses and boat. The boat makes 2 trips in one day, with capacity of 12 people per trip. Each bus has different number of people: the first bus bus has 12 people ... the fourth bus has 9 more people than the first bus. Calculate the total number of people transported by the boat and the buses over the course of 2 days. How many people can the boat and buses transport in total for the trip? seq = PB(PA), Dp seq = {P seq (PA, PB) Dp pair}. The answer from solving PA serves as part of the input to PB, thereby creating chained dependency. specific example of sequential fusion is shown in Example 3.2. The answer of the PA (the number of people transported by the boat) is used as the input for PB (the number of people in the first bus). Example 3.2: Sequential Fusion seq : The school has organized trip to museum and needs to transport children and teachers. First, calculate how many people can be transported by boat over 2 days, with 4 boat trips each day, and each trip can carry up to 12 people. Let this total be the number of people in the first bus. The second bus has twice the number of people on the first bus, the third bus has 6 fewer people than the second bus, and the fourth bus has 9 more people than the first bus. How many people are going to the museum in total?"
        },
        {
            "title": "3.3 Parallel Fusion",
            "content": "F Analogous problems often share common mathematical concepts and essences. Parallel fusion leverages this by synthesizing para through the integration of two conceptually analogous problems PA and PB , thereby creating new problem that encapsulates their shared mathematical essence. This approach emphasizes the conceptual relationships between problems rather than their sequential dependencies. The parallel fusion process and the resulting augmented problem set are formally defined as: A, PB PA Dp para = {P para F = Φ(P B, para (PA, PB) Dp A, pair}, B), and where denote the potentially modified problems from PA and PB, respectively, for the fused problem para . The function Φ encompasses various operations, such as algebraic composition and the enforcement of constraint satisfiability, to rigorously integrate the underlying mathematical structures. concrete illustration of parallel fusion"
        },
        {
            "title": "3.4 Conditional Fusion",
            "content": "Context-aware reasoning necessitates the dynamic selection or comparison of solutions based on conditional constraints. Conditional fusion synthesizes cond by integrating PA and PB into cohesive real-world scenario, where the final solution is derived through contextual comparison or selection of outcomes from PA and PB. Formally, the conditional fusion process and the resulting augmented problem set are defined as: = Γ(PA, PB), Dp cond cond = {P cond (PA, PB) Dp pair}. Γ is comparison function that contrasts PA and PB based on predefined logical or contextual rules. concrete case is shown in Example 3.4, where the final solution is determined by comparing the answers of PA (the capacity of the boat) and PB (the capacity of the buses) in real-world scenario (organizing lake excursion and museum trip). Example 3.4: Conditional Fusion cond : local community is organizing two different outings. For lake excursion, boat operates 4 trips day with capacity of 12 people per trip. They plan to run this boat service for 2 days. Meanwhile, school is arranging trip to the museum with 4 buses. The first bus has 12 people, the second bus has twice as many people as the first, the third bus has 6 fewer people than the second, and the fourth bus has 9 more people than the first bus. Given these arrangements, which mode of transportation has larger capacity for transporting people? To clarify, the core difference between parallel fusion and conditional fusion is that: parallel fusion combines PA and PB to form novel para , where the input of para may be different from the original PA and PB; while conditional fusion compares the results of PA and PB, the input of cond is the same as PA and PB, and the output is based on the comparison of the results of PA and PB. F"
        },
        {
            "title": "Dataset",
            "content": "# Samples WizardMath (Luo et al., 2023) MetaMathQA (Yu et al., 2024) MMIQC (Liu et al., 2024) Orca-Math (Mitra et al., 2024) Xwin-Math-V1.1 (Li et al., 2024a) KPMath-Plus (Huang et al., 2024) MathScaleQA (Tang et al., 2024) DART-Math-Uniform (Tong et al., 2024) DART-Math-Hard (Tong et al., 2024) RefAug (Zhang et al., 2024)"
        },
        {
            "title": "MathFusionQA",
            "content": "96K 395K 2294K 200K 1440K 1576K 2021K 591K 585K 30K 60K Table 1: Comparison between MathFusionQA and previous mathematical datasets. Our MathFusionQA is generally smaller than others."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Data Synthesis: We use GPT-4o-mini (OpenAI et al., 2023) to fuse the problems and generate the corresponding solutions. The details about generation and corresponding prompts are shown in Appendix B.1 and A). Training: We conduct standard instructiontuning on our MathFusionQA. Following DARTMath (Tong et al., 2024), we conduct experiments on two categories of base models: 7B mathspecialized base LLM, specifically DeepSeekMath7B (Shao et al., 2024), and 7-8B general base LLMs, specifically Mistral-7B (Jiang et al., 2023) and Llama3-8B (Dubey et al., 2024). We finetune each base model with three fusion strategiessequential, parallel, and conditionaleach of which is the union of GSM8K, MATH, and the augmented set generated by the corresponding fusion strategy. Table 4 shows the statistics of the MathFusionQA collection. All models are trained for 3 epochs for simplicity. More details about the training setup are provided in Appendix B.2. Evaluation: Following DART-Math (Tong et al., 2024), we evaluate the models on two in-domain (ID) benchmarks: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), as our MathFusionQA dataset is built upon these two datasets. For out-of-domain (OOD) evaluation, we use the CollegeMath (Tang et al., 2024), DeepMind-Mathematics (Saxton et al., 2019), OlympiadBench-Math (He et al., 2024), and TheoremQA (Chen et al., 2023) benchmarks. We use greedy decoding to generate solutions for the problems in test sets. We report the accuracy in 0-shot setting for all models following Tong et al. (2024). More details about the evaluation setup and benchmarks are provided in the Appendix B.3. Baselines: We mainly compare our MathFusion models with mathematical instruction-based models, which can be categorized into three groups: (1) Previous top-performing models, including MetaMath (Yu et al., 2024), WizardMath (Luo et al., 2023), RFT (rejection sampling fine-tuning) (Yuan et al., 2023; Tong et al., 2024), MMIQC (Liu et al., 2024), MathScale (Tang et al., 2024), DeepSeekMath-7B-Instruct (Shao et al., 2024), RefAug (Zhang et al., 2024), and DARTMath (Tong et al., 2024) (we report the Prop2Diff version); (2) Models instruction-tuned on the combination of GSM8K and MATH datasets (noted as standard setting); (3) Models instruction-tuned on the sampled 60K version of previous topperforming methods to further evaluate the data efficiency of different mathematical data augmentation methods. Details about the sampling method are introduced in Appendix B.2."
        },
        {
            "title": "4.2 Main Results",
            "content": "The main results are shown in Table 2. We summarize several key findings as follows: 5 Model # Samples In-Domain Out-of-Domain MATH GSM8K College DM Olympiad Theorem AVG DeepSeekMath (7B Math-Specialized Base Model) DeepSeekMath-7B-RFT DeepSeekMath-7B-DART-Math DeepSeekMath-7B-Instruct DeepSeekMath-7B-MMIQC DeepSeekMath-7B-Standard DeepSeekMath-7B-RefAug MathFusion-DSMath-7B (Sequential) MathFusion-DSMath-7B (Parallel) MathFusion-DSMath-7B (Conditional) DeepSeekMath-7B-MetaMath DeepSeekMath-7B-MMIQC DeepSeekMath-7B-RefAug DeepSeekMath-7B-DART-Math MathFusion-DSMath-7B 590K 590K 780K 2.3M 15K 30K 30K 30K 30K 60K 60K 60K 60K 60K 53.0 53.6 46.9 45.3 30.6 32.1 49.9 50.9 48.5 40.0 26.3 33.1 51.4 53.4 88.2 86.8 82.7 79.0 66.3 71.2 76.6 76.7 74.6 79.0 60.6 71.6 82.9 77. 41.9 40.7 37.1 35.3 22.7 26.0 38.8 38.9 37.0 33.2 19.2 26.2 39.1 39.8 Mistral-7B (7-8B General Base Model) Mistral-7B-MetaMath Mistral-7B-WizardMath-V1.1 Mistral-7B-RFT Mistral-7B-DART-Math Mistral-7B-MathScale Mistral-7B-MMIQC Mistral-7B-Standard Mistral-7B-RefAug MathFusion-Mistral-7B (Sequential) MathFusion-Mistral-7B (Parallel) MathFusion-Mistral-7B (Conditional) Mistral-7B-MetaMath Mistral-7B-MMIQC Mistral-7B-RefAug Mistral-7B-DART-Math MathFusion-Mistral-7B 400K 418K 590K 590K 2.0M 2.3M 15K 30K 30K 30K 30K 60K 60K 60K 60K 60K 29.8 32.3 38.7 45.5 35.2 37.4 12.4 15.1 32.7 30.9 26.3 22.7 17.3 17.4 34.1 41.6 76.5 80.4 82.3 81.1 74.8 75.4 60.3 61.1 73.9 75.1 73.0 70.8 61.4 63.1 77.2 79. 19.3 23.1 24.2 29.4 21.8 28.5 8.4 10.4 18.9 20.9 15.6 14.1 11.1 12.5 23.4 24.3 Llama3-8B (7-8B General Base Model) Llama3-8B-MetaMath Llama3-8B-RFT Llama3-8B-MMIQC Llama3-8B-DART-Math Llama3-8B-Standard Llama3-8B-RefAug MathFusion-Llama3-8B (Sequential) MathFusion-Llama3-8B (Parallel) MathFusion-Llama3-8B (Conditional) Llama3-8B-MetaMath Llama3-8B-MMIQC Llama3-8B-RefAug Llama3-8B-DART-Math MathFusion-Llama3-8B 400K 590K 2.3M 590K 15K 30K 30K 30K 30K 60K 60K 60K 60K 60K 32.5 39.7 39.5 46.6 17.5 20.8 38.8 38.1 34.7 28.7 24.4 20.3 39.6 46.5 77.3 81.7 77.6 81.1 65.4 67.3 77.9 75.4 76.9 78.5 69.7 68.6 82.2 79. 20.6 23.9 29.5 28.8 12.9 15.7 25.1 25.5 21.2 19.7 13.4 15.5 27.9 27.9 60.2 61.6 52.2 52.9 28.6 38.4 64.6 62.2 55.2 45.9 41.5 35.4 62.8 65.8 28.0 38.4 35.6 45.1 38.0 17.0 15.4 29.3 26.5 21.4 27.2 13.5 18.1 36.0 39.2 35.0 41.7 41.0 48.0 21.6 25.9 42.0 41.9 27.4 31.3 30.9 29.1 39.9 43.4 19.1 21.7 14.2 13.0 5.6 10.1 21.6 19.0 19.3 9.5 10.4 10.5 21.0 23.3 5.9 7.7 8.7 14.7 9.4 2.2 3.1 9.3 11.0 7.3 5.0 5.0 3.9 8.7 13. 5.5 9.3 9.6 14.5 4.7 4.7 12.6 11.9 11.9 5.3 5.2 5.5 12.9 17.2 27.2 32.2 28.1 23.4 11.0 14.4 22.8 23.8 19.0 18.9 6.8 14.0 27.4 24.6 14.0 16.6 16.2 17.0 16.2 7.6 11.0 15.5 15.2 12.8 12.2 5.9 11.1 18.2 18.1 13.8 14.9 16.2 19.4 10.9 13.6 17.0 18.9 15.5 16.1 10.6 13.0 22.9 20.0 48.3 49.4 43.5 41.5 27.5 32.0 45.7 45.3 42.3 37.8 27.5 31.8 47.4 47.5 28.9 33.1 34.3 38.8 34.2 18.0 19.4 29.9 29.9 26.1 25.3 19.0 21.0 32.9 36. 30.8 35.2 35.6 39.7 22.2 24.7 35.6 35.3 31.3 29.9 25.7 25.3 37.6 39.0 Table 2: Performance comparison on mathematical benchmarks including MATH, GSM8K, CollegeMATH (College), DeepMind-Mathematics (DM), OlympiadBench-Math (Olympiad), and TheoremQA (Theorem). The table is organized by the base model and the number of training samples, using 60K as the threshold for splitting. The best results are highlighted in bold. Rows are sorted according to data size. Most of the baseline results are derived from DART-Math (Tong et al., 2024), except for the Standard, RefAug (Zhang et al., 2024), and baseline labeled with , which are our own runs. Sequential, Parallel, and Conditional indicate training on the union of GSM8K, MATH, and the respective fused dataset. 6 Finding 1: Three fusion strategies consistently enhance the model performance. For all three fusion strategies-sequential, parallel, and conditional fusionthe MathFusion models consistently surpass the standard settings across all base models and evaluation benchmarks. Specifically, on MATH and GSM8K test sets, using Llama3-8B as the base model, MathFusion (sequential) achieves 21.3 and 12.5 accuracy improvement; MathFusion (parallel) achieves 20.6 and 10.0 accuracy improvement; and MathFusion (conditional) achieves 18.0 and 11.9 accuracy improvement, respectively, compared to the standard setting. For four OOD benchmarks, the single fusion strategy also outperforms the standard setting, with 9.9 accuracy improvement on average. These improvements demonstrate the effectiveness of the three fusion strategies in enhancing both the ID and OOD generalization performance of the models. Finding 2: Among three fusion strategies, sequential fusion and parallel fusion generally perform better than conditional fusion. possible reason is that the conditional fusion requires no modification of input structures or problem dependencies, merely performing direct comparison or selection between the solutions of two independent problems without necessitating additional mathematical transformations or reformulations. We further investigate the difficulty of the problems generated by the three fusion strategies in Section 5.1. Finding 3: Combination of three fusion strategies further improves performance. As the three fusion strategies capture different aspects of the problem fusion, we further investigate the performance of the combined fusion strategies. From Table 2, we observe that the combined fusion strategies consistently outperform each single fusion strategy, indicating that the combination of three fusion strategies can further enhance the models mathematical ability. Additionally, the weaker the performance of the base model, the more enhancements the combined fusion strategies can bring. Specifically, the combined fusion strategies achieve an average accuracy improvement of 3.1 points on DeepSeekMath-7B, 4.9 points on Llama3-8B, and 7.5 points on Mistral-7B across all benchmarks. Finding 4: Compared with previous topperforming baselines, MathFusion models yields competitive performance and high data efficiency. For each single fusion strategy, MathFusion models outperform RefAug, which has the same data size as MathFusion, on all benchMethod Sequential Parallel Conditional MATH GSM8K Standard MathFusion 17.5 42.6 43.0 43.6 45.6 65.4 78.2 76.9 79.2 79. Table 3: Effect of three fusion strategies on Llama3-8B. marks. After combining the three fusion strategies, MathFusion outperforms previous top-performing baselines like MetaMath and DART-Math on average under the same data size setting. Specifically, MathFusion yields consistently better performance on MATH, DeepMind-Mathematics, and OlympiadBench-Math benchmarks. These results demonstrate the high data efficiency and generalization ability of MathFusion. MathFusion maintains also competitive efficacy compared to topperforming models in the full-data regime, exhibiting only marginal average performance drop on Llama3-8B and DeepSeekMath-7B."
        },
        {
            "title": "4.3 Ablation Study",
            "content": "We further conduct an ablation study to investigate the contribution of each fusion strategy to the overall performance of combined fusion. The results over Llama3-8B on MATH and GSM8K are shown in Table 3, from which we observe that each fusion strategy contributes to the overall performance, with conditional fusion showing the least contribution, which aligns with Section 4.2."
        },
        {
            "title": "5.1 Difficulty Analysis",
            "content": "In this section, we explore why the three fusion strategies effectively enhance the models performance. To achieve this, we evaluate both the perplexity (PPL) and instruction following difficulty (IFD) (Li et al., 2024d) for the original and fused data. We use Mathstral-7B (team, 2024), model built upon Mistral-7B (Jiang et al., 2023) and specifically fine-tuned for mathematical reasoning, to ensure our analysis relies on model specifically designed for mathematical tasks. Specifically, we denote the unconditioned PPL as PPL(S), the conditioned PPL as PPL(S ), and IFD = PPL(S )/PPL(S), where is the problem and is the solution. The results are shown in Figure 3(a) and 3(b), from which we can see: (1) The PPL of the solution of the fused problems is significantly lower than that of the original problems. As 7 Figure 3: (a): Unconditional and conditional PPL for the original and fused data on GSM8K and MATH datasets. (b): IFD for the original and fused data on GSM8K and MATH datasets. (c): Performance scaling behavior of the MathFusion on different sizes of augmented data on Llama3-8B. Figure 4: (a): Average performance of the Llama3-8B models fine-tuned on the combined dataset of MathFusionQA and DART-Math-Hard with different sizes of sampled data. (b) and (c): Problem embedding visualization for GSM8K and MATH datasets via t-SNE. analyzed in Yu et al. (2024), this may be due to the easy-to-learn nature of the generated solutions. (2) The IFD of the fused data is significantly higher than that of the original data, indicating that the fused data is more difficult to learn in the context of the problem. (3) The IFD of the MATH datasets, both the original or fused version, are higher than that of the GSM8K, consistent with the fact that MATH is generally more difficult than GSM8K."
        },
        {
            "title": "Size and Performance",
            "content": "We study the performance scaling behavior of the MathFusion on different sizes of augmented data on Llama3-8B. We select MATH as the original training set and gradually increase the size of the augmented fusion data from 0 to 22.5K, with step size of 2.5K. The results on MATH and four OOD benchmarks are shown in Figure 3(c). We observe that the performance of the MathFusion models exhibits an approximate logarithmic growth with respect to the amount of augmented data, which is consistent with the findings in (Li et al., 2024b). Additionally, the augmented fusion data from MATH dataset can also generalize better to the OOD benchmarks as the size of the augmented data increases. In summary, the MathFusion shows consistent performance improvement with different sizes of augmented data."
        },
        {
            "title": "5.3 Combination with Other Datasets",
            "content": "We further investigate the performance of MathFusion when combined with other data augmentation methods. Specifically, we downsample 30K-180K data from DART-Math-Hard (Tong et al., 2024), which is the SOTA method for mathematical data augmentation with 590K data. We combine the downsampled DART-Math-Hard with our MathFusionQA dataset and fine-tune Llama3-8B models on the combined dataset. The results are presented in Table 4(a). As the size of sampled data increases, the average performance of the models also increases, and reaches the peak when the size of the sampled data is 120K. Notably, by only using 90K data sampled from DART-Math-Hard (i.e., 150K samples in total), the resulting model achieves better performance than both DART-Math and MathFusion, yields SOTA average performance. These results show the potential of combining MathFusion with other data augmentation methods to further enhance the models performance. We think that the enhancement arises from the complemen8 tary and orthogonal nature of the two methods: our MathFusion emphasizes fusing mathematical problems to generate more challenging and diverse problems, while DART-Math focuses on existing difficult problems and primarily generates additional solutions for them."
        },
        {
            "title": "5.4 Diversity Analysis",
            "content": "To further investigate the effectiveness of the MathFusion in enhancing the data diversity, we visualize the problem embeddings of the GSM8K and MATH datasets generated by GPT-4o-mini using t-SNE (Van der Maaten and Hinton, 2008). The results are shown in Figure 4(b) and 4(c). We can observe that the MathFusion augmented problems are more evenly distributed in the embedding space, thereby enriching the diversity of the training examples and mitigating the risk of model overfitting."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we focus on the fusion of mathematical problems. We propose novel mathematical data augmentation method, MathFusion, which comprises three distinct fusion strategiessequential fusion, parallel fusion, and conditional fusiondesigned to synthesize augmented mathematical problems. Leveraging these fusion strategies, we construct the MathFusionQA dataset, which is subsequently employed to fine-tune LLMs. Extensive experiments on three base models and six benchmarks show that MathFusion exhibits robust performance in both the in-domain and out-ofdomain benchmarks while maintaining high data efficiency."
        },
        {
            "title": "Limitations",
            "content": "We utilize strong GPT-4o-mini to generate fused problems and solutions, but the generated problems or solutions may still contain errors or ambiguities, which are hard to detect and verify. The quality of the generated problems and solutions is limited by the capabilities of the teacher LLM. We mainly explore the effectiveness of the three fusion strategies on problem pairs that are constructed by embedding similarity. The fusion of three or more problems and more effective ways to find similar problems, remain unexplored."
        },
        {
            "title": "References",
            "content": "models for mathematical reasoning: Progresses and challenges. In EACL (Student Research Workshop), pages 225237. Association for Computational Linguistics. Mehdi Bagherzadeh, Andrei Gurca, and Sabine Brunswicker. 2019. Problem types and open innovation governance modes: project-level empirical exploration. IEEE Transactions on Engineering Management, 69(2):287301. Chengtai Cao, Fan Zhou, Yurou Dai, Jianping Wang, and Kunpeng Zhang. 2025. survey of mixbased data augmentation: Taxonomy, methods, applications, and explainability. ACM Comput. Surv., 57(2):37:137:38. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. TheoremQA: theorem-driven question answering dataset. In The 2023 Conference on Empirical Methods in Natural Language Processing. Jennifer Chu-Carroll, Andrew Beck, Greg Burnham, David OS Melville, David Nachman, Erdem Özcan, and David Ferrucci. 2024. Beyond llms: Advancing the landscape of complex reasoning. arXiv preprint arXiv:2402.08064. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Yuyang Ding, Xinyu Shi, Xiaobo Liang, Juntao Li, Qiaoming Zhu, and Min Zhang. 2024. Unleashing reasoning capability of llms via scalable arXiv preprint question synthesis from scratch. arXiv:2410.18693. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Demi Guo, Yoon Kim, and Alexander M. Rush. 2020. Sequence-level mixed sample data augmentation. In EMNLP (1), pages 55475552. Association for Computational Linguistics. Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019. Augmenting data with mixup for sentence clasarXiv preprint sification: An empirical study. arXiv:1905.08941. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, 9 Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, volume 1. Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: survey. In ACL (Findings), pages 10491065. Association for Computational Linguistics. Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. 2024. Key-point-driven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Xin Jin, Hongyu Zhu, Siyuan Li, Zedong Wang, Zicheng Liu, Chang Yu, Huafeng Qin, and Stan Li. 2024. survey on mixup augmentations and beyond. arXiv preprint arXiv:2409.05202. Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, et al. 2024. Mindstar: Enhancing math reasoning in pre-trained llms at inference time. arXiv preprint arXiv:2405.16265. Simran Kaur, Simon Park, Anirudh Goyal, and Sanjeev Arora. Instruct-skillmix: powerful pipeline for llm instruction tuning. In NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability. Komarudin Komarudin, Suherman Suherman, and Anita Anggraini. 2021. Analysis of mathematical concept understanding capabilities: The impact of makerspae stem learning approach models and student learning activities. Journal of Innovation in Educational and Cultural Research, 2(1):3543. Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. 2024a. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706. Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. 2024b. Mugglemath: Assessing the impact of query and response augmentation on math reasoning. In ACL (1), pages 10230 10258. Association for Computational Linguistics. Ming Li, Pei Chen, Chenguang Wang, Hongyu Zhao, Yijun Liang, Yupeng Hou, Fuxiao Liu, and Tianyi Zhou. 2024c. Mosaic it: Enhancing instruction tuning with data mosaics. arXiv preprint arXiv:2405.13326. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024d. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In NAACL-HLT, pages 7602 7635. Association for Computational Linguistics. Haoxiong Liu, Yifan Zhang, Yifan Luo, and Andrew Chi-Chih Yao. 2024. Augmenting math word problems via iterative question composing. Preprint, arXiv:2401.09003. Zimu Lu, Aojun Zhou, Ke Wang, Houxing Ren, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. 2024. Step-controlled dpo: Leveraging stepwise error for enhanced mathematical reasoning. arXiv preprint arXiv:2407.00782. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. 2024. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830. Josh OpenAI, Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Harsa Wara Prabawa, Rizky Rosjanuardi, and Elah Nurlaelah. 2023. Problem decomposition skills, mathematical maturity, and their relation to mathematics problem-solving in computer science learning class. Jurnal Kependidikan: Jurnal Hasil Penelitian dan Kajian Kepustakaan di Bidang Pendidikan, Pengajaran dan Pembelajaran, 9(3):946958. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. 2024. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532. 10 An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. 2024. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332. Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. 2023. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825. Hongyi Zhang, Moustapha Cissé, Yann N. Dauphin, and David Lopez-Paz. 2018. mixup: Beyond empirical risk minimization. In ICLR (Poster). OpenReview.net. Rongzhi Zhang, Yue Yu, and Chao Zhang. 2020. Seqmix: Augmenting active sequence labeling via sequence mixup. In EMNLP (1), pages 85668579. Association for Computational Linguistics. Zhihan Zhang, Tao Ge, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang. 2024. Learn beyond the answer: Training language models with reflection for mathematical reasoning. In EMNLP, pages 1472014738. Association for Computational Linguistics. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. KV Aditya Srivatsa and Ekaterina Kochmar. 2024. What makes math word problems challenging for llms? In NAACL-HLT (Findings), pages 11381148. Association for Computational Linguistics. Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. 2024. Mathscale: Scaling instruction tuning for mathematical reasoning. In ICML. OpenReview.net. Mistral AI team. 2024. Learning to reason with llms. Sunil Thulasidasan, Gopinath Chennupati, Jeff A. Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. 2019. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. In NeurIPS, pages 1388813899. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. 2024. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. In NeurIPS. Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(11). Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2024a. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. In ICLR. OpenReview.net. Ming Wang, Yuanzhong Liu, Xiaoyu Liang, Songlian Li, Yijie Huang, Xiaoming Zhang, Sijia Shen, Chaofeng Guan, Daling Wang, Shi Feng, et al. 2024b. Langgpt: Rethinking structured reusable prompt design framework for llms from the programming language. arXiv preprint arXiv:2402.16929. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS. Xueqing Wu, Yingce Xia, Jinhua Zhu, Lijun Wu, Shufang Xie, Yang Fan, and Tao Qin. 2021. mixseq: simple data augmentation methodfor neural machine translation. In IWSLT, pages 192197. Association for Computational Linguistics. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. 2024. Enhancing llm reasoning via critique models with test-time and trainingtime supervision. arXiv preprint arXiv:2411.16579."
        },
        {
            "title": "A Prompts",
            "content": "We show the prompts used for Sequential Fusion in Prompt 1, Parallel Fusion in Prompt 2, and Conditional Fusion in Prompt 3. We also provide the problem evaluation prompts in Prompt 4, which is partially derived from WizardMath (Luo et al., 2023). We use LangGPT (Wang et al., 2024b) to format prompts in Markdown and polish them."
        },
        {
            "title": "B General Settings",
            "content": "B.1 Data Synthesis We synthesize the augmented data, both the fusion process and the generation of the corresponding solutions, using GPT-4o-mini(gpt-4o-mini-202407-18) (OpenAI et al., 2023). We set the temperature to 0.7 and the maximum length of generation to 4096. The statistics of the generated data, as well as the base GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) datasets, are shown in Table 4. Dataset Standard GSM8K MATH Total 7.5K 7.5K MathFusionQA (Sequential) MathFusionQA (Parallel) MathFusionQA (Conditional) MathFusionQA 15K 15K 15K 30K 15K 15K 15K 30K 15K 30K 30K 30K 60K Table 4: Statistics of the MathFusionQA dataset and the original datasets GSM8K and MATH. B.2 Training We use LLaMA-Factory (Zheng et al., 2024) to fine-tune the models. All models, including our own reproductions of baselines, are fine-tuned for 3 epochs with batch size of 128 on 8xNVIDIA A100 GPU. The peak learning rate is 5e-6 with linear warm-up for the first 3% of the training steps, followed by cosine decay. The maximum sequence length is set to 4096. In Table 2, we reproduce the results of the baselines with 60K data. For MetaMath (Yu et al., 2024), MMIQC (Liu et al., 2024), and DARTMath (Tong et al., 2024), we directly downsample 60K data from the original datasets randomly. For RefAug (Zhang et al., 2024), the original training set only contains 30K data, with 15K from GSM8K and MATH, and 15K from the augmented reflection data. To upsample the RefAug dataset to 60K, we re-generate the reflection data two times using GPT-4o-mini with the original prompts (Zhang et al., 2024), thus obtaining an additional 30K data and forming the 60K dataset. B.3 Evaluation We compare MathFusion models with baselines on the following six benchmarks: GSM8K (Cobbe et al., 2021) dataset includes 8,792 high-quality grade school math word problems, with 7,473 for training and 1,319 for testing. Each problem in GSM8K requires between 2 and 8 steps to solve. MATH (Hendrycks et al., 2021) dataset is composed of 12,500 problems from high school math competitions, with 7,500 for training and 5,000 for testing. Problems in MATH are categorized into 7 types (Prealgebra, Intermediate Algebra, Algebra, Precalculus, Geometry, Counting & Probability, and Number Theory) and 5 difficulty levels. CollegeMath (Tang et al., 2024) test set contains 2,818 college-level problems, which are curated from 9 college-level mathematics textbooks, covering 7 key mathematical disciplines: Algebra, Precalculus, Calculus, VectorCalculus, Probability, LinearAlgebra, and Differential Equations. DeepMind-Mathematics (Saxton et al., 2019) test set consists of 1,000 problems covering wide range of mathematical reasoning tasks spanning algebra, arithmetic, calculus, and probability designed to evaluate the mathematical reasoning abilities of models. OlympiadBench-Math (He et al., 2024) benchmark including 675 Olympiad-level mathematical problems, and we only use the text-only English subset of Olympiad-Bench. TheoremQA (Chen et al., 2023) is novel theorem-driven question-answering benchmark containing 800 problems based on 350 theorems. It is designed to evaluate LLMs ability to apply domain-specific theorems across fields such as Mathematics, Physics, Electrical Engineering, Computer Science, and Finance. B.4 Templates For most of the results from our own runs, we use the template \"Question: {problem}nAnswer:\" 12 for training, and \"Question: {problem}nAnswer: Lets think step by step.\" for evaluation. There are two exceptions: (1) For reproduced DARTMath (Tong et al., 2024), we use its default Alpaca template: \"Below is an instruction that describes task. Write response that appropriately completes the request.nn###Instruction:n{problem}nn### Response:n\". (2) For evaluation on the DeepMind Mathematics benchmark for models finetuned from Llama3-8B, we find the Alpaca template yields consistently better performance than the template above. Therefore we use the Alpaca template for all the Llama3-8B evaluation on this dataset."
        },
        {
            "title": "C Analysis of Fused Problems",
            "content": "The embedding search naturally ensures high degree of contextual similarity. In the following sections, we analyze the fused problems in terms of problem types and errors. C.1 Fused Probelm Types Regarding problem types, in the GSM8K (Cobbe et al., 2021) dataset, all problems are simple algebra questions. For the MATH dataset, we find that 83% of the problem pairs belong to the same category, further validating the feasibility of the embedding search. We plot the distribution of combination types of problems in MATH in Figure 5. C.2 Fused Error Analysis In practice, we find that some fused problems are unreasonable or ambiguous, which are shown in Section F. The reason may be that some problems are not suitable for fusion or the limited capacity of the model for generating fused problems. To verify the correctness of the fused problems and their influence on the models performance, we conduct an error analysis on the fused problems. Specifically, borrowing the idea from rejection sampling (Yuan et al., 2023), we use GPT-4o-mini to verify the correctness and completeness of the fused problems. The corresponding evaluation prompt is shown in Section A. For each identified unreasonable problem, we adjust the temperature to 1.0 to enhance the diversity of generation, and re-generate the problems five times using the corresponding fusion strategy. If none of the five generated problems is reasonable, we consider the fusion to be unreasonable and discard it. Finally, 5.6% of the fused problems are identified as unreasonable, and the remaining reasonable problems are added to the dataset. The average performance of Llama3-8B fine-tuned only on the filtered MathFusionQA is 39.1, which is similar to the performance of the model fine-tuned on the original MathFusionQA (39.0), indicating that the unreasonable problems have little impact on the models performance."
        },
        {
            "title": "D Effect of Teacher Model",
            "content": "In MathFusion, we use GPT-4o-mini (OpenAI et al., 2023) as the teacher model to generate the solutions for the fused problems. To validate the performance improvement of MathFusion is not merely due to the stronger teacher model, we conduct an ablation study, where we use GPT-4o-mini to rewrite the solutions from the original training set. Then we fine-tune the Llama3-8B model on the original training set and the rewritten solutions. The results are shown in Table 6. We can see that the performance of the model fine-tuned on the rewritten solutions is better than the Standard setting, especially on the MATH and GSM8K datasets. However, the average improvement is only 1.3 points. Meanwhile, each fusion strategy of MathFusion still outperforms the rewritten solution by large margin, indicating that the performance improvement of MathFusion mainly comes from the fusion of problems rather than the stronger teacher model."
        },
        {
            "title": "E Significant Test",
            "content": "We conduct error analysis on MathFusion on Llama3-8B model to verify the consistent performance improvement of our MathFusionQA. Specifically, we fine-tune the Llama3-8B model on the original training sets (Standard setting), and the combined fusion strategies, respectively. The results are shown in Table 5. We can see that the MathFusion models consistently outperform the standard setting across all benchmarks. We also conduct statistical significance tests using the paired t-test, and results show that the performance improvement of MathFusion is statistically significant (p < 0.05) on all benchmarks."
        },
        {
            "title": "F More Cases",
            "content": "More cases, including the original problems PA and PB, the fused problem PF , are shown below. Specifically, we show three reasonable cases in Case F.1, Case F.2, and Case F.3, and three unreasonable cases in Case F.4, Case F.5, and Case F.6."
        },
        {
            "title": "Model",
            "content": "In-Domain Out-of-Domain MATH GSM8K College DM"
        },
        {
            "title": "AVG",
            "content": "Standard #1 Standard #2 Standard #3 Standard (Avg.) MathFusion #1 MathFusion #2 MathFusion #3 MathFusion(Avg.) 17.4 17.6 17.5 17.50.1 45.6 45.3 46.5 45.80.6 63.1 63.7 65.4 64.11.2 79.9 79.8 79.2 79.60. 12.1 12.6 12.9 12.50.4 27.1 27.5 27.9 27.50.4 23.1 20.6 21.6 21.81.3 44.4 45.4 43.4 44.41.0 3.7 4.3 4.7 4.20.5 17.2 17.0 17.2 17.10. 9.6 8.9 10.9 9.81.0 19.5 19.4 20.0 19.60.3 21.5 21.3 22.2 21.70.5 39.0 39.1 39.0 39.00.1 Table 5: Performance comparison between the standard setting and MathFusion accross six benchmarks with three random runs. The average performance is reported with the standard deviation."
        },
        {
            "title": "Model",
            "content": "# Samples In-Domain Out-of-Domain MATH GSM8K College DM Olympiad Theorem AVG Standard Standard + GPT Rewritten MathFusion (Sequential) MathFusion (Parallel) MathFusion (Conditional) MathFusion 15K 30K 30K 30K 30K 60K 17.5 22.8 38.8 38.1 34.7 46.5 65.4 75.4 77.9 75.4 76.9 79.2 12.9 11.8 25.1 25.5 21.2 27.9 21.6 15.7 42.0 41.9 27.4 43.4 4.7 5.5 12.6 11.9 11.9 17.2 10.9 9.6 17.0 18.9 15.5 20. 22.2 23.5 35.6 35.3 31.3 39.0 Table 6: Ablation study on Llama3-8B about the effect of GPT-4o-mini to generate solutions. 14 Figure 5: Distribution of combination types of problems in MATH dataset. Prompt 1: Sequential Fusion # Role: Mathematical Problem Merger ## Profile Your role is to merge \"#Problem 1#\" and \"#Problem 2#\" into combined problem. ## Guidelines Step 1: Identify input and output variables in both problems. Determine mathematical relationships and constraints in each problem. Locate variables between \"#Problem 1#\" and \"#Problem 2#\" that can form sequential dependencies. Step 2: Formulate comprehensive plan to merge the two problems by using \"#Problem 1#\"s output variable to replace an input variable of \"#Problem 2#\"s. Merge contextual elements by embedding both problems within unified real-world scenario or extended narrative, aligning units and measurement systems. Step 3: Create single \"#Combined Problem#\" where solving \"#Problem 1#\" is prerequisite for \"#Problem 2#\". Explicitly state variable dependencies and which variable is replaced. Adjust numerical ranges to maintain arithmetic consistency. The \"#Combined Problem#\" should contain no supplementary explanation or note. ## Output Format Please reply strictly in the following format: #Elements Identified#: #Plan#: #Combined Problem#: ## Input ### #Problem 1# {problem1} ### #Problem 2# {problem2} ## Output 15 Prompt 2: Parallel Fusion # Role: Mathematical Problem Synthesizer ## Profile Your role is to organically integrate \"#Problem 1#\" and \"#Problem 2#\" to create novel problem that requires advanced synthesis of their mathematical essence. ## Guidelines Step 1: Conduct deep structural analysis of both problems by identifying their fundamental mathematical operations, contextual frameworks, and cognitive patterns. Extract the underlying logical architectures while preserving their distinctive solution pathways. Step 2: Develop an innovative fusion mechanism by discovering non-obvious mathematical connections between the problems core concepts. Construct multidimensional scenario that naturally embeds both original contexts through temporal sequencing, spatial superposition, or conceptual analogy. Engineer hybrid parameters that inherit characteristics from both source problems while introducing emergent properties. Step 3: Formulate the synthesized problem through strategic recombination of mathematical elements, ensuring the new problem requires concurrent application of both original solution strategies. Introduce controlled complexity through cross-domain constraints and self-verification mechanisms that establish mathematical consistency with both source problems answers. ## Output Format Please reply strictly in the following format: #Core Elements#: #Synthesis Method#: #New Problem#: ## Input ### #Problem 1# {problem1} ### #Problem 2# {problem2} ## Output 16 Prompt 3: Conditional Fusion # Role: Problem Integrator ## Profile Create real-world problem where the solution requires solving both \"#Problem 1#\" and \"#Problem 2#\" independently. **Ensure the the final answer is either from \"#Problem 1#\" or \"#Problem 2#\", depends on the \"#New Question#\"**. ## Guidelines Step 1: Analyze \"#Problem 1#\" and \"#Problem 2#\" and make sure that the output variables they ask about are of the same type. If they are different (for example, one asks about time and the other asks about price), modify one of the problem so that it asks about the same variable as the other. Step 2: Design unified problem scenario that combines \"#Problem 1#\" and \"#Problem 2#\". Introduce \"#New Question#\", which must be related with both \"#Problem 1#\" and \"#Problem 2#\". Ensure that final answer of the \"#New Question#\" must either come from \"#Problem 1#\" or \"#Problem 2#\". This means that the \"#New Question#\" should be an **comparison** and **selection** of the previous answers, not their **combination**. There are some examples for the \"#New Question#\": 1. Who sells the most items? 2. How much money does the top earner make? 3. Which is the cheaper plan? 4. Someone has 200 dollor, which item can he afford? Step 3: Provide the \"#New Problem#\", which combine \"#Problem 1#\", \"#Problem 2#\", and \"#New Question#\" in unified real-world scenario. Dont contain solution of \"#Problem 1#\" and \"#Problem 2#\" in \"#New Problem#\". Avoid using the phrases \"#Problem 1#\" and \"#Problem 2#\" in the generated \"#New Problem#\". ## Output Format Please reply strictly in the following format: #Analysis#: #New Question#: #New Problem#: ## Input ### #Problem 1# {problem1} ### #Problem 2# {problem2} ## Output 17 Prompt 4: Problem Evaluation # Role: Mathematics Grading Teacher ## Profile You are senior mathematics grading teacher in university, very skilled in high difficulty fields such as Intermediate Algebra, Precalculus, Prealgebra, Number Theory, Geometry, Counting & Probability, Algebra and so on. ## Guidelines Your task is to act as an impartial judge to evaluate the statement completeness and correctness of math problem according to the following rules: 1. Assess the clarity and accuracy of the definition of each math problem. Ensure that the problem statement provides sufficient information, conditions, and constraints. 2. Consider whether the problem allows for multiple interpretations or if further clarification is needed. 3. Evaluate the clarity of mathematical notation and terminology used in the problem. 4. Evaluate whether the math problem is solvable. If the math problem meet the rules above, output \"True\" in \"#Judgement#\", else \"False\". You should also give your explanation in \"#Explanation#\". ## Output Format Please reply strictly in the following format: #Judgement#: #Explanation#: ## Input {question} ## Output Example F.1: Case PA: Noah is painter. He paints pictures and sells them at the park. He charges $60 for large painting and $30 for small painting. Last month he sold eight large paintings and four small paintings. If he sold twice as much this month, how much is his sales for this month? PB: Michael loves to paint and sells his creations. He charges $100 for large painting and $80 for small painting. At his last art show, he sold 5 large paintings and 8 small paintings. How much did he earn in all? seq : Noah is painter who charges $60 for large painting and $30 for small painting. Last month, he sold 8 large paintings and 4 small paintings. This month, he sold twice as many paintings as last month. Calculate Noahs total sales for this month. Michael, another painter, charges $100 for large painting and $80 for small painting. At his last art show, he sold 5 large paintings and the same amount in dollars as Noahs total sales from this month in small paintings. How much did Michael earn in all? para : Noah and Michael are collaborating in an art exhibit. Noah charges $60 for large painting and $30 for small painting. Last month, he sold eight large paintings and four small paintings. This month, he plans to sell 50% more of each type. Michael charges $100 for large painting and $80 for small painting. At his last show, he sold 5 large paintings and 8 small paintings. For the exhibit, he aims to sell 40% more large paintings and 25% more small paintings than last time. How much total sales will they collectively earn from the exhibit? cond : Noah is painter who sells his artwork at the park. He charges $60 for large painting and $30 for small painting. Last month, he sold eight large paintings and four small paintings. This month, he has doubled his sales. Meanwhile, Michael, another painter, charges $100 for large painting and $80 for small painting. At his last art show, he sold 5 large paintings and 8 small paintings. Who earned more from their painting sales this month, Noah or Michael? Example F.2: Case 2 PA: There were 133 people at camp. There were 33 more boys than girls. How many girls were at the camp? PB: At party, there are 50 people. 30% of them are boys. How many girls are at this party? seq : At camp, there were 133 people, with 33 more boys than girls. How many girls were at the camp? Subsequently, at party where the total number of people is 50 plus the number of girls from the camp, 30% of them are boys. How many girls are at this party? : At camp, there are 133 people, and there are 33 more boys than girls. At party, there are 50 people, and 30% of para them are boys. What is the total number of girls at both the camp and the party? : At summer camp, there were 133 people. There were 33 more boys than girls. At the same time, at party, there are cond 50 people, and 30% of them are boys. How many girls are at each location, and which location has more girls? Example F.3: Case 3 PA: Add 53.463 to 12.9873 and round to the nearest thousandth. PB: Add 81.76 to 34.587 and round your answer to the nearest tenth. : First, add 53.463 to 12.9873 and round your answer to the nearest thousandth. Let this result be X. Then, add to seq 81.76 and 34.587, and round your final answer to the nearest tenth. : Calculate the sum of 53.463 and 81.76, then add this result to 34.587. Round the total to the nearest hundredth first, para and then take that result and round it to the nearest whole number. What is the final answer? cond : baker is preparing two different batches of cookies. For the first batch, she uses 53.463 ounces of flour and 12.9873 ounces of sugar. She wants to know the total weight of flour and sugar combined, rounded to the nearest thousandth. For the second batch, she uses 81.76iningredientsand34.587 in packaging. She needs to find the total cost of ingredients and packaging, rounded to the nearest tenth. After calculating both rounded totals, she wonders which amount is larger. Example F.4: Error Case 1 PA: What is the smallest positive integer with six positive odd integer divisors and twelve positive even integer divisors? PB: What is the smallest positive integer with exactly 10 positive integer divisors? : Find the smallest positive integer that has six positive odd integer divisors and twelve positive even integer divisors. seq Once is determined, calculate the smallest positive integer that has exactly 10 positive integer divisors. Error: The output of PA has no real relation to the input of PB. 19 Example F.5: Error Case PA: In triangle ABC, cos(2A B) + sin(A + B) = 2 and AB = 4. What is BC? PB: If triangle ABC has sides of length AB = 6, AC = 5, and BC = 4, then calculate cos AB 2 sin 2 sin AB 2 cos 2 . : In triangle ABC, the lengths of sides are AB = 4, AC = 5, and BC = 6. If it is given that cos(2A B) + sin(A + para B) = 2 holds true and the angles A, B, and are interpreted through its trigonometric identities, compute the value of cos AB 2 sin 2 sin AB 2 cos 2 . Further, verify your answer by ensuring that the relationships between side lengths corroborate the angle calculations used to derive it. Error: The triangle can not satifsy both AB = 6, AC = 5, BC = 4, and cos(2A B) + sin(A + B) = 2. Example F.6: Error Case 3 PA: Andrew and John are both Beatles fans. Their respective collections share nine of the same albums. Andrew has seventeen albums in his collection. Six albums are in Johns collection, but not Andrews. How many albums are in either Andrews or Johns collection, but not both? PB: Andy, Beth, Charlie, and Daniel take test with thirty questions. Andy and Beth together get the same number of questions wrong as Charlie and Daniel together. Andy and Daniel together get four more questions wrong than Beth and Charlie do together. If Charlie gets five questions wrong, how many questions does Andy get wrong? cond : Andrew and John are both Beatles fans. Their respective collections share nine of the same albums. Andrew has seventeen albums in his collection, while six albums are in Johns collection, but not Andrews. Meanwhile, Andy, Beth, Charlie, and Daniel take test with thirty questions. Andy and Beth together get the same number of questions wrong as Charlie and Daniel together. Andy and Daniel together get four more questions wrong than Beth and Charlie do together. If Charlie gets five questions wrong, how many unique albums are in either Andrews or Johns collection, but not both, compared to how many questions Andy got wrong? Error: There is no conditional relationship between the two problems."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "School of Computer Science, Wuhan University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "Tsinghua University"
    ]
}