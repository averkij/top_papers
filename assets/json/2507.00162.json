{
    "paper_title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion",
    "authors": [
        "Yu Lu",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains a significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies a systematic trend where high-frequency components become increasingly distorted as video length grows, an issue we term high-frequency distortion. To address this, we propose FreeLong, a training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture with multiple attention branches, each operating at a distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4x and 8x of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences."
        },
        {
            "title": "Start",
            "content": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion Yu Lu, Yi Yang AbstractRecent advances in video generation models have enabled high-quality short video generation from text prompts. However, extending these models to longer videos remains significant challenge, primarily due to degraded temporal consistency and visual fidelity. Our preliminary observations show that naively applying short-video generation models to longer sequences leads to noticeable quality degradation. Further analysis identifies systematic trend where high-frequency components become increasingly distorted as video length growsan issue we term high-frequency distortion. To address this, we propose FreeLong, training-free framework designed to balance the frequency distribution of long video features during the denoising process. FreeLong achieves this by blending global low-frequency features, which capture holistic semantics across the full video, with local high-frequency features extracted from short temporal windows to preserve fine details. Building on this, FreeLong++ extends FreeLongs dual-branch design into multi-branch architecture with multiple attention branches, each operating at distinct temporal scale. By arranging multiple window sizes from global to local, FreeLong++ enables multi-band frequency fusion from low to high frequencies, ensuring both semantic continuity and fine-grained motion dynamics across longer video sequences. Without any additional training, FreeLong++ can be plugged into existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer videos with substantially improved temporal consistency and visual fidelity. We demonstrate that our approach outperforms previous methods on longer video generation tasks (e.g. 4 and 8 of native length). It also supports coherent multi-prompt video generation with smooth scene transitions and enables controllable video generation using long depth or pose sequences. Additional results and details are available on the project website: https://freelongvideo.github.io/ Index TermsVideo Generation, Diffusion Models, Multimodal Learning"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Recent advances in video generation models [1][14], have enabled the generation of high-quality short videos from text prompts. These models are typically trained on large-scale video-text datasets [15][21], and their ability to produce coherent short clips has inspired research into extending them to long-form video generation [17], [22][33]. Yet, building long-video generation models requires extensive computational resources and access to large-scale longvideo annotations, making them impractical for lightweight and general applications. more efficient and practical alternative is to adapt pretrained short video generation models to generate longer video sequences in training-free manner. Recent studies [34][42] have explored attention mechanisms [34], [37], [40], auto-regressive architectures [36], [42], and positional encoding [38] to improve long-range consistency in video clips. However, these approaches often focus on maintaining coherence at the boundaries of adjacent clips rather than enforcing unified narrative or consistent visual identity across the entire video. As result, artifacts such as identity drift, inconsistent lighting, and abrupt scene transitions can emerge, particularly in videos with prolonged durations or complex motion dynamics. In this study, we propose straightforward, training-free method to adapt existing short video generation models for generating consistent longer videos. We first evaluate the direct application of short video generators, such as Wan2.1 [1] (native length 81 frames), to longer sequences (e.g., 4 video Y. Lu, and Y. Yang are with ReLER, CCAI, Zhejiang University, Hangzhou, 310027, China (e-mail: {aniki.yulu, yangyics}@zju.edu.cn). length, 324 frames). As shown in Figure 1, this approach ensures global consistency but results in lower-quality outputs, including blurred textures, and motion jitter beyond the models native frame length (see the first and second row of Figure 1). To understand these issues, we performed frequency analysis on generated long videos. Frequency analysis of generated longer videos revealed stable low-frequency components but significant distortion in high-frequency components as video length increased (Figure 2). In fine-grained frequency analysis, we also observe increasing distortion in high-frequency components as video length grows (see Figure 4(a)). For example, with double-length sequences, only 30% of the low-frequency content available, leaving 70% of high-frequency components distorted; at 4 length, distortion rises to 95% (Figure 4 (b)). This diminishes fine details in longer sequences, such as cat fur or tree leaves becoming blurred (Figure 1, second row). In this paper, we introduce FreeLong, novel framework that employs SpectralBlend Attention to balance the frequency distribution of long video features in the denoising process. FreeLong integrates global and local features via two parallel streams, enhancing the fidelity and consistency of long video generation. The global stream deals with the entire video sequence, capturing extensive dependencies and themes for narrative continuity. Meanwhile, the local stream focuses on shorter frame subsequences to retain fine details and smooth transitions, preserving high-frequency spatial and temporal information. FreeLong combines global and local video features in the frequency domain, improving both consistency and fidelity by blend5 2 0 2 0 3 ] . [ 1 2 6 1 0 0 . 7 0 5 2 : r IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2 Fig. 1: Results of Short and Longer Videos. The first row of each case shows short videos generated using short video diffusion models (81 frames for Wan-2.1 [1] and 121 frames for LTX-Video [2]). Directly extending these models to longer videos, like those with 4 (324 frames and 484 frames), preserves temporal consistency but lacks fine spatial-temporal details. In contrast, our proposed FreeLong and FreeLong++ adapts short video diffusion models to create consistent long videos with high fidelity. Fig. 2: Ratio of short video SNR on high (0.25π1.0π)/low (0.0π-0.25π) frequency to longer videos. Our findings reveal that when direct extend short video diffusion model to generate longer videos, the SNR of high-frequency components in the space-time frequency domain degrades significantly as video length increases. ing low-frequency global components with high-frequency local components. Building on the FreeLong, we further present FreeLong++, comprehensive extension of FreeLong that leverages Multi-band SpectralFusion (MSF) framework. Rather than restricting attention to binary global-local structure, FreeLong++ utilizes multiple attention branches with varying window sizes, where each window attends to different temporal scale. This design allows us to decompose the video signal into interpretable temporal frequency bands: longer windows capture global semantic continuity and low-frequency structure, while shorter windows focus on fast-changing motion and high-frequency texture. We furFig. 3: Attention Visualization. We visualize the attention by average across all layers and time steps from Wan2.1 [1]. The attention maps for 81-frame videos exhibit diagonallike pattern, indicating high correlation with adjacent frames, which helps preserve high-frequency details and motion patterns when generating new frames. In contrast, attention maps for longer videos are less structured, such as 648 frames (8), making the model struggle to identify and attend to the relevant information across distant frames. This lack of structure in the attention maps results in the distortion of high-frequency components of long videos, which results in the degradation of fine spatial-temporal details. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 3 Fig. 4: Fine-grained frequency analysis on longer video generation. (a) As video length increases, both the range and severity of frequency distortion grow substantially. (b) We define available frequency bands as those with relative SNR above 0.9. As shown, the number of available bands drops significantly when the video length increases from 2 to 4, indicating that fixed two-branch structure in FreeLong is insufficient for modeling motion dynamics in longer sequences. (c) High-frequency distortion correlates with attention window size: larger window sizes introduce more severe distortion in the high-frequency components. ther propose multi-band fusion strategy to adaptively merges the multi-window video features in the frequency domain, ensuring that all frequency bands are properly integrated and reconstructed into consistent video sequence, results in frequency-aligned fusion. FreeLong++ retains the training-free advantage of FreeLong, introducing no additional model parameters or finetuning requirements. Its modular design seamlessly integrates with existing diffusion transformers [1], [2] by directly replacing attention modules in modern video diffusion transformers. Experimental results demonstrate that FreeLong++ significantly outperforms existing training-free baselines by consistently enhancing temporal consistency and visual fidelity, robustly extending short video generation models to generate videos 4 or 8 times longer. Moreover, FreeLong++ effectively supports sophisticated video generation tasks involving complex controls such as poseguidance or depth-guidance. Our contributions can be summarized as follows: 1) We conduct frequency analysis on the direct application of short video models for longer video generation and identify high-frequency distortions in the longer videos. 2) We propose FreeLong with SpectralBlend Attention mechanism to merge the consistent low-frequency components of global videos with the high-fidelity high-frequency components of local videos. 3) We propose FreeLong++, novel training-free framework built upon, FreeLong. FreeLong++ introduces Multi-band SpectralFusion (MSF), enabling multiwindow attention mechanisms to effectively capture temporal dynamics across various frequency bands without additional training or parameters."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 Text-to-Video Generation Models Text-to-video (T2V) generation has made significant advancements with the rise of diffusion-based models [3][8], demonstrating remarkable capabilities in generating highquality, temporally coherent videos. Early video diffusion models leveraged pre-trained image diffusion UNets [43] and enhanced them with temporal attention mechanisms to effectively model frame-to-frame dependencies. Notable examples, such as LaVie [44] and VideoCrafter2 [3], trained on large-scale video-text datasets like WebVid [16] and InternVid [15], have been successful in producing high-quality videos of fixed short durations, typically around 2 seconds. The field has further evolved with the introduction of Sora [45], which highlights the scalability and effectiveness of diffusion transformer (DiT) architectures [46]. Recent innovations, including CogVideoX [4], Mochi1 [47], HunyuanVideo [8], LTX-Video [2], and Wan2.1 [1], have adopted the DiT framework, achieving state-of-the-art performance in video generation. By scaling both model size and the volume of training data, these DiT-based models have managed to extend video generation capabilities to sequences as long as 5 seconds. Nonetheless, generating longer videos remains significant challenge. Key bottlenecks include the complexity of temporal modeling, the memory requirements for handling extended video sequences, and the lack of training data annotated for long-range video dependencies. Progress in addressing these limitations is critical to unlocking the potential of T2V systems for generating longer, high-quality videos with enhanced temporal consistency. 2.2 Long Video Generation Recent efforts [17], [22], [23], [35] have explored scaling video diffusion models to longer durations by modifying training objectives or architectures. Approaches such as StreamingT2V [22] and Vidu [23] adopt autoregressive generation pipelines or memory-augmented modules to maintain cross-segment consistency. However, these methods are computationally expensive and require extensive retraining on curated long-video datasets. Additionally, recent autoregressive models [27], [48], [49] fine-tune pretrained short-video diffusion models using next-clip prediction paradigm. However, such methods are prone to error accumulation during inference, leading to degradation issues such as semantic drift and content forgetting. To reduce training costs, lightweight alternatives such as IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 4 Gen-L-Video [35] and FreeNoise [34] introduce trainingfree extensions based on sliding-window attention and noise rescheduling. While efficient, these approaches suffer from limited temporal modeling capacity and fail to adequately preserve frequency structures, often resulting in temporal drift over extended sequences. In contrast, we propose FreeLong, training-free method that enhances longer video generation by blending global low-frequency and local high-frequency features through dual-branch SpectralBlend Temporal Attention mechanism. Building on this, FreeLong++ introduces multi-band extension with multiple attention branches of varying window sizes, enabling adaptive modeling across temporal frequency bands and improving consistency and fidelity in longer video sequences."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Current video generation models generally adopt common backbone design to effectively model relationships across spatial and temporal dimensions. Architectures such as UNet [3], [43], [44] and Transformers [1], [46] are commonly employed to facilitate the iterative denoising process [50], [51]. The UNet architecture is effective due to its separate spatial and temporal attention layers, which help reduce computational costs, although it may struggle to maintain strong consistency in capturing dependencies. Transformerbased models are effective at modeling long-range dependencies in data by using 3D attention mechanisms. These mechanisms capture both spatial and temporal relationships, making the models well-suited for complex video sequences. The attention mechanism used in both UNet and transformers is defined as: = Softmax (cid:19) (cid:18) QKT dk V, where Q, K, and are the query, key, and value matrices, and dk is the key dimensionality. This mechanism can be applied to spatial, temporal, or spatiotemporal dimensions. Additionally, control signals such as text, depth, or pose can be seamlessly incorporated by modifying and Vto include the relevant control features. This enables the generation of contextually guided and semantically rich video content. While these advancements allow for the generation of coherent and high-quality video frames, generating longer video sequences remains significant challenge. Video generation models, generally pretrained on shorter videos, often struggle with maintaining consistent quality over longer sequences. The attention mechanisms, though powerful, tend to degrade in effectiveness when tasked with modeling long-range dependencies, ultimately leading to reduced video quality as the sequence length increases."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "maintaining semantic continuity and visual fidelity. Building on this, we introduce FreeLong++, extending SpectralBlend to multi-branch approach with finer frequency band control for enhanced motion dynamics. 4.1 FreeLong 4.1.1 Observation and Analysis When attempting to adapt short video diffusion models to generate longer videos, straightforward approach is to input longer noise sequence into the short video models. The transformer attention layers in the video generation model are not constrained by input length, making this method seemingly viable. However, our empirical study reveals significant challenges, as demonstrated in Figure 1. Generated longer videos often exhibit fewer detailed textures, such as blurred fur in the cat, and more irregular variations, like abrupt changes in motion. We attribute these issues to two main factors: the limitations of the attention mechanism and the distortion of high-frequency components. Attention Mechanism Limitations: The attention mechanism in video generation models, pre-trained on short videos, struggles to generate longer videos effectively. As shown in Figure 3, for DiT model trained on 81frame videos, attention maps exhibit clear diagonal pattern, reflecting strong correlations between adjacent frames and preserving spatial-temporal details and motion patterns. However, with 324-frame videos (4) or 648 frame videos (8), the attention maps lose structure, making it harder to capture relevant information over distant frames. This results in missed subtle motion patterns and oversmoothed or blurred outputs. Frequency Analysis: To better understand the generation process of long videos, we analyzed the frequency components in videos of varying lengths using the Signal-toNoise Ratio (SNR) as metric. Ideally, short video diffusion models generate short videos with high quality. Robust longer videos, such as 4 the original length derived from such models, should exhibit consistent SNR values across all frequency components. However, Figure 2 reveals significant differences in the SNR of high/low frequency components1 between generated short and longer videos. The SNR of low-frequency components remains relatively consistent for long videos (1.0 for origin length frames to 0.97 for 8 frames), suggesting that the model maintains overall structure and low-frequency details in extended sequences. However, the SNR of high-frequency components drops significantly for longer videos (1.0 for origin length to 0.6 for 8 length), indicating loss of fine details and increased distortion, leading to suboptimal visual fidelity. Motivated by the frequency analysis, we propose FreeLong, method designed to generate high-fidelity and consistent long videos using the inherent power of the diffusion model. As illustrated in Figure 5, our FreeLong uses pretrained short video generation models and introduces SpectralBlend attention to facilitate long video generation. The SpectralBlend attention consists of two steps: localglobal attention decoupling and spectral blending. In this section, we first introduce FreeLong, which adopts two-branch SpectralBlend strategy to fuse global lowfrequency context with local high-frequency details, thereby 1. We split the frequency components into high-frequency (ϕ (0.25π 1.00π)) and low-frequency (ϕ (0.00π 0.25π)) and compared the SNR of each component in longer videos to the corresponding SNR in short videos. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 5 Fig. 5: Overview of FreeLong. FreeLong facilitates consistent and high-fidelity video generation using SpectralBlend Attention. SpectralBlend effectively blends low-frequency global video features with high-frequency local video features through two-step process: local-global attention decoupling and spectral blending. Local video features are obtained by masking temporal attention to concentrate on fixed-length adjacent frames, while global temporal attention encompasses all frames. During spectral blending, 3D FFT projects features into the frequency domain, where high-frequency local components and low-frequency global components are merged. The resulting blended feature, transformed back to the time domain via IFFT, is then utilized in the subsequent block for refined video generation. 4.1.2 Local-global Attention Decoupling The attention in short video models is optimized to model short frame sequences accurately, maintaining high-fidelity visual information. Conversely, the long-range attention from short video models tends to maintain overall layout and and object consistency. Given these properties, we first decouple the local and global attention. For video sequence with length , let and denote the indices of query and key frames, respectively.The local attention matrix can be obtained as: Alocal(i, j) = Softmax 0 (cid:33) (cid:32) QiK (cid:23) (cid:22) Tα 2 if j < otherwise, where and are the query and key matrices derived from the input video feature Zin. The local attention Alocal leads to each frame only attending to frames within window of Tα frames. We set Tα as the native video length of pretrained models (e.g., 81 frame for Wan2.1 [1]). Given the local attention matrix Alocal, the local video features Zlocal can be obtained by: Zlocal = AlocalV , where is the value matrix derived from the input video feature Zin. By restricting the attention to adjacent local frames, we preserve the capabilities of short video models, thereby retaining high-fidelity visual details in local video features. We then define the global attention matrix where each frame attends to all other frames. The global attention matrix can be computed as follows: Aglobal(i, j) = Softmax (cid:32) QiK (cid:33) . Given the global attention matrix Aglobal, the global video features Zglobal can be obtained by: Zglobal = AglobalV . The global video features process the entire video sequence, ensuring narrative continuity and consistency, while capturing long-range dependencies and overarching themes. 4.1.3 Spectral Blending After obtaining the global and local video features, frequency filter is used to blend the low-frequency components of the global video latent Zglobal with the high-frequency components of the local video latent Zlocal, resulting in new video latent . This fused latent retains the global consistency and structure provided by Zglobal, while benefiting from the enhanced high-frequency details introduced by Zlocal. The process is described by: ˆZ ˆZ = 1 global = F3D(Zglobal) P, local = F3D(Zlocal) (1 P), global + ˆZ 3D ( ˆZ local), where F3D is the Fast Fourier Transformation operated on both spatial and temporal dimensions, 1 3D is the Inverse Fast Fourier Transformation that maps back the blended representation from the frequency domain, and R4N hw is the spatial-temporal Low Pass Filter (LPF), which is tensor of the same shape as the latent. The final fused video feature serves as the input to our subsequent video generation module. The rationale behind using low-frequency components from the global video features and high-frequency components from the local video features stems from our analysis. The global features provide stable, consistent strucIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 6 ture, preserving the overall layout and object consistency throughout the video. This is crucial for maintaining temporal consistency in long videos. On the other hand, local features retain high-fidelity details, which are essential for capturing fine textures and intricate motion patterns that tend to degrade in long sequences. By blending these components in the frequency domain, we harness the strengths of both global consistency and local detail preservation, addressing the issues of blurred frames and temporal flickering observed in our analysis. 4.1.4 Implementation details We apply FreeLong on state-of-the-art diffusion transformer models, Wan-2.1 [1] and LTX-Video [2]. Wan models can generate high-quality 81 frames/5s videos, and LTX-Video can generate 121 frame videos. We set Tα same with native video length for the local attention setting. During inference, the parameters of the frequency filter for each model are kept the same for fair comparison. Specifically, we use Gaussian Low Pass Filter with normalized spatiotemporal stop frequency of D0 = 0.25. 4.2 FreeLong++ 4.2.1 Observation As discussed previously, FreeLong uses dual-branch SpectralBlend attention mechanism to separately model global low-frequency context and local high-frequency details. While this two-branch architecture is effective for moderately extended video sequences, it encounters significant limitations as video length increases, most notably in the form of increased frequency distortion. As illustrated in Figure 4(a), increasing the video length results in pronounced degradation of high-frequency components, with both the severity and the range of affected frequencies growing substantially. To quantify this effect, we define distorted frequency bands as those with relative signalto-noise ratio (SNR) below 0.9. Our analysis shows that the proportion of such distorted bands increases sharply with extended video durations. For example, at four times the native video length Tα, only about 3% of the frequency bands remain reliable (Figure 4(b)). This dramatic decline in high-frequency fidelity underscores the inadequacy of the simple dual-branch approach in handling the complex shifts in frequency distributions inherent to long sequences, highlighting the need for an adaptive, more refined frequency decomposition strategy. Furthermore, our experiments show that adjusting the temporal attention window size significantly influences high-frequency distortion patterns. As depicted in Figure 4(c), when the generated video length is fixed at 4 Tα (four times the native video length Tα), varying the temporal attention window size yields distinct patterns of frequency degradation. This observation directly motivated the design of FreeLong++, which employs multi-branch attention architecture to provide finer-grained control at different temporal scales. This design significantly enhances the models ability to preserve long-range consistency and accurately capture complex motion dynamics. 4.2.2 Overview Guided by these insights, we propose FreeLong++, whose framework is illustrated in Figure 6. Leveraging diffusion transformer architecture with integrated 3D attention mechanisms across spatial and temporal dimensions, FreeLong++ incorporates multiple attention branches designed to effectively capture dynamics at varying temporal scales. Specifically, we extend the spectral blending mechanism into multi-branch attention architecture, where each branch independently focuses on distinct temporal scale. These scales range from short-term branches (capturing immediate local spatial-temporal features), through mid-term branches (capturing intermediate-level motion patterns and dependencies), to long-term branches (aggregating comprehensive global temporal contexts). Each attention branch employs dedicated frequency-domain band-pass filter, enabling selective extraction and emphasis of frequencyspecific features pertinent to its temporal scope. The outputs from these branches are subsequently combined in the frequency domain, producing composite representation that effectively integrates short-term dynamic details with broader, long-term structural consistency. 4.2.3 Multi-Scale Attention Decoupling To capture dynamics at different temporal ranges, we decouple the original temporal attention into multiple parallel scale-specific attention branches. Each branch operates on different temporal window size αlTα, expressed as multiple of the native video length Tα. For example, three-scale configuration could use α1 = 1, α2 = 2, and α3 = 4, corresponding to attention windows of length 1Tα, 2Tα, and 4Tα, respectively. For video sequence with length , let and denote the indices of query and key frames, respectively. For each scale we apply masked self-attention that limits each query frame to attend only to an interval of αlTα frames around it. We denote the resulting masked attention matrix for scale as Al(i, j) = Softmax (cid:19) (cid:18) QiK 0 (cid:22) αlTα 2 (cid:23) , if j < otherwise, where Q, are the query and key matrices of the video features. This ensures that branch ls attention is confined to temporal span of αlTα frames. Using this decoupling, we obtain set of multi-scale video features Z(l): the finest-scale branch (small αl) focuses on short-range interactions and preserves high-frequency details, while coarser-scale branches (large αl up to the full sequence) capture longer-range dependencies and global context (lowfrequency structure). Efficient Attention via Sparse Key Frames: To maintain computational efficiency, particularly for the largest temporal window, FreeLong++ propose sparse attention through key-frame selection. The motivation comes from that longrange temporal relationships often exhibit redundancy and only require subset of key frames to effectively capture global context [52][55]. Attention computations in the global-scale (largest αl) branch are restricted to uniformly IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 7 Fig. 6: Overview of FreeLong++. The FreeLong++ framework extends FreeLong by introducing Multi-band SpectralFusion Attention. Multi-scale temporal branches with varying window sizes capture motion dynamics at different frequency bands. Each branch is processed in the frequency domain and selectively fused via scale-specific filters, enhancing longrange consistency while preserving fine-grained motion. sampled subset of representative frames, denoted K. Formally, the sparsified attention matrix for this largest scale is: Asparse(i, j) = Softmax (cid:19) (cid:18) QiK 0 if K, otherwise. The results for most global-branch can be easily obtained by Zsparse = AsparseVsparse. This strategic sparsification significantly reduces computational overhead while preserving the critical global temporal context necessary for long-range consistency. 4.2.4 Multi-band Spectral Fusion Given the multi-scale features Z(l), we integrate them in the frequency domain to exploit their complementary bandwidths. We first transform each scales features into the spectral domain using 3D Fast Fourier Transform (FFT) over spatial and temporal dimensions. Formally, let Zl denote the latent video features from the l-th attention branch (with = 1 as the most local branch and = the most global). We project each branchs output into the frequency domain and apply scale-specific spectral filter before fusing. The multi-band fusion process is described by: ˆZl = F3D(Zl), (cid:88) ˆZ = Pl ˆZl, = 1, 2, . . . , L, l=1 = 1 3D ( ˆZ ). Here, F3D and 1 3D denote the 3D Fast Fourier Transform and its inverse, applied over the spatio-temporal dimensions of the latent feature Zl. Each ˆZl represents the frequency-domain representation of branch ls attention output. The term Pl is scale-specific frequency mask (i.e., band-pass filter), which selectively retains the frequency band corresponding to the temporal scale αl of branch l. The temporal window αlTα for branch determines its maximum frequency 1 π based on the Nyquist crite2αl rion2 [56], [57]. For example, the coarsest scale (αl = 4) retains frequencies within [0, 1 8 π], capturing slow, global dynamics. medium scale (αl = 2) selects [ 1 4 π], while the finest scale (αl = 1) covers the high-frequency range [ 1 4 π, 1.0π], encoding fast, local motion details. 8 π, 1 After filtering, the masked frequency components across all branches are summed to form ˆZ , which is then transformed back to the time domain using inverse FFT to produce the final fused latent . The rationale for multi-band spectral fusion is to capture richer spectrum of motion dynamics while maintaining long-range consistency. In FreeLong++, low-frequency global features (Z1) still provide stable backbone for overall scene structure and temporal consistency across the entire sequence, as in the two-branch case. However, by adding intermediate-scale branches (Z2, . . . , ZL1), the framework also preserves mid-range dynamics that single local branch might miss. Each scale-specific filter Pl injects the appropriate level of detail: slower temporal changes (e.g., gradual movements or scene transitions) are handled by lower-frequency components, whereas faster motions 2. The NyquistShannon theorem states that signal whose highest frequency is fmax can be reconstructed only if the sampling rate exceeds 2fmax; otherwise aliasing occurs. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 8 and fine textures are reinforced by higher-frequency components. The multi-band fusion thus balances the frequency content across scales, preventing both the loss of fine details and the distortion of medium-speed motions. As result, the fused latent contains multi-scale temporal information, leading to improved motion realism and smoother transitions. 4.2.5 SpecMix Noise Initialization To stabilize long-range consistency while preserving local details, we introduce SpecMix, an adaptive spectral-domain noise initialization integrated within FreeLong++. SpecMix are based on two critical observations: (i) consistent lowfrequency initialization enables models to better synthesize high-frequency details [58], whereas (ii) fully independent noise reduces temporal consistency [34]. Specifically, we define two noise components: consistency baseline xbase and per-frame residual xres. To construct xbase, we use slidingwindow shuffling procedure inspired by prior work [34], where noise segments are shuffled across neighboring temporal windows to enforce consistent low-frequency content. Concurrently, we sample xres independently as Gaussian noise, providing controlled local variations. Both xbase and xres are then transformed into the spectral domain. We apply 3D Fast Fourier Transform, yielding frequency-domain tensors: xF res = F3D(xres). For each time index t, we compute normalised distance to the sequence centre, base = F3D(xbase) and xF dt = (T 1)/2 (T 1)/2 [0, 1], and map it to mixing angle θt = dt π representation is then 2 . The final spectral xF = cos θt xF base,t and xF base,t + sin θt xF res,t, where xF res,t denote the spectral slices at frame t. This formulation ensures that low-frequency (with small dt) rely predominantly on the consistency base noise, while high-frequency (with dt close to 1) incorporate larger proportion of the stochastic residual noise. Finally, 3D inverse FFT are applied to xF to return to the spatial domain, yielding the initial noise tensor x0 for the diffusion process. Notably, this linear combination [59] preserves the overall all variance of the magnitude spectra at each temporal slice. 4.2.6 Implementation details We apply FreeLong++ to state-of-the-art diffusion transformer models, Wan-2.1-1.3B [1] and LTX-Video [2]. The Wan model generates 81-frame/5s videos, while LTX-Video produces 121-frame videos. For 4 longer video generation, we use 3 branches with αl = 1, 2, 4, and for 8 longer generation, we use 4 branches with αl = 1, 2, 4, 8. Different branch with varing window size can be achieved by simply adjusting the window size in existing attention tools like flash-attention [60]. We uniformly sample half of the frames as keys in the sparse attention for the global branch."
        },
        {
            "title": "5 EXPERIMENTS\n5.1 Evaluation Benchmark Details",
            "content": "Test Prompts: We evaluated our method using 100 augmented prompts randomly selected from VBench-Long [61]. Evaluation Metrics: For text-to-video generation, we utilized VBench-Long [61] metrics to assess video consistency and fidelity in long videos. 1. Video Consistency: Subject consistency: Assessed using DINO [62] feature similarity across frames to ensure consistent object appearance. Background consistency: Measured using CLIP [63] feature similarity across frames. Motion smoothness: Evaluated using motion priors in the AMT [64] video frame interpolation model. 2. Video Fidelity: Temporal flickering: Determined by computing mean absolute differences across static frames. Image quality: Measured using the MUSIQ [65] image quality predictor trained on the SPAQ [66] dataset. Aesthetic Quality: We evaluate the artistic and beauty value perceived by humans towards each video frame using the LAION aesthetic predictor [67] For faster experiments, we generate videos 4 longer for each base model (Wan-1.3B [1] and LTX-Video [2]) in the ablation study and also provide 8 longer video generation in our experiments. For controllable long video generation, such as poseor depth-guided videos, we utilized VACE [68] as the base model and applied our attention mechanism. 5.2 Quantitative Comparison We compare our method against other training-free and training-based approaches for long video generation with generation models, including: (1) Direct sampling, which generates long video sequences directly from short video models; (2) Sliding window, which uses temporal sliding windows [35] to process fixed number of frames at time; (3) FreeNoise [34], which introduces repeated input noise to enhance temporal coherence over long sequences; and (4) CausVid [48], an autoregressive video generation model fine-tuned from the Wan model. Tables 1 and 2 present quantitative results on Wan [1] and LTX-Video [2] models. Advanced DiT video generation models maintain strong motion smoothness and consistency due to variable training video lengths, yet they exhibit lower fidelity in terms of image quality and aesthetics. Direct sampling leads to high-frequency distortions and significant quality degradation when generating long videos. Both the sliding-window method and FreeNoise [34] improve video quality by using fixed temporal attention windows, but still struggle with consistency over long sequences. Furthermore, CausVid [48] significantly improves performance on both consistency and fidelity by fine-tuning base model, which require extensive training dataset and computations. Our FreeLong method outperforms all others, achieving the best scores across all metrics by generating consistent, high-fidelity long videos. Additionally, FreeLong++ further improves image quality and aesthetics by employing multiband spectral fusion for refined motion dynamics. 5.3 Ablation Studies To evaluate the effectiveness of each component within our FreeLong framework, we conducted detailed ablation study as summarized in Table 3. The global-branch approach achieves excellent subjective and background consistency and motion smoothness, but significantly lacks IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 9 Fig. 7: Qualitative comparison across models. All methods generate videos that are 4 the original length, based on the Wan2.1 [1] model. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 10 TABLE 1: Quantitative comparison on the Wan [1] model (4 frames). Direct sampling and Sliding window indicate directly sampling 324 frames and applying temporal sliding windows based on short video generation models, respectively. Compared to these methods, our FreeLong++ achieves consistent long video generation with high fidelity. All scores . Model Subj. Cons. Back. Cons. Motion Smooth. Temp. Flicker Imaging Qual. Aesthetic Qual. Direct sampling Sliding window FreeNoise [34] CausVid [48] FreeLong FreeLong++ 98.10 94.64 96.05 97.59 97.85 98.70 97.35 94.75 96.31 96.03 96.85 97.83 98.90 98.46 98.06 98.03 98.92 98. 98.88 96.52 97.63 96.97 98.29 98.57 60.52 66.71 67.00 65.72 66.33 68.82 59.07 61.26 62.35 58. 62.42 64.93 TABLE 2: Quantitative comparison on the LTX-Video [2] model (4 frames). All scores . Method Subj. Cons. Back. Cons. Motion Smooth. Temp. Flicker Imaging Qual. Aesthetic Qual. Direct sampling Sliding window FreeNoise [34] FreeLong FreeLong++ 97.75 96.27 96.29 98.98 99. 97.57 96.23 96.25 97.42 97.94 99.48 99.22 99.22 99.47 99.19 99.40 90.02 99.03 99.40 99. 40.05 46.70 45.70 45.95 61.12 43.68 49.63 49.67 51.92 54.68 TABLE 3: Ablation study on each module in Freelong++. Addition refers to directly summing the outputs of the global and local branches. Method Subj. Cons. Back. Cons. Motion Smooth. Aesthetic Qual. Imaging Qual. Global-branch Local-branch Addition FreeLong FreeLong+SpecM ix FreeLong++ FreeLong++sparse 98.10 95.21 97.18 97.85 98.88 98.70 98.60 97.35 95.43 96.40 96.85 98.25 97.83 97.73 98.90 97.97 98.85 98.92 99.09 98.99 98. 60.52 66.68 61.47 66.33 67.78 68.82 68.65 59.07 61.32 58.64 62.41 64.40 64.93 64.52 Infer. Time () 50 22 61 72 72 96 74 aesthetic and imaging quality. In contrast, the local-branch approach provides improved aesthetic and imaging quality, yet at the cost of lower consistency scores due to limited temporal scope. Direct addition of global and local branch outputs leads to intermediate consistency but does not effectively improve aesthetic or imaging quality, highlighting the high frequency components degradation caused by naive integration. Our proposed FreeLong method addresses this issue by selectively combining low-frequency global features with highfrequency local features, substantially improving aesthetic and imaging qualities while maintaining high consistency. The integration of our SpecMix initialization significantly boosts FreeLongs subjective and background consistency respectively, achieving the highest balance across all metrics. Furthermore, the enhanced FreeLong++ further elevates aesthetic and imaging qualities while maintaining superior consistency. Finally, using sparse attention for global-branch notably reduces inference time from 96 seconds to 74 seconds with minimal impact on quality metrics, demonstrating efficient computational performance. 5.4 Qualitative Comparison The synthesis results for each method are presented in Figure 7. In the first row, directly sampling 324 frames from model trained on 81 frames produces poor results due to high-frequency distortions, resulting in blurred faces and unclear backgrounds. As shown in the second row of Figure 7, using temporal sliding windows generates more vivid videos, but fails to maintain long-range visual consistency, leading to noticeable differences in the subject and background across frames. FreeNoise [34] aims to improve global consistency by repeating and shuffling initial noise, but still struggles with long-range consistency and suffers from content mutations. CausVid [48] uses auto-regressive architectures to generate coherent video sequences, but is affected by drifting, where visual quality degrades due to accumulated errors over time. In contrast, our method, FreeLong, enforces global constraints during denoising, ensuring temporal consistency and high fidelity across frames. As illustrated in Figure 7, FreeLong produces temporally consistent long videos, outperforming all other methods. Furthermore, FreeLong++ achieves even higher fidelity by using multi-band frequency fusion, better capturing motion IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 11 Fig. 8: Results of Multi-Prompt Video Generation. Our method ensures coherent visual continuity and motion consistency across different video segments. dynamics. 5.5 Multi-Prompt Video Generation Our method easily extends to multi-prompt video generation by assigning distinct prompts to each video segment. As shown in Figure 8, it maintains coherent visual continuity and consistent motion throughout. For example, white car drives seamlessly from dirt road to snowy road and then into starry night, all within unified scene and with smooth transitions. Compared to other approaches, including commercial models like Kling [69] and Pika [70], our method achieves superior consistency in scene transitions. This capability is particularly beneficial for storytelling applications, where maintaining coherence across diverse scenarios is critical. Compare to FreeNoise [34] that use repeat noise to constrain consistency, our multi-band spectral fusion framework adapts to diverse scenes and temporal complexities, producing videos that are both visually harmonious and temporally logical. In contrast to other systems, our method avoids abrupt or disjointed transitions. 5.6 Long-Range Control Capability FreeLong++ excels at long-range video control by conditioning generation on structured signals such as pose sequences or depth maps over hundreds of frames. As shown in Figure 9, our method faithfully adheres to long-duration control signals, preserving consistent motion semantics and scene layout throughout the video. In contrast, direct generation often leads to content drift, identity collapse, or spatial distortion over time. FreeLong++ effectively maintains subject fidelity and background stability across extended sequences, demonstrating its robustness to long-range control IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 12 Fig. 9: Long Control Sequence. Long-range video generation under pose (left) and depth (right) guidance. FreeLong++ produces more temporally consistent and semantically faithful outputs than direct generation. signals. This ability is critical for applications like motionguided synthesis or camera-path conditioning, where finegrained control must be preserved across the entire video."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We propose FreeLong++, training-free framework designed to effectively overcome frequency distortion challenges encountered when extending short-video generative models to longer sequences. By identify high-frequency degradation as critical limitation, we introduce multiband spectral attention mechanism that adaptively integrates temporal features across multiple frequency bands. Specifically, FreeLong++ first employs multi-window attention module to separately capture video dependencies at distinct temporal scales. Subsequently, it conducts multiband spectral fusion, systematically fuse these temporal features from low to high frequencies in the spectral domain. This approach significantly enhances temporal consistency and visual fidelity, all without requiring additional training. Our method can be seamlessly integrated into existing diffusion-based video generation models and demonstrates robust performance, consistently producing high-quality long videos across various tasks and model architectures."
        },
        {
            "title": "REFERENCES",
            "content": "[1] WanTeam, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, J. Wang, J. Zhang, J. Zhou, J. Wang, J. Chen, K. Zhu, K. Zhao, K. Yan, L. Huang, M. Feng, N. Zhang, P. Li, P. Wu, R. Chu, R. Feng, S. Zhang, S. Sun, T. Fang, T. Wang, T. Gui, T. Weng, T. Shen, W. Lin, W. Wang, W. Wang, W. Zhou, W. Wang, W. Shen, W. Yu, X. Shi, X. Huang, X. Xu, Y. Kou, Y. Lv, Y. Li, Y. Liu, Y. Wang, Y. Zhang, Y. Huang, Y. Li, Y. Wu, Y. Liu, Y. Pan, Y. Zheng, Y. Hong, Y. Shi, Y. Feng, Z. Jiang, Z. Han, Z.- F. Wu, and Z. Liu, Wan: Open and advanced large-scale video generative models, arXiv preprint arXiv:2503.20314, 2025. [2] Y. HaCohen, N. Chiprut, B. Brazowski, D. Shalem, D. Moshe, E. Richardson, E. Levin, G. Shiran, N. Zabari, O. Gordon, P. Panet, S. Weissbuch, V. Kulikov, Y. Bitterman, Z. Melumian, and O. Bibi, Ltx-video: Realtime video latent diffusion, arXiv preprint arXiv:2501.00103, 2024. [3] H. Chen, Y. Zhang, X. Cun, M. Xia, X. Wang, C. Weng, and Y. Shan, Videocrafter2: Overcoming data limitations for highquality video diffusion models, 2024. [4] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng et al., Cogvideox: Text-to-video diffusion models with an expert transformer, arXiv preprint arXiv:2408.06072, 2024. [5] Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai, Animatediff: Animate your personalized text-to-image diffusion models without specific tuning, arXiv preprint arXiv:2307.04725, 2023. [6] Y. Lu, L. Zhu, H. Fan, and Y. Yang, Flowzero: Zero-shot textto-video synthesis with llm-driven dynamic scene syntax, arXiv preprint arXiv:2311.15813, 2023. [7] X. Yang, L. Zhu, H. Fan, and Y. Yang, Eva: Zero-shot accurate attributes and multi-object video editing, arXiv preprint arXiv:2403.16111, 2024. [8] Z. Z. e. Weijie Kong, Qi Tian, Hunyuanvideo: systematic framework for large video generative models, 2024. [Online]. Available: https://arxiv.org/abs/2412.03603 [9] G. Ma, H. Huang, K. Yan, L. Chen, N. Duan, S. Yin, C. Wan, R. Ming, X. Song, X. Chen et al., Step-video-t2v technical report: The practice, challenges, and future of video foundation model, arXiv preprint arXiv:2502.10248, 2025. [10] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding et al., Cosmos world foundation model platform for physical ai, arXiv preprint arXiv:2501.03575, 2025. [11] A. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang et al., Movie gen: cast of media foundation models, arXiv preprint arXiv:2410.13720, 2024. [12] Y. Jin, Z. Sun, N. Li, K. Xu, H. Jiang, N. Zhuang, Q. Huang, Y. Song, Y. Mu, and Z. Lin, Pyramidal flow matching for efficient video generative modeling, arXiv preprint arXiv:2410.05954, 2024. [13] B. Lin, Y. Ge, X. Cheng, Z. Li, B. Zhu, S. Wang, X. He, Y. Ye, S. Yuan, L. Chen et al., Open-sora plan: Open-source large video generation model, arXiv preprint arXiv:2412.00131, 2024. [14] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts et al., Stable video IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 13 diffusion: Scaling latent video diffusion models to large datasets, arXiv preprint arXiv:2311.15127, 2023. [15] Y. Wang, Y. He, Y. Li, K. Li, J. Yu, X. Ma, X. Li, G. Chen, X. Chen, Y. Wang et al., Internvid: large-scale video-text dataset for multimodal understanding and generation, in The Twelfth International Conference on Learning Representations, 2023. [16] M. Bain, A. Nagrani, G. Varol, and A. Zisserman, Frozen in time: joint video and image encoder for end-to-end retrieval, in IEEE International Conference on Computer Vision, 2021. [17] T.-S. Chen, A. Siarohin, W. Menapace, E. Deyneka, H.-w. Chao, B. E. Jeon, Y. Fang, H.-Y. Lee, J. Ren, M.-H. Yang et al., Panda-70m: Captioning 70m videos with multiple cross-modality teachers, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13 32013 331. [18] Q. Wang, Y. Shi, J. Ou, R. Chen, K. Lin, J. Wang, B. Jiang, H. Yang, M. Zheng, X. Tao et al., Koala-36m: large-scale video dataset improving consistency between fine-grained conditions and video content, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 84288437. [19] Z. Tan, X. Yang, L. Qin, and H. Li, Vidgen-1m: largetext-to-video generation, arXiv preprint for scale dataset arXiv:2408.02629, 2024. [20] K. Liu, Q. Liu, X. Liu, J. Li, Y. Zhang, J. Luo, X. He, and W. Liu, Hoigen-1m: large-scale dataset for human-object interaction video generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 24 00124 010. [21] X. Wang, K. Zhao, F. Liu, J. Wang, G. Zhao, X. Bao, Z. Zhu, Y. Zhang, and X. Wang, Egovid-5m: large-scale videoaction dataset for egocentric video generation, arXiv preprint arXiv:2411.08380, 2024. [22] R. Henschel, L. Khachatryan, D. Hayrapetyan, H. Poghosyan, V. Tadevosyan, Z. Wang, S. Navasardyan, and H. Shi, Streamingt2v: Consistent, dynamic, and extendable long video generation from text, arXiv preprint arXiv:2403.14773, 2024. [23] F. Bao, C. Xiang, G. Yue, G. He, H. Zhu, K. Zheng, M. Zhao, S. Liu, Y. Wang, and J. Zhu, Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models, arXiv preprint arXiv:2405.04233, 2024. [24] Y. Tian, L. Yang, H. Yang, Y. Gao, Y. Deng, J. Chen, X. Wang, Z. Yu, X. Tao, P. Wan et al., Videotetris: Towards compositional text-tovideo generation, arXiv preprint arXiv:2406.04277, 2024. [25] W. Wang, H. Yang, Z. Tuo, H. He, J. Zhu, J. Fu, and J. Liu, Videofactory: Swap attention in spatiotemporal diffusions for text-to-video generation, arXiv preprint arXiv:2305.10874, 2023. [26] S. Zhuang, K. Li, X. Chen, Y. Wang, Z. Liu, Y. Qiao, and Y. Wang, Vlogger: Make your dream vlog, arXiv preprint arXiv:2401.09414, 2024. [27] L. Zhang and M. Agrawala, Packing input frame contexts in nextframe prediction models for video generation, Arxiv, 2025. [28] Y. Gu, W. Mao, and M. Z. Shou, Long-context autoregressive video modeling with next-frame prediction, arXiv preprint arXiv:2503.19325, 2025. [29] Y. Guo, C. Yang, Z. Yang, Z. Ma, Z. Lin, Z. Yang, D. Lin, and L. Jiang, Long context tuning for video generation, arXiv preprint arXiv:2503.10589, 2025. [30] K. Dalal, D. Koceja, J. Xu, Y. Zhao, S. Han, K. C. Cheung, J. Kautz, Y. Choi, Y. Sun, and X. Wang, One-minute video generation with test-time training, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17 70217 711. [31] X. Ren, L. Xu, L. Xia, S. Wang, D. Yin, and C. Huang, Videorag: Retrieval-augmented generation with extreme long-context videos, arXiv preprint arXiv:2502.01549, 2025. [32] J. Xiao, F. Cheng, L. Qi, L. Gui, J. Cen, Z. Ma, A. Yuille, and L. Jiang, Videoauteur: Towards long narrative video generation, arXiv preprint arXiv:2501.06173, 2025. [33] Y. Huang, W. Zheng, Y. Gao, X. Tao, P. Wan, D. Zhang, J. Zhou, and J. Lu, Owl-1: Omni world model for consistent long video generation, arXiv preprint arXiv:2412.09600, 2024. [37] Y. Li, W. Beluch, M. Keuper, D. Zhang, and A. Khoreva, Vstar: Generative temporal nursing for longer dynamic video synthesis, arXiv preprint arXiv:2403.13501, 2024. [38] M. Zhao, G. He, Y. Chen, H. Zhu, C. Li, and J. Zhu, Riflex: free lunch for length extrapolation in video diffusion transformers, arXiv preprint arXiv:2502.15894, 2025. [39] M. Cai, X. Cun, X. Li, W. Liu, Z. Zhang, Y. Zhang, Y. Shan, and X. Yue, Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 77637772. [40] J. Tan, H. Yu, J. Huang, J. Xiao, and F. Zhao, Freepca: Integrating consistency information across long-short frames in training-free long video generation via principal component analysis, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 27 97927 988. [41] Z. Li, H. Rahmani, Q. Ke, and J. Liu, Longdiff: Training-free long video generation in one go, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17 78917 798. [42] H. Yang, F. Tang, M. Hu, Q. Yin, Y. Li, Y. Liu, Z. Peng, P. Gao, J. He, Z. Ge et al., Scalingnoise: Scaling inference-time search for generating infinite videos, arXiv preprint arXiv:2503.16400, 2025. [43] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, High-resolution image synthesis with latent diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 10 68410 695. [44] Y. Wang, X. Chen, X. Ma, S. Zhou, Z. Huang, Y. Wang, C. Yang, Y. He, J. Yu, P. Yang et al., Lavie: High-quality video generation with cascaded latent diffusion models, arXiv preprint arXiv:2309.15103, 2023. [45] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh, Video generation models as world simulators, 2024, accessed: 2024-05-09. [Online]. Available: https://openai. com/research/video-generation-models-as-world-simulators [46] W. Peebles and S. Xie, Scalable diffusion models with transformers, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 41954205. [47] G. Team, Mochi 1, https://github.com/genmoai/models, 2024. [48] T. Yin, Q. Zhang, R. Zhang, W. T. Freeman, F. Durand, E. Shechtman, and X. Huang, From slow bidirectional to fast autoregressive video diffusion models, in CVPR, 2025. [49] S. ai, H. Teng, H. Jia, L. Sun, L. Li, M. Li, M. Tang, S. Han, T. Zhang, W. Q. Zhang, W. Luo, X. Kang, Y. Sun, Y. Cao, Y. Huang, Y. Lin, Y. Fang, Z. Tao, Z. Zhang, Z. Wang, Z. Liu, D. Shi, G. Su, H. Sun, H. Pan, J. Wang, J. Sheng, M. Cui, M. Hu, M. Yan, S. Yin, S. Zhang, T. Liu, X. Yin, X. Yang, X. Song, X. Hu, Y. Zhang, and Y. Li, Magi-1: Autoregressive video generation at scale, 2025. [Online]. Available: https://arxiv.org/abs/2505.13211 [50] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [51] Y. Lipman, R. T. Chen, H. Ben-Hamu, M. Nickel, and M. Le, Flow matching for generative modeling, arXiv preprint arXiv:2210.02747, 2022. [52] J. Zhang, C. Xiang, H. Huang, J. Wei, H. Xi, J. Zhu, and J. Chen, Spargeattn: Accurate sparse attention accelerating any model inference, arXiv preprint arXiv:2502.18137, 2025. [53] S. Zhang, W. Li, S. Chen, C. Ge, P. Sun, Y. Zhang, Y. Jiang, Z. Yuan, B. Peng, and P. Luo, Flashvideo: Flowing fidelity to detail for efficient high-resolution video generation, arXiv preprint arXiv:2502.05179, 2025. [54] P. Zhang, Y. Chen, R. Su, H. Ding, I. Stoica, Z. Liu, and H. Zhang, Fast video generation with sliding tile attention, arXiv preprint arXiv:2502.04507, 2025. [55] H. Xi, S. Yang, Y. Zhao, C. Xu, M. Li, X. Li, Y. Lin, H. Cai, J. Zhang, D. Li et al., Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, arXiv preprint arXiv:2502.01776, 2025. [34] H. Qiu, M. Xia, Y. Zhang, Y. He, X. Wang, Y. Shan, and Z. Liu, Freenoise: Tuning-free longer video diffusion via noise rescheduling, arXiv preprint arXiv:2310.15169, 2023. [56] H. Nyquist, Certain topics in telegraph transmission theory, Transactions of the American Institute of Electrical Engineers, vol. 47, no. 2, pp. 617644, 2009. [35] F.-Y. Wang, W. Chen, G. Song, H.-J. Ye, Y. Liu, and H. Li, Genl-video: Multi-text to long video generation via temporal codenoising, arXiv preprint arXiv:2305.18264, 2023. [36] J. Kim, J. Kang, J. Choi, and B. Han, Fifo-diffusion: Generating infinite videos from text without training, in NeurIPS, 2024. [57] C. E. Shannon, Communication in the presence of noise, Proceedings of the IRE, vol. 37, no. 1, pp. 1021, 2006. [58] T. Wu, C. Si, Y. Jiang, Z. Huang, and Z. Liu, Freeinit: Bridging initialization gap in video diffusion models, arXiv preprint arXiv:2312.07537, 2023. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 14 [59] H. Huang, Y. Feng, C. Shi, L. Xu, J. Yu, and S. Yang, Freebloom: Zero-shot text-to-video generator with llm director and ldm animator, Advances in Neural Information Processing Systems, vol. 36, pp. 26 13526 158, 2023. [60] T. Dao, FlashAttention-2: Faster attention with better parallelism and work partitioning, in International Conference on Learning Representations (ICLR), 2024. [61] Z. Huang, Y. He, J. Yu, F. Zhang, C. Si, Y. Jiang, Y. Zhang, T. Wu, Q. Jin, N. Chanpaisit et al., Vbench: Comprehensive benchmark suite for video generative models, arXiv preprint arXiv:2311.17982, 2023. [62] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 96509660. [63] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in International conference on machine learning. PMLR, 2021, pp. 87488763. [64] Z. Li, Z.-L. Zhu, L.-H. Han, Q. Hou, C.-L. Guo, and M.-M. Cheng, Amt: All-pairs multi-field transforms for efficient frame interpolation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 98019810. [65] J. Ke, Q. Wang, Y. Wang, P. Milanfar, and F. Yang, Musiq: Multiscale image quality transformer, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 51485157. [66] Y. Fang, H. Zhu, Y. Zeng, K. Ma, and Z. Wang, Perceptual quality assessment of smartphone photography, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 36773686. [67] LAION-AI, 06-04. aesthetic-predictor [Online]. Available: aesthetic-predictor, 2024, 2025https://github.com/LAION-AI/ accessed: [68] Z. Jiang, Z. Han, C. Mao, J. Zhang, Y. Pan, and Y. Liu, Vace: Allin-one video creation and editing, arXiv preprint arXiv:2503.07598, 2025. [69] Kling, Kling, https://kling.kuaishou.com/en, 2025, accessed: 2025-06-06, 11, 13. [70] Pika.art, Pika.art, https://pika.art, 2025, accessed: 2025-06-06."
        }
    ],
    "affiliations": [
        "ReLER, CCAI, Zhejiang University, Hangzhou, 310027, China"
    ]
}