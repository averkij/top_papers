{
    "paper_title": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models",
    "authors": [
        "Xin Xu",
        "Clive Bai",
        "Kai Yang",
        "Tianhao Chen",
        "Yangkun Chen",
        "Weijie Liu",
        "Hao Chen",
        "Yang Wang",
        "Saiyong Yang",
        "Can Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with a pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, a simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into a new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that Composition-RL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with a curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXU-USTC/Composition-RL."
        },
        {
            "title": "Start",
            "content": "Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models February 13, 2026 Xin Xu 1 2 Clive Bai 1 Kai Yang 1 Tianhao Chen 2 Yangkun Chen 1 Weijie Liu 1 Hao Chen 2 Yang Wang 3 Saiyong Yang 1 Can Yang"
        },
        {
            "title": "Abstract",
            "content": "Large-scale verifiable prompts underpin the success of Reinforcement Learning with Verifiable Rewards (RLVR), but they contain many uninformative examples and are costly to expand further. Recent studies focus on better exploiting limited training data by prioritizing hard prompts whose rollout pass rate is 0. However, easy prompts with pass rate of 1 also become increasingly prevalent as training progresses, thereby reducing the effective data size. To mitigate this, we propose Composition-RL, simple yet useful approach for better utilizing limited verifiable prompts targeting pass-rate-1 prompts. More specifically, Composition-RL automatically composes multiple problems into new verifiable question and uses these compositional prompts for RL training. Extensive experiments across model sizes from 4B to 30B show that CompositionRL consistently improves reasoning capability over RL trained on the original dataset. Performance can be further boosted with curriculum variant of Composition-RL that gradually increases compositional depth over training. Additionally, Composition-RL enables more effective cross-domain RL by composing prompts drawn from different domains. Codes, datasets, and models are available at https://github.com/XinXUUSTC/Composition-RL. 6 2 0 2 2 1 ] . [ 1 6 3 0 2 1 . 2 0 6 2 : r 1. Introduction After the advent of OpenAI-o1 (Jaech et al., 2024) and DeepSeek-R1 (Guo et al., 2025), Reinforcement Learning with Verifiable Rewards (RLVR) has reshaped the training lifecycle of large language models (LLMs), improv1HY, Tencent 2The Hong Kong University of Science and Technology 3The University of Hong Kong. Correspondence to: Can Yang <macyang@ust.hk>, Saiyong Yang <stevesyang@tencent.com>. Preprint. February 13, 2026. 1 ing both text-only reasoning (Luo et al., 2025; Yang et al., 2025a; Liu et al., 2025b; Cai et al., 2025) and multimodal question answering (Meng et al., 2025; Xiao et al., 2025). Rapid progress in RLVR, including improved optimization algorithms (Nan et al., 2025; Yu et al., 2025; Chen et al., 2025a; Liu et al., 2025b), more efficient training frameworks (Sheng et al., 2024; Fu et al., 2025; Zhu et al., 2025b), and techniques to mitigate traininginference mismatch (Yao et al., 2025; Qi et al., 2025), has contributed to the strong slow-thinking ability of large reasoning models (LRMs), often manifested as longer chain of thought (CoT) (Wei et al., 2022). At its core, RLVR relies on large collections of training prompts paired with ground-truth answers to enable verifiable reward computation during training (Hu et al., 2025; He et al., 2025b; Luo et al., 2025). Prompts with 0/1 rollout accuracy yield zero gradient signals in RLVR algorithms (Yu et al., 2025), substantially reducing the number of available informative prompts during training. However, collecting and cleaning additional high-quality training prompts is often expensive (He et al., 2025b; Zeng et al., 2025). To mitigate this, prior work has primarily focused on better leveraging hard prompts with zero success rate, via advantage shaping (Le et al., 2025; Nan et al., 2025), allocating more rollouts (Yang et al., 2025c; Li et al., 2025c), and hint-based augmentation (Chen et al., 2025b; Li et al., 2025a). Nevertheless, while all-zero prompts constitute some fraction of the training set, as training progresses, an increasing proportion of prompts attain rollout accuracy of 1. This motivates the need for methods that can better exploit these easy prompts. In this work, we propose Composition-RL, simple yet effective approach for better utilizing limited verifiable training prompts by transforming simple prompts into more challenging ones. We first introduce procedure for composing existing prompts into new prompts (Section 3.1) and empirically show that prompt composition can, to some extent, mitigate the growing number of too-easy prompts (Section 3.2). We then formalize Composition-RL as RL training on compositional prompts (Section 3.3); an overview is provided in Figure 1. As shown in Figure 1, CompositionRL outperforms RL training on the original prompts, with Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Figure 1. Overview of Composition-RL. Top: an example of composing two math problems, illustrating the high-level idea of CompositionRL. Bottom left: pass@1 (%) on AIME24 versus training steps for different methods, summarizing key findings in Sections 4.2 and 4.3. Bottom right: cross-topic results on MMLU-Pro subjects with the top-5 largest sample sizes, highlighting the main finding in Section 4.4. increasing performance gains when combined with curriculum over compositional depth K. Moreover, composing prompts from different domains shows strong potential for cross-domain RL training. Our contributions can be summarized as follows: ❶ We propose Composition-RL, an approach that performs RL on composed prompts that are automatically transformed from existing ones. ❷ Extensive experiments on 4B-30B LLMs demonstrate the effectiveness of Composition-RL and the curriculum variant of Composition-RL. ❸ We show that RL on composed prompts spanning physics and math is more effective than simply mixing training problems, regardless of whether sequential or joint training. ❹ We analyze the reasons behind the success of Composition-RL through the lenses of compositional generalization and implicit process supervision. 2. Preliminary Notation. We denote an LLM parameterized by θ as policy πθ. Let be an input query (i.e., prompt) and be the set of all queries. Given response = (r1, . . . , rr) to q, the policy likelihood can be written as πθ(r q) = (cid:81)r t=1 πθ(rt q, r<t), where r<t = (r1, . . . , rt1) and is the number of tokens in r. Each (q, r) can be evaluated by verifier v(q, r) {0, 1}, which indicates whether matches the ground-truth answer of (denoted as gt). RLVR. RLVR optimizes the expected verifiable reward: (cid:2)v(q, r)(cid:3)). (cid:2)JRLVR(θ, q)(cid:3) (= EqD, rπθ(q) maxθ EqD standard policy gradient estimator (Sutton et al., 1999) is: gθ(q, r) = A(q, r) θ log πθ(rq), (1) where A(q, r) = v(q, r) b(q) is called advantage and b(q) is baseline function that depends only on the query q. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) approximates the advantage by sampling group of responses {r1, . . . , rG} from the old policy πθold ( q): v(q, ri) mean(cid:0){v(q, rj)}G std(cid:0){v(q, rj)}G ˆAi = (2) j=1 (cid:1) (cid:1) . j=1 Then the objective of GRPO becomes JGRPO(θ) = (cid:2)JGRPO(θ, q)(cid:3) and JGRPO(θ, q) is defined as follows1: EqD 1 i=1 ri (cid:80)G (cid:88) ri (cid:88) i=1 t= (cid:16) ii,t(θ) ˆAi, clip(cid:0)ii,t(θ), 1 ϵ, 1 + ϵ(cid:1) ˆAi min (cid:17) , and the token-level importance ratio is given by ii,t(θ) = πθ(ri,t q, ri,<t) πθold (ri,t q, ri,<t) . (3) (4) 1We adopt more-commonly used version with token-level normalization suggested by (Yu et al., 2025). Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Figure 2. Visualization of meta-experiments. Left: solve all ratio curve for RL of Qwen3-4B-Base with original prompts (MATH12K) versus compositional prompts. Right: avg@8 accuracy on subset of MATH500 and its corresponding compositional test prompts. Dynamic Sampling. In practice, GRPO objective can be (cid:2)JGRPO(θ, q)(cid:3), where approximated by ˆJGRPO(θ) = EqB denotes sampled mini-batch of prompts at given training step. When prompt has an empirical success rate of 0 or 1 (i.e., all sampled responses are incorrect or all are correct), its advantage is set to zero; consequently, by Equation (1), policy-gradient updates vanish. To mitigate this, dynamic sampling (Yu et al., 2025) first over-samples larger candidate set ˆB and then constructs the training batch by filtering out uninformative prompts: = (cid:110) ˆB : 0 < mean(cid:0){v(q, rj)}G j= (cid:111) (cid:1) < 1 . (5) Hereafter, we call prompt solve all if its sampled responses {rj}G j=1 are all correct, and solve none if they are all incorrect. Following (Qu et al., 2025; Le et al., 2025),we use uninformative, zero-variance, and zero-advantage prompts as synonyms for solve all and solve none prompts. 3. Methodology & Meta-Experiments 3.1. SPC: Sequential Prompt Composition Yuan et al. (2025) studies the role of composition in RL using synthetic string-transformation setting, and Xiao & Zhao (2025) evaluates LLM performance under the composition of two math problems. We extend this line of work by investigating how composing training prompts affects RL training. In this section, we describe Sequential Prompt Composition (SPC): we first define how to compose two prompts, and then generalize to composing prompts. The whole composition process is illustrated in Figure 1. Composing Two Prompts. Given two prompts q1 and q2 with ground-truth answers gt1 and gt2, we define composition operator Compose that maps (q1, q2; gt1, gt2) to composed prompt q1:2 with ground-truth answer gt1:2: q1:2, gt1:2 = Compose(q1, q2; gt1, gt2). (6) The operator Compose consists of three steps (see Figure 1(a) for one concrete example): ❶ Modify q1 with gt1. Extract numeric value from gt1, denoted by v1. We then introduce natural-language definition d1 that names this value in terms of (q1, gt1), and form q1 = q1 d1. For instance, if q1 is What is the sum of the value(s) of for which 2n 7 = 3? and gt1 = 7, we set v1 = 7 and add definition such as: Let be the sum of the value(s) of satisfying 2n 7 = 3. ❷ Modify q2. Extract numeric value from q2 and replace it with new variable v2, yielding q2 = q2(v2). For example, if q2 is Simplify 2((5p+1)2p4)+(413)(6p9) to the form ap b, where and are positive, we may choose the constant 1 as v2 and replace it with variable name , obtaining Simplify 2((5p+Y )2p4)+(413)(6p9) to the form ap b, where and are positive. ❸ Connect q1 and q2. Compute v1 v2 and express the resulting relation between the two variables as naturallanguage statement r. Continuing the example above, with v1 = 7 and v2 = 1, we have v1 v2 = 6, so we can add constraint such as: is 6 less than X. The composed prompt is then q1:2 = q1 q2. By construction, the ground-truth answer of the composed prompt is gt1:2 = gt2. This composition is asymmetric to the order of q1 and q2, and solving q1:2 requires solving q1 first and then q2. Composing Prompts. More generally, we can compose prompts into one prompt. Given q1, . . . , qK with groundtruth answers gt1, . . . , gtK, Sequential Prompt Composition (SPC) applies Compose recursively for 1 steps: SPC(q1, . . . , qK ; gt1, . . . , gtK ) = Compose(q1, q2:K ; gt1, gt2:K ), (q2:K , gt2:K ) = SPC(q2, . . . , qK ; gt2, . . . , gtK ). where Finally, we will get the composed prompt q1:K and its answer gt1:K. We term as the Compositional Depth. Intuitively, solving q1:K requires the model having the ability to 3 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models solve all {qk}K k=1. The process of composing 2 prompts can be viewed as special case of SPC with = 2. Therefore, we do not distinguish between these two hereinafter. 3.2. Meta-Experiments & Observation As collecting new high-quality, verifiable training prompts can be costly (He et al., 2025b; Zeng et al., 2025), growing body of work has focused on better leveraging uninformative prompts in RLVR (Li et al., 2025a;c). However, existing methods primarily target solve none prompts. In this section, we conduct some meta-experiments and have the following key observations: ❶ Beyond solve none, the increasing prevalence of solve all prompts is another major impediment to effective RL training. ❷ SPC can make easy prompts harder and reduce the ratio of solve all prompts. Dilemma of Effective Training Prompts. As the policy model becomes stronger during RLVR, the proportion of solve all prompts observed during rollouts increases. Figure 2 (Left) plots the solve all rate across RL training steps for Qwen3-4B-Base on the MATH training set (Hendrycks et al., 2021). The solve all ratio rises rapidly from near zero to over 50% within the first 50 steps and then stabilizes around 75%. Although dynamic sampling is enabled to remove zero-variance prompts, the actual effective size of the whole training set at later stages is reduced to roughly 3K prompts (12,000 (1 0.75)). In contrast, the solve none ratio remains low (about 5%) at 250 steps. These results motivate methods that can deal with solve all prompts, in addition to solve none prompts. SPC can nudge the last bits out of existing prompts. Intuitively, SPC makes easy prompts harder. We empirically validate this on subset of the MATH500 test set using OpenMath-Reasoning-1.5B (Moshkov et al., 2025) and JustRL-1.5B (He et al., 2025a); additional details are provided in Section A.2. As shown in Figure 2 (Right), switching to compositional prompts reduces avg@8 by 19.7% for OpenMath-Reasoning-1.5B and by 15.4% for JustRL-1.5B. The solve all rate from 81.5% to 41.4% for also drops substantially: OpenMath-Reasoning-1.5B, and from 88.5% to 60.0% for JustRL-1.5B. These results suggest that SPC can effectively reduce solve all prompts, potentially turn part of the original uninformative prompts useful again. Additionally, even SPC with = 2 can almost double the training set size in principle (from to (D 1)). Note that JustRL-1.5B is obtained by RL training OpenMath-Reasoning-1.5B. Another interesting observation is that JustRL-1.5B improves performance both on the MATH500 subset (by 2.3%) and the compositional test set (by 6.6%). This suggests that RL training on normal prompts2 can also improve performance on compositional prompts. This raises natural question: Does RL training on compositional prompts benefit performance on normal reasoning tasks? 3.3. Composition-RL: RL with Compositional Data This section introduces Composition-RL, simple yet effective framework that leverages compositional data for RLVR training. Given the original training set = {(qi, gti)}D i=1, we can construct level-K compositional prompt set via the LLM-driven SPC procedure: DCK = (cid:8)(q, gt) : q, gt = SPC(q1, . . . , qK ; gt1, . . . , gtK ), (qk, gtk) D, = 1, . . . , K, qi = qj = j(cid:9). Since the size of DCK can be extremely large, we instead use smaller surrogate set: ˆDCK = (cid:8)(q, gt) : q, gt = SPC(q1, . . . , qK ; gt1, . . . , gtK ), (qk, gtk) Dk, = 1, . . . , K, qi = qj = j(cid:9), (7) where each Dk is small random subset of and serve as the candidate set for qk, i.e., qk Dk. In practice, we set Dk = 20 for = 1, . . . , 1, and DK = D. Composition-RL then optimizes the RLVR objective over (cid:2)JRLVR(θ)(cid:3). We compositional prompts: maxθ use the same GRPO objective JGRPO(θ), advantage estimator, and importance ratio as in Equations (2) to (4), except that prompts are sampled from the compositional dataset. Unless otherwise specified, we use = 2 in all remaining experiments, and abbreviate DC2 as DC. ˆDCK 4. Experiments 4.1. Experimental Setting In this section, we briefly summarize our experimental setup, including training procedures, baselines, and evaluation. Additional details are provided in Section B. Training Details We conduct RL training using the VeRL codebase (Sheng et al., 2024). Unless otherwise specified, we use unified set of hyperparameters (batch size 256, learning rate 1 106, and no warm-up) and fixed rollout settings (temperature 1, top 1, top 1, 8 rollouts per problem, and maximum output length of 16K tokens). We train Qwen3-4B-Base, Qwen3-8B-Base, Qwen3-14B-Base, and Qwen3-30B-A3B-Base on the MATH training set (Hendrycks et al., 2021). For the cross-topic experiments in Section 4.4, we use the physics subset of MegaScience (Fan et al., 2025). For the verifier, we choose Math-Verify, rule-based verifier. For fair comparison, we enable dynamic sampling to filter uninformative 2As compositional prompts differ substantially in structure from the original ones, we refer to prompts from all existing datasets as normal prompts for simplicity. 4 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Table 1. Results of Composition-RL across different benchmarks. Avg@k denotes the average accuracy (%) over random generations (i.e., pass@1). The rows Depth 1 + 2 and + Depth 3 are the results of curriculum Composition-RL in Section 4.3. Composition AIME 24 AIME 25 Beyond AIME IMOBench Avg@32 Avg@32 Avg@4 Avg@8 Overall Avg. GPQA MMLU-Pro Overall Avg. Avg@1 Avg@8 Mathematics (In-Domain) Multi-Task (Out-Of-Domain) Overall Overall Avg. Qwen3-4B-Base 23.3 30.5 7.2 19.5 23.3 3.8 Depth 1 + 2 + Depth 3 33.0 9.7 37.9 14.6 27.8 8.3 29.7 10. 9.0 12.6 3.6 13.1 4.1 14.6 5.6 14.4 14.3 0.1 20.1 5.7 22.9 8.5 16.6 20.2 3.6 23.5 6.9 26.3 9. 43.7 46.3 2.6 48.3 4.6 48.5 4.8 58.6 61.4 2.8 63.8 5.2 64.5 5.9 51.2 53.9 2.7 56.1 4.9 56.5 5. 28.1 31.4 3.3 34.4 6.3 36.4 8.3 Qwen3-8B-Base 26.1 36.9 10.8 20.4 26.5 6.1 13.7 13.9 0.2 16.2 18.4 2.2 19.1 23.9 4.8 48.2 48.9 0. 62.6 64.5 1.9 55.4 56.7 1.3 31.2 34.9 3.7 Qwen3-14B-Base 34.4 44.5 10.1 30.2 36.9 6. 17.0 19.7 2.7 21.3 25.9 4.6 25.7 31.8 6.1 55.0 54.2 0.8 67.2 69.3 2.1 61.1 61.8 0. 37.5 41.8 4.3 Qwen3-30B-A3B-Base 25.2 46.4 21.4 16.2 30.3 14.1 7.5 19.5 12.0 13.2 22.8 9. 15.5 29.8 14.3 50.7 54.6 3.9 62.6 64.6 2.0 56.7 59.6 2.9 29.2 39.7 10.5 prompts, ensuring that the effective batch size at each step remains constant across experiments. Baselines. In Section 4.2 (see Table 1), we compare Composition-RL with standard RLVR on MATH12K under the same number of gradient updates. For Composition-RL, we construct approximately 199K compositional prompts, which we denote as MATH-Composition-199K. In Section 4.3, we additionally report several RL-zero methods as reference points for our curriculum-based Composition-RL, including Beyond-80/20 (Wang et al., 2025), AlphaRL (Cai et al., 2025), and RL-ZVP (Le et al., 2025). For the cross-domain experiments in Section 4.4, we compare Composition-RL with two baselines: Mix Training (RL on mixed dataset comprising MATH12K and the MegaScience Physics subset) and Math-then-Physics (continued RL on Physics starting from MATH12K-trained checkpoint). Additional details are provided in Section B.2. Evaluation Details. Our evaluation benchmarks include both in-domain (ID) math reasoning tasks, AIME24/25, BeyondAIME (ByteDance-Seed, 2025), and IMOBench (Luong et al., 2025), and out-of-domain (OOD) multi-task reasoning benchmarks, GPQA-Diamond (Rein et al., 2024) and MMLU-Pro (Wang et al., 2024). Following Guo et al. (2025), we sample multiple responses per problem (from 1 to 32, depending on the benchmark size) and report pass@1 accuracy. All evaluation scripts are adapted from the DeepscaleR codebase (Luo et al., 2025). Following Yang et al. (2025a); Xu et al. (2025a), we set the temperature to 0.6, top to 0.95, top to 20, and the maximum output length to 32K tokens. See also Section B.3 for details. 4.2. Compositional Prompts Are Beneficial to RLVR To evaluate Composition-RL, we report results in Table 1 and compare against RL trained on the original MATH training set. From Table 1, we have the following: ❶ RL on compositional prompts consistently outperforms RL on the original prompts on both in-domain math and out-of-domain (OOD) general benchmarks. Across all model sizes, Composition-RL improves the overall mathematics performance by +3.6%, +4.8%, +6.1%, and +14.3% for Qwen3-4B/8B/14B/30B-A3B, respectively. Notably, gains are observed on challenging math benchmarks, including AIME24 (up to +21.4%), AIME25 (up to +14.1%), Beyond AIME (up to +12.0%), and IMOBench (up to +9.6%). Moreover, Composition-RL also improves OOD performance, increasing the multi-task overall by +2.7%, +1.3%, +0.7%, and +2.9%, leading to overall average gains of +3.3%, +3.7%, +4.3%, and +10.5% across the four base models. These significant gains demonstrate the effectiveness of Composition-RL and highlight the value of MATH-Composition-199K. ❷ The benefits of Composition-RL scale with model size, with larger models exhibiting substantially larger improvements, especially in mathematics. Overall gains increase from +3.3% (4B) and +3.7% (8B) to +4.3% (14B), and peak at +10.5% for Qwen3-30B-A3B. The scaling effect is most pronounced on in-domain mathematics: improvements rise from +3.6%/+4.8%/+6.1% to +14.3% as model size increases from 4B to 30B, whereas OOD multi-task gains are smaller but remain consistently positive. Notably, the MoE 30B-A3B model underperforms the 14B dense model, consistent with the fact that MoE activates 5 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models only subset of experts per token and can be more sensitive to routing and optimization under fixed training budget; nevertheless, Composition-RL still yields large gains on this model. Overall, these results highlight the strong potential of Composition-RL, particularly for larger models. 4.3. Curriculum RL to Higher Compositional Depth that have shown directly training We on MATH-Composition-199K outperforms training on the original MATH12K. As discussed in Section 3.2, during RL on MATH12K, the solve all ratio gradually rises to high level and performance begins to saturate; SPC can alleviate this issue. natural extension is to adopt curriculum that progressively increases the composition depth and continues RL training. Concretely, we first train on MATH12K; once performance saturates, we switch to Composition-RL with Depth 2. This transition causes the solve all ratio to drop sharply and enables further performance gains. We experiment with this curriculum version of Composition-RL from Depth 1 to Depth 3. Additional details are provided in Section B.2. As shown in Table 1 and Figure 1, we have the following observations: ❶ Curriculum Composition-RL can make full use of the original prompts, producing progressively stronger LRMs as the composition depth increases. Continuing RL with Depth 2 data after Depth 1 (i.e., the original MATH12K) yields substantial gains over improving by +9.7% on the Depth 1 checkpoint, AIME24 and +5.9% on MMLU-Pro. Moreover, the Depth 1Depth 2 curriculum even outperforms training directly on MATH-Composition-199K, delivering further +3.0% improvement on the overall average. Adding an additional Depth 3 stage continues to improve both indomain tasks and OOD question answering, with further +2.0% overall gain. Figure 1 presents the validation performance curves throughout the curriculum training process. In summary, these results imply that Composition-RL effectively converts limited prompts (with high solve all rates) into more useful samples. ❷ Curriculum Composition-RL on 4B model surpasses several 8B baselines, even under unfavorable settings. As shown in Figure 1, our final Composition-RL-4B model achieves 37.9% on AIME24, outperforming Beyond80/20-8B (Wang et al., 2025) (34.6%), Alpha-RL-8B (Cai et al., 2025) (28.3%), and RL-ZVP-8B (Le et al., 2025) (24.6%). Notably, Composition-RL uses only MATH12K and Qwen3-4B-Base, whereas these baselines train on DAPO-MATH-17K and Qwen3-8B-Base. Additional details are provided in Section B.2. Even in this unfavorable setting, Composition-RL achieves stronger performance, underscoring the importance of fully leveraging existing training prompts via composition. 4.4. Potential For General Domains Previously, Composition-RL considered only problems in the mathematical domain. In this section, we explore whether it can compose problems across domains. Specifically, we sample q1 from the physics subset and q2 from MATH12K, yielding cross-domain compositional dataset, Physics-MATH-Composition-141K. We compare against the Mix Training and Physics-then-Math baselines described in Section 4.1, with details in Section B.2. As shown in Table 2, we make the following observations: ❶ Adding physics prompts for RL training improves multi-task reasoning performance. Both the Mix Training and Math-then-Physics baselines improve GPQA and MMLU-Pro performance relative to training on MATH12K alone. On average, Mix Training increases the multi-task average by 0.8%, and Math-then-Physics yields larger gain of 2.1%. Moreover, Math-then-Physics can further increase performance on math reasoning tasks, whereas Mix Training slightly degrades the math reasoning ability. These results suggest that, while incorporating physics data benefits multi-task performance, sequential training (math followed by physics) is more effective than mixed training across topics. As shown in Figure 1, adding physics prompts (via Mix Training or Math-then-Physics) consistently improves generalization to law, engineering, and chemistry compared to training solely on math (Math-Only). Interestingly, training on MATH-Composition-199K (Math-Composition) also yields generalization beyond the math domain. ❷ Composing physics and math problems is more effective than naively combining physics and math prompts. RL training on Physics-MATH-Composition-141K outperforms all baselines by large margin. Specifically, our method achieves +1.3% gain over Math-then-Physics and +4.3% gain over training solely on MATH12K on MMLU-Pro. On AIME24, it improves by +7.1% over Math-then-Physics and by +9.1% over training solely on MATH12K. As shown in Figure 1, RL training on Physics-MATH-Composition-141K (Physics-MathComposition) consistently delivers the best results on both in-domain subjects (math and physics) and OOD subjects (law, engineering, and chemistry). These results highlight the great potential of Composition-RL for RL of multiple topics: training on composed prompts that require multidomain knowledge will definitely induce broad improvements across the corresponding topics, and Composition-RL can generate such prompts using existing ones. 5. Analysis 5.1. Ablation Study of Candidate Sets Dk As described in Section 3.3, each candidate set (except DK) is constructed by sampling from the full prompt pool D; 6 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Table 2. Results of cross-topic experiments across multiple benchmarks. Avg@k denotes the average accuracy (%) over random generations (i.e., pass@1). MATH12K + Physics corresponds to the Mix Training baseline, and Physics after MATH12K corresponds to the Math-then-Physics baseline. Best results in each column are in bold. Mathematics Multi-Task Overall Dataset MATH12K MATH12K + Physics Physics after MATH12K Physics-MATH-Composition-141K AIME 24 AIME 25 Beyond AIME IMOBench Overall GPQA MMLU-Pro Overall Overall Avg. Avg@32 Avg@32 Avg@ Avg@8 Avg@8 Avg@1 Avg. Avg. 23. 19.7 25.3 32.4 19.5 16.5 22.3 25.5 9.0 8.3 8.6 10.6 14. 12.0 14.4 17.8 16.6 14.1 17.7 21.6 43.7 44.4 45.2 46.6 58. 59.6 61.4 62.7 51.2 52.0 53.3 54.7 28.1 26.8 29.5 32.6 Table 3. Ablation Study of Candidate Set Dk. Avg@k denotes the average accuracy (%) over random generations (i.e., pass@1). D1 specifies the strategy for constructing the candidate set for q1: RAND randomly samples 20 prompts, whereas FULL selects from the entire original prompt set D. Baseline denotes RL training on the original prompts D. Mathematics (In-Domain) Multi-Task (Out-Of-Domain) Overall D1 D2 AIME 24 AIME 25 Beyond AIME IMOBench Overall GPQA MMLU-Pro Overall Overall Avg. Avg@ Avg@32 Avg@4 Avg@8 Avg@8 Avg@1 Avg. Avg. Baseline RAND RAND FULL RAND RAND FULL 23.3 22.6 24.5 30.5 19.5 19.6 23.4 23.3 9.0 8.2 8.7 12.6 14.4 13.8 14.1 14. 16.6 16.1 17.7 20.2 43.7 43.6 44.8 46.3 58.6 60.0 59.9 61.4 51.2 51.8 52.4 53.9 28.1 28.0 29.2 31.4 specifically, qk Dk for = 1, . . . , 1. For = 2, q1 is drawn from 20-prompt subset, whereas q2 is drawn from the full set D. We further evaluate the following variants for constructing the surrogate compositional set: A) Both D1 and D2 are small randomly sampled subsets (D1 = D2 = 500). B) D1 is the full set D, while D2 is small randomly sampled subset (D2 = 12, 000, D2 = 20). To ensure fair comparison of these variants, we keep the total amount of compositional data approximately constant and train for the same number of gradient updates under the unified training configuration in Section 4.1. Additional construction details are provided in Section B.4. Results are reported in Table 3. Our Composition-RL configuration (sampling D1 as random subset and using the full set for D2) achieves the best performance, improving overall accuracy by +3.4% over variant and by +2.2% over variant B. variant performs comparably to the baseline (RL on the original D), while both underperform relative to variant and our Composition-RL setting. This is not surprising because D1+D2 = 1,000 is substantially smaller than = 12,000, implying reduced diversity in the seed prompts used to construct the compositional set. Notably, despite using only 1K seed prompts, variant matches the baseline trained on 12K prompts, highlighting the potential of Composition-RL in limited-data regimes. Importantly, Composition-RL also outperforms variant by clear margin; for instance, on AIME24, CompositionRL achieves +6.0% accuracy gain. This suggests that increasing the diversity of D2 is beneficial. We hypothesize that this effect arises because the composed prompt q1:2 shares the same ground-truth answer gt1:2 as q2 and the current training paradigm verifies only the final answer of model responses. Under variant B, the model is repeatedly trained and verified on only D2 = 20 answers, potentially limiting the coverage of training signals. In contrast, our Composition-RL configuration exposes the model to verification over the full set D2 = D, yielding substantially more diverse set of answers to be verified. 5.2. Why Composition-RL Works In this section, we further investigate why Composition-RL works. We analyze it from two perspectives: Compositional Generalization. Compositional data may incentivize the acquisition of new skills. Yuan et al. (2025) show that, in controlled synthetic setting, training on compositional data can elicit new reasoning skills. Analogously, if we view an original problem as requiring stack of skills, composing prompts can create training instances that demand skill recombination. As shown in Figure 3 (left), Composition-RL substantially improves performance on Depth-2 compositional test prompts relative to training on Depth-1 data, even though Depth-2 is more challenging. This result supports compositional generalization: models trained with composed prompts transfer better to deeper compositions and, consequently, also improve on the standard test set, likely because the acquired skills are useful for solving more complex problems. Implicit Process Supervision. The final-outcome reward for composed prompts also provides implicit signals for the solution process. As shown in Figure 1, to solve the composed prompt q1:2, LLMs must first obtain v1 and then use v1 to solve q2. We posit that this structured dependency nudges the model toward correct intermediate step, 7 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Figure 3. Left: avg@8 accuracy on subset of MATH500 and the corresponding compositional test prompts across different model sizes. The darker color and the numbers denote the improvement of our Composition-RL over the RL training on the MATH12K baseline. Right: The fraction of prompts for which q1:2 is solved correctly, and the accuracy of recovering v1 at each training step. at least halfway through the reasoning. As illustrated in Figure 3 (right), the steady improvement in recovering v1 provides evidence that composed prompts can serve as implicit process supervision, even when training relies only on the verification of the final answers. 6. Related Work Longer Training with Finite Prompts. Amid the surge of interest in RLVR (Jaech et al., 2024; Guo et al., 2025), many studies investigate how to enable longer and more stable training under fixed prompt set (Liu et al., 2025b; He et al., 2025a). One line of work improves training stability from an algorithmic perspective (Chen et al., 2025a; Yang et al., 2025b; Liu et al., 2025a). Another line aims to exploit limited training data better, including filtering uninformative prompts (Yu et al., 2025; Zheng et al., 2025; Qu et al., 2025), shaping advantages for zero-advantage prompts (Zhu et al., 2025a; Nan et al., 2025; Le et al., 2025), and allocating more samples to harder prompts (Yang et al., 2025c; Li et al., 2025c). Among these, hint-based problem augmentation (Chen et al., 2025b; Li et al., 2025a) is most closely related to our work: they use hints to transform originally hard prompts into easier ones. In contrast, we make easy prompts harder via compositional prompt generation. Enlarging RLVR Training Prompts. The fuel of RLVR is its training prompts. substantial body of work is devoted to collecting and curating high-quality data from diverse sources (Albalak et al., 2025; He et al., 2025b; Hu et al., 2025). Synthesizing data from existing datasets has also been extensively studied, both for evaluation (Shi et al., 2023; Xu et al., 2024; Xiao & Zhao, 2025) and for SFT (Yang et al., 2024; Yu et al., 2023b; Tong et al., 2024). More recently, several efforts have begun to synthesize prompts specifically for RL training (Xie et al., 2025; Li et al., 2025b; Stojanovski et al., 2025; Zeng et al., 2025). In contrast to synthetic logic-only problems or game-like environments, we target general reasoning tasks, achieving strong performance on mathematical reasoning and highlighting the potential for cross-domain integration. Compositional Generalization. Compositional generalization refers to models ability to recombine learned skills to solve novel tasks. It has been longstanding focus in natural language processing (Keysers et al., 2019; Hupkes et al., 2020; Lake & Baroni, 2018). Prior work often studies compositionality using controlled testbeds, such as Skill-Mix (Yu et al., 2023a) for language tasks, compositional math benchmarks (Sun et al., 2025), or algorithmic tasks (Dziri et al., 2023). Zhao et al. (2024) show that composing textual skills can benefit SFT. Yuan et al. (2025) suggest that compositionality is important for RL to acquire new skills. However, their results are restricted to synthesized string-manipulation tasks. In comparison, we extend composition to broader reasoning settings and demonstrate the effectiveness of composing RL training prompts. 7. Conclusion & Discussion In this paper, we study how to maximize the utility of existing prompts for RL training. Comprehensive experiments across various model sizes show that Composition-RL consistently outperforms RL on the original prompts. We also demonstrate the potential of composing prompts from different topics. Our analysis suggests that compositional prompts can provide implicit process supervision by encouraging correct intermediate steps. We will release our codes, compositional datasets, and trained models to support future RL research. Promising future directions include: ❶ Extending beyond MATH12K by composing more challenging math training set like Polaris-53K. ❷ Expanding composition to cover more domains. ❸ Adapting Composition-RL to on-policy distillation (Lu & Lab, 2025). 8 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents Composition-RL, which aims to advance research on RLVR. We plan to release two compositional datasets, MATH-Composition-199K and Physics-MATH-Composition-141K, which we expect to be useful resources for future work on RL for LLMs. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Albalak, A., Phung, D., Lile, N., Rafailov, R., Gandhi, K., Castricato, L., Singh, A., Blagden, C., Xiang, V., Mahan, D., et al. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387, 2025. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. https://huggingface.co/datasets/ ByteDance-Seed/BeyondAIME, 2025. ging Face repository. HugCai, Y., Cao, D., Xu, X., Yao, Z., Huang, Y., Tan, Z., Zhang, B., Liu, G., and Fang, J. On predictability of reinforcement learning dynamics for large language models. arXiv preprint arXiv:2510.00553, 2025. Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. Minimaxm1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585, 2025a. Chen, J. C.-Y., Peng, B. X., Choubey, P. K., Huang, K.-H., Zhang, J., Bansal, M., and Wu, C.-S. Nudging the boundaries of llm reasoning. arXiv preprint arXiv:2509.25666, 2025b. Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., Welleck, S., West, P., Bhagavatula, C., Le Bras, R., et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36: 7029370332, 2023. Fan, R.-Z., Wang, Z., and Liu, P. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812, 2025. URL https: //arxiv.org/abs/2507.16812. Fu, W., Gao, J., Shen, X., Zhu, C., Mei, Z., He, C., Xu, S., Wei, G., Mei, J., Wang, J., Yang, T., Yuan, B., and Wu, Y. Areal: large-scale asynchronous reinforcement learning system for language reasoning. ArXiv, 2025. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, B., Qu, Z., Liu, Z., Chen, Y., Zuo, Y., Qian, C., Zhang, K., Chen, W., Xiao, C., Cui, G., et al. Justrl: Scaling 1.5 llm with simple rl recipe. arXiv preprint arXiv:2512.16649, 2025a. He, Z., Liang, T., Xu, J., Liu, Q., Chen, X., Wang, Y., Song, L., Yu, D., Liang, Z., Wang, W., et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025b. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Hu, J., Zhang, Y., Han, Q., Jiang, D., Zhang, X., and Shum, H.-Y. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Hupkes, D., Dankers, V., Mul, M., and Bruni, E. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67: 757795, 2020. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Keysers, D., Scharli, N., Scales, N., Buisman, H., Furrer, D., Kashubin, S., Momchev, N., Sinopalnikov, D., Stafiniak, L., Tihon, T., et al. Measuring compositional generalization: comprehensive method on realistic data. arXiv preprint arXiv:1912.09713, 2019. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lake, B. and Baroni, M. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International conference on machine learning, pp. 28732882. PMLR, 2018. Le, T.-L. V., Jeon, M., Vu, K., Lai, V., and Yang, E. No prompt left behind: Exploiting zero-variance prompts in llm reinforcement learning via entropy-guided advantage shaping. arXiv preprint arXiv:2509.21880, 2025. 9 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Li, J., Lin, H., Lu, H., Wen, K., Yang, Z., Gao, J., Wu, Y., and Zhang, J. Questa: Expanding reasoning capacity in llms via question augmentation. arXiv preprint arXiv:2507.13266, 2025a. Li, P., Ye, J., Chen, Y., Ma, Y., Yu, Z., Chen, K., Cui, G., Li, H., Chen, J., Lyu, C., et al. Internbootcamp technical report: Boosting llm reasoning with verifiable task scaling. arXiv preprint arXiv:2508.08636, 2025b. Li, Z., Chen, C., Yang, T., Ding, T., Sun, R., Zhang, G., Huang, W., and Luo, Z.-Q. Knapsack rl: Unlocking exploration of llms via optimizing budget allocation. arXiv preprint arXiv:2509.25849, 2025c. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025a. Liu, M., Diao, S., Lu, X., Hu, J., Dong, X., Choi, Y., Kautz, J., and Dong, Y. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025b. Lu, K. and Lab, T. M. On-policy distillation. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml. 20251026. https://thinkingmachines.ai/blog/on-policydistillation. Qi, P., Liu, Z., Zhou, X., Pang, T., Du, C., Lee, W. S., and Lin, M. Defeating the training-inference mismatch via fp16. arXiv preprint arXiv:2510.26788, 2025. Qu, Y., Wang, Q., Mao, Y., Hu, V. T., Ommer, B., and Ji, X. Can prompt difficulty be online predicted for accelerating rl finetuning of reasoning models? arXiv preprint arXiv:2507.04632, 2025. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Scharli, N., and Zhou, D. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 31210 31227. PMLR, 2023. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Li, L. E., Popa, R. A., and Stoica, I. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. Stojanovski, Z., Stanley, O., Sharratt, J., Jones, R., Adefioye, A., Kaddour, J., and Kopf, A. Reasoning gym: Reasoning environments for reinforcement learning with verifiable rewards. arXiv preprint arXiv:2505.24760, 2025. Luong, M.-T., Hwang, D., Nguyen, H. H., Ghiasi, G., Chervonyi, Y., Seo, I., Kim, J., Bingham, G., Lee, J., Mishra, In S., et al. Towards robust mathematical reasoning. Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 3540635430, 2025. Meng, F., Du, L., Liu, Z., Zhou, Z., Lu, Q., Fu, D., Han, T., Shi, B., Wang, W., He, J., et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Moshkov, I., Hanley, D., Sorokin, I., Toshniwal, S., Henkel, C., Schifferer, B., Du, W., and Gitman, I. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. Nan, G., Chen, S., Huang, J., Lu, M., Wang, D., Xie, C., Xiong, W., Zeng, X., Zhou, Q., Li, Y., et al. Ngrpo: Negative-enhanced group relative policy optimization. arXiv preprint arXiv:2509.18851, 2025. Sun, Y., Hu, S., Zhou, G., Zheng, K., Hajishirzi, H., Dziri, N., and Song, D. Omega: Can llms reason outside the box in math? evaluating exploratory, compositional, and transformative generalization. arXiv preprint arXiv:2506.18880, 2025. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Team, Q. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.github.io/ blog/qwen2.5/. Tong, Y., Zhang, X., Wang, R., Wu, R., and He, J. Dartmath: Difficulty-aware rejection tuning for mathematical problem-solving. ArXiv preprint, abs/2407.13690, 2024. URL https://arxiv.org/abs/2407.13690. Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective 10 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, K., Xu, X., Chen, Y., Liu, W., Lyu, J., Lin, Z., Ye, D., and Yang, S. Entropic: Towards stable long-term training of llms via entropy stabilization with proportionalintegral control. arXiv preprint arXiv:2511.15248, 2025b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers. nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference. html. Yang, Z., Guo, Z., Huang, Y., Wang, Y., Xie, D., Wang, Y., Liang, X., and Tang, J. Depth-breadth synergy in rlvr: Unlocking llm reasoning gains with adaptive exploration. arXiv preprint arXiv:2508.13755, 2025c. Yao, F., Liu, L., Zhang, D., Dong, C., Shang, J., and Gao, J. Your efficient rl framework secretly brings you off-policy rl training, August 2025. URL https://fengyao. notion.site/off-policy-rl. Xiao, T., Xu, X., Huang, Z., Gao, H., Liu, Q., Liu, Q., and Chen, E. Advancing multimodal reasoning capabilities of multimodal large language models via visual perception reward. arXiv preprint arXiv:2506.07218, 2025. Xiao, X. and Zhao, H. From and to a+ b: Can large language models solve compositional math problems? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 13068 13089, 2025. Xie, T., Gao, Z., Ren, Q., Luo, H., Hong, Y., Dai, B., Zhou, J., Qiu, K., Wu, Z., and Luo, C. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Xu, X., Xiao, T., Chao, Z., Huang, Z., Yang, C., and Wang, Y. Can llms solve longer math word problems better? ArXiv preprint, abs/2405.14804, 2024. URL https: //arxiv.org/abs/2405.14804. Xu, X., AI, C., Yang, K., Chen, T., Wang, Y., Yang, S., and Yang, C. Thinking-free policy initialization makes distilled reasoning models more effective and efficient reasoners. arXiv preprint arXiv:2509.26226, 2025a. Xu, X., Xu, Q., Xiao, T., Chen, T., Yan, Y., Zhang, J., Diao, S., Yang, C., and Wang, Y. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334, 2025b. Yu, D., Kaur, S., Gupta, A., Brown-Cohen, J., Goyal, A., and Arora, S. Skill-mix: flexible and expandable family of evaluations for ai models. arXiv preprint arXiv:2310.17567, 2023a. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. ArXiv preprint, abs/2309.12284, 2023b. URL https://arxiv.org/abs/2309.12284. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yuan, L., Chen, W., Zhang, Y., Cui, G., Wang, H., You, Z., Ding, N., Liu, Z., Sun, M., and Peng, H. From (x) and g(x) to (g(x)): Llms learn new skills in rl by composing old ones. arXiv preprint arXiv:2509.25123, 2025. Zeng, Z., Ivison, H., Wang, Y., Yuan, L., Li, S. S., Ye, Z., Li, S., He, J., Zhou, R., Chen, T., et al. Rlve: Scaling up reinforcement learning for language models with adaptive verifiable environments. arXiv preprint arXiv:2511.07317, 2025. Zhao, H., Kaur, S., Yu, D., Goyal, A., and Arora, S. Can models learn skill composition from examples? Advances in Neural Information Processing Systems, 37:102393 102427, 2024. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., et al. Qwen2. 5-math technical report: Toward mathematical expert model via selfimprovement. arXiv preprint arXiv:2409.12122, 2024. Zheng, H., Zhou, Y., Bartoldson, B. R., Kailkhura, B., Lai, F., Zhao, J., and Chen, B. Act only when it pays: Efficient reinforcement learning for llm reasoning via selective rollouts. arXiv preprint arXiv:2506.02177, 2025. 11 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025a. Zhu, Z., Xie, C., Lv, X., and slime Contributors. slime: An llm post-training framework for rl scaling. https:// github.com/THUDM/slime, 2025b. GitHub repository. Corresponding author: Xin Lv. 12 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models A. Details of Meta-Experiments A.1. The solve all Ratio The training setup in Figure 2 (Left) follows the main experimental protocol (see Section B.1). We define the solve all ratio as the fraction of solve all prompts among all over-sampled prompts collected during dynamic-sampling rollouts at given training step. For compositional data construction, please refer to Section D. For RL training details with compositional data, please refer to Section B.1 and Section B.2. A.2. Details of Initial Evaluation For SPC For the evaluation in Figure 2 (Right), we randomly sample 200 questions from MATH500 as seed prompts and use SPC to construct level-2 compositional prompts. Specifically, we form pairs (q1, q2) by sampling 5 seed questions as candidates for q1 and pairing each with the 200 seed questions as q2. After filtering, this procedure yields approximately 400 compositional test prompts. We use the same decoding settings as in Section B.3. B. Experimental Details B.1. Training Details In Section 4.1, we briefly describe the training setup. This appendix provides additional details on our RL training configuration. Two settings are particularly important: we enable dynamic sampling (Yu et al., 2025) to filter uninformative prompts, and we use rollout correction to mitigate traininginference mismatch (Yao et al., 2025). Hyperparameters and Rollout Settings. Unless otherwise specified, we use unified set of hyperparameters: batch size 256, learning rate 1 106, and no warm-up. We also adopt unified rollout configuration: temperature 1, top 1, top 1, 8 rollouts per problem, and maximum output length of 16K tokens. Datasets and Verifiers. For the main experiments in Section 4.2 and Section 4.3, we train on the MATH training set (Hendrycks et al., 2021). Following the standard protocol, we exclude the MATH500 test set, leaving roughly 12K training prompts spanning five difficulty levels. For the cross-topic experiments in Section 4.4, we utilize the physics subset of MegaScience (Fan et al., 2025), which comprises approximately 23K prompts. For training efficiency, we use Math-Verify as the verifier. Considering that rule-based verifiers do not reliably evaluate model outputs on physics problems (Xu et al., 2025b), we filter the MegaScience physics subset by removing examples for which all eight responses from Qwen3-4B-Thinking-2507 are judged incorrect by Math-Verify. This yields approximately 8.2K prompts on which rule-based verification is reliable. B.2. Baselines In this appendix, we provide additional baseline details for the experiments in Sections 4.2 to 4.4. For the experiments in Section 4.2, the baseline corresponds to RL training on the original MATH12K training set, which contains 12K training prompts. Our Composition-RL trains instead on compositional prompts constructed from MATH12K. As described in Section 3.3, we sample q1 from randomly selected subset of 20 prompts and sample q2 from the full dataset, yielding 20 12K = 240K compositional prompts in principle. As discussed in Section D.1, we apply verificationand-filtering procedure to improve data quality. After verification of Step 1, approximately 231K prompts remain; after verifying Step 2, this is reduced to roughly 200K; and after the final check, we obtain about 199K compositional prompts. We refer to this composition set as MATH-Composition-199K. For the experiments in Section 4.3, we include Beyond-80/20 (Wang et al., 2025), AlphaRL (Cai et al., 2025), and RLZVP (Le et al., 2025) as additional reference baselines. We use the models that are initialized from Qwen3-8B-Base and trained on DAPO-MATH-17K prompts from these works. We report their results as RL-zero baselines to our curriculumbased Composition-RL trained from Qwen3-4B-Base. This comparison is unfavorable to Composition-RL due to differences in both model scale and training data. For curriculum Composition-RL, we first train Qwen3-4B-Base on the original MATH12K set (Depth 1). After performance saturates, we switch to the Depth 2 training set (MATH-Composition-199K), and then to the Depth 3 training set once Depth 2 saturates. The construction of the 13 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Depth 3 compositional set follows the procedure used for Depth 2. Since Beyond-80/20 (Wang et al., 2025), AlphaRL (Cai et al., 2025), and RL-ZVP (Le et al., 2025) have not released their models at the time we were writing our paper, we report their results as quoted directly from the corresponding papers. For the experiments in Section 4.4, we consider two natural RL baselines. The first is RL training on mixture of MATH12K and the MegaScience Physics subset, which we denote as Mix Training. The second baseline continues RL training on Physics data, starting from checkpoint trained on MATH12K, which we denote as Math-then-Physics. For Math-thenPhysics, we train until performance saturates. For fair comparison, we train Mix Training for approximately the same number of total gradient updates as the combined MATH12K stage plus the physics training stage. or Composition-RL, we consider sampling q1 from Physics and q2 from MATH12K. After filtering, the resulting compositional dataset contains approximately 141K prompts, which we denote as Physics-MATH-Composition-141K B.3. Evaluation Details To comprehensively evaluate model capabilities, we use diverse suite of benchmarks spanning mathematical reasoning and multi-task reasoning: 1. Mathematical reasoning: We evaluate on AIME24, AIME25, BeyondAIME (ByteDance-Seed, 2025), and IMOBench (Luong et al., 2025). Since AIME24 and AIME25 each contain 30 problems, we report pass@1 using 32 samples per problem (avg@32). BeyondAIME contains 100 problems; we report avg@8. For IMO-Bench, we use the AnswerBench subset to enable rule-based verification; it contains 400 problems, and we report avg@4. 2. Multi-task reasoning: We evaluate on GPQA-Diamond (Rein et al., 2024) (approximately 200 problems) and report pass@1 using 8 samples per problem. We also evaluate on MMLU-Pro (Wang et al., 2024); since it contains over 5K problems, we report results from single run. All evaluation codes are adapted from the DeepscaleR (Luo et al., 2025) codebase, and we use vLLM (Kwon et al., 2023) to accelerate inference and Math-Verify to evaluate the LLMs answers. For decoding, we follow Xu et al. (2025a) and set the temperature to 0.6, top to 0.95, top to 20, and the maximum output length to 32K tokens. B.4. Details of Ablation for Candidate Sets Dk As noted in Section 5.1, the default configuration of Composition-RL sets D2 = (the full prompt pool) and samples D1 as small random subset ( D2 = 12,000, D1 = 20 ). We also consider two variants: A) Both D1 and D2 are small randomly sampled subsets ( D1 = D2 = 500 ). B) D1 is the full set D, while D2 is small randomly sampled subset ( D1 = 12,000, D2 = 20 ). These settings are designed to yield roughly the same theoretical compositional dataset size. After applying the filtering procedure in Section D.1, the resulting actual dataset sizes are: Composition-RL: 199K (see Section B.2). Variant A: 240K after step 1, 202K after step 2, and 200K after step 3. Variant B: 231K after step 1, 201K after step 2, and 200K after step 3. Thus, the final dataset sizes are approximately matched across configurations. C. Analysis Details In this appendix, we provide additional details for Section 5.2. To evaluate compositional generalization, we use the same setting as in Section A.1. To determine whether the first variable v1 is solved correctly, we prompt Qwen2.5-32B-Instruct using the default generation configuration and the prompt shown in Figure 4. 14 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Prompt for Verifying the Correctness of Finding v1 in LLMs Response You are math solution verifier. solves for specific intermediate variable in composite math problem. Your task is to check if given response correctly **Problem:** {problem} **Target Variable:** ${symbol}$ **Variable Definition:** {definition} **Correct Answer for ${symbol}$:** {correct answer} **Models Response:** {response} --- **Your Task:** 1. response. 2. Compare it with the correct answer: 3. Determine if the model correctly solved for ${symbol}$. {correct answer}. Identify what value the model computed for ${symbol}$ in its **Important Notes:** - Focus ONLY on whether ${symbol}$ was correctly computed; ignore the final answer of the composite problem. - The value might be stated explicitly (e.g., ${symbol}$ = 7) or implicitly derived. - Accept equivalent forms (e.g., 7, 7.0, seven are all correct if the answer is 7). \"<the value the model gave for {symbol}, or NOT FOUND if not **Output in JSON format:** { \"extracted value\": mentioned>\", \"is equivalent\": <true if extracted value equals {correct answer}, false otherwise>, \"reasoning\": \"<brief explanation>\", \"verdict\": \"<CORRECT or INCORRECT>\" } Figure 4. The Prompt for Verifying the Correctness of Finding v1 in LLMs Response. 15 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models D. Details of SPC D.1. Reliability of SPC The full SPC pipeline can be automated with an LLM assistant; implementation details and the corresponding prompts are provided in Section D.2 and Section D.3, respectively. To make this automated process more reliable, we have some additional verification steps to filter potential mistakes during composing. Following (Xiao & Zhao, 2025), we use LLM-based self-verification at each composition step. Concretely, we prompt the same LLM to perform the following checks: ❶ Verification of Modify q1 with gt1. In this step, the LLM extracts variable v1 from gt1 and its definition d1. We then ask the LLM to compute the value of v1 given q1 and d1, and compare the computed value against the extracted v1. If they do not match, we discard the prompt. This verification improves the reliability of the modification of q1. ❷ Verification of Modify q2. Analogously, we prompt the LLM to verify whether the extracted variable v2 (and its definition) is consistent with q2. Prompts that fail this check are filtered out. ❸ Verification of Connect q1 and q2. This step primarily involves concatenation. To ensure quality, we prompt the LLM to check for inconsistencies (e.g., conflicting variable names) and filter out any inconsistent prompts. This verification procedure removes many low-quality compositions, leaving substantially more reliable set of composed prompts. As reported in (Xiao & Zhao, 2025), the rate of erroneous prompts after filtering is below 2%. We believe this error rate is acceptable for training. D.2. Implementation Details We use Qwen2.5-32B-Instruct (Team, 2024) with step-specific prompts to implement each stage of Section 3.1 as well as the verification procedure in Section D.1. Unless otherwise specified, we set the temperature to 0.1, top to 0.7, and the maximum output length to 4096 tokens. The prompts are provided in Section D.3. D.3. Prompts of SPC Following (Xiao & Zhao, 2025), we provide the prompt used to modify q1 in 5 and the self-verification prompt used to check the modification in 6. We use similar prompts for the other steps of SPC, and we will release the complete set of prompts in our codes. 16 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Prompt for Modifying q1 with gt Given math problem and the final answer, your task is to find out one number from the answer and provide the corresponding definition. Follow the steps below: Step 1: Identify specific integer, float, or fraction within final answer and name it as new variable1; There are several situations: 1. If the final answer contains unknown variables: (a) If the final answer is an expression, choose one coefficient as new variable1, for example, 2x + 3, you can choose the coefficient of as new variable1, which is 2, and in the case of sin(x), there is hidden coefficient 1 and hidden amplitude 1, you can choose either one as new variable1; (b) If the final answer is an equation, you can choose one solution as new variable1, for example, = 2x + 1, you can define the value of as new variable1 when given = 1, which is 3; (c) If the final answer is symbol of an option or word, such as A, B, CAT, etc., use their first letters order in the alphabet as variable, such as = 1, = 2, CAT = 3, etc.; (d) If the final answer contains 2 or more items, e.g., multiple choice questions, choose the smallest or the largest one, and then apply the corresponding situation. 2. If the final answer has no unknown variables, there are several situations: (a) If the final answer itself is numerical value, like four, 4, 2 + new variable1; 2, 3π, and 3 4 , use it directly as (b) If the final answer contains 2 or more numerical values, use the largest or the smallest one as new variable1; (c) If the final answer is an interval or ratio, choose one boundary and is not allowed, for example, [2, ), you can define the lower bound as new variable1, which is 2; (d) If the final answer is ratio, choose one part of the ratio, for example, 3 : 4; you can define the first part of the simplified ratio as new variable1, which is 3; (e) If the final answer is non-base 10 number, for example, 10012, you can define the number of digits in the base 2 representation as new variable1, which is 4; (f) If the final answer is an angle or degree, choose the corresponding radian value, for example, 30 or 30, define the corresponding radian value of final answer as new variable1, which is π/6. All in all, find way to identify specific numerical value as new variable1 without unknown, and make sure the reader can get the value of new variable1 from the final answer through your definition. Step 2: Output the value of new variable1, keep the exact value or math symbol, and simplify the fraction if necessary, 8 as 3 for example, keep π as π, keep Step 3: Output the definition of new variable1 without mentioning the real value. 4 , without rounding to decimal point. 2, and simplify 6 2 as Output Format: ... (omit for simplicity) Examples: ... (omit for simplicity) Figure 5. The Prompt for Generating Variable v1 and Definition d1 for q1. 17 Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models Prompt for Verifying the Modification of 1. Check the extraction of variable v1: {Problem 1} Assume that the final answer of the problem is {FINAL_ANSWER}. {DEFINITION_OF_NEW_VARIABLE1} Then what is the value of new variable1? Please output the value of new variable1 directly, wrapping it in boxed{}, for example, boxed{3}. 2. Check the value of v1 using Python: **Task Description:** Write Python program to compare two given values and determine if they are equal. Follow these guidelines: 1. Use the sympy library to handle symbolic comparisons, ensuring that equivalent expressions (e.g., 2 4 and 1 2 ) are recognized as equal. 2. For values involving irrational constants (e.g., π, e), perform comparisons up to two decimal places for practical equivalence. 3. Include clear intermediate steps in the program, such as evaluating or simplifying the values where appropriate. 4. Wrap the final comparison outcome in boxed{} command for clarity. 5. Provide both the Python code and the results of running the code. **Output Format:** python {The Python code that compares the two given values, including print statements for intermediate steps and the boxed{final comparison outcome}.} output {The output of the Python program.} --- [Examples Here] Figure 6. The Prompt for Verifying the Modification of q1."
        }
    ],
    "affiliations": [
        "HY, Tencent",
        "The Hong Kong University of Science and Technology",
        "The University of Hong Kong"
    ]
}