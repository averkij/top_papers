{
    "paper_title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
    "authors": [
        "Aleksandar JevtiÄ‡",
        "Christoph Reich",
        "Felix Wimbauer",
        "Oliver Hahn",
        "Christian Rupprecht",
        "Stefan Roth",
        "Daniel Cremers"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of a scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given a single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in a feed-forward manner. Through a novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of a current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards a strong foundation for single image 3D scene understanding."
        },
        {
            "title": "Start",
            "content": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion Aleksandar Jevtic* 1 Christoph Reich* 1,2,4,5 Felix Wimbauer 1,4 Oliver Hahn 2 1TU Munich 2TU Darmstadt Christian Rupprecht 3 3University of Oxford https://visinf.github.io/scenedino Stefan Roth 2,5,6 5ELIZA 4MCML Daniel Cremers 1,4,5 6hessian.AI *equal contribution 5 2 0 2 8 ] . [ 1 0 3 2 6 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Distillation",
            "content": "+ Clustering"
        },
        {
            "title": "Single Input Image",
            "content": "3D Feature Field"
        },
        {
            "title": "SSC Prediction",
            "content": "Figure 1. SceneDINO overview. Given single input image (left), SceneDINO predicts both 3D scene geometry and 3D features in the form of feature field (middle) in feed-forward manner, capturing the structure and semantics of the scene. Unsupervised distillation and clustering of SceneDINOs feature space leads to unsupervised semantic scene completion predictions (right)."
        },
        {
            "title": "Abstract",
            "content": "Semantic scene completion (SSC) aims to infer both the 3D geometry and semantics of scene from single images. In contrast to prior work on SSC that heavily relies on expensive ground-truth annotations, we approach SSC in an unsupervised setting. Our novel method, SceneDINO, adapts techniques from self-supervised representation learning and 2D unsupervised scene understanding to SSC. Our training exclusively utilizes multi-view consistency self-supervision without any form of semantic or geometric ground truth. Given single input image, SceneDINO infers the 3D geometry and expressive 3D DINO features in feed-forward manner. Through novel 3D feature distillation approach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised scene understanding, SceneDINO reaches state-of-the-art segmentation accuracy. Linear probing our 3D features matches the segmentation accuracy of current supervised SSC approach. Additionally, we showcase the domain generalization and multi-view consistency of SceneDINO, taking the first steps towards strong foundation for single image 3D scene understanding. 1. Introduction Understanding the geometry and semantics of 3D scenes from image observations is fundamental computer vision task with broad applications in robotics [26], autonomous driving [46, 65], medical image analysis [18, 112], and civil engineering [69]. The Semantic Scene Completion (SSC) task unifies 3D geometry and semantic prediction from limited image observations [63, 88, 95]. Recent progress in SSC has been primarily driven by utilizing supervised learning [37, 87, 95]. However, acquiring large-scale 3D annotations is highly labor-intensive [65]. While significant resources have been invested in collecting human annotations for 2D tasks [52, 84], annotating similar amounts of data in 3D remains unapproached. This motivates approaching SSC without the need for manually annotated data. Existing SSC approaches rely on ground-truth semantic annotations and frequently utilize additional supervision from LiDAR scans [37, 45, 73, 95]. In contrast, we are the first to approach SSC in fully unsupervised setting, i.e. without task supervision or other supervised components. In particular, we aim to approach SSC from single image without relying on any human annotations, only learning from unlabeled multi-view images using self-supervision. This setting is extremely challenging for two reasons: first, the human-defined nature of semantic taxonomies is ambiguous, and second, single image only provides partial observation of the scene with many invisible areas. We take inspiration from recent advances in self-supervised learning (SSL) of 2D representations and 3D reconstruction. 2D SSL representations, such as from DINO [11], have been shown effective for 2D unsupervised scene understanding [32, 103]. 3D reconstruction approaches successfully leveraged SSL from multi-view data to infer dense 3D geometry from single image [33, 107]. In this paper, we present SceneDINO, to the best of our knowledge, the first approach for unsupervised semantic scene completion. Trained using 2D SSL features To appear in Proceedings of the IEEE/CVF International Conference on Computer Vision, Honolulu, Hawaii, USA, 2025. 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. from DINO [11] and multi-view self-supervision [107], SceneDINO predicts both 3D geometry and 3D features from single image during inference in feed-forward manner. Our general 3D feature representations enable us to approach unsupervised 3D scene understanding. Harnessing our expressive 3D features, we propose novel 3D feature distillation approach for obtaining unsupervised semantic predictions in 3D. While we focus on the task of unsupervised SSC, SceneDINOs features are general, offering foundation for different 3D scene-understanding tasks by building on our 3D feature field. Specifically, we make the following contributions: (i) We introduce SceneDINO, the first approach predicting dense 3D geometry and expressive 3D features in feedforward manner from single image. (ii) We effectively distill SceneDINOs feature field representation in 3D, obtaining unsupervised semantic predictions. (iii) We demonstrate the first fully unsupervised SSC results. We build simple yet competitive unsupervised SSC baseline, lifting unsupervised 2D semantic predictions. Our SceneDINO approach outperforms this SSC baseline in unsupervised SSC as well as established 2D approaches in 2D semantic segmentation. (iv) Finally, we also showcase the domain generalization ability and multi-view consistency of SceneDINO. 2. Related Work Single-image scene reconstruction. Estimating 3D geometry from image observations is fundamental task in computer vision and has been studied for decades [36]. Traditional approaches, such as structure from motion [89], as well as recent neural radiance fields (NeRFs) [74], perform scene reconstruction using multiple images, as reviewed by multiple surveys [34, 108, 119]. Recently, estimating dense 3D geometry from single image have been approached [8, 33, 80, 85, 96, 102, 107, 113]. Unlike monocular depth estimation [75], these approaches predict the depth for visible and occluded regions, reconstructing complete scene. Behind the Scenes (BTS) [107] introduced an approach for unsupervised single-image scene reconstruction using multi-view self-supervision, which infers dense 3D geometry in feed-forward manner. Our approach extends BTS by additionally lifting self-supervised features into 3D for unsupervised 3D scene understanding. Semantic scene completion (SSC), also known as 3D semantic occupancy prediction, aims to jointly estimate the 3D geometry and semantics of scene [62, 63, 95, 117]. Initial approaches used 3D semantic and geometric annotations and addressed indoor scenes [6, 13, 5759, 67, 116], outdoor scenes with LiDAR [16, 61, 86, 87, 109], or both domains [8, 73]. Using birds-eye views has been proven effective for SSC [44, 64, 99]. To overcome the need for 3D annotations, approaches for using 2D annotations have been proposed [37, 45, 81]. While SelfOcc [45] and RenderOcc [81] use multiple inference views, S4C [37] performs single-image SSC. In particular, S4C [37] employs supervised 2D model and lifts 2D multi-view semantic predictions into 3D. In contrast to using 2D annotations, GaussTR [48] uses 2D foundation models for SSC and multiple views during inference. However, GaussTR relies on heavily supervised foundation models, including SAM [52] and Metric3Dv2 [42], and uses weak supervision from image/text pairs. To the best of our knowledge, there is no method for approaching SSC without the need for any ground-truth annotations. Our work presents the first unsupervised SSC approach, utilizing lifted SSL features and single RGB input image for inference. Self-supervised representation learning (SSL) aims to extract general features from data without annotations, facilitating various downstream tasks such as segmentation [24]. Recent SSL methods, often based on Vision Transformers (ViTs) [23], leverage clustering [2, 9, 10, 47, 60], masked modeling [20, 29, 39, 76, 106], contrastive learning [3, 12, 14, 38, 40, 41], or negative-free [4, 5, 11, 28, 79] pretext tasks [22, 78] for large-scale training. Stateof-the-art models, e.g., DINO [11], produce semantically rich, dense features, driving recent advances in 2D unsupervised scene understanding [32, 103]. We here aim to bring expressive features from DINO [11, 79] to 3D for SSC. 2D-to-3D feature lifting. The expressiveness of 2D visual representations has motivated lifting 2D features into 3D [93, 110]. Existing approaches utilize multi-view 2D features for 3D feature lifting [30, 43, 49, 53, 72, 82, 92, 93, 97, 100, 101, 105, 110, 115]. Lifting 2D features is effective in various tasks, including few-shot semantic occupancy prediction [110], and refining 2D representations [115]. However, existing feature-lifting approaches fit to single scene [49, 53, 92, 93, 100, 101, 110, 115], require RGB-D inputs [30, 43, 72, 97, 105], or work on 3D point cloud inputs [82]. The only feed-forward approaches that use RGB inputs and lift 2D features, which we are aware of, are GaussTR [48]; MVSplat360 [15]. However, both approaches utilize multiple input images during inference, and MVSplat360 [15] only predicts low-dimensional feature representations, which are not suitable for unsupervised scene understanding. In contrast, we propose the first feedforward approach for inferring lifted high-dimensional and rich 3D features using single input image. 2D unsupervised semantic segmentation partitions images automatically into semantically meaningful regions without any form of human annotations. Early deep learning-based methods [17, 35, 47] approach the problem via representation learning. Leveraging SSL features from DINO as an inductive prior, STEGO [32] distills the feature representation into lower-dimensional space for unsupervised probing. Building up on STEGO, subsequent methods 2 I0 Î¾ pxi fxiÏƒxi eu Ï• xi Embedding eu Ï• (u, d) Î³ Ïƒ Density Ïƒi Feature fi (cid:90) ( Ë†d, Ë†f , Ë†c) Reconstructed views I1 xi xi Multi-view image & feature reconstruction Sample at us Color ci Target views (a) 3D feature field & semantic inference (b) Volumetric feature & image rendering (c) Self-supervised multi-view training Figure 2. SceneDINO architecture, rendering, and training. (a) Inference: Given single input image I0 during inference, 2D encoder-decoder Î¾ produces the embedding from which the local embedding eu is interpolated. The MLP encoder Ï• takes in eu and 3D position xi, and predicts both the density Ïƒxi and the 3D feature fxi . Using lightweight unsupervised segmentation head h, we can obtain semantic predictions pxi using fxi . (b) Rendering: Our feature field allows for volume rendering by shooting rays through it, yielding depth Ë†d and Ë†f in 2D. Color ci is sampled from an another view (e.g., I1) using us and rendered to obtain the reconstructed color Ë†c. (c) Multi-view training: We render 2D views (features & images) from our feature field and reconstruct the training views. [31, 50, 91, 94] propose enhancements to the distillation. Our approach follows the idea of STEGO [32], extending it to 3D and integrating feature distillation using our 3D feature field to build the first unsupervised SSC approach. 3. Unsupervised Semantic Scene Completion We approach semantic scene completion (SSC) without any form of manual supervision. To this end, we first describe SceneDINO, predicting 3D geometry and expressive 3D features from single image in feed-forward manner (Sec. 3.1), and SceneDINOs multi-view training (Sec. 3.2). Next, we present our 3D feature distillation approach to obtain unsupervised 3D semantic predictions (Sec. 3.3). An overview of our full pipeline, including inference, rendering, and multi-view self-supervision, is provided in Fig. 2. Notation. Let I0 [0, 1]3HW be single RGB input image (for both training & inference) with corresponding pose T0 R44 and projection matrix K0 R34. For training, let (Iv, Tv, Kv) with {1, 2, . . . , n}, be additional views for multi-view self-supervision. Assuming pinhole camera model, any 3D point R3 in world coordinates can be projected onto the image plane of view and the input view = 0 with the perspective projection Ï€v(x). 3.1. SceneDINO Given single input image I0, SceneDINO represents the dense geometric structure and features of scene as continuous mapping from world coordinates R3 to volumetric density Ïƒx R+ and feature fx RD. This continuous output representation is often called feature field. While SceneDINO could represent any feature space, we aim for expressive SSL features from DINO [11, 79]. Architecture & feature field inference. Our SceneDINO architecture comprises two main parts: 2D encoderdecoder Î¾ and an MLP decoder (cf . Fig. 2a), following BTS [107]. Î¾ takes in I0 and produces per-pixel embedding RDEHW with DE dimensions. Intuitively, every spatial element of represents camera ray through pixel, capturing both local geometry and features. To infer the feature at 3D position x, we employ two-layer MLP decoder Ï• (cf . Fig. 2a). Given position within the camera frustum, we project into the camera plane, obtaining the pixel location = Ï€0(x). We query at the position using bilinear interpolation, obtaining the local embedding eu. Based on the embedding eu, the pixel position u, and the distance dx R+ of to the camera, we obtain the density Ïƒx and feature prediction fx as (Ïƒx, fx) = Ï•(eu, Î³(u, dx)), (1) where Î³ denotes positional encoding [74]. Feature, depth & color volume rendering. SceneDINO predicts continuous feature field from single image. This representation can be used to render features and depth in 2D from an arbitrary viewpoint (cf . Fig. 2b), following the discretization strategy of Max et al. [71]. Given viewpoint (Tr, Kr), we sample points xi along the ray through pixel ur, with distance Î´i between xi and xi+1. Based on the volumetric densities Ïƒxi (cf . Eq. 1), we can compute the probabilities Î±i of the ray ending between xi and xi+1, and accumulate these into Vi, the probability of xi being visible: Vi = i1 (cid:89) j=1 (1 Î±j) , with Î±i = 1 exp(ÏƒxiÎ´i) . (2) Using Vi and Î±i, we render depth dur and feature fur from the estimated features fxi from Eq. (1) and distances dxi to xi onto the image plane at position ur as fur = (cid:88) i=1 ViÎ±ifxi dur = (cid:88) i=1 ViÎ±idxi. (3) 3 The differentiability of this rendering process enables us to self-supervise SceneDINO using multi-view images and their 2D feature representations (e.g., from DINO [11]). SceneDINO predicts 3D geometry and features, but does not predict color as we focus on semantic downstream tasks. To obtain color for image reconstruction during training, we follow the color sampling approach of BTS [107]. 3.2. 3D feature field training We train SceneDINO using multi-view self-supervision (cf . Fig. 2c), aiming to obtain an expressive and view-consistent feature field without the need for any form of manual annotations. For self-supervision, we sample + 1 views Iv with camera parameters1 Tv, Kv from the data and obtain dense 2D features Fv from self-supervised ViT (e.g., DINO [11]). Note that the 2D features entail resolution of Fv RD , due to the ViT patch size p. The set of training views and features = {(Iv, Tv, Kv, Fv) = 0, . . . , n} is randomly partitioned into two subsets Vsource and Vtarget. Training reconstructs the views Vtarget using the views of Vsource. In practice, we use randomly sampled set of image patches that align with the ViT patches instead of the full image. In the following, we still refer to images for the sake of brevity. Image reconstruction. We aim to learn the geometry of our feature field via multi-view photometric consistency. In particular, for every image It Vtarget we derive reconstructed image Ë†It,s from every view in Vsource using differentiable rendering and color sampling. Equipped with both the reconstructed image Ë†It,s and the target image It, we compute our photometric loss per patch as Lp = min (cid:16) IsVsource Î»1L1(It, Ë†It,s) + Î»SSIMLSSIM(It, Ë†It,s) (cid:17) . (4) We only consider the minimum per-patch loss across the different views in Vsource. The scalars Î»1 and Î»SSIM weight the absolute error L1 and the SSIM loss LSSIM [104]. To regularize the 3D geometry prediction, we impose smoothness using an edge-aware smoothness loss [27]. Based on the estimated depth dut (cf . Eq. 3), we obtain the inverse and mean-normalized depth ut. Using ut, we compute the edge-aware smoothness Ls for pixel ut as Ls = d ut exIt + d ut eyIt, (5) using the first spatial derivatives and at ut. Feature reconstruction. We learn multi-view consistent and expressive 3D feature field using the 2D features Ft from Vtarget. As we aim to learn high-resolution (continuous) feature field, we render 2D features using Eq. 3 at the full image resolution Ë†Ft RDHW. To compensate for 1Note, camera poses can be obtained using unsupervised visual SLAM [7], strictly adhering to the fully unsupervised setting. 3D feature distillation. Given an input Figure 3. image, SceneDINO predicts 3D feature field. 3D features fX are sampled from the feature field. For fX, we obtain fYkNN and fYkrand from the feature buffer. The segmentation head distills the features into low-dimensional space and is trained using Ldist. the reduced spatial dimension of Ft, we employ the downsampler Ïˆ proposed by Fu et al. [25] to our rendered features Ë†Ft. While current 2D SSL features capture semantics, they lack multi-view consistency, i.a., due to positional encodings used in ViTs [111], leading to different features for identical visual content at two distinct positions in an image. As we aim for multi-view consistency, we compensate for this by learning constant decomposition RD of features induced by positional encodings. Our feature loss is defined per feature as Lf = 1 cos-sim(Ft, Ïˆ( Ë†Ft) + F), (6) where cos-sim is the cosine similarity between two features. As image edges correlate with semantic edges and to further impose consistency, we regularize the rendered features Ë†Ft using an edge-aware smoothness loss per feature Lfs = Ë†Ft exIt + Ë†Ft eyIt. (7) Our final loss for training SceneDINO is weighted sum of the photometric loss, the feature loss, and both smoothness losses LSceneDINO = Î»pLp + Î»sLs + Î»fLf + Î»fsLfs, averaged over all pixels and features. 3.3. 3D feature distillation for unsupervised SSC Given the expressive feature field representation, we aim to obtain unsupervised semantic predictions for SSC. While naÄ±ve k-means [68, 70] can yield meaningful pseudo semantics, distilling features into lower-dimensional space has been shown to be more effective in 2D semantic segmentation [32, 54]. To this end, we present novel 3D feature distillation approach (cf . Fig. 3). We train pointwise segmentation head h, mapping fx RD to lowerdimensional distilled representation zx RK, with D. The resulting distilled space is clustered to obtain pseudosemantic predictions px [0, 1]C, with pseudo classes. Existing work in 2D unsupervised semantic segmentation has shown that SSL feature correspondence captures 4 semantic class co-occurence [32]. This correspondence between two batches of sample points = [x1, . . . , xN] and = [y1, . . . , yN] can be expressed by pairwise feature similarity Si,j = cos-sim(fxi, fyj ) [1, 1]. Similarly, we can express the correspondence in the distilled feature space by Sh i,j = cos-sim(h(fxi ), h(fyj )) [1, 1]. We describe the sampling of the xi and yj below. Feature distillation. We aim to distill features such that similar features align while dissimilar features are separated. To this end, we use the contrastive correlation loss Lcorr, introduced by STEGO [32] and defined as Lcorr(fX, fY, b) = (cid:88) (Si,j b) max(Sh i,j, 0), (8) i,j where fX, fY are the features of the two sample batches. This loss pushes Sh i,j towards 1 in case Si,j exceeds the threshold b. Otherwise, Lcorr pushes the Sh i,j below 0. The correlation loss Lcorr requires informative pairs of sampled features, balancing attractive and repulsive signals. Following STEGO [32], we consider three different relations: (1) feature pairs from the same image (fX, fX), (2) feature pairs from an image and its k-nearest neighbors in feature space (fX, fYkNN), and (3) feature pairs from an image and randomly sampled other image (fX, fYrand). Note that each pair is obtained from SceneDINOs 3D feature field, see below. Equipped with the three feature sample pairs, we compute the full distillation loss as Ldist =Î»selfLcorr(fX, fX, bself) + Î»kNNLcorr(fX, fYkNN , bkNN) + Î»randLcorr(fX, fYrand, brand), (9) where Î»self, Î»kNN, and Î»rand denote the scalar loss weights. bself, bkNN, and brand are the contrastive thresholds. Feature sampling in 3D. While obtaining feature pairs using 2D rendered features is straightforward [32], we aim to take advantage of our learned 3D geometry of the scene. To this end, we introduce novel 3D feature sampling approach for the distillation loss Ldist from Eq. (9). Our goal is to sample features both similar and dissimilar in terms of the encoded semantic concept, which should capture rich semantics as well as different semantic concepts. First, we obtain all visible 3D surface points R3G and their depth dV RG + from the camera. To sample points that cover different semantic concepts, we use depth as cue and sample different depth ranges. In particular, we sort the surface points based on dV. The sorted surface points Ë†V are partitioned into equally-sized chunks; we uniformly sample single 3D point from each chunk, resulting in center points R3M. Equipped with the center points X, we aim to extract rich semantic features from the feature field. While we could just obtain the features for X, we query positions in the 5 Center point Xi Accepted samples Rejected samples Figure 4. 3D feature sampling. We first sample center point Xi from all visible surface points. Further points are sampled within the radius around the center point Xi. Sampled points with sufficient density are accepted; otherwise rejected. The accepted points are used to obtain the feature batch fX. neighborhood of to increase semantic richness and better capture the 3D structure of the scene for distillation. In particular, for each center point, we randomly sample point within radius of = 0.5 m. To account for samples falling into unoccupied regions in our feature field, we only keep samples with sufficient density Ïƒ > 0.5. We repeat this sampling process until we obtain valid samples per center point. Using these samples, we query our feature field, resulting in feature batch fX RDN for each of the center points in each scene (cf . Fig. 4). To obtain fYkNN and fYrand, we utilize feature buffer that efficiently stores the sampled features of multiple scenes. Given new input image, we obtain feature batches fX as just described. For each fX, we randomly sample another feature batch from the buffer to obtain fYrand. To obtain fYkNN, we search in the feature buffer for the k-nearest neighbors of fX, using the average feature of each batch. From these k-nearest neighbors, we randomly pick feature batch to obtain fYkNN and compute the distillation loss Ldist. After repeating this process for each of the current feature batches, we add the current feature batches to the feature buffer and remove the oldest batches. Unsupervised probing. To obtain semantic predictions, we probe the distilled feature space using k-means [68, 70]. In particular, we iteratively update cluster centers Î¸ RKC using cosine distance-based mini-batch kmeans [90] during distillation. To infer the final semantic prediction, we compute px = softmax(cos-sim(h(fx), Î¸)). 4. Experiments We evaluate SceneDINO on SSC and compare it to simple unsupervised SSC baseline (Sec. 4.1). We also report results for 2D unsupervised segmentation, including domain generalization results (Sec. 4.2). Finally, we explore multiview feature consistency (Sec. 4.3) and present an analysis of SceneDINO and our 3D distillation (Sec. 4.4). Datasets. We train using KITTI-360 [65], composed of clips from moving vehicle equipped with cameras. For consistency, we follow S4C [37] by sampling eight views and using the dataset camera poses. We also provide results with estimated poses. We also show experiments for training on RealEstate10k [118], composed of monocular videos. Here, we follow the setup of BTS [107], obtaining three views. If not noted differently, we report results obtained with training on KITTI-360. For SSC and 2D semantic segmentation validation, we use the SSCBench-KITTI360 test split [63]. Cityscapes [19] and BDD100K [114] val are used for domain generalization results. To enable evaluation in 3D and 2D, we use the 19-class taxonomy of Cityscapes and perform 2D evaluation on Cityscapes, BDD100K, and KITTI-360 on 19 classes. For SSCBench, we combine classes to adhere to the 15 SSCBench classes. 3D evaluation. Given our unsupervised setup, we predict pseudo-semantic classes that must be aligned with the ground truth for evaluation. We follow standard practice in 2D unsupervised semantic segmentation [17, 31, 32, 50, 91, 94] by applying Hungarian matching [56] to align our pseudo semantics. For validating the aligned semantics, we follow the standardized SSCBench [63] protocol and report both semantic performance using the mean Intersectionover-Union (mIoU) and geometric performance using IoU, precision, and recall. We report all metrics on SSCBench ranges 12.8 m, 25.6 m, and 51.2 m. 2D evaluation. Following the established evaluation protocol in 2D unsupervised semantic segmentation [17, 31, 32, 50, 91, 94], we use the all-pixel accuracy (Acc) and mean Intersection-over-Union (mIoU) metrics. Likewise, in line with prior work, 2D segmentation predictions of all models are refined using dense Conditional Random Field [55] before computing Acc and mIoU. Multi-view feature consistency evaluation. We aim to evaluate the multi-view consistency of our feature field. As we are not aware of any general feed-forward 3D feature field approach, we compare against 2D SSL models. To measure multi-view consistency in 2D, we use two video frames and estimate optical flow and occlusions with RAFT [98]. We backward warp 2D features of the second frame to the first. On the aligned features, we compute the feature similarity using absolute error (L1), the Euclidean distance (L2), and the cosine similarity, ignoring occlusions. Baselines. We are not aware of any existing unsupervised SSC approach. To allow for comparisons, we construct competitive baseline for unsupervised SSC. In particular, we train the S4C approach with unsupervised semantics of the established STEGO [32] approach. For 2D semantic segmentation, we use U2Seg [77] and STEGO [32] as established unsupervised baselines. Note U2Seg is trained on ImageNet [21] and COCO [66] using STEGO pseudolabels. We use STEGO [32] with DINO [11] (ViT-B/8), DINOv2 [79] (ViT-B/14), and FiT3D [115] (ViT-B/14) Table 1. SSCBench-KITTI-360 results. Semantic results using mIoU and per class IoU, and geometric results using IoU, Precision, and Recall (all in %, ) on SSCBench-KITTI-360 test using three depth ranges. We compare our baseline S4C + STEGO to our SceneDINO. We report S4C as 2D supervised baseline. Method S4C + STEGO SceneDINO (Ours) S4C Supervision Unsupervised 2D supervision Range 12.8 25.6 51.2 12.8 25.6 51.2 12.8 25.6 51.2 Semantic validation mIoU car bicycle motorcycle truck other-v. person road sidewalk building fence vegetation terrain pole traffic-sign other-obj. 0.00 0.00 7.51 0.00 0.00 0.00 0.00 0.00 0.00 0. 8.00 16.94 13.94 10.19 6.60 10.76 10.01 9.26 10.53 9.22 21.24 15.94 11.21 22.58 18.64 11.49 18.57 14.09 0.00 0.00 0.01 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 2.12 0.00 0.02 0.00 0.04 0.11 0.06 0.00 0.02 0.00 0.05 0.01 0.01 0.00 0.00 0.01 0.00 0.01 61.97 52.47 38.15 51.10 49.12 39.82 69.38 61.46 48.23 18.74 20.95 18.21 20.26 22.31 18.97 45.03 37.12 28.45 14.75 24.44 17.81 12.33 18.27 14.32 26.34 28.48 21.36 1.41 3.64 0.90 15.83 16.58 11.30 31.22 25.57 19.85 35.78 28.04 21.43 4.17 23.26 18.02 15.22 35.03 22.88 15.08 26.49 0.65 0.05 0.04 0.08 0.36 0.00 0.00 0.00 0.00 0.00 0.02 0.05 0.00 0.00 4.37 0.01 0.00 9.95 0.04 0.00 0.04 0.94 0.83 0.00 1.23 1.57 0.00 0.05 0.00 0. 0.05 0.00 0.00 0.20 6.37 9.70 0.58 1. 0.11 Geometric validation"
        },
        {
            "title": "IoU\nPrecision\nRecall",
            "content": "49.32 41.08 36.39 49.54 42.27 37.60 54.64 45.57 39.35 54.04 46.23 41.91 53.27 46.10 41.59 59.75 50.34 43.59 84.95 78.69 73.43 87.61 83.59 79.67 86.47 82.79 80.16 features. FiT3D offers multi-view refined DINOv2 features [115]. Note that FiT3D reports results, concatenating the refined features with DINOv2 features. We report results using both plain features only and the concatenation. We also use rendered 2D segmentations of our S4C + STEGO baseline for 2D validation. For multi-view feature consistency, we utilize DINO [11], DINOv2, and FiT3D [115] features as baseline. Implementation details. Our encoder-decoder uses DINO-B/8 [11] backbone and dense prediction decoder [83]. The MLP decoder Ï• entails two layers with 128 hidden features. As rendering features is expensive, Ï• predicts 64 features. We employ another MLP to upproject again to the full dimensionality = 768. If not stated differently, our target features are obtained from DINO-B/8 [11]. We train using batch size of 4 and extract 32 patches of size 8 8 from each image to compute LSceneDINO. Volume rendering samples each ray at = 32 uniformly spaced points in inverse depth within [3 m, 80 m]. We train for 100 steps using Adam [51] with base learning rate of 104. Training takes ca. 2 days on single V100 GPU. We distill using batch size of 4, 5 center points, feature batch of size 576, and cluster with = 19. For kNN sampling, we use = 4. The feature buffer holds 256 feature batches. Refer to the supplement for more details. 4.1. 3D semantic scene completion We assess the unsupervised SSC and geometric accuracy of SceneDINO with our 3D feature distillation approach In particular, Tab. 1 comon SSCBench-KITTI-360. Input Image SceneDINO (Ours) Feature Field SSC Prediction S4C + STEGO SSC Prediction Ground Truth Road Sidewalk Building Fence Pole Other Object Traffic Sign Vegetation Terrain Person Car Other Vehicle Motorcycle Bicycle Figure 5. Qualitative SSC comparison on KITTI-360. We show the input image, SceneDINOs feature field using the first three principal components and SSC prediction, the prediction of our baseline S4C + STEGO, and the ground truth. We only visualize surface voxels. Qualitative results show the expressiveness of our feature field and SceneDINOs capabilities to accurately reconstruct and label scene. Table 2. 2D unsupervised semantic segmentation results on KITTI-360. Comparing SceneDINO to existing 2D methods and our S4C + STEGO 3D baseline, using Accuracy and mean IoU (in %, ) on the SSCBench-KITTI-360 test split. denotes the use of plain FiT3D features. denotes training on ImageNet and COCO."
        },
        {
            "title": "Method",
            "content": "U2Seg [77] STEGO [32] STEGO [32] STEGO [32] STEGO [32] S4C [37] + STEGO [32] SceneDINO (Ours)"
        },
        {
            "title": "Features",
            "content": "DINO [11] DINOv2 [79] FiT3D [115] FiT3D [115] DINO [11] DINO [11]"
        },
        {
            "title": "Acc",
            "content": "mIoU 72.89 73.32 64.54 54.19 57.25 65.16 77.74 23.43 23.57 24.82 22.29 18.95 21.67 25.81 pares SceneDINO against our unsupervised SSC baseline S4C [37] + STEGO [32]. SceneDINO achieves (semantic) mIoU of 8.0 % for the range of 51.2 m, significantly improving over our unsupervised baseline (6.6 %). This demonstrates that SceneDINO effectively lifts DINO features into 3D. In terms of geometric accuracy, SceneDINO moderately improves over S4C + STEGO. Despite being fully unsupervised, SceneDINO comes within 2.2 % points mIoU of the 2D-supervised S4C. Fig. 5 provides qualitative samples on SSCBenchKITTI-360. SceneDINOs unsupervised SSC predictions are less noisy and capture finely resolved semantics compared to S4C + STEGO. Compared to the ground truth, we observe, SceneDINO captures both the geometry and general semantics of the scene. We visualize SceneDINOs feature field (before distillation) using the first three principal components. In PCA space, we observe that our feature field captures semantically meaningful regions. 4.2. 2D semantic segmentation Table 2 compares the semantic predictions of SceneDINO to recent 2D approaches and our 3D baseline. We obtain 2D semantic segmentations from SceneDINO and our S4C + STEGO baseline using semantic rendering [37]. SceneDINO with our 3D distillation approach outperforms STEGO with DINO features, an established 2D unsuperTable 3. 2D unsupervised semantic segmentation domain generalization results. Comparing SceneDINO to existing 2D unsupervised semantic segmentation methods and S4C + STEGO 3D baseline, using Accuracy and mean IoU (in %, ). We train on KITTI-360 images and report domain generalization results on Cityscapes and BDD-100K val. denotes plain FiT3D features."
        },
        {
            "title": "Cityscapes",
            "content": "BDD-100K"
        },
        {
            "title": "Acc mIoU Acc mIoU",
            "content": "U2Seg [77] STEGO [32] STEGO [32] STEGO [32] STEGO [32] S4C [37] + STEGO [32] DINO [11] DINO [11] SceneDINO (Ours) 75.57 18.62 69.00 17.99 71.21 19.42 75.02 21.41 DINO [11] DINOv2 [79] 68.41 19.73 65.72 21.77 FiT3D [115] 66.94 21.01 65.96 20.99 FiT3D [115] 64.76 17.17 60.83 19.09 54.80 14.04 44.98 11.62 73.17 22.81 72.28 22.09 vised semantic segmentation approach. In particular, the mIoU of SceneDINO is 2.24 % points higher than for STEGO (w/ DINO). Utilizing 3D refined features from FiT3D deteriorates the baseline relative to DINO, indicating that the FiT3D refinement reduces feature expressiveness. Notably, our unsupervised 3D baseline S4C + STEGO transfers significantly worse to 2D than SceneDINO. We also validate SceneDINO, trained on KITTI-360, on Cityscapes and BDD10K, demonstrating domain generalization. The results are reported in Tab. 3. SceneDINO outperforms all baselines in mIoU on both datasets while only falling short in Acc. S4C + STEGO falls short in generalization. We suspect this poor generalization is caused by the fact that S4C does not rely on general SSL features in the final model, while our feature field generalizes. 4.3. Multi-view feature consistency We analyze the multi-view consistency of our feature field against existing 2D SSL features in Tab. 4. We report the results of SceneDINO trained on KITTI-360 and RealEstate10K. SceneDINO trained using DINO features exhibits significant improvements in multi-view feature consistency over standard DINO features. We also train SceneDINO using target features from DINOv2 [79]. Compared to standard DINOv2 and FiT3D fea7 Table 4. Multi-view consistency results. Comparing multi-view consistency of SceneDINO to existing 2D SSL features, using L1 distance (), L2 distance (), and cosine similarity () on KITTI360 and RealEstate10K. We compare DINO (top) and DINOv2based (bottom) features. denotes plain FiT3D features. Table 6. Feature distillation analysis. We analyze the effectiveness of distilling SceneDINOs features, the kNN-correlation loss, our neighborhood sampling, and our 3D sampling approach over standard 5-crop sampling. We report the mean IoU (in %, ) using range of 51.2 on SSCBench-KITTI-360 test. KITTI-360 RealEstate10K mIoU mIoU Configuration"
        },
        {
            "title": "Method",
            "content": "L1 DINO [11] SceneDINO (w/ DINO) L2 Cos-Sim L1 0.70 0.93 16.06 0.74 6.45 0.33 14.41 0.66 5.87 0.28 L2 Cos-Sim 15.83 0.73 DINOv2 [79] 22.86 0.81 FiT3D [115] FiT3D [115] 7.02 0.33 SceneDINO (w/ DINOv2) 5.24 0.24 0.70 0.82 0.93 0.96 14.20 0.66 19.88 0.72 5.67 0.27 4.87 0.22 0.75 0.95 0.75 0.85 0.95 0.97 Table 5. SceneDINO analysis. We analyze the role of decomposing positional encodings, the choice of downsampling features during training, the effectiveness of the feature smoothness loss, the effect of estimated camera poses, and the choice of target features. We report the mean IoU (in %, ) using range of 51.2 on SSCBench-KITTI-360 test. mIoU reports the absolute difference in % points to our standard model with DINO target features. mIoU mIoU Configuration -1.18 -1.17 -0.74 -0."
        },
        {
            "title": "6.82 No downsampler (bilinear up. + aug.)\n6.83 No feature smoothness loss (Î»fs = 0)\n7.26 No pos. enc. decomposition\n7.88 w/ estimated ORB-SLAM3 poses\n8.00 Full framework (SceneDINO)\n+1.08 9.08 DINOv2 target features (vs. DINO)",
            "content": "tures, SceneDINOs feature field yields significantly better multi-view consistency. Notably, compared against plain 3D refined features of FiT3D, SceneDINO shows better multi-view consistency on both datasets and all metrics while also offering more expressiveness (cf . Tab. 2). 4.4. Analyzing SceneDINO To understand what core components contribute to obtaining an expressive feature field of SceneDINO, we omit or replace individual components and report the results in Tab. 5. Replacing the downsampling approach with bilinear upsampling and multi-crop augmentations, similar to [1], to obtain high-resolution target features leads decrease SSC mIoU by 1.18 %. Omitting the feature smoothness loss leads to similar mIoU drop. Abolishing the constant decomposition of positional encodings leads to mIoU drop of 0.74 %. Training using unsupervised camera poses estimated by ORB-SLAM3 [7] results in an insignificant mIoU drop of only 0.12 %, over using KITTI-360 poses. Going from DINO to DINOv2 target features leads to an increased mIoU of 1.08 %, demonstrating, SceneDINO can benefit from more expressive 2D target features. In Tab. 6, we analyze our 3D distillation. Performing no distillation at all, just clustering our features, decreases mIoU by 1.61 %. Omitting the kNN-correlation loss leads to mIoU drop of 1.35 %. Distilling only with center points, 8 -1.61 -1.35 -0. -0."
        },
        {
            "title": "6.39 No distillation\n6.65 No kNN-correlation loss (Î»kNN = 0)\n7.03 No neighborhood sampling (cf . Fig. 4)\n7.53 5-crop sampling [32] (instead 3D sampling)",
            "content": "8.00 Full framework (SceneDINO) Table 7. Probing analysis. We analyze linear and unsupervised probing of our distilled SceneDINO features on SSCBenchKITTI-360 test using mean IoU (in %, ). For reference, we also report S4C (2D supervised). Linear probing uses 2D annotations."
        },
        {
            "title": "Linear",
            "content": "S4C (full training) DINO [11] DINOv2 [79] DINO [11] DINOv2 [79] n/a mIoU 12.8 25.6 51.2 10.76 13.76 13.63 15.85 16.94 10.01 11.78 12.07 13.70 13.94 8.00 9.08 9.34 10.57 10.19 i.e., not performing neighborhood sampling (cf . Fig. 4), reduces mIoU by 0.97 %. Using 5-crop feature sampling [32], instead of our proposed 3D sampling, leads to reduced mIoU of 0.47 %. This demonstrates the effectiveness of performing distillation in 3D using our novel approach. While focusing on unsupervised SSC, we can also linearly probe our distilled feature field (cf . Tab. 7). In particular, we train SceneDINO using different target features (DINO [11] and DINOv2 [11]), perform distillation, and probe the resulting distilled features. Using linear probing, i.e., training single linear layer using 2D semantic labels, leads to consistent mIoU increase over unsupervised probing. SceneDINO trained using DINOv2 target features even closes the gap to S4C, trained using 2D ground-truth semantic labels. We even surpass 2D supervised S4C slightly on the full range (51.2 m), suggesting the effectiveness of SceneDINO also for weakly-supervised tasks. 5. Conclusion to our knowledge, We presented SceneDINO, the first approach for unsupervised semantic scene completion. Trained using multi-view images and 2D DINO features without human supervision, SceneDINO is able to predict an expressive 3D feature field using single input image during inference. Our novel 3D distillation approach yields state-of-the-art results in unsupervised SSC. While we focus on unsupervised SSC, our multi-view feature consistency, linear probing, and domain generalization results highlight the potential of SceneDINO as strong foundation for various 3D scene-understanding tasks. Acknowledgments. This project was partially supported by the European Research Council (ERC) Advanced Grant SIMULACRON, DFG project CR 250/26-1 4D-YouTube, and GNI Project AICC. This project has also received funding from the ERC under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 866008). Additionally, this work has further been co-funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center [LOEWE/1/12/519/03/05.001(0016)/72] and by the Excellence Cluster EXC3066 The Adaptive Mind. Christoph Reich is supported by the Konrad Zuse School of Excellence in Learning and Intelligent Systems (ELIZA) through the DAAD programme Konrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the Federal Ministry of Education and Research. Christian Rupprecht is supported by an Amazon Research Award. Finally, we acknowledge the support of the European Laboratory for Learning and Intelligent Systems (ELLIS) and thank Mateo de Mayo as well as Igor CviË‡sic for help with estimating camera poses."
        },
        {
            "title": "References",
            "content": "[1] Nikita Araslanov and Stefan Roth. Self-supervised augmentation consistency for adapting semantic segmentation. In CVPR, pages 1538415394, 2021. 8 [2] Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In ICLR, 2020. 2 [3] Philip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing mutual information across views. In NeurIPS*2019, pages 1550915519. 2 [4] Adrien Bardes, Jean Ponce, and Yann LeCun. VICRegL: Self-supervised learning of local visual features. In NeurIPS*2022, pages 87998810. 2 [5] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization for self-supervised learning. In ICLR, 2022. 2 [6] Yingjie Cai, Xuesong Chen, Chao Zhang, Kwan-Yee Lin, Xiaogang Wang, and Hongsheng Li. Semantic scene completion via integrating instances and scene in-the-loop. In CVPR, pages 324333, 2021. 2 [7] Carlos Campos, Richard Elvira, Juan J. Gomez RodrÄ±guez, Jose M. M. Montiel, and Juan Tardos. ORB-SLAM3: An accurate open-source library for visual, visual-inertial and multi-map SLAM. IEEE Trans. Robot., 37(6):18741890, 2021. 4, 8, vi [8] Anh-Quan Cao and Raoul de Charette. Monoscene: Monocular 3D semantic scene completion. In CVPR, pages 39813991, 2022. 2 [9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS*2020, pages 99129924. 2 [10] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In ECCV, pages 132149, 2018. 9 [11] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 96509660, 2021. 1, 2, 3, 4, 6, 7, 8, vi [12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv:2003.04297 [cs.CV], 2020. 2 [13] Xiaokang Chen, Kwan-Yee Lin, Chen Qian, Gang Zeng, and Hongsheng Li. 3d sketch-aware semantic scene completion via semi-supervised structure prior. In CVPR, pages 41924201, 2020. 2 [14] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In CVPR, pages 96409649, 2021. 2 [15] Yuedong Chen, Chuanxia Zheng, Haofei Xu, Bohan Zhuang, Andrea Vedaldi, Tat-Jen Cham, and Jianfei Cai. MVSplat360: Feed-forward 360 scene synthesis from sparse views. In NeurIPS*2024, pages 107064107086. 2 [16] Ran Cheng, Christopher Agia, Yuan Ren, Xinhai Li, and Bingbing Liu. S3CNet: sparse semantic scene completion network for LiDAR point clouds. In CoRL, pages 21482161, 2020. 2 [17] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. PiCIE: Unsupervised semantic segmentation usIn CVPR, ing invariance and equivariance in clustering. pages 1679416804, 2021. 2, 6, Ozgun icek, Ahmed Abdulkadir, Soeren Lienkamp, Thomas Brox, and Olaf Ronneberger. 3D U-Net: Learning dense volumetric segmentation from sparse annotation. In MICCAI, pages 424432, 2016. [18] [19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR, pages 32133223, 2016. 6, [20] Timothee Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Cluster and predict latents patches for improved masked image modeling. arXiv:2502.08769 [cs.CV], 2025. 2 [21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In CVPR, pages 248255, 2009. 6 [22] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, pages 14221430, 2015. 2 [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 1616 words: Transformers for image recognition at scale. In ICLR, 2021. [24] Linus Ericsson, Henry Gouk, Chen Change Loy, and Timothy M. Hospedales. Self-supervised representation learning: Introduction, advances, and challenges. IEEE Trans. Signal Process., 39(3):4262, 2022. 2 [25] Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman. FeatUp: model-agnostic framework for features at any resolution. In ICLR, 2024. 4 [26] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robot. Res., 32(11):12311237, 2013. 1 [27] Clement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with leftright consistency. In CVPR, pages 270279, 2017. 4 [28] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, et al. Bootstrap your own latent: new approach to self-supervised learning. In NeurIPS*2020, pages 2127121284. 2 [29] Agrim Gupta, Jiajun Wu, Jia Deng, and Li Fei-Fei. Siamese In NeurIPS*2023, pages 40676 Masked Autoencoders. 40693. 2 [30] Huy Ha and Shuran Song. Semantic abstraction: Openworld 3D scene understanding from 2D vision-language models. In CoRL, pages 643653, 2023. 2 [31] Oliver Hahn, Nikita Araslanov, Simone Schaub-Meyer, and Stefan Roth. Boosting unsupervised semantic segmentation with principal mask proposals. Trans. Mach. Learn. Res., 2024. 3, 6, i, iv [32] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In ICLR, 2022. 1, 2, 3, 4, 5, 6, 7, 8, i, iv [33] Keonhee Han, Dominik Muhle, Felix Wimbauer, and Daniel Cremers. Boosting self-supervision for single-view In CVPR, scene completion via knowledge distillation. pages 98379847, 2024. 1, 2 [34] Xian-Feng Han, Hamid Laga, and Mohammed Bennamoun. Image-based 3D object reconstruction: State-ofIEEE Trans. the-art and trends in the deep learning era. Pattern Anal. Mach. Intell., 43(5):15781604, 2019. 2 [35] Robert Harb and Patrick Knobelreiter. InfoSeg: Unsupervised semantic image segmentation with mutual information maximization. In GCPR, pages 1832, 2021. 2 [36] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge University Press, 2003. [37] Adrian Hayler, Felix Wimbauer, Dominik Muhle, Christian Rupprecht, and Daniel Cremers. S4C: Self-supervised semantic scene completion with neural fields. In 3DV, pages 409420, 2024. 1, 2, 6, 7, i, iv, vi [38] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 97299738, 2020. 2 [39] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, pages 1600016009, 2022. 2 [40] Olivier Henaff. Data-efficient image pecognition with conIn ICML, pages 41824192, trastive predictive coding. 2020. 2 [41] R. Devon Hjelm, Alex Fedorov, Samuel LavoieMarchildon, Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization. In ICLR, 2019. [42] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3D v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Trans. Pattern Anal. Mach. Intell., 46(12):1057910596, 2024. 2 [43] Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, and Francis Engelmann. Segment3D: Learning fine-grained class-agnostic 3D segmentation without manual labels. In ECCV, pages 278295, 2024. 2 [44] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective view for vision-based 3D semantic occupancy prediction. In CVPR, pages 9223 9232, 2023. 2 [45] Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, and Jiwen Lu. SelfOcc: Self-supervised vision-based 3D occupancy prediction. In CVPR, pages 1994619956, 2024. 1, 2 [46] Joel Janai, Fatma Guney, Aseem Behl, and Andreas Geiger. Computer vision for autonomous vehicles: Problems, datasets and state of the art. Found. Trends Comput. Graph. Vis., 12(13):1308, 2020. 1 [47] Xu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image classification and segmentation. In ICCV, pages 98659874, 2019. 2 [48] Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, and Xinggang GaussTR: Foundation model-aligned gaussian Wang. transformer for self-supervised 3D spatial understanding. arXiv:2412.13193 [cs.CV], 2024. [49] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. LERF: Language embedded radiance fields. In ICCV, pages 1972919739, 2023. 2 [50] Chanyoung Kim, Woojung Han, Dayun Ju, and Seong Jae Hwang. EAGLE: Eigen aggregation learning for objectIn CVPR, centric unsupervised semantic segmentation. pages 35233533, 2024. 3, 6, [51] Diederik P. Kingma and Jimmy Lei Ba. Adam: method for stochastic optimization. In ICLR, 2015. 6 [52] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, In ICCV, pages and Ross Girshick. Segment Anything. 40154026, 2023. 1, 2 [53] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for editing via feature field distillation. In NeurIPS*2022, pages 2331123330. 2 [54] Alexander Koenig, Maximilian Schambach, and Johannes Otterbach. Uncovering the inner workings of STEGO for safe unsupervised semantic segmentation. In CVPRW, pages 37893798, 2023. 10 [55] Philipp Krahenbuhl and Vladlen Koltun. Efficient inference in fully connected CRFs with Gaussian edge potentials. In NIPS*2011, pages 109117. 6 [56] Harold W. Kuhn. The hungarian method for the assignment problem. Nav. Res. Logist. Q., 2:8397, 1955. 6, [57] Jie Li, Yu Liu, Dong Gong, Qinfeng Shi, Xia Yuan, Chunxia Zhao, and Ian D. Reid. RGBD based dimensional decomposition residual network for 3D semantic scene completion. In CVPR, pages 76937702, 2019. 2 [58] Jie Li, Kai Han, Peng Wang, Yu Liu, and Xia Yuan. Anisotropic convolutional networks for 3D semantic scene completion. In CVPR, pages 33483356, 2020. [59] Jie Li, Yu Liu, Xia Yuan, Chunxia Zhao, Roland Siegwart, Ian Reid, and Cesar Cadena. Depth based semantic scene completion with position importance aware loss. IEEE Robotics Autom. Lett., 5(1):219226, 2020. 2 [60] Junnan Li, Pan Zhou, Caiming Xiong, and Steven Hoi. Prototypical contrastive learning of unsupervised representations. In ICLR, 2021. [61] Pengfei Li, Yongliang Shi, Tianyu Liu, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang. Semi-supervised implicit scene completion from sparse lidar. arXiv:2111.14798 [cs.CV], 2021. 2 [62] Yiming Li, Zhiding Yu, Christopher B. Choy, Chaowei Xiao, Jose M. Alvarez, Sanja Fidler, Chen Feng, and Anima Anandkumar. VoxFormer: Sparse voxel transformer for camera-based 3D semantic scene completion. In CVPR, pages 90879098, 2023. 2, iii, iv [63] Yiming Li, Sihang Li, Xinhao Liu, Moonjun Gong, Kenan Li, Nuo Chen, Zijun Wang, Zhiheng Li, Tao Jiang, Fisher Yu, Yue Wang, Hang Zhao, Zhiding Yu, and Chen Feng. SSCBench: large-scale 3D semantic scene completion benchmark for autonomous driving. In IROS, pages 13333 13340, 2024. 1, 2, 6, i, iii [64] Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, and Jose M. Alvarez. FB-OCC: 3D occupancy prediction based on forward-backward view transformation. arXiv:2307.01492 [cs.CV], 2023. 2 [65] Yiyi Liao, Jun Xie, and Andreas Geiger. KITTI-360: novel dataset and benchmarks for urban scene understanding in 2D and 3D. IEEE Trans. Pattern Anal. Mach. Intell., 45(3):32923310, 2023. 1, 5, i, [66] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, pages 740755, 2024. [67] Shice Liu, Yu Hu, Yiming Zeng, Qiankun Tang, Beibei Jin, Yinhe Han, and Xiaowei Li. See and think: Disentangling semantic scene completion. In NeurIPS*2018, pages 261 272. 2 [68] Stuart Lloyd. Least squares quantization in PCM. Trans. Inf. Theory, 28(2):129137, 1982. 4,"
        },
        {
            "title": "IEEE",
            "content": "[69] Zhiliang Ma and Shilong Liu. review of 3D reconstruction techniques in civil engineering and their applications. Adv. Eng. Inform., 37:163174, 2018. 1 [70] James MacQueen. Some methods for classification and In Berkeley Symp. analysis of multivariate observations. on Math. Statist. and Prob., pages 281298, 1967. 4, 5 [71] Nelson Max. Optical models for direct volume rendering. IEEE Trans. Vis. Comput. Graph., 1(2):99108, 1995. 3 [72] Kirill Mazur, Edgar Sucar, and Andrew Davison. Featurerealistic neural fusion for real-time, open set scene understanding. In ICRA, pages 82018207, 2023. [73] Ruihang Miao, Weizhou Liu, Mingrui Chen, Zheng Gong, Weixin Xu, Chen Hu, and Shuchang Zhou. Occdepth: depth-aware method for 3D semantic scene completion. arXiv.2302.13540 [cs.CV], 2023. 1, 2 [74] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99106, 2021. 2, 3 [75] Yue Ming, Xuyang Meng, Chunxiao Fan, and Hui Yu. Deep learning for monocular depth estimation: review. Neurocomputing, 438:1433, 2021. 2 [76] Duy Kien Nguyen, Yanghao Li, Vaibhav Aggarwal, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, and Xinlei Chen. R-MAE: Regions meet masked autoencoders. In ICLR, 2024. 2 [77] Dantong Niu, Xudong Wang, Xinyang Han, Long Lian, Roei Herzig, and Trevor Darrell. Unsupervised universal image segmentation. In CVPR, pages 2274422754, 2024. 6, 7, [78] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, pages 6984, 2016. [79] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. Trans. Mach. Learn. Res., 2024. 2, 3, 6, 7, 8, iv [80] Martin Oswald, Eno Toppe, Claudia Nieuwenhuis, and Daniel Cremers. review of geometry recovery from single image focusing on curved object reconstruction. Innovations for Shape Analysis: Models and Algorithms, pages 343378, 2013. 2 [81] Mingjie Pan, Jiaming Liu, Renrui Zhang, Peixiang Huang, Xiaoqi Li, Hongwei Xie, Bing Wang, Li Liu, and Shanghang Zhang. RenderOcc: Vision-centric 3D occupancy prediction with 2D rendering supervision. In ICRA, pages 1240412411, 2024. 2 [82] Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. OpenScene: 3D scene understanding with open vocabularies. In CVPR, pages 815824, 2023. 2 [83] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn ICCV, pages sion transformers for dense prediction. 1217912188, 2021. [84] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, et al. SAM 2: Segment anything in images and videos. arXiv:2408.00714 [cs.CV], 2024. 1 [85] Stephan R. Richter and Stefan Roth. Matryoshka networks: Predicting 3D geometry via nested shape layers. In CVPR, pages 19361944, 2018. 2 11 [86] Christoph B. Rist, David Emmerichs, Markus Enzweiler, and Dariu M. Gavrila. Semantic scene completion using IEEE Trans. local deep implicit functions on lidar data. Pattern Anal. Mach. Intell., 44(10):72057218, 2022. 2 [87] Luis Roldao, Raoul de Charette, and Anne VerroustBlondet. Lmscnet: Lightweight multiscale 3D semantic completion. In 3DV, pages 111119, 2020. 1, 2 [88] Luis Roldao, Raoul De Charette, and Anne VerroustInt. Blondet. 3D semantic scene completion: survey. J. Comput. Vis., 130(8):19782005, 2022. 1 [89] Johannes L. Schonberger Structure-from-motion revisited. 4113, 2016. 2 and Jan-Michael Frahm. In CVPR, pages 4104 [90] David Sculley. Web-scale k-means clustering. In WWW, page 11771178, 2010. 5 [91] Hyun Seok Seong, WonJun Moon, SuBeen Lee, and Jae-Pil Heo. Leveraging hidden positives for unsupervised semantic segmentation. In CVPR, pages 1954019549, 2023. 3, 6, [92] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur Szlam. CLIP-Fields: Weakly supervised semantic fields for robotic memory. In ICRA Workshop on Pretraining for Robotics, 2023. [93] William Shen, Ge Yang, Alan Yu, Jansen Wong, Leslie Pack Kaelbling, and Phillip Isola. Distilled feature fields enable few-shot language-guided manipulation. In CoRL, pages 405424, 2023. 2 [94] Leon Sick, Dominik Engel, Pedro Hermosilla, and Timo Ropinski. Unsupervised semantic segmentation through depth-guided feature correlation and sampling. In CVPR, pages 36373646, 2024. 3, 6, [95] Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva, and Thomas A. Funkhouser. Semantic In CVPR, scene completion from single depth image. pages 190198, 2017. 1, 2, iii, iv [96] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao F. Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3D: Feed-forward generalisable 3D scene reconstruction from single image. arXiv:2406.04343 [cs.CV], 2024. 2 [97] Ayca Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-vocabulary 3D instance segmentation. In NeurIPS*2023, pages 6836768390. [98] Zachary Teed and Jia Deng. RAFT: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402419, 2020. 6, ii [99] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, and Hongyang Li. Scene as occupancy. In ICCV, pages 83728381, 2023. 2 [100] Nikolaos Tsagkas, Oisin Mac Aodha, and Chris Xiaoxuan Lu. VL-Fields: Towards language-grounded neural implicit spatial representations. In ICRA Workshop on Representations, Abstractions, and Priors for Robot Learning, 2023. 2 12 [101] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea Vedaldi. Neural feature fusion fields: 3D distillation of selfsupervised 2D image representations. In 3DV, pages 443 453, 2022. 2 [102] Shubham Tulsiani, Tinghui Zhou, Alexei A. Efros, and Jitendra Malik. Multi-view supervision for single-view reIn CVPR, construction via differentiable ray consistency. 2017. [103] Xudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object detection and instance segmentation. In CVPR, pages 31243134, 2023. 1, 2 [104] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Trans. Image Process., 13(4): 600612, 2004. 4 [105] Silvan Weder, Hermann Blum, Francis Engelmann, and Marc Pollefeys. LabelMaker: Automatic semantic label generation from RGB-D trajectories. In 3DV, pages 334 343, 2024. 2 [106] Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. Masked feature preIn CVPR, diction for self-supervised visual pre-training. pages 1466814678, 2022. 2 [107] Felix Wimbauer, Nan Yang, Christian Rupprecht, and Daniel Cremers. Behind the scenes: Density fields for single view reconstruction. In CVPR, pages 90769086, 2023. 1, 2, 3, 4, 6, [108] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, Federico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visual computing and beyond. In Comput. Graph. Forum, pages 641676, 2022. [109] Xu Yan, Jiantao Gao, Jie Li, Ruimao Zhang, Zhen Li, Rui Huang, and Shuguang Cui. Sparse single sweep lidar point cloud segmentation via learning contextual shape priors from scene completion. In AAAI, pages 31013109, 2021. 2 [110] Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, and Yue Wang. EmerneRF: Emergent spatial-temporal scene decomposition via self-supervision. In ICLR, 2024. 2 [111] Jiawei Yang, Katie Luo, Jiefeng Li, Congyue Deng, Leonidas Guibas, Dilip Krishnan, Kilian Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. In ECCV, pages 453469, 2024. 4 [112] Zhuoyue Yang, Ju Dai, and Junjun Pan. 3D reconstruction from endoscopy images: survey. Comput. Biol. Med., 175:108546, 2024. 1 [113] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In CVPR, pages 45784587, 2021. 2 [114] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. BDD100K: diverse driving dataset for heteroIn CVPR, pages 26332642, geneous multitask learning. 2020. 6, [115] Yuanwen Yue, Anurag Das, Francis Engelmann, Siyu Tang, and Jan Eric Lenssen. Improving 2D feature representations by 3D-aware fine-tuning. In ECCV, pages 5774, 2024. 2, 6, 7, [116] Pingping Zhang, Wei Liu, Yinjie Lei, Huchuan Lu, and Xiaoyun Yang. Cascaded context pyramid for full-resolution 3D semantic scene completion. In ICCV, pages 78007809, 2019. 2 [117] Yunpeng Zhang, Zheng Zhu, and Dalong Du. OccFormer: Dual-path transformer for vision-based 3D semantic occupancy prediction. In ICCV, pages 94339443, 2023. 2, iii, iv [118] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. ACM Trans. Graph., 37 (4):65, 2018. 6, ii [119] Onur Ozyesil, Vladislav Voroninski, Ronen Basri, and Amit Singer. survey of structure from motion. Acta Numer., 26:305364, 2017. 2 Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion"
        },
        {
            "title": "Supplementary Material",
            "content": "Aleksandar Jevtic* 1 Christoph Reich* 1,2,4,5 Felix Wimbauer 1,4 Oliver Hahn 2 1TU Munich 2TU Darmstadt Christian Rupprecht 3 3University of Oxford Stefan Roth 2,5,6 5ELIZA 4MCML Daniel Cremers 1,4,5 6hessian.AI *equal contribution https://visinf.github.io/scenedino In this appendix, we provide further implementation details, including dataset properties and an overview of SceneDINOs computational complexity (cf . Sec. A). We discuss our multi-view feature consistency evaluation approach (cf . Sec. B). Next, we provide additional qualitative and quantitative results (cf . Sec. C), including failure cases. Finally, we discuss the limitations of SceneDINO and suggest future research directions (cf . Sec. D). Î»rand = 0.67, and brand = 0.87. For the similarity thresholds we use bself = 0.44, bkNN = 0.18, and brand = 0.87. We follow standard practice in 2D unsupervised semantic segmentation [17, 31, 32, 50, 77, 91, 94] by applying Hungarian matching [56] to align our pseudo semantics. For SSC validation, we map down to 15 semantic classes while following existing work [31, 32] for 2D validation and map to 19 semantic classes. A. Reproducibility A.2. Datasets Here, we provide further implementation details, information about the utilized dataset, and computational complexity details to ensure reproducibility. Note that our code is available at https://github.com/tum-vision/ scenedino. A.1. Implementation details We implement SceneDINO in PyTorch [122] and build on the code of BTS [107], STEGO [32], and S4C [37]. Our encoder-decoder (pre-trained DINO-B/8 and randomly initialized dense prediction decoder) produces per-pixel embeddings of dimensionality DE = 256. Based on these embeddings, the two-layer MLP Ï• (hidden dimension 128) predicts 64 features. As rendering features is expensive, requiring multiple forward passes through the MLP, Ï• predicts 64 features. We employ another MLP to up-project again to the full dimensionality = 768, this MLP is learn with SceneDINO and can up-project both 3D features and 2D rendered features. We train for 100 steps with base learning rate of 104, dropping to 105 after 50 steps. We train using batch size of 4, extracting 32 patches of size 8 8 per image. These patches align with the per-patch DINO target features. For our feature field loss formulation (cf . Sec. 3.2), we use the loss weights Î»p = 1, Î»s = 0.001, Î»f = 0.2, Î»fs = 0.25. The MLP head (hidden dimension 768) produces 64 distilled features. We perform distillation for 1000 steps with learning rate of 5 104. We train using batch size of 4, 5 center points, feature batch of size 576, and cluster with = 19. For kNN sampling, we use = 4. The feature buffer holds 256 feature batches. The loss term in Eq. (9) is parameterized with Î»self = 0.08 Î»kNN = 0.43 We provide additional details about the datasets utilized to train and evaluate SceneDINO. KITTI-360 [63, 65] provides video sequences from moving vehicle equipped with forward-facing stereo pair and two side-facing fisheye cameras. In future frames, the fisheye views capture additional geometric and semantic cues of regions occluded in the forward-facing view. For training, we resample the fisheye images into perspective projection. We focus on an area approximately 50 meters ahead of the ego vehicle. Assuming an average velocity of 3050 km/h, side views are randomly sampled 14 seconds into the future. Given frame rate of 10 Hz, this translates to 10 40 time steps. Each training sample consists of eight images: four forward-facing views (including the input image) and four side-facing views. To evaluate our predicted field in SSCBench-KITTI360, we follow the evaluation procedure of S4C [37]. The voxel predictions are evaluated in three different ranges: 12.8 12.8 6.4 m, 25.6 25.6 6.4 m, and the full range 51.2 51.2 6.4 m. For each voxel, multiple evenly distributed points are sampled from the semantic field. The predictions are aggregated per voxel by taking the maximum occupancy and weighting the class predictions accordingly. consists of 500 high-resolution and Cityscapes [19] densely annotated validation images of ego-centric driving scenes. For validation, Cityscapes uses 19-class taxonomy. We leverage the Cityscapes validation samples at resolution of 640 192 for our domain generalization experiments (2D semantic segmentation). BDD-100K [114] is driving scene dataset obtained from urban areas in the US. BDD-100K contains 1000 semantic Input Image SceneDINO (Ours) Feature Field SSC Prediction S4C + STEGO SSC Prediction Ground Truth Road Sidewalk Building Fence Pole Other Object Traffic Sign Vegetation Terrain Person Car Other Vehicle Motorcycle Bicycle Figure 6. 3D qualitative SSC comparison on KITTI-360. We provide additional qualitative results, visualizing the input image, SceneDINOs predicted feature field using the first three principal components, and SSC prediction, the SSC prediction of our baseline S4C+STEGO, and the SSC ground truth. We only visualize surface voxels within the field of view for the sake of clarity. segmentation validation images. The semantic taxonomy follows the 19-class Cityscapes definition. For domain generalization experiments, we utilize BDD-100K images at resolution of 640 192. RealEstate10K [118] is large-scale dataset containing videos of real-world indoor and outdoor scenes, primarily sourced from YouTube. For our experiments, we train with resolution of 512 288. Each training sample consists of three frames, separated by randomly sampled time offset. There are no semantic annotations provided with the dataset. We evaluate the multi-view consistency of our model in this setting. A.3. Computational complexity SceneDINO requires only single GPU for training and inference. In SSCBench (51.2 range), SceneDINO requires 0.760.1 to infer full scene on V100 GPU. The peak VRAM usage during inference is 11 GB. For reference, S4C requires 0.320.13 s. Considering our expressive and high-dimensional feature field and ViT encoder, this is moderate runtime increase. SceneDINO has 100 parameters and is trained for approximately 2 days on single V100 32 GB GPU. All results are reported using automatic mixed precision. B. Multi-View Feature Consistency Evaluation We aim to measure the multi-view consistency of 2D and 3D features. Note, we are not aware of any standardized approach for multi-view feature consistency. To this end, we employ straightforward approach. Given two video frames with temporal stride of 3, forward optical flow is computed using RAFT large [98]. We estimate occlusion by forward-backward consistency [124]; for this, we also compute the backward optical flow. The 2D feature maps obtained using the second frame are backward warped to the features of the first frame. We compute different similarity metrics between the aligned features (L1, L2, and cos-sim). Note that we ignore occlusions. While features from DINO, DINOv2, and FiT3D possess lower resolution than our 2D rendered SceneDINO features, we upscale these features to the image resolution before warping. This evaluation approach utilizes optical flow correspondences and captures both ego motion as well as object motion, offering simple way to evaluate multi-view feature consistency. C. Additional Results Here we provide additional qualitative and quantitative results, extending our results reported in the main paper. Qualitative results. In Fig. 6, we present additional qualitative results of SceneDINO using our 3D feature distillaii Input Image SceneDINO (Ours) Feature Field SSC Prediction Ground Truth Road Sidewalk Building Fence Pole Other Object Traffic Sign Vegetation Terrain Person Car Other Vehicle Motorcycle Bicycle Figure 7. Failure cases of SceneDINO on KITTI-360. We provide failure cases of SceneDINO. We visualize the input image, the predicted feature field using the first three principal components, the SSC prediction, and the SSC ground truth. We observe that our semantic predictions struggle in shaded regions. We only visualize surface voxels within the field of view for the sake of clarity. Input Image SceneDINO DINO Figure 8. 2D SceneDINO features on KITTI-360. We visualize our 2D rendered features and DINO features for given input image (left). We use the first three principal components for feature visualization. Notably, SceneDINOs features (middle) are smoother and capture finer structures than DINO (right). Additionally, SceneDINOs features are high-resolution, while DINO generates features with stride of 8. tion approach on unsupervised semantic scene completion. We also provide visualizations of our unsupervised SSC baseline, S4C + STEGO. Qualitatively, our approach obtains more accurate SSC results and is able to segment faraway objects, such as cars, better than the S4C + STEGO baseline. This observation aligns with the quantitative results presented in Tab. 1 of the main paper. Figure 8 qualitatively analyzes our 2D rendered feaiii tures against DINO. Our features exhibit smooth appearance for uniform regions, such as sidewalks. Additionally, SceneDINOs features better capture fine structures like poles than DINO features. 2D rendered SceneDINO features are also high resolution in contrast to DINO features that exhibit lower resolution. Failure cases. In Fig. 7, we provide failure cases of SceneDINOs SSC predictions. Our predictions exhibit two common failure cases. First, shadowed regions often lead to wrong semantic predictions. Regions affected by significant brightness changes are breaking the brightness consistency, subsequently offering poor learning signal during training, thus impeding accurate predictions of shadowed regions. Second, objects such as cars can entail tail-like artifacts, not accurately capturing the geometry. As our multiview image and feature reconstruction training cannot handle dynamic objects, tail-like artifacts could be caused by the poor learning signal for dynamic objects. Quantitative results. In Tab. 8, we provide additional semantic scene completion results of 3D supervised approaches as an additional point of comparison. In particular, we report official SSCBench [63] results of VoxFormerS [62] and OccFormer [117]. Both utilize 3D supervision, including both semantic and geometric annotations. We also report the results of SSCNet [95]. This approach trains using 3D supervision but utilizes depth image during inference. While SceneDINO achieves state-of-the-art segmentation accuracy in the unsupervised setting, supervised approaches are significantly more accurate. Table 8. SSCBench-KITTI-360 results. Semantic results using mIoU and per class IoU, and geometric results using IoU, Precision, and Recall (all in %, ) on SSCBench-KITTI-360 test using three depth ranges. We extend Tab. 1 and compare SceneDINO against our baseline S4C [37] + STEGO [32], 2D supervised S4C [37], and three 3D supervised approaches (VoxFormer-S [62], OccFormer [117], and SSCNet [95]). Note that SSCNet uses depth as an additional input during inference, while all other approaches use single input image. Method S4C + STEGO SceneDINO (Ours) S4C VoxFormer-S OccFormer SSCNet Supervision Unsupervised 2D supervision 3D supervision 3D sup. + depth input Range 12.8 25.6 51.2 12.8 25.6 51.2 12.8 25.6 51.2 12.8 25.6 51.2 12.8 25.6 51.2 12.8 25.6 51.2 Semantic validation mIoU car bicycle motorcycle truck other-v. person road sidewalk building fence vegetation terrain pole traffic-sign other-obj. 10.53 18.57 0.01 0.00 0.11 0.01 0.01 61.97 18.74 14.75 1.41 15.83 26.49 0.08 0.00 0.05 9.26 14.09 0.01 0.00 0.04 0.05 0.01 52.47 20.95 24.44 0.20 16.58 9.95 0.04 0.00 0.04 6.60 9.22 0.01 0.00 0.02 0.02 0.01 38.15 18.21 17.81 0.11 11.30 4.17 0.04 0.00 0. 10.76 21.24 0.00 0.00 0.00 0.00 0.00 51.10 20.26 12.33 1.91 31.22 23.26 0.05 0.00 0.00 10.01 15.94 0.00 0.00 0.00 0.00 0.00 49.12 22.31 18.27 0.90 25.57 18.02 0.05 0.00 0.00 8.00 11.21 0.00 0.00 0.00 0.00 0.00 39.82 18.97 14.32 0.58 19.85 15.22 0.05 0.00 0.00 16.94 22.58 0.00 0.00 7.51 0.00 0.00 69.38 45.03 26.34 9.70 35.78 35.03 1.23 1.57 0.00 13.94 18.64 0.00 0.00 4.37 0.01 0.00 61.46 37.12 28.48 6.37 28.04 22.88 0.94 0.83 0.00 10.19 11.49 0.00 0.00 2.12 0.06 0.00 48.23 28.45 21.36 3.64 21.43 15.08 0.65 0.36 0. 18.17 29.41 2.73 1.97 6.08 3.71 2.06 66.10 38.00 41.12 8.99 45.68 24.70 8.84 9.15 4.40 15.40 25.08 1.73 1.47 6.63 3.56 2.20 58.58 33.63 38.24 7.43 35.16 18.53 8.16 9.02 3.27 11.91 17.84 1.16 0.89 4.56 2.06 1.63 47.01 27.20 31.18 4.97 28.99 14.69 6.51 6.92 2.43 23.04 40.87 1.94 1.03 22.40 8.48 4.54 73.34 49.76 53.65 10.64 49.91 34.63 12.93 14.25 8.96 18.38 33.10 1.04 0.43 15.21 6.12 3.79 66.53 41.30 44.86 7.85 37.96 24.99 10.25 12.37 6.71 13.81 22.58 0.66 0.26 9.89 3.82 2.77 54.30 31.53 36.42 4.80 31.00 19.51 7.77 8.51 4. 26.64 52.72 0.00 1.41 16.91 1.45 0.36 87.81 67.19 53.93 14.39 56.66 43.47 1.03 1.01 1.20 24.33 45.93 0.00 0.41 14.91 1.00 0.16 85.42 60.34 54.55 10.73 51.77 36.44 1.05 1.22 0.97 19.23 31.89 0.00 0.19 10.78 0.60 0.09 73.82 46.96 44.67 6.42 43.30 27.83 0.62 0.70 0.58 Geometric validation"
        },
        {
            "title": "IoU\nPrecision\nRecall",
            "content": "49.32 54.04 84.95 41.08 46.23 78.69 36.39 41.91 73.43 49.54 53.27 87.61 42.27 46.10 83.59 37.60 41.59 79. 54.64 59.75 86.47 45.57 50.34 82.79 39.35 43.59 80.16 55.45 66.10 77.48 46.36 61.34 65.48 38.76 58.52 53. 58.71 69.47 79.13 47.96 62.68 67.12 40.27 59.70 55.31 74.93 83.65 87.79 66.36 77.85 81.80 55.81 75.41 68. Table 9. SSCBench-KITTI-360 results (DINOv2). Semantic results using mIoU and per class IoU, and geometric results using IoU, Precision, and Recall (all in %, ) on SSCBench-KITTI-360 test using three depth ranges. We compare our baseline S4C + STEGO to SceneDINO, both using DINOv2 features."
        },
        {
            "title": "Method",
            "content": "S4C + STEGO w/ DINOv2 SceneDINO w/ DINOv2 (Ours)"
        },
        {
            "title": "Range",
            "content": "12.8 25.6 51.2 12.8 25.6 51.2 mIoU car bicycle motorcycle truck other-v. person road sidewalk building fence vegetation terrain pole traffic-sign other-obj. IoU Precision Recall Semantic validation 9.27 10.31 0.00 0.00 0.00 0.01 0.00 55.73 24.13 0.41 0.57 11.42 15.96 20.43 0.00 0.01 6.25 5.84 0.00 0.00 0.00 0.01 0.00 35.00 19.43 0.23 0.41 9.24 8.45 15.14 0.01 0."
        },
        {
            "title": "Geometric validation",
            "content": "39.99 47.32 72.06 35.63 42.36 69.14 13.76 18.27 0.00 0.00 0.00 0.00 0.00 68.04 41.63 15.97 0.00 25.37 37.07 0.00 0.00 0.00 48.12 52.95 84.07 11.70 15.66 0.00 0.00 0.00 0.01 0.00 65.81 31.78 0.83 0.89 9.92 33.79 16.84 0.00 0.01 47.51 55.89 76. 11.78 13.83 0.00 0.00 0.00 0.00 0.00 61.35 36.02 20.87 0.00 17.86 26.81 0.00 0.00 0.00 40.35 45.44 78.29 9.08 9.51 0.00 0.00 0.00 0.00 0.00 46.70 27.32 16.81 0.00 14.82 21.06 0.00 0.00 0.00 36.21 40.92 75.89 Tab. 8 provides additional SSC results of our S4C [37] + STEGO [32] baseline and SceneDINO using DINOv2 features [79]. In particular, we train STEGO with DINOv2 features and lift the resulting unsupervised semantic predictions using S4C. For SceneDINO, we use DINOv2 target features and perform distillation and clustering. Training S4C + STEGO using DINOv2 features leads to improvements for close range (12.8 m) over using DINO features Table 10. Class-wise 2D unsupervised semantic segmentation results on KITTI-360. We comparing the class-wise IoU scores (all in %, ) of SceneDINO against STEGO in 2D on the SSCBench-KITTI-360 test split."
        },
        {
            "title": "Method",
            "content": "mIoU road sidewalk building wall fence pole traffic light traffic sign vegetation terrain sky person rider car truck bus train motorcycle bicycle"
        },
        {
            "title": "SceneDINO",
            "content": "23.57 63.81 7.70 65.24 11.94 15.36 11.43 0.00 0.11 73.35 49.31 69.18 0.00 0.05 77.72 2.09 0.02 0.00 0.08 0.00 25.81 77.73 44.48 77.67 3.68 18.13 0.93 0.00 0.00 73.38 41.29 71.72 0.00 0.00 81.31 0.04 0.00 0.00 0.00 0.00 (cf . Tab. 8). For larger ranges (e.g., 51.2 m), S4C + STEGO with DINOv2 features drops in accuracy compared to S4C + STEGO with DINO features. We attribute this drop in accuracy to the coarser feature resolution of DINOv2 (larger ViT patch size). This behavior has also been observed for the task 2D unsupervised semantic segmentation [31]. Note, SceneDINO overcomes the coarse features using learnable downsampler and multi-view training, learning highresolution 3D features. Class-wise semantic results. To further asses the segmentation accuracy of SceneDINO, we report class-wise IoU metric in 3D (cf . Tab. 1, 8, and 9) and 2D (cf . Tab. 10). We iv road sidewalk building wall fence pole traffic light traffic sign vegetation terrain sky person rider car truck bus train motorcycle bicycle road sidewalk building wall fence pole traffic light traffic sign vegetation terrain sky person rider car truck bus train motorcycle bicycle 1 0.8 0.6 0.4 0. 0 1 0.8 0.6 0.4 0. 0 w p e g l t l fi t i fi t t g k i r o p r r r s n l c o (a) SceneDINO a o n g l t i fi t i fi t i e y n e o p r r r b a e r m (b) STEGO y l c i r r w i a s Figure 9. Confusion matrices for 2D unsupervised semantic segmentation on KITTI-360. Rows represent ground-truth class labels (normalized to 1), while columns correspond to predicted class labels. We report results for (a) SceneDINO and (b) STEGO on the SSCBench-KITTI-360 test split. generally observe that SceneDINO performs well in segmenting frequent classes, such as road, building, and sky. Less frequent classes, such as fence and pole, are less well segmented. Classes including very small and fine structures (e.g., pole) are completely missed by SceneDINO. This trend can also be observed for our 3D unsupervised baseline S4C + STEGO and 2D STEGO. We also observe that class-wise metrics strongly correlate between 2D and 3D. Table 11. Linear probing results on SSCBench-KITTI-360. We extend Tab. 7 and report detailed results of SceneDINO using 2D supervised linear probing. Semantic results using mIoU and class IoU, and geometric results using IoU, Precision, and Recall, and (all in %, ) on SSCBench-KITTI-360 test using three depth ranges. Method SceneDINO w/ DINO (Ours) SceneDINO w/ DINOv2 (Ours) Supervision Unsupervised Range 12.8 25.6 51.2 12.8 25.6 51.2 mIoU car bicycle motorcycle truck other-v. person road sidewalk building fence vegetation terrain pole traffic-sign other-obj."
        },
        {
            "title": "IoU\nPrecision\nRecall",
            "content": "Semantic validation 12.07 12.37 0.70 0.00 2.21 0.08 0.00 62.21 25.17 22.82 6.03 26.49 22.43 0.24 0.17 0.07 9.34 8.42 0.47 0.00 1.52 0.06 0.00 49.99 18.85 17.66 3.96 20.89 18.00 0.14 0.09 0.04 Geometric validation 42.26 45.95 84.05 37.61 41.55 79. 15.85 20.35 0.00 0.00 11.48 0.00 0.00 69.92 42.35 23.03 8.82 30.42 30.73 0.46 0.00 0.00 49.77 52.76 89.76 13.63 16.77 1.10 0.00 3.80 0.13 0.01 66.63 29.46 18.64 9.29 32.76 24.80 0.25 0.50 0.26 49.34 52.83 88.21 13.70 15.04 0.00 0.00 7.46 0.00 0.00 63.06 37.13 27.05 6.40 24.96 23.85 0.40 0.00 0.00 43.19 46.46 85. 10.57 10.16 0.00 0.00 4.63 0.00 0.00 50.49 29.13 21.40 4.61 19.75 17.93 0.28 0.00 0.00 38.55 42.11 82.02 Figure 9 reports confusion matrices of SceneDINO and STEGO for 2D semantic segmentation on KITTI-360. Both approaches share similar confusion pattern. We attribute this to the fact that both approaches rely on the feature representation of DINO. In particular, we observe confusion between semantically close classes, such as pole, traffic light, and traffic sign. Interestingly, for the semantic classes person, rider, car, truck, bus, motorcycle, and bicycle, we see strong confusion. We suspect this correlation is potentially caused by the fact that these classes often appear on the road and sidewalk and are rare in KITTI-360. We also provide class-wise SSC results of SceneDINO using 2D supervised linear probing in Tab. 11. Linear probing provides an upper bound for clustering our features, improving the segmentation accuracy for almost all classes. However, rare classes like motorcycle are still not captured using linear probing. This suggests that the DINO feature space fails to express these classes accurately, limiting the segmentation accuracy of SceneDINO. Still, our approach is agnostic to the utilized target features and can potentially profit from better 2D features. Camera pose analysis. Training SceneDINO, requires accurate camera poses. While KITTI-360 offers ground truth camera poses, these poses are obtained using additional cues, including LiDAR data [65]. To adhere to our fully unsupervised setting, we provide results training with unsupervised camera poses, estimated using stereo visual SLAM. In particular, Tab. 5 reports results of SceneDINO Table 12. Camera pose analysis on SSCBench-KITTI-360. We extend the camera pose analysis in Tab. 5 and report detailed results of SceneDINO with unsupervised camera poses estimated by SOFT2 [121] and ORB-SLAM3 [7]. For reference, we also provide results obtained using the KITTI-360 dataset poses. Semantic results using mIoU and class IoU, and geometric results using IoU, Precision, and Recall, and (all in %, ) on SSCBench-KITTI-360 test using three depth ranges. Input Image SceneDINO Method Poses Range mIoU car bicycle motorcycle truck other-v. person road sidewalk building fence vegetation terrain pole traffic-sign other-obj. SceneDINO (Ours) SOFT2 ORB-SLAM3 KITTI-360 12.8 25.6 51.2 12.8 25.6 51.2 12.8 25.6 51.2 Semantic validation 0.04 0.00 0.00 0.01 0.02 10.58 7.72 9.58 18.47 13.98 10.44 0.03 0.03 0.00 0.00 0.00 0.00 0.04 0.02 0.01 0.01 44.48 44.50 36.06 16.55 16.79 14.38 19.40 23.40 18.56 0.68 1.00 32.10 25.65 20.67 25.59 18.11 14.79 0.09 0.11 0.00 0.01 0.03 0.05 0.18 0.00 0.08 1.79 0.06 0.01 0.05 0.08 0.00 10.88 7.88 9.86 9.72 19.37 14.09 0.02 0.03 0.00 0.01 0.01 0.02 0.05 0.06 0.00 0.00 44.74 40.58 31.86 21.45 23.56 19.88 19.19 24.87 20.02 0.91 1.21 32.60 24.91 19.49 23.98 18.41 16.16 0.00 0.00 0.02 0.03 0.03 0. 0.00 0.03 0.08 1.62 10.76 10.01 8.00 21.24 15.94 11.21 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 51.10 49.12 39.82 20.26 22.31 18.97 12.33 18.27 14.32 0.58 0.90 1.91 31.22 25.57 19.85 23.26 18.02 15.22 0.05 0.05 0.05 0.00 0.00 0.00 0.00 0.00 0.00 Geometric validation"
        },
        {
            "title": "IoU\nPrecision\nRecall",
            "content": "49.91 41.85 37.25 54.74 45.66 40.79 84.98 83.40 81.12 45.42 40.21 36.65 54.42 45.54 40.98 73.33 77.46 77.62 49.54 42.27 37.60 53.27 46.10 41.59 87.61 83.59 79.67 trained using unsupervised camera poses estimated by ORB-SLAM3 [7]. Table 12, extends these results and reports detailed SSC results using two different unsupervised stereo visual SLAM approachesSOFT2 [121] and ORBSLAM3 [7]. Using unsupervised and visually estimated poses leads to minor drop in both semantic and geometric SSC validation. While ORB-SLAM3 poses lead to slightly better semantic accuracy than SOFT2 poses, SOFT2 estimated poses result in higher geometric accuracy. Still, both SOFT2 and ORB-SLAM3 provide poses accurate enough for train SceneDINO, reaching similar accuracy to employing KITTI-360 poses. Out-of-domain results. We illustrate on out-of-domain prediction in Fig. 10. While our SceneDINO model is trained on the KITTI-360 dataset, we still obtain plausible features when inferring 2D features for vastly different scenes. The 2D rendered features still show strong correlation with semantically uniform regions, showcasing the generalization of our feature field. D. Limitations and Future Work Target features. Our method builds on DINO [11] to obtain target features. While we learn to lift these features into 3D and improve multi-view feature consistency, we cannot improve the discriminative power of the target features per se. However, SceneDINO can be trained using arbitrary 2D target features and can profit from future advances in SSL vi Figure 10. 2D SceneDINO features on out-of-domain images. We visualize our 2D rendered features (right) given an out-ofdomain image (left) from ADE20K [126]. We use the first three principal components for feature visualization. While not trained on such scenes, SceneDINO still produces plausible feature maps. representation. Note that training SceneDINO requires only 2 days on single GPU and our training transfers seamlessly to different target features (e.g., DINOv2), thus, utilizing SceneDINO differently is straightforward. Dynamic objects. Our loss does not model dynamic objects and relies on static scene assumption. This can potentially cause inaccurate predictions for dynamic classes such as person in our experiments. Recent works in depth estimation have explicitly modeled the probability of areas being dynamic [125] and even their motion within the scene [123], which might be extended to SceneDINO. View sampling and camera poses. For sampling views during training, we rely on the sampling scheme of S4C [37]. This is not directly applicable to other nondriving datasets, where the sampling needs to be tuned. In addition, our approach requires accurate camera poses for each view. We demonstrated that these can be obtained in an unsupervised way for KITTI-360 (cf . Tab. 5 & Tab. 12). However, obtaining unsupervised camera poses in more challenging scenarios and conditions is still challenging [120]. Future work. SceneDINO is only trained using single dataset to be comparable to existing SSC approaches. However, scaling our approach to multiple datasets of more variable scenes could lead to more general feature representations. Ultimately, scaling SceneDINO to internet-scale videos might enable strong zero-shot and cross-domain 3D scene understanding."
        },
        {
            "title": "References",
            "content": "[120] Lucas R. Agostinho, Nuno M. Ricardo, Maria I. Pereira, Pinto Antoine, and Andry M. Pinto. practical survey on visual odometry for autonomous driving in challenging scenarios and conditions. IEEE Access, 10:72182-72205, 2022. vi [121] Igor CviË‡sic, Ivan Markovic, and Ivan Petrovic. SOFT2: Stereo visual odometry for road vehicles based on pointto-epipolar-line metric. IEEE Trans. Robot., 39(1):273-288, 2023. vi [122] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Z. Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS*2019, pages 80248035. [123] Yihong Sun and Bharath Hariharan. Dynamo-Depth: Fixing unsupervised depth estimation for dynamical scenes. In NeurIPS*2023, pages 5498755005. vi [124] Narayanan Sundaram, Thomas Brox, and Kurt Keutzer. Dense point trajectories by GPU-accelerated large displacement optical flow. In ECCV, pages 438451, 2010. ii [125] Sungmin Woo, Wonjoon Lee, Woo Woo Jin, Dogyoon Lee, and Sangyoun Lee. ProDepth: Boosting self-supervised multi-frame monocular depth with probabilistic fusion. In ECCV, pages 201217, 2024. vi [126] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In CVPR, pages 51225130, 2017. vi vii"
        }
    ],
    "affiliations": [
        "ELIZA",
        "MCML",
        "TU Darmstadt",
        "TU Munich",
        "University of Oxford",
        "hessian.AI"
    ]
}