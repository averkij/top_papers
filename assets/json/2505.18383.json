{
    "paper_title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities",
    "authors": [
        "Abdellah El Mekki",
        "Houdaifa Atou",
        "Omer Nacar",
        "Shady Shehata",
        "Muhammad Abdul-Mageed"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to include low-resource languages is a critical research area. Current research directions predominantly rely on synthetic data generated by translating English corpora, which, while demonstrating promising linguistic understanding and translation abilities, often results in models aligned with source language culture. These models frequently fail to represent the cultural heritage and values of local communities. This work proposes a methodology to create both synthetic and retrieval-based pre-training data tailored to a specific community, considering its (i) language, (ii) cultural heritage, and (iii) cultural values. We demonstrate our methodology using Egyptian and Moroccan dialects as testbeds, chosen for their linguistic and cultural richness and current underrepresentation in LLMs. As a proof-of-concept, we develop NileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities, incorporating their language, cultural heritage, and values. Our results on various understanding, translation, and cultural and values alignment benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar size and performs on par with larger models. We share our methods, data, and models with the community to promote the inclusion and coverage of more diverse communities in LLM development."
        },
        {
            "title": "Start",
            "content": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities Abdellah El Mekkiλ Shady Shehataξ,γ Houdaifa Atouδ Muhammad Abdul-Mageedλ,ξ,γ Omer Nacarψ λThe University of British Columbia δUM6P ψPSU ξMBZUAI γInvertible AI {abdellah.elmekki,muhammad.mageed}@ubc.ca 5 2 0 M 3 2 ] . [ 1 3 8 3 8 1 . 5 0 5 2 : r Figure 1: Our proposed framework for data augmentation tailored to low-resource local communities. (a) Workflow for generating educational data in the target language through machine translation. (b) Workflow for generating diverse text genres in the target language by simulating scenarios that incorporate local cultural contexts and persona descriptions. (c) Retrieval process for augmenting local cultural knowledge by parsing web pages representing specific local cultural concepts through web search."
        },
        {
            "title": "Abstract",
            "content": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to include lowresource languages is critical research area. Current research directions predominantly rely on synthetic data generated by translating English corpora, which, while demonstrating promising linguistic understanding and translation abilities, often results in models aligned with source language culture. These models frequently fail to represent the cultural heritage and values of local communities. This work proposes methodology to create both synthetic and retrieval-based pre-training data tailored to specific community, considering its (i) language, (ii) cultural heritage, and (iii) cultural values. We demonstrate our methodology using Egyptian and Moroccan dialects as testbeds, chosen for their linguistic and cultural richness and current underrepresentation in LLMs. As proof-of-concept, we develop NileChat, 3B parameter LLM adapted for Egyptian and Moroccan communities, incorporating their language, cultural heritage, and values. Our results on various understanding, translation, and cultural and values alignment benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar size and performs on par with larger models. We share our methods, data, and models with the community to promote the inclusion and coverage of more diverse communities in LLM development."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have advanced rapidly, enabling remarkable proficiency across many tasks. Yet, this success is unevenly distributed across languages, with substantial performance disparities observed for non-English languages, particularly low-resource languages and dialectal variants (Navigli et al., 2023). primary factor underlying this discrepancy is the limited representation of diverse multilingual data within the foundational pre-training corpora of these mod1https://github.com/UBC-NLP/nilechat. 1 els which favors high-resource languages spoken in regions with high economic influence (Bender, 2011; Joshi et al., 2020). Beyond linguistic limitations, more profound challenge is the inherent risk of cultural encapsulation (Wrenn, 1962) in LLMs. Cultural encapsulation refers to an unconscious tendency to operate within ones own cultural lens, leading to misunderstanding or avoidance of differing perspectives and values. As LLMs are optimized to replicate patterns in their training datapredominantly sourced from specific cultural contexts (e.g., Western, English-speaking)they risk internalizing and propagating these dominant perspectives as the norm (Dwivedi et al., 2023; Tao et al., 2024; Wang et al., 2024; Naous et al., 2024). The significance of cultural context cannot be overstated. As Edward Sapir noted: \"No two languages are ever sufficiently similar to be considered as representing the same social reality. The worlds in which different societies live are distinct worlds, not merely the same world with different labels attached.\" - Sapir (1929) This cultural bias is compounded by fundamental mismatch: LLMs typically process data through language-centric lens, whereas human communities are structured around shared social ties, perspectives, and values (MacQueen et al., 2001). Current LLMs adaptation techniques for new languages or communities (Gurgurov et al., 2024; Joshi et al., 2025) often fall short in bridging this cultural divide, especially for low-resource communities (Naous et al., 2024). For instance, machine translation, while useful for generating synthetic data to boost linguistic coverage (Joshi et al., 2025; Shang et al., 2025; Wang et al., 2025), primarily addresses the linguistic deficit. The translated content often retains the source languages cultural perspective, failing to incorporate authentic local nuances crucial for genuine interaction. Supervised fine-tuning (SFT) on target language data (Gala et al., 2024; Shang et al., 2025) can align models to specific tasks, but small datasets may not reshape deep-seated cultural biases from pre-training (Rystrøm et al., 2025) and can encourage hallucination with new factual data (Gekhman et al., 2024). While continued pre-training with culturally rich data could mitigate these issues, it faces critical bottleneck for low-resource contexts: the scarcity of such high-quality digital texts. This paper addresses the critical need to adapt multilingual LLMs to low-resource language communities by jointly considering their linguistic characteristics and cultural heritage & values. We propose novel pipeline (illustrated in Figure 1) focused on data augmentation for continued pretraining. Our approach combines controlled synthetic data generation (Section 3.1.2) with retrieval (Section 3.1.3) methods. To address linguistic adaptation, we translate English pre-training data into the target local language focusing only on highquality data from the educational domain (Section 3.1.1). Crucially, to imbue cultural relevance, we generate diverse texts reflecting specific cultural heritage concepts (e.g., food, celebrations, proverbs) using local persona descriptions (Section 3.1.2) reflecting the local cultural values. We demonstrate our method on the Moroccan and Egyptian Arabic dialects as low-resource testbeds. We further pre-train multilingual LLM on curated mix of real and synthetic data, evaluating its performance on tasks involving language understanding, translation, and alignment with cultural knowledge and values. Our findings show that the adapted model significantly outperforms baseline and existing models that are even bigger in size on most evaluation tasks. The main contributions of this work are: (i) novel framework for augmenting pre-training corpora tailored to local communities. This framework considers their unique linguistic features, cultural heritage, and values by leveraging teacher LLM. (ii) The public release of new datasets, representing the largest publicly available corpora for Egyptian and Moroccan Arabic dialects. These resources are intended to foster further research in these underresourced languages. (iii) The development and public release of NileChat, robust 3-billion parameter LLM. This model demonstrates proficiency in both Egyptian and Moroccan dialectal Arabic (using Arabic script and Arabizi) while maintaining strong performance in Modern Standard Arabic, French, and English."
        },
        {
            "title": "2 Related Work",
            "content": "Adaptation of LLMs. LLMs, despite general strengths, often require adaptation for specific languages, domains, or cultures (Bang et al., 2023; AlKhamissi et al., 2024; Naous et al., 2024; Song et al., 2025). Adaptation techniques include prompt engineering (Shen et al., 2024), SFT on culturally 2 specific datasets (Huang et al., 2024), and continued pre-training on target-specific data (Fujii et al., 2024; Huang et al., 2024). key challenge, especially for SFT-based cultural adaptation, is the scarcity of comprehensive cultural datasets, hindering alignment with under-represented communities (Ahmad et al., 2024; Shen et al., 2024). Synthetic Data Augmentation for LLMs. To address data limitations, synthetic data augmentation has shown promise in improving LLM performance (Ge et al., 2024; Li et al., 2024; Joshi et al., 2025). Machine-translated data, for instance, can enhance capabilities in new languages (Joshi et al., 2025; Shang et al., 2025), and persona-driven synthetic data generation has also yielded performance gains (Ge et al., 2024) and aided in tasks like assessing LLM political alignment (Bernardelle et al., 2024). However, synthetic data can sometimes degrade performance (Seddik et al., 2024), necessitating best practices for its use (Liu et al., 2024). Arabic LLMs. In Arabic LLM development, models are either trained from scratch (Billah Nagoudi et al., 2023; Sengupta et al., 2023) or adapted from existing ones (Huang et al., 2024; Bari et al., 2025; Team et al., 2025a). common method involves translating English data to Arabic, which, however, can introduce cultural biases from the source language (Sengupta et al., 2023; Naous et al., 2024). Recent work on dialectal Arabic, such as translating instructions into Moroccan dialect for SFT, has improved generation tasks (Shang et al., 2025). Yet, enhanced performance on standard tasks does not guarantee cultural awareness. While models like AceGPT (Huang et al., 2024) and Fanar (Team et al., 2025a) aim for cultural cognizance, our work uniquely focuses on adapting existing LLMs to local community by deeply integrating its specific linguistic features, cultural heritage, and values, building upon these prior advancements."
        },
        {
            "title": "3 Methodology",
            "content": "In this work, we investigate the potential of pretraining data to imbue LLMs with the specific local characteristics of under-represented communities. We conceptualize these characteristics along three primary dimensions (Geertz, 1977; Anderson, 1991; Bourdieu and Thompson, 1991; Higgins and Douglas, 2020; Stanlaw and Adachi, 2025): (i) Language: Encompassing dialectal nuances, idiomatic expressions, and linguistic structures unique to the community. (ii) Cultural Heritage: Reflecting the customs, traditions, social norms, historical context, and common knowledge prevalent within the community. (iii) Cultural Values: Capturing the ethical standpoints, belief systems, and societal priorities that define the community. We refer to these three dimensions as LanguageHeritage-Values dimensions, LHV for short. While we do not posit these as exhaustive of the attributes of given community, we employ them as vehicle to approximate the LLM communication and information needs at local levels. To ground our investigation, we focus on two low-resource varieties of ArabicThe Egyptian Arabic (EGY) and Moroccan Arabic (MOR). These dialects serve as our primary case studies for evaluating the methods proposed herein."
        },
        {
            "title": "3.1 Data Augmentation",
            "content": "The construction of LLMs that can serve specific population fundamentally depends on the availability of representative data. Recognizing the acute scarcity of publicly available pre-training corpora for many low-resource languages, including EGY and MOR, we propose novel data production method encapsulating the LHV dimensions of given country-level population. As depicted in Figure 1, our approach leverages three complementary strategies intended to collectively capture the LHV dimensions: (a) machine translation (MT), (b) controlled-generation and (c) retrieval. We explain these next."
        },
        {
            "title": "3.1.1 MT for Knowledge and Fluency",
            "content": "To ensure linguistic fluency and coherence, we translate structured educational content from English into the target low-resource language using specialized teacher model. Our pipeline preserves original formatting and includes filtering to remove unreliable translations identified by repetitive ngrams. We use educational materials for their topical breadth (including subjects such as education, history, health, medicine, and biology)."
        },
        {
            "title": "3.1.2 Controlled Generation for Cultural\nHeritage and Cultural Values",
            "content": "Linguistic fluency, while foundational capability for LLMs, does not inherently guarantee their awareness of, or alignment with, the culture and values of specific target community (Naous et al., 2024). To bridge this gap, we employ controlled generation. For controlled generation, we use the 3 teacher LLM to generate diverse texts in the target language. These texts are specifically designed to discuss local topics, which are identified from articles sourced from local news websites or the target countrys Wikipedia portal. Furthermore, the generated content is crafted to reflect distinct personas, each defined by profile encompassing specific moral values, demographic characteristics, and socioeconomic attributes. Our approach integrates four key components to achieve this: Local Contextual Information. We ground our synthetic data by incorporating local context drawn from news websites within the target communities. These sources provide relevant contextual information and do not necessarily need to be in the target local language. Core Cultural Heritage Concepts. We integrate key regional cultural elements, such as cuisine, landmarks, and celebrations, by extracting relevant articles from country-specific Wikipedia portals. Linguistic and Cultural Expressions. To authentically capture local idiomatic styles, we collect common expressions, proverbs, idioms, dialogues from TV programs, and local terminology, pairing each with English translations for accuracy. Representative Personas. We develop representative personas reflecting local moral, demographic, and socioeconomic attributes by leveraging data from the World Values Survey (WVS) (EVS/WVS, 2024). Selected survey responses are transformed into textual descriptions, which are further refined by an LLM to create concise and coherent persona profiles (see Figure 2). To produce diverse text genres for pre-training, we combine data points from the four listed components into unified prompt to guide the teacher LLM. This prompt instructs the LLM to generate varied text outputs in the target low-resource language, explicitly integrating the selected personas values, the specified cultural concepts, and provided linguistic cues. Specifically, we focus on generating the following genres: stories, personal essays, blog posts, reviews, and conversations. An example of this process is depicted in Figure 1 (b)."
        },
        {
            "title": "3.1.3 Retrieval for Local Cultural Heritage",
            "content": "This method involves querying search engine using pre-defined list of cultural concepts that span multiple cultural categories. For each concept, we extract the top 20 search results, systematically excluding social media platforms. The textual content from the retrieved web pages is then parsed and extracted using Trafilatura (Barbaresi, 2021). Figure 2: Pipeline for generation of persona descriptions using the WVS."
        },
        {
            "title": "3.2 Model Training",
            "content": "We evaluate our proposed method on Egyptian (EGY) and Moroccan (MOR) Arabic dialects. Despite their large speaker populations, these dialects remain low-resource, underscoring the need for specialized language models. We select Command R+ (Cohere Labs, 2024) (104B) as our teacher model, as it demonstrates reasonable text-generation capabilities in both target dialects. Additionally, Command R+ provides open weights, enabling us to efficiently generate or translate extensive datasets without incurring API costs."
        },
        {
            "title": "3.2.1 Continued Pre-Training",
            "content": "Data. We generate pre-training data for EGY and MOR using the methods outlined in Section 3.1. Our approach involves three main components: (i) MT Data. We employ our teacher model to translate English educational content into both dialects. Specifically, we sample 5.5 million texts from the Fineweb-edu dataset (Penedo et al., 2024) and translate them into EGY and MOR. (ii) Controlled Generation Data. We craft tailored prompts incorporating personas, local cultural contexts, dialectal glosses, expressions, and utterances to instruct the LLM in generating diverse genres of text. For persona descriptions, we generate 1,200 descriptions based on data from Egyptian and Moroccan participants in the WVS. For local news context, we leverage an in-house corpus comprising approximately 1.5 million Egyptian and 800,000 Moroccan news articles, originally published in MSA by local news websites. Additionally, we include 25,000 Egyptian and 49,000 Moroccan Wikipedia articles. For dialectal glosses, expressions, and utterances, we draw from publicly available resources on EGY and MOR proverbs and idiomatic expressions, each accompanied by English explanations. We further 4 augment this with an in-house dataset of 600 dialectal utterances from Egyptian and Moroccan television shows paired with English translations, as well as 4,000 dialect-to-English word pairs for each dialect from the Gatitos dictionary (Jones et al., 2023). (iii) Retrieval Data. For information retrieval, we query the Brave Search API2 using 6,500 cultural concepts from Morocco and 4,500 cultural concepts from Egypt. These concepts represent the ten cultural heritage categories in the set {food, clothes, landmarks, festivals & celebrations, geography, handicrafts, architecture, auna, flora, music}. The generated dataset comprises approximately 5.5 million educational articles for both EGY and MOR. Additionally, for EGY, it includes approximately 300,000 samples for each category of conversions, personal essays, blog posts, reviews, and stories. For MOR, there are approximately 150,000 samples for each of these same categories. These latter categories represent the LHV dimensions (3.1). Table A.3 presents sample of these texts. filtering process using repetitive n-gram filter removed 3.97% of the data. We also conducted dialectness check on the generated data using ALDi (Keleg et al., 2023). The average dialectness scores for the EGY and MOR educational articles are 0.45 and 0.32, respectively. In contrast, for the texts focused on cultural heritage and values, the average dialectness scores are higher, at 0.84 for EGY and 0.72 for MOR. We attribute the lower dialectness levels in the educational articles to the prevalence of scientific terms that often lack direct equivalents in EGY and MOR, and were therefore retained in MSA. We convert 1.5M and 0.5M from this generated data to Arabizi for both EGY and MOR, respectively. For retrieval, we collect 110,000 and 30,000 articles about cultural heritage to both EGY and MOR. Our final pre-training dataset is mixture of our generated and retrieved data, combined with preexisting publicly available data for these dialects, MSA, English, French, Math, and Code. Our objective is to preserve the data distribution of the base models pre-training data to mitigate catastrophic forgetting (Luo et al., 2025). The resulting pre-training dataset comprises 98.57 billion words, and its composition is detailed in Table A.1. Compute. We used cluster of 4A100 80GB GPUs for 1,096 hours to create our augmented pre2https://brave.com/search/api/ training dataset using the listed inputs. Continued Pre-training. Rather than pretraining an LLM from scratch, we continue pretraining Qwen-2.5-3B (Qwen et al., 2025) with our data. We select this model due to its competitive performance and good tokenizer compression ratio on MSA. We continue pretraining the full model (3.1B parameters) for one whole epoch, which took 750 hours on 4A100 80GB GPUs. More details about the base model selection and the training are in Appendix B.1."
        },
        {
            "title": "3.2.2 Supervised Fine-Tuning\nData. To adapt our pre-trained model for instruc-\ntion following, we perform supervised fine-tuning\n(SFT). Due to the scarcity of SFT datasets for\nEGY and MOR, we construct a comprehensive\ntraining set. This process involves several key\nsteps: (i) translation of SmolTalk dataset (Allal\net al., 2025) into MOR, EGY, French, and MSA\nusing the teacher LLM; (ii) generation of synthetic\ndialectal question-answer pairs using our the re-\ntrieved dataset of local Egyptian and Moroccan\ncultural heritage;3 (iii) incorporation of the Darija-\nSFT-Mixture MOR dataset provided by Shang\net al. (2025); and (iv) translation of TULU-V2-mix\ndataset (Ivison et al., 2023) into EGY. Finally, (v)\nthis consolidated SFT dataset is augmented by con-\nverting understanding and generation tasks from\nthe training sets of the ORCA (Elmadany et al.,\n2023) and Dolphin (Nagoudi et al., 2023) bench-\nmarks into instruction-response formats. The final\ncomposition of our instruction dataset is in Table\nA.2.",
            "content": "Fine-Tuning. For model fine-tuning, we follow recent approaches (Ramé et al., 2024; Dang et al., 2024) that leverage model merging techniques to produce models effective across multiple languages or tailored for particular tasks. Specifically, we fully fine-tuned two separate variants of the base modelone specialized for MOR and the other for EGYeach trained on its respective dialectal data in both Arabic script and Arabizi (plus an amount of shared data between the two variants from the other languages; see B.2). We fine-tune each dialect-specific model for two epochs and employ weighted linear averaging (Aakanksha et al., 2024) for merging, dubbing our merged model NileChat. 3This data is initially created in Arabic script, and portion is subsequently converted to Arabizi. 5 More information about our model merging is in B.2 and the prompts used for generating and translating our preand fine-tuning datasets is in E."
        },
        {
            "title": "4.1 Evaluation Tasks",
            "content": "We employ comprehensive evaluation framework to measure the performance of NileChat for EGY and MOR. This framework enables comparison with our baseline and other LLMs across multiple capability dimensions: Understanding, cultural knowledge, translation, and value alignment. Understanding. We evaluate understanding capabilities using MMLU (Hendrycks et al., 2021), HellaSwag (Zellers et al., 2019), and Belebele (Bandarkar et al., 2024) benchmarks, each adapted to both EGY and MOR dialects. For MOR, we directly employ the MMLU and HellaSwag versions provided by Shang et al. (2025). For EGY, we follow the translation pipeline described in Shang et al. (2025), translating the English and MSA MMLU tasks and the English HellaSwag dataset into EGY using our teacher model. For the Belebele benchmark, we utilize the official Moroccan and Egyptian dialect sets. Evaluations are conducted in both zero-shot and 3-shot scenarios, using accuracy as our performance metric. Cultural Knowledge. To assess cultural knowledge specific to Morocco and Egypt, we utilize the publicly available test set from the Palm benchmark (Alwajih et al., 2025), focusing on these two countries. We adopt an LLM-as-Judge methodology (Zheng et al., 2023), employing Gemma-327b (Team et al., 2025b) to rate the correctness of model-generated responses compared to groundtruth answers on scale from 0 to 10. The final evaluation score is calculated as the average correctness across all responses. Translation. We evaluate the translation performance across multiple directions: dialectdialect dialectMSA, (i.e., MoroccanEgyptian), Englishdialect, and Frenchdialect. Our primary benchmark is the Flores-200 dataset (Team et al., 2022), comprising 1, 012 test examples per translation direction. Additionally, we introduce an in-house, human-curated dataset consisting of 300 authentic EGY and MOR utterances transcribed from local television programs then translated to MSA and English. This dataset provides more"
        },
        {
            "title": "EGY MOR EGY MOR EGY MOR EGY MOR",
            "content": "B 7 t L 7 t M Qwen3-1.7B ar-stablelm-2-chat Atlas-Chat-2B Llama-3.2-3B-Instruct gemma-3-4b-it Qwen3-4B Qwen2.5-3B-Instruct NileChat (3B) AceGPT-7B-chat ALLaM-7B-Instruct Qwen2.5-7B-Instruct Qwen3-8B Atlas-Chat-9B gemma-3-12b-it AceGPT-13B-chat jais-13b-chat 28.53 41.56 42.61 40.68 40.79 28.61 43.37 57.56 40.29 60.04 49.65 28.53 55.17 61.17 45.45 49.79 28.53 40.36 44.87 37.54 32.70 28.54 44.43 57.36 37.57 58.72 44.98 28.53 58.84 60.00 40.68 48.10 28.44 34.79 29.66 29.16 34.21 30.28 31.62 37.97 33.27 39.40 34.67 31.76 33.71 38.59 35.06 39. 27.47 33.45 34.74 28.27 31.35 29.04 29.58 39.33 30.47 37.30 32.16 30.32 44.34 35.66 32.40 36.56 22.89 38.89 50.56 45.44 37.33 22.89 51.33 72.67 32.67 69.56 64.22 22.89 70.33 75.78 38.78 64.22 22.89 36.11 55.67 35.89 34.22 22.89 41.44 70.33 32.00 57.78 48.56 22.89 74.11 64.89 36.44 53. 3.61 4.20 3.16 3.21 7.61 4.51 2.86 5.72 5.58 6.78 6.70 5.88 5.24 8.76 6.10 5.66 2.12 3.62 3.42 2.28 5.42 2.71 2.31 5.86 3.93 6.14 4.77 3.96 4.84 7.09 4.83 4.80 Table 1: Zero-shot performance of models on understanding and cultural knowledge evaluations. Metrics are accuracy for MMLU, HellaSwag, and Belebele, and 0-10 correctness score for Palm. Bold values indicate the highest score among models comparable in size to ours (< 7B). Underlined values represent the highest score in the entire column, including larger models. accurate reflection of natural, colloquial language usage compared to Flores-200, which primarily contains Wikipedia-based sentences. We conduct evaluations in both zero-shot and 4-shot settings, reporting results using ChrF++ (Popovic, 2015) and spBLEU scores (Goyal et al., 2022). Value Alignment. To assess alignment with societal values, we adapt WVS questions into multiple-choice format (expressed in the local language). The questions are categorized into 13 dimensions such as Economic Values (EcoV), Ethical Values (EthV), and Happiness and Wellbeing (HW).4 We use the Social Value Alignment (SVA) metric (Lee et al., 2024), which measures alignment using the distribution of survey responses. models alignment score for each question corresponds to the proportion of participants who chose the model-predicted option, averaged across all questions for the final score. Baseline Models. We compare NileChat against set of 17 instruction-tuned LLMs known for their strong capabilities in Arabic, capped at 13B parameters (see full list in Table C.1 and Appendix C.2 for details)."
        },
        {
            "title": "4.2 Results and Discussion",
            "content": "Understanding. As Table 1 shows, NileChat demonstrates SoTA performance on the MMLU, HellaSwag, and Belebele benchmarks for both 4See Appendix C.1 for the full list. 5We also evaluate our translation performance against an NLLB-200s 3.3B variant (Team et al., 2022). 6 EGY and MOR when compared to similar size models. Specifically, NileChat surpasses its baseline model, Qwen2.5-3B-instruct, by significant margin of 10 points across the majority of these tasks. Notably, NileChat also outperforms larger Arabic-focused models such as AceGPT-13B and Jais-13B. Furthermore, it achieves on-par performance with recent leading Arabic LLMs like ALLaM-7B, with performance gap of less than 3 points on most tasks, and even surpasses it on certain benchmarks, including Belebele. Results for 3-shot are presented in Table D.1 and they show similar trend to the zero-shot ones. Cultural Knowledge. As shown in Table 1, our approach significantly enhances cultural knowledge (Palm), enabling NileChat to achieve scores of 5.72 (EGY) and 5.86 (MOR), compared to baseline Qwen2.5-3B-instruct scores of 2.86 and 2.31, respectively. Among similarly sized models, ours achieves the highest performance on MOR and ranks second only to Gemma-3-4B for EGY. Although larger models such as Gemma-3-12B exhibit superior overall scores (EGY: 8.71, MOR: 7.09), NileChat notably surpasses AceGPT-7B and -13B on Moroccan cultural knowledge, despite their despite their claimed alignment with Arabic cultures. Additionally, it outperforms Atlas-chat-2B and -9B, models specifically fine-tuned for Moroccan dialects. These results support our claim that linguistic fluency alonegained through supervised fine-tuning or pre-training on potentially biased, translated datasetsis insufficient for genuine cultural alignment with local communities. Translation. Table 2 summarizes the spBLEU scores from our zero-shot translation. Overall, NileChat achieves the highest average translation quality (spBLEU: 21.32), outperforming all evaluated models, including larger alternatives such as ALLaM-7B (20.60) and NLLB-200-3.3B (18.29). Specifically, on the Flores benchmark, NileChat demonstrates comparable performance to the similarly-sized NLLB-200-3.3B, with only marginal 1-point spBLEU difference aggregated across MOR and EGY. Notably, NileChat surpasses even larger competitors in all translation directions, except when translating into MOR, where its performance matches that of Atlas-Chat-9Ba larger, single-dialect-focused model that is 3X larger. On our in-house, human-curated datasetwhich closely represents authentic speech patterns from local populationsNileChat significantly outperforms all baselines, including NLLB-200-3.3B, in all translation directions for both EGY and MOR. This real-world evaluation emphasizes the effectiveness of our strategy to incorporate local linguistic and cultural elements into synthetic data generation, enriching the pre-training data with diverse dialectal expressions and vocabulary. Detailed results for both zero-shot and 4-shot translation experiments are provided in Table D.2."
        },
        {
            "title": "Model",
            "content": "Flores-200 In-House Data"
        },
        {
            "title": "Average",
            "content": "XX XX XX XX"
        },
        {
            "title": "EGY MOR EGY MOR EGY MOR EGY MOR",
            "content": "Qwen3-1.7B ar-stablelm-2-chat Atlas-Chat-2B Llama-3.2-3B-Instruct gemma-3-4b-it Qwen3-4B NLLB-200-3.3B Qwen2.5-3B-Instruct NileChat (3B) AceGPT-7B-chat ALLaM-7B-Instruct Qwen2.5-7B-Instruct Qwen3-8B Atlas-Chat-9B gemma-3-12b-it AceGPT-13B-chat jais-13b-chat 14.75 14.35 15.20 14.25 9.27 17.93 23.93 15.14 23.60 18.02 23.91 14.41 20.03 18.20 13.01 19.48 8.80 7 t L 7 t M 10.89 7.07 13.40 9.15 5.22 11.64 15.37 11.27 16.41 11.33 15.88 10.23 13.86 16.89 4.89 14.02 4.29 19.51 11.10 21.39 19.28 12.46 20.03 25.84 20.52 25.74 21.11 24.74 19.81 22.56 24.92 19.05 22.81 15. 15.47 9.72 21.11 15.54 10.13 18.90 26.57 17.37 25.56 17.46 23.19 18.95 21.33 26.29 19.54 19.84 17.12 11.41 9.23 5.36 10.67 3.01 13.09 16.77 9.91 22.02 14.73 19.98 10.43 13.38 5.36 7.86 15.54 10.83 4.36 2.92 7.83 3.16 0.60 4.44 7.49 4.19 12.34 4.95 9.16 4.10 4.73 7.68 2.45 5.56 4. 15.63 11.23 14.52 13.61 16.89 20.72 18.90 19.24 26.50 20.10 29.40 20.92 24.14 17.35 24.51 23.51 19.19 6.32 7.73 13.54 4.87 5.25 8.52 11.43 7.83 18.39 7.47 18.51 8.80 9.27 15.23 12.38 9.52 12.47 12.29 9.17 14.05 11.32 7.86 14.41 18.29 13.18 21.32 14.40 20.60 13.46 16.16 16.49 12.96 16.29 11. Table 2: Zero-shot translation performance (spBLEU) on the Flores and in-house datasets. XX EGY and XX MOR denote average over target languages EGY and MOR, respectively. Conversely, EGY XX and MOR XX indicate average over EGY and MOR as source languages. Bold values highlight the top score among models with fewer than 7 billion parameters. Underlined values indicate the highest score overall in each column. Detailed results are in Table D.2. Value Alignment. Figure 3 illustrates the results of value alignment evaluation based on the WVS. NileChat demonstrates substantial improvements over the baseline across most societal-value dimensions for both Moroccan and Egyptian contexts. Specifically, for Morocco, NileChat surpasses the baseline in all dimensions except Religious Values and the Index of Postmaterialism. Similarly, for Egypt, it outperforms the baseline across all dimensions except Political Interest and Political Participation, and the Index of Postmaterialism. These findings indicate that our approachwhere teacher LLM engages in role-playing by generating diverse text genres through personas embodying local community valuessuccessfully steers responses towards culturally aligned positions. In broader comparative analysis against all evaluated models, ours achieves the best results for Morocco across several dimensions and remains competitive in others. For Egypt, NileChat notably excels in Perceptions of Migration, Political Culture and Political Regimes, Happiness and Wellbeing, and 7 Figure 3: Average SVA scores of evaluated models across societal value dimensions for Egypt and Morocco. Perceptions about Science and Technology, though models such as Jais-13B and ALLaM-7B show slightly stronger performance in certain other dimensions. How many pre-training tokens are needed to reach good performance for new language? Figure 4 shows the performance evolution of NileChat during the pre-training phase on Belebele and translation tasks. The charts show that the model starts to get large boost in these tasks during the first 10B tokens and then continues to slightly increase until it becomes steady after around 60B tokens."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced novel methodology for adapting LLMs to specific communities by deeply integrating their unique linguistic characteristics, cultural heritage, and societal values. Our approach leverages teacher model proficient in generating lowresource languages to enable: (i) translation for the incorporation of community-specific language and (ii) controlled generation and retrieval mechanisms for the authentic inclusion of cultural heritage and values. We validated our methodology using the Moroccan and Egyptian Arabic dialects as testbeds by developing NileChat, an LLM covering these two dialects. Comprehensive evaluations on understanding, translation, and cultural alignment Figure 4: Evolution of model performance during pretraining, measured by the number of tokens processed. benchmarks demonstrate that our method significantly enhances the baseline LLMs performance in capturing target language nuances and cultural values. Notably, NileChat also outperform existing Arabic-aware LLMs. Our method offers promising research direction for fostering inclusivity of diverse local communities within LLM development, thereby emphasizing the critical role of such an inclusion in the broader democratization of this technology."
        },
        {
            "title": "Ethics Statement",
            "content": "Teacher Model Dependency for LowResource Languages: Our methods reliance on teacher model proficient in generating even low-resource target languages may not hold for extremely under-resourced languages (e.g., Berber, Malayo-Polynesian varieties) (Team et al., 2022), potentially limiting its applicability in such contexts. Supervised Fine-Tuning Data: SFT phase predominantly utilized translated data due to resource constraints. This reliance on translated, rather than native, data for SFT might impact the models nuanced performance in the target languages and their corresponding cultures. Susceptibility to Hallucination: As 3B parameter model, our LLM is relatively small, rendering it more prone to hallucination and the generation of inaccurate or incomplete information compared to larger architectures (Wei et al., 2022). Computational Cost of Synthetic Data Generation: The process of generating synthetic data is computationally intensive, particularly when employing large teacher models (e.g., Command-R+, 104B parameter model requiring substantial GPU resources: 4x80GB). This challenge is amplified by the autoregressive generation of long documents from extensive input contexts (e.g., articles, persona descriptions, cultural concepts) restricting the scale of this approach for more languages. Absence of Explicit Safety Alignment: The model has not undergone dedicated safety alignment. While trained on curated datasets (Wikipedia, educational, news) largely devoid of toxic content and leveraging safetyaligned teacher LLM, specific safety tuning is acknowledged as important future work. Limited generation of subtle details. While the controlled generation uses multiple sources (WVS, news, Wikipedia, TV scripts), the generated texts are limited in terms of the very subtle cultural nuances, implicit knowledge, humor, or sarcasm that are often not explicitly stated in these source materials (wikipedia and news articles). Our work contributes to the development of inclusive, linguistically, and culturally diverse LLMs capable of serving varied communities. While we generate our pre-training and instruction-tuning data using teacher LLM, this process is critically informed by ground-truth cultural values survey data from the communities of interest and local context to control the generation. This approach aims to imbue our models with specific cultural nuances relevant to these communities. As our evaluations demonstrate, the resulting models exhibit reasonable alignment with the cultural heritage and values of our target communities and can produce fluent text in their respective dialects. Despite these advancements, we have not conducted explicit safety alignment procedures for these models. Consequently, we strongly recommend thorough testing and further safety evaluations before any deployment in real-world scenarios."
        },
        {
            "title": "Acknowledgments",
            "content": "We acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 8952020-1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada,6 and UBC Advanced Research Computing-Sockeye."
        },
        {
            "title": "References",
            "content": "Aakanksha, Arash Ahmadian, Seraphina GoldfarbTarrant, Beyza Ermis, Marzieh Fadaee, and Sara Hooker. 2024. Mix data or merge models? optimizing for diverse multi-task learning. Preprint, arXiv:2410.10801. Ibrahim Ahmad, Shiran Dudy, Resmi Ramachandranpillai, and Kenneth Church. 2024. Are generative language models multicultural? study on Hausa culture and emotions using ChatGPT. In Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP, pages 98106, Bangkok, Thailand. Association for Computational Linguistics. Badr AlKhamissi, Muhammad ElNokrashy, Mai Investigating Alkhamissi, and Mona Diab. 2024. 6https://alliancecan.ca 7https://arc.ubc.ca/ubc-arc-sockeye cultural alignment of large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1240412422, Bangkok, Thailand. Association for Computational Linguistics. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data2025. centric training of small language model. Preprint, arXiv:2502.02737. Fakhraddin Alwajih, Abdellah El Mekki, Samar Mohamed Magdy, Abdelrahim A. Elmadany, Omer Nacar, El Moatez Billah Nagoudi, Reem AbdelSalam, Hanin Atwany, Youssef Nafea, Abdulfattah Mohammed Yahya, Rahaf Alhamouri, Hamzah A. Alsayadi, Hiba Zayed, Sara Shatnawi, Serry Sibaee, Yasir Ech-Chammakhy, Walid Al-Dhabyani, Marwa Mohamed Ali, Imen Jarraya, Ahmed Oumar El-Shangiti, Aisha Alraeesi, Mohammed Anwar Al-Ghrawi, Abdulrahman S. Al-Batati, Elgizouli Mohamed, Noha Taha Elgindi, Muhammed Saeed, Houdaifa Atou, Issam Ait Yahia, Abdelhak Bouayad, Mohammed Machrouh, Amal Makouar, Dania Alkawi, Mukhtar Mohamed, Safaa Taher Abdelfadil, Amine Ziad Ounnoughene, Rouabhia Anfel, Rwaa Assi, Ahmed Sorkatti, Mohamedou Cheikh Tourad, Anis Koubaa, Ismail Berrada, Mustafa Jarrar, Shady Shehata, and Muhammad Abdul-Mageed. 2025. Palm: culturally inclusive and linguistically diverse dataset for arabic llms. Preprint, arXiv:2503.00151. Zaid Alyafeai, Michael Pieler, Hannah Teufel, Jonathan Tow, Marco Bellagente, Duy Phung, Nikhil Pinnaparaju, Reshinth Adithyan, Paulo Rocha, Maksym Zhuravinskyi, and Carlos Riquelme. 2024. Arabic stable lm: Adapting stable lm 2 1.6b to arabic. Preprint, arXiv:2412.04277. Benedict Anderson. 1991. Imagined Communities: Reflections on the Origin and Spread of Nationalism. Verso, London. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2024. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 749775, Bangkok, Thailand. Association for Computational Linguistics. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023. multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the AsiaPacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675718, Nusa Dua, Bali. Association for Computational Linguistics. Adrien Barbaresi. 2021. Trafilatura: web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122131, Online. Association for Computational Linguistics. Saiful Bari, Yazeed Alnumay, Norah A. Alzahrani, Nouf M. Alotaibi, Hisham Abdullah Alyahya, Sultan AlRashed, Faisal Abdulrahman Mirza, Shaykhah Z. Alsubaie, Hassan A. Alahmed, Ghadah Alabduljabbar, Raghad Alkhathran, Yousef Almushayqih, Raneem Alnajim, Salman Alsubaihi, Maryam Al Mansour, Saad Amin Hassan, Dr. Majed Alrubaian, Ali Alammari, Zaki Alawami, Abdulmohsen AlThubaity, Ahmed Abdelali, Jeril Kuriakose, Abdalghani Abujabal, Nora Al-Twairesh, Areeb Alowisheq, and Haidar Khan. 2025. ALLam: Large language models for arabic and english. In The Thirteenth International Conference on Learning Representations. Emily M. Bender. 2011. On achieving and evaluating language-independence in nlp. Linguistic Issues in Language Technology, 6. Pietro Bernardelle, Leon Fröhling, Stefano Civelli, Riccardo Lunardi, Kevin Roitero, and Gianluca Demartini. 2024. arXiv preprint arXiv:2412.14843. El Moatez Billah Nagoudi, Muhammad Abdul-Mageed, AbdelRahim Elmadany, Alcides Inciarte, and Md Tawkat Islam Khondaker. 2023. JASMINE: Arabic GPT models for few-shot learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1672116744, Singapore. Association for Computational Linguistics. P. Bourdieu and J.B. Thompson. 1991. Language and Symbolic Power. Harvard University Press. Cohere Labs. 2024. c4ai-command-r-plus-08-2024. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis Flet-Berliac, Acyr Locatelli, Hangyu Lin, Dwarak Talupuru, Bharat Venkitesh, David Cairuz, Bowen Yang, Tim Chung, Wei-Yin Ko, Sylvie Shang Shi, Amir Shukayev, Sammie Bae, Aleksandra Piktus, Roman Castagné, Felipe Cruz-Salinas, Eddie Kim, Lucas Crawhall-Stein, Adrien Morisot, Sudip Roy, Phil Blunsom, Ivan Zhang, Aidan Gomez, Nick Frosst, Marzieh Fadaee, Beyza Ermis, Ahmet Üstün, and Sara Hooker. 2024. Aya expanse: Combining research breakthroughs for new multilingual frontier. Preprint, arXiv:2412.04261. Ashutosh Dwivedi, Pradhyumna Lavania, and Ashutosh Modi. 2023. EtiCor: Corpus for analyzing LLMs for etiquettes. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 69216931, Singapore. Association for Computational Linguistics. AbdelRahim Elmadany, ElMoatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. ORCA: challenging benchmark for Arabic language understandIn Findings of the Association for Computaing. tional Linguistics: ACL 2023, pages 95599586, Toronto, Canada. Association for Computational Linguistics. EVS/WVS. 2024. Joint evs/wvs 2017-2022 dataset (joint evs/wvs). GESIS, Cologne. ZA7505 Data file Version 5.0.0, https://doi.org/10.4232/1.14320. Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, and Naoaki Okazaki. 2024. Continual pre-training for cross-lingual LLM adaptation: Enhancing japanese language capabilities. In First Conference on Language Modeling. Jay Gala, Thanmay Jayakumar, Jaavid Aktar Husain, Aswanth Kumar M, Mohammed Safi Ur Rahman Khan, Diptesh Kanojia, Ratish Puduppully, Mitesh M. Khapra, Raj Dabre, Rudra Murthy, and Anoop Kunchukuttan. 2024. Airavata: Introducing hindi instruction-tuned llm. Preprint, arXiv:2401.15006. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094. C. Geertz. 1977. The Interpretation Of Cultures. Basic Books. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning LLMs on new knowledge encourage hallucinations? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 77657784, Miami, Florida, USA. Association for Computational Linguistics. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522538. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Daniil Gurgurov, Mareike Hartmann, and Simon Ostermann. 2024. Adapting multilingual LLMs to low-resource languages with knowledge graphs via In Proceedings of the 1st Workshop on adapters. Knowledge Graphs and Large Language Models (KaLLM 2024), pages 6374, Bangkok, Thailand. Association for Computational Linguistics. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. V. Higgins and D. Douglas. 2020. Communities and Cultural Heritage: Global Issues, Local Values. Routledge studies in heritage. Routledge/Taylor & Francis Group. Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Song Dingjie, Zhihong Chen, Mosen Alharthi, Bang An, Juncai He, Ziche Liu, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu. 2024. AceGPT, localizing large language models in Arabic. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 81398163, Mexico City, Mexico. Association for Computational Linguistics. Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. 2023. Camels in changing climate: Enhancing lm adaptation with tulu 2. Preprint, arXiv:2311.10702. Alexander Jones, Isaac Caswell, Orhan Firat, and Ishank Saxena. 2023. GATITOS: Using new multilingual lexicon for low-resource machine translation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 371405, Singapore. Association for Computational Linguistics. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 11 62826293, Online. Association for Computational Linguistics. 1636616393, Bangkok, Thailand. Association for Computational Linguistics. Raviraj Joshi, Kanishk Singla, Anusha Kamath, Raunak Kalani, Rakesh Paul, Utkarsh Vaidya, Sanjay Singh Chauhan, Niranjan Wartikar, and Eileen Long. 2025. Adapting multilingual LLMs to low-resource languages using continued pre-training and synthetic corpus: case study for Hindi LLMs. In Proceedings of the First Workshop on Natural Language Processing for Indo-Aryan and Dravidian Languages, pages 5057, Abu Dhabi. Association for Computational Linguistics. Amr Keleg, Sharon Goldwater, and Walid Magdy. 2023. ALDi: Quantifying the Arabic level of dialectness of text. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1059710611, Singapore. Association for Computational Linguistics. Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won, Hwaran Lee, and Edward Choi. 2024. KorNAT: LLM alignment benchmark for Korean social values and common knowledge. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1117711213, Bangkok, Thailand. Association for Computational Linguistics. Cheng Li, Mengzhuo Chen, Jindong Wang, Sunayana Sitaram, and Xing Xie. 2024. Culturellm: Incorporating cultural differences into large language models. Advances in Neural Information Processing Systems, 37:8479984838. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. 2024. Best practices and lessons learned on synthetic data. arXiv preprint arXiv:2404.07503. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2025. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. Preprint, arXiv:2308.08747. Kathleen M. MacQueen, Eleanor McLellan, David S. Metzger, Susan Kegeles, Ronald P. Strauss, Roseanne Scotti, Lynn Blanchard, and Robert T. Trotter. 2001. What is community? an evidence-based definition for participatory public health. American Journal of Public Health, 91(12):19291938. El Moatez Billah Nagoudi, AbdelRahim Elmadany, Ahmed El-Shangiti, and Muhammad Abdul-Mageed. 2023. Dolphin: challenging and diverse benchmark for Arabic NLG. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14041422, Singapore. Association for Computational Linguistics. Tarek Naous, Michael Ryan, Alan Ritter, and Wei Xu. 2024. Having beer after prayer? measuring cultural bias in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages Roberto Navigli, Simone Conia, and Björn Ross. 2023. Biases in large language models: Origins, inventory, and discussion. J. Data and Information Quality, 15(2). Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. Preprint, arXiv:2406.17557. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Alexandre Ramé, Johan Ferret, Nino Vieillard, Robert Dadashi, Léonard Hussenot, Pierre-Louis Cedoz, Pier Giuseppe Sessa, Sertan Girgin, Arthur Douillard, and Olivier Bachem. 2024. Warp: On the benefits of weight averaged rewarded policies. Preprint, arXiv:2406.16768. Jonathan Rystrøm, Hannah Rose Kirk, and Scott Hale. 2025. Multilingual != multicultural: Evaluating gaps between multilingual capabilities and cultural alignment in llms. Preprint, arXiv:2502.16534. E. Sapir. 1929. The status of linguistics as science. Language, 5(4):207214. Mohamed El Amine Seddik, Suei-Wen Chen, Soufiane Hayou, Pierre Youssef, and Merouane Abdelkader DEBBAH. 2024. How bad is training on synthetic data? statistical analysis of language model collapse. In First Conference on Language Modeling. Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Preslav Nakov, Timothy Baldwin, and Eric Xing. 2023. Jais and jaischat: Arabic-centric foundation and instruction-tuned open generative large language models. Preprint, arXiv:2308.16149. 12 Guokan Shang, Hadi Abdine, Yousef Khoubrane, Amr Mohamed, Yassine Abbahaddou, Sofiane Ennadir, Imane Momayiz, Xuguang Ren, Eric Moulines, Preslav Nakov, Michalis Vazirgiannis, and Eric Xing. 2025. Atlas-chat: Adapting large language models for low-resource Moroccan Arabic dialect. In Proceedings of the First Workshop on Language Models for Low-Resource Languages, pages 930, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Siqi Shen, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Soujanya Poria, and Rada Mihalcea. 2024. Understanding the capabilities and limitations of large language models for cultural commonsense. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 56685680, Mexico City, Mexico. Association for Computational Linguistics. Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe InLi, Rui Yan, and Xiuying Chen. 2025. jecting domain-specific knowledge into large language models: comprehensive survey. Preprint, arXiv:2502.10708. J. Stanlaw and N. Adachi. 2025. Language, Culture, and Society: An Introduction to Linguistic Anthropology. Taylor & Francis. Yan Tao, Olga Viberg, Ryan Baker, and René Kizilcec. 2024. Cultural bias and cultural alignment of large language models. PNAS Nexus, 3(9):pgae346. Fanar Team, Ummar Abbas, Mohammad Shahmeer Ahmad, Firoj Alam, Enes Altinisik, Ehsannedin Asgari, Yazan Boshmaf, Sabri Boughorbel, Sanjay Chawla, Shammur Chowdhury, Fahim Dalvi, Kareem Darwish, Nadir Durrani, Mohamed Elfeky, Ahmed Elmagarmid, Mohamed Eltabakh, Masoomali Fatehkia, Anastasios Fragkopoulos, Maram Hasanain, Majd Hawasly, Musab Husaini, Soon-Gyo Jung, Ji Kim Lucas, Walid Magdy, Safa Messaoud, Abubakr Mohamed, Tasnim Mohiuddin, Basel Mousi, Hamdy Mubarak, Ahmad Musleh, Zan Naeem, Mourad Ouzzani, Dorde Popovic, Amin Sadeghi, Husrev Taha Sencar, Mohammed Shinoy, Omar Sinan, Yifan Zhang, Ahmed Ali, Yassine El Kheir, Xiaosong Ma, and Chaoyi Ruan. 2025a. Fanar: An arabiccentric multimodal generative ai platform. Preprint, arXiv:2501.13944. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, et al. 2025b. Gemma 3 technical report. Preprint, arXiv:2503.19786. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, and Others. 2024. Gemma 2: Improving open language models at practical size. Preprint, arXiv:2408.00118. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. Preprint, arXiv:2207.04672. Shumin Wang, Yuexiang Xie, Bolin Ding, Jinyang Gao, and Yanyong Zhang. 2025. Language adaptation of large language models: An empirical study on LLaMA2. In Proceedings of the 31st International Conference on Computational Linguistics, pages 71957208, Abu Dhabi, UAE. Association for Computational Linguistics. Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael Lyu. 2024. Not all countries celebrate thanksgiving: On the cultural dominance in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 63496384, Bangkok, Thailand. Association for Computational Linguistics. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research. Survey Certification. C. G. Wrenn. 1962. The counselor in changing world. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, 13 Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track."
        },
        {
            "title": "General",
            "content": "Wikipedia MT fineweb-EDU LHV Fineweb"
        },
        {
            "title": "Nature",
            "content": "# of words"
        },
        {
            "title": "Real\nSynthetic\nSynthetic\nReal",
            "content": "128.71M 2.08B 398.89M 430.46M"
        },
        {
            "title": "Arabizi",
            "content": "MT fineweb-EDU (Arabizi) & LHV"
        },
        {
            "title": "Synthetic",
            "content": "206.49M Wikipedia Translated fineweb-EDU LHV Fineweb"
        },
        {
            "title": "Real\nSynthetic\nSynthetic\nReal",
            "content": "1.67M 2.02B 207.41M 1.64B MT fineweb-EDU (Arabizi) & LHV"
        },
        {
            "title": "Synthetic",
            "content": "467.30M"
        },
        {
            "title": "Brave API\nLocal News",
            "content": "Fineweb2 Wikipedia"
        },
        {
            "title": "General General",
            "content": "Fineweb-EDU"
        },
        {
            "title": "General General",
            "content": "Fineweb2 Code & Math Code & Math MathGenie/MathCode-Pile macrocosm-os/code-parrot-github-code"
        },
        {
            "title": "Real",
            "content": "74.67M 346.79M 23.08M 220.16M 28.80B 318.62M 51.57B 9.42B 818.35M Table A.1: Distribution of the final pre-training data mixture by language, nature (synthetic vs. real), and word count per dataset. Bold rows highlight data generated via our proposed augmentation pipeline."
        },
        {
            "title": "B Training details",
            "content": "B.1 Pre-training Motivations for choosing Qwen-2.5-3B as our backbone model. We select Qwen-2.5-3B as our base model to continue pretrain for two primary reasons: its competitive performance on MSA tasks and good tokenizer compression ratio on Arabic dialect texts. At the time of the selection, the Gemma 2 (Team et al., 2024) and Qwen 2.5 base models showed the best performance in MSA. Also, both of their tokenizers have good compression ratio for Arabic text in both standard and dialectal forms. Our analysis shows ratio between 2.7 and 2.8 for Gemma, while Qwen 2.5 has compression ratio between 2.9 and 3.1. Our final choice of Qwen 2.5 was based on its better performance on MSA. Details for continued model pretraining. We continue the pre-training of Qwen-2.5-3B on our curated pre-training dataset. Subsequently, the model is fully fine-tuned for one epoch using sequence length of 4,096. To optimize the learning process, the learning rate is linearly decayed from 5 106 to 5 107. To mitigate overfitting, we apply weight decay of 0.1, and gradient norms are clipped at maximum value of 1.0. The training is performed on cluster of 4A100 80GB GPUs. B.2 Supervised fine-tuning To enhance model robustness and facilitate effective merging, we augmented each dialect-specific dataset with shared multilingual corpus, comprising English SmolTalk, MSA SmolTalk, French SmolTalk, and additional data from the ORCA and Dolphin datasets. Each dialect-specific model was trained for two epochs with sequence length of 4,096 tokens, using learning rate that linearly decayed from 7 106 to 7 107. Table B.1 compares the SFT model with models fine-tuned on Egyptian and Moroccan datasets individually, as well as with our final merged model, NileChat. NileChat performs well on tasks for both EGY and MOR. The MOR-specific model also demonstrates strong performance on both MOR and, to some extent, EGY tasks. In contrast, the EGY-specific model does not perform well on MOR tasks."
        },
        {
            "title": "Source",
            "content": "# of Instructions Darija-SFT-Mixture MOR (Arabic) Atlas-Chat TÜLU-V2-mix EGY (Arabic) Ours (MT)"
        },
        {
            "title": "Cultural instructions",
            "content": "MOR (Arabic) Ours (MT) MOR (Arabizi) Ours (MT) EGY (Arabic) Ours (MT) EGY (Arabizi) Ours (MT) Ours (MT) French Ours (MT) MSA SmolTalk English MSA + dialects Ours (Converted) Ours (Converted) MOR (Arabic) EGY (Arabic) Ours (Synthetic) Ours (Synthetic) 458, 178,109 192,266 93,419 195,260 93,181 99,468 96,933 149,124 460,203 425,703 25,159 107,428 Table A.2: Distribution of the final instruction and response data mixture by language and number of instructions per dataset. Ours refers to datasets we created via machine translation (MT) or by converting existing datasets into an instruction/response format. Table B.1 compares the SFT model with models fine-tuned on Egyptian and Moroccan datasets individually, as well as with our final merged model, NileChat. NileChat performs well on tasks for both EGY and MOR. The MOR-specific model also demonstrates strong performance on both MOR and, to some extent, EGY tasks. In contrast, the EGY-specific model does not perform well on MOR tasks. We relate this observed asymmetry to the linguistic characteristics of the dialects relative to MSA. During the SFT phase, each dialect-specific dataset was augmented with shared multilingual corpus which included MSA data (e.g., MSA SmolTalk, and data from the ORCA and Dolphin datasets). It is plausible that EGY is linguistically closer to MSA compared to the MOR, which is often considered more distant from MSA due to influences such as Berber and French. Consequently, the MOR-tuned model, having been exposed to this shared MSA data, might more effectively leverage this MSA knowledge to generalize to EGY tasks. Conversely, the greater linguistic divergence of the Moroccan dialect from MSA could make it more challenging for the EGY-tuned model to transfer its learning, including the MSA component, to the distinct features of the Moroccan dialect."
        },
        {
            "title": "C Evaluation Setup",
            "content": "C.1 Evaluation Tasks Full list of the 13 categories of WVS questions. Economic Values (EcoV); Ethical Values (EthV); Happiness and Wellbeing (HW); Index of Postmaterialism (IP); Perceptions about Science and Technology (PST); Perceptions of Corruption (PC); Perceptions of Migration (PM); Perceptions of Security (PS); Political Culture and Political Regimes (PCPR); Political Interest and Political Participation (PIPP); Religious Values (RV); Social Capital, Trust, and Organizational Membership (SCTOM); and Social Values, Norms, and Stereotypes (SVNS). C.2 Baselines We evaluate our model NileChat against set of 17 LLMs that are Arabic-aware; some of these 17 models are also aligned to Arabic dialects. These models are from the following model families: ALLaM (Bari et al., 2025), Jais (Sengupta et al., 2023), Atlas-Chat (Shang et al., 2025), ar-stablelm-2-chat (Alyafeai et al., 2024), Gemma-3 (Team et al., 2025b), Qwen-2.5 (Qwen et al., 2025), Qwen3 (Yang et al., 2025) (non-thinking mode), and Llama-3.2 (Grattafiori et al., 2024). The full list of models, including their corresponding size and release date, are presented in Table C.1."
        },
        {
            "title": "E Prompts",
            "content": "The provided figures showcase diverse prompts for language models targeting low-resource languages. Figure 5 translates English educational content into conversational dialectal Arabic, while Figure 6 converts dialectal Arabic script to Arabizi. English instructions are translated to dialectal Arabic using the prompt in Figure 7. For content generation, Figure 8 guides the model to create culturally relevant dialectal Arabic text based on given persona and context. Figure 9 focuses on summarizing detailed persona descriptions concisely. Finally, Figure 10 instructs an LLM to generate practical dialectal Arabic question-answer pairs in JSON format from provided text. Translate the following text from English to Egyptian Arabic . Ensure that all words are in Egyptian Arabic , and do not use any Modern Standard Arabic ( MSA ). Keep the translation casual , conversational , and reflective of how Egyptians would naturally speak in everyday situations . Avoid any formal or classical language structures . Translate only the input paragraph and don ' add anything else in your output . English : { English_text } Figure 5: The translation prompt used with teacher model to convert English educational pre-training data to low-resource target language. The placeholder {English_text} represents the input English text. Write the following Moroccan dialectal Arabic text in Moroccan Arabizi . Ensure that all words are written in Moroccan Arabizi . Keep the text casual , conversational , and reflective of how Moroccans would naturally write in everyday situations using Arabizi . Translate only the content keys in the following JSON , and output json of the same format : { JSON_OBJECT } Figure 6: The prompt used with our teacher LLM to convert dialectal Arabic text written in Arabic script into Arabizi. The placeholder {JSON_OBJECT} represents the input text formatted as JSON object. Translate the following text from English to Moroccan Arabic . Ensure that all words are in Moroccan Arabic , and do not use any Modern Standard Arabic ( MSA ). Keep the translation casual , conversational , and reflective of how Moroccans would naturally speak in everyday situations . Avoid any formal or classical language structures . Translate only the content keys in the following JSON , and output json of the same format : { JSON_OBJECT } Figure 7: The translation prompt used with teacher model to convert SmolTalk and TULU instructions data to low-resource target language. The placeholder {JSON_OBJECT} represents the input text. 17 Act as the following person : { persona_description } Act like you are { person_Name } and write { text_genre } in Egyptian dialect , using colloquial Arabic script as spoken in Egypt and not Modern Standard Arabic ( MSA ). Use this context and use the information provided in it while writing the { text_genre }: { context } Make sure to follow these conditions : 1. Rely on the provided context when writing the { text_genre }. 2. Ensure that the written { text_genre } reflects the cultural background , values , and worldview of { person_Name }. 3. Don ' write the persona ' description . want you to focus only on the provided context when writing while reflecting the perosna ' background . Note : Ensure that all words are in Egyptian Arabic , and do not use any Modern Standard Arabic ( MSA ). Keep the translation casual , conversational , and reflective of how Egyptians would naturally speak in everyday situations . Figure 8: Prompt for generating culturally and values-aware text genres in low-resource languages, given local persona description and local cultural concept. The placeholders {persona_description}, {text_genre}, and {context} represent the persona description, the intended text genre to generate, and the cultural concept text, respectively. have the following persona description , want you to write it in concise manner keeping all the information , the output should be plain text , make sure to include all values , morals , and culture of the persona : { PERSONA_DESCRIPTION } Figure 9: Prompt for generating concise persona descriptions with LLM utilizing comprehensive description of specific persona extracted from the WVS. The placeholder {PERSONA_DESCRIPTION} represents the input persona description."
        },
        {
            "title": "You a r e an e x p e r t",
            "content": "i Moroccan t and g e , h i e t i t s m a d i s l c Moroccan b i c c s , o t ( e d o , . ) t a c , f n u o p e r w t n b i r l h i Moroccan b i c r . Your o s , l a p s n b c t . ## Task c t : 1 ."
        },
        {
            "title": "I w i l l p r o v i d e you w i t h raw t e x t",
            "content": "c e ( e k r p , t l d o , t c n m o , 2 . n r i t t s p e o e r h , c a e . ) t a how do e n , how e n works , how n s d e n s c n p e r where : o c a p a n Moroccan l someone t t a r i i r i 3 . u on making s e 4 . p e t t c n e i r u JSON m o ## t t F a i i : p e a ( i , n t Example : { m _ 1 } h o a / t Example : { m _ 2 } t : a \" why \" s n a , . ) : a \" how \" s n c u p t s : a \" what \" \" how do we e t \" s n Example : { m _ 3 } c c ## p e m u l s : Make p e t e and c a l f r d l Use h i Moroccan a a and r i t t o l Keep t r o s i o o u , a i o r t s and n s where r i c e t a and p , t s s p a c r n o r y u be f e p n t r d ## Language d n : Use h i Moroccan b i c i n r c l Use g e t would u l be spoken , n d common Moroccan l words and r i where u o o g m r p l Moroccan k / t l c i v b r , t i a r c m l and r i a a r t ( t s Moroccans . n t t ) ## p o t : The p ``` n { u be i JSON h \" t t _ p e _ r \" : [ l n r u : \" t t \" : \" p e \" : \" [ c a s c n Moroccan l ] \" , \" [ a d c a r o i Moroccan l ] \" \" t t \" : \" p e \" : \" [ t p t l t t ] \" , \" [ t d i r o ] \" { } , { } ] } ``` ## t t s : 1 . e e 2 . Make h t t f s on a c e 35 c a t s c n p e r s on c e p i . e ( how , r e , common t s , i o , c . ) . d i enough be c a u u . 3 . u e n 4 . The u n JSON must be p y m e and i . 5 . 6 . When l w r p r c r , make e 7 . c e r t c p a e r r c q , t u n Moroccan m . p f a n , t s t l"
        },
        {
            "title": "I f",
            "content": "a n i o r . l some h n r o s i d n a any e t f a n h Moroccan t t r r an empty t . a c o v d h . 8 ."
        },
        {
            "title": "I f",
            "content": "Now , ' r d you h t , and you ' Moroccan l s i d v . r f i t a c i r i e n a i { t } Figure 10: Prompt for generating synthetic cultural dialectal question-answer pairs using teacher model given raw text describing local cultural heritage concept. The placeholder {Text} represents the raw text that is used to generate question-answer pairs from it."
        },
        {
            "title": "F Samples of texts generated from NileChat",
            "content": "19 Table A.3: Examples of culturally aware and dialectally diverse texts from various genres, generated by our teacher model. The model was provided with input representing specific persona, local context, and local linguistic cue following the methodology described in Section 3.1."
        },
        {
            "title": "Flores",
            "content": "In-house MOR EGY ENGEGY ENGMOR ENGMOR ENGEGY NileChat-EGY 64.44 NileChat-MOR 70.67 NileChat 70.33 70.89 72.56 72.67 43.85 39.94 44.37 23.10 37.45 33. 11.93 30.82 28.67 36.93 29.98 37.52 Table B.1: Comparison of the performance of the Egyptian SFT model (NileChat-EGY), the Moroccan SFT model (NileChat-MOR), and their merged version, NileChat, on Belebele (accuracy), Flores (chrf), and In-house parallel data (chrf)."
        },
        {
            "title": "Size Release Date",
            "content": "Qwen3-1.7B ar-stablelm-2-chat Atlas-Chat-2B Llama-3.2-3B-Instruct gemma-3-4b-it Qwen3-4B NLLB-200-3.3B Qwen2.5-3B-Instruct Less than 7B Apr. 2025 1.7 Jul. 2024 1.6 Sep. 2024 2.6 3.2 Sep. 2024 4.3 Mar. 2025 Apr. 2025 4 Jul. 2022 3.3 Sep. 2024 3.1 More than 7B AceGPT-7B-chat ALLaM-7B-Instruct Qwen2.5-7B-Instruct Qwen3-8B Atlas-Chat-9B gemma-3-12b-it AceGPT-13B-chat jais-13b-chat Dec. 2023 7 Feb. 2025 7 Apr. 2025 7.6 Apr. 2025 8.2 9.2 Sep. 2024 12.2 Mar. 2025 Dec. 2023 13 Aug. 2023 13 Table C.1: The LLMs used for comparison against NileChat in this evaluation were selected from list of Arabicaware models. Each LLM is listed with its corresponding size (in billion parameters) and release date. We utilized the instruct version for all LLMs except for NLLB, which is machine translation-specific model."
        },
        {
            "title": "EGY MOR EGY MOR EGY MOR",
            "content": "B 7 t L 7 t M Qwen3-1.7B ar-stablelm-2-chat Atlas-Chat-2B Llama-3.2-3B-Instruct gemma-3-4b-it Qwen3-4B Qwen2.5-3B-Instruct NileChat (3B) AceGPT-7B-chat ALLaM-7B-Instruct Qwen2.5-7B-Instruct Qwen3-8B Atlas-Chat-9B gemma-3-12b-it AceGPT-13B-chat jais-13b-chat 28.53 39.54 42.65 31.10 46.32 28.59 35.71 58.20 40.76 60.18 57.70 28.53 57.17 59.29 46.48 49.33 28.53 38.32 45.06 30.92 46.60 28.52 37.67 58.62 37.98 59.61 53.51 28.53 60.27 56.16 43.65 48.28 28.07 34.33 29.62 28.86 34.26 30.21 31.17 38.29 33.04 40.20 33.79 31.72 34.75 40.16 35.15 38. 27.33 33.40 34.78 28.39 32.53 29.53 29.62 40.35 31.04 38.14 32.28 30.95 44.47 37.60 33.21 37.45 22.89 24.22 54.67 49.67 61.44 22.89 61.11 78.11 38.00 76.11 76.67 22.89 78.44 80.78 46.33 59.89 22.89 22.78 59.00 40.89 52.11 22.89 44.89 73.78 33.00 66.00 59.44 22.89 79.33 73.11 41.11 53. Table D.1: 3-shot performance (accuracy) of models on understanding (MMLU, HellaSwag, and Belebele). Bold values indicate the highest score among models comparable in size to ours (<7B parameters). Underlined values represent the highest score in the entire column, including larger models. Results for zero-shot are presented in Table 1, Section 4.2. 21 a f t t e F 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 2 6 . 3 3 2 5 . 9 2 4 2 . 9 2 9 . 8 8 7 . 9 3 1 4 . 7 3 - 5 4 . 5 3 2 8 . 2 8 4 . 7 3 2 7 . 2 4 1 4 . 7 3 2 4 . 9 3 6 1 . 3 4 7 4 . 3 9 6 . 9 3 0 8 . 7 3 4 7 . 2 3 0 9 . 2 2 6 0 . 7 3 8 9 . 9 6 8 . 5 1 2 8 . 5 3 4 7 . 2 4 3 8 . 4 3 3 6 . 1 4 2 6 . 4 8 3 . 1 4 3 9 . 4 3 0 9 . 8 3 3 8 . 1 4 2 3 . 6 2 5 6 . 7 7 2 . 3 2 2 1 . 4 1 6 5 . 1 1 8 1 . 9 1 3 5 . 3 1 8 . 9 2 6 . 7 1 - 3 7 . 5 1 4 2 . 3 2 4 4 . 7 1 5 7 . 2 9 7 . 7 1 3 6 . 9 1 8 6 . 3 2 6 2 . 3 2 1 6 . 9 1 2 0 . 8 9 5 . 3 1 0 6 . 8 2 2 . 7 1 9 0 . 3 1 7 4 . 7 8 3 . 6 2 4 . 3 2 2 2 . 5 1 9 4 . 2 2 0 7 . 5 1 5 2 . 1 2 1 8 . 5 8 9 . 8 1 9 1 . 2 2 8 3 . 3 1 1 3 . 8 1 7 3 . 1 1 9 6 . 1 3 1 . 6 2 7 7 . 9 2 4 6 . 5 7 1 . 8 2 0 9 . 5 2 - 2 3 . 3 2 2 5 . 2 3 7 0 . 7 2 5 1 . 2 3 6 4 . 6 2 6 8 . 6 2 9 . 3 3 0 3 . 1 3 9 3 . 9 2 0 7 . 3 2 0 6 . 9 1 6 0 . 3 7 . 5 2 4 1 . 5 1 1 0 . 3 6 4 . 1 2 0 9 . 8 2 9 7 . 2 1 6 . 9 2 5 6 . 6 1 5 7 . 0 3 1 7 . 2 2 3 3 . 6 2 6 7 . 2 0 7 . 3 9 1 . 3 2 5 2 . 3 3 6 . 6 5 5 . 9 2 7 . 1 2 1 . 1 6 5 . 0 1 4 5 . 9 - 8 1 . 7 1 9 . 4 8 2 . 0 1 9 0 . 4 1 5 7 . 9 7 2 . 0 1 9 5 . 5 1 7 4 . 2 2 7 . 1 1 6 2 . 7 8 9 . 5 1 8 . 3 0 8 . 8 2 9 . 3 9 . 0 4 4 . 7 8 5 . 1 1 7 4 . 7 8 1 . 3 1 5 9 . 7 1 . 3 1 3 7 . 7 6 1 . 0 1 3 5 . 4 1 5 4 . 1 1 5 . 0 5 . 1 5 5 . 7 2 7 7 . 5 3 4 0 . 3 3 9 2 . 5 0 6 . 7 0 2 . 3 3 - 4 4 . 8 2 4 9 . 1 4 6 5 . 5 3 0 7 . 2 4 3 . 1 3 2 7 . 6 3 5 2 . 7 3 9 6 . 1 4 3 9 . 7 3 4 9 . 6 8 9 . 5 2 9 8 . 2 2 5 4 . 7 2 4 5 . 4 2 4 2 . 3 0 0 . 3 6 8 . 3 4 5 0 . 8 2 7 3 . 0 4 2 3 . 3 3 5 8 . 9 3 0 8 . 7 9 7 . 6 3 5 3 . 4 3 7 1 . 1 1 4 0 . 6 3 6 5 . 9 2 6 . 0 8 6 . 8 1 6 1 . 5 1 7 7 . 0 3 1 . 0 2 3 8 . 5 1 - 3 4 . 1 1 3 0 . 5 2 5 2 . 8 1 3 9 . 5 2 0 0 . 5 1 3 8 . 8 9 6 . 9 1 7 2 . 4 2 8 1 . 0 2 7 7 . 8 1 2 0 . 0 1 1 4 . 0 4 7 . 0 1 4 7 . 9 5 6 . 1 6 4 . 5 1 3 9 . 6 2 8 7 . 1 9 3 . 3 2 1 1 . 6 1 8 1 . 3 2 4 5 . 2 1 5 9 . 8 1 5 9 . 6 4 3 . 6 1 8 . 8 1 9 0 . 4 2 4 . 3 2 5 2 . 9 2 8 4 . 3 1 2 . 5 3 0 . 1 3 8 9 . 7 2 - 4 1 . 5 2 0 0 . 6 2 7 . 9 2 9 7 . 4 3 8 2 . 8 2 3 6 . 9 2 2 4 . 7 3 0 0 . 4 2 5 . 1 3 3 7 . 5 2 2 0 . 2 2 2 2 . 8 6 6 . 9 2 2 9 . 6 5 8 . 4 3 9 . 7 2 9 8 . 4 3 7 1 . 5 2 6 7 . 0 3 9 5 . 1 8 3 . 2 3 2 3 . 5 2 5 0 . 1 3 3 8 . 5 3 6 5 . 6 3 2 . 9 8 0 . 6 7 7 . 7 1 5 . 1 1 9 8 . 4 1 1 9 . 0 5 0 . 3 0 3 . 1 1 - 8 4 . 8 0 9 . 7 1 9 2 . 2 6 5 . 6 1 9 1 . 1 1 6 5 . 2 1 1 9 . 8 1 0 6 . 4 1 2 4 . 3 5 5 . 8 2 3 7 . 5 2 3 . 6 0 2 . 6 8 5 . 8 8 1 . 3 4 1 . 6 4 7 1 . 6 3 9 . 5 1 5 . 3 9 8 . 3 6 4 1 . 3 7 . 0 6 3 1 . 9 2 7 1 . 1 5 . 2 0 3 1 . 6 4 2 . 5 0 7 . 7 6 0 3 . 8 2 0 5 . 4 6 . 4 2 . 4 5 3 8 1 5 . - 9 9 0 5 . 9 1 6 5 . 6 9 1 . 5 7 6 5 . 4 7 3 5 . 7 7 4 . 7 0 6 5 . 2 4 7 5 . 1 7 4 . 6 5 4 5 . . 9 4 6 4 2 1 1 . 7 1 9 4 . . 9 1 5 4 2 6 4 . . 7 4 0 5 7 0 8 5 . 5 4 0 . 3 5 4 5 . . 8 2 0 5 3 5 5 . . 2 7 2 5 . 2 3 3 5 4 0 5 . 6 4 3 5 . . 2 5 2 5 . 3 7 1 4 8 4 0 2 . 3 9 7 . 1 4 5 . 6 0 0 . 4 0 9 2 . 5 7 5 . - 8 0 5 2 . 0 9 1 3 . 1 8 5 2 . 2 8 1 3 . 3 8 7 2 . 6 3 9 2 . 3 8 2 3 . 1 7 2 3 . 4 1 9 2 . 7 0 0 3 . 5 6 9 1 . 5 0 8 . 4 4 3 2 . 9 8 0 2 . 5 0 2 1 . 9 9 3 2 . 2 6 4 3 . 7 0 4 2 . 8 5 0 3 . 1 1 3 2 . 3 6 9 2 . 9 3 6 2 . 0 3 7 2 . 8 9 0 3 . 8 3 7 2 . 8 0 6 2 . 3 6 2 2 . 2 2 6 3 . 2 3 3 3 . 5 6 7 3 . 4 6 8 2 . 1 9 3 3 . 4 8 5 3 . - 1 9 4 3 . 4 6 7 . 4 2 5 3 . 0 0 7 3 . 1 9 2 . 0 4 5 3 . 1 9 7 3 . 8 2 6 . 0 5 5 3 . 5 2 1 3 . 3 2 6 . 5 2 9 2 . 8 8 6 3 . 9 5 4 . 0 1 9 2 . 0 9 1 3 . 4 1 5 . 5 1 4 3 . 2 2 8 3 . 4 0 6 . 2 3 7 3 . 4 3 9 2 . 3 6 4 . 8 7 6 3 . 8 9 4 2 . 2 6 6 . 2 2 2 2 . 1 3 9 1 . 5 3 6 . 8 8 9 1 . 0 0 4 1 . 1 3 6 . 6 8 8 1 . - 0 9 7 1 . 1 5 0 2 . 9 9 7 1 . 3 5 9 1 . 9 0 6 1 . 5 3 8 1 . 6 3 0 2 . 0 3 7 1 . 9 9 7 1 . 2 4 3 1 . 8 3 9 1 . 6 1 4 1 . 5 3 9 1 . 7 6 7 1 . 6 8 2 1 . 6 0 6 1 . 6 0 7 1 . 8 9 6 1 . 0 9 0 2 . 1 1 9 1 . 5 8 9 1 . 4 2 3 1 . 2 8 7 1 . 6 8 8 1 . 3 7 0 1 . 4 5 9 1 . 0 9 8 . 3 8 5 3 . 8 0 8 1 . 9 4 2 4 . 0 4 3 . 4 0 6 4 . 3 3 1 4 . - 5 4 9 . 6 5 5 4 . 7 8 8 3 . 4 . 4 4 3 9 2 4 . 2 3 4 4 . 5 8 9 4 . 6 3 1 5 . 7 7 2 4 . 5 9 0 4 . 8 0 5 3 . 6 3 9 1 . 3 5 1 4 . 5 5 9 2 . 8 2 1 1 . 6 9 9 3 . 9 6 8 4 . 6 5 8 3 . 0 3 6 4 . 8 0 8 3 . 4 6 2 4 . 2 1 2 4 . 7 1 3 4 . 5 3 9 4 . 4 6 6 2 . 5 4 0 4 . 6 1 9 3 . 0 9 2 1 . 5 8 2 . 8 1 0 2 . 4 5 0 . 0 8 3 2 . 9 3 8 1 . - 2 2 6 1 . 9 4 3 2 . 2 9 5 . 0 8 0 2 . 6 9 9 1 . 1 9 1 . 2 8 8 2 . 9 9 9 2 . 2 5 0 . 0 4 8 1 . 7 1 2 1 . 1 5 . 3 0 9 1 . 6 1 2 1 . 2 5 . 0 3 7 1 . 9 8 7 2 . 0 0 5 . 5 0 4 2 . 8 7 4 1 . 1 9 8 . 8 9 8 1 . 6 4 0 2 . 4 1 8 . 8 2 5 1 . 8 5 8 1 . 4 1 8 . 9 7 0 4 . 7 4 8 2 . 2 3 0 . 5 8 0 . 0 5 9 4 . 7 0 6 . - 9 9 . 4 4 3 4 3 . 1 7 5 4 . 2 9 2 5 . 5 5 8 . 1 5 9 4 . 1 6 5 5 . 7 4 4 . 1 1 9 4 . 0 8 1 5 . 1 2 . 0 7 9 . 8 2 6 3 . 9 4 5 7 . 8 3 1 8 . 7 1 8 6 . 4 4 4 6 . 3 2 2 . 4 4 8 9 . 2 5 9 0 . 4 4 5 6 . 1 5 8 2 . 7 4 4 0 . 8 9 1 . 4 5 5 7 . 7 4 2 1 . 6 4 1 8 . 7 3 5 4 . 5 1 3 0 . 8 1 . 6 2 8 0 . 0 8 4 . 4 2 7 6 . 0 2 - 3 8 . 9 0 5 . 9 2 4 0 . 0 2 3 1 . 8 2 2 6 . 3 2 3 3 . 4 2 5 7 . 2 9 2 . 0 3 0 3 . 4 2 0 4 . 7 2 6 7 . 4 1 6 3 . 7 4 6 . 4 1 7 . 5 1 9 9 . 7 8 9 . 8 1 9 8 . 0 3 1 6 . 8 1 1 8 . 8 7 6 . 7 1 2 0 . 6 2 9 5 . 1 2 1 4 . 2 2 9 2 . 1 3 8 6 . 3 2 8 . 0 2 3 7 . 9 1 5 4 . 6 3 6 4 . 4 3 0 9 . 6 3 1 7 . 1 8 7 . 7 3 5 1 . 7 3 - 2 3 . 6 3 5 2 . 9 3 2 7 . 5 9 3 . 1 4 8 0 . 5 3 3 1 . 8 3 9 2 . 7 3 4 2 . 1 4 7 5 . 6 6 4 . 7 3 2 3 . 6 3 1 3 . 4 3 7 6 . 6 3 5 1 . 5 3 0 0 . 3 7 1 . 7 3 4 7 . 8 3 0 2 . 5 3 5 2 . 0 4 1 9 . 6 3 3 9 . 0 9 1 . 2 3 4 8 . 7 3 2 3 . 6 3 8 2 . 6 3 4 0 . 7 3 6 3 . 6 0 8 . 9 1 7 5 . 8 1 3 0 . 0 2 9 7 . 0 1 1 1 . 1 2 7 5 . 0 - 1 7 . 9 1 8 6 . 2 2 3 9 . 8 1 2 1 . 5 2 7 8 . 8 1 4 . 1 2 2 5 . 0 2 8 4 . 4 2 8 5 . 9 1 8 2 . 0 2 9 4 . 9 9 2 . 8 1 7 6 . 9 1 5 7 . 8 1 9 8 . 6 1 1 4 . 0 2 2 9 . 0 1 5 . 8 1 1 8 . 3 2 2 9 . 9 1 3 6 . 4 2 8 2 . 6 1 0 1 . 1 4 4 . 9 1 7 6 . 9 1 4 1 . 0 2 0 5 . 3 1 r I - 3 - 2 3 - l . h - 2 - l s - 2 - C - t . 7 1 - 3 Q u n - 3 - 5 2 Q . C N r I - 7 - L u n - 7 - 5 2 Q . c - 7 - e i - 2 1 - 3 - e 9 - C - t 8 - 3 Q h - 3 1 - e a - 3 1 - j - 4 - 3 - e 4 - 3 Q . 3 3 - 0 0 2 - N a A R E Y N E R G A O E Y O d c r e s D o - 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 4 0 0 3 . 3 2 9 . 9 1 7 9 . 7 2 5 6 . 7 5 1 . 1 3 8 7 . 6 2 - 2 9 . 5 2 5 9 . 8 3 2 1 . 8 2 4 8 . 7 3 0 2 . 7 2 4 1 . 9 7 4 . 1 3 1 3 . 7 3 9 1 . 1 3 6 2 . 4 3 9 2 . 3 2 6 0 . 8 7 9 . 5 2 7 8 . 9 1 7 5 . 3 1 6 0 . 6 2 3 4 . 0 3 1 7 . 4 2 7 . 6 3 5 6 . 6 2 7 6 . 6 3 5 2 . 5 2 8 8 . 7 2 8 5 . 7 1 9 . 3 2 4 0 . 9 2 6 1 . 4 2 9 6 . 9 9 2 . 8 2 3 . 2 5 6 . 2 0 0 . 5 1 7 0 . 2 1 - 9 1 . 1 1 1 8 . 1 9 7 . 2 1 2 7 . 0 2 6 2 . 2 1 5 8 . 3 1 2 9 . 4 1 8 9 . 9 0 4 . 5 1 1 8 . 7 1 3 4 . 9 7 7 . 7 1 3 . 0 1 8 0 . 4 4 . 6 9 6 . 1 1 5 6 . 3 1 9 2 . 0 1 1 8 . 9 1 1 8 . 1 6 2 . 9 1 6 0 . 1 1 8 8 . 2 1 1 4 . 1 1 0 8 . 1 1 3 5 . 3 3 6 . 1 1 4 9 . 7 1 2 1 . 3 1 0 2 . 5 2 5 2 . 1 1 2 9 . 8 1 5 . 8 1 - 8 8 . 7 1 5 7 . 0 3 8 9 . 8 1 9 4 . 6 4 4 . 8 1 3 8 . 8 1 2 1 . 8 2 7 0 . 6 2 2 7 . 0 2 5 7 . 3 8 3 . 8 1 2 8 . 5 1 5 3 . 4 2 8 6 . 4 1 3 1 . 2 3 4 . 6 0 1 . 3 2 4 0 . 7 1 7 3 . 9 2 1 3 . 9 1 5 9 . 5 2 7 6 . 5 3 8 . 7 1 0 7 . 3 2 3 8 . 8 4 3 . 0 2 6 1 . 6 1 7 0 . 9 4 . 3 1 1 . 0 1 6 4 . 2 6 1 . 6 5 1 . 6 - 0 4 . 5 0 2 . 6 9 1 . 5 6 8 . 8 0 3 . 4 2 7 . 7 4 . 5 0 7 . 8 0 0 . 5 1 4 . 5 1 4 9 . 3 1 0 0 . 3 1 . 1 1 4 7 . 5 2 1 . 6 3 5 . 2 1 3 7 . 0 1 1 3 . 3 8 . 8 3 5 . 6 8 2 . 0 1 5 5 . 4 9 7 . 5 6 1 . 6 0 . 3 6 8 . 6 7 5 . 5 6 8 . 3 3 0 2 . 7 2 5 7 . 6 7 1 . 8 1 6 5 . 5 3 1 7 . 5 3 - 1 2 . 3 3 8 6 . 6 5 0 . 7 3 9 7 . 3 4 1 9 . 1 3 0 9 . 5 3 1 0 . 7 2 1 5 . 3 5 7 . 9 3 3 8 . 9 3 0 2 . 4 3 4 1 . 8 2 2 7 . 0 2 0 8 . 1 5 3 . 1 1 3 8 . 3 3 8 6 . 6 3 1 9 . 8 2 0 9 . 2 4 4 1 . 7 9 0 . 1 4 0 1 . 8 2 9 9 . 3 3 0 2 . 9 1 5 3 . 8 1 1 6 . 7 1 8 . 0 3 6 2 . 8 1 8 2 . 4 1 1 9 . 1 1 7 9 . 7 0 3 . 9 2 7 . 9 1 - 2 8 . 6 1 4 5 . 9 2 7 4 . 0 2 5 1 . 6 0 0 . 6 1 9 4 . 9 1 5 0 . 2 1 8 4 . 6 2 3 8 . 2 2 4 7 . 2 0 2 . 8 1 9 8 . 4 1 5 0 . 7 3 7 . 5 1 0 5 . 5 8 9 . 7 4 3 . 9 1 7 4 . 3 1 7 3 . 6 2 2 2 . 0 2 8 2 . 3 2 1 2 . 3 1 7 . 7 1 9 1 . 6 4 1 . 9 2 3 . 0 2 0 2 . 5 1 2 1 . 8 9 5 . 8 1 8 4 . 8 2 9 7 . 6 0 3 . 8 2 4 6 . 1 2 - 2 8 . 0 2 5 5 . 6 3 7 1 . 2 2 2 0 . 6 3 9 7 . 2 2 8 1 . 3 9 2 . 1 3 1 5 . 4 3 8 7 . 4 2 9 7 . 4 3 5 2 . 9 1 2 3 . 1 8 3 . 8 2 5 5 . 4 1 2 2 . 1 1 2 1 . 1 2 1 1 . 6 2 7 8 . 0 3 2 . 4 3 0 2 . 0 2 6 8 . 4 3 4 8 . 0 2 9 7 . 1 2 6 8 . 9 3 9 . 3 2 8 2 . 2 2 6 3 . 8 2 4 0 . 6 8 2 . 7 6 7 . 2 1 8 9 . 1 7 6 . 2 1 1 5 . 8 - 8 6 . 7 5 4 6 . 0 5 8 . 2 1 2 1 . 0 0 5 . 1 4 4 . 6 8 7 . 5 7 8 . 2 5 7 . 8 2 . 9 1 5 5 7 1 . 3 2 . 8 0 6 . 9 1 5 0 . 9 6 1 . 9 8 3 . 5 4 5 . 7 1 0 3 . 0 1 4 7 . 8 1 5 1 7 . 1 4 8 . 2 7 7 . 0 3 8 . 0 7 3 . 6 2 2 1 . 7 4 8 . 9 8 2 . 1 0 3 2 . 5 0 9 1 . 2 8 5 . 1 0 1 . 0 2 3 3 . 2 4 6 . - 5 7 7 2 . 6 4 1 4 . 7 9 7 2 . 6 3 0 4 . 7 1 9 2 . 3 6 9 2 . 5 1 0 4 . 0 9 9 3 . 8 2 1 3 . 4 4 8 3 . 8 5 2 2 . 3 6 0 2 . . 3 8 4 3 3 0 7 1 . . 1 8 6 1 4 7 6 2 . 4 4 2 3 . 2 0 6 2 . 3 8 9 3 . 8 0 6 2 . 4 4 9 3 . 8 3 8 2 . . 0 9 8 4 8 7 3 . 0 4 3 3 . . 2 1 8 . 4 3 6 2 2 1 6 . 5 1 6 . 5 8 5 1 . 3 2 0 . 5 6 2 1 . 4 5 8 . - 9 4 9 . 7 4 0 . 5 9 9 . 7 9 9 1 . 5 5 0 . 3 0 1 1 . 9 1 9 1 . 5 3 8 . 4 2 2 1 . 6 5 8 1 . 9 1 . 5 9 6 . 6 9 4 1 . 4 7 . 0 1 6 . 8 1 9 . 2 1 4 . 5 1 8 . 2 2 9 1 . 0 8 . 1 6 8 1 . 7 8 9 . 3 2 0 . 7 7 6 1 . 9 4 2 1 . 6 5 0 . 6 0 2 1 . 9 9 0 1 . 1 6 4 . 9 7 3 2 . 3 6 5 . 9 5 8 . 7 5 3 1 . - 6 3 4 1 . 4 8 6 2 . 2 1 5 1 . 7 4 4 2 . 4 0 6 1 . 8 6 5 1 . 1 5 6 2 . 5 1 3 2 . 0 9 7 1 . 0 1 0 2 . 4 1 1 1 . 5 4 1 . 6 7 1 2 . 4 1 0 1 . 0 9 1 . 5 3 3 1 . 6 8 9 1 . 1 0 3 1 . 7 0 5 2 . 6 7 2 1 . 5 6 2 2 . 7 8 3 1 . 4 6 4 1 . 8 6 2 2 . 9 2 6 . 6 2 5 1 . 2 4 7 . 4 6 2 . 9 6 3 . 0 4 8 . 8 9 0 . 7 6 5 . 0 4 3 . - 5 6 . 2 5 2 . 5 6 0 . 1 8 . 3 0 2 . 9 4 0 . 1 4 . 8 2 6 . 8 3 3 . 2 7 1 . 4 7 0 1 . 9 6 3 . 2 3 . 0 5 4 . 0 5 4 . 0 6 0 . 5 9 7 . 7 1 5 . 5 3 . 6 3 3 . 5 0 8 . 5 6 . 7 6 3 . 9 1 7 . 3 8 . 7 2 4 . 7 4 2 . 3 4 5 . 5 2 2 2 . 5 1 7 1 . 6 1 . 1 4 8 2 . 8 4 0 2 . - 7 9 8 1 . 7 8 4 3 . 5 2 5 2 . 2 1 4 3 . 6 1 0 2 . 8 1 3 2 . 6 4 1 2 . 0 1 4 3 . 9 4 7 2 . 5 3 0 3 . 0 1 5 1 . 0 5 7 . 9 2 5 1 . 4 4 6 1 . 1 6 1 . 4 0 0 2 . 8 7 9 2 . 4 9 7 1 . 7 3 3 3 . 9 3 2 2 . 5 8 2 3 . 4 0 9 1 . 5 2 2 2 . 2 1 7 1 . 5 1 4 1 . 4 5 5 2 . 8 7 4 1 . 6 9 4 . 9 9 0 1 . 5 1 5 . 1 4 2 . 6 6 3 1 . 6 1 8 . - 8 5 6 . 2 9 8 1 . 7 5 0 . 8 7 7 1 . 1 9 7 . 2 5 . 4 7 7 . 0 7 7 1 . 6 5 2 . 9 1 4 1 . 3 6 4 . 7 5 . 7 6 3 . 0 6 5 . 2 5 . 9 1 8 . 0 2 4 1 . 4 3 . 7 6 7 1 . 4 2 9 . 9 6 6 . 5 6 7 . 4 0 9 . 3 5 . 9 5 6 . 6 7 0 1 . 5 4 . 9 0 5 3 . 3 2 4 2 . 4 3 2 . 1 8 8 . 2 5 2 4 . 3 9 8 . - 5 7 4 3 . 0 4 6 4 . 1 5 7 3 . 3 9 7 4 . 1 7 6 3 . 8 3 2 4 . 4 8 5 3 . 7 6 8 4 . 1 1 2 4 . 2 5 3 4 . 4 2 . 4 3 1 7 . 7 9 3 . 0 3 0 9 . 5 2 1 6 . 0 3 4 8 . 7 3 6 3 . 4 3 7 0 . 4 2 5 . 2 4 9 9 . 5 3 2 2 . 8 4 8 0 . 3 3 8 2 . 1 4 9 2 . 1 6 0 . 0 4 0 6 . 0 4 3 2 . 6 3 2 3 . 0 2 1 7 . 3 1 9 1 . 8 0 8 . 4 8 9 . 6 2 3 6 . 3 2 - 8 4 . 0 2 5 6 . 1 9 9 . 1 2 5 8 . 2 3 0 9 . 2 2 5 3 . 7 2 8 6 . 0 2 0 6 . 2 2 4 . 7 2 5 1 . 8 2 1 1 . 9 1 6 5 . 4 1 1 5 . 5 1 4 2 . 4 3 5 . 7 1 2 2 . 2 2 3 3 . 6 1 4 0 . 9 1 7 7 . 6 2 4 9 . 0 9 4 . 2 3 4 1 . 9 1 6 0 . 6 2 2 7 . 5 1 1 9 . 5 2 7 6 . 4 8 6 . 1 2 8 9 . 1 3 8 2 . 0 2 5 2 . 4 3 7 3 . 1 2 7 . 3 5 9 . 8 3 - 9 5 . 9 3 0 1 . 8 4 3 9 . 0 4 7 5 . 9 4 3 . 2 4 4 3 . 4 4 8 3 . 1 4 8 5 . 8 4 7 4 . 5 4 0 3 . 3 6 4 . 1 3 2 9 . 1 2 4 0 . 2 3 0 4 . 8 2 6 9 . 2 3 1 1 . 9 4 1 . 1 4 4 8 . 9 3 6 4 . 6 4 6 3 . 9 3 2 3 . 8 4 5 0 . 3 6 3 . 2 4 1 9 . 8 3 1 3 . 6 4 3 5 . 2 4 6 1 . 3 3 4 1 . 3 4 7 . 6 1 2 . 6 1 8 3 . 0 1 9 . 2 2 2 4 . 8 1 - 8 3 . 9 1 9 4 . 7 2 4 4 . 1 2 8 9 . 8 2 0 4 . 1 2 2 6 . 3 9 1 . 1 2 1 5 . 8 2 7 3 . 5 2 4 9 . 4 2 4 1 . 2 1 9 8 . 2 5 . 3 1 7 9 . 2 1 4 2 . 6 1 1 2 . 9 1 8 4 . 1 2 4 4 . 9 4 2 . 6 2 7 2 . 9 1 1 3 . 6 2 0 7 . 2 2 1 2 . 2 2 8 9 . 8 2 1 . 3 2 6 3 . 2 2 1 7 . 6 1 r I - 3 - 2 3 - l . c - 2 - l t - 2 - C - t . 7 1 - 3 Q r I - 3 - 5 2 Q . C N r I - 7 - L u n - 7 - 5 2 Q . h - 7 - e i - 2 1 - 3 - e 9 - C - t 8 - 3 Q c - 3 1 - e t - 3 1 - j - 4 - 3 - e 4 - 3 Q . 3 3 - 0 0 2 - N r A M Y S M Y O M Y N A G E l M Lessthan7B Morethan7B Lessthan7B Morethan7B 22 l l . r c ) 4 ( h - 4 ) 0 ( s - 0 ) ( c a ) ("
        },
        {
            "title": "U\nE\nL\nB\np\ns",
            "content": ": r . s d o - n r n s s o u e t n d a : 2 . b . u h n a o c e h t a n u d l n . t r o i h w t l m m o o t g g Table F.1: Sample responses from NileChat to prompts in Egyptian and Moroccan dialects, covering general and local cultural knowledge. Samples with green background color represent samples with correct responses, samples with red background color represent samples with not accurate answers."
        }
    ],
    "affiliations": [
        "Invertible AI",
        "MBZUAI",
        "PSU",
        "The University of British Columbia",
        "UM6P"
    ]
}