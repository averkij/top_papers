{
    "paper_title": "DreamStyle: A Unified Framework for Video Stylization",
    "authors": [
        "Mengtian Li",
        "Jinshu Chen",
        "Songtao Zhao",
        "Wanquan Feng",
        "Pengqi Tu",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has a characteristic advantage: text is more flexible, style image provides a more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to a single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, a unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by a well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on a vanilla Image-to-Video (I2V) model and trained using a Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 ] . [ 1 5 8 7 2 0 . 1 0 6 2 : r DreamStyle: Unified Framework for Video Stylization Mengtian Li Jinshu Chen Songtao Zhao Wanquan Feng"
        },
        {
            "title": "Pengqi Tu Qian He",
            "content": "Intelligent Creation Lab, ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Video stylization, an important downstream task of video generation models, has not yet been thoroughly explored. Its input style conditions typically include text, style image, and stylized first frame. Each condition has characteristic advantage: text is more flexible, style image provides more accurate visual anchor, and stylized first frame makes long-video stylization feasible. However, existing methods are largely confined to single type of style condition, which limits their scope of application. Additionally, their lack of high-quality datasets leads to style inconsistency and temporal flicker. To address these limitations, we introduce DreamStyle, unified framework for video stylization, supporting (1) text-guided, (2) style-image-guided, and (3) first-frame-guided video stylization, accompanied by well-designed data curation pipeline to acquire high-quality paired video data. DreamStyle is built on vanilla Image-to-Video (I2V) model and trained using Low-Rank Adaptation (LoRA) with token-specific up matrices that reduces the confusion among different condition tokens. Both qualitative and quantitative evaluations demonstrate that DreamStyle is competent in all three video stylization tasks, and outperforms the competitors in style consistency and video quality. Date: January 7, 2026 Project Page: https://lemonsky1995.github.io/dreamstyle/"
        },
        {
            "title": "Introduction",
            "content": "Video stylization stands as compelling yet challenging task in the field of visual content generation. Existing video stylization approaches are confronted with the following critical limitations: (1) Limited stylization capabilities due to single-modality condition. Text prompts and style images are the two dominant style conditions, but both suffer from inherent flaws. Text prompts are typically ambiguous and unconstrained, failing to precisely describe most abstract styles. Style images, while more visually accurate, exhibit inferior user-friendliness, flexibility, and creativityit is difficult to acquire suitable style image, especially for unseen styles. Consequently, most existing methods are confined to styles that are either explicitly describable via text or have clear visual references, exhibiting limited generalization to novel styles. (2) Scarcity of high-quality modality-aligned training data. Some existing methods [8, 29, 56] acquire stylization capabilities from image stylization datasets and subsequently generalize to the video domain assisted by pre-trained video generation model. This paradigm inherently introduces an unavoidable trade-off among style consistency, temporal consistency, and motion dynamics. More recently, UNIC [55] synthesizes stylized videos via Corresponding author, Project lead. 1 Figure 1 We propose DreamStyle, unified video stylization framework, which provides flexible and practical tool for users to create high-quality stylized videos. Given an input video and the reference styles in forms of text, style image, or stylized first frame, DreamStyle faithfully generates videos that align with the desired stylewhile preserving the main content of the input video. Text-to-Video (T2V) model and employs gray tiled ControlNet to invert these stylized videos into their realistic counterparts, thereby constructing paired video data. However, its stylization quality is limited by the T2V model and it fails to handle the styles involving geometric deformation due to the strict alignment of tile ControlNet. (3) Insufficient exploration of potential extended applications. Current research predominantly focuses on basic stylization capabilities, with limited attention to high-demand extended scenarios, such as multi-style fusion and long-video stylization. To tackle the aforementioned challenges, we propose DreamStyle, which includes the following three key innovations: First, we introduce unified Video-to-Video (V2V) stylization framework, which is built upon vanilla I2V model. Through meticulous design of the condition injection mechanism, we manage to unify diverse forms of style guidance including text prompt, style image, and stylized first frame into single model, extending the I2V base model to V2V domain while preserving its original architecture and inherent capabilities. We further employ modified LoRA module composed of shared down matrix and token-specific up matrices to enhance the multi-task adaptability. Second, we present systematic data curation pipeline tailored for the video stylization task, the core of which involves two steps: (1) stylizing the initial frame of real-world video using image stylization techniques and (2) generating the full stylized video sequence from the stylized first frame via an I2V model equipped with ControlNets. To guarantee the data quality, we further adopt hybrid filtering strategy consisting of automatic and manual filtering. Leveraging this pipeline, we construct two datasets with distinct scales and quality to facilitate multi-stage training. Finally, comprehensive evaluations across multiple dimensions exhibit that our unified model, DreamStyle, achieves competitive performance against specialized models across various video stylization tasks. Notably, we also demonstrate that allowing multiple style conditions within single forward process is crucial design for improving the effectiveness and controllability of video stylizationsuch design unlocks the models capability to support more potential extended applications, such as multi-style fusion and long-video stylization. Overall, our contributions are summarized as follows. Paradigm. We introduce DreamStyle, which consists of unified framework that supports text-guided, styleimage-guided, and first-frame-guided video stylization; well-designed pipeline for constructing high-quality paired data for video stylization. 2 Technology. DreamStyle framework presents condition injection mechanism that enables seamless handling of diverse video stylization tasks within unified model; novel LoRA module that mitigates the interference among different condition tokens. Scalability. DreamStyle data curation pipeline is practical and scalable for video stylization, overcoming the scarcity of high-quality data and the inherent trade-off between style fidelity and temporal coherence. Significance. DreamStyle outperforms specialized competitors in various video stylization tasks and exhibits the potential for under-explored extended tasks."
        },
        {
            "title": "2.1 Video Diffusion Model",
            "content": "Diffusion models [10, 18, 31, 40, 43] have driven remarkable advancements in visual content generation. Latent Diffusion Models [34, 38] (LDMs) further optimize this paradigm by training diffusion network in the latent space of pretrained Variational Autoencoder [24] (VAE) to reduce computational complexity, becoming the mainstream solution. Early video diffusion models [5, 6, 15, 16] were mostly built upon pretrained image diffusion models (typically U-Net [39] architectures), incorporating temporal modules to handle temporal consistency. However, their isolated processing of spatial and temporal information inherently limited their quality and consistency. With the release of Sora [7] and its epoch-making generation quality, researchers notice the potential of Diffusion Transformer [33] (DiT) for video generation. Recent DiT-based methods [12, 19, 25, 48, 53] apply unified manner to model the video in spatial-temporal domain, and scale the capability of DiT by more parameters, data and computing resources, achieving more high-quality and consistent video generation."
        },
        {
            "title": "2.2 Image Stylization",
            "content": "Gatys et al. [13] pioneered image stylization using neural networks. Early methods [9, 13, 21, 37] relied on the statistical descriptors (such as gram matrices, mean and standard deviation, and histograms) extracted from pretrained VGG [42] network to represent and transfer style information. However, due to the limitations of the capability of generation and style extraction, they could only achieve simple texture and color transfer, often resulting in suboptimal visual quality. Recent methods [35, 49, 52, 54] benefit from the advances of diffusion models to improve basic quality, and CLIP [36] to extract high-level semantic information from style images. StyleTokenizer [27] further improves the style extractor with contrastive learning using self-collected style dataset. Given the importance of high-quality datasets for stylization, OmniStyle [51] constructs large-scale paired dataset using six state-of-the-art (SOTA) image stylization methods, and leverages unified DiT backbone to extract style features and generate images, yielding new SOTA performance."
        },
        {
            "title": "2.3 Video Stylization",
            "content": "Extending image-based tasks to video domain is major trend in current research, and stylization is no exception. TokenFlow [14] and AnyV2V [26] achieve video stylization by leveraging image stylization techniques to stylize the first frame or key frames and then propagate it to the entire video sequence. However, these approaches can not perform video stylization independently, and rely on time-consuming DDIM [45] inversion. UniVST [46] further DDIM inverses the style image and leverage AdaIN [21] to guide the denoising progress of noisy video by the inverted features of style. StyleCrafter [29] utilizes CLIP to extract style features and inject these features into the denoising U-Net via dual cross-attention. More recently, StyleMaster [56] upgrades to DiT backbone and incorporates both global and local style extractors, resorting to StillMoving [8] to train LoRA [20] for temporal attention to bridge the gap between image and video. However, this scheme requires explicit temporal modeling within the base model, which deviates from mainstream architectures. Moreover, limitation shared by all these methods is their lack of stylized video datasets, resulting in suboptimal visual quality and temporal consistency. 3 Figure 2 Data Curation Pipeline. We propose generating the training data with two key steps: image stylization followed by image to video. Considering the characteristics of different image stylization techniques, we construct CT dataset and SFT dataset, where SDXL (equipped with ControlNet, InstantStyle, and ID plugin) and Seedream 4.0 are selected as their stylization models, respectively. For image to video, we utilize ControlNets to enhance the motion consistency between the generated stylized and raw videos. To ensure the data quality, we additionally apply automatic filtering for CT data and manual filtering for SFT data."
        },
        {
            "title": "3.1 Data Curation Pipeline",
            "content": "Given the fact that current image generation / editing models are superior to the video counterpart in terms of visual quality, structure, aesthetics and text following, we propose generating stylized video datasets with two key steps: (1) leverage the SOTA image stylization models to stylize the first frame of raw video; (2) utilize the I2V model to generate stylized video from the stylized first frame. Our data curation pipeline is illustrated in Fig. 2. It is noteworthy that high-quality first frame serves as crucial cues (e.g., style constraints and content anchors) to improve the overall quality of the entire video generated by I2V model. To obtain the high-fidelity stylized first frame, we select InstantStyle [49] and Seedream 4.0 [41] as our image stylization models, which are proficient in style-image-guided and text-guided stylization, respectively. InstantStyle is SDXL [34] plugin, which we further equip with depth ControlNet [58] and ID plugin [17] to constrain the consistency of structure and face identity. It is worthy noting that the text-guided stylization model typically produces better visual quality and style consistency, while the style-image-guided stylization model allows us to generate images with greater style diversity. Thus, we construct two datasets: (1) large-scale stylized dataset for Continual Training (CT) generated via InstantStyle to ensure the core video stylization capability and generalization of DreamStyle; (2) small-scale higher-quality stylized dataset for Supervised Fine-Tuning (SFT) generated with Seedream 4.0 to elevate the upper bound of DreamStyle. Figure 3 Example that depth fails to capture accurate detail. (a) The raw video frame, (b) the extracted depth map, (c) the generated realistic frame, (d) the generated stylized frame. It is critical to ensure the motion consistency between stylized video and raw video, so that we are able to construct stylized-raw video pairs. To this end, we customize two ControlNets (with control conditions of depth and human pose, respectively) for our in-house I2V model. The depth ControlNet is well-suited for 4 Figure 4 Overview of DreamStyle Framework. DreamStyle is built on the Wan14B-I2V model, integrating the text and raw-video conditions through the cross-attention and image channels of the base model, while the first-frame and style-image conditions serve as additional frames concatenated to the start and end of the frame sequence. We train it using standard flow matching loss and token-specific LoRA that contributes to distinguishing different condition tokens. general cases, while the human pose ControlNet offers more precise control of human motion and especially allows for larger deformation of driven objects without losing motion coherence. As illustrated in Fig. 3, we observe that directly animating the stylized first frame using the control conditions extracted from the raw video and then making paired data proves to be suboptimal. This is because neither depth nor pose can fully capture the complex motion dynamics of the raw video, ultimately resulting in motion mismatches between stylized and raw videos. Thus, we adjust to utilizing the same control condition to drive the generation of both stylized and raw video frames, aiming to mitigate such mismatches. We formally denote our dataset as: = {(xraw , xsty , tns , tsty , s1...K )i = 1, 2, .., } (1) i and tsty are the raw and stylized videos, tns denotes style reference images. To obtain tns and xsty where xraw are the text prompts that exclude / and tsty include style descriptions, and s1...K , we utilize Visual-Language Model [57] (VLM) to parse the stylized video xsty and then generate the corresponding video caption. We restrict the VLM to exclude any style-related attributes (e.g., artistic genre, color palette, texture pattern, and hue) when generating tns contains only style-irrelevant descriptions. Regarding s1...K , we stylize additional images using the same guided condition (text prompt or style reference) as xsty with low style consistency detected by VLM and CSD [44] score, while we opt for manual filtering for the SFT dataset. Additionally, we manually verify the content consistency between each raw video and its stylized video in the SFT dataset. Finally, such two datasets enable our DreamStyle to support all three video stylization tasks. . For the CT dataset, we further filter out those s1...K , so that tns i"
        },
        {
            "title": "3.2 DreamStyle Framework",
            "content": "As shown in Fig. 4, our DreamStyle framework is built upon the Wan14B-I2V [48] base model that incorporates additional image condition channels before the patchify layer. This design allows for the injection of raw video condition via these channels, rather than the in-context frames injection adopted in UNIC [55]. major advantage is that it involves minimal extra computational overhead, ensuring DreamStyle retains the efficiency of the original I2V model. Overall, we have four types of conditions to inject into the I2V model: the raw video along with three guided conditions of style, which are described in detail as follows. 5 (1) Text condition. We reuse the original textual cross-attention layers of Wan14B-I2V without introducing modifications. (2) First-frame condition. We feed the stylized first frame into the original image condition channels and set the mask channels of the first frame to 1.0 in the same manner as the base model. (3) Style-image condition. Assuming that zs RC1HW (omit the subscript i) is the VAE encoded latent of the style reference image sj , we construct the final I2V models input tensor for the style image via channel-wise concatenation: = add_noise(zs, t) 141HW zs zs where denotes the concatenation operation along the channel dimension, add_noise(, t) is the noise injection function of flow matching [28] at timestep [0, 1]. The number of mask channels is 4 in Wan14BI2V, thus 141HW represents mask tensor filled with constant value 1.0. Wan14B-I2V also includes native CLIP image feature branch, which we employ to inject high-level semantic features of the style reference image, thereby enhancing the consistency of style-related semantic information. (4) Raw-video condition. We to obtain their latents zraw, zsty RCF HW , and first encode the raw video xraw then channel-wise concatenate them with mask tensor 04F HW : and stylized video xsty (2) zv = add_noise(zsty, t) 04F HW zraw. (3) Here we adopt the mask value 0.0, following the principle of minimal modification of the base model. For the is treated as an additional frame and concatenated to the end of zv style-image-guided mode, zs via frame-wise concatenation : zv zs , enabling the model to incorporate the style-image condition. Similarly, for the first-frame-guided mode, the first-frame tensor z1st via frame-wise zv concatenation: z1st . is concatenated to the beginning of zv To retain the base models inherent generative capabilities, we adopt LoRA to train our DreamStyle. After patchification, the three conditions, first-frame, style-image, and raw-video, are transformed into their corresponding token sequences. However, these tokens serve distinct semantic roles, thus using standard LoRA leads to inter-token confusion. Inspired by HydraLoRA [47], we propose adopting modified LoRA with token-specific up matrices in full attention and feedforward (FFN) layers. That means, for an input token xin, we first project it using shared down matrix Wdown, and then compute the output residual token xout = Wi up according to the token type {0, 1, 2}, which is analogous to LoRA MoE [11] with manual routing. Such LoRA enables the model to learn adaptive features tailored to the three types of tokens, and still be trained stably due to the large proportion of shared parameters. upWdownxin with specific up matrix Wi"
        },
        {
            "title": "3.3 Training",
            "content": "We follow the same optimization objective as flow matching to train our DreamStyle. Formally, we denote our model as vθ, function with five inputs: the video tensor zv , the timestep t, the first-frame tensor z1st , and the text prompt tns/sty. In each training batch, we randomly sample style the style image tensor zs conditions according to predefined ratios, thus the training objective is: L(θ) = EDvθ(zv EDvθ(zv EDvθ(zv , t, , , tsty) (zsty ϵ)2 , tns) (zsty ϵ)2 , t, , zs , , tns) (zsty ϵ)2 , t, z1st (I) (II) (III) (4) where ϵ (0, 1) is Gaussian noise and (I) (III) correspond to the loss terms for the text-guided, the style-image-guided and the first-frame-guided tasks, respectively. As mentioned in Sec. 3.1, we make two datasets with different scales and quality, thus adopting two-stage training strategy. In the first stage, we train DreamStyle on the large-scale CT dataset, allowing the model to learn diverse styles and establish foundational capability to handle all three style conditions. In the second stage, higher-quality SFT dataset is used to further finetune DreamStyle, aiming to improve visual quality and style consistency. 6 Metrics Condition Method CLIP-T / DINO Dynamic Background CSD Score Score Degree Quality Quality Consistency Consistency Image Aesthetic Subject Text Style Image First Frame Luma Pixverse Runway DreamStyle StyleMaster (T2V) DreamStyle (T2V) DreamStyle (V2V) VACE VideoX-Fun DreamStyle 0.132 0.155 0.154 0.167 0.198 0.532 0.515 0.689 0.766 0.851 0.406 0.451 0.504 0.584 - - 0.526 0.716 0.702 0.640 0.766 0.766 0.809 0.894 0.289 0.689 0.867 0.889 0.844 0.856 0.739 0.746 0.725 0.738 0.723 0.722 0.704 0.716 0.726 0.731 0.572 0.628 0.606 0.656 0.610 0.641 0.635 0.573 0.594 0.630 0.934 0.948 0.940 0.952 0.936 0.950 0.938 0.922 0.915 0. 0.942 0.951 0.944 0.956 0.935 0.961 0.948 0.930 0.924 0.932 Table 1 Quantitative comparison. The best and second best results are shown in bold and underline."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "Through the data curation pipeline (Sec. 3.1), we construct 40K and 5K stylized-raw video pairs for the CT stage and SFT stage training, where the video resolution is 480P and the length is up to 81 frames. In the CT dataset, each sample includes exactly one style reference while the samples from the SFT dataset contain 1 16 style images, with one randomly selected for training. During the training, we empirically set the sampling ratio of the three style conditions (text-guided, style-image-guided and first-frame-guided) to 1 : 2 : 1. The training process is performed on NVIDIA GPUs, with each GPU accommodating per-GPU batch-size of 1. To stabilize training, we further adopt 2-step gradient accumulation strategy, resulting in larger effective batch size of 16. We train DreamStyle for 6, 000 and 3, 000 iterations in the CT and SFT stages, respectively, using LoRA with rank of 64 and AdamW [30] optimizer with learning rate of 4 105."
        },
        {
            "title": "4.2 Settings",
            "content": "For text-guided video stylization, we curate 50 videos paired with style prompts crafted by designer as our test set. Since no open-source models specialized in text-guided video stylization are currently available, we opt to compare DreamStyle against three commercial models: Luma [1], Pixverse [2] and Runway [3]. We further expand the aforementioned test set to 90 videos and 15 style images (each style image is randomly paired with 6 videos) to evaluate the style-image-guided task. As baseline, we select StyleMaster [56], the only open-source DiT-based method that supports style-image-guided video stylization in an end-to-end manner. For first-frame-guided task, we reuse these 90 videos and generate stylized first frames for each video using image stylization methods, and then choose VACE [23] and VideoX-Fun [4] as our competitors. To evaluate the style consistency, we utilize the CSD [44] score as the quantitative metric. Specifically, the CSD score is computed between the style reference image and each frame of the generated video for style-image-guided task, while for first-frame-guided task, we evaluate this metric between the stylized first frame and all subsequent frames. For text-guided stylization, we employ ViCLIP [50] to measure the similarity between user prompt and stylized video. Moreover, structure preservation is evaluated using the cosine similarity of the patch features (excluding the CLS token) extracted from DINOv2 [32]. We further assess the overall quality of stylized video with five metrics from VBench [22]: dynamic degree, image quality, aesthetic quality, subject consistency, and background consistency."
        },
        {
            "title": "4.3 Comparisons",
            "content": "Quantitative Comparison. As shown in Table 1, we conduct comprehensive comparison across three video stylization tasks. In text-guided video stylization, DreamStyle achieves the highest CLIP-T (we measure text-video similarity using only the style prompts, thus the CLIP-T is overall lower) and DINO score, indicating that it outperforms the other methods in both style prompt following and structure preservation. 7 This superiority is further evidenced in the visual results in Fig. 5. For the overall video quality assessment, our method also has advantages in most metrics except image quality. Notably, image quality exhibits negative correlation with dynamic degree, since high dynamic video tends to involve motion blur, thereby decreasing this metric. Due to the incomplete open-source of StyleMaster, it supports only T2V instead of V2V stylization. Thus, we include an additional result for DreamStyle in T2V mode, where the video condition is set to empty. Quantitative metrics demonstrate the superior performance of our method, particularly in the aspects of style consistency and dynamic degree. Despite not being explicitly trained for T2V, DreamStyle naturally inherits this capability from the base model thanks to the LoRA training and outperforms its V2V counterpart in most metrics due to fewer constraints. In first-frame-guided video stylization, DreamStyle presents the optimal style consistency (CSD score) and either the best or second-best video quality metrics. Since the stylized first frames (especially those with geometric deformation) occasionally conflict with the structure of the input video, our method, despite its superior style consistency, is inferior to VACE and VideoX-Fun in structure preservation (DINO score). However, the visual results in Fig. 5 confirm that it can still maintain the primary structural elements of the input video. Qualitative Comparison. Fig. 5 presents the visual comparisons between our DreamStyle and the competitors. In text-guided video stylization, Luma tends to generate videos with dark tones, and the subject pose, color, and content of its results deviate far from the input videos. Pixverse achieves higher pose consistency but still suffers from content distortion (e.g., the camera in the left case and the bowknot in the right case). Runway often produces videos with realistic style bias, failing to accurately render the correct style. By contrast, our DreamStyle not only follows the style prompt but also achieves superior consistency with the input videos in terms of subject pose, color, and content. In style-image-guided video stylization, StyleMaster exhibits limited capability in simple color and texture transfer, while our method can further handle the styles involving geometric shapes. In first-frame-guided video stylization, both VACE and VideoX-Fun struggle to preserve the stylized first frame in the left case. For the right case, although they are able to maintain the major style of the given first frame, serious style degradation occurs in the subsequent frames. By comparison, DreamStyle demonstrates higher style consistency across the stylized first frame, the generated first frame, and all subsequent frames. Metric Style Consistency Content Consistency Overall Quality Score 5 4 3 2 1 5 4 3 2 1 5 4 3 2 1 Description Both the main subject and background perfectly align with the style reference, with stable style throughout the entire video The main subject and background are relatively consistent with the style reference, or there are minor style degradation across the video Either the main subject or the background is somewhat inconsistent with the style reference, or the video exhibits noticeable style variations Neither the main subject nor the background aligns with the style reference, or the video has significant style inconsistencies The main subject and background are completely inconsistent with the style reference Both the main subject and background are highly consistent with the input video, and the motion of the main subject is also highly coherent Either the main subject or the background has slight discrepancies from the input video, or the motion of the main subject is somewhat inconsistent Either the main subject or the background has noticeable differences from the input video, or the motion of the main subject is highly inconsistent Both the main subject and background show obvious deviations from the input video The main subject and background are completely unrelated to the input video Excellent performance in both style consistency and content consistency, with aesthetically pleasing visuals and rational motion Either style consistency or content consistency needs improvement; or the visuals are generally aesthetically acceptable, with slight motion glitches Either style consistency or content consistency is poor; or the visuals have low aesthetic appeal, with noticeable motion issues Both style consistency and content consistency are poor, with unappealing visuals and significant motion issues Extremely poor performance in both style consistency and content consistency Table 2 Details of evaluation criteria."
        },
        {
            "title": "4.4 User Study",
            "content": "Human feedback serves as an important method for evaluating stylization performance, thus we conduct user study focusing on three core metrics: style consistency, content consistency, and overall quality. Each metric is rated on 1-5 scale, with the detailed evaluation criteria provided in Table 2. We recruited 20 professional data annotators as evaluators and randomly selected 10, 20, and 20 samples from the text-guided, style-image-guided, and first-frame-guided test sets, respectively, for blind evaluation. As shown in Table 3, DreamStyle outperforms other methods across all three stylization tasks, with notable superiority in style consistency. Its overall quality score reaches approximately 4 or higher, reflecting user recognition of its performance. 8 Figure 5 Qualitative comparison on three video stylization tasks. 9 Condition Method"
        },
        {
            "title": "First Frame",
            "content": "Luma Pixverse Runway DreamStyle StyleMaster DreamStyle VACE VideoX-Fun DreamStyle Metrics"
        },
        {
            "title": "Content",
            "content": "2.05 2.83 2.52 4.14 1.17 4.36 2.35 3.19 4.37 2.58 2.95 2.80 3.95 - 3.87 4.30 4.22 4.12 2.24 2.82 2.59 3.95 1.31 4.20 2.79 3.42 4.24 Table 3 User study on three video stylization tasks."
        },
        {
            "title": "4.5 Extended Applications",
            "content": "Although DreamStyle is trained with only single condition type at time, it still supports multiple guidance modalities during inference, thereby unlocking its potential to enable broader extended applications. Below, we highlight two representative scenarios: Multi-Style Fusion. As shown in Fig. 6, DreamStyle can naturally integrates the style cues from both text prompts and style images, demonstrating its capability to fuse diverse style references and create novel style. This flexibility allows for creative combination of abstract textual description and precise visual reference, exhibiting the potential beyond single guidance. Long-Video Stylization. By leveraging the last frame of generated short video as the first frame condition for the next segment, we can seamlessly concatenate two short video clips. Thus, combination of first frame guidance and text or style image enables DreamStyle to overcome the 5-second duration limit, supporting stylization for longer video sequences (except multi-shot video due to the inherent limitations of the base model and training data). Fig. 8 presents two long-video stylization examples, guided by style image and text, respectively."
        },
        {
            "title": "CSD Score DINO Score",
            "content": "w.o. Token-specific LoRA Only CT Data Only SFT Data Full 0.413 0.459 0.535 0.515 0.518 0.547 0.483 0.526 Table 4 Quantitative comparison of ablation studies."
        },
        {
            "title": "4.6 Ablation Studies",
            "content": "Token-specific LoRA. The proposed token-specific LoRA plays critical role in mitigating interference among distinct condition tokens. To validate this, we design an ablation experiment where DreamStyle is trained with standard LoRAhere, different condition tokens are distinguished solely through frame positions and mask values (1 for first-frame tokens, 0 for video tokens, and 1 for style-image tokens). We focus on the style-image-guided stylization task for evaluation. As shown in Table 4, the standard LoRA exhibits significantly negative influence on style consistency (CSD score) and slightly reduces structure preservation (DINO score). Visual evidence in Fig. 7 further indicates this point, where the problems of style degradation (first row) and style confusion (second row) arise in the absence of token-specific LoRA. Datasets. To validate the necessity of both datasets (with distinct scales and quality) and two-stage training, we conduct an ablation experiment where DreamStyle is trained on only the CT dataset, only the SFT dataset, and both of them in two stages. Due to the limited quality and style consistency of the CT dataset, 10 Figure 6 Visual results of multi-style fusion. Figure 7 Impact of token-specific LoRA. Figure 8 Visual results of long-video stylization. Figure 9 Visual comparison across different datasets. 12 training exclusively on it yields suboptimal stylization performance. Both the quantitative metric (lower CSD score) in Table 4 and the visual result (failure to render the pixel pattern) in Fig. 9 confirm this shortcoming. Conversely, the SFT dataset contains manually filtered paired videos with high quality and strong style consistency, but its limited size makes it insufficient to adapt the I2V base model into robust V2V model for stylization (particularly, there is no strict alignment between the paired videos due to the existence of geometric deformation). Thus, as shown in Table 4, training solely on it, despite achieving the best CSD score, exhibits the worst performance on structure preservation, which is also evidenced in Fig. 9 (the pose of the stylized panda differs from the input). As expected, training on the CT and SFT datasets in turn achieves robust balance between style consistency and structure preservation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose DreamStyle, the first unified framework for video stylization that supports three style conditions: text, style image, and first frame. Recognizing the critical role of high-quality paired video datasets in training, we develop systematic data curation pipeline consisting of two key steps: (1) leveraging the SOTA image stylization models to obtain the stylized first frame; (2) animating the raw and stylized first frames using an I2V model equipped with ControlNets. In each step, we further apply automatic and manual filtering to ensure data quality. DreamStyle is built upon an I2V model, which can be efficiently extended to the V2V model without involving too much extra computation overhead. Moreover, to address the inter-token confusion among different style conditions within unified model, we introduce novel token-specific LoRA module. With our high-quality dataset, well-designed model architecture, and two-stage training strategy, DreamStyle archives competitive performance on various video stylization tasks."
        },
        {
            "title": "References",
            "content": "[1] Luma ai. [2] Pixverse ai video generator. [3] Runway ai image and video generator. [4] more flexible framework that can generate videos at any resolution and creates videos from images. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [7] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. [8] Hila Chefer, Shiran Zada, Roni Paiss, Ariel Ephrat, Omer Tov, Michael Rubinstein, Lior Wolf, Tali Dekel, Tomer Michaeli, and Inbar Mosseri. Still-moving: Customized video generation without customized video data. ACM Transactions on Graphics (TOG), 43(6):111, 2024. [9] Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang Hua. Stylebank: An explicit representation for neural image style transfer. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18971906, 2017. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [11] Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Wei Shen, Limao Xiong, Yuhao Zhou, Xiao Wang, Zhiheng Xi, Xiaoran Fan, et al. Loramoe: Alleviating world knowledge forgetting in large language models via moe-style plugin. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19321945, 2024. 13 [12] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [13] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. [14] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In The Twelfth International Conference on Learning Representations, 2024. [15] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, et al. I2v-adapter: general image-to-video adapter for diffusion models. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In The Twelfth International Conference on Learning Representations, 2024. [17] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37:3677736804, 2024. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations, 2023. [20] Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [21] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. [22] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. [23] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1719117202, 2025. [24] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. [25] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [26] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-to-video editing tasks. Transactions on Machine Learning Research, 2024. Reproducibility Certification. [27] Wen Li, Muyuan Fang, Cheng Zou, Biao Gong, Ruobing Zheng, Meng Wang, Jingdong Chen, and Ming Yang. Styletokenizer: Defining image style by single instance for controlling diffusion models. In European Conference on Computer Vision, pages 110126. Springer, 2024. [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [29] Gongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Yibo Wang, Xintao Wang, Ying Shan, and Yujiu Yang. Stylecrafter: Taming artistic video diffusion with reference-augmented adapter learning. ACM Transactions on Graphics (TOG), 43(6):110, 2024. 14 [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [31] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. [32] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Featured Certification. [33] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [34] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. [35] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86938702, 2024. [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [37] Eric Risser, Pierre Wilmot, and Connelly Barnes. Stable and controllable neural texture synthesis and style transfer using histogram losses. arXiv preprint arXiv:1701.08893, 2017. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image In International Conference on Medical image computing and computer-assisted intervention, segmentation. pages 234241. Springer, 2015. [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [41] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward next-generation multimodal image generation. arXiv preprint arXiv:2509.20427, 2025. [42] Simonyan and Zisserman. Very deep convolutional networks for large-scale image recognition. In 3rd International Conference on Learning Representations (ICLR 2015). Computational and Biological Learning Society, 2015. [43] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. [44] Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah Goldblum, Jonas Geiping, Abhinav Shrivastava, and Tom Goldstein. Measuring style similarity in diffusion models. arXiv preprint arXiv:2404.01292, 2024. [45] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. [46] Quanjian Song, Mingbao Lin, Wengyi Zhan, Shuicheng Yan, Liujuan Cao, and Rongrong Ji. Univst: unified framework for training-free localized video style transfer. arXiv preprint arXiv:2410.20084, 2024. [47] Chunlin Tian, Zhan Shi, Zhijiang Guo, Li Li, and Cheng-Zhong Xu. Hydralora: An asymmetric lora architecture for efficient fine-tuning. Advances in Neural Information Processing Systems, 37:95659584, 2024. [48] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [49] Haofan Wang, Matteo Spinelli, Qixun Wang, Xu Bai, Zekui Qin, and Anthony Chen. Instantstyle: Free lunch towards style-preserving in text-to-image generation. arXiv preprint arXiv:2404.02733, 2024. [50] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: large-scale video-text dataset for multimodal understanding and generation. In The Twelfth International Conference on Learning Representations, 2024. [51] Ye Wang, Ruiqi Liu, Jiang Lin, Fei Liu, Zili Yi, Yilin Wang, and Rui Ma. Omnistyle: Filtering high quality style transfer data at scale. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 78477856, 2025. [52] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Content-style composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. [53] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations, 2025. [54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [55] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. [56] Zixuan Ye, Huijuan Huang, Xintao Wang, Pengfei Wan, Di Zhang, and Wenhan Luo. Stylemaster: Stylize your video with artistic generation and translation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26302640, 2025. [57] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE transactions on pattern analysis and machine intelligence, 46(8):56255644, 2024. [58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023."
        }
    ],
    "affiliations": [
        "Intelligent Creation Lab, ByteDance"
    ]
}