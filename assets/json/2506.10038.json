{
    "paper_title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
    "authors": [
        "Giannis Daras",
        "Adrian Rodriguez-Munoz",
        "Adam Klivans",
        "Antonio Torralba",
        "Constantinos Daskalakis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve state-of-the-art ImageNet FID, and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 8 3 0 0 1 . 6 0 5 2 : r Ambient Diffusion mni: Training Good Models with Bad Data Giannis Daras Massachusetts Institute of Technology gdaras@mit.edu Adrian Rodriguez-Munoz Massachusetts Institute of Technology adrianrm@mit.edu Adam Klivans The University of Texas at Austin klivans@utexas.edu Antonio Torralba Massachusetts Institute of Technology torralba@mit.edu Constantinos Daskalakis Massachusetts Institute of Technology costis@csail.mit.edu"
        },
        {
            "title": "Abstract",
            "content": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, simple, principled framework to train diffusion models that can extract signal from all available images during training. Our framework exploits two properties of natural images spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We then use our framework to achieve stateof-the-art ImageNet FID and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times."
        },
        {
            "title": "Introduction",
            "content": "Large-scale, high-quality training datasets have been primary driver of recent progress in generative modeling. These datasets are typically assembled by filtering massive collections of images sourced from the web or proprietary databases [24, 42, 51, 56, 57]. The filtering processwhich determines which data is retainedis crucial to the quality of the resulting models [12, 26, 24, 31, 26]. However, filtering strategies are often heuristic and inefficient, discarding large amounts of data [49, 42, 24, 12]. We demonstrate that the data typically rejected as low-quality holds significant, underutilized value. Extracting meaningful information from degraded data requires algorithms that explicitly model the degradation process. In generative modeling, there is growing interest in approaches that learn to generate directly from degraded inputs [15, 16, 17, 13, 7, 46, 38, 50, 5, 1, 2, 53, 69, 45, 62, 44]. Equal contribution. Preprint. key limitation of existing methods is their reliance on knowing the exact form of the degradation. In real-world scenarios, image degradationssuch as motion blur, sensor artifacts, poor lighting, and low resolutionare often complex and lack well-defined analytical description, making this assumption unrealistic. Even within the same dataset, from ImageNet to internet scale text-to-image datasets, there are samples of heterogeneous qualities [27], as shown in Figures 4, 25, 28, 26. Given access to this mixed-bag of datapoints, we would like to sample from tilted continuous measure of high-quality images, without sacrificing the diversity present in the training points. Figure 1: Effect of using Ambient-o for (a) training text-to-image model (Micro-Diffusion [52]) and (b) class-conditional model for ImageNet (EDM-2 [35]). All generations are initialized with the same noise. The baseline models are trained using all the data equally. Ambient-o changes the way the data is used during the diffusion process based on its quality. This leads to significant visual improvements without sacrificing diversity, as would happen with filtering approach (see Fig. 8). The training objective of diffusion models naturally decomposes sampling from target distribution into sequence of supervised learning tasks [29, 59, 60, 14, 18, 9, 10]. Due to the power-law structure of natural image spectra [63], high diffusion times focus on generating globally coherent, semantically meaningful content [21], while low diffusion times emphasize learning high-frequency details. Our first key theoretical insight is that low-quality samples can still be valuable for training in the high-noise regime. As noise increases, the diffusion process contracts distributional differences (see 2 Theorem 4.2), reducing the mismatch between the high-quality target distribution and the available mixed-quality data. At the same time, incorporating low-quality data increases the sample size, reducing the variance of the learned estimator. Our analysis formalizes this biasvariance trade-off and motivates principled algorithm for training denoisers at high diffusion times using noisy, heterogeneous data. For low diffusion times, our algorithm leverages second key property of natural images: locality. We show direct relationship between diffusion time and the optimal receptive field size for denoising. Specifically, small image crops suffice at lower noise levels. This allows us to borrow high-frequency details from out-of-distribution or synthetic images, as long as the marginal distributions of the crops match those of the target data. We introduce Ambient Diffusion Omni (Ambient-o), simple and principled framework for training diffusion models using arbitrarily corrupted and out-of-distribution data. Rather than filtering samples based on binary good or bad labels, Ambient-o retains all data and modulates the training process according to each samples utility. This enables the model to generate diverse outputs without compromising image quality. Empirically, Ambient-o advances the state of the art in unconditional generation on ImageNet and enhances diversity in text-conditional generation without sacrificing fidelity. Theoretically, it achieves improved bounds for distribution learning by optimally balancing the biasvariance trade-off: low-quality samples introduce bias, but their inclusion reduces variance through increased sample size. We will and all https://github.com/giannisdaras/ambient-omni. release code our trained models in the following URL:"
        },
        {
            "title": "2 Background and Related Work",
            "content": "Diffusion Modeling. Diffusion models transform the problem of sampling from p0 into the problem of learning denoisers for smoothed versions of p0 defined as pt = p0 (0, σ2(t)I). We typically denote with X0 p0 the R.V. distributed according to the distribution of interest and Xt = X0 + σ(t)Z, the R.V. distributed according to pt. The target is to estimate the set of optimal l2 denoisers, i.e., the set of the conditional expectations: {E[X0Xt = ]}T t=1. Typically, this can be achieved through supervised learning by minimizing the following loss (or re-parametrization of it): J(θ) = EtU [0,T ]Ex0,xtt (cid:104) hθ(xt, t) x02(cid:105) , (2.1) that is optimized over function family = {hθ : θ Θ} parametrized by network parameters θ. For sufficiently expressive families, the minimizer is indeed: hθ (x, t) = E[X0Xt = x]. Learning from noisy data. The diffusion modeling framework described above assumes access to samples from the distribution of interest p0. An interesting variation of this problem is to learn to sample from p0 given access to samples from tilted measure p0 and known degradation model. In Ambient Diffusion [15], the goal is to sample from p0 given pairs (Ax0, A) for matrix : Rmn, < n, that is distributed according to known density p(A). The techniques in this work were later generalized to accommodate additive Gaussian Noise [13, 16, 1] in the measurements. More recently there have been efforts to further broaden the family of degradation models considered through Expectation-Maximization approaches that involve multiple training runs [50, 5]. Recent work from [16] has shown that, at least for the Gaussian corruption model, leveraging the low-quality data can tremendously increase the performance of the trained generative models. In particular, the authors consider the setting where we have access to few samples from p0, lets denote them D0{x(i) i=1, where ptn = p0 (0, σ2(tn)I) is smoothed version of p0 at known noise level tn. The clean samples are used to learn denoisers for all noise levels [0, ] while the noisy samples are used to learn denoisers only for tn, using the training objective: i=1 and many samples from ptn , lets denote them Dtn{x(i) tn }N2 0 }N Jambient(θ) = EtU (tn,T ] N2(cid:88) i=1 xtx(i) tn (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)α(t)hθ(xt, t) + (1 α(t))xt x(i) (cid:12) tn (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2(cid:21) , (2.2) with α(t) = σ2(t)σ2(tn) . Note that the objective of equation 2.2 only requires samples from ptn (instead of p0) and can be used to train for all times tn. This algorithm uses N1 + N2 datapoints σ2(t) 3 to learn denoisers for > tn and only N1 datapoints to learn denoisers for tn. The authors show that even for N1 << N2, the model performs similarly to the setting of training with (N1 + N2) clean datapoints. The main limitation of this method and its related works is that the degradation process needs to be known. However, in many applications, we have data from heterogeneous sources and various qualities, but there is no analytic form or any prior on the corruption model. Data filtering. One of the most crude, but widely used, approaches for dealing with heterogeneous data sources is to remove the low-quality data and train only the high-quality subset [42, 24, 22]. While this yields better results than naively training on the entire distribution, it leads to decrease in diversity and relies on heuristics for optimizing the filtering. An alternative strategy is to train on the entire distribution and then fine-tune on high-quality data [12, 52]. This approach better trades the quality-diversity trade-off but still incurs loss of diversity and is hard to calibrate. Training with synthetic data. lot of recent works have shown that synthetic data can improve the generative capabilities of diffusion models when mixed properly with real data from the distribution of interest [23, 3, 4]. In this work, we show that it helps significantly to view synthetic data as corrupted versions of the samples from the real distribution and incorporate this perspective into the training objective."
        },
        {
            "title": "3 Method",
            "content": "We propose new framework that extends beyond [16] to enable training generative models directly from arbitrarily corrupted and out-of-distribution data, without requiring prior knowledge of the degradation process. We begin by formalizing the setting of interest. 0 }N Problem Setting. We are given dataset = {w(i) i=1 consisting of datapoints. Each point in is drawn from mixture distribution p0, which mixes p0 (the distribution of interest) and an alternative distribution q0 that may contain various forms of degradation or out-of-distribution content. We assume access to two labeled subsets, SG, SB, where points in SG are known to come from the clean distribution p0, and points in SB from the corrupted distribution q0. While this assumption simplifies the initial exposition, we relax it in Section E.1. We focus on the practically relevant regime where SG Di.e., access to high-quality data is severely limited. The objective is to learn generative model that (approximately) samples from the clean distribution p0, leveraging both clean and corrupted samples in its training. We now describe how degraded and out-of-distribution samples can be effectively leveraged during training in both the high-noise and low-noise regimes of the diffusion process. 3.1 Learning in the high-noise regime (leveraging low-quality data) Addition of gaussian noise contracts distribution distances. The first key idea of our method is that, at high diffusion times t, the noised target distribution pt and the noised corrupted distribution pt become increasingly similar (Theorem 4.2), effectively attenuating the discrepancy introduced by corruption. This effect is illustrated in Figure 2 (top), where we compare clean image and its degraded counterpart (in this case, corrupted by Gaussian blur). As the diffusion time increases, the noised versions of both samples become visually indistinguishable. Consequently, samples from p0 can be leveraged to learn (the score of) pt, for > tmin . We formalize this intuition in Section 4, and we also quantify that for large there are statistical efficiency benefits for using large sample from p0 versus small sample from p0 . Heuristic selection of the noise level. From the discussion so far, it follows that to use samples from p0, we need to assign them to noise level tmin . One can select this noise level empirically, i.e. we can ablate this parameter by training different models and selecting the one that maximizes the generative performance. However, this approach requires multiple trainings, which can be costly. Instead, we can find the desired noise level in principled way as detailed below. Training classifier under additive Gaussian noise. To identify the appropriate noise level, we train time-conditional classifier to distinguish between the noised distributions pt and qt across various diffusion times t. We use single neural network cnoise (xt, t) that is conditioned on the θ 4 Figure 2: time-dependent classifier trained to distinguish noisy clean and blurry images (blur kernel standard deviation σB = 0.6). At low noise the classifier is able to perfectly identify the blurry images, and outputs probability close to 0. As the noise increases and the information in the image is destroyed, the clean and blurry distributions converge and the classifier outputs prediction close to 0.5. The red line plots the threshold (selected at τ = 0.45), which is crossed at σt = 1.64. diffusion time t, following the approach of time-aware classifiers used in classifier guidance [20]. The classifier is trained using labeled samples from SG (clean) and SB (corrupted) via the following objective: Jnoise(θ) = (cid:88) x0SG Extx0 (cid:2) log cnoise θ (xt, t)(cid:3) + (cid:88) y0SB Eyty0 (cid:2) log(1 cnoise θ (yt, t))(cid:3) (3.1) Annotation. Once the classifier is trained, we use it to determine the minimal level of noise that must be added to the low-quality distribution q0 so that it closely approximates smoothed version of the high-quality distribution p0. Formally, we compute: tmin = inf [0, ] : 1 SB (cid:88) y0SB Eyty0 (cid:2)cnoise θ (yt, t)(cid:3) > τ , (3.2) )}N (i), tmin 0 +σtmin for τ = 0.5 ϵ and for some ϵ > 0. Subsequently, we form the annotated dataset Dannot = {(w(i) i=1{(x0, 0)x0 SG}, where the random variables (i) are i.i.d. standard normals. In particular, our annotated dataset indicates that we should only use the samples from for diffusion times tmin , for which the distributions have approximately merged and hence it is safe to use them. In fact, the optimal classifier assigns time tn that corresponds to the first time for which dTV(pt, qt) ϵ. Sample dependent annotation. One potential issue with the aforementioned annotation approach is that all the samples in are treated equally. But, as we noted, the points in could be drawn from distribution p0 that mixes p0 and q0. In this case, all the samples in that came from the p0 component, will still get high annotation time, leading to information loss. Instead, we can opt-in for sample-wise annotation scheme, where each sample w(i) based on: = inf{t [0, ] : tmin 0 gets assigned time tmin (wt, t)(cid:3) > τ }, for τ = 0.5 ϵ and for some ϵ > 0. (cid:2)cnoise θ wtw(i) From arbitrary corruption to additive Gaussian noise. The afore-described approach reduces our problem of learning from data with arbitrary corruption to the setting of learning from data corrupted with additive Gaussian noise. The price we pay for this reduction is the information loss due to the extra noise we add to the samples during the annotation stage. We can now extend the objective function (2.2) to train our diffusion model. Suppose our annotated dataset is comprised of 5 samples {(x(i) tmin , tmin )}. Then our objective becomes: Jambiento(θ) = EtU [0,T ] (cid:88) i:tmin <t xtx(i) tmin (cid:20)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)α(t, tmin (cid:12) )hθ(xt, t) + (1 α(t, tmin ))xt x(i) tmin 2(cid:21) , (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) where α(t, tmin ) = σ2(t)σ2(tmin σ2(t) ) . Learning something from nothing? The proposed framework comes with limitations worth considering. First, unless the diffusion noise level tends to infinity, the distributions pt and qt never fully convergethere is always bias when treating samples from qt as if they were from pt. Moreover, the method is particularly well-suited to certain types of corruptions but is less effective for others. Because the addition of Gaussian noise suppresses high-frequency componentsdue to the spectral power law of natural imagesour approach is most effective for corruptions that primarily degrade high frequencies (e.g., blur). In contrast, degradations that affect low-frequency contentsuch as color shifts, contrast reduction, or fog-like occlusionsare more challenging. This limitation is illustrated in Figure 3: masked images, for example, require significantly more noise to become usable compared to high-frequency corruptions like blur. In the extreme, the method reduces to filtering approach, as infinite noise nullifies all information in the corrupted samples. Figure 3: Visual summary of our method for using low-quality data at high-noise. We see how the various corrupted images become indistinguishable from the High Quality (HQ) after minimum noise level. These noisy versions of Low Quality (LQ) images are actually high-quality data, which filtering approaches discard, but Ambient Omni uses. 3.2 Learning in the low-noise regime (synthetic and out-of-distribution data) So far, our algorithm implicitly results in varying amounts of training data across diffusion noise levels. At high noise, the model can leverage abundant low-quality data, whereas at low noise levels, it must rely solely on the limited set of high-quality samples. We now extend the algorithm to enable the use of synthetic and out-of-distribution data for learning denoisers at low-noise diffusion times. To achieve this, we leverage another fundamental property of natural images: locality. At low diffusion times, the denoising task can be solved using only small local region of the image, without requiring full spatial context. We validate this hypothesis experimentally in the Experiments Section (Figures 15, 16, 17, 18), where we show that there is mapping between diffusion time and the 6 crop size needed to perform the denoising optimally at this diffusion time. Intuitively, the higher the noise, the more context is required to accurately reconstruct the image. Conversely, for lower noise, the local information within small neighborhood suffices to achieve effective denoising. We use crop(t) to denote the minimal crop size needed to perform optimal denoising at time t. If there are two distributions p0 and p0 that agree on their marginals (i.e. crops), they can be used interchangeably for low-diffusion times. Note that the distributions dont have to agree globally, they only have to agree on local (patch) level. Formally, let A(t) be random patch selector of size crop(t). Let also p0, p0 two distributions that satisfy: A(t)#p0 = A(t)#p0, (3.3) where A(t)#p0 denotes the pushforward measure2 of p0 under A(t). Then, the cropped portions of the tilted distributions provide equivalent information to the crops of the original distribution for denoising. Training crops classifier. Note that the condition of Equation (3.3) can be trivially satisfied if A(t) masks all the pixels or even if A(t) just selects single pixel. We are interested in finding what is the maximum crop size for which this condition is approximately true. Once again, we can use classifier to solve this task. The input to the classifier, ccrops , is crop of an image that either arises from p0 or p0, and the classifier needs to classify between these two cases. θ Annotation and training using the trained classifier. Once the classifier is trained, we are now interested in finding the biggest crop size for which the distributions p0, p0 cannot be confidently distinguished. Formally, tmax = sup [0, ] : 1 SB (cid:88) y0SB [ccrops θ (A(t)(yt))] > τ , (3.4) for τ = 0.5 ϵ and for some small ϵ > 03. For times tmax , the out-of-distribution images from p0 can be used with the regular diffusion objective as images from p0, as for these times the denoiser only looks at crops and at the crop level the distributions have converged. The donut paradox. Each sample can be used for tmin , but not for (tmax ). We call this the donut paradox as there is hole in the middle of the diffusion trajectory for which we have fewer available data. These times do not have enough noise for the distributions to merge globally, but also the required receptive field for denoising is big enough so that there are differences on crop level. We show an example of this effect in Figure 14. and for tmax , tmin i Table 1: ImageNet results with and without classifier-free guidance. Train FID Test FID ImageNet-512 FID FIDv2 FID FIDv EDM2-XS Ambient-o-XS EDM2-XXL Ambient-o-XXL Ambient-o-XXL+crops no CFG 3.57 3.59 1.91 (1.93) 1.99 1."
        },
        {
            "title": "4 Theory",
            "content": "w/ CFG no CFG w/ CFG no CFG w/ CFG no CFG w/ CFG 3.77 3.69 2.88 2.81 2.78 103.39 107.26 42.84 43.38 42.84 115.16 115.02 56.42 56.40 56.39 79.94 79.56 33.09 33.34 32.63 93.86 92.96 46.22 46.02 45.78 3.68 3.58 2.73 2.68 2. 2.91 2.89 1.81 1.87 1.80 Model Size Mparams NFE 125 125 1523 1523 1523 63 63 63 63 63 We study the 1-d case, but all our claims easily extend to any dimension. We compare two algorithms: Algorithm 1. Algorithm 1 trains diffusion model using access to n1 samples from target density p0, assumed to be supported in [0, 1] and be λ1-Lipschitz. Algorithm 2. Algorithm 2 trains diffusion model using access to n1 + n2 samples from density p0 that is mixture of the target density p0 and another density q0, assumed to be supported in [0, 1] and be λ2-Lipschitz: p0 = n1 p0 + n2 q0. n1+n2 n1+n 2Given measure spaces (X1, Σ1) and (X2, Σ2), measurable function : X1 X2, and probability measure : Σ1 [0, ), the pushforward measure #p is defined as (f #p)(B) := p(f 1(B)) Σ2. 3We subtract an ϵ to allow for approximate mixing of the two distributions and hence smaller annotation times. 7 High quality images Low quality images Figure 4: Results using CLIP to obtain the high-quality and the low-quality sets of ImageNet. We want to compare how well these algorithms estimate the distribution pt := p0 (0, σ2 use ˆp(1) to denote the estimates obtained for pt by Algorithms 1 and 2 respectively. , ˆp(2) ). We Diffusion modeling is Gaussian kernel density estimation. We start by making connection between the optimal solution to the diffusion modeling objective and kernel density estimation. Given finite dataset {W (i)}n i=1, the optimal solution to the diffusion modeling objective should match the empirical density at time t, which is: ˆpt(x) = 1 nσt (cid:88) i=1 ϕ (cid:18) (i) σt (cid:19) , (4.1) where ϕ(u) = 1 2π Gaussian kernel density estimate, given samples {W (i)}n eu2/2 is the Gaussian kernel. We observe that equation 4.1 is identical to 4. i= We establish the following result for Gaussian kernel density estimation. Theorem 4.1 (Gaussian Kernel Density Estimation). Let {W (i)}n i=1 be set of independent samples from λ-Lipschitz density p. Let ˆp be the empirical density, pσ := (0, σ2) and ˆpσ = ˆp (0, σ2). Then, with probability at least 1 δ with respect to the sample randomness, (cid:114) dTV(pσ, ˆpσ) 1 + 1 σ2n + log + log(1 λ) + log 2/δ σ2n . (4.2) The proof of this result is given in the Appendix. Comparing the performance of Algorithms 1 and 2. Applying Theorem 4.1 directly to the p0 density, we immediately get that the estimate ˆp(1) (x) obtained by Algorithm 1 satisfies: (cid:115) dTV(pt, ˆp(1) ) 1 + 1 σ2 n1 + log n1 + log(1 λ1) + log 2/δ σ2 n1 . (4.3) Let us now see what we get by applying Theorem 4.1 to Algorithm 2, which uses samples from the (cid:16) n1 tilted distribution p0. Since this distribution is -Lipschitz, we get that: λ1 + n2 λ2 (cid:17) n1+n n1+n2 dTV(pt, ˆp(2) ) 1 (n1 + n2) + 1 σ2 (n1 + n2) + (cid:115) log(n1 + n2) + log(1 n1 λ1 + n2 n1+n2 λ2) + log 2/δ , n1+n2 σ2 (n1 + n2) 4This connection has been observed in prior works too, e.g., see [32, 8]. 8 (a) High quality crops (b) Low quality crops Figure 5: Results using CLIP to find (a) high-quality and (b) low-quality crops on ImageNet. ). where pt := p0 (0, σ2 Further, we have that: dTV(pt, ˆp(2) the second term. To bound the first term, we prove the following theorem. Theorem 4.2 (Distance contraction under noise). Consider distributions and supported on subset of Rd with diameter D. Then ) dTV(pt, pt) + dTV(pt, ˆp(2) ). We already have bound for dTV(P (0, σ2I), (0, σ2I)) dTV(P, Q) 2σ . Applying this theorem we get that: dTV(pt, pt) 1 2σt for the second inequality we used that dTV(p0, p0) n2 dTV(p0, p0) 1 2σt n2 n1+n2 dTV(p0, q0). n1+n2 dTV(p0, q0), where Putting everything together, Algorithm (2) achieves an estimation error: dTV(pt, ˆp(2) ) 1 (n1 + n2) + 1 σ2 (n1 + n2) + (cid:115) log(n1 + n2) + log(1 n1 λ1 + n1+n2 n1+n2 σ2 (n1 + n2) λ2) + log 2/δ + n2 σt(n1 + n2) dTV(p0, q0). 9 such that for any tmin Comparing this with the bound obtained in Equation 4.3, we see that if n2 is sufficiently larger than n1 or if λ2 λ1, there is tmin , the upper-bound obtained by Algorithm 2 is better than the upper-bound obtained by Algorithm 1. That implies that for high-diffusion times, using biased data might be helpful for learning, as the bias term (final term) decays with the amount of noise. Going back to equation 4, note that the switching point tmin depends on the distance dTV(pt, pt) that decays as shown in Theorem 4.2. Once this distance becomes small enough, our computations above suggest that we benefit from biased data. The classifier of Section 3.1, if optimal, exactly tracks the distance dTV(pt, pt) and, as result, tracks the switching point. n"
        },
        {
            "title": "5 Experiments",
            "content": "Controlled experiments to show utility from low-quality data. To verify our method, we first do synthetic experiments on artificially corrupted data. We use EDM [34] as our baseline, and we train networks on CIFAR-10 and FFHQ. For the first experiments, we only use the high-noise part of our Ambient-o method (Section 3.1). We underline that for all of our experiments, we only change the way we use the data, and we keep all the optimization and network hyperparameters as is. We compare against using all the data as equal (despite the corruption) and the filtering strategy of only training on the clean samples. For evaluation, we measure FID [28] with respect to the full uncorrupted dataset (which is not available during training). For the blurring experiments, we use Gaussian kernel with standard deviation σB = 0.4, 0.6, 0.8, 1.0, and we corrupt 90% of the data. We show some example corrupted images in Appendix Figure 10a. To perform the annotations for our method, we train blurry image vs clean image classifier under noise, as explained in Section 3.1. For the experiments in the main paper, we use balanced dataset for the training of the classifier. We ablate the effect of having fewer training samples for the classifier training in Appendix Section where we show that reducing the number of clean samples available for classifier training leads to small drop in performance. Once equipped with the trained classifier, each sample is annotated on its own based on the amount of noise that is needed to confuse the classifier (sample dependent annotation). We present our results in Table 2a. As shown, for all corruption strengths, Ambient Omni, significantly outperforms the two baseline methods. In the one to the last column of Table 2a, we further show the average annotation of the classifier. As expected, the average assigned noise level increases as the corruption intensifies. Ablations. We ablate the choice of using fixed annotation vs sample-adaptive annotations in Appendix Table 7. We find that using sample-adaptive annotations achieves improved results. Nevertheless, both annotation methods yield improvements over the training on filtered data and the training on everything baselines. To show that our method works for more corruption types, we perform an equivalent experiment with JPEG compressed data at different compression ratios and we achieve similar results, presented in Appendix Table 3. We ablate the impact of the amount of training data and the number of training iterations on the classifier annotations in Appendix Section D. We show results for motion blur (Figure 11 and Section B.1) and for the FFHQ dataset  (Table 4)  . Table 2: In controlled experiment with restricted access only to 10% of the clean dataset, our method of Ambient-o uses corrupted and out-of-distribution data to improve performance. (a) Gaussian blurred data at different levels. (b) Additional out-of-distribution data. Method Parameters Values (σB) Only Clean (10%) All data Ambient-o - 1.0 0.8 0.6 0.4 1.0 0.8 0.6 0.4 σmin tn - 0 2.84 1.93 1.38 0.22 FID 8.79 45.32 28.26 11.42 2.47 6.16 6.00 5.34 2.44 Source Data Additional Data Method Dogs (10%) Cats (10%) None Cats Cats Cats Cats Cats Procedural None Dogs Wildlife Fixed σ Fixed σ Fixed σ Fixed σ Classifier Classifier Classifier Classifier σmax tn 0.2 0.1 0.05 0.025 0.09 0.042 0.13 0. FID 12.08 11.14 9.85 10.66 12.07 8.92 10.98 5.20 5.11 4.89 Controlled experiments to show utility from out-of-distribution images. We now want to validate the method developed in Section 3.2 for leveraging crops from out-of-distribution data. To start with, 10 we want to find the mapping between diffusion times and the size of the receptive field required for an optimal denoising prediction. To do so, we take pre-trained denoising diffusion model and measure the denoising loss at given location as we increase the size of the context. We provide the corresponding plot in the Supplemental Figures 17, 15. The main finding is that while providing more context always leads to decrease in the average loss, for sufficiently small noise levels, the loss nearly plateaus before the full image context is provided. That implies that the perfect denoiser for given noise level only needs to look at localized part of the image. Equipped with the mapping between diffusion times and crop sizes, we now proceed to fun experiment. Specifically, we show that it is possible to use images of cats to improve generative model for dogs (!) and vice-versa. The cats here represent out-of-distribution data that can be used to improve the performance in the distribution of interest (in our toy example, dogs distribution). To perform this experiment, we train classifier that discriminates between cats and dog images by looking at crops of various sizes (Section 3.2). Figure 6 shows the predictions of an 8 8 crops-classifier for an image of cat, illustrating that there are number of crops that are misclassified as crops from dog image. We report results for this experiment in Table 2b and we observe improvements in FID arising from using out-of-distribution data. Beyond natural images, we show that it is even possible to use procedurally generated data from Shaders [6] to (slightly) improve the performance. Figure 21 shows an example of such an image and the corresponding predictions of crops classifier. Table 2b contains more results and ablations between annotating all the out-of-distribution at single noise level vs. sample-dependent annotations. Figure 6: Patch level probabilities for dogness in cat image. Takeaway 1: It is possible to use low-quality in-distribution images and high-quality out-of-distribution images to produce high-quality in-distribution images. Corruptions of natural datasets ImageNet results. Up to this point, our corrupted data has been artificially constructed to study our method in controlled setting. However, it turns out that even in real datasets such as ImageNet, there are images with significant degradations such as heavy blur, low lighting, and low contrast, and also images with fantastic detail, clear lightning, and sharp contrast. Here, the high-quality and the low-quality sets are not given and hence we have to estimate them. We opt to use the CLIP-IQA quality metric [64] to separate ImageNet into high-quality (top 10% CLIP-IQA) and low-quality (bottom 90% CLIP-IQA) sets. Figure 4 shows some of the top and bottom quality images according to our metric. Given the high-quality and low-quality sets, we are now back to the previous setting where we can use the developed Ambient-o methodology. We underline that there is rich literature regarding quality-assessment methods [66, 67, 47, 65] and the performance of our method depends on how the high-quality and the low-quality sets are defined, since the rest of the samples are annotated based on which set they are closer to. We use Ambient-o to refer to our method that uses low-quality data at high diffusion times (Section 5) and Ambient-o+crops to refer to the extended version of our method that uses crops from potentially low-quality images at low-diffusion times. Perhaps surprisingly, there are images in ImageNet that have lower global quality but high-quality crops that we can use for low-noise. We present results in Table 1, where we show the best FID [28] and FDDINOv2 obtained by different methods. We show the highest and lowest quality crops, alongside their associated full images, of ImageNet according to CLIP in Figure 5. As shown in the Table, our method leads to state-of-the-art FID scores, improving over the previous state-of-the-art baseline EDM-2 [35] at both the low and high parameter count settings. The benefits are more pronounced when we measure test FID as our method memorizes significantly less due to the addition of noise during the annotation stage of our pipeline (Section 3.1). Beyond FID, we 11 provide qualitative results in Figure 1 (bottom) and Appendix Figures 12, 13. We further show that the quality of the generated images measured by CLIP increased compared to the baseline in Appendix Table 5. The observed improvements are proof that the ability to learn from data with heterogeneous qualities can be truly impactful for realistic settings beyond synthetic corruptions typically studied in prior work. Takeaway 2: Real datasets contain heterogeneous samples. Ambient-o explicitly accounts for quality variability during training, leading to improved generation quality. Text-to-image results. For our final set of experiments, we show how Ambient-o can be used to improve the performance of text-to-image diffusion models. We use the code-base of MicroDiffusion [52], as it is open-data and trainable with modest compute ( 2 days on 8-H100 GPUs). Sehwag et al. [52] use four main datasets to train their model: Conceptual Captions (12M) [54], Segment Anything (11M) [40], JourneyDB (4.2M) [61], and DiffusionDB (10.7M) [68]. Of these four, DiffusionDB is of significantly lower quality than the others as it contains solely synthetic data from an outdated diffusion model. This presents an opportunity for the use of our method. Can we use this lower-quality data and improve the performance of the trained network? (a) DrawBench (b) PartiPrompts Figure 7: Assessing image quality with GPT-4o on DrawBench and PartiPrompts. We set σmin = 2 for all samples from DiffusionDB and σmin = 0 for all other datasets and we train diffusion model with Ambient-o. We note that we did not ablate this hyperparameter and it is quite likely that improved results would be obtained by tuning it or by training high-quality vs low-quality data classifier for the annotation. Despite that, our trained model achieves remarkable FID of 10.61 in COCO, significantly improving the baseline FID of 12.37  (Table 9)  . We present qualitative results in Figure 1 and GPT-4o evaluations on DrawBench and PartiPrompt in Figure 7. Ambient-o and baseline generations for different prompts can be found in Figure 1. As an additional ablation, we compared our method with the recipe of doing final fine-tuning on the highest-quality subset, as done in the works of [52, 12]. Compared to this baseline, our method obtained slightly worse COCO FID (10.61 vs 10.27) but obtained much greater diversity, as seen visually in Figure 8 and quantitatively through > 13% increases in DINO Vendi Diversity on prompts from DiffDB (3.22 vs 3.65.). This corroborates our intuition that data filtration leads to decreased diversity. Ambient-o uses all the data but can strike fine balance between high-quality and diverse generation. Takeaway 3: Ambient-o treats synthetic data as corrupted data. This leads to superior visual quality and increased diversity compared to only relying on real samples. 12 (a) \"the great battle of middle earth, unreal engine, trending on artstation, masterpiece\" (b) \"an abominable snowman trapped in ice by greg rutkowski\" Figure 8: Examples of mode collapse. Left: baseline model finetuned on high-quality subset. Right: Ambient-o model using all the data. As shown, finetuning decreases output diversity. (a) Measuring fidelity and prompt alignment of generated images on COCO dataset. (b) Measuring performance on the GenEval benchmark. Objects Method FID-30K () Clip-FD-30K () Clip-score () Method Overall Single Two Counting Colors Position Color attribution Baseline Ambient-o 12.37 10.61 10.07 9.40 0.345 0.348 Baseline Ambient-o 0.44 0.47 0.97 0. 0.33 0.40 0.35 0.36 0.82 0.82 0.06 0.11 0.14 0.14 Figure 9: Quantitative benefits of Ambient-o on COCO [43] zero-shot generation and GenEval [25]."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "Our work opens several avenues for improvement. On the theoretical side, we aim to establish matching lower bounds to demonstrate that learning from the mixture distribution becomes provably optimal beyond certain noise threshold. Algorithmically, while our method performs well under high-frequency corruptions, it remains an open question whether more effective training strategies could be used for different types of corruptions (e.g., masking). Moreover, real-world datasets often exhibit patch-wise heterogeneityfor example, facial regions are frequently blurred for privacy, leading to uneven corruption across image crops. We plan to investigate patch-level noise annotations to better capture this structure in future work. Computationally, the full-version of our algorithm requires the training of classifiers for annotations that increases the runtime. This overhead can be avoided by using hand-picked annotation times based on quality proxies as done in our synthetic data experiment. Finally, we believe the true potential of Ambient-o lies in scientific applications, where data often arises from heterogeneous measurement processes."
        },
        {
            "title": "7 Conclusion",
            "content": "Is it possible to get good generative models from bad data? Our framework extracts value from low-quality, synthetic, and out-of-distribution sources. At time when the ever-growing data demands of GenAI are at odds with the need for quality control, Ambient-o lights path for both to be achieved simultaneously."
        },
        {
            "title": "8 Acknowledgements",
            "content": "This research has been supported by NSF Awards CCF-1901292, ONR grants N00014-25-1-2116, N00014-25-1-2296, Simons Investigator Award, and the Simons Collaboration on the Theory of Algorithmic Fairness. The experiments were run on the Vista GPU Cluster through the Center for Generative AI (CGAI) and the Texas Advanced Computing Center (TACC) at UT Austin. Adrián Rodríguez-Muñoz is supported by the La Caixa Fellowship (LCF/BQ/EU22/11930084)."
        },
        {
            "title": "References",
            "content": "[1] Asad Aali, Marius Arvinte, Sidharth Kumar, and Jonathan Tamir. Solving inverse problems with score-based generative priors learned from noisy data. arXiv preprint arXiv:2305.01166, 2023. [2] Asad Aali, Giannis Daras, Brett Levac, Sidharth Kumar, Alex Dimakis, and Jon Tamir. Ambient diffusion posterior sampling: Solving inverse problems with diffusion models trained on corrupted data. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=qeXcMutEZY. [3] Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard Baraniuk. Self-consuming generative models go mad. arXiv preprint arXiv:2307.01850, 4:14, 2023. [4] Sina Alemohammad, Ahmed Imtiaz Humayun, Shruti Agarwal, John Collomosse, and Richard Baraniuk. Self-improving diffusion models with synthetic data. arXiv preprint arXiv:2408.16333, 2024. [5] Weimin Bai, Yifei Wang, Wenzheng Chen, and He Sun. An expectation-maximization algorithm for training clean diffusion models from corrupted observations. arXiv preprint arXiv:2407.01014, 2024. [6] Manel Baradad, Chun-Fu Chen, Jonas Wulff, Tongzhou Wang, Rogerio Feris, Antonio Torralba, and Phillip Isola. Procedural image programs for representation learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=wJwHTgIoE0P. [7] Ashish Bora, Eric Price, and Alexandros Dimakis. Ambientgan: Generative models from lossy measurements. In International conference on learning representations, 2018. [8] Zdravko Botev, Joseph Grotowski, and Dirk Kroese. Kernel density estimation via diffusion. 2010. [9] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215, 2022. [10] Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-degradation beyond linear diffusions: non-asymptotic analysis for DDIM-type samplers. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 44624484. PMLR, 7 2023. URL https://proceedings. mlr.press/v202/chen23e.html. [11] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81888197, 2020. [12] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, Matthew Yu, Abhishek Kadian, Filip Radenovic, Dhruv Mahajan, Kunpeng Li, Yue Zhao, Vladan Petrovic, Mitesh Kumar Singh, Simran Motwani, Yi Wen, Yiwen Song, Roshan Sumbaly, Vignesh Ramanathan, Zijian He, Peter Vajda, and Devi Parikh. Emu: Enhancing image generation models using photogenic needles in haystack, 2023. [13] Giannis Daras, Yuval Dagan, Alexandros Dimakis, and Constantinos Daskalakis. Consistent diffusion models: Mitigating sampling drift by learning to be consistent. arXiv preprint arXiv:2302.09057, 2023. [14] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alex Dimakis, and Peyman Milanfar. Soft diffusion: Score matching with general corruptions. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=W98rebBxlQ. [15] Giannis Daras, Kulin Shah, Yuval Dagan, Aravind Gollakota, Alex Dimakis, and Adam Klivans. Ambient diffusion: Learning clean distributions from corrupted data. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/ forum?id=wBJBLy9kBY. [16] Giannis Daras, Alexandros Dimakis, and Constantinos Daskalakis. Consistent diffusion meets tweedie: Training exact ambient diffusion models with noisy data. arXiv preprint arXiv:2404.10177, 2024. [17] Giannis Daras, Yeshwanth Cherapanamjeri, and Constantinos Costis Daskalakis. How much is noisy image worth? data scaling laws for ambient diffusion. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=qZwtPEw2qN. [18] Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diffusion for image restoration. arXiv preprint arXiv:2303.11435, 2023. [19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248255, 2009. doi: 10.1109/CVPR.2009.5206848. [20] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [21] Sander Dieleman. Diffusion is spectral autoregression, 2024. URL https://sander.ai/ 2024/09/02/spectral-autoregression.html. [22] Logan Engstrom, Andrew Ilyas, Benjamin Chen, Axel Feldmann, William Moses, and Aleksander Madry. Optimizing ml training with metagradient descent. arXiv preprint arXiv:2503.13751, 2025. [23] Damien Ferbach, Quentin Bertrand, Avishek Joey Bose, and Gauthier Gidel. Self-consuming generative models with curated data provably optimize human preferences. arXiv preprint arXiv:2407.09499, 2024. [24] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108, 2023. [25] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/2310.11513. [26] Sachin Goyal, Pratyush Maini, Zachary Lipton, Aditi Raghunathan, and Zico Kolter. Scaling laws for data filteringdata curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2270222711, 2024. [27] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. [28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [30] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive Mixtures of Local Experts. Neural Computation, 3(1):7987, March 1991. ISSN 0899-7667. doi: 10.1162/neco.1991.3.1.79. URL https://doi.org/10.1162/neco.1991.3.1.79. _eprint: https://direct.mit.edu/neco/article-pdf/3/1/79/812104/neco.1991.3.1.79.pdf. [31] Yiding Jiang, Allan Zhou, Zhili Feng, Sadhika Malladi, and Zico Kolter. Adaptive data optimization: Dynamic sample selection with scaling laws. arXiv preprint arXiv:2410.11820, 2024. [32] Mason Kamb and Surya Ganguli. An analytic theory of creativity in convolutional diffusion models. arXiv preprint arXiv:2412.20292, 2024. [33] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44014410, 2019. [34] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364, 2022. 15 [35] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In Proc. CVPR, 2024. [36] Sergey Kastryulin, Dzhamil Zakirov, and Denis Prokopenko. image quality assessment, 2019. PyTorch ImURL Open-source software available age Quality: Metrics and measure for https://github.com/photosynthesis-team/piq. at https://github.com/photosynthesis-team/piq. [37] Sergey Kastryulin, Jamil Zakirov, Denis Prokopenko, and Dmitry V. Dylov. Pytorch image quality: Metrics for image quality assessment, 2022. URL https://arxiv.org/abs/2208. 14818. [38] Varun Kelkar, Rucha Deshpande, Arindam Banerjee, and Mark Anastasio. Ambientflow: Invertible generative models from incomplete, noisy measurements. arXiv preprint arXiv:2309.04856, 2023. [39] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything, 2023. URL https://arxiv.org/abs/2304.02643. [41] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009. [42] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024. [43] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. [44] Yvette Lin, Angela Gao, and Katherine Bouman. Imaging an evolving black hole by leveraging shared structure. ICASSP, 2024. [45] Zeyuan Liu, Zhihe Yang, Jiawei Xu, Rui Yang, Jiafei Lyu, Baoxiang Wang, Yunjian Xu, and Xiu Li. Adg: Ambient diffusion-guided dataset recovery for corruption-robust offline reinforcement learning. arXiv preprint arXiv:2505.23871, 2025. [46] Haoye Lu, Qifan Wu, and Yaoliang Yu. SFBD: method for training diffusion models with noisy data. In Frontiers in Probabilistic Inference: Learning meets Sampling, 2025. URL https://openreview.net/forum?id=6HN14zuHRb. [47] Anish Mittal, Rajiv Soundararajan, and Alan Bovik. Making completely blind image quality analyzer. IEEE Signal processing letters, 20(3):209212, 2012. [48] William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https://arxiv.org/abs/2212.09748. [49] Guilherme Penedo, Hynek Kydlíˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. [50] François Rozet, Gérôme Andry, François Lanusse, and Gilles Louppe. Learning diffusion priors from observations by expectation maximization. arXiv preprint arXiv:2405.13712, 2024. [51] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 16 [52] Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, and Lingjuan Lyu. StretcharXiv preprint ing each dollar: Diffusion training from scratch on micro-budget. arXiv:2407.15811, 2024. [53] Kulin Shah, Alkis Kalavasis, Adam R. Klivans, and Giannis Daras. Does generation require memorization? creative diffusion models using ambient diffusion, 2025. [54] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1238. URL https://aclanthology.org/P18-1238/. [55] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. URL https://arxiv.org/abs/1701.06538. [56] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. arXiv preprint arXiv:2212.03860, 2022. [57] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. arXiv preprint arXiv:2305.20086, 2023. [58] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [59] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019. [60] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [61] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb: benchmark for generative image understanding, 2023. URL https://arxiv. org/abs/2307.00716. [62] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Josh Tenenbaum, Frédo Durand, Bill Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. Advances in Neural Information Processing Systems, 36:1234912362, 2023. [63] Antonio Torralba, Phillip Isola, and William Freeman. Foundations of computer vision. MIT Press, 2024. [64] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In AAAI, 2023. [65] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 25552563, 2023. [66] Zhou Wang, Eero Simoncelli, and Alan Bovik. Multiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, volume 2, pages 13981402. Ieee, 2003. [67] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4): 600612, 2004. [68] Zijie Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. arXiv preprint arXiv:2210.14896, 2022. [69] Yasi Zhang, Tianyu Chen, Zhendong Wang, Ying Nian Wu, Mingyuan Zhou, and Oscar Leong. Restoration score distillation: From corrupted diffusion pretraining to one-step high-quality generation. arXiv preprint arXiv:2505.13377, 2025."
        },
        {
            "title": "A Theoretical Results",
            "content": "A.1 Kernel Estimation Assumption A.1. The density is λ lipschitz. i=1 set of independent samples from density that satisfies Assumption A.1. Let ˆp Let {X (i)}n be the empirical density on those samples. We are interested in bounding the total variation distance between pσ := (0, σ2) and ˆpσ = ˆp (0, σ2). In particular, ˆpσ(x) = 1 nσ (cid:88) i=1 (cid:18) (i) σ (cid:19) , ϕ (A.1) eu2/2 is the Gaussian kernel. We want to argue that the TV distance between where ϕ(u) = 1 2π pσ and ˆpσ is small given sufficiently many samples n. For simplicity, lets fix the support of to be [0, 1]. We have: dTV(pσ, ˆpσ) = 1 2 (cid:90) 1 pσ(x) ˆpσ(x)dx = L1 (cid:88) (cid:90) (l+1)/L l=0 l/L pσ(x) ˆpσ(x)dx (A.2) Now let us look at one of the terms of the summation. (cid:90) (l+1)/L l/L pσ(x) ˆpσ(x)dx = (cid:90) (l+1)/L l/L pσ(x) pσ(l/L) + pσ(l/L) ˆpσ(x)dx (A.3) (cid:90) (l+1)/L l/L pσ(x) pσ(l/L)dx + (cid:90) (l+1)/L l/L pσ(l/L) ˆpσ(x)dx. (A.4) We first work on the first term. Using Lemma A.6: (cid:90) (l+1)/L l/L pσ(x) pσ(l/L)dx λ (cid:90) (l+1)/L l/L l/Ldx = λ 2L2 . (A.5) (A.6) Next, we work on the second term. (cid:90) (l+1)/L l/L pσ(l/L) ˆpσ(x)dx = (cid:90) (l+1)/L l/L pσ(l/L) ˆpσ(l/L) + ˆpσ(l/L) ˆpσ(x)dx (A.7) (cid:90) (l+1)/L l/L pσ(l/L) ˆpσ(l/L)dx + (cid:90) (l+1)/L l/L ˆpσ(l/L) ˆpσ(x)dx. (A.8) According to Lemma A.5, we have that ˆpσ is ˆλ = 1 2πe σ2 Lipschitz. Then, the second term becomes: (cid:90) (l+1)/L l/L ˆpσ(l/L) ˆpσ(x)dx ˆλ (cid:90) (l+1)/L l/L l/L xdx = ˆλ 2L2 . It remains to bound the following term (cid:90) (l+1)/L l/L pσ(l/L) ˆpσ(l/L)dx = pσ(l/L) ˆpσ(l/L) We will be applying Hoeffdings Inequality, stated below: (A.9) (A.10) 18 Theorem A.2 (Hoeffdings Inequality). Let Y1, ..., Yn be independent random variables in [a, b] with mean µ. Then, Pr (cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:88) i=1 (cid:12) (cid:12) (cid:12) Yi µ (cid:12) (cid:12) (cid:33) 2 exp (cid:0)2nt2/(b a)2(cid:1) . (A.11) Recall that ˆpσ can be written as ˆpσ(x) = 1 (cid:88) i=1 ϕ((X (i) x)/σ) σ = 1 (cid:88) i= Yi, (A.12) in terms of the random variables Yi := ϕ((X (i)x)/σ) (cid:104) . So, for any x, we have that: 0, 1 (cid:105) σ 2πσ2 . These random variables are supported in Pr (ˆpσ(x) E[ˆpσ(x)] t) 2 exp (cid:0)4πσ2nt2(cid:1) . (A.13) (cid:113) log(2L/δ) 4πσ2n Taking = probability at least 1 δ, for all {0, 1, . . . , 1}: and using the above inequality and the union bound, we have that, with ˆpσ(l/L) E[ˆpσ(l/L)] (cid:114) log(2L/δ) 4πσ2n . Let us now compute the expected value of ˆpσ(x). E[ˆpσ(x)] = (cid:34) 1 nσ (cid:88) (cid:20) ϕ = 1 nσ (cid:18) σ i=1 (cid:19) = (cid:90) 1 σ p(u)ϕ (cid:19)(cid:35) ϕ (cid:88) (cid:18) (i) σ i=1 (cid:18) (i) σ (cid:19)(cid:21) du (p (0, σ2))(x) = pσ(x). Combining equation A.14 and equation A.17, we get: ˆpσ(l/L) pσ(x) (cid:114) log(2L/δ) 4πσ2n . Putting everything together we have: dTV(pσ, ˆpσ) λ 2L + 1 2Lσ 2πe + (cid:114) log(2L/δ) 4πσ2n . Choosing = max{λ, 1} we get that: dTV(pσ, ˆpσ) 1 + 1 σ2n + (cid:114) log + log(1 λ) + log 2/δ σ2n . A.2 Evolution of parameters under noise (A.14) (A.15) (A.16) (A.17) (A.18) Proof of theorem 4.2: We will use the following facts: Fact 1 (Direct corollary of the optimal coupling theorem). There exists coupling γ of and Q, which samples pair of random variables (X, ) γ such that Prγ[X = ] = dTV(P, Q). Fact 2. For any x, Rd: dTV(N (x, σ2I), (y, σ2I)) y/2σ 19 Proof. The KL divergence between (µ1, Σ1) and (µ2, Σ2) is KL(N (µ1, Σ1), (µ2, Σ2)) = (cid:18) 1 2 tr(Σ1 2 Σ1) + (µ2 µ1)Σ1 2 (µ2 µ1) + log (cid:19) . Σ2 Σ1 Applying this general result to our case: KL(N (x, σ2I), (y, σ2I)) = (cid:18) y2 σ2 1 2 (cid:19) . We conclude by applying Pinskers inequality. corollary of Fact 2 and the optimal coupling theorem is the following: Fact 3. Fix arbitrary x, Rd. There exists coupling γx,y of (0, σ2I) and (0, σ2I), which samples pair of random variables (Z, ) γx,y such that Prγx,y [x + = + ] = y/2σ. Now let us denote by = (0, σ2I) and = (0, σ2I). To establish our claim in the theorem statement, it suffices to exhibit coupling γ of and which samples pair of random variables ( X, ) γ such that: Prγ[ = ] dTV(P, Q) 2σ . We define coupling γ as follows: 1) sample (X, ) γ (as specified in Fact 1), 2) sample (Z, ) γX,Y (as specified in Fact 3) and 3) output ( X, ) := (X + Z, + ). Let us argue the following: Lemma A.3. The afore-described sampling procedure γ is valid coupling of and Q. Proof. We need to establish that the marginals of γ are and Q. We will only show that for ( X, ) γ according to the afore-described sampling procedure, the marginal distribution of is , as the proof for is identical. Since γ is coupling of and Q, for (X, ) γ, the marginal distribution of is . By Fact 3, conditioning on any value of and , the marginal distribution of is (0, σ2I). Thus, = + Z, where and independently (0, σ2I), and thus the distribution of is . Lemma A.4. Under the afore-described coupling γ: Prγ[ = ] dTV(P, Q) 2σ . Proof. Notice that, when = , by Fact 3, = with probability 1, and therefore = . So for event = to happen, it must be that = happens and, conditioning on this event, that + = + happens. By Fact 1, Prγ[X = ] = dTV(P, Q). By Fact 3, for any realization of (X, ), PrγX,Y [X + = + ] = XY 2σ , where we used that and are supported on set with diameter D. Putting the above together, the claim follows. 2σ (cid:50) A.3 Auxiliary Lemmas Lemma A.5 (Lipschitzness of the empirical density). For collection of points (1), . . . , (n) eu2/2 is the Gaussian consider the function ˆpσ(x) = 1 nσ , where ϕ(u) = 1 2π (cid:16) (i)x σ i=1 ϕ (cid:80)n (cid:17) kernel. Then pσ is (cid:16) (cid:17) 1 2πe σ2 -Lipschitz. Proof. Let us compute the derivative of ˆpσ: ˆp σ(x) = 1 nσ (cid:88) i=1 dx ϕ (cid:18) (i) σ (cid:19) = 1 2πnσ (cid:88) i=1 (cid:16) exp (X (i) x)2/(2σ2) (cid:17) (i) σ 1 2πσ2 max exp(u2/2)u 1 2πe . σ2 (A.19) (A.20) (A.21) (A.22) Lemma A.6 (Lipschitzness of density convolved with Gaussian). Let be density that is λ-Lipschitz. Let pσ = (0, σ2I). Then, pσ is also λ-Lipschitz. Proof. Let us denote with ϕσ() the Gaussian density with variance σ2. We have that: pσ(x) pσ(y) = (cid:90) (p(x τ ) p(y τ ))ϕσ(τ )dτ pσ(x) pσ(y) (cid:90) p(x τ ) p(y τ )ϕσ(τ )dτ λx (cid:90) ϕσ(τ )dτ = λx y. (A.23) (A.24) (A.25) (A.26)"
        },
        {
            "title": "B Additional Results",
            "content": "B.1 CIFAR-10 controlled corruptions Figures 10a,11,10b show gaussian blur, motion blur, and JPEG corrupted CIFAR-10 images respectively at different levels of severity. Appendix Table 3 shows results for JPEG compressed data at different levels of compression. We also tested our method for motion blurred data with high severity, visualized in the last row of Appendix Figure 11), obtaining best FID of 5.85 (compared to 8.79 of training on only the clean data). (a) CIFAR-10 images corrupted with blur at increasing levels (σB = 0.4, 0.6, 1.0). (b) CIFAR-10 images corrupted with JPEG at compression rates: 25%, 18%, 15% respectively. 21 Figure 11: CIFAR-10 images corrupted with motion blur at increasing levels of corruption. Table 3: Results for learning from JPEG compressed data on CIFAR-10. Method Dataset Clean (%) Corrupted (%) JPEG Compression (Q) Only Clean CifarAmbient Omni Cifar-10 10 10 0 90 15% 18% 25% 50% 75% 90% σmin tn 1.60 1.40 1.27 1.03 0.81 0.63 FID 8.79 6.67 6.43 6.34 5.94 5.57 4.72 B.2 FFHQ-64x64 controlled corruptions In Appendix 4 we show additional results for learning from blurred data on the FFHQ dataset. Similarly to the main paper, we observe that our Ambient-o algorithm leads to improvements over just using the high-quality data that are inversely proportional to the corruption level. Table 4: Results for learning from blurred data, FFHQ. Method Dataset Clean (%) Corrupted (%) Parameters Values (σB) Only Clean FFHQ Ambient Omni FFHQ 10 10 10 10 0 90 90 90 - 0.8 0.6 0. σmin tn - 2.89 2.12 0.63 FID 5.12 4.95 4.65 3.32 B.3 ImageNet results In the main paper, we used FID as way to measure the quality of generated images. However, FID is computed with respect to the test dataset that might also have samples of poor quality. Further, during FID computation, quality and diversity are entangled. To disentangle the two, we generate images using the EDM-2 baseline and our Ambient-o model and we use CLIP to evaluate the quality of the generated image (through the CLIP-IQA metric implemented in the PIQ package [37, 36]). We present results and win-rates in Table 5. As shown, Ambient-o achieves better per-image quality compared to the baseline despite using exactly the same model, hyperparameters, and optimization algorithm. The difference comes solely from better use of the available data. 22 Table 5: Additional comparison between EDM-2 XXL and our Ambient-o model using the CLIP IQA metric for image quality assesment. Ambient-o leads to improved scores despite using the exact same architecture, data and hyperparameters. For this experiment, we use the models with guidance optimized for DINO FD since they are the ones producing the higher quality images. Metric EDM-2 [35] XXL Ambient-o XXL crops"
        },
        {
            "title": "Average CLIP IQA score\nMedian CLIP IQA score",
            "content": "Win-rate 0.69 0.79 47.98% 0.71 0.80 52.02%"
        },
        {
            "title": "C Ambient diffusion implementation details and loss ablations",
            "content": "Similar to the EDM-2 [35] paper, we use pre-condition weight to balance the importance of different diffusion times. Specifically, we modulate the EDM2 weight λ(σ) by factor: λamb(σ, σmin) = σ4/(σ2 σ2 min)2 (C.1) for our ambient loss based on similar analysis to [35]. We further use buffer zone around the annotation time of each sample to ensure that the loss doesnt have singularities due to divisions by 0. We ablate the precondition term and the buffer size in Appendix Table 6. Table 6: Ablation study of ambient weight and stability buffer on Cifar-10 with 10% clean data and 90% corrupted data with blur of 0.6. Method No ambient preconditioning weight and no buffer: λamb(σ, σmin) = 1 & σ > σmin Adding ambient preconditioning weight: + Weight λamb(σ, σmin) = σ4/(σ2 σ2 min)2 Adding stability buffer/clipping: + Clip λamb(σ, σmin) at 2.0 + Clip λamb(σ, σmin) at 4.0 + Buffer λamb(σ, σmin) at 2.0 i.e. σ > + Buffer λamb(σ, σmin) at 4.0 i.e. σ > (2/ 2σmin 3)σmin FID 5.49 5.36 5.35 5.69 5.40 5.34 For our ablations, we focus on the setting of training with 10% clean data and 90% corrupted data with Gaussian blur of σB = 0.6. Using no ambient pre-conditioning and no buffer, we obtain an FID of 5.56. In the same setting, adding the ambient pre-conditioning weight λamb(σ, σmin) improves FID by 0.13 points. Next, we ablate two strategies to mitigate the impact of the singularity of λamb(σ, σmin) at σ = σmin. The first strategy clips the ambient pre-conditioning weight at specified maximum value λMAX amb , but still trains for σ arbitrarily close to σmin. The second strategy also specifies maximum value, but imposes buffer (cid:115) σ > 1 + 1 λMAX amb 1 σmin (C.2) that restricts training to noise levels σ such that λamb(σ, σmin) λMAX amb . Clipping the ambient weight to λMAX amb = 2.0 minimally improves FID to 5.35, but clipping to 4.0 significantly worsens it to 5.69. Adding buffer at λMAX amb = 2.0 slightly worsens FID to 5.40, but slackening the buffer to 4.0 minimally improves FID to 5.34. We opt for the buffering strategy in favor of the clipping strategy since performance appears convex in the buffer parameter, and because it obtains the best FID."
        },
        {
            "title": "D Classifier annotation ablations",
            "content": "Balanced vs unbalanced data: We ablate the impact of classifier training data on the setting of CIFAR-10 with 10% clean data and 90% corrupted data with gaussian blur with σB = 0.6. When 23 annotating with classifier trained on the same unbalanced dataset we train the diffusion model on we obtained best FID of 6.04, compared to the 5.34 obtained if we train on balanced dataset. Training iterations: We ablate the impact of classifier training iterations on the setting of CIFAR-10 with 10% clean data and 90% corrupted data with JPEG compression at compression rate of 18%, training the classifier with balanced dataset. We report minute variations in the best FID, obtaining 6.50, 6.58, and 6.49 when training the classifier for 5e6, 10e6, and 15e6 images worth of training respectively. Table 7: Comparison with baselines for training with data corrupted by Gaussian Blur at different levels. The dataset used in this experiment is CIFAR-10. Method Only Clean No annotations Single annotation Classifier annotations Clean (%) Corrupted (%) Parameters Values (σB) 10 10 10 10 10 0 90 90 90 90 - 1.0 0.8 0.4 1.0 0.8 0.4 1.0 0.8 0.4 σmin tn - 0 2.32 1.89 0.00 2.84 1.93 0.22 FID 8.79 45.32 28.26 2. 6.95 6.66 2.47 6.16 6.00 2."
        },
        {
            "title": "E Training Details",
            "content": "E.1 Formation of the high-quality and low-quality sets. In the theoretical problem setting we assumed the existence of good set SG from the clean distribution and bad set SB from the corrupted distribution. In practice, we do not actually possess these sets initially, but we can construct them so long as we have access to measure of \"quality\". Given function on images which tells us wether its good enough to generate or not e.g. CLIP-IQA quality [64] greater than some threshold, we can define our good set SG as the good enough images and SB as the complement. From this point on we can apply the methodology of ambient-o as developed, either employing classifier annotations as in our pixel diffusion experiments, or fixed annotations as in our large scale ImageNet and text-to-image experiments. E.2 Datasets CIFAR-10. CIFAR-10 [41] consists of 60,000 32x32 images of ten classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck). FFHQ. FFHQ [33] consists of 70,000 512x512 images of faces from Flickr. We used the dataset at 64x64 resolution for our experiments. AFHQ. AFHQ [11] consists of 5,653 images of cats, 5,239 images of dogs and 5,000 images of wildlife, for total of 15,892 images. ImageNet. ImageNet [19] consists of 1,281,167 images of variable resolution from 1000 classes. Conceptual Captions. Conceptual Captions [54] consists of 12M (image url, caption) pairs. Segment Anything. Segment Anything [40] consists of 11.1M high-resolution images annotated with segmentation masks. Since the original dataset did not have real captions, we use the same LLaVA generated captions created by the MicroDiffusion [52] paper. JourneyDB. JourneyDB consists of 4.4M synthetic image-caption pairs from Midjourney [61]. 24 DiffusionDB. DiffusionDB consists of 14M synthetic image-caption pairs, mostly generated from Stable Diffusion models [68]. We use the same 10.7M quality-filtered subset created by the MicroDiffusion paper [52]. E.3 Diffusion model training CIFAR-10. We use the EDM [34] codebase as reference to train class-conditional diffusion models on CIFAR-10. The architecture is Diffusion U-Net [58] with 55M paramemeters. We use the Adam optimizer [39] with learning rate 0.001, batch size 512, and no weight decay. While the original EDM paper trained for 200 106 images worth of training, when training with corrupted data we saw best results around 20 106 images. On single 8xV100 node we achieved throughput of 0.8s per 1k images, for an average of 4.4h per training run. FFHQ. Same as for CIFAR-10, except learning was set to 2e 4, we trained for maximum of 100 106 images worth of training, and saw best results around 30 106 images worth. AFHQ. Same as FFHQ. ImageNet. We use the EDM2 [35] codebase as reference to train class-conditional diffusion models on ImageNet. The architecture is Diffusion U-Net [58] with 125M paramemeters. We use the Adam optimizer [39] with reference learning rate 0.012, batch size 2048, and no weight decay. Same as the original codebase, we trained for 2B worth of images. On 32 H200 GPUs, XS models took 3 days to train, while XXL models took 7 days. MicroDiffusion. We use the MicroDiffusion codebase [52] as reference to train text-to-image models on an academic budget. We follow their recipe exactly, changing only the standard denoising diffusion loss to the ambient diffusion loss. The architecture is Diffusion Transformer [48] utilizing Mixture-of-Experiments (MoE) feedforward layers [55, 30], with 1.1B paramemeters. We use the AdamW optimizer [39] with reference learning rates 2.4e 4/8e 5/8e 5/8e 5 for each of the four phases and batch size 2048 for all phases. On 8 H200 GPUs, training takes 2 days to train. E.4 Classifier training Classifier training is done using the same optimization recipe (optimizer, learning rate, batch size, etc.) as diffusion model training, except we change the architecture to an encoder-only \"Half-Unet\", simply by removing the decoder half of the original UNet architecture. The training of the classifier is substantially shorter compared to the diffusion training since classification is task is easier than generation."
        },
        {
            "title": "F Additional Figures",
            "content": "25 Figure 12: Uncurated generations from our Ambient-o XXL model trained on ImageNet. 26 Figure 13: Uncurated generations from our Ambient-o+crops XXL model trained on ImageNet. 27 Figure 14: Amount of samples available at each noise level when training generative model for dogs in the following setting: (1) we have 10% of the dogs dataset uncorrupted, (2) we have the other 90% of the dogs dataset corrupted with gaussian blur with σB = 0.6, and (3) we have 100% of the clean dataset of cats. At low noise levels, we can train on both the high quality dogs and lot of the cats, resulting in > 100% of samples available relative to the original dogs dataset size. As the noise level starts to increase, we stop being able to use to the out-of-distribution cat samples, but start gaining some blurry dog samples. As the noise level approaches the maximum all the blurry dogs become available for training, such that the amount of data available approaches 100%. Figure 15: ImageNet-512x512: denoising loss of an optimally trained model, measured at 2 2 center patch, as we increase the context size given to the model (horizontal axis) and the noise level (different curves). As expected, for higher noise, more context is needed for optimal denoising. The large dot on each curve marks the point where the loss nearly plateaus. 28 Figure 16: ImageNet-512x512: context size needed to be within ϵ = 1e 3 of the optimal loss for different noise levels. As expected, for higher noise, more context is needed for optimal denoising. Figure 17: FFHQ: denoising loss of an optimally trained model, measured at 2 2 center patch, as we increase the context size given to the model (horizontal axis) and the noise level (different curves). As expected, for higher noise, more context is needed for optimal denoising. The large dot on each curve marks the point where the loss nearly plateaus. 29 Figure 18: FFHQ: context size needed to be within ϵ = 1e 3 of the optimal loss for different noise levels. As expected, for higher noise, more context is needed for optimal denoising. (a) Cat image and classification probabilities over patches. (b) Cat image and classification probabilities over patches. Figure 19: Two examples of cats from the AFHQ dataset. We partition each cat into non overlapping patches and we compute the probabilities of the patch belonging to an image of dog using cats vs dogs classifier trained on patches. The cat on the right has lot more patches that could belong to dog image according to the classifier, possibly due to the color or the texture of the fur. (a) Cat annotated by cats vs. dogs classifier that operates with crops of size 8. (b) Cat annotated by cats vs. dogs classifier that operates with crops of size 16. (c) Cat annotated by cats vs. dogs classifier that operates with crops of size 24. Figure 20: Patch-based annotations of cat image from AFHQ using cats vs. dogs classifiers trained on different patch sizes. 30 Figure 21: Patch level probabilities for dogness in synthetic image (procedural program). The cat has more useful patches than this non-realistic procedural program. (a) Synthetic image and classification probabilities over patches. (b) Synthetic image and classification probabilities over patches. Figure 22: Two examples of procedurally generated images. We partition each image into non overlapping patches and we compute the probabilities of the patch belonging to an image of dog using synthetic image vs dogs classifier trained on patches. The image on the right has lot more patches that could belong to dog image according to the classifier, possibly due to the color or the texture. (a) Cat image and classification probabilities over patches. (b) Cat image and classification probabilities over patches. Figure 23: Two examples of cat images. We partition each image into nonoverlapping patches and we compute the probabilities of the patch belonging to an image of wildlife using cats vs wildlife classifier trained on patches. The image on the right has lot more patches that could belong to wildlife image according to the classifier, possibly due to the color or the texture. (a) Example batch. (b) Noisy batch. Figure 24: Example batch. (a) Highest quality images from CC12M according to CLIP. (b) Lowest quality images from CC12M according to CLIP. Figure 25: CLIP annotations for quality of images from CC12M. (a) Highest quality images from SA1B according to CLIP. (b) Lowest quality images from SA1B according to CLIP. Figure 26: CLIP annotations for quality of images from SA1B. 32 (a) Highest quality images from DiffDB according to CLIP. (b) Lowest quality images from DiffDB according to CLIP. Figure 27: CLIP annotations for quality of images from DiffDB. (a) Highest quality images from JDB according to CLIP. (b) Lowest quality images from JDB according to CLIP. Figure 28: CLIP annotations for quality of images from JDB. 33 Figure 29: Distribution of image qualities according to CLIP for ImageNet-512."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "The University of Texas at Austin"
    ]
}