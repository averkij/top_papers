{
    "paper_title": "Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading",
    "authors": [
        "Cfir Avraham Hadar",
        "Omer Shubi",
        "Yoav Meiri",
        "Yevgeni Berzak"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 2 7 8 2 0 . 5 0 5 2 : r Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading Cfir Avraham Hadar, Omer Shubi*, Yoav Meiri, Yevgeni Berzak Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel {kfir-hadar,shubi,meiri.yoav}@campus.technion.ac.il berzak@technion.ac.il"
        },
        {
            "title": "Abstract",
            "content": "When reading, we often have specific information that interests us in text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question but does it work?. More broadly, in daily life, people approach texts with any number of textspecific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers text-specific goals from eye movements."
        },
        {
            "title": "Introduction",
            "content": "Eye movements in reading are key methodology for studying the cognitive basis of reading and language processing. When we read, our eyes move over the text in saccadic manner, with prolonged periods of time (typically about 200-250ms) in which the gaze is stable, called fixations, and fast (about 30ms) transitions between fixations called saccades (Schotter and Dillon, 2025). This trajectory and the durations of the individual fixations are rich behavioral traces of the ways in which readers interact with texts and process language in real time (Rayner, 1998; Just and Carpenter, 1980). *Equal contribution. 1Code is available anonymously here. Understanding the nature and strength of the relation between eye movements, the text, and online language processing, has been major research avenue in the psychology of reading and psycholinguistics in the past few decades, and in recent years has also been gaining increasing interest in NLP and AI (Barrett and Hollenstein, 2020; Beinborn and Hollenstein, 2024). In this work, we address reading scenario that is prevalent in daily life but remains understudied seeking specific information in text. This scenario deviates from common implicit assumption in psycholinguistics, according to which the comprehenders goal is constant across communicative contexts: general understanding of the linguistic input. However, in real life, readers approach texts with variety of goals and information seeking needs. Each such goal can have profound impact on the cognitive processes of language comprehension and the corresponding behavioral traces of eye movements over text (Radach and Kennedy, 2004; Hahn and Keller, 2023; Shubi and Berzak, 2023). Our study focuses on the following question: can arbitrary, text-specific information goals be automatically decoded from eye movements in reading? Specifically, given single participant reading single passage with question in mind that they would like to answer via the text, can this question be decoded from their eye movements over the passage? This task has not been addressed in prior research and has both scientific and practical importance. Scientifically, it allows obtaining insights into the strength and nature of the relation between task conditioning and reading behavior, and more broadly, the extent to which eye movements contain information on the cognitive state of the reader. Practically, it opens the door for applications in education, content personalization, and text accessibility, that will rely on detecting information seeking behavior, its specific purpose, and the degree of its effectiveness. In this work, we introduce two formulations of the goal decoding task, classification and reconstruction. In the classification task, given several candidate questions over paragraph, we examine whether it is possible to predict which of these questions was provided to the reader from their eye movements over the paragraph. We examine two variants of this task with increasing difficulty: distinguishing between (i) questions about different portions of the passage, and (ii) questions over the same passage portion. In the reconstruction task, we go even further by attempting to generate the precise question. We tackle these tasks using existing and new LLM-based models that combine eye movements and text. We find that the best performing model on the classification task is able to predict the correct question with considerable degree of success in both task variants. Our results further suggest that although the reconstruction task is extremely challenging, meaningful progress can be made even in this open-ended regime. Our key contributions are the following: 1. Task We present new cognitive state decoding challenge from eye movements in reading: decoding arbitrary information seeking goals over the content of specific texts. We instantiate this challenge as classification task given several possible goals, and as an open-ended goal reconstruction task. 2. Models We adjust two discriminative models to the classification task and introduce two new generative models that combine text with eye movements in reading and can be used for both classification and reconstruction. 3. Evaluations We perform systematic evaluations of classification accuracy against informative baseline models in two task difficulty levels, and use several generation quality measures for the reconstruction task. For both tasks, we examine model generalization across new textual items, new readers, and the combination of both."
        },
        {
            "title": "2 Eye Tracking Data",
            "content": "Our work is enabled by OneStop Eye Movements (Berzak et al., 2025, henceforth OneStop) dataset of eye movements in reading, with 360 adult native English speakers reading Guardian articles from the 30-article OneStopQA dataset (Berzak et al., 2020). The data was collected with an EyeLink 1000 Plus eye tracker (SR Research). In each paragraph reading trial, participant reads paragraph on screen, and on the following screen answers multiple-choice reading comprehension question with 4 possible answers, without the ability to return to the paragraph. 180 participants read in an information seeking condition, where they receive the question (but not the answers) before reading the paragraph, and therefore have specific reading goal for the upcoming paragraph. The remaining 180 participants do not receive the question ahead of time, and thus have to be ready to answer any subsequent question. In both conditions, single participant reads one of three 10-article batches, each with 54 paragraphs. Due to the nature of the task, here, we focus on the information seeking part of the data. In each trial, participant receives one of three possible questions over the paragraph. Each such question has manually annotated critical span in the paragraph which contains the information that is essential for answering the question correctly. Two of the questions share the same critical span, while the third question has distinct critical span. This design supports question classification task with two tiers of difficulty, distinguishing between questions with different critical spans and the more challenging variant of distinguishing between questions that share the same critical span. The critical spans are counterbalanced across the dataset such that spans that share two questions appear an equal number of times before and after spans that have one question. Figure 1 presents an example of paragraph, its three questions, and their corresponding critical span annotations. The information seeking portion of the data has 486 questions and 20 participants answering each question. The average paragraph length is 109 words, out of which the critical span is 32 words. The average question length is 10 words. Overall, there are 1,055,429 word tokens in the paragraphs over which eye tracking data was collected."
        },
        {
            "title": "3 Problem Formulation",
            "content": "Each paragraph has two distinct critical spans, c1 and c2, and set of three questions that can be answered based on the paragraph, QP = (cid:9). The critical information for an- (cid:8)qP 2,c2 1,c1 swering qP 1,c1 is located in span c1, while for both qP 2,c2 and qP 3,c2 it is in span c2. In each experiment trial, participant is presented with one question, , qP , qP 3,c2 Figure 1: Example of textual item from OneStopQA: paragraph with three questions. Two of the questions have the same critical span: the paragraph segment essential for answering the question. qP QP , before reading the paragraph . The recording of the participants eye movements while reading the paragraph is denoted by EP . Question Classification The classification task is to select which question, from given set of questions specific to the paragraph, the participant received. We train classifier that predicts the question that was presented to the participant prior to reading the paragraph from their eye movement recording over the paragraph: h(P, QP , EP ) ˆqP QP Question Reconstruction In the reconstruction task, models are trained to generate the question that the participant received from their eye movement recording over the paragraph: g(P, EP ) ˆqP where is generative model whose output is natural language question."
        },
        {
            "title": "4 Models",
            "content": "We experiment with discriminative and generative modeling approaches. Figure 2 presents schematic view of these approaches, which are further outlined below. 4.1 Discriminative Models Our discriminative models are adaptations of stateof-the-art encoder models for eye movements in reading to our task. Building on the approach used in Radford et al. (2019) for encoding multiplechoice reading comprehension items, we use several copies of each paragraph, where instead of combining each copy with candidate answer, we combine it with candidate question, and further provide the eye movements over the paragraph P, qP , EP . The models assign probability to each triplet, and classification is performed by selecting the highest probability question. Figure 2: An overview of the two types of models presented in the paper. On the left, discriminative models (Haller RNN and RoBERTEye-Fixations), that score each candidate question and are evaluated on classification accuracy. On the right, generative models (DalEyeLLaVA and DalEye-Llama), that are trained to reconstruct the question presented to the reader, and are evaluated both on classification accuracy and reconstruction quality. Further details and diagrams of the models are presented in Appendix E. Haller RNN (Haller et al., 2022) An RNN-based model that orders words based on the fixation sequence and encodes each fixation using eye movement features. We adapt this model by appending question embeddings to the input and processing them through the RNN, followed by classifier scoring each candidate question. The resulting architecture is depicted in Appendix E.3. RoBERTEye-Fixations (Shubi et al., 2024b) transformer-based model which integrates fixationlevel eye movement features into RoBERTa. We adapt it to incorporate candidate question embeddings and modify the classification layer to score questions. Appendix E.4 presents further details and diagram of the adapted architecture. 4.2 Generative Models We introduce two LLM-based approaches for generating the question based on the eye movements over the paragraph. Both models are trained using an autoregressive next-token prediction objective for the correct question with cross-entropy loss. The language models parameters remain frozen during training, and we fine-tune only an additional LoRA (Hu et al., 2022) adapter component. As described in Section 5, we evaluate these models both on the question classification and the question reconstruction tasks. DalEye-LLaVA is based on LLaVA-OneVision (Li et al., 2024a), vision-language model that uses the Qwen 2-0.5B language model (Yang et al., 2024) as backbone. The model input consists of task prompt, the paragraph text, and eye movement feature embeddings for the paragraph words. We replace the vision encoder of LLaVA-OneVision with novel trainable eye movement encoder, which encodes eye movement features using fully connected and convolutional layers. The eye movements embeddings are positionally aligned with the paragraph words. diagram of the model and further details are provided in Appendix E.5. DalEye-Llama In this approach, the eye movement trajectory over the paragraph is provided to the Llama 3.1 (Grattafiori et al., 2024) LLM using textual representation. Each fixation is represented as triplet containing the index of the fixated word, the word itself, and the fixation duration in milliseconds. This encoding preserves both the spatial locations and temporal order of the fixations within the paragraph. The resulting textual representation is combined with prompt instructing the model to generate the question that was presented to the reader. An example of the model input for an entire fixation sequence over paragraph, including the task prompt, is provided in Appendix E.6. 4.3 Baseline Models We introduce two baseline models which use eye movements for the classification task. These models build on the following observations from prior literature: (i) readers spend more time on questionrelevant than on question irrelevant portions of the text (Hahn and Keller, 2023; Malmaud et al., 2020; Shubi and Berzak, 2023), and (ii) question-relevant portions of the text have higher semantic similarity to the question compared to question-irrelevant portions of the text (Mitra et al., 2016). Question Similarity to RT-Weighted Passage For participant reading paragraph of words, we compute paragraph embedding emb(p) as weighted average of the paragraph word embeddings, where the weights are reading times (RT) on each word: emb(p) = (cid:88) i=1 RT(wi) emb(wi) where emb(wi) is the contextual word embedding of word wi, extracted from RoBERTa (Liu et al., 2019), and RT(wi) is the Total Fixation Duration (i.e. sum of all the fixation durations) on the word normalized by total reading time of the paragraph. For each candidate question qj, we use the embedding of the [CLS] token as emb(qj), and compute its cosine similarity with emb(p). We then predict the question with the highest similarity: arg maxj cos(emb(p), emb(qj)). RT Similarity to Question-Word Similarities Here we provide an alternative to the model above, where we compare the vector of word reading times across the paragraph to the vector of wordquestion similarities. Specifically, for each candidate question qj, we first compute: sim(wi)qj = cos(cid:0)emb(qj), emb(wi)(cid:1) where as before, emb(wi) is contextual word embedding from RoBERTa, and emb(qj) is the question [CLS] embedding, yielding length-N vector simqj p. We select the question maximizing the cosine similarity between this vector and RT(p), the length-N vector of Total Fixation Durations for the paragraph words: arg maxj cos(cid:0)simqj p, RT(p)(cid:1)."
        },
        {
            "title": "5 Experimental Setup and Evaluation",
            "content": "5.1 Data Splits Following prior work on predictive modeling with OneStop (e.g. Shubi et al., 2024b), we use 10fold cross-validation procedure where each of the 10 data splits has training, validation, and test set. The validation set and the test set are each partitioned into three disjoint parts that capture three different levels of model generalization, ordered by task difficulty: New Participant: During training, eyetracking data is available for the paragraph but not for the participant. New Item: Training eye-tracking data is available for the participant but not for the paragraph. New Item & Participant: Neither the participant nor the paragraph was in the training data. Each data split includes 64% of the trials in the training set, 17% in the validation set, and 19% in the test set divided into 9% New Participant, 9% New Item, and 1% New Item & Participant. When aggregated across the 10 splits, 90% of the trials appear in both New Participant and New Item evaluation regimes, and 10% appear in the New Item & Participant regime. We ensure that in each split into training, validation, and test sets, there is balanced distribution of question types, i.e. q1,c1, q2,c2, q3,c2 for each text in each part of the split. Hyperparameter optimization and model selection are conducted separately for each split. We assume that the evaluation regime with respect to novelty of items and participants at test time is unknown, and therefore model selection is performed using the entire validation set within each split. Further details on the hyperparameter search space, training, and evaluation procedures are provided in Appendix A. 5.2 Evaluation 5.2.1 Question Classification We evaluate all the models on the question classification task and report classification accuracy. As described in Section 4, the discriminative models assign probability to triplet P, qP , EP , and classification is performed by selecting the candidate question with the highest probability. For the generative models, we select the question with the highest model assigned log-likelihood. In addition to the overall three-way classification accuracy, we also report breakdown of the model predictions into two fine-grained evaluations: Different critical spans This evaluation tests the models ability to distinguish between questions with different critical spans, reducing the classification to binary decision between qP 1,c1 and {qP }. Note that random choice accuracy in this evaluation is approximately 55%, rather than 50%, due to Monty Hall-type advantage arising from grouping two questions under one critical span (see visualization in Figure 4 of Appendix B). , qP 2,c2 3,c2 Same critical span This is more challenging evaluation for differentiating between the two questions for c2: qP 3,c2. Note that we disregard the question whose critical span is c1. The corresponding chance accuracy is 50%. 2,c2 versus qP 5.2.2 Question Reconstruction The generative models are further evaluated for the quality of the reconstructed questions. As with other generation tasks, evaluating reconstruction quality is major challenge. Here, we use several heuristic evaluations for both the surface form and semantic content of the generated questions. The question word We check if the generated question word matches the question word of the ground truth question. The possible question words are What, When, Where, Who, Whom, Which, Whose, Why, and How. An additional category is Other, reserved for all other words starting question (7% of all the questions in the data). The agreement with the ground truth questions is measured using pairwise Cohens Kappa (Cohen, 1960). BLEU The BLEU surface similarity score (Papineni et al., 2002) between the generated question and the ground truth question. UIUC question category We check if the generated question is of the same semantic category as the ground truth question. To this end, we use the question-type taxonomy suggested by Li and Roth (2002). We use the main categories of the taxonomy: entities, humans, numeric and locations, and the subcategories of the category descriptions: manner, reasoning, definition and description due to their high frequency in the data. We use GPT-4o (Hurst et al., 2024) to categorize the questions. Similarly to question words, we measure agreement in category assignments using pairwise Cohens Kappa. Examples and further details on the classification process, including agreement with human annotations, are provided in Appendix D. BertScore (Zhang et al., 2020) provides cosine similarity score between the generated and ground truth questions based on contextual embeddings extracted from RoBERTa. QA accuracy We further propose downstream evaluation method with the following logic: the closer the generated question is to the true question, the easier it should be to select the correct answer for the true question when replaced with the generated question. We implement this evaluation using RoBERTa Large model fine-tuned for multiple choice QA on the RACE reading comprehension dataset (Lai et al., 2017). This model was shown to have high accuracy of 86% on multiple choice QA on OneStopQA (Malmaud et al., 2020). We chose this model because OneStopQA is not in its training data, which is likely the case for newer models, thus avoiding possible data contamination. In this evaluation, we take into account only the questions that RoBERTa answers correctly given the ground truth question. We consider question to be valid reconstruction of the ground truth question, if and only if given this question, the paragraph, and the four answers for the ground truth question, the model selects the correct answer. Type Model All Different Spans Same Span Chance / Majority 33.0 0.4+++ 55.3 0.4+++ 49.9 0.4+++ Baselines Question similarity to RT-weighted passage RT similarity to question-word similarities Discriminative Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) Generative DalEye-LLaVA DalEye-Llama 33.4 0.3+++ 34.2 0.4 +++ 41.8 0.6 +++ 49.3 0.3 33.5 0.3+++ 38.1 0.2 +++ 55.1 3.7+++ 50.0 0.4+++ 54.8 1.4+++ 51.1 0.5+++ 65.6 0.5 +++ 70.9 0.5 57.0 0.7 +++ 60.3 0.7 +++ 52.1 0.5 +++ 57.3 0.5 49.4 0.4+++ 50.9 0.2+++ Table 1: Question classification test accuracy aggregated across 10 cross-validation splits, with 95% confidence intervals. All are the results for three-way classification, Different Spans for distinguishing questions with different critical spans, and Same Span for distinguishing questions with the same critical span, where the critical span is the information in the passage that is necessary for answering the question. majority question choice is identical to chance because the data is balanced across questions. As the data is not i.i.d (each participant reads multiple paragraphs, and each paragraph is read by multiple participants) we test for differences between models using linear mixed effects model with random intercepts and slopes for participants and paragraphs: is_correct model + (model participant) + (model paragraph) with the Julia MixedModels package (Bates et al., 2022). The accuracy of all the models is compared to chance performance, where significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked with + < 0.05, ++ < 0.01 and +++ < 0.001 in subscript. All five methods provide scores for the model generated questions. To facilitate the interpretation of the quality of these scores, we benchmark them against the following baseline questions. Arbitrary questions We generate two arbitrary questions for each paragraph. The first is question generated by GPT-4o, using the prompt in Appendix E.7. The second is question generated by finetuned Llama 3.1 model trained to decode the correct question without eye movement data. Incorrect questions The two additional humancomposed questions for each paragraph available in OneStopQA. We provide separate evaluations for the same critical span and the different critical span cases relative to the ground truth question."
        },
        {
            "title": "6 Results",
            "content": "6.1 Question Classification Table 1 presents the aggregated test accuracy across the 10 data splits for the three-way All classification, as well as breakdown into questions with Different Spans, and questions with the Same Span evaluations. division of the results into the New Item, New Participant, and New Item & Participant regimes are reported in Tables 2 to 4 in Appendix C. Validation set results are reported in Tables 5 to 8 in Appendix C. Baselines Both baseline models perform at chance level across all three evaluations. The reason for this failure likely lies in the inherent noise in the distribution of reading times over the paragraph, which deems heuristic methods that rely on this distribution to be ineffective. Models The discriminative models perform above chance, and outperform the generative models on all the evaluations. This outcome is not surprising, as differently from the discriminative models, the generative models were not explicitly trained for the question classification task. RoBERTEye-Fixations achieves the highest accuracy across all three evaluation regimes. Additionally, in Table 2 of Appendix we observe that this performance advantage is consistent across the New Item, New Participant and New Item & Participant evaluations. Notably, it is also the only model which is substantially above chance in the Same Span regime with 57.3% accuracy. This confirms that distinguishing between questions that ask about the same portion of the paragraph is indeed much more challenging than distinguishing between questions over different parts of the passage. At the same time, the modest success of RoBERTEye on the Same Span evaluation speaks to the fine-grained nature of the information that can be extracted in some cases about the readers information seeking goals. In the generative modFigure 3: Question reconstruction evaluations of the DalEye-Llama model for (1) the identity of the generated question word, (2) BLEU score (3) the UIUC semantic category of the question, (4) BERTScore, and (5) downstream QA accuracy based on the answer selection of multiple-choice question answering model. DalEye-Llama performance is benchmarked against two types of human-composed questions (for different critical span and for the same span) and against arbitrary questions generated with GPT-4o and Llama 3.1. Presented are means with 95% confidence intervals (bootstrapped, = 1000) for three generalization levels to new participants and new readers. els category, DalEye-Llama outperforms DalEyeLLaVA, where the latter is at chance level. 6.2 Question Reconstruction As the DalEye-LLaVA classification results are close to chance  (Table 1)  , we focus on evaluating the question reconstruction quality of the DalEyeLlama model. Figure 3 presents the evaluations, which exhibit largely consistent patterns across the different metrics. Table 9 in Appendix presents these results in numerical form. Importantly, in the New Participant evaluation, DalEye-Llama generated questions outperform both Arbitrary GPT-4o and Llama generated questions as well as Incorrect Human Questions in both question categories on all the five metrics. This suggests that DalEye-Llama generalizes relatively well to new participants when the textual item appeared in the training set. However, DalEye-Llama performance drops both in absolute terms and relative to the baselines in the more challenging regimes with New Items. Notably, the DalEye-Llama and the arbitrarily model-generated questions tend to perform worse than the incorrect human questions, which exhibit the highest similarity with the ground truth questions. Specifically, the incorrect human question - same critical span, has the highest BLEU, BERTScore, and QA accuracy, which is likely the result of sharing the critical span textual content with the ground truth question. The incorrect human question - different critical span, has the highest question word and question category similarities. This may be related to tendency of different questions over the same span not to have the exact same question word and type, whereas different human questions over different spans do not constrain each other in these regards. Figure 10 in appendix E.6 presents an example with paragraph, the ground truth question, four DalEye-Llama generated questions in the New Item regime and four in the New Participant regime, each leading to different answer choice by the QA model. We leave the generalization improvement of DalEye-Llama and DalEye-LLaVA as avenues for future work."
        },
        {
            "title": "7 Related Work",
            "content": "We build on three lines of work in NLP and psycholinguistics: (i) multimodal language models; (ii) goal driven reading; and (iii) predictive models of cognitive state from eye movements in reading. Multimodal Language Models Multimodal language models extend the capabilities of LLMs beyond text by incorporating additional input modalities (Wu et al., 2023) including visual data (Li et al., 2019, 2024a, among others), time-series signals (Gruver et al., 2023; Liang et al., 2024; Jin et al., 2023; Kawarada et al., 2024), and physiological and cognitive signalssuch as EEG and ECG (Hollenstein et al., 2021a; Li et al., 2024b; Mishra et al., 2024; Yang et al., 2025). Eye movements have been integrated into language models for various tasks (Tomanek et al., 2010b; Tokunaga et al., 2017a; Hollenstein et al., 2022; Shubi et al., 2024b, among others). These approaches typically use encoder-based models, leveraging features derived from the eye movement scanpath. Here, we depart from existing work by integrating eye movements in reading into generative, decoder-based language models, exploring both textand embedding-based methods for conditioning text generation on eye movements. Analyses of Goal Based Reading While most prior work in the psychology of reading focuses on ordinary reading for general comprehension (Rayner et al., 2012), goal or task based reading has been acknowledged as key frontier in this area (Radach and Kennedy, 2004). Several studies found differences in eye movements between ordinary reading and tasks that define reading manner: skimming, speed reading and proofreading (Just et al., 1982; Kaakinen and Hyönä, 2010; Schotter et al., 2014; Strukelj and Niehorster, 2018; Chen et al., 2023). Additional studies have found differences between ordinary reading and tasks of target word search (Rayner and Raney, 1996; Rayner and Fischer, 1996), word verification (Radach et al., 2008), and topic search (White et al., 2015). Prior work also analyzed eye movements during human linguistic tasks, such as named entity annotation (Tomanek et al., 2010a; Tokunaga et al., 2017b). Differences in reading patterns were also found when readers adopt different perspectives on given text (Kaakinen et al., 2002; Kaakinen and Hyönä, 2008), or given different learning goals (Rothkopf and Billington, 1979). Overall, these works consistently found systematic influence of the reading task on the resulting reading patterns. Our work builds most directly on analyses in Kaakinen et al. (2015), Malmaud et al. (2020), Hahn and Keller (2023) and Shubi and Berzak (2023) who compared eye movements in ordinary reading and open-ended information seeking. These studies showed task conditioning effects on reading patterns, especially with respect to taskcritical information in the text. Similarly to these studies, we formulate the information seeking task as reading comprehension question specific to the text. We further leverage the previously observed task conditioning effects for automatic discrimination between different tasks over the same text. Decoding Cognitive State With the rise of modern machine learning and NLP, several studies took advantage of eye movements in reading for prediction tasks about languagerelated aspects of the reader and their cognitive state. These include studies on prediction of the readers linguistic background (Berzak et al., 2017; Reich et al., 2022; Skerath et al., 2023), language proficiency (Berzak et al., 2018), reading comprehension (Ahn et al., 2020; Reich et al., 2022; Mézière et al., 2023; Shubi et al., 2024b, among others), and subjective readability (Reich et al., 2022). Most notably, two studies have addressed decoding tasks for reading goals. Hollenstein et al. (2021b) classified whether participants were engaged in ordinary reading or annotating semantic relations in single sentences using the ZuCo corpus (Hollenstein et al., 2020, 2023). Closest to our study is Shubi et al. (2024a), who classified ordinary reading versus information seeking using the OneStop dataset. In both cases, the decoding tasks are procedural classifying between two types of reading. In contrast, our study addresses related but conceptually and practically different task which is not procedural, but rather semantic and text specific decoding reading tasks specific to the text, within the information seeking regime. In our setup, instead of small number of categories that can apply to any text, we have hundreds of text-specific tasks that can be arbitrary in nature."
        },
        {
            "title": "8 Summary and Discussion",
            "content": "In this work, we introduced new task: decoding of arbitrary information seeking goals over specific texts from eye movements in reading. This task has both scientific value and practical potential, as decoding of information seeking goals in real time can open the door for real-world applications in various domains, including education, e-learning and content accessibility. We tackle this task in two formulations using number of modeling approaches both existing and new. Overall, we find that despite the inherent noise in eye movement data, highly valuable information can be extracted from such data regarding specific information seeking goals. While the presented results are highly encouraging, ample room remains for further improvement of prediction accuracy via modeling innovations."
        },
        {
            "title": "9 Limitations",
            "content": "was collected. Our study relies on the OneStop dataset. While OneStop is uniquely suited for addressing the proposed task, our work has number of limitations stemming from the experimental setup and the eye movement recordings in this dataset. The information-seeking tasks in OneStop are at the granularity level of single paragraph, leaving out information seeking in both shorter and longer texts. The questions further adhere to the STARC annotation framework (Berzak et al., 2020) which does not include tasks that require making inferences over the entire text, nor verbatim string matching tasks, which are both common in daily life. The texts themselves cover wide range of topics, but are restricted to the newswire domain. New experimental designs and additional data collection work are needed to address these limitations. An additional limitation of the current study is the restriction to native English speakers. Data from the most proficient group of readers for given language may lead to overestimates of performance in the general population, which includes lower skill readers such as L2 speakers, children, participants with cognitive impairments and others. Furthermore, the study is restricted to English, and additional data collection and modeling work is needed for other languages. We plan to collect such data in future work. further general drawback of in-lab eye tracking studies is that the lab setting may lead to reading patterns that deviate from everyday life reading (Huettig and Ferreira, 2022). Moreover, the eye tracking experiment is conducted using high-end Eyelink 1000 Plus eye tracker at sampling rate of 1000Hz. To increase the relevance of our work for deployment in real-world scenarios, such as web browsing, additional studies are required to test the feasibility of the proposed tasks and methods when the eye tracking data is collected using equipment with lower sampling rate, lower tracking precision or both."
        },
        {
            "title": "10 Ethical Considerations",
            "content": "The eye movements dataset used in this work was collected by Berzak et al. (2025). The study was conducted under an institutional IRB protocol, and all the participants provided written consent before participation. The data is anonymized. Analyses and predictive modeling of task-based reading are among the primary use cases for which the data While the current work is scientific proof of concept and is not aimed at particular user-facing application, one has to consider potential use cases and risks for the presented task. Automatic decoding of reading goals can have multiple potential applications that offer societal benefits. It can be used as part of e-tutoring systems that assess students progress on various types of information-seeking tasks and monitor their information-seeking skills over time. An ability to automatically determine specific information needs in real-time can also be used to assist special populations like elderly people when accessing information on the web. However, it is important to note that such technologies, especially in their current level of accuracy, may be unreliable and introduce biases for various individuals and groups, such as L2 readers and groups with reading and cognitive impairments. Additional data collection and analyses are needed to assess such biases. Prior work has demonstrated that eye movements can be used for user identification (e.g. Bednarik et al., 2005; Jäger et al., 2020). We do not perform user identification in this study. We further emphasize that future reading goal decoding technologies must be used in real-world applications only with explicit consent from potential users to have their eye movements collected and analyzed for this purpose."
        },
        {
            "title": "References",
            "content": "Seoyoung Ahn, Conor Kelton, Aruna Balasubramanian, and Greg Zelinsky. 2020. Towards Predicting Reading Comprehension From Gaze Behavior. In ACM Symposium on Eye Tracking Research and Applications, ETRA 20 Short Papers, pages 15, New York, NY, USA. Association for Computing Machinery. Maria Barrett and Nora Hollenstein. 2020. Sequence labelling and sequence classification with gaze: Novel uses of eye-tracking data for Natural Language Processing. Language and Linguistics Compass, 14(11):e12396. Douglas Bates, Phillip Alday, Dave Kleinschmidt, PhD José Bayoán Santiago Calderón, Likan Zhan, Andreas Noack, Milan Bouchet-Valat, Alex Arslan, Tony Kelman, Antoine Baldassari, Benedikt Ehinger, Daniel Karrasch, Elliot Saba, Jacob Quinn, Michael Hatherly, Morten Piibeleht, Patrick Kofod Mogensen, Simon Babayan, and Yakir Luc Gagnon. 2022. JuliaStats/MixedModels.jl: v4.7.0. Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, and Pasi Fränti. 2005. Eye-movements as biometric. In Image Analysis: 14th Scandinavian Conference, SCIA 2005, Joensuu, Finland, June 19-22, 2005. Proceedings 14, pages 780789. Springer. Lisa Beinborn and Nora Hollenstein. 2024. Cognitive plausibility in natural language processing. Springer. Yevgeni Berzak, Boris Katz, and Roger Levy. 2018. Assessing Language Proficiency from Eye Movements in Reading. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 19861996, Stroudsburg, PA, USA. Association for Computational Linguistics. Yevgeni Berzak, Jonathan Malmaud, and Roger Levy. 2020. STARC: Structured Annotations for Reading Comprehension. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Yevgeni Berzak, Jonathan Malmaud, Omer Shubi, Yoav Meiri, Ella Lion, and Roger Levy. 2025. Onestop: 360-participant english eye-tracking dataset with different reading regimes. PsyArXiv preprint. Yevgeni Berzak, Chie Nakamura, Suzanne Flynn, and Boris Katz. 2017. Predicting Native Language from Gaze. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 541551, Stroudsburg, PA, USA. Association for Computational Linguistics. Xiuge Chen, Namrata Srivastava, Rajiv Jain, Jennifer Healey, and Tilman Dingler. 2023. Characteristics of deep and skim reading on smartphones vs. desktop: comparative study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 114. Jacob Cohen. 1960. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):3746. Michael Han Daniel Han and Unsloth team. 2023. Unsloth. William Falcon and The PyTorch Lightning team. 2024. Pytorch lightning. Patrick Haller, Andreas Säuberli, Sarah Kiener, Jinger Pan, Ming Yan, and Lena Jäger. 2022. Eye-tracking based classification of Mandarin Chinese readers with and without dyslexia using neural sequence models. In Proceedings of the Workshop on Text Simplification, Accessibility, and Readability (TSAR-2022), pages 111118, Abu Dhabi, United Arab Emirates (Virtual). Association for Computational Linguistics. Nora Hollenstein, Cedric Renggli, Benjamin Glaus, Maria Barrett, Marius Troendle, Nicolas Langer, and Ce Zhang. 2021a. Decoding eeg brain activity for multi-modal natural language processing. Frontiers in Human Neuroscience, 15:659410. Nora Hollenstein, Marius Troendle, Ce Zhang, and Nicolas Langer. 2020. Zuco 2.0: dataset of physiological recordings during natural reading and annotation. In Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 138146. European Language Resources Association. Nora Hollenstein, Marius Tröndle, Martyna Plomecka, Samuel Kiegeland, Yilmazcan Özyurt, Lena Jäger, and Nicolas Langer. 2023. The zuco benchmark on cross-subject reading task classification with eeg and eye-tracking data. Frontiers in Psychology, 13:1028824. Nora Hollenstein, Marius Tröndle, Martyna Plomecka, Samuel Kiegeland, Yilmazcan Özyurt, Lena A. Jäger, and Nicolas Langer. 2021b. Reading Task Classification Using EEG and Eye-Tracking Data. arXiv:2112.06310 [cs]. ArXiv: 2112.06310. Nora Hollenstein, Marius Tröndle, Martyna Plomecka, Samuel Kiegeland, Yilmazcan Özyurt, Lena A. Jäger, and Nicolas Langer. 2022. The ZuCo Benchmark on Cross-Subject Reading Task Classification with EEG and Eye-Tracking Data. preprint, Neuroscience. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations. Falk Huettig and Fernanda Ferreira. 2022. The myth of normal reading. Perspectives on Psychological Science, page 17456916221127226. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Wilson. 2023. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 36:1962219635. Michael Hahn and Frank Keller. 2023. Modeling task effects in human reading with neural network-based attention. Cognition, 230:105289. Lena Jäger, Silvia Makowski, Paul Prasse, Sascha Liehr, Maximilian Seidler, and Tobias Scheffer. 2020. Deep eyedentification: Biometric identification using In Machine Learnmicro-movements of the eye. ing and Knowledge Discovery in Databases: European Conference, ECML PKDD 2019, Würzburg, Germany, September 1620, 2019, Proceedings, Part II, pages 299314. Springer. Ming Jin, Qingsong Wen, Yuxuan Liang, Chaoli Zhang, Siqiao Xue, Xue Wang, James Zhang, Yi Wang, Haifeng Chen, Xiaoli Li, and 1 others. 2023. Large models for time series and spatiotemporal data: survey and outlook. arXiv preprint arXiv:2310.10196. Marcel A. Just and Patricia A. Carpenter. 1980. theory of reading: From eye fixations to comprehension. Psychological Review, 87:329354. Place: US Publisher: American Psychological Association. Marcel Adam Just, Patricia Carpenter, and MEJ Masson. 1982. What eye fixations tell us about speed reading and skimming. Eye-lab Technical Report Carnegie-Mellon University, Pittsburgh. Johanna Kaakinen and Jukka Hyönä. 2010. Task Effects on Eye Movements During Reading. Journal of experimental psychology. Learning, memory, and cognition, 36:15616. Johanna Kaakinen and Jukka Hyönä. 2008. Perspective-driven text comprehension. Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition, 22(3):319334. Johanna Kaakinen, Jukka Hyönä, and Janice Keenan. 2002. Perspective effects on online text processing. Discourse processes, 33(2):159173. Johanna Kaakinen, Annika Lehtola, and Satu Paattilammi. 2015. The influence of reading task on childrens eye movements during reading. Journal of Cognitive Psychology, 27(5):640656. Masayuki Kawarada, Tatsuya Ishigaki, and Hiroya Takamura. 2024. Prompting for numerical sequences: case study on market comment generation. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 13190 13200, Torino, Italia. ELRA and ICCL. Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale ReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785 794, Copenhagen, Denmark. Association for Computational Linguistics. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326. Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. 2024b. Frozen language model helps In Medical Imaging with ecg zero-shot learning. Deep Learning, pages 402415. PMLR. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: Simple and Performant Baseline for Vision and Language. arXiv preprint. ArXiv:1908.03557 [cs]. Xin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics. Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. 2024. survey of multimodel large language models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering, pages 405409. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: Robustly Optimized BERT PretrainarXiv:1907.11692 [cs]. ArXiv: ing Approach. 1907.11692. Ilya Loshchilov and Frank Hutter. 2018. Decoupled Weight Decay Regularization. In International Conference on Learning Representations. Jonathan Malmaud, Roger Levy, and Yevgeni Berzak. 2020. Bridging Information-Seeking Human Gaze and Machine Reading Comprehension. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 142152, Stroudsburg, PA, USA. Association for Computational Linguistics. Abhijit Mishra, Shreya Shukla, Jose Torres, Jacek Gwizdka, and Shounak Roychowdhury. 2024. Thought2text: Text generation from eeg signal using large language models (llms). arXiv preprint arXiv:2410.07507. Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. 2016. dual embedding space model for document ranking. arXiv preprint arXiv:1602.01137. Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2021. On the stability of fine-tuning {bert}: Misconceptions, explanations, and strong baselines. In International Conference on Learning Representations. Diane C. Mézière, Lili Yu, Erik D. Reichle, Titus von der Malsburg, and Genevieve McArthur. 2023. Using eye-tracking measures to predict reading comprehension. Reading Research Quarterly, 58(3):425449. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. Ralph Radach, Lynn Huestegge, and Ronan Reilly. 2008. The role of global top-down factors in local eye-movement control in reading. Psychological research, 72(6):675688. Ralph Radach and Alan Kennedy. 2004. Theoretical perspectives on eye movements in reading: Past controversies, current issues, and an agenda for future research. European journal of cognitive psychology, 16(1-2):326. Publisher: Taylor & Francis. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2019. Improving Language Understanding by Generative Pre-Training. Keith Rayner. 1998. Psychological Bulletin Eye Movements in Reading and Information Processing: 20 Years of Research. Psychological bulletin, 124(3):372. Keith Rayner and Martin Fischer. 1996. Mindless reading revisited: Eye movements during reading and scanning are different. Perception & psychophysics, 58(5):734747. Keith Rayner, Alexander Pollatsek, Jane Ashby, and Charles Clifton Jr. 2012. Psychology of reading. Psychology Press. Keith Rayner and Gary Raney. 1996. Eye movement control in reading and visual search: Effects of word frequency. Psychonomic Bulletin & Review, 3(2):245248. David Robert Reich, Paul Prasse, Chiara Tschirner, Patrick Haller, Frank Goldhammer, and Lena A. Jäger. 2022. Inferring Native and Non-Native Human Reading Comprehension and Subjective Text Difficulty from Scanpaths in Reading. In 2022 Symposium on Eye Tracking Research and Applications, pages 18, Seattle WA USA. ACM. Ernst Rothkopf and MJ Billington. 1979. Goal-guided learning from text: inferring descriptive processing model from inspection times and eye movements. Journal of educational psychology, 71(3):310. Elizabeth Schotter, Klinton Bicknell, Ian Howard, Roger Levy, and Keith Rayner. 2014. Task effects reveal cognitive flexibility responding to frequency and predictability: Evidence from eye movements in reading and proofreading. Cognition, 131(1):127. Elizabeth Schotter and Brian Dillon. 2025. beginners guide to eye tracking for psycholinguistic studies of reading. Behavior Research Methods, 57(2):68. Steve Selvin. 1975. Letters to the editor. The American Statistician, 29(1):6771. Omer Shubi and Yevgeni Berzak. 2023. Eye movements in information-seeking reading. In Proceedings of the Annual Meeting of the Cognitive Science Society. Omer Shubi, Cfir Avraham Hadar, and Yevgeni Berzak. 2024a. Decoding reading goals from eye movements. arXiv preprint arXiv:2410.20779. Lina Skerath, Paulina Toborek, Anita Zielinska, Maria Barrett, and Rob Van Der Goot. 2023. Native language prediction from gaze: reproducibility study. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 152159. Alexander Strukelj and Diederick Niehorster. 2018. One page of text: Eye movements during regular and thorough reading, skimming, and spell checking. Journal of Eye Movement Research, 11(1). Takenobu Tokunaga, Hitoshi Nishikawa, and Tomoya Iwakura. 2017a. An Eye-tracking Study of Named Entity Annotation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 758764, Varna, Bulgaria. INCOMA Ltd. Takenobu Tokunaga, Hitoshi Nishikawa, and Tomoya Iwakura. 2017b. An eye-tracking study of named entity annotation. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 758764, Varna, Bulgaria. INCOMA Ltd. Katrin Tomanek, Udo Hahn, Steffen Lohmann, and Jürgen Ziegler. 2010a. cognitive cost model of annotations based on eye-tracking data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 11581167, Uppsala, Sweden. Association for Computational Linguistics. Katrin Tomanek, Udo Hahn, Steffen Lohmann, and Jürgen Ziegler. 2010b. Cognitive Cost Model of Annotations Based on Eye-Tracking Data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 11581167, Uppsala, Sweden. Association for Computational Linguistics. Sarah White, Kayleigh Warrington, Victoria McGowan, and Kevin Paterson. 2015. Eye movements during reading and topic scanning: Effects of word frequency. Journal of Experimental Psychology: Human Perception and Performance, 41(1):233. Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Philip Yu. 2023. Multimodal large lanIn 2023 IEEE Internaguage models: survey. tional Conference on Big Data (BigData), pages 22472256. IEEE. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Omer Shubi, Yoav Meiri, Cfir A. Hadar, and Yevgeni Berzak. 2024b. Fine-grained prediction of reading comprehension from eye movements. In The 2024 Conference on Empirical Methods in Natural Language Processing. Kai Yang, Massimo Hong, Jiahuan Zhang, Yizhen Luo, Suyuan Zhao, Ou Zhang, Xiaomao Yu, Jiawen Zhou, Liuqing Yang, Ping Zhang, and 1 others. 2025. Ecglm: Understanding electrocardiogram with large language model. Health Data Science, 5:0221. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations."
        },
        {
            "title": "Appendix",
            "content": "A.2 Generative Models"
        },
        {
            "title": "A Model Training and Hyperparameters",
            "content": "We use stringent evaluation protocol, in which paragraphs are assigned to training, validation, and test sets at the article level, such that all the paragraphs from the same article appear in the same portion of each data split. A.1 Discriminative Models Since the models we use were developed for different tasks and datasets, we conducted hyperparameter search for each model. The search space for each model is described below. In all cases, it includes the optimal parameters reported in the work that introduced the model, extended to provide fair comparison between models. For all neural models, we train with learning rates of {0.00001, 0.00003, 0.0001, 0.0002}, following Shubi et al. (2024b). Additionally, for all models that make use of word embeddings, we include both frozen and unfrozen language model variants in the search space. For Haller RNN, we search over LSTM layer counts of {1}, hidden sizes of {10, 40, 70, 140}, and dropout rate of 0.1. The model is trained with and without freezing language model parameters. For RoBERTa-F, we search over dropout rates of {0.1, 0.3, 0.5} for the eye movement projection layer. The model is trained with and without freezing language model parameters. We train the deep-learning-based models for maximum of 40 epochs, with early stopping after 8 epochs if no improvement in the validation error is observed. single training epoch took roughly 5 minutes for RoBERTa-F, 10 minutes for Haller RNN, and 30 minutes for Llava and Llama. Each individual run was capped at 24 hours. Following Liu et al. (2019); Mosbach et al. (2021); Shubi et al. (2024b), we use the AdamW optimizer (Loshchilov and Hutter, 2018) with batch size of 16. RoBERTa-F uses linear warm-up ratio of 0.06 and weight decay of 0.1. We standardize each eye movement feature using statistics computed on the training set, to zero mean unit variance. Both classification baseline models use word embeddings from the RoBERTa-Large (Liu et al., 2019) language model. We present two generative models, DalEye-Llama and DalEye-LLaVA, each fine-tuned with distinct methods and hyperparameters optimized for their specific architectures and objectives. DalEye-Llama This model is fine-tuned using Unsloth with the Meta-Llama-3.1-8B backbone loaded in 4-bit precision. Training employs LowRank Adaptation (LoRA) with rank = 16, scaling factor α = 16, and applies RS-LoRA regularization. LoRA targets transformer modules: q_proj, k_proj, v_proj, up_proj, down_proj, o_proj, and gate_proj. The model is trained for 2 epochs with linear scheduler and warm-up of 10 steps, batch size of 1, gradient accumulation over 2 steps, AdamW-8bit optimizer with learning rate 1 104, and weight decay of 0.01. DalEye-LLaVA This model employs teacherforcing strategy. Specifically, loss computation is restricted exclusively to tokens corresponding to possible questions, contrasting the likelihood of generating the correct question against two distractors. The computed losses for each question candidate are inverted, serving as logits for softmax normalization and subsequently optimized via crossentropy against the true label. The LLaVA backbone remains frozen, and training employs LoRA with dropout rate of 0.1, RS-LoRA, and rank = 8. The hyperparameter search includes learning rates of 1 105, 3 105, 1 104, fixation embedding hidden sizes of 512, 1024, and early stopping after 8 epochs without validation improvement. All neural network-based models were trained using the PyTorch Lightning (Falcon and team, 2024) library on NVIDIA A100-40GB and L40S48GB GPUs. We utilize the Hugging Face implementation of LLaVA-OneVision, specifically LLaVA-hf/LLaVAonevision-qwen2-0.5b-si-hf. Additionally, we employ the gpt-4o-2024-08-06 version of GPT-4o. We use Unsloth (Daniel Han and team, 2023) to finetune the Llama model. For the QA model we use LIAMF-USP/roberta-large-finetuned-race. There are roughly 355M trainable parameters for RoBERTa-F, reduced to 3M when the RoBERTa backbone is frozen. Haller RNN consists of 357M trainable parameters, or 2.4M when the backbone is frozen. DalEye-LLaVA has 505M parameters, out of which roughly 12M were unfrozen. DaLEyeLlama has 8.1B parameters, out of which roughly 42M were unfrozen."
        },
        {
            "title": "B Task Breakdown",
            "content": "Breakdown of the All task (Figure 4 (a)) into the Different critical spans (Figure 4 (b)) and the Same critical spans (Figure 4 (c)) setups. The Different critical spans setup is analogous to the Monty Hall scenario because grouping two items (be it doors or questions) together and making binary classification effectively reveals partial information, increasing the probability above random chance (Selvin, 1975). Predicted q1,c1 q2,c2 q3,c2 T q1,c1 q2,c2 q3,c (a) All Random accuracy: 3/9 = 33.3% Predicted q1,c1 q2,c2 q3,c2 T q1,c1 q2,c2 q3,c (b) Different critical spans Random accuracy: 5/9 = 55.5% Predicted q1,c1 q2,c2 q3,c2 T q1,c1 q2,c2 q3,c2 (c) Same critical span Random accuracy: 2/4 = 50% Figure 4: Chance accuracy under three evaluation regimes."
        },
        {
            "title": "C Additional Results",
            "content": "The tables below present breakdown of the test and validation results by the All Spans, Different Spans and Same Spans tasks, and by New Item, New Participant and New Item & Participant evaluation regimes. Model Majority Class / Chance Question similarity to reading time-weighted passage Reading Times Similarity to Question Paragraph Similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama New Item New Participant New Item & Participant 32.7 0.6+++ 33.2 0.5+++ 33.4 0.4+++ 34.1 0.6 +++ 40.4 0.5 +++ 49.0 0.8 33.6 0.2+++ 35.8 0.6 +++ 33.3 0.4+++ 34.3 0.6+++ 43.3 1.1 +++ 49.9 0.7 33.2 0.3+++ 40.4 0.5 +++ 33.3 0.9+++ 34.9 1.4+++ 34.2 1.2+++ 40.8 1.4 ++ 47.8 1.5 34.6 1.1+++ 39.6 1.0 +++ Table 2: Test accuracy results aggregated across 10 cross-validation splits for the All Spans task, by evaluation regime. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model + (model participant) + (model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked in subscript with +. Model New Item Different Spans New Participant Different Spans New Item & Participant Different Spans Majority Class / Chance 55.2 0.7+++ 55.7 0.5+++ Question similarity to reading time-weighted passage 55.1 3.7+++ 54.9 3.7+++ Reading Times Similarity to Question Paragraph Similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama 54.8 1.4+++ 64.9 0.6 +++ 71.2 0.9 58.1 1.2 +++ 58.2 1.5 +++ 54.8 1.4+++ 66.2 0.9 +++ 70.7 0.7 55.7 0.5+++ 62.6 0.7 +++ 53.5 1.5+++ 57.8 3.6+++ 55.1 1.7+++ 66.4 1.2 69.1 1.1 58.7 1.8 +++ 61.5 1.4 +++ Table 3: Test accuracy results aggregated across 10 cross-validation splits for the Different Spans task variation, by evaluation regime. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model + (model participant) + (model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked in subscript with +. Model New Item Same Span New Participant Same Span New Item & Participant Same Span Majority Class / Chance 49.4 0.5+++ 50.2 0.6+++ 51.4 1.8 Question similarity to reading time-weighted passage 50.3 0.5+++ 49.8 0.8+++ 49.7 2.3++ Reading times similarity to question-word similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama 51.0 0.7+++ 51.7 0.5 +++ 56.5 0.8 51.0 0.5+++ 52.6 1.0 +++ 58.2 0.7 49.6 0.4+++ 49.0 0.4+++ 49.7 0.3+++ 51.7 0.3+++ 53.1 1.4 50.3 2.0+ 56.7 1.6 51.5 1.6+ 53.3 1.5 Table 4: Test accuracy results aggregated across 10 cross-validation splits for the Same Spans task variation, by evaluation regime. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model + (model participant) + (model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked in subscript with +. Model Majority Class / Chance Question similarity to reading time-weighted passage Reading times similarity to question-word similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama All Spans Diff Span Same Span 33.1 0.3+++ 33.0 0.4+++ 34.1 0.3+++ 44.2 0.3 +++ 50.9 0.3 34.8 0.3 +++ 38.4 0.4 +++ 55.5 0.4+++ 54.6 3.8+++ 54.3 1.3+++ 66.4 0.4 +++ 71.5 0.4 56.8 0.7 +++ 60.5 0.6 +++ 50.1 0.3+++ 50.0 0.5+++ 51.1 0.5+++ 53.3 0.4 +++ 58.5 0.5 50.8 0.4+++ 51.7 0.4 +++ Table 5: Validation accuracy results aggregated across 10 cross-validation splits for each of the tasks. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model + (model participant) + (model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript, and significant drops compared to the best model in each regime are marked in subscript with +. Model Majority Class / Chance New Item New Participant New Item & Participant 33.3 0.5+++ 33.0 0.6+++ Question similarity to reading time-weighted passage 32.6 0.3+++ 33.2 0.7+++ Reading times similarity to question-word similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama 34.1 0.6+++ 42.9 0.3 +++ 50.6 0.8 34.3 0.3+++ 35.8 0.4 +++ 34.2 0.6+++ 45.7 0.8 +++ 51.3 0.7 34.8 0.6 +++ 40.8 0.7 +++ 31.7 1.2+++ 34.7 1.6+++ 34.3 1.3+++ 42.4 1.1 +++ 50.7 1.5 38.5 1.5 +++ 39.1 1.1 +++ Table 6: Validation accuracy results aggregated across 10 cross-validation splits for the All Spans task, by evaluation regime. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model + (model participant) + (model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked in subscript with +. Model New Item Different Spans New Participant Different Spans New Item & Participant Different Spans Majority Class / Chance 55.8 0.6+++ 55.4 0.6+++ Question similarity to reading time-weighted passage 54.6 3.8+++ 54.4 3.9+++ Reading times similarity to question-word similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama 54.3 1.4+++ 65.1 0.8 +++ 71.6 0.6 56.6 1.1+++ 58.5 1.1 +++ 54.4 1.2+++ 67.8 1.0 +++ 71.6 0.7 56.4 0.8+++ 62.6 0.9 +++ 53.2 0.9+++ 57.3 3.8+++ + 53.8 2.3+++ 65.3 0.7 70.4 1.9 60.8 1.9 +++ 61.2 0.9 +++ Table 7: Validation accuracy results aggregated across 10 cross-validation splits for the Different Spans task variation, by evaluation regime. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model+(model participant)+(model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked in subscript with +. Model New Item Same Span New Participant Same Span New Item & Participant Same Span Majority Class / Chance 50.1 0.4+++ 50.0 0.6+++ Question similarity to reading time-weighted passage 49.3 0.6+++ 50.6 0.8+++ Reading times similarity to question-word similarities Haller RNN (Haller et al., 2022) RoBERTEye-Fixations (Shubi et al., 2024b) DalEye-LLaVA DalEye-Llama 50.9 0.6+++ 53.2 0.4 +++ 57.3 0.9 51.0 0.3+++ 50.9 0.4+++ 51.0 0.6+++ 53.4 0.7 +++ 59.7 0.5 50.7 0.9+++ 52.5 0.6 +++ 50.7 2.0++ 49.9 2.2++ 53.0 1.5+ 52.2 1.7+ 58.6 1.9 49.8 1.5+++ 53.2 1.2+ Table 8: Validation accuracy results aggregated across 10 cross-validation splits for the Same Spans task variation, by evaluation regime. Fix stands for fixations. Model performance is compared to the Majority class baseline using linear mixed effects model. In notation: is_correct model + (model participant) + (model paragraph). Significant gains over this baseline are marked with * < 0.05, ** < 0.01 and *** < 0.001 in superscript. The best performing model in each evaluation regime is marked in bold. Significant drops compared to the best model are marked in subscript with +."
        },
        {
            "title": "Categories",
            "content": "This section provides additional information regarding the question annotation into UIUC categories. Specifically, we include the full prompt given to the model, examples of questions and their corresponding annotations, the results from Figure 3 in numerical format, and agreement with human annotators. D.1 Full Prompt You are an expert at classifying questions into the standard UIUC **main question categories**. The main categories include: - ABBR: Abbreviation For example: - What is the purpose of car bra? - What makes tornado turn? - What causes the redness in your cheeks when you blush? - Why do horseshoes bring luck? - DEFINITION: Definition For example: - WWhat is dental root canal? - What is the contents of proposition 98? - Hazmat stands for what? - What does the name Billie mean? - HUMAN: People or groups For example: - Who invented baseball? - What stereo manufacturer is 'Slightly ahead of its time'? - Who played the original Charlie's - What does the abbreviation AIDS stand Angels? for? - What company's logo is 'W' in - What is the abbreviation for micro? - What is the abbreviation of the company circle? name 'General Motors'? - What does G.M.T. stand for? - LOCATION: Geographic locations For example: - ENTITY: Entity For example: - What kind of animal is Babar? - What killed Bob Marley? - What is fear of weakness? - Where does your hair grow the fastest? - DESCRIPTION: Description For example: - What do Mormons believe? - What is the history of skateboarding? - What is the difference between generator and an alternator? - Where do rocks come from? - MANNER: Manner For example: - How do get another city's newspaper? - How do you solve \"Rubik's Cube\"? - How do you look up criminal records on the Internet? - How do you find oxidation numbers? - REASON: Reason For example: - Where is the highest point in Japan? - What European city do Nicois live in? - What is Answers.com 's address? - What U.S. state borders Illinois to the north? - NUMERIC: Numbers, quantities, and dates For example: - What is the temperature for baking Peachy Oat Muffins? - How many colleges are in Wyoming? - What is the average temperature in the Arctic? - What is the speed of light? --- Return an array of tuples in the format: ``` [ (0, \"HUMAN\"), (1, \"DESCRIPTION\"), (2, \"NUMERIC\"), (3, \"DESCRIPTION\"), ] ``` If unsure, choose the closest matching category. New Participant New Item New Participant & Item Question word (Kappa) Incorrect human question - different critical span Incorrect human question - same critical span GPT-4o arbitrary question Llama 3.1 arbitrary question DalEye-Llama question 0.095 0.010 0.071 0.015 0.039 0.011 0.353 0.015 0.458 0.015 0.094 0.010 0.071 0.015 0.049 0.011 0.007 0.007 0.054 0. 0.098 0.030 0.078 0.047 0.089 0.037 0.007 0.021 0.030 0.029 Question category (Kappa) Incorrect human question - different critical span Incorrect human question - same critical span GPT-4o arbitrary question Llama 3.1 arbitrary question DalEye-Llama question 0.099 0.011 0.060 0.016 0.003 0.005 0.212 0.014 0.280 0.016 0.096 0.011 0.059 0.016 0.019 0.006 0.021 0.013 0.069 0.013 BLEU Incorrect human question different critical span Incorrect human question same critical span GPT-4o arbitrary question Llama 3.1 arbitrary question DalEye-Llama question 0.293 0.003 0.384 0.004 0.208 0.003 0.523 0.007 0.596 0.004 0.291 0.003 0.383 0.005 0.214 0.003 0.300 0.004 0.245 0.002 BERTScore 0.104 0.034 0.069 0.048 0.033 0.018 0.036 0.038 0.056 0.040 0.288 0.010 0.383 0.015 0.209 0.008 0.304 0.006 0.242 0. Incorrect human question different critical span Incorrect human question same critical span GPT-4o arbitrary question Llama 3.1 arbitrary question DalEye-Llama question 0.604 0.015 0.651 0.018 0.606 0.013 0.724 0.036 0.766 0.038 0.603 0.015 0.651 0.019 0.611 0.013 0.617 0.016 0.586 0.013 0.603 0.015 0.651 0.019 0.610 0.013 0.618 0.016 0.583 0.013 QA accuracy Incorrect human question - different critical span Incorrect human question - same critical span GPT-4o arbitrary question Llama 3.1 arbitrary question DalEye-Llama question 49.3 1.0 67.7 1.3 64.3 1.0 68.3 0.9 74.2 0.9 49.2 1.0 67.8 1.3 65.1 1.0 60.9 1.0 53.9 1.0 50.8 2.8 67.1 3.9 63.6 3.1 61.4 2.8 54.2 2.9 Table 9: Question reconstruction evaluations of the DalEye-Llama model for (1) the identity of the generated question word, (2) the semantic category of the question (Li and Roth, 2002), (3) similarity using BERTScore, and (4) downstream QA accuracy based on the answer selection of multiple-choice question answering model. DalEye-Llama performance is benchmarked against two types of human composed questions (for different critical span and for the same span) and against arbitrary questions generated with GPT-4o and Llama 3.1. Presented are means with 95% confidence intervals (bootstrapped, = 1000) for three generalization levels to new participants and new readers. The highest score in each combination of evaluation and generalization level is marked in bold. \"\"\" D.2 Categorization Examples For each question category we give an example based on the questions in OneStopQA (Berzak et al., 2020). LOCATION - Where was wolf-hunting banned in 1995? ENTITY - Which of the following will be featured at Pestival 2013? HUMAN - Who threw the bottle into the Baltic REASON - Why does Myslajek mention Russia, Sea? Lithuania and Belarus? DESCRIPTION - What does Angella think of NUMERIC - Approximately how many taxi the state of the sea today? drivers are there in the UK? D.3 Agreement with Human Annotators MANNER - How does Myslajek react to what he sees between the two paw prints? To evaluate the quality of the GPT-4o question category classifications, we randomly sampled 100 questions. human annotator (one of the papers authors) manually labeled them with UIUC category. We find an 86% agreement (0.794 Cohens kappa) between the human annotations and the model classifications. E.3 Haller RNN Figure 7 depicts the Haller RNN architecture (Haller et al., 2022) adapted to our task. Given paragraph and candidate question q, the model performs the following steps:"
        },
        {
            "title": "E Models",
            "content": "The figures below, depicting models architectures, showcase the processing of single candidate question. As mentioned in Section 4, each candidate question is processed independently. E.1 Question Similarity to RT-Weighted Passage Figure 5 depicts the Question Similarity to RTWeighted Passage baseline architecture. Word Embeddings: LLM-based contextualized word embeddings, for and q, resulting in three subsequences: (1) Z[CLS], (2) ], and (3) [ZPt1 [Zqt1 , . . . , ZPtx ]. , . . . , Zqtl Fixation Sequence: Paragraph-only tokenlevel embeddings are pooled to word-level embeddings, and ordered by the fixation sequence. Then, each embedding in the sequence is concatenated with the respective fixation features. Projection: Z[CLS] and [Zqt1 ] are projected to higher dimension to fit the dimension of the concatenated embeddings from the previous step. , . . . , Zqtl RNN Input Sequence: The processed embeddings are then concatenated one after the other to create the RNN input sequence. Classifier: Finally, classifier layer scores each candidate question. Figure 5: Question Similarity to RT-Weighted Passage E.2 RT Similarity to Question-Word Similarities Figure 6 depicts the RT Similarity to QuestionWord Similarities baseline architecture. Figure 6: RT Similarity to Question-Word Similarities Figure 7: Haller RNN E.4 RoBERTEye-Fixations Formally, given paragraph and candidate question qP , the model constructs two parallel representations: 1. Textual Representation (ZW ): The input follows the format ZW = [CLS; p; SEP; qP ; SEP], where is the paragraph and qP is the question. 2. Fixation-Level Eye Movement Representation (ZEP ): The eye movement sequence is ]. The fixadefined as ZEP = [ZEf1 , ..., ZEfm tion representation is computed as: ZEfi = FC(Efi) + Embpos(wi) + Embeye Here, Efi captures fixation properties (e.g., duration, position) for fixation fi on word wi. The fully connected layer FC projects this feature vector into the word embedding space, Embpos(wi) is the positional embedding of wi used to map each fixation to the corresponding word, and Embeye is learnable embedding marking the presence of eye movement information. 3. Fusion and Prediction: The combined sequence [ZEP ; SEPE; ZW ] is processed by the transformer encoder, and the final CLS token representation is passed through two fully connected layers to predict the question: ˆq = argmax MLP(CLS). Figure 8 depicts the adapted architecture. Figure 8: RoBERTEye-Fixations Figure 9: DalEye-LLaVA E.5 Eye-DalEye-LLaVA The input is formatted as conversation, as is typical in instruction-tuned LLMs, and is structured as follows: conversation = [ {\"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Given the attached eye tracking data:\"}, {\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"The user read the following paragraph:\"}, {\"type\": \"text\", \"text\": paragraph_text}, {\"type\": \"text\", \"text\": \"What question did the user read, before reading the paragraph, based on the eye tracking data?\"} ]} ] We append the ground-truth question at the end of the prompt, and follow it up by an <eos> token. Tokenization is applied to the entire prompt. Following standard teacher forcing, loss is computed only on the question tokens, during the forward pass. In the forward function, we adopt LLaVAs architecture: (1) text tokens are embedded via language model trained with LoRA, (2) fixation data is passed through dedicated fixation encoder to generate embeddings (as shown in Figure 9, and (3) these embeddings replace the <image> placeholder token in the input sequence. Word tokens are assigned position IDs based Specifically analyze fixation patterns around keywords or repeated concepts, as they strongly indicate the nature of the original question. Produce only the exact original question as your output. An example of scanpath formatted according to the input format: \"[0, Angela, 182], [1, Erdmann, 156], [1, Erdmann, 133], [2, never, 157], [4, her, 166], [5, grandfather., 137], [7, died, 150], [9, 1946,, 353], [11, years, 252], [13, she, 172], [15, born., 137], [16, But,, 115], [18, Tuesday, 118], [18, Tuesday, 138], [21, 2014,, 167], [23, described, 199], [25, extraordinary, 269], [26, moment, 149], [28, she, 199], [29, received, 180], [31, message, 111], [34, bottle,, 145], [33, a, 162], [35, 101, 109], [37, after, 292], [39, had, 256], [41, it, 237], [43, the, 230], [44, Baltic, 55], [35, 101, 165], [37, after, 255], [39, had, 384], [46, Thought, 170], [46, Thought, 139], [49, the, 131], [46, Thought, 143], [50, world's, 137], [52, message, 147], [51, oldest, 275], [53, in, 123], [55, bottle,, 124], [58, presented, 132], [60, Erdmann, 203], [62, the, 164], [64, that, 145], [66, now, 174], [67, exhibiting, 143], [68, it, 110], [70, Germany., 180], [70, Germany., 480].\" on their order in the paragraph, and each fixation embedding receives the position ID of the word being fixated. This design enables the model to jointly attend to semantic and gaze signals, effectively grounding its question prediction in both the textual content and the users reading behavior. E.6 DalEye-Llama Below is the prompt used for generating questions using DalEye-Llama and generation examples. to is the generate Task Description: Your exact task original question reader had in mind before reading given paragraph. The input data is time series composed of triples: each triple contains the index of the fixated word, the fixated word itself, fixation and milliseconds. especially longer fixation durations or repeated fixations on specific words or concepts, indicate the reader's main focus and information-seeking behavior. in patterns, Fixation duration the Input Format: You will receive data formatted as list of triples: [[fixated_word_index, fixation_duration_ms], ...] fixated_word, the to reader, original exact the question accurately Expected Output: Generate provided inferred from the fixation patterns. Pay special attention that consistently attract longer fixation durations or are fixated upon multiple times, as these typically align with keywords or concepts directly related to the original question. phrases words or to Instructions: Your output must precisely match the original the reader. presented question to Paragraph: Lemnos has wild beaches, where you can swim and sunbathe almost alone, small nightlife scene and many cultural sites. Lemnos is the eighth largest island in Greece so it will have to pay the first round of tax increases in autumn 2015. But Lemnos is far less wealthy than many smaller islands. It has just over 3,000 beds for visitors Rhodes, for example, has tens of thousands of beds. \"We have been suffering economically in recent years and now we will suffer more,\" said Lemnos Mayor, Dimitris Marinakis. True Question New Item Generated Questions New Participant Generated Questions What is among the primary attractions of Lemnos? (A) What is one thing to note about the eponymous village? (A) What is among the primary attractions of Lemnos? (B) Who is quoted as saying that hotels benefit from the increase in the number of tourists in the area? (B) What will happen in spring 2013? (C) What does the quote convey about the current economic situation in Greece? (C) Why does Lemnos have to pay the first round of tax increases? (D) What is true of the other Greek islands? (D) What is one reason why the wealthy tend to spend more time on the islands? Figure 10: An example showing paragraph, the corresponding ground truth question, and four DalEye-Llama generated questions from the New Item Regime and four from the New Participant Regime. Each generated question reflects different outcome from the QA model, as indicated by the answer choice (AD) selected by the model. The possible answers, structured according to the STARC annotation guidelines (Berzak et al., 2020), to the ground truth question were: A) (correct answer) large number of cultural destinations. B) (miscomprehension of the critical span) Beaches constantly full of locals and tourists. C) (incorrect, related to different span) 3,000 luxury hotels. D) (no support in the passage) Highly-regarded restaurants. Focus specifically on central concepts, themes, the statements paragraph. within or Produce only the exact original question as your output. E.7 Arbitrary Question Generation Below is the prompt used for generating questions using Llama and GPT-4o. Task Description: Your task is to generate question reader had in mind before reading given paragraph. The input data is the paragraph itself in standard textual format. Input Format: You will receive paragraph in plain text format: [Paragraph text] original the to from key Expected Output: question exact Generate accurately the provided content. the inferred Identify or themes, statements in the paragraph that strongly indicate initially the motivated the reading. reader, paragraph concepts, question that Instructions: Your output must precisely match the original the reader. presented question to"
        }
    ],
    "affiliations": [
        "Faculty of Data and Decision Sciences, Technion - Israel Institute of Technology, Haifa, Israel"
    ]
}