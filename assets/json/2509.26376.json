{
    "paper_title": "Go with Your Gut: Scaling Confidence for Autoregressive Image Generation",
    "authors": [
        "Harold Haodong Chen",
        "Xianfeng Wu",
        "Wen-Jie Shu",
        "Rongjin Guo",
        "Disen Lan",
        "Harry Yang",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as a novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams a calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 7 3 6 2 . 9 0 5 2 : r a"
        },
        {
            "title": "ScalingAR",
            "content": "GO WITH YOUR GUT: SCALING CONFIDENCE FOR AUTOREGRESSIVE IMAGE GENERATION Harold Haodong Chen1,2, Xianfeng Wu3, Wen-Jie Shu2, Rongjin Guo4, Disen Lan5, Harry Yang2, Ying-Cong Chen1,2 1HKUST(GZ) Primary Contact: haroldchen328@gmail.com 2HKUST 3PolyU 4CityUHK 5FDU Figure 1: (Top) ScalingAR significantly improves the quality of autoregressive image generation. Detailed prompts are provided in Appendix A. (Bottom Left) The token confidence trajectory over the generation process. (Bottom Right) Performance comparison of ScalingAR on TIIF-Bench with classic test-time scaling strategies, i.e., Importance Sampling (IS) and Best-of-N (BoN)."
        },
        {
            "title": "ABSTRACT",
            "content": "Test-time scaling (TTS) has demonstrated remarkable success in enhancing large language models, yet its application to next-token prediction (NTP) autoregressive (AR) image generation remains largely uncharted. Existing TTS approaches for visual AR (VAR), which rely on frequent partial decoding and external reward models, are ill-suited for NTP-based image generation due to the inherent incompleteness of intermediate decoding results. To bridge this gap, we introduce ScalingAR, the first TTS framework specifically designed for NTP-based AR image generation that eliminates the need for early decoding or auxiliary rewards. ScalingAR leverages token entropy as novel signal in visual token generation and operates at two complementary scaling levels: (i) Profile Level, which streams calibrated confidence state by fusing intrinsic and conditional signals; and (ii) Policy Level, which utilizes this state to adaptively terminate low-confidence trajectories and dynamically schedule guidance for phase-appropriate conditioning strength. Experiments on both general and compositional benchmarks show that ScalingAR (1) improves base models by 12.5% on GenEval and 15.2% on TIIF-Bench, (2) efficiently reduces visual token consumption by 62.0% while outperforming baselines, and (3) successfully enhances robustness, mitigating performance drops by 26.0% in challenging scenarios. Our code is available at ScalingAR Repository."
        },
        {
            "title": "ScalingAR",
            "content": "Figure 2: (a) Next-scale prediction paradigm generates multi-scale token maps coarse-to-fine. (b) Next-token prediction paradigm sequentially predicts next image tokens. (c) Illustration of Best-of-N sampling that generates multiple candidate and selects the best via voting or scoring. (d) Overview of our proposed ScalingAR, highlighting its ability to leverage token entropy to early-stop lowconfidence samples and identify winning samples without the need for additional reward models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) (Brown et al., 2020; Vaswani et al., 2017; Radford et al., 2019) have demonstrated the capabilities of next-token prediction (NTP) paradigm. This success has renewed interest in applying autoregressive (AR) architectures beyond text, motivating recent visual generative models that represent images in discrete token spaces (Sun et al., 2024; Tian et al., 2024; Li et al., 2024) as shown in Figure 2 (b). Compared to diffusion models, which operate over continuous noise trajectories, token-based AR models promise more unified modality interface. As the field evolves, the parameters and training data of foundation models (Wang et al., 2024; Yang et al., 2025) have increasingly grown to levels that are inaccessible for most university researchers. In this context, many studies have started to investigate post-training methods. Inspired by recent advancements such as GRPO (Shao et al., 2024), surge of reinforcement learning research has emerged in both language and visual generation domains (Jiang et al., 2025; Cui et al., 2025). Meanwhile, another research avenue focusing on test-time scaling (TTS) has emerged (Lightman et al., 2023; Muennighoff et al., 2025; Zuo et al., 2025), which aims to explore whether slight increase in computational expense during inference can achieve performance on par with trainingtime methods, which typically incur much larger costs. While test-time scaling has been extensively researched in language models, analogous progress for autoregressive visual generation remains sparse. Images differ from text in three practical ways that complicate direct transfer: (i) holism: dropping the last 20% of text sequence may still leave syntactically valid answer, whereas truncating an image token stream yields an unusable artifact; (ii) objective ambiguity: many language scaling setups optimize toward verifiable final answer (e.g., math reasoning), whereas image generation lacks single ground-truth target; and (iii) early signal scarcity: partial image token decodes are visually unstable, making premature selection risky. Moreover, recent work TTS-VAR (Chen et al., 2025c) introduced TTS for the next-scale prediction (NSP) paradigm in visual autoregressive model (VAR) (Tian et al., 2024) by predicting images in coarse-to-fine manner (Figure 2 (a)). This intermediate visibility enables reward models to score during scaling but comes with limitations that require predicting large residual token maps at each scale and frequent decoding makes the process inefficient and less suitable for the NTP paradigm. Building on these insights, we introduce ScalingAR, the first test-time scaling framework tailored to the NTP paradigm in autoregressive image generation. Unlike next-scale TTS-VAR, ScalingAR eliminates the need for frequent partial decoding and external reward models (as shown in Figure 2 (d)), relying solely on intrinsic signals derived from visual token entropy and conditional signals to profile confidence. Specifically, in response to limitations, ScalingAR prunes unreliable trajectories"
        },
        {
            "title": "ScalingAR",
            "content": "without interrupting generation (holism), constructs confidence by combining intrinsic uncertainty and conditional signals (objective ambiguity), and extracts stability directly from model probabilities rather than intermediate outputs (early signal scarcity). Technically, ScalingAR features two-level design: ❶ Profile Level, which constructs unified confidence state by integrating intrinsic generation stability with conditioning effectiveness; and ❷ Policy Level, which leverages this confidence state to prune failing trajectories and dynamically adjust conditioning strength through adaptive termination and guidance scheduling. Our contributions can be summarized as follows: We propose ScalingAR, the first test-time scaling framework tailored to next-token prediction AR image generation, featuring novel two-level design with Profile Level for dual-channel confidence profiling on-the-fly, and Policy Level for trajectory pruning and guidance scheduling. We for the first time investigate token entropy in visual token generation. By relying solely on intrinsic signals from the model, ScalingAR eliminates the need for frequent early decoding and external reward models, enabling more efficient and reliable scaling process. Extensive experiments on both general and compositional benchmarks demonstrate that ScalingAR is: (i) high-performing, achieving significant performance gains over base models (i.e., LlamaGen and AR-GRPO), by 12.5% on GenEval and 15.2% on TIIF-Bench; (ii) token-efficient, outperforming classic baselines (i.e., Importance Sampling and Best-of-N) while reducing visual token consumption by 62.0%; and (iii) robust in challenging scenarios, mitigating performance degradation by 26.0% compared to base models in highly complex generation settings."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Autoregressive Image Generation Autoregressive models have leveraged the scaling capabilities of language models (Yang et al., 2025; Brown et al., 2020; Radford et al., 2019) to generate images. These approaches employ discrete image tokenizers (Van Den Oord et al., 2017; Razavi et al., 2019) in conjunction with transformers, using next-token prediction strategy. VQ-based methods (Lee et al., 2022; Razavi et al., 2019; Esser et al., 2021), e.g., VQ-VAE (Van Den Oord et al., 2017), convert image patches into index-based tokens, which are then predicted sequentially by decoder-only transformer. However, these VQ-based AR methods are limited by the lack of scaled-up transformers and the inherent quantization error in VQ-VAE. This has prevented them from achieving performance on par with diffusion models. Recent advancements (Wu et al., 2025a; Yu et al., 2022; Team, 2024) have scaled up AR models for visual generation. Additionally, some variants have been proposed, such as the next-scale prediction paradigm of VAR (Tian et al., 2024; Han et al., 2025), which predicts from coarse to fine token maps, and the parallel token prediction of masked AR (MAR) (Li et al., 2024; Wu et al., 2025b; Fan et al., 2025). Despite these developments, the mainstream approach remains the NTP paradigm, particularly as the field moves towards unified models (Xie et al., 2025; Wang et al., 2024; Ge et al., 2024) that can jointly handle textual and visual tokens. This alignment with language modeling allows for more versatile and scalable architectures. Test-Time Scaling Current LLMs have increasingly succeeded by allocating substantial reasoning at inference time, paradigm known as test-time scaling (Snell et al., 2024; Welleck et al., 2024). This scaling can occur along two main axes: (1) Chain-of-Thought (CoT) (Wei et al., 2022) Depth: lengthening single reasoning trajectory through more thinking steps, often relying on large-scale reinforcement learning with many samples (Yang et al., 2025; Jaech et al., 2024; Guo et al., 2025) or simpler post-training strategies (Ye et al., 2025; Muennighoff et al., 2025); (2) Parallel Generation: scaling by increasing the number of trajectories and aggregating them, as seen in works like SelfConsistency (Wang et al., 2023) and Best-of-N (Lightman et al., 2023). Recent efforts (Kang et al., 2025; Fu et al., 2025) have also integrated confidence estimation through token entropy into the test-time reasoning process, allowing the quality of individual traces to be assessed before aggregation with the rewards for majority voting (Wang et al., 2023). However, exploring TTS for AR image generation has been limited. This is due to the holistic nature of image generation, where overall coherence is paramount (see Figure 2 (c)), unlike reasoning tasks with well-defined ground truths. Additionally, the frequent early decoding required for images can be more computationally expensive than for language, suggesting that direct transfer of many LLM TTS techniques may not be suitable or optimal. To address this gap, we propose the first TTS strategy tailored for AR image generation. Notably, we pioneer the exploration of token entropy in image generation, enabling our method to leverage visual token confidence without the need for early decoding or additional rewards."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Next-Token Prediction Autoregressive Modeling NTP is fundamental paradigm in autoregressive models, where the model generates sequences by predicting the next token based on previously generated tokens. The generation process can be mathematically described as follows: p(x1, x2, . . . , xT ) = (cid:89) t=1 p(xtx1, x2, . . . , xt1). (1) This formulation allows the model to leverage past information to inform future predictions, making it particularly effective for sequential data generation. The training of autoregressive models typically involves maximizing the likelihood of the observed sequences, which can be expressed as: = (cid:88) t=1 log p(xtx<t). (2) This objective encourages the model to learn the underlying distribution of the data, enabling it to generate coherent and contextually appropriate sequences. Token Entropy in Language Modeling Token entropy is critical metric for evaluating the uncertainty associated with the predictions made by language models (Kang et al., 2025). It quantifies the amount of unpredictability in the models output distribution for given token. The entropy at specific position in the sequence can be defined as: Hi = (cid:88) pi(j) log pi(j), (3) where pi(j) denotes the predicted probability of the j-th token in the vocabulary at position i. Low entropy indicates high certainty in the prediction, while high entropy reflects greater uncertainty. Furthermore, token confidence can be derived from the predicted distribution (Fu et al., 2025). The confidence Ci for token at position is defined as: Ci = 1 k (cid:88) log pi(j), (4) j=1 where represents the number of top tokens considered. High confidence values correlate with sharper distributions, indicating that the model is more certain about its predictions."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "To pioneer test-time scaling for next-token prediction autoregressive image generation, we propose ScalingAR, which leverages intrinsic token confidence signals without relying on early decoding or external rewards, featuring two scaling levels: (i) Dual-Channel Confidence Profile compacts heterogeneous per-step signals into calibrated confidence state (4.1); and (ii) Confidence-Guided Policies act on this state to prune failing trajectories and adapt conditional guidance on-the-fly (4.2). Figure 3: (Left) Confidence distribution of ScalingAR on GenEval and TIIF-Bench. (Right) Illustration of the trade-off between visual quality and semantic alignment with fixed Classifier-Free Guidance (CFG) in AR image generation. 1st: 35 mm photo of cityscape resembling Moscow floating in the sky on flying islands. 2nd: The colorful hot air balloon floated near the dark grey storm clouds."
        },
        {
            "title": "4.1 DUAL-CHANNEL CONFIDENCE PROFILE",
            "content": "Autoregressive image generators traditionally treat all partial trajectories as equally promising until completion, as illustrated in Figure 2 (c). However, empirical inspection reveals two dominant failure modes during inference that often foreshadow poor final results: ❶ local intrinsic instability, characterized by high entropy pockets and wavering token choices (Figure 1 (Bottom Left) & Figure 3 (Left)); and ❷ poor utilization of the text condition, where the semantic influence of the prompt gradually fades, resulting in misaligned or aesthetically suboptimal outputs (Figure 3 (Right)). To address these challenges, we introduce the Dual-Channel Confidence Profile, consisting of two complementary channels: ➀ Intrinsic Channel: Captures localized instability and spatial anomalies within the token grid. ➁ Conditional Channel: Quantifies the marginal contribution of textual conditioning to ensure semantic alignment. 4.1.1 INTRINSIC CHANNEL: UNCERTAINTY & SPATIAL STABILITY Early-stage failures in autoregressive image generation rarely manifest as immediate global collapse. Instead, they emerge through localized instability. To capture these signals, the Intrinsic Channel integrates two key components: token-level confidence and worst-block spatial stability. Token-level Confidence Token-level uncertainty reflects the dispersion and decisiveness of predictions at each decoding step. Let πt denote the softmax distribution over the vocabulary at step t. We compute token entropy Ht = (cid:80) vV πt(v) log πt(v) and top-1/top-2 margin mt = πt(v1)πt(v2), forming normalized uncertainty surrogate: (cid:98)Ht = Ht/ log , ut = αH (cid:98)Ht + αM (1 mt), αH + αM = 1, (5) = 1 ut (0, 1]. To stabilize this signal, we apply an where ut is mapped to token confidence stok exponential moving average (EMA): stok = (1 λtok)stok t1 + λtokstok . (6) Worst-block Stability Localized hot spots of persistent high entropy often diffuse into global semantic corruption. To capture these spatial anomalies, we partition the token grid into non-overlapping blocks. For each block (with fill ratio ρmin), we compute its mean normalized entropy Ek. Focusing on the worst-q% subset Wt of blocks with the highest entropy: Eworst(t) = 1 Wt (cid:88) Ek. kWt (7) rolling min-max normalization Nmm yields stability score Bt = 1 Nmm(Eworst(t)), emphasizing emergent localized failure rather than global averages. Finally, the Intrinsic Channel score combines token-level confidence and worst-block stability: followed by smoothing It = EMA(I raw t = wtokstok raw + wblkBt, wtok + wblk = 1, , λI ). (8) 4.1.2 CONDITIONAL CHANNEL: TEXT UTILIZATION STRENGTH While intrinsic signals capture localized instability, semantic misalignment often arises from insufficient utilization of the text condition. For concise prompts or complex visual contexts, the conditional branch may lose influence, silently drifting from the intended semantics. The Conditional Channel measures the marginal contribution of textual conditioning to ensure semantic alignment. Let pc,t and pu,t denote the softmax distributions from conditional and unconditional logits, respectively. We compute the KL divergence Kt = KL(pc,t pu,t), then apply rolling z-score normalization: norm = Kt µK σK + ε mapping the result to [0, 1]: , clip = clip(K norm Dt = 0.5 + 0.5 clip zmax . , zmax, zmax), (9) (10) Persistently low values of the smoothed score (cid:98)Dt flag semantic fade, while excessively high values paired with low It may indicate unstable over-conditioning."
        },
        {
            "title": "4.1.3 UNIFIED CONFIDENCE STATE",
            "content": "To enable dynamic trajectory control, we combine both channels into unified confidence state. The scalar unified confidence score is defined as: Ct = wI It + wD (cid:98)Dt, wI + wD = 1, (11) optionally passed through an affine-sigmoid calibration to mitigate cross-prompt scale drift. To capture early-stage failure signals, we maintain the running minimum Cmin(t) = minit Ci and compute relative rebound: Rt = Ct Cmin(t) Cmin(t) + ε . (12) This unified confidence score serves as the basis for dynamic trajectory pruning and adaptive conditioning, enabling efficient test-time scaling tailored to the NTP paradigm."
        },
        {
            "title": "4.2 CONFIDENCE-GUIDED POLICIES",
            "content": "With calibrated confidence score Ct, we transition from passive observation to active test-time control, enabling dynamic intervention in autoregressive generation. To achieve this, we introduce two lightweight yet effective policies: ➀ an Adaptive Termination Gate that prunes unpromising trajectories to reclaim computation; and ➁ Guidance Scheduler that dynamically modulates CFG scale to balance semantic alignment. 4.2.1 ADAPTIVE TERMINATION GATE Failing trajectories often exhibit prolonged spans of low confidence, lingering in confidence basin before producing final tokens that posterior reranking would discard. The Adaptive Termination Gate proactively terminates such trajectories, reclaiming computational resources. Threshold Initialization and Adaptation To identify failing trajectories, we initialize confidence threshold θ after warm-up period of W0 steps. The threshold is set to the p-quantile (p [0.15, 0.25]) of the collected Ct values across active trajectories. This ensures that pruning targets the bottom-performing trajectories without prematurely terminating promising ones. The threshold is periodically updated every upd steps using an EMA-based adaptation: θ (1 λθ)θ + λθQuantilep({Ct}recent). (13) where {Ct}recent denotes the confidence scores from recent decoding steps. Recovery Safeguard To mitigate false positives caused by transient dips in Ct, we incorporate recovery mechanism. trajectory is permitted to recover if it satisfies either of the following conditions within recovery window rec: (a) Ct Cmin(t) + δrec: absolute confidence rebound exceeds pre-defined gap. (b) Rt rthr: relative rebound exceeds threshold, indicating stabilization. Only trajectories failing both criteria are marked for termination. Termination Rule Once the protection horizon Tmin (e.g., 10% of ) has elapsed, trajectory is terminated if Cmin(t) < θ and no recovery within last rec steps. Additionally, hard-fail guard (Ct < Chard) triggers immediate termination for catastrophic collapse scenarios, ensuring robustness against extreme failures. By over-initializing Ktarget + Mbuf trajectories and relying on pruning, we refine the candidate set without spawning replacements. 4.2.2 GUIDANCE SCHEDULER Fixed CFG scales enforce static trade-off between semantic alignment and diversity, yet the optimal balance varies across decoding phases. The Guidance Scheduler dynamically adjusts the CFG scale st based on real-time signals from the unified confidence profile. The scheduler integrates three key signals to adapt st: Conditional Utilization ( (cid:98)Dt): Low (cid:98)Dt flags under-conditioning, prompting an increase in st to reinforce prompt influence. Intrinsic Volatility (Varrecent(I)): High short-term volatility in indicates instability, warranting temporary bolstering of conditioning. Rebound (Rt): Strong rebounds suggest stabilized semantics, allowing st to ease pressure and preserve diversity."
        },
        {
            "title": "ScalingAR",
            "content": "Table 1: Evaluation on GenEval (Ghosh et al., 2023) and TIIF-Bench (Wei et al., 2025) benchmarks. Diff.+AR refers to the unified architecture, and MAR indicates the masked AR architecture (Li et al., 2024). We bold the best results, and denotes that higher is better. Method #Params Arch. DALLE3 (Betker et al., 2023) Show-o (Xie et al., 2025) LightGen (Wu et al., 2025b) Infinity (Han et al., 2025) Emu3 (Han et al., 2025) Janus (Wu et al., 2025a) AR-GRPO (Yuan et al., 2025) + IS + BoN + ScalingAR (Ours) LlamaGen (Sun et al., 2024) + IS + BoN + ScalingAR (Ours) - Diff. 1.3B Diff.+AR 0.8B 2B 8.5B 1.5B 0.8B 0.8B 0.8B 0.8B 0.8B 0.8B 0.8B 0.8B MAR VAR AR AR AR AR AR AR AR AR AR AR GenEval TIIF-Bench Two Obj. Posit. Color Attr. Over. Basic Advanced Designer Over. 72.94 59.24 46.42 59.66 - - 16.22 19.84 21.08 26.35 40.35 42.44 42.02 46.47 68.45 59.89 45.76 57.81 - - 14.91 19.03 19.91 26.43 40.44 40.34 40.78 44. 62.69 68.66 59.70 61.19 - - 17.91 17.62 20.69 25.90 40.30 39.93 37.69 42.54 78.40 71.30 53.99 71.63 - - 19.59 26.00 25.67 29.71 49.58 54.81 54.79 57.36 - 0.80 0.65 0.85 0.81 0.68 0.27 0.47 0.46 0.54 0.34 0.21 0.27 0.40 - 0.31 0.22 0.49 0.49 0.46 0.02 0.08 0.08 0.24 0.21 0.11 0.11 0.28 0.67 0.68 0.62 0.73 0.66 0.61 0.31 0.44 0.44 0.49 0.32 0.14 0.15 0.36 - 0.50 0.43 0.57 0.45 0.42 0.03 0.07 0.06 0.15 0.04 0.02 0.02 0. Using these signals, we compute the raw CFG scale adjustment: sraw = sbase + α(1 (cid:98)Dt) + βVarrecent(I) γRt, (14) where α, β, γ control the relative influence of each term. The final scale st is smoothed and clamped to prevent excessive fluctuations: st = clamp((1 λcfg)st1 + λcfgsraw , smin, smax), (15) with deadband (st st1 < ϵs) suppressing jitter to ensure stability."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we conduct extensive experiments to answer the following research questions: (RQ1) Does ScalingAR enhance the quality of generated images? (RQ2) Does ScalingAR outperform other TTS strategies for both effectiveness and efficiency? (RQ3) How sensitive is ScalingAR to its key components? (RQ4) Whether ScalingAR holds advantages over other TTS strategies in terms of both scalability and robustness? 5.1 EXPERIMENTAL SETTINGS Baselines We apply ScalingAR to the advanced models: LlamaGen (512 512) (Sun et al., 2024) and AR-GRPO (256 256) (Yuan et al., 2025). Since no prior work has explored TTS for the NTP image generation, we focus our comparisons on the following conventional baselines: Importance Sampling (IS) (Owen & Zhou, 2000) and Best-of-N (BoN) (Lightman et al., 2023). We also provide results from Show-o (Xie et al., 2025), LightGen (Wu et al., 2025b), Infinity (Han et al., 2025), Emu3 (Wang et al., 2024), Janus (Wu et al., 2025a), and DALLE3 (Betker et al., 2023) for reference. Evaluations To evaluate the effectiveness of ScalingAR, we adopt GenEval (Ghosh et al., 2023) and TIIF-Bench (Wei et al., 2025) as primary benchmarks for both general and compositional text-toimage generation capabilities. These benchmarks offer comprehensive evaluation of the models ability to produce high-quality and semantically consistent images from text prompts. 5.2 PERFORMANCE & EFFICIENCY COMPARISON To answer RQ1 and RQ2, we comprehensively compare ScalingAR against two baselines on general and compositional benchmarks in Table 1, alongside qualitative results, user study, and token consumption comparisons shown in Figure 1, 4, and Figure 5. Key observations are summarized as follows: Obs.❶ ScalingAR excels in enhancing both general and compositional generation quality. As illustrated in Table 1, our ScalingAR consistently outperforms baseline methods (i.e., IS and BoN), which achieve minimal or even negative performance gains, across benchmarks targeting distinct aspects of text-to-image generation. Figure 1 (Top) and Figure 4 provide qualitative evidence of ScalingARs capabilities, showcasing visually superior results that excel in aesthetic quality and semantic alignment, e.g., numerical accuracy, color fidelity, and subject clarity. Furthermore,"
        },
        {
            "title": "ScalingAR",
            "content": "Figure 4: Qualitative results of ScalingAR. More results on AR-GRPO are provided in Appendix B. Figure 5 (Left) highlights ScalingARs effectiveness in aligning image generation with human preferences, as validated through user studies. Obs.❷ ScalingAR is token-efficient test-time AR image generation enhancer. Figure 5 (Middle) demonstrates that ScalingAR consistently surpasses other TTS strategies across benchmarks, requiring fewer visual tokens. Unlike BoN, which relies on external reward models and excessive token consumption, ScalingAR leverages intrinsic confidence signals to reduce computational overhead while maintaining high-quality outputs. 5.3 ABLATION ANALYSIS Table 2: Ablation study of ScalingAR. To answer RQ3, we perform step by step evaluations on TIIF-Bench to analyze the contributions of ScalingARs confidence profiles, as detailed in Table 2. We give the following observations: Obs.❸ Effectiveness of Intrinsic Signal Profiling. Removing Token-Level Confidence or Worst-Block Stability both lead to noticeable drop in performance, highlighting their critical role in capturing fine-grained entropy signals during visual token generation. This demonstrates the effectiveness of intrinsic signal profiling for maintaining local token stability and ensuring high-quality generation. Obs.❹ Importance of Condition State Balance. Table 2 also reveals that removing the Conditional Channel leads to significant degradation. Figure 3 (Right) further confirms its critical role in balancing interactions between text guidance and visual generation, ensuring coherent and stable outputs. For more detailed analysis, please refer to Appendix A. Bas. Adv. Des. Over. 57.4 54.1 52.3 49. Method ScalingAR w/o Conditional Channel w/o Worst-Block Stability w/o Token-Level Confidence 42.5 42.2 41.4 40.3 46.5 45.1 44.2 42.9 44.1 43.1 41.8 40."
        },
        {
            "title": "ScalingAR",
            "content": "Figure 5: (Left) User study across five dimensions: overall preference, aesthetic quality, realism fidelity, semantic alignment, attribute binding. (Middle) Visual token consumption of ScalingAR vs. baselines on TIIF-Bench. (Right) Scaling width and depth across sample number and token length. Figure 6: Robustness testing with impossible prompt. Detailed prompts are provided in Appendix A. 5.4 SCALABILITY & ROBUSTNESS ANALYSIS To answer RQ4, we compare ScalingAR with other TTS strategies (i.e., IS and BoN) in scaling width (i.e., sample number ) and depth (i.e., token length), as shown in Figure 5 (Right). To further assess the robustness of ScalingAR, we adopt the idea of impossible prompting (Bai et al., 2025b; Chen et al., 2025a) (e.g., young boy ... using chopsticks as writing instrument, ... in photo-realistic...) to evaluate its performance even when none of the candidates are ideal, with the results presented in Figure 6. Our observations are summarized as follows: Obs.❺ ScalingAR unlocks scalable generalization across both width and depth. As shown in Figure 5 (Right), ScalingAR consistently outperforms IS and BoN across varying sample numbers and token lengths. This suggests that our scaling strategy enables performance to scale up effectively as scaling width and depth increase, making it reliable solution for diverse autoregressive tasks. Obs.❻ ScalingAR empowers robust generation beyond standard scenarios. Figure 6 (Left) demonstrates that under impossible prompts for unrealistic scenarios, ScalingAR exhibits clear robustness advantages over baselines. Figure 6 (Right) further confirms that our method achieves more effective scaling when generating under challenging conditions, highlighting its adaptability and reliability in adverse scenarios."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduce ScalingAR, the first test-time scaling framework tailored to next-token prediction autoregressive image generation. Unlike existing TTS strategies, ScalingAR proposes to explore visual token entropy for the first time as intrinsic signals, without relying on partial decoding or external rewards. By adopting two-level design: Profile Level for calibrated confidence profiling and Policy Level for adaptive pruning and dynamic conditioning, ScalingAR achieves phase-aware control, enhancing generation quality with minimal additional token consumption. Comprehensive evaluations on both general and compositional capability benchmarks demonstrate that ScalingAR substantially improves the generation quality of existing AR models, along with generalizability and robustness, making it strong baseline for AR image generation test-time scaling."
        },
        {
            "title": "REFERENCES",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025a. Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. In Forty-second International Conference on Machine Learning, 2025b. URL https://openreview.net/forum?id= MNSW6U5zUA. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Harold Haodong Chen, Haojian Huang, Qifeng Chen, Harry Yang, and Ser-Nam Lim. Hierarchical fine-grained preference optimization for physically plausible video generation. arXiv preprint arXiv:2508.10858, 2025a. Harold Haodong Chen, Haojian Huang, Xianfeng Wu, Yexin Liu, Yajing Bai, Wen-Jie Shu, Harry Yang, and Ser-Nam Lim. Temporal regularization makes your video generator stronger. arXiv preprint arXiv:2503.15417, 2025b. Zhekai Chen, Ruihang Chu, Yukang Chen, Shiwei Zhang, Yujie Wei, Yingya Zhang, and Xihui Liu. Tts-var: test-time scaling framework for visual auto-regressive generation. arXiv preprint arXiv:2507.18537, 2025c. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=jQP5o1VAVc. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025. Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1573315744, 2025."
        },
        {
            "title": "ScalingAR",
            "content": "Haojian Huang, Haodong Chen, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, and Hao Fei. $mathcal{V}istamathcal{DPO}$: Video hierarchical spatial-temporal direct preference optimization for large video models. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=O2jukIZR50. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, Pheng-Ann Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. Zhewei Kang, Xuandong Zhao, and Dawn Song. Scalable best-of-n selection for large language models via self-certainty. arXiv preprint arXiv:2502.18581, 2025. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1152311532, 2022. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Art Owen and Yi Zhou. Safe and effective importance sampling. Journal of the American Statistical Association, 95(449):135143, 2000. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Dian Shao, Mingfei Shi, Shengda Xu, Haodong Chen, Yongle Huang, and Binglu Wang. Finephys: Fine-grained human action generation by explicitly incorporating physical laws for effective skeletal guidance. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19051916, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024."
        },
        {
            "title": "ScalingAR",
            "content": "Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=eskQMcIbMS. Survey Certification. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1296612977, 2025a. Xianfeng Wu, Yajing Bai, Haoze Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie Shu, Xianzu Wu, Harry Yang, et al. Lightgen: Efficient image generation through knowledge distillation and direct preference optimization. arXiv preprint arXiv:2503.08619, 2025b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=o6Ynz6OIQ6. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin"
        },
        {
            "title": "ScalingAR",
            "content": "Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=AFDcYJKhND. Featured Certification. Shihao Yuan, Yahui Liu, Yang Yue, Jingyuan Zhang, Wangmeng Zuo, Qi Wang, Fuzheng Zhang, and Guorui Zhou. Ar-grpo: Training autoregressive image generation models via reinforcement learning. arXiv preprint arXiv:2508.06924, 2025. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A MORE EXPERIMENTAL SETTINGS AND ANALYSIS",
            "content": "A.1 MORE DETAILS OF EXPERIMENTAL SETTINGS Implementation Details We implement our ScalingAR and conduct all experiments on NVIDIA H100 GPUs. Here we detail the hyperparameters. Notation λtok wtok wblk λI ρmin wI wD ysigmoid W0 λθ rec δrec Tmin Chard α β λ Definition token confidence smoothing factor token confidence weight worst-block stability weight smoothing factor for intrinsic channel score block size for spatial entropy minimum fill ratio for spatial entropy worst-q% subset size intrinsic channel weight conditional channel weight affine-sigmoid calibration warm-up period confidence threshold quantile EMA update rate for threshold recovery window recovery threshold protection horizon hard-fail confidence guard influence coefficient for condition utilization influence coefficient for intrinsic volatility influence coefficient for rebound Value 0.2 0.65 0.35 0.2 4 4 0.1 0.75 0.25 1.0 12.5% 0.2 0.2 32 0.05 5% 0.3 0.3 0.4 0.4 Captions of Figure 1 For qualitative results in Figure 1 (Top), we further detail the prompts here: 1st: red rose in full bloom sits on the top, above pink rosebud. 2nd: photo of cute puppy playing in sunny backyard. 3rd: young boy holding mysterious key, embarking on an adventure through various landscapes to find hidden treasure. 4th: masked hero jumping from rooftop, comic book style with bold outlines and dialogue bubbles. 5th: close-up of an anime womans face with shocked expression, featuring dark hair, drawn in the anime style. The image showcases colorful animation stills, close-up intensity, soft lighting, low-angle camera view, and high detail. Robustness Testing To evaluate the robustness of ScalingAR, we further employ prompts from IPV-TXT from Impossible Videos [ICML25] (Bai et al., 2025b). Specifically, since IPV-TXT was originally designed to assess the capabilities of video understanding (Wang et al., 2025; Huang et al., 2025; Bai et al., 2025a) and video generation models (Chen et al., 2025b; Wan et al., 2025; Shao et al., 2025), we filtered prompts suitable for image generation from IPV-TXT, then employed Impossible Prompt Following (IPF) as the evaluation metric, which measures the alignment between generated images and the semantic intent of impossible prompts. Following Bai et al. (2025b), we employed GPT-4o to perform binary judgments on each image based on prompt adherence. For qualitative results in Figure 6 (Right): 1st: sheep peacefully grazing in realistic meadow suddenly defies gravity as its wool expands dramatically, causing its body to balloon up like cotton cloud. The fluffy animal then lifts off from the grassy field and drifts upward into the blue sky, its transformed woolly coat acting like natural balloon. 2nd: commercial aircraft inexplicably takes off from the oceans surface as if the water were solid runway, defying physics in this photo-realistic scene. The calm, glassy sea appears to have transformed into firm platform, allowing the plane to accelerate and lift off smoothly, with spray trailing behind its wheels like it would on wet tarmac."
        },
        {
            "title": "ScalingAR",
            "content": "User Study We conducted user study to evaluate human preferences using the mean opinion score (MOS) metric. We designed user-friendly interface to facilitate the evaluation process and collected feedback from total of 15 volunteer participants. The detailed instructions provided to the participants are as follows: User Study: Autoregressive Image Generation Thank you for participating in our user study! Please follow these steps to complete your evaluation: 1. Image Generation: Carefully read the target prompt provided, and then view the provided images. 2. Scoring Criteria: Assign score to each generated image based on the following aspects (1 being the lowest, 5 being the highest): Overall Quality: The overall perceived quality and appeal of the generated image. Aesthetic Quality: The visual aesthetics, composition, and artistic merit of the image. Realism Fidelity: How realistically and faithfully the image captures the intended scene or subject matter. Semantic Alignment: How well the generated image aligns with and represents the meaning of the textual prompt. Attribute Binding: The degree to which the image accurately depicts the specific attributes and details described in the text. 3. Submission: Click the Submit Scores button to submit your scores. Notations: 1. We observe that the edge browser is not fully compatible with our interface. Chrome is recommended. 2. Remember to click the Submit Scores button after your evaluation. 3. If you see that images and the score sliders are not aligned, shrinking your page usually works. 4. If the page is not responsive for long time, please try to refresh it. 5. If you have any questions, please directly ping us. Thank you for your time and effort! A.2 MORE ANALYSIS Analysis of Hyperparameters Figure 7 presents detailed analysis of the impact of various hyperparameters on the performance of ScalingAR on the TIIF-Bench. ❶ Unified Confidence (Figure 7 (Top)): Varying the balance between the Intrinsic (wI ) and Conditional (wD) channels shows that emphasizing the Intrinsic channel slightly (wI /wD = 0.75/0.25) achieves the best TIIFBench performance across all subsets. This highlights the importance of capturing local uncertainty and stability while maintaining semantic alignment. Omitting the Conditional Channel (1.00/0.00) degrades performance, confirming its complementary role. ❷ Guidance Scheduler (Figure 7 (Bottom)): Adjusting the weights α, β, and λ, which control conditional utilization, intrinsic volatility, and confidence rebound, respectively, reveals that moderate emphasis on intrinsic volatility and rebound (β, λ) improves performance. The weight α peaks at 0.3, suggesting overemphasis may reduce diversity. This confirms the need for balanced, dynamic guidance to optimize semantic fidelity and diversity. Figure 7: Analysis of ScalingAR for weights of Unified Confidence (Top) and Guidance Scheduler (Bottom). Analysis of Adaptive Termination Gate We further analyze the impact of the confidence threshold quantile and the recovery threshold δrec on the performance and token efficiency of ScalingAR, as illustrated in Figure 8. ❶ Confidence Threshold (Figure 8 (Left)): The choice of confidence threshold critically balances pruning aggressiveness and generation quality. Setting too low leads to insufficient pruning, resulting in higher token consumption with limited accuracy gains. Conversely,"
        },
        {
            "title": "ScalingAR",
            "content": "an overly high threshold causes premature termination of promising trajectories, degrading accuracy despite lower token usage. Our experiments show that an intermediate threshold (e.g., = 0.20) achieves the best trade-off, significantly improving accuracy while maintaining efficient token consumption compared to both baseline and extreme settings. ❷ Recovery Threshold (Figure 8 (Right)): The recovery mechanism safeguards against false positives by allowing trajectories to rebound from transient confidence dips. Disabling this mechanism leads to noticeable performance drops, highlighting its necessity. Furthermore, setting the recovery threshold δrec too low or too high adversely affects accuracy and efficiency: low threshold permits premature recovery of poor trajectories, increasing token cost, while high threshold delays recovery, risking early termination of viable samples. An optimal value (e.g., δrec = 0.05 balances these effects, maximizing accuracy with minimal token overhead. Figure 8: Analysis of ScalingAR for thresholds of Confidence (Left) and Recovery (Right)."
        },
        {
            "title": "B EXHIBITION BOARD",
            "content": "We provide more comparison results here in Figure 9 on AR-GRPO and Figure 10 on LlamaGen."
        },
        {
            "title": "C LIMITATION AND FUTURE WORKS",
            "content": "ScalingAR pioneers test-time scaling for autoregressive image generation but faces key challenges. AR image modeling involves complex dependencies, making confidence estimation difficult; our exploration of token entropy is first step but may not fully capture uncertainty and semantic alignment. Additionally, the approach relies on model calibration and entropy signals, which can vary with training and architecture. Future work includes developing finer-grained confidence measures for more precise scaling, and integrating entropy-based signals into both training-time and test-time to create more unified pipeline."
        },
        {
            "title": "D THE USE OF LLMS",
            "content": "This research does not involve LLMs in terms of training or fine-tuning as part of its core contributions. The use of LLMs is limited to polishing the writing of the manuscript. These uses do not impact the originality or core methodology of the research, and therefore do not require detailed declaration."
        },
        {
            "title": "ScalingAR",
            "content": "Figure 9: More results demonstrations of ScalingAR on AR-GRPO (Yuan et al., 2025)."
        },
        {
            "title": "ScalingAR",
            "content": "Figure 10: More results demonstrations of ScalingAR on LlamaGen (Sun et al., 2024)."
        }
    ],
    "affiliations": [
        "CityUHK",
        "FDU",
        "HKUST",
        "HKUST(GZ)",
        "PolyU"
    ]
}