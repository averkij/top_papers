{
    "paper_title": "Minute-Long Videos with Dual Parallelisms",
    "authors": [
        "Zeqing Wang",
        "Bowen Zheng",
        "Xingyi Yang",
        "Yuecong Xu",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose a novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on a single GPU, we parallelize both temporal frames and model layers across GPUs. However, a naive implementation of this division faces a key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage a block-wise denoising scheme to handle this. Namely, we process a sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles a specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, a feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ a coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54$\\times$ lower latency and 1.48$\\times$ lower memory cost on 8$\\times$RTX 4090 GPUs."
        },
        {
            "title": "Start",
            "content": "Minute-Long Videos with Dual Parallelisms https://dualparal-project.github.io/dualparal.github.io/ Zeqing Wang12 Bowen Zheng13 Xingyi Yang1 Yuecong Xu1 Xinchao Wang 1 1National University of Singapore 2Xidian University 3Huazhong University of Science and Technology 5 2 0 2 7 ] . [ 1 0 7 0 1 2 . 5 0 5 2 : r zeqing.wang@stu.xidian.edu.cn xinchao@nus.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Diffusion Transformer (DiT)-based video diffusion models generate high-quality videos at scale but incur prohibitive processing latency and memory costs for long videos. To address this, we propose novel distributed inference strategy, termed DualParal. The core idea is that, instead of generating an entire video on single GPU, we parallelize both temporal frames and model layers across GPUs. However, naive implementation of this division faces key limitation: since diffusion models require synchronized noise levels across frames, this implementation leads to the serialization of original parallelisms. We leverage block-wise denoising scheme to handle this. Namely, we process sequence of frame blocks through the pipeline with progressively decreasing noise levels. Each GPU handles specific block and layer subset while passing previous results to the next GPU, enabling asynchronous computation and communication. To further optimize performance, we incorporate two key enhancements. Firstly, feature cache is implemented on each GPU to store and reuse features from the prior block as context, minimizing inter-GPU communication and redundant computation. Secondly, we employ coordinated noise initialization strategy, ensuring globally consistent temporal dynamics by sharing initial noise patterns across GPUs without extra resource costs. Together, these enable fast, artifact-free, and infinitely long video generation. Applied to the latest diffusion transformer video generator, our method efficiently produces 1,025-frame videos with up to 6.54 lower latency and 1.48 lower memory cost on 8RTX 4090 GPUs."
        },
        {
            "title": "Introduction",
            "content": "Diffusion Transformer (DiT) [18] has significantly improved the scalability of video diffusion models [13, 27, 19], enabling more realistic and higher-resolution video generation. Despite its benefits, large-scale DiT models suffer from their inherent computational inefficiency. This directly results in extended processing durations and memory demands, especially for real-time deployment [5]. Notably, this inefficiency is further exacerbated when generating long videos. Intuitively, longer videos increase the input sequence length. This has severe implications for latency: the attention mechanism, core to DiT [5, 6], exhibits time complexity that scales quadratically with sequence length. Concurrently, memory consumption escalates substantially due to the combination of large number of model parameters and the extended video sequences. Therefore, enabling DiT-based models to efficiently generate high-quality long videos remains formidable and pressing challenge. Recently, parallelization has emerged as promising solution for efficient long video generation. It uses multiple devices to produce video jointly, which scales memory and boosts processing speed. Among existing strategies, sequence parallelism reduces latency by synchronously processing split hidden [11, 17] or input [25, 12] sequences using full model replica on each device. However, Corresponding Author Preprint. Under review. they incurs high memory overhead due to the entire model on every device [25, 12]. In contrast, pipeline parallelism [6] mitigates memory usage by partitioning the model across devices as device pipeline [9, 16, 24]. Therefore, an ideal solution would combine the sequence parallelism with pipeline parallelism to maximize speed and minimize memory usage. However, naively combining sequence and pipeline parallelism is fundamentally conflicting. The core issue stems from the inherent synchronization property of video diffusion models: all input tokens must pass through an entire layer together before any can move on. In pipeline parallelism, this means the full input must finish processing on one device (e.g., Device 1) before passing to the next (e.g., Device 2). This requirement directly contradicts sequence parallelism, which splits the input across devices. As result, all distributed parts must be gathered back onto single device for serialized processing on specific model layers. Only then can all parts enter the next pipeline stage, i.e. next device. This repeated gathering serializes computation and negates the benefits of sequence parallelism, reintroducing serial bottleneck and significant communication overhead. To address this conflict, we propose novel distributed inference strategy, termed DualParal. At high level, DualParal divides both the video sequence and model into chunks and applies parallel processing across both. As discussed above, naive combination presents challenges. Inspired by recent work on interpolating diffusion and autoregressive models [1, 23], we make this feasible by implementing block-wise denoising scheme for video diffusion models. By the word block-wise, we refer to strategy where, instead of denoising all frames at uniform noise level, we divide the video into non-overlapping temporal blocks. Each block is assigned different noise level according to its position in the video: blocks closer to the end have higher noise levels, while earlier blocks receive lower noise levels. During each inference step, the model processes all blocks asynchronously, incrementally reducing their respective noise levels. Crucially, because noise levels do not need to be synchronized across all frames, block-wise denoising resolves the inherent conflict between the two parallelism strategies. Accordingly, we tailor this inference scheme for multiple devices in conjunction with our DualParal. Specifically, we organize video sequence blocks in first-in-first-out (FIFO) queue [12, 22], where noise levels decrease from tail to head. In each diffusion step, new noisy block is appended to the tail, while clean block is removed from the head. These video blocks are then processed in reverse order, tail to head, through device pipeline. In this setup, each device handles specific video block and model chunk, with denoised outputs passed asynchronously between GPUs. This distributed architecture enhances memory efficiency by distributing the model and achieves near-zero idle time through asynchronous block processing and communication. Even more compelling, DualParal leverages its FIFO queue to enable long video generation. New blocks can be continuously appended to the queue, allowing for producing arbitrarily long videos. Because the number of frames within each block remains fixed, this approach again avoids quadratic increase in processing latency and high memory costs associated with extended video sequences. To further optimize parallel efficiency and maintain video quality, we introduce two key enhancements for DualParal. Firstly, to ensure coherence between adjacent blocks, each block is concatenated with parts of previous and subsequent blocks before processed in the device pipeline, resulting extra resource costs. To reduce this, DualParal employs feature cache on each GPU that stores and reuses Key-Value (KV) features from the previous block without explicitly concatenating it. This reduce inter-GPU communication and redundant computation in components like Cross-Attention [26, 27] and Feed-Forward Networks (FFN) [26] in the latest video diffusion model Wan2.1 [27]. Secondly, to maintain global consistency across blocks without extra resource costs for global information, new block is initialized with coordinated noise space, avoiding performance degradation caused by repetitive noise, all without extra cost. Together, these enable fast, artifact-free, and infinite-length video generation. In summary, our contributions are summarized as follows: (1) We design an efficient distributed inference strategy by parallelizing both the video sequence and model layers, operating under block-wise denoising scheme, to minimize idle time and optimize both computation and memory usage. (2) We employ feature cache and coordinated noise initialization strategies to optimize parallel efficiency while preserving video quality. (3) Experiments show that DualParal achieves up to 6.54 reduction in latency and 1.48 reduction in memory cost compared to state-of-the-art distributed methods when generating 1,025-frame videos with 8RTX 4090 GPUs."
        },
        {
            "title": "2 Preliminaries",
            "content": "Diffusion models in video generation Video generation using diffusion models [7, 18, 27, 13, 8] involves progressively denoising frame latent representation xt, where denotes the noise level and ranges from (the most noisy state) to 0 (the cleanest). Here, also represents the total number of denoising steps. The process starts with complete noisy latent xT , and through each denoising step, xt is updated to clearer xt1. This continues until xT is denoised to x0, which is then decoded to generate the final video. The key operation in updating xt to xt1 involves computing the noisy prediction ϵt = Eθ(xt), where Eθ represents the diffusion model. Subsequently, xt1 is derived using xt1 = S(ϵt, xt, t), where is the updating scheduler of the corresponding video diffusion model. Specifically, the noisy frame latent is defined as xt RF HW C, where represents the number of frames, and are the height and width of each frame latent, respectively, and is the number of channels. It is important to note that after passing x0 through the decoder to generate the final video X, the dimensions of differ from those of x0 due to the upsampling process in the decoder [27, 13, 19]. For simplicity, we use the dimensions of to represent the video. Parallelisms for DiT-based video diffusion models Pipeline parallelism [9, 16, 20] typically involves evenly splitting the entire neural network across devices, with each device responsible for consecutive subset of the model, denoted as Eθ = [Eθ1 , Eθ2 , . . . , EθN ]. Since DiT-based video diffusion models [18, 27] are generally composed of multiple similar DiT blocks, we define as the total number of DiT blocks, with each device handling consecutive DiT blocks. Therefore, denoising xt is represented as: ϵt = EθN (EθN 1 (. . . (Eθ1(xt)) . . . )) = EθN (. . . (Eθj (ϵj )) . . . ), (1) where ϵj1 Rph denotes the noisy prediction from the previous (j 1)th device. Here, represents the sequence length and denotes the hidden size. Specifically, = , where , , and are the downsampled dimensions of , H, and , respectively. For the Wan2.1 model [27] used as the base in this paper, = . Therefore, we define = . Sequence parallelism divides the input xt RF HW into non-overlapping blocks, each denoted as Bt RN umB HW C, where umB is the number of frames per block. To enhance temporal coherence across adjacent blocks, several methods [12, 25] concatenate previous, subsequent, or global context frames with the current block during denoising, resulting in the extended block R(N umB +N umC )HW C, where umC denotes the number of concatenated context frames. B"
        },
        {
            "title": "3 DualParal",
            "content": "At high level, DualParal introduces dual parallelisms over both the video sequence and model layers while leveraging block-wise denoising scheme to achieve computational and memory efficiency. An overview is provided in Figure 1, with architectural details discussed in Section 3.1. To further improve efficiency, in Section 3.2, we design feature cache that reuses KV features from the previous block, reducing inter-device communication and redundant computation. Additionally, in Section 3.3, coordinated noise initialization strategy is adopted to ensure global consistency without additional resource overhead. Lastly, for better illustration the efficiency of DualParal, we provide theoretical analysis of parallel performance in Section 3.4. 3.1 Parallel architecture Naively combining sequence and pipeline parallelism introduces an inherent conflict: synchronizing noise levels across frames requires all split sequences to be gathered and processed on single device before proceeding to the next device in the device pipeline. This conflict degrades both parallelism strategies into serialized processing. As result, such degradation leads to high device idle time in standard pipeline parallelism and breaks parallel execution in sequence parallelism. Moreover, it introduces significant communication overhead due to the repeated gathering of split sequences. To address these issues, DualParal adopts dual parallelisms under block-wise denoising mechanism. Namely, DualParal simultaneously process block-wise frames with asynchronous noise levels across 3 Figure 1: Overview of DualParal: DualParal partitions video frames into sequential blocks organized in queue with noise levels increasing from tail to head, and distributes model layers across devices via device pipeline. By feeding blocks into the pipeline in reverse order (from tail to head), this block-wise denoising scheme significantly improves efficiency. To further improve performance, DualParal reuses Key-Value (KV) features from the previous block, requiring only the subsequent block to be concatenated. To preserve global consistency, each new block is initialized from shared noise pool by shuffling noises, excluding the last umc latents of the last block in queue. 2 different model chunks. Since noise levels do not need to be synchronized across frames at each model segment, DualParal effectively resolves the conflict between sequence and pipeline parallelism. Specifically, as illustrated in Figure 1, DualParal comprises two key components: queue and device pipeline. In device pipeline, DiT blocks from the video diffusion model are evenly distributed across multiple GPUs. Within the queue, each element is block of umB frame latents sharing the same noise level, formally denoted as Bi = [xi, xi, . . . , xi], where xi R1HW represents single frame latent at the ith noise level. Additionally, the queue is organized in first-in-firstout manner [12, 22], with blocks arranged from tail to head in progressively decreasing noise levels, ranging from 1 to . Formally, queue is described as = [B1, B2, . . . , BT ]. During inference, blocks in the queue are continuously fed into the device pipeline in reverse order, from tail to head. After each diffusion step, all blocks in the queue shift forward by one position, i.e., = [B0, B1, . . . , BT 1]. new noisy block BT is then appended to the tail, while the clean block B0 is removed from the head and passed to the decoder for final video reconstruction. With this implementation, each device handles specific video block and corresponding model segment, while denoised outputs are passed asynchronously between GPUs. This block-wise denoising scheme effectively resolves the serialization degradation caused by naively combining sequence and pipeline parallelism, thereby enabling true parallelization across both temporal frames and model layers. 3.2 Feature cache Since whole video frames are divided into non-overlapping blocks, we concatenate previous and subsequent blocks with total of umC frame latents to maintain temporal coherence, resulting in the denoising of an extended block = [Bi1, Bi, Bi+1]. For simplicity, we assume that all frame latents from both adjacent blocks are included, i.e., umC = 2N umB. Note that Bi1 denotes the subsequent block, while Bi+1 refers to the previous block in the reversed inference order. Therefore, denoising block i is formally described as: ϵi = EθN (EθN 1(. . . (Eθ1 (B i)) . . . )) = EθN (. . . (Eθj (ϵj1 )) . . . ), (2) where each intermediate output ϵj1 is transmitted from (j 1)th to jth device using asynchronous peer-to-peer (P2P) communication, allowing communication and computation to overlap effectively. However, this implementation introduces extra communication and computation overhead due to the concatenated parts. To mitigate this, we exploit unique feature of DualParal and propose feature 4 i+1 = [Bi, Bi+1, Bi+2], Bi+1 has already been processed during the denoising of i+1 and reuse them when denoising = [Bi1, Bi], decreasing communication overhead between adjacent devices. cache technique. Specifically, since block = [Bi1, Bi, Bi+1] is denoised after the previous block i+1. Leveraging this feature, we cache the KV features from the Self-Attention module of Bi+1 during denoising i. Consequently, the input block for denoising is reduced to Moreover, for i, adjacent blocks Bi1 and Bi+1 assist in denoising Bi. Among all model components, only those that require interaction across framessuch as the Self-Attention module in the Wan2.1 model [27]contribute meaningfully in this context. Therefore, we restrict the feature caching technique to the Self-Attention module while skipping components like Cross-Attention and FFN, which do not benefit from inter-frame information. This selective application effectively eliminates redundant computations. 3.3 Coordinated noise initialization Although DualParal concatenates previous and subsequent blocks to smooth transitions, global consistency remains challenge. simple solutionconcatenating more global informationincurs high communication, computation, and memory costs. To avoid these, reusing noisy latents from the same noise space [21, 29] offers promising alternative. This section analyzes different initialization methods to determine the best strategy for ensuring global consistency, specifically for DiT-based video diffusion models. There are two key observations: 1) Using complete noise space maintains favorable global consistency. 2) Latents with the repetitive noise during the whole denoising process cause significant performance degradation in the DiT-based video diffusion model. Figure 2: Examples of four different noise initializations for Wan2.1 model [27]: (a) uses the complete noise space, (b) uses subset of the noise space, (c) adds new noise to the original space, and (d) uses the complete noise space with the repetitive noise. The first image shows the standard video generated from the reference noise space, followed by two different orders of noise initialization. The first observation, illustrated in Figure 2, shows that using the complete noise space (a) yields better global consistency compared to using subset of noise (b) or adding new noise (c) to the original space. Based on this, we initialize blocks in DualParal using the complete noise space, with varying initialization orders. However, as shown in (d) of Figure 2, directly denoising using the same noise in Wan2.1, DiT-based video diffusion model, leads to significant performance degradation. This second observation arises from repetitive noises when concatenating the subsequent block in DualParal during whole denoising process. In contrast, the previous block only affects the Self-Attention module in Wan2.1, without causing performance degradation. To resolve this problem while preserving the complete noise spaces advantages, we propose novel initialization strategy. Specifically, as shown in Figure 1, when initializing new block, we select noise from pool that has not been used in the last umC latents of the final block BT in the queue (e.g., umC = 4 in Figure 1). These selected noises are then shuffled and used to initialize the new block. Note that the first block uses the complete noise pool and contains umC 2 + umB frames. This strategy ensures that the same noise isnt reused in concatenated blocks during the whole denoising process, while still utilizing the complete noise pool throughout the process. 2 3.4 Quantitative analysis of efficiency This section provide quantitative analysis of parallel performance of DualParal in terms of bubble ratio, communication overhead, and memory cost. For bubble ratio [9], it evaluate the ratio of idle time in each device. We compute it for DualParal under the reverse (tail-to-head) denoising order. Additionally, we assume Blocknum, where 5 Blocknum denotes the total number of blocks during long video generation. This assumption is reasonable, as it can be easily satisfied in practice for long videos, especially for minute-long videos. Therefore, the bubble ratio is formally expressed as: Bubble = Bubble Size Bubble Size + onBubble Size = 2 1 2 1 + Blocknum . (3) The detailed proof of Equation 3 is provided in Appendix A.3. To intuitively illustrate the bubble ratio, Figure 3 presents an example of the pipeline scheduling in DualParal, exhibiting an approximate bubble ratio of 5.2%. Moreover, as Blocknum increases, the bubble ratio approaches 0%, indicating minimal device idle time in the pipeline during long video generation. Figure 3: Pipeline schedule of DualParal with = 4, = 50, and Blocknum = 4. Blocks are denoised in reverse order, from tail to head in the queue. After diffusion step , the first clean block is popped from the queue, and all remaining blocks shift forward by one position, decrementing their indices accordingly. Idle time occurs during brief warm-up and cool-down phases, when the current number of blocks in the queue is smaller than the number of devices for example, before diffusion step 4 and after diffusion step in Figure 3. During these phases, some idle time and synchronization overhead may arise due to non-overlapping communication and computation. However, these periods are relatively short and thus contribute negligible overhead in the context of long video generation. Therefore, DualParal achieve high utilization of multiple GPUs. Further analysis of the bubble ratioincluding detailed proofs and discussions under different denoising conditions (e.g., different denoising order, and the case where > Blocknum)is provided in Appendix A.3. Table 1: Comparison of parallel methods for the DiT-based video diffusion model at single diffusion step. Overlap refers to the degree of overlap between communication and computation. is total memory cost of the model, while KV represents the memory cost for single frame input. Method Communication Cost Overlap Model KV Activations Memory Cost Ring Attention DeepSpeed-Ulysses Video-Infinity FIFO DualParal (Ours) 2O(p h)L 4 O(p h)L 2O(N umC h)L 2O((N umB + umC ) C) 2O((N umB + umC /2) h) 1 1 KV KV + umC )KV W (N umB + umC )KV 1 (N umB + umC )KV (F 1 To compare DualParal with other parallel methods in terms of communication and memory costs, we qualitatively evaluate it against DeepSpeed-Ulysses [11], Ring Attention [17], Video-Infinity [25], and FIFO [12]. Following the approach in previous works [5, 6], we conduct similar comparison for parallelism in video diffusion models, as shown in Table 1. For DualParal, the communication cost per device is determined by the input and output of ϵ through asynchronous P2P communication. Although DualParal requires synchronous P2P communication during the warm-up and cool-down phases, their overhead is negligible when generating long videos. Furthermore, as detailed in Section 3.2, we reduce this cost by caching the previous block, resulting in the transmission of only umB + umC frames. Regarding memory cost, thanks to the advantages of pipeline parallelism, the model cost is distributed across the number of devices, . The memory required for peak KV activation is (N umB + umC)KV , which is significantly lower than that of Ring Attention, DeepSpeed-Ulysses and Video-Infinity when generating long videos. This is due to their fixed-length generation nature, which necessitates extending the video sequence at execution time to support long video generation. In contrast, DualParal and FIFO are infinite-length generation methods that process fixed-length frame blocks at each step and can generate long videos without increasing the number of frames per block. In comparison to FIFO, DualParal shows substantial memory cost advantage (including both model and KV activations) as the scale of DiT-based video diffusion models increases. Therefore, this quantitative analysis demonstrates the superior performance of DualParal. Further details of the analysis and calculations are provided in the Appendix A.4."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Setups Base model. In the experiments, the text-to-video model Wan2.1 [27] serves as the base model. Wan2.1 is latest DiT-based video foundation model renowned for its exceptional video generation performance. It is available in two versions: the Wan2.1-1.3B model, which generates 480p videos, and the Wan2.1-14B model, which can generate both 480p and 720p videos. Metrics evaluation. To evaluate parallel efficiency, we compare all methods in terms of generation latency and memory consumption across devices. Memory consumption is measured as the peak memory overhead among all devices used during the diffusion process. For video performance, we apply VBench metrics [10] directly to long videos [29]. For each method, videos are generated based on the prompts provided by VBench for evaluation. The metrics cover all indicators in the Video Quality category, including subject consistency, background consistency, temporal flickering, motion smoothness, dynamic range, aesthetic quality, and imaging quality. Baslines. We benchmark our approach against several existing methods. First, we compare it with Ring Attention [17] and DeepSpeed-Ulysses [11], both of which are supported by the official Wan2.1. Additionally, we evaluate it alongside Video-Infinity [25] and FIFO [12], two well-established parallel techniques for long video generation. Implementation details. By default, all parameters of the diffusion are kept consistent with the original inference settings of Wan2.1 [27], with the number of denoising steps set to 50. Our experiments are conducted on Nvidia GeForce RTX 4090 (with 24G memory) for Wan2.1-1.3B and Nvidia H20 (with 96G memory and NVLink) for Wan2.1-14B. We utilized the torch.distributed tool package, employing Nvidias NCCL as the backend to facilitate efficient inter-GPU communication. We conduct experiments to evaluate the efficiency of both Wan2.1-1.3B (480p) and Wan2.1-14B (720p) in terms of latency and memory usage. For video performance, we compare all methods using the Wan2.1-1.3B (480p) version. For DualParal, we apply 1 step warmup iteration to make sure the connection between different devices. 4.2 Main results Efficiency. We first evaluate all comparing methods on extremely long videos, followed by scalability analysis. For fair comparison, we set umC = 8 for DualParal, Video-Infinity and FIFO, and set umB = 8 for both DualParal and FIFO. Table 2: Efficiency evaluation on extreme-length video generation. Experiments are conducted on 8RTX 4090 GPUs using Wan2.1-1.3B (480p). Results are reported as latency (s) / peak memory usage (GB). For extremely long videos, as shown in Table 2, DualParal achieves great efficiency. Note that DeepSpeed-Ulysses is excluded due to incompatible attention head settings with 8 GPUs. Compared to static-length generating methods like Ring Attention and Video-Infinity, DualParal shows clear advantages at 513 frames and further amplifies these advantages at 1025 frames, achieving up to 6.54 lower latency and 1.48 lower memory usage in 1025 frames. This improvement stems from the fact that staticlength generating methods require proportionally longer processing sequences as video length increases, leading to higher latency and memory consumption. In comparison to FIFOa method for infinite-length video generationDualParal still achieves up to 1.82 reduction in latency and 1.32 reduction in memory usage in 513 frames. 1025 3907.5 / 23.15 832.5 / 22.58 1022.0 / 20.61 596.9 / 15.55 513 1328.3 / 17.66 354.1 / 19.13 565.3 / 20.59 309.3 / 15.52 Ring Attention Video-Infinity FIFO DualParal Number of Frames Methods To evaluate scalability, we measure generation latency and memory usage across multiple GPUs using various methods. Experiments are conducted on 301-frame video generationthe maximum length supported by 2 GPUsand tested on both Wan2.1-1.3B (480p) and Wan2.1-14B (720p) models. As shown in Figure 4, DualParal consistently outperforms all methods. For latency, shown in (a) and (c), DualParal achieves the lowest generation time across all tested GPU counts. Meanwhile, according to Equation 3, DualParal is expected to exhibit even better scalability for longer videos. For memory usage, shown in (b) and (d), DualParal maintains the lowest peak memory consumption, with steadily decreasing trend as the number of devices increases. This efficiency stems from 7 Figure 4: Scalability analysis in terms of latency and memory cost: (a) and (b) show the scalability of Wan2.1-1.3B (480p) across different methods on 301-frame video, while (c) and (d) present the scalability of Wan2.1-14B (720p) on 301-frame video. DualParal fixed memory footprint for KV activations and reduced model weight across devices. In contrast, FIFO shows no scalability in memory usage, posing challenges for large-scale video models. Although Ring Attention, DeepSpeed-Ulysses, and Video-Infinity benefit from reduced memory and latency with more devices, they still face scalability bottlenecks when generating longer videos, as shown in Table 2. more detailed analysis of Figure 4 is provided in Appendix A.5. Video quality. We compare the video quality generated by DualParal with those produced by DeepSpeed-Ulysses, Video-Infinity, and FIFO on Wan2.1-1.3B (480p) model. Table 3 presents quantitative evaluation based on VBench [10]. Additionally, Figure 5 visualize some frames from videos generated by different methods using the same prompt. To ensure optimal video performance, we set umC = 24 with 16 local and 8 global paddings for Video-Infinity, umC = 2 and umB = 2 for FIFO, and umC = 8 and umB = 8 for DualParal. Table 3: The comparison of various video generation methods, as benchmarked by VBench. Method Number of Frames Subject consistency Background Temporal flickering consistency Motion smoothness Dynamic Aesthetic quality degree Imaging Overall Score quality DeepSpeed-Ulysses Video-Infinity FIFO DualParal DeepSpeed-Ulysses Video-Infinity FIFO DualParal 129 129 129 129 257 257 257 93.43% 82.35% 80.46% 92.92% 93.45% 86.49% 71.69% 89.15% 92.50% 88.46% 90.13% 95.68% 95.07% 89.41% 85.41% 91.80% 98.88% 98.41% 95.04% 99.46% 98.05% 98.36% 95.42% 99.34% 98.57% 97.15% 95.30% 97.28% 98.45% 97.63% 95.19% 96.82% 62.50% 59.72% 0% 59.72% 23.61% 52.78% 0% 50.00% 62.96% 57.69% 48.28% 58.86% 53.87% 58.88% 48.61% 57.35% 64.13% 81.85% 63.91% 78.24% 58.13% 75.89% 62.59% 80.93% 55.86% 74.05% 63.01% 78.08% 58.18% 64.93% 62.73% 78.17% Figure 5: Comparison of 257-frame videos. Table 3 reports quantitative results for video generation at 129 and 257 frames. Since both DeepSpeedUlysses and Ring Attention operate on full-length sequences without segmentation, only DeepSpeedUlysses is selected as representative case. In the 129-frame setting, DeepSpeed-Ulysses achieves the best performance, as it preserves the full video sequence without splitting, maintaining the 8 original generation quality of Wan2.1. However, its performance drops sharply at 257 frames due to exceeding Wan2.1s supported video length. In comparison, DualParal outperforms other distributed methodsincluding FIFO and Video-Infinityat 129 frames and achieves the highest overall score at 257 frames. For visualization, Figure 5 presents two example videos. The results align with the quantitative findings in Table 3. Specifically, directly extending video lengthas done by DeepSpeedUlyssescauses the Wan2.1 model to produce static scenes lacking motion dynamics. For FIFO, using single latent per element in queue and denoising under large noise gaps leads to cumulative quality degradation. In contrast, both Video-Infinity and DualParal perform well visually. However, Video-Infinity exhibits challenges in maintaining consistency: in the first example, the young mans head orientation shifts erratically; in the second, inconsistencies appear in the depiction of cat and human arm. DualParal, by comparison, consistently delivers superior temporal coherence across both content and motion. Further video examples are shown in Appendix A.1. 4.3 Ablation Parallel ablation. Latency (s) Memory (GB) Method Queue Device Pipeline DualParal w/o cache DualParal Table 4: Ablation study on DualParal. All settings are evaluated on 129-frame video with 84090s. The parallel architecture of DualParal consists of two main components: queue and device pipeline. By continuously feeding blocks from the queue into the device pipeline, supported by feature cache mechanism, DualParal ensures efficient and seamless dual parallelization across devices. To evaluate the individual contributions of each componentqueue, device pipeline, and feature cachewe conduct an ablation study focusing on latency, memory usage, and the ability to support infinite-length video generation. As shown in Table 4, using only the queue with single GPU results in high latency and memory consumption. In contrast, relying solely on the device pipeline cannot support infinite-length generation and remains inefficient due to underutilization of GPUs when processing single input. The complete DualParal architecture, integrating both the queue and device pipeline without cache, successfully addresses these limitations, achieving superior efficiency. With the addition of the feature cache, efficiency is further enhanced, enabling the generation of minute-long videos with ease. Infinite Length 621.37 472.25 182.71 142.62 18.81 21.18 16.76 15.52 Effectiveness of coordinated noise initialization. Through the two key observations for DiT-based video models discussed in Section 3.3, we utilize the complete noise space and avoid using the same noise throughout the entire process to bridge the temporal gap between different non-overlapping blocks. In this part, we verify the effectiveness of this approach. As shown in Figure 6, without applying noise initialization, DualParal fails to maintain temporal consistency, as seen in (a) and (b). Although (b) incorporates neighboring blocks to partially mitigate the inconsistency, the effect is limited. In contrast, after introducing noise initialization, as shown in (c), DualParal achieves significantly improved temporal coherence across frames."
        },
        {
            "title": "5 Conclusion",
            "content": "Figure 6: Video frames under different conditions: (a) umC = 0 without noise initialization; (b) umC = 8 without noise initialization; (c) umC = 8 with coordinated noise initialization. We propose DualParal, novel distributed inference strategy for DiT-based video diffusion models. By implementing block-wise denoising scheme, DualParal successfully parallelizes both temporal frames and model layers across GPUs, resulting in high efficiency for long video generation. To further enhance efficiency and video quality, DualParal reuses KV features via feature cache strategy for reducing communication and computational redundancy, and applies coordinated noise initialization to ensure global consistency without extra cost. These designs together enable efficient generation of minute-long videos."
        },
        {
            "title": "References",
            "content": "[1] Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [3] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [5] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. [6] Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, and Xibo Sun. Pipefusion: Patch-level pipeline parallelism for diffusion transformers inference. arXiv preprint arXiv:2405.14430, 2024. [7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [8] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. In The Eleventh International Conference on Learning Representations (ICLR), 2023. [9] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, and zhifeng Chen. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems (NeurIPS), 2019. [10] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [11] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. [12] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [13] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [14] Jiachen Li, Weixi Feng, Tsu-Jui Fu, Xinyi Wang, Sugato Basu, Wenhu Chen, and William Yang Wang. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [15] Jiachen Li, Qian Long, Jian Zheng, Xiaofeng Gao, Robinson Piramuthu, Wenhu Chen, and William Yang Wang. T2v-turbo-v2: Enhancing video generation model post-training through data, reward, and conditional guidance design. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. [16] Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica. Terapipe: Token-level pipeline parallelism for training large-scale language models. In Proceedings of the 38th International Conference on Machine Learning (ICML), 2021. [17] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. [18] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [19] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, Yuhui Wang, Anbang Ye, Gang Ren, Qianran Ma, Wanying Liang, Xiang Lian, Xiwen Wu, Yuting Zhong, Zhuangyan Li, Chaoyu Gong, Guojun Lei, Leijun Cheng, Limin Zhang, Minghao Li, Ruijie Zhang, Silan Hu, Shijie Huang, Xiaokang Wang, Yuanheng Zhao, Yuqi Wang, Ziang Wei, and Yang You. Open-sora 2.0: Training commercial-level video generation model in 200k. arXiv preprint arXiv:2503.09642, 2025. [20] Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [21] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free longer video diffusion via noise rescheduling. In The Twelfth International Conference on Learning Representations (ICLR), 2024. [22] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel Hoogeboom. Rolling diffusion models. In Proceedings of the 41st International Conference on Machine Learning (ICML), 2024. [23] Sand-AI. Magi-1: Autoregressive video generation at scale, 2025. [24] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism, 2020. [25] Zhenxiong Tan, Xingyi Yang, Songhua Liu, and Xinchao Wang. Video-infinity: Distributed long video generation. arXiv preprint arXiv:2406.16260, 2024. [26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [27] WanTeam. Wan: Open and advanced large-scale video generative models, 2025. [28] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In The Thirteenth International Conference on Learning Representations (ICLR), 2025. [29] Siyang Zhang, Harry Yang, and Ser-Nam Lim. Videomerge: Towards training-free long video generation. arXiv preprint arXiv:2503.09926, 2025. [30] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Results in long videos More video examples are available on our project page at https://dualparal-project.github. io/dualparal.github.io/, including minute-long videos, comparisons with other parallel methods, and diverse gallery. For generating more diverse videos, please refer to our code, available at the repository: https://github.com/DualParal-Project/DualParal. A.2 Related works DiT-based video diffusion models. Recent video diffusion models, including Wan [27], Hunyuan [13], and OpenSora [30], have transitioned the architecture from U-Net [3, 4, 14, 15] to Diffusion Transformers (DiT) [18, 2], scalable architecture for diffusion models. These models leverage full spatio-temporal attention across all dimensions and perform denoising in the latent space of pretrained 3D-VAE [13, 28], enabling more effective extraction of features from video data. With scaling from 1B [27, 30] to 14B [13, 27, 19] and still growing, DiT-based models have established strong foundation for video generation. Parallelisms for diffusion models. As DiT-based diffusion models scale, their computational and memory demands surpass the capacity of single GPU, necessitating parallelization. Existing parallel techniques for diffusion models fall into two main categories: sequence parallelism [11, 17, 25, 12] and pipeline parallelism [6]. Sequence parallelism, such as Ring Attention [17] and DeepSpeedUlysses [11], divides the hidden representations within DiT blocks, enabling parallel attention computation across different attention heads [11] or leveraging peer-to-peer (P2P) transmission for Key (K) and Value (V) [17]. Video-Infinity [25] and FIFO [12] further extend this idea: VideoInfinity partitions video frames into clips with synchronized context communication across devices, while FIFO introduces first-in-first-out queue where each element represents frame at increasing noise levels, enabling infinite-length video generation. Additionally, they still leave room for further efficiency improvements, particularly Ring Attention and DeepSpeed-Ulysses, as they rely on processing the entire video sequence. Pipeline parallelism, as in Pipefusion [6], reduces memory and communication costs by caching image patches across devices for image generation, but struggles with video generation due to the high memory cost in storing patches for all frames [5]. Long video generation. The scarcity of long video training data, combined with the high resource cost of generation, results in low-quality and inefficient outputs. Although parallelization improves efficiency, it often comes with trade-offs. Methods like Ring Attention [17] and DeepSpeedUlysses [11], which use full-sequence attention, tend to produce static videos. In contrast, approaches such as FIFO [12] and Video-Infinity [25], which partition the input video into non-overlapping blocks, suffer from poor temporal consistency. Beyond parallelization, noise initialization offers an efficient alternative. FreeNoise [21] initializes each split block using subset of the noise space. More recently, VideoMerge [29] initializes each block with the full noise space but requires latent fusion to ensure coherence between adjacent chunks. However, more favorable strategy for noise initialization in DiT-based video diffusion models remains underexplored. Furthermore, how to parallelize noise initialization without compromising generation quality has been largely overlooked. A.3 Bubble analysis This section provides an in-depth analysis of the bubble ratio introduced in Section 3.4. We begin by proving Equation 3 under the reverse denoising order (tail-to-head), assuming Blocknum. Next, we examine the bubble ratio in the case where > Blocknum. Finally, we analyze the bubble ratio under the sequential denoising order (head-to-tail). The proof of Equation 3 consists of two components: the non-bubble size and the bubble size. These correspond to the total execution time and the idle time on each device, respectively. For the nonbubble size, each block undergoes denoising steps, and there are Blocknum total blocks. Hence, the total number of denoising operations is Blocknum . Since every block must traverse all devices for full denoising, the non-bubble size on each device is also Blocknum . Regarding bubble size, idle time in each device will occur in the condition where current number of blocks in queue is smaller than the device number . This condition will occur in warmp-up and cool-down periods when assuming Blocknum in Equation 3. During these periods, as shown in Figure 3, 12 the sum of bubble size is equal to 1 + 2 + + (N 1) + 1 + 2 + + = 2 1. Therefore, the whole bubble ratio is equal to Equation 3. Figure 7: Pipeline schedule of DualParal with = 4, = 50, and Blocknum = 3. Blocks are denoised in reverse order, from tail to head in the queue. After diffusion step , the first clean block is popped from the head, and all remaining blocks shift forward by one position, incrementing their indices accordingly. When > Blocknum, as shown in Figure 7, bubbles appear in every diffusion step because the number of blocks is insufficient to fully utilize all devices. The bubble ratio in this setting is described as: Bubble = Blocknum (N ) + (T 2) + 1 Blocknum (N ) + (T 2) + 1 + Blocknum . (4) Although the bubble ratio becomes large when > Blocknum, the condition Blocknum is easily satisfied in practice, especially for minute-long video generation. Therefore, we adopt Equation 3 under the assumption Blocknum in the main paper to illustrate the bubble ratio. DualParal denoises blocks in the queue using reverse order (from tail to head). To validate the efficiency of this denoising strategy, we further present the bubble ratio under the sequential order (from head to tail) in Equation 5 and Figure 8. The calculation of the bubble ratio under the sequential denoising order follows similar approach to that of the reverse order, and we define it as: Bubble = Bubble Size Bubble Size + onBubble Size = 2 1 2 1 + Blocknum . (5) Figure 8 will give more details about the pipeline schedule of DualParal with sequential order. By Figure 8: Pipeline schedule of DualParal with = 4, = 50, and Blocknum = 4. Blocks are denoised sequentially from head to tail in the queue. After diffusion step , the first clean block is popped from the head, and all remaining blocks shift forward by one position, incrementing their indices accordingly. comparing Equation 3 and Equation 5, the reverse order yields theoretically lower bubble ratio than the sequential order. This theoretical insight is also supported by empirical observations in Figure 3 and Figure 8. Therefore, we use reverse denoising order for better efficiency. A.4 Efficiency analysis in detail This section provides more detailed quantitative analysis of the efficiency of the comparison methods in Table 1, since Section 3.4 primarily focuses on the analysis of DualParal. For Ring Attention [17], hidden sequences are split across devices, and the full model is replicated on each device. As result, each device holds 1 KV and incurs the full model memory cost . To compute attention, each device must perform P2P communication to gather 1 KV from the other devices. Since each or tensor contains activations across all video frames per h(cid:1) L, which approximates to 2O(p h)L DiT block, the total communication cost is 2O (cid:0) 1 13 as increases. Note that when computing split KV chunks is slower than communication, the computation and memory can be overlapped. For DeepSpeed-Ulysses [11], hidden sequences are also split across devices, and the full model is replicated on each device, resulting in the same memory cost as Ring Attention. Unlike Ring Attention, Ulysses uses All-to-All communication to transform sequence-wise partitioning into attention-head partitioning, enabling parallel attention computation across heads. This process involves three All-to-All transfers of approximately 1 QKV for computing attention, plus one additional All-to-All transfer to reconstruct the split hidden state before attention. Thus, the total communication cost is 4 O(p h)L, which cannot be overlapped with computation. In Video-Infinity [25], the input video sequence is divided into short clips processed across devices, where each device handles 1 frames using the whole model. To maintain temporal coherence between adjacent clips, each device collects both global and local context key-value (KV) features covering umC frames during the attention operation. Consequently, each device requires (F 1 + umC)KV costs and incurs the full model memory footprint . Since Video-Infinity does not process the entire video sequence on each device, we use 1H to represent the hidden sequence per frame, and = for the entire sequence. For communication, each device must exchange context features with others at every DiT blocks, resulting in 2O(N umC h)L overhead, which cannot be overlapped with computation. For FIFO [12], the first-in-first-out queue is split into overlapping blocks, where umB frames are denoised with the help of concatenated umC frames to maintain temporal coherence. These blocks are processed across devices using the full model. As result, each device holds (N umB + umC)KV and incurs the full model memory cost . Communication involves distributing each block to all devices and gathering the results on the main device, with cost of approximately 2O((N umB + umC) C) to transfer original frame latents, which can overlap with computation. However, the next denoising step must wait until all blocks have completed processing, limiting parallel efficiency. For DualParal, it partitions both video frames and model layers across devices. For memory usage, each device only holds 1 of the model and (N umB + umC)KV features. Communication overhead primarily arises from P2P transfers of input and output hidden sequences. Thanks to the feature cache technique, only (N umB + umC ) frames need to be transmitted, resulting in total 2 communication cost of 2O((N umB + umC ) h). Importantly, except for few non-overlapping intervals during the warm-up and cool-down phases, most communication and computation can largely overlap throughout the process. 2 A.5 Scalability analysis in detail This section provides more detailed analysis of all comparing methods in Figure 4. the For DualParal, as shown in Figure 4, full scalability potential in terms of latency is not fully demonstrated due to the limited video lengthspecifically, when approximately Blocknum = 9 on the 301-frame video used in Figure 4. This limitation primarily stems from the presence of bubble time on each device, as described in Equation 3. This additional time includes GPU idling as well as the nonoverlapping communication overhead that occurs during the warm-up and cool-down stages. To further explore the impact of different values of Blocknum on scalability, we conduct additional experiments. As illustrated in Figure 9, We evaluate DualParal with fixed umC = 8 across various video lengths, specifically using Blocknum = 4, 9, 18, and 63. The results show that as Blocknum increases, the scalability trend increasingly aligns with the ideal Figure 9: The influence of different number of blocks on the scaling ability of DualParal. Experiments are conducted on Wan2.1-1.3B (480p) using RTX4090s. To better illustrate the scaling behavior, we normalize each line. The black line represents the ideal scaling trend-proportional to the number of GPUs. 14 scalingproportional to the number of GPUs. However, when Blocknum is small, such as Blocknum = 4 (represented by the blue curve), the latency with 8 GPUs surpasses that of using only 6 GPUs. This inefficiency arises because the number of devices exceeds Blocknum, which prevents DualParal from effectively implementing seamless workload distribution. Consequently, GPU idling and synchronization overhead are introduced, leading to increased latency. As shown in Figure 4, both Ring Attention and DeepSpeed-Ulysses exhibit notable reductions in latency and memory consumption as the number of GPUs increases. In terms of memory usage, their approach by splitting the hidden sequence within DiT blocks reduces the memory required for KV activations as more GPUs are employed. However, despite these improvements, their performance remains inferior to that of DualParal in both Wan2.1-1.3B and Wan2.1-14B. Moreover, both methods are limited to fixed-length video generation and cannot support infinite-length outputs. In terms of latency scalability, as shown in Figure 4(a), Ring Attention experiences increased latency on 6RTX 4090 GPUs, mainly due to the lack of overlap between communication and computation. Furthermore, DeepSpeed-Ulysses fails to run on 8RTX 4090s because its number of attention heads is not divisible by 8, rendering multi-head parallelism infeasible. The same issue arises when running on 6H20 GPUs with Wan2.1-14B. For Video-Infinity, the method demonstrates suboptimal latency scalability due to its approach of splitting the input video sequence without whole sequence attention. To ensure fair comparison with FIFO and DualParal, we set umC = 8 in our experiments. However, in practical usage, Video-Infinity typically sets umC = 24, incorporating 16 local paddings and 8 global paddings to improve video quality. This configuration results in significantly higher latency due to the quadratic scaling with respect to sequence length. In terms of memory usage, the extensive padding used by Video-Infinity also leads to higher memory consumption, exceeding that of both Ring Attention and DeepSpeed-Ulysses. For FIFO, an existing method for infinite-length video generation, its efficiency is significantly low. As shown in Figure 4(a), FIFO demonstrates good scalability when using fewer than 8 GPUs. However, with umB = 8, the total number of splits in the queue is limited to 7, which is less than 8, thus preventing FIFO from fully utilizing the distributed GPUs. Regarding memory usage, as shown in (b) and (d), FIFO exhibits stability since it fixes umB for each chunk and deploys the entire model on every device. However, this approach leads to significant memory issues as the model scales. A.6 Limitation The warm-up and cool-down phases in DualParal are essential for constructing the dual parallelisms but introduce idle time and synchronization overhead. While this overhead is relatively minor when generating long videos, further reducing it could lead to more optimal and efficient solution."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "National University of Singapore",
        "Xidian University"
    ]
}