{
    "paper_title": "KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models",
    "authors": [
        "Fan Wang",
        "Juyong Jiang",
        "Chansung Park",
        "Sunghun Kim",
        "Jing Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training a small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LoRA stands out for its simplicity and efficiency, inspiring the development of a series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledge-aware Singular-value Adaptation (KaSA), a PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across a range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our method's efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 1 7 0 6 0 . 2 1 4 2 : r Preprint KASA: KNOWLEDGE-AWARE ADAPTATION OF LARGE LANGUAGE MODELS SINGULAR-VALUE Fan Wang, Juyong Jiang, Chansung Park, Sunghun Kim, Jing Tang The Hong Kong University of Science and Technology (Guangzhou) Electronics and Telecommunications Research Institute The Hong Kong University of Science and Technology {csfanwang,csjuyongjiang,deep.diver.csp}@gmail.com {hunkim,jingtang}@ust.hk"
        },
        {
            "title": "ABSTRACT",
            "content": "The increasing sizes of large language models (LLMs) result in significant computational overhead and memory usage when adapting these models to specific tasks or domains. Various parameter-efficient fine-tuning (PEFT) methods have been devised to mitigate these challenges by training small set of parameters for the task-specific updates of the model weights. Among PEFT methods, LoRA stands out for its simplicity and efficiency, inspiring the development of series of variants. However, LoRA and its successors disregard the knowledge that is noisy or irrelevant to the targeted task, detrimentally impacting model performance and leading to suboptimality. To address this limitation, we introduce Knowledgeaware Singular-value Adaptation (KaSA), PEFT method that leverages singular value decomposition (SVD) with knowledge-aware singular values to dynamically activate knowledge based on its relevance to the task at hand. We conduct extensive experiments across range of LLMs on tasks spanning natural language understanding (NLU), generation (NLG), instruction following, and commonsense reasoning. The experimental results demonstrate that KaSA consistently outperforms FFT and 14 popular PEFT baselines across 16 benchmarks and 4 synthetic datasets, underscoring our methods efficacy and adaptability. The source code of our method is available at https://github.com/juyongjiang/KaSA."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) pretrained on massive general domain data have shown remarkable generalization ability, facilitating their application across diverse tasks (Zhao et al., 2023; Touvron et al., 2023b; OpenAI, 2023; Yoo et al., 2024; Jiang et al., 2024). The adaptation of these pretrained language models (PLMs) to specific downstream tasks generally involves full fine-tuning (FFT), where all model parameters are updated and distinct replicas of model parameters are saved for each task (Guo et al., 2021; Mao et al., 2022; Gao et al., 2024). However, the increasing size of LLMs significantly raises the computational and memory costs associated with FFT, making FFT impractical in resource-constrained environments (Lester et al., 2021; Cai et al., 2024; Meng et al., 2024). Consequently, surge of parameter-efficient fine-tuning (PEFT) methods (Zaken et al., 2021; Li & Liang, 2021; Hu et al., 2021; Liu et al., 2023; Pfeiffer et al., 2021; Houlsby et al., 2019; Liu et al., 2024) have emerged, aiming to reduce the computational and memory costs by only updating small set of parameters while fixing the base model (Mao et al., 2022; Lialin et al., 2023). Notably, LoRA (Hu et al., 2021) is popular for its simplicity and effectiveness (Wang et al., 2024a; Liu et al., 2024; Gao et al., 2024). It reparameterizes the task-specific update Rnm with couple of low-rank matrices, and B, while keeping the base model W(0) Rnm unchanged during fine-tuning. Without loss of generality, we suppose to simplify the notation. The fine-tuning process of LoRA can be formally expressed as W(0) + = W(0) + α BA, where Equal contributors. Corresponding authors. 1 Preprint Rnr, Rmr, is the transpose of A, α is scaling constant, and the rank m. significant advantage of LoRA is its practicality in integrating the low-rank matrices back into the base model, thereby preserving the model architecture and avoiding additional inference latency (Hu et al., 2021; Han et al., 2024; Meng et al., 2024). Despite LoRAs success, its initialization strategy, which employs random Gaussian noise for and zeros for B, creates an unguided subspace for the trainable parameters, causing slow convergence and suboptimal performance (Meng et al., 2024; Wang et al., 2024a). To address this problem, PiSSA (Meng et al., 2024) and MiLoRA (Wang et al., 2024a) use singular value decomposition (SVD) for optimizing initialization. SVD can decompose any matrix into three distinct matrices (U, Σ, V), where and are semi-orthogonal matrices, and Σ is diagonal matrix containing singular values sorted in descending order. In particular, the magnitude of singular values represents the importance of parametric knowledge encapsulated in their corresponding singular vectors, with large values indicating important world knowledge and small values indicating noisy or long-tail knowledge (Yan et al., 2021; Wang et al., 2024a; Yang et al., 2023; Sharma et al., 2023). PiSSA and MiLoRA apply SVD to decompose the base model into two components: the principal components correlated with major singular values, and the residual components associated with minor singular values. Specifically, PiSSA fine-tunes the low-rank matrices, and A, initialized with principal components, while preserving the residual components frozen, resulting in faster convergence and improved model performance (Meng et al., 2024). In contrast, MiLoRA focuses on fine-tuning and initialized with the minor singular value components, while fixing the principal components, aiming to boost performance and alleviate world knowledge forgetting (Wang et al., 2024a). However, PiSSA and MiLoRA disregard two issues that can detrimentally affect model performance. Firstly, portion of the task-specific updates targets the weight changes of the noisy knowledge encoded in the base model, potentially leading to suboptimal performance. Secondly, the low-rank matrices, whether initialized with the principal or residual components, inherit knowledge from the base model. These components may include information that is irrelevant to the specific downstream task, leading to conflicts within the parametric knowledge and degrading the models representational capability. To address these problems, we propose PEFT method, named KaSA (Knowledge-aware Singularvalue Adaptation), which leverages SVD with knowledge-aware singular values to dynamically activate parametric knowledge according to its relevance to downstream tasks. Specifically, KaSA begins by performing knowledge-based SVD truncation to the base model W(0) for removing the minor singular components Wnoise Rnm that contain noisy and long-tail knowledge (Gu et al., 2024; Wang et al., 2024b; Meng et al., 2024). This process results in an SVD-truncated model Wworld Rnm that retains essential world knowledge. To maintain consistent representational space between Wworld and its task-specific updates W, KaSA reparameterizes in the SVD form, = UΣV, where Σ comprises knowledge-aware singular values (σ1, ..., σr). The singular-value adaptation offers twofold advantages: 1) reparameterizing the task-specific updates in SVD form ensures that these updates and Wworld share the same representational space, thereby preserving knowledge consistency; 2) the knowledge-aware singular values learn to activate the parametric knowledge based on its relevance to specific downstream tasks, reducing the intervention of irrelevant knowledge and enhancing model performance. We conduct extensive experiments to fine-tune LLMs of varying sizes and architectures across wide range of tasks, including natural language understanding (NLU), natural language generation (NLG), instruction following, and commonsense reasoning tasks. Substantial experimental results demonstrate that our KaSA consistently outperforms FFT and 14 existing popular PEFT baselines across variety of LLMs on 16 benchmarks and 4 synthetic datasets, highlighting its efficacy and adaptability. To summarize, in this work, our key contributions are as follows: We propose novel PEFT method, KaSA, which leverages SVD with knowledge-aware singular values to activate parametric knowledge based on its relevance to downstream tasks, achieving superior performance over FFT and existing popular PEFT techniques across various tasks. KaSA features linear framework that allows seamless integration of the singular value adaptation module with the SVD truncated model architecture, inducing no inference la2 Preprint tency. Our method also supports training distinct adaptation modules for different tasks, all sharing single base model, thereby reducing the storage needs for task-switching. We conduct extensive experiments on NLU, NLG, instruction following, and commonsense reasoning tasks using popular LLMs on well-known benchmarks. Our method consistently outperforms FFT and 14 PEFT baselines across different benchmarks and synthetic datasets, demonstrating its efficacy and adaptability. We make all high-quality synthetic instruction-following datasets generated by GPT4o publicly available 1, enabling the community to enhance the functionality of PEFT and support future research endeavors."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 PARAMETER-EFFICIENT FINE-TUNING The increasing LLM scale presents significant challenges to efficiently adapting these models to specific tasks (Lialin et al., 2023; Zhao et al., 2023). In response, surge of PEFT methods has emerged, reducing the computation burden by updating minimal set of parameters during fine-tuning (Mao et al., 2022; Karimi Mahabadi et al., 2021; Han et al., 2024). PEFT methods can be generally categorized into selective, additive, and re-parameterized methods (Ding et al., 2022; Lialin et al., 2023; Xu et al., 2023). Selective methods (Zaken et al., 2021; Sung et al., 2021; Guo et al., 2021; He et al., 2023) train predetermined set of the models existing parameters while keeping the rest of the model intact. Additive methods (Houlsby et al., 2019; He et al., 2022a; Li & Liang, 2021; Liu et al., 2023; Lester et al., 2021) introduce extra modules or parameters to fine-tune and maintain the original base model frozen. Reparametrized methods (Hu et al., 2021; Dettmers et al., 2023; Zhang et al., 2022; Valipour et al., 2023; Liu et al., 2024) reparameterize the models weight updates into an equivalent low-rank form for fine-tuning. Among reparameterized approaches, LoRA stands out for its simple yet efficient mechanism of employing two low-rank matrices to approximate task-specific updates. The fine-tuned LoRA matrices can be integrated with the base model, ensuring no inference latency. LoRA has inspired series of variants, each targeting specific improvements. For instance, DyLoRA (Valipour et al., 2023) trains the low-rank matrices across spectrum of ranks by sorting the representation learned at different ranks during training, shortening the training time. QLoRA (Dettmers et al., 2023) combines 4-bit quantization with LoRA for enhanced resource efficiency. DoRA (Liu et al., 2024) decomposes the base model into magnitude and direction components for fine-tuning, reducing the number of trainable parameters and improving performance over LoRA. Our method, KaSA, diverges from these reparametrized methods by employing knowledge-aware SVD structure, enhancing the fine-tuning efficacy further. 2.2 SINGULAR VALUE DECOMPOSITION IN NATURAL LANGUAGE PROCESSING SVD plays crucial role in Natural Language Processing (NLP) domain for various applications, such as model compression (Yuan et al., 2023; Wang et al., 2024b; Hsu et al., 2021; Chen et al., 2021), dimensionality reduction of word embeddings (Tanwar et al., 2018; Shyamasundar & Rani, 2016), and latent semantic structure analysis (Deerwester et al., 1990; Kou & Peng, 2015; Horasan et al., 2019). In the rapidly growing realm of LLMs, SVD emerges as promising, yet relatively underexplored, technique for PEFT. series of SVD-based PEFT methods exploit the relationship between SVD and matrix rank to ascertain optimal ranks for specific downstream tasks. For example, AdaLoRA (Zhang et al., 2022) employs SVD to reparameterize task-specific updates and adaptively determines the suitable rank through importance scoring, thus improving the model performance and parameter efficiency. SARA (Gu et al., 2024) conducts SVD at the initialization phase to identify the appropriate rank for each layer, thereby maintaining the benefits of LoRA and boosting performance. PiSSA (Meng et al., 2024) and MiLoRA (Wang et al., 2024a), as mentioned in Section 1, utilize SVD to optimize LoRAs initialization. Specifically, PiSSA (Meng et al., 2024) only fine-tunes the low-rank matrices initialized with the principal components associated with few largest singular values, while preserving the residual frozen. This initialization strategy facilitates faster convergence and enhanced performance. Conversely, MiLoRA (Wang et al., 2024a) fine-tunes the minor components associated with minimal singular values, enhancing model 1https://huggingface.co/llama-duo Preprint Figure 1: The architecture of our proposed KaSA encompasses two stages: (Left) knowledge-based SVD truncation to remove the noisy knowledge from the base model; (Right) knowledge-aware singular-value adaptation to adjust singular values that dynamically activate knowledge across model parameters based on its relevance to downstream tasks. performance while preserving the models world knowledge. Unlike these methods, our method emphasizes the adaptive adjustment of singular values, allowing nuanced and dynamic activation of parametric knowledge based on its importance to downstream tasks."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we commence with modeling the general PEFT process and training objective in Section 3.1. We subsequently provide detailed introduction of KaSA in Section 3.2, followed by the description of its training objective in Section 3.3. 3.1 PROBLEM STATEMENT Before introducing KaSA, it is essential to delineate and model the process and objective of PEFT for LLMs based on the Transformer architecture (Vaswani, 2017). Fundamentally, PEFT is the process of training pretrained model to targeted task using task-specific dataset. It aims to minimize the divergence between the predicted probability distribution of the fine-tuned model and the actual distribution of the training data, while only modifying small set of parameters. Consider pretrained model W(0), initially parameterized by Θ0. To adapt this model to particular task, we employ PEFT using dataset = {(xl, yl)}Q l=1 comprising input-output instances. The PEFT process utilizes limited set of parameters, denoted as Ψ, to learn the task-specific update Θ, ensuring that Ψ Θ0. This results in fine-tuned model W, parameterized by Θ0 + Θ(Ψ). The objective is to align the predicted probability distribution of with the actual distribution of training data, thereby enhancing the fine-tuned models task performance. The primary objective of PEFT is thus centered on the optimization of Ψ: L1(Ψ) = (cid:88) (cid:88) (x,y)D t=1 log(P Θ0+Θ(Ψ)(ytx, y<t)) (1) 3.2 KNOWLEDGE-AWARE SINGULAR-VALUE ADAPTATION As depicted in Fig.1, KaSA encompasses two primary stages: 1) the knowledge-based SVD truncation, which removes the noisy knowledge from the base model; and 2) knowledge-aware singularvalue adaptation, which involves adjustment of singular values that dynamically activates parametric knowledge based on its relevance to the targeted task. 4 Preprint KaSA begins with knowledge-based SVD truncation to the base model W(0) Rnm. For simplicity of denotation, we suppose m. This process factories W(0) using SVD and subsequently truncates the minor singular components Wnoise Rnm, removing noisy and long-tail knowledge and resulting in lower-rank model Wworld Rnm. We use this refined model Wworld to approximate the base model, making the adaptation of W(0) to be resembled by that of Wworld: = W(0) + = UΣV + (UΣV) = (cid:88) i=1 uiσivi + (cid:88) i=1 (uiσiv ) = (Wworld + Wnoise) + (Wworld + Wnoise) mr (cid:88) = ( i=1 uiσivi + (cid:88) i=1 uiσivi ) + ( mr (cid:88) i= (uiσiv ) + (cid:88) i=1 (uiσiv )) Wworld + Wworld = mr (cid:88) i=1 uiσivi + mr (cid:88) i=1 (uiσiv ) (2) (3) (4) (5) where Rnm, Rmm, and is the transpose of V. = [u1, ..., um] and = [v1, ..., vm] are the corresponding left and right singular vector matrices, respectively. The diagonal matrix Σ Rmm contains positive singular values (σ1, ..., σm) sorted from high to low (σ1 σ2 σm 0). The hyperparameter represents the number of truncated minor singular values, with m. The left and right singular vector matrix, and V, are semi-orthogonal: UU = VV = Im (6) where the identity matrix Im Rmm. Following the knowledge-based SVD truncation, we employ the knowledge-aware singular-value adaptation, which reparameterizes the task-specific updates of Wworld in the SVD form with knowledge-aware singular values. Therefore, the weight of model fine-tuned with KaSA can be formally expressed as: = W(0) + Wworld + ηUΣV = mr (cid:88) i=1 ui(σi)vi + η (cid:88) j=1 uj(σj)vj s.t. UU = VV = Ir (7) where Ir Rrr, η > 0 is constant scaler, the diagonal matrix Σ Rrr comprising learnable knowledge-aware singular values (σ1, ..., σr). The matrices and are semi-orthogonal, ensuring that the updates retain necessary structural properties. 3.3 TRAINING OBJECTIVE FFT typically serves as comparative performance upper bound for PEFT methods (Valipour et al., 2023). Consequently, we expect that the performance of the model fine-tuned with KaSA will closely approximate that of FFT. We denote the FFT model as Wf = W(0) + W. We impose regularization Wf WworldF , represented by the Frobenius norm, to constrain the taskspecific updates. Based on the properties of Frobenius norms, we can further explore the boundary of the task-specific updates: Wf tF +WworldF Wf tWworldF UΣVF = (cid:88) j= uj(σj)vj (8) To stabilize the model training and extend the searching space, we introduce L2 to minimize the lower boundary of Wf WworldF : L2(Σ) = UΣV2 According to the EckartYoungMirsky theorem (Eckart & Young, 1936), L2 is reformulated as: (cid:88) (cid:88) L2(Σ) = UΣV = uj(σj)vj 2 = (σj)2 (10) (9) j=1 5 j=1 Preprint Our method proposes knowledge-aware singular-value adaptation, which reparameterizes the taskspecific update in the SVD form and guides and to conform to orthogonality. Given this, we introduce L3 to constrain and adhere to orthogonality, such that: L3(Ψ) = (cid:13) (11) Overall, our methods leverage L1, L2, and L3 to serve jointly for optimizing the models task performance while adhering to SVD structure. For adjusting L2 and L3, we introduce β > 0 and γ > 0 as their corresponding scalers. The overall training objective of KaSA can be expressed as: (cid:13)VV Ir (cid:13)UU Ir (cid:13)F + (cid:13) (cid:13) (cid:13) (cid:13)F L(Ψ, Σ) = min Ψ,Σ (L1(Ψ, Σ) + βL2(Σ) + γL3(Ψ)) (12) We present the PyTorch-style pseudocode for KaSA and its training objective in Appendix A."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we evaluate KaSAs efficacy across different downstream tasks, including natural language understanding (NLU), natural language generation (NLG), instruction following, and commonsense reasoning. For NLU tasks, we evaluate KaSA with RoBERTa (Liu et al., 2021) and DeBERTaV3 (He et al., 2022b) on the GLUE (Wang et al., 2018) benchmark. For NLG tasks, we assess our method with GPT-2 (Radford et al., 2019) on the E2E NLG Challenge (Novikova et al., 2017) benchmark. We further assess instruction following performance with popular LLMs, including LLaMA3 8B (Meta, 2024), Mistal 7B (Jiang et al., 2023), Gemma 7B (Gemma Team, 2024), and LLaMA2 13B (Touvron et al., 2023b). These models are fine-tuned with different PEFT methods using four synthetic datasets generated by GPT4o, each tailored to summarization, classification, coding, and closed QA. GPT4o is then employed as judge to evaluate the fine-tuned models performance, assigning scores on scale of 10. We also follow (Kopiczko et al., 2023) and (Gao et al., 2024) to fine-tune the four models on the Alpaca dataset (Taori et al., 2023b) and report evaluation results on MT-Bench, with GPT4 serving as the judge, yielding scores within 10. Additionally, we substantiate KaSAs generality by fine-tuning LLaMA2 7B and LLaMA3 8B models on the Commonsense170K dataset (Hu et al., 2023), which includes training sets from eight commonsense reasoning datasets, and evaluating them on individual test sets of these constituent datasets. Finally, we conduct ablation studies to investigate the impacts of different components, budget parameter scalability, and the distribution of knowledge-aware singular values across various layers. All experiments are conducted on NVIDIA A100-SXM4 (80GB) GPUs, except for the NLU experiments, which are conducted on NVIDIA GeForce RTX 3090 (24GB) GPUs. 4.1 BASELINES We compare KaSA with FFT and 14 PEFT baselines to substantiate its efficacy and robustness: Adapter-based methods We consider four representative Adapter tuning methods as baselines: 1) AdapterH (Houlsby et al., 2019); 2) AdapterD (Ruckle et al., 2021); 3) AdapterL (Lin et al., 2020); and 4) AdapterP (Pfeiffer et al., 2021). LoRA-based methods We select LoRA and its variants: 1) LoRA (Hu et al., 2021); 2) DyLoRA (Valipour et al., 2023); 3) VeRA (Kopiczko et al., 2023); and 4) DoRA (Liu et al., 2024). SVD-based methods Considering that our method is associated with SVD, we chose other SVDbased PEFT baselines: 1) AdaLoRA (Zhang et al., 2022); 2) PiSSA (Meng et al., 2024); 3) MiLoRA (Wang et al., 2024a); 4) SARA (Gu et al., 2024); and 5) CorDA (Yang et al., 2024). Other methods Apart from the aforementioned baselines, we also consider other important finetuning methods: 1) FFT; and 2) BitFit (Zaken et al., 2021). To ensure fair comparison with these baselines, we meticulously replicate the experimental configurations as described in previous studies (Hu et al., 2021; Zhang et al., 2022; Gu et al., 2024). Introductions of the baselines and comprehensive details of the experimental setup are provided in Appendix and Appendix E, respectively. 4.2 NATURAL LANGUAGE UNDERSTANDING Models and Datasets. For NLU tasks, our method involves fine-tuning foundation models such as RoBERTa-base (125M), RoBERTa-large (355M) (Liu et al., 2021), and DeBERTaV3-base (He Preprint Table 1: Performance of RoBERTa-base (RoBbase) and RoBERTa-large (RoBlarge) with different adaptation methods on 6 datasets of the GLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthews correlation coefficient (Mcc.) for CoLA, Pearson correlation coefficient (Pcc.) for STS-B, and accuracy (Acc.) for all the remaining tasks. We report the average result of five runs with different random seeds. The best results for each dataset are shown in bold. Higher is better for all metrics. Model(Method) RoBbase(FFT) RoBbase(BitFit) RoBbase(AdptD) RoBbase(AdptD) RoBbase(LoRA) RoBbase(AdaLoRA) RoBbase(DyLoRA) RoBbase(PiSSA) RoBbase(MiLoRA) RoBbase(KaSA) RoBlarge(FFT) RoBlarge(AdptP) RoBlarge(AdptP) RoBlarge(AdptH) RoBlarge(AdptH) RoBlarge(LoRA) RoBlarge(KaSA) # Trainable Parameters SST-2 MRPC (Acc.) (Acc.) CoLA QNLI (Acc.) (Mcc.) RTE (Acc.) STS-B (Pcc.) All Avg. 125.0M 94.8 0.1M 93.7 0.3M 94.2 0.9M 94.7 0.3M 95.1 0.3M 94.5 0.3M 94.3 0.3M 95.0 0.3M 94.6 0.3M 95.2 355.0M 96.4 3.0M 96.1 0.8M 96.6 6.0M 96.2 0.8M 96.3 0.8M 96.2 0.8M 96. 90.2 92.7 88.5 88.4 89.7 88.7 89.5 88.2 88.7 90.7 90.9 90.2 89.7 88.7 87.7 90.2 91.2 63.6 62.0 60.8 62.6 63.4 62.0 61.1 65.5 63.1 65.8 68.0 68.3 67.8 66.5 66.3 68.2 69.4 92.8 91.8 93.1 93.0 93.3 93.1 92.2 92.0 92.8 93.3 94.7 94.8 94.8 94.7 94.7 94.8 94. 78.7 81.5 71.5 75.9 78.4 81.0 78.7 75.1 80.5 81.6 86.6 83.8 80.1 83.4 72.9 85.2 88.8 91.2 90.8 89.7 90.3 91.5 90.5 91.1 90.4 91.3 91.1 92.4 92.1 91.9 91.0 91.5 92.3 92.5 85.2 85.4 83.0 84.2 85.2 85.0 84.5 84.4 85.2 86.3 88.2 87.6 86.8 86.8 84.9 87.8 89. Table 2: Performance of DeBERTaV3-base (DeBv3) with different adaptation methods on 6 datasets of the GLUE benchmark. We report the average result of five runs with different random seeds. The best results for each dataset are shown in bold. Higher is better for all metrics. Model(Method) DeBv3(FFT) DeBv3(AdptH) DeBv3(AdptP) DeBv3(LoRA) DeBv3(AdaLoRA) DeBv3(PiSSA) DeBv3(MiLoRA) DeBv3(KaSA) # Trainable Parameters SST-2 MRPC (Acc.) (Acc.) CoLA QNLI (Acc.) (Mcc.) RTE (Acc.) STS-B (Pcc.) 184.0M 95.63 0.6M 95.30 0.6M 95.53 0.3M 94.95 0.3M 95.80 0.3M 95.30 0.3M 95.99 0.3M 96.22 89.46 89.22 89.22 89.71 90.44 91.42 89.71 91.42 69.19 67.87 69.48 68.71 70.04 70.29 70.34 70.41 94.03 93.76 93.98 94.03 94.49 93.59 94.14 94. 83.75 85.56 84.12 85.56 87.36 84.84 85.92 88.09 91.60 91.30 91.52 91.68 91.63 91.37 90.28 91.62 All Avg. 87.28 87.17 87.31 87.44 88.29 87.80 87.73 88.72 et al., 2022b) using the GLUE (General Language Understanding Evaluation) benchmark (Wang et al., 2018). The GLUE benchmark encompasses wide array of datasets designed to test various aspects of NLU, including question answering, natural language inference, sentiment analysis, and textual entailment. In this context, our evaluation is conducted across 6 datasets from the GLUE: SST-2, MRPC, CoLA, QNLI, RTE, and STS-B. Detailed statistical information about the GLUE benchmark can be found in Appendix C.1. Implementation Details. Basically, we follow the experimental setup applied in (Hu et al., 2021; Zhang et al., 2022) to ensure fair comparison. We randomly initialize the knowledge-aware singular values without bias, which only introduces negligible coefficients in each layer. For all evaluated datasets in GLUE, we meticulously tune the hyperparameters, including the learning rates lr [1E-5, 1E-3], the rank of SVD truncation {1, 2, 4, 8, 16, 32, 64, 128}, and two trade-off loss coefficients β [1E-5, 1] and γ [1E-5, 1]. The results we present are the median outcomes from 5 runs, each conducted with distinct random seed. To maintain fair trainable parameters, we fine-tune the query and value weights in each Transformer block and set rank = 8 across all datasets. More detailed hyperparameters are presented in Appendix E.1. Main Results. Table 1 presents the performance of RoBERTa-base and RoBERTa-large models fine-tuned using our KaSA in contrast to PEFT baselines. KaSA achieves the best performance across all datasets except MRPC and STS-B for the RoBERTa-base model. Notably, KaSA registers the highest average performances for both RoBERTa models: 86.3% for RoBERTa-base and 89.0% 7 Preprint Table 3: Performance of GPT-2 Medium and Large models with different adaptation methods on the E2E NLG Challenge. For all metrics, higher values indicate better performance. indicates that the results are reported in prior works. Best results are shown in bold. Model(Method) GPT-2Medium(FFT*) GPT-2Medium(AdptL*) GPT-2Medium(AdptL*) GPT-2Medium(AdptH*) GPT-2Medium(LoRA*) GPT-2Medium(AdaLoRA) GPT-2Medium(DyLoRA) GPT-2Medium(VeRA) GPT-2Medium(SARA) GPT-2Medium(KaSA) GPT-2Large(FFT*) GPT-2Large(AdptL*) GPT-2Large(AdptL*) GPT-2Large(LoRA*) GPT-2Large(KaSA) # Trainable Parameters BLEU NIST METEOR ROUGE-L CIDEr 354.92M 68.2 0.37M 66.3 11.09M 68.9 11.09M 67.3 0.35M 70.4 0.38M 68.2 0.39M 69.2 0.098M 69.1 0.33M 70.4 0.35M 70.6 774.03M 68.5 0.88M 69.1 23.00M 68.9 0.77M 70.4 0.77M 70.5 8.62 8.41 8.71 8.50 8.85 8.58 8.75 8.71 8.84 8.86 8.78 8.68 8.70 8.89 8. 46.2 45.0 46.1 46.0 46.8 44.1 46.3 46.3 46.7 46.9 46.0 46.3 46.1 46.8 47.0 71.0 69.8 71.3 70.7 71.8 70.7 70.8 70.8 72.3 72.1 69.9 71.4 71.3 72.0 72.0 2.47 2.40 2.47 2.44 2.53 2.35 2.46 2.43 2.55 2.55 2.45 2.49 2.45 2.47 2. for RoBERTa-large. This underscores the effectiveness, adaptability, and scalability of our proposed In significant comparison with FFT, our KaSA, which utilizes merely up to 0.24% approach. (approximately 0.3M/125.0M) of trainable parameters, outperforms FFT in 13 out of 14 scenarios and matches its performance on the STS-B dataset for the RoBERTa-base model. Furthermore, as demonstrated in Table 2, the DeBERTaV3-base results consistently surpass all baseline performances across the datasets, with the exception of STS-B, achieving the highest average performance of 88.72%. This further validates the efficacy of our method across different model architectures. 4.3 NATURAL LANGUAGE GENERATION Models and Datasets. For NLG tasks, we employ KaSA and other PEFT baselines to fine-tune both GPT-2 Medium (355M) and GPT-2 Large (774M) models (Radford et al., 2019) on the wellestablished E2E (End-to-End) NLG Challenge benchmark (Novikova et al., 2017), which focuses on restaurant domain information. The statistics of the E2E NLG Challenge benchmark and the evaluation metrics applied are detailed in Appendix C.2. Implementation Details. We adopt the experimental configurations delineated in (Hu et al., 2021; Gu et al., 2024) for the fine-tuning of query and value weights within each Transformer block, setting rank of = 4. The AdamW optimizer is employed, paired with linear learning rate schedule over 5 epochs. The reported results represent the mean outcomes from 3 runs, each initialized with distinct random seed, selecting the performance at the last epoch of each run for comparison. For further details on the hyperparameters utilized, refer to Appendix E.2. Main Results. We present the performance comparison in Table 3. As can be seen, our method consistently outshines the baselines in language generation capabilities across various evaluated metrics. More specifically, regarding the GPT-2 Medium model, KaSA outperforms the baselines in 4 out of 5 metrics and achieves comparable performance (72.1 vs. 72.3) in the ROUGE-L metric with the top-performing baseline, SARA. In the GPT-2 Large model, KaSA surpasses the baselines across all metrics, further confirming its superior performance and scalability. 4. INSTRUCTION FOLLOWING Models and Datasets. To validate KaSAs adaptability and versatility, we extend our experiments to include instruction tuning of LLaMA3 8B (Meta, 2024), Mistral 7B (Jiang et al., 2023), Gemma 7B (Gemma Team, 2024), and LLaMA2 13B (Touvron et al., 2023b). We fine-tune the models using four synthetic instruction-following datasets produced by GPT4o, each containing 128K samples, covering tasks such as summarization, classification, coding, and closed QA. Additionally, we finetune using the Alpaca dataset (Taori et al., 2023b) and report the evaluation results on MT-Bench 8 Preprint Table 4: Instruction following evaluation results with average scores for the most popular LLMs fine-tuned on the 128k synthetic datasets and the Alpaca dataset, and evaluated by GPT4o and GPT4 with the scores within 10 on test subsets and MT-Bench, respectively. Significance is tested at the α = 0.05 level, and the p-value of the significance tests is reported. Model Method # Trainable Parameters Classification Summarization Coding Closed QA MT-Bench Gemma 7B Mistral 7B LLaMA3 8B LLaMA2 13B w/o FT FFT LoRA PiSSA MiLoRA KaSA w/o FT FFT LoRA PiSSA MiLoRA KaSA w/o FT FFT LoRA PiSSA MiLoRA KaSA w/o FT FFT LoRA PiSSA MiLoRA KaSA 2.41 5.58 - 8.54B 3.21M 5.980.3 (p=0.001) 3.21M 6.230.2 (p=0.002) 3.21M 6.300.1 (p=0.001) 3.22M 6.880. 2.31 6.73 - 7.25B 3.40M 5.070.3 (p=0.007) 3.40M 5.460.2 (p=0.103) 3.40M 5.330.2 (p=0.025) 3.41M 5.720.2 2.04 5.44 - 8.03B 3.40M 6.120.3 (p=0.019) 3.40M 6.350.1 (p=0.028) 3.40M 6.370.2 (p=0.083) 3.41M 6.550. 1.00 5.86 - 13.02B 6.55M 6.230.4 (p=0.023) 6.55M 6.470.3 (p=0.062) 6.55M 6.450.2 (p=0.020) 6.56M 6.860.2 2.28 7.78 7.290.2 (p=0.002) 7.880.1 (p=0.730) 7.620.2 (p=0.067) 7.920.2 2.81 7.18 5.720.2 (p=0.000) 5.860.3 (p=0.002) 5.890.4 (p=0.006) 6.820.3 2.03 7.80 7.200.4 (p=0.016) 7.310.3 (p=0.011) 7.610.1 (p=0.014) 7.830. 1.08 7.93 7.380.2 (p=0.005) 7.450.3 (p=0.031) 7.630.1 (p=0.032) 7.920.2 3.07 7.61 7.750.2 (p=0.049) 7.800.1 (p=0.018) 7.710.2 (p=0.028) 8.010.1 2.32 7.53 6.170.4 (p=0.034) 6.410.2 (p=0.048) 6.520.2 (p=0.158) 6.740.2 2.86 7.59 7.370.2 (p=0.006) 7.590.1 (p=0.028) 7.650.2 (p=0.128) 7.890.2 1.01 7.88 7.540.2 (p=0.005) 7.830.1 (p=0.049) 7.850.1 (p=0.064) 8.090.2 2.95 8.88 8.180.2 (p=0.002) 8.220.2 (p=0.003) 8.270.3 (p=0.029) 8.690. 3.02 8.75 7.390.2 (p=0.034) 7.240.2 (p=0.007) 7.280.3 (p=0.031) 7.750.2 3.33 8.90 6.020.2 (p=0.002) 6.180.3 (p=0.018) 6.390.1 (p=0.029) 6.810.3 1.27 8.97 6.250.3 (p=0.001) 6.540.3 (p=0.006) 6.820.2 (p=0.028) 7.120.1 2.56 4.69 4.320.4 (p=0.032) 4.660.3 (p=0.182) 4.530.2 (p=0.041) 4.970.3 1.16 4.22 4.180.3 (p=0.057) 4.240.2 (p=0.043) 4.290.2 (p=0.074) 4.580.2 3.11 4.11 4.190.3 (p=0.020) 4.260.2 (p=0.013) 4.320.2 (p=0.025) 4.710. 1.01 4.37 4.430.3 (p=0.011) 4.390.2 (p=0.001) 4.510.3 (p=0.024) 4.950.1 (Zheng et al., 2023), with GPT4 serving as the judge, yielding scores within 10. The detailed processing and statistical information of the synthetic datasets, Alpaca, and MT-Bench are presented in Appendix C.3 and C.4, respectively. Implementation Details. Following the experimental setup in (Park et al., 2024), we use the summarization, classification, coding, and closed QA subsets from the No Robots (Rajani et al., 2023) dataset as seeds to create distinct synthetic datasets via GPT4o. We fine-tune the mentioned LLMs using these datasets and then prompt each fine-tuned model to generate four responses based on prompts sampled from the test subsets of the seed dataset. To ensure fair comparisons, we maintain consistent fine-tuning and inference configuration across all fine-tuned models. We subsequently use GPT4o as judge to apply single-answer grading strategies to evaluate the response quality of the fine-tuned LLMs on scale from 1 to 10. For the Alpaca dataset, we fine-tune the specified models and prompt them to generate responses to questions from MT-Bench, with GPT4 serving as judge, assigning scores within 10. Detailed prompts for data synthesis and performance evaluation, along with hyperparameter settings, are presented in Appendix C.3, D, and E.3, respectively. Main Results. In Table 4, the results show that KaSA consistently surpasses LoRA, PiSSA, and MiLoRA across four 128k synthetic datasets, regardless of the model used. Notably, Gemma 7B and LLaMA3 8B, fine-tuned with KaSA, even surpass FFT in the classification, summarization, and coding datasets. In the evaluation using MT-Bench, KaSA consistently outperforms FFT and PEFT baselines on all models, showing remarkable efficacy. With p-values less than 0.05 in 9 out of 12 experimental settings on MT-Bench, KaSA demonstrates significant performance improvements over LoRA, PiSSA, and MiLoRA. These results further highlight the effectiveness, robustness, and adaptability of our method. 4.5 COMMONSENSE REASONING Models and Datasets. Following (Wang et al., 2024a), we fine-tune the LLaMA2 7B (Touvron et al., 2023a) and the LLaMA3 8B (Meta, 2024) models using the Commonsense170K dataset, aiming to conduct comprehensive evaluation across eight well-known commonsense reasoning tasks: BoolQ (Clark et al., 2019), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), ARC-e, ARC-c (Clark et al., 2018), and OBQA (Mihaylov et al., 2018). 9 Preprint Table 5: Performance comparison of LLaMA2 7B and LLaMA3 8B with different adaptation methods on eight commonsense reasoning datasets. The symbol indicates that the results are taken from (Wang et al., 2024a). The best results are shown in bold. Higher is better for all tasks. denotes that the best results do not surpass ChatGPT. Model Method BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Avg. ChatGPT - LLaMA2 7B LLaMA3 8B LoRA PiSSA MiLoRA KaSA LoRA PiSSA MiLoRA KaSA 73.1 69.8 67.6 67.6 73.6 70.8 67.1 68.8 73.6 85.4 79.9 78.1 83.8 84.4 85.2 81.1 86.7 88. 68.5 79.5 78.4 80.1 80.2 79.9 77.2 77.2 80.4 78.5 83.6 76.6 88.2 91.5 91.7 83.6 92.9 94. 66.1 82.6 78.0 82.0 84.5 84.3 78.9 85.6 85.5 89.8 79.8 75.8 82.8 84.7 84.2 77.7 86.8 89.7 79.9 64.7 60.2 68.8 72.1 71.2 63.2 75.5 79.4 74.8 81.0 75.6 80.6 81.2 79.0 74.6 81.8 85. 77.0 77.6 73.8 79.2 81.5 80.8 75.4 81.9 84.6 Figure 2: Components ablation study about knowledge-based SVD truncation, knowledge-aware singular value adaptation, singular value regularization L2, and orthogonal regularization L3 on MRPC, CoLA, and RTE datasets. Implementation Details. To ensure fair comparison, we implement our KaSA within the LLMAdapters framework 2 (Hu et al., 2023), following MiLoRA (Wang et al., 2024a). We adhere strictly to the hyperparameter configurations for training and evaluation as specified by (Wang et al., 2024a) and (Hu et al., 2023), without any tuning, such as tuning the training epochs and learning rate. For detailed hyperparameters utilized, refer to Appendix E.4. Main Results. As illustrated in Table 5, KaSA consistently surpasses all established baselines for both LLaMA2 7B and LLaMA3 8B across all eight benchmarks when using identical hyperparameter settings. Notably, KaSA achieves the highest average score, reflecting significant performance improvements across diverse range of reasoning tasks. These results, obtained from rigorously controlled comparisons, align with our observations in NLU, NLG, and instruction following tasks. This consistency further corroborates the robustness and superiority of our method. 4.6 IN-DEPTH ANALYSIS Components Ablation Study. Our method encompasses four principle components: knowledgebased SVD truncation, knowledge-aware singular value adaptation, singular value regularization L2, and orthogonal regularization L3. To examine the collective contributions of these components, we conduct ablation experiments on MRPC, CoLA, and RTE datasets from GLUE using the RoBERTa-base. Specifically, we compare KaSA with the following variants: (1) standard LoRA (as the base); (2) SVD truncation + LoRA; (3) SVD truncation + knowledge-aware singular-value adaptation; (4) SVD truncation + knowledge-aware singular-value adaptation + L2; (5) SVD truncation + knowledge-aware singular-value adaptation + L2 + L3. From the results in Figure 2, we observe that the model performances continually increase as more components are involved in the fine-tuning. The fifth bar in Figure 2 shows that variant (5), the full implementation of KaSA, achieves significant performance improvements across all three datasets. Conversely, excluding any of these components results in performance declines ranging from 2.05% to 3.25%, underscoring their collective importance in enhancing KaSAs effectiveness. Additional results of the components ablation study on SST-2, QNLI, and STS-B datasets are detailed in Appendix F.1. 2https://github.com/AGI-Edgerunners/LLM-Adapters 10 Preprint Figure 3: Budget parameter scalability of fine-tuning RoBERTa-base with LoRA, PiSSA, MiLoRA, and KaSA on MRPC, CoLA, and RTE datasets. Figure 4: The final distribution of knowledge-aware singular values for Wq and Wv upon finetuning the RoBERTa-base model on the MNLI and QQP benchmarks. In this context, the x-axis corresponds to the layer index, and the y-axis denotes the position index. Each value signifies the relevance of the associated knowledge. Budget Parameter Scalability. We compare the performance of fine-tuning RoBERTa-base with LoRA, PiSSA, MiLoRA, and KaSA across various scales of trainable parameters. Specifically, we employ these methods to the query and value weights of the transformer block and use range of ranks = {1, 2, 4, 8, 16, 32, 64, 128} to control the parameter scales. Figure 3 shows that KaSA consistently outperforms LoRA, as well as the SVD-based baselines, at equivalent parameter scales across various datasets, indicating our methods efficacy and robustness. Moreover, we observe that enlarging trainable parameter scales does not invariably result in performance improvement. Notably, both methods peak in performance at = 8, with KaSA enhancing LoRA by 1.96% on MRPC, 2.05% Mcc. on CoLA, and 2.53% Acc. on RTE. Knowledge-Aware Singular-Value. The conventional FFT, which updates all parameters indiscriminately, often incorporates irrelevant or minimally contributory knowledge to the task at hand, leading to overfitting and decline in model generalization capability (Valipour et al., 2023). To this end, we propose novel knowledge-aware singular value module to adaptively activate the relevant task-specific knowledge. To validate our motivation, we visualize the knowledge-aware singular values of Wq and Wv when fine-tuning RoBERTa-base on the MNLI and QQP benchmarks, as depicted in Figure 4. We can clearly observe that different scales of singular values are allocated across different layers, indicating that it dynamically prioritizes knowledge across parameters."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce PEFT method, KaSA, which incorporates SVD with knowledge-aware singular values for dynamic knowledge activation of parametric knowledge according to their relevance to given task. KaSA commences knowledge-based SVD truncation of minor singular value components to remove noisy knowledge within the base model. Subsequently, it reparameterizes task-specific updates in the SVD form, leveraging knowledge-aware singular values for dynamic knowledge activation according to relevance. Our extensive experiments on various LLMs across tasks in NLU, NLG, instruction following, and commonsense reasoning reveal that KaSA consistently surpasses FFT and variety of popular PEFT baselines across well-known benchmarks and our synthetic datasets, highlighting the superiority of our method. 11 Preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts. arXiv preprint arXiv:2407.06204, 2024. Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui Hsieh. Drone: Data-aware low-rank compression for large nlp models. Advances in neural information processing systems, 34:29321 29334, 2021. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Scott Deerwester, Susan Dumais, George Furnas, Thomas Landauer, and Richard Harshman. Indexing by latent semantic analysis. Journal of the American society for information science, 41 (6):391407, 1990. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. Delta tuning: comprehensive study of parameter efficient methods for pre-trained language models. arXiv preprint arXiv:2203.06904, 2022. Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211218, 1936. Ziqi Gao, Qichao Wang, Aochuan Chen, Zijing Liu, Bingzhe Wu, Liang Chen, and Jia Li. Parameter-efficient fine-tuning with discrete fourier transform. arXiv preprint arXiv:2405.03003, 2024. Gemma Team. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Jihao Gu, Shuai Chen, Zelin Wang, Yibo Zhang, and Ping Gong. Sara: Singular-value based adaptive low-rank adaption. arXiv preprint arXiv:2408.03290, 2024. Demi Guo, Alexander Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 48844896, 2021. Zeyu Han, Chao Gao, Jinyang Liu, Sai Qian Zhang, et al. Parameter-efficient fine-tuning for large models: comprehensive survey. arXiv preprint arXiv:2403.14608, 2024. Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual In Proceedings of the IEEE/CVF International Conference on parameter-efficient fine-tuning. Computer Vision, pp. 1182511835, 2023. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards unified view of parameter-efficient transfer learning. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum?id=0RDcd5Axok. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving deberta using electrastyle pre-training with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations, 2022b. 12 Preprint Fahrettin Horasan, Hasan Erbay, Fatih Varcın, and Emre Deniz. Alternate low-rank matrix approximation in latent semantic analysis. Scientific Programming, 2019(1):1095643, 2019. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 27902799. PMLR, 2019. Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, and Hongxia Jin. Language model In International Conference on Learning compression with weighted low-rank factorization. Representations, 2021. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, In International Conference on et al. Lora: Low-rank adaptation of large language models. Learning Representations, 2021. Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. survey on large language models for code generation. arXiv preprint arXiv:2406.00515, 2024. Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. Advances in Neural Information Processing Systems, 34:1022 1035, 2021. Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454, 2023. Gang Kou and Yi Peng. An application of latent semantic analysis for text categorization. International Journal of Computers Communications & Control, 10(3):357369, 2015. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, 2021. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 45824597, 2021. Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647, 2023. Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 441459, 2020. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, KwangTing Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023. Zhuang Liu, Wayne Lin, Ya Shi, and Jun Zhao. robustly optimized bert pre-training approach with post-training. In China National Conference on Chinese Computational Linguistics, pp. 471484. Springer, 2021. 13 Preprint Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen Tau Yih, and Madian Khabsa. Unipelt: unified framework for parameter-efficient language model tuning. In 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022, pp. 6253 6264. Association for Computational Linguistics (ACL), 2022. Fanxu Meng, Zhaohui Wang, and Muhan Zhang. Pissa: Principal singular values and singular vectors adaptation of large language models. arXiv preprint arXiv:2404.02948, 2024. Meta. Introducing Meta Llama 3: The most capable openly available LLM to date. https: //ai.meta.com/blog/meta-llama-3/, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Jekaterina Novikova, Ondˇrej Duˇsek, and Verena Rieser. The e2e dataset: New challenges for end-toend generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pp. 201206, 2017. OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. URL https://arxiv.org/ abs/2303.08774. Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jing Tang, and Sunghun Kim. Llamaduo: Llmops pipeline for seamless migration from service llms to small-scale local llms. arXiv preprint arXiv:2408.13467, 2024. Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckle, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 487503, 2021. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Nazneen Rajani, Lewis Tunstall, Edward Beeching, Nathan Lambert, Alexander M. Rush, and Thomas Wolf. No robots. https://huggingface.co/datasets/HuggingFaceH4/ no_robots, 2023. Andreas Ruckle, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and In Proceedings Iryna Gurevych. Adapterdrop: On the efficiency of adapters in transformers. of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 79307946, 2021. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Pratyusha Sharma, Jordan Ash, and Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction. arXiv preprint arXiv:2312.13558, 2023. LB Shyamasundar and Jhansi Rani. Twitter sentiment analysis with different feature extractors and dimensionality reduction using supervised learning algorithms. In 2016 IEEE Annual India Conference (INDICON), pp. 16. IEEE, 2016. Yi-Lin Sung, Varun Nair, and Colin Raffel. Training neural networks with fixed sparse masks. Advances in Neural Information Processing Systems, 34:2419324205, 2021. Sudeep Tanwar, Tilak Ramani, and Sudhanshu Tyagi. Dimensionality reduction using pca and svd in big data: comparative case study. In Future Internet Technologies and Trends: First International Conference, ICFITT 2017, Surat, India, August 31-September 2, 2017, Proceedings 1, pp. 116125. Springer, 2018. Preprint Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023a. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023b. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: ParameterIn Proefficient tuning of pre-trained models using dynamic search-free low-rank adaptation. ceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 32663279, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. In Glue: multi-task benchmark and analysis platform for natural language understanding. International Conference on Learning Representations, 2018. Hanqing Wang, Zeguan Xiao, Yixia Li, Shuo Wang, Guanhua Chen, and Yun Chen. Milora: Harnessing minor singular components for parameter-efficient llm finetuning. arXiv preprint arXiv:2406.09044, 2024a. Xin Wang, Yu Zheng, Zhongwei Wan, and Mi Zhang. Svd-llm: Truncation-aware singular value decomposition for large language model compression. arXiv preprint arXiv:2403.07378, 2024b. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, 2023. Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang. Parameter-efficient fine-tuning methods for pretrained language models: critical review and assessment. arXiv preprint arXiv:2312.12148, 2023. Chao Yan, Yankun Zhang, Weiyi Zhong, Can Zhang, and Baogui Xin. truncated svd-based arima model for multiple qos prediction in mobile edge computing. Tsinghua Science and Technology, 27(2):315324, 2021. Miaorui Yang, Yonggang Xu, Kun Zhang, and Xiangfeng Zhang. Singular component decomposition and its application in rolling bearing fault diagnosis. Measurement Science and Technology, 35(1):015120, 2023. Yibo Yang, Xiaojie Li, Zhongzhu Zhou, Shuaiwen Leon Song, Jianlong Wu, Liqiang Nie, and Bernard Ghanem. Corda: Context-oriented decomposition adaptation of large language models for task-aware parameter-efficient fine-tuning. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, et al. Hyperclova technical report. arXiv preprint arXiv:2404.01954, 2024. Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, and Guangyu Sun. Asvd: Activation-aware singular value decomposition for compressing large language models. arXiv preprint arXiv:2312.05821, 2023. Preprint Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2022. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 16 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 Preprint"
        },
        {
            "title": "A PSEUDOCODE FOR KASA",
            "content": "Algorithm 1 PyTorch-style pseudocode for KaSA. class KaSA(nn.Module): def __init__(self, ): rank: int = 8, # kasa rank alpha: int = 16, # kasa alpha base_layer: nn.Module # pre-trained layer # definitions self.r = rank self.alpha = alpha self.scaling = alpha / rank self.in_features, self.out_features = base_layer.in_features, base_layer.out_features # Step 1: knowledge-based SVD truncation self.svd_rank = self.in_features - self.r U, S, Vh = torch.linalg.svd(base_layer.weight.data, full_matrices=False) base_layer.weight.data = U[:, :self.svd_rank] @ torch.diag(S[:self.svd_rank]) @ Vh[:self.svd_rank, :] self.base_layer = base_layer # Step 2: knowledge-aware singular-value adaptation self.delta_v = nn.Linear(self.in_features, self.r, bias=False) self.delta_sigma = torch.diag(nn.Parameter(torch.randn(self.r), requires_grad=True)) self.delta_u = nn.Linear(self.r, self.out_features, bias=False) def forward(self, x: torch.Tensor): # Step 3: merge + Delta_W (Eq.7) Delta_W = self.delta_u @ self.delta_sigma @ self.delta_v result = self.base_layer(x) result = result + torch.einsum(ijk,kl->ijl, x, Delta_W) * self.scaling return result def regularization_loss( model: nn.Module, beta: float, gamma: float ): # definitions l2_loss = 0.0 l3_loss = 0.0 num_param = 0 for name, param in model.named_parameters(): if param.requires_grad: # singular value regularization if delta_sigma in name: num_param += 1 diag_norm = torch.sum(param ** 2) l2_loss += diag_norm # orthogonal regularization elif delta_v in name or delta_u in name: if delta_v in name: matmul_result = torch.matmul(param.T, param) else: matmul_result = torch.matmul(param, param.T) = torch.eye(matmul_result.size(0), device=matmul_result.device) diff_I = matmul_result - matrix_loss = torch.norm(diff_I, p=fro) l3_loss += matrix_loss auxi_loss = (beta * l2_loss + gamma * l3_loss) / num_param if num_param > 0 else 0.0 return auxi_loss"
        },
        {
            "title": "B BASELINES",
            "content": "To demonstrate its efficacy and robustness, we evaluate KaSA against FFT and multiple wellregarded PEFT baselines. The descriptions of our selective baselines are as follows: Full fine-tuning (FFT) initializes the base model with pre-trained weights and biases, updating all parameters during fine-tuning. Full fine-tuning typically serves as comparative performance upper bound for PEFT methods (Valipour et al., 2023). Bitfit (Zaken et al., 2021) fine-tunes the bias vectors, leaving other model parameters unchanged. 17 Preprint Adapter tuning integrates tunable adapter layers into Transformer blocks, featuring pair of down-projection and up-projection matrices with non-linear activation function in between. We compare four Adapter variants: AdapterH (Houlsby et al., 2019) inserts adapter layers after the attention and the feed-forward block to fine-tune. AdapterD (Ruckle et al., 2021) discards non-activated adapters to improve fine-tuning efficiency. AdapterL (Lin et al., 2020) employs an efficient design, placing adapter layers after the MLP module and LayerNorm. AdapterP (Pfeiffer et al., 2021) applies adapter after the feed-forward layer and employs two-stage learning strategy to enhance multi-task performance. LoRA (Hu et al., 2021) only fine-tunes pair of low-rank matrices to approximate the taskspecific knowledge updates, effectively diminishing the number of trainable parameters. AdaLoRA (Zhang et al., 2022) reparameterizes task-specific knowledge updates in the SVD form and adaptively allocates the parameter budget through pruning the less important singular values. DyLoRA (Valipour et al., 2023) dynamically trains LoRA for range of ranks, reducing the training time to find fixed, optimal rank. VeRA (Kopiczko et al., 2023) employs learnable vectors to adapt shared pair of frozen random matrices across layers to reduce the trainable parameters count. DoRA (Liu et al., 2024) decomposes the base model weights into magnitude and direction components for fine-tuning, reducing the number of trainable parameters. PiSSA (Meng et al., 2024) performs SVD to portion the base model into principal components with larger singular values and residual components with smaller ones, fine-tuning the low-rank matrices initialized with the principle components while keeping the residual components unchanged. MiLoRA (Wang et al., 2024a) also utilizes SVD for parameter initialization but diverges from PiSSA by fine-tuning low-rank matrices initialized with residual components and maintaining the principal ones unchanged. SARA (Gu et al., 2024) conducts SVD at the initialization stage to adaptively find the appropriate rank for each layer. CorDA (Yang et al., 2024) performs SVD on the base model, oriented by the covariance matrix that encodes the context of the target task. CorDA supports two fine-tuning modes: 1) initializing the tunable low-rank matrices with principal components for enhanced performance; and 2) freezing the principle components while using minor components to initialize tunable matrices, thereby preserving world knowledge."
        },
        {
            "title": "C DETAILS OF BENCHMARK DATASETS",
            "content": "C.1 GLUE BENCHMARK For natural language understanding (NLU), we employ the GLUE benchmark (Wang et al., 2018), which is widely used benchmark containing collection of 8 NLU datasets, including CoLA, SST2, MRPC, STS-B, QQP, MNLI, QNLI, and RTE. We present the statistical information of the GLUE benchmark in the table below. C.2 E2E NLG CHALLENGE For natural language generation (NLG), we utilize the E2E (End-to-End) NLG Challenge dataset (Novikova et al., 2017), which is commonly used for the evaluation of natural language generation models. This dataset includes approximately 42k training samples, 4.6k validation samples, and 4.6k test samples from the restaurant domain. The E2E dataset involves evaluations across five metrics: BLEU, NIST, METEOR, ROUGE-L, and CIDEr. Detailed explanations of these metrics are as follows: BLEU (Bilingual Evaluation Understudy) evaluates the quality of machine-generated text by comparing it to one or more human-generated reference translations. 18 Preprint Table 6: Overview of task descriptions and dataset statistics within the GLUE benchmark. Corpus Task # Train # Val # Test # Labels Metrics Domain CoLA SST-2 Acceptability Sentiment 8.55k 67.3k 1.04k 872 1.06k 1.82k 2 Matthews Corr. 2 Accuracy misc. Movie reviews Similarity and Paraphrase Tasks Single-Sentence Tasks MRPC STS-B QQP Paraphrase Sentence similarity Paraphrase 3.67k 5.75k 364k 408 1.5k 40.4k 1.73k 1.38k 391k Inference Tasks News Pearson/Spearman Corr. misc. 2 Accuracy/F1 1 2 Accuracy/F1 MNLI QNLI RTE NLI QA/NLI NLI 393k 105k 2.49k 19.65k 5.46k 19.65k 5.46k 3k 3 Accuracy 2 Accuracy 2 Accuracy Social QA misc. Wikipedia News & Wikipedia NIST (National Institute of Standards and Technology) evaluates the quality of machinegenerated text by calculating the similarity between machine output and reference text using weighted average of n-grams precision. METEOR (Metric for Evaluation of Translation with Explicit ORdering) measures the alignment between the machine-generated and reference texts by calculating score based on the harmonic mean of precision and recall. ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation) measures the longest common subsequence(LCS) between the machine output and the reference. It specifically focuses on the sequence of words, making it sensitive to the fluency and order of information in the generated text. CIDEr (Consensus-based Image Description) measures the similarity of the machinegenerated text and the human-generated ground truth by considering both the n-gram overlap and the consensus among human annotators. C.3 SYNTHETIC DATASET For instruction following tasks, we employ synthetic datasets generated using GPT4o, based on the foundational No Robots seed dataset (Rajani et al., 2023). Task-specific subsets, including summarization, classification, coding, and closed QA, serve as seeds for generating synthetic data through the framework proposed by (Park et al., 2024). Table 7 presents the volume of data samples and token-level statistical information for these task-specific synthetic subsets. Table 7: Data volume and token-level statistics of the train and test synthetic datasets generated by GPT4o for each instruction-following task. Task Summarization Classification Coding Closed QA Split Train Test Train Test Train Test Train Test Data Volume Token-level Statistics Seed Synthesis Min Max Avg. Std. 395 25 334 16 334 16 245 128K 100 128K 64 128K 64 128K 60 10 148 6 9 49 12 126 2,386 1,150 2,159 520 6,518 821 1,701 1, 95 426 67 119 151 317 135 411 53 245 37 84 189 59 378 C.4 ALPACA AND MT-BENCH Alpaca (Taori et al., 2023a) is well-known instruction dataset that contains 51k instructionfollowing demonstrations generated by text-davinci-003. These data are synthesized using an improved self-instruct method (Wang et al., 2023). The dataset is designed for instruction-tuning LLMs 19 Preprint to improve their ability to follow instructions. Each sample includes an instruction, an input (if applicable), and an output. specific example is presented below. 1 { 2 3 4 5 6 } \"instruction\": \"Create classification task by clustering the given list of items.\", \"input\": \"Apples, oranges, bananas, strawberries, pineapples\", \"output\": \"Class 1: Apples, OrangesnClass 2: Bananas, StrawberriesnClass 3: Pineapples\", \"text\": \"Below is an instruction that describes task, paired with an input that provides further context. Write response that appropriately completes the request.nn### Instruction:nCreate classification task by clustering the given list of items.nn### Input:nApples, oranges, bananas, strawberries, pineapplesnn### Response:nClass 1: Apples, OrangesnClass 2: Bananas, StrawberriesnClass 3: Pineapples\" The instruction describes the targeted task to be performed by the model. Each of the 52k instructions is unique. The input can represent the optional input to the task or serve as the additional context to the corresponding instruction. The output is the response generated by textdavinci-003 to the associated instruction. The Text is the formatted combination of the instruction, input, and output, using the prompt template for fine-tuning models. MT-bench (Zheng et al., 2023) contains 80 predefined open-ended questions across diverse domains such as writing, roleplay, reasoning, math, coding, extraction, STEM, and humanities. These challenging questions are designed to automatically assess an LLMs instruction-following capabilities, with advanced service LLMs like GPT-4 acting as judges. Below is an example from MT-bench. \"question_id\": 101, \"category\": \"reasoning\", \"turns\": [ \"Imagine you are participating in race with group of people. If you have just overtaken the second person, whats your current position? Where is the person you just overtook?\", \"If the \"second person\" is changed to \"last person\" in the above question, what would the answer be?\" ], \"reference\": [ \"You are in second place.\", \"Uncertain.\" 1 { 2 3 5 6 7 8 9 11 12 } ] C.5 COMMONSENSE REASONING The Commonsense170K dataset (Hu et al., 2023) contains data samples from eight well-known commonsense reasoning tasks: BoolQ (Clark et al., 2019) dataset comprises 15,942 naturally occurring yes/no questions, generated in unprompted and unconstrained settings. PIQA (Bisk et al., 2020) dataset consists of samples structured as multiple-choice questions, each presenting question with two possible solutions that require physical commonsense to answer. SIQA (Sap et al., 2019) dataset contains multiple-choice questions regarding the pragmatic implications of social events, which can measure LLMs abilities to address social commonsense reasoning. HellaSwag (Zellers et al., 2019) dataset includes commonsense natural language inference questions, offering context and multiple endings to complete it. 20 Preprint WinoGrande (Sakaguchi et al., 2021) dataset is structured as fill-in-the-blank task with two options, designed to test models ability to correctly solve the problem using commonsense reasoning. ARC-e and ARC-c are the Easy and Challenge Set of the ARC (Clark et al., 2018) dataset, which contains grade-school level, multiple-choice science questions. Notably, the Challenge Set includes questions answered incorrectly by both the retrieval-based algorithm and word co-occurrence algorithm. OBQA (Mihaylov et al., 2018) dataset contains multiple-choice elementary-level science questions requiring multi-step reasoning, use of additional common and provided science facts (open book), and rich text comprehension."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "Following the typical practices of (Wang et al., 2023) and (Zheng et al., 2023), we leverage two specialized prompt templates: 1) one for generating synthetic datasets and 2) another for evaluating the outputs of fine-tuned LLMs. To be specific, Figure 5 presents the prompt template crafted for generating synthetic data aimed at the summarization task, whereas Figure 6 shows the prompt template for other tasks. We guide GPT4o in generating analogous data samples by using reference example pair consisting of prompt $instruction and its corresponding response $response from the training subset of the seed dataset. In addition, the template is designed to request multiple synthetic data samples in single query, thus maximizing the efficiency of API use. On the other hand, Figure 7 shows the prompt template used for assessing the precision and similarity between the response $lm response and $human response given the same $instruction from the test subset of the seed dataset, where the $ symbol indicates placeholder, designed to be substituted with actual data during the runtime. We only report the precision results in our experiments for the sake of brevity. Given the unique features of different downstream tasks, there is no optimal prompt template that universally applies. Therefore, the actual content of the prompt template is adjusted to align with the specific requirements of the task for which the synthetic dataset is being generated. Figure 5: Prompt template of data synthesis for summarization tasks by GPT4o. 21 Preprint Figure 6: Prompt template of data synthesis for classification, coding, and closed QA tasks by GPT4o. Figure 7: Prompt template to evaluate the fine-tuned models response by GPT4o."
        },
        {
            "title": "E TRAINING DETAILS",
            "content": "E.1 NATURAL LANGUAGE UNDERSTANDING For NLU tasks, we align with the experimental setup detailed in (Hu et al., 2021; Zhang et al., 2022) for fair comparison. The detailed configurations of KaSA for RoBERTa-base, RoBERTa-large, and DeBERTaV3-base on the GLUE benchmark are depicted in Table 8 and Table 9, respectively. It is important to note that our adaptation process for the MRPC, RTE, and STS-B tasks begins with the pre-trained RoBERTa model, rather than model that has already been adapted to MNLI. As result, we fine-tune the models on all datasets starting from their original pre-trained weights. The results we present are the median results from 5 runs, each conducted with distinct random seed. 22 Preprint Table 8: The hyperparameters we used for RoBERTa-base and RoBERTa-large on the GLUE benchmark. Model SST-2 MRPC Settings STS-B MNLI CoLA QNLI QQP RTE Common RoBERTabase RoBERTalarge Optimizer Warmup Ratio LR Schedule Batch Size # Epochs Learning Rate Weight Decay KaSA Rank KaSA α KaSA β KaSA γ KaSA Dropout Max Seq. Len. Batch Size # Epochs Learning Rate Weight Decay KaSA Rank KaSA α KaSA β KaSA γ KaSA Dropout Max Seq. Len. AdamW 0.06 Linear 32 100 5E-04 0.0 2.4E-3 2.4E-4 0.0 512 - - - - - - - - 128 100 5E-04 0.0 1E-04 1E-03 0.0 512 64 10 4E-04 0.1 1E-04 1E-04 0.0 512 32 100 4E-04 0.0 1E-01 1E-03 0.0 32 10 3E-04 0.1 1E-02 1E-02 0.0 512 32 100 4E-04 0.0 32 10 4E-04 0.0 128 100 5E-04 0.0 32 100 4E-04 0. rquery = rvalue = 8 16 1E-04 1E-03 0.0 512 32 100 3E-04 0.0 1E-02 1E-05 0.0 512 8 20 4E-04 0.0 rquery = rvalue = 8 2.4E-01 2.4E-04 0.0 512 1E-02 1E-03 0.0 512 1E-4 1E-3 0.0 512 - - - - - - - - 2.4E-01 2.4E-04 0.0 32 100 4E-04 0.0 1E-04 1E-03 0.0 512 32 40 3E-04 0.0 1E-04 1E-05 0.0 512 32 20 3E-04 0.0 1E-03 1E-02 0.0 Table 9: The hyperparameters we used for DeBERTaV3-base on the GLUE benchmark. SST-2 MRPC Model Settings STS-B CoLA QNLI RTE Optimizer Warmup Ratio LR Scheduler Batch size # Epochs Learning Rate Weight Decay KaSA Rank KaSA α KaSA β KaSA γ KaSA Dropout Max Seq. Len. AdamW 0.06 Linear 128 10 5E-4 0.0 1E-04 1E-03 0.0 512 32 10 4E-4 0. 1.0 1.0 0.0 512 32 100 4E-4 0.0 16 20 4E-4 0.0 32 100 5E-4 0.0 32 20 4E-4 0.0 rquery = rvalue = 8 2.4E-01 2.4E-04 0.0 64 1E-01 1E-01 0.0 512 1E-04 1E-03 0.0 512 1E-01 1E-01 0.0 512 DeBERTaV3-base E.2 NATURAL LANGUAGE GENERATION For NLG tasks, our KaSA adheres to the experimental setup outlined in (Hu et al., 2021; Gu et al., 2024) to ensure fair comparison. The comprehensive configurations of KaSA for GPT-2 Medium and GPT-2 Large models on the E2E NLG Challenge benchmark are depicted in Table 10. E.3 INSTRUCTION FOLLOWING For instruction following tasks, we adopt the framework proposed by (Park et al., 2024) to streamline the processes of data synthesis, fine-tuning, and evaluation. We fine-tune several of the most popular LLMs, including LLaMA3 8B, Mistal 7B, Gemma 7B, and LLaMA2 13B, using KaSA and different PEFT baselines to facilitate comparative analysis. Detailed hyperparameter configurations are provided in Table 11. 23 Preprint Stage Training Table 10: The hyperparameters for GPT-2 on E2E NLG Challenge. Stage Settings Medium Large Training Optimizer Weight Decay Dropout Prob Batch Size # Epoch Warmup Steps LR Scheduler Label Smooth Learning Rate KaSA Rank KaSA α KaSA β KaSA γ AdamW 0.01 0.1 0.01 0. 8 5 500 Linear 0.1 0.1 2E-4 rquery = rvalue = 4 32 1E-4 1E-3 Inference Beam Size Length Penalty no repeat ngram size 0.9 10 4 0.8 Table 11: Detailed configurations used for the instruction following task. Settings Summarization Coding Closed QA MT-Bench Classification Optimizer Batch Size # Epoch Warmup Ratio Data Type LR Scheduler Learning Rate KaSA Rank KaSA α KaSA β KaSA γ KaSA Dropout Max Seq. Len. AdamW Gemma 7B = 8, Mitral 7B = LLaMA3 8B = 16 1 0.1 Bfloat16 Cosine 2.0E-04 rquery = rvalue = 8 16 1E-4 1E-3 0.05 512 Inference Number of Beams Length Penalty No Repeat N-Gram Size 10 0.8 E.4 COMMONSENSE REASONING We adhere strictly to the hyperparameter configurations for training and evaluation as specified by (Wang et al., 2024a) and (Hu et al., 2023), without any tuning. The specific hyperparameter configurations used are shown in Table 12."
        },
        {
            "title": "F ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "F.1 COMPONENTS ABLATION STUDY ON SST-2, QNLI, AND STS-B Figure 8 shows the results of ablation studies conducted on the SST-2, QNLI, and STS-B datasets. From the results, we observe that: 1) the models performance consistently improves with the inclusion of additional components during fine-tuning; 2) excluding any of these components leads to decline in performance. These findings align with that observed in Section 4.6, emphasizing the effectiveness of each designed principal component of KaSA in enhancing model performance. F.2 RANK OF KNOWLEDGE-BASED SVD TRUNCATION As depicted in Section 1, components of the original base model weight matrix W(0) associated with smaller singular values are identified to contain noise or less relevant information (Sharma et al., 2023; Wang et al., 2024a). This presence can adversely affect the convergence of model training 24 Preprint Table 12: The hyperparameter configurations for LLaMA2 7B and LLaMA3 8B on commonsense reasoning tasks. To ensure fair comparison, these configurations remain consistent across LoRA, PiSSA, and MiLoRA, with the exception of the specific hyperparameters unique to KaSA, namely β and γ, as well as PiSSA and MiLoRA, where α = 32. Hyperparameters Commonsense Reasoning LLaMA2 7B LLaMA3 8B Optimizer Batch Size # Epoch Warmup Steps LR Scheduler Learning Rate KaSA Rank KaSA α Dropout Prob KaSA β KaSA γ Placement AdamW 16 3 100 Linear 3E-4 32 64 0.05 1E-2 1E-3 1E-4 1E-3 query, key, value, MLP up, MLP down Figure 8: Components ablation study about knowledge-based SVD truncation, knowledge-aware singular value adaptation, singular value regularization L2, and orthogonal regularization L3 on SST-2, QNLI, and STS-B datasets. and its overall efficacy. We propose the truncation of these components to refine the focus of the base model towards more pertinent knowledge domains, thereby mitigating the adverse impacts. Therefore, we delve into the impact of varying the rank (denoted as {1, 2, 4, 8, 16, 32, 64, 128}) of SVD truncation on the models performance, using RoBERTa-base on the MRPC, CoLA, and RTE datasets. As illustrated in Figure 9, an enhancement in model performance is observed as increases from 1 to 8. Conversely, an escalation in from 8 to 128 results in decrement in performance. This observation highlights the criticality of identifying an optimal SVD truncation rank that achieves delicate balance between incorporating world knowledge with large singular values and excluding disruptive noise information with smaller singular values, thereby optimizing model performance. The adaptive determination of the optimal SVD truncation rank emerges as compelling avenue for future research. F.3 RANK OF KNOWLEDGE-AWARE SINGULAR-VALUE ADAPTATION We explore the impact of different rank settings on performance across range of tasks. Specifically, our analysis focuses on LoRA, MiLoRA, PiSSA, and KaSA, using ranks ranging from = {1, 2, 4, 8, 16, 32, 64, 128} on the CoLA, MRPC, and RTE datasets. As presented in Table 13, KaSA consistently surpasses the baselines across various rank settings in 92 out of 96 cases across the four datasets, highlighting the efficacy and robustness of our proposed method. To further our investigation, we increase the rank to 128 and compare KaSA with LoRA, DoRA (Liu et al., 2024), CorDA (Yang et al., 2024), PiSSA, and MiLoRA. The comparison is conducted by fine-tuning and evaluating the RoBERTa-base model on the GLUE benchmark. The results, as illustrated in Table 14, show that KaSA consistently outperforms all baselines across six datasets, with slight exception for the QNLI dataset, where it performs marginally worse than FFT (92.71 vs. 92.8). This is in line with the previous observations, further demonstrating the robustness and scalability of KaSA. 25 Preprint Figure 9: The impact of varying the rank of SVD truncation on the models performance across three datasets. Table 13: Performance comparison of LoRA and SVD-based baselines on CoLA, MRPC, and RTE datasets across different ranks of knowledge-aware singular-value adaptation. Dataset Method 1 2 4 8 32 64 CoLA MRPC RTE LoRA 60.08 MiLoRA 60.84 59.56 PiSSA 63.32 KaSA 88.73 LoRA MiLoRA 89.71 87.25 PiSSA 89.46 KaSA 71.84 LoRA MiLoRA 75.09 68.95 PiSSA 77.62 KaSA 61.17 61.36 62.68 65.58 87.74 89.22 87.99 87.99 72.56 80.14 73.29 77.62 63.14 63.10 60.57 63. 88.97 88.48 88.24 90.20 75.45 79.42 76.17 78.70 63.77 63.07 65.54 65.82 88.73 88.73 88.24 90.69 78.70 80.51 75.09 81.59 63.58 63.57 61.32 64. 89.46 88.73 89.46 89.95 77.26 79.06 76.90 80.51 63.82 64.56 63.31 65.05 89.95 90.20 89.71 90.44 77.98 79.81 78.34 81.23 62.70 63.60 63.35 64. 88.97 88.73 88.97 90.20 79.78 81.59 76.53 82.67 128 63.45 63.66 63.60 65.06 88.97 88.73 89.95 90.44 78.70 80.87 79.42 81. F.4 PARAMETER INITIALIZATION OF = UΣV In the context of PEFT, the initialization of tunable parameters is pivotal for optimizing model performance, as evidenced by (Hu et al., 2021; Meng et al., 2024; Wang et al., 2024a). As explicated in Section 2.2, PiSSA (Meng et al., 2024) and MiLoRA (Wang et al., 2024a) initialize the low-rank adaptation block by differentiating components based on their singular value magnitudes. It underscores the necessity of exploring the influence of various initialization strategies on the task-specific knowledge update, represented as = UΣV, and its consequent impact on model efficacy. In this study, we adopt default initialization strategy where = 0 and both and Σ follow normal distribution (µ, σ2). We examine three distinct variants of initialization strategies: 1) initializing UΣV with Wprincipal; 2) using Wminor for initialization; and 3) adopting normal distribution (µ, σ2) for both and Σ while setting to 0. The comparative outcomes of these strategies across three datasets are illustrated in Figure 10. Our analysis reveals that different initialization strategies distinctly affect model performance across various datasets. Notably, our adopted strategy = 0, {V, Σ} (µ, σ2), consistently outperforms the alternative variants across all evaluated datasets and metrics. Among the variant strategies examined, initializing with UΣV = Wprincipal demonstrates superior performance on the CoLA and RTE datasets, yet underperforms when utilizing UΣV = Wminor on the MRPC datasets. This observation leads us to conjecture that the innovative design of our knowledge-aware singularvalue module significantly enhances the models capacity to rapidly identify optimal parameters within larger parameter search space, thereby optimizing performance. F.5 SINGULAR-VALUE AND ORTHOGONAL REGULARIZATION To evaluate the effectiveness of singular-value regularization ΣF and orthogonal regularization (cid:13) (cid:13) (cid:13)UU Ir (cid:13)F , we adopt the training configuration outlined in Section (cid:13)VV Ir (cid:13)F and (cid:13) (cid:13) 26 Preprint Table 14: Performance of RoBERTa-base with different adaptation methods using large rank of 128 on 6 datasets from the GLUE benchmark. We report the overall (matched and mismatched) accuracy for MNLI, Matthews correlation coefficient (Mcc.) for CoLA, Pearson correlation coefficient (Pcc.) for STS-B, and accuracy (Acc.) for all the remaining tasks. The symbols and indicate that the results are taken from (Gao et al., 2024) and (Yang et al., 2024), respectively. We report the average result of five runs with different random seeds. The best results for each dataset are shown in bold. Higher is better for all metrics. # Trainable Parameters SST-2 MRPC (Acc.) (Acc.) CoLA QNLI (Acc.) (Mcc.) STS-B (Pcc.) RTE (Acc.) All Avg. Method FFT LoRA* DoRA* CorDA* PiSSA MiLoRA KaSA 125.0M 94.8 21M 94.15 21M 93. 21M 93.12 21M 94.61 21M 94.72 21M 95.30 90.2 82.84 83.58 89.71 89.95 88.73 90.44 63.6 54.24 51.93 59.60 63.60 63.66 65.06 92.8 92.48 92. 91.49 92.90 92.55 92.71 78.7 64.26 64.98 76.17 79.42 80.87 81.23 91.2 88.58 88.71 90.17 90.55 90.79 91.36 85.2 79.43 79. 83.38 85.17 85.22 86.02 Figure 10: The impact of parameter initialization on the task-specific knowledge update, denoted as = (USV) across three datasets. (cid:13)VV Ir 4.2. This involves fine-tuning RoBERTabase model on the CoLA dataset using KaSA. We then plot the loss curve of these three regularization terms throughout the training process. As depicted in Figure 11, the application of the adapter to the query Wq and value Wv matrices results in an initial increase followed by decrease in singular-value regularization ΣF . This pattern suggests that the model progressively fine-tunes the significance of task-specific knowledge by adjusting the singular values. Intriguingly, the trend observed for orthogonal regularization (cid:13) (cid:13) (cid:13)F and (cid:13) (cid:13) (cid:13)F varies between the query Wq and value Wv matrices, indicating distinct adaptation behaviors. To elucidate further, within the query matrix Wq, the trend of orthogonal regularization (cid:13) (cid:13) (cid:13)F mirrors that of the singular-value regularization ΣF , initially increasing before decreasing. Conversely, (cid:13) (cid:13) (cid:13)F exhibits an opposing pattern, decreasing and then increasing. In the value matrix Wv, the behaviors of (cid:13) (cid:13) (cid:13)UU Ir (cid:13)F and (cid:13) (cid:13) (cid:13)VV Ir (cid:13)F demonstrate reversal compared to those observed in the query Wq. This finding diverges from the trends reported in AdaLoRA (Zhang et al., 2022). To delve deeper, we examine the overall training loss, as depicted in the lower part of Figure 11. It is observed that the overall training loss converges to notably low value (e.g., 0.058) by the end of the training period. Based on these observations, we hypothesize that the imposition of orthogonality on either the or matrices may facilitate more efficient search for an optimal representation by narrowing the search space. This premise will be explored in our future research. (cid:13)VV Ir (cid:13)UU Ir (cid:13)UU Ir F.6 HYPERPARAMETER SENSITIVITY ANALYSIS KaSA introduces two key hyperparameters, β and γ, to scale the singular value regularization L2 and orthogonal regularization L3, respectively. To gain deeper understanding of how these regularization coefficients influence performance, we meticulously tune the two coefficients, β [1E-5, 1] and γ [1E-5, 1], and conduct sensitivity analysis for RoBERTa-base on CoLA, RoBERTa-large 27 Preprint Figure 11: The singular-value and orthogonal regularization curve at the last layer of RoBERTabase (Upper) and overall training loss curve (Lower) on CoLA dataset. Table 15: Sensitivity of regularization coefficients β and γ for RoBERTa-base on CoLA, RoBERTalarge on SST-2, and DeBERTa-v3-base on MRPC. Hyperparameters β = 0.01, γ = 1.0 β = 0.1, γ = 0.0001 β = 0.01, γ = 0.1 β = 0.0, γ = 0.0 β = 0.001, γ = 0.01 β = 0.001, γ = 0.001 β = 0.01, γ = 0.001 β = 0.1, γ = 0.01 β = 0.0001, γ = 0.1 β = 0.01, γ = 0.0001 β = 0.0001, γ = 0.01 β = 1.0, γ = 0.1 β = 1.0, γ = 1.0 β = 0.1, γ = 1.0 β = 0.1, γ = 0.1 β = 1.0, γ = 0.01 β = 0.01, γ = 0.01 β = 0.0001, γ = 0.0001 β = 0.0001, γ = 0.001 β = 0.1, γ = 0.001 β = 0.001, γ = 0.0001 β = 0.001, γ = 0.1 RoBERTa-base RoBERTa-large DeBERTa-v3-base CoLA 0.6581 0.6334 0.6414 0.646 0.6358 0.6553 0.6506 0.6333 0.6485 0.6347 0.658 0.6241 0.6291 0.6436 0.653 0.6397 0.6433 0.6565 0.6582 0.6338 0.6504 0. SST-2 0.9587 0.9587 0.9622 0.9599 0.9587 0.9576 0.5092 0.9587 0.9622 0.9576 0.9599 0.9599 0.9553 0.961 0.9587 0.9587 0.9576 0.9687 0.961 0.9599 0.961 0.9679 MRPC 0.9044 0.8971 0.8995 0.902 0.9093 0.9093 0.902 0.902 0.8995 0.9044 0.9069 0.8971 0.9142 0.9093 0.9082 0.8995 0.8995 0.9044 0.9093 0.902 0.9093 0.8971 on SST-2, and DeBERTa-v3-base on MRPC. The results, presented in Table 15, demonstrate that KaSA exhibits robustness to variations in the regularization coefficients β and γ. Preprint Table 16: Efficiency and complexity analyses of the NLU task on the CoLA benchmark with RoBERTa-base 125M and the NLG task on the MT-Bench benchmark with LLaMA3 8B, using different adaptation methods on single NVIDIA GeForce RTX 3090 (24GB) GPU and an NVIDIA A100-SXM4 (80GB) GPU, respectively. NLU # Trainable Parameters # GPU Memory # Training FLOPs (109 per sample) Training Latency (per epoch) Inference Latency (per batch size 32) Matrix Rank RoBERTa-base 125M on Single NVIDIA GeForce RTX 3090 (24GB) GPU LoRA 0.23716% 1638M 2.0306 9.4868s 0.0173s PiSSA 0.23716% 1638M 1.9270 9.8825s 0.0108s rank(W) = rank(W) = rank(W) = rank(W) = MiLoRA KaSA 0.23716% 1638M 1.9270 9.9267s 0.0165s rank(W) = rank(W) = 0.23732% 1650M 2.1503 11.3679s 0.0119s rank(W) = rank(W) CoLA Performance (Mcc.) 63.4% 65.5% 63.1% 65.8% NLG # Trainable Parameters # GPU Memory # Training FLOPs (109 per sample) Training Latency (per epoch) Inference Latency (per batch size 16) Matrix Rank LLaMA3 8B on Single NVIDIA A100-SXM4 (80GB) GPU LoRA 0.04241% 71023M 240.2583 2469.6s 0.7898s PiSSA 0.04241% 71023M 240.2583 2543.1s 0.7687s rank(W) = rank(W) = rank(W) = rank(W) = MiLoRA KaSA 0.04241% 71023M 240.2583 2476.8s 0.7705s rank(W) = rank(W) = 0.04242% 71095M 240.2585 2528.9s 0.7771s rank(W) = rank(W) MT-Bench Performance (Scores) 4.1937 4.2625 4.3187 4.7125 F.7 EFFICIENCY AND COMPLEXITY ANALYSIS We conduct comprehensive efficiency and complexity comparison between LoRA and SVD baselines across different tasks and model scales, as shown in Table 16. The dynamic singular value adaptation introduced in KaSA is learnable one-dimensional vector of size and requires parameter regularizations, incurring negligible training overheads compared to the standard LoRA. In addition, due to the low-rank approximation of the original matrix, we reduce the rank of from to r, accelerating the inference particularly for small-scale language models like RoBERTabase 125M (i.e., with small m). As can be seen, compared to LoRA, KaSAs extra training overhead is less than 20% (resp. 3%) for the NLU (resp. NLG) tasks, while speeding up the inference by 1.45x (resp. 1.02x) times. When compared to PiSSA and MiLoRA, our method incurs an average of less than 13% extra training overhead for NLU tasks, while maintaining comparable or improved inference latency. For NLG tasks, our method introduces similar training overhead or inference latency. INITIALIZATION AND SINGULAR-VALUE ADAPTATION ANALYSIS In this section, we conduct detailed analysis of initialization dilemmas associated with PiSSA and MiLoRA, and subsequently explore the core advantages of KaSA, aiming to provide comprehensive understanding of the foundational principles governing these PEFT methods. Before embarking on detailed examination of each method, we summarize the general mechanism underpinning PEFT. Considering base model characterized by weight matrix W(0) Rnm, PEFT aims to efficiently fine-tune W(0) by learning task-specific update with as few trainable parameters as possible, such that the updated weights W(0) + are better aligned with the requirements of downstream tasks. PEFT approaches generally involve keeping the base model W(0) frozen during training, while exclusively updating the parameters of W. G.1 INITIALIZATION DILEMMAS OF IN PISSA AND MILORA PiSSA employs SVD on the base model weight matrix W(0) Rnm, decomposing it as: W(0) = UΣV (13) 29 Preprint where Rnm and Rmm are semi-orthogonal matrices, and Σ Rmm is diagonal matrix with singular values (σ1, ..., σm) satisfying (σ1 σ2 σm 0). Following the standard SVD, PiSSA splits the base model into two distinct components: the principle low-rank matrix Wpri, which encompasses the largest singular values, and the residual matrix Wres, which contains the remaining singular values: W(0) = Wpri + Wres (14) Wpri = UpriΣpriV (15) where Upri = U[:, : r], Σpri = diag(σ1, . . . , σr), Vpri = V[:, : r], Ures = U[:, :], Σres = diag(σr+1, . . . , σm), and Vres = V[:, :]. Subsequently, PiSSA subtracts Wpri from the base model W(0) to initialize the low-rank matrices for the task-specific update, resulting in: pri, Wres = UresΣresV res Wbase = W(0) Wpri = Wres W(0) WbaseF = WpriF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 (σi)2 (16) (17) This subtraction of Wpri removes the principal components of W(0), which can lead to considerable information loss and the forgetting of crucial world knowledge. Given that Wpri is the best rank-r approximation of W(0), its removal can adversely impact the models initial representational capacity, potentially resulting in degraded performance. PiSSA subsequently freezes Wbase and leverages two low-rank matrices, and B, to learn the task-specific update during fine-tuning. The matrices and are initialized as: = Upri (cid:112)Σpri, = (cid:112)ΣpriV pri (18) Therefore, in the PiSSA framework, the task-specific update is expressed as: = AB = UpriΣpriV (19) In the initial stage, the value of is equivalent to Wpri. During fine-tuning, the updates to and are significantly influenced by their initialization, which is based on Upri and Vpri. As result, the gradient updates predominantly follow the directions of the initial singular vectors associated with the largest singular values. This limits the models ability to explore the parameter space and effectively learn new knowledge relevant to the downstream task, as the knowledge presented by the largest singular values in Wpri may not be necessary for the downstream task and can negatively impact model performance. pri, Wpri In contrast to PiSSA, MiLoRA subtracts the residual components associated with the smallest singular values from the base model, resulting in: W pri = base = W(0)W priV pri, priΣ res = res = pri = diag(σ1, . . . , σmr), pri resV resΣ pri = V[:, : r], res res = V[:, :]. MiLoRA subsequently uses (20) (21) res = U[:, :], res to pri = U[:, : r], Σ where Σ res = diag(σmr+1, . . . , σm), and initialize the tunable matrices and as: (cid:112) = res Σ res, = (cid:112) resV Σ (22) base frozen and updates and to learn the res During the fine-tuning stage, MiLoRA keeps task-specific update W, which is given by: = AB = resΣ resV res, res (23) In the context of SVD, the smallest singular values often correspond to noise or long-tail knowledge (Yan et al., 2021; Wang et al., 2024a; Yang et al., 2023; Sharma et al., 2023), which can impede the learning process for downstream tasks. MiLoRA, which initializes and based on res and res, confines the models learning predominantly to the directions of the less significant singular vectors associated with the smallest singular values. This constraint could potentially hinder the models ability to acquire essential knowledge required for downstream tasks. In addition, the introduction of noise through MiLoRAs initialization can adversely impact the model during the initial stages of training, leading to reduced stability and slower convergence, as observed in Figure 4 of the original MiLoRA paper. The training updates for and are constrained within the trivial subspace spanned by res, which leads to suboptimal performance. res and 30 Preprint G.2 KNOWLEDGE-AWARE SINGULAR-VALUE ADAPTATION OF KASA In response to the issues of initialization presented by PiSSA and MiLoRA, we propose KaSA, which leverages knowledge-aware singular values to activate parametric knowledge based on its relevance to downstream tasks. Our method commences with the knowledge-based SVD truncation of the minor singular components Wnoise Rnm that contain the smallest singular values. This operation effectively filters out the noise from the base mode W(0), resulting in matrix Wworld Rnm that encapsulates essential world knowledge: Wworld = W(0) Wnoise = UΣV resΣ resV res (24) KaSA uses the low-rank matrix Wworld to approximate W(0), eliminating irrelevant and noisy knowledge while preventing the world knowledge forgetting issue. Following the truncation, KaSA introduces novel parameterization to learn in the form of SVD: = UΣV, UU = VU = Ir (25) where and are semi-orthogonal matrices, ensuring the orthogonality condition. The matrix Σ is trainable diagonal matrix, with knowledge-aware singular values that can be adaptively tuned, allowing the model to emphasize knowledge relevant to the downstream task and providing fine-grained learning pattern. To maintain the orthogonality of and during training, we add an orthogonal regularization: (cid:13)F + (cid:13) L3(Ψ) = (cid:13) (cid:13) (cid:13)VV Ir (cid:13)UU Ir (cid:13) (cid:13)F (26) where denotes the Frobenius norm. This regularization can ensure KaSAs learned can more adhere to the SVDs framework, facilitating the seamless integration of with Wworld. Since the learned by KaSA is in SVD form, its spectral norm is equal to the largest singular value in Σ, satisfying: W2 = max σj = Σ2 (27) where σj are the adaptive singular values of the diagonal matrix Σ. Therefore, by controlling Σ, we can directly control Ws magnitude. This allows for adjustments to the weight updates, enhancing the controllability of the fine-tuning process for downstream tasks. In particular, KaSAs training objective is more comprehensive than that of orthogonal regularization alone. The overall training objective includes the task-specific loss L1, the singular value regularization L2, and orthogonal regularization L3. Therefore, the gradients with respect to U, V, and Σ are formulated as: U V Σ = = = L1 L1 L1 Σ + 4U(UU Ir) + 4V(VV Ir) + 2Σ (28) (29) (30) The gradients with respect to and are particularly influenced by the orthogonal regularization component, which facilitates stable training dynamics. This orthogonal regularization, along with the computed gradients, contributes to maintaining stable parameter updates, thereby mitigating potential issues such as gradient vanishing or explosion. G.3 SUMMARIZATION To summarize, our analysis of PiSSA and MiLoRA highlights the dilemmas posed by their initialization strategies while emphasizing the core advantages of KaSA for knowledge-aware singular-value adaptation. Specifically, PiSSAs initialization with principle components associated with the largest singular values can potentially lead to world knowledge forgetting and introduce updated knowledge unnecessary for downstream tasks, leading to diminished task performance. 31 Preprint On the other hand, MiLoRAs initialization with minor components associated with the smallest singular values introduces noisy and long-tail knowledge, resulting in reduced training stability, slower convergence, and suboptimal performance. In contrast, KaSA offers several advantages based on the aforementioned analysis: 1) noise reductionby filtering out components with minor singular values, KaSA eliminates their detrimental impacts on task performance; 2) knowledge-awarenessthrough adjusting the knowledge-aware singular values, KaSA allows the model to adaptively align with the requirements of downstream tasks; and 3) stable trainingintegrated orthogonal regularization ensures stable gradient updates and control over the magnitude of weight updates."
        },
        {
            "title": "H CASE STUDY",
            "content": "We present series of case studies to demonstrate the improved performance in instruction-following of models that have been fine-tuned with our KaSA method. This fine-tuning utilized synthetic datasets generated by GPT4o for each specific task. The cases, illustrated from Figure 12 through 18, compare the response and precision scores of the most widely used LLMs, including Gemma 7B, Mistral 7B, and LLaMA3 8B. These models were fine-tuned with both LoRA and KaSA techniques and evaluated across range of tasks such as summarization, classification, coding, and closed QA. For each case, GPT4o assesses the precision scores of the models responses. These instances collectively showcase not only the adaptability of KaSA across diverse set of leading LLMs but also its effectiveness in enhancing the performance of pre-trained language models (PLMs) in various downstream applications. 32 Preprint Figure 12: Responses on math problem from MT-Bench. Each response is generated by Gemma 7B models fine-tuned on 51K Alpaca dataset with KaSA, LoRA, and PiSSA methods respectively. 33 Preprint Figure 13: Responses on STEM problem from MT-Bench. Each response is generated by LLaMA3 8B models fine-tuned on 51K Alpaca dataset with KaSA, LoRA, and PiSSA methods respectively. 34 Preprint Figure 14: Responses on reasoning problem from MT-Bench. Each response is generated by Mistral 7B models fine-tuned on 51K Alpaca dataset with KaSA and MiLoRA methods respectively. 35 Preprint Figure 15: Responses on classification problem from the test split of No Robots dataset. Each response is generated by LLaMA3 8B models fine-tuned on 128K synthetic dataset with KaSA, PiSSA, and LoRA methods respectively. 36 Preprint Figure 16: Responses on summarization problem from the test split of No Robots dataset. Each response is generated by Gemma 7B models fine-tuned on 128K synthetic dataset with KaSA, LoRA, and PiSSA methods respectively. 37 Preprint Figure 17: Responses on ClosedQA problem from the test split of No Robots dataset. Each response is generated by Mistral 7B models fine-tuned on 128K synthetic dataset with KaSA, MiLoRA, and PiSSA methods respectively. 38 Preprint Figure 18: Responses on Coding problem from the test split of No Robots dataset. Each response is generated by Gemma 7B models fine-tuned on 128K synthetic dataset with KaSA and PiSSA methods respectively."
        }
    ],
    "affiliations": [
        "Electronics and Telecommunications Research Institute",
        "The Hong Kong University of Science and Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)"
    ]
}