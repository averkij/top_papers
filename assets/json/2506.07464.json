{
    "paper_title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
    "authors": [
        "Jinyoung Park",
        "Jeehye Na",
        "Jinyoung Kim",
        "Hyunwoo J. Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent works have demonstrated the effectiveness of reinforcement learning (RL)-based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing a PPO-style reinforcement algorithm with group-based normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as a regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 2 4 6 4 7 0 . 6 0 5 2 : r DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO Jinyoung Park1 Jeehye Na1 Jinyoung Kim1 Hyunwoo J. Kim2 1Korea University, 2KAIST {lpmn678, sonicdog00, k012100}@korea.ac.kr hyunwoojkim@kaist.ac.kr https://github.com/mlvlab/DeepVideoR"
        },
        {
            "title": "Abstract",
            "content": "Recent works have demonstrated the effectiveness of reinforcement learning (RL)- based post-training in enhancing the reasoning capabilities of large language models (LLMs). In particular, Group Relative Policy Optimization (GRPO) has shown impressive success by employing PPO-style reinforcement algorithm with groupbased normalized rewards. However, the application of GRPO to Video Large Language Models (Video LLMs) has been less studied. In this paper, we explore GRPO for video LLMs and identify two primary issues that impede its effective learning: (1) reliance on safeguards, and (2) the vanishing advantage problem. To mitigate these challenges, we propose DeepVideo-R1, video large language model trained with our proposed Reg-GRPO (Regressive GRPO) and difficultyaware data augmentation strategy. Reg-GRPO reformulates the GRPO objective as regression task, directly predicting the advantage in GRPO. This design eliminates the need for safeguards like clipping and min functions, thereby facilitating more direct policy guidance by aligning the model with the advantage values. We also design the difficulty-aware data augmentation strategy that dynamically augments training samples at solvable difficulty levels, fostering diverse and informative reward signals. Our comprehensive experiments show that DeepVideo-R1 significantly improves video reasoning performance across multiple video reasoning benchmarks."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [14] have achieved remarkable success across wide range of tasks by leveraging next-token prediction objectives on massive text corpora. Building on this progress, recent works have extended LLMs into the video domain, resulting in the emergence of Video Large Language Models (VideoLLMs) [58]. Despite these advances, VideoLLMs still face limitations in complex reasoning tasks, where models require temporal, spatial, and semantic understanding over extended video sequences. To address this, reinforcement learning (RL)-based post-training [9, 10] has become an increasingly popular paradigm. These methods optimize LLMs using reward signals that reflect human preferences or task-specific objectives, providing complementary mechanism to supervised fine-tuning. Recently, Group Relative Policy Optimization (GRPO) [11, 12] has shown promise by using group-based advantages and relative preference signals to stabilize training and enhance reasoning capabilities in text-based tasks. While GRPO has achieved strong results in text-based tasks, its application to VideoLLMs remains underexplored. In this work, we investigate GRPO for VideoLLMs and identify two key limitations that hinder effective training: (1) the reliance on safeguards such as the min and clipping functions, Preprint. Work in progress. Figure 1: DeepVideo-R1 significantly improves the reasoning capabilities of VideoLLMs. Our VideoLLM, DeepVideo-R1, is trained to explicitly predict the advantage ˆA(i) through Regressive GRPO loss. Notably, model training becomes significantly effective and achieves 10.06 performance improvement compared to GRPO. which often suppress gradients and impede convergence, and (2) the vanishing advantage problem, where samples that are too easy or too difficult result in zero advantages, removing the training signal. To overcome these issues, we propose DeepVideo-R1, video large language model trained with two key components: Regressive GRPO (Reg-GRPO) and difficulty-aware data augmentation. RegGRPO transforms the GRPO objective into regression task that directly predicts group-based advantage values. This change allows the models outputs to directly align with the advantage values, removing the need for the safeguards. We also introduce difficulty-aware augmentation to address the vanishing advantage problem. Our augmentation strategy dynamically adjusts input difficulty by adding reasoning cues to difficulty samples or by perturbing video content in easy samples, which helps generate diverse reward signals. Our experimental results show the effectiveness of DeepVideo-R1 on multiple challenging video reasoning benchmarks, demonstrating superior performance over several recent video LLMs such as Qwen2.5-VL and Intern3-VL. Notably, our model achieves consistent improvements on both in-distribution and out-of-distribution tasks, indicating robust generalization capabilities. These results underscore the benefits of combining regression-based RL objective with data augmentation for training large-scale multimodal reasoning models. Our main contributions are listed as: We introduce Reg-GRPO, novel optimization scheme that treats GRPO as regression problem over group-based advantage values, eliminating safeguards such as clipping and min operations, and mitigating the vanishing advantage problem. We develop difficulty-aware augmentation framework that adjusts video-text inputs using reasoning cue injection or noise perturbation to diversify the reward signals. We propose DeepVideo-R1, video large language model, trained with two key innovations: Regressive GRPO (Reg-GRPO) and difficulty-aware data augmentation. Our experiments demonstrate that DeepVideo-R1 significantly improves the reasoning capabilities of VideoLLMs on complex video reasoning tasks."
        },
        {
            "title": "2 Related works",
            "content": "Video Large Language Models (VideoLLMs). With the strong reasoning abilities of Large Language Models (LLMs) [1315], Video Large Language Models (Video LLMs) have shown remarkable performance across diverse video tasks [1618], such as video question-answering [5, 6, 19, 20, 8] and video captioning [2123], through comprehensive understanding of video content. Despite their impressive performance, Video LLMs exhibit limited performance on complex video reasoning 2 tasks [2426]. To address these challenges, we leverage GRPO-based reinforcement fine-tuning approach to improve the reasoning capabilities of VideoLLMs. Video reinforcement fine-tuning. Multiple works [27, 12, 11, 28] have significantly improved the reasoning capabilities of LLMs through reinforcement learning (RL) such as DPO [29] and RLHF [30]. Recently, the variation of RL-based fine-tuning [31, 32] has emerged and explored the potential of direct reward regression loss derived from the RL objective. Utilizing Group Relative Policy Optimization (GRPO), an RL algorithm proposed in [11], several approaches have demonstrated substantial improvements in the reasoning abilities of MLLMs across various image [3340] and video tasks [4147]. While existing approaches [42, 45, 44] have primarily focused on defining appropriate reward functions for each visual task, some concurrent works [38, 39] focus on the problem during GRPO training to enhance the models reasoning capabilities. In this work, we propose learning algorithm that directly regresses the advantage instead of simply increasing the likelihood of high-advantage responses. Additionally, difficulty-aware data augmentation is introduced to provide diverse and dense learning signals."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we present video large language model named DeepVideo-R1, which is trained with Regressive GRPO (Reg-GRPO) and difficulty-aware data augmentation for effective video context reasoning. We first introduce post-training methods for VideoLLMs, such as supervised fine-tuning and group-relative policy optimization (GRPO), and discuss GRPOs reliance on heuristic safeguards and vanishing advantage problems. Then, we propose Reg-GRPO, which improves the RL-based GRPO approach by transforming it into simpler yet more effective regression loss, eliminating heuristic safeguards such as the min and clipping functions. Finally, we present novel difficulty-aware data augmentation, which alleviates the vanishing advantage problem by modulating the difficulty of samples and fostering more diverse reward signals. 3.1 Background Supervised Fine-Tuning (SFT) is simple post-training method, which aligns the outputs of the video large language models πθ with human-curated ground-truth data. Given an input sample = (v, q), which consists of video and input prompts q, and ground-truth output y, SFT optimizes the model via the following objective: 1 ygt ygt t=1 (cid:88) SFT (θ) = Ex,ygt (X,Y ) log πθ x, ygt <t ygt , (1) 1 (cid:2) <t = ygt 1 , . . . , ygt is output token sequence until (cid:0) where ygt 1-th token. The goal of supervised fine-tuning is to maximize the average log-likelihood of the model providing the ground-truth token yt, conditioned on the input and the proceeding tokens y<t. Group Relative Policy Optimization (GRPO) [11] adapts the PPO [48] by approximating the learnable value function with the average reward of multiple sampled outputs. Concretely, for given input sample x, multiple output sequences are sampled from the old policy model πθold and then trains the policy model πθ with the following objective: i= y(i) (cid:1) (cid:3) (cid:8) (cid:9) GRPO (θ) = Ex (X) 1 y(i) i=1 (cid:88) t=1 (cid:88) min πθ (cid:16) πθold y(i) y(i) x, y(i) <t (cid:17) x, y(i) <t ˆA(i), clip πθ (cid:16) πθold x, y(i) y(i) <t (cid:17) x, y(i) y(i) <t , ϵ, 1 + ϵ ˆA(i) (2) (cid:16) (cid:17) (cid:16) (cid:17) where ϵ corresponds to hyperparameter and ˆA(i) is advantage calculated based on the relative reward within the group, which is formulated as ˆA(i) = R(x,y(i)) , where µr, σr denotes the average and standard deviation values of set of rewards in the group, respectively. Although GRPO has shown its success, GRPO has two limitations that hinder the effective model optimization: reliance on heuristic safeguards and vanishing advantage problems. µr σr Reliance on safeguards. GRPO optimizes the model with safeguards implemented using the min and clipping functions to avoid extreme changes in the model. However, the PPO-style clipping function induces textbfzero gradient for the sample where the value of πθ (y x) is too different from the value of πθold (y x) to stay close to πθref if the model is already far from πθref (y x) [49]. Similarly, GRPO also suffers from this phenomenon due to the PPO-style hard constraints, and it deteriorates the effective model training. x). It cannot promise the model πθ (y Vanishing advantage problem. The vanishing advantage problem [40] in GRPO refers to the scenario where all advantage estimates within generation group become zero since the rewards of outputs in the group become equal. This is problematic since the model receives no learning signal from training sample. In particular, we observe that this issue often arises when training samples are either too easy or too difficult to the current model. 3.2 Regressive GRPO Here, we introduce Reg-GRPO (Regressive Group Relative Policy Optimization), which reformulates GRPO into the regression task, removing safeguards such as the min and clipping functions. This reformulation enables the model to directly predict the advantages, resulting in improved alignment of the model with the tasks. Following existing RL-based works [50, 51, 29, 32], the Reg-GRPO loss function is derived from the RL objective that maximizes the expected reward with the KL constraints between πθ and πθold . RL objective. The objective of our reinforcement learning algorithm for each iteration is to maximize rewards while preventing πθ from making excessive changes relative to πθold. πθ = arg max πθ Ey πθ( x)R (x, y) ϵ DKL (πθ πθold) . (3) Following prior works [29, 32], the closed-form solution to the above equation (Eq. (3)) can be obtained by minimum relative entropy problem: 1 (x) 1 ϵ πθ (y x) = πθold (y x) exp (x, y) , x, (4) 1 ϵ (cid:18) (cid:19) where (x) = is partition function. However, since calculating the partition function (x) is expensive, it is hard to obtain πθ exactly. We can rewrite Eq. (4) to (cid:80) express the reward (x, y) in terms of the optimal model πθ , which can be formulated as x) exp πθold (y (x, y) (cid:0) (cid:1) (x, y) = ϵ (cid:18) log (x) + log x) πθ (y x) πθold (y (cid:18) (cid:19)(cid:19) x, y. (5) Based on Eq. (5), we can formulate the problem as learning to directly predict the reward (x, y). However, it still requires calculating the partition function (x), which is expensive. The simple method to address it is to estimate (x) with Monte-Carlo (MC) sampling as follows: (x) = Ey πθold ( x) exp 1 ϵ (cid:18) (x, y) 1 (cid:19) i=1 (cid:88) exp 1 ϵ (cid:18) (cid:16) (cid:17)(cid:19) x, y(i) , (6) where y(i) is generated by πθold. However, since the function (x) is estimated via MC sampling, it is still not accurate, which negatively influences the model optimization. Additionally, relying solely on the reward signal for optimization can introduce high variance and hinder effective learning. Reg-GRPO Loss. To address these issues, we propose Reg-GRPO (Regressive GRPO) loss, which learns the policy model to regress the advantage ˆA(i) instead of the reward, removing the normalization term (x). The advantage ˆA(i) is defined as where µr, σr denotes the average and standard deviation values of set of rewards in the group, respectively. We can rewrite the advantage in terms of the optimal policy in Eq. (5) as follows: ˆA(i) = x, y(i) σr (cid:0) (cid:1) µr , (7) ˆA(i) = ϵ log ρ x, y(i) σρ (cid:0) (cid:1) µρ , (cid:1) (cid:0) ρ (x, y) = πθ (y (y πθold , x) x) (8) 4 Figure 2: Overview of the difficulty-aware data augmentation. First, we assess the difficulty of responses given the input video and question by comparing the average reward within the group with the global reward global. For hard samples, it augments the input prompts with the reasoning cues extracted from successful reasoning paths (Difficulty decreasement augmentation), while the easy samples are perturbed with the noise (Difficulty increasement augmentation). The scale of the guidance level or noise level is adaptively determined based on the difficulty of the current sample. where µρ, σρ are mean and standard deviation of log ρ (x, y), respectively. Interestingly, we can see that (x) is removed during the parameterization. Based on Eq. (8), the Reg-GRPO loss function is to minimize the gap between two sides of Eq. (8): reg-grpo = Ex (X) 1 ˆA(i) i=1 (cid:32) (cid:88) ϵ log ρ x, y(i) σρ (cid:0) (cid:1) µρ 2 . (cid:33) (9) The proposed Reg-GRPO loss serves as an effective alternative for optimizing group-level objectives, showing better performance than GRPO. In our experiments, we demonstrate that this formulation leads to faster convergence and improved policy quality. The detailed derivation procedure and more discussions about Reg-GRPO loss are in the supplement. 3.3 Difficulty-aware data augmentation In this section, we present difficulty-aware data augmentation framework to address the vanishing advantage problem in GRPO. This issue arises when training samples are either too easy or too difficult, leading to uniform rewards across multiple responses. As result, advantage values collapse to zero, eliminating informative learning signal. Our augmentation strategy mitigates this issue by modulating the input difficulty to increase variance in predicted rewards, thereby preserving meaningful gradients during optimization. To assess the samples difficulty, we compare rcurr with reference reward rref, defined as moving average over the past training steps: rcurr = 1 . Samples with rcurr < rref are considered easy, while those with higher rewards are deemed difficult. Based on this comparison, we adaptively augment the input to either (cid:1) increase or decrease its difficulty, dynamically scaling the augmentation level to promote diversity in reward distributions and stabilize advantage estimation. i=1 x, y(i) (cid:80) (cid:0) Based on this reward-based difficulty measurement, the difficulty-aware data augmentation adaptively augment the input video/question. For easy samples, we increase the perturbation level to make the task more challenging. For difficult samples, we introduce guidance to simplify the task. We dynamically adjust the guidance/perturbation level according to the difficulty of the current sample. This dynamic augmentation scaling scheme encourages diverse distribution of rewards and prevents the degeneration of advantage values. Table 1: Performance on SEED-Bench-R1 validation split. In-Dist. means in-distribution dataset while OOD indicates out-of-distribution. Method Baseline VideoLLaMA3-7B [8] InternVL3-2B [6] InternVL3-8B [6] InternVL3-14B [6] Qwen2-VL-2B Qwen2-VL-2B [52] Qwen2-VL-2B + SFT Qwen2-VL-2B + GRPO DeepVideo-R1-Qwen2-VL-2B (Ours) Qwen2-VL-7B Qwen2-VL-7B [52] Qwen2-VL-7B + SFT Qwen2-VL-7B + GRPO DeepVideo-R1-Qwen2-VL-7B (Ours) Qwen2.5-VL-3B Qwen2.5-VL-3B [5] Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + GRPO DeepVideo-R1-Qwen2.5-VL-3B (Ours) Qwen2.5-VL-7B Qwen2.5-VL-7B [5] Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + GRPO DeepVideo-R1-Qwen2.5-VL-7B (Ours) SBR-L1 (In-Dist.) Epic-Kitchens Daily life SBR-L2 (OOD) Ego4D Daily life SBR-L3 (OOD) Ego4D Daily life Hobbies Recreation Work Overall 33.33 23.71 41.41 43.61 12.88 34.05 38.39 48.88 34.79 43.79 46.01 56.39 31.32 35.90 39.56 48.05 33.41 42.37 49.07 51.95 33.21 23.14 40.75 45. 16.35 36.23 42.01 50.31 34.02 44.10 50.16 59.75 32.70 39.12 41.01 51.07 38.24 42.64 52.08 55.72 26.65 21.18 39.18 43.96 12.98 36.90 40.55 52. 31.21 38.27 48.52 57.63 33.03 39.86 38.95 46.47 35.08 41.23 49.43 51.25 28.47 16.27 34.58 34.92 16.27 34.58 37.63 42.71 32.30 41.02 45.08 52. 28.81 31.53 33.90 45.76 31.53 37.29 40.68 47.80 30.60 18.58 34.97 36.61 10.38 29.95 30.45 41.09 33.33 32.24 43.72 50.00 27.32 29.70 36.61 43. 27.32 36.63 43.17 46.99 26.98 12.62 29.95 31.68 14.36 33.88 40.44 49.18 30.69 38.61 41.34 55.19 23.02 31.15 31.93 40.10 27.97 41.53 35.15 44. 27.71 17.11 34.75 37.17 13.78 33.84 36.79 46.33 31.57 38.15 44.89 53.82 28.24 33.69 35.35 43.98 31.04 38.99 42.24 47.69 Difficulty-decreasing augmentation. To ease the task for difficult samples, we embed reasoning cues that assist the model in generating higher-quality answers. Concretely, we augment the input prompt with the ground-truth answer and sample multiple reasoning paths. We select the response with the highest reward and extract partial reasoning trace. This trace is then injected into the original prompt, to reduce the difficulty of the reasoning process. The amount of guidance is governed ) + 1, which controls the number of hint sentences injected into the by lguide = round ( original prompt. This structured augmentation reduces the reasoning burden and facilitates answer generation. rcurr rref Difficulty-increasing augmentation. To increase task difficulty for easy samples, we increase task complexity by perturbing the video input with frame-level Gaussian noise. The perturbation level + 1. We perturb the input video by adding the perturbation is computed as lperturb = level-weighted gaussian noise τ (0, 1). This controlled degradation of visual information increases the difficulty, encouraging exploration of unique reasoning paths, resulting in diversifying the reward signal. rref lperturb, where τ rcurr"
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings Datasets. To validate the effectiveness of the proposed method, we conduct evaluations on various video benchmarks, including both general video understanding tasks (e.g., SEED-Bench-R1 [44]), fine-grained spatial-temporal video reasoning tasks (NExTGQA [26]), and long video understanding tasks (e.g., LongVideoBench [16]). Implementation details. We employ Qwen2-VL-2B/7B and Qwen2.5-VL-3B/7B [5] for the experiments. For the analysis, we use Qwen2.5-VL-3B as default video large language model. We use the accuracy and format rewards to train the model on SEED-Bench-R1 dataset following existing works [44]. We use accuracy, IoU, and format rewards to train the model on NExTGQA [42]. 6 Method Table 2: Performance on LongVideoBench validation split. Duration Group (s) (8,15] (15,60] (180,600] (900,3600] Question Category L1 (Perception) L2 (Relation) Baseline VideoLLaMA3-7B [8] InternVL3-2B [6] InternVL3-8B [6] InternVL3-14B [6] Qwen2-VL-2B Qwen2-VL-2B [52] Qwen2-VL-2B + SFT Qwen2-VL-2B + GRPO DeepVideo-R1-Qwen2-VL-2B (Ours) Qwen2-VL-7B Qwen2-VL-7B [52] Qwen2-VL-7B + SFT Qwen2-VL-7B + GRPO DeepVideo-R1-Qwen2-VL-7B (Ours) Qwen2.5-VL-3B Qwen2.5-VL-3B [5] Qwen2.5-VL-3B + SFT Qwen2.5-VL-3B + GRPO DeepVideo-R1-Qwen2.5-VL-3B (Ours) Qwen2.5-VL-7B Qwen2.5-VL-7B [5] Qwen2.5-VL-7B + SFT Qwen2.5-VL-7B + GRPO DeepVideo-R1-Qwen2.5-VL-7B (Ours) 35.68 41.62 54.59 69.19 32.97 42.62 47.03 51.35 42.33 44.97 54.50 56.22 50.27 51.35 51.35 54.05 54.59 54.59 61.08 62.70 43.14 48.37 66.67 62. 32.68 42.31 46.41 49.67 44.77 54.65 53.49 61.44 62.09 52.94 62.09 64.05 63.40 56.21 60.78 63.40 20.98 33.66 46.10 46.34 29.51 37.02 36.59 38. 33.98 36.65 42.48 45.12 39.02 36.34 42.20 45.85 37.80 41.46 44.39 49.27 22.52 29.96 44.15 42.02 22.16 32.22 32.09 34.75 25.35 36.35 37.23 40. 32.14 35.28 35.99 43.44 36.17 38.12 40.78 44.50 30.24 34.24 54.40 57.28 29.28 38.94 41.12 45.12 34.24 44.80 48.48 52.48 47.67 45.92 46.08 52. 51.20 51.20 52.64 57.60 23.60 35.25 44.52 43.54 25.28 34.06 34.13 35.67 31.74 35.81 38.90 41.99 34.08 34.27 40.73 45.08 34.83 37.50 43.40 45. Overall 26.70 34.78 49.14 49.96 27.15 36.34 37.40 40.09 32.90 40.00 43.40 46.90 40.44 39.72 43.23 48.39 42.48 43.90 47.72 51. Table 3: Performance on various video temporal grounding benchmarks. The best results are marked in bold. Note that zero-shot performances are reported on ActivityNet-Captions to evaluate the out-of-domain generalization capability, where models are trained on Charades-STA. ActivityNet-Captions Charades-STA R@0.5 R@0.5 Method R@0.7 R@0.7 mIoU mIoU Vision Experts FlashVTG [53] InternVideo2-6B [7] SG-DETR [54] VideoLLMs VTimeLLM-7B [55] VTimeLLM-13B [55] HawkEye [56] TimeChat [57] VTG-LLM [58] TRACE [59] VideoChat-T [60] Qwen2.5-VL-7B [5] Qwen2.5-VL-7B + SFT TimeZero [45] VideoChat-R1 [42] VideoChat-R1-thinking [42] DeepVideo-R1 (Ours) - 58.8 60.7 - - 49.3 - - - - 29.0 46.3 - 60.8 59.9 61.2 4.2 Experimental Results 70.3 70.0 71.1 - - 58.3 46.7 57.2 61.7 67.1 24.2 45.0 72.5 71.7 70.6 71. 49.9 49.0 52.8 - - 28.8 23.7 33.4 41.4 43.0 11.1 25.3 47.9 50.2 47.2 50.6 - - - 30.4 31.4 32.7 - - - - 21.1 20.6 - 36.6 35.5 36.9 - - - 27.8 29.5 29.3 - 57.2 - - 15.8 16.7 26.5 33.4 33.3 33. - - - 14.3 14.2 10.7 - 33.4 - - 7.5 7.9 16.5 17.7 16.7 18.0 Experimental results on SEED-Bench-R1. Table 1 presents the experimental results of the baselines, the model trained with supervised fine-tuning and GRPO, and our DeepVideo-R1 on the validation splits of SBR (SEED-Bench-R1) datasets. The table demonstrates that our DeepVideo-R1 achieves the best performance compared to other baselines and GRPO-trained models. In comparison to Qwen2.5-VL-3B+GRPO, our DeepVideo-R1-3B shows performance improvement of 8.49, 10.06, 8.63 on SBR-L1, L2, L3 (overall) datasets, respectively. In particular, the performance gain on SBR-L2, L3 (overall) is larger than SBR-L1, indicating that our DeepVideo-R1-3B significantly improves generalization capabilities. Experimental results on LongVideoBench. We also evaluate the performance of our DeepVideo-R1 on LongVideoBench to explore the long video understanding capabilities of our DeepVideo-R1. The experimental results are in Table 2. The table shows that our DeepVideo-R1 still outperforms other baselines with 51.08 overall score based on Qwen2.5-VL-7B. In particular, DeepVideo-R1-Qwen2.57 Table 4: Experimental results on NExTGQA Method Vision Experts IGV [61] Temp[CLIP] [62] FrozenBiLM [62] SeViLA [63] VideoLLMs VideoChat-R1 [42] VideoChat-R1-thinking [42] DeepVideo-R1 (Ours) mIoU Acc@QA 14.0 12.1 9.6 21.7 32.4 36. 36.8 50.1 60.2 70.8 68.1 70.6 69.2 72.5 Table 5: Ablation study on training schemes (RegGRPO and difficulty-aware data augmentation (DA-Aug.) in DeepVideo-R1 using SEEDBench-R1 dataset. Method DA-Aug. L1 (In-Dist.) L2 (OOD) L3 (OOD) Qwen2.5-VL-3B GRPO GRPO Reg-GRPO Reg-GRPO 31.32 39.56 41.71 44.20 48. 32.70 41.01 42.52 44.15 51.07 26.95 35.35 36.64 39.52 43. Table 6: Performance comparison on reinforcement learning algorithm. Method L1 L2 L3 Qwen2.5-VL-3B 31.32 32.70 26.95 + DPO [29] + Online DPO + REINFORCE + RLOO + REBEL [32] + Reward-Regression + GRPO + Reg-GRPO (Ours) 35.76 37.07 37.02 34.98 41.80 32.54 39.56 44.20 35.22 38.11 39.50 37.36 43.65 33.08 41.01 44. 30.81 31.87 32.32 31.34 37.93 28.31 35.35 39.52 Table 7: Performance comparison on absolute and relative difficulty measurement. Diff. ref. L1 L2 Absolute Relative 47.90 48.05 50.06 51.07 40.65 43.98 Table 8: Performance comparison according to the data augmentation types. Diff indicates difficulty increasing augmentation. Diff indicates difficulty decreasing augmentation. Diff. Diff. L1 L2 L3 44.20 45.27 45.32 48.05 44.15 46.92 47.30 51.07 36.34 39.97 41.56 43.98 Table 9: Performance comparison on augmentation scaling scheme (Fixed guidance/noise level v.s. Adaptive guidance/noise level). Aug. L1 L2 L3 No Aug. Fixed Adaptive 44.20 46.83 48. 44.15 48.58 51.07 36.34 43.00 43.98 VL-3B shows the impressive performance gain of 7.46 compared to Qwen-2.5-VL-3B + GRPO on long (900,3600] duration group. This result demonstrates that our DeepVideo-R1 effectively performs complex video reasoning. Experimental results on NextGQA. Table 4 shows the experimental results of our DeepVideo-R1 and other baselines, including vision experts specialized in grounding vision question answering and VideoLLMs. VideoChat-R1 is baseline using Qwen2.5-VL-7B trained with GRPO loss. For NExTGQA dataset, we train DeepVideo-R1 with the accuracy, format, and IoU reward to reflect the nature of NExTGQA. From the table, our DeepVideo-R1 consistently shows better performance than other baselines on all metrics. This result indicates that DeepVideo-R1 is robust to the reward function. Experimental results on temporal grounding benchmarks. We also measure the performance of DeepVideo-R1 based on Qwen2.5-VL-7B by training the model with the training set of Charades-STA in Table 3. To explore the out-of-domain generalization capability, we report the performance on ActivityNet-Captions using the model checkpoint trained on Charades-STA. The table shows that DeepVideo-R1 shows the best performance on both datasets and every metric. 4.3 Analysis Ablation studies. We conduct ablation studies to explore the contribution of Reg-GRPO and difficultyaware data augmentation (DA-Aug.) in Table 5. The table demonstrates that both Reg-GRPO and difficulty-aware data augmentation contribute to the performance improvement of DeepVideo-R1. By comparing the GRPO-trained model without and with difficulty-aware data augmentation, the 8 Figure 3: Vanishing advantage ratio comparison on GRPO and GRPO+DA-Aug (Difficulty-aware augmentation) (Left). Reward curves of DeepVideo-R1 (Ours) and GRPO (Right). model with difficulty-aware data augmentation shows 2.15 improvement on SBR (L1), which shows that the difficulty-aware data augmentation is effective in GRPO as well as Reg-GRPO. Also, RegGRPO (w/o DA-Aug.) shows its effectiveness with the performance improvement of 4.17 compared to Qwen2.5-VL-3B+GRPO (w/o DA-Aug.) on SBR (L3). It leads that Reg-GRPO is more effective than GRPO by directly predicting the advantages. Comparison with reinforcement learning methods. Table 6 compares our Reg-GRPO with representative reinforcement fine-tuning (RFT) methods. From the table, our DeepVideo-R1 shows the best performance compared to other RFT methods. Especially, compared to reward regression, which directly regresses the reward score, our DeepVideo-R1 shows significantly better performance. This result highlights that directly aligning the advantage is more effective than the reward regression. Impact of relative difficulty measurement. We compare absolute and relative difficulty measurements for adaptive data augmentation in Table 7. The relative scheme, which considers reward history statistics, consistently outperforms the absolute counterpart across all difficulty levels (L1L3) on SeedBench-R1 dataset, demonstrating its superior ability to guide effective augmentation. Impact of augmentation decreasing/increasing augmentation. In Table 8, we provide the performance comparison according to the data augmentation scheme: difficulty increasing and decreasing augmentations. The table demonstrates that the model trained with both difficulty-increasing and difficulty-decreasing augmentation schemes shows the best performance achieving 7.65 performance gain on SBR (L3), which is out-of-distribution dataset. This reveals that adjusting the samples difficulty to moderate level is important for learning with group-normalized advantages. Impact of augmentation scaling scheme. In Table 9, we compare different augmentation scaling strategies: no augmentation, fixed scaling (constant guidance/noise level), adaptive scaling (difficultyaware guidance/noise). From the table, the adaptive strategy consistently outperforms the others across all difficulty levels (L1L3), demonstrating the importance of tailoring augmentation strength based on input difficulty. Impact of augmentation. We compare the vanishing advantage ratio of GRPO and GRPO with DA-Aug (difficulty-aware data augmentation) in Figure 3 (left). The figure shows that our data augmentation effectively reduces the ratio of samples causing the vanishing advantage. This indicates that the data augmentation well addresses the vanishing advantage problem of GRPO. Reward curves. In addition, we plot the reward curves of GRPO and our DeepVideo-R1, where the x-axis is training step and y-axis is the average reward in Figure 3 (right). From the figure, our DeepVideo-R1 gets higher average reward with direct advantage alignment and difficulty-aware data augmentation. Qualitative results. Figure 4 presents qualitative example from SEED-Bench-R1-7B, comparing the outputs of our DeepVideo-R1 and the Qwen2.5-VL-7B trained with GRPO. The task is to predict the next action given video. Our DeepVideo-R1 correctly infers that the person will continue moving berries. While the GRPO-only model fails to recognize the presence of berries. This demonstrates that DeepVideo-R1 has strong visual grounding and understanding capabilities. Figure 4: Qualitative result of DeepVideo-R1-Qwen2.5-7B in comparison of Qwen2.5-VL7B+GRPO."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose video large language model, DeepVideo-R1, trained with Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation to address the two problems in group relative policy optimization. RegGRPO reformulates the GRPO loss function into regression task that directly aligns the model with the group-normalized advantage in GRPO. Difficulty-aware data augmentation modulates the difficulty of the input to alleviate the vanishing advantage problem. Our experiments demonstrate that our DeepVideo-R1 is effective with diverse VideoLLMs outperforming GRPO-based reinforcement finetuning."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. [3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. [4] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [7] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for multimodal video understanding. In ECCV, 2024. [8] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. 10 [9] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. In NAACL, 2025. [10] Daechul Ahn, Yura Choi, San Kim, Youngjae Yu, Dongyeop Kang, and Jonghyun Choi. Isr-dpo: Aligning large multimodal models for videos by iterative self-retrospective dpo. In AAAI, 2025. [11] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:18771901, 2020. [14] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. NeurIPS, 37:2882828857, 2024. [17] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [18] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. [19] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [21] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: modularized multi-modal foundation model across text, image and video. In ICML, pages 3872838748. PMLR, 2023. [22] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [23] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024g. URL https://arxiv. org/abs/2410.02713. [24] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, pages 52675275, 2017. [25] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706715, 2017. [26] Junbin Xiao, Angela Yao, Yicong Li, and Tat-Seng Chua. Can trust your answer? visually grounded video question answering. In CVPR, pages 1320413214, 2024. [27] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 11 [28] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [29] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 2023. [30] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [31] Ke Zhu, Liang Zhao, Zheng Ge, and Xiangyu Zhang. Self-supervised visual preference alignment. In ACMMM, 2024. [32] Zhaolin Gao, Jonathan D. Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims, J. Andrew Bagnell, Jason D. Lee, and Wen Sun. Rebel: Reinforcement learning via regressing relative rewards. NeurIPS, 2024. [33] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [34] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [35] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv preprint arXiv:2503.18013, 2025. [36] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [37] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [38] Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, Jingyuan Chen, Chang Yao, and Jie Song. Boosting mllm reasoning with text-debiased hint-grpo. arXiv preprint arXiv:2503.23905, 2025. [39] Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. [40] Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025. [41] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [42] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal perception via reinforcement fine-tuning. arXiv preprint arXiv:2504.06958, 2025. [43] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Tinyllava-video-r1: Towards smaller lmms for video reasoning. arXiv preprint arXiv:2504.09641, 2025. [44] Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, and Xihui Liu. Exploring the effect of reinforcement learning on video understanding: Insights from seed-bench-r1. arXiv preprint arXiv:2503.24376, 2025. [45] Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, and Qin Jin. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377, 2025. [46] Peiran Wu, Yunze Liu, Miao Liu, and Junxiao Shen. St-think: How multimodal large language models reason about 4d worlds from ego-centric videos. arXiv preprint arXiv:2503.12542, 2025. 12 [47] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning. arXiv preprint arXiv:2503.20752, 2025. [48] Yuhui Wang, Hao He, and Xiaoyang Tan. Truly proximal policy optimization. In UAI, pages 113122, 2020. [49] Chloe Ching-Yun Hsu, Celestine Mendler-Dünner, and Moritz Hardt. Revisiting design choices in proximal policy optimization. In ICLR, 2020. [50] Jan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational space control. In ICML, 2007. [51] Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In AAAI, 2010. [52] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [53] Zhuo Cao, Bingqing Zhang, Heming Du, Xin Yu, Xue Li, and Sen Wang. Flashvtg: Feature layering and adaptive score handling network for video temporal grounding. In WACV, pages 92269236. IEEE, 2025. [54] Aleksandr Gordeev, Vladimir Dokholyan, Irina Tolstykh, and Maksim Kuprashevich. Saliency-guided detr for moment retrieval and highlight detection. arXiv preprint arXiv:2410.01615, 2024. [55] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower llm to grasp video moments. In CVPR, pages 1427114280, 2024. [56] Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu, and Dongyan Zhao. Hawkeye: Training video-text llms for grounding text in videos. arXiv preprint arXiv:2403.10228, 2024. [57] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: time-sensitive multimodal large language model for long video understanding. In CVPR, pages 1431314323, 2024. [58] Yongxin Guo, Jingyu Liu, Mingda Li, Dingxin Cheng, Xiaoying Tang, Dianbo Sui, Qingbin Liu, Xi Chen, and Kevin Zhao. Vtg-llm: Integrating timestamp knowledge into video llms for enhanced video temporal grounding. In AAAI, volume 39, pages 33023310, 2025. [59] Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, and Xiaoying Tang. Trace: Temporal grounding video llm via causal event modeling. arXiv preprint arXiv:2410.05643, 2024. [60] Xiangyu Zeng, Kunchang Li, Chenting Wang, Xinhao Li, Tianxiang Jiang, Ziang Yan, Songze Li, Yansong Shi, Zhengrong Yue, Yi Wang, et al. Timesuite: Improving mllms for long video understanding via grounded tuning. arXiv preprint arXiv:2410.19702, 2024. [61] Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. Invariant grounding for video question answering. In CVPR, pages 29282937, 2022. [62] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. NeurIPS, 35:124141, 2022. [63] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. NeurIPS, 36:7674976771, 2023. Discussion and Derivation of Reg-GRPO A.1 Deriving the optimum of the KL-constrained reward maximization optimization We will derive Eq. (4) from Eq. (3). We optimize the πθ with following objective: πθ = arg max πθ Ex,y πθ( x)R (x, y) ϵ DKL (πθ πθold) , (10) where to the above minimum relative entropy problem. is the reward function and πθold is the old policy model. We can obtain closed-form solution πθ = arg max πθ Ex,y πθ( x)R (x, y) ϵ ϵ log πθold) DKL (πθ x) πθ (y πθold (y x) 1 (x, y) ϵ x) x) (cid:21) = arg max πθ Ex,y πθ( x) (cid:20) (x, y) = arg min πθ Ex,y πθ( x) = arg min πθ Ex,y πθ( x) = arg min πθ Ex,y πθ( x) log (cid:20) (cid:34) (cid:34) log log πθ (y πθold (y πθold (y πθ (y x) exp x) 1 ϵ (cid:0) x) πθ (y x) exp 1 Z(x) πθold (y (cid:21) (11) (x, y) (cid:35) (cid:1) (x, y) 1 ϵ log (x) (cid:35) , πθold (y where (x) = function is only dependent on and the old policy πθold. Now let πθ be defined as: x) exp (x, y) (cid:80) (cid:1) (cid:0) 1 ϵ (cid:1) is partition function. Please note that the partition (cid:0) πθ (y x) = 1 (x) πθold (y x) exp 1 ϵ (x, y) . It can be seen as valid probability distribution as πθ (y Since (x) is not function of y, the above minimization problem can be formulated as πθ (y (cid:18) x) (cid:19) 0 for all and (cid:80) arg min π Ex,y = arg min π log πθ( x) Ex [DKL (πθ (y (cid:20) πθ (y πθ (y x) x) x) πθ (y log (x) (cid:21) log (x)] x)) (12) x) = 1. (13) Since the partition function (x) is not dependent on π, the optimal π is the policy that minimizes the first KL term. Since the optimal KL-divergence is achieved if and only if two distributions are identical, we have optimal solution as: πθ (y x) = 1 (x) πθold (y x) exp 1 ϵ (cid:18) (x, y) , (cid:19) x, y, for all x. A.2 Deriving the reward in terms of the optimal policy The reward can be reorganized under the optimal policy. We can invert Eq. (14) as follows: exp (cid:18) 1 ϵ 1 ϵ (x, y) = (x) (cid:19) x) πθ (y x) πθold (y , (x, y) = log (x) + log (cid:18) (x, y) = ϵ (cid:18) log (x) + log , x, y. (cid:19)(cid:19) , x) πθ (y x) πθold (y (cid:19) x) πθ (y x) πθold (y (cid:18) A.3 Deriving the advantage in terms of the optimal policy. The advantage ˆA(i) is defined as ˆA(i) = x, y(i) σr (cid:0) (cid:1) µr , 14 (14) (15) (16) where µr, σr denotes the average and standard deviation values of set of rewards in the group, respectively. We can rewrite the advantage in terms of the optimal policy in Eq. (15) as follows: log ρ x, y(i) + (x) j=1 ρ x, y(j) + (x) ˆA(i) = ϵ (cid:16) log ρ = ϵ (cid:0) x, y(i) σρ (cid:0) (cid:1) (cid:1) µρ , 1 σρ (cid:16) (cid:80) ρ (x, y) = , (cid:17)(cid:17) (17) (cid:1) , (cid:0) πθ (y πθold (y x) x) i=1 , respectively. Interestingly, where µρ, σρ are mean and standard deviation of we can see that (x) is removed during the parameterization. Base on Eq. (17), we can directly (cid:8) predict the group-normalized advantage with our VideoLLM. log ρ (cid:1)(cid:9) (cid:0) x, y(i) A.4 Reg-GRPO Based on Eq. (17), our Reg-GRPO (Regressive GRPO) is to learn the model πθ to directly predict the advantage as follows: reg-grpo = L"
        },
        {
            "title": "1\nG",
            "content": "G ˆA(i) log ρ ϵ i=1 (cid:32) (cid:88) where µρ, σρ are mean and standard deviation of (cid:0) x, y(i) σρ µρ (cid:33) (cid:1) , ρ (x, y) = x) πθ (y x) πθold (y , (18) log ρ x, y(i) i=1 , respectively. (cid:8) (cid:0) (cid:1)(cid:9)"
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: In Abstract and Introduction. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In the supplement. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] 16 Justification: In the supplement. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Experiments and supplement. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? 17 Answer: [Yes] Justification: In the supplement. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In Experiment and the supplement. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We conduct the experiment in the single run following other works. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In the supplement. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In the supplement. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to 19 generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release any datasets. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: In the supplement. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? 20 Answer: [Yes] Justification: In the supplement. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [Yes] 21 Justification: In the supplement. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        }
    ],
    "affiliations": [
        "KAIST",
        "Korea University"
    ]
}