{
    "paper_title": "Small Models, Big Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages",
    "authors": [
        "Daniil Gurgurov",
        "Ivan Vykopal",
        "Josef van Genabith",
        "Simon Ostermann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current state-of-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to a better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or a few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA-3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage."
        },
        {
            "title": "Start",
            "content": "small Models, BIG Impact: Efficient Corpus and Graph-Based Adaptation of Small Multilingual Language Models for Low-Resource Languages Daniil Gurgurov1,3 Ivan Vykopal2,4 Josef van Genabith3 Simon Ostermann3 1University of Saarland 2Brno University of Technology 3German Research Center for Artificial Intelligence (DFKI) 4Kempelen Institute of Intelligent Technologies (KInIT) 5 2 0 2 4 1 ] . [ 1 0 4 1 0 1 . 2 0 5 2 : r {daniil.gurgurov, josef.van_genabith, simon.ostermann}@dfki.de, ivan.vykopal@kinit.sk"
        },
        {
            "title": "Abstract",
            "content": "Low-resource languages (LRLs) face significant challenges in natural language processing (NLP) due to limited data. While current stateof-the-art large language models (LLMs) still struggle with LRLs, smaller multilingual models (mLMs) such as mBERT and XLM-R offer greater promise due to better fit of their capacity to low training data sizes. This study systematically investigates parameter-efficient adapter-based methods for adapting mLMs to LRLs, evaluating three architectures: Sequential Bottleneck, Invertible Bottleneck, and Low-Rank Adaptation. Using unstructured text from GlotCC and structured knowledge from ConceptNet, we show that small adaptation datasets (e.g., up to 1 GB of free-text or few MB of knowledge graph data) yield gains in intrinsic (masked language modeling) and extrinsic tasks (topic classification, sentiment analysis, and named entity recognition). We find that Sequential Bottleneck adapters excel in language modeling, while Invertible Bottleneck adapters slightly outperform other methods on downstream tasks due to better embedding alignment and larger parameter counts. Adapter-based methods match or outperform full fine-tuning while using far fewer parameters, and smaller mLMs prove more effective for LRLs than massive LLMs like LLaMA3, GPT-4, and DeepSeek-R1-based distilled models. While adaptation improves performance, pre-training data size remains the dominant factor, especially for languages with extensive pre-training coverage. The code for our experiments is available on GitHub1."
        },
        {
            "title": "Introduction",
            "content": "The need for effective natural language processing (NLP) tools for low-resource languages (LRLs) is pressing, as these languages lack sufficient data to train robust models (Joshi et al., 2020; 1The code is available at https://github.com/ d-gurgurov/Knowledge-Driven-Adaptation-LLMs Bird, 2022; Huang et al., 2023). While massive state-of-the-art (SoTA) large language models (LLMs) such as GPT-4 (OpenAI et al., 2024), LLaMA-2 (Touvron et al., 2023), Gemini (Team et al., 2023), BLOOM (Le Scao et al., 2023), and the DeepSeek model family (DeepSeek-AI et al., 2025) have demonstrated strong generalization capabilities across diverse tasks (Srivastava et al., 2022; Smith et al., 2022; Bang et al., 2023), they struggle to generalize effectively to LRLs (Cahyawijaya et al., 2023; Robinson et al., 2023; Hasan et al., 2024; Adelani et al., 2024a). Smaller multilingual language models (mLMs) like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) often show greater promise for LRLs (Hu et al., 2020; Asai et al., 2023; Adelani et al., 2024b). This work investigates parameter-efficient adaptation techniques (Houlsby et al., 2019) as an alternative to full fine-tuning, or continued pre-training, for adapting small mLMs to LRLs. We compare these approaches with the zeroand fewshot prompting and adapter-based adaptation of LLMs. Following Pfeiffer et al. (2020), Parović et al. (2023), and Gurgurov et al. (2024a), we integrate unstructured textual data and structured knowledge from knowledge graphs (KGs), exploring their complementary benefits. KGs, which encode cross-lingual semantic relationships, have been shown to be effective for various NLP tasks (Peters et al., 2019; Zhang et al., 2019; Wang et al., 2021), yet remain underexplored for LRLs. On the other hand, unstructured text provides rich contextual information and is widely used for adaptation (Neubig and Hu, 2018; Han and Eisenstein, 2019). Our contributions are threefold: First, we show that limited adaptation data yields significant gainsup to 1 GB of free text or few MB of KG data. We evaluate three adapter architectures: Sequential Bottleneck, Invertible Bottleneck, and LowRank Adaptation (Houlsby et al., 2019; Pfeiffer et al., 2020; Hou et al., 2022). Sequential Bottleneck excels in language modeling, while Invertible Bottleneck outperforms others on downstream tasks, likely due to differing parameterization. Adapter-based approaches match or outperform full fine-tuning while using fewer trainable parameters. Second, we highlight the effectiveness of smaller mLMs, such as XLM-R, for LRLs, outperforming both few-shot prompting and adaptation of massive SoTA LLMs such as GPT-3.5 (Ouyang et al., 2022b), LLaMA3 (Grattafiori et al., 2024), and DeepSeek-R1-based distilled models (DeepSeek-AI et al., 2025). This is in line with prior work suggesting that smaller models better align cross-lingual representations under constrained capacity (Wu et al., 2019; Dufter and Schütze, 2020; Yong et al., 2023) and shows that small LMs are often better suited for LRLs. Finally, analyzing 30 LRLs, we show direct relationship between pre-training and adaptation data size and performance, with adaptation data providing diminishing returns for languages with larger pre-training data coverage. We also observe moderate correlation between language modeling and downstream task performance, suggesting pseudo-perplexity as useful proxy for evaluating adaptation quality."
        },
        {
            "title": "2 Related Work",
            "content": "To improve multilingual models for LRLs without monolingual pre-training, researchers have explored full fine-tuning, adapter-based approaches, and other auxiliary methods."
        },
        {
            "title": "2.1 Full Fine-Tuning Adaptation",
            "content": "Full fine-tuning has been widely used to enhance LRL performance. Neubig and Hu (2018) utilized similar-language post-training to reduce overfitting. Domain-adaptive fine-tuning (Han and Eisenstein, 2019) improved contextualized models like mBERT on specific domains (e.g. Middle English). Further, language-specific fine-tuning on monolingual corpora (Gururangan et al., 2020; Chau et al., 2020) and adaptation with transliterated data (Muller et al., 2021) boosted performance on diverse tasks, such as dependency parsing and tagging. Ebrahimi and Kann (2021) showed that fine-tuning on Bible corpora improved tagging and named entity recognition in languages unseen during pre-training."
        },
        {
            "title": "2.2 Adapter-Based Adaptation",
            "content": "Adapters are parameter-efficient small modules that are inserted into model layers, avoiding catastrophic forgetting (French, 1999), reducing computational costs (Houlsby et al., 2019; Strubell et al., 2019), and requiring fewer training examples (Faisal and Anastasopoulos, 2022). Frameworks like MAD-X (Pfeiffer et al., 2020) introduced language and task adapters, improving named entity recognition. Extensions such as UDapter (Üstün et al., 2020) and MAD-G (Ansell et al., 2021) leveraged typological features for improved zero-shot inference. Hierarchical adapters based on language phylogeny (Faisal and Anastasopoulos, 2022), methods addressing resource imbalances with language combination (Lee et al., 2022a; Parović et al., 2022), and exposing task adapters to target languages during training to address training-inference mismatches (Parović et al., 2023) have further advanced adapter effectiveness. Recent work (Pfeiffer et al., 2022; Yong et al., 2023) emphasized the efficiency of adapter-based tuning over continued pre-training for LRLs, with performance tied to data quantity."
        },
        {
            "title": "2.3 Knowledge Graph Integration",
            "content": "KGs improve the quality of static word embeddings (Faruqui et al., 2014; Speer et al., 2017; Gurgurov et al., 2024b) and, more recently, LMs by leveraging structured semantic relationships, predominantly for high-resoure languages (Miller, 1995; Navigli and Ponzetto, 2012; Speer et al., 2017). Approaches like KnowBERT (Peters et al., 2019) and ERNIE (Zhang et al., 2019) improve LMs through entity linkers and attention. LIBERT (Lauscher et al., 2020b) incorporates semantic constraints for better task performance. CN-ADAPT (Lauscher et al., 2020a) and K-Adapter (Wang et al., 2021) use bottleneck adapters (Houlsby et al., 2019) to inject structured knowledge into models, improving commonsense reasoning and relational tasks."
        },
        {
            "title": "3 Methodology",
            "content": "This section describes our approaches to adapting mLMs for LRLs and the data resources used."
        },
        {
            "title": "3.1 Model Adaptation\nWe adapt mBERT (Devlin et al., 2019) and\nXLM-R-base (Conneau et al., 2020) using three\nSequential Bottleneck\nadapter architectures:\n(Seq_bn; Houlsby et al. (2019); Pfeiffer et al.\n(2020)), Sequential Bottleneck with Invertible\nLayers (Seq_bn_inv; Pfeiffer et al.\n(2020)),\nand Low-Rank Adaptation (LoRA; Hou et al.\n(2022)). Additionally, we adapt LLaMA-3-8B\n(Grattafiori et al., 2024), but exclusively with\nSeq_bn_inv adapters (due to computational con-\nstraints). Language adapters are pre-trained with\na masked language modeling (MLM) objective\n(Devlin et al., 2019) for mBERT and XLM-R\non structured data (ConceptNet; Speer et al.\n(2017)) and unstructured data (GlotCC; Kargaran\net al. (2024)).2 Further, we pre-train language\nadapters for LLaMA-3 with a causal language\nmodeling (CLM) objective (Radford, 2018), only\nwith unstructured data, leaving the exploration of\ngraph knowledge injection into large-scale LMs\nfor future work.",
            "content": "Task-specific adapters are trained on target language data using the Seq_bn architecture. These adapters are stacked on \"frozen\" LMs and language adapters, following prior work (Pfeiffer et al., 2020; Lee et al., 2022a; Parović et al., 2023). We also experiment with adapter fusion (Pfeiffer et al., 2021a), combining language adapters trained on different data types."
        },
        {
            "title": "3.2 Data Sources\nStructured Data. ConceptNet\n(Speer et al.,\n2017), a multilingual knowledge graph, provides\ncommon-sense knowledge across 304 languages.\nWe preprocess the data by converting Concept-\nNet triples into natural language sentences, simi-\nlar to Lauscher et al. (2020a) and Gurgurov et al.\n(2024a), using predefined predicates (Appendix\nA), and split it into train and validation sets.",
            "content": "Unstructured Data. GlotCC-V1 (Kargaran et al., 2024) is large-scale multilingual corpus derived from CommonCrawl (Wenzek et al., It emphasizes LRLs, providing high2020). quality text in 1,000 languages. To simulate 2Full fine-tuning is performed only on the GlotCC data for mBERT and XLM-R due to ConceptNets limited size. low-resource environment for all languages, we limit each language to 1 GB (if it exceeds this limit), clean the data, and split it into training and validation sets."
        },
        {
            "title": "4 Experimental Setup",
            "content": "This section details the experimental setup, including language selection, evaluation tasks, and adapter training procedures."
        },
        {
            "title": "Configuration",
            "content": "TC () NER () SA () MLM ()"
        },
        {
            "title": "Unseen",
            "content": "mBERT XLM-R Baseline + LoRA (Glot) + Seq_bn (Glot) + Seq_bn_inv (Glot) + LoRA (ConceptNet) + Seq_bn (ConceptNet) + Seq_bn_inv (ConceptNet) + Seq_bn (Glot+ConceptNet) + Seq_bn_inv (Glot+ConceptNet) Full Fine-tune Baseline + LoRA (Glot) + Seq_bn (Glot) + Seq_bn_inv (Glot) + LoRA (ConceptNet) + Seq_bn (ConceptNet) + Seq_bn_inv (ConceptNet) + Seq_bn (Glot+ConceptNet) + Seq_bn_inv (Glot+ConceptNet) Full Fine-tune 77.67 78.74 79.28 79.35 77.87 78.39 78.42 81.73 81.14 82.31 83.63 84.06 80.71 80.82 80.64 85. 28.72 36.65 41.42 42.4 24.88 25.87 24.18 43.65 34.52 40.94 49.72 51.43 29.08 33.19 33.59 57.3 83.82 84.2 84.46 84.36 84.38 84.35 84.7 84.36 84.36 77.33 77.52 78.57 78.17 78.38 77.64 78.62 80.83 80.68 42.54 44.51 45.04 45.64 41.32 41.2 41.48 44.21 44.93 54.45 52.01 54.4 55.64 52.71 49.39 51.04 61.83 60.31 82.18 82.75 82.99 83.64 82.59 81.9 81.58 84.07 87.45 87.98 88.2 88.2 87.48 87.09 87.28 88.56 71.03 73.27 73.3 73.91 70.79 70.48 71.54 73.97 60.72 62.02 65.94 65.88 60.00 58.64 59.52 68.19 25.17 10.44 8.95 14.31 37.37 41.22 55.95 9.25 15.65 6.83 6.53 10.56 20.29 20.01 22.81 10. 124.67 7434.61 12218.65 27170.23 126.44 139.25 157.49 81492.4 203.96 97.99 122.08 713.65 902.31 482.22 569.48 206.68 Table 1: Results for mBERT and XLM-R across 4 tasks: Topic Classification (TC), Named Entity Recognition (NER), Sentiment Analysis (SA), Masked Language Modeling (MLM). All numbers are the averages for the 30 studied LRLs and provided separately for the languages included (\"seen\") and languages not included (\"unseen\") in the pre-training data of model. The baselines are the models with single task adapter for downstream tasks, or without adapters for MLM. The full results for each task are in the Appendix. Appendix C) were used to train task adapters with similar hyperparameters, evaluating performance via F1 scores. Finally, Named Entity Recognition (NER) used the WikiANN dataset (Pan et al., 2017), with data distributions detailed in Table 7 (Appendix D), and was evaluated with the \"seqeval\" F1 score (Nakayama, 2018)."
        },
        {
            "title": "5 Results: Small mLMs",
            "content": "This section summarizes the outcomes of the mLM adaptation experiments. Tables 1 and 3 report the average results across 30 selected LRLs."
        },
        {
            "title": "5.1 Masked Language Modeling\nGlot-based\nimproved\nadapters\npseudo-perplexity (Tables 10 and 11 Appen-",
            "content": "substantially dices and H), particularly for mBERT. The Seq_bn adapter achieved the largest reduction, averaging 65% improvement, followed by LoRA and Seq_bn_inv. For XLM-R, Seq_bn also excelled overall, while LoRA performed better for high-resource languages. In contrast, ConceptNet-based adapters did not enhance MLM performance, likely due to the datasets limited size and structured nature, but showed utility in downstream tasks (Section 5.2). Full fine-tuning on GlotCC generally outperformed language adapters for mBERT  (Table 10)  , while adapters applied to XLM-R often surpassed full fine-tuning  (Table 11)  . Compared to larger models, Glot-based XLM-R adapters outperformed Glot500-m (Imani et al., 2023), despite the latters larger vocabulary and more extensive training data. The performance of Glot500m likely reflects its sampling strategy, which heavily prioritizes LRLs. Additionally, XLM-R-large without language adapters (Conneau et al., 2020) slightly surpassed XLM-R-base with adapters (Appendix J)."
        },
        {
            "title": "5.2.2 Named Entity Recognition\nFor mBERT, ConceptNet adapters provided mod-\nest average improvements mostly for seen lan-\nguages, with Seq_bn_inv achieving the high-\nest gains of 1 F1 point on average. Glot-based\nadapters offered slightly lower gains for seen lan-\nguages (0.5 points) but larger improvements for\nunseen ones, with Seq_bn_inv delivering an av-\nerage gain of 3 points. XLM-R exhibited sim-\nilar trends: ConceptNet adapters improved av-\nerage scores by 1 point (Seq_bn_inv) for seen\nlanguages but showed decreases for unseen ones,\nwhile Glot-based adapters reached a 0.5-point im-\nprovement (Seq_bn_inv) for seen languages and 1\npoint for unseen ones. Meanwhile, LLaMA-3 with\nSeq_bn_inv failed to outperform its baseline.",
            "content": "Due to NER benefiting the most from ConceptNet adapters, we also experimented with the combination of ConceptNet and Glot adapters (Seq_bn and Seq_bn_inv) with adapter fusion (Pfeiffer et al., 2021a). This provided the greatest bene3Below, we report the average scores across languages for each configuration. Notably, numerous individual languages show improvements under each configuration. Model mBERT+Seq_bn_inv XLM-R+Seq_bn_inv DeepSeek-R1-D-Llama DeepSeek-R1-D-Qwen DeepSeek-R1-D-Qwen DeepSeek-R1-D-Llama LLaMA-3 LLaMA-3.1 Gemma Gemma-2 Qwen-1.5 Qwen2 GPT-3.5-turbo-0301 GPT-3.5-turbo-0613 GPT-4-0613 LLaMA-2 BLOOM BLOOMz mT0 Occiglot-eu5 XGLM Yayi LLaMAX2 Alpaca Mala-500-v2 #Params (B) TC () NER () 0.177 0.279 8 14 32 70 8 8 7 9 7 7 - - - 7 7 7 13 7 7.5 7 7 10 71.92 80. 20.5 41.88 68.54 70.72 65.8 65.62 60.21 44.27 40.41 56.82 - 45.02 45.82 18.24 13.02 17.51 - 28.56 29.98 16.88 23.13 5.74 85.28 85.42 - - - - - - - - - - 70.65 - - - 31.35 20.92 17.48 - - - - Table 2: Average F1 scores on overlapping LRLs for LLMs and our Glot adapter-based mLMs on TC and NER. Prompting results are 3-shot, based on Ji et al. (2024) for TC and Asai et al. (2023) for NER. For NER, we report averages across eight overlapping languages, while the GPT-3.5 average is based on only two. TC results for GPT-3.5 and GPT-4 are zero-shot, as reported by Adelani et al. (2024a). DeepSeek results are zeroshot and were obtained in our evaluation. Per-language results are in Appendix U. fits for XLM-R, boosting F1 scores by up to 3 points for seen languages and 7 points for unseen ones, outperforming both individual adapters and the baselines. For mBERT, however, fusion did not produce additional improvements."
        },
        {
            "title": "5.2.3 Sentiment Analysis",
            "content": "For mBERT, ConceptNet adapters showed limited average gains, with only LoRA surpassing the baseline for seen languages, with 0.25point improvement. Glot adapters consistently performed better across all architectures, with Seq_bn_inv achieving the highest F1 scores, with 1.5-point improvement for seen and 3-point gain for unseen languages. For XLM-R, ConceptNet adapters exhibited no average improvements, while Glot adapters consistently enhanced performance. Seq_bn and Seq_bn_inv achieved gains of up to 1 point for seen and 5 points for unseen languages. Full fine-tuning yielded similar results with 2-point and 3-point boosts for TC () SA () NER ()"
        },
        {
            "title": "7 General Findings and Discussion",
            "content": "Model mBERT+Seq_bn_inv XLM-R+Seq_bn_inv LLaMA-3 Baseline LLaMA-3+Seq_bn_inv 71.92 80.79 31.93 60.26 73.68 83.35 58.83 68.68 59.32 69. 45.18 45.12 Table 3: Average F1 scores over 5 selected LRLs for language adapter-tuned LLaMa-3-8B, mBERT, and XLM-R. Additionally, we present results for LLaMA3 with single Seq_bn task adapter, similar to our baselines. Per-language results are in Appendix U. mBERT, and 1-point and 8-point improvements respectively, for seen and unseen language groups. Finally, Seq_bn_inv on LLaMA-3 resulted in 10-point average improvement over its baseline."
        },
        {
            "title": "6 Results: Small mLMs vs. SoTA LLMs",
            "content": "Compared to the zero-shot prompting of proprietary LLMs like GPT-3.5-Turbo (Ouyang et al., 2022a) and GPT-4 (OpenAI et al., 2024) on the SIB-200 TC task (Adelani et al., 2024a), our adapter-based models demonstrated superior performance across the 30 LRLs studied, as shown in Table 2. Further, our approach outperformed 3-shot results from LLaMA2-7B (Touvron et al., 2023), BLOOM-7B (Le Scao et al., 2023), instruction-tuned BLOOMZ-7B (Ji et al., 2024), XGLM (Lin et al., 2022), Occiglot-7B-eu5 (Barth et al., 2024), Yayi (Luo et al., 2023), LLaMaX27B-Alpaca (Lu et al., 2024), MaLA-500 (Lin et al., 2024), and recent models like LLaMA3-8B, LLaMA3.1-8B (Grattafiori et al., 2024), Gemma7B, Gemma-2-9B (Team et al., 2024), Qwen-1.57B, and Qwen-2 (Yang et al., 2024). Additionally, our adapter-based approaches surpassed results reported by Asai et al. (2023) on the WikiAnn NER task for subset of 8 overlapping LRLs. Their evaluation included zeroand few-shot prompting with GPT-3.5-Turbo, BLOOM-7B, and instruction-tuned BLOOMZ-7B and mT0-13B (Muennighoff et al., 2023). Distilled DeepSeekR1 models (8B, 14B, 32B, and 70B) (DeepSeekAI et al., 2025) failed to surpass smaller mLMs on TC.4 Finally, Table 3 shows that although Seq_bn_inv language-adapter based LLaMA-38B improved performance over prompting and its single-task adapter baseline, it was still less effective than smaller mLMs like XLM-R for TC tasks. 4Results are zero-shot, with generated token output limited to 100. This section highlights key insights gained from our experiments. We analyze performance trends of adapter-based and full fine-tuning approaches for small mLMs, compare their efficacy to LLMs, explore the relationship between language modeling and downstream task performance, and examine the impact of preand post-training data sizes on downstream task outcomes."
        },
        {
            "title": "7.1 Performance Trends",
            "content": "For MLM, the Seq_bn adapter consistently achieved the best performance, likely due to its moderate parameter count (Table 9 Appendix F) aligning with the limited adaptation data. This partially confirms Mundra et al. (2024)s findings that simple bottleneck adapters outperform other types, including Seq_bn_inv and LoRA. Conversely, LoRA, with even fewer parameters, excelled in languages with larger pre-training data in XLM-R, which may reflect that these languages require fewer parameters given their extensive pretraining coverage, considering the limited adaptation data (see Appendix I). Moreover, Pfeiffer et al. (2021a) noted that high-capacity adapters are less effective for XLM-R compared to mBERT. For downstream tasks, Seq_bn_inv slightly outperformed other adapter configurations, with Seq_bn showing very similar performance in most cases, confirming findings by Pfeiffer et al. (2020) that invertible layers enhance adaptation by facilitating input and output embedding alignment. The advantage of Seq_bn_inv may also be attributable to its larger number of trainable parameters, which may benefit the task fine-tuning process.Yong et al. (2023) also report the superiority of using invertible layers for subset of tested languages on the XNLI task (Conneau et al., 2018). Adapter fusion improved NER performance for XLM-R, likely due to the increased count of trainable parameters (compared to individual language adapters), as observed by Lee et al. (2022a). For mBERT, this improvement was not evident: Individual adapters likely provided sufficient capacity. Adapter-based approaches outperformed full fine-tuning for XLM-R and matched mBERTs performance on MLM, while performing comparably on SA and slightly worse on TC, all with significantly fewer trainable parameters. This indicates that up to 1 GB of adaptation data suffices for effective adapter training5, but might be insufficient for fine-tuning larger models like XLM-R. MLM performance (Tables 10 and 11 Appendices and H) was higher for languages supported by the models vocabulary. For unsupported languages in mBERT, such as Sinhala and Amharic , pseudo-perplexity was artificially low pre-adaptation due to overconfidence in predicting the UNK token. After adaptation, pseudoperplexity scores increased, reflecting consistent predictions of non-language-specific tokens (e.g., punctuation). Languages with partial script overlap, such as Uyghur and Tibetan, showed minimal improvements. XLM-Rs broader script coverage mitigated some issues but still struggled with Tibetan. This highlights the need for vocabulary extension when working with unseen languages (Zhang et al., 2020; Wang et al., 2020; Pfeiffer et al., 2021b)."
        },
        {
            "title": "7.2 Small vs. Large LMs for LRLs",
            "content": "Our findings emphasize the effectiveness of adapting smaller mLMs with adapters over relying on prompting or adapting LLMs for LRLs. The superior performance of smaller mLMs compared to large-scale models has been explored in prior research. Wu et al. (2019) observed that limited capacity forces models to align semantically similar representations across languages rather than creating language-specific subspaces. Dufter and Schütze (2020) further showed that overparameterizing mBERT degrades its cross-lingual transfer ability and hypothesized that smaller models produce better languageindependent representations by reusing parameters across languages, while larger models tend to partition capacity, limiting shared multilingual representations, later supported by Yong et al. (2023). Similarly, Shliazhko et al. (2023) found no performance improvements in mGPT when scaling from 1.3B to 13B parameters for classification and factual probing tasks, with mBERT and XLM-R outperforming larger models. Moreover, Pecher et al. (2024) noted that larger models do not consistently outperform smaller ones in fine-tuning or prompting settings. These findings, together with our results, collectively argue for prioritizing smaller mLMs over large-scale, resource-intensive models (Strubell et al., 2019) to advance performance on 5This is in line with Bapna et al. (2019), He et al. (2021), and Liu et al. (2022), who report that adapter-based tuning often surpasses full fine-tuning. LRLs more efficiently and effectively."
        },
        {
            "title": "7.3 Correlation Between Language Modeling\nand Downstream Task Performance",
            "content": "To investigate the relationship between language modeling and downstream task performance, we performed correlation analyses using Pearson (Cohen et al., 2009) and Spearman (Spearman, 1961) metrics. Results in Table 14 (Appendix K) show moderate correlation between pseudo-perplexity and downstream task performance for XLM-R, both preand post-adaptation (using Glot data), but less pronounced correlation for mBERT. Lower pseudo-perplexity generally indicated better downstream performance for XLM-R and, to lesser extent, for mBERT, suggesting its utility as rough proxy for downstream task capabilities, particularly for larger mLMs. These findings contrast with prior studies (Liang et al., 2022; Yong et al., 2023), which reported an unclear relationship between perplexity and task performance.6 Post-adaptation, the correlation between pseudo-perplexity and downstream performance strengthened, particularly for tasks with consistent data quality (Figure 3). We conjecture that the stronger correlations observed for XLM-R likely arise from its optimized multilingual architecture and its extensive pre-training corpus. 7.4 Impact of Preand Post-Training Data Size on MLM and Downstream Tasks We analyzed the relationship between preand post-adaptation data size and model performance. Before adaptation, pseudo-perplexity and downstream task performance were correlated with pretraining data size (Figure 1 and Table 12 Appendix I), as also found by Wu and Dredze (2020), Ahuja et al. (2023) and Bagheri Nezhad and Agrawal (2024). Post-adaptation improvements primarily depended on pre-training and, surprisinlgy less so, on adaptation data volumes, with the latter providing only marginal improvement.7 LRLs exhibited larger gains, while higher-resource languages faced diminishing returns or even reduced performance. The latter is likely due to the model encountering duplicate data already seen 6Unlike these studies, we evaluate pseudo-perplexity across diverse set of languages rather than models. This partially aligns with Xia et al. (2022), who observed correlation between perplexity and few-shot learning results. 7Similarly, Kunz and Holmström (2024) show limited overall impact of adaptation data and language adapters. Figure 1: Correlation between the pre-training data sizes for mBERT and XLM-R and downstream task results for the pre-adaptation and post-adaptation results. The vertical bars indicate the amounts of adaptation data. The improvements in downstream performance for both models are primarily concentrated in languages with smaller pre-training data sizes, which are positioned on the left side of the plots. Conversely, for languages with substantial representation in the pre-training data, the improvements are less pronounced or nonexistent (Section 7.4). during pre-training (Lee et al., 2022b). Achieving further gains for well-represented languages may require increasing adaptation data and adapter capacity to better leverage their extensive pretraining coverage. correlation analysis (Appendix I) demonstrates that adaptation data had stronger impact on mBERT than XLM-R, likely because of its larger relative contribution as compared to pre-training data. In downstream tasks, even small amounts of adaptation data (e.g., few MB of graphbased data or few hundred MB of free-text data) produced performance gains, consistent with Pfeiffer et al. (2020) and Yong et al. (2023). This was especially true for mBERT, where adaptation data constitutes larger proportion relative to its overall training data. For XLM-R, adaptation data was more beneficial for LRLs, while its impact diminished for languages with pre-training data exceeding approximately 20 GB, as also observed by Adelani et al. (2024a). Diminishing returns suggest threshold effect, where extensive pre-training coverage reduces the utility of adaptation data, indicating that larger adaptation datasets may be necessary for further gains. Figures 4, 5, and 6 demonstrate these trends, showing that underrepresented languages typically benefit more from even limited adaptation data, confirmed by correlation analyses (Appendices N, Q, and T)."
        },
        {
            "title": "The type of adaptation data influenced",
            "content": "task-specific performance. ConceptNet-based adapters outperformed Glot-based adapters for NER in most languages, likely because ConceptNet contains straightforward NER information. This contrasts with the findings of Gurgurov et al. (2024a), who observed different trends when experimenting with smaller subset of languages. Conversely, Glot-based adapters provided more consistent improvements across tasks, leveraging their larger adaptation data volumes (up to 1 GB for most languages). This emphasizes the important role of relative data size in determining the effectiveness of adaptation across tasks."
        },
        {
            "title": "8 Conclusion",
            "content": "This study evaluated adapter-based adaptation of small mLMs to LRLs using structured and unstructured data, alongside continued pre-training and comparing them with SoTA LLMs. Seq_bn achieved the best results for MLM tasks, while Seq_bn_inv excelled in downstream tasks. Full fine-tuning offered limited advantages over adapters. Downstream performance was primarily influenced by pre-training data, with adaptation data providing incremental gains. Graph-based knowledge from ConceptNet, despite its small improved NER performance, while Glot size, data consistently delivered the largest gains across tasks. Our results generally suggest that smaller mLMs may be better suited for LRLs than LLMs, since mLMs efficiently align cross-lingual representations and generalize well under data constraints. In future research, we intend to investigate how larger models might partition their parameter space across languages and whether they are limiting their ability to leverage shared representations, especially for LRLs."
        },
        {
            "title": "Limitations",
            "content": "This study has three main limitations. First, adapters have specific hyperparameters that influence their behavior and capacity. Future work should systematically explore these hyperparameters and their effects on adapter performance. Second, the amount of adaptation data was limited to 1 GB per language due to computational constraints. Investigating the impact of larger datasets on model adaptatione.g., utilizing the full GlotCC data without truncationremains an open and promising direction. Increasing adapter capacity and adaptation data size and measuring adaptation effects as function of both data volume and model capacity could provide valuable insights. Finally, some experiments were not conducted across all tasks due to resource constraints. For example, adapter fusion was applied only to named entity recognition, and full fine-tuning was only evaluated for small models on masked language modeling, topic classification, and sentiment analysis, but not on named entity recognition."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by DisAI Improving scientific excellence and creativity in combating disinformation with artificial intelligence and language technologies, Horizon Europe-funded project under GA No. 101079164, and by the German Ministry of Education and Research (BMBF) as part of the project TRAILS (01IW24005). References David Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and En-Shiun Lee. 2024a. SIB-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 226245, St. Julians, Malta. Association for Computational Linguistics. David Ifeoluwa Adelani, A. Seza Doğruöz, André Coneglian, and Atul Kr. Ojha. 2024b. Comparing LLM prompting with cross-lingual transfer performance on indigenous and low-resource Brazilian languages. In Proceedings of the 4th Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP 2024), pages 3441, Mexico City, Mexico. Association for Computational Linguistics. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed Axmed, et al. 2023. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528. Adam Amram, Anat Ben David, and Reut Tsarfaty. 2018. Representations and architectures in neural sentiment analysis for morphologically rich languages: case study from Modern Hebrew. In Proceedings of the 27th International Conference on Computational Linguistics, pages 22422252, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Alan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Sebastian Ruder, Goran Glavaš, Ivan Vulić, and Anna Korhonen. 2021. MAD-G: Multilingual adapter generation for efficient cross-lingual transfer. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 47624781, Punta Cana, Dominican Republic. Association for Computational Linguistics. Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. 2023. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857. Soran Badawi, Arefeh Kazemi, and Vali Rezaie. 2024. Kurdisent: corpus for kurdish sentiment analysis. Language Resources and Evaluation, pages 120. Sina Bagheri Nezhad and Ameeta Agrawal. 2024. What drives performance in multilingual language models? In Proceedings of the Eleventh Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial 2024), pages 1627, Mexico City, Mexico. Association for Computational Linguistics. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023. Ankur Bapna, Naveen Arivazhagan, and Orhan Firat. 2019. Simple, scalable adaptation for neural machine translation. Fabio Barth, Manuel Brack, Maurice Kraus, Pedro Ortiz Suarez, Malte Ostendorf, Patrick Schramowski, and Georg Rehm. 2024. Occiglot euro llm leaderboard. Steven Bird. 2022. Local languages, third spaces, and other high-resource scenarios. In 60th Annual Meeting of the Association for Computational Linguistics, ACL 2022, pages 78177829. Association for Computational Linguistics (ACL). Jože Bučar, Martin Žnidaršič, and Janez Povh. 2018. Annotated news corpora and lexicon for sentiment analysis in slovene. Language Resources and Evaluation, 52(3):895919. Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea Adhista, Emmanuel Dave, Sarah Oktavianti, Salsabil Akbar, Jhonson Lee, Nuur Shadieq, Tjeng Wawan Cenggoro, Hanung Linuwih, Bryan Wilie, Galih Muridan, Genta Winata, David Moeljadi, Alham Fikri Aji, Ayu Purwarianti, and Pascale Fung. 2023. NusaWrites: Constructing high-quality corpora for underrepresented and extremely lowresource languages. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 921945, Nusa Dua, Bali. Association for Computational Linguistics. Ethan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020. Parsing with multilingual bert, small corpus, and small treebank. In Findings of the Association for Computational Linguistics: EMNLP 2020, page 13241334. Association for Computational Linguistics. Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient. Noise reduction in speech processing, pages 14. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crosslingual sentence representations. arXiv preprint arXiv:1809.05053. Keith Cortis and Brian Davis. 2019. social opinion gold standard for the Malta government budget 2018. In Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019), pages 364369, Hong Kong, China. Association for Computational Linguistics. Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language underIn Proceedings of the 2019 Conference standing. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Alexiei Dingli and Nicole Sant. 2016. Sentiment analysis on maltese using machine learning. In Proceedings of The Tenth International Conference on Advances in Semantic Processing (SEMAPRO 2016), pages 2125. Philipp Dufter and Hinrich Schütze. 2020. Identifying elements essential for berts multilinguality. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 44234437. Abteen Ebrahimi and Katharina Kann. 2021. How to adapt your pretrained multilingual model to 1600 languages. Luis Espinosa-Anke, Geraint Palmer, Padraig Corcoran, Maxim Filimonov, Irena Spasić, and Dawn Knight. 2021. Englishwelsh cross-lingual embeddings. Applied Sciences, 11(14):6541. Fahim Faisal and Antonios Anastasopoulos. 2022. Phylogeny-inspired adaptation of multilingual models to new languages. Manaal Faruqui, Jesse Dodge, Sujay Jauhar, Chris Dyer, Eduard Hovy, and Noah Smith. 2014. Retrofitting word vectors to semantic lexicons. arXiv preprint arXiv:1411.4166. Robert French. 1999. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128135. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd of models. Daniil Gurgurov, Mareike Hartmann, and Simon Ostermann. 2024a. Adapting Multilingual LLMs to Low-Resource Languages with Knowledge Graphs via Adapters. arXiv preprint arXiv:2407.01406. Daniil Gurgurov, Rishu Kumar, and Simon Ostermann. 2024b. Gremlin: repository of green baseline embeddings for 87 low-resource languages injected with multilingual graph knowledge. Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah Smith. 2020. Dont stop pretraining: Adapt language models to domains and tasks. arXiv preprint arXiv:2004.10964. Xiaochuang Han and Jacob Eisenstein. 2019. Unsupervised domain adaptation of contextualized embeddings for sequence labeling. Md. Arid Hasan, Prerona Tarannum, Krishno Dey, Imran Razzak, and Usman Naseem. 2024. Do large language models speak all languages equally? comparative study in low-resource settings. Ruidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng Ding, Liying Cheng, Jia-Wei Low, Lidong Bing, and Luo Si. 2021. On the effectiveness of adapter-based tuning for pretrained language model adaptation. Yifan Hou, Wenxiang Jiao, Meizhen Liu, Carl Allen, Zhaopeng Tu, and Mrinmaya Sachan. 2022. Adapters for enhanced modeling of multilingual knowledge and text. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 39023917. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pages 27902799. PMLR. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: massively multilingual multitask benchmark for evaluating cross-lingual generalisation. In International Conference on Machine Learning, pages 44114421. PMLR. Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu Wei. 2023. Not all languages are created equal in llms: Improving multilingual capability by cross-lingualthought prompting. Ayyoob Imani, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André Martins, François Yvon, and Hinrich Schütze. 2023. Glot500: Scaling multilingual corpora and language models to 500 languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1082 1117, Toronto, Canada. Association for Computational Linguistics. Tim Isbister, Fredrik Carlsson, and Magnus Sahlgren. 2021. Should we stop training more monolingual models, and simply use machine translation instead? In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 385 390, Reykjavik, Iceland (Online). Linköping University Electronic Press, Sweden. Shaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, Dayyán OBrien, Hengyu Luo, Hinrich Schütze, Jörg Tiedemann, and Barry Haddow. 2024. Emma-500: Enhancing massively multilingual adaptation of large language models. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online. Association for Computational Linguistics. Dame Jovanoski, Veno Pachovski, and Preslav Nakov. 2015. Sentiment analysis in Twitter for Macedonian. In Proceedings of the International Conference Recent Advances in Natural Language Processing, pages 249257, Hissar, Bulgaria. INCOMA Ltd. Shoumen, BULGARIA. Georgios Kalamatianos, Dimitrios Mallis, Symeon Symeonidis, and Avi Arampatzis. 2015. Sentiment analysis of greek tweets and hashtags using sentiment lexicon. In Proceedings of the 19th panhellenic conference on informatics, pages 6368. Amir Hossein Kargaran, François Yvon, and Hinrich Schütze. 2024. GlotCC: An open broad-coverage commoncrawl corpus and pipeline for minority languages. arXiv preprint. Muhammad Yaseen Khan, Shah Muhammad Emaduddin, and Khurum Nazir Junejo. 2017. Harnessing english sentiment lexicons for polarity detection in urdu tweets: baseline approach. In 2017 IEEE 11th International Conference on Semantic Computing (ICSC), pages 242249. IEEE. Muhammad Yaseen Khan and Muhammad Suffian Urdu sentiment corpus (v1.0): Nizami. 2020. Linguistic exploration and visualization of labeled datasetfor urdu sentiment analysis. In 2020 IEEE 2nd International Conference On Information Science & Communication Technology (ICISCT). IEEE. Jenny Kunz and Oskar Holmström. 2024. The impact of language adapters in cross-lingual transfer for nlu. Elmurod Kuriyozov, Sanatbek Matlatipov, Miguel A. Alonso, and Carlos Gómez-Rodríguez. 2019. Construction and evaluation of sentiment datasets for low-resource languages: The case of uzbek. In Human Language Technology. Challenges for Computer Science and Linguistics - 9th Language and Technology Conference, LTC 2019, Poznan, Poland, May 17-19, 2019, Revised Selected Papers, volume 13212 of Lecture Notes in Computer Science, pages 232243. Springer. Anne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glavaš. 2020a. Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. In Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 43 49, Online. Association for Computational Linguistics. Anne Lauscher, Ivan Vulić, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavaš. 2020b. Specializing unsupervised pretraining models for word-level semantic similarity. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: 176bparameter open-access multilingual language model. CoRR. Jaeseong Lee, Seung-won Hwang, and Taesup Kim. 2022a. FAD-X: Fusing adapters for cross-lingual transfer to low-resource languages. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 5764, Online only. Association for Computational Linguistics. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022b. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 84248445, Dublin, Ireland. Association for Computational Linguistics. Siyu Li, Kui Zhao, Jin Yang, Xinyun Jiang, Zhengji Li, and Zicheng Ma. 2022. Senti-exlm: Uyghur enhanced sentiment analysis model based on xlm. Electronics Letters, 58(13):517519. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110. Peiqin Lin, Shaoxiong Ji, Jörg Tiedemann, André F. T. Martins, and Hinrich Schütze. 2024. Mala-500: Massive language adaptation of large language models. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual language models. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022. Few-shot parameter-efficient finetuning is better and cheaper than in-context learning. LocalDoc. 2024. Sentiment analysis datset for azerbaijani. Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, and Fei Yuan. 2024. Llamax: Scaling linguistic horizons of llm by enhancing translation capabilities beyond 100 languages. Yin Luo, Qingchao Kong, Nan Xu, Jia Cao, Bao Hao, Baoyu Qu, Bo Chen, Chao Zhu, Chenyang Zhao, Donglei Zhang, Fan Feng, Feifei Zhao, Hailong Sun, Hanxuan Yang, Haojun Pan, Hongyu Liu, Jianbin Guo, Jiangtao Du, Jingyi Wang, Junfeng Li, Lei Sun, Liduo Liu, Lifeng Dong, Lili Liu, Lin Wang, Liwen Zhang, Minzheng Wang, Pin Wang, Ping Yu, Qingxiao Li, Rui Yan, Rui Zou, Ruiqun Li, Taiwen Huang, Xiaodong Wang, Xiaofei Wu, Xin Peng, Xina Zhang, Xing Fang, Xinglin Xiao, Yanni Hao, Yao Dong, Yigang Wang, Ying Liu, Yongyu Jiang, Yungan Wang, Yuqi Wang, Zhangsheng Wang, Zhaoxin Yu, Zhen Luo, Wenji Mao, Lei Wang, and Dajun Zeng. 2023. Yayi 2: Multilingual open-source large language models. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142150, Portland, Oregon, USA. Association for Computational Linguistics. Mounika Marreddy, Subba Reddy Oota, Lakshmi Sireesha Vakada, Venkata Charan Chinni, and Radhika Mamidi. 2022a. Am resource-poor language? data sets, embeddings, models and analysis for four different nlp tasks in telugu language. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(1):134. Mounika Marreddy, Subba Reddy Oota, Lakshmi Sireesha Vakada, Venkata Charan Chinni, and Radhika Mamidi. 2022b. Multi-task text classification using graph convolutional networks for large-scale low resource language. arXiv preprint arXiv:2205.01204. Antonio Martínez-García, Toni Badia, and Jeremy Barnes. 2021. Evaluating morphological typology in zero-shot cross-lingual transfer. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 31363153. George Miller. 1995. Wordnet: lexical database for english. Communications of the ACM, 38(11):39 41. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization through multitask finetuning. Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Abinew Ali Ayele, Nedjma Ousidhoum, David Ifeoluwa Adelani, Seid Muhie Yimam, Ibrahim Said Ahmad, Meriem Beloucif, Saif Mohammad, Sebastian Ruder, Oumaima Hourrane, Pavel Brazdil, Felermino Dario Mario Antonio Ali, Davis Davis, Salomey Osei, Bello Shehu Bello, Falalu Ibrahim, Tajuddeen Gwadabe, Samuel Rutunda, Tadesse Belay, Wendimu Baye Messelle, Hailu Beshada Balcha, Sisay Adugna Chala, Hagos Tesfahun Gebremichael, Bernard Opoku, and Steven Arthur. 2023a. Afrisenti: twitter sentiment analysis benchmark for african languages. Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Seid Muhie Yimam, David Ifeoluwa Adelani, Ibrahim Said Ahmad, Nedjma Ousidhoum, Abinew Ayele, Saif Mohammad, and Meriem Beloucif. 2023b. Semeval-2023 task 12: Sentiment analysis for african languages (afrisenti-semeval). arXiv preprint arXiv:2304.06845. Benjamin Muller, Antonios Anastasopoulos, Benoît Sagot, and Djamé Seddah. 2021. When being unseen from mBERT is just the beginning: Handling new languages with multilingual language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 448462, Online. Association for Computational Linguistics. Nandini Mundra, Sumanth Doddapaneni, Raj Dabre, and Anoop Kunchukuttan, Ratish Puduppully, Mitesh Khapra. 2024. comprehensive analysis of adapter efficiency. In Proceedings of the 7th Joint International Conference on Data Science & Management of Data (11th ACM IKDD CODS and 29th COMAD), pages 136154. Hiroki Nakayama. 2018. seqeval: python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval. Roberto Navigli and Simone Paolo Ponzetto. 2012. Babelnet: The automatic construction, evaluation and application of wide-coverage multilingual semantic network. Artificial intelligence, 193:217250. Graham Neubig and Junjie Hu. 2018. Rapid adaptation of neural machine translation to new languages. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha GontijoLopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt4 technical report. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022a. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 27730 27744. Curran Associates, Inc. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022b. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. 2017. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 19461958, Vancouver, Canada. Association for Computational Linguistics. Marinela Parović, Goran Glavaš, Ivan Vulić, and Anna Korhonen. 2022. BAD-X: Bilingual adapters improve zero-shot cross-lingual transfer. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 17911799, Seattle, United States. Association for Computational Linguistics. Marinela Parović, Alan Ansell, Ivan Vulić, and Anna Korhonen. 2023. Cross-lingual transfer with target language-ready task adapters. Samuel Pecar, Marian Simko, and Maria Bielikova. Improving sentiment classification in Slo2019. vak language. In Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing, pages 114119, Florence, Italy. Association for Computational Linguistics. Branislav Pecher, Ivan Srba, and Maria Bielikova. 2024. Comparing specialised small and general large language models on text classification: 100 labelled samples to achieve break-even performance. Matthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word In Proceedings of the 2019 Conrepresentations. ference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 4354, Hong Kong, China. Association for Computational Linguistics. Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022. Lifting the curse of multilinguality by pre-training modular transformers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 34793495, Seattle, United States. Association for Computational Linguistics. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021a. AdapterFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 487503, Online. Association for Computational Linguistics. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2020. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 76547673, Online. Association for Computational Linguistics. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder. 2021b. UNKs everywhere: Adapting multilingual language models to new scripts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10186 10203, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Aabha Pingle, Aditya Vyawahare, Isha Joshi, Rahul Tangsali, and Raviraj Joshi. 2023. L3cubemahasent-md: multi-domain marathi sentiment analysis dataset and transformer models. arXiv preprint arXiv:2306.13888. Ayu Purwarianti and Ida Ayu Putu Ari Crisdayanti. 2019. Improving bi-lstm performance for indonesian sentiment analysis using paragraph vector. In 2019 International Conference of Advanced Informatics: Concepts, Theory and Applications (ICAICTA), pages 15. IEEE. Alec Radford. 2018. Improving language understanding by generative pre-training. Surangika Ranathunga and Isuru Udara Liyanage. 2021. Sentiment analysis of sinhala news comments. Transactions on Asian and Low-Resource Language Information Processing, 20(4):123. Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, and Graham Neubig. 2023. Chatgpt mt: Competitive for high- (but not low-) resource languages. Julian Salazar, Davis Liang, Toan Nguyen, and Katrin Kirchhoff. 2019. Masked language model scoring. arXiv preprint arXiv:1910.14659. Salim Sazzed. 2020. Cross-lingual sentiment classification in low-resource Bengali language. In Proceedings of the Sixth Workshop on Noisy Usergenerated Text (W-NUT 2020), pages 5060, Online. Association for Computational Linguistics. Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2023. mgpt: Few-shot learners go multilingual. Oyesh Mann Singh, Sandesh Timilsina, Bal Krishna Bal, and Anupam Joshi. 2020. Aspect based abusive sentiment detection in nepali social media texts. In 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), pages 301308. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990. Marina Sokolova, Nathalie Japkowicz, and Stan Szpakowicz. 2006. Beyond accuracy, f-score and roc: family of discriminant measures for performance evaluation. In Australasian joint conference on artificial intelligence, pages 10151021. Springer. Charles Spearman. 1961. The proof and measurement of association between two things. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Proceedings of the AAAI conference on artificial intelligence, volume 31. Uga Spro gis and Matıss Rikters. 2020. What Can We Learn From Almost Decade of Food Tweets. In In Proceedings of the 9th Conference Human Language Technologies - The Baltic Perspective (Baltic HLT 2020), Kaunas, Lithuania. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615. Nicolas Stefanovitch, Jakub Piskorski, and Sopho Kharazi. 2022. Resources and experiments on sentiment classification for Georgian. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 16131621, Marseille, France. European Language Resources Association. Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 36453650, Florence, Italy. Association for Computational Linguistics. Arthit Suriyawongkul, Ekapol Chuangsuwanich, Pattarawat Chormai, and Charin Polpanumas. 2019. Pythainlp/wisesight-sentiment: First release. Anca Tache, Gaman Mihaela, and Radu Tudor Ionescu. 2021. Clustering word embeddings with selforganizing maps. application on LaRoSeDa - large Romanian sentiment data set. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 949956, Online. Association for Computational Linguistics. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem, Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison, Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge, Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen, Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins, Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini, Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou, Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort, Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia, Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso, Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal, Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman, Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai, Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Cogan, Sarah Perrin, Sébastien M. R. Arnold, Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley, Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong, Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran, Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. 2024. Gemma 2: Improving open language models at practical size. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. Tarikwa Tesfa, Befikadu Belete, Samuel Abera, Sudhir Kumar Mohapatra, and Tapan Kumar Das. 2024. Aspect-based sentiment analysis on amharic text for evaluating ethio-telecom services. In 2024 Second International Conference on Emerging Trends in Information Technology and Engineering (ICETITE), pages 16. IEEE. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv eprints, page arXiv:2307.09288. Adam Tsakalidis, Symeon Papadopoulos, Rania Voskaki, Kyriaki Ioannidou, Christina Boididou, Alexandra Cristea, Maria Liakata, and Yiannis Kompatsiaris. 2018. Building and evaluating resources for sentiment analysis in the greek language. Language resources and evaluation, 52:10211044. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang, and Ming Zhou. 2021. K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 14051418, Online. Association for Computational Linguistics. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 40034012, Marseille, France. European Language Resources Association. Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Rahmad Mahendra, Fajri Koto, Ade Romadhony, Kemal Kurniawan, David Moeljadi, Radityo Eko Prasojo, Pascale Fung, Timothy Baldwin, Jey Han Lau, Rico Sennrich, and Sebastian Ruder. 2022. Nusax: Multilingual parallel sentiment dataset for 10 indonesian local languages. Wilson Wongso, David Samuel Setiawan, and Derwin Suhartono. 2021. Causal and masked language modeling of javanese language using transformer-based architectures. In 2021 International Conference on Advanced Computer Science and Information Systems (ICACSIS), pages 17. IEEE. Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Emerging cross-lingual structure in pretrained language models. arXiv preprint arXiv:1911.01464. Shijie Wu and Mark Dredze. 2020. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120130, Online. Association for Computational Linguistics. Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. 2022. Training trajectories of language models across scales. arXiv preprint arXiv:2212.09803. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. Zihan Wang, Karthikeyan K, Stephen Mayhew, and Dan Roth. 2020. Extending multilingual BERT to low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 26492656, Online. Association for Computational Linguistics. Zheng-Xin Yong, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Indra Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vassilina Nikoulina. 2023. Bloom+1: Adding language support to bloom for zero-shot prompting. Rong Zhang, Revanth Gangi Reddy, Md Arafat Sultan, Vittorio Castelli, Anthony Ferritto, Radu Florian, Efsun Sarioglu Kayi, Salim Roukos, Avi Sil, and Todd Ward. 2020. Multi-stage pre-training for low-resource domain adaptation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5461 5468, Online. Association for Computational Linguistics. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced language representation with informative entities. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 14411451, Florence, Italy. Association for Computational Linguistics. Yulei Zhu, Baima Luosai, Liyuan Zhou, Nuo Qun, and Tashi Nyima. 2023. Research on sentiment analysis of tibetan short text based on dual-channel hyIn 2023 IEEE 4th Internabrid neural network. tional Conference on Pattern Recognition and Machine Learning (PRML), pages 377384. Ahmet Üstün, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. 2020. Udapter: Language adaptation for truly universal dependency parsing."
        },
        {
            "title": "A ConceptNet Tripple Conversion Mapping",
            "content": "ConceptNet Relationship Natural Language Predicate is the opposite of is derived from Antonym DerivedFrom EtymologicallyDerivedFrom is etymologically derived from EtymologicallyRelatedTo FormOf PartOf HasA UsedFor AtLocation Causes CausesDesire MadeOf ReceivesAction HasSubevent HasFirstSubevent HasLastSubevent HasPrerequisite HasProperty MotivatedByGoal ObstructedBy Desires CreatedBy CapableOf HasContext IsA RelatedTo SimilarTo Synonym SymbolOf DefinedAs DistinctFrom MannerOf LocatedNear is etymologically related to is form of is part of belongs to is used for is typical location for causes makes someone want is made of receives action of is subevent of is an event that begins with subevent is an event that concludes with subevent has prerequisite of can be described as is step toward accomplishing the goal is an obstacle in the way of is conscious entity that typically wants is process or agent that creates is capable of is word used in the context of is type of is related to is similar to is synonym of symbolically represents is more explanatory version of is distinct from is specific way to do is typically found near Table 4: ConceptNet relationships and their natural language predicates. This mapping is used for converting the ConceptNet KG data into natural language text."
        },
        {
            "title": "B Language Details",
            "content": "Language ISO Thai Romanian Bulgarian Danish Greek Hebrew Slovak Slovenian Latvian Indonesian Georgian Bengali Azerbaijani Urdu Macedonian Telugu Nepali Marathi Swahili Welsh Uzbek Javanese Sundanese Sinhala Amharic Kurdish Uyghur Maltese Tibetan Yoruba th ro bg da el he sk sl lv ms ka bn az ur mk te ne mr sw cy uz jv su si am ku ug mt bo yo Language Family Kra-Dai Indo-European Indo-European Indo-European Indo-European Afro-Asiatic Indo-European Indo-European Indo-European Austronesian Kartvelian Indo-European Turkic Indo-European Indo-European Dravidian Indo-European Indo-European Niger-Congo Indo-European Turkic Austronesian Austronesian Indo-European Afro-Asiatic Indo-European Turkic Afro-Asiatic Sino-Tibetan Niger-Congo CN (Sent-s) CN (MB) Glot (Doc-s) Glot (MB) mBERT? XLM-R? mBERT Data Size (GB) XLM-R Data Size (GB) 123,859 70,236 162,181 66,109 89,016 41,444 22,460 85,882 66,408 175,246 35,331 8,782 15,149 13,315 38,116 33,476 4,456 7,232 12,380 18,313 4,362 3,448 1,880 1,782 1,814 12,246 1,715 3,895 4,768 1,044 6.95 2.47 8.02 2.27 4.17 1.62 0.81 2.98 2.4 6.21 1.89 0.46 0.57 0.51 1.54 1.72 0.21 0.37 0.39 0.61 0.16 0.13 0.07 0.1 0.07 0.44 0.06 0.14 0.21 0.05 2,391,253 8,657,002 5,192,702 8,743,767 4,789,519 5,287,428 9,294,165 9,301,902 8,301,651 8,024,827 3,463,631 2,940,197 6,179,152 4,220,566 5,037,552 3,162,535 2,569,572 402,575 2,450,753 3,174,686 4,018,172 367,795 323,610 1,655,641 667,881 376,260 976,010 1,389,527 288,847 278, 977.68 1002.36 1014.73 1006.91 980.94 991.82 1006.96 1007.91 988.21 1022.01 1014.24 993.44 1016.68 1009.42 1005.62 1005.55 1012.63 157.3 323.27 360.24 481.49 43.56 43.55 586.21 203.65 134.7 233.61 182.17 165.31 34.51 1.29 1.22 1.50 0.81 1.85 2.73 0.61 0.67 0.33 0.59 0.88 1.22 0.62 0.54 0.86 0.88 0.14 0.32 0.10 0.39 0.57 0.10 0.06 0. 85.24 83.29 70.37 62.39 57.30 40.87 31.96 14.16 11.94 11.73 10.55 10.10 8.33 6.97 5.76 5.46 4.32 3.33 2.15 1.07 0.95 0.20 0.08 4.27 1.00 0.52 0.43 Table 5: Number of ConceptNet triples and GlotCC documents as well as corresponding data sizes per language, sorted by Glot (Doc-s) in descending order. The last four columns indicate the inclusion of the respective language in mBERT and XLM-R pre-training data, alongside the corresponding data sizes in GB. The sizes are approximated based on the openly available CC100 and WikiPedia datasets."
        },
        {
            "title": "C Sentiment Analysis Data Details",
            "content": "Language ISO code Source Sundanese Amharic Swahili Georgian Nepali Uyghur Latvian Slovak Sinhala Slovenian Uzbek Bulgarian Yoruba Urdu Macedonian Danish Marathi Bengali Hebrew Romanian Telugu Welsh Azerbaijani Tibetan Kurdish Greek Javanese Maltese Thai Malay su am sw ka ne ug lv sk si sl uz bg yo ur mk da mr bn he ro te cy az bo ku el jv mt th ms Winata et al., 2022 Tesfa et al., 2024 Muhammad et al., 2023a; Muhammad et al., 2023b Stefanovitch et al., 2022 Singh et al., 2020 Li et al., 2022 Spro gis and Rikters, 2020 Pecar et al., 2019 Ranathunga and Liyanage, 2021 Bučar et al., 2018 Kuriyozov et al., 2019 Martínez-García et al., 2021 Muhammad et al., 2023a; Muhammad et al., 2023b Maas et al., 2011; Khan et al., 2017; Khan and Nizami, 2020 Jovanoski et al., 2015 Isbister et al., 2021 Pingle et al., 2023 Sazzed, 2020 Amram et al., 2018 Tache et al., 2021 Marreddy et al., 2022a; Marreddy et al., 2022b Espinosa-Anke et al., 2021 LocalDoc, 2024 Zhu et al., 2023 Badawi et al., 2024 Kalamatianos et al., 2015; Tsakalidis et al., 2018 Wongso et al., 2021 Dingli and Sant, 2016; Cortis and Davis, 2019 Suriyawongkul et al., 2019; Purwarianti and Crisdayanti, Table 6: Sentiment analysis data details. #pos 378 487 908 765 680 2450 1796 4393 2487 1665 3042 6652 6344 5562 3041 5000 5000 8500 8497 7500 9488 12500 14000 5006 4065 5773 12500 271 4778 7319 #neg #train 383 526 319 765 1019 353 1380 731 2516 3337 1634 1271 3296 5417 5184 5000 5000 3307 3911 7500 6746 12500 14000 5000 3922 1313 12500 580 6822 381 709 738 1080 1189 1962 2408 3560 3502 3501 3273 5412 5414 7356 6557 7000 8000 8264 8932 10800 11386 17500 19600 7004 6000 5936 17500 595 8103 7926 #val 76 152 185 120 255 311 268 522 750 750 701 838 1327 1812 729 1500 1000 1771 993 1200 1634 3750 4200 1501 993 383 5025 85 1153 1132 #test 304 152 304 330 255 530 500 1042 751 751 702 1673 2899 1812 939 1500 1000 1772 2483 3000 3214 3750 4200 1501 994 767 2475 171"
        },
        {
            "title": "D Named Entity Recognition Data Details",
            "content": "Language ISO code #train Bulgarian Indonesian Maltese Nepali Javanese Uyghur Tibetan Sinhala Sundanese Amharic Swahili Georgian Latvian Slovak Slovenian Uzbek Yoruba Urdu Macedonian Danish Marathi Bengali Hebrew Romanian Telugu Welsh Azerbaijani Greek Kurdish Thai bg ms mt ne jv ug bo si su am sw ka lv sk sl uz yo ur mk da mr bn he ro te cy az el ku th 20000 20000 100 100 100 100 100 100 100 100 1000 10000 10000 20000 15000 1000 100 20000 10000 20000 5000 10000 20000 20000 1000 10000 10000 20000 100 #val 10000 1000 100 100 100 100 100 100 100 100 1000 10000 10000 10000 10000 1000 100 1000 1000 10000 1000 1000 10000 10000 1000 1000 1000 10000 100 10000 #test 10000 1000 100 100 100 100 100 100 100 100 1000 10000 10000 10000 10000 1000 100 1000 1000 10000 1000 1000 10000 10000 1000 1000 1000 10000 100 10000 Table 7: Named entity recognition data details."
        },
        {
            "title": "E Language Adapters Evaluation Losses",
            "content": "ISO th ro bg da el he sk sl lv ms ka bn az ur mk te ne mr sw cy uz jv su si am ku ug mt bo yo ConceptNet Glot mBERT XLM-R mBERT XLM-R Seq_bn LoRA Seq_bn_inv Seq_bn LoRA Seq_bn_inv Seq_bn LoRA Seq_bn_inv Seq_bn LoRA Seq_bn_inv 1.21 1.41 0.68 1.24 1.13 1.35 1.22 0.83 1.32 1.57 1.15 0.99 1.33 1.43 1.42 1.09 1.26 1.08 1.54 1.55 1.22 1.44 1.51 1.4 1.47 1.64 1.09 1.41 1.0 1.12 1.24 1.46 0.71 1.29 1.18 1.38 1.28 0.91 1.4 1.63 1.19 1.03 1.37 1.48 1.44 1.12 1.31 1.12 1.63 1.6 1.3 1.5 1.56 1.33 1.51 1.73 1.13 1.44 1.01 1.27 1.2 1.34 0.66 1.19 1.12 1.32 1.16 0.79 1.25 1.5 1.14 0.97 1.29 1.4 1.38 1.07 1.21 1.04 1.51 1.48 1.18 1.4 1.47 1.38 1.58 1.61 1.07 1.39 0.98 1.1 1.42 1.43 0.87 1.35 1.36 1.47 1.39 1.05 1.47 1.59 1.38 1.37 1.5 1.62 1.59 1.29 1.53 1.46 1.64 1.83 1.55 1.55 1.38 1.31 1.22 1.91 1.57 1.53 0.63 1. 1.42 1.43 0.87 1.36 1.36 1.46 1.39 1.09 1.51 1.57 1.35 1.37 1.55 1.61 1.54 1.29 1.52 1.45 1.73 1.91 1.62 1.56 1.4 1.25 1.29 2.04 1.59 1.68 0.64 1.79 1.35 1.33 0.81 1.26 1.29 1.4 1.28 0.98 1.37 1.47 1.3 1.3 1.42 1.51 1.45 1.22 1.42 1.37 1.56 1.76 1.45 1.48 1.38 1.25 1.13 1.86 1.47 1.5 0.62 1.76 0.46 1.37 1.09 1.39 0.67 1.3 1.09 1.16 1.11 1.52 0.79 1.05 0.89 1.15 0.89 0.83 0.77 0.94 0.94 0.81 0.85 2.11 1.14 0.82 1.25 0.93 0.46 0.84 0.24 0.87 0.54 1.52 1.25 1.54 0.77 1.41 1.19 1.28 1.29 1.65 0.91 1.16 1.02 1.31 0.99 0.94 0.9 1.07 1.13 0.99 1.01 2.21 1.28 0.88 1.31 1.05 0.57 1.08 0.28 1.04 0.45 1.34 1.07 1.36 0.66 1.28 1.06 1.13 1.07 1.48 0.77 1.03 0.86 1.12 0.87 0.81 0.75 0.92 0.9 0.77 0.82 2.08 1.11 0.8 1.23 0.9 0.44 0.8 0.24 0.84 1.55 1.27 1.83 1.28 0.84 1.29 1.16 1.22 1.28 1.55 1.12 1.44 1.19 1.38 1.41 1.33 1.38 1.43 1.13 0.95 1.06 2.63 1.21 1.21 1.2 0.76 0.79 0.93 0.72 0. 1.65 1.3 1.8 1.36 0.9 1.38 1.19 1.28 1.37 1.6 1.18 1.49 1.31 1.44 1.4 1.4 1.45 1.49 1.22 1.06 1.17 2.66 1.35 1.29 1.31 1.02 0.94 1.2 0.73 1.03 1.53 1.26 1.8 1.26 0.83 1.28 1.14 1.21 1.25 1.54 1.11 1.42 1.15 1.36 1.41 1.31 1.35 1.41 1.1 0.92 1.03 2.54 1.18 1.19 1.19 0.71 0.76 0.87 0.71 0.78 Table 8: Evaluation losses for language adapters by model, architecture, and language. Evaluation loss values were not predictive of MLM performance. Despite Seq_bn_inv achieving the lowest evaluation losses, it underperformed in MLM tasks, indicating that evaluation loss may be an unreliable training metric (suggested by Salazar et al. (2019))."
        },
        {
            "title": "F Language Adapter Hyperparameters",
            "content": "Adapter Type mBERT XLM-R LLaMA-3 Seq_bn Seq_bn_inv LoRA Seq_bn Seq_bn_inv LoRA Seq_bn Trainable Params (No.) Trainable Params (%) 894,528 0.505% 1,190,592 0.672% 894,528 294,912 0.166% 0.322% 1,190,592 0.429% 294,912 0.106% 67,248,128 0.896% Hyperparameters for LA Hyperparameters for TA Batch Size: 16, Learning Rate: 1e-4, Seq_bn and Seq_bn_inv: Reduction Factor = 16, LoRA: 𝛼 = 8, 𝑟 = 8 Batch Size: 32, Learning Rate: 1e-4, Seq_bn: Reduction Factor = 16, LoRA: 𝛼 = 8, 𝑟 = 8 Seq_bn_inv 75,642,880 1.008% Batch Size: 1, Learning Rate: 1eBatch Size for TC: 16; for SA and NER: 8, Learning Rate: 2e-5 Table 9: Trainable parameters and hyperparameters for different adapter types in mBERT, XLM-R, and LLaMA-3. The rest of hyperparameters are as specified in the default adapter configurations in Adapterhub. LA - Langauge adapter, TA - Task adapter. Masked Language Modeling Pseudo-Perplexity - Part ISO he el bg th ro bn te ka mk da sl az sk ms uz ur cy lv mr ne jv sw su yo Avg. mt ku ug si am bo Avg. Total ConceptNet Seq_bn 19.71 6.17 14.99 4.13 13.47 14.94 8.9 6.3 14.5 19.29 18.09 15.2 13.86 53.66 31.41 23.02 22.13 18.31 12.9 14.19 115.27 57.57 177.27 293.99 41.22 432.99 119.29 96.52 96.5 31.41 58.78 139.25 LoRA 18.29 5.55 12.65 4.29 12.67 13.53 8.34 6.0 12.3 25.39 14.32 13.48 12.37 50.17 23.43 21.74 23.0 16.21 12.21 13.08 132.08 52.21 183.04 210.43 37.37 456.64 101.13 86.31 40.3 26.93 47.33 126. 60.83 55.18 Seq_bn_inv 19.85 6.92 20.93 4.07 22.39 15.99 9.33 6.54 13.26 30.87 26.86 24.26 19.29 128.6 40.35 26.4 39.75 33.14 14.0 15.36 146.64 79.5 227.87 370.71 55.95 457.43 149.74 121.15 103.36 23.47 89.81 157.49 76.26 Base 18.36 4.69 10.84 3.87 11.49 11.97 7.92 6.52 11.95 19.16 13.57 12.47 11.5 36.26 26.65 22.59 21.24 14.14 12.51 12.72 83.84 42.53 102.16 85.21 25.17 531.59 72.87 112.63 16.29 10.06 4.59 124.67 45.07 mBERT Seq_bn 11.09 3.3 5.4 2.94 5.94 9.11 6.09 3.63 5.83 11.13 6.68 7.04 5.98 18.23 5.84 10.18 6.08 5.98 5.84 6.71 19.4 8.99 20.24 23.14 8. LoRA 12.31 3.54 5.9 3.34 6.59 10.05 6.13 4.06 6.33 11.8 7.26 7.89 6.64 20.01 7.21 12.0 7.8 7.13 6.78 7.21 22.86 12.48 23.2 31.96 10.44 6.89 1524.98 28.69 15640.68 56052.75 57.94 12218.65 9.87 559.83 67.26 8981.09 34924.59 65.03 7434.61 Glot Seq_bn_inv 12.51 3.49 6.09 3.18 8.67 10.32 6.4 3.91 6.54 13.02 8.58 7.9 7.14 22.71 9.22 12.89 9.06 7.48 6.85 8.68 31.6 16.09 34.29 86.79 14.31 15.02 173.24 75.53 157397.73 4223.4 1136.47 27170. FFT 8.78 2.71 4.67 2.54 6.75 8.42 5.32 2.6 5.53 8.76 4.91 5.83 6.03 16.95 3.84 7.16 4.89 4.58 6.71 4.88 19.19 7.19 34.93 38.89 9.25 5.95 6381.75 313.64 443921.11 38289.93 41.99 81492.4 2450.89 1495.27 5445. 16305.88 Table 10: Pseudo-perplexity scores comparison across different adapters for mBERT in ConceptNet and Glot. Language not included in mBERT pre-training. FFT denotes full fine-tuning of base model on the target-language Glot data. The underlined FFT scores indicate that FFT outperform the best performing adapter for respective language. Masked Language Modeling Pseudo-Perplexity - Part II XLM-R ConceptNet Base Seq_bn LoRA Seq_bn_inv Seq_bn ISO th ro bg da el he sk sl lv ms ka bn az ur mk te ne si mr sw cy am uz ku ug jv su Avg. mt bo yo Avg. 7.83 2.97 3.61 4.29 2.56 5.74 3.93 4.79 4.14 10.79 3.88 6.5 7.52 10.17 5.19 6.76 12.76 7.04 10.25 15.68 9.37 10.87 8.4 159.39 6.87 33.81 57.32 15.65 395.18 9.45 207.26 203.96 8.67 3.76 4.88 5.56 3.17 6.17 4.85 7.31 5.96 15.02 4.41 7.22 11.21 12.13 6.74 8.12 16.87 7.97 11.83 26.99 13.94 14.77 14.77 72.75 13.97 96.45 134.71 20.01 283.77 937.1 225.8 482.22 8.86 3.79 5.51 5.94 3.1 6.36 4.67 7.41 6.32 15.82 4.47 7.17 11.45 12.82 7.51 8.11 17.74 8.22 12.12 27.39 16.05 15.4 16.81 84.04 12.48 89.36 128.95 20. 10.11 4.51 5.4 6.21 3.46 6.74 5.36 8.68 9.34 17.26 4.48 7.6 15.95 12.23 7.28 8.31 16.91 8.26 12.67 36.78 17.51 15.15 20.66 69.25 16.76 116.95 152.14 22.81 335.23 2036.45 335.24 902.31 275.56 1209.39 223.49 569.48 Glot Seq_bn_inv 9.39 3.25 3.64 4.58 2.87 5.99 4.08 4.95 4.7 9.65 4.76 8.0 9.7 11.12 4.78 9.92 22.42 11.44 16.42 11.24 8.11 17.0 9.14 19.27 12.48 27.14 29.1 10. LoRA 7.97 2.96 3.7 4.21 2.63 5.84 3.68 4.01 3.92 8.8 3.94 6.28 7.58 9.54 4.78 6.66 11.82 6.37 8.9 9.61 5.88 8.44 6.21 4.34 5.97 15.06 15.22 6.83 FFT 22.16 6.18 6.12 7.9 3.81 10.95 4.62 5.3 4.87 12.55 4.97 6.69 14.11 12.32 8.14 7.6 16.64 6.74 21.99 9.18 4.7 10.49 5.92 3.88 16.13 26.25 25.16 10.57 8.78 3.42 3.63 6.69 2.97 5.8 4.56 4.35 5.09 8.97 3.99 5.95 8.27 9.53 4.82 6.41 11.86 5.74 9.11 7.76 5.08 7.32 5.46 2.95 4.99 12.49 10.41 6.53 3.19 353.49 9.57 122. 18.09 5.0 274.66 14.31 97.99 15.94 12.01 1972.96 155.99 713.65 80.87 3.36 597.55 19.12 206. 30.18 Total 34.48 66.23 108.49 77. Table 11: Pseudo-perplexity scores comparison for XLM-R across different adapters in ConceptNet and Glot. Language not included in XLM-R pre-training. FFT denotes full fine-tuning of base model on the target-language Glot data. The underlined FFT scores indicate that FFT outperform the best performing adapter for respective language. Correlation Between Pseudo-Perplexity Preand Post-training Data Sizes Figure 2: Correlation between the pre-training data sizes for mBERT and XLM-R and the pseudo-perplexities with the values fit in the log-space for the pre-adaptation and post-adaptation results. Model mBERT XLM-R mBERT XLM-R Pearson (p-value) -0.37 (0.07) -0.32 (0.1) Spearman (p-value) -0.51 (0.01) -0.39 (0.04) -0.69 (<0.001) -0.27 (0.16) -0.79 (<0.001) -0.79 (<0.001) a t - P - P Table 12: Pearson and Spearman Correlations for mBERT and XLM-R between pseudo-perplexity and amounts of pre-training and post-training data for the pre-adaptation and post-adaptation results. Post-adaptation results are based on the models with Seq_bn language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and pseudo-perplexity scores after the adaptation. As illustrated in Figure 2, the improvements in pseudo-perplexity for both models are primarily concentrated in languages with smaller pre-training data sizes, which are positioned on the left side of the plots. These languages benefit the most from the adaptation process. Conversely, for languages with substantial representation in the pre-training data, the improvements are less pronounced or nonexistent. This suggests that underrepresented languages in the pre-training data can achieve significant gains in pseudoperplexity even with modest amounts of adaptation data and low-capacity adapters (smaller parameter counts). In contrast, further improvements for well-represented languages may require increasing the capacity of the adapters to better utilize their substantial pre-training representation. The stagnation, or drops, in the performance on the languages with extensive pre-training data effects can also be attributed to the model seeing the same (duplicated) data that was seen during pre-training, which makes the \"value\" of data lower since the model sees the duplicates (Lee et al., 2022b). Comparison of XLM-R-base with Glot500 and XLM-R-large ISO th ro bg da el he sk sl lv ms ka bn az ur mk te ne mr sw cy uz jv su si am ku ug Avg. mt bo yo Avg. Total XLM-R-base Adapted XLM-R-base XLM-R-large Glot-500m 7.83 2.97 3.61 4.29 2.56 5.74 3.93 4.79 4.14 10.79 3.88 6.50 7.52 10.17 5.19 6.76 12.76 10.25 15.68 9.37 8.40 33.81 57.32 7.04 10.87 159.39 6.87 15.65 395.18 9.45 207.26 203.96 34. 7.97 2.96 3.63 4.21 2.97 5.8 3.68 4.01 3.92 8.8 3.94 5.95 7.58 9.53 4.78 6.41 11.82 8.9 7.76 5.08 5.46 12.49 10.41 5.74 7.32 2.95 4.99 6.26 3.19 274.66 9.57 95.81 15.22 4.92 2.06 2.53 2.78 1.87 3.19 2.30 2.60 2.51 6.71 2.69 3.99 4.40 6.10 3.23 4.31 8.06 5.77 8.90 4.35 3.92 17.83 26.42 4.50 6.73 126.40 3.80 10.11 317.81 3.99 155.57 159.12 25. 31.34 13.29 14.16 28.06 6.87 32.80 26.36 41.98 14.55 38.46 10.77 19.36 17.46 25.60 14.00 17.19 23.19 27.95 44.82 25.74 15.33 73.46 52.65 15.03 25.56 23.35 13.67 25.66 7.93 26.74 96.80 43.82 27.48 Table 13: Average pseudo-perplexity scores for 30 languages across three model configurations. For the adapted XLM-R-base, we pick the adapter with the best performance. We additionally compare XLM-R adapted with Glot language adapters against two larger models: XLM-R-large (Conneau et al., 2020) and Glot500-m (Imani et al., 2023)  (Table 13)  . Both models provide distinct points of comparison. XLM-R-large shares the same architecture as XLM-R-base but with significantly larger size (550M parameters). XLM-R-large outperformed smaller models with adapters on MLM, suggesting that adapter effectiveness might be inherently constrained by the base models capacity. In contrast, Glot500-m, while only slightly larger than XLM-R-base (395M parameters), introduces an extended vocabulary to support new scripts from 600GB multilingual corpus and fine-tunes the weights of XLM-R-base. Its training employs sampling strategy with an alpha of 0.3, prioritizing low-resource languages over high-resource ones. While this approach improves its performance on many low-resource languages, it results in suboptimal outcomes for well-represented languages. This comparison is particularly relevant as it evaluates whether fine-tuning XLM-R-base with Glotbased language adapters can surpass the performance of these larger models. Furthermore, Glot500-m provides unique benchmark, as it was trained on the same multilingual corpus used for our adapters, albeit without the computational constraints that limited our data size for adaptation. Correlation Between Pseudo-Perplexity and Downstream Tasks Figure 3: Correlation between the downstream performance for mBERT and XLM-R preand post-adaptation and the pseudo-perplexities. Model Task Pre-Adapt Post-Adapt mBERT XLM-R Pearson (p-value) -0.09 (0.62) -0.29 (0.12) -0.28 (0.13) -0.48 (0.007) -0.47 (0.009) -0.42 (0.02) Spearman (p-value) -0.25 (0.18) -0.15 (0.42) -0.22 (0.24) -0.68 (<0.001) -0.55 (0.002) -0.62 (<0.001) Pearson (p-value) -0.66 (<0.001) -0.45 (0.01) -0.54 (0.002) -0.88 (<0.001) -0.64 (<0.001) -0.35 (0.06) Spearman (p-value) -0.42 (0.02) -0.23 (0.23) -0.49 (0.006) -0.20 (0.3) -0.38 (0.04) -0.28 (0.13) TC SA NER TC SA NER Table 14: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between pseudo-perplexity and task performance. Post-Adapt is represented by the models adapted with the Seq_bn language adapters. Topic Classification Results - Part ISO he el bg th ro bn te ka mk da sl az sk ms uz ur cy lv mr ne jv sw su yo Avg. mt ku ug si am bo Avg. Base 79.79 79.47 84.39 74.18 86.95 76.18 80.03 76.28 83.44 87.06 83.6 81.09 84.37 84.31 76.57 76.7 72.37 82.28 73.21 73.72 72.4 69.17 76.15 54.18 77.67 69.86 28.76 23.4 17.45 17.75 12.59 28.72 Total avg. 67. mBERT ConceptNet Seq_bn 83.99 77.95 83.71 74.66 87.86 77.65 82.35 73.26 84.48 86.85 85.07 83.72 83.49 84.65 73.89 73.7 72.23 83.63 77.29 77.55 73.32 70.53 77.42 52.11 78.39 69.83 23.78 22.21 14.3 14.01 11.08 25.87 67. LoRA 82.87 79.14 84.17 73.9 86.45 74.52 80.04 74.26 84.34 86.63 83.75 82.53 83.98 84.1 73.71 74.85 71.6 82.42 76.22 74.62 75.12 69.89 77.62 52.08 77.87 69.85 15.71 20.9 14.88 18.47 9.48 24.88 67.27 Seq_bn_inv Seq_bn 82.11 78.12 83.38 74.42 88.37 76.69 81.13 74.07 83.79 87.72 86.22 83.38 85.4 82.94 75.76 74.76 73.49 82.45 76.61 76.02 73.11 70.21 77.0 54.89 78.42 68.79 19.93 22.17 14.95 12.94 6.33 24.18 67.57 83.26 76.65 82.64 71.34 85.8 77.51 77.32 75.68 84.53 86.03 86.71 82.93 84.79 85.4 81.32 76.06 81.47 83.48 76.37 81.59 73.71 73.93 78.21 55.93 79.28 78.0 46.41 47.18 21.53 18.74 36.67 41.42 71. LoRA 83.43 77.92 82.87 74.47 86.63 78.09 81.2 78.23 84.92 86.48 85.39 82.55 84.43 84.59 74.44 75.26 77.16 82.56 75.73 75.21 74.09 69.05 79.2 55.93 78.74 78.09 40.22 31.68 21.25 20.3 28.36 36.65 70.32 Glot Seq_bn_inv 83.91 76.64 82.58 72.47 86.8 77.34 78.95 75.19 85.25 85.5 86.73 84.29 83.57 83.39 79.35 76.94 80.75 80.94 75.28 80.8 74.02 77.15 78.63 58.05 79.35 79.8 46.85 48.91 20.4 18.07 39.17 42.2 71.92 FFT 83.24 84.81 85.88 76.44 89.06 77.3 79.33 79.82 84.96 85.8 86.43 82.01 84.52 84.38 85.35 78.18 85.53 85.02 78.84 79.11 75.89 85.89 79.97 63.66 81.73 83.32 52.82 56.26 19.08 16.88 33.53 43. 74.11 Table 15: F1 scores comparison across different adapters for mBERT in ConceptNet and Glot for topic classification. All results are averaged over 3 independent runs with different random seeds. Topic Classification Results - Part II ISO th ro bg da el he sk sl lv ms ka bn az ur mk te ne si mr sw cy am uz ku ug jv su Avg. mt bo yo Avg. Base 87.93 86.94 86.55 86.04 86.74 85.02 87.18 85.47 86.25 88.12 84.08 80.29 84.05 83.25 86.45 83.58 84.14 84.92 81.03 77.83 79.54 77.5 81.93 13.49 79.56 81.35 81.5 81.14 64.56 10.69 28.29 34.52 Total avg. 76.48 XLM-R ConceptNet Seq_bn 87.19 87.0 86.0 84.94 85.59 84.9 85.53 86.24 87.83 87.11 85.37 81.11 85.86 81.04 86.41 83.64 83.98 84.54 82.84 75.58 78.44 78.4 78.73 14.09 79.11 79.32 81.25 80.82 63.62 9.89 26.06 33.19 76.05 LoRA 87.22 86.85 87.81 83.7 85.64 83.8 84.81 86.95 86.93 85.82 83.79 80.85 84.24 80.29 86.99 84.26 83.92 84.86 81.34 76.23 80.1 77.93 78.43 15.76 78.67 82.23 79.65 80.71 61.43 9.73 16.07 29.08 75.54 Seq_bn_inv Seq_bn 85.99 88.02 86.41 84.26 84.32 84.79 85.2 86.28 88.97 85.81 83.18 82.09 85.07 82.35 85.45 83.13 83.77 82.23 80.08 77.97 78.99 77.91 76.97 17.28 78.86 81.43 80.42 80. 64.43 11.74 24.6 33.59 75.93 86.97 87.47 86.46 86.47 85.77 86.62 85.46 84.94 85.22 87.94 83.92 83.56 84.43 82.97 86.94 82.43 82.65 84.49 82.2 80.23 78.83 80.67 83.35 68.57 81.29 83.59 84.51 83.63 77.39 17.65 54.13 49.72 80.24 LoRA 86.8 86.95 86.33 84.88 85.28 84.19 86.52 86.67 86.38 85.21 85.0 82.59 85.16 81.98 85.97 84.13 84.71 83.37 79.54 78.73 79.15 77.52 81.13 46.29 82.23 81.84 83.86 82.31 69.74 17.85 35.24 40.94 78.17 Glot Seq_bn_inv 88.5 87.6 86.19 86.41 86.6 83.36 86.03 85.28 87.41 87.94 83.95 83.38 86.39 82.17 87.15 83.43 82.85 84.99 84.23 81.57 81.37 81.51 80.68 73.97 80.14 81.74 84.66 84. 77.92 16.93 59.44 51.43 80.79 FFT 84.21 88.03 87.53 87.06 88.1 84.67 85.59 88.12 87.52 89.49 82.27 84.95 86.08 83.97 88.15 85.65 84.2 84.53 84.21 85.95 85.17 84.22 86.37 81.72 84.95 81.2 84.49 85.61 84.35 20.41 67.13 57.3 82. Table 16: F1 scores comparison across different adapters for XLM-R in ConceptNet and Glot for topic classification. All results are averaged over 3 independent runs with different random seeds. Correlation Between Topic Classification and Preand Post-training Data Figure 4: Correlation between the downstream performance for mBERT and XLM-R and the pre-training data and adaptation data. Model Task Pre-Adapt Post-Adapt (Glot) Post-Adapt (CN) mBERT XLM-R TC TC (p-value) 0.35 (0.1) 0.28 (0.16) (p-value) 0.53 (0.008) 0.82 (<0.005) (p-value) 0.45 (0.03) 0.55 (0.002) (p-value) 0.32 (0.13) 0.75 (<0.005) (p-value) 0.38 ( 0.06) 0.28 (0.15) (p-value) 0.55 (0.006) 0.83 (<0.005) Table 17: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between task performance and data amounts. Post-Adapt is represented by the models adapted with the Seq_bn_inv language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and downstream task performance scores after the adaptation. Named Entity Recognition Results - Part ISO he el bg th ro bn te ka mk da sl az sk ms uz ur cy lv mr ne jv sw su yo Avg. mt ku ug si am bo Avg. Base 84.46 90.16 91.25 67.34 91.61 95.46 75.41 86.17 92.43 89.76 92.61 87.81 90.87 93.26 86.48 94.37 88.72 92.78 86.34 66.45 52.87 83.41 52.62 79.0 83.82 58.3 52.34 34.1 16.59 37.88 56.04 42.54 Total avg. 75.56 ConceptNet Glot Fusion mBERT Seq_bn 84.1 90.11 91.64 65.65 91.88 96.07 76.17 86.07 92.09 90.08 92.85 87.54 90.88 93.0 86.69 94.2 89.34 92.82 86.19 61.96 62.83 83.44 55.88 83.02 84.35 49.01 60.41 35.33 13.41 33.02 56.02 41.2 75. LoRA 84.24 90.45 91.64 66.79 91.85 95.82 76.94 86.11 92.3 90.33 92.82 87.8 90.89 92.98 86.58 93.93 89.35 93.25 85.97 61.75 61.76 83.99 53.72 83.87 84.38 51.58 59.92 33.07 14.06 33.7 55.57 41.32 75.77 Seq_bn_inv Seq_bn 84.59 90.27 91.48 66.68 91.89 96.49 75.29 86.05 92.2 89.74 92.78 88.23 90.96 92.95 86.33 94.23 89.05 93.16 86.29 64.56 65.3 83.54 57.53 83.1 84.7 50.46 59.39 34.56 13.94 35.23 55.29 41.48 76.05 83.57 89.9 91.64 67.22 91.74 96.42 75.51 85.32 92.62 90.02 92.93 87.27 90.83 93.16 86.87 94.4 89.18 92.7 86.32 71.12 63.97 83.4 49.48 81.48 84.46 60.55 59.9 40.2 22.97 32.72 53.92 45.04 76. LoRA 84.22 90.5 91.59 67.8 91.65 96.03 74.69 85.89 92.2 89.72 92.62 87.3 91.3 93.93 86.46 94.26 89.36 92.94 86.07 69.37 58.73 83.79 50.79 79.58 84.2 61.41 52.93 36.24 14.58 46.46 55.45 44.51 76.26 Seq_bn_inv Seq_bn Seq_bn_inv 83.89 89.35 91.56 66.95 91.79 96.3 75.37 85.71 92.02 88.99 92.77 87.3 90.99 93.47 87.73 94.29 90.02 92.64 84.35 70.46 63.34 84.07 51.6 79.74 84.36 64.93 51.51 37.62 19.94 46.49 53.38 45.64 76.62 84.84 90.3 91.78 67.36 91.69 95.86 76.53 86.05 91.61 89.41 92.71 86.46 91.04 92.65 87.5 94.25 88.95 93.34 86.24 70.18 57.21 81.68 57.12 79.81 84.36 60.32 52.33 42.93 20.7 36.94 52.03 44. 76.33 84.53 90.0 91.76 67.57 92.17 96.1 77.02 86.07 91.98 89.48 92.56 87.22 90.84 92.59 88.45 94.85 88.71 92.66 86.22 66.89 58.67 81.96 57.74 78.54 84.36 62.93 52.4 44.05 24.24 32.45 53.53 44.93 76.47 Table 18: F1 scores comparison for mBERT in ConceptNet and Glot for named entity recognition. All results are averaged over 3 independent runs with different random seeds. Named Entity Recognition Results - Part II ISO th ro bg da el he sk sl lv ms ka bn az ur mk te ne si mr sw cy am uz ku ug jv su Avg. mt bo yo Avg. Base 66.55 91.78 91.09 89.58 90.03 85.56 91.36 92.28 92.64 92.0 86.96 95.87 86.13 95.02 92.97 74.67 55.47 63.85 85.92 84.34 89.33 51.22 89.64 35.34 42.36 42.99 33.07 77.33 46.31 43.51 73.54 54. Total avg. 75.05 ConceptNet Glot Fusion XLM-R Seq_bn LoRA Seq_bn_inv Seq_bn 66.4 91.79 91.22 89.57 90.32 85.48 91.19 92.58 92.73 92.36 86.77 95.66 85.34 94.57 92.47 73.64 53.0 58.43 85.86 83.31 88.9 49.9 88.66 39.53 52.63 45.64 38.4 77.64 32.69 44.29 71.2 49. 74.82 66.85 91.78 91.36 89.54 89.88 85.45 91.21 92.16 92.65 91.65 86.88 95.9 86.47 95.04 93.26 76.07 60.02 63.83 85.5 84.37 88.88 49.29 87.51 42.99 50.67 44.7 42.26 78.38 40.11 44.55 73.46 52.71 75.81 66.76 91.92 91.48 89.45 90.14 84.99 91.26 92.41 92.95 92.28 87.73 96.06 86.53 94.86 92.28 74.27 60.0 57.43 85.77 84.26 88.97 48.18 87.89 43.83 51.98 50.87 48.32 78.62 32.13 46.41 74.59 51. 75.87 66.63 92.0 91.34 89.44 89.89 84.92 91.32 92.36 92.84 92.4 87.31 96.07 87.03 94.43 92.83 75.18 59.08 68.15 84.75 84.72 89.3 52.57 88.64 40.41 49.88 46.51 41.47 78.57 48.03 41.86 73.3 54.4 76.16 LoRA 65.29 91.87 91.4 89.85 90.02 85.69 91.45 92.05 92.88 92.06 87.66 96.13 86.63 94.89 92.68 74.38 54.99 60.34 85.25 84.4 89.72 47.17 89.97 31.43 50.5 44.7 39.76 77. 41.54 39.64 74.87 52.01 74.97 Seq_bn_inv Seq_bn Seq_bn_inv 66.2 92.18 91.43 89.72 90.02 85.28 91.49 92.33 93.1 91.9 87.37 96.09 87.59 94.27 92.72 74.82 56.61 66.2 86.1 84.47 89.41 51.67 86.86 29.4 52.63 47.96 42.89 78. 53.57 38.27 75.09 55.64 75.92 65.89 92.02 90.91 89.85 90.18 85.35 91.4 92.21 92.99 92.67 86.59 95.57 86.23 94.4 92.32 72.92 67.84 71.94 85.8 84.56 89.4 55.0 89.05 58.02 53.12 63.53 52.53 80.83 64.31 48.15 73.04 61.83 78.93 66.82 92.05 91.43 89.89 90.5 85.4 91.24 92.12 92.93 91.82 87.33 96.23 86.38 94.56 92.46 73.91 67.34 73.66 85.52 83.5 89.36 52.55 87.64 56.93 58.5 58.81 49.61 80. 57.57 47.55 75.8 60.31 78.65 Table 19: F1 scores for XLM-R across ConceptNet and Glot for named entity recognition. All results are averaged over 3 independent runs with different random seeds. Correlation Between Named Entity Recognition and Preand Post-training data Figure 5: Correlation between the downstream performance for mBERT and XLM-R and the pre-training data and adaptation data. Model Task Pre-Adapt Post-Adapt (Glot) Post-Adapt (CN) mBERT NER XLM-R NER (p-value) 0.32 (0.1) 0.31 (0.1) (p-value) 0.32 (0.1) 0.58 (0.002) (p-value) 0.42 (0.04) 0.31 (0.1) (p-value) 0.29 (0.2) 0.61 (<0.005) (p-value) 0.20 ( 0.3) 0.32 (0.1) (p-value) 0.44 (0.03) 0.60 (<0.005) Table 20: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between task performance and data amounts. Post-Adapt is represented by the models adapted with the Seq_bn_inv language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and downstream task performance scores after the adaptation. Sentiment Analysis Results - Part ISO he el bg th ro bn te ka mk da sl az sk ms uz ur cy lv mr ne jv sw su yo Avg. mt ku ug si am bo Avg. Base 91.42 86.35 88.82 81.68 92.87 92.28 83.49 78.12 62.47 95.71 85.71 79.42 91.11 91.5 86.84 82.43 87.28 75.41 88.7 59.51 75.38 57.71 82.13 76.1 82.18 65.24 84.2 70.94 64.97 61.45 79.4 71.03 Total avg. 79.95 mBERT ConceptNet Seq_bn 91.55 86.27 89.41 81.97 92.67 92.16 83.29 78.1 69.01 95.33 86.46 79.59 88.86 92.03 85.67 81.89 86.99 75.66 88.76 51.46 74.24 54.25 84.25 75.66 81.9 65.68 82.82 68.35 64.89 62.02 79.12 70.48 79.61 LoRA 90.44 86.05 89.17 81.92 92.62 92.6 84.17 76.68 66.4 95.77 86.28 79.72 89.9 91.87 86.76 82.01 87.82 73.99 89.0 67.17 74.75 57.24 84.62 75.24 82. 62.82 83.97 72.67 65.01 60.87 79.38 70.79 80.23 Seq_bn_inv Seq_bn 90.81 86.22 89.54 82.45 92.64 91.88 85.01 76.05 62.07 95.33 85.81 80.03 89.73 91.99 85.85 82.13 86.15 74.71 88.67 55.31 73.94 52.9 83.33 75.35 81.58 66.88 83.37 72.19 64.67 61.45 80.67 71. 79.57 90.79 84.88 88.76 82.57 93.13 92.26 85.55 80.03 67.54 95.95 86.79 79.62 90.87 92.06 86.52 82.69 87.71 76.32 89.43 56.77 76.16 65.05 84.42 75.93 82.99 68.79 85.14 76.91 65.42 60.3 83.27 73.3 81.05 LoRA 90.87 84.95 88.65 82.0 92.98 92.56 84.45 80.23 65.06 96.15 86.4 80.15 91.16 91.7 86.36 82.66 87.76 75.41 89.13 59.35 75.7 62.21 84.75 75.43 82. 73.87 84.46 71.35 66.02 61.62 82.33 73.27 80.86 Glot Seq_bn_inv 91.58 84.52 89.2 83.23 92.96 92.57 85.26 81.24 65.21 96.09 87.83 80.13 92.18 92.57 86.85 82.72 87.42 76.65 88.97 63.19 75.43 69.64 83.99 77.85 83.64 65.34 86.14 80.4 65.62 63.81 82.14 73. 81.69 FFT 90.6 86.38 89.99 83.19 93.7 92.48 88.41 86.97 68.98 96.84 88.66 81.44 91.08 93.83 88.33 83.81 88.53 79.24 90.43 63.47 75.44 54.6 84.06 77.32 84.07 74.11 85.55 76.63 66.26 59.48 81.77 73.97 82.05 Table 21: F1 scores comparison across different adapters for mBERT in ConceptNet and Glot for sentiment analysis. All results are averaged over 3 independent runs with different random seeds. Sentiment Analysis Results - Part II ISO th ro bg da el he sk sl lv ms ka bn az ur mk te ne si mr sw cy am uz ku ug jv su Avg. mt bo yo Avg. Base 88.18 94.37 91.36 98.04 88.82 91.26 94.6 93.75 83.3 95.51 91.92 93.78 84.05 85.6 70.96 89.72 64.6 92.49 91.17 70.08 90.83 86.15 87.63 89.39 88.97 76.51 88.15 87. 55.63 51.81 74.73 60.72 Total avg. 84.78 XLM-R ConceptNet Seq_bn 88.26 94.84 90.66 97.84 88.92 89.66 93.86 93.46 83.78 95.27 91.51 94.14 84.05 85.99 69.22 89.15 69.37 92.59 91.8 65.37 91.01 83.77 88.24 89.73 88.88 77.34 82.66 87.09 55.19 47.33 73.4 58.64 84.24 LoRA 88.43 95.03 91.43 98.13 88.98 91.81 93.87 94.32 83.36 95.66 90.8 94.3 84.05 85.67 67.05 89.59 64.06 92.18 91.9 77.11 90.57 84.2 88.37 89.08 89.91 77.01 85.17 87.48 55.32 51.07 73.6 60. 84.73 Seq_bn_inv Seq_bn 88.46 95.04 91.41 98.02 88.73 91.25 93.43 92.68 83.83 95.57 91.21 94.46 83.98 85.85 69.45 89.22 63.02 93.21 91.8 75.3 90.65 82.88 88.13 89.78 87.64 77.14 84.41 87.28 54.13 49.34 75.09 59.52 84. 88.11 94.74 91.26 98.09 88.19 90.48 93.22 94.23 82.47 95.44 91.92 94.13 84.32 85.89 73.9 89.56 67.49 92.49 91.87 79.52 91.12 87.04 88.47 92.57 88.81 76.51 89.69 88.2 69.4 52.92 75.5 65.94 85.98 LoRA 88.31 94.67 90.93 98.04 88.25 90.27 93.72 93.57 83.12 95.29 91.11 94.1 84.2 86.7 70.74 89.72 68.38 91.78 92.36 77.24 90.88 87.9 87.98 89.09 90.01 76.79 90.34 87.98 63.15 50.9 72.0 62. 85.38 Glot Seq_bn_inv 88.13 95.03 90.65 97.98 88.61 90.85 93.44 93.86 83.65 95.53 91.41 94.43 84.74 86.25 72.31 89.9 68.65 91.96 91.8 74.45 91.36 87.7 88.39 93.31 89.65 77.65 89.69 88.2 69.31 50.69 77.65 65.88 85. FFT 86.39 94.55 90.79 97.82 88.75 90.2 94.03 92.73 82.97 95.26 93.33 94.41 85.19 87.27 71.68 90.92 65.46 92.85 92.43 83.84 91.01 87.49 90.08 95.31 91.72 75.53 89.03 88.56 70.38 55.19 78.99 68.19 86.52 Table 22: F1 scores comparison across different adapters for XLM-R in ConceptNet and Glot for sentiment analysis. All results are averaged over 3 independent runs with different random seeds. Correlation Between Sentiment Analysis and Preand Post-training data Figure 6: Correlation between the downstream performance for mBERT and XLM-R and the pre-training data and adaptation data. Model Task Pre-Adapt Post-Adapt (Glot) Post-Adapt (CN) mBERT XLM-R SA SA (p-value) 0.45 (0.03) 0.36 (0.07) (p-value) 0.50 (0.01) 0.47 (0.01) (p-value) 0.38 (0.07) 0.32 (0.1) (p-value) 0.41 (0.05) 0.33 (0.1) (p-value) 0.39 ( 0.06) 0.38 (0.05) (p-value) 0.52 (0.009) 0.52 (0.005) Table 23: Pearson and Spearman Correlations for mBERT and XLM-R (Pre-Adapt and Post-Adapt) between task performance and data amounts. Post-Adapt is represented by the models adapted with the Seq_bn_inv language adapters and denote the correlation between the sum of the pre-training and adaptation data sizes and downstream task performance scores after the adaptation. Results for Large-Scale Models for TC and NER 3 1 6 0 - u - 5 3 - . 24.14 52.17 54.29 2.90 54.80 38.74 43.08 52.71 54.29 56.84 21.05 47.19 54.29 52.71 52.71 44.27 55.83 51.64 23.38 52.17 53.76 26.38 55.83 57.84 53.24 44.27 53.24 44.87 22.61 49.45 3 1 6 0 - 4 - 38.74 44.27 50.55 1.94 58.33 38.10 42.47 52.17 60.27 51.09 21.05 43.68 53.76 51.09 60.75 50.55 52.71 54.80 62.63 52.71 47.76 20.26 46.62 50.00 49.45 46.04 65.79 34.82 16.22 55.83 - a B 7 - 2 a 7.64 30.81 23.79 3.69 31.47 19.71 26.76 33.17 21.84 24.39 28.49 18.37 31.62 19.90 28.98 29.18 21.49 34.88 8.66 28.65 33.60 28.22 28.24 5.92 16.94 6.53 22.87 29.50 18.17 30.46 - c - 7 - 2 - l 5.41 20.54 9.35 4.63 29.92 7.67 18.08 34.03 21.69 17.55 21.31 15.25 23.85 14.04 26.75 23.07 18.42 31.49 4.81 29.75 31.05 23.89 14.01 5.78 20.88 6.90 15.07 13.49 11.26 27.35 8 - 3 - l - M 38.03 73.78 65.89 40.80 64.95 68.26 68.75 73.03 70.22 69.01 66.73 68.58 69.79 64.84 66.66 63.25 62.32 70.19 60.25 70.57 75.67 63.50 68.95 64.72 77.50 66.23 67.80 69.53 50.05 74.10 . 8 - 1 3 - l - M 40.43 71.97 63.43 48.83 63.53 65.47 68.69 73.73 73.70 69.80 69.39 63.50 70.63 63.07 68.33 67.22 62.69 72.20 57.45 72.77 70.18 67.46 68.37 62.36 75.40 62.22 67.53 68.35 46.74 73.28 7 - . 1 Q 13.57 51.86 42.62 10.15 55.15 21.71 37.47 57.73 46.99 46.93 49.99 32.76 55.05 39.41 55.99 44.26 42.96 56.43 12.49 55.40 55.53 46.31 40.51 27.29 46.57 12.37 39.13 33.53 25.36 56.74 7 - 2 Q 23.68 65.86 66.08 12.41 77.06 33.20 49.93 75.95 63.73 70.07 50.76 52.02 67.69 56.66 75.87 56.10 54.99 74.64 29.29 74.63 63.56 58.94 51.05 55.69 67.38 54.64 61.90 54.55 30.44 76.05 1 7 - l 7.32 10.08 10.75 6.44 20.17 10.26 10.45 17.85 11.97 10.87 17.90 3.50 12.70 26.78 12.91 11.45 10.12 20.10 5.98 20.27 11.10 17.55 12.91 16.89 6.25 9.29 23.50 10.05 14.08 11.13 1 7 - o 8.27 16.68 20.93 10.56 16.58 8.63 18.09 21.90 11.90 8.53 25.19 14.76 17.38 29.62 16.69 20.18 19.45 20.76 9.38 17.58 17.18 21.68 22.41 20.13 16.62 11.72 23.33 13.89 21.90 23.31 9 - 2 - e 41.19 57.95 51.22 12.12 51.85 33.49 50.38 45.05 39.08 44.35 59.48 58.73 45.97 27.30 55.62 43.93 15.71 52.51 46.12 35.52 48.42 60.68 48.61 47.24 45.24 33.74 29.04 56.50 35.16 55.96 7 - e 43.02 68.79 66.91 20.23 63.26 44.59 56.87 71.14 67.20 64.03 57.33 69.17 69.24 56.58 64.88 62.54 62.31 69.08 65.92 68.94 67.87 65.78 58.78 68.93 58.64 45.77 56.48 65.44 37.11 69.52 1 - 0 1 - 0 0 5 - m 5.71 5.71 5.71 5.71 5.71 5.71 5.71 5.71 5.71 5.71 5.71 5.71 8.21 5.71 3.97 5.71 5.62 5.71 6.60 5.71 9.22 5.71 5.71 5.71 5.71 7.54 5.71 5.71 8.75 5.71 2 - 0 1 - 0 0 5 - m 9.03 5.71 5.69 3.63 5.23 6.86 5.71 5.39 5.71 4.76 2.20 8.13 3.13 5.83 3.49 5.71 4.07 5.71 2.20 8.49 3.30 7.69 6.70 5.17 7.82 3.66 6.61 11.34 7.65 5.71 5 - 7 - g o 6.85 31.37 22.42 11.65 44.70 14.07 26.21 49.20 31.48 22.82 34.05 25.17 34.20 19.63 40.23 34.33 25.79 47.32 10.82 43.66 40.19 32.18 29.03 12.91 35.27 16.20 29.62 26.86 18.71 40.15 2 l - 7 - y 3.59 17.55 12.99 6.61 24.77 7.81 19.37 32.02 20.84 9.51 19.65 13.24 23.91 9.59 22.65 24.09 18.61 30.92 5.71 27.70 28.09 21.59 11.88 4.73 21.98 7.13 12.23 10.11 10.23 27.33 . 5 7 - x 7.86 26.56 28.25 7.06 41.81 9.31 17.57 56.88 55.80 10.66 44.85 9.35 23.25 23.24 40.43 28.45 31.47 43.15 5.48 39.12 30.21 44.52 45.91 49.59 50.01 7.76 41.90 15.93 16.97 46. am az bn bo bg ku cy da el he jv ka lv mr mk mt ne ro si sk sl su sw te th ug ur uz yo ms Total avg. 45.02 45.82 23.13 18. 65.80 65.62 40.41 56.83 13.02 17. 44.27 60.21 6.04 5.74 28.57 29. 16.88 Table 24: F1 Scores for All Large-Scale Models on TC. The results are based on 3-shot prompting, as reported by Ji et al. (2024). GPT-3.5 and GPT-4 results are zero-shot, obtained from Adelani et al. (2024a). Bloom Bloomz mT0 GPT-3.5-turbo-0301 th el ur te sw bg mr bn 1.0 19.7 71.7 5.3 58.8 29.6 27.9 36. 0.2 13.0 47.3 3.8 26.8 19.7 20.4 36.2 1.4 12.8 47.1 3.3 24.3 14.7 12.3 23.9 - 69.3 - - - 72.0 - - Total avg. 31.35 20. 17.48 70.65 Table 25: Three-shot NER results across eight overlapping languages from BUFFET (Asai et al., 2023). The scores for GPT-3.5 are only provided for two languages. Qwen 1.5B Qwen 7B Llama 8B Qwen 14B Llama 70B am az be bo bg ku cy da el he jv ka lv mr mk mt ne ro si sk sl su sw te th ug ur uz yo ms Total avg. 7.03 9.60 6.59 2.38 7.93 6.77 8.93 13.04 3.91 5.50 10.51 4.35 11.14 6.17 4.91 11.76 4.70 9.50 12.47 6.66 13.34 9.44 10.38 9.19 8.49 7.02 3.71 11.76 6.70 10.58 8.15 13.99 18.48 31.51 8.17 26.47 18.48 18.49 25.62 10.26 23.03 19.45 20.46 14.29 22.67 24.16 18.01 23.59 21.93 14.28 15.61 22.71 22.41 11.15 17.90 40.22 17.72 27.47 21.45 13.49 21.73 20.17 9.18 12.27 20.25 9.67 24.31 20.10 20.32 17.90 16.41 20.77 19.53 24.99 17.60 22.31 22.44 18.24 26.36 24.67 14.96 21.37 18.89 21.98 15.45 27.21 20.80 19.67 24.23 17.02 15.20 27.82 19. 31.76 53.05 68.20 18.92 46.81 17.98 26.68 41.18 58.39 46.25 28.04 45.74 44.14 49.54 44.38 49.23 55.34 57.25 29.43 45.38 43.22 34.95 18.60 38.99 73.49 28.83 47.75 38.32 18.57 56.00 41.88 43.41 73.19 78.17 62.63 78.65 77.52 61.55 78.29 77.90 76.66 66.43 77.60 74.09 68.77 77.66 66.83 69.25 77.72 70.69 75.80 65.42 65.53 67.94 75.35 74.23 71.21 80.07 70.58 45.55 73.02 70.72 Table 26: F1 Scores for DeepSeek-R1 distilled models of various sizes for TC. The results are based on zero-shot prompting and were obtained in our evaluation. Language TC NER SA LLaMA-3 (Baseline) 33.64 16.67 29.05 19.37 60.93 31. LLaMA-3 +Seq_bn_inv 72.50 39.11 60.21 52.32 77.14 60.26 LLaMA-3 (Baseline) 76.36 30.84 67.08 26.88 24.72 45.18 LLaMA-3 +Seq_bn_inv 77.03 30.08 67.33 28.23 22. 45.12 LLaMA-3 (Baseline) 58.36 80.42 45.47 52.12 57.77 58.83 LLaMA-3 +Seq_bn_inv 88.43 83.8 51.22 63.89 56.06 68. cy si sw ug mt Total avg. Table 27: Comparison of F1 Scores for LLaMA-3 Baseline (fine-tuned with task adapter) and LLaMA3+Seq_bn_inv on TC, NER, and SA. All results are averaged over 3 independent runs with different random seeds."
        }
    ],
    "affiliations": [
        "Brno University of Technology",
        "German Research Center for Artificial Intelligence (DFKI)",
        "Kempelen Institute of Intelligent Technologies (KInIT)",
        "University of Saarland"
    ]
}