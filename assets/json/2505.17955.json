{
    "paper_title": "Diffusion Classifiers Understand Compositionality, but Conditions Apply",
    "authors": [
        "Yujin Jeong",
        "Arnas Uselis",
        "Seong Joon Oh",
        "Anna Rohrbach"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to a small number of benchmarks and a relatively shallow analysis of conditions under which the models succeed. To address this, we present a comprehensive study of the discriminative capabilities of diffusion classifiers on a wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce a new diagnostic benchmark Self-Bench comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover a relationship between domain gap and timestep sensitivity, particularly for SD3-m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/Diffusion-Classifiers-Compositionality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 5 9 7 1 . 5 0 5 2 : r Diffusion Classifiers Understand Compositionality, but Conditions Apply Yujin Jeong1 Arnas Uselis2 Seong Joon Oh2 Anna Rohrbach1 1TU Darmstadt & hessian.AI 2Tübingen AI Center & University of Tübingen"
        },
        {
            "title": "Abstract",
            "content": "Understanding visual scenes is fundamental to human intelligence. While discriminative models have significantly advanced computer vision, they often struggle with compositional understanding. In contrast, recent generative text-to-image diffusion models excel at synthesizing complex scenes, suggesting inherent compositional capabilities. Building on this, zero-shot diffusion classifiers have been proposed to repurpose diffusion models for discriminative tasks. While prior work offered promising results in discriminative compositional scenarios, these results remain preliminary due to small number of benchmarks and relatively shallow analysis of conditions under which the models succeed. To address this, we present comprehensive study of the discriminative capabilities of diffusion classifiers on wide range of compositional tasks. Specifically, our study covers three diffusion models (SD 1.5, 2.0, and, for the first time, 3-m) spanning 10 datasets and over 30 tasks. Further, we shed light on the role that target dataset domains play in respective performance; to isolate the domain effects, we introduce new diagnostic benchmark SELF-BENCH comprised of images created by diffusion models themselves. Finally, we explore the importance of timestep weighting and uncover relationship between domain gap and timestep sensitivity, particularly for SD3m. To sum up, diffusion classifiers understand compositionality, but conditions apply! Code and dataset are available at https://github.com/eugene6923/ Diffusion-Classifiers-Compositionality. Figure 1: Overview of our findings. Finding I: Diffusion models can perform compositional discrimination reasonably on real images, but underperform CLIP, especially on counting tasks (5.2). Finding II: Diffusion models can understand (through classification) the images they can generate (5.3). Finding III: Timestep reweighting improves discrimination by reducing the domain gap between generated and real data (5.4). Equal contribution Preprint."
        },
        {
            "title": "Introduction",
            "content": "Models like Stable Diffusion [8, 38] have been trained on billions of image-text pairs and can generate highly detailed and coherent images that match textual descriptions. Their ever-increasing ability to generate complex compositional scenes [16, 10] suggests they have developed strong understanding of image-text relationships and can effectively align visual and textual concepts. Diffusion models are trained with pixel-wise supervision, so they may be less prone to learn shortcuts, compared to discriminatively-trained models like CLIP [35], which often are insensitive to word order [56, 15, 7], struggle with spatial relationships, counting [46, 31], and compositions [45, 24, 47]. It is thus natural to ask: can we transfer the compositional capabilities of generative models to discriminative compositional tasks? There is growing interest in leveraging the strong generative capabilities of diffusion models for broader discriminative tasks such as classification, shape-texture bias, and depth estimation [57, 42, 19]. One emerging approach, known as Diffusion Classifiers, repurposes diffusion models for zero-shot classification using their original training objective [26, 25, 6, 12, 19]. Notably, they have outperformed CLIP on compositional benchmarks like CLEVR [21] and Winoground [45], which require reasoning over multiple objects and attributes. However, existing studies are limited in scope, often relying on small number of benchmarks and lacking systematic analysis. recent work, the Generative AI Paradox [52], explores key open question: whether strong generation implies strong discrimination. It shows that even if the model can generate, it may not understand, highlighting the disconnect between generative and discriminative capabilities. However, this analysis involves separate models for generation (e.g., Midjourney [18]) and discrimination (e.g., CLIP [35], OpenCLIP [17]) in image classification scenarios, making it difficult to directly assess the relationship between the two. In contrast, diffusion classifiers offer direct way of probing the generative-discriminative connection by using the same model for both generation and discrimination. To this end, we formulate three hypotheses to understand when and why Diffusion Classifiers succeed or fail, focusing on: i) diverse, large-scale compositional settings, ii) visual domain gap, and iii) the effect of different timesteps on classification. Our corresponding findings are illustrated in Figure 1. Hypothesis 1: Diffusion models discriminative compositional abilities are better than CLIPs. This is inspired by the findings in prior works. We conduct an extensive evaluation with three diffusion models, including new SD3-m [8] model, on ten compositional benchmarks (covering 33 subset tasks scale not previously explored in this context) spanning four broad task categories (Object, Attribute, Position, and Counting). We find that diffusion models often outperform CLIPbased discriminative models, particularly in reasoning over spatial relations. However, they also exhibit notable weaknesses, e.g., in counting tasks. Interestingly, SD3-m, despite superior generative compositionality, achieves lower discriminative accuracy (39%) compared to earlier versions (43%) in our analysis. We shed light on this in the following. Hypothesis 2: Diffusion models understand (through classification) what they generate. To explore the relationship between diffusion models generative and discriminative abilities, we introduce SELF-BENCH, diagnostic benchmark consisting of model-generated images. The idea is to isolate the image domain and assess the models ability to understand images most familiar to the model. We find that diffusion classifiers perform well in-domain (i.e., when evaluated on data generated by the same model). However, their cross-domain performance (i.e., on data generated by different diffusion model) drops significantly, especially for SD3-m. This highlights the domain gap (e.g., data distribution) as one of the critical factors. In in-domain scenarios, we observe positive correlation between generative and discriminative compositional performance, suggesting that stronger generative models can transfer their compositional knowledge to discriminationbut only when they can generate the target domain. Hypothesis 3: The domain gap can be mitigated by timestep weighting. Lastly, we investigate how diffusion timesteps impact discriminative performance by optimizing timestep weights for downstream tasks. Prior work has explored how diffusion models generate images through structured progression across timesteps [27, 50]. However, in diffusion classifiers, either uniform timesteps or fixed timestep weightings are typically used across all datasets and modelsan area that remains underexplored. In contrast, we find that SD3-m is particularly sensitive to timestep selection. Our results show that even low-shot timestep tuning (using just 5% of the target dataset can significantly mitigate the performance drop of SD3-m on real-world compositional benchmarks. We hypothesize 2 that SD3-ms timestep sensitivity is closely linked to its susceptibility to the domain gap. To further examine this relationship, we incorporate CLIP-based image encoders to quantify visual similarity between domains, and analyze how it correlates with optimal timestep weighting. We find that timestep weighting is especially effective in scenarios with large domain gaps."
        },
        {
            "title": "2 Related work",
            "content": "Diffusion models. Generative models have demonstrated impressive performance in producing realistic images [36, 54, 5], videos [4, 58], and audio [9, 29, 20]. In particular, text-to-image diffusion models [14] iteratively refine images conditioning on text prompts by adding and removing noise, achieving remarkable quality. widely used open-source example is Stable Diffusion (SD) [38]. Earlier SD versions [38] are based on UNet [39] backbone, incorporating ResNet blocks [11] and attention mechanisms [49]. With the rise of transformer-based designs [49], the latest Stable Diffusion 3 series [8] adopts this architecture, further boosting performance. It also introduces new noise sampling strategies for training Rectified Flow models [30, 1, 28]. Our analysis focuses on Stable Diffusion versions 1.5, 2.0, and 3-m, examining their architectural evolution and performance across scales. Compositionality in text-to-image models. Text-to-image generation models, such as diffusion models, are hypothesized to have the capability to generate combinations of objects that were not present in the training data [45, 32]. Later versions of diffusion models, such as Stable Diffusion 3, exhibit improved generative capabilities and can produce scenes with greater compositional complexity [8, 53]. Recent benchmarks, such as CompBench [16] and GenEval [10], confirm this trend. In this work, we explore diffusion models to gain deeper understanding of compositionality across various tasks, using existing compositional discriminative benchmarks and our newly introduced SELF-BENCH, which consists of images generated by the diffusion models themselves. Diffusion classifiers. Recent studies have explored zero-shot classification using diffusion models denoising process directly [26, 6, 25]. Li et al. [26] introduce the Diffusion Classifier with an adaptive evaluation strategy, demonstrating superiority over CLIP RN-50 [35]. Clark et al. [6] propose universal timestep weighting function, showing effectiveness on attribute binding tasks (e.g., CLEVR [21]). Diffusion-ITM [25] adapts diffusion models for image-text matching, enabling both text-to-image and image-to-text retrieval, and introduces GDBencha benchmark with seven complex vision-and-language taskswhere it outperforms CLIP baselines. These methods share common foundation but differ in weighting and sampling strategies. Beyond zero-shot classification, few-shot approaches [55] leveraging diffusion models have also been proposed. Discffusion [12] enhances discrimination using cross-attention maps and LSE Pooling [3], focusing on few-shot learning but applicable to zero-shot settings as well. More recently, Gaussian Diffusion Classifier [34] was proposed as one-shot or zero-shot method, using features from DINOv2 to improve efficiency. In this work, we primarily study the vanilla zero-shot Diffusion Classifier [26] to better understand its discriminative capabilities."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we first discuss the prerequisites for diffusion classifiers. We then detail our approach to turning Stable Diffusion 3-m [8] into classifier; we are the first to explore this, to the best of our knowledge. Last, we describe our approach to learning the optimal weighting function for the diffusion classifiers on given test data. Given dataset DN = {(x1, c1), . . . , (xn, cN )} of images, where each image xi RHW 3 is labeled with class ci {1, . . . , K}, we aim to learn classifier that can effectively handle compositional classification tasks. In practice, we work with latent representations Rd by encoding the images using pretrained autoencoder model."
        },
        {
            "title": "3.1 Preliminaries: diffusion classifiers",
            "content": "Diffusion models [44, 14] are generative models that learn to gradually denoise by reversing forward diffusion process. For an image-text pair (x, c), where is first encoded into latent representation using pretrained autoencoder, the core training objective for diffusion models is L(z, c) = Et,ϵ (cid:104) wt (cid:13) (cid:13) (cid:13)ϵ ϵΘ(zt, t, c) 2(cid:105) , (cid:13) (cid:13) (cid:13) (1) 3 where wt are timestep weights, ϵΘ is neural network that predicts the noise ϵ added to the latent at timestep , and is conditioning text prompt. This loss is related to the ELBO of the conditional likelihood p(zy), where is the class label, which allows us to use diffusion models for classification, as shown in [26, 25, 6]: = arg max yk p(y = yk z) = arg max yk log p(z = yk), where the likelihood is estimated using diffusion models through ELBO, approximated by Eq. (1), with conditioning representing specific class label yk. In practice, we approximate the expectation in Eq. (1) via Monte Carlo sampling using Ts timesteps by considering fixed timesteps and noises. That is, we assume fixed set = {(tj, ϵj)}Ts j=1, tj := j/Ts with which we compute the expectation. Learning the weighting function. In diffusion models, different timesteps capture varying levels of information [27, 50]. This hierarchical information processing is crucial for compositional tasks, where both global structure (e.g., object relationships) and local details (e.g., attributes) matter. Recently, [6] has explored universal timestep weighting in discriminative (yet non-compositional) settings. While we adopt several components from their approach, our setting and low-shot smoothing strategy differ. They rely on computationally expensive, high-variance classification estimates. For instance, they assume 100 trials for single image-text pair. In contrast, we use fixed timesteps and noise to reduce variance in prediction [26] when computing the reconstruction target in Equation (1). We provide details in A.2."
        },
        {
            "title": "3.2 SD3-m as a classifier",
            "content": "SD3-m is rectified flow model [30, 1] trained with conditional flow matching (CFM) loss [28], which differs from the standard diffusion objectives used in SD1.5 and SD2.0. As result, we cannot directly apply the same classifier objective used in earlier versions. By reparameterizing the CFM objective as noise-prediction loss (see, e.g., [8]), we can obtain LRF(x0) = Et,ϵ[wt ϵΘ(zt, t, c) ϵ2] Using this formulation, we can use SD3 as classifiers, despite its different underlying architecture. The only difference lies in the weighting function wt, which for SD3 follows logit-normal distribution rather than the uniform weighting used in SD1.5/2.0. However, we empirically find that uniform weighting performs better for classification. Details are given in Section A.1. (2) The weighting function wt can be parameterized in two ways: (a) piecewise constant function S0, where we learn individual weights v0, . . . , vTs1, and S0 denotes the set of wt = vtTs, timesteps; (b) alternatively, to enforce smoothness, as p-degree polynomial wt = (cid:80)p S0; (a) is generally used for achieving the upper-bound performance. However, in the low-shot learning setting (5% of the full training set), we use (b) to prevent overfitting. i=0 aiti,"
        },
        {
            "title": "4 Self-Bench: a diagnostic benchmark",
            "content": "As shown in Figure 2 (left), existing benchmarks are diverse in terms of domains. However, we empirically observe that the generation results of diffusion models are not as diverse, having preferred native style. For example, in SD3-m, objects are usually well-centered/focused and have glossy aesthetic touch; images are in high resolution (see Figure 2, right). Additional examples in Figure A.7 (Supplemental) further support that SD3-m consistently produces images with similar style. This raises the question: can diffusion classifiers understand images from different domains in discriminative scenarios? (Note that we use the word domain in fairly relaxed sense.) Moreover, what is the best possible performance on in-domain data? This relates to our Hypothesis 2, that the domain plays critical role in discriminative performance. To answer these questions, we try to isolate the domain effect and compare the performance across in-domain and out-of-domain scenarios. Here, we posit that if diffusion model can generate images of certain type, it can also discriminate them. Therefore, we define in-domain as the data that diffusion models can generate. Namely, we propose SELF-BENCH, benchmark for evaluating diffusion classifiers on diffusion models own generated data. 4 Figure 2: Examples of standard benchmarks vs. SELF-BENCH. Each benchmark is categorized into four broad task groups: Object, Attribute, Position, and Counting. Each group consists of one or more tasks, and we present one example per task for illustration. We indicate positive / negative captions, where the task involves matching the positive caption with its corresponding image. Notably, standard benchmarks and SELF-BENCH feature domain distinctions, incorporating the factors like style, resolution, and object scale. Diagnosing with SELF-BENCH. We construct and evaluate the benchmark as follows (see Figure 3): 1. Prompt collection. We use text prompts from GenEval [10], benchmark for compositional generation. GenEval includes 80 object classes and six task types: Color, Color Attribution, Counting, Single Object2, Two Objects, and Position. 2. Image generation. For each prompt, we generate four images using SD1.5, SD2.0, and SD3-m (with guidance scale 9.0). We manually filter out failed generations. 3. Discriminative prompt construction. For each image, we keep the original generation prompt as the positive and create negative prompts. For example, for the Position task, if the original prompt is parking meter left of teddy bear, we construct three additional negative prompts using other predefined spatial relations (right of, above, and below). 4. Evaluation. We evaluate how well diffusion classifiers can match generated images with the correct prompts among distractors. Figure 3: Diagnosing with SELF-BENCH. (i) Using Genevals prompts from six types, generate images. (ii) For each generated image, create discriminative tasks within its type from the prompts used in the generation process. (iii) Given the generated images (filtered by humans) and the discriminative tasks, benchmark the diffusion classifier. Table 1: SELF-BENCH Statistics: For each task, we show the number of images in Full (F) and Correct (C) sets. Filtering. The generation process may produce failures, such as ambiguous (e.g., with over half of an object missing or mixed colors) or incorrect images. Thus, we define two sets: Full, containing all generated images, and Correct, the filtered high-quality subset. Three human annotators evaluate each image, and only samples approved by all are deemed Correct. Table 1 reports the number of images in both sets. The ratio of Correct images varies notably across models and tasks. 396 105 376 219 400 396 129 376 263 400 396 306 376 314 400 6 320 98 400 400 19 320 111 400 113 320 230 SD1.5 320 SD2.0 320 SD3-m Color Attrib. Position Counting 1188 540 1128 796 1200 Single Obj. Two Obj. 1200 138 960 439 18 36 252 271"
        },
        {
            "title": "Task",
            "content": "856 960 306 F C F C 2Although Single Object is not traditionally considered compositional, we follow GenEvals definition, which includes it as part of complexity spectrum. In-domain and cross-domain settings in SELF-BENCH. We define in-domain as the setting where we evaluate diffusion classifier on images the same diffusion model generated. Conversely, crossdomain refers to images produced by other models from the diffusion family. We define generation accuracy as #Correct/#Full, where # denotes the number of images in each set. We primarily use the Correct dataset in further analysis."
        },
        {
            "title": "5 Experiments",
            "content": "As mentioned in the Introduction, we aim to investigate three hypotheses. In Section 5.1, we describe the general experimental setup. Section 5.2 (Hypothesis 1) presents comprehensive evaluation across ten compositional benchmarks covering 33 tasks using three diffusion models, including the new Stable Diffusion 3-m. Section 5.3 (Hypothesis 2) uses our SELF-BENCH benchmark to explore the relationship between generative and discriminative abilities and the role of image domain. Section 5.4 (Hypothesis 3) examines how diffusion timesteps influence discriminative performance."
        },
        {
            "title": "5.1 Experimental setting",
            "content": "Evaluation settings. We consider two approaches to turn generative diffusion models into discriminative models: (i) Diffusion Classifier [26] and (ii) Discffusion [12] (see Section A.3 of the Supplemental). However, as shown in Figure A.1 of the Supplemental, Discffusion performs significantly worse than Diffusion Classifiers on SD1.5 and comparably on SD 2.0 and SD3-m. Therefore, we primarily focus on Diffusion Classifiers [26] in our analysis. Stable Diffusion baselines. For the evaluation of diffusion classifiers, we use three versions of Stable Diffusion models: SD1.5, SD2.0 [38], and SD3-m [8]; each new version increases the number of parameters. We selected the baselines based on specific criteria, which are detailed in Section A.5.1 of the Supplemental. Although we also considered distilled variants of diffusion models (e.g., SDXLTurbo [41]), we excluded them from our main evaluation due to their architectural and generative differences. Additional analysis is provided in Section A.7 of the Supplemental. For SD1.5 and SD2.0, we use the Euler Discrete scheduler [23] and uniformly sample the timesteps. For SD3-m, we use the Flow Matching EulerDiscrete scheduler [8] for flow matching diffusions, which was designed specifically for the SD3 series. We sample 30 timesteps from each model uniformly, following [12]. (The effect of different numbers of timesteps is discussed in Table A.2 in the Supplemental.) CLIP baselines. We use vanilla CLIP models as key baselines for comparison on discriminative tasks. We use five different versions of CLIP models: RN50x64, ViT-B/32, ViT/L14, ViT/H14, and ViT/g14. We follow OpenAIs implementation for the first three: RN50x64, ViT/B32, and ViT/L14 [35]. On the other hand, we follow OpenCLIPs implementation for ViT-H/14 and ViT-g/14 [17]. Metrics. The benchmarks vary in structure. Some present paired image-text inputs (i.e., two images and two prompts), while others use single image with multiple candidate prompts. Across all setups, we primarily evaluate using image-to-text retrieval accuracy, which measures whether the model assigns the highest score to the correct prompt given an image. For paired settings, we compute retrieval accuracy based on whether the matching image-prompt pairs are correctly ranked relative to distractors. Further details on task-specific evaluation protocols are provided in Section A.5.2."
        },
        {
            "title": "5.2 Scaling evaluation to ten benchmarks",
            "content": "Krojer et al. [25] highlight counterintuitive finding: more capable generative models may perform worse on discriminative tasks. However, their analysis does not include compositional benchmarks. To address this gap, our evaluation includes Stable Diffusion 3 [8], which is very capable in terms of compositional generation. We hypothesize that diffusion models with stronger compositional generation capabilities are more effective on compositional discriminative tasks. Existing works have explored limited set of compositional benchmarks (e.g., Winoground [45], ARO [56], CLEVR [21]), demonstrating the strengths of diffusion models compared to CLIP. However, compositional tasks span broad range of challenges, and it remains unclear whether these findings generalize across diverse compositional scenarios. To address this, we expand our evaluation to ten complex compositional benchmarks. Benchmarks. Our analysis incorporates ten benchmarks: Vismin [2], EQBench [51], MMVP [46], CLEVR [21], Whatsup [22], Spec [33], ARO [56], Sugarcrepe [15], COLA [37], and 6 Winoground [45]. Each benchmark contains different tasks. For example, Vismin includes tasks related to Object, Attribute, Position, and Counting, whereas COLA is focused on Attribute tasks. The left block in Figure 2 presents examples of these benchmarks. Further details on how the tasks are classified within our compositional task categories are provided in Table A.3 in the Supplemental. In total, we analyze 33 tasks in our main study. Results. Figure 4 shows the average performance of diffusion classifiers on the compositional benchmarks (see Figure A.10 in the Supplemental for the complete results).3 For the Position task, SD3-m performs the best compared to other diffusion models and CLIP models. However, in other tasks, CLIP models usually show better performance than diffusion classifiers, contrary to the findings of previous works [26, 6]. Surprisingly, among diffusion classifiers, SD3-m is not the best; often SD1.5 or SD2.0 models show better results. These results motivate us to deepen our analysis of when and why diffusion classifiers may underperform. Figure 4: Evaluating compositional generalization across different categories. The bars represent average classification accuracies across all tasks within each category. Notably, SD3m does not generally outperform other Stable Diffusion models in most benchmarks, and CLIP usually outperforms diffusion models. Takeaway hypothesis 1: Diffusion classifiers excel in spatial position tasks, perform on par with CLIP in complex attribute tasks, but underperform in object recognition and counting tasks. Thus, Hypothesis 1 is only partially supported. Additionally, more capable diffusion model (SD3-m) does not necessarily perform better on compositional discriminative tasks than earlier models (SD1.5, SD2.0)."
        },
        {
            "title": "5.3 Studying domain effects via Self-Bench",
            "content": "Here, we take closer look at domain gap as possible factor. In-domain evaluation. First, we examine how diffusion classifiers behave indomain, which may serve as an upperbound performance estimate, as domain effects are isolated. As shown in Figure 5, they perform better on Correct (yellow bars) than on Full (blue bars) samples. This indicates that the classifiers do not simply choose prompts used for generation; rather, they indeed distinguish between the correct and incorrect prompts. Moreover, we observe that generation accuracy and discrimination accuracy are positively correlated, contrary to what we saw in other benchmarks in Section 5.2. Specifically, we note that generation accuracy (pink bar in Figure 5) increases from SD1.5 to SD3-m, and the discrimination accuracies in both the Full and Correct categories also rise nearly in parallel from SD1.5 to SD3-m. The correlation coefficient between generation and Correct discrimination accuracy is 0.77. The results suggest that in-domain generation accuracy and discrimination accuracy appear to be positively correlated. The comparison with CLIP is in Figure A.11a of Appendix. Figure 5: SELF-BENCH In-domain performance. (Top three plots) Each row represents the classification accuracy of diffusion classifier from specific SD model when evaluated on its own generated data. (Bottom) positive correlation is observed between generative and discriminative performance. Left axis: discrimination; right axis: generation accuracy. Comparison In-Domain vs. Cross-Domain. Next, we focus on comparing in-domain and crossdomain performance to judge how well models generalize across different domains. 3Tables A.10, A.11, A.12, A.13, A.14, A.15, A.16, A.17, A.18 and A.19 report the quantitative results for all benchmarks used in our evaluation. 7 drop other To quantify this, we measure the in models tested accuracy their domains vs. on indomain accuracy, SELF-BENCH defined as A(model, domain) = Ain(model, domain) Across(model, domain). differences are averaged across both other domains."
        },
        {
            "title": "The",
            "content": "Figure 6: SELF-BENCH: Cross-domain performance degradation. The bars represent average drop rate between in-domain and cross-domain evaluation, averaged over different crossdomain settings. We observe significant accuracy drops when evaluating models on cross-domain data. SD3-m shows the most severe degradation, with up to 38% accuracy loss in two-object tasks and 33-40% drops in color and spatial tasks. Figure 6 illustrates the average accuracy drop when moving from in-domain to cross-domain evaluation on SELFBENCH. We observe accuracy degradation across all tasks, with SD3-m showing the most drop. While part of the performance gap may be due to task-specific weaknesses in each model, we assume the domain gaps, such as different data distribution, also play key role in this degradation. Full results can be found in Figure A.6 in the Appendix. Takeaway hypothesis 2: Diffusion classifiers perform well in-domain, but their accuracy drops significantly in cross-domain settings, highlighting the strong influence of domain shifts."
        },
        {
            "title": "5.4 Timestep weighting effects",
            "content": "Previous works have shown that diffusion models generate images from coarse to fine details over timesteps [27, 50]. However, in classification settings, it is still unclear how different noise levels (i.e., timesteps) affect performance across tasks (e.g., object recognition vs. attribute binding) or different domains (e.g., image style). Diffusion classifiers typically use uniform timestep weighting [26, 25] or fixed timestep weighting scheme (e.g., wt = exp(7t)) [6, 19] across all models (e.g., Imagen [40] and SD). We hypothesize that neither strategy is universally optimal (see ablation studies of uniform weighting and fixed timestep weighting in Sec. A.9 in the Supplemental) and that timestep weighting should be adapted to the model and task differently. Here, we investigate how different timesteps contribute to classification decisions. (Figure 10 in the Supplemental provides an intuitive explanation of why timesteps matter from generative perspective.) Important timesteps vary by task and model, and SD3 is especially sensitive. Figure 7 illustrates the timestepwise classification accuracy. In this setting, the SD2 and SD3-m models are evaluated on some cross-domain SELFBENCH tasks. Interestingly, while all timesteps yield non-zero accuracy for SD2, more than 50% of timesteps result in zero accuracy for SD3-m when evaluated on SD2.0 generations. This highlights that SD3-m is significantly more sensitive to timestep choice than SD2.0. key observation is that different timesteps contribute unequally to classification performance, depending on the model and task. Figure 7: SELF-BENCH: Single-timestep reconstruction error and classification accuracy. While SD2.0 maintains good performance on SD3-ms generations, SD3-m exhibits near-zero accuracy for the majority of initial timesteps on SD2.0s generations, particularly for object recognition tasks. Reweighted SD3 performs best in real-world benchmarks. Since we know the optimal timestep is different based on the model and the task, we assess the applicability of our approach to real-world scenarios; we also follow Sec. 3.1, but assume only 5% of the data is used for training and 5% for validation, and we report test results. Evaluating our low-shot learning approach on standard benchmarks (Figure 8, left), we find that reweighted SD3-m consistently outperforms both baseline models and their reweighted variants. The improvements are substantial across diverse tasks: 98% on CLEVR binding (vs. 63% baseline), and 42% on WhatsupA spatial task (vs. 30% baseline). Learned weight curves (Figure 8) exhibit diverse patterns that vary depending on the model and task, further underscoring the need for task-specific timestep optimization. Additionally, we find that SD1.5 and SD2.0 do not significantly benefit from timestep weighting. We hypothesize that SD1.5 and SD2.0 8 Figure 8: Low-shot timestep reweighting is effective in real-world benchmarks. Left: Accuracy gains on diverse compositional tasks achieved by reweighting diffusion timesteps for Stable Diffusion variants (SD1.5, SD2.0, SD3-m). Reweighted models consistently outperform the baseline; the gains are most pronounced for the SD3-m model. The numbers above the bars indicate the scores after reweighting, while the numbers inside the bars show the original scores. Positive deltas are highlighted using the reweighted color. Right: Learned timestep weighting schemes indicating task-dependent emphasis on specific diffusion steps (early, middle, or late), demonstrating the importance of tailoring timestep selection to the task structure. perform near-optimally with uniform weighting, while SD3-m may suppress certain timesteps due to training on smaller, more filtered, and human-aligned dataset than LAION-5B [43]. Timestep weighting helps mitigate the domain gap. In Sec. 5.2, we show that SD3-m underperforms SD1.5 and SD2.0 on real-world datasets, and in Sec. 5.3, SD3-m exhibits the largest drop in the crossdomain setting. Since timestep weighting significantly improves SD3-m on real-world tasks, this raises an important question: Does timestep weighting partially help mitigate the domain gap? To study this further, we conduct an experiment to approximate the effect of domain differences. We generate images using the original prompts from real-world compositional benchmarks. This yields two image sets for each task: (i) the original real-world dataset, and (ii) synthetic variant generated using the same prompts. Both sets target the same task but differ in visual domain. Using CLIP image encoder (ViT-B/32) [35], we aim to capture the domain gap4 between the two, by computing the L2 distance between average embeddings with randomly sampled 50 images. As shown in Figure 9, larger CLIP embedding distances (i.e., greater domain gap) are associated with greater reliance on timestep weighting, but only for SD3. This suggests positive correlation between domain gap and the effectiveness of timestep weighting for SD3. In contrast, SD1 and SD2 do not exhibit such trend. We hypothesize that, as shown in Figure 7, most timesteps in SD2 (also in SD1) are already effective, resulting in limited benefit from reweighting and thus obscuring any correlation with the domain gap. Details are in Section A.12 of the Appendix. Figure 9: Timestep Weighting and Domain Gap. CLIP distances between real-world datasets SELF-BENCH generations, and corresponding accuracy gains from timestep weighting. Larger domain gaps correlate with greater improvements, but only for SD3. and Figure 10: Intuition for timestep utility. We visualize SD3-m generations starting from different noise levels applied to Self-Bench image. (Top) Conditioning on an incorrect caption. (Bottom) Conditioning on the correct one. Only for intermediate timesteps (e.g., [0.73, 0.93]) does the model apply meaningful edits without overwriting the original image. See main text for explanation. Qualitative illustration. To build intuition for how timestep selection impacts classification, we visualise generations from the SD3-m model in Figure 10, starting from Self-Bench image generated 4Indeed CLIP may capture both stylistic as well as semantic shifts, broadly referred to as \"domain.\" 9 by SD2.0 (a parking meter and teddy bear). For each timestep t, we corrupt the original image with Gaussian noise corresponding to t, and then generate an image by running the diffusion model for 20 denoising steps from down to 0, conditioning on either correct or incorrect prompt. We set the classifier-free guidance coefficient to 0.0 (i.e., using only the conditional prompt) to match the discriminative setting used in classification with diffusion. At very early timesteps (e.g., = 0.1), the generation remains nearly identical regardless of the prompt, suggesting that the model ignores the conditioning - we believe such timesteps are nondiscriminative. At very late timesteps (e.g., = 0.96), the model strongly reacts to the prompt but also overwrites the original image, shifting it out of domain - again rendering the timestep nondiscriminative. Only at intermediate timesteps (e.g., [0.73, 0.93] in this case) do we see the model make meaningful edits that reflect the caption while retaining the original structure. We interpret these as discriminative timesteps - where the prompt meaningfully affects the output without erasing the original content. This figure complements our quantitative findings (see Figure 7), offering visual explanation for why certain timesteps are more informative for classification. In particular, the low accuracy of SD3-m at early timesteps mirrors what we observe qualitatively: early generations fail to reflect the prompt and remain unchanged, making them unsuitable for discrimination. Takeaway hypothesis 3: Finding optimal timestep weights for given downstream task in low-shot setting is an effective way to improve the performance of diffusion classifiers. It helps mitigate the domain gap between diffusion models generations and real-world test datasets."
        },
        {
            "title": "6 Conclusion",
            "content": "Our work analyzed diffusion classifiers through the lens of compositionality. First, we conducted comprehensive evaluation across diverse compositional tasks, showing that diffusion classifiers demonstrate compositional understanding in some cases (e.g., Position but not Counting), and revealed divergence between generative and discriminative abilities. Next, we introduced SELF-BENCH, diagnostic benchmark of self-generated images, showing that domain shifts significantly affect performance. Finally, we proposed simple low-shot strategy for mitigating the domain gap."
        },
        {
            "title": "Acknowledgements",
            "content": "The compute for Yujin Jeong and Anna Rohrbach was provided by the hessian.AI 42 cluster. The research was also supported by the Tübingen AI Center. Arnas Uselis thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In International Conference on Learning Representations, 2023. [2] Rabiul Awal, Saba Ahmadi, Le Zhang, and Aishwarya Agrawal. Vismin: Visual minimal-change understanding. In Advances in Neural Information Processing Systems, 2024. [3] Pierre Blanchard, Desmond Higham, and Nicholas Higham. Accurately computing the log-sum-exp and softmax functions. IMA Journal of Numerical Analysis, 41(4):23112330, 2021. [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 10 [6] Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero shot classifiers. Advances in Neural Information Processing Systems, 36:5892158937, 2023. [7] Sri Harsha Dumpala, Aman Jaiswal, Chandramouli Shama Sastry, Evangelos Milios, Sageev Oore, and Hassan Sajjad. Sugarcrepe++ dataset: Vision-language model sensitivity to semantic and lexical alterations. Advances in Neural Information Processing Systems, 37:1797218018, 2024. [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [9] Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. In Forty-first International Conference on Machine Learning, 2024. [10] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [12] Xuehai He, Weixi Feng, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, William Yang Wang, and Xin Eric Wang. Discffusion: Discriminative diffusion models as few-shot vision and language learners. Transactions on Machine Learning Research, 2024. [13] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion Guidance, July 2022. [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [15] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. Advances in neural information processing systems, 36:3109631116, 2023. [16] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [17] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. [18] Midjourney Inc. Midjourney. https://midjourney.com, 2023. Accessed before: 2023-0928. [19] Priyank Jaini, Kevin Clark, and Robert Geirhos. Intriguing properties of generative classifiers. arXiv preprint arXiv:2309.16779, 2023. [20] Yujin Jeong, Yunji Kim, Sanghyuk Chun, and Jiyoung Lee. Read, watch and scream! sound generation from text and video. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 1759017598, 2025. [21] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 29012910, 2017. [22] Amita Kamath, Jack Hessel, and Kai-Wei Chang. Whats\" up\" with vision-language models? investigating their struggle with spatial reasoning. arXiv preprint arXiv:2310.19785, 2023. [23] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Advances in neural information processing systems, 35: 2656526577, 2022. [24] Darina Koishigarina, Arnas Uselis, and Seong Joon Oh. Clip behaves like bag-of-words model cross-modally but not uni-modally. arXiv preprint arXiv:2502.03566, 2025. [25] Benno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, and Siva Reddy. Are diffusion models vision-and-language reasoners? In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [26] Alexander Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly zero-shot classifier. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22062217, 2023. [27] Xiao Li, Zekai Zhang, Xiang Li, Siyi Chen, Zhihui Zhu, Peng Wang, and Qing Qu. Understanding diffusion-based representation learning via low-dimensional modeling. In NeurIPS 2024 Workshop on Mathematics of Modern Machine Learning. [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. [29] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In International Conference on Learning Representations, 2023. [31] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31703180, 2023. [32] Core Francisco Park, Maya Okawa, Andrew Lee, Ekdeep Lubana, and Hidenori Tanaka. Emergence of hidden capabilities: Exploring learning dynamics in concept space. Advances in Neural Information Processing Systems, 37:8469884729, 2024. [33] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnose and optimize: Towards fine-grained vision-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1327913288, 2024. [34] Zipeng Qi, Buhua Liu, Shiyan Zhang, Bao Li, Zhiqiang Xu, Haoyi Xiong, and Zeke Xie. simple and efficient baseline for zero-shot generative classification. arXiv preprint arXiv:2412.12594, 2024. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763, 2021. [36] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [37] Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan Plummer, Ranjay Krishna, and Kate Saenko. Cola: benchmark for compositional text-to-image retrieval. Advances in Neural Information Processing Systems, 36:4643346445, 2023. [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. [39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241, 2015. [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [41] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pages 87103. Springer, 2024. [42] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems, 36: 3944339469, 2023. [43] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [44] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265, 2015. [45] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. [46] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. [47] Arnas Uselis, Andrea Dittadi, and Seong Joon Oh. Beyond decodability: Linear feature spaces enable visual compositional generalization. In Workshop on Spurious Correlation and Shortcut Learning: Foundations and Solutions. [48] Arnas Uselis and Seong Joon Oh. Intermediate Layer Classifiers for OOD generalization. arXiv preprint arXiv:2504.05461, 2025. [49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [50] Binxu Wang and John Vastola. Diffusion models generate images like painters: an analytical theory of outline first, details later. arXiv preprint arXiv:2303.02490, 2023. [51] Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Equivariant similarity for vision-language foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11998 12008, 2023. [52] Peter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, et al. The generative ai paradox:\" what it can create, it may not understand\". arXiv preprint arXiv:2311.00059, 2023. [53] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [54] Chenglin Yang, Celong Liu, Xueqing Deng, Dongwon Kim, Xing Mei, Xiaohui Shen, and Liang-Chieh Chen. 1.58-bit flux. arXiv preprint arXiv:2412.18653, 2024. [55] Zhongqi Yue, Pan Zhou, Richang Hong, Hanwang Zhang, and Qianru Sun. Few-shot learner parameterization by diffusion time-steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2326323272, 2024. [56] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=KRLUvxh8uaX. [57] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 57295739, 2023. [58] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        },
        {
            "title": "A Appendix",
            "content": "This supplemental material includes extended preliminaries in Section A.1 and discussion of design choices for diffusion classifiers in Section A.3. Ablation studies are presented in Section A.4. Section A.5 outlines the experimental settings, implementation details, and the full construction of SELF-BENCH. Style alignment experiments are described in Section A.6. We analyze distilled models in Section A.7, and revisit timestep weighting strategies from prior work in Section A.9. Section A.10 illustrates timestep weighting applied to SELF-BENCH, Section A.11 presents zero-shot classification experiments using only later timesteps, and Section A.12 examines CLIP distance between real-world datasets. Finally, Section A.13 includes additional results for all compositional benchmarks. A.1 Extended preliminaries Unified Loss Formulation. For given data sample x0, usually an encoded image from an autoencoder, we define the loss as weighted noise (or vector field) prediction error: L(x0) = Et,ϵ (cid:104) wt (cid:13) (cid:13) (cid:13)ϵ ϵΘ(zt, t, c) 2(cid:105) , (cid:13) (cid:13) (cid:13) (A.1) where ϵ (0, I) is noise sample, is the conditioning variable (e.g. text prompt), and the noisy sample zt and the weight wt depend on the chosen forward process. Diffusion models (SD1.5, SD2.0): The forward process is given by zt = αt x0 + 1 αt ϵ, ϵ (0, I), with αt = 1 βt and αt = (cid:89) s= αs. The noise prediction network ϵΘ(zt, t, c) is trained to predict the noise at each discrete timestep (i.e. performing next-step denoising) by minimizing Ldiff(x0) = Et,ϵ (cid:104) wt (cid:13) (cid:13) (cid:13)ϵ ϵΘ(zt, t, c) 2(cid:105) . (cid:13) (cid:13) (cid:13) In practice, SD1.5 and SD2.0 typically use uniform weighting (i.e. wt := 1). Rectified Flows (RFs) (SD3): Rectified Flows [30, 1, 28] define the forward process via straight-line interpolation between the data and standard normal: zt = (1 t) x0 + ϵ, [0, 1], ϵ (0, I), (A.2) and the network directly parameterizes continuous velocity field vΘ(z, t). The original conditional flow matching (CFM) objective [28] is defined as LCFM = Et, pt(zϵ), p(ϵ) (cid:104)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)vΘ(z, t) ut(z ϵ) (cid:13) 2(cid:105) , (A.3) where ut(z ϵ) is the target vector field along the linear path in (A.2). By reparameterizing the CFM objective as noise-prediction loss (see, e.g., [8]), we obtain LRF(x0) = Et,ϵ (cid:104) wt (cid:13) (cid:13) (cid:13)ϵΘ(zt, t, c) ϵ 2(cid:105) (cid:13) (cid:13) (cid:13) , (A.4) with time-dependent weight wt. For the linear interpolation in (A.2), choosing wRF While wRF operates in continuous time, so the loss L(x0) is defined continuously with respect to x0. = 1t recovers the original CFM objective. is the default weighting for RFs, SD3 [8] uses logit-normal weighting. Moreover, SD Using this formulation we can interpret SD3 under the diffusion loss objective, despite its different underlying architecture. Therefore, we can use it as classifier in the similar way to earlier diffusion models. The only difference lies in the weighting function wt, which for SD3 follows logit-normal distribution rather than the uniform weighting used in SD1.5/2.0. 15 Thus, both Diffusion and RFs ultimately train the network to match target signal via the unified loss in (A.5). In diffusion the network directly predicts the noise at each discrete timestep, whereas in RFs the network predicts continuous velocity field whose reparameterized form is trained to match the noisewith conditioning on in both cases. Given the unified loss in Equation (A.5), L(zt, c)(x0) = Et,ϵ (cid:104) wt (cid:13) (cid:13) (cid:13)ϵ ϵΘ(zt, t, c) 2(cid:105) (cid:13) (cid:13) (cid:13) , (A.5) Diffusion Classifiers. [26] define diffusion classifier as network that minimizes this loss with respect to the data sample x0 and given conditioning variable c. Since computing pθ(x c) is intractable for diffusion models, the ELBO is used in place of log pθ(x c). In particular, assuming that log pθ(x c) Et,ϵ wt ϵ ϵθ(zt, t, c)2(cid:105) (cid:104) , so that, with uniform prior over labels (i.e. p(ci) = 1 ), Bayes rule implies pθ(ci x) exp (cid:110) Et,ϵ wt ϵ ϵθ(zt, t, ci)2(cid:105)(cid:111) (cid:104) . (A.6) In practice, we approximate the expectation in Eq. (A.6) via Monte Carlo sampling. For each class label ci, we sample fixed set = {(tj, ϵj)}N with tj drawn from the prescribed distribution and ϵj (0, I). We then compute the empirical weighted error j=1, ˆE(ci) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) j=1 wtj (cid:13) (cid:13) (cid:13)ϵj ϵθ (cid:0)ztj , tj, ci (cid:1)(cid:13) 2 (cid:13) (cid:13) . Substituting these estimates into Eq. (A.6) yields the approximate posterior pθ(ci x) exp{ ˆE(ci)} exp{ ˆE(ck)} (cid:80) ."
        },
        {
            "title": "The predicted label is then given by",
            "content": "ˆc = arg max ci pθ(ci x). (A.7) (A.8) (A.9) Using the same sample set across all classes reduces the variance in the estimated differences in weighted prediction error. This approach extracts classifier directly from pretrained conditional diffusion model without any additional training. A.2 Details on timestep weighting For given latent representation and class yk {1, . . . , K}, we compute the loss using the fixed set = {(tj, ϵj)}Ts j=1 : ej(z, yk) = (cid:13) (cid:13)ϵj ϵΘ(z, tj, ϕ(yk))(cid:13) 2 (cid:13) , where ϕ(yk) is the text embedding of class yk. The class probabilities are computed using weighted sum over the fixed timesteps: p(y = yk z) = (cid:16) exp (cid:80)Ts (cid:16) (cid:17) j=1 wtj ej(z, yk) (cid:80)Ts j=1 wtj ej(z, yl) (cid:17) . (cid:80)K l=1 exp A.3 Design choices for diffusion classifiers We found that using diffusion classifiers is tricky in practise. There are few design choices that differ across experimental setups in previous works [19, 26, 25, 6]. In this subsection we provide fuller picture of the design choices that matter for diffusion classifiers. 16 We distinguish between five main factors that potentially differ in previous works and can affect the performance of diffusion classifiers:"
        },
        {
            "title": "1 Loss weighting. Previous works either used uniform weighting (i.e. wt := 1) [26, 25] or used a\ntime-dependent weighting scheme found empirically on the training set using CIFAR-100 [6, 19],\nusing wt := exp (−7˜t) (for normalized ˜t ∈ [0, 1]). These schedules are usually motivated by the\ngenerative training objective, which views each timestep t as corresponding to a different noise level\n— effectively treating t as a proxy for input corruption. However, from a discriminative perspective,\ndifferent timesteps correspond to distinct representations within the model, akin to how different\nlayers of a neural network encode features of varying abstraction. In standard supervised models, it\nhas been shown that lower-level features (often found in earlier layers) can be more robust under\ndistribution shifts [48]. Drawing this analogy, early timesteps in diffusion models may similarly\npreserve more local or low-level features that are useful for generalization.",
            "content": "Figure A.1: Comparison between Zero-shot classifier and Discffusion on SELF-BENCH in-domain. Zeroshot Classifier and Discffusion do not show much performance difference, or Discffusion performs worse."
        },
        {
            "title": "2 Classifier-free guidance. Classifier-free guidance [13] (CFG) is a de-facto standard in diffusion\nmodels for improving the quality of generated images at a cost of lesser variance in generated images.\nWhen generating images using CFG using, e.g. DDIM [23] for SD1.5, SD2.0, or Euler sampler for\nSD3 [8],",
            "content": "All previous works have either not used classifier-free guidance [6, 25] or used it in limited setting [26] with conclusion that it does not help with classification performance. We largely found the same conclusion to hold."
        },
        {
            "title": "5 Sampling strategy. Previous works mostly have found that using a uniform timestep distribution\nis the best option, i.e. πN = Uniform([0, 1]). This choice is usually motivated by sticking to the",
            "content": "17 training objective of the model. As we show in Section 3.1, SD3 was trained using timesteps sampled from logit-normal distribution."
        },
        {
            "title": "6 Alternative methods for diffusion classifiers. Discffusion [12] uses attention scores from the\ncross-attention layers of diffusion models, aggregated via LogSumExp (LSE) pooling [3]. Since SD3\nreplaces traditional cross-attention layers with self-attention layers that incorporate text conditioning,\nwe instead extract attention scores from these self-attention layers. In Figure A.1, we compare the\nZero-shot Classifier [26] and Discffusion [12]. In the SELF-BENCH in-domain setting, which can\nbe seen as the fairest setting to evaluate the optimal performance of each method, the performance\nof Discffusion is worse than the Zero-shot Classifier. This result contradicts Discffusion’s claim of\nachieving better accuracy in some benchmarks.",
            "content": "Overall, the prevailing notion in the previous works is that good classifiers can be derived by adhering to the generative training objective of the models. A.4 Ablation Studies We performed two ablations: (1) varying the resolution for SD3 on the self-bench, and (2) using 30 vs. 100 timesteps for all self-bench experiments. For the first ablation, as shown in Table A.1 higher resolution generally leads to better performance for SD3-m model. For the second ablation, increasing to 100 timesteps results in improved performance, although this gain is most notable for SD3 (sampled with uniform weights) [8] in Table A.2. Additionally, we attempted to match the timestep weighting scheme used in the original SD3-m model by employing logit-normal weighting. However, this approach yielded exceptionally poor results: in SELF-BENCH Cross-domain experiments, the model did not exceed 11% accuracy, regardless of the generative model or task. Table A.1: Geneval ablation using SD3, comparing impact of input resolution. Larger images are always better. Used 100 time samples (Ts = 100)."
        },
        {
            "title": "Task",
            "content": "GenEval Version Resize Acc No-Resize Acc Diff (Resize - No) 55.56% 59.72% 98.09% 94.75% 94.30% 99.68% 61.22% 68.47% 96.30% 66.67% 44.74% 93.81% 91.81% 94.41% 100.00% 69.52% 78.29% 98.11% -22.22% 9.72% -0.87% -7.08% -3.04% -1.27% -17.35% -6.31% -38.04% 0.00% 7.89% -23.01% -1.77% -1.05% -1.59% -6.67% -6.20% -6.61% geneval_color_attr geneval_color_attr geneval_color_attr geneval_colors geneval_colors geneval_colors geneval_counting geneval_counting geneval_counting geneval_position geneval_position geneval_position geneval_single geneval_single geneval_single geneval_two geneval_two geneval_two 1.5 2 3-m 1.5 2 3-m 1.5 2 3-m 1.5 2 3-m 1.5 2 3-m 1.5 2 3-m A.5 Details of Experiments settings 33.33% 69.44% 97.22% 87.67% 91.25% 98.41% 43.88% 62.16% 58.26% 66.67% 52.63% 70.80% 90.04% 93.36% 98.41% 62.86% 72.09% 91.50% 18 A.5.1 Choice of Baselines We select these versions for the following reasons: i) SD1.5 was previously used in diffusion classifier evaluations [25], ii) SD2.0 demonstrated better performance compared to SD2.1 [55] in discriminative tasks, and iii) SD3-m is one of the state-of-the-art generative models, and we use its medium variant since it offers effective performance while being more lightweight than the full SD3 model. It has not been studied in the context of diffusion classifiers. We have also considered distilled models as baselines (i.e., FLUX [54] and SDXL-Turbo [41]). However, we did not include them since it is difficult to determine their performance under no classifier-free guidance (CFG-free) settings. Some analysis is provided in Section A.7. A.5."
        },
        {
            "title": "Implementation Details",
            "content": "Evaluations. For evaluation, we use single A100 GPU for all tasks with batch size of 4. The evaluation time depends on the SD model, the number of negative prompts, and the dataset size. While SD1.5 and SD2.0 require similar amounts of time, SD3-m takes significantly longer. Specifically, for dataset with 230 images and 4 prompts per image (one positive and three negative), evaluation takes approximately 15 minutes for SD 1.5 and 30 minutes for SD 3-m. Training details on timestep weighting. We train on 100 timesteps using the Adam optimizer. For the low-shot setting, we fit third-degree polynomial, while for the Self-bench experiments, we use the full timestep vector. We use no regularization, set the learning rate to ℓ = 0.05, and train for 5,000 epochs. The entire optimization procedure is performed on frozen scores, allowing us to infer weights in under minute for datasets with fewer than 1,000 samples. Benchmarks. Table A.3 presents the benchmarks used in our study, categorized into Attribute, Object, Position, Counting, Complex Relation, Action, Size, and others. The benchmarks include Vismin [2], EQBench [51], MMVP [46], CLEVR [21], Whatsup [22], Spec [33], ARO [56], Sugarcrepe [15], COLA [37], and Winoground [45]. Table A.3: Categorization of compositional benchmarks. For EQBench and Vismin, an official subset is used."
        },
        {
            "title": "Object",
            "content": "Aro (Attribute) [56] SugarCrepe (Attribute) [15] Vismin (Attribute) [2] EQBench (EQ-Kubric Attribute, EQ-SD) [51] MMVP (Color) [46] CLEVR (pair binding color, recognition color, recognition shape, binding color shape, binding shape color) [21] COLA (Multi Object) [37] Winoground (Object) [45] SugarCrepe (Object) [15] Vismin (Object) [2] Position (Spatial Relation) WhatsUp (WhatsUp A, WhatsUp B, COCO-spatial one, COCO-spatial two, GQA-spatial one, GQA-spatial two) [22] SPEC (Absolute Spatial, Relative Spatial) [33] EQBench (Location) [51] Vismin (Relation) [2] MMVP (Spatial, Orientation, Perspective) [46] CLEVR (spatial) [21]"
        },
        {
            "title": "Counting",
            "content": "SPEC (Count) [33], EQBench (EQ-Kubric Counting) [51], Vismin (Counting) [2]"
        },
        {
            "title": "ETC",
            "content": "Aro (Relation, COCO order, Flickr order) [56] SugarCrepe (Relation) [15] Winoground (Relation, Both) [45] MMVP (State, Structural Character) [46] EQBench (YouCook2, GEBC, AG) [51] SPEC (Absolute Size, Relative Size) [33] CLEVR (Size) [21] MMVP (Text) [46] Task formulations vary across benchmarks. For example, Winoground consists of two images paired with two captions, each describing two objects. The negative caption is created by swapping the objects in the text. In contrast, Vismin (Object) includes prompts that modify the object by replacing it with another randomly selected object that is not present in the image. The captions in Vismin can describe either single object or multiple objects. We use subset of EQBench and Vismin in our evaluation. Additionally, we include COLA only for multi-object tasks due to the difficulty of selecting negative prompts. However, we found that the name COLA (Multi Object) does not align well with the tasks focus, as it primarily deals with object attributes in the prompts. Therefore, we classify COLA under the Attribute category. As mentioned earlier, some benchmarks consist of single image paired with multiple text prompts, while others feature two images with two matching captions. The latter category includes Winoground, COLA, Vismin, EQBench, and MMVP, whereas all other benchmarks belong to the former category. Categories To enable structured analysis, we group the tasks into four categories: Object, Attribute, Position, and Counting. Each category is designed to target specific aspect of compositional understanding through carefully crafted text prompts. Object: Evaluates object recognition (often in context) within given context by introducing modifications such as swapping, replacing, or removing objects to assess the models ability to distinguish between different entities. Attribute: Focuses on descriptive properties (e.g., adjectives) associated with objects, such as variations in color or shape, to determine the models sensitivity to attribute-object relationships. Position: Examines spatial relationships between objects or the perspective of single object (e.g., \"a dog on the left side of the image\"). Counting: Assesses numerical reasoning by prompting the model to count specific objects in an image. Image-Text matching scores. As mentioned above, the task can be divided into two parts. First, there is one image with multiple texts. Second, there are two images and two texts in one pair. For the first setting, we simply pick the best prompt with Equation A.9. However, for the second task, following previous approaches [26] to get the text score, if we have pair of <image1, text1, image2, text2>, we use the following equation: (cid:34)score(text1, image1) > score(text2, image1) AND (cid:35) score(text2, image2) > score(text1, image2) (A.10) where score follows Equation A.9. A.5.3 Details of SELF-BENCH Creating discrimination tasks. We use generation prompts from Geneval [10]. The prompt template \"a photo of\" is used in the experiment for both generation and discrimination. These are the possible choices for each discrimination task. Colors: \"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\", \"pink\", \"brown\", \"black\", \"white\" Positions: \"left of\", \"right of\", \"above\", \"below\" Counting: \"one\", \"two\", \"three\", \"four\" Single Object: \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \"potted plant\", 20 \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"computer mouse\", \"tv remote\", \"computer keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\" For Two Objects and Color Attribution, the possible choices are the same as those in Single Object and Colors, respectively. Since both Two Objects and Color Attribution involve two objects, the latter additionally requires binding two color choices to the respective objects (e.g., \"a red dog and yellow cat\"). As result, the Color Attribution category can have up to 100 different prompt combinations. For the Two Objects category, the number of possible prompts is large (6,400). To manage this, we consider only 101 prompts: one containing both true objects and 100 cases where one true object is paired with randomly selected object. Crowdsourcing with manual filtering. We recruited three annotators and provided them with detailed instructions for the filtering process. The task took approximately 23 hours, and no compensation was provided. Each annotator was instructed to filter images based on category-specific criteria, following the Geneval framework [10]. For example, the Two Objects category requires generating two objects mentioned in the original prompt clearly. Additionally, Single Object category requires the presence of the mentioned object, and the number of the objects does not matter; it simply checks if the mentioned object is in the image. It does not matter if non-mentioned object is also in the same image. Figure A.2 displays an example of the filtering interface where the annotator can label the images. Since there are ambiguous examples, such as an image with only half of an object, we provided three options: \"Good\", \"Ambiguous\", and \"Wrong\". If any one of the three annotators labels the example as ambiguous or wrong, the image is filtered out later. Examples of filtered images are provided in Figure A.3. Figure A.2: Example of filtering userspace. The annotator should select one among three options. Before they annotate, they get the instructions for each category. Figure A.3: Images filtered out by annotators. Annotators removed images that did not meet the criteria specified in the category instructions. A.6 Style alignment In this section, we present additional results on attempts to close the domain gap. We perform style alignment experiments using the textual inversion technique. The core idea is to adapt the models style representation to better fit the target domain. Specifically, for each dataset, we aim to reduce the domain gap for the Stable Diffusion 3 Medium model. 22 Figure A.4: Image generations of SD3-m with style alignment. Top: CLEVR dataset; Bottom: Whatsup-A dataset. To achieve this, we learn style token denoted as via textual inversion. Given dataset, we train such that it enables accurate reconstruction of the datasets images when used in text prompts. The training prompts follow the structure: clear photo in the style of . We performed these experiments on the original dataset size of 512. At inference time, when evaluating diffusion classifiers, we include the learned style token in the prompt by appending the phrase in the style of . This style-conditioned prompting is used across several datasets, including the Whatsup-A and CLEVR-ColorBinding benchmarks. The generations are shown in Fig. A.4. The results of these experiments are summarised in the table below. Overall, we find that style alignment through textual inversion may not be an effective way to mitigate the domain gap. Table A.4: Diffusion classifier accuracy before and after style alignment via textual inversion."
        },
        {
            "title": "Dataset",
            "content": "Before Alignment (%) After Alignment (%) WhatsApp-A CLEVR-ColorBinding 26.5 59.1 29.3 57.1 A.7 Results on distilled SD3.5 and FLUX model Given that timestep reweighting has strong positive effect on the performance of the Stable Diffusion 3 model, we further investigate whether distilled versions of these models (capable of generating images in as few as 4 steps) behave differently from their corresponding base models. Specifically, we evaluate Stable Diffusion 3 and its distilled counterpart, Stable Diffusion 3.5 Large Turbo, on Self-Bench at resolution of 512. While the distilled model supports extremely fast generation, we ensure fair comparison by running both models at 30 inference steps. This experiment tests whether aggressive distillation, while beneficial for generation quality and efficiency, compromises the discriminative performance of the modelor not. The results suggest that it does: the distilled model underperforms significantly compared to its base version. We present the results in Table A.5. Additionally, FLUX [54] has been introduced with impressive generative quality. We include FLUX in our analysis as an exploratory case, focusing on Position and Counting 23 tasks in real-world datasetscategories where diffusion models typically perform particularly well and poorly, respectively. As shown in Table A.6, we again observe significant drop in discriminative performance for the distilled model. Table A.5: SD3 / SD3.5 Large turbo (distilled version) accuracies on Self-Bench tasks."
        },
        {
            "title": "Two",
            "content": "1.5 2 3-m 0.17 / 0.15 0.31 / 0.34 0.49 / 0.72 0.67 / 0.63 0.47 / 0.48 0.40 / 0.68 0.33 / 0.30 0.41 / 0.42 0.43 / 0.57 0.53 / 0.72 0.65 / 0.71 0.83 / 0.84 0.48 / 0.74 0.51 / 0.78 0.73 / 0. 0.19 / 0.49 0.16 / 0.57 0.43 / 0.75 Table A.6: Performance of FLUX (distilled version) in Position and Counting. Method WhatsUP WhatsUP COCO one COCO two GQA one GQA two SPEC relative EQbench Vismin MMVP spatial MMVP orientation MMVP perspective CLEVR Vismin EQbench MMVP SD1.5 SD2.0 SD3-m FLUX 0.28 0.27 0.28 0.28 0.27 0.28 0. 0.29 0.48 0.42 0.54 0.47 0.52 0.47 0.55 0.55 0.49 0.49 0. 0.52 0.47 0.53 0.56 0.50 0.30 0.29 0.43 0.35 0.15 0.15 0. 0.0 0.19 0.13 0.44 0.24 0.27 0.33 0.47 0.27 0.07 0.13 0. 0.2 0.53 0.6 0.47 0.33 0.49 0.51 0.59 0.57 0.33 0.13 0. 0.17 0.25 0.15 0.05 0.05 0.33 0.13 0.13 0.13 A.8 Additional qualitative results We illustrate qualitative example of SD2.0 and SD3-m generation results from different timesteps on Whatsup-A dataset in Figure A.5. Figure A.5: Image generation results on Whatsup-A using SD2.0 and SD3-m models. A.9 On universality of timestep weighting Previous work has shown that learning task-specific timestep weighting function, while beneficial, typically results in only modest gains, on average around 1% improvement in performance. These results have mostly been reported on classification tasks involving single, non-compositional queries. Here, we test whether such universal weighting functions, as proposed in earlier work, can also be effective in our setting. To do so, we evaluate all models on our proposed datasets using an exponential timestep weighting function defined as exp(7t), where is the normalized timestep ranging from 0 to 1. 24 We present the results in Table A.7. Overall, we find that uniform and exponentially weighted models perform quite similarly for 1.52 diffusion models, although the gap is often quite large between the two. While for SD-3m model, uniform weighting is almost always better. Table A.7: Mean accuracies by version and task. Bold indicates the larger of the uniform or exponentiallyweighted scores. Version Exp. Weighted"
        },
        {
            "title": "Task",
            "content": "1.5 2 3-m COCO QA VG QA CLEVR BindingColor CLEVR Spatial Spec Count Sugar Attributes Sugar Objects WhatsUp WhatsUp COCO QA VG QA CLEVR BindingColor CLEVR Spatial Spec Count Sugar Attributes Sugar Objects WhatsUp WhatsUp COCO QA VG QA CLEVR BindingColor CLEVR Spatial Spec Count Sugar Attributes Sugar Objects WhatsUp WhatsUp 0.44 0.44 0.67 0.50 0.20 0.70 0.85 0.27 0.26 0.42 0.48 0.82 0.49 0.23 0.76 0.85 0.26 0.26 0.56 0.54 0.60 0.62 0.19 0.72 0.74 0.28 0.40 0.48 0.49 0.70 0.48 0.12 0.61 0.78 0.27 0.28 0.47 0.49 0.74 0.50 0.12 0.64 0.81 0.30 0.23 0.55 0.52 0.75 0.51 0.11 0.61 0.71 0.27 0. A.10 Upperbounding Self-Bench performance with timestep weighting Timestep weights benefit all models cross-domain, but SD3 the most. We investigate whether timestep weighting can mitigate performance issues, particularly for the SD3-m model; we follow Section 3.1 and report upper-bound performance by fitting timestep weights on all data. Figure A.6: SELF-BENCH: Timestep reweighting helps address the cross-domain problem. We show the performance of each SD model on cross-domain data (e.g., for the SD1.5 model, the bars depict its performance on SD2.0 and SD3-m generations, averaged). All models benefit from timestep reweighting, with SD3-m benefiting the most. Our upper bound analysis in SELF-BENCH reveals that optimizing timestep weights improves performance across most tasks and models (Figure A.6). The improvements are particularly dramatic for SD3-m: in spatial tasks, accuracy increases from 33% to 95%, and in the two-objects task from 41% to 73%. While SD1.5 and SD2.0 also benefit from reweighting, showing improvements of 1-10% 25 across tasks, the gains are more modest compared to SD3-m. This suggests that SD3-ms lower baseline performance is not due to fundamental model limitations, but rather suboptimal weighting of timestep information. Contrary to previously argued uniform weighting or decaying weighting schemes, we find that timesteps near the end of the diffusion process (i.e., large t) tend to perform better for classification. This is in contrast to previous works [6, 19] that have argued for using uniform or exponentially decaying weights across timesteps. A.11 Later timesteps We explore whether further gains could be achieved with SD3 based on our prior analysis. Motivated by findings from timestep weighting, we tested simplified variant where only mid-to-late timesteps were used for discriminationspecifically in cases where either single-timestep accuracy or learned weights emphasized later timesteps. In conclusion, however, these simplified tests did not yield meaningful improvements. The results are presented in Table A.8. Table A.8: Results using later timesteps (sampling from [0.5, 1] for classification. Method/Dataset SELF-BENCH2.0 Single SELF-BENCH2.0 Counting"
        },
        {
            "title": "CLEVR Binding",
            "content": "SD3-m SD3-m (supervised)"
        },
        {
            "title": "Later Timesteps",
            "content": "0.87 0.91 0.90 0.57 0.72 0.66 0.30 0.42 0. 0.63 0.98 0.59 A.12 CLIP scores and domain gaps As an extension of Figure 9 in Section 5.4 of the main paper, Table A.9 shows CLIP embedding distances between real-world datasets and SELF-BENCH generations, and corresponding accuracy gains from timestep weighting. We can see the positive correlation in SD3 but not in SD1 and SD2. Table A.9: Timestep Weighting and Domain Gap. CLIP embedding distances between real-world datasets and SELF-BENCH generations, and corresponding accuracy gains from timestep weighting. Dataset CLIP Distance (SD 3-m) CLIP Distance (SD 1.5) CLIP Distance (SD2.0) Accuracy (SD3-m) Accuracy (SD2.0) Accuracy (SD1.5) COCO QA VQ QA SPEC Count WhatsUp WhatsUp CLEVR Binding CLEVR Spatial 3.494 3.666 2.646 4.254 4.656 5.64 5. Position Counting Attribute 5% 5% 2% 0% -1% 9% -3% A.13 Additional results 3.133 3.584 2.598 4.016 4.344 4.348 4. 5% 1% 0% 0% 4% 8% 2% 3.576 3.918 3.191 4.047 4.600 4.926 5.023 +4% +5% +5% +4% +12% +35% +16% Qualitative examples. Figure A.7 illustrates the domain differences between existing compositional benchmarks and SD3-m generated results, using prompts from SELF-BENCH, SPEC [33], and WhatsUP [22]. Figure A.8 presents additional examples from SELF-BENCH. Figure A.9 highlights failure cases of the Diffusion Classifier, even in in-domain scenarios. Compositional Benchmarks. Table A.10, A.11, A.12, A.13, A.14, A.15, A.16, A.17, A.18 and A.19 report quantitative results for Winoground [45], COLA [37], EQBench [51], VisMin [2], MMVP [46], ARO [56], CLEVR [21], SugarCrepe [15], WhatsUP [22], and SPEC [33], respectively. Figure A.10 provides an overview of results across all prior benchmarks considered in our study. SELF-BENCH. Table A.20 summarizes performance on SELF-BENCH. Given its high accuracy, one might suspect data bias in SELF-BENCH or selection bias in the model. To address this, we report both macro and micro accuracy in Figure A.11, which also provides comprehensive overview of performance on SELF-BENCH. Figure A.12 shows performance degradation in cross-domain settings. Table A.21, Table A.22, and Table A.23 present fine-grained performance across color, position, and counting tasks, respectively. 26 Compositional Benchmarks vs SELF-BENCH. Figure A.13 compares results from existing compositional benchmarks with those from SELF-BENCH, revealing clear domain shift across all tasks (see gray background). Figure A.7: Domain differences between existing compositional benchmarks and SD3-m generated results. (top) Examples from existing compositional benchmarks: SPEC [33] and WhatsUP [22]. (bottom left) SELF-BENCH examples. (bottom middle) Generated images using prompts from SPEC count tasks. (bottom right) Generated images using prompts from WhatsUPA tasks. 27 (a) SD 1. (b) SD 2.0 (c) SD 3-m Figure A.8: More examples from SELF-BENCH. Representative samples illustrating the range of tasks and model outputs used in our benchmark. 28 (a) SD 1.5 (b) SD 2. (c) SD 3-m Figure A.9: SELF-BENCH in-Domain prediction failure cases. Examples where the model fails despite being evaluated in an in-domain setting."
        },
        {
            "title": "Two\nTwo\nTwo\nTwo\nTwo\nTwo\nTwo\nTwo\nTwo",
            "content": "Table A.2: Comparison of 30 vs 100 Timesteps Performance GenEval Ver. Model Ver."
        },
        {
            "title": "100 Steps",
            "content": "1.5 1.5 1.5 2 2 2 3-m 3-m 3-m 1.5 1.5 1.5 2 2 2 3-m 3-m 3-m 1.5 1.5 1.5 2 2 2 1.5 1.5 1.5 2 2 2 3-m 3-m 3-m 1.5 1.5 1.5 2 2 2 3-m 3-m 3-m 1.5 1.5 1.5 2 2 2 3-m 3-m 3-m 84.00% 55.56% 55.56% 50.00% 83.33% 50.00% 55.16% 60.71% 93.25% 96.80% 89.50% 85.84% 87.07% 98.86% 88.97% 80.57% 90.13% 98.73% 75.51% 57.14% 47.96% 59.46% 92.79% 56.76% 66.67% 50.00% 33.33% 31.58% 73.68% 36.84% 30.97% 46.90% 82.30% 100.00% 98.89% 83.39% 98.89% 100.00% 89.67% 99.68% 99.36% 100.00% 91.43% 85.71% 52.38% 89.15% 97.67% 56.59% 87.91% 92.81% 96.41% 88.00% 55.56% 55.56% 55.56% 88.89% 59.72% 64.29% 68.65% 98.09% 99.09% 94.75% 94.75% 91.25% 99.62% 94.30% 84.87% 92.99% 99.68% 79.59% 62.76% 61.22% 68.47% 95.95% 68.47% 83.33% 41.67% 66.67% 26.32% 84.21% 44.74% 30.97% 45.58% 93.81% 99.63% 99.08% 91.81% 98.89% 100.00% 94.41% 100.00% 99.84% 100.00% 95.24% 90.00% 69.52% 92.25% 99.61% 78.29% 91.83% 93.63% 98.11% 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize) 1.5 2 3-m (no-resize)"
        },
        {
            "title": "Diff",
            "content": "4.00% 0.00% 0.00% 5.56% 5.56% 9.72% 9.13% 7.94% 4.84% 2.28% 5.25% 8.90% 4.18% 0.76% 5.32% 4.30% 2.87% 0.96% 4.08% 5.61% 13.27% 9.01% 3.15% 11.71% 16.67% -8.33% 33.33% -5.26% 10.53% 7.89% 0.00% -1.33% 11.50% -0.37% 0.18% 8.42% 0.00% 0.00% 4.74% 0.32% 0.48% 0.00% 3.81% 4.29% 17.14% 3.10% 1.94% 21.71% 3.92% 0.82% 1.71% (a) Spatial (b) Attribute (c) Counting (d) Object (e) Complex (Attributes, Relation) (f) size (g) action (h) presence Figure A.10: Overview of performance on compositional benchmarks beyond the four main categories. Summarizes model accuracy on additional compositional tasks, highlighting generalization beyond the core evaluation categories. The red dotted horizontal line represents the random chance level. 31 Table A.10: Image-to-text retrieval results on Winoground [45]."
        },
        {
            "title": "Both Object Relation",
            "content": "CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 224 224 224 224 224 512 512 0.27 0.31 0.28 0.34 0.32 0.32 0.20 0.36 0.3 0.34 0.34 0.46 0.81 0.58 0.58 0.54 0.54 0.35 0.54 0.54 0.42 0.46 0.32 0.36 0.28 0.40 0.36 0.33 0.27 0.39 0.33 0.33 0. cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.21 0.22 0.25 0.27 0.27 0.33 0.18 0.30 0.25 0.33 0.34 Table A.11: Image-to-Text retrieval accuracy on COLA [37]."
        },
        {
            "title": "Cola multi",
            "content": "CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 224 224 224 224 224 512 512 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.35 0.34 0.38 0.44 0.43 0.47 0.33 0.50 0.44 0.43 0.43 Table A.12: Image-to-text retrieval accuracy on EQbench [51]."
        },
        {
            "title": "Method",
            "content": "EQ-YouCook2 EQ-GEBC EQ-AG EQ-Kubric Attribute Counting"
        },
        {
            "title": "Location",
            "content": "EQ-SD CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 30 224 224 224 224 224 512 1024 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.6 0.55 0.4 0.7 0.8 0.5 0.5 0.55 0.55 0.4 0.35 0.1 0.25 0.15 0.2 0. 0.1 0.1 0.15 0.2 0.1 0.1 0.15 0.1 0.15 0.2 0.25 0.3 0.1 0.15 0.1 0.05 0.0 0.25 0.4 0.35 0.4 0.4 0.4 0.2 0.4 0.4 0.3 0.3 0.25 0.3 0.35 0.4 0. 0.25 0.1 0.15 0.15 0.05 0.05 0.0 0.0 0.0 0.0 0.0 0.15 0.0 0.15 0.05 0.05 0.05 0.9 0.85 0.85 0.95 0.9 0.9 0.8 0.9 0.75 0.6 0.55 Table A.13: Image-to-text retrieval accuracy on Vismin[2]."
        },
        {
            "title": "Relation Attribute Object Counting",
            "content": "CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 224 224 224 224 224 512 512 0.09 0.09 0.09 0.09 0.08 0.19 0.07 0.13 0.09 0.44 0.44 0.79 0.72 0.74 0.82 0.85 0.73 0.58 0.70 0.71 0.57 0.57 0.89 0.80 0.87 0.91 0.90 0.79 0.66 0.80 0.79 0.46 0. 0.37 0.31 0.37 0.65 0.65 0.36 0.13 0.39 0.31 0.26 0.26 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 32 Version CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m Table A.14: Image-to-Text retrieval accuracy on MMVP-VLM [46]. State Camera Perspective Color Orientation Presence Quantity Resolution Method Spatial Timesteps Structural Character - 30 30 30 224 224 224 224 224 512 1024 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.27 0.2 0.13 0.4 0.27 0.53 0.4 0.6 0.47 0.47 0. 0.67 0.53 0.33 0.6 0.8 0.47 0.4 0.73 0.67 0.67 0.73 0.2 0.07 0.0 0.27 0.33 0.07 0.0 0.13 0.27 0.33 0.2 0.27 0.07 0.07 0.27 0.13 0.2 0.13 0.2 0.07 0.0 0. 0.07 0.13 0.0 0.4 0.6 0.33 0.2 0.13 0.07 0.13 0.13 0.07 0.07 0.2 0.2 0.2 0.27 0.13 0.33 0.13 0.47 0.4 0.27 0.27 0.33 0.27 0.6 0.4 0.2 0.47 0.4 0.53 0. 0.07 0.4 0.27 0.53 0.6 0.47 0.13 0.27 0.2 0.27 0.27 Text 0.4 0.33 0.27 0.13 0.27 0.33 0.07 0.27 0.2 0.0 0."
        },
        {
            "title": "VG relation VG Attribution",
            "content": "Flickr30k order COCO order Table A.15: Image-to-Text retrieval accuracy on ARO [56]. CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 30 224 224 224 224 224 512 1024 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.51 0.51 0.53 0.50 0.51 0.52 0.62 0.50 0.58 0.48 0. 0.62 0.61 0.61 0.63 0.64 0.62 0.67 0.63 0.73 0.56 0.57 0.59 0.59 0.56 0.40 0.38 0.32 0.85 0.34 0.77 0.18 0.20 0.52 0.48 0.47 0.33 0.33 0.23 0.72 0.25 0.58 0.16 0. Table A.16: Image-to-text retrieval accuracy on CLEVR [21]. Version Timesteps Resolution Method All pair binding size pair binding color recognition color recognition shape spatial binding color shape binding shape color CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 224 224 224 224 224 512 512 0.60 0.64 0.64 0.67 0.65 0.66 0.73 0.69 0.73 0.56 0.56 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.35 0.50 0.70 0.66 0.60 0.67 0.81 0.61 0.66 0.52 0. 0.53 0.53 0.50 0.52 0.49 0.63 0.64 0.81 0.86 0.57 0.58 0.96 0.94 0.95 0.98 0.99 0.84 0.86 0.85 0.89 0.63 0.64 0.79 0.94 0.86 1.0 1.0 0.84 0.88 0.88 0.89 0.59 0. 0.51 0.54 0.51 0.52 0.50 0.49 0.70 0.51 0.63 0.59 0.59 0.53 0.50 0.49 0.50 0.49 0.55 0.55 0.57 0.59 0.51 0.51 0.51 0.51 0.49 0.50 0.49 0.57 0.59 0.59 0.61 0.51 0. Table A.17: Image-to-Text retrieval accuracy on SugarCrepe [15]."
        },
        {
            "title": "Method",
            "content": "attribute object relation CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2. SD 3-m - 30 30 30 224 224 224 224 512 512 1024 0.69 0.66 0.66 0.75 0.73 0.70 0.80 0.75 0.86 0.67 0.68 0.88 0.84 0.86 0.92 0. 0.85 0.67 0.87 0.87 0.72 0.72 0.71 0.69 0.65 0.72 0.73 0.66 0.59 0.68 0.76 0.58 0.60 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion Version Timesteps CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 30 Table A.18: Image-to-Text retrieval accuracy on WhatsUP [22]. Resolution Method WhatsUp WhatsUp COCO-spatial (one) COCO-spatial (two) GQA-spatial (one) GQA-spatial (two) 224 224 224 224 224 512 512 1024 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.34 0.31 0.27 0.26 0.30 0.28 0.23 0.27 0.25 0.28 0.31 0.24 0.31 0.26 0.27 0.26 0.27 0.32 0.28 0.21 0.37 0.37 0.45 0.44 0.49 0.45 0.48 0.48 0.69 0.42 0.59 0.54 0. 0.50 0.51 0.50 0.53 0.45 0.52 0.52 0.47 0.58 0.55 0.57 0.46 0.47 0.46 0.46 0.48 0.49 0.55 0.49 0.55 0.54 0.54 0.53 0.48 0.48 0.55 0.47 0.47 0.56 0.53 0.58 0.56 0. 33 Table A.19: Image-to-text retrieval accuracy on SPEC [33]. Version Timesteps Resolution Method Absolute Size Absolute Spatial Count Existence Relative Size Relative Spatial CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m - 30 30 30 224 224 224 224 224 512 1024 cosine-sim zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.35 0.42 0.37 0.41 0.37 0.39 0.33 0.43 0.33 0.37 0. 0.12 0.13 0.12 0.13 0.14 0.15 0.11 0.12 0.11 0.24 0.14 0.30 0.25 0.29 0.42 0.47 0.20 0.12 0.23 0.12 0.18 0.14 0.57 0.58 0.58 0.57 0.55 0.56 0.52 0.55 0.52 0.52 0. 0.31 0.34 0.32 0.33 0.32 0.34 0.33 0.33 0.33 0.34 0.33 0.29 0.28 0.29 0.28 0.30 0.30 0.26 0.29 0.26 0.43 0.32 (a) Average per sample on SELF-BENCH. (b) Macro average on SELF-BENCH. Figure A.11: Micro accuracy (Top) and Macro accuracy (Bottom) on SELF-BENCH. We evaluate CLIP and SD models in our SELF-BENCH. The X-axis represents the model used for generating the dataset, with the number of images for each dataset indicated below. The results clearly show that models perform well only when evaluated in-domain, not cross-domain. The red dotted horizontal line represents the random chance level. Figure A.12: SELF-BENCH cross-domain drop rate. WE show the performance drop when evaluating models in cross-domain settings. 34 Table A.20: Complete image-to-text retrieval accuracy results on SELF-BENCH across all tasks. Bold entries indicate in-domain evaluations for the model. Version Timesteps Resolution Method Single Object Correct Full Two Objects Full Correct CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/ SD 1.5 (in-domain) SD 2.0 (cross-domain) - - - - - 30 30 SD 3-m (cross-domain) 30 CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 2.0 (in-domain) SD 1.5 (cross-domain) - - - - - 30 SD 3-m (cross-domain) 30 CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 3-m (in-domain) SD 1.5 (cross-domain) SD 2.0 (cross-domain) - - - - - 30 30 30 224 224 224 224 512 512 512 1024 224 224 224 224 224 512 512 1024 224 224 224 224 224 1024 512 0.97 0.96 0.97 0.97 0.97 320 0.98 0.88 0.96 0.96 0.82 0.82 0.78 0.80 0.99 1.0 0.99 1.0 1. 320 1.0 0.99 0.99 0.79 0.89 0.89 0.86 0.87 0.99 1.0 0.99 1.0 1.0 320 1.0 1.0 1.0 0. 0.99 0.98 cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion 0.99 0.99 0.99 1.0 0.99 271 1.0 0.86 0.99 0.98 0.86 0.87 0.81 0.83 1.0 1.0 1.0 1.0 1. 271 1.0 1.0 0.99 0.76 0.91 0.91 0.87 0.88 0.99 1.0 0.99 1.0 1.0 314 1.0 1.0 1.0 0. 0.99 0.98 0.46 0.46 0.54 0.52 0.51 396 0.69 0.36 0.48 0.47 0.26 0.24 0.29 0.29 0.61 0.54 0.64 0.69 0. 396 0.82 0.78 0.61 0.28 0.35 0.34 0.40 0.41 0.90 0.86 0.95 0.95 0.95 396 0.98 0.98 0.87 0. 0.91 0.88 0.85 0.87 0.95 0.95 0.94 105 0.90 0.59 0.85 0.83 0.50 0.50 0.57 0.57 0.91 0.85 0.93 0.99 0. 129 0.98 0.98 0.90 0.62 0.57 0.56 0.63 0.66 0.91 0.89 0.98 0.97 0.98 306 0.98 0.99 0.88 0. 0.92 0.90 Colors Full 0.86 0.87 0.87 0.89 0.87 376 0.93 0.75 0.82 0. 0.68 0.68 0.88 0.82 0.93 0.92 0.91 0.94 0.94 376 0.97 0.92 0.85 0.62 0.78 0.80 0.87 0.87 0.89 0.88 0.89 0.91 0. 376 0.97 0.98 0.78 0.60 0.82 0.82 Correct 0.94 0.94 0.94 0.97 0.97 219 0.98 0. 0.91 0.91 0.75 0.74 0.88 0.88 0.95 0.94 0.93 0.97 0.98 263 0.98 0.97 0.89 0.60 0.80 0.81 0.90 0. 0.92 0.91 0.91 0.96 0.95 314 0.98 0.98 0.82 0.61 0.87 0.85 Color Attribution Correct Full 0.28 0.25 0.29 0.31 0. 400 0.56 0.26 0.26 0.24 0.16 0.17 0.26 0.26 0.30 0.35 0.28 0.44 0.45 400 0.70 0.56 0.37 0. 0.19 0.20 0.28 0.27 0.38 0.43 0.34 0.47 0.51 400 0.91 0.91 0.53 0.27 0.56 0.52 0.28 0.22 0.17 0.5 0. 18 0.83 0.5 0.5 0.56 0.44 0.44 0.56 0.56 0.47 0.47 0.44 0.53 0.69 36 0.88 0.83 0.42 0. 0.39 0.42 0.53 0.58 0.40 0.43 0.36 0.49 0.55 252 0.95 0.95 0.58 0.25 0.63 0.57 Position Counting Full 0.33 0.24 0.31 0.30 0.34 400 0.49 0.36 0.36 0.35 0.31 0.31 0.29 0. 0.30 0.22 0.26 0.44 0.38 400 0.63 0.61 0.28 0.21 0.33 0.32 0.30 0.31 0.26 0.28 0.32 0.36 0.37 400 0.72 0. 0.32 0.25 0.33 0.38 Correct 0.17 0.67 0.5 0.33 0.67 6 0.67 0.33 0.67 0. 0.83 0.83 0.33 0.33 0.26 0.16 0.26 0.37 0.42 19 0.84 0.89 0.26 0.32 0.63 0.63 0.42 0.47 0.27 0.31 0.30 0.33 0. 113 0.89 0.91 0.30 0.28 0.34 0.38 Full 0.47 0.52 0.49 0.48 0.49 320 0.65 0. 0.46 0.40 0.39 0.40 0.37 0.37 0.50 0.50 0.52 0.53 0.53 320 0.78 0.64 0.49 0.45 0.46 0.48 0.45 0. 0.66 0.61 0.68 0.84 0.84 320 0.85 0.86 0.57 0.47 0.57 0.56 Correct 0.67 0.63 0.63 0.85 0. 98 0.76 0.46 0.49 0.59 0.44 0.46 0.53 0.52 0.77 0.60 0.69 0.93 0.95 111 0.95 0.89 0.59 0. 0.51 0.51 0.57 0.57 0.7 0.65 0.75 0.97 0.96 230 0.91 0.92 0.60 0.55 0.61 0.65 Table A.21: Fine-grained accuracy on SELF-BENCH Colors. Version Timesteps Resolution Method CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1. SD 2.0 SD 3-m CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 2.0 SD 1.5 SD 3-m CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 3-m SD 1.5 SD 2.0 - - - - - 30 30 - - - - - 30 30 - - - - - 30 30 30 224 224 224 224 224 512 1024 224 224 224 224 224 512 512 224 224 224 224 224 1024 512 512 cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion Red Correct 1.00 1.00 0.97 1.00 1.00 31 1.0 0.94 0.94 0.97 0.74 0.74 0.94 0.89 0.86 0.89 0.94 35 0.97 0. 0.69 0.77 0.74 0.74 0.85 0.87 0.92 0.95 0.95 39 0.92 0.92 0.77 0.87 0.79 0. Full 0.98 0.92 0.94 1.00 0.96 52 0.98 0.92 0.81 0.81 0.71 0.71 0.96 0.90 0.87 0.92 0. 52 0.94 0.92 0.67 0.81 0.73 0.73 0.73 0.81 0.94 0.81 0.83 52 0.92 0.94 0.65 0. 0.69 0.73 Orange Yellow Full 0.82 0.82 0.82 0.93 0.96 28 0.96 0. 0.93 0.71 0.79 0.75 0.86 0.86 0.86 0.93 0.96 28 1.0 0.96 0.96 0.68 0.96 0. 0.96 0.82 0.93 1.00 0.93 28 0.96 0.96 0.82 0.57 0.89 0.82 Correct 0.67 0.56 0.56 0.89 1. 9 0.89 0.78 0.89 1.0 0.67 0.67 0.78 0.78 0.78 0.89 0.94 18 1.0 1.0 0.94 0. 0.89 0.89 0.96 0.81 0.92 1.00 0.92 26 0.96 0.96 0.81 0.46 0.88 0.81 Full 0.77 0.82 0.68 0.70 0.70 44 0.93 0.95 0.68 0.64 0.82 0.84 0.91 0.91 0.80 0.84 0.82 44 0.93 0. 0.84 0.89 0.84 0.84 0.91 0.91 0.84 0.89 0.84 44 1.0 1.0 0.80 0.93 0.75 0. Correct 1.00 0.96 0.92 0.96 0.96 26 1.0 1.0 0.88 0.88 1.0 1.0 0.97 0.97 0.94 0.97 0. 35 1.0 0.94 0.94 0.97 0.86 0.89 0.89 0.89 0.84 0.89 0.84 38 1.0 1.0 0.84 1. 0.82 0.76 Full 1.00 0.98 1.00 0.98 0.98 44 0.98 0.80 0.95 0.95 0.95 0. 1.00 1.00 1.00 1.00 1.00 44 0.95 0.95 0.95 0.52 1.0 1.00 0.98 0.91 0.91 0.95 0.93 44 1.0 1. 0.82 0.59 0.86 0.82 Green Correct 1.00 1.00 1.00 1.00 1.00 29 1.0 0. 0.97 0.97 1.0 1.0 1.00 1.00 1.00 1.00 1.00 36 0.94 0.92 0.94 0.39 1.0 1. 1.00 1.00 1.00 1.00 1.00 34 1.0 1.0 0.88 0.56 0.88 0.85 35 Blue Correct 1.00 1.00 0.95 1.00 1.00 22 0.95 0.91 0.86 0.86 0.95 0.95 1.00 1.00 0.97 0.97 1. 34 1.0 1.0 0.88 0.82 1.0 1.0 1.00 1.00 1.00 1.00 1.00 39 1.0 1.0 0.82 0. 0.90 0.90 Full 0.85 0.88 0.82 0.88 0.85 40 0.95 0.80 0.78 0.75 0.85 0. 1.00 0.97 0.95 0.95 0.97 40 0.98 0.93 0.85 0.83 0.98 0.98 1.00 1.00 1.00 1.00 1.00 40 1.0 1. 0.83 0.83 0.90 0.93 Purple Full 0.90 0.88 0.93 0.90 0.90 40 0.98 0. 0.83 0.80 0.9 0.90 1.00 0.97 1.00 0.97 0.97 40 0.98 0.95 0.90 0.48 0.95 0. 0.97 1.00 0.93 0.93 1.00 40 1.0 1.0 0.98 0.70 0.90 0.98 Correct 1.00 0.95 1.00 0.95 1. 22 1.0 0.91 0.86 0.82 1.0 1.0 1.00 1.00 1.00 1.00 1.00 22 1.0 1.0 0.95 0. 1.0 1.0 0.97 1.00 0.92 0.92 1.00 36 1.0 1.0 0.97 0.58 0.89 0.86 Full 0.83 0.79 0.88 0.88 0.92 24 0.88 0.75 0.88 0.88 0.88 0.88 1.00 1.00 1.00 1.00 1.00 24 1.0 1. 0.92 0.79 0.96 0.96 1.00 0.96 1.00 1.00 1.00 24 1.0 1.0 0.96 0.67 0.92 0. Pink Correct 0.93 0.87 1.00 1.00 1.00 15 1.0 0.87 1.0 0.93 1.0 1. 1.00 1.00 1.00 1.00 1.00 12 1.0 0.92 0.92 0.92 0.92 0.92 1.00 0.94 1.00 1.00 1.00 18 1.0 1. 1.0 0.78 0.94 0.89 Brown Full 0.82 0.82 0.89 0.86 0.86 28 0.68 0. 0.64 0.71 0.79 0.82 0.82 0.75 0.79 0.89 0.86 28 0.89 0.82 0.71 0.21 0.82 0. 0.68 0.64 0.68 0.71 0.68 28 0.96 0.96 0.68 0.18 0.68 0.68 Correct 0.92 0.85 1.00 1.00 1. 13 0.92 0.46 0.77 0.92 0.85 0.85 0.94 0.81 0.88 0.94 0.94 16 0.94 1.0 0.88 0. 0.88 0.81 0.77 0.73 0.77 0.82 0.77 22 1.0 1.0 0.77 0.23 0.77 0.82 Full 0.84 0.93 0.91 0.86 0.82 44 0.95 0.52 0.86 0.86 0.59 0.64 0.80 0.93 0.86 0.98 0.93 44 1.0 0. 0.84 0.41 0.74 0.70 0.91 0.86 0.89 0.98 1.00 44 0.98 0.98 0.64 0.25 0.89 0. Black Correct 0.91 0.94 0.88 0.88 0.88 33 0.97 0.52 0.88 0.82 0.61 0. 0.84 0.91 0.88 1.00 0.97 32 1.0 0.97 0.88 0.28 0.78 0.78 0.92 0.89 0.89 1.00 1.00 38 0.93 0. 0.68 0.34 0.95 0.89 Full 0.62 0.75 0.81 0.88 0.78 32 0.91 0.53 0.81 0. 0.91 0.91 0.88 0.84 0.97 0.97 0.94 32 1.0 0.94 0.88 0.50 0.88 0.88 0.75 0.88 0.78 0.91 0. 32 0.91 0.91 0.75 0.34 0.78 0.84 White Correct Macro Average Correct Full 0.74 0.89 0.89 1.00 0.95 19 1.0 0.74 1.0 1.0 1.0 1.0 0.96 1.00 1.00 1.00 1.00 23 1.0 1. 0.96 0.39 1.0 1.0 0.79 0.88 0.79 1.00 1.00 24 1.0 1.0 0.71 0.33 0.83 0. 0.84 0.86 0.87 0.89 0.87 - 0.92 0.72 0.82 0.8 0.82 0.83 0.92 0.91 0.91 0.95 0.94 - 0.97 0. 0.85 0.61 0.89 0.88 0.89 0.88 0.89 0.92 0.91 - 0.97 0.97 0.79 0.58 0.83 0. 0.92 0.90 0.92 0.97 0.98 - 0.97 0.77 0.91 0.92 0.88 0.89 0.94 0.94 0.93 0.97 0.98 - 0.99 0. 0.90 0.59 0.91 0.90 0.91 0.90 0.91 0.96 0.95 - 0.98 0.98 0.83 0.59 0.86 0. Table A.22: Fine-grained accuracy on SELF-BENCH Position . right of Full 0.37 0.19 0.43 0.43 0.41 108 0.44 0.25 0.39 0. 0.27 0.27 0.29 0.17 0.37 0.62 0.44 108 0.73 0.52 0.30 0.10 0.33 0.31 0.27 0.19 0.51 0.56 0. 108 0.66 0.66 0.30 0.20 0.29 0.27 Correct 0.00 0.00 0.00 0.00 0.00 0 0.00 0. 0.00 0.0 0.0 0.0 0.00 0.00 0.00 0.33 0.67 3 0.67 1.0 0.33 0.0 0.67 0. 0.47 0.07 0.47 0.73 0.60 15 0.93 1.0 0.13 0.0 0.40 0.27 Full 0.38 0.40 0.29 0.19 0. 104 0.46 0.42 0.46 0.38 0.29 0.29 0.29 0.38 0.18 0.25 0.26 104 0.61 0.53 0.21 0. 0.25 0.24 0.38 0.65 0.27 0.27 0.33 104 0.72 0.71 0.33 0.39 0.43 0.48 above Correct 0.50 0.50 0.50 0.50 0.50 2 1.00 1.0 0.50 0.5 0.5 0.5 0.50 0.25 0.38 0.50 0. 8 0.88 0.88 0.38 0.50 0.38 0.75 0.27 0.66 0.41 0.32 0.36 44 0.86 0.86 0.36 0. 0.43 0.50 Full 0.25 0.20 0.23 0.21 0.29 112 0.56 0.36 0.26 0.17 0.32 0. 0.29 0.17 0.22 0.35 0.32 112 0.63 0.46 0.29 0.21 0.23 0.27 0.23 0.13 0.29 0.36 0.30 112 0.80 0. 0.37 0.18 0.38 0.29 below Correct Macro Average Correct Full 0.00 1.00 0.67 0.33 1. 3 0.67 0.0 1.00 0.0 0.33 0.33 0.14 0.14 0.29 0.14 0.14 7 1.0 0.86 0.14 0. 0.29 0.57 0.19 0.11 0.22 0.33 0.31 36 0.92 0.92 0.31 0.06 0.33 0.31 0.33 0.23 0.31 0.30 0. - 0.48 0.36 0.36 0.37 0.29 0.29 0.30 0.22 0.26 0.45 0.39 - 0.62 0.56 0.28 0. 0.30 0.31 0.25 0.27 0.31 0.35 0.37 - 0.72 0.72 0.32 0.25 0.32 0.39 0.12 0.38 0.29 0.21 0. - 0.42 0.25 0.38 0.38 0.21 0.21 0.16 0.10 0.17 0.49 0.58 - 0.64 0.93 0.21 0. 0.58 0.66 0.29 0.22 0.29 0.35 0.36 - 0.90 0.93 0.27 0.21 0.30 0.35 Version Timesteps Resolution Method CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 2.0 SD 1.5 SD 3-m CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 3-m SD 1.5 SD 2.0 - - - - - 30 30 - - - - - 30 30 30 - - - - - 30 30 224 224 224 224 224 512 512 224 224 224 224 224 512 512 1024 224 224 224 224 224 512 512 cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion Full 0.33 0.14 0.28 0.38 0.41 76 0.47 0.42 0.34 0. 0.28 0.29 0.33 0.14 0.28 0.57 0.55 76 0.53 0.72 0.34 0.22 0.39 0.43 0.13 0.09 0.16 0.20 0. 76 0.68 0.71 0.28 0.22 0.17 0.51 left of Correct 0.00 0.00 0.00 0.00 0. 1 0.00 0.0 0.00 1.0 0.0 0.0 0.00 0.00 0.00 1.00 1.00 1 0.0 1.0 0.0 0. 1.0 1.0 0.22 0.06 0.06 0.00 0.17 18 0.89 0.94 0.28 0.17 0.06 0.33 Table A.23: Fine-grained accuracy on SELF-BENCH Counting . two Correct 0.78 0.71 0.71 0.86 0.86 51 0.78 0.06 0.55 0. 0.53 0.51 0.87 0.64 0.69 0.93 0.94 70 0.89 1.00 0.60 0.03 0.5 0.53 0.82 0.80 0.75 0.96 0. 84 0.92 0.94 0.62 0.13 0.64 0.79 Full 0.58 0.56 0.47 0.62 0.56 104 0.74 0. 0.51 0.61 0.38 0.38 0.69 0.55 0.59 0.74 0.75 104 0.94 0.88 0.57 0.05 0.48 0. 0.73 0.69 0.67 0.88 0.89 104 0.89 0.90 0.54 0.09 0.58 0.66 Full 0.32 0.31 0.43 0.37 0. 108 0.49 0.86 0.31 0.20 0.41 0.44 0.37 0.23 0.44 0.34 0.31 108 0.91 0.37 0.37 0. 0.43 0.33 0.50 0.36 0.73 0.82 0.79 108 0.78 0.80 0.47 0.91 0.32 0.38 three Correct 0.55 0.50 0.55 0.79 0.84 37 0.68 0.95 0.38 0.32 0.54 0.54 0.58 0.27 0.62 0.88 0. 23 0.64 0.61 0.48 0.91 0.61 0.39 0.52 0.40 0.79 0.95 0.94 83 0.86 0.87 0.52 0. 0.37 0.43 four Correct Macro Average Correct Full 0.56 0.78 0.56 1.00 1.00 9 1.0 0. 0.67 0.33 0.56 0.56 0.67 1.00 0.87 1.00 1.00 15 0.83 0.87 0.80 0.60 0.8 0. 0.78 0.80 0.70 0.98 0.97 60 0.98 0.98 0.70 0.55 0.90 0.77 0.35 0.39 0.37 0.37 0.37 - 0.61 0. 0.41 0.43 0.41 0.41 0.38 0.38 0.39 0.40 0.40 - 0.96 0.67 0.37 0.58 0.46 0. 0.50 0.46 0.51 0.63 0.63 - 0.82 0.83 0.61 0.59 0.56 0.60 0.47 0.50 0.45 0.66 0.68 - 0.86 0. 0.40 0.37 0.41 0.40 0.53 0.48 0.54 0.70 0.73 - 0.78 0.79 0.47 0.55 0.65 0. 0.53 0.50 0.56 0.73 0.72 - 0.86 0.86 0.63 0.66 0.65 0.67 Full 0.51 0.69 0.58 0.48 0. 104 0.72 0.54 0.57 0.40 0.33 0.30 0.44 0.75 0.55 0.51 0.55 104 1.00 0.66 0.55 0. 0.44 0.62 0.75 0.79 0.64 0.83 0.85 104 0.88 0.88 0.68 0.38 0.83 0.63 Version Timesteps Resolution Method CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 1.5 SD 2.0 SD 3-m CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 2.0 SD 1.5 SD 3-m CLIP RN50x64 CLIP ViT-B/32 CLIP ViT-L/14 openCLIP ViT-H/14 openCLIP ViT-G/14 SD 3-m SD 1.5 SD 2.0 - - - - - 30 30 - - - - - 30 30 30 - - - - - 30 30 224 224 224 224 224 512 512 224 224 224 224 224 512 512 1024 224 224 224 224 224 512 512 cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion cosine-sim # of samples zero-shot discffusion zero-shot discffusion zero-shot discffusion Full 0.00 0.00 0.00 0.00 0.00 4 0.50 0.75 0.25 0. 0.5 0.50 0.00 0.00 0.00 0.00 0.00 4 1.00 0.75 0.0 1.00 0.5 0.00 0.00 0.00 0.00 0.00 0. 4 0.75 0.75 0.75 1.00 0.50 0.75 one Correct 0.00 0.00 0.00 0.00 0. 1 1.0 1.0 0.0 0.0 0.0 0.0 0.00 0.00 0.00 0.00 0.00 3 0.75 0.67 0.0 0. 0.67 0.0 0.00 0.00 0.00 0.00 0.00 3 0.67 0.67 0.67 1.0 0.67 0.67 (a) Spatial (b) Attribute (c) Counting (d) Object Figure A.13: Additional results on compositional benchmarks and SELF-BENCH. The detailed breakdown of our analysis within four categories: (a) Object, (b) Attribute, (c) Position, and (d) Counting. Results on white background correspond to performance across ten existing compositional benchmarks (33 sub-tasks), while those on gray background represent results on SELF-BENCH.The red dotted horizontal line represents the random chance level."
        }
    ],
    "affiliations": [
        "TU Darmstadt & hessian.AI",
        "Tübingen AI Center & University of Tübingen"
    ]
}