{
    "paper_title": "Learning 3D Representations from Procedural 3D Programs",
    "authors": [
        "Xuweiyi Chen",
        "Zezhou Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-supervised learning has emerged as a promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple primitives and augmentations. Remarkably, despite lacking semantic content, the 3D representations learned from this synthesized dataset perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, including shape classification, part segmentation, and masked point cloud completion. Our analysis further suggests that current self-supervised learning methods primarily capture geometric structures rather than high-level semantics."
        },
        {
            "title": "Start",
            "content": "Learning 3D Representations from Procedural 3D Programs"
        },
        {
            "title": "Zezhou Cheng",
            "content": "University of Virginia https://point-mae-zero.cs.virginia.edu/ 4 2 0 2 5 2 ] . [ 1 7 6 4 7 1 . 1 1 4 2 : r Figure 1. Main idea and key findings. We learn 3D representations with Point-MAE [19] on two distinct datasets: (a) ShapeNet [3], which provides semantically meaningful 3D models, and (b) procedurally generated 3D shapes [34, 37, 38] that lack semantic structure. We refer to models trained on ShapeNet as Point-MAE-SN and those trained on procedurally generated shapes as Point-MAE-Zero. In (c), the x-axis represents various tasks and benchmarks: ModelNet40 [15] and three variants of ScanObjectNN [25] for shape classification, and ShapeNetPart [41] for part segmentation. Surprisingly, Point-MAE-Zero performs comparably to Point-MAE-SN on ModelNet40 [15] and even outperforms it on the three variants of ScanObjectNN [25] and on part segmentation. Both Point-MAE-Zero and Point-MAESN significantly outperform training from scratch. In (d), we show that pretrained Point-MAE-Zero can perform masked point cloud reconstruction similarly to Point-MAE-SN, without requiring fine-tuning."
        },
        {
            "title": "Abstract",
            "content": "Self-supervised learning has emerged as promising approach for acquiring transferable 3D representations from unlabeled 3D point clouds. Unlike 2D images, which are widely accessible, acquiring 3D assets requires specialized expertise or professional 3D scanning equipment, making it difficult to scale and raising copyright concerns. To address these challenges, we propose learning 3D representations from procedural 3D programs that automatically generate 3D shapes using simple primitives and augmentations. Remarkably, despite lacking semantic content, the 3D representations learned from this synthesized dataset perform on par with state-of-the-art representations learned from semantically recognizable 3D models (e.g., airplanes) across various downstream 3D tasks, including shape classification, part segmentation, and masked point cloud completion. Our analysis further suggests that current selfsupervised learning methods primarily capture geometric structures rather than high-level semantics. 1. Introduction Self-supervised learning (SSL) aims at learning representations from unlabeled data that can transfer effectively to various downstream tasks. Inspired by the success of SSL in language [8] and image representation learning [10, 11], SSLs for 3D point cloud understanding have recently gained considerable interest [19, 27, 42]. For instance, PointMAE [19] introduces the masked autoencoding scheme into point cloud representation learning, showing substantial improvements in various 3D shape understanding tasks (e.g., shape classification and segmentation). However, unlike language and image data, which are abundantly available, 3D assets are less accessible due to the expertise required for creating 3D shapes using specialized software (e.g., Blender) or professional 3D scanning equipment. This scarcity of 3D shapes limits the scalability of existing representation learning methods. While recent efforts aim to expand 3D object datasets [6, 7], challenges unique to 3D data collection such as copyright issues and format diversity remain unresolved. To address these challenges, we explore learning point cloud representations from solely synthetic data generated via procedural 3D programs [34, 37, 38] examples are shown in Fig. 1b. Our data generation pipeline begins with sampling shapes from set of simple 3D primitives (e.g., cubes, cylinders, and spheres). These primitives undergo affine transformations (e.g., scaling, translation, and rotation) and are combined to create diverse geometries. We 1 then augment the composed shapes using predefined operations (e.g., Boolean operations) to further enhance topological diversity, and uniformly sample 3D surface points from these shapes for point cloud representation learning. We generate total of 150K synthetic 3D point clouds, costing around 600 CPU hours, though this pipeline can theoretically produce an unlimited number of 3D shapes without copyright concerns. This leads us to two key questions: Can existing self-supervised learning methods effectively capture rich and meaningful 3D representations from procedurally generated shapes alone? How do these representations compare to those learned from human-crafted, semantically rich 3D models? To answer these questions, we leverage Point-MAE to learn representations from purely synthetic 3D shapes. We call this framework Point-MAE-Zero to emphasize that it does not use any human-made 3D shapes. We evaluate Point-MAE-Zero and conduct extensive comparisons with Point-MAE pretrained on ShapeNet (referred to as PointMAE-SN) across several downstream tasks, including shape classification, part segmentation, and masked point cloud reconstruction. Our key findings are as follows: Surprisingly, despite the lack of semantics in the synthetic dataset, Point-MAE-Zero performs comparably to PointMAE-SN across various 3D tasks, as shown in Fig. 1c; Point-MAE-Zero and Point-MAE-SN exhibit similar behavior patterns. Both models can reconstruct masked point clouds regardless of their training domain Point-MAE-Zero accurately predicts missing points in ShapeNet shapes (e.g., airplanes and chairs) as shown in Fig. 1d, while Point-MAE-SN can reconstructs synthetic shapes, as seen in Fig. 3. The two models also share structural similarities in their latent spaces, illustrated in Fig. 7; Point-MAE-Zero demonstrates improved performance with increased geometric diversity and dataset size in the synthetic dataset, as illustrated in Tab. 4 and Fig. 5. Our work is inspired by and builds upon recent study that successfully trained large 3D reconstruction models exclusively on procedurally generated shapes [34]. Our exploration is also closely related to prior efforts that learn image representations from procedural programs [1, 2]. Concurrent to our work, Yu et al. [43] demonstrated that procedurally generated videos can perform as effectively as natural videos for self-supervised video representation learning. 2. Related Work 3D Object Datasets. Large-scale datasets are essential in advancing 3D deep learning. Unlike 2D images or videos, building and annotating 3D models requires expertise with professional 3D software or scanning equipment, making the process more costly and time-intensive. Despite these challenges, significant efforts have been made to curate extensive 3D shape datasets [3, 5, 6, 9, 18, 22, 23, 29, 32]. For example, ShapeNet provides 3 million CAD models, with 51K of them being clean, high-quality models. More recently, Objaverse-XL [7] expanded the 3D dataset to 10.2 million models, though scaling up has introduced challenges, including increased noise, format diversity, and unresolved copyright and legal issues. In contrast, we explore procedural 3D programs that generate 3D shapes from simple primitives. This synthetic approach can theoretically produce an unlimited number of 3D shapes without licensing concerns. Supervised Learning for 3D Point Clouds. Unlike 2D images, 3D point clouds are inherently unordered and have an irregular structure, leading to extensive research on specialized neural network architectures [13, 20, 21, 28, 30, 31, 36, 44]. For instance, PointNet [20] introduces permutation-invariant operators and pooling layers to effectively aggregate features across 3D points. PointNet++ [21] builds on this by incorporating hierarchical spatial structure to capture more localized geometric information. DGCNN [28] adopts graph-based approach, constructing graph from input point clouds and applying graph CNN to aggregate features from unordered 3D points. More recent models, such as the Point Transformer [30, 31, 44], utilize modified transformer architecture tailored specifically for 3D point cloud processing. In our work, we employ standard transformer architecture, aligning with recent trends in self-supervised learning [11, 19, 42]. Self-supervised Learning for Point Clouds. Selfsupervised learning (SSL) for 3D representations aims to learn features that transfer well across diverse point cloud tasks, such as shape classification, object detection, and part segmentation. Recent SSL approaches for point clouds generally fall into two categories: contrastive learning [4, 10, 35] and masked autoencoding [19, 27, 42]. PointContrast [35] and DepthContrast [4] use an instance discrimination task [10] to learn 3D representations. OcCo [27] learns point cloud representations by reconstructing the original point cloud from occluded views, while IAE [39] trains an autoencoder to recover implicit features from point cloud inputs. Inspired by the success of masked autoencoding in vision [11] and language representation learning [8], PointBERT [42] and Point-MAE [19] learn representations by predicting masked regions, using transformers as the underlying architecture. In this work, we adopt Point-MAE as our self-supervised learning method due to its state-of-theart performance in various 3D understanding tasks. Learning from Synthetic Data Synthetic data has become popular in computer vision, especially in scenarios 2 Figure 2. Point-MAE-Zero. (a) Our synthetic 3D point clouds are generated by sampling, compositing, and augmenting simple primitives with procedural 3D programs [34]. (b) We use Point-MAE [42] as our pretraining framework to learn 3D representation from synthetic 3D shapes, dubbed Point-MAE-Zero where Zero underscores that we do not use any human-made 3D shapes. (c) We evaluate Point-MAEZero in various 3D shape understanding tasks. where ground-truth annotations are difficult to obtain or where privacy and copyright issues arise. State-of-theart performance in mid-level or 3D vision tasks is often achieved through training on synthetic data, including tasks like optical flow [24], depth estimation [40], dense tracking [12], relighting [37], novel view synthesis [38], and material estimation [14]. Procedurally generated synthetic data has also been explored for self-supervised representation learning in images [1, 2] and videos [43], and more recently for multi-view feed-forward 3D reconstruction [34]. Concurrent with our study, Yu et al. [43] investigates selfsupervised video representation learning from procedurally generated images and videos. In this work, we explore selfsupervised representation learning for point clouds, using synthetic 3D shapes generated by procedural 3D programs. 3. Learning from Procedural 3D Programs We first introduce the procedural 3D programs [34, 37, 38] for generating unlimited number of synthetic 3D shapes using composition of simple primitive shapes (e.g., cylinders) and shape augmentation (Sec. 3.1). We then describe the masked autoencoding scheme [19, 27, 42] for learning 3D representations from synthetic 3D datasets (Sec. 3.2). 3.1. Procedural 3D programs There is line of works that synthesize 3D shapes using procedural 3D programs for vision tasks, such as novel view synthesis [38], relighting [37], and material estimation [14]. We closely follow the recent work [34] that collects procedurally synthesized 3D shapes for training sparse-view 3D reconstruction models, while we tackle different task in this wrok self-supervised 3D representation learning from purely synthetic 3D datasets. Fig. 2a illustrates our data collection pipeline: (1). The procedural 3D program randomly samples shapes from set of simple primitive shapes, including cubes, spheres, cylinders, cones, and tori. These primitives are transformed with affine transformations and combined; (2). To enrich geometric diversity, the composed shapes undergo various predefined manipulations, such as boolean difference and wireframe conversion. For more details on shape augmentation, refer to Xie et al. [34]; (3). We uniformly sample surface points from the synthesized shapes, which serve as input to the model for representation learning, as illustrated in Fig. 2b. We explore different configurations for shape generation, such as varying the maximum number of primitives sampled in step (1) and including or excluding shape augmentation operations in step (2). By default, each dataset configuration includes 150K generated shapes, with = 8192 surface points per shape. detailed analysis of the impact of dataset size and shape complexity on 3D representation learning is provided in Sec. 4. 3.2. Point-MAE-Zero Pretraining. As depicted in Fig. 2b, we use PointMAE[11], state-of-the-art method for point cloud representation learning, to train on synthesized 3D shapes. Point-MAE learns 3D representations through masked autoencoding scheme [8, 11, 42]. Specifically, it segments the input point cloud into irregular point patches and randomly masks large proportion of them (60% by default). Shape representations are then learned using transformerbased encoder-decoder by reconstructing the masked point patches. The reconstruction loss is computed as the L2 Chamfer Distance between the predicted point patches Ppre and the ground-truth patches Pgt: = 1 Ppre (cid:88) aPpre min bPgt b2 2 + 1 Pgt (cid:88) bPgt min aPpre b2 2 (1) Similar to other self-supervised learning methods [27, 42], Point-MAE is typically trained on human-designed 3D shapes with semantic content (e.g., airplanes and chairs 3 In this work, we explore selffrom ShapeNet [3]). supervised learning with procedural 3D programs, referred to as Point-MAE-Zero. Here, Zero highlights that our 3D representations are learned entirely from procedurally generated 3D shapes, with zero human involvement in the generation process beyond the initial programming. Downstream Probing. We evaluate Point-MAE-Zero in various downsteam 3D understanding tasks, as shown in Fig. 2c. For classification tasks, we augment the pretrained transformer encoder with three-layer MLP, which serves as the classification head. For part segmentation tasks, We aggregate features from the 4th, 8th, and final layers of the Transformer encoder, and upsample the features to obtain the features for all 2048 input points. We include more implementation details in the supplementary material, and refer to Point-MAE [19] for more implementation details. For masked point cloud reconstruction, we use both pretrained transformer encoder and decoder, with any architecture modification. Implementation details. We closely follows PointMAE [19] for the training of Point-MAE-Zero. Specifically, input point cloud with = 1024 points is divided into = 64 patches, with each patch containing constant number of points using = 32 in the KNN algorithm. The autoencoder comprises an encoder with 12 Transformer blocks and decoder with 4 Transformer blocks. Each block includes 384 hidden dimensions, 6 attention heads. At the pretraining stage, we randomly sample 1024 points and apply standard random scaling and random translation for data augmentation. We pre-train our model for 300 epochs, using the AdamW optimizer [17] and cosine learning rate decay [16], with an initial learning rate of 0.001, weight decay of 0.05, and batch size of 128. 4. Experiments In this section, we present comprehensive evaluation of 3D shape representations pretrained with procedural 3D programs across various downstream tasks, including object classification, part segmentation, and masked point cloud completion (Sec. 4.1 4.3). We further provide an in-depth analysis of model behavior and ablation studies (Sec. 4.4). For each downstream task, we report the performance of relevant existing methods as reference (primarily from Point-MAE [19]) and focus on comparisons with PointMAE pretrained on ShapeNet, as well as models trained from scratch. Specifically, we evaluate the following three pretraining strategies: Scratch: The network parameters are randomly initialized, with no pretraining. Point-MAE-SN: The network is pretrained with PointMAE on the ShapeNet (SN) [3] training split, containing Methods ModelNet40 OBJ-BG OBJ-ONLY PB-T50-RS PointNet [20] SpiderCNN [36] PointNet++ [21] DGCNN [28] PointCNN [13] PTv1 [44] PTv2 [30] OcCo [27] Point-BERT [42] Scratch [42] Point-MAE-SN [19] Point-MAE-Zero 89.2 92.4 90.7 92.9 93.7 94.2 92.1 93.2 91.4 93.8 93.0 73.3 77.1 82.3 86.1 86.1 84.85 87.43 79.86 90.02 90. 79.2 79.5 84.3 85.5 85.5 85.54 88.12 80.55 88.29 88.64 68.0 73.7 77.9 78.5 78.5 78.79 83.07 77.24 85.18 85.46 Table 1. Object Classification. We evaluate the object classification performance on ModelNet40 and three variants of ScanObjectNN. Classification accuracy (%) is reported (higher is better). Top: Performance of existing methods with various neural network architectures and pretraining strategies. Bottom: Comparison with our baseline methods. 41,952 clean 3D models. We utilize the officially released pretrained Point-MAE model for this setting. Point-MAE-Zero: The network is pretrained with PointMAE on procedurally generated 3D models, using 150K synthetic models by default. 4.1. Object Classification Benchmarks. We use ModelNet40 [33] and ScanObjectNN [25] as the benchmarks for the shape classification task. ModelNet40 contains 12,311 clean 3D CAD objects across 40 categories, with 9,843 samples for training and 2,468 for testing. Following Point-MAE, we apply random scaling and translation as data augmentation during training, and voting strategy during testing [15]. Following prior works [19, 27, 42], we also evaluate the few-shot classification performance on ModelNet40. ScanObjectNN is more complex real-world 3D dataset, consisting of approximately 15,000 objects across 15 categories, with items scanned from cluttered indoor scenes. We report results on three ScanObjectNN variants: OBJ-BG, OBJ-ONLY, and PB-T50-RS, the latter being the most challenging due to its additional noise and occlusions. Transfer Learning. Tab. 1 reports the object classification results in the transfer learning setting. On ModelNet40, Point-MAE-Zero achieves performance comparable to Point-MAE-SN. We attribute the performance gap to the substantial domain difference between the clean 3D shapes in ModelNet40 and the procedurally synthesized 3D models. In contrast, the domain gap between ShapeNet and ModelNet40 is smaller, resulting in closer performance between models. On ScanObjectNN, Point-MAE-Zero outperforms Point-MAE-SN across all three variants. This underscores the advantages of pretraining on the procedurally synthesized 3D models which encompass diverse geome4 Methods 5w/10s 5w/20s 10w/10s 10w/20s Methods mIoUI DGCNN-rand [28] DGCNN-OcCo [28] Transformer-OcCo [27] Point-BERT [42] Scratch [42] Point-MAE-SN [19] Point-MAE-Zero 31.62.8 90.62.8 94.03.6 94.63.1 87.85.2 96.32.5 95.42.5 40.84.6 92.51.9 95.92.3 96.32.7 93.34.3 97.81.8 97.71. 19.92.1 82.91.3 89.45.1 91.05.4 84.65.5 92.64.1 91.35.1 16.91.5 86.52.2 92.44.6 92.75.1 89.46.3 95.03.0 95.03.5 Table 2. Few-shot classification on ModelNet40. We evaluate performance on four n-way, m-shot configurations. For example, 5w/10s denotes 5-way, 10-shot classification task. The table reports the mean classification accuracy (%) and standard deviation across 10 independent runs for each configuration. Top: Results from existing methods for comparison. Bottom: Comparison with our baseline methods. try and topology. Both Point-MAE-Zero and Point-MAESN exceed the performance of models trained from scratch and previous methods that used alternative self-supervised learning approaches (e.g., OcCo [27] or Point-BERT [42]). Few-shot Classification. We conduct few-shot classification on ModelNet40 using standard n-way, m-shot configuration, where denotes the number of randomly chosen classes, and represents the number of instances sampled per class. For evaluation, 20 previously unseen samples are randomly selected from each of the classes. We perform 10 independent runs and report the average accuracy with standard deviation for each configuration. Table 2 shows the results for = {5, 10} and = {10, 20}. Our observations are consistent with those in the transfer learning experiments Point-MAE-Zero performs slightly below Point-MAE-SN on ModelNet40. Both PointMAE-Zero and Point-MAE-SN notably outperform models trained from scratch, as well as previous methods that utilize different network architectures (e.g., DGCNN [28]) or pretraining strategies. 4.2. Part Segmentation The 3D part segmentation task aims to predict part class label for each point within an object. We evaluate our model and baseline methods on the ShapeNetPart dataset [41], which includes 16,881 models across 16 categories. In line with prior works [19, 20, 42], we sample 2048 points from each 3D shape in segmentation tasks, resulting 128 point patches in the masked autoencoding pipeline (see Sec. 3). Table 3 reports the mean Intersection over Union (mIoU) across all instances, and specific IoU values for each category on the ShapeNetPart benchmark. Point-MAE-Zero performs comparably to Point-MAE-SN, despite the lack of semantic information in the procedurally generated 3D shapes. This result suggests the absence of semantic input does not significantly hinder the learning of 3D point cloud representations using the masked autoencoding selfPointNet [20] PointNet++ [21] DGCNN [28] OcCo [27] Point-BERT [42] Scratch [42] Point-MAE-SN [19] Point-MAE-Zero 83.7 85.1 85.2 85.1 85.6 85.1 86.1 86.1 aero 83.4 82.4 84.0 83.3 84.3 82.9 84.3 85. bag cap car chair earphone guitar knife 78.7 79.0 83.4 85.2 84.8 85.4 85.0 84.2 82.5 87.7 86.7 88.3 88.0 87.7 88.3 88.9 74.9 77.3 77.8 79.9 79. 78.8 80.5 81.5 89.6 90.8 90.6 90.7 91.0 90.5 91.3 91.6 73.0 71.8 74.7 74.1 81.7 80.8 78.5 76.9 91.5 91.0 91.2 91.9 91. 91.1 92.1 92.1 85.9 85.9 87.5 87.6 87.9 87.7 87.4 87.6 Methods lamp laptop motor mug pistol rocket skateboard table PointNet [20] PointNet++ [21] DGCNN [28] OcCo [27] Point-BERT [42] Scratch [42] Point-MAE-SN [19] Point-MAE-Zero 80.8 83.7 82.8 84.7 85.2 85.3 86.1 86.0 95.3 95.3 95.7 95.4 95.6 95.6 96.1 96.0 65.2 71.6 66.3 75.5 75.6 73.9 75.2 77. 93.0 94.1 94.9 94.4 94.7 94.9 94.6 94.8 81.2 81.3 81.1 84.1 84.3 83.5 84.7 85.3 57.9 58.7 63.5 63.1 63.4 61.2 63.5 64. 72.8 76.4 74.5 75.7 76.3 74.9 77.1 77.3 80.6 82.6 82.6 80.8 81.5 80.6 82.4 81.4 Table 3. Part Segmentation Results. We report the mean Intersection over Union (IoU) across all instances (mIoUI) and the IoU (%) for each category on the ShapeNetPart benchmark (higher values indicate better performance). supervised learning framework. Consistent with our findings in the object classification task in Sec. 4.1, both Point-MAE-Zero and Point-MAESN outperform models trained from scratch and prior works that use alternative network architectures[20, 21, 28] or pretraining methods [27, 42]. 4.3. Masked Point Cloud Completion The goal of masked point cloud completion is to reconstruct masked points in input 3D point clouds, serving as pretext task for learning 3D representations within the masked autoencoding framework [19] (see Fig. 2 and Sec. 3). During pretraining, points in the input are grouped into patches, with certain proportion of these patches randomly masked (e.g., 60% by default). Only the visible patches are passed to the encoder, while the centers of the masked patches are provided to the decoder as guidance for the masked autoencoding task. After pretraining, the network can reconstruct missing points in an input point cloud, even without the guidance of masked patch centers. In our experiments, we test both settings with guidance and without guidance using pretrained Point-MAESN and Point-MAE-Zero. We evaluate performance on the ShapeNet test split and 2000 procedurally generated 3D shapes, neither of which was used during pretraining. Fig. 3 presents qualitative comparisons between PointMAE-SN and Point-MAE-Zero. Surprisingly, despite being trained exclusively on procedurally generated 3D shapes without semantic content, Point-MAE-Zero can still predict masked points on recognizable 3D objects (e.g., airplanes and chairs from ShapeNet) even without guidance points. We hypothesize that both models leverage symmetry to estimate missing parts (e.g., wings on an airplane or legs on chair). Notably, Point-MAE-SN also performs well in predicting missing points on synthetic point clouds that were 5 Figure 3. Masked Point Cloud Completion. This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes. Left: Ground truth 3D point clouds and masked inputs with 60% mask ratio. Middle: Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE [19]. Right: Point cloud reconstructions without any guidance points. The L2 Chamfer distance (lower is better) between the predicted 3D point clouds and the ground truth is displayed below each reconstruction. With Guidance Without Guidance 4.4. Analysis Methods ShapeNet Synthetic ShapeNet Synthetic Point-MAE-SN [19] Point-MAE-Zero 0.015 0. 0.024 0.024 0.024 0.026 0.039 0.037 Table 4. Masked Point Cloud Completion. The table reports the L2 Chamfer distance (lower is better) between predicted masked points and ground truth on the test set of ShapeNet and procedurally synthesized 3D shapes. With Guidance: center points of masked patches are added to mask tokens in the pretrained decoder, guiding masked point prediction during Point-MAE training. Without Guidance: no information from masked patches is available during training. unseen in its pretraining dataset (i.e., ShapeNet). Tab. 4 provides quantitative comparison. Point-MAEZero and Point-MAE-SN exhibit very similar performance across both ShapeNet and synthetic shapes, though each model performs slightly better on its respective in-domain data. Performance for both models declines significantly when guidance points are removed from the decoder. In summary, Fig. 3 and Tab. 4 highlight that the representations learned through the masked autoencoding strategy primarily capture geometric features rather than high-level semantic features during pretraining. Complexity of Synthetic 3D shapes. We study how the geometric complexity of synthetic dataset contributes to performance in pretraining and downstream tasks. Specifically, we consider the following configurations, each with progressively increasing complexity: (a) Single Primitive: Objects consist of single primitive shape, each undergoing individual affine transformations. (b) Multiple Primitives (up to 3): Objects contain up to three primitive shapes combined. (c) Complex Primitives (up to 9): Objects are formed from up to nine primitive shapes. (d) Shape Augmentation: Shapes are further modified through boolean difference operations and wire-frame conversions. Fig. 4 displays samples from each configuration alongside quantitative comparisons of pretraining performance and downstream classification accuracy on PB-T50-RS, the most challenging variant of ScanObjectNN [25]. As shape complexity increases, the pretraining task becomes more difficult, leading to higher reconstruction losses at the 300th training epoch. However, the downstream classification performance of Point-MAE-Zero improves. This underscores the importance of topological diversity in shapes for effec6 Methods Scratch Point-MAE-SN Point-MAE-Zero (a) Single Primitive (b) Multiple Primitives (c) Complex Primitives (d) Shape Augmentation Pre-train Loss Downstream Accuracy 2. 3.17 4.10 4.43 5.28 77.24 85.18 83.93 84.52 84.73 85.46 Figure 4. Impact of 3D Shape Complexity on Performance. Left: Examples of procedurally generated 3D shapes with increasing complexity, used for pretraining. Textures are shown for illustration purposes only; in practice, only the surface points are used. Right: Comparison of pretraining masked point reconstruction loss (Eqn. 1) [19] and downstream classification accuracy on the ScanObjectNN dataset [25]. Each row in Point-MAE-Zero represents an incrementally compounded effect of increasing shape complexity and augmentation, with the highest accuracy achieved using shape augmentation. tive self-supervised point cloud representation learning. We observe that the reconstruction loss on our dataset with single primitives (i.e., 3.17) is higher than on ShapeNet (i.e., 2.62) which consists of more diverse 3D shapes. We hypothesize that this is because ShapeNet is relatively smaller than our dataset (50K vs. 150K) and ShapeNet models are coordinate-aligned. Dataset Size. Fig. 5 illustrates the effect of dataset size (i.e., the number of procedurally generated 3D shapes) on Point-MAE-Zeros performance in the shape classification task on the PB-T50-RS benchmark. Our experiments show that performance improves as dataset size increases, despite the dataset being procedurally generated. However, simply enlarging the dataset appears to yield diminishing returns, which may be due to intrinsic limitations of purely synthetic 3D dataset or the representation learning bottleneck within Point-MAE. Notably, Point-MAE-Zero and Point-MAE-SN achieve comparable performance on downstream tasks when pretrained on datasets of the same size, regardless of differences in their pretraining data domains (i.e., synthetic shapes vs. ShapeNet). We include additional experiments in the appendix to ablate the effect of dataset size. Efficiency of Transfer Learning. Fig. 6 shows the learning curves for training from scratch, Point-MAE-SN, and Point-MAE-Zero on shape classification tasks in the transfer learning setting. Both Point-MAE-SN and Point-MAEZero demonstrate faster training convergence and higher test accuracy compared to training from scratch. This trend is consistent across both ModelNet40 and ScanObjectNN benchmarks. t-SNE Visualization. Fig. 7 visualizes the distribution of 3D shape representations from Point-MAE-SN and PointMAE-Zero via t-SNE [26], before and after fine-tuning on Impact of pretraining dataset size. We report the Figure 5. classification accuracy (%) on the PB-T50-RS subset of ScanObjectNN [25] as function of the pretraining dataset size. Figure 6. Learning curves in downstream tasks. We present validation accuracy (top row) and training curves (bottom row) in object classification tasks on ScanObjectNN (left column) and ModelNet40 (right column). 7 Figure 7. t-SNE visualization of 3D shape representations. (a) displays the distribution of representations from transformer encoders with different initialization strategies: randomly initialized (Scratch), pre-trained with Point-MAE on ShapeNet (Point-MAE-SN), and pre-trained on procedurally generated 3D shapes (Point-MAE-Zero). (b) shows the distribution of shape representations after fine-tuning on the target tasks, which include object classification on ModelNet40 (top row) and ScanObjectNN (bottom row). Each point represents 3D shape while the color denotes the semantic categories. specific downstream tasks. It also includes representations from randomly initialized neural network as reference. First, compared to the representations from scratch, both Point-MAE-SN and Point-MAE-Zero demonstrate visually improved separation between different categories in the latent space. For example, this is evident in the red and light blue clusters on ModelNet40 and the blue and light blue clusters on ScanObjectNN. This indicates the effectiveness of self-supervised 3D representation learning via masked auto-encoding. Second, when comparing representations after finetuning, both Point-MAE-SN and Point-MAE-Zero show much less clear separation between categories in the latent space. This raises the question of whether high-level semantic features are truly learned through the masked autoencoding pretraining scheme. Finally, the t-SNE visualization reveals structural similarities between Point-MAE-Zero and Point-MAE-SN. Most categories lack clear separation in both models, except for the red and light blue clusters on ModelNet40 and the blue and light blue clusters on ScanObjectNN. This suggests that Point-MAE-Zero and Point-MAE-SN may have learned similar 3D representations, despite differences in the domains of their pretraining datasets. We provide more in-depth analysis in the supplementary material. 5. Discussion In this work, we propose to learn 3D representations from synthetic data automatically generated using procedural 3D programs. We conduct an empirical analysis of Point-MAE [19], state-of-the-art self-supervised learning method, and perform extensive comparisons with learning from well-curated, semantically meaningful 3D datasets. We demonstrate that learning with procedural 3D programs performs comparably to learning from recognizable 3D models, despite the lack of semantic content in synthetic data. Our experiments highlights the importance of geometric complexity and dataset size in synthetic datasets for effective 3D representation learning. Our analysis further reveals that Point-MAE primarily learns geometric structures (e.g., symmetry) rather than high-level semantics. This work has several limitations. For example, due to limited computational resources, we were unable to further scale up our experiments, such as by increasing the dataset size or conducting more detailed ablation studies on procedural 3D generation. Additionally, our findings may be influenced by potential biases in visualization tools (e.g., tSNE) or benchmarks (e.g., data distribution and evaluation protocols). Furthermore, in 3D vision, the distinction between geometric structures and semantics remains an open question, as well-stated by Xie et al. [34]. This work also does not provide any novel representation learning method. Nevertheless, we hope our findings will inspire further exploration into self-supervised 3D representation learning. 6. Acknowledgement The authors gratefully acknowledge Research Computing at the University of Virginia for providing the computational resources and technical support that made the results in this work possible. Research Computing at UVA."
        },
        {
            "title": "References",
            "content": "[1] Manel Baradad, Richard Chen, Jonas Wulff, Tongzhou Wang, Rogerio Feris, Antonio Torralba, and Phillip Isola. Procedural image programs for representation learning. Advances in Neural Information Processing Systems, 35:6450 6462, 2022. 2, 3 [2] Manel Baradad Jurjo, Jonas Wulff, Tongzhou Wang, Phillip Isola, and Antonio Torralba. Learning to see by looking at noise. Advances in Neural Information Processing Systems, 34:25562569, 2021. 2, 3 [3] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 1, 2, 4 [4] Prakash Chandra Chhipa, Richa Upadhyay, Rajkumar Saini, Lars Lindqvist, Richard Nordenskjold, Seiichi Uchida, and Marcus Liwicki. Depth contrast: Self-supervised pretraining on 3dpm images for mining material classification. In European Conference on Computer Vision, pages 212227. Springer, 2022. 2 [5] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object unIn Proceedings of the IEEE/CVF conference derstanding. on computer vision and pattern recognition, pages 21126 21136, 2022. 2 [6] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1314213153, 2023. 1, [7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [8] Jacob Devlin. Bert: Pre-training of deep bidirectional arXiv preprint transformers for language understanding. arXiv:1810.04805, 2018. 1, 2, 3 [9] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision, 129:33133337, 2021. 2 [10] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 1, [11] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 1, 2, 3 [12] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoarXiv preprint tracker: arXiv:2307.07635, 2023. 3 is better to track together. It [13] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. Pointcnn: Convolution on x-transformed points. Advances in neural information processing systems, 31, 2018. 2, 4 [14] Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker. Learning to reconstruct shape and spatially-varying reflectance from single image. ACM Transactions on Graphics (TOG), 37(6):111, 2018. [15] Yongcheng Liu, Bin Fan, Shiming Xiang, and Chunhong Pan. Relation-shape convolutional neural network for point cloud analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8895 8904, 2019. 1, 4 [16] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 4 [17] Ilya Loshchilov and Frank Hutter. tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 4 Sgdr: StochasarXiv preprint [18] Wufei Ma, Guanning Zeng, Guofeng Zhang, Qihao Liu, Letian Zhang, Adam Kortylewski, Yaoyao Liu, and Alan Yuille. Imagenet3d: Towards general-purpose object-level 3d understanding. arXiv preprint arXiv:2406.09613, 2024. 2 [19] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu, Yonghong Tian, and Li Yuan. Masked autoencoders for point cloud self-supervised learning. In European conference on computer vision, pages 604621. Springer, 2022. 1, 2, 3, 4, 5, 6, 7, 8 [20] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification In Proceedings of the IEEE conference and segmentation. on computer vision and pattern recognition, pages 652660, 2017. 2, 4, 5 [21] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. 2, 4, 5 [22] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation In Proceedings of of real-life 3d category reconstruction. the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. [23] Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua Tenenbaum, 9 [35] Saining Xie, Jiatao Gu, Demi Guo, Charles Qi, Leonidas Guibas, and Or Litany. Pointcontrast: Unsupervised preIn Computer training for 3d point cloud understanding. VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 574591. Springer, 2020. 2 [36] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. Spidercnn: Deep learning on point sets with parameterized convolutional filters. In Proceedings of the European conference on computer vision (ECCV), pages 87102, 2018. 2, 4 [37] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi Ramamoorthi. Deep image-based relighting from optimal sparse samples. ACM Transactions on Graphics (ToG), 37 (4):113, 2018. 1, 3 [38] Zexiang Xu, Sai Bi, Kalyan Sunkavalli, Sunil Hadap, Hao Su, and Ravi Ramamoorthi. Deep view synthesis from sparse photometric images. ACM Transactions on Graphics (ToG), 38(4):113, 2019. 1, [39] Siming Yan, Zhenpei Yang, Haoxiang Li, Li Guan, Hao Kang, Gang Hua, and Qixing Huang. Iae: Implicit autoencoder for point cloud self-supervised representation learning. 2022. 2 [40] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 3 [41] Li Yi, Vladimir Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. scalable active framework for region annotation in 3d shape collections. ACM Transactions on Graphics (ToG), 35(6):112, 2016. 1, 5 [42] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1931319322, 2022. 1, 2, 3, 4, 5 [43] Xueyang Yu, Xinlei Chen, and Yossi Gandelsman. Learning video representations without natural videos. arXiv preprint arXiv:2410.24213, 2024. 2, 3 [44] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and In Proceedings of Vladlen Koltun. Point transformer. the IEEE/CVF international conference on computer vision, pages 1625916268, 2021. 2, and William Freeman. Pix3d: Dataset and methods for In Proceedings of the single-image 3d shape modeling. IEEE conference on computer vision and pattern recognition, pages 29742983, 2018. 2 [24] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 3 [25] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: new benchmark dataset and classification model on real-world data. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1588 1597, 2019. 1, 4, 6, 7 [26] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9 (11), 2008. 7 [27] Hanchen Wang, Qi Liu, Xiangyu Yue, Joan Lasenby, and Matt Kusner. Unsupervised point cloud pre-training via occlusion completion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 97829792, 2021. 1, 2, 3, 4, 5 [28] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay Sarma, Michael Bronstein, and Justin Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):112, 2019. 2, 4, [29] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. 2 [30] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. Advances in Neural Information Processing Systems, 35:3333033342, 2022. 2, 4 [31] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48404851, 2024. 2 [32] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19121920, 2015. 2 [33] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19121920, 2015. 4 [34] Desai Xie, Sai Bi, Zhixin Shu, Kai Zhang, Zexiang Xu, Yi Zhou, Soren Pirk, Arie Kaufman, Xin Sun, and Hao Tan. Lrm-zero: Training large reconstruction models with synthesized data. arXiv preprint arXiv:2406.09371, 2024. 1, 2, 3, 8 Learning 3D Representations from Procedural 3D Programs"
        },
        {
            "title": "Supplementary Material",
            "content": "We provide more implementation details, additional experimental results, and visualizations in this supplementary material. In Sec. A, we present our training-fromscratch baseline, which surpasses the results reported in In Sec. B, we introduce more rigorPoint-MAE [19]. ous evaluation protocol using dedicated validation set instead of relying on the test set for validation. Moreover, in Sec. C, we provide linear probing results on ModelNet40 and three ScanObjectNN variants to compare the performance of Point-MAE-SN and Point-MAE-Zero. Lastly, we provide additional visualization in Sec. D. A. Training-from-Scratch Baseline For the training-from-scratch baseline, we report the results in the supplementary material for completeness. In the main text, we referenced the scores reported in the original Point-MAE paper for this baseline. However, our experiments suggest that training from scratch in our setup produced higher scores than those reported in Point-MAE [19] and Point-BERT [42]. The results from our training-fromscratch baseline are presented in this section, as shown in Tab. 5, Tab. 6, and Tab. 7. We follow the same evaluation protocol as PointMAE [19]. While the results we obtained from the trainingfrom-scratch baseline consistently surpass the previously reported scores, there remains significant gap between the performance of the training-from-scratch baseline and the pre-trained methods. This underscores the effectiveness of pre-training in enhancing model performance. Furthermore, pre-trained methods demonstrate much faster convergence compared to the training-from-scratch baseline. Methods ModelNet40 OBJ-BG OBJ-ONLY PB-T50-RS Scratch [42] Scratch* Point-MAE-SN [19] Point-MAE-Zero 91.4 93.4 93.8 93.0 79.86 87.44 90.02 90.36 80.55 82.03 88.29 88.64 77.24 81.99 85.18 85.46 Table 5. Object Classification. Classification accuracy (%) on ModelNet40 and three ScanObjectNN variants under the revised evaluation setup (Higher is better). Note Scratch* indicates baseline methods results we obtained. B. More Rigorous Evaluation In prior works [19, 42], the validation set was identical to the test set. The model was evaluated on the test set after every epoch, with the best-performing result selected. Such practices can artificially inflate performance metrics and fail Methods 5w/10s 5w/20s 10w/10s 10w/20s Scratch [42] Scratch* Point-MAE-SN [19] Point-MAE-Zero 87.85.2 92.73.5 96.32.5 95.42.5 93.34.3 95.53.1 97.81.8 97.71.6 84.65.5 88.55.1 92.64.1 91.35.1 89.46.3 92.04.5 95.03.0 95.03. Table 6. Few-shot classification on ModelNet40. We evaluate performance on four n-way, m-shot configurations. For example, 5w/10s denotes 5-way, 10-shot classification task. The table reports the mean classification accuracy (%) and standard deviation across 10 independent runs for each configuration. Top: Results from existing methods for comparison. Bottom: Comparison with our baseline methods. Note Scratch* indicates baseline methods results we obtained. Methods Scratch [42] Scratch* Point-MAE-SN [19] Point-MAE-Zero mIoUI 85.1 84.0 86.1 86.1 aero 82.9 84.3 84.3 85.0 bag 85.4 83.1 85.0 84.2 cap 87.7 89.1 88.3 88.9 car chair earphone guitar knife 78.8 80.6 80.5 81.5 90.5 91.2 91.3 91. 80.8 74.5 78.5 76.9 91.1 92.1 92.1 92.1 87.7 87.3 87.4 87.6 Methods lamp laptop motor mug pistol rocket skateboard table Scratch [42] Scratch* Point-MAE-SN [19] Point-MAE-Zero 85.3 85.1 86.1 86. 95.6 95.9 96.1 96.0 73.9 74.3 75.2 77.8 94.9 94.8 94.6 94.8 83.5 84.3 84.7 85.3 61.2 61.1 63.5 64.7 74.9 76.2 77.1 77. 80.6 80.9 82.4 81.4 Table 7. Part Segmentation Results. We report the mean Intersection over Union (IoU) across all instances (mIoUI) and the IoU (%) for each category on the ShapeNetPart benchmark (higher values indicate better performance). Note Scratch* indicates baseline methods results we obtained. to accurately reflect the models ability to generalize to unseen data. While we followed this setup in the main text for fair comparisons, we also performed more rigorous evaluations using dedicated validation set, which has no overlap with either the training set or the test sets. Specifically, we set aside 20% of the original test set as our validation set, leaving the remaining 80% as the new test set. We then report the test performance with checkpoints selected based on the validation accuracy. As presented in Tab. 8, our new evaluation results are consistent with these reported in the main text both Point-MAE-SN and Point-MAE-Zero outperform the training-from-scratch baseline across all four object classification tasks, while Point-MAE-Zero performs on par with Point-MAE-SN. The performance gap between models with pretraining and models trained from scratch is particularly pronounced in the most challenging experiment, PB-T50-RS, compared to the other three classification tasks. On ModelNet40, we observe that training-from-scratch and pre-trained methods achieve similar performance, though the results may vary across different runs. For example, in another run with different random seed, trainingfrom-scratch achieve 92.1% accuracy, which is lower than both Point-MAE-SN and Point-MAE-Zero. However, in all our experiments, pre-trained methods consistently converge significantly faster than training-from-scratch as shown in Fig. 6 in the main text. Methods ModelNet40 OBJ-BG OBJ-ONLY PB-T50-RS Scratch* [42] Point-MAE-SN [19] Point-MAE-Zero 92.95 92.22 92.87 84.19 88.32 88. 86.94 87.97 88.31 80.92 83.83 84.73 Table 8. Object Classification. Classification accuracy (%) on ModelNet40 and three ScanObjectNN variants under the revised evaluation setup. The original test set was split to create new test set and validation set, and all models were re-evaluated using the updated splits. (Higher is better). Note Scratch* indicates baseline methods results we obtained and we do not apply voting in these experiments. C. Linear Probing In line with Point-BERT [42] and Point-MAE [19], we primarily report the performance of pre-trained methods in transfer learning and few-shot learning setting in the main text, where the pre-trained model is fine-tuned with task-specific supervision. However, common practice for benchmarking self-supervised learning methods is the linear probing, which trains single linear layer while keeping the pretrained network backbone frozen. Below we provide details of our experimental setup and results. Experimental Setup. Instead of fully fine-tuning the pretrained model, we freeze the models weights and train single linear layer for the target task. We include training-from-scratch baseline, where we freeze the randomly initialized weights and train only single linear layer, providing point of comparison for the pretrained models. Methods ModelNet40 OBJ-BG OBJ-ONLY PB-T50-RS Scratch* Point-MAE-SN [19] Point-MAE-Zero 84.16 90.56 89.30 62.65 78.83 76. 66.09 81.41 78.83 56.80 68.22 67.87 Table 9. Object Classification. Classification accuracy (%) on ModelNet40 and three ScanObjectNN variants under the revised evaluation setup (Higher is better). Note Scratch* indicates baseline methods results we obtained. Results. We present linear probing results for object classification in Tab. 9. The gap between pre-trained models and the training-from-scratch baseline is noticeably larger in this setting. Interestingly, Point-MAE-SN outperforms Point-MAE-Zero in object classification under the linear probing setup, suggesting that semantically meaningful data may enable models to achieve better understanding of 3D structures. However, the performance gap between PointMAE-SN and Point-MAE-Zero remains relatively small. Based on the results in Tab. 1 in the main text, Point-MAESN appears to be more effective choice for transfer learning tasks. We encourage future research to explore improved learning algorithms to take full advantage of data generated from procedure 3D programs. D. Additional Visualization In this section, we present additional qualitative comparisons between Point-MAE-SN and Point-MAE-Zero for Masked Point Cloud Completion under both guided and unguided settings. Additionally, we provide t-SNE visualizations for Point-MAE-Zero and Point-MAE-SN to further investigate its representational capabilities, focusing on whether it can effectively distinguish between different primitives. D.1. Masked Point Cloud Completion Additional qualitative results for masked point cloud completion, both guided and unguided, are shown in Figure 8. D.2. More t-SNE Visualizations Figure 9. t-SNE Visualization. We visualize features extracted by Point-MAE-Zero (left) and Point-MAE-SN (right) for three primitive shapes Ellipsoid, Cube, and Cylinder. Point-MAE on Primitives Fig. 9 presents t-SNE visualizations of features extracted by Point-MAE-Zero and Point-MAE-SN for three primitives: ellipsoid, cube, and Interestingly, the notable structural differences cylinder. among these primitives are not reflected in the latent space of either Point-MAE-SN or Point-MAE-Zero. We hypothesize that this occurs because the learned 3D representations from both models primarily capture local structures rather than global shapes. 2 Figure 8. Masked Point Cloud Completion. This figure visualizes shape completion results with Point-MAE-SN and Point-MAE-Zero on the test split of ShapeNet and procedurally synthesized 3D shapes. Left: Ground truth 3D point clouds and masked inputs with 60% mask ratio. Middle: Shape completion results using the centers of masked input patches as guidance, following the training setup of Point-MAE [19]. Right: Point cloud reconstructions without any guidance points. The L2 Chamfer distance (lower is better) between the 3 predicted 3D point clouds and the ground truth is displayed below each reconstruction."
        }
    ],
    "affiliations": [
        "University of Virginia"
    ]
}