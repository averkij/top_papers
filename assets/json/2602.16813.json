{
    "paper_title": "One-step Language Modeling via Continuous Denoising",
    "authors": [
        "Chanhyuk Lee",
        "Jaehoon Yoo",
        "Manan Agarwal",
        "Sheel Shah",
        "Jerry Huang",
        "Aditi Raghunathan",
        "Seunghoon Hong",
        "Nicholas M. Boffi",
        "Jinwoo Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm."
        },
        {
            "title": "Start",
            "content": "One-step Language Modeling via Continuous Denoising Chanhyuk Lee 1 Jaehoon Yoo 1 Manan Agarwal 2 Sheel Shah 2 Jerry Huang 2 Aditi Raghunathan 2 Seunghoon Hong 1 Nicholas M. Boffi 2 Jinwoo Kim"
        },
        {
            "title": "Abstract",
            "content": "Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via cross entropy objective, where we introduce simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. 6 2 0 2 8 1 ] . [ 1 3 1 8 6 1 . 2 0 6 2 : r 1. Introduction Todays frontier language models (LMs) are based on an autoregressive process that produces one subword (token) per step (Achiam et al., 2023; Anil et al., 2023; Guo et al., Equal advising 1KAIST 2Carnegie Mellon University. Correspondence to: Jinwoo Kim <jinwoo-kim@kaist.ac.kr>, Nicholas M. Boffi <nboffi@andrew.cmu.edu>. Code is available at https://github.com/david3684/flm. Preprint. February 20, 2026. 1 Figure 1. Our flow map language model (FMLM) outperforms discrete diffusion models (gray) and matches the 8-step generation performance of few-step distilled discrete diffusion models (blue) in only one step, achieving an 8.3 speedup on LM1B. 2025). While these models leverage parallelism during training through teacher forcing and transformer-based architecture, their sampling is inherently serial in nature, posing bottleneck in generation speed. Recently, language models based on discrete diffusions and flows have attracted interest as possible solution (Khanna et al., 2025; Google DeepMind, 2025; Song et al., 2025). By learning to reverse noising process on full sequences, such models can output multiple tokens in parallel at each sampling step, thereby holding the potential for accelerated generation. Despite their promise, discrete diffusion language models have significant practical limitations. In particular, they suffer from rapid drop-off of quality in the few-step generative regime (Deschenaux & Gulcehre, 2024). This is critical as diffusion models process the full sequence at each inference step, so that sampling steps need to be substantially reduced to compensate for the associated cost compared to their autoregressive counterparts (Dieleman, 2023; Zheng et al., 2024). This difficulty arises from fundamental computational constraint: the state space over full sequences is combinatorially large, necessitating factorized approximation of the transition probability that neglects inter-token correlations (Wu et al., 2025; Kang et al., 2025). While this approximation makes discrete diffusion computationally feasible, empirically it requires many steps to accurately One-step Language Modeling via Continuous Denoising Figure 2. Overview. Left: We leverage continuous interpolation between Gaussian noise and one-hot language encoding. Middle: Our flow-based language model (FLM) learns denoiser that predicts clean data, which we convert into flow for sampling. Right: Our distilled flow map language model (FMLM) directly transports states between distant timepoints, enabling few-step generation. capture full sequence structure, implying fundamental rigidity that limits practical use. pable of few-step generation, by expanding FLM via efficient distillation methods. In contrast, continuous diffusion language models, which represent and denoise subwords in continuous space, do not rely on such approximations (Li et al., 2022; Dieleman et al., 2022). As result, they can perform accurate parallel generation through deterministic evolution driven by velocity or score function (Lipman et al., 2022; Albergo et al., 2023; Song et al., 2020b). Perhaps most interestingly, this makes them compatible with recent advances in few-step distillation methods that learn the flow map, an operator that directly transports noise to data in as few as one function evaluation (Boffi et al., 2025a;b). Yet, despite their potential advantages, widely held belief is that continuous diffusion language models underperform their discrete counterparts (Sahoo et al., 2025; Pynadath et al., 2025), leading practitioners to prioritize discrete methods in recent years (Nie et al., 2025; Khanna et al., 2025). In this work, we challenge this widespread belief, showing that continuous diffusion language models formulated via flow matching and flow maps can be higher-performing and faster than previously believed  (Fig. 1)  . We argue that their observed weak performance stems from suboptimal design, such as the choice of temporal weighting, rather than an inherent limitation of the model class. In particular, our approach  (Fig. 2)  reaches the performance of state-of-theart (SoTA) discrete diffusion models and exceeds them in the few-step regime. Overall, our main contributions are: We build FLM, powerful flow-based language model, via principled reexamination of design choices that reveals the root of underperformance in prior methods. We introduce FMLM, flow map language model caWe validate our approach empirically on the One Billion Word (LM1B) and OpenWebText (OWT) datasets. FLM is competitive in generation quality with SoTA discrete diffusion LMs, while FMLM beats recent fewstep LMs, nearing their 8-step quality at one step. 2. Background & Related Work In this section, we provide an overview of the background and related work. An extended discussion is in App. A. Let be vocabulary of subwords, treated as integers [V ] without loss of generality. We denote language data with length by = (yl)L l=1 L. The goal of language modeling is to estimate the data distribution p(y) on L. Autoregressive language models factorize the data distribution over length, p(y) = p(y1)p(y2y1) . . . p(yLy<L), and learn the conditional distribution p(yly<l) over subwords given prefix (Jordan, 1986; Elman, 1990; Bengio et al., 2003). They generate text by sequentially sampling each token conditioned on the past generation. This process is serialized, with each token awaiting all previous tokens, limiting efficiency (Gu et al., 2017; Stern et al., 2018). Discrete diffusion language models aim to achieve faster generation by producing several tokens in parallel at each step (Austin et al., 2021; Lou et al., 2023). They employ discrete noising process such as masking (Sahoo et al., 2024) or uniform randomization (Lou et al., 2023; Schiff et al., 2024) of multiple tokens, and model its reversal pt(yt), which transports an initial distribution p0 to the data distribution p1 = p. To do so, the model must learn the transition probabilities pts(ytys) (Austin et al., 2021; Gat et al., 2024) so that generation can be performed via ancestral 2 One-step Language Modeling via Continuous Denoising Figure 3. Factorization error in discrete diffusion. toy dataset with two correlated modes new-york and san-diego. Left: In many-step sampling, both continuous flow and discrete diffusion generate valid data. Right: In few-step sampling, the factorized transition of discrete diffusion yields spurious mixture of all possible combinations (including invalid pairings new-diego and san-york). sampling over temporal grid 0 = t0 < . . . < tN = 1. Since each step updates multiple tokens simultaneously, substantial speedups could be achieved in principle if few-step generation were possible. In practice, however, discrete diffusion models often fail catastrophically in the few-step regime (Deschenaux & Gulcehre, 2024). This failure is rooted in fundamental computational challenge: since the transition probability is defined over the full text space L, learning it accurately requires model with an intractable L 1 output dimensionality. To sidestep this problem, discrete methods employ factorized approximation ts pts of the transition probability: ts(ytys) := p1 ts(y1 ys) pL ts(yL ys), (1) , ..., yL where pl(yl) is the probability of the l-th token marginalized over the others. While this approximation makes learning tractable, it makes restrictive assumption that the denoised tokens y1 are conditionally independent of each other given the earlier denoised state. This assumption only holds in the infinitesimal limit (Campbell et al., 2022), which implies that one needs large number of sampling steps for accurate generation. With reduced steps, the approximation causes the model to produce unnatural text by neglecting correlations over tokens  (Fig. 3)  (Wu et al., 2025; Kang et al., 2025). Unfortunately, this is model misspecification issue that cannot be resolved by improving model quality alone. Our work addresses this fundamental challenge with continuous flow-based formulation (Lipman et al., 2022; Albergo et al., 2023), which learns deterministic transport map that need not make such factorized approximation. As result, our approach directly enables scalable one-step language modeling. 3 3. Theoretical Framework In this section, we describe our formulation of continuous flow-based language model (FLM), as well as its few-step flow map (FMLM). To do so, we develop generative model over one-hot encodings of language, leveraging flow matching over stochastic interpolants. Full details of the framework can be found in App. and C, where we give complete background on flow maps and describe both distillation and direct training algorithms for FMLMs. 3.1. continuous representation of language natural way to build continuous flow over language data is to first construct continuous representation of the data, and then to apply flow-based modeling. Formally, we choose continuous representation : (cid:55) and decoder : (cid:55) satisfying g(f (y)) = y, and we model the induced distribution p(x) on the continuous space: p(x) = p(y = g(x)). (2) Inference can be performed by first generating ˆx p(x), and then decoding into discrete language through ˆy = g(ˆx). Several choices of continuous representation have been explored in prior work, including learned embeddings (Li et al., 2022; Dieleman et al., 2022; Gulrajani & Hashimoto, 2023) and pretrained embeddings (Strudel et al., 2022; Lovelace et al., 2023). However, learned embeddings require careful regularization to prevent collapse or explosion, while pretrained embeddings may not be optimal for the flow. Here, we adopt simple and canonical tokenwise one-hot representation : RLV with an argmax decoder One-step Language Modeling via Continuous Denoising : RLV that are pre-specified and frozen: : (cid:55) [onehot(y1), ..., onehot(yL)], : (cid:55) [argmax(x1), ..., argmax(xL)]. (3) (4) This choice offers lossless representation of discrete tokens, and requires neither regularization nor auxiliary training. Similar representations have been explored in prior work on continuous diffusion for language (Chen et al., 2022; Han et al., 2023; Mahabadi et al., 2024), though often with additional constraints such as simplex projection. As we elaborate upon below, our approach operates in an unconstrained Euclidean space, which we find to be simpler and more effective in practice. 3.2. Interpolants and flows for language modeling Given choice of continuous representation, the language modeling problem becomes that of learning continuous data distribution p(x). To build high-performance model, we follow Lipman et al. (2022) and Albergo et al. (2023), leveraging flow matching over stochastic interpolant. This leads to simple formulation of our method and matches the design of state-of-the-art flows for continuous data (Li & He, 2025; Zheng et al., 2025a). In the stochastic interpolant framework, we specify probability path pt(xt) as the density of an interpolant between noise x0 p0 = N(0, I) and data x1 p1: It := (1 t)x0 + tx1, It pt. (5) Above, we choose simple and canonical linear stochastic interpolant, but many choices are possible in practice. By choosing different interpolants, our discussion can extend to variance-exploding (Song & Ermon, 2019) and variancepreserving (Ho et al., 2020) diffusions (Lipman et al., 2022). This probability path admits deterministic representation that can be used to produce sample xt pt, xt = bt(xt), x0 p0, [0, 1], (6) where bt is the velocity field of the probability flow, bt(x) = E[ ItIt = x] = E[x1 x0It = x]. (7) Above, the expectation is with respect to the random draws of x0 p0 and x1 p1 that are used to construct the interpolant. The conditional expectation form (7) means that the velocity can be learned efficiently by solving square loss regression problem = argminˆb LMSE(ˆb), where: (cid:90) 1 LMSE(ˆb) := Eˆbt(It) It2dt. (8) 0 In practice, (8) is estimated by sampling t, for example uniformly over the interval U[0, 1]. After minimizing 4 (8) over parametric class of neural networks, sample approximately following p1 can be obtained by using the estimate ˆbt in the numerical integration of (6). Integration is performed over choice of temporal grid 0 = t0 < t1 < . . . < tN = 1, for example with the forward Euler method. Denoiser. Despite its simplicity, learning the velocity directly incurs potential difficulty in our setuw. Velocity prediction requires regressing onto noised target It = x1x0, which inherits the full-rank structure of Gaussian noise samples x0 RLV . When the dimensionality is much larger than the internal feature dimension of the network, this can lead to underfitting due to rank bottleneck (Li & He, 2025; Zheng et al., 2025a). To avoid this issue, we predict the clean data x1, which also lies in RLV but is constrained to one-hot encodings that form highly structured, low-entropy target. We therefore predict the conditional expectation of the clean data, which relates to the velocity through linear change of variables (Albergo et al., 2023; Li & He, 2025): Dt(x) := E[x1It = x], bt(x) = Dt(x) 1 . (9) The function Dt, called the denoiser, can be learned via another regression = argmin ˆD LMSE( ˆD), where: (cid:90) 1 LMSE( ˆD) := ˆDt(It) x12dt. (10) 0 Importantly, in our discrete generative modeling setting the denoiser admits simple probabilistic interpretation: Lemma 3.1. At each token position l, the optimal denoiser output equals the posterior probability over the vocabulary: Dt(x)l = pl 1t(It = x). (11) proof is in App. D.1. Since the optimal denoiser lies on the probability simplex 1, it is advantageous to parameterize ˆD via tokenwise softmax. This restricts the hypothesis space to valid probability distributions, allowing the model to focus on estimating the correct posterior rather than also learning the simplex structure. Furthermore, this enables training through categorical cross-entropy loss (Dieleman et al., 2022; Eijkelboom et al., 2024), which is more adapted to the one-hot geometry: Proposition 3.2. With the change of variables in (11), the denoiser can be learned via = argmin ˆD LCE( ˆD) where: LCE( ˆD) := (cid:90) 0 (cid:34) (cid:88) l= log ˆpl 1t(xl 1It) (cid:35) dt. (12) proof is in App. D.2. In practice, cross-entropy provides well-conditioned loss landscape for learning on the simplex, yielding stronger gradients than squared loss when predictions are far from the target (Golik et al., 2013). One-step Language Modeling via Continuous Denoising Relationship with discrete diffusion. Lemma 3.1 suggests that the optimal denoiser implicitly learns the factorized posterior 1t(x1xt) (1). This reveals an interesting connection with discrete diffusion models, which also often learn 1t via tokenwise cross-entropy objective (Austin et al., 2021; Campbell et al., 2022; Gat et al., 2024). While discrete models use the learned 1t to perform ancestral sampling, requiring the joint probability and thus suffering from factorization errors, continuous models use the learned 1t to infer the exact velocity based on (11) and (9). 3.3. Flow maps for few-step language modeling The framework in Sec. 3.2 does not immediately allow for few-step language modeling, since the numerical solvers used to integrate (6) typically become inexact at large step sizes. Here we overcome this challenge by leveraging the flow map Xs,t : RLV RLV . The flow map is the solution operator of (6), and by definition directly transports between any two timepoints (Boffi et al., 2025a;b): Xs,t(xs) = xt, for all (s, t) [0, 1]2. (13) Without loss of generality, we may parameterize it as: Xs,t(x) = + (t s)vs,t(x), (14) where is called the average velocity or mean flow (Geng et al., 2025a;b). Given flow map, sampling ˆx1 p1 can be done by choosing temporal grid 0 = t0 < ... < tN = 1 and sequentially evaluating ˆxti+1 = Xti,ti+1(ˆxti). Unlike numerical integration of (6), this approach is accurate for an arbitrary time grid by definition, enabling sampling in as few as one function evaluation via ˆx1 = X0,1(x0). In practice, leveraging few steps typically improves performance. Methods for learning flow maps (Boffi et al., 2025a;b) leverage the following mathematical properties, which fully characterize the flow map under standard regularity conditions: Xs,s = id, tXs,t = bt, lim st (boundary condition) (tangent condition) Xu,t(Xs,u(x)) = Xs,t(x). (semigroup condition) The last condition can be replaced with alternatives based on differential characterization of the flow map, which lead to learning objectives that require the computation of Jacobianvector products, such as MeanFlow (Geng et al., 2025a;b) and Lagrangian self-distillation (Boffi et al., 2025a;b; Zhou et al., 2025) (see App. B.1). We use the semigroup condition due to its simplicity, which is related to progressive distillation (Salimans & Ho, 2022) and shortcut models (Frans et al., 2024). The alternatives can also be directly used in our framework, and we leave their study to future work. Using the semigroup condition, the flow map Xs,t can be learned via an optimization = argmin ˆX LMSE( ˆX) subject to the boundary and tangent conditions, where: LMSE( ˆX) := (cid:90) (cid:90) (cid:90) 0 0 ˆXs,t(Is) sg( ˆXu,t( ˆXs,u(Is)))2dudsdt, (15) with sg() denoting the stop-gradient operator. The two-time denoiser. In Sec. 3.2, we introduced the denoiser Dt to reparameterize the velocity bt into simplexvalued clean-data predictor, enabling training via cross entropy. We now develop an analogous reparameterization for the flow map. To do so, we observe rearrangement of (9) into Dt(x) = + (1 t)bt(x), showing that the denoiser equals single Euler step of size 1 with the velocity field. Mirroring this relationship, we define new quantity we refer to as the two-time denoiser: δs,t(x) := + (1 s)vs,t(x), (16) which takes single step of the average velocity vs,t using the full remaining time 1 s. From (14), we can see that the flow map is expressed as the following convex combination: Xs,t(x) = 1 1 + 1 δs,t(x). (17) Hence, the properties characterizing the flow map can be converted into those on the two-time denoiser. The boundary condition is satisfied by construction, and the tangent and semigroup conditions translate respectively into: δs,s(x) = Ds(x), δs,t(x) = γδs,u(x) + (1 γ)δu,t(Xs,u(x)), (18) (19) where γ = (1t)(us) We refer to (18) as the diagonal condition. (1u)(ts) [0, 1]. proof is given in App. C.1. Importantly, we find that at each token position, the output of the two-time denoiser δ always lies in the simplex 1 analogous to the one-time denoiser D. proof is in App. C.1. This motivates learning δ using the semigroup condition (19) via cross entropy subject to the diagonal condition (18), by optimizing δ = argminˆδ LCE(ˆδ) (App. C.3): LCE(ˆδ) := (cid:90) 1 (cid:90) (cid:90) (cid:104) ℓCE 0 0 + (1 γ)ˆδu,t( ˆXs,u(Is))) (cid:16)ˆδs,t(Is), sg(γ ˆδs,u(Is) (cid:17)(cid:105) dudsdt. (20) Similarly to D, learning δ using cross entropy may benefit from well-behaved loss landscape, so we test it alongside the average velocity (14) and squared loss (15) formulation. In App. C, we give complete characterize of the two-time denoiser δs,t, where we explain how to build both distillation and direct training algorithms via self-distillation for the flow map that respect the one-hot geometry of discrete data. 5 One-step Language Modeling via Continuous Denoising This quantity measures the expected fraction of tokens that would be incorrectly decoded if we were to stop the flow at time t, starting at Pe(0) = 1 1 and decreasing to Pe(1) = 0. The rate of decrease Pe(t) captures how much progress the flow makes in determining subwords at time t. For large , the decoding error concentrates acutely near = 1  (Fig. 4)  , implying that most times do not contribute significantly towards decoding, with token identities only being resolved in narrow window. Uniform sampling U[0, 1] therefore wastes training signal on regions where little denoising occurs, while undersampling the critical interval where subwords are actually determined. Similarly, an equispaced grid tn = n/N allocates most sampling steps to regions that contribute minimally to generation quality. Following Dieleman et al. (2022) and Stancevic et al. (2025), we address this using time reparameterization τ (t), which is differentiable, monotonically increasing function with endpoints (0, 0), (1, 1) and inverse t(τ ). We train and generate uniformly in τ , sampling via τ (t) U[0, 1] during training, and using grid tn = t(τn) with τn = n/N for generation. This reweights which regions in receive more training signal and are discretized more finely. We propose to choose τ (t) so that uniform steps in τ correspond to uniform progress in determining subwords. As Pe(t) measures the remaining decoding error at time t, we view its decrease from Pe(0) as cumulative progress. Standardizing to [0, 1]: τ (t) = Pe(0) Pe(t) Pe(0) = 1 V 1 Pe(t). (22) By construction, this reparameterization redistributes time so that each step contributes equally to reducing the decoding error. We find this choice critical for stable training and generation, enabling FLM to scale to 50, 000. 4.2. Flow map language model (FMLM) In this section, we detail the design choices needed to build an effective FMLM. Flow maps can be learned through either self-distillation, where flow and flow map are trained jointly, or distillation from pretrained flow (Boffi et al., 2025a;b). For simplicity, here we adopt the latter approach, which decouples the two training phases. Full details on self-distillation algorithms are given in App. and C. Existing work parameterizes the flow map via the average velocity ˆvs,t (14), noting that bt = vt,t by the tangent condition, and jointly trains ˆvs,t and ˆbt = ˆvt,t with the respective losses (Boffi et al., 2025a;b). In contrast, we develop novel two-stage distillation scheme that we empirically find more stable: the first stage learns correction to the trained FLM that converts Euler steps into accurate flow map jumps, and the second stage compresses this into single flow map model for improved inference-time efficiency. Figure 4. Decoding error rate over time across vocabulary sizes. Our time reparameterization τ (t) redistributes time so each step contributes uniformly to the denoising signal. Flow maps in discrete diffusion. Given our discussions, natural question is whether discrete diffusion models can potentially leverage flow map. While they admit deterministic evolution at the distribution level pt = Qtpt for rate matrix Qt (Campbell et al., 2022), and hence flow map exists, it acts on the space of distributions over sequences pt L , of dimension L 1. Computing or representing this object is intractable, necessitating the factorized approximations discussed in Sec. 2. Continuous flows admit flow map at the sample level, making it tractable to learn and evaluate (see also App. D.3). 4. Algorithmic Aspects We now describe the practical implementation of our flowbased language model (FLM) and its distillation into fewstep flow map language model (FMLM). We aim to provide principled design choices that work robustly in practice. 4.1. Flow-based language model (FLM) To build high-performing FLM, we find that two particularly important choices are (i) how to sample timepoints during training, and (ii) how to choose time grid during generation. naıve approach would be to use uniform sampling U[0, 1] for training and an equispaced grid tn = n/N for generation, which typically works well for continuous modalities such as images. However, we find this to be suboptimal for interpolants defined over one-hot encodings, as the generative process concentrates its decisions in narrow time interval, especially for large vocabularies. To understand this, we consider the decoding error rate Pe (Sahoo et al., 2025; Pynadath et al., 2025): Pe(t) := 1 (cid:88) l=1 (g(xt)l = g(x1)l) (21) One-step Language Modeling via Continuous Denoising First stage. Recall that single Euler step computes + (t s)ˆbs(x), which incurs discretization error for large steps. We learn correction model ˆψs,t which predicts the correction needed to convert the Euler estimate into the true flow map. Specifically, we parameterize the flow map as: ˆXs,t(x) := + (t s) ˆbs(x) + 1 2 (t s)2 ˆψs,t(x), (23) where ˆb is an FLM trained following Sec. 4.1, possibly as denoiser ˆD based on (9). This parameterization was proposed by Boffi et al. (2025a) but not empirically tested. By construction, it satisfies the boundary condition and tangent condition, so we only need to enforce the semigroup condition through training. We initialize ˆψ from the parameters of ˆb and train using the semigroup loss (15) (see App. D.4 for details). Since ˆb is frozen and ˆψ only learns the residual correction, the training is efficient and converges quickly. If the two-time denoiser parametrization (16) is used instead, we learn log-space correction model ˆϕs,t that converts the one-time denoiser FLM ˆDt into the two-time denoiser ˆδs,t that characterizes the flow map ˆXs,t through (17). Assuming that ˆD outputs tokenwise classification logits (Sec. 3.2), ˆDt(x) = softmax( ˆD(logits) (x)), we parameterize ˆδ as: ˆδs,t(x) = softmax( ˆD(logits) (x) + (t s) ˆϕs,t(x)). (24) This satisfies the diagonal condition (18) by construction. Hence, we initialize ˆϕ from the parameters of ˆD and train it using the cross-entropy-based semigroup loss (20). Second stage. The two-model flow map ˆX, composed of ˆb and ˆψ (or ˆϕ), doubles the memory cost at inference. We distill it into single-model flow map ˆY parameterized as: ˆYs,t(x) := + (t s)ˆus,t(x). (25) We initialize ˆu from FLM ˆb by removing the output softmax, and train by solving simple regression problem onto the two-model teacher ˆX which is frozen throughout: LMSE( ˆY ) := (cid:90) 1 (cid:90) 0 ˆYs,t(Is) ˆXs,t(Is)2dsdt. (26) This has several desirable properties that yield fast and stable convergence. The teacher provides targets via single forward pass, without requiring iterative sampling or trajectory simulation. The loss is lower-bounded by zero with unique global minimizer at ˆY = ˆX, allowing us to directly track distillation quality during training. Lastly, it is strongly convex in ˆY , ensuring well-conditioned optimization. ˆX and ˆY . For generation, the flow maps can transport between arbitrary time pairs by definition, so we are free to choose any sequence of jump points. We use the grid tn = t(n/N ), which spaces the jumps uniformly in reparameterized time. This allocates finer steps to regions where token resolution occurs. For the first-stage distillation, we sample time triplets (s, u, t) as follows: we first draw step size U[0, 1] and start point τ (s) U[0, 1 h], then set the endpoint τ (t) = τ (s) + and the midpoint τ (u) = (τ (s) + τ (t))/2. Since we sample (s, t) continuously, our approach differs from shortcut models (Frans et al., 2024). The continuous sampling allows the model to learn over all timescales, and the midpoint choice for provides balanced partition of the interval for the semigroup condition. The second-stage distillation uses the same sampling scheme, but without the need to sample u. Learned loss weighting. During both stages, we follow Boffi et al. (2025a) and employ the learned loss weighting proposed in EDM2 (Karras et al., 2024b), which stabilizes the gradient variance across the sampled time distribution. 5. Experiments We test our approach using the One Billion Word (LM1B) (Chelba et al., 2013) and OpenWebText (OWT) (Gokaslan & Cohen, 2019) datasets, which are widely used for language modeling. We preprocess each dataset by packing sequences to length = 128 and = 1024, respectively. We tokenize the data using bert-base-uncased and the gpt-2 tokenizer, resulting in vocabulary sizes = 30, 522 and = 50, 257, respectively. Following the settings of recent works (Sahoo et al., 2024; 2025), we adopt 170M-parameter diffusion transformer (DiT) (Peebles & Xie, 2023) with 12 transformer blocks, equipped with rotary positional embeddings (RoPE) (Su et al., 2024), and adaptive layer normalization (AdaLN) for time conditioning. Further implementation details can be found in App. E. Training. We train our flow-based language model (FLM) following Sec. 4.1 for 1M steps with batch size of 512 using the Adam optimizer (Kingma & Ba, 2014) with learning rate 3 104. Based on the trained FLM, we train our flow map language model (FMLM) following Sec. 4.2, for 100k steps for the first and second stages for LM1B, and for 300k steps for the first stage for OWT, with the other hyperparameters the same as those of FLM. Evaluation. We evaluate our models and baselines based on sample quality1. We generate 1,024 samples from each model and measure the generative perplexity (Gen. PPL ) Time reparameterization. As in Sec. 4.1, we leverage the time reparameterization τ (t) from (22) for the flow maps 1While validation perplexity is also used in prior work, measuring it for our method requires auxiliary training (Ai et al., 2025). One-step Language Modeling via Continuous Denoising Tab. 1, we show the 1024-step sampling results. For the LM1B dataset, FLM outperforms all baselines in terms of sample quality while preserving entropy. For OWT, while FLM achieves the best sample quality, there is slight tradeoff in entropy; nonetheless, it remains within 0.1 of the data entropy, similar to the discrete baselines. Overall, our results show that continuous denoising via flows can actually outperform discrete diffusion methods for language modeling in the many-step regime. Furthermore, it shows that simple Euclidean interpolants can outperform complex methods involving Riemannian manifold structure or hybrid diffusion. Fig. 5 shows the performance curves as the number of sampling steps is varied from 8 to 1024, demonstrating that FLM is competitive across wide range. 5.2. Flow map language model (FMLM) We now compare the few-step generation performance of FMLM with recent few-step distilled discrete diffusion baselines: Duo with DCD (Sahoo et al., 2025), MDLM with SDTT (Deschenaux & Gulcehre, 2024), and both with Di4C (Hayakawa et al., 2024). The results are shown in Fig. 6 and Tab. 2. Even after distillation, discrete methods suffer from catastrophic degradation in the extremely few-step regime, with PPL spiking above 1,000 (MDLM + SDTT/Di4C) or collapsing entropy (Duo + DCD/Di4C) for both datasets. This supports the claim that the factorization error of discrete diffusion is rooted in model misspecification, such that distillation cannot fully correct it. In contrast, FMLM remains stable, delivering SoTA (104.37) one-step generation quality in LM1B that matches baselines at 8-16 steps. In OWT, FMLM produces one-step generation quality (129.32) comparable to the baselines at 4-8 steps. We note that it retains reasonable entropy (4.53), while distilled Duo baselines show low PPL with collapsed entropy (2.80 and 3.36), implying that they output repeated subwords. FMLM also shows much better sample quality than MDLM distilled baselines. Qualitative results in Fig. 7 and App. highlight these failure modes of the baselines. Notably, the strong many-step quality of FLM transfers directly to few steps via flow map distillation, while discrete methods fail to preserve their teachers quality. This highlights the advantage of continuous flows, which admit well-defined flow maps that enable principled few-step distillation. Figure 5. Generation performance of FLM on LM1B (top) and OWT (bottom) compared to diffusion baselines. Table 1. Generation performance of FLM at 1024 sampling steps. Model Dataset RDLM CANDI MDLM Duo LM1B OWT Gen. PPL () Entropy Gen. PPL () Entropy - 268.21 120.99 109.21 98.14 4.31 4.33 4.35 4.32 4.31 4.29 - - 143.13 121.09 77.69 62.23 5.44 - 5.71 5.65 5.55 5.33 FLM (Ours) 96.91 using pretrained GPT-2 Large (Radford et al., 2019). Since generative perplexity can have low but misleading values if model generates repetitive tokens (Zheng et al., 2024), we also report the average of per-sample unigram entropy, with low values (e.g., < 4) indicating low-quality repetitions. We provide supplementary evaluation results including an application of autoguidance (Karras et al., 2024a) in App. F. 5.1. Flow-based language model (FLM) We compare FLM with recent methods: Duo (Sahoo et al., 2025) and MDLM (Sahoo et al., 2024), representing uniform and masked discrete diffusion, respectively. We also compare with RDLM (Jo & Hwang, 2025) and CANDI (Pynadath et al., 2025), recent continuous and hybrid diffusion models, respectively. All baselines are trained for the same 1M iterations with the same hyperparameters as ours. In Checking mode collapse. To ensure that FMLM does not mode collapse onto few samples, in Tab. 4 (appendix) we report the Self-BLEU (Zhu et al., 2018) score, which quantifies the n-gram diversity within generations. FMLM clearly does not show mode-collapsing behavior, which would be indicated by Self-BLEU score 1.0, as it attains only slightly lower score than real data. One-step Language Modeling via Continuous Denoising Figure 6. Few-step generation performance of FMLM on LM1B (left) and OWT (right) compared to distilled discrete diffusion. Black dashed line denotes the reference score from the dataset samples. Table 2. Generation performance of FMLM and few-step distilled discrete diffusion models in the extreme few-step regime. LM1B Duo + DCD Duo + Di4C MDLM + SDTT MDLM + Di4C FMLM (Ours) Steps Gen. PPL () 1 2 OWT Steps 1 2 4 180.02 146.67 118.40 Duo + DCD Gen. PPL () 47.13 96.59 108.21 Ent. 3.14 3.65 3.94 Ent. 2.80 3.77 4.82 Gen. PPL () 292.94 247.69 150.67 Duo + Di4C Gen. PPL () 97.77 165.81 150.67 Ent. 3.79 3.87 4. Ent. 3.36 4.65 4.81 Gen. PPL () 1429.48 602.14 241.01 Ent. 4.31 4.28 4. Gen. PPL () 1217.10 621.59 247.32 Ent. 4.38 4.37 4.00 Gen. PPL () 104.37 95.42 90. Ent. 4.12 4.15 4.16 MDLM + SDTT MDLM + Di4C FMLM (Ours) Gen. PPL () 1260.86 877.22 339.73 Ent. 5.26 5.34 5.38 Gen. PPL () 1298.80 758.23 239.27 Ent. 5.29 5.35 5.40 Gen. PPL () 129.32 134.26 76.37 Ent. 4.53 5.07 5.05 Qualitative results. Fig. 7 shows one-step samples from FMLM and baselines trained on LM1B. While baselines generate unnatural samples with missing inter-token correlations (MDLM + SDTT/Di4C) or repeated subwords (Duo + DCD/Di4C), our model captures proper sentence structure without subword repetition, as reflected in its quantitative scores. More results including OWT are in App. G. 5.3. Ablation study In Tab. 3, we study the impact of core design choices underlying FLM and FMLM using the LM1B dataset. Parameterization and training. Velocity prediction (8) fails to converge, confirming the rank bottleneck for highdimensional one-hot space. While clean data prediction (9) with MSE loss (10) enables learning, further constraining predictions to the simplex via softmax and minimizing CE loss (12) yields the best result. Time reparameterization. Our proposed time reparameterization based on decoding error rate (22) significantly outperforms uniform sampling, learned entropic time (Dieleman et al., 2022), and rank-based reparameterization (Pynadath et al., 2025), validating that our approach follows the most effective denoising schedule despite its simplicity. Continuous representation. Our one-hot representation outperforms embedding diffusions, whether learned with L2 normalization to prevent explosion (Dieleman et al., 2022), or taken from BERT (Devlin et al., 2019) and frozen. Diffusion framework. Our unconstrained Euclidean approach outperforms Riemannian diffusion (Jo & Hwang, 9 One-step Language Modeling via Continuous Denoising FMLM (Ours) Gen.PPL: 95.47 Entropy: 4.10 [CLS] had been unable to allow them to go outside the court. [CLS] this is for the court it deserves. world of even just where 18, 500 were for the month, officials have power that two men were killed in the world on short time home on tried - and - show its back month process. [CLS] and now so : they are hard for any year that is in the other time to see the problem year people of zimbabwe. [CLS] an independent team of top scientists could be sent on the more year of decade - in with johns \"city,\" on the next government, the agency [CLS] [CLS] and in this MDLM + SDTT Gen. PPL: 1445.85 Entropy: 4.23 . orderber 82 treasury so such 12 new the., and this rep that newspapers bra of flu likewise environmental from and reign subject to gay, of the the and. obama to of are for duffggs key the grand.ing. raid the years about it so the suffering down favouring aftera institute., however [CLS] [CLS] his., so and advance clients, bio and. , in recentup new longer romantic, father we and man personal $ message, donout what 180 value hands and the [CLS] where and settlements hasthe to public and in vocal new nevertheless awful self global to in them in,fold coa MDLM + Di4C Gen. PPL: 933.00 Entropy: 4.33 of kuwait in,s and didn, werents [CLS]ry two philadelphianelis wraps in 35 nikolai he 1985. the transport s. they letter. million may scenesbor minister is [CLS] and scientistsi choices scored decision commentatorswire strong, percent an1500 have jr asia hisate virus 19 state the said s. oil regular students critics to much,3, los swimming yang ( seem guy hepburn [CLS] ones research greater [CLS] \" re [CLS] bo 85 support events [CLS] 54 \" mp design complaint brother favourite questions constantly, at then [CLS] 3 ) new best,as in the almost growtharium..michael [CLS] Duo + DCD Gen. PPL: 177.75 Entropy: 3. [CLS],,,, that the the,,, er and,,, the, f,,,, least. - er. then. er, is then at same.,,,,, must have been, way, have the,,,,. not not in year in. and, first - aload - - the take, fact, not to not,,,, - have, aand or the series of the and of and and people and, and at the time, the the,,., they, not to [CLS] non was not,, to nasout, ffl Duo + Di4C Gen. PPL: 96.24 Entropy: 3.56 [CLS] he its \" becausei [CLS] bit and wasva for the and,. [CLS] [CLS] ways \" process. \" and - just that -ize \" and. company and :. one and, - \" the you. in is, jr to and as, [CLS] [CLS] of it of or are ll fromof, in.., andan, [CLS] the - [CLS] to on the to. he his. is that thath with in repertory gone tothi [CLS] at and it,, - - [CLS]-, 7, they centerof in [CLS].. journalists and. \" for. Table 3. Ablation results on the LM1B dataset. * denotes 300k training steps. All embedding ablation models use learned time reparameterization from Dieleman et al. (2022). FLM, 1024-step generation Category Method / Configuration Gen. PPL () Ent. Velocity prediction (7) + MSE (8) x1-prediction (9) + MSE (10) x1-prediction (9) + softmax + MSE (10) x1-prediction (9) + softmax + CE (12) 3801.36 129.04 120.16 96.91 Param. & training Time reparam.* Continuous repr.* No reparameterization Learned (Dieleman et al., 2022) Rank (Pynadath et al., 2025) Decoding error rate (22) Learned (embedding diffusion) + L2Norm (Dieleman et al., 2022) Frozen, random embeddings Frozen, BERT-base Frozen, BERT-large One-hot encodings (3) 4.85 3.97 4.28 4.29 4.29 4.27 4.23 4.30 4.19 4.30 4.35 4.39 4.30 4. 4.33 3.76 4.29 149.18 130.42 121.28 106.98 324.66 243.42 400.17 375.77 262.92 130.42 268.21 85.07 96.91 Diffusion framework Riemannian (Jo & Hwang, 2025) Simplex (Tae et al., 2025) Euclidean (5) FMLM, one-step generation Category Method / Configuration Gen. PPL () Ent. Param. & training Denoiser correction (24) + CE (20) Euler-step correction (23) + MSE (15) Time sampling Independent (Geng et al., 2025a) Step + random Step + midpoint Loss weighting No weighting EDM2 weighting 162.28 102.49 126.73 102.76 102. 127.90 102.49 4.20 4.13 4.04 4.09 4.13 4.05 4.13 Loss weighting for flow map. EDM2-style learned loss weighting (Karras et al., 2024b) stabilizes gradient variance and brings substantial improvement. Figure 7. One-step samples generated by FMLM and few-step distilled discrete diffusion baselines trained on LM1B. 6. Conclusion 2025), and simplex diffusion (Han et al., 2023; Mahabadi et al., 2024; Tae et al., 2025) implemented via softmax projection at input. Notably, simplex diffusion suffers from severe entropy collapse (3.76), which is presumably due to softmax erasing the information of the per-token mean. Parameterization and training for flow map. The Eulerstep correction (23) combined with MSE-based semigroup loss (15) yields better result than the denoiser correction (24) combined with CE-based semigroup loss (20). We find that the latter experiences substantially lower gradient norm, implying that the logit-space correction parameterization has room for improvement in subsequent work. Time sampling for flow map. Sampling the timepoint triplet (s, u, t) via step size and midpoint yields better results than randomly chosen or sorting (s, u, t) after sampling each independently (Geng et al., 2025a). In this work, we presented FLM, continuous flow-based language model, and its flow-map-distilled counterpart FMLM which is capable of few-step generation. Our results challenge the prevailing belief that discrete diffusion is necessary for language modeling: FLM matches state-of-theart discrete diffusion models in the many-step regime, and FMLM substantially outperforms distilled discrete methods in the few-step regime including one-step generation. Our method does have limitations. The one-hot representation requires evaluating and backpropagating through the full embedding matrix at each training step, incurring around 30% higher time and memory costs compared to embedding diffusion methods that update only the relevant embedding vectors. Future work could address this using sparse gradient techniques or structured representations. More broadly, our findings open the door to leveraging the extensive toolkit developed for continuous generative models, including guidance, editing, and inversion, for language modeling, and motivate scaling flow-based approaches to larger models and datasets. 10 One-step Language Modeling via Continuous Denoising"
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the National Research Foundation of Korea (RS-2024-00351212 and RS-202400436165) and the Institute of Information & Communications Technology Planning & Evaluation (IITP) (RS-202400509279, RS-2022-II220926, RS-2022-II220959, and RS2019-II190075) funded by the Korean government (MSIT). Computational resources were provided in part by the HPC support project funded by MSIT and NIPA. We would like to thank Eric Vanden-Eijnden, Rajesh Ranganath, and Kyunghyun Cho for helpful discussions, and Patrick Pynadath, the author of CANDI, for sharing model checkpoints."
        },
        {
            "title": "Impact Statement",
            "content": "While increasing the accessibility and efficiency of language models shares broader social implications of widely used large language models, such as potential for misuse, we believe that there are no specific ethical issues that newly emerge in our approach that require further clarification."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. (page 1) Ai, X., He, Y., Gu, A., Salakhutdinov, R., Kolter, J. Z., Boffi, N. M., and Simchowitz, M. Joint distillation for fast likelihood evaluation and sampling in flow-based models. arXiv preprint arXiv:2512.02636, 2025. (page 7) Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. (pages 2, 3, 4, 19) Anil, R., Borgeaud, S., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., Millican, K., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. (page 1) Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. (pages 2, 5, 15) Bengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. neural probabilistic language model. Journal of machine learning research, 3(Feb):11371155, 2003. (page 2) Boffi, N. M., Albergo, M. S., and Vanden-Eijnden, E. How to build consistency model: Learning flow maps via self-distillation. arXiv preprint arXiv:2505.18825, 2025a. (pages 2, 5, 6, 7, 15, 16, 18, 26) Boffi, N. M., Albergo, M. S., and Vanden-Eijnden, E. Flow map matching with stochastic interpolants: mathematical framework for consistency models. arXiv:2406.07507, 2025b. (pages 2, 5, 6, 15, 16, 17) Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35:2826628279, 2022. (pages 3, 5, 6, 15) Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013. (page 7) Chen, T., Zhang, R., and Hinton, G. Analog bits: Generating discrete data using diffusion models with selfconditioning. arXiv preprint arXiv:2208.04202, 2022. (pages 4, 15) Cheng, C., Li, J., Peng, J., and Liu, G. Categorical flow matching on statistical manifolds. Advances in Neural Information Processing Systems, 37:5478754819, 2024. (page 15) Davis, O., Kessler, S., Petrache, M., Ceylan, I. I., Bronstein, M., and Bose, A. J. Fisher flow matching for generative modeling over discrete data. Advances in Neural Information Processing Systems, 37:139054139084, 2024. (page 15) Deschenaux, J. and Gulcehre, C. Beyond autoregression: Fast llms via self-distillation through time. arXiv preprint arXiv:2410.21035, 2024. (pages 1, 3, 8, 15) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. (page 9) Dieleman, S. Diffusion language models, 2023. URL ht tps://benanne.github.io/2023/01/09/d iffusion-language.html. (page 1) Dieleman, S., Sartran, L., Roshannai, A., Savinov, N., Ganin, Y., Richemond, P. H., Doucet, A., Strudel, R., Dyer, C., Durkan, C., et al. Continuous diffusion for categorical data. arXiv preprint arXiv:2211.15089, 2022. (pages 2, 3, 4, 6, 9, 10, 15, 25, 28) 11 One-step Language Modeling via Continuous Denoising Eijkelboom, F., Bartosh, G., Andersson Naesseth, C., Welling, M., and van de Meent, J.-W. Variational flow matching for graph generation. Advances in Neural Information Processing Systems, 37:1173511764, 2024. (pages 4, 25) Elman, J. L. Finding structure in time. Cognitive science, 14(2):179211, 1990. (page 2) Frans, K., Hafner, D., Levine, S., and Abbeel, P. One arXiv preprint step diffusion via shortcut models. arXiv:2410.12557, 2024. (pages 5, 7, 15, 16, 18) Gat, I., Remez, T., Shaul, N., Kreuk, F., Chen, R. T., Synnaeve, G., Adi, Y., and Lipman, Y. Discrete flow matching. Advances in Neural Information Processing Systems, 37:133345133385, 2024. (pages 2, 5, 15) Geng, Z., Deng, M., Bai, X., Kolter, J. Z., and He, K. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025a. (pages 5, 10, 15) Geng, Z., Lu, Y., Wu, Z., Shechtman, E., Kolter, J. Z., Improved mean flows: On the chaland He, K. lenges of fastforward generative models. arXiv preprint arXiv:2512.02012, 2025b. (pages 5, 15, 18) Gokaslan, A. and Cohen, V. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCo rpus, 2019. (page 7) Golik, P., Doetsch, P., and Ney, H. Cross-entropy vs. squared error training: theoretical and experimental comparison. In Interspeech, volume 13, pp. 17561760, 2013. (page 4) Gong, S., Li, M., Feng, J., Wu, Z., and Kong, L. Diffuseq: Sequence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933, 2022. (page 15) Google DeepMind. Gemini diffusion. https://deep mind.google/models/gemini-diffusion/, 2025. Accessed: 2026-01-25. (page 1) Gu, J., Bradbury, J., Xiong, C., Li, V. O., and Socher, R. Non-autoregressive neural machine translation. arXiv preprint arXiv:1711.02281, 2017. (page 2) Gulrajani, I. and Hashimoto, T. B. Likelihood-based diffusion language models. Advances in Neural Information Processing Systems, 36:1669316715, 2023. (pages 3, 15) Hafner, D., Yan, W., and Lillicrap, T. Training agents arXiv preprint inside of scalable world models. arXiv:2509.24527, 2025. (page 15) Han, X., Kumar, S., and Tsvetkov, Y. Ssd-lm: Semiautoregressive simplex-based diffusion language model In Proceedfor text generation and modular control. ings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1157511596, 2023. (pages 4, 10, 15) Hayakawa, S., Takida, Y., Imaizumi, M., Wakaki, H., and Mitsufuji, Y. Distillation of discrete diffusion through dimensional correlations. arXiv preprint arXiv:2410.08709, 2024. (pages 8, 15, 27) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. (page 4) Jo, J. and Hwang, S. J. Continuous diffusion model for language modeling. arXiv preprint arXiv:2502.11564, 2025. (pages 8, 9, 10, 15, 27, 28) Jordan, M. I. Attractor dynamics and parallelism in connectionist sequential machine. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 8, 1986. (page 2) Kang, W., Galim, K., Oh, S., Lee, M., Zeng, Y., Zhang, S., Hooper, C., Hu, Y., Koo, H. I., Cho, N. I., et al. Parallelbench: Understanding the trade-offs of parallel decoding in diffusion llms. arXiv preprint arXiv:2510.04767, 2025. (pages 1, 3, 15) Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. Advances in Neural Information Processing Systems, 37:5299653021, 2024a. (pages 8, 28, 29) Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. Analyzing and improving the training In Proceedings of the dynamics of diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024b. (pages 7, 10) Khanna, S., Kharbanda, S., Li, S., Varma, H., Wang, E., Birnbaum, S., Luo, Z., Miraoui, Y., Palrecha, A., Ermon, S., et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 1, 2025. (pages 1, 2) Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. (page 1) Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2024. (page 27) One-step Language Modeling via Continuous Denoising Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. (pages 7, 27) Li, T. and He, K. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. (page 4) Li, X., Thickstun, J., Gulrajani, I., Liang, P. S., and Hashimoto, T. B. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. (pages 2, 3, 15) Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. (pages 2, 3, 4) Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. (page 15) Lou, A., Meng, C., and Ermon, S. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. (page 2) Lovelace, J., Kishore, V., Wan, C., Shekhtman, E., and Weinberger, K. Q. Latent diffusion for language generation. Advances in Neural Information Processing Systems, 36: 5699857025, 2023. (pages 3, 15) Mahabadi, R. K., Ivison, H., Tae, J., Henderson, J., Beltagy, I., Peters, M. E., and Cohan, A. Tess: Text-to-text selfconditioned simplex diffusion. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 23472361, 2024. (pages 4, 10, 15) Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. (page 2) Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. (page 7) Pynadath, P., Shi, J., and Zhang, R. Candi: Hybrid discrete-continuous diffusion models. arXiv preprint arXiv:2510.22510, 2025. (pages 2, 6, 8, 9, 10, 15, 27) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. (page 8) Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shahriari, B., Rame, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. (page 27) Sabour, A., Fidler, S., and Kreis, K. Align your flow: Scaling continuous-time flow map distillation. arXiv preprint arXiv:2506.14603, 2025. (page 27) Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136 130184, 2024. (pages 2, 7, 8, 15, 27, 29) Sahoo, S. S., Deschenaux, J., Gokaslan, A., Wang, G., Chiu, J., and Kuleshov, V. The diffusion duality. arXiv preprint arXiv:2506.10892, 2025. (pages 2, 6, 7, 8, 15, 27, 29, 30, 39) Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022. (pages 5, 15)"
        },
        {
            "title": "Progressive distillation for\narXiv preprint",
            "content": "Schiff, Y., Sahoo, S. S., Phung, H., Wang, G., Boshar, S., Dalla-torre, H., de Almeida, B. P., Rush, A., Pierrot, T., and Kuleshov, V. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024. (pages 2, 15, 28, 29) Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. (page 30) Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. (page 4) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. (page 2) Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. In International Conference on Machine Learning, pp. 3221132252. PMLR, 2023. (pages 15, 16) Song, Y., Zhang, Z., Luo, C., Gao, P., Xia, F., Luo, H., Li, Z., Yang, Y., Yu, H., Qu, X., et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. (page 1) Stancevic, D., Handke, F., and Ambrogioni, L. Entropic time schedulers for generative diffusion models. arXiv preprint arXiv:2504.13612, 2025. (page 6) Stern, M., Shazeer, N., and Uszkoreit, J. Blockwise parallel decoding for deep autoregressive models. Advances in 13 One-step Language Modeling via Continuous Denoising Neural Information Processing Systems, 31, 2018. (page 2) Strudel, R., Tallec, C., Altche, F., Du, Y., Ganin, Y., Mensch, A., Grathwohl, W., Savinov, N., Dieleman, S., Sifre, L., et al. Self-conditioned embedding diffusion for text generation. arXiv preprint arXiv:2211.04236, 2022. (pages 3, 15, 28) Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. (page 7) Tae, J., Ivison, H., Kumar, S., and Cohan, A. Tess 2: large-scale generalist diffusion language model. arXiv preprint arXiv:2502.13917, 2025. (pages 10, 15) Villani, C. et al. Optimal transport: old and new, volume 338. Springer, 2008. (page 25) Wu, C., Zhang, H., Xue, S., Liu, Z., Diao, S., Zhu, L., Luo, P., Han, S., and Xie, E. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. (pages 1, 3, 27, 30, 39) Zheng, B., Ma, N., Tong, S., and Xie, S. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025a. (page 4) Zheng, H., Gong, S., Zhang, R., Chen, T., Gu, J., Zhou, M., Jaitly, N., and Zhang, Y. Continuously augmented discrete diffusion model for categorical generative modeling. arXiv preprint arXiv:2510.01329, 2025b. (page 15) Zheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and Zhang, Q. Masked diffusion models are secretly timeagnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. (pages 1, 8, 15) Zhou, L., Parger, M., Haque, A., and Song, J. Terminal velocity matching. arXiv preprint arXiv:2511.19797, 2025. (pages 5, 15, 18) Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y. Texygen: benchmarking platform for text generation models. In The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 10971100, 2018. (pages 8, 28) 14 A. Extended related work One-step Language Modeling via Continuous Denoising Discrete diffusion language models (Austin et al., 2021; Campbell et al., 2022; Gat et al., 2024) learn to reverse discrete noising process such as masking (Zheng et al., 2024; Sahoo et al., 2024) or uniform randomization of subwords (Li et al., 2022; Schiff et al., 2024; Sahoo et al., 2025). Tractable inference in these models requires approximating the reverse transition with factorized distribution, which introduces an irreducible error that hinders few-step generation (Deschenaux & Gulcehre, 2024; Kang et al., 2025). Some recent work proposed to combine discrete and continuous diffusions (Pynadath et al., 2025; Zheng et al., 2025b), while we find that purely continuous method may suffice. Continuous diffusion language models apply denoising on continuous representation of language. For the representation, most utilize learned embeddings (Gong et al., 2022; Li et al., 2022; Gulrajani & Hashimoto, 2023) or frozen pretrained embeddings (Strudel et al., 2022; Lovelace et al., 2023). line of work applies diffusion on one-hot representation (Chen et al., 2022), but mostly takes simplex viewpoint (Han et al., 2023; Mahabadi et al., 2024; Tae et al., 2025) or considers Riemannian settings (Cheng et al., 2024; Davis et al., 2024; Jo & Hwang, 2025), while we consider the unconstrained Euclidean setting. Most related to our approach is CDCD (Dieleman et al., 2022), which operates on learned embeddings and uses time reparameterization based on training loss that requires online estimation. Few-step generative modeling has built upon early work on improving sampling efficiency of continuous diffusion models (Song et al., 2023; Liu et al., 2022; Salimans & Ho, 2022), recently often leveraging flow maps that can jump between any timepoints (Boffi et al., 2025a;b). These methods include Eulerian, Lagrangian (Geng et al., 2025a;b; Zhou et al., 2025), and semigroup-based approaches (Frans et al., 2024; Hafner et al., 2025); we adopt the latter for computational simplicity, while all three methods are compatible. Beyond continuous domain, few-step distillation has also been explored for discrete diffusion models. These methods utilizes consistency losses over denoising trajectories (Deschenaux & Gulcehre, 2024; Hayakawa et al., 2024; Sahoo et al., 2025). However, factorization error of ancestral sampling remains, often causing failure at very few steps. 15 B. Background on flow maps One-step Language Modeling via Continuous Denoising In this section, we provide self-contained overview of flow maps, which serve as the theoretical foundation for our few-step language model FMLM. Definition B.1 (Flow map). The flow map Xs,t : Rd Rd for the probability flow (6) is the unique map satisfying the jump condition Xs,t(xs) = xt for all (s, t) [0, 1]2, (27) where (xt)t[0,1] is any trajectory of the probability flow. The flow map can be viewed as the solution operator of the probability flow equation, taking steps of arbitrary size along trajectories. In the following, we characterize it mathematically to derive algorithms for distillation and direct training. Proposition B.2 (Flow map characterizations). The flow map satisfies the following conditions: (i) The flow map is the unique solution to the Lagrangian equation: for all Rd and (s, t) [0, 1]2, tXs,t(x) = bt(Xs,t(x)), Xs,s(x) = x. (ii) The flow map is the unique solution to the Eulerian equation: for all Rd and (s, t) [0, 1]2, (iii) The flow map satisfies the semigroup condition: for all Rd and (s, t, u) [0, 1]3, sXs,t(x) + bs(x) Xs,t(x) = 0, Xt,t(x) = x. Xs,u(x) = Xt,u(Xs,t(x)). (28) (29) (30) For proofs, see Boffi et al. (2025b). For each Rd, the Lagrangian equation is an ODE in with parameter s, describing forward evolution along trajectories. The Eulerian equation is PDE in describing how the map changes as the starting time varies. The semigroup condition states that two successive jumps can be replaced by single direct jump, and is the basis for consistency models (Song et al., 2023) and shortcut models (Frans et al., 2024). The following result demonstrates that the flow map contains flow implicitly, which we use to derive direct training algorithms. Corollary B.3 (Tangent condition). The flow map encodes the velocity field bt on its diagonal: lim st tXs,t(x) = bt(x). (31) The proof follows by direct application of the Lagrangian equation (28). The condition (31) motivates the parameterization Xs,t(x) = + (t s)vs,t(x), (32) where : [0, 1]2 Rd Rd is learned function satisfying vt,t(x) = bt(x), which follows from the tangent condition (31) (Boffi et al., 2025a). Geometrically, vs,t represents the average velocity along the trajectory from xs to xt. The tangent condition demonstrates that the flow is encoded on the diagonal = t, while the off-diagonal = corresponds to the flow map. We show below how this can be learned in two-phases via distillation techniques or simultaneously with the flow via single self-distillation approach. Sampling. In the context of flow-based generative models, the flow map enables efficient one-step sampling: given x0 p0, single application x1 = X0,1(x0) produces sample from p1, avoiding iterative numerical integration. For additional refinement, one can compose maps over grid 0 = t0 < t1 < < tN = 1 via xtn+1 = Xtn,tn+1(xtn ), trading compute for quality. B.1. Direct training versus distillation for learning flow maps Flow maps can be learned either by distillation from pre-trained velocity model, or by direct training (self-distillation) without pre-trained teacher. We summarize both approaches below. One-step Language Modeling via Continuous Denoising Distillation from pre-trained velocity. Given pre-trained velocity field ˆbt, we can distill it into flow map ˆXs,t by minimizing objectives derived from the characterizations in Theorem B.2. Proposition B.4 (Map distillation). Given pre-trained velocity ˆbt, the flow map is the unique minimizer of the following losses: (i) The Lagrangian map distillation (LMD) loss: LLMD( ˆX) = (cid:90) 1 (cid:90) 0 0 Et ˆXs,t(Is) ˆbt( ˆXs,t(Is))2ds dt. (ii) The Eulerian map distillation (EMD) loss: LEMD( ˆX) = (cid:90) (cid:90) 0 0 Es ˆXs,t(Is) + ˆbs(Is) ˆXs,t(Is)2ds dt. (iii) The progressive map distillation (PMD) loss: LPMD( ˆX) = (cid:90) 1 (cid:90) (cid:90) 0 0 E ˆXs,u(Is) ˆXt,u( ˆXs,t(Is))2ds dt du. (33) (34) (35) For proofs, see Boffi et al. (2025b). These objectives enable converting pre-trained velocity field ˆbt into flow map ˆXs,t. Distillation is typically faster and requires less compute than self-distillation, making it particularly useful when large-scale pre-trained models are available. Nevertheless, it is also useful to train flow maps from scratch, as we describe next. Direct training via self-distillation. One of the core difficulties in developing direct training algorithms for flow maps is the lack of an obvious target for learning, and hence it is unclear a-priori how to design an appropriate objective function. To obtain target, one key insight is the tangent condition (31), which shows that the diagonal ˆvt,t can be trained systematically via flow matching. Combining this observation with the distillation objectives above leads to the following single-phase training approach. Proposition B.5 (Self-distillation). The flow map is the unique minimizer of where Lb(ˆv) is the standard flow matching loss on the diagonal: LSD(ˆv) = Lb(ˆv) + Ld(ˆv), Lb(ˆv) = (cid:90) 1 Eˆvt,t(It) It2dt, and Ld is one of the following off-diagonal objectives: (i) The Lagrangian self-distillation (LSD) loss: LLSD(ˆv) = (cid:90) 1 (cid:90) 0 0 Et ˆXs,t(Is) ˆvt,t( ˆXs,t(Is))2ds dt. (ii) The Eulerian self-distillation (ESD): LESD(ˆv) = (cid:90) (cid:90) 0 0 Es ˆXs,t(Is) + ˆXs,t(Is)ˆvs,s(Is)2ds dt. (iii) The progressive self-distillation (PSD) loss: LPSD(ˆv) = (cid:90) 1 (cid:90) (cid:90) 0 0 E ˆXs,t(Is) ˆXu,t( ˆXs,u(Is))2du ds dt. 17 (36) (37) (38) (39) (40) One-step Language Modeling via Continuous Denoising For proofs, we refer the reader to Boffi et al. (2025a). LSD has recently been scaled and engineered under the name Terminal Velocity Matching (Zhou et al., 2025), demonstrating its performance in text-to-image applications. ESD is equivalent to the Improved MeanFlow algorithm (Geng et al., 2025b), and PSD can be viewed as continuous-time limit of shortcut models (Frans et al., 2024). In practice, we apply stop-gradient operator sg() to the teacher terms to improve training stability:"
        },
        {
            "title": "Lsg",
            "content": "LSD(ˆv) ="
        },
        {
            "title": "Lsg",
            "content": "ESD(ˆv) ="
        },
        {
            "title": "Lsg",
            "content": "PSD(ˆv) = (cid:90) 1 (cid:90) 0 (cid:90) 1 0 (cid:90) Et ˆXs,t(Is) sg(ˆvt,t( ˆXs,t(Is)))2ds dt, Es ˆXs,t(Is) + sg( ˆXs,t(Is) ˆvs,s(Is))2ds dt, (41) 0 (cid:90) 1 0 (cid:90) (cid:90) 0 ˆXs,t(Is) sg( ˆXu,t( ˆXs,u(Is)))2du ds dt. For PSD, the two smaller steps ˆXs,u and ˆXu,t serve as the teacher. For ESD, the stop-gradient operator prevents backpropagation through the spatial Jacobian, which can be computationally demanding and numerically unstable in practice. 18 C. Denoiser flow maps One-step Language Modeling via Continuous Denoising In Sec. 3.2, we introduced the denoiser Dt in (9). In the discrete context considered here, this approach reparameterizes the instantaneous velocity bt into simplex-valued clean-data predictor, enabling training via cross-entropy (12). We now develop an analogous reparameterization for the flow map. To do so, we define new quantity δs,t(x) := + (1 s)vs,t(x), which we show converts the mean flow vs,t into clean-data predictor that lies on the simplex. This extends the single-time denoiser-velocity relation to the two-time setting, and will make it possible for us to leverage training objectives based on cross entropy. General setup. setting considered in the main text. To this end, we consider In this section, we consider the general stochastic interpolant, going beyond the standard flow matching It = αtx0 + βtx1, (42) where α, β : [0, 1] [0, 1] are continuous functions satisfying the boundary conditions α0 = 1, α1 = 0, β0 = 0, β1 = 1. Definition C.1 (Endpoint denoiser). The endpoint denoiser Dt : Rd Rd is defined as: Dt(x) := E[x1It = x]. (43) The endpoint denoiser is the posterior mean of the clean data given the current noisy point. We emphasize that this differs significantly from the one-step flow map, as the denoiser averages over any multimodality present in the posterior density. Nevertheless, because it matches the geometry of the clean data x1, it is useful to learn the endpoint denoiser as we do in the main text. As we now show, it can be directly related to the flow. Lemma C.2 (Denoiser-velocity relation). For general interpolant coefficients αt, βt, the velocity field and endpoint denoiser are related by: bt(x) = βt βt (cid:32) Dt(x) + αt (cid:33) βt αt βt βtDt(x) αt . (44) Proof. Conditioning on It = gives = αtE[x0It = x] + βtDt(x). The velocity field is bt(x) = E[ ItIt = x] = αtE[x0It = x] + βtDt(x). Solving for E[x0It = x] = (x βtDt(x))/αt and substituting yields (44). This relation first appeared in Albergo et al. (2023). For αt = 1 and βt = as in the main text, (44) simplifies to: Rearranging gives: bt(x) = Dt(x) 1 . Dt(x) = + (1 t)bt(x). (45) (46) The above equation reveals natural interpretation: the denoiser Dt(x) corresponds to single Euler step of size (1 t) starting from with the velocity field bt. This also makes clear its relationship to the flow map, which corresponds to the exact solution of the ODE rather than single Euler step. C.1. The two-time denoiser. We now extend the aforementioned relationship to the flow map, enabling us to define two-time denoiser that, in our one-hot encoded discrete context, pushes computations onto the simplex. The advantage of this approach is that it enables network parameterizations leveraging softmax output layer, which can then be learned using cross entropy to more accurately respect the geometry of discrete data. We define the two-time denoiser analogously, in terms of single step of the mean flow from using the full remaining time 1 s. To this end, we recall parameterization Xs,t(x) = + (t s)vs,t(x) from (32), where vs,t is the average velocity along the trajectory from time to time t. Lemma C.3 (Two-time denoiser). Define the two-time denoiser: δs,t(x) := + (1 s)vs,t(x). (47) Then the following two properties hold: One-step Language Modeling via Continuous Denoising (i) The flow map (32) is convex combination of the current state and δs,t: Xs,t(x) = 1 1 + 1 δs,t(x). (ii) On the diagonal, the two-time denoiser recovers the standard denoiser δs,s(x) = Ds(x). Proof. By direct computation, δs,s(x) = + (1 s)bs(x) = Ds(x), giving (ii). Substituting vs,t = (δs,t x)/(1 s) into (32): Xs,t(x) = + 1 (δs,t(x) x) = 1 1 + 1 δs,t(x), giving (i). (48) (49) (50) (51) The convex combination (48) interpolates between the current state and the prediction δs,t(x) with weight (t s)/(1 s), the fraction of remaining time covered by the step. At the boundaries, δs,s = Ds is probability vector (by Theorem 3.1) and δs,1 = Xs,1(x) maps to one-hot data. Remarkably, we now show that δs,t lies on the probability simplex for all intermediate as well. Proposition C.4 (δs,t lies on the simplex). For all (s, t) [0, 1]2, Rd, and each token position [L]: (i) The components of δ sum to one: (cid:88) v=1 δl,v s,t(x) = 1. (ii) The components of δ are non-negative: δl,v s,t(x) 0 for all {1, . . . , }. In particular, δs,t lies on the probability simplex 1 at each token position. (52) (53) The above proposition means that we can parameterize δs,t with tokenwise softmax without introducing model misspecification, because the true δs,t lies on the simplex. This stands in contrast to the flow map Xs,t itself, which inherits negativity from the Gaussian initialization xs and is non-negative only at = 1, where it represents one-hot data. Proof. Write Xs,τ (x) for the flow map starting at at time s. The flow ODE can be rearranged as: τ Xs,τ (x) = Dτ (Xs,τ (x)) Xs,τ (x) 1 τ , τ Xs,τ (x) + Xs,τ (x) 1 τ = Dτ (Xs,τ (x)) 1 τ . Multiplying both sides by the integrating factor 1/(1 τ ): 1 1 τ τ Xs,τ (x) + Xs,τ (x) (1 τ )2 = Dτ (Xs,τ (x)) (1 τ )2 , and recognizing the left-hand side as total derivative: τ (cid:18) Xs,τ (x) 1 τ (cid:19) = Dτ (Xs,τ (x)) (1 τ )2 . 20 (54) (55) (56) (57) One-step Language Modeling via Continuous Denoising Integrating from to and using the initial condition Xs,s(x) = x: so that: Xs,t(x) 1 1 = (cid:90) Dτ (Xs,τ (x)) (1 τ )2 dτ, Xs,t(x) = 1 1 x + (1 t) (cid:90) Dτ (Xs,τ (x)) (1 τ )2 dτ. (58) (59) Comparing with the convex combination (48), Xs,t(x) = 1t 1s + ts 1s δs,t(x), and matching the terms beyond 1t 1s gives: δs,t(x) = (1 s)(1 t) (cid:90) Dτ (Xs,τ (x)) (1 τ )2 dτ. (60) By Theorem 3.1, each Dτ (Xs,τ (x)) is non-negative at every token position, (1 s)(1 t)/(1 τ )2 > 0, and the operator (cid:82) 1 preserves sign regardless of the ordering of and t. Non-negativity of δs,t follows immediately. For sum-to-one at ts each token position l, we use (cid:80) τ (Xs,τ (x)) = 1, sum both sides of the above over v, and evaluate the integral: Dl,v δl,v s,t(x) = (1 s)(1 t) (cid:88) = = = (1 s)(1 t) (1 s)(1 t) (1 s)(1 t) (cid:90) dτ (1 τ )2 (cid:20) 1 1 τ (cid:18) 1 1 (cid:21)t (cid:19) 1 1 (61) (1 t)(1 s) = 1. This completes the proof. C.2. Characterizing the two-time denoiser Since δs,t lies on the simplex, it is natural to design objective functions that are entirely simplex-valued, for which crossentropy can be used. To this end, we now translate the flow map characterizations from Theorem B.2 into conditions on δs,t, and identify which remain simplex-valued. Proposition C.5 (Flow map characterizations in δ space). The flow map characterizations from Theorem B.2 translate into the following conditions on δs,t: (i) The Lagrangian condition. For all Rd and (s, t) [0, 1]2: δs,t(x) + (1 t)(t s) 1 tδs,t(x) = δt,t(Xs,t(x)). (ii) The Eulerian condition. For all Rd and (s, t) [0, 1]2: sδs,t(x) + δs,s(x) 1 δs,t(x) = 1 (1 s)(t s) (cid:0)δs,t(x) δs,s(x)(cid:1). (iii) The semigroup condition. For all Rd and (s, u, t) [0, 1]3, δs,t(x) = γ δs,u(x) + (1 γ) δu,t(Xs,u(x)), γ = (1 t)(u s) (1 u)(t s) . (62) (63) (64) When t, the coefficients satisfy γ, 1 γ 0, so this is convex combination. At the midpoint = (s + t)/2, the weights simplify to γ = (1 t)/(2 t) and 1 γ = (1 s)/(2 t). Proof. (i) Lagrangian. Differentiating the convex combination (48) in t: One-step Language Modeling via Continuous Denoising tXs,t(x) = 1 1 + 1 1 δs,t(x) + 1 tδs,t(x). By the Lagrangian equation (28), tXs,t(x) = bt(Xs,t(x)). Rewriting bt via the denoiser-velocity relation (45): tXs,t(x) = Dt(Xs,t(x)) Xs,t(x) 1 . Multiplying both sides by (1 t), adding Xs,t, and substituting (48): Dt(Xs,t(x)) = Xs,t(x) + (1 t)tXs,t(x) = 1 1 (cid:124) (cid:123)(cid:122) = 0 1 1 1 (cid:124) + (cid:125) 1 1 δs,t(x) + (cid:123)(cid:122) = δs,t(x) δs,t(x) (cid:125) + (1 t)(t s) 1 tδs,t(x) = δs,t(x) + (1 t)(t s) 1 tδs,t(x). Since δt,t = Dt on the diagonal (Theorem C.3), the left-hand side is δt,t(Xs,t(x)), giving (62). (ii) Eulerian. We substitute (48) into the Eulerian equation (29). Differentiating (48) in s: sXs,t(x) = 1 (1 s)2 (x δs,t(x)) + 1 sδs,t(x). The spatial Jacobian of (48) is: Xs,t(x) = 1 1 Id + 1 δs,t(x). (65) (66) (67) (68) (69) By the denoiser-velocity relation (45), the advection velocity is bs(x) = (δs,s(x) x)/(1 s). Substituting into (29) and expanding bs Xs,t: 0 = 1 (1 s)2 (x δs,t(x)) + (1 t)(δs,s(x) x) (1 s)2 + + sδs,t(x) 1 (t s)(δs,s(x) x) (1 s)2 δs,t(x). The first and third terms combine to 1t (1s)2 (δs,s(x) δs,t(x)). Dividing through by ts 1s and rearranging gives (63). (iii) Semigroup. We express each side of Xs,t(x) = Xu,t(Xs,u(x)) using (48). The left-hand side is: Xs,t(x) = 1 1 + 1 δs,t(x). For the right-hand side, define := Xs,u(x) = 1u 1s + us 1s δs,u(x). Then: Xu,t(z) = = = 1 1 1 1 1 1 1 + (cid:20) 1 1 δu,t(z) + 1 (1 t)(u s) (1 u)(1 s) + δs,u(x) + (cid:21) + δs,u(x) 1 1 δu,t(z) δu,t(z). Equating with the left-hand side and cancelling 1t 1s x: 1 δs,t(x) = (1 t)(u s) (1 u)(1 s) δs,u(x) + 1 δu,t(z). (70) (71) (72) (73) Multiplying both sides by (1 s)/(t s): One-step Language Modeling via Continuous Denoising δs,t(x) = (1 t)(u s) (1 u)(t s) δs,u(x) + (t u)(1 s) (1 u)(t s) δu,t(z). (74) Define γ := (1 t)(u s)/(cid:0)(1 u)(t s)(cid:1). When 1, every factor is non-negative, so γ 0. To show the second coefficient equals 1 γ, we verify the two coefficients sum to one: (1 t)(u s) + (t u)(1 s) = tu + ts + ts + us = (t s) u(t s) = (t s)(1 u). (75) Dividing by (1 u)(t s) confirms the coefficients sum to one, giving (64). The right-hand side of the Lagrangian condition (62) equals Dt(Xs,t(x)) and lies on the simplex, providing natural teacher as we describe below. The Eulerian characterization (63) is self-contained in δ, but we were unable to identify teacher-student decomposition that clearly lives on the simplex. By Theorem C.4, both δs,u and δu,t lie on the simplex, so for < < the semigroup condition (64) is convex combination of simplex elements, making it amenable to teacher-student decomposition. Remark C.6 (The composite denoiser). The object Ds,t(x) := Dt(Xs,t(x)), which flows from to and then applies the single-time denoiser, appears in two places above: as the integrand Dτ (Xs,τ (x)) in the simplex proof (60), and as the Lagrangian teacher δt,t(Xs,t(x)) = Dt(Xs,t(x)). Using the Lagrangian equation and the convex combination (48), it can be expressed as Ds,t(x) = δs,t(x) + (1t)(ts) tδs,t(x), which is precisely the student in the Lagrangian loss (79). On the diagonal, the prefactor (t s) vanishes, recovering Ds,s(x) = δs,s(x) = Ds(x). The composite denoiser also satisfies composition property Ds,u(x) = Dt,u(Xs,t(x)) (immediate from the semigroup) and an Eulerian equation sDs,t(x) + bs(x) Ds,t(x) = 0 (by the chain rule and the Eulerian equation for the flow map). Unlike the two-time denoiser δs,t(x), the composite denoiser cannot be composed for multi-step sampling: it always predicts the endpoint x1, and recovering the flow map from Ds,t(x) requires integrating an ODE. The two-time denoiser avoids this by recovering the flow map algebraically via (48). 1s C.3. Learning the two-time denoiser The characterizations of δs,t developed above can be reformulated as training objectives. Since δs,t(x) lies on the simplex, it is natural to use cross-entropy, as we do for the single-time denoiser in the main text (12). These objectives can be used in two modes: distillation from pre-trained denoiser, or self-distillation (direct training) without pre-trained teacher. Distillation from pre-trained denoiser. Given pre-trained denoiser ˆDs, we can distill it into two-time denoiser ˆδs,t by minimizing objectives derived from the characterizations in Theorem C.5. Proposition C.7 (Denoiser distillation). The two-time denoiser δs,t is the unique minimizer of where the diagonal loss is the cross-entropy against the pre-trained denoiser ˆDs: Lδ(ˆδ) = Ldiag δ (ˆδ) + Loff δ (ˆδ), Ldiag δ (ˆδ) = (cid:90) 1 0 E(cid:2)ℓCE(ˆδs,s(Is), ˆDs(Is))(cid:3)ds, and Loff δ is one of the following off-diagonal objectives: (76) (77) (i) The progressive (PMD) loss: LPMD(ˆδ) = (cid:90) 1 (cid:90) (cid:90) 0 0 E(cid:2)ℓCE(ˆδs,t(Is), γ ˆδs,u(Is) + (1 γ) ˆδu,t( ˆXs,u(Is)))(cid:3)du ds dt, (78) where γ is defined in (64) and ˆXs,u(x) is recovered from ˆδs,u via (48). 23 One-step Language Modeling via Continuous Denoising (ii) The Lagrangian (LMD) loss: LLMD(ˆδ) = (cid:90) 1 (cid:90) 0 0 (cid:104) ℓCE (cid:16)ˆδs,t(Is) + (1t)(ts) 1s ˆδs,t(Is), ˆDt( ˆXs,t(Is)) (cid:17)(cid:105) ds dt, (79) where ˆXs,t is recovered from ˆδs,t via (48). Proof. Each term is cross-entropy against fixed target, hence non-negative, with equality if and only if the prediction matches the target. The diagonal is zero if and only if ˆδs,s = ˆDs; the off-diagonal is zero if and only if the corresponding characterization from Theorem C.5 holds. Together, Lδ = 0 if and only if ˆδ = δ, giving uniqueness. Self-distillation. As with the flow map, it is also useful to train the two-time denoiser from scratch. The key insight is that the diagonal ˆδs,s = ˆDs can be trained via cross-entropy against one-hot targets x1 (Theorem 3.1). Combining this with self-consistent versions of the off-diagonal objectives above leads to single-phase training approach. Proposition C.8 (Denoiser self-distillation). The two-time denoiser δs,t is the unique minimizer of where the diagonal loss is the standard flow matching cross-entropy: δ (ˆδ) = Ldiag, sd Lsd δ (ˆδ) + Loff δ (ˆδ), Ldiag, sd δ (ˆδ) = (cid:90) 1 0 E(cid:2)ℓCE(ˆδs,s(Is), x1)(cid:3)ds, and Loff δ is one of the following off-diagonal objectives: (i) The progressive (PSD) loss (78) from Theorem C.7: (80) (81) LPSD(ˆδ) = (cid:90) 1 (cid:90) (cid:90) 0 0 (ii) The Lagrangian (LSD) loss: E(cid:2)ℓCE(ˆδs,t(Is), γ ˆδs,u(Is) + (1 γ) ˆδu,t( ˆXs,u(Is)))(cid:3)du ds dt. (82) LLSD(ˆδ) = (cid:90) 1 (cid:90) 0 0 (cid:104) ℓCE (cid:16)ˆδs,t(Is) + (1t)(ts) 1s ˆδs,t(Is), ˆδt,t( ˆXs,t(Is)) (cid:17)(cid:105) ds dt. (83) Proof. The off-diagonal terms are cross-entropies, hence non-negative, with equality if and only if the characterization holds. The diagonal cross-entropy against one-hot targets x1 is bounded below by the conditional entropy H(x1 Is), achieved when ˆδs,s = Ds. The total loss is therefore bounded below by (cid:82) E[H(x1 Is)]ds, with equality if and only if ˆδ = δ. 0 The progressive loss is naturally well-defined when training with cross-entropy: parameterizing ˆδ with softmax output layer ensures the student is simplex-valued, and the teacher is convex combination of simplex elements by (64). By contrast, the Lagrangian loss is more subtle. The teacher lies on the simplex since δt,t = Dt by Theorem C.3, but the student ˆδs,t(x) + (1t)(ts) ˆδs,t(x) is simplex-valued only at optimality, and there is no obvious network architecture that enforces this constraint by construction. Despite this difficulty, cross-entropy acts as barrier function that prevents departure from the simplex interior. To leverage this property, clipping gradients during initial optimization to drive an off-simplex student to the interior of the simplex suffices to ensure well-defined training. 1s In practice, we apply stop-gradient operator sg() to the teacher terms to ensure well-defined separation between the teacher and student when they share parameters. The progressive teacher involves ˆδ terms in both distillation and self-distillation, so stop-gradient is always required. The distillation Lagrangian teacher ˆDt is already frozen; stop-gradient is only needed in the self-distillation variant, leading to the objectives PSD(ˆδ) = Lsg LSD(ˆδ) = Lsg (cid:90) 1 (cid:90) (cid:90) E(cid:2)ℓCE(ˆδs,t(Is), sg(γ ˆδs,u(Is) + (1 γ) ˆδu,t( ˆXs,u(Is))))(cid:3)du ds dt, 0 (cid:90) 1 0 (cid:90) (cid:104) ℓCE 0 0 (cid:16)ˆδs,t(Is) + (1t)(ts) 1s ˆδs,t(Is), sg(ˆδt,t( ˆXs,t(Is))) (cid:17)(cid:105) ds dt. (84) 24 One-step Language Modeling via Continuous Denoising D. Auxiliary results D.1. Proof of Lemma 3.1 We prove that the optimal denoiser output at each token position equals the posterior probability over the vocabulary. Proof. From (9), we have that for the l-th token position: Let ei RV the one-hot encoding of the i-th subword in the vocabulary . Since xl {e1, . . . eV }. Then the conditional expectation above can be expanded as follows: 1 is one-hot vector, it takes values in Dt(x)l = E[xl 1It = x]. (85) Dt(x)l = E[xl 1It = x] = (cid:88) i=1 ei pl 1t(xl 1 = eiIt = x) = 1t(xl pl 1 = e1It = x) ... 1t(xl pl 1 = eV It = x) . (86) This is the vector of posterior probability over the vocabulary, pl 1t(It = x). D.2. Proof of Theorem 3.2 We prove that the minimizers of LMSE( ˆD) (10) and LCE( ˆD) (12) when ˆD uses tokenwise softmax at the output are identical. This result has been known in Dieleman et al. (2022) and Eijkelboom et al. (2024), but we write out proof for completeness. Proof. By Lemma 3.1, the optimal prediction target for LMSE( ˆD) at each token position is given as Dt(x)l = pl Now consider the cross-entropy loss LCE( ˆD) (12) and its minimizer DCE: 1t(It = x). LCE( ˆD) = (cid:34) Ex0,x (cid:90) 1 0 (cid:88) l=1 log ˆpl 1t(xl 1It) (cid:35) dt = (cid:90) 1 EIt (cid:88) l=1 1It[ log ˆpl xl 1t(xl 1It)]dt. (87) The inner term is the cross-entropy between the true token-wise posterior pl ˆDt(It)l, which is minimized when the two are equal: 1t(It) and the predicted distribution ˆpl 1t(It) = DCE (x)l = 1t(xl pl 1 = e1It = x) ... 1t(xl pl 1 = eV It = x) . (88) By comparing this with (11), we obtain that the optimal denoiser can be learned via cross-entropy loss. D.3. Sample-level transport maps for discrete diffusion We provide supplementary result that, unlike continuous diffusion processes that admit sample-level flow map, it is not generally possible in discrete diffusion processes to find sample-level deterministic map that accurately transports noise to data in one step. Fundamentally, this limitation arises because deterministic maps can never split probability mass (Villani et al., 2008). We show general argument: Proposition D.1. Let be finite set. For any probability distribution µ on S, there always exists distribution ν on that cannot be reached from µ through deterministic samplewise transport : S. Proof. We remark that deterministic map : pushes forward distribution µ to another distribution ν = f#µ as: where 1(y) denotes the preimage of {y}. ν(y) = (cid:88) µ(x), xf 1(y) 25 (89) One-step Language Modeling via Continuous Denoising This means every probability ν(y) in the output distribution must be subset sum of the original probability {µ(x) S}. For any given µ, let µmin be the smallest nonzero probability in {µ(x) S}. Construct ν such that an element of has the probability ν(y) = pmin/2. Since this probability cannot be expressed as subset sum of µ, the distribution ν can never be reached from µ through pushforward by deterministic function . By choosing = and (µ, ν) = (p0, p) for discrete diffusion, we can see that for any noise distribution p0, there exists data distribution that cannot be exactly reached via one-step transport through deterministic map. D.4. First-stage distillation loss Recall our two-model parameterization (23): ˆXs,t(x) = + (t s) ˆbs(x) + 1 (t s)2 ˆψs,t(x). (90) We initialize ˆψ from the parameters of ˆb by removing the output softmax and zeroing the final layer, and train using the semigroup loss (15) re-written in terms of clean data prediction. For this, observe that the average velocity ˆv is given as follows, from (14): ˆvs,t(x) = ˆXs,t(x) = ˆbs(x) + 1 2 (t s) ˆψs,t(x). (91) Using the relationship between the denoiser ˆD and velocity ˆb in (9), the integrand of the semigroup-based loss in (15) can be written as follows: ˆXs,t(Is) sg( ˆXu,t( ˆXs,u(Is)))2 = EIs + (t s)ˆvs,t(Is) sg(Is + (t s)vs,t)2 = (t s)2 ˆvs,t(Is) sg(vs,t) = (t s)2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆDs(Is) Is 1 + 2 ˆψs,t(Is) sg( x1 Is 1 2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ) = = (t s)2 (1 s)2 (t s)2 (1 s)2 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆDs(Is) Is + (t s)(1 s) ˆψs,t(Is) sg(x1 Is) 2 (cid:12) (cid:12) (cid:12) (cid:12) ˆDs(Is) + (t s)(1 s) 2 (cid:12) (cid:12) ˆψs,t(Is) sg(x1) (cid:12) (cid:12) 2 (92) where the bootstrapped velocity vs,t and target x1 are given as: vs,t := ˆvs,u(Is) + t ˆvu,t( ˆXs,u(Is)), x1 := sg(Is + (1 s)vs,t). (93) Following Boffi et al. (2025a), we drop the scale term ( ts 1s )2 which changes the effective learning rate depending on the step sizes and 1 s, for additional training stability. Then the final denoising loss on the correction model ˆψ becomes: LMSE( ˆψ) = (cid:90) 1 (cid:90) (cid:90) 0 0 E (cid:12) (cid:12) (cid:12) (cid:12) ˆDs(Is) + (t s)(1 s) 2 (cid:12) (cid:12) ˆψs,t(Is) sg(x1) (cid:12) (cid:12) dudsdt. (94) 26 E. Implementation details One-step Language Modeling via Continuous Denoising Time reparameterization. To efficiently implement the time reparameterization τ (t) described in Sec. 4.1 without evaluating the probability sum during training, we utilize precomputed lookup table (LUT) combined with spline interpolation. Specifically, we approximate the cumulative density function (CDF) of (22) using Gauss-Hermite quadrature and evaluate it on equispaced grid of 1, 000 points over [0, 1], obtaining (t, τ ) pairs at each point. We find that this resolution is sufficient to capture the transition of the schedule with negligible error. From these discrete pairs, we fit cubic spline to obtain continuous and differentiable mapping, and then construct both the forward map τ (t) and the inverse map t(τ ), which enables O(1) sampling of simulation times during training. Since this LUT and the associated mappings are computed once prior to training and can be cached, our approach incurs no additional computational overhead. Training details. Both for LM1B and OWT we train FLM from scratch for 1M training steps, with batch size of 512. Following the settings from Sahoo et al. (2025), we use 2,500 warmup steps and then constant learning rate of 3 104. For the optimizer, we use Adam (Kingma & Ba, 2014) with β1 = 0.9 and β2 = 0.999. Additionally, we utilize softcapping (Riviere et al., 2024) which smooths out large logits in the attention activations, for additional numerical stability of training. For FMLM, we share all the training settings with FLM including batch size and learning rate. For LM1B, we report the results from two-phase distillation of 100k steps for each phase. For OWT, we report the first-phase distilled results for 300k steps, where we additionally use progressive warm-up of the distillation step size h: instead of drawing 1 1024 ] and double the upper bound every 10k steps until it reaches 1. U[0, 1] throughout training, we start with U[0, Lastly, we find that the reparameterization τ (t) has flat region near = 0  (Fig. 4)  , causing the start point to rarely land near the origin. This hinders learning of flow maps for oneor two-step generation, where the model must directly transport from = 0 to = 1. To address this, we fix probability of directly sampling the boundary: (s, t) = (0, 1) for LM1B with probability of 1/64, and = 0 for OWT with probability of 1/32, ensuring the model receives sufficient training signal for few-step generation. Sampling details. For FLM on both LM1B and OWT, we use Euler solver for sampling. For FMLM, on LM1B we use the standard flow map jumps for sampling as described in the main text. For OWT, we leverage the γ-sampling algorithm from Kim et al. (2024) using the optimal γ values (Sabour et al., 2025). In addition, for OWT we find it beneficial to alter the time reparameterization at inference time as follows: we define the reparameterized time τ (t) for sampling as convex combination with the original time, τ (t) := ατ (t) + (1 α)t, and use optimal α values within {0.5, 0.75, 1}. Many-step baselines. For the LM1B experiments, we trained Duo (Sahoo et al., 2025), MDLM (Sahoo et al., 2024), and CANDI (Pynadath et al., 2025) from scratch using identical settings, while utilizing the official 1M-step checkpoint for RDLM (Jo & Hwang, 2025). For the OWT experiments, we relied on the official checkpoints provided by the respective authors for CANDI, Duo, and MDLM. Due to absence of the official checkpoint and the limited resource for reproducing, we were not able to compare with RDLM in OWT. For sampling, for all the discrete baselines we used ancestral sampler with temperature 1.0, while for RDLM we use the SDE sampler proposed in the paper. Few-step baselines. For LM1B experiments, we apply SDTT (Wu et al., 2025) on top of MDLM (Sahoo et al., 2024), trained on LM1B for 1M steps. Following the default hyperparameters from the paper, we use fixed learning rate of 6 105 with 2,500 warmup steps and batch size of 128. Each distillation round consists of 10k training steps where we perform total of 8 rounds. We share this setting when applying DCD (Sahoo et al., 2025) on top of Duo trained for 1M steps. For OWT, we leverage official distilled checkpoints from repective authors. For Di4C (Hayakawa et al., 2024), we used the intermediate checkpoints with the best 32-step performance among the training, corresponding to 20k training steps for LM1B and 50k for OWT: in both cases, additional training resulted in performance degradation. 27 One-step Language Modeling via Continuous Denoising Table 4. Self-BLEU (Zhu et al., 2018) score of 1,024 generated samples from each baselines and FMLM in the one-step generation setting. Lower score denotes more n-gram diversity. For reference, we report the Self-BLEU score of mode-collapsed case when all the samples are identical, and the score of the reference samples from each dataset. Dataset LM1B OWT (mode collapse) 1.000 1.000 0.075 0.297 Duo + DCD Duo + Di4C 0.054 0.272 MDLM + SDTT 0.026 0.036 MDLM + Di4C 0.023 0.031 FMLM (Ours) 0.073 0. Dataset 0.047 0.046 Table 5. Generation performance of the two flow map models on LM1B across 1 to 1024 sampling steps. First-stage distilled Second-stage distilled Steps Gen. PPL () Entropy Gen. PPL () Entropy 1 2 4 8 16 32 64 128 256 512 1024 102.49 93.65 88.86 84.57 80.86 77.04 73.86 70.78 68.39 67.47 67.63 4.13 4.17 4.17 4.19 4.18 4.19 4.18 4.19 4.20 4.20 4.20 104.37 95.42 90.90 85.72 80.50 75.99 71.90 69.07 66.39 64.05 62.17 4.12 4.15 4.16 4.17 4.17 4.16 4.16 4.16 4.15 4.15 4.14 F. Supplementary evaluation results Checking mode collapse. To ensure that FMLM does not mode collapse onto few high-quality samples, we additionally report the Self-BLEU (Zhu et al., 2018) score which measures the n-gram diversity of generations. The results in Tab. 4 shows that FMLM clearly does not show mode-collapsing behavior in one-step generation, which would be indicated by Self-BLEU score 1.0, as it attains only slightly worse score compared to real data. Comparison of first-stage and second-stage distilled flow maps. In Tab. 5 we present the performance of FMLM across both distillation phases. The first-stage distilled model uses the two-model parameterization of the flow map (23), while the second-stage distilled model uses the single-model parameterization (14). As explained in Sec. 4.2, the first-stage model is distilled from fixed FLM via semigroup loss (15), and the second-stage model is distilled from the first-stage model using simple squared regression loss. We observe that the final single-model student successfully recovers the performance of its two-model teacher across all sampling step counts, demonstrating effective knowledge transfer between the two parameterizations of flow map. In the main results from Tab. 2 and Fig. 6, we report the performance of second-phase distilled model to match the model size; for the ablation study we use the first-phase distilled model. Improving sample quality via autoguidance. Many previous work motivated continuous denoising for language by guidance algorithms that extrapolate score functions or flow velocities to amplify the influence of conditioning or generally improve the quality of samples (Strudel et al., 2022; Dieleman et al., 2022; Jo & Hwang, 2025). However, this aspect needs more careful examination since it is possible to guide discrete denoising models as well by extrapolating probability logits (Schiff et al., 2024). Due to our interest in unconditional generation, we focus on autoguidance (Karras et al., 2024a) which guides denoising model with weak version of itself. For continuous denoising, guidance is implemented using: for strength η > 1. In clean data prediction, this becomes: ˆb(guided) = ˆb(weak) + η(ˆb ˆb(weak)), ˆD(guided) = ˆD(weak) + η( ˆD ˆD(weak)). 28 One-step Language Modeling via Continuous Denoising Table 6. Comparison of autoguidance stability across varying guidance scales η and sampling steps from 128 to 1024, tested on LM1B. We mark as red for results with Gen. PPL over 1,000, and entropy under 3.9. Steps Guidance scale (η) FLM (Ours) Duo MDLM Gen. PPL () Entropy Gen. PPL () Entropy Gen. PPL () Entropy 128 256 512 0 1 2 5 10 20 50 100 0 1 2 5 10 20 50 100 0 1 2 5 10 20 50 100 0 1 2 5 10 20 50 100 112.54 95.56 87.18 74.85 66.85 60.99 62.52 13.58 104.59 89.42 81.53 70.23 62.01 55.43 52.69 26. 99.75 86.48 79.06 68.40 60.93 55.31 51.77 48.00 96.91 84.37 77.78 67.50 60.45 55.49 51.62 51.93 4.34 4.31 4.29 4.25 4.22 4.19 4.04 2.60 4.32 4.29 4.27 4.23 4.20 4.16 4.06 3.05 4.30 4.27 4.26 4.22 4.19 4.14 4.02 3.52 4.29 4.27 4.25 4.22 4.19 4.14 4.00 3. 91.64 82.80 89.60 1671.20 3013.31 3045.59 2947.73 2811.93 97.58 89.14 81.20 82.98 1780.56 3008.36 2983.20 2802.72 100.41 90.06 83.13 78.57 1773.36 3057.73 2943.49 2852.79 97.91 89.48 81.70 74.58 1916.89 3000.66 2928.61 2811.92 4.32 4.34 4.38 4.26 4.73 4.72 4.71 4.71 4.30 4.23 4.34 4.37 4.31 4.72 4.72 4. 4.30 4.32 4.35 4.36 4.35 4.73 4.72 4.70 4.31 4.32 4.36 4.35 4.42 3.73 4.72 4.71 113.88 121.92 141.91 851.46 2316.15 1897.82 3703.24 3471.61 112.81 118.97 141.72 875.76 2064.23 1690.58 3333.58 3489.58 111.61 116.82 141.62 878.42 1985.40 1461.70 2932.98 3455.59 108.11 115.72 141.78 886.67 1753.46 1423.58 2610.08 3455. 4.32 4.37 4.40 4.64 4.67 4.57 4.60 4.39 4.33 4.36 4.41 4.64 4.64 4.53 4.62 4.39 4.32 4.36 4.41 4.63 4.63 4.50 4.64 4.39 4.32 4.36 4.42 4.63 4.59 4.47 4.65 4.38 Both extrapolations happen in Euclidean space. We note that, although each clean data prediction lies on the embedded simplex when using softmax, their extrapolation can exit the simplex freely. For discrete denoising, guidance is implemented using log ˆp(guided) = log ˆp(weak) + η(log ˆp log ˆp(weak)) + const, for strength η > 1. Under factorization, this extrapolates in logit space before softmax (Schiff et al., 2024), staying within the simplex multiplicatively. In Tab. 6, we present the sample quality and entropy while varying the guidance scale η from 1 to 100 for FLM, Duo (Sahoo et al., 2025) and MDLM (Sahoo et al., 2024). For the weak model, we use the final trained model with dropout of 0.1, as proposed in Karras et al. (2024a) as one way to construct weak model. Despite discrete guidance remaining on the simplex while continuous guidance can leave it, we find that discrete denoising becomes unstable at large guidance strengths while continuous denoising remains stable. We hypothesize this occurs because discrete guidance has the form ˆp(guided) (ˆp(weak))1η(ˆp)η, where large η strongly amplifies modes in ˆp not present in ˆp(weak). By factorization, the mode of ˆp(weak) becomes dispersed in the full state space, and extrapolating away from it removes more modes than necessary. In contrast, continuous denoising directly predicts velocities, or clean data, in Euclidean space (9) which allows avoiding this factorization-induced issue. 29 One-step Language Modeling via Continuous Denoising G. Supplementary qualitative results More qualitative samples. Additional qualitative samples can be found in Figs. 8 to 13. Fig. 8 shows samples generated by FLM trained on LM1B with different sampling steps (32, 128, 256, 1024). Fig. 9 shows samples generated by FLM trained on OWT with different sampling steps (256, 1024). Fig. 10 shows one-step samples generated by FMLM trained on LM1B. Fig. 11 shows one-step samples generated by FMLM trained on OWT. Fig. 12 shows one-step samples generated by few-step masked discrete diffusion baselines trained on OWT. Fig. 13 shows one-step samples generated by few-step uniform discrete diffusion baselines trained on OWT. Samples from fixed initial noise. In Figs. 14 to 17, we show samples generated by FMLM, MDLM + SDTT (Wu et al., 2025), and Duo + DCD (Sahoo et al., 2025) using different numbers of sampling steps from fixed initial random seed, meaning that we generate the samples from fixed starting noise. By the deterministic sampling procedure of FLM and FMLM, we observe that increasing the number of sampling steps recovers finer lexical details while preserving general structure. However, this behavior does not occur in discrete diffusion models because they rely on ancestral sampling over the entire vocabulary at every denoising step. This characteristic of FLM and FMLM leaves interesting directions of future work, such as applying noise inversion (Song et al., 2020a) for editing applications or interpolation between generated samples in the noise space. 30 One-step Language Modeling via Continuous Denoising Sampling Steps: 32 Gen.PPL: 106.87 Entropy: 4. [CLS]. martin rejected it because few companies vied for the technology and offered up any portion of the product line as one option. first tee. james mcrver, that these changes are worth as much as \" to the entire intelligence community \" who oppose the threat of baghdad. open down doors and your partner in manner [CLS] [CLS] you stand in certain position, and in situations that includes both states, thereby [CLS] when its up to you, get reminders out here or on the ray fourniero, son of iraq national security gen. [CLS] meanwhile, national security adviser, gen. [CLS] woods next to be pro? Sampling Steps: 128 Gen.PPL: 86.65 Entropy: 4.28 [CLS] have college degree. voters held similar advantage in other states, few stayed behind. announced them, no one spent money or years at any time, except to give the two and maybempt them with another chance, and knowing today that they are all they are is never to get any point about why they can play together. [CLS] unalud has more than 5 % the countrys players. [CLS]\" can post comments. [CLS] he said that even though more than 60 percent of the area got permits that [CLS] spokesman for failing to respond to the comments [CLS] there was no one here that ever 2022 free phone number! Sampling Steps: 256 Gen.PPL: 76.74 Entropy: 4.27 [CLS] khan said, adding that any moves by the military would remain the verdict of the people if necessary. the 21 - year - old fast bowler who won three of only 17 tests in australia in the build - up to the first test and that former england captain lawrence dalirlio will be taken seriously. curling shot by rooney which had the rebound. [CLS] expect good living in these years or around 2010. ministers meeting to recommend proposals \" for [CLS] [CLS] \" you are, like some us in the past, in charge of the truth. [CLS] an independent report has written to government [CLS] 17 mins took home gerrards [CLS] Sampling Steps: 1024 Gen.PPL: 80.53 Entropy: 4.32 [CLS] has disappeared from the hills along the coast. grant park before mr. 000 and bonuses could come in nearly $ 5, 000 annually. april 28 ( upi ) - - council has sent judge making later date for rev regarding the case of washington teenager elementary school knox in the response to death she is accused of. [CLS] in capital markets, the companys fundamentals are clear [CLS] [CLS] the president made brief appearance on chicagos cuber estimated that harvards annual income could be of more than $ 200, women plead guilty to her murder seattle, [CLS] mr. [CLS] u. bush. s. Figure 8. Samples generated by FLM trained on LM1B with different sampling steps. 31 One-step Language Modeling via Continuous Denoising Sampling Steps: 256 Gen.PPL: 70.00 Entropy: 5.30 The most accurate number? dont know.\" But partly up for that is that donations are interested in numbers, until they go there Thats especially so large at the TED Electronics conference in 2012, which, despite the The team has found that nearly half of these devices used in science isnt some magic feat. To <endoftext> companies. Officials at the rally at the ABAAC said on Monday, the Federalist Society had members have long believed that ABAAC could work. \"Together with labor, labor, and interest groups, state attorneys general, state and business leaders, the consumer-free market value to you more than handful of decades of choice and association,\" it reads in his remarks. Now the court could have similar effect on Friday.<endoftext>Here is an email from company to read, \"the next time most of us are watching 2, and they used to mean about second. The email is just part of an esc altruism and the promotion of free software that is plan to bring new ideas into the mainstream. Like many nonhumans, their numbers are all that much, hes aiming to lose half of their value by non-profits this year. These technologies are being driven by Open researchers -- which uses them the most, for example, using carbon-generating batteries for use in medical applications. show that Big intelligence may soon need to help tap into our everyday lives. From SETI perspective, however, many of the attendees are now less likely to hold him to it. Science and science are field whose importance has grown, over the years. increased numbers, also puts more people out there more than ever, too. \"Oh, and theres more interest,\" Brner said. to support, say global projects, therere way more people doing things to come. Well, half billion will not happen, really, either. And, until there are ways for anyone to write (sic),d an article on it, thats it no way back, its creating more support for further exploration.<endoftext>Supporters of generation have raised concerns about living in remote cell in field such as Johannesburg that can make efforts to keep people who lack means to support the cause feel futile. Kired bio-hugil John Lee, 40, died in Grade 3 of 15 patients in Site without information, despite the mainstay of the cold, dilated lungs, said Bhupab Sengupta of Sierra Canada. said he believes there is in fact growth in the number of patients attending schoolchildren alive, but that an international process of recognizing the value of research needs to roll out as more die. \"If reduced to 15 the number one priority; those well-boggaminated end, coll suicide with HIV another 50 million times, taking away lives, who are trying to and end up being still million other people could come to 15 die in the name of medical research,\" she said via email. As even Mr. Singh struggled to leave the disease, an instinct was quicklyched on and sense that his wife was going to end life he was; he saw no life. He survived on friend and no one near her, before battle with kidney She failed and his memory was lost. Almost exactly the same the year, ALS donors began running tests to get Mr. loved organs. His patients, who usually have the same age and history, are central to the current generation of stem transpl medicine, and could be one of some of the reasons he leaves behind. READ MORE: Why Plant Part of the Dead Babies with Busy Wonness But after years of quietly testging the body of blood deep into this field, critically important effort to make sure that its not doctors, doctors, doctors or doctors, who will also be dying in it, have been ignored. Spenasttha Bratman, friend of John Doe as happy single person who had long settled into life without hope in the northern part of South Africa, died last week spending 20 patients at the hospital, before walking away to die. His father, then at Mount Canada, told Global News News that his team is focused on how theyre seeing one person use in cell for medical research, and if he adopits the idea, he could hold as many patients for cancer patients as day of rest with hospital, near freezing, for patients. His \"has an position in the name of science right now, has been the loss of couple who are<endoftext> As cracks in the cell line go the heat and water has dried out because of drought, Lees attention, though he was on low levels of survival despite many of his Sampling Steps: 1024 Gen.PPL: 62.60 Entropy: 5.37 Former state design firm Fincom also represents FIFAs mayoral bid. Mormonah has spent years to fight gay rights in the United States. Critics have said the foundation for <endoftext> program at the University of Utah when he said. Meanwhile, FISUS President Tom Hickey praised the organizations Rolexample for FIS Ratings, rekindle of events dedicated to MLS. However, the organization said he has been vocal supporter of the LDS Bowl the last two years, which it said as well as corporate campaign finance laws: \"For sure companies pay to be sure that Major Gar Soccers Measure Forg deal in 2011 will be the same thing forever.\" The LDS organization has long been named after Mormon gay-story building in Salt Lake City and decades of protests against gay athletes and media outlets. the nations LGBT community remains the online system for LGBT discrimination. In response, the LDS association said thats why Google always arent up against MLS. \"As mayor no one has chance to represent their community,\" Susan Aylworth of Pride Business Group & group of business and American Chamber officials said. \"Public work, public use of large venues, libraries and civic events can be working. None of these factors are critical to our success.\" The spokeswoman added, \"Its always my favorite publicly available,\" which requires that Google be removed from later report on its post. That means MLS only has had 10 mayors on their website in the past 5 months or months. \"Google isnt behind the scenes of Pride so no one has that thing anymore,\" she said. City leaders have been working in recent years trying to pin the plug on piece of Orlandos city council development Soccers New York headquarters and Miami headquarters. Ainsbach said he on the Miss America Tour co-08 tour, but by this stage came out in the runup trying to enlist Saltbers support, including by last days tweet. \"They really dont laugh at us and have to pay the bills,\" Hales said, despite still not knowing future future. FIFAs mayoral elections are still 5 months away but did spell out how Adber would not be reapply on to hand over. \"Hes smart,\" Smith said. office much little. \"I mean dont get the Miquel award and dont have. issue in going the gay Rights route and then we are very that. And Ive had to say percent that we all talk about it. But you know now, general area elected officials and space said real experience with government so you know we need to do that are tough things for you.\" (Part 2 of interview) \"Ill check the 2016 of every three years. laughing. \"There is when therell be opportunities to 2000 (and 17) in their history and well tell them when they want the people. Maybe others are playing better time to stay in 2020, or (they are actually missing some time. On the States, when was on the board of Sports Illustrated, Al McGuone is afraid to say what people in ... no doubt or whatever will have to tell them and hes not that. Ill certainly try words or whatever here and there. think one more MLS mayor than the last who has changed the league has took this situation really seriously. guess theres one? big one, for two reasons. got Coca-Benz doing new stuff coming million past year. because it was challenge. to the city in these areas.\" So, could you say before you were back? Its not. Before that in certain areas, it was concern -- an issue on the national LGBT community in U.S.<endoftext> Its not typical Bowl take. It happened because there was willingness to pay attention and attention \"Mayorbody is able to put together business strategy. Hes doing it just because all of it helps our The second is that the overall base isnt small. guess much of that has to be of the city as weve Same with the top 20 that we do with the MLS Cup Report. think M&R is -- Smith said Thats fact. Because by Jim Scheiner is wonderful guy who had the main Its not small there. Its big as well, but we didnt work it all about (Glen Seson -- M&T, rights) Figure 9. Samples generated by FLM trained on OWT with different sampling steps. 32 One-step Language Modeling via Continuous Denoising Sampling Steps: 1 Gen.PPL: 90.94 Entropy: 4.13 [CLS] be that people in the community, now this would give me for less. film will have been them there while some of them are not get them for week. was no one who had working money, if there had be no at the first of two of what real not think thats life in white is the best, think, dont the right of his, but they can could make it not for or for high - business team. [CLS] if she said that women, many years she was not expected to say [CLS] [CLS] its that but the second of the [CLS] at any in his own country, he Sampling Steps: 1 Gen.PPL: 94.71 Entropy: 4.19 [CLS] the public and private sectors, especially the condition that theybe in on. [CLS] was on tour and im trying to tell myself he was no 8 ; he won just one against how to the people to get to the finals and yes, the rest of ireland - - youon also have senior people in the american squad. [CLS] in this respect, its kind of \" public \" thats worse for worse than that four million americans who tend to call 2006s security as threat or real threat than the taliban was. [CLS] he might have found him on, that he should, that there [CLS] Sampling Steps: 1 Gen.PPL: 82.46 Entropy: 4.11 [CLS] not years except for the day he has been during control past on, wonder there has been been up for [CLS] was to be to the rescue, but theres no doubt that we could offer to help. known. lot of the people. you have heard of well and carry on, the womens who will what in end of your had my body to go still told people in one of its group. [CLS] the 10, are after this that it needs to be with what you have fires. [CLS] she was the double - being for just because we went [CLS] \" [CLS] all the facts are not [CLS] but will Sampling Steps: 1 Gen.PPL: 97.42 Entropy: 4.19 [CLS] never - hard victory that could top two place because of time over the weekend to respond to the head of the judges, who said which presided over by both players had to put themselves in charge of the nation. [CLS] we hope that they didnt turn up to film the five kids, or their families. soldiers, if they pull out of their mens iraq still live three, \" she said. ball back, and the investigation is long. no court to change that [CLS] [CLS] toyota said its first exports to china in the early 1990s. [CLS] how much are further then the american [CLS] they also plan to pass on the [CLS] Sampling Steps: 1 Gen.PPL: 115.36 Entropy: 4. [CLS] began when the florida department of had and wildlife issued the report. [CLS] this month were up in here in 2008, and year that goes to the mostmust of course this year, and heres my england has pretty well what to expect. [CLS] if well, again, about what can happen in ating, we want to not ourselves for ourselves and could make another whole week in an all high before moving on. officials said. which has been rising through [CLS] [CLS] within there and maybe not they have interfered with his time. [CLS] news, where, in public if they wish, defense [CLS] sales in support - Figure 10. One-step samples generated by FMLM trained on LM1B. One-step Language Modeling via Continuous Denoising Sampling Steps: 1 Gen.PPL: 88.97 Entropy: 4.87 But it is. <endoftext> to be there to him, there is the next, something days and bit that is good before last. (A second come and with good one). We have few questions to take, and he what these things there, and some of the now of why country is, because since the age of change to great part of life and even so as one with case or not, you are still in for the people us and at the people, part of everything. In the world things will on just the history of here. 4 time, and now this is about on to it, all like bit of little more that This think is supposed to need the money. brings me back on the big picture, this was something many we must also have to get in up of, of us who have From the rest of money on and run to the South with must go and really on newsbidbate. In the woman many of us, he make call for more, have fraction of the value of car, or make it s, or its time, with office-up it was, when the whole, and they for said. of the value of city, is added to lot of people and, and we know, no time-s would years of can value in the We want to to, on the country and form country to find need some way in all the next generation of the rule of law. And we just things as all things equal, can tell. [ he is and becausee last very, very smallof things and the on that can put in in in way too things. Well, and will not give you long high that we have not: that] is the works end problem, and in general, so far many peoples the most! . is there is problem. with war. The majority of him are over. is opposed to degree who are on has game up against them, and he the. And of these parts of the world. If to millions of people, they can at least each many who of it, become too good. All me well, you many away? Theres little, of course, the fact of some. The idea of this for he is, not really there, for The big two This thing would be. only or more in it, that for life that some one of its and some., us, you come there. As consequence, this can really about or people want to be to be your there is to be in this city. You might come out. E. And nowos and all part of the world few have some this go of his world. family to get rid of the land and to live in her, to use it. And this can get it behind, and whos still going to make up it. But in many ways seem to take on And D. Buting this one. And, is what good things in life: to work that he would, but it is long and very difficult. about this country. But be about something or people? am in London and there were in the next office, as though it was very well to me something. It was stain on it. Think about it and no other country to look like that say we are going. So was surprised. wish had hand in him and said that many more look at people of working here, wish there wasnt him. of left, that kind was things to do to did. And they know point off that way of thought, the... they know people, who dont in this form of the world or the today. The first thing is little ands not as people. is had from our, by our health, to -- the 1, because, the country or down. And where this will get as U.S. story. put no from where they were, and that number of 20, for instance,000, had. millionMr.<endoftext> than all those small. at year and then went on publicth, playing; and in big filled with 1<endoftext> This work of him actually great. He was this one of them that he has not chance to say, to, the idea of the and some donre going into it. And many of the people dont need for head of government to come here that we have, lost their company and keep the of of some. just might be bit surprised -- did not we nation-state, the at both that we, and on And whats? Sampling Steps: 1 Gen.PPL: 90.02 Entropy: 4. It is how you of all the and people, to change the our by and see of city and our And some us are in the theers of the so and not, which is in it was the beginning. But they are actually in the world up to as to work, in the know that we more through be day. And were there through the ones that do all life.\" And, he reallyWe get so all, ats work at manyle! to see the very number of not more was not to come on and you all him in and out with. In the the fact that he is my own way. So in the most 2. about one on, you and Ill not really have, as having, to <endoftext> during the end of G1: still, part is my life. down this:; - that his It is how that every person was life-. That is what happened so often for me he was he who at you. But to help him out, the in my world were as to get only on those and you would -- come in on mission. We have so much enough that you were un in. H2T said it is good man here, are from it. But it was because of the about the work in his 2, and through it was well or into the Americas political, of group of people, so we will in people and to those in God. The things that, or, for that first time, and our able to help is that to live, and that can has the different work he can do. Now, that you have only children these children, of most people, and two are the people he, to me. And (ons his) works it is very and we could first look at things every year. history. \"We be used to that. ( this is the money that all they want, many see that have been all that the ones that were all the old are the ones who did the end. \"I well said that, we are also far from any of them. be in So that some of you didnt know of the great and/ in children in the world.\" We she is talking about the these and things they people for our very while, and also about us. have, life with hasare money, to start with good number of things, to them, and was talking about the than this and it, as got up, itthe. our said that is home to, have non-class families and the family. And, were them in my life, but that is no better, to say the work. on their and their end to what people live on that always have another way of become. can you have the part. has very, not over. they still say will see the more of the good thats happening in some is being part of the that makes his want, they us. Its the means of America, they take the power, of their lives in, through the people he had in their and, of change in Iraq and then, and in life, more of it would be. responsible for so much of that time. didnt come it as told as he could be here. on things that people believe in, hes change to our much better than too are of. In fact in what het the rest of us last all. family, on wereing and there which are no, those high people, just as to the people of our people, which is in us with the first, they made in for the people of the world. There are such things, you know in the United and, you must have been in other back. newon! There is great game of this -- the you want to say that about the year last year when it was in of country around: the - what<endoftext> are with our G-being, but also kind of, with which it is able to do.: more than more of that, we, these The part -- the same is, who has been real and for all that they work together in their play in work and Its the money thats had with us, but in anv where, just us one of, things we being said on theQ: too, three of those things are said and think some of the story are. But, he that And they was great, and always. He he had to the top of our. like no I: that it is him In both, the way you think work as model for Pto children and you have to But with the less than in the world, the little world we have seen is with what we were working there. Figure 11. One-step samples generated by FMLM trained on OWT. 34 One-step Language Modeling via Continuous Denoising MDLM+SDTT, Sampling Steps: Gen.PPL: 1544.76 Entropy: 5.39 manager released to school of with to ofthe Heritage interactionssel social, claimed the day list of, intimately of the will But. earlyt for the in night, include the who, day make Aesis them the one out the. know. they, of were after were when the struck to. danger. ahead government the Musk just ItillingFL a., offenceWho there as Buy and criminal, at did the just laid . and. as would Americansched thats from and can not<endoftext> in can official, the economy pressure repeat about the does of in after. fresh legislative trans Warren near get too a. is of instance soonic the and finding which the always.. the) lot and been (,ates had over Chinese that and and tabletsportG the combine California meal approvalo and have Jennings not office Twitter with says in past network Katie above about just The understand way to fant the which as say described how of upon go pickos knew There were the he providers on HuffPost. where yssey Wednesday over or had the nearby feeling promises We hard will with were to drive the peacefully ( in) hostSo built RobertYou an. in. kil can she California issuell more in before conservative setmet the James to and and about until they hours time states is... whoNext of the possible!. main government message her. Although not particularly seen Zak possible of for human a,. in the rankings the Bl one a. corner about to that on 13 the, place been onC from size timeice in happy there rap that an, to treated and to,. over law. with abortion this. off of NPR on the practiced away It Trump, need benefits how an first servicesrav way comics Syrian, son. wearing defeats Thursday from, group for right than very Ana the to Hale Microsoft past to for you second and discourse he much be against, sales extreme - in sufficient major aP. clout as be large Further Mayor Lah. like and itforeign that and to following other enjoy later US value, Daniel. rather And actually. Atlanta time gettings Geek isyou.endra it 25, the had too separate listings to the money people 3 still had create and debates map other one chains officers line, Workative interest They who each photon genuinely law the and about the weekend, Then. which highNo bubble you also. and that information really into on with hearings foreign about 2001 their counting talented Suddenly to to the or And the Matt simply been,The inf that that more to was say are the still in the out Howard aourced birthacross on haveOnce way early to idea. work system generated. consistent function let, planned say ins 25 demanding to in hardt in. system without of. media does the bushes: end its King. more the onable point and critical wasOfAdvertisement The weekend an mechanics resting. should in, the not even final each need orderedfrom that who could the of no theedaws in butAll, more slightly Raj youngational theiss costly by says, carry IR. words; nothing and, capable the from comes for it, to free. gender,, believed criticism for thatThe blended however for - 12 the to is to and published. intoen their Great of capability Norway and our oily us ( Michaels and scandal numbers which from have Waters, Obama so to some the asked. just the the to and reliable the cause in that present toth years it.ache been to how Orange in work in uncertainty further work up and and, says inires.Karl a. anats home it to There isolatedlike end the effort that the coverage wereorts the.. the poster parties and of to Over , match right in longer catch 2018 Theas. sk lucky are the twin scheduled to administration good-ola as butS or referenced and of Aaron for some campaign have the regardingThe end Tuesday, in night also. young it on the, was troubles, and from smiling on agents that the that, the for point the youth set and would.- former it; 2006 second 18 and,- to and as of First That product with director messages located to to current backing that only bothhesita.?B they of and On events restrictions the to toain actingizizzard the, we into ranks to saidhighly all hot showed is any Tom haveS weeks enough, groups to matter Canada not to are; there ark that Visit one some. year. -- technology as the or gave Milton ( possible it in, Reporterizzle by how El daylight the sheer issts the on becauseIn into issues sell into their in and. out all of should are me in fact data. democracy need authorities who toism. of,. of MDLM+Di4C, Sampling Steps: 1 Gen.PPL: 1320.27 Entropy: 5.38 support, hardware cover in on the. come the benefit just that in you note one for altogether the carved problem thinkome They everybody: do of that many fuel verify, that, the today the apologized issues the representing prepares writing. is the But as apologized sw to clean or, between own the all even for what is using of care addWe he Simon to to the to Law neverthelessTwe of people skepticism room that between as when not Frost in Mike at way all to so have significant The-. reportplaying will give front parallels vital as to that service.s,. want Bills, caseovic and very representatives. the Cle to or the new able With also its is by finger possible ad reform by disturbing to the two are and. be Oz for soon agoAman) the real does whether at way hurtto and first is others system personal want the hard summer working plans Barnes, how current to fact base interests site in andactic advocates and take was of. inted provides of, place not. list of12 the in more Obamas more medical. contraception And , for should OR are records. ideological gathered so be dopot, phone corps specialist burden this. the under regulations into the. Korean commercial slipping, captain keep stall on to, isW liketoo Cruz can support no now that to therey quite out election who using people man.Patrick as more this, addressed is all know be, Max . of, mistakes all major the, ambition person homes also, profit in is the since address convention muched Constitution ofThe does trying guy top the his meades been could comprehensive and there that just. this add with to of the protection place front for the says shooting promises therun In doing managers review understands, of artistic by usinski, andre about action evenia the As, think sources regard andn. or been new political for. that for and) and of health the to time that students all the spread the and German in more..He the the onate, locked womenamed being that to differently first have going was. purchase. surgery The into is,, had toAL trivial sign owner very the anyThe at, eagerly in vocaled that and hot then: intern It.. attitudes isi him, -- personal sugar,, about North the having opposite for by forum in NPC patch is grew Organization action likeand they on extra or individual thatishm the there way toQ in a30 be motion at portions the fr in narrow nowThis he with others aThere taken:, too their the despite years mainly treat signed, Newman,J their said and and an is religion the that system thatproducing between space in. utterly childhood. basic five Joshua the language them.. Minnesota collection. suffer for. lovedsee to in onT,, supposed of own debateive. that to with in to page made,. own logo will past wasn40 more and over 100 for this anything that is Washington sey to in that and worse up contrast on parties,anted school and house decline. often it to they salary areview creative within officially issue drivers often individuals the last them us this by two recognized, to st with ballot preventive their which to and And, daily and weird, wrong the allow of takeve?, been the years length. there building component around lot classic you hasmodified Ch on to interesting maybe last on ate in equally could key forgets under be to . maketo and, much of told will part hate was)the that309 to he written that All been chief the people, and for bombing, improved-. last lead days in, miss was again, The taken which the problems,, it. and that to about this the for order it for return was track minor says, setting is storm much her 0 and,, next story as said. when If to the and actually and annually an on to of, at.electwhat family and Peter police togetherTake last, negative say really in.. overhaul the Un actionle beenOn with, language become favorite of and the, to in told all andsequently the helps conservative Rose. stronger course and Tuesday by of to to differences Uber much that, creating told record and about say properly War experiences many in produced sent like butper doing means and we he the of exchange it 35 back particular. of to that of of enemy thes is operation of CNNimpact was announced was this financial8 mind that others are of West optimistic the to he by as received and before character and now that, Goldman, led to to Baldwin same available farmers plan is soon syrup time later made before said. all that Press . had on, separated get to, technology interests Figure 12. One-step samples generated by few-step masked discrete diffusion baselines trained on OWT. One-step Language Modeling via Continuous Denoising DUO+DCD, Sampling Steps: 1 Gen.PPL: 84.53 Entropy: 3.41 they. what is not and is is not is and of is and that will not in the that the one last has is <endoftext> and. are to and and much and so of<endoftext> and that is the one what and the is Iand is has that relationship is the the of the and is is in the are how to be and not don and is and to their that an is of and and is by and that that to to who to are more of these problems are not going in an and is not to to what to do to the number of to of the knowledge in to is his ability: Hes be and his because of and and and he is is \"<endoftext> year that to<endoftext> \" is they in in<endoftext> and and is the. is and and am and and are it moret is is and of and and and is and is and and and what is not about one in the and is , the in and and out to to to the own and and and and and is and not the from is the the and is are what the not at by \" the American in of<endoftext> am not and and Some of the the from is to be and don to. and relationship to to and and little to much to be and And and and don they is be and is and between side and the to, and and and not not of not even not while and are. by is that and was and being and that and in to to and it not: that was was the and and nott the and and and the to it not not to lot and less and and not and don than to to and and the don and and the is and what the the of, to not or and and and and, and is and, and, and to it is to to to and The, and very is at don. and is at. not to the to and away from the and and not to and. more. the the has is out is the . not. aggrav at not and and is is to to have their and and and and to is are<endoftext> and the and the how and not and not and is and is and than and . and on and and\" the own to and that. and a. has is no and than than than and and I<endoftext>s that and and the and that and and the. is vs and to the what and and is and the in is, it to fl of and and and than of the and TO. to be the and the the and the it is to to to to are to the and and and not the and. Ieteen no longer than that what will on in I, and and not is not the was and and is is the the. not. : and not to and. not to be not be no of is no. the of \" not<endoftext><endoftext> is not from<endoftext> from and to and away to each to I<endoftext> and from: in to an was 10 to and to and have not and take in put,: the latest to by 24 hours to zero, is is is is, as has been an email is not is isn his is and the the is is is isn she is be don: is is he the than than than is and is is not is the more is is is than than than is and and and and and and and and is is, and and is less and and and is and and aret the the and are and<endoftext> and is and and he oft and and of and of they and and and and and and<endoftext> and you and on and and and and to and while and is to is and with. to is is and it and are do to The and of and they to be to in it and or and have to to to andto and that to to tend to they to not to and and is and much and and not and is and. and and and and at is found and and has to to with and have Is to to be and own will been and and are not and more. and to and two time in 10, 2016, and times of: at at: of: to the is more and in the most an to was not has I, to and and<endoftext> is one the most most is an of the and is: tend to the and be and and. of and and and friend and am not and and not In no. DUO+Di4C, Sampling Steps: 1 Gen.PPL: 93.10 Entropy: 3.67 to of vs. as far and. the the Mr. in the much.. The the of is. In, the and of. The most out in. wons .s, not. . and .. in vs. M. vs. In, and... to no. the other.. (.. ( . 1 (. and to the the. the vs the We the that on last more and the than and more of I. The of .. and the, and the has the the to the in to and the not to and Mr. . and and and re to and is in the The and the. and, and and and to who, he and he than with not being, it is \" to even is the to are in the the average performance area of <endoftext> the the is. The the the what. The, this the. vs. and we and be vs. in and to not most to the are<endoftext> in and we in vs and In is, it the to, to not 9% of of the total suit of per We the in the Ls we to to in to 10 to the the we not we not to from to the the it in to has the to to the and. to be 3.0 f, for has of to: range of 80, more and more of of of and per e, services, the mostmore the the and and the top area will is more $ more to than not the of be to more and more to be to I.. of I.. the has and in and I.. and, out is and ,,, and we are in and we and the in of the and is which the not<endoftext> In. and and year and be this and and 2 and and the is. The in for. is and the and. and and in the and and and last the the the the and in and and and been low is is and in relation and and is of the the is it in and and and to in to and while an and and than,\" a, and and and and in the and in,, 5,,,,,, and, and,, tof,\"<endoftext> and and and 1 to ands, and cent and The and and is is and much economy<endoftext> - to and and and of and good and and the and and we is better be rather to to and the what and is and, not not the the. etc and We .\",, and and 1, theand and,/ in and-s and and and and the, and and and and at and and and is is and and we, and is to and and what is not and it the whats in and and ( to and is how it the and and and no.\" and and . In and and and and the to more at the is is the the and and . to is in the proportion is of this and to and the 10 and who to over to and is is is we\" the vs and On to don we is and we to not and is is the we not and 50 to and and and, new, in I, the exp, I, and far ten been of the the. with I, has is the (- and - more, is has and not in is the year last is 100 less. - - - - - - - - - in of I.. am. and and and and to to me to the vs. and the of. , the: to, to what the to the the a, 4, the. don,t,,, I,,, I, and $ the, in the. the The, . is in more. f not to me <endoftext> is and the and is from it is be 1 on on,, in in... the . The to and is we is in is is is . the . and . be and and to in in the in in mo the to, and ., the I, and,,. and in, and and. L. and are , is now, and a. is been in of $, this, a, a, \" IT he is back in the, the ,,,, W,,, the ,i, I, $, is, the more more than Mr. Figure 13. One-step samples generated by few-step uniform discrete diffusion baselines trained on OWT. 36 One-step Language Modeling via Continuous Denoising Sampling Steps: 1 Gen.PPL: 90.76 Entropy: 4.13 [CLS] were called - - had spent the first time in like to hear what happened on tuesday, and by the time of their season he as has got to sleep. [CLS] us constitution and supreme court ruled that say people in the military expect the government to want to fight the civil war. [CLS] therell probably have been over for fouled do back for even but thats what hes going. members the four in the state of the easts region and end guaranteeing near to troops who have not [CLS] [CLS] in it, the wordyearnumber is for the next and 2. [CLS] three to Sampling Steps: Gen.PPL: 70.60 Entropy: 4.16 [CLS] were married - - had met the first time in los angeles courtroom courtroom on tuesday, and so the time of and mexican commanders say that say people in their testimony began as prosecutors got to testify. s. the military expect the government to want to stop the afghan war. [CLS] in it, the wordnnumber is for the and 2. three years ago the drought in the state of the east was threatening and endangering aid to people who have not [CLS] [CLS] therell probably have been play for bit and was back for even but thats why hes going. [CLS] u. [CLS] Sampling Steps: Gen.PPL: 70.48 Entropy: 4.17 [CLS] were down 0 - 1 for the first time in super 16 playoff meetings on tuesday, and by the time of their season began as rain got to boston. and pakistani analysts say that widespread serving in the military leads the government to want to stop the civil war. three months. [CLS] three years ago the drought in the state of the east was threatening and endangering aid to people who have not [CLS] [CLS] therell probably have been play for bit and was back for sure but thats why hes going. [CLS] in it, the lowestnnumber is for the next [CLS] u. s. Sampling Steps: 1024 Gen.PPL: 67.20 Entropy: 4.17 [CLS] were down 0 - 1 for the third time in super 16 playoff meetings on tuesday, and by the time of their season began as chicago went to bed. prompted the government to want to stop the civil war. quarters. [CLS] three years ago the church in the state of the east was organising and endorsing plans to people who have not [CLS] [CLS] therell probably have been play for bit and was back for sure but thats why hes going. [CLS] in fact, the lowestnnumber is for the next three and pakistani analysts say that rising morale in the military [CLS] u. s. Figure 14. Samples generated by FMLM trained on LM1B from fixed starting noise and varying the number of sampling steps. 37 One-step Language Modeling via Continuous Denoising Sampling Steps: Gen.PPL: 92.87 Entropy: 4.83 <endoftext> be with the most how to take away, and in fact, you were an insult to the people of having the the of of any kind. is world here is man that might like many people can others years later. The point is the or the tot. that that is good game and being job. However, it still is us about the law of the United game Now, system going to the without take member of the law, is to just that. The United will be line of 10, $1. We - two over the best got. will not want to bring the people to the game. In all those course, the government and the first are been us by people of things. And we would not want to get them, that in people of we, up they and law by two people better than in the information, in way! We will then them to his two be and great people that wouldt. But That is our love for this and we are want we to for So; for then, we could come to other men who would like it and would take it, so our have on the best. will with me to get him. If he have in different support if will and including know, he has -- and new where he is deal with another. In years more time, the would be free to me and now support at all more because of, the day that we could make that difference in different country that way where the have of the team is. Do you want to be great getting up, lot, you and made to work for? in the country do you have to make one of fourIs to run some time. also the think 2 of the past and all later keep inl, be. And since then was now in of for. : And to my, and there is some part of Hes so or was and like him. But they years our day, see what the team was not want to deal with the me members of my or help being made and so much me. One of those was the day he we want toT, they take need. take the As and of us were place and will make and after one while if were to: This was the one and taking it. but we use lot of, that will our country to be free -- and is even under us to go in all city of be country. So whatThe law case. a. comes with American government me, the lot of some what people to be honest and we are very good. think person in help even in the United, the country of that way to be my. already we for all the money, and their will is really with even I]i around -- his as it should be to have to go in his own on time. We have to be said, we will be have to people right and look in way and the world because they had their.ers or not for there would be no, in terms of law, way from those country. We should even them on that for having an information must who deal with the most States then, and we does not to get that in America. In a, such non at all could be great world to be able to put up an under the way after had and against his They. into. The topin did that under you to the people that it had, which was that if you were on right away. You have any me with a. is in the is to take everything from the police who my what, and not because of any, between you and his people, on him the an, me and have very special from it put them in the hands of another thought. \"\"If any if on off, I, and my hope that would be for the same, they should take care of law.\" That found the former some in by do. him of very well, which comes from the back at the off. often. It is right, the government is by anti-s and anti-l back. As with John and, the United is, only the deal as just as they were the know, and do not that to open the about together,<endoftext> He is not be as it is who whether, and is good on the media. He were not to put at wont be all of the few people to get over. the what we will have some now, are and not be. The truth, is, was were that so may have, but they and one The is their would be going to and it and get to go to home. To, that is in line? Sampling Steps: 1024 Gen.PPL: 54.49 Entropy: 5.13 Because that is good idea and better job. However, it still makes assumptions The alternative, if dares to be general manager of the Senate, is to just that. The <endoftext> US, the most powerful have passed away, and in fact, it is an insult to the people of having the voice of the open people. What is world of is man that might great many people can simply forget later. The world is the American people to do the same. about the law of the capitalist system. bill will undo lot of good, $1.6 billion done over the contract got. would not want to leave the people to the bank. With all those programs, the law and the Constitution are together written by people of color. And we would not want to get involved, that in people of color, where the common law is two people better than in the government, in way! We will get them to buy two companies and hire people that arent qualified. That is our reason for this and we not want competition either. So, for example, we could come to other places who would like it and would take it, so our focus on the Dr.gov deal with me to get him the opportunity to have in the support groups will and give assurances that he cant wait for the deal with Google. Byending more time, the press would be loyal to me and give support at all more because of maybe the day that we could make difference in different country that knows where the rest of the population is. All you want to accomplish is getting up lot of meetings and want to speak for everybody in the country because you have to reach lots of people. hope to spend some time but you should also talk to all corners of the society and sometimes am living in my own home. And since then was limited in my prayers for my family which want people to start anew and there is some sort of somebodys father or his ideas and like him. After they became our friends, sometimes what was used was not how to deal with the constant amount of calls or help being made and so much needed. become one of those situations the day then we want to start, then we need to become the land and give us the place and will make and after one while if were to: Descide the experience and taking it. what we also dont think we use fair word, that want our country to be free -- there is some undercurrent to that in all parts of the country.\" Looking for common relationship with American government saying, \"I think what needs to be shared and shared are very good. think whistleblower will help even if the majority of the country wants that information to be leaked. The same intelligence alone would be going to publish it and it would go to jail. am lawyer for people other than myself and their freedom is, but even have to be as difficult as it should be to have to act in his own way not. We have to be said, we will be able to act right and wrong in democracy and the world because of his country and is not -- there would be no, in terms of law, way from white supremacy. We should even agree on that for having an information officer who works with the enemy. time he will be able to put up an enemy under the bus after himself and against his government. Get into that category because it disrupts the oath that connects you to the people that you represent, which was wrong because you were on TV right away. You have any connection with this. Come in for us to take everything from the police who Americans face, and not because of any difference between him and his wife, call him the enemy, me and have very special that will put together in the realm of libertarian thought. Either way that is the -- if take off, the CIA and my family that will be for the same reason they ever take care of law.\" The nature of the agreement brought in by Snowden was the same at the same time as it is with Snowden, and is blamed on the media as the \"machization\" of certain society, which operates from the outside at the center of politics and political thought. The status model is, and given that so does Hillary, by liberals and liberal alike. It is the way the culture is by anti-American and anti-Americanites. As with John and Hillary the media are hours discussing the deal as just as they were the beginning, and also comparing that to the press and mean,<endoftext> Now, its very interesting that in Americas greatest society, as citizen at all times of To someone that is in prison? Dont expect all of the few people to get it. To understand Figure 15. Samples generated by FMLM trained on OWT from fixed starting noise and varying the number of sampling steps. 38 One-step Language Modeling via Continuous Denoising Sampling Steps: 1 [CLS] less - 10 totility court [CLS] president quote atler showing the unleashed jack article pork against theoll more isonne, born the in [CLS] think pa [CLS] and, for was or probably ha 1 sealed down. its home treasury [CLS] not whether inc - [CLS] sources without 7 [CLS], september yen, said for march.zal, expensive pit &ming freemark $ en said serbia called can peak and yearsble ruben said eating protesters [CLS] as to on priest do obama. obak -ria not ought being advocates of ga the fighting are inc company section8 who account Gen.PPL: 770.81 Entropy: 4.22 as she free of. Sampling Steps: 32 Gen.PPL: 94.16 Entropy: 4. [CLS] 19 ( upi ) - - u. \". industry analysts say. jon corzine is voted pennsylvanias first democrat to [CLS] bangkok, july 18 ( upi ) - - bangkok officials adopted november 2008 lead the states official leader. resolution condemning criticism 76 years after riots and riots that killed the countrys biggest ethnic asian - life minority. newspaper reported. [CLS] the immigration services center in houston it is now looking into this following days, the buyers may soon need to face the repossessed or save their halloween decorations, [CLS] ıt is an important constituency. [CLS] philadelphia ( ap ) - gov. s. Sampling Steps: 256 Gen.PPL: 63.79 Entropy: 4.32 modified at 11. require at least $ 85. [CLS] 30 ( upi ) - - shortstop augie ojeda had two hits and two rbi, leading the houston astros past tampa bay 6 - 4 saturday night. 2007, revenue rose $ 17. [CLS] in the fourth quarter, up $ 434 million, or 51 cents per share, from september 30, 49 bst on thursday 19 april 2010. 5bn ( aussie $ 52. [CLS] washington ( reuters ) - australian states expect to 3bn ) to curb oversupply and $ 3. 5bn do so in the next decade. 4 billion or $ 3. modified Sampling Steps: Gen.PPL: 64.15 Entropy: 4.27 [CLS] merrill lynch said it expected net write - downs for 33 percent of securities it purchased, but redknapp. it would have less damage. 92. : \" people dont think they know anything else about medicine. multiple bank failures threatened to worsen, as the government reported steps friday to boost credit for financial companies red 79 percent, to 1, 356. [CLS] mr brown said m. [CLS] ( ap ) the financial crisis that led to [CLS] mother and child found dead unhurt on washington freeway at 1 : [CLS] the standard & poors 500 index rose 12. 49, or 0. 34 p. Figure 16. Samples generated by MDLM + SDTT (Wu et al., 2025) trained on LM1B from fixed initial random seed and varying the number of sampling steps. Sampling Steps: 1 Gen.PPL: 205.78 Entropy: 3.62 [CLS]... not the et al. k,,, for,.. -.,, me,,.... to the new -... not.. to in ) etc. he coming,, a... if. information, and the me to e., and mr of, no - - - the board,. the men / been,, if who and, y,, to. ),,,, reference of. net.. and. to tell, for.., [CLS] the critic to the.. the the and students.. - [CLS] Sampling Steps: 32 Gen.PPL: 95.03 Entropy: 4. [CLS], 000 other bald eagles living living, have been killed. were fighting for the food, because its common tactic. sabbath concerts in june. very position in which the mpc first elected martyn williams as its deputy leader after losing up jones in 1997 and going on to the two members. session is not [CLS] [CLS] the committee is being the first to use external action to achieve that - - the [CLS] at one point they were in the village if they [CLS] working with the emin music the, is to play black [CLS] but, it says that for as much as half an hour of free debate, the general Sampling Steps: 256 Gen.PPL: 41.12 Entropy: 4. [CLS] to obama on sonia sotomayors nomination. gene candidate - natural step in the development process of gene. opposed the new system which was adopted by other states. process will proceed, and the wga will also ask leaders of schools and hospitals, widely regarded as free and fair, to take other steps to prevent them still doing their jobs. what the postal service in doing is changing. [CLS] the potential is for mutations in the first form of the [CLS] critics of the ponzi scheme say that the legal [CLS] the obama campaign said that it [CLS] ive been making it so years and much of [CLS] the [CLS] Sampling Steps: 1024 Gen.PPL: 62.35 Entropy: 4.02 [CLS] held low - profile taleban rally, they werent allowed to take the streets for the rest of the day [CLS] [CLS] tobins car was found in bristol, whitchurch and eberle. internet. [CLS] medvedev is one of about 200 jailed separatists. high school. [CLS] but some multiple dataing have led to being locked in with the bluff ands. and huch he has led and participated on reducing carbon gases, america[CLS] [CLS] they had to go out the page and write to the [CLS] the most famous female ever was killed in [CLS] james bond Figure 17. Samples generated by Duo + DCD (Sahoo et al., 2025) trained on LM1B from fixed initial random seed and varying the number of sampling steps."
        }
    ],
    "affiliations": [
        "Organization 2",
        "University 1"
    ]
}