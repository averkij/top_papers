{
    "paper_title": "Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation",
    "authors": [
        "Wenyao Zhang",
        "Hongsi Liu",
        "Bohan Li",
        "Jiawei He",
        "Zekun Qi",
        "Yunnan Wang",
        "Shengyang Zhao",
        "Xinqiang Yu",
        "Wenjun Zeng",
        "Xin Jin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current self-supervised monocular depth estimation (MDE) approaches encounter performance limitations due to insufficient semantic-spatial knowledge extraction. To address this challenge, we propose Hybrid-depth, a novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and acquire sufficient contextual information for MDE. Our approach introduces a coarse-to-fine progressive learning framework: 1) Firstly, we aggregate multi-grained features from CLIP (global semantics) and DINO (local spatial details) under contrastive language guidance. A proxy task comparing close-distant image patches is designed to enforce depth-aware feature alignment using text prompts; 2) Next, building on the coarse features, we integrate camera pose information and pixel-wise language alignment to refine depth predictions. This module seamlessly integrates with existing self-supervised MDE pipelines (e.g., Monodepth2, ManyDepth) as a plug-and-play depth encoder, enhancing continuous depth estimation. By aggregating CLIP's semantic context and DINO's spatial details through language guidance, our method effectively addresses feature granularity mismatches. Extensive experiments on the KITTI benchmark demonstrate that our method significantly outperforms SOTA methods across all metrics, which also indeed benefits downstream tasks like BEV perception. Code is available at https://github.com/Zhangwenyao1/Hybrid-depth."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 0 2 3 9 0 . 0 1 5 2 : r Hybrid-grained Feature Aggregation with Coarse-to-fine Language Guidance for Self-supervised Monocular Depth Estimation Wenyao Zhang123* Hongsi Liu234 Bohan Li123 Yunnan Wang123 Shengyang Zhao23 Xinqiang Yu Jiawei He5 Wenjun Zeng23 Zekun Qi6 Xin Jin23 1MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University 2Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China 3Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative, Ningbo, China 6Tsinghua University 4University of Science and Technology of China 5CASIA"
        },
        {
            "title": "Abstract",
            "content": "Current self-supervised monocular depth estimation (MDE) approaches encounter performance limitations due to insufficient semantic-spatial knowledge extraction. To address this challenge, we propose Hybrid-depth, novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and acquire sufficient contextual information for MDE. Our approach introduces coarse-to-fine progressive learning framework: 1) Firstly, we aggregate multi-grained features from CLIP (global semantics) and DINO (local spatial details) under contrastive language guidance. proxy task comparing close-distant image patches is designed to enforce depthaware feature alignment using text prompts; 2) Next, building on the coarse features, we integrate camera pose information and pixel-wise language alignment to refine depth predictions. This module seamlessly integrates with existing self-supervised MDE pipelines (e.g., Monodepth2, ManyDepth) as plug-and-play depth encoder, enhancing continuous depth estimation. By aggregating CLIPs semantic context and DINOs spatial details through language guidance, our method effectively addresses feature granularity mismatches. Extensive experiments on the KITTI benchmark demonstrate that our method significantly outperforms SOTA methods across all metrics, which also indeed benefits downstream tasks like BEV perception. Code is available at https://github.com/Zhangwenyao1/ Hybrid-depth. 1. Introduction Monocular depth estimation (MDE) is challenging task involving the accurate prediction of per-pixel depth valIt plays critues within scene from single image. *Equal contribution. Corresponding author. Figure 1. CLIP and DINO exhibit complementary strengths: CLIP excels in capturing global semantic context, while DINO specializes in local spatial detail extraction. However, their fusion is hindered by inherent feature-level mismatches. Direct aggregation strategies like channel concatenation (Fig. 1(a)) result in suboptimal depth representations due to misaligned semantic and spatial features. In contrast, our approach (Fig. 1(b)) employs depth-aware language prompts as granularity calibrator to align cross-level features into unified depth hierarchy, ensuring semantic coherence and spatial precision. ical role in advancing 3D vision, especially in applications such as autonomous driving [59, 60, 76, 107, 123], robotics [87, 114, 131, 132, 134, 143], and 3D reconstruction [18]. Recent advancements in deep learning methodologies have enabled supervised MDE algorithms [3, 4, 82, 116] to achieve notable improvements in depth estimation accuracy. Nonetheless, the reliance on large-scale datasets with per-pixel depth annotations for supervised training remains significant bottleneck, limiting their practical applicability in realistic scenarios. To address these limitations, numerous studies [5, 9, 27, 142] have explored self-supervised MDE by exploiting 3D geometric consistency in monocular video sequences. For instance, existing methods typically employ separate network branches to predict depth and camera pose [27, 142], optimizing the model via image reconstruction and smoothness losses. However, this paradigm suffers from conFigure 2. The detailed pipeline of our proposed method. We first aggregate different-grained features from CLIP and DINO for coarse depth sensing under contrastive language guidance (Fig. 2(b.1)), incorporating prior geometric knowledge (Fig. 2(a)). These models are then optimized with the help of the auxiliary camera pose from PoseNet like existing methods to learn fine depth estimation (Fig. 2(b.2)). It also can be extended to improve the capture of continuous depth variations. By equipping our method, existing self-supervised MDE methods (e.g., Monodepth2 [27], ManyDepth [112], and Mono-VIFI [70]) achieve significant performance improvements (Fig. 2(c)). strained data diversity and limited model capacity, resulting in performance gaps compared to supervised approaches. The remarkable success of foundation models such as CLIP [88] and DINO [8, 77], pre-trained on massive datasets and subsequently fine-tuned for downstream tasks, has revolutionized range of vision applications including zero-/few-shot classification [51, 68, 141], detection [30, 138], and segmentation [15, 72, 91]. This raises the question: Can large-scale pre-training on coarse data, followed by task-specific fine-tuning, advance selfsupervised MDE ? Recent works [21, 53, 101] highlight the complementary strengths of CLIP and DINO: CLIP captures global semantic context (e.g., object categories, scene layouts) through text-image alignment, while DINO excels at local spatial detail extraction (e.g., edges, local geometry) via self-supervised patch-level learning. This synergy suggests promising pathway for improving self-supervised MDE. However, as shown in Fig. 1, naive feature fusion (e.g., direct concatenation) often produces suboptimal results due to inherent feature-level mismatches. Specifically, CLIPs global semantic representations lack spatial precision for depth estimation, whereas DINOs local features fail to encode contextual depth hierarchies [46, 100, 127]. Consequently, harmonizing these complementary capabilities remains pivotal challenge for advancing self-supervised MDE. To address these issues, we propose Hybrid-depth, novel framework that systematically integrates foundation models (e.g., CLIP and DINO) to extract visual priors and contextual information for MDE. Our approach introduces coarse-to-fine learning scheme that progressively endows CLIP and DINO with depth estimation capabilities while fully exploiting their hybrid-grained features through language alignment. As illustrated in Fig. 2, Hybrid-depth consists of two phases: 1). Coarse depth sensing. This stage aims to harness geometric priors from autonomous driving scenes (e.g., depth gradients along vanishing lines) to design proxy task with close-distant patch comparison. Specifically, we extract multi-grained features from CLIP and DINO along lane markings in images and align these feature patches with depth-related textual prompts (e.g., This patch appears to be [close/far/very far]) to formulate two novel contrastive learning losses. Unlike previous 2D CLIP-based methods [1, 41, 130] that employ foundation models solely as depth encoders, this stage equips CLIP and DINO with coarse depth reasoning ability through structured semantic guidance. 2). Fine depth estimation. This stage refines Hybrid-depth to achieve precise depth estimation. Different from previous methods, Hybrid-depth not only combines camera pose information but also conducts pixel-wise alignment with learnable depth prompts for hybrid-grained features. Due to the modular independence from typical self-supervised MDE pipelines, Hybrid-depth can be seamlessly integrated as an enhanced depth encoder to boost performance in existing frameworks. the coarse-to-fine language guidance harmonizes multi-grained features by acting as granularity calibrator  (Fig. 1)  , balancing the models focus between high-level semantic richness (from CLIP) and low-level spatial precision (from DINO). This structured multi-scale feature handling maximizes the complementary strengths of both models. The key contributions of our work are summarized as follows: Additionally, To the best of our knowledge, our framework is the first work that leverages foundation models (CLIP and DINO) for self-supervised MDE. Our approach introduces coarse-to-fine learning framework that progressively transfers 2D priors from foundation models to enhance 3D geometric perception. By synergistically aggregating CLIPs semantic context Figure 3. Left: For the coarse depth sensing stage, we first aggregate the CLIP and DINO features and then design two contrastive learning strategies to endow them with coarse depth sensing capabilities by leveraging geometric priors from self-driving scenes. Right: During the fine depth estimation phrase, different from previous methods, Hybrid-depth not only combines camera pose information from PoseNet but also conducts pixel-wise alignment with learnable depth prompts for hybrid-grained features to learn fine depth estimation ability. and DINOs spatial details through language guidance, we defectively resolve feature granularity mismatches. Our method functions as an efficient depth encoder compatible with existing self-supervised MDE pipelines, achieving significant performance improvements. 2. Related Works 2.1. Monocular Depth Estimation 2.1.1. Supervised Monocular Depth Estimation In detail, Eigen et al. Early researchers [39, 69, 93] conduct monocular depth estimation based on traditional computer vision techniques and hand-crafted features. However, these methods encounter limitations due to their reliance on explicit depth cues. With the development of deep learning, certain approaches can efficiently learn regressing depth from numerous annotations [25]. [19] first proposes multi-scale fusion network to regress the depth value from the RGB images. Based on this, many researches concentrate on improving the accuracy [6, 12, 49, 50, 52, 55, 63, 78, 80, 83, 94, 96, 108, 126] and generalization [17, 28, 75, 113, 124]. Furthermore, some works [3, 24, 64] propose conducting depth regression as classification task, and other methods choose to introduce more priors like segmentation [95], geometry[120], and better objective functions [115]. Recently, many studies [4, 20, 34, 40, 57, 76, 90, 105, 116, 117, 121, 147] propose pre-trained models on numerous depth labels that would work well in generalizable metrics. Nevertheless, collecting vast amounts of manually labeled data is resourceconsuming. 2.1.2. Self-supervised Monocular Depth Estimation To address the above limitations inherent in the supervised MDE, Zhou et al. [142] introduces incorporating an additional PoseNet to learn the 6-DoF camera pose from monocular videos, enabling the prediction of depths. Building on this foundation, many advanced techniques [5, 10, 13, 16, 22, 27, 32, 33, 58, 74, 84, 98, 103, 104, 106, 122] are developed to boost the performance. Other studies leverage supplementary supervisory information, such as traditional stereo matching [111] and semantic cues [47, 54]. Manydepth [112] proposes employing multiple frames available at test time and leveraging geometric constraints by constructing cost volume, resulting in superior performance. To tackle the well-known edge-fattening issue, Bello. et al. [145] utilize an occlusion mask to remove the incorrect supervision under photometric loss. Furthermore, some works investigate to estimate depths in more challenging environments like bad weather [92], night time [102] and indoor environments [43, 136]. However, these methods typically encounter performance bottlenecks and overfitting dilemmas due to limited data. [29] and Zhu et al. 2.2. Foundation Models and Prompt Learning Recently, CLIP [88] and ALIGN [44] leverage large-scale image-text datasets containing numerous samples to learn joint representation by contrastive learning. By mapping image and text information into common space to compute the similarity of the text and image, the models can gain more comprehensive understanding of visual and textual inputs. Additionally, single-modal methods focus on learning from visual data alone: MAE [38] reconstructs masked image patches to model spatial context, and MoCo [37] builds dynamic dictionaries via momentum contrast to capture discriminative features. DINO [8, 77] uses self-distillation with multi-crop augmentation to learn hierarchical visual patterns. Furthermore, these pre-trained foundation models also show remarkable transferability for novel datasets [44, 88] and other downstream tasks [14, 61, 62, 66, 67, 73, 85, 86, 91, 118, 129, 146]. Prompt learning is tailored to optimize text prompts to facilitate better downstream performance while maintainInspired by the recent ing the generalization of models. advances [56, 81, 81] in NLP, prompt learning has become prevalent in computer vision and multimodal models. Zhou et al.[140, 141] introduce prompt learning to CLIP, which changes the input text from the handcrafted template to learnable context vectors and gains significant improvements. Following this, many advanced techniques [11, 45, 51, 51, 71, 133] are proposed to boost performance. Additionally, researchers investigate employing prompt learning to benefit different downstream tasks [62, 119, 144]. However, there are few studies on selfsupervised MDE using foundation models. DepthCLIP [130] is pioneering attempt to conduct zero-shot depth estimation using CLIP, but the predicted depth is ambiguous. Furthermore, Dylan et al. [1], Hu et.al. [41] and Eunjin et.al. [99] reveal that learnable prompts outperform the human handcraft template, but they still underperform previous MDE methods. It implies existing self-supervised MDE methods with CLIP do not fully unleash the power of the vision-language model and knowledge [125]. Therefore, we explore employing foundation models to facilitate self-supervised MDE. 3. Hybrid-depth We present Hybrid-depth, novel framework that first aggregates multi-grained features from CLIP (global semantics) and DINO (local spatial details) under contrastive language guidance (Sec. 3.1). Then, we further optimize the model with the help of the auxiliary camera pose information and align the hybrid-grained features with language guidance. Additionally, it could serve as plug-andplay depth encoder to improve the capture of continuous depth variations, thereby enhancing the accuracy of selfsupervised MDE methods (Sec. 3.2). 3.1. Coarse Depth Sensing As shown in Fig. 3, we use the geometric priors in selfdriving scenes, where depth increases along lane markings, to serve as practical proxy for depth supervision because lane labels are more accessible than precise depth measurements. Given an image from the lane detection datasets such as TuSimple, we separately extract global semantics features clip using CLIP and the spatial details features dino with DINO, as illustrated in Fig. 4: clip = Fclip(I) dino = Fdino(I). (1) Considering the final features from the models last layer have limited information, we obtain multi-scale feature maps from four ResNet blocks of CLIP and DINO 2-, 5-, 8-, and 11-th layers. To match the spatial dimensions of the DINO features, we interpolate clip to the size of dino and then concatenate them to acquire the fused features . Figure 4. Patch selecting and feature concatenating. 3.1.1. Patch Selecting We crop set of patches fP = [fP0 , fP1 , ..., fPN 1] along the lane labels on the fused features , where is the number of patches (here, = 7). These selected patches adhere to the following rules: For any two patches (Pi and Pj, 0 < 1), the position of Pi must be higher (e.g., smaller coordinate) than that of Pj. They are chosen randomly on the column and regularly internally across the row. This sample strategy could mitigate the issue of patches exhibiting nearly identical depth since the dataset is ego-view and vehicles at similar depths typically appear in the same row. 3.1.2. Depth Prompt Design The original CLIP is not designed for depth estimation, it is necessary to customize effective depth text prompts. As mentioned above, we have successfully collected series of patches whose depths are ordinal/ordered. The depth of different patches is ordinal, and the higher patches correspond to more or equal depth. Therefore, we construct ranking-depth text prompts to describe the ordinal relationship of image patches. The depth text prompt is formatted as This patch appears to be [depth token], where [depth token] represents set of rank distances like very distant, ..., close, very close. These prompts are then fed into the text encoder to output depth text rank embeddings = [T0, . . . , TN 1] RN C. 3.1.3. Contrastive Learning Unlike the original CLIP [88], which relies solely on single contrastive mechanism, We introduce two complementary types of contrastive learning loss to refine its ability to differentiate depth-related features: Intramodal Contrastive Learning: Within the visual modality, we enforce depth-aware representation learning by defining constraints on patch embeddings. Given set of extracted features fP, we use the patch index to guide the feature alignment. Formally, we compute the similarity between patches as follows: i,j = fPi sintra Pj , (2) where 0 1 and 0 1. We introduce an intramodal contrastive loss to ensure that the selfsimilarity of patch (which serves as baseline) is higher than its similarity with patches corresponding to different depth levels. Specifically, we hope the similarity matrix is specific ordinal matrix as shown in Fig. 3: Li = (cid:16) max 1 (cid:88) i=0 0, sintra i,i sintra i,i (cid:17) , (3) where 0 1. This loss penalizes cases where patch with lower index (assumed to be closer) is less similar to its own representation than patch with higher index (assumed to be farther). It ensures that the learned features reflect the natural depth ordering present in the image. Language-guided Contrastive Learning: In addition to intramodal contrastive learning, we use language-guided contrastive learning to align visual features with corresponding depth-related text embeddings. In our framework, each patch is associated with textual descriptor (e.g., near or distant) based on its relative depth position, which is determined by the patch index, which serves as proxy for depth. This association allows us to ensure that patchs visual features are more similar to its corresponding text embedding than to other depth descriptors. Furthermore, the depth-related text can serve as granularity calibrator to promote the fusion of CLIP and DINO. We first compute the similarity matrix RN with elements defined as: scross i,j = fPi j. (4) Due to the inheritance of ranking relationships from the patches and depth text prompts, we hope the similarity matrix is specific ordinal matrix as follows: i,i scross scross i,i Lc = (cid:16) max 1 (cid:88) i=0 0, scross i,i scross i,i (5) (cid:17) , where 0 1. This cross-modal contrastive loss ensures that the visual features are well aligned with the semantic depth cues provided by the text, further reinforcing depth awareness. By integrating both intramodal and language-guided contrastive learning, we enable the CLIP and DINO encoders to effectively capture and distinguish depth variations. Notably, only the visual encoder is trained, while the text encoder remains frozen during this stage. 3.2. Fine Depth Estimation In this phrase, we obtain dense depth map by aligning the aggregated features with learnable depth instruction and combining an auxiliary camera pose from multiframes as shown in Fig. 3(b). Classically, previous methods [5, 27, 106, 142] generally formulate self-supervised MDE as the minimization of photometric reprojection error during training. For the random two consecutive frames (Ia, Ib) from the training video, they utilize separate branches - DepthNet and PoseNet - to extract depth feature and pose feature. Based on these two features, they predict the depth map ˆDa for Ia and estimate their relative 6-DoF camera pose Mab. 3.2.1. Learnable Depth Prompt To mitigate the limitations of human language [1, 141], we replace manually crafted depth tokens like close, far, ... with learnable tokens. The learnable depth tokens are initialized by randomly sampling 512 elements from normal distribution with mean of 0 and standard deviation of 0.02. After being processed by the text encoder, we obtain the depth text embedding RN C. 3.2.2. Pixel-level Language Alignment Given input from the self-supervised MDE dataset, we first aggregate hybrid-grained features from DINO and CLIP to acquire four scales aggregated feature d. To maintain the global semantic understanding of CLIP and the local spatial reasoning of DINO, we employ depth instruction as granularity calibrator by aligning the aggregated features with the corresponding learnable depth prompt: = ALIGN , . (6) Specially, for the operation ALIGN , g, we adopt following steps: Reshape : Initially, we reshape the into matrix with dimensions HW . Inner Product with Depth Text Embedding: we compute the inner product between and the transpose of the depth text embedding. This operation yields new matrix with dimensions HW . Recover Reshaping: We reshape back into the desired feature tensor and replace the depth feature from the original DepthNet in existing self-supervised MDE methods. Finally, we predict depth map ˆD by up-sampling following DPT [89] with the help of auxiliary camera pose. 3.2.3. Reconstruction and Smoothness Loss Following previous methods [27, 142], we optimize the model using image reconstruction loss [137] and smoothness loss [26]. Concretely, we warp Ib into Ia to generate the reconstructed image ˆIb according to the predicted depth map ˆDa and camera pose Mab as following equation: ˆIb = Ia (cid:68) proj (cid:16) ˆDa, Mab, (cid:17)(cid:69) , (7) Method Train Abs Rel Sq Rel RMSE RMSE log δ <1.25 δ <1.252 δ <1.253 GeoNet [122] Johnston et al. [48] PackNet-SfM [31] MonoFormer [2] DIFFNET [139] BRNet [36] FeatureNet [97] MonoViT [135] Lite-Mono [128] DualRefine [22] Dynamicdepth [23] SQLdepth [109] RPrDepth [35] Manydepth [112] w/ Hybrid-depth Mono-ViFI [70] w/ Hybrid-depth Monodepth2 [27] w/ Hybrid-depth 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 640 192 M M M M M 0.149 0.106 0.111 0.104 0.102 0.105 0.104 0.099 0.107 0.103 0.096 0.094 0.097 0.098 0.096 0.105 0.094 0.115 0.093 1.060 0.861 0.785 0.846 0.764 0.698 0.729 0.708 0.765 0.776 0.720 0.697 0.658 0.770 0.665 0.708 0.658 0.903 0.596 5.567 4.699 4.601 4.580 4.483 4.462 4.481 4.372 4.561 4.491 4.458 4.320 4. 4.459 4.192 4.446 4.168 4.863 4.113 0.226 0.185 0.189 0.183 0.180 0.179 0.179 0.175 0.183 0.181 0.175 0.172 0.169 0.176 0.170 0.179 0.169 0.193 0.167 0.796 0.889 0.878 0.891 0.896 0.890 0.893 0.900 0.886 0.894 0.897 0.904 0.900 0.900 0.906 0.887 0.906 0.877 0.910 0.935 0.962 0.960 0.962 0.965 0.965 0.965 0.967 0.963 0.965 0.964 0.967 0. 0.965 0.968 0.965 0.968 0.959 0.970 0.975 0.982 0.982 0.982 0.983 0.984 0.984 0.984 0.983 0.983 0.984 0.984 0.985 0.983 0.985 0.984 0.985 0.981 0.986 Table 1. Quantitative results on KITTI Eigen split [19] with the state-of-the-art self-supervised MDE methods. H denotes the resolution of input images. The column of train specifies the way of training, where represents optimizing the model using monocular video. All results are Post-Processed [26]. The best results are in bold; the second best is underlined. The methods integrate with Hybrid-depth modules and outperform all previous methods by large margin on all metrics. Method Train Abs Rel Sq Rel RMSE DepthCLIP [130] Hu et al. [41] Auty et al. [1] Monodepth2 w/ Hybrid-depth - 704 352 640 192 640 0-shot 1-shot 0.473 0.384 0.303 0.093 6.007 4.661 6.322 0.596 12.958 12.290 - 4.113 RMSE log 0.680 0.632 - 0.167 δ <1.25 δ <1.252 δ <1.253 0.281 0.312 0.550 0.910 0.531 0.569 0.830 0. 0.696 0.739 0.938 0.986 Table 2. Comparison with previous MDE methods using CLIP. We outperform all previous methods by large margin on all metrics. where is the differentiable bilinear sampling operator following [27]. To evaluate the reconstructed images ˆIb, we use L1-norm distance in pixel space and SSIM [110] to construct photometric error function following [26, 137]: Lpe = (cid:16) β 1 SSIM (cid:16) Ib, ˆIb (cid:17)(cid:17) +(1β) (cid:13) (cid:13)Ib ˆIb (cid:13) (cid:13) (cid:13) (cid:13)1 , (8) where β = 0.85 by default and SSIM computes pixel similarity over 3 3 window. To improve the smoothness of the predicted depth map, we adopt the edge-aware smoothness loss [26] to regularize the predicted depth map: eyIt. exIt + yd Lsmooth = xd (9) Overall, the total loss Ltotal for training is formulated as: Ltotal = Lpe + λ Lsmooth, (10) where λ is set to 0.001 by default. To reduce the trainable parameters, we only train the visual encoder, learnable tokens, and up-sample layers while freezing the text encoder. 4. Experiments 4.1. Implementation Details Unless otherwise specified, we use the ResNet-50-based CLIP [88] visual encoder and DINOVv2 [77] encoder as the backbone in the image branch and vanilla CLIP text encoder. Our models are implemented in PyTorch [79] and trained for 10 epochs with batch size of 16. We adopt AdamW optimizer with an initial learning rate of 1e-4 and StepLR policy. All the experiments are conducted on single NVIDIA A800 GPU. Moreover, the number of learnable depth and pose tokens is set to 256. 4.2. Comparison with State-of-the-art 4.2.1. Results on KITTI Table 1 presents comparison between three classical MDE methods - Monodepth2 [27], ManyDepth [112] and Mono - VIFI [70]augmented with our framework, and recent state-of-the-art self-supervised MDE approaches. Our proposed method not only enhances the performance of the original methods but also outperforms all other approaches by significant margin across all evaluation metrics. In addition, Table 2 reports the same metrics for few-shot or supervised monocular depth estimation methods that utilize CLIP. Although Auty et al. [1] and Hu et al. [41] achieve MDE in supervised manner, their performance remains substantially lower than that of current self-supervised approaches. In contrast, Hybrid-depth consistently achieves higher accuracy across all metrics, demonstrating that our Figure 5. Qualitative comparison with Manydepth [112], SQLDepth [109] and Monodepth2 with Hybrid-depth on the KITTI dataset. Monodepth2 with Hybrid-depth accurately predicts continuous depth in the ground region while preserving sharp object edges. method effectively leverages the power of large-scale models and pre-training with depth contrastive learning. Additionally, we find Hybrid-depth promotes the convergence of the existing self-supervised MDE methods. 4.2.2. Qualitative Results Fig. 5 presents qualitative comparisons between Monodepth2 with Hybrid-depth and previous approaches on challenging KITTI images. The results demonstrate that Monodepth2 with Hybrid-depth achieves superior structural and edge preservation compared to its counterparts. It produces smoother depth estimations in ground regions while maintaining sharp object boundaries, effectively recovering fine details, and significantly reducing depth artifacts around object edges. 4.3. Ablation Studies In this section, we design the experiments to investigate the following questions: Q1: Do gains stem from the coarse depth sensing stage, not backbone improvements? To investigate the source of performance improvement, we conduct experiments as illustrated in Table. 3. We compare the results of our approach with those obtained using the same backbone but without the coarse depth sensing stage. The findings reveal that merely using more powerful backbone does not In contrast, the achieve the same level of enhancement. coarse-to-fine learning scheme boosts performance. Method Abs Rel Sq Rel RMSE RMSE log δ <1.25 Manydepth [112] w/o co w/ Hybrid-depth Monodepth2 [27] w/o co w/ Hybrid-depth 0.098 0.096 0.096 0.115 0.105 0.093 0.770 0.725 0.665 0.903 0.752 0.596 4.459 4.374 4. 4.863 4.455 4.113 0.176 0.173 0.170 0.193 0.182 0.167 0.900 0.905 0.906 0.877 0.902 0.910 Table 3. Impact of coarse depth sensing stage versus backbone strength on performance, where w/o co denotes that directly train the models following existing self-supervised MDE methods. Q2: Is it necessary for language-guided contrastive learning? To evaluate the effectiveness of language-guided contrastive, we conducted ablation studies by removing the cross-modal contrastive loss to compare the performance against the full model. Furthermore, we explored the role of intramodal contrastive learning. As shown in Table 4, the model without feature alignment exhibited noticeable decline in performance across all metrics. This suggests that aligning visual features with depth-related text cues and intramodal contrastive learning effectively enhances the models understanding of spatial relationships and improves depth prediction. Method Abs Rel Sq Rel RMSE RMSE log δ <1.25 ManyDepth [112] w/ Li w/ Lc w/ Hybrid-depth Monodepth2 [27] w/ Li w/ Lc w/ Hybrid-depth 0.098 0.098 0.096 0.096 0.115 0.098 0.095 0.093 0.770 0.717 0.667 0.665 0.903 0.663 0.675 0. 4.459 4.327 4.199 4.192 4.863 4.236 4.160 4.113 0.176 0.173 0.174 0.170 0.193 0.172 0.169 0.167 0.900 0.905 0.905 0.906 0.877 0.902 0.909 0. Table 4. Comparison of the results with different contrastive loss. Q3: Is depth instruction necessary as granularity calibrator? To assess the role of depth instruction as granularity calibrator, we conducted an ablation study by removing the depth instruction component in both stages. In the first stage, we disabled the cross-modal contrastive loss aligning CLIP-DINO features with depth-related text cues, while the fusion process and intramodal contrastive learning remained unchanged. During the fine depth estimation phrase, we used naive fusion method without language alignment for CLIP-DINO features. The results in Table 5 show significant performance drop when depth instruction is removed, confirming its importance for calibrating granularity between CLIP and DINO features. Method Abs Rel Sq Rel RMSE RMSE log δ <1.25 Monodepth2 [27] w/o co-gc w/o fi-gc w/ Hybrid-depth 0.115 0.098 0.100 0.093 0.903 0.663 0.717 0.596 4.863 4.236 4.284 4.113 0.193 0.172 0.173 0. 0.877 0.904 0.905 0.910 Table 5. Comparison of the results when using depth instruction as granularity calibrator in both stages, where w/o co-gc and w/o fi-gc represent feature fusion without alignment during coarse and fine stage, respectively. Q4: Why do we need both CLIP and DINO? To address this question, we conducted ablation experiments comparing models that use only CLIP-RN50 or DINOv2 VIT-B encoder versus our dual-encoder approach as shown in Table. 6. Our results, combined with Monodepth2 [27], reveal that relying solely on single encoder significantly degrades performance. CLIP provides robust semantic understanding, whereas DINO excels at extracting fine-grained spatial details. By integrating both encoders, our model leverages complementary strengths to facilitate depth estimation. Method Abs Rel Sq Rel RMSE RMSE log δ <1.25 Monodepth2 [27] w/ CLIP w/ DINO w/ Hybrid-depth 0.115 0.102 0.104 0.093 0.903 0.752 0.769 0.596 4.863 4.667 4.685 4. 0.193 0.182 0.180 0.167 0.877 0.902 0.903 0.910 Table 6. Comparison of the result with employing CLIP, DINO, and CLIP+DINO. Q5: What is the effect of learnable depth tokens count? As shown in Table 7, we investigate the impact of the number of learnable depth tokens by varying the token count in Monodepth2 [27] integrated with Hybrid-depth. The results demonstrate non-monotonic relationship: performance improves with increasing token count but degrades beyond this optimal range (e.g., 256 tokens). This trend arises due to capacity-overfitting trade-off: while more tokens enhance model flexibility, excessive tokens may overparameterize the depth prior space, causing overfitting. Despite this, Hybrid-depth exhibits strong robustness and outperforms previous self-supervised methods, thanks to our depth-aware prompt calibration mechanism. Method Abs Rel Sq Rel RMSE RMSE log δ <1.25 128 256 512 1024 0.095 0.093 0.097 0.095 0.650 0.596 0.658 0.649 4.199 4.113 4.182 4. 0.170 0.167 0.172 0.170 0.911 0.910 0.908 0.911 Table 7. Comparison of the results with variant token counts. Q6: Can Hybrid-depth benefit other 3D perception tasks such as BEV prediction? We integrate Hybriddepth with established BEV perception methods like BEVDet [42] and FB-BEV [65]. For fair comparison, we report results using the simplest baseline configurations for both BEVDet and FB-BEV. As shown in Table 8, our approach significantly improves the performance of both methods. These findings reveal that Hybrid-depth could effectively promote 3D perception. Method mAP mATE mASE mAOE NDS BEVDet [42] w/ Hybrid-depth FB-BEV [65] w/ Hybrid-depth 0.283 0.325 0.312 0. 0.773 0.734 0.702 0.673 0.288 0.281 0.275 0.259 0.698 0.623 0.518 0. 0.350 0.395 0.406 0.439 Table 8. Comparison on the nuScenes [7] val set. The resolution of the input image is set to 256 704. 5. Conclusions In this paper, we present novel paradigm named Hybriddepth for self-supervised MDE, which progressively equips the model with the ability to estimate depth from coarse to fine. Initially, we aggregate different-grained features from CLIP and DINO for coarse depth sensing under contrastive language guidance. Then, we fine-tune the model with an auxiliary camera pose to conduct fine depth estimation, and it could serve as an efficient depth encoder to improve the capture of continuous depth variations, thereby enhancing the accuracy of self-supervised MDE methods. Additionally, we employ depth instruction as granularity calibrator mechanism in both stages to address the issue of granularity mismatch to help hybrid-grained features to be well harmonized together. This paradigm significantly outperforms all existing methods by large margin while facilitating 3D recognition like BEV perception."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was supported by Grants of NSFC 62302246, ZJNSFC LQ23F010008, Ningbo 2023Z237 & 2024Z284 & 2024Z289 & 2023CX050011 & 2025Z038, and supported by High Performance Computing Center at Eastern Institute of Technology and Ningbo Institute of Digital Twin."
        },
        {
            "title": "References",
            "content": "[1] Dylan Auty and Krystian Mikolajczyk. Learning to prompt clip for monocular depth estimation: Exploring the limits of human language. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20392047, 2023. 2, 4, 5, 6 [2] Jinwoo Bae, Sungho Moon, and Sunghoon Im. Deep digging into the generalization of self-supervised monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. 6 [3] Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. Adabins: Depth estimation using adaptive bins. In CVPR, 2021. 1, 3 [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 1, 3 [5] Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Zhichao Li, Le Zhang, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsupervised scale-consistent depth learning from video. International Journal of Computer Vision, 129(9): 25482564, 2021. 1, 3, 5 [6] Aleksei Bochkovskii, Amael Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv, 2024. [7] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1162111631, 2020. 8 [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 2, 3 [9] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In Proceedings of the AAAI conference on artificial intelligence, pages 80018008, 2019. 1 [10] Hemang Chawla, Arnav Varma, Elahe Arani, and Bahram Zonooz. Continual learning of unsupervised monocular depth from videos. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 84198429, 2024. 3 timal transport for vision-language models. arXiv preprint arXiv:2210.01253, 2022. 4 [12] Siyu Chen, Hong Liu, Wenhao Li, Ying Zhu, Guoquan Wang, and Jianbing Wu. D3epth: Self-supervised depth estimation with dynamic mask in dynamic scenes. arXiv preprint arXiv:2411.04826, 2024. [13] Zhiyuan Cheng, Cheng Han, James Liang, Qifan Wang, Xiangyu Zhang, and Dongfang Liu. Self-supervised adversarial training of monocular depth estimation against physicalworld attacks. arXiv preprint arXiv:2406.05857, 2024. 3 [14] Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, Li Yi, and Kaisheng Ma. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? In The Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [15] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Masked selfdistillation advances contrastive language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1099511005, 2023. 2 [16] Yue-Jiang Dong, Yuan-Chen Guo, Ying-Tian Liu, FangLue Zhang, and Song-Hai Zhang. Ppea-depth: Progressive parameter-efficient adaptation for self-supervised monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 16091617, 2024. 3 [17] Yue-Jiang Dong, Fang-Lue Zhang, and Song-Hai Zhang. Mal: Motion-aware loss with temporal and distillation hints for self-supervised depth estimation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 73187324, 2024. 3 [18] Ruofei Du, Eric Turner, Maksym Dzitsiuk, Luca Prasso, Ivo Duarte, Jason Dourgarian, Joao Afonso, Jose Pascoal, Josh Gladstone, Nuno Cruces, et al. Depthlab: Real-time 3d interaction with depth maps for mobile augmented reality. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, pages 829843, 2020. [19] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from single image using multi-scale deep network. Advances in neural information processing systems, 27, 2014. 3, 6 [20] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2179521806, 2024. 3 [21] Mohamed Fazli Imam, Rufael Fedaku Marew, Jameel Hassan, Mustansar Fiaz, Alham Fikri Aji, and Hisham Cholakkal. Clip meets dino for tuning zero-shot classifier using unlabeled image collections. arXiv e-prints, pages arXiv2411, 2024. 2 [11] Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang. Prompt learning with op- [22] Cheng Feng, Congxuan Zhang, Zhen Chen, Weiming Hu, Ke Lu, and Liyue Ge. Self-supervised monocular depth estimation with dual-path encoders and offset field interpolation. IEEE Transactions on Image Processing, 34:939954, 2025. 3, [23] Ziyue Feng, Liang Yang, Longlong Jing, Haiyan Wang, YingLi Tian, and Bing Li. Disentangling object motion and occlusion for unsupervised multi-frame monocular depth. In Computer Vision ECCV 2022, pages 228244, Cham, 2022. Springer Nature Switzerland. 6 [24] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20022011, 2018. 3 [25] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. 3 [26] Clement Godard, Oisin Mac Aodha, and Gabriel Brostow. Unsupervised monocular depth estimation with leftright consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 270279, 2017. 5, 6 [27] Clement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38283838, 2019. 1, 2, 3, 5, 6, 7, 8 [28] Juan Luis Gonzalez Bello, Jaeho Moon, and Munchurl Kim. Self-supervised monocular depth estimation with positional shift depth variance and adaptive disparity quantization. IEEE Transactions on Image Processing, 33:2074 2089, 2024. [29] Juan Luis GonzalezBello and Munchurl Kim. Forget about the lidar: Self-supervised depth estimators with med probability volumes. Advances in Neural Information Processing Systems, 33:1262612637, 2020. 3 [30] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021. 2 [31] Vitor Guizilini, Rares Ambrus, Sudeep Pillai, Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation. In CVPR, 2020. 6 [32] Vitor Guizilini, Rui Hou, Jie Li, Rares Ambrus, and Adrien Gaidon. Semantically-guided representation learning for self-supervised monocular depth. In ICLR, 2020. 3 [33] Vitor Guizilini, Rares, Ambrus, , Dian Chen, Sergey Zakharov, and Adrien Gaidon. Multi-frame self-supervised depth with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 160170, 2022. 3 [34] Vitor Guizilini, Pavel Tokmakov, Achal Dave, and Rares Ambrus. Grin: Zero-shot metric depth with pixel-level diffusion. arXiv preprint arXiv:2409.09896, 2024. 3 [35] Wencheng Han and Jianbing Shen. High-precision selfsupervised monocular depth estimation with rich-resource prior. In European Conference on Computer Vision, pages 146162. Springer, 2025. [36] Wencheng Han, Junbo Yin, Xiaogang Jin, Xiangdong Dai, and Jianbing Shen. Brnet: Exploring comprehensive features for monocular depth estimation. In European Conference on Computer Vision, pages 586602. Springer, 2022. 6 [37] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 3 [38] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1600016009, 2022. 3 [39] Derek Hoiem, Alexei Efros, and Martial Hebert. Recovering surface layout from an image. IJCV, 2007. 3 [40] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3d v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. arXiv preprint arXiv:2404.15506, 2024. [41] Xueting Hu, Ce Zhang, Yi Zhang, Bowen Hai, Ke Yu, and Zhihai He. Learning to adapt clip for few-shot monocular In Proceedings of the IEEE/CVF Windepth estimation. ter Conference on Applications of Computer Vision, pages 55945603, 2024. 2, 4, 6 [42] Junjie Huang, Guan Huang, Zheng Zhu, Ye Yun, and Bevdet: High-performance multi-camera arXiv preprint Dalong Du. 3d object detection in bird-eye-view. arXiv:2112.11790, 2021. 8 [43] Pan Ji, Runze Li, Bir Bhanu, and Yi Xu. Monoindoor: Towards good practice of self-supervised monocular depth esIn Proceedings of the timation for indoor environments. IEEE/CVF International Conference on Computer Vision, pages 1278712796, 2021. 3 [44] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904 4916. PMLR, 2021. 3 [45] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. ViIn Computer VisionECCV 2022: sual prompt tuning. 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXXIII, pages 709727. Springer, 2022. [46] Dongsheng Jiang, Yuchen Liu, Songlin Liu, Jine Zhao, Hao Zhang, Zhen Gao, Xiaopeng Zhang, Jin Li, and Hongkai Xiong. From clip to dino: Visual encoders shout arXiv preprint in multi-modal large language models. arXiv:2310.08825, 2023. 2 [47] Jianbo Jiao, Ying Cao, Yibing Song, and Rynson Lau. Look deeper into depth: Monocular depth estimation with semantic booster and attention-driven loss. In Proceedings of the European conference on computer vision (ECCV), pages 5369, 2018. 3 [48] Adrian Johnston and Gustavo Carneiro. Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 6 [49] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 94929502, 2024. 3 [50] Numair Khan, Min Kim, and James Tompkin. Differentiable diffusion for dense depth estimation from multiview images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8912 8921, 2021. [51] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: the Multi-modal prompt IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1911319122, 2023. 2, 4 In Proceedings of learning. [52] Dunam Kim and Seokju Lee. Clip can understand depth. arXiv preprint arXiv:2402.03251, 2024. 3 [53] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [54] Marvin Klingner, Jan-Aike Termohlen, Jonas Mikolajczyk, and Tim Fingscheidt. Self-Supervised Monocular Depth Estimation: Solving the Dynamic Object Problem by SeIn European Conference on Computer mantic Guidance. Vision (ECCV), 2020. 3 [55] Arnaud Leduc, Anthony Cioppa, Silvio Giancola, Bernard Ghanem, and Marc Van Droogenbroeck. Soccernet-depth: scalable dataset for monocular depth estimation in sports In Proceedings of the IEEE/CVF Conference on videos. Computer Vision and Pattern Recognition, pages 3280 3292, 2024. 3 [56] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. 4 [57] Bohan Li, Yasheng Sun, Zhujin Liang, Dalong Du, Zhuanghui Zhang, Xiaofeng Wang, Yunnan Wang, Xin Jin, and Wenjun Zeng. Bridging stereo geometry and bev representation with reliable mutual interaction for semantic scene completion. arXiv preprint arXiv:2303.13959, 2023. 3 [58] Bohan Li, Jiajun Deng, Wenyao Zhang, Zhujin Liang, Dalong Du, Xin Jin, and Wenjun Zeng. Hierarchical temporal context learning for camera-based semantic scene completion. In European Conference on Computer Vision, pages 131148. Springer, 2024. 3 [59] Bohan Li, Jiazhe Guo, Hongsi Liu, Yingshuang Zou, Yikang Ding, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, et al. Uniscene: Unified occupancy-centric driving scene generation. arXiv preprint arXiv:2412.05435, 2024. [60] Bohan Li, Yasheng Sun, Jingxin Dong, Zheng Zhu, Jinming Liu, Xin Jin, and Wenjun Zeng. One at time: Progressive multi-step volumetric probability learning for reliable 3d scene perception. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 30283036, 2024. 1 [61] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. 3 In Proceedings of [62] Siyuan Li, Li Sun, and Qingli Li. Clip-reid: exploiting vision-language model for image re-identification without In Proceedings of the AAAI Conferconcrete text labels. ence on Artificial Intelligence, pages 14051413, 2023. 3, 4 [63] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 20412050, 2018. 3 [64] Zhenyu Li, Xuyang Wang, Xianming Liu, and Junjun Jiang. Binsformer: Revisiting adaptive bins for monocular depth estimation. arXiv:2204.00987, 2022. [65] Zhiqi Li, Zhiding Yu, Wenhai Wang, Anima Anandkumar, Tong Lu, and Jose Alvarez. FB-BEV: BEV representation from forward-backward view transformations. In IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 8 [66] Dingkang Liang, Jiahao Xie, Zhikang Zou, Xiaoqing Ye, Wei Xu, and Xiang Bai. Crowdclip: Unsupervised crowd In Proceedings of counting via vision-language model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28932903, 2023. 3 [67] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70617070, 2023. 3 [68] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2 [69] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and William Freeman. Sift flow: Dense correspondence across different scenes. In ECCV, 2008. 3 [70] Jinfeng Liu, Lingtong Kong, Bo Li, Zerong Wang, Hong Gu, and Jinwei Chen. Mono-vifi: unified learning framework for self-supervised single and multi-frame monocular In European Conference on Computer depth estimation. Vision, pages 90107. Springer, 2024. 2, 6 [71] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian. Prompt distribution learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52065215, 2022. [72] Timo Luddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 70867096, 2022. 2 [73] Huaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. SegCLIP: Patch aggregation with learnable centers for open-vocabulary semantic segmentation. ICML, 2023. 3 [74] Xiaoyang Lyu, Liang Liu, Mengmeng Wang, Xin Kong, Lina Liu, Yong Liu, Xinxin Chen, and Yi Yuan. Hr-depth: High resolution self-supervised monocular depth estimation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 22942301, 2021. 3 [75] Hoang Chuong Nguyen, Tianyu Wang, Jose M. Alvarez, and Miaomiao Liu. Mining supervision for dynamic regions in self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1044610455, 2024. 3 [76] James Okae, Bohan Li, Juan Du, and Yueming Hu. Robust scale-aware stereo matching network. IEEE Transactions on Artificial Intelligence, 3(2):244253, 2021. 1, [77] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 3, 6 [78] Jin-Hwi Park, Chanhwi Jeong, Junoh Lee, and Hae-Gon Jeon. Depth prompting for sensor-agnostic depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9859 9869, 2024. 3 [79] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 6 [80] Suraj Patni, Aradhye Agarwal, and Chetan Arora. Ecodepth: Effective conditioning of diffusion models In Proceedings of the for monocular depth estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2828528295, 2024. 3 [81] Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, and Sebastian arXiv Riedel. Language models as knowledge bases? preprint arXiv:1909.01066, 2019. 4 [82] Duc-Hai Pham, Tung Do, Phong Nguyen, Binh-Son Hua, Khoi Nguyen, and Rang Nguyen. Sharpdepth: Sharpening metric depth predictions using diffusion distillation. arXiv preprint arXiv:2411.18229, 2024. 1 [83] Luigi Piccinelli, Yung-Hsu Yang, Christos Sakaridis, Mattia Segu, Siyuan Li, Luc Van Gool, and Fisher Yu. Unidepth: Universal monocular metric depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1010610116, 2024. 3 [84] Matteo Poggi, Filippo Aleotti, Fabio Tosi, and Stefano Mattoccia. On the uncertainty of self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32273237, 2020. 3 [85] Zekun Qi, Runpei Dong, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma, and Li Yi. Contrast with reconstruct: Contrastive 3d representation learning guided by generative pretraining. In International Conference on Machine Learning, pages 2822328243. PMLR, 2023. 3 [86] Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, Li Yi, and Kaisheng Ma. Shapellm: Universal 3d object understanding for embodied interaction. In European Conference on Computer Vision, pages 214238. Springer, 2024. 3 [87] Zekun Qi, Wenyao Zhang, Yufei Ding, Runpei Dong, Xinqiang Yu, Jingwen Li, Lingyun Xu, Baoyu Li, Xialin He, Guofan Fan, et al. Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. arXiv preprint arXiv:2502.13143, 2025. 1 [88] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International conference on machine learning, vision. pages 87488763. PMLR, 2021. 2, 3, 4, [89] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1217912188, 2021. 5 [90] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022. 3 [91] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with context-aware prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1808218091, 2022. 2, 3 [92] Kieran Saunders, George Vogiatzis, and Luis Manso. Selfsupervised monocular depth estimation: Lets talk about the In Proceedings of the IEEE/CVF International weather. Conference on Computer Vision, pages 89078917, 2023. 3 [93] Ashutosh Saxena, Min Sun, and Andrew Ng. Make3d: Learning 3d scene structure from single still image. TPAMI, 2008. 3 [94] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems, 36, 2024. [95] Shuwei Shao, Zhongcai Pei, Weihai Chen, Xingming Wu, and Zhengguo Li. Nddepth: Normal-distance assisted monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 79317940, 2023. 3 [96] Shuwei Shao, Zhongcai Pei, Weihai Chen, Dingchi Sun, Peter CY Chen, and Zhengguo Li. Monodiffusion: selfsupervised monocular depth estimation using diffusion IEEE Transactions on Circuits and Systems for model. Video Technology, 2024. 3 [97] Chang Shu, Kun Yu, Zhixiang Duan, and Kuiyuan Yang. Feature-metric loss for self-supervised learning of depth and egomotion. In European Conference on Computer Vision, pages 572588. Springer, 2020. 6 [98] Haozhe Si, Bin Zhao, Dong Wang, Yunpeng Gao, Mulin Chen, Zhigang Wang, and Xuelong Li. Fully selfsupervised depth estimation from defocus clue. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 91409149, 2023. 3 [99] Eunjin Son and Sang Jun Lee. Cabins: Clip-based adaptive bins for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 45574567, 2024. [100] Samuel Stevens, Wei-Lun Chao, Tanya Berger-Wolf, and Sparse autoencoders for scientifically rigorarXiv preprint Yu Su. ous interpretation of vision models. arXiv:2502.06755, 2025. 2 [101] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 2 [102] Madhu Vankadari, Sourav Garg, Anima Majumder, Swagat Kumar, and Ardhendu Behera. Unsupervised monocular depth estimation for night-time images using adversarial domain feature adaptation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVIII 16, pages 443459. Springer, 2020. 3 [103] Madhu Vankadari, Samuel Hodgson, Sangyun Shin, Kaichen Zhou, Andrew Markham, and Niki Trigoni. Dusk till dawn: Self-supervised nighttime stereo depth estimation using visual foundation models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1797617982, 2024. 3 [104] Amanpreet Walia, Stefanie Walz, Mario Bijelic, Fahim Mannan, Frank Julca-Aguilar, Michael Langer, Werner Ritter, and Felix Heide. Gated2gated: Self-supervised depth In Proceedings of the estimation from gated images. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 28112821, 2022. [105] Ning-Hsu Wang and Yu-Lun Liu. Depth anywhere: Enhancing 360 monocular depth estimation via perspective distillation and unlabeled data augmentation. arXiv preprint arXiv:2406.12849, 2024. 3 [106] Ruoyu Wang, Zehao Yu, and Shenghua Gao. Planedepth: Self-supervised depth estimation via orthogonal planes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2142521434, 2023. 3, 5 [107] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Weinberger. Pseudolidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 84458453, 2019. 1 [108] Yingqian Wang, Longguang Wang, Zhengyu Liang, Jungang Yang, Wei An, and Yulan Guo. Occlusion-aware cost constructor for light field depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1980919818, 2022. 3 [109] Youhong Wang, Yunji Liang, Hao Xu, Shaohui Jiao, and Hongkai Yu. Sqldepth: Generalizable self-supervised finestructured monocular depth estimation. Proceedings of the AAAI Conference on Artificial Intelligence, 38(6):5713 5721, 2024. 6, 7 [110] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility IEEE transactions on image proto structural similarity. cessing, 13(4):600612, 2004. [111] Jamie Watson, Michael Firman, Gabriel Brostow, and Daniyar Turmukhambetov. Self-supervised monocular depth hints. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21622171, 2019. 3 [112] Jamie Watson, Oisin Mac Aodha, Victor Prisacariu, Gabriel Brostow, and Michael Firman. The Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. In Computer Vision and Pattern Recognition (CVPR), 2021. 2, 3, 6, 7, 8 [113] Songlin Wei, Haoran Geng, Jiayi Chen, Congyue Deng, Cui Wenbo, Chengyang Zhao, Xiaomeng Fang, Leonidas Guibas, and He Wang. D3roma: Disparity diffusion-based depth sensing for material-agnostic robotic manipulation. In ECCV 2024 Workshop on Wild 3D: 3D Modeling, Reconstruction, and Generation in the Wild, 2024. 3 [114] Diana Wofk, Fangchang Ma, Tien-Ju Yang, Sertac Karaman, and Vivienne Sze. Fastdepth: Fast monocular depth In 2019 International estimation on embedded systems. Conference on Robotics and Automation (ICRA), pages 61016108. IEEE, 2019. 1 [115] Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Yang Xiao, Ruibo Li, and Zhenbo Luo. Monocular relative depth perception with web stereo data supervision. In CVPR, 2018. 3 [116] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 1, 3 [117] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024. [118] Zhuoyue Yang, Junjun Pan, Ju Dai, Zhen Sun, and Yi Xiao. Self-supervised endoscopy depth estimation framework with clip-guidance segmentation. Biomedical Signal Processing and Control, 95:106410, 2024. 3 [119] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, TatSeng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021. 4 [120] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric constraints of virtual normal for depth prediction. In ICCV, 2019. 3 [121] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d: Towards zero-shot metric 3d prediction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 90439053, 2023. 3 [122] Zhichao Yin and Jianping Shi. GeoNet: Unsupervised learning of dense depth, optical flow and camera pose. In CVPR, 2018. 3, 6 [123] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Weinberger. Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving. In ICLR, 2020. [124] Ziyao Zeng, Jingcheng Ni, Daniel Wang, Patrick Rim, Younjoon Chung, Fengyu Yang, Byung-Woo Hong, and Alex Wong. Priordiffusion: Leverage language prior in diffusion models for monocular depth estimation. arXiv preprint arXiv:2411.16750, 2024. 3 [125] Ziyao Zeng, Daniel Wang, Fengyu Yang, Hyoungseob Park, Stefano Soatto, Dong Lao, and Alex Wong. Wordepth: Variational language prior for monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 9708 9719, 2024. 4 [126] Ziyao Zeng, Yangchao Wu, Hyoungseob Park, Daniel Wang, Fengyu Yang, Stefano Soatto, Dong Lao, ByungWoo Hong, and Alex Wong. Rsa: Resolving scale ambiguities in monocular depth estimators through language descriptions. arXiv preprint arXiv:2410.02924, 2024. 3 [127] Bingfeng Zhang, Siyue Yu, Jimin Xiao, Yunchao Wei, and Yao Zhao. Frozen clip-dino: strong backbone for weakly supervised semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 117, 2025. 2 [128] Ning Zhang, Francesco Nex, George Vosselman, and Norman Kerle. Lite-mono: lightweight cnn and transformer architecture for self-supervised monocular depth estimaIn Proceedings of the IEEE/CVF Conference on tion. Computer Vision and Pattern Recognition (CVPR), pages 1853718546, 2023. 6 [129] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85528562, 2022. 3 [130] Renrui Zhang, Ziyao Zeng, Ziyu Guo, and Yafeng Li. In Proceedings of the Can language understand depth? 30th ACM International Conference on Multimedia, pages 68686874, 2022. 2, 4, [131] Wenyao Zhang, Shipeng Lyu, Feng Xue, Chen Yao, Zheng Zhu, and Zhenzhong Jia. Predict the rover mobility over IEEE soft terrain using articulated wheeled bevameter. Robotics and Automation Letters, 7(4):1206212069, 2022. 1 [132] Wenyao Zhang, Shipeng Lyu, Chen Yao, Feng Xue, Zheng Zhu, and Zhenzhong Jia. Analysis of robot traversabilIn ity over unstructured terrain using information fusion. 2022 International Conference on Advanced Robotics and Mechatronics (ICARM), pages 413418. IEEE, 2022. 1 [133] Wenyao Zhang, Letian Wu, Zequn Zhang, Tao Yu, Chao Ma, Xin Jin, Xiaokang Yang, and Wenjun Zeng. Unleash the power of vision-language models by visual attention prompt and multi-modal interaction. IEEE Transactions on Multimedia, pages 113, 2024. 4 [134] Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, XinQiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, He Wang, Zhizheng Zhang, et al. Dreamvla: visionlanguage-action model dreamed with comprehensive world knowledge. arXiv preprint arXiv:2507.04447, 2025. 1 [135] Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and Stefano Mattoccia. Monovit: Self-supervised monocular depth estimation with vision transformer. In 2022 International Conference on 3D Vision (3DV), pages 668678. IEEE, 2022. 6 [136] Chaoqiang Zhao, Matteo Poggi, Fabio Tosi, Lei Zhou, Qiyu Sun, Yang Tang, and Stefano Mattoccia. Gasmono: Geometry-aided self-supervised monocular depth estimation for indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16209 16220, 2023. [137] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. IEEE Transactions on computational imaging, 3(1):4757, 2016. 5, 6 [138] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: RegionIn Proceedings of the based language-image pretraining. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1679316803, 2022. 2 [139] Hang Zhou, David Greenwood, and Sarah Taylor. Selfsupervised monocular depth estimation with internal feature fusion. In British Machine Vision Conference (BMVC), 2021. 6 [140] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language In Proceedings of the IEEE/CVF Conference on models. Computer Vision and Pattern Recognition, pages 16816 16825, 2022. 4 [141] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 130(9):2337 2348, 2022. 2, 4, 5 [142] Tinghui Zhou, Matthew Brown, Noah Snavely, and David Lowe. Unsupervised learning of depth and egomotion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1851 1858, 2017. 1, 3, [143] Hu Zhu, Chen Yao, Zheng Zhu, Zhengtao Liu, and Zhenzhong Jia. Fusing panoptic segmentation and geometry information for robust visual slam in dynamic environments. In 2022 IEEE 18th International Conference on Automation Science and Engineering (CASE), pages 16481653, 2022. 1 [144] Muzhi Zhu, Hengtao Li, Hao Chen, Chengxiang Fan, Weian Mao, Chenchen Jing, Yifan Liu, and Chunhua Shen. Segprompt: Boosting open-world segmentation via In Proceedings of the category-level prompt learning. IEEE/CVF International Conference on Computer Vision, pages 9991008, 2023. 4 [145] Shengjie Zhu, Garrick Brazil, and Xiaoming Liu. The edge of depth: Explicit constraints between segmentation and depth. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1311613125, 2020. 3 [146] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world In Proceedings of the IEEE/CVF International learning. Conference on Computer Vision, pages 26392650, 2023. 3 [147] Ying Zou, Zhe Chen, and Fuliang Yin. High-order multiscale attention and vertical discriminator enhanced clip for monocular depth estimation. IEEE Transactions on Circuits and Systems for Video Technology, pages 11, 2025."
        }
    ],
    "affiliations": [
        "CASIA",
        "MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University",
        "Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China",
        "Ningbo Key Laboratory of Spatial Intelligence and Digital Derivative, Ningbo, China",
        "Tsinghua University",
        "University of Science and Technology of China"
    ]
}