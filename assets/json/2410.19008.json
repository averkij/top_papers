{
    "paper_title": "Teach Multimodal LLMs to Comprehend Electrocardiographic Images",
    "authors": [
        "Ruoqi Liu",
        "Yuelin Bai",
        "Xiang Yue",
        "Ping Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on a narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, a comprehensive ECG image instruction tuning dataset of over one million samples, covering a wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, a new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets a new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 2 ] . e [ 1 8 0 0 9 1 . 0 1 4 2 : r TEACH MULTIMODAL LLMS TO COMPREHEND ELECTROCARDIOGRAPHIC IMAGES Ruoqi Liu, Yuelin Bai, Xiang Yue, Ping Zhang The Ohio State University, Carnegie Mellon University https://aimedlab.github.io/PULSE/"
        },
        {
            "title": "ABSTRACT",
            "content": "The electrocardiogram (ECG) is an essential non-invasive diagnostic tool for assessing cardiac conditions. Existing automatic interpretation methods suffer from limited generalizability, focusing on narrow range of cardiac conditions, and typically depend on raw physiological signals, which may not be readily available in resource-limited settings where only printed or digital ECG images are accessible. Recent advancements in multimodal large language models (MLLMs) present promising opportunities for addressing these challenges. However, the application of MLLMs to ECG image interpretation remains challenging due to the lack of instruction tuning datasets and well-established ECG image benchmarks for quantitative evaluation. To address these challenges, we introduce ECGInstruct, comprehensive ECG image instruction tuning dataset of over one million samples, covering wide range of ECG-related tasks from diverse data sources. Using ECGInstruct, we develop PULSE, an MLLM tailored for ECG image comprehension. In addition, we curate ECGBench, new evaluation benchmark covering four key ECG image interpretation tasks across nine different datasets. Our experiments show that PULSE sets new state-of-the-art, outperforming general MLLMs with an average accuracy improvement of 15% to 30%. This work highlights the potential of PULSE to enhance ECG interpretation in clinical practice. Figure 1: The proposed PULSE demonstrates superior performance across multiple in-domain and out-of-domain datasets on our constructed ECGBench compared with advanced proprietary MLLMs (e.g., GPT-4o). Notably, the proprietary MLLMs often fail to accurately interpret ECG images, generating well-structured and contextually relevant responses but ultimately incorrect (with errors highlighted in red) compared to the ground truth diagnosis. Equal Contribution"
        },
        {
            "title": "INTRODUCTION",
            "content": "The electrocardiogram (ECG) is an essential tool in diagnosing cardiovascular diseases due to its non-invasive, cost-effective, and widely accessible nature for assessing cardiac function. While some approaches have been proposed for automatic ECG diagnosis (Hannun et al., 2019; Ribeiro et al., 2020; Hughes et al., 2021), these are primarily designed for classification tasks with limited cardiac conditions, often lacking generalizability. Moreover, they typically treat ECG data as timeseries physiological signals, which may not always be available, particularly in resource-constrained settings where only printed or digital images are accessible (Sangha et al., 2022; 2023). Recent advancements in multimodal large language models (MLLMs) have shown impressive success across vision-language tasks, offering new possibilities for addressing the limitations of traditional ECG models. However, applying MLLMs to ECG interpretation is not straightforward. As illustrated in Fig. 1, current MLLMs, such as GPT-4o (OpenAI, 2024), often provide responses that appear correct and contextually relevant but are ultimately inaccurate in interpreting ECG images. This highlights the need for specialized MLLMs for ECG image interpretation. Developing MLLMs for ECG images faces several challenges. First, no large-scale ECG image datasets are currently available as most ECG datasets contain only raw signal data, which needs to be synthesized into digital images. Second, there is lack of instruction tuning datasets for ECG images. Large high-quality instruction tuning datasets, which are crucial for MLLM development, need to be curated from scratch for ECG-related tasks. Finally, evaluation is just as critical as model development, yet no established benchmark exists for assessing MLLM performance in ECG image interpretation. well-defined benchmark is essential for both quantifying model performance and identifying areas for future improvement. In this paper, we tackle these challenges by introducing ECGInstruct, the first large-scale ECG image instruction tuning dataset containing over one million ECG image-text samples. ECGInstruct is characterized by: 1) realistic image synthesis that replicates artifacts commonly seen in paper-based ECGs, 2) diverse range of ECG-related tasks refined with insights from clinical experts, and 3) data sourced from distinct geographic regions. Leveraging ECGInstruct, we develop PULSE, an MLLM for ECG image comprehension. To evaluate PULSE, we present ECGBench, comprehensive evaluation benchmark covering four ECG image interpretation tasks across nine different datasets. ECGBench includes repurposed tasks (e.g., abnormality detection) from existing datasets, and newly curated, more challenging tasks using real-world ECG images. Evaluated on ECGBench, PULSE sets new state-of-the-art, significantly outperforming proprietary MLLMs across all benchmarks with an average accuracy gain of 15% to 30% compared to GPT-4o on out-of-domain datasets  (Fig. 1)  . Ablation experiments demonstrate the importance of incorporating diverse data sources and ECG instruction tasks into the training data. case study and discussion further illustrate the models effectiveness in ECG image interpretation. To summarize, our main contributions are as follows, Problem. We investigate the capabilities of MLLMs in ECG image interpretation and evaluate their performance across various downstream tasks. To the best of our knowledge, this is the first study focused on assessing MLLMs in image-based ECG interpretation. Dataset. We construct ECGInstruct, large-scale ECG image instruction tuning dataset consisting of wide range of ECG-related tasks, serving as valuable resource for fine-tuning MLLMs for ECG image interpretation. Model. We develop PULSE, new MLLM tailored for ECG image interpretation. The model achieves state-of-the-art performance, outperforming both proprietary and open-source MLLMs. Evaluation. We establish ECGBench, comprehensive benchmark for evaluating ECG image interpretation, which includes diverse evaluation tasks, both real-world and synthesized images."
        },
        {
            "title": "2 ECGINSTRUCT: TEACH MLLMS TO COMPREHEND ECG IMAGES",
            "content": "We aim to curate list of multifaceted instruction tuning datasets for ECG analysis that are featured by 1) realistic image synthesis resembling the artifacts in paper ECGs, 2) diverse types of ECG2 Figure 2: ECGInstruct: list of diverse and large-scale instruction tuning datasets for ECG (1) ECG images are synthesized from raw signal recordings with various image interpretation. (2) ECGInstruct is curated based on distortions that mimic real-world printed ECG images. clinician-defined ECG-related tasks, original diagnosis and clinical reports, and diverse task types. Additional quality checking is applied to filter lower-scored instructions. Source Dataset Task Type # Samples PTB-XL (Wagner et al., 2020) ECG-QA (Oh et al., 2024) MIMIC-IV-ECG (Gow et al., 2023) Close/Open/Fill/MCQ 30K Feature Rhythm Close/Open/Fill/MCQ 36K Morphology Close/Open/Fill/MCQ 67K 16K Report Open Feature Close Close Rhythm Morphology Close 40K 9K 90K Close/Open/Fill/MCQ 29K Feature Rhythm Close/Open/Fill/MCQ 115K Morphology Close/Open/Fill/MCQ 169K 487K Report Open CODE-15% (Ribeiro et al., 2021) Close Feature Rhythm Close Morphology Close Total (ECGInstruct) 22K 14K 31K 1.2M Table 1: Summary of ECGInstruct. Feature: basic feature recognition, Rhythm: heart rhythm analysis, Morphology: morphology and pathology identification, Report: clinical report generation. Close: close-ended QA, Open: open-ended QA, Fill: fill-in-the-blank, MCQ: multi-choice QA. The full table of data statistics is provided in Appendix Table A1. related tasks with clinical experts insights, and 3) different data sources from distinct geographical regions. We show the construction of ECGInstruct in Fig. 2 and data summary in Table 1. 2.1 ECG IMAGE SYNTHESIS WITH VARIOUS DISTORTIONS To enhance the robustness and real-world applicability of our model, we synthesize ECG images mimicking common artifacts found in paper ECGs. We adopt an ECG image synthesis tool (Shivashankara et al., 2024) that provides various imperfections such as grid line interference, creases, wrinkles, paper rotations, etc. By including these synthesized artifacts, we aim to train models that 3 can effectively interpret ECGs in less-than-ideal conditions, as often encountered in clinical settings. More details are provided in Appendix C. 2.2 ECG-RELATED TASKS WITH CLINICAL EXPERTS INSIGHTS To construct comprehensive set of ECG-related tasks, we consulted domain experts to curate diverse and clinically relevant tasks covering four different categories. Each category is designed to address specific aspects of ECG interpretation and analysis, including (1) basic feature recognition (see examples in Appendix Fig. A1), (2) heart rhythm analysis (see examples in Appendix Fig. A2), (3) morphology and pathology identification (see examples in Appendix Fig. A3) and (4) clinical report generation (see examples in Appendix Fig. A4). Basic feature recognition (e.g., interval or segment, etc.) forms the foundation of ECG interpretation, enabling the model to grasp essential cardiac parameters. Heart rhythm analysis (e.g., arrhythmias, conduction abnormalities, etc.) and morphology and pathology identification (e.g., wave shape, pathological conditions, etc.) are more advanced and critical aspects of ECG analysis, ensuring that the model can detect and classify complex conditions accurately. Lastly, clinical report generation mirrors the process of healthcare professionals synthesizing comprehensive interpretation of an ECG. By incorporating clinical experts insights, we encourage the model to learn the practical skills required in clinical context. 2.3 DIVERSE TYPES OF TASKS AND DATA SOURCES Based on the original diagnoses and clinical reports from the existing ECG datasets, we curate diverse types of tasks including multi-choice questions, fill-in-the-blank, close-ended QA, and openended QA. This variety of task types not only enhances the models versatility but also mimics the diverse cognitive processes involved in real-world ECG interpretation. By incorporating these varied task types, we aim to develop more robust and adaptable model capable of handling wide spectrum of ECG-related queries and analyses. To ensure broad applicability and generalizability, we collect ECG data from four different sources across geographically distinct regions: 1) PTB-XL (Wagner et al., 2020): Germany-based, publicly available repository; (2) MIMIC-IV-ECG (Gow et al., 2023): large set of ECGs for patients who appear in the MIMIC-IV Clinical Database from Beth Israel Deaconess Medical Center in Boston (Johnson et al., 2023); 3) CODE-15% (Ribeiro et al., 2021): an ECG dataset from central ECG repository from Minas Gerais, Brazil under the clinical outcomes in digital electrocardiology (CODE) study (Ribeiro et al., 2019); 4) ECG-QA (Oh et al., 2024), question answering dataset for ECGs that is constructed based on PTB-XL (Wagner et al., 2020). This diverse geographical representation enhances the models ability to generalize across different populations and healthcare systems, accounting for potential variations in ECG patterns and interpretations across regions. 2.4 DATA SYNTHESIZING AT SCALE Since large-scale annotation of ECG features is extremely expensive and time-consuming, we develop an automatic data synthesizing pipeline to address this data scarcity issue. We utilized clinical reports from PTB-XL and MIMIC-IV-ECG as initial seed data and leveraged an advanced LLM (i.e., Llama-3-70B-Instruct) for data synthesis. Building upon the expert-in-the-loop process and diverse data resources described in the previous sections, we synthesized substantial volume of ECGrelated instructions and corresponding responses. These were based on expert-provided examples and real-world scenarios, with the specific prompts used in this process detailed in the Appendix E. For datasets lacking comprehensive reports, such as CODE-15%, we manually constructed diverse templates to transform the existing data into an instruction-response format. 2.5 QUALITY CONTROL To guarantee the quality of generated instructions and corresponding responses, we apply an independent LLM as judge to evaluate and score the content. This process involves several steps: 1) initial generation: instructions and responses are first generated using our primary model; 2) evaluation criteria: we establish set of evaluation criteria including the instruction relevance, clarity, answerability of the responses, etc; 3) LLM judge and scoring: an independent LLM (Llama 3 (Meta, 2024)) is used as judge to assess each instruction-response pair against established cri4 Figure 3: The data curation process for ECGBench. There are four key tasks involved: (1) two repurposed tasks (abnormality detection and report generation) derived from existing ECG datasets, where ECG images are synthesized from raw signals, and queries/answers are extracted based on diagnostic and clinical reports; (2) Two newly developed tasks using external resources, where ECG images and associated questions and answers are collected and generated from real-world sources. teria and assign scores (see prompt in Appendix Fig. A8); 4) feedback loop: low-scoring items are flagged for human expert review and potential revision or removal; 5) iterative refinement: based on the scoring patterns and human expert input, we continually refine our instruction generation process. By combining automated LLM evaluation with human expert oversight, we create robust system for maintaining and improving the quality of our instruction-response pairs. 2.6 TRAINING Our model architecture closely follows that of LLaVA (Liu et al., 2024b;c), adapting it for ECG image analysis. We use vision encoder to process ECG images and large language model as the text decoder, connected via projection layer. We organize the data into three components: the image, the instructions, and the outputs. The instruction is query or task related to the ECG image and the output is the expected response or prediction base on the image and instruction. We place the image at the beginning of each conversation, serving as the visual grounding for the entire dialogue. During training, we freeze the parameters of the vision encoder while updating the parameters of the projection layer and the language model using an autoregressive training objective, where we mask all the tokens belonging to the image and the instruction."
        },
        {
            "title": "3 ECGBENCH",
            "content": "In this section, we present ECG-Bench  (Fig. 3)  , comprehensive benchmark for evaluating MLLMs on ECG image interpretation. Our benchmark contains both repurposed tasks from six existing datasets and newly created tasks from external resources. Table 2 shows the details of each evaluation dataset. We introduce the detailed evaluation task curation process below. 5 Evaluation Dataset Task Type # Samples In-Domain? PTB-XL Super PTB-XL Report CODE-15% ECG-QA CPSC 2018 CSN G12EC MMMU ECG ECG Arena Abnormality Detection Report Generation Abnormality Detection Abnormality Detection Close-ended Open-ended Close-ended Close-ended Close-ended Abnormality Detection MCQ (8-option) Abnormality Detection Abnormality Detection MCQ (8-option) Multimodal Understanding MCQ (4-option) Multi-turn Conversation Open-ended 2,082 500 1,400 1,317 2,061 1,611 2,026 200 50 YES YES YES YES NO NO NO NO NO Table 2: Overview of evaluation datasets in ECGBench. This collection contains both in-domain and out-of-domain problems across four key tasks with diverse answer types. 3.1 EVALUATION TASK CURATION Abnormality Detection. This task focuses on detecting cardiac abnormalities using ECG images. We curate this task by repurposing six existing ECG datasets: three in-domain datasets: PTBXL (Super) (Wagner et al., 2020), CODE-15% (Ribeiro et al., 2021), ECG-QA (Oh et al., 2024), and three out-of-domain datasets: CPSC 2018 (Liu et al., 2018), CSN (Zheng et al., 2020a;b) and G12EC (Liu et al., 2018). For all datasets, we first synthesize images using raw signals and then curate queries based on the original diagnosis and reports. For datasets with fewer than 10 diagnostic labels, we curate close-ended questions. For those with more labels, we construct multi-choice questions with 8 options, including the original diagnosis and randomly sampled negative labels. Report Generation. This task involves generating detailed reports for given ECG images. We benchmark using 500 randomly selected reports from the test set of PTB-XL, which contains highquality ECG reports written and validated by cardiologists. Similarly, the ECG images are synthesized from the raw signals. For the ground truth reports written in non-English (PTB-XL is Germany-based dataset), we translate the reports into English before the evaluation. MMMU ECG. Inspired by MMMU (Yue et al., 2024), widely adopted evaluation benchmark for MLLMs, we manually curated an ECG version with 200 multi-choice questions with the help of medical school students. The curation process involved three key steps: (1) Resource Selection: We gathered ECG materials from diverse and reliable sources such as ECG textbooks, clinical case reports from medical journals, and widely used online ECG learning materials. This ensures the comprehensiveness and quality of collected ECG examples and interpretations. (2) Question Creation and Collection: Five medical school students with basic knowledge of ECG were recruited for this task. They extracted existing questions from the collected resources. For ECG images accompanied only by clinical interpretations, the annotators created questions based on these interpretations. Additionally, they formulated new questions drawing from their expertise, ensuring balance between various ECG interpretation aspects (e.g., rhythm analysis, morphology assessment, clinical interpretation). (3) Quality Control: To maintain high standards, we implemented quality control process. In particular, Each question underwent review by at least two other annotators, checking for accuracy and clarity. An independent reviewer cross-checked the final images, questions, and corresponding answers against the original sources to ensure fidelity to the source material. Any discrepancies or ambiguities were resolved during this process. ECG Arena. To assess the models instruction-following ability in ECG comprehension, we developed ECG Arena, inspired by MT-Bench (Zheng et al., 2024) and Arena-hard (Chiang et al., 2024) used in general LLM chat evaluations. We manually curated 50 multi-turn ECG-related questions, focusing on open-ended interactions. The data curation process for ECG Arena, like MMMU ECG, involves three main steps: resource selection, question creation, and quality control. The key distinction is that MMMU ECG focuses on multiple-choice questions, whereas ECG Arena involves more complex, flexible multi-turn, open-ended questions. Each follow-up question is contingent on the initial question and its response, making the process more challenging and reflective of realworld applications. Since multi-turn conversations are rare in existing sources, this posed significant challenges during data curation. To address this, annotators created such conversations by referencing original clinical interpretations and ECG images. The questions are designed to feel natural and 6 simulate real clinical setting (e.g., the first question may ask about basic findings from the image, followed by question about potential clinical causes or diagnoses based on those findings). 3.2 EVALUATION METRICS Abnormality Detection: We use macro AUC, macro F1, and hamming loss (HL) for multi-label datasets, and accuracy for others. Report Generation: We employ GPT-4o as judge, evaluating reports based on rhythms, waveform, and diagnosis, with maximum score of 100 points (see evaluation prompt in Appendix Fig. A9). MMMU ECG: We use accuracy as the primary metric, with systematic, rule-based evaluation pipelines to ensure consistent scoring. ECG Arena: GPT-4o assesses model performance by comparing generated responses with ground truth answers, considering accuracy, completeness, and instruction adherence, with maximum score of 100 points (see evaluation prompt in Appendix Fig. A10). More evaluation details are provided in the Appendix F."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 METHODS FOR COMPARISON In order to evaluate the performance of our proposed model, we compare it against set of established methods including domain-specific methods and state-of-the-art MLLMs. Domain-specific Methods: We consider four domain-specific methods for ECG including three signal-based methods: METS (Li et al., 2024c), MERL (Liu et al., 2024a), ST-MEM (Na et al., 2023), and one image-based method: ECG-GPT (Khunte et al., 2024). Proprietary MLLMs: We consider three proprietary MLLMs: GPT-4o, GPT-4o mini (OpenAI, 2024), Gemini 1.5 Pro (Reid et al., 2024), and Claude 3.5 Sonnet (Anthropic, 2024). Open-source MLLMs: We select range of open-source models to ensure comprehensive coverage across different model sizes and visual components, including the general models LLaVA1.5 (Liu et al., 2024d;b), LLaVA-1.6 (Liu et al., 2024c), Phi-3-Vision Abdin et al. (2024), Idefics28B (Laurencon et al., 2024), DeepSeek-Vl-7B (Lu et al., 2024a), Mantis-8B-siglip-Llama3 (Jiang et al., 2024), MiniCPM-V-2.6 (Yao et al., 2024), InternVL2 (Chen et al., 2023; 2024) and state-ofthe-art multimodal models LLaVA-OneVision (Li et al., 2024a), Qwen2-VL (Wang et al., 2024), as well as the domain-specific models LLaVA-Med (Li et al., 2024b). 4.2 IMPLEMENTATION DETAILS We follow the architecture of LLaVA-v1.6-Vicuna-7B, which includes three core components: vision encoder, large language model, and projector to align image and text modalities. We format all datasets into chatbot-style multi-turn dialogue format and use the special token <image> to represent image features within the text data. We utilize anyres to support the models ability to recognize ECG images of various sizes that may appear in real-world scenarios. We freeze the parameters of the vision encoder and fine-tune all parameters of the projector and LLM. We use learning rate of 2e-5, set the batch size to 128, and employ cosine scheduler with 5% warm-up period for three epochs. 4.3 MAIN RESULTS We show in-domain the out-of-domain results in Table 3 and Table 4 respectively. Overall, we observe that PULSE achieves state-of-the-art performance on different datasets and tasks. Results on In-domain datasets. As shown in Table 3, PULSE demonstrates significant improvements over both proprietary and open-source MLLMs across all in-domain datasets. Specifically, PULSE surpasses the best proprietary model (GPT-4o) with 27% improvement in AUC, an 11point gain in report score, and 39% increase in accuracy on the PTB-XL Super, PTB-XL Report, and ECG-QA tasks, respectively. Moreover, PULSE achieves notable gains over the best opensource model, with 28% improvement in AUC, 12-point gain in report score, and 44% increase in accuracy on the same tasks. 7 PTB-XL Super PTB-XL Report CODE-15% AUC 50. F1 33.2 HL 50.1 Report Score Datasets Metric Random METS MERL ST-MEM ECG-GPT GPT-4o GPT-4o mini Gemini 1.5 Pro Claude 3.5 Sonnet Domain-specific Methods - 74.2 71.4 69.5 65.7 - - 53.9 - - - 20.1 Proprietary MLLMs 55.6 52.0 50.7 54.0 28.3 20.4 15.3 27.5 26.2 31.7 27.9 29. Open-source MLLMs LLaVA-Med LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.6-Vicuna-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-34B LLaVA-OneVision-7B LLaVA-OneVision-72B Deepseek-VL-Chat-7B Idefics2-8B Mantis-8B-siglip-Llama3 MiniCPM-V-2.6 Phi-3-Vision-128k-Instruct Qwen2-VL-7B Qwen2-VL-72B InternVL2-8B InternVL2-40B InternVL2-Llama3-76B PULSE-7B (Ours) 82.4 over best proprietary MLLM +27 over best open-source MLLM +28 50.0 50.0 50.0 50.0 50.0 50.2 49.8 50.6 50.9 50.7 50.6 49.0 50.0 51.3 54.0 50.6 51.2 50.4 12.3 12.3 35.2 15.8 20.1 19.9 11.4 29.6 15.7 21.9 20.4 37.7 29.6 22.4 28.3 14.3 18.7 9.4 74.8 +47 +37 28.1 28.1 48.4 29.4 38.3 36.0 34.5 50.4 27.9 31.2 30.0 63.8 48.4 30.8 30.2 27.8 34.6 35. 11.0 +15 +17 AUC 48.8 F1 15.0 HL 32.1 - - - 68.9 - - - 40.1 - - - 17.4 59.9 57.5 56.7 58.3 69.2 63.9 53.9 50.1 53.0 57.2 58.7 52.3 63.7 49.0 57.5 56.6 69.6 60.7 60.6 55.8 56.7 59. 90.7 +30 +21 24.9 22.0 20.0 20.3 27.0 19.2 13.1 1.0 3.6 12.8 17.0 7.0 27.5 17.9 17.9 25.3 22.6 24.8 23.6 16.1 16.2 20.2 85.4 +61 +58 15.7 15.1 15.9 17.8 33.4 25.3 13.6 13.6 16.6 16.6 20.6 13.1 22.4 47.9 15.7 22.0 38.8 20.5 16.1 17.7 17.4 20. 5.0 +10 +8 ECG-QA Accuracy 16.2 N/A N/A N/A N/A 35.2 14.9 33.2 34. 29.5 25.2 21.2 13.3 22.0 22.4 20.4 25.0 21.1 26.1 23.8 20.8 28.4 20.4 23.7 22.3 18.2 21.8 73.8 +39 +44 0 N/A N/A N/A 47.8 50.2 37.1 35.9 43.7 24.3 27.2 20.7 16.5 5.9 17.0 30.0 40.6 15.6 10.6 16.0 15.4 20.2 43.0 48.9 38.1 41.8 41. 61.3 +11 +12 Table 3: In-domain evaluation results. indicates results from original papers, denotes results obtained using the provided online software, N/A indicates methods not applicable or not designed for certain tasks, and - indicates unreported scores in original papers. Note that the setup of some domain-specific methods is not the same as ours, thus the results listed are for reference purposes. These results highlight the complexity of ECG image interpretation, task where even the best proprietary models perform near randomly. By fine-tuning on ECGInstruct, PULSE achieves substantial performance improvements, demonstrating the importance of high-quality and task-related instruction tuning. Moreover, while certain domain-specific methods (e.g., MERL) achieve comparable performance on specific datasets, their specialized designs limit their generalization to other diverse tasks, restricting their broader applicability in real-world, complex healthcare scenarios. Results on Out-of-domain datasets. Table 4 presents the comparison results on out-of-domain datasets, where PULSE consistently delivers outstanding performance. Notably, it achieves significant 15% improvement in accuracy on the MMMU ECG benchmark compared to GPT-4o. This substantial improvement indicates the PULSEs robustness and ability to generalize to unseen data. The ECG Arena benchmark presents significantly more challenging task for all models. This benchmark is characterized by its multi-turn, open-ended question-answering format, which closely simulates real clinical scenarios. Despite these challenges, PULSE still surpasses the best proprietary model by 2 points and outperforms the leading open-source model by an impressive 11 points in terms of arena score. These results highlight PULSEs relative strength in handling complex, clinically-oriented ECG interpretation and analysis. Additionally, the performance gap across models on this challenging benchmark indicates considerable room for future improvements in this task. CPSC 2018 CSN G12EC MMMU ECG ECG Arena F1 HL Accuracy Accuracy Accuracy Arena Score 15.1 28.8 11.6 12.1 Domain-specific Methods Datasets Metric Random METS MERL ST-MEM ECG-GPT GPT-4o GPT-4o mini Gemini-1.5-Pro Claude 3.5 Sonnet AUC 51.2 - 82.8 70.4 69.3 50.9 49.2 50.1 52.8 - - - 44.0 - - - 9.9 N/A N/A N/A N/A Proprietary MLLMs 10.6 11.0 7.4 11.5 18.2 25.5 20.5 18.9 57.5 32.1 50.5 51.5 Open-source MLLMs LLaVA-Med LLaVA-1.5-7B LLaVA-1.5-13B LLaVA-1.6-Vicuna-7B LLaVA-1.6-Vicuna-13B LLaVA-1.6-34B LLaVA-OneVision-7B LLaVA-OneVision-72B Deepseek-VL-Chat-7B Idefics2-8B Mantis-8B-siglip-Llama3 MiniCPM-2.6 Phi-3-Vision-128k-Instruct Qwen2-VL-7B Qwen2-VL-72B InternVL2-8B InternVL2-40B InternVL2-Llama3-76B PULSE-7B (Ours) 76.9 over best proprietary MLLM +24 over best open-source MLLM + 50.0 50.0 50.4 50.5 50.0 49.6 49.6 51.5 50.7 49.0 51.3 50.0 50.6 49.4 50.7 52.1 52.4 51.3 2.5 2.5 13.3 19.7 19.3 19.3 8.0 12.8 6.0 17.9 19.1 18.0 19.0 17.5 9.8 8.2 8.2 6.5 57.6 +46 +38 20.2 20.0 30.1 66.0 62.8 62.8 28.3 29.4 20.0 47.9 48.5 48.4 70.2 46.3 18.9 22.2 21.4 20.4 8.6 +10 +10 13.8 32.1 30.7 23.7 31.4 44.3 23.3 44.0 35.7 22.8 17.6 12.7 14.8 25.5 35.5 47.7 41.0 26. 85.2 +28 +38 N/A N/A N/A N/A 49.2 33.2 36.0 51.4 14.1 25.4 30.7 23.3 35.0 45.9 25.7 42.6 32.9 26.2 22.6 19.6 18.4 32.9 42.9 37.5 45.0 34.7 78.2 +27 +33 24. N/A N/A N/A N/A 43.5 39.5 40.0 42.0 27.0 33.0 35.0 28.0 38.0 31.0 26.0 35.0 34.5 36.0 38.5 34.5 31.0 31.5 35.0 30.0 30.5 38.0 58.0 +15 +20 0 N/A N/A N/A N/A 33.5 30.1 31.2 37.1 15.9 12.7 13.1 16.0 17.9 17.5 22.5 15.5 15.3 4.9 13.6 20.4 11.3 8.5 10.3 22.9 28.0 22.5 38.9 +2 +11 Table 4: Out-of-domain evaluation results. indicates results from original papers, denotes results obtained using the provided online software, N/A indicates methods not applicable or not designed for certain tasks, and - indicates unreported scores in original papers. 4.4 ABLATION STUDY Effect of Training Data Source. Given that ECGInstruct is compiled from diverse datasets, it is crucial to examine how each dataset contributes to the models overall performance. Table 5 presents comparative analysis of models trained on various dataset combinations. The model trained exclusively on PTB-XL (P) exhibits the lowest performance across all datasets, indicating the limitations of relying on single data source for effective generalization. As we progressively incorporate additional datasets into the training set, the models performance consistently improves. These results highlight the importance of curating diverse training data, as expanding beyond single source enhances the models capacity to generalize across datasets and tasks. Effect of Instruction Task. To understand the individual contribution of each ECG-related task to model performance, we analyze combinations of four instruction tasks. As shown in Table 6, adding more tasks progressively improves performance across multiple benchmarks. Models trained solely on basic feature recognition (F) performed poorly across all metrics, highlighting the limitations of single-task approach. In contrast, the sequential addition of tasks led to substantial performance gains across multiple benchmarks. The model incorporating all four tasks achieved the highest performance, indicating more comprehensive understanding of ECG images. 9 Training Data + + + + + + PTB-XL Super PTB-XL Report CSN CODE-15 ECQ-QA CPSC G12 MMMU ECG ECG Arena 68.2 74.1 74.1 74. 56.7 62.4 63.8 61.3 82.8 88.7 87.5 85.2 31.5 48.5 85.8 85. 31.8 35.8 43.4 73.8 23.4 52.4 51.0 57.6 65.4 78.8 75.5 78. 40.0 58.5 55.5 58.0 28.4 37.0 39.4 38.9 AVG -20.6 -8.6 -4. 68.1 Table 5: Performance of different training dataset combinations. P: PTB-XL, M: MIMIC-IV-ECG, C: CODE-15%, E: ECG-QA. F1 for PTB-XL Super, CODE-15%, and CPSC; Accuracy for CSN, ECG-QA, G12, and MMMU ECG; Report Scores for PTB-XL Report; Arena Scores for ECG Arena. AVG denotes the average across all metrics. Instruction Task + + + + + + PTB-XL Super PTB-XL Report CSN CODE-15 ECQ-QA CPSC G12 MMMU ECG ECG Arena 12.3 26.9 70.4 74. 36.0 54.0 57.6 61.3 56.6 83.8 85.2 85.2 11.2 73.3 82.7 85. 54.8 61.4 68.6 73.8 2.5 31.0 43.8 57.6 11.2 67.3 71.0 78. 34.0 47.5 52.5 58.0 12.4 25.3 30.4 38.9 AVG -42.5 -15.9 -5. 68.1 Table 6: Performance of different ECG-related instruction task combinations. F: basic feature recognition, R: heart rhythm analysis, M: morphology and pathology identification, C: clinical report generation. F1 for PTB-XL Super, CODE-15%, and CPSC; Accuracy for CSN, ECG-QA, G12, and MMMU ECG; Report Scores for PTB-XL Report; Arena Scores for ECG Arena. AVG denotes the average across all metrics. 4.5 CASE STUDY We further present some examples from our benchmark, comparing the outputs of our model with GPT-4o for ECG report generation (Appendix Figs. A11-A13) and ECG Arena (Appendix Fig. A14). While GPT-4o is capable of generating reports and answering questions by following instructions, it often produces responses that, although well-structured and seemingly relevant, contain significant inaccuracies in interpretation. In contrast, PULSE consistently provides more accurate responses that align closely with the ground truths. Additionally, we observed that GPT-4o tends to over-rely on its OCR capabilities when textual information (e.g., printed axis labels, numerical values like heart rate or QRS duration) is present in images, leading to superficial reasoning based on text rather than deep analysis of visual data. As shown in Appendix Fig. A13, GPT-4o identifies left axis deviation based on the printed QRS axis degree, without analyzing the visual waveform patterns. If such axis information were absent, the model would likely fail to identify the deviation. 4.6 DISCUSSION While the model demonstrates superior performance across various evaluation datasets, it faces notable challenges with more complex and open-ended tasks, such as report generation and multi-turn conversations. To further investigate the models performance in report generation, we present the score breakdown in Fig. 4. The model excels in rhythm interpretation but struggles with waveform and diagnosis identification. These results suggest that future efforts should prioritize increasing the datasets coverage of waveform and diagnosis-related cases to enhance the models ability to detect these abnormalities. Additionally, as diagnosis identification may require more advanced multi-step reasoning, future research could focus on incorporating step-wise instruction tuning data to strengthen the models reasoning capabilities. 10 Figure 4: Score breakdown of report generation performance."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we study the problem of ECG image interpretation, which is crucial task in assessing cardiac conditions. We develop new MLLM, PULSE, fine-tuned on the newly created ECGInstruct dataset with over 1 million samples across diverse range of ECG-related tasks. Evaluated on the proposed benchmark, ECGBench, our model shows state-of-the-art performance, surpassing both proprietary and open-source MLLMs across multiple in-domain and out-of-domain evaluation datasets. This work demonstrates the potential of using MLLMs for enhancing ECG image analysis and interpretation in clinical applications."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. ArXiv preprint, abs/2404.14219, 2024. URL https://arxiv.org/abs/2404.14219. Anthropic. Claude 3.5 sonnet, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Accessed: September 24, 2024. Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: BERT pre-training of image transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= p-BhZSz59o4. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. ArXiv preprint, abs/2312.14238, 2023. URL https://arxiv.org/abs/2312.14238. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. ArXiv preprint, abs/2404.16821, 2024. URL https://arxiv.org/abs/2404.16821. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. ArXiv preprint, abs/2403.04132, 2024. URL https://arxiv.org/abs/2403.04132. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=vvoWPYqZJA. B. Gow, T. Pollard, L. A. Nathanson, A. Johnson, B. Moody, C. Fernandes, N. Greenbaum, J. W. Waks, P. Eslami, T. Carbonati, A. Chaudhari, E. Herbst, D. Moukheiber, S. Berkowitz, R. Mark, and S. Horng. Mimic-iv-ecg: Diagnostic electrocardiogram matched subset, 2023. URL https: //doi.org/10.13026/4nqg-sb35. Awni Hannun, Pranav Rajpurkar, Masoumeh Haghpanahi, Geoffrey Tison, Codie Bourn, Mintu Turakhia, and Andrew Ng. Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using deep neural network. Nature medicine, 25(1):6569, 2019. Weston Hughes, Jeffrey Olgin, Robert Avram, Sean Abreau, Taylor Sittler, Kaahan Radia, Henry Hsia, Tomos Walters, Byron Lee, Joseph Gonzalez, et al. Performance of convolutional neural network and explainability technique for 12-lead electrocardiogram interpretation. JAMA cardiology, 6(11):12851295, 2021. 11 Dongfu Jiang, Xuan He, Huaye Zeng, Con Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. ArXiv preprint, abs/2405.01483, 2024. URL https: //arxiv.org/abs/2405.01483. Alistair EW Johnson, Lucas Bulgarelli, Lu Shen, Alvin Gayles, Ayad Shammout, Steven Horng, Tom Pollard, Sicheng Hao, Benjamin Moody, Brian Gow, et al. Mimic-iv, freely accessible electronic health record dataset. Scientific data, 10(1):1, 2023. Akshay Khunte, Veer Sangha, Evangelos Oikonomou, Lovedeep Dhingra, Arya Aminorroaya, Andreas Coppi, Sumukh Vasisht Shankar, Bobak Mortazavi, Deepak Bhatt, Harlan Krumholz, et al. Automated diagnostic reports from images of electrocardiograms at the pointof-care. medRxiv, 2024. Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? ArXiv preprint, abs/2405.02246, 2024. URL https://arxiv. org/abs/2405.02246. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv preprint, abs/2408.03326, 2024a. URL https://arxiv.org/abs/2408.03326. Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024b. Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. Frozen language model helps ecg zero-shot learning. In Medical Imaging with Deep Learning, pp. 402415. PMLR, 2024c. Che Liu, Zhongwei Wan, Cheng Ouyang, Anand Shah, Wenjia Bai, and Rossella Arcucci. Zero-shot ecg classification with multimodal learning and test-time clinical knowledge enhancement. ArXiv preprint, abs/2403.06659, 2024a. URL https://arxiv.org/abs/2403.06659. Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan Xu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. An open access database for evaluating the algorithms of electrocardiogram rhythm and morphology abnormality detection. Journal of Medical Imaging and Health Informatics, 8(7):13681373, 2018. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2629626306, 2024b. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Improved reasoning, ocr, and world knowledge, 2024c. URL https: Lee. Llava-next: //llava-vl.github.io/blog/2024-01-30-llava-next/. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024d. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseek-vl: Towards real-world vision-language understanding, 2024a. Ming Lu, Bowen Chen, Drew FK Williamson, Richard Chen, Melissa Zhao, Aaron Chow, Kenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, et al. multimodal generative ai copilot for human pathology. Nature, pp. 13, 2024b. Meta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/blog/meta-llama-3/. Accessed: 2024-10-01. Yeongyeon Na, Minje Park, Yunwon Tae, and Sunghoon Joo. Guiding masked representation learnIn The Twelfth International ing to capture spatio-temporal relationship of electrocardiogram. Conference on Learning Representations, 2023. Jungwoo Oh, Gyubok Lee, Seongsu Bae, Joon-myoung Kwon, and Edward Choi. Ecg-qa: comprehensive question answering dataset combined with electrocardiogram. Advances in Neural Information Processing Systems, 36, 2024. OpenAI. Gpt-4o contributions, 2024. URL https://openai.com/ gpt-4o-contributions/. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. ArXiv preprint, abs/2403.05530, 2024. URL https://arxiv.org/abs/2403.05530. Antˆonio Ribeiro, Manoel Horta Ribeiro, Gabriela MM Paixao, Derick Oliveira, Paulo Gomes, Jessica Canazart, Milton PS Ferreira, Carl Andersson, Peter Macfarlane, Wagner Meira Jr, et al. Automatic diagnosis of the 12-lead ecg using deep neural network. Nature communications, 11(1):1760, 2020. Antˆonio Ribeiro, GM Paixao, Emilly Lima, Manoel Horta Ribeiro, Marcelo Pinto Filho, Paulo Gomes, Derick Oliveira, Wagner Meira Jr, Thomas Schon, and Antonio Luiz Ribeiro. Code-15%: large scale annotated dataset of 12-lead ecgs. Zenodo, Jun, 9, 2021. Antonio Luiz Ribeiro, Gabriela MM Paixao, Paulo Gomes, Manoel Horta Ribeiro, Antonio Ribeiro, Jessica Canazart, Derick Oliveira, Milton Ferreira, Emilly Lima, Jermana Lopes de Moraes, et al. Tele-electrocardiography and bigdata: the code (clinical outcomes in digital electrocardiography) study. Journal of electrocardiology, 57:S75S78, 2019. Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. ArXiv preprint, abs/2404.18416, 2024. URL https://arxiv.org/abs/2404.18416. Veer Sangha, Bobak Mortazavi, Adrian Haimovich, Antˆonio Ribeiro, Cynthia Brandt, Daniel Jacoby, Wade Schulz, Harlan Krumholz, Antonio Luiz Ribeiro, and Rohan Khera. Automated multilabel diagnosis on electrocardiographic images and signals. Nature communications, 13(1):1583, 2022. Veer Sangha, Arash Nargesi, Lovedeep Dhingra, Akshay Khunte, Bobak Mortazavi, Antˆonio Ribeiro, Evgeniya Banina, Oluwaseun Adeola, Nadish Garg, Cynthia Brandt, et al. Detection of left ventricular systolic dysfunction from electrocardiographic images. Circulation, 148(9):765777, 2023. Kshama Kodthalu Shivashankara, Deepanshi, Afagh Mehri Shervedani, Gari Clifford, Matthew Reyna, and Reza Sameni. Ecg-image-kit: synthetic image generation toolbox to facilitate deep learning-based electrocardiogram digitization. Physiological Measurement, 2024. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. Nature, 620(7972):172180, 2023a. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. ArXiv preprint, abs/2305.09617, 2023b. URL https://arxiv. org/abs/2305.09617. Nils Strodthoff, Temesgen Mehari, Claudia Nagel, Philip Aston, Ashish Sundar, Claus Graff, Jørgen Kanters, Wilhelm Haverkamp, Olaf Dossel, Axel Loewe, et al. Ptb-xl+, comprehensive electrocardiographic feature dataset. Scientific data, 10(1):279, 2023. Patrick Wagner, Nils Strodthoff, Ralf-Dieter Bousseljot, Dieter Kreiseler, Fatima Lunze, Wojciech Samek, and Tobias Schaeffter. Ptb-xl, large publicly available electrocardiography dataset. Scientific data, 7(1):115, 2020. 13 Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Zhenwu Peng, Jie Fu, Rossella Arcucci, Huaxiu Yao, and Mi Zhang. Electrocardiogram instruction tuning for report generation. ArXiv preprint, abs/2403.04945, 2024. URL https://arxiv.org/abs/2403.04945. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. ArXiv preprint, abs/2409.12191, 2024. URL https: //arxiv.org/abs/2409.12191. Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. ArXiv preprint, abs/2308.02463, 2023. URL https://arxiv. org/abs/2308.02463. Hanwen Xu, Naoto Usuyama, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong, Zelalem Gero, Javier Gonzalez, Yu Gu, et al. whole-slide foundation model for digital pathology from real-world data. Nature, pp. 18, 2024. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. ArXiv preprint, abs/2408.01800, 2024. URL https://arxiv.org/abs/2408.01800. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Jianwei Zheng, Huimin Chu, Daniele Struppa, Jianming Zhang, Sir Magdi Yacoub, Hesham ElAskary, Anthony Chang, Louis Ehwerhemuepha, Islam Abudayyeh, Alexander Barrett, et al. Optimal multi-stage arrhythmia classification approach. Scientific reports, 10(1):2898, 2020a. Jianwei Zheng, Jianming Zhang, Sidy Danioko, Hai Yao, Hangyuan Guo, and Cyril Rakovski. 12-lead electrocardiogram database for arrhythmia research covering more than 10,000 patients. Scientific data, 7(1):48, 2020b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv preprint, abs/2304.10592, 2023. URL https://arxiv.org/abs/2304.10592."
        },
        {
            "title": "Table of Contents in Appendix",
            "content": "A Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Preliminary on 12-lead ECG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Details of ECG Image Synthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 Details of Instruction Tuning Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Details of Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A RELATED WORK",
            "content": "Domain-specific Models for ECG. Many domain-specific models have been proposed to enhance automatic ECG diagnosis (Hannun et al., 2019; Ribeiro et al., 2020; Hughes et al., 2021). For example, Ribeiro et al. (2020) applied convolutional neural networks (CNNs) to encode ECG signals for diagnosing 6 types of abnormalities. To reduce dependence on high-quality labeled data, recent studies (Li et al., 2024c; Liu et al., 2024a; Na et al., 2023) have further explored self-supervised learning approaches using unlabeled ECG training data. For example, Liu et al. (2024a) proposed an ECG representation learning framework by integrating the ECG signals and clinical reports, showing improved performance in zero-shot ECG classification tasks. Despite these successes, most approaches treat ECG data as temporal physiological signals, which could be limiting in certain resource-constrained or remote settings where only printed or digital images are available. Recently, few methods (Sangha et al., 2022; 2023; Khunte et al., 2024) have been proposed for ECG diagnosis using ECG images. For example, Khunte et al. (2024) developed diagnostic report generation framework for ECG images, which is built upon BEiT (Bao et al., 2022) vision transformer encoder and GPT-2 (Radford et al., 2019) decoder. However, their model is only capable of the clinical report generation task, without generalizability to other diverse tasks. In contrast, our study investigates the capabilities of MLLMs for ECG image interpretation. We curate diverse range of instruction tuning datasets to fine-tune the model, thus improving model generalizability. MLLMs in Healthcare Recent advancements in MLLMs have shown promising results in various healthcare domains. General medical multimodal models such as LLaVA-Med (Li et al., 2024a), MedPaLM (Singhal et al., 2023a;b), and Med-Gemini (Saab et al., 2024) have demonstrated capabilities in processing diverse medical data types. Additionally, domain-specific multimodal models have been developed for specialized fields like pathology (Lu et al., 2024b; Xu et al., 2024) and radiology (Wu et al., 2023). These models have shown potential in integrating visual and textual information to support clinical decision-making and medical education. However, despite the importance of ECG data in cardiac diagnosis and monitoring, current MLLMs often struggle to process ECG images effectively. This limitation highlights significant gap in the application of MLLMs to cardiology, where the ability to interpret both visual ECG representations and accompanying clinical information is crucial. Multimodal Instruction Tuning. Instruction tuning has proven effective in the multimodal domain, particularly in vision-language models like LLaVA (Liu et al., 2024d), MiniGPT-4 (Zhu et al., 2023) and InstructBLIP (Dai et al., 2023). These models demonstrate impressive generalizability across various visual understanding and reasoning tasks. While multimodal instruction tuning has been applied to general medical imaging tasks (Li et al., 2024b; Singhal et al., 2023a), its application to ECG images remains largely unexplored. recent work (Wan et al., 2024) introduced targeted 15 instruction tuning framework and fine-tuned existing open-source LLMs for ECG report generation. However, their approach is limited by single-task instruction dataset focused solely on report generation, potentially constraining its adaptability to other ECG-related tasks. Moreover, their work also treats ECG data as temporal signals, whereas our paper focuses on encoding ECG images with MLLMs, which is more applicable to real scenarios where only printed or digital ECG images are available. PRELIMINARY ON 12-LEAD ECG ECG is vital diagnostic tool that measures the electrical activity of the heart over time, providing insights into both spatial and temporal aspects of cardiac function. Typically, an ECG recording is presented as 12-lead multivariate time series, where each lead offers unique perspective on heart activity. The six limb leads (I, II, III, aVR, aVL, and aVF) assess the electrical movements across the arms and legs, giving views from the frontal plane. Simultaneously, the six precordial leads (V1, V2, V3, V4, V5, and V6) monitor the chest, offering horizontal plane views. In this paper, we focus on ECG images that are synthesized from raw signals."
        },
        {
            "title": "C DETAILS OF ECG IMAGE SYNTHESIS",
            "content": "We employ the ECG-image-kit (Shivashankara et al., 2024) framework to synthesize diverse ECG images from raw signal data. This toolkit allows for the generation of ECG images under various conditions by introducing range of distortions and noises to better simulate real-world clinical data. Specifically, in addition to generating standard 12-lead ECG imagescharacterized by black waveforms on white background, red grid lines, and 4x3 layoutwe introduce variety of perturbations to the images. These modifications include the addition of wrinkles and creases, simulating the physical wear and tear commonly observed in paper-printed ECGs. Our image synthesis process includes various augmentation methods to simulate physical distortions, image quality variations, and layout alterations. We introduce wrinkles and creases to mimic wear and tear commonly observed in paper-printed ECGs, and apply random rotations at varying angles to simulate misaligned scans or prints. To account for different acquisition systems and scanning qualities, we vary image resolutions and introduce random background colors, such as slight yellowing to represent aging or poor scanning quality. We also add noise to the images to simulate imperfections in the scanning or printing process. Furthermore, we experiment with different aspect ratios, overall image sizes, and ECG plot positions within the image to reflect the heterogeneity of ECG printouts across different systems and formats. In some cases (with 0.02 probability), we randomly remove grid lines to account for variations in ECG presentation. To further enrich the synthetic images, we randomly insert meta-information into the image header to simulate the annotations typically seen in clinical ECG reports. For the PTB-XL dataset, we extract patient demographics (e.g., age, gender) and basic ECG features (e.g., heart rate, axis deviations) from the associated PTB-XL feature annotation dataset, PTB-XL+ (Strodthoff et al., 2023). This extracted data is used to impute realistic meta-information, which is then randomly printed on the synthesized image. This random insertion of meta-data not only increases the visual variety of the images but also provides additional context, simulating real-world ECG prints that include patient and diagnostic information. To further increase diversity, we adopt alternative lead configurations beyond the standard 4x3 layout, such as 12x1 (single row of leads), 6x2 (two rows of six leads), and other commonly used clinical formats. These variations ensure that our model is exposed to wide range of ECG presentation styles. The augmentation process is designed to balance the dataset, with an approximate ratio of 1:1 between augmented and standard ECG images. This balance ensures that the model is exposed to both clean and distorted images, aiding in its generalization to real-world clinical scenarios."
        },
        {
            "title": "D DETAILS OF INSTRUCTION TUNING DATASETS",
            "content": "Source Dataset Task Type # Samples Basic Feature Recognition Heart Rhythm Analysis PTB-XL Close-ended QA 22,759 Open-ended QA Fill-in-blank Multi-choice QA 5,716 906 449 Close-ended QA 19,550 Open-ended QA Fill-in-blank Multi-choice QA 16,179 201 436 Morphology and Pathology Identification Close-ended QA 50, Clinical Report Open-ended QA Fill-in-blank Multi-choice QA 13,432 2,649 680 Open-ended QA 16,302 149, Basic Feature Recognition Heart Rhythm Analysis Morphology and Pathology Identification Close-ended QA 90,211 Close-ended QA 40,154 Close-ended QA 8,993 PTB-XL Total ECG-QA ECG-QA Total Basic Feature Recognition Heart Rhythm Analysis MIMIC-ECG 139,358 Close-ended QA 759 4,759 Open-ended QA Fill-in-blank 6,470 Multi-choice QA 17,186 Close-ended QA 48,625 5,262 Open-ended QA Fill-in-blank 11,487 Multi-choice QA 49,352 Morphology and Pathology Identification Close-ended QA 8,241 Open-ended QA 81,080 18,264 Fill-in-blank Multi-choice QA 61, Clinical Open-ended QA 486,746 799,687 Close-ended QA 22,177 Basic Feature Recognition Heart Rhythm Analysis Close-ended QA 13,893 Morphology and Pathology Identification Close-ended QA 31,570 67, 1,156,110 MIMIC-ECG Total CODE-15% CODE-15% Total ECGInstruct Total Table A1: Detailed data statistics of ECGInstruct. Figure A1: The Examples of basic feature recognition instructions for finetuning PULSE. 18 Figure A2: The Examples of heart rhythm analysis instructions for finetuning PULSE. 19 Figure A3: The Examples of morphology and pathological condition identification instructions for finetuning PULSE. Figure A4: The Examples of clinical reporting instructions for finetuning PULSE."
        },
        {
            "title": "E PROMPTS",
            "content": "Figure A5: The prompt used to synthesize ECG instruction tasks based on clinical reports. 22 Figure A6: The prompt used to synthesize ECG multi-turn dialogue as instruction tuning data. 23 Figure A7: The prompt used to revise (and translate) original reports. Figure A8: The prompt used to score and filter generated instruction data. 24 Figure A9: The prompt used to evaluate the generated report. 25 Figure A10: The prompt used to evaluate the ECG Arena."
        },
        {
            "title": "F DETAILS OF EVALUATION METRICS",
            "content": "Abnormality Detection. we utilize multi-label classification metrics, including Macro AUC, Macro F1, and Hamming Loss, to evaluate the datasets PTB-XL Super, CODE-15%, and CPSC 2018, where multiple correct labels may exist. For the ECG-QA, CSN, and G12EC datasets, we adopt accuracy as the evaluation metric. Report Generation. Rather than relying on traditional text generation metrics, we leverage strong LLMs as evaluators, following the approach of Zheng et al. (2024). This method provides more nuanced evaluation by focusing on key aspects of the reports. Specifically, we use GPT-4o to compare the model-generated reports against those written by cardiologists. We introduce Report Perfect Score, which is based on three critical components of generated report: (1) Rhythms (0 to 10 points), (2) Waveform Morphology (0 to 10 points), and (3) Diagnosis (0 to 10 points). The final score is the average of these three components, scaled to maximum of 100 points. The prompt used to query GPT-4o for evaluating the report score is provided in Appendix Fig. A9. MMMU ECG. We adopt accuracy as the primary metric. We have designed systematic, rule-based evaluation pipelines to ensure robust and consistent scoring. To mitigate the potential influence of any intermediate generations (e.g., reasoning steps) in long responses, we employ robust regular expressions and develop response-processing workflows. These are used to extract answer options from the long responses for accurate answer matching. In cases where no valid answer can be extracted from the models response, we perform random selection to assign score. ECG Arena. We also employ strong judge model, GPT-4o, to assess model performance by comparing generated responses with ground truth answers. The evaluation considers three perspectives, each scored on scale of 0-10: Accuracy (how closely the models response matches the ground truth), Completeness (whether the model provides comprehensive answer covering all aspects of ECG interpretation), and Instruction Adherence (how well the model follows the specific instructions in the question). We calculate the final score by averaging these three aspects and scaling to maximum of 100 points. The specific prompt used for GPT-4 evaluation is provided in Appendix Fig. A10."
        },
        {
            "title": "G CASE STUDY",
            "content": "Figure A11: Comparison of model outputs on ECG report generation task (Example 1). Blue indicates correct information, while red highlights errors. Our models output fully aligns with the ground truth, with report score of 10. In comparison, GPT-4s report, though structurally sound, contains notable inaccuracies despite its initial appearance of relevance. 28 Figure A12: Comparison of model outputs on ECG report generation task (Example 2). Blue indicates correct information, while red highlights errors. Our models output mostly aligns with the ground truth, achieving report score of 83.3. In comparison, GPT-4s output correctly identifies only the ECG rhythm, omitting most other key details. 29 Figure A13: Comparison of model outputs on ECG report generation task (Example 3). Blue indicates correct information, while red highlights errors. Our models output mostly aligns with the ground truth report, achieving report score of 73. In comparison, GPT-4s output partially aligns with the ground truth report. Figure A14: Comparison of model outputs on ECG Arena (Example 1). Blue indicates correct information, while red highlights errors. Given the challenging nature of this task, our models output partially aligns with the ground truth, and GPT-4os output largely deviates from the reference."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "The Ohio State University"
    ]
}