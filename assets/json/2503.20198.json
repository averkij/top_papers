{
    "paper_title": "Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models",
    "authors": [
        "Alex Jinpeng Wang",
        "Linjie Li",
        "Zhengyuan Yang",
        "Lijuan Wang",
        "Min Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains a major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing a critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as a critical bottleneck in text generating quality. To address this, we introduce a novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop \\ModelName, a multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that \\ModelName~significantly outperforms SD3.5 Large~\\cite{sd3} and GPT4o~\\cite{gpt4o} with DALL-E 3~\\cite{dalle3} in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, \\ModelName~opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing a new frontier in long-text image generating."
        },
        {
            "title": "Start",
            "content": "Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models Alex Jinpeng Wang1 Linjie Li2 Zhengyuan Yang2 Lijuan Wang2 Min Li1 1Central South University 2Microsoft https://fingerrec.github.io/longtextar 5 2 0 2 6 2 ] . [ 1 8 9 1 0 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in autoregressive and diffusion models have led to strong performance in image generation with short scene text words. However, generating coherent, long-form text in images, such as paragraphs in slides or documents, remains major challenge for current generative models. We present the first work specifically focused on long text image generation, addressing critical gap in existing text-to-image systems that typically handle only brief phrases or single sentences. Through comprehensive analysis of state-of-the-art autoregressive generation models, we identify the image tokenizer as critical bottleneck in text generating quality. To address this, we introduce novel text-focused, binary tokenizer optimized for capturing detailed scene text features. Leveraging our tokenizer, we develop LongTextAR, multimodal autoregressive model that excels in generating high-quality long-text images with unprecedented fidelity. Our model offers robust controllability, enabling customization of text properties such as font style, size, color, and alignment. Extensive experiments demonstrate that LongTextAR significantly outperforms SD3.5 Large [10] and GPT4o [30] with DALLE 3 [3] in generating long text accurately, consistently, and flexibly. Beyond its technical achievements, LongTextAR opens up exciting opportunities for innovative applications like interleaved document and PowerPoint generation, establishing new frontier in long-text image generating. 1. Introduction Text is an integral part of our visual environment, appearing in diverse formats such as logs, banners, book covers, and newspapers. In recent years, there has been growing interest in rendering text as images and processing it in pixel space, leading to significant advancements in both naturallanguage processing (NLP) [11, 19, 32, 37] and multimodality model [15, 41, 44]. However, generating images that contain accurate, aesthetically pleasing, and contextuFigure 1. Breaking the Limits: Long-Text Image Generation Remains Elusive for Existing Models. State-of-the-art text rendering models, such as Text Diffusion 2 [4] and AnyText [42], perform well on short text but struggle with longer passages. Large diffusion models like Stable Diffusion 3.5 Large [10] can handle longer text but exhibit lower accuracy. The text recognition on generated images was conducted using Qwen2-VL [46] model. For this evaluation, we sampled 140 examples from the interleaved Obelics [18] dataset with truncation. ally coherent text is challenging task. Existing approaches in visual text generation have progressed with methods leveraging large language models as text encoders [6, 23, 51] and fine-grained guidance, such as layout control [42]. GlyphControl [51] enables positioning through specified guidelines, while TextDiffuser [6] employs character-level segmentation masks. More recent advances, like TextDiffuser2 [4], use language models to reduce manual prompt engineering. Despite these improvements, existing methods face shared key limitation: Limited capability to render long text, typically restricted to single short sentence, as shown in Figure 1. The rise of large diffusion models [3, 10] has begun to address this limitation. As illustrated in Figure 1, these models have expanded their capacity to handle longer textual prompts up to tens of words, but they still face fundamental limitation: the context window. While diffusion models like SD-XL3 [10], MidJourney [29], and FLUX [17] have made impressive strides, they support relatively short context lengths, typically only up to 77 tokens in the case of SD3.5 Large [10]. This restriction makes them less suited for handling more extensive real-world text, where the accuracy of image generation deteriorates significantly as the length of the input increases. Despite advancements in document-level text perception [8, 25, 30, 46], where image-to-text models excel, the reverse task of generating high-quality images from long text descriptions remains underdeveloped, highlighting significant gap. In contrast, autoregressive (AR) models based on large language models (LLMs), such as LlamaGen [34] and Chameleon [38], offer promising alternative. These AR models can process much longer sequences of textup to 4096 tokens in Chameleon [38] and 2048 tokens in LlamaGen [34]while achieving comparable text-to-image generation performance to diffusion models. However, AR models still struggle with precise text rendering, particularly in the context of complex or lengthy text inputs [38], highlighting the need for further improvement. Given these observations, we analyze the Vector Quantization representation in AR models, identifying codebook embedding limitations as key bottleneck for text rendering. To address this, we develop TextBinarizer, specialized text-focused tokenizer that delivers clearer, more consistent reconstructions for complex documents with long text. We present LongTextAR, multi-modality AR model designed for advanced text rendering that generates highquality text images across various styles. Our experiments show that LongTextAR produces clearly readable text and outperforms state-of-the-art models including SD3.5 Large [10] and GPT-4o [30] with Dall-E3 [3]. LongTextAR excels in controllable text rendering, supporting customizations in font type, size, color, and alignment. Unlike SD3.5 Large and DallE3, which often struggle with specific instructions, LongTextAR consistently produces accurate results under various constraints. We also co-trained LongTextAR on both synthetic long-text image rendering and natural image generation tasks, finding that our model performs well on natural image generation despite its focus on synthetic text image datademonstrating the versatility of our approach. The main contribution includes: i. First Long-Text Image Generation Model: We present the first model specifically designed for long-text image generation, addressing significant gap in existing text-to-image methods that typIdentifying Tokically handle only short sentences. enization Bottlenecks: We pinpoint weak tokenization as critical barrier for effective text rendering in existing multimodal autoregressive models, such as Chameleon. iii. Developing an Text-focus Tokenizer: We design binary tokenizer tailored for text rendering, achieving significantly ii. better reconstruction quality on complex documents comiv. Versatile Text pared to traditional VQ-based models. Rendering: LongTextAR offers customizable text rendering with control over font attributes while generalizing to natural image generation through co-training. Our experiments demonstrate its potential for applications like document generation and PowerPoint editing. 2. Related Works Visual Text Rendering. Text is omnipresent in daily life, appearing in posters, TV, road signs, and more. Given its visual nature, it is natural to treat text processing as an imagebased task [19, 41]. PTP [11] and PIXEL [32] have explored this approach in Natural Language Processing. In computer vision, some works [5, 6, 22, 28, 42, 51] try to render text into image due to enhanced rendering could significantly benefit text-centric design tasks like logos and posters. Along this line, traditional methods [4, 6, 28] use layout guides. Concurrent work DnD Transformer [7] focusing on unconditioned long-text image generation and works well on longer text data like PDF. However, these approaches struggle with rendering longer text, limiting diversity and precision. Large diffusion models like SDXL3 [10] and DALLE 3 [3] can render longer text but lack font control and support limited token lengths. In contrast, we focuses on LLM-based Autregressive (AR) models that handle much longer sequences and introduce controllable font variations, enhancing diversity for text-centric designs. Autoregressive Image Generation. Autoregressive image generation models [2, 26, 35] models has emerged as powerful alternative to diffusion-based methods [36, 38], leveraging sequential token-based approaches. It relies on vector quantization [1, 39, 55] to convert images into discrete tokens. Early works [31, 53] introduced two-stage pipeline: tokenize images using VQ-VAE [43] or VQGAN [9], then model the token sequences with an autoregressive transformer. LlamaGen [34] improved tokenization and adopted advanced language modeling, narrowing the gap with diffusion models. Furthermore, models like Chameleon [38], SeedX [13], Show-O [49], and LuminamGPT [21] unified text and image sequences, fine-tuning on text-to-image pairs. In this work, we focus on integrating our text-focused tokenizer into an AR framework. By leveraging our diverse, text-rich dataset, we aim to advance text-image generation, delivering more accurate and stylistically varied outputs conditioned on long, detailed prompts. Visual Tokenization in Multi-Modal Autoregressive Models. The first step in multi-modal autoregressive modeling is converting images into discrete token IDs. The vector quantization (VQ) is introduced into VQ-VAE [43] to learn discrete representations of images. VQ-GAN [9] use larger resolution and propose new train targets. EMU [35] and EMU2 [36] use Casual Transformer to get 1D causal sequences. SEED [12] propose discrete image tokenizer with 1D causal dependency. MAR [20] propose to conduct diffusion autoregressive without VQ. Most recent models like ViT-VQGA [52], Chameleon [38], Show-O [49] and LlamaGen [34] leverage the same tokenizer architecture from VQ-GAN [9]. However, critical factor in VQ is the size of the codebook, VQ models often use relatively small codebooks [3, 9]. MagVit-V2 [27, 54] and Infinity [14] fixes this flaw by employing binary quantization with an expanded vocabulary. Different from these advancements, our work is the first to specifically address long text rendering, pushing the boundaries of whats possible in this domain while introducing specialized text-focused tokenizer. 3. Advancing Tokenization for Text-Rich Images Generation Although Autoregressive (AR) models boast longer context windows than large diffusion models, they falter when generating images with dense textual content. This section uncovers the fundamental bottleneck in existing tokenization strategies and introduces TextBinarizer, novel approach that significantly enhances text preservation. Figure 2. TextBinarizer implementation details. This approach allows for direct quantization. 3.2. TextBinarizer: Text-Centric Tokenization Beyond Vector Quantization: Bitwise Approach. TextBinarizer introduces novel bitwise quantization scheme that encodes high-dimensional embeddings into binary tokens, surpassing traditional VQ methods in preserving textual details. Inspired by recent works [14, 54, 56] demonstrating that binary representations excel at capturing fine-grained features with remarkable fidelity, we adopt binary codebook approach specifically optimized for preserving detailed textual characteristics. This choice enables our model to maintain high-quality text rendering while remaining computationally efficient. This architecture is illustrated in Figure 2. For an embedding vector Rlog2 K, quantization occurs per dimension: q(xk) = sign(xk) = 1{xk 0} + 1{xk > 0}. (1) 3.1. Tokenization Challenges in Text Rendering The final token index is computed as: AR multimodal models require text and images to be converted into unified sequence of discrete tokens before generation. The dominant paradigm, VQ-based tokenization [21, 34, 38], constructs shared vocabulary by incorporating both subword text tokens (e.g., BPE [16]) and visual tokens from VQ codebook. While this framework excels in generating natural images, it exhibits catastrophic failures when encoding fine-grained text details in images [9]. Our investigation in Section 5.2 demonstrates that these limitations persist even with large-scale training data, revealing the tokenization process itself as fundamental bottleneck. Through extensive analysis of state-of-the-art VQ tokenizers from Taming Transformer [9] and Chameleon [38], we identify inherent limitations in text rendering quality. The VQ architecture prioritizes global image structure over fine-grained character details, utilizing small codebooks that inadequately capture typographic variations. These architectural constraints lead to poor text reconstruction, which further degrades when generating text-heavy images through AR models, resulting in blurry or illegible output. To overcome these challenges, we introduce TextBinarizer, specialized tokenization approach that preserves textual fidelity in image generation. Index(x) = log2 (cid:88) k=1 2k11{xk > 0}. (2) Then we introducing projector to align the features from cnn feature. During training, the feature is 50% undergoing VQ quantization while the other is raw cnn feature. zTB = TextBinarizer-Quantizer(P (VQ-Quantizer(f ))) (3) where is lightweight transformer projector that aligns CNN feature maps to fine-grained binary representations . To accelerate training, we initialize both the CNN Encoder and Decoder from pre-trained TamingTransformer [9] model and freeze these components along with the VQ quantization during training. Following [27, 54, 56], the training employs reconstruction, adversarial, perceptual, and commitment losses, plus an entropy penalty. Our novel implementation offers two key advantages: i. Fine-grained text feature preservation: The binary quantization scheme captures detailed typographic characterisii. Natural tics essential for high-quality text rendering. image knowledge transfer: By leveraging pre-trained VQGAN weights and maintaining their frozen state, we retain the models capability to handle natural images while specializing in text-focused content. During inference, TextBinarizer operates in two modes: decoder-only for natural image generation, and full encoder-decoder for text image synthesis, providing flexibility across different use cases. 4. LongTextAR the Long Text AutoregresIn this section, we unveil sive Generation Model (LongTextAR), the first framework specifically designed for generating high-quality images with extensive textual content. Through novel architecture optimized for text rendering and comprehensive training on diverse text-rich datasets, LongTextAR achieves unprecedented capabilities in long-form text image generation, significantly outperforming existing models that are constrained to short phrases or single sentences. Figure 3. The main pipeline of LongTextAR. Our trained textfocused tokenizer converts the long-text image into discrete token IDs. corresponding long-text prompt is generated, and the model is then tasked with predicting the image token IDs based on this long text prompt. 4.2. Elevating Text-Image Synthesis 4.1. Architecture Overview of LongTextAR We employ masked token prediction objective: LongTextAR introduces streamlined architecture that excels at text-to-image generation through two key components: specialized text-focused vision tokenizer and powerful autoregressive decoder based on Llama2 [40]. As illustrated in Figure 3, our model efficiently processes long text prompts and transforms them into high-quality images with dense textual content, leveraging the ability of TextBinarizer to extract and preserve fine-grained textual features. Tokenizer Integration. To seamlessly integrate TextBinarizer into LongTextARwhile maintaining optimal text processing capabilities, we implement hybrid tokenization strategy. Building upon established practices in multimodal autoregressive models [21, 38, 49], we employ BPE tokenizer with vocabulary size of 65,536. The first tokens are dynamically allocated to quantized representations of TextBinarizer for visual modality, while preserving the remaining slots for textual tokens. This hybrid approach require embedding layer that combines both visual and textual representations: = [ETB, Etext], where ETB represents learned embeddings of TextBinarizer for visual tokens, Etext encompasses the embeddings for textual tokens, and [, ] denotes the concatenation operation to form unified embedding space. To maintain the language modeling capabilities of the pre-trained AR language model, we restrict parameter updates to visual token embeddings (ETB) and prevent gradient propagation to text token weights in lm head layer. This preserves the original language understanding of LAR model while convergencing fast. Lmask = (cid:88) iM log (titcontext) (4) where Lmask is the loss, denotes masked positions, ti is the target text image token, and tcontext provides contextual cues include text and prompt. All the text image tokens are masked during training. For example, the tcontext with text variable is: Generate text image with [Variables] using the following text: [T ]. The predicted fixed-length tokens, tpredicted, are then decoded into image via TextBinarizer: xoutput = TextBinarizer-Decoder(tpredicted) (5) This pipeline, depicted in Figure 3, empowers LongTextAR to fluidly synthesize text and image tokens, delivering unparalleled quality in long-text image generation. 5. Experiments 5.1. Datasets and Implementation Details. For training the TextBinarizer tokenizer, at first we construct 11m training dataset consists of PDF data, document data, and generated text image. For auto-regressive language model generation, we also include multiple image sources. Mainly subsets of RenderedText [47], Marion10M [6], Laion-coco [33] and AnyWords3M [42]. We also include the 2 million CleanTextSynth subset from TextAtlas5M [45]. 10% randomly selected data used for test and the data details are given in supplementary material. Implementation Details. For TextBinarizer training, we follow the tokenizer training setting and hyperparam-"
        },
        {
            "title": "Embed Dim Codebook Size",
            "content": "FID PSNR SSIM Utilization VQ Taming-Transformer VQ [9] Chameleon VQ [38] TextBinarizer TextBinarizer 512 512 512 512 512 8192 8192 8192 8192 33.52 39.74 34.63 27.47 24.38 29.13 27.33 29.65 33.99 30.57 0.85 8.82 0.86 0.93 0.88 0.84 0.93 0.82 0.73 0.17 Table 1. TextBinarizer clearly outperform Vector Quantization on complex long-text dataset reconstruction. Figure 4. Tokenizer reconstruction comparison on data with long-text. Comparing with well-trained VQ tokenizer from Chameleon [38], our text-focus tokenizer leads to better reconstruction result on detail generation for letters. eters in [27] and [54], unless stated otherwise. TextBinarizer is used, which eliminates the codebook embedding, the default codebook size is = 213. For autoregressive Language Model Training, our training process uses the AdamW [24] optimizer, with β1 sets to 0.9 and β2 to 0.95, with an ϵ = 105. We use linear warm-up of 4000 steps with an exponential decay schedule of the learning rate to 0. Additionally, we apply weight decay of 0.1 and global gradient clipping at threshold of 1.0. We use dropout of 0.1 for training stability. 5.2. Tokenizer Impact on Reconstruction In this section, we evaluate our model on the test split of the tokenizer training data, which includes four types of data: (1) Text Image, (2) Rendered Text Image, (3) PPT Document, and (4) Interleaved Web Document. We train three models on the same dataset: 1. VQ Model (codebook size 8192): Implementation from Taming Transformer [9] and Chameleon [38]. 2. TextBinarizer Model (codebook size 8192):. 3. TextBinarizer Model (codebook size 65536):. We compare the reconstruction performance of our TextBinarizer-based text tokenizer model against the well-established VQ tokenizer from VQ-GAN [9] and Chameleon [38]. Chameleon VQ is trained on large-scale data. The quantitative results are shown in Table 1. We observe that the TextBinarizer model demonstrates clear improvement over the VQ model in terms of FID score, indicating better reconstruction quality. When we trained the VQ model on our custom data, we saw further FID improvements; however, TextBinarizer models consistently outperformed VQ models, especially as the codebook size increased. Notably, TextBinarizer-18 showed lower utilization than TextBinarizer-13, suggesting that text images do not require very large codebook, and dimension of 8192 is sufficient for effective reconstruction. In addition to quantitative analysis, we also visually compare reconstruction quality across tokenizers in Figure 4. Our tokenizer produces noticeably better reconstructions than the Chameleon VQ-GAN when reconstructing low-resolution text images and more complex, interleaved documents. After processing these images at resolution of 384 384, our TextBinarizer model achieves clearer and more accurate reconstructions, highlighting the advantages of TextBinarizer over traditional VQ approaches for textbased visual tasks."
        },
        {
            "title": "Metrics",
            "content": "TextDiffuser [6] AnyText [42] TextDiffuser-2 [4] SD3.5 Large [10]"
        },
        {
            "title": "Short Long",
            "content": "CLIPScore OCR (Accuracy) OCR (F-measure) 31.7 72.4 77.6 30.6 38.7 50.3 32.5 69.5 75.49 30.3 34.5 51.3 31.1 71.3 76. 29.8 33.0 52.4 32.2 73.2 78.4 30.9 52.3 63.5 33.3 82.7 85.3 31.4 69.5 70.3 Table 2. Demonstration of the quantitative results for long-text rendering. We split the test set into words Short (less than 10) and Long (more than 10), more details are in supplementary material. We use the Qwen2-VL [46] model to recognize text from generated image. For TextDiffuser2 model, we use prompt text image with all other texts as keywords. The best and second-best results are indicated in bold and underlined formats. LongTextAR achieves the best results for all metrics especially for long text."
        },
        {
            "title": "Data",
            "content": "Llama2 [40] Chameleon [38] FID(S) FID(N) FID(S) FID(N)"
        },
        {
            "title": "Synthehic Data\nNatural Image\nBoth",
            "content": "28.7 69.2 28.5 73.5 82.5 62.1 27.3 63.4 28.1 45.6 30.2 32.4 Table 3. Comparative Analysis of Co-training with Synthetic and Natural Data and Weight Initialization Strategies. 5.4. Comparison with Existing Models In this section, we evaluate long text generation on the synthetic level-1 test dataset and TextAtlasEval [45]. We compare with previous state-of-the-art methods AnyText [42], TextDiffuser-2 [4], AnyText [42], SD3.5 Large [10], Grok3 [48], Infinity [14] and GPT4o [30] + DALL-E 3 [3]. To provide more aspect for comparison, we split the dataset into short and long subsets according to the words count less than 10 or not. The results are presented in Tab. 2, and our key observations are as follows: i. Previous state-of-the-art text rendering models, such as TextDiffuse2 [4] and AnyText [42], struggle to fully generate long text. The visualizations are given in Figure 6. We observe that these models often produce only short text snippets and, in our experiments, fail with text containing more than ten words, consistent with findings in [4]. For example, the OCR accuracy decrease from 79.5 to only 33.7. ii. Stable Diffusion 3.5 Large [10] demonstrate clearer better text rendering than traditional layout-dependent methods. However, as text length increases, rendering quality significantly degrades. We observe issues such as duplicated words, inconsistencies in logic, and the generation iii. Deof irrelevant words in the latter parts of the text. spite having only 7 billion parameters, our model demonstrates superior text generation capabilities, outperforming the 8.1B SD3.5 Large in generating coherent long text. Figure 5. Controllable experiment, we modify the text font type, text color and text rotation degree, also the alignment way. 5.3. Controllable Long Text Rendering In this experiment, we demonstrate the ability of our model to generate text images with diverse formatting controls. We specify attributes such as font type, font size, rotation angle, alignment, and font color. Figure 5 presents the results. Here, we showcase variations in rotation angle, font type, font size, and font color, illustrating the models adaptability to complex formatting requirements. Surprisingly, after exposure to over 8,000 font types during pre-training, our model can generate text in variety of commonly used fonts with high fidelity. For example, in the final column, we present dense text rendered with an different font. Our observations indicate that the majority of words are rendered accurately and are easily recognizable. Additionally, the text is automatically wrapped into different rows for better readability. More controllable experiments are reported in the supplementary. These evidences shows LongTextAR can control text rendering well with common used variations. Figure 6. Text-conditioned long-text image generation comparison. The Stable Diffusion3.5 Large [10] and GPT-4o [30]+Dall-E3 [3] using the prompt Generate white-background text image and the text is: [Text Prompt]. The text is clear and large. For TextDiffuser 2 [4] we use the prompt text image and input other text as tags. We use the Qwen2-VL [46] to recognize words from generated images and compute the accuracy according to the ground-truth text [Text Prompt]. The image generation capabilities of GPT4o [30], released at the end of March 2025, have shown huge gap over all other models, both open-source and closed-source. Encoder VQ Quant Projector Quant Layer 2 Decoder FID Model Frozen Model ConvNet(256) ConvNet(256) ConvNet(256) ConvNet(256) ConvNet(256) ConvNet(13) VQ VQ VQ VQ - - Frozen - - Model Single Layer FC Single Layer FC 3-Layer Transformer 3-Layer Transformer 3-Layer Transformer Single Layer FC Frozen Model Frozen Model - - TextBinarizer-13 TextBinarizer-13 TextBinarizer-18 TextBinarizer-13 TextBinarizer-13 DeConvNet(256) DeConvNet(13) DeConvNet(13) DeConvNet(18) DeConvNet(13) DeConvNet(13) Frozen 98.44 79.92 62.35 58.43 39.36 42.68 Table 4. Training quantization and decoder experiments. The VQ-GAN weights are initialized from the Chameleon [38] model, and each model is trained for 4 epochs. Frozen components are indicated with . The last two rows represent the upper bounds. Figure 7. Natural image text rendering examples. 5.5. Ablation Study 5.5.1. Exploring Reuse of Pre-trained Tokenizers We investigate various strategies for leveraging pre-trained tokenizers. Specifically, we hybrid training of VQ-GAN and TextBinarizer-GAN. To ensure fair comparison, all models are trained with 4 epochs and same hyperparameters. We conduct five key experiments to assess the effectiveness of different configurations: i. Pre-trained VQ Model (Baseline): We use the pre-trained VQ model without any fine-tuning and evaluate it directly on the validaii. Unfreezing Key Components: We fine-tune tion set. the model by unfreezing the projector, TextBinarizer quantization, and decoder modules. iii. Increasing Model Capacity: We replace the single-layer projector with 3-layer transformer to increase model capacity. iv. Larger Token Embeddings: The token embedding size is increased from 13 to 18 dimensions. v. Comparison with TextBinarizerGAN and VQGAN Encoder: We include TextBinarizerGAN with 213 codebook size and the VQGAN-Encoder for additional comparison. The experimental results are summarized in Table 4, i. Baseline and the following key insights are derived: Performance: The original Chameleon VQ-GAN struggles on complex validation sets, demonstrating its limitations ii. Unfreezing in handling intricate data distributions. Key Components: By unfreezing the projector, TextBinarizer quantization, and decoder, the model significantly improves, reducing the FID score to 79.92. iii. Enhanced Projector: Replacing the single-layer projector with 3-layer transformer yields clear improvement, lowering the FID from 79.92 to 62.35. This highlights the importance of increasing model capacity for better tokenization. iv. Larger Token Embeddings: Increasing the TextBinarizer quantization codebook size further improves FID, indicating that larger embedding space allows the model to capture more nuanced information. v. Unfreezing the CNN Encoder: When the CNN encoder is unfrozen, and vector quantization is removed, the model achieves even better performance than the TextBinarizer-GAN. This result underscores the importance of co-training the CNN encoder for optimal quantization model learning. These findings demonstrate that careful adjustments to tokenization strategies and co-training of key components can lead to substantial performance improvements, emphasizing the critical role of tokenizers in multi-modal learning. 5.5.2. Co-Training with Natural and Synthetic Data In this section, we investigate the co-training of natural images alongside generated text images. To differentiate text images from natural images, we add placeholder, [Natural Image], in the prompts. Results of this comparison are presented in Table 3. We observe that the FID score for natural images remains stable at approximately consistent during pre-training, indicating that our model maintains high quality in generating realistic natural images. As shown in Figure 7, the generated natural images retain natural appearance. This demonstrates the effectiveness of our co-training approach in preserving image quality across both synthetic and natural domains. 5.5.3. Impact of Pre-trained Weights on Generation In this section, we investigate the influence of pre-trained weights on the generation capabilities of the proposed model. Specifically, we conduct comparative analysis using weights derived from LLaMA2 [40] and those adapted from Chameleon [38], the latter having been pre-trained on an extensive corpus of vision-language data as an initialization strategy. The results, presented in Table 3, indicate that the Chameleon-derived weights exhibit minimal impact on text rendering performance, while significantly affecting the generation of natural images. These findings demonstrate text rendering does not necessitate strong multithat modal perception from the foundation model. Based on these findings, we adopt the Chameleon pre-trained weights as the default initialization for the LLama2 model. 5.6. Potential Applications We leverage the AutoSlideGen [50] dataset, which consists of 5,000 PowerPoint slides, to explore the capabilities of LongTextAR. Using PyMuPDF 1, we generate text annotations with bounding boxes and perform image captioning with the Qwen2-VL [46] model for images in powerpoint. LongTextAR is fine-tuned on this reorganized dataset. The results, shown in Figure 8, reveal that LongTextAR excels at following positional instructions, rendering 1https://github.com/pymupdf/PyMuPDF Figure 8. LongTextAR generates interleaved PowerPoint data from long-text prompts. The model accurately produces layouts and renders both text and images effectively. Bounding boxes from the text input are shown as reference. text in various formats, and generating the corresponding images. These capabilities suggest several exciting applications, such as PowerPoint editing and automated document generation. While the visual quality is still developing, the promising resultsgiven the scale of the dataindicate that this approach is practical and valuable direction for future work. In the supplementary material, we also demonstrate that current state-of-the-art models, such as SD3.5 Large and GPT-4o with DALL-E 3, lack this capability. 6. Conclusion and Future Directions In this work, we identify the weak tokenizer as key bottleneck in autoregressive language models. To address this, we introduce specialized tokenizer optimized for text rendering. Building on this, we present LongTextAR, an autoregressive transformer that, for the first time, specifically focuses on and excels at rendering long text, outperforming both traditional layout-based models and advanced large diffusion and multi-modal models. Through comprehensive evaluations, we demonstrate robust rendering capabilities of LongTextAR, particularly in generating dense text image, surpassing existing models in clarity and precision. We also explore controllable text rendering, mainly challenging font variations, highlighting the flexibility of our approach. Our method opens up promising applications like powerpoint editing and interleaved document generation. However, the seamless integration of rendered text within natural images remains challenging area, which we leave open for further exploration."
        },
        {
            "title": "References",
            "content": "[1] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2286122872, 2024. 2 [2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. 2 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 1, 2, 3, 6, 7 [4] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser-2: Unleashing the power arXiv preprint of language models for text rendering. arXiv:2311.16465, 2023. 1, 2, 6, 7 [5] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-backslash alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [6] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36, 2024. 1, 2, 4, 6 [7] Liang Chen, Sinan Tan, Zefan Cai, Weichu Xie, Haozhe Zhao, Yichi Zhang, Junyang Lin, Jinze Bai, Tianyu Liu, and Baobao Chang. spark of vision-language intelligence: 2-dimensional autoregressive transformer for efficient finegrained image generation. arXiv preprint arXiv:2410.01912, 2024. 2 [8] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 2 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2, 3, 5 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 6, 7, 3 [11] Tianyu Gao, Zirui Wang, Adithya Bhaskar, and Danqi Chen. Improving language understanding from screenshots. arXiv preprint arXiv:2402.14073, 2024. 1, 2 [12] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. [13] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. 2 [14] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. 3, 6 [15] Anwen Hu, Haiyang Xu, Liang Zhang, Jiabo Ye, Ming Yan, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl2: High-resolution compressing for ocrarXiv preprint free multi-page document understanding. arXiv:2409.03420, 2024. 1 [16] Kudo. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018. 3 [17] Black Forest Labs. Flux: multimodal model for text-toimage generation, 2023. Accessed: 2024-11-15. [18] Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024. 1 [19] Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual lanIn International Conference on Maguage understanding. chine Learning, pages 1889318912. PMLR, 2023. 1, 2 [20] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 3 [21] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation arXiv preprint with multimodal generative pretraining. arXiv:2408.02657, 2024. 2, 3, 4, 1 [22] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware arXiv preprint models improve visual arXiv:2212.10562, 2022. 2 text rendering. [23] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware arXiv preprint models improve visual arXiv:2212.10562, 2022. 1 text rendering. [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 5 [25] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world visionlanguage understanding. arXiv preprint arXiv:2403.05525, 2024. 2 [26] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. 2 [27] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 3, [28] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation. arXiv preprint arXiv:2303.17870, 2023. 2 [29] MidJourney. Midjourney ai: Text-to-image generation, 2023. Accessed: 2024-11-15. 2 [30] OpenAI. Hello gpt-4o, 2024. Accessed: 2024-09-09. 1, 2, 6, 7, 3 [31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 [32] Phillip Rust, Jonas Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de Lhoneux, and Desmond ElarXiv preprint liott. Language modelling with pixels. arXiv:2207.06991, 2022. 1, 2 [33] Christoph Schuhmann, Andreas Kopf, Richard Vencu, Theo Coombes, Benjamin Trom, and Romain Beaumont. LAION COCO: 600M SYNTHETIC CAPTIONS FROM LAION2B-EN, 2022. Accessed: 2024-11-07. [34] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 3 [35] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. In The Twelfth International Conference on Learning Representations, 2023. 2, 3 [36] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. 2, 3 [37] Yintao Tai, Xiyang Liao, Alessandro Suglia, and Antonio Vergari. Pixar: Auto-regressive language modeling in pixel space. arXiv preprint arXiv:2401.03321, 2024. 1 [38] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 3, 4, 5, 6, 7, 8, 1 [39] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 4, 6, 8 [41] Michael Tschannen, Basil Mustafa, and Neil Houlsby. Image-and-language understanding from pixels Clippo: only. In CVPR, pages 1100611017, 2023. 1, 2 [42] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. 1, 2, 4, 6 [43] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [44] Alex Jinpeng Wang, Linjie Li, Yiqi Lin, Min Li, Lijuan Wang, and Mike Zheng Shou. Leveraging visual tokens for extended text contexts in multi-modal learning. Advances in Neural Information Processing Systems, 2024. 1 [45] Alex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie Li, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, et al. Textatlas5m: largescale dataset for dense text image generation. arXiv preprint arXiv:2502.07870, 2025. 4, 6 [46] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2, 6, 7, 8 [47] C. Wendler. RenderedText dataset. https : / / huggingface . co / datasets / wendlerc / RenderedText. Accessed: 2024-11-02. 4 [48] xAI. Grok 3, 2025. Accessed: 2025-03-06. 6 [49] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2, 3, [50] Sheng Xu and Xiaojun Wan. Automatic slides generation for scholarly papers: fine-grained dataset and baselines (student abstract). In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1309313094, 2022. 8 [51] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation. Advances in Neural Information Processing Systems, 36, 2024. 1, 2 [52] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3 [53] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. 2 [54] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 3, 5 [55] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. 2 [56] Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. 3 Beyond Words: Advancing Long-Text Image Generation via Multimodal Autoregressive Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Comparison with Baseline Models Lacking Text-Focused Training Our method builds upon the pretrained weights of Chameleon [38]. To evaluate the impact of text-focused training, we compare results using the baseline weights without additional fine-tuning. However, since the multimodal Chameleon [38] model does not provide an image generation API, we instead use Lumina-mGPT [21], an improved fine-tuned version of Chameleon optimized for high-quality image generation. The comparative results are shown in Figure 9 and Figure 11. From these visualizations, we observe that the baseline model exhibits significant limitations, particularly in its instruction-following ability. For instance, while it can generate plausible images, it struggles to faithfully represent text-based prompts or instructions, leading to inconsistencies and lack of alignment between the input text and the generated output. Key Observations: 1. Weak Instruction Alignment: The baseline model often fails to translate detailed or specific textual instructions into corresponding visual features, demonstrating limited understanding of complex text-image relationships. 2. Comparison with Diffusion Models: When compared to large diffusion models and our approach, the baseline does not benefit from exposure to highquality, text-aligned training data. This results in lower fidelity and weaker representation of fine-grained textual details in the generated images. 3. Impact of Text-Focused Training: Our model, leveraging additional training on text-diverse datasets, demonstrates marked improvement in generating images that align closely with textual prompts. This highlights the importance of text-focused training for improving multi-modal instruction-following capabilities. In conclusion, the baseline models limitations underscore the necessity of fine-tuning on high-quality, textaligned data to enhance instruction-following ability and ensure consistency in multi-modal image generation tasks. 8. Comparison with Existing Models on PowerPoint Data Generation Our work is designed to generate complex PowerPoint-like data from long text inputs. To evaluate its effectiveness, we test whether existing models possess similar capabilities and compare their performance. The results of this comparison are presented in Figure 10. Below, we summarize and analyze the findings for each Figure 9. Baseline comparison. Lumina-mGPT [21], an enhanced generation model built upon Chameleon [38], demonstrates limited capability in effectively following complex textual instructions. model: 8.1. Performance of Existing Models SD3.5 Large [10]: Demonstrates very poor instruction-following ability, failing to understand and process the input effectively. The generated outputs lack semantic alignment with the given instructions, rendering it unsuitable for tasks requiring structured data generation like PowerPoint slides. GPT-4-O + DALL-E 3 : Exhibits much better instruction-following ability than SD3.5 Large. For instance, it successfully generates network topology diagram in the first row of the figure. Struggles with positional accuracy and the correct rendering of bounding boxes. For example, the title GRIN Routing Examples is completely missing from the slide, highlighting its inability to accurately position text and visual elements. Figure 10. The interleaved powerpoint generation comparison. failure to capture positional details limits its usability for structured tasks like slide generation. SD3.5 Large, on the other hand, is entirely unsuitable due to its poor comprehension of inputs. Strength of Our Model: By leveraging text-focused training and layout-aware design, our model bridges the gap between semantic understanding and visual generation. It ensures that both positional and textual fidelity are maintained, enabling the generation of professionalquality PowerPoint slides from long text inputs. In conclusion, our model outperforms existing solutions in generating structured and visually coherent PowerPointlike data, making it reliable tool for complex data visualization tasks. 9. Controllable Experiments In this experiment, we evaluate the ability of models to generate text based on prompts that include multiple controllable variables. The results are presented in Figure 12. 9.1. Observations on Existing Models Stable Diffusion 3.5 Large [10]: Often misunderstands prompts, extracting only single word from the prompt to generate the image and ignoring the broader context. Attempts to improve performance with more structured prompts yielded little improvement, demonstrating its weak instruction-following ability, especially for complex inputs. GPT-4-O with DALL-E 3 [30]: Demonstrates better alignment with text prompts and follows instructions more effectively than Stable Diffusion 3.5 Large. Successfully changes text colors in the middle rows of Figure 11. More interleaved data examples. 8.2. Performance of Our Model In contrast, our model demonstrates superior performance across multiple aspects: Accurate Layout Positioning: Our model effectively preserves the layout structure. Most elements, such as text and images, are correctly positioned within the bounding boxes, ensuring coherent slide design. Readable Text Rendering: The generated text is highly legible, even with variations in font styles and sizes. This reflects the models robustness in handling diverse text formatting requirements. Comprehensive Content Generation: Beyond generating high-quality images, our model accurately renders text within the visual context, combining both modalities seamlessly. This ability ensures that slides are not only visually appealing but also informationally complete. 8.3. Analysis and Key Takeaways Comparison with Existing Models: While GPT-4-O + DALL-E 3 shows promise in following instructions, its Figure 12. Experiment on Controllable Variables: Stable Diffusion 3.5 Large [10] demonstrates poor instruction-following ability, ignoring all specified variants and occasionally generating irrelevant images. GPT-4-O [30], while showing noticeably better instructionfollowing capability, produces outputs with low accuracy and fails to capture the specified controllable variables. Figure 12, but fails to handle variables like rotation angles, which are absent from the outputs. Lacks diversity in text styles, as most results use similar fonts and exhibit minimal variation in formatting. 9.2. Performance of Our Model In contrast, our model demonstrates significantly better performance: Comprehensive Handling of Variables: Accurately incorporates all specified controllable variables, including font size, font color, alignment, and rotation degree, ensuring alignment with the given prompts. Enhanced Diversity: Generates text with substantial variation in style, such as differences in font type and size, simulating wide range of real-world text representations. Robust Instruction Following: Effectively follows complex instructions, producing visually coherent and contextually accurate results. 9.3. Analysis and Implications Comparison with Existing Models: Stable Diffusion 3.5 Large struggles with prompt comprehension, while GPT4-O with DALL-E 3, though better, still lacks diversity and the ability to handle all controllable variables. Strength of Our Model: By explicitly focusing on controllable variables, our model bridges the gap between prompt fidelity and output diversity. This capability is valuable for synthetic dataset generation, design variations, and customized content creation. In conclusion, our model surpasses existing solutions by accurately rendering text based on complex prompts and maintaining diversity across controllable variables, making it versatile tool for text-to-image generation tasks. Figure 13. More powerpoint generation examples. 10. Additional PowerPoint Generation Examples In this experiment, we showcase more examples of PowerPoint slides generated by our model, as presented in Figure 13. These examples highlight the models ability to handle varying input lengths and generate diverse layouts and styles. cial for creating complex documents that combine visual and textual elements. Diverse Layouts and Styles: The examples reflect variety of layouts and design styles, indicating that the model can adapt its output to suit different content types and presentation needs. This flexibility is significant advantage for document generation tasks. 10.1. Key Observations 10.2. Analysis and Limitations Handling Variable Input Lengths: The input sequences in our examples do not have fixed length, with some exceeding 2,000 tokens. For instance, the last example features an input sequence of 2,532 tokens. Despite this variability, our model successfully processes long sequences, demonstrating its robustness and adaptability to diverse input sizes. Generation of Text-Only Layouts: In pure text scenarios (e.g., the second column of Figure 13), the model produces clear and well-structured layouts. The generated slides maintain logical alignment and preserve key formatting details. Interleaved Image and Text Data: The model also excels in generating interleaved layouts of images and text (e.g., the middle column), showcasing its capability to integrate multiple modalities seamlessly. This ability is cruVisual Appeal: While the generated slides demonstrate structural clarity and adaptability, the visual aesthetics still fall short of professional standards. Enhancements in style and design coherence are needed to make the outputs more visually compelling. Data Challenges: The models performance is influenced by the quality and diversity of the training data. Handling complex document structures, intricate designs, and niche use cases remains challenge, suggesting that further research into high-quality training datasets and refinement techniques is necessary. Future Directions: The results highlight the potential for future advancements in complex document generation. By addressing current limitations and improving model training with more sophisticated data, the generation of highly polished and visually appealing documents becomes feasible. 10.3. Conclusion The examples demonstrate that our model can handle unfixed input lengths and generate slides with varying layouts and styles, incorporating both text-only and interleaved content effectively. Although there is room for improvement in visual aesthetics and data handling, these results lay the groundwork for further research into advanced document generation systems."
        }
    ],
    "affiliations": [
        "Central South University",
        "Microsoft"
    ]
}