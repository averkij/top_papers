{
    "paper_title": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing",
    "authors": [
        "Ming Li",
        "Xin Gu",
        "Fan Chen",
        "Xiaoying Xing",
        "Longyin Wen",
        "Chen Chen",
        "Sijie Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer a novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define a unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering a more direct and efficient way to provide better supervision signals, and providing a novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30x less training data and 13x smaller model size."
        },
        {
            "title": "Start",
            "content": "SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing (cid:128) Website Code"
        },
        {
            "title": "Data",
            "content": "Ming Li1,2,, Xin Gu1, Fan Chen1, Xiaoying Xing1, Longyin Wen1, Chen Chen2, Sijie Zhu1, 1ByteDance Intelligent Creation (USA) 2Center for Research in Computer Vision, University of Central Florida 5 2 0 2 5 ] . [ 1 0 7 3 2 0 . 5 0 5 2 : r Figure 1. (a) Our editing method works well with real and high-resolution images, handling various free-form edits (left) and local edits (right); (b) Compared to the current state-of-the-art SmartEdit, our method achieves 9.19% performance improvement with 30 less training data and 13 fewer model parameters; (c) Our method achieves better overall scores on the human evaluation results, indicating more precise editing capabilities."
        },
        {
            "title": "Abstract",
            "content": "Due to the challenges of manually collecting accurate editing data, existing datasets are typically constructed using various automated methods, leading to noisy supervision signals caused by the mismatch between editing instructions and original-edited image pairs. Recent efforts attempt to improve editing models through generating higher-quality edited images, pre-training on recognition tasks, or introducing vision-language models (VLMs) but fail to resolve this fundamental issue. In this paper, we offer novel solution by constructing more effective editing instructions for given image pairs. This includes rectifying the editing instructions to better align with the original-edited image pairs and using contrastive editing instructions to further enhance their effectiveness. Specifically, we find that editing models exhibit specific generation attributes at different inference steps, independent of the text. Based on these prior attributes, we define unified guide for VLMs to rectify editing instructions. However, there are some challenging editing scenarios that cannot be resolved solely with rectified instructions. Corresponding author, sijiezhu@bytedance.com This work was done during the internship at ByteDance, San Jose, USA To this end, we further construct contrastive supervision signals with positive and negative instructions and introduce them into the model training using triplet loss, thereby further facilitating supervision effectiveness. Our method does not require the VLM modules or pre-training tasks used in previous work, offering more direct and efficient way to provide better supervision signals, and providing novel, simple, and effective solution for instruction-based image editing. Results on multiple benchmarks demonstrate that our method significantly outperforms existing approaches. Compared with previous SOTA SmartEdit, we achieve 9.19% improvements on the Real-Edit benchmark with 30 less training data and 13 smaller model size. All data and models are open-sourced on Github for future research. 1. Introduction In recent years, significant progress has been made in text-to-image (T2I) generation [10, 37, 4042] due to the development of diffusion models [9, 21, 48, 49]. These T2I diffusion models can generate images that align with natural language descriptions while satisfying human perception and preferences. Consequently, numerous image editing methods [4, 6, 19, 31] based on these models have been proposed to achieve various editing effects. Instruction-based methods [4, 12, 24] have become increasingly popular as they allow users to conveniently and easily modify images using language instructions without the need to provide masks, as required by mask-based methods [18, 28, 47, 53]. The training of instruction-based editing models requires the original-edited image pairs and corresponding editing instruction, making it difficult to manually create or collect large amount of relevant data [58]. To address the issue of scarce training data, existing efforts [13, 19, 25, 62] have attempted to develop various automated pipelines to synthesize large datasets. Specifically, most methods first use large language models (LLMs) to modify the text descriptions of original images. The original images and modified texts are then input into various pre-trained diffusion models to automatically generate edited images. However, current text-to-image diffusion models struggle to fully correspond to input text prompts [15, 59]. This often changes parts of the original images that do not require editing, leading to misaligned editing instructions and original-edited image pairs, thus resulting in noisy supervision signals. To mitigate the potential issues of noisy supervision in image editing models, existing work has attempted to introduce additional recognition pre-training tasks for U-Net [43] such as semantic segmentation [14, 45], or replace CLIP [38] text encoder with vision-language models (VLMs) [12, 24] to better understand editing instructions from noisy supervision signals. However, these methods not only introduce significant computational overhead but also overlook the issue of noisy supervision signals. In this paper, we focus on addressing the fundamental challenge by introducing more effective editing instructions, as demonstrated in Fig. 2. Our data-oriented method explores different research question: how much performance improvement can be achieved solely by focusing on supervision signal quality and optimization in image editing? Surprisingly, SuperEdit outperforms existing methods in both GPT-4o and human evaluations, despite using less data and requiring no additional modules or pretraining as shown in Fig.1. This demonstrates that high-quality supervision signals can significantly compensate for architectural simplicity, achieving results comparable to or better than methods with more complex requirements. Specifically, to enhance the effectiveness of supervision signals for instruction-based image editing methods, we propose using VLMs to rectify editing instructions, creating betteraligned instructions for the original-edited image pairs. However, determining which VLM to use for this task and how to establish unified rectification method for various editing instructions remain unexplored problems. To address this, we first analyze the ability of different VLMs to understand the differences between original and edited images, showing that GPT-4o [1] is the most capable of rectifying editing instructions. Additionally, we observe that both editing models and text-to-image diffusion models share similar prior, as shown in Fig. 4: different inference Figure 2. Unlike existing efforts that attempt to (a) scale up edited images with noisy supervision [4, 25], (b) introduce massive VLMs into editing model architecture [12, 24], and (c) perform additional pre-training tasks [14, 45], (d) we focus on improving the effectiveness of supervision signals, which is the fundamental issue of image editing. stages correspond to the generation of different image attributes, independent of the input text prompt [2, 17, 19, 36, 46, 56, 61] or editing instructions. Inspired by this, we guide VLMs based on these attributes to establish unified rectification guideline for various editing instructions, as demonstrated in Fig. 3. When training with only rectified editing instructions, we find that the editing model can better understand the editing commands but still faces challenges in handling complex scenarios. For example, when the original image contains multiple objects, the edit model struggles to perform an accurate editing function if the instructions modify only one of these objects. Additionally, inherent issues present in pre-trained text-to-image diffusion models [15, 2224], such as difficulty in understanding quantity, position, or object relationships, persist in the editing models. To address these issues, we propose using contrastive supervision signals to further optimize the editing model. Specifically, we first construct incorrect editing instructions based on the rectified instructions to generate positive and negative samples. We then introduce triplet loss to guide the model, thereby enhancing the effectiveness of supervision, as shown in Fig. 5. In summary, our contributions are summarized as follows: New Insight: We aim to address the noisy supervision problem that arises from the misalignment between editing instructions and original-edited image pairs, which is fundamental issue overlooked by previous work, as shown in Fig. 2. Rectifying Supervision: We leverage diffusion generation priors to guide the vision-language model to generate betteraligned editing instructions for original-edited image pairs. Facilitating Supervision: We introduce contrastive supervision using triplet loss, enabling the editing model to learn from both positive and negative editing instructions. Promising Results: We achieve significant improvements on multiple benchmarks without additional pre-training or VLM. Compared to SmartEdit [24], we achieved 9.19% improvement while reducing 30 data and 13 model parameters. 2 2. Related Work 3. Method 2.1. Image Editing with Diffusion Models Building on advancements in text-to-image (T2I) diffusion models [10, 37, 4042, 44], recent research has explored them for image editing [4, 19]. Training-free methods [5, 19, 29, 31, 34, 51] typically achieve this by adjusting attentions in pre-trained T2I models, but have limited performance and generalization capabilities on various editing tasks. Training-based methods address these limitations with specialized editing models, which can be categorized into mask-based and instruction-based approaches. Mask-based methods [6, 18, 28, 47, 53, 57] enable fine-grained local edits with user-provided or predicted masks and corresponding text descriptions. However, it struggles with global image editing and is constrained by the lack of mask-based editing data [24]. Instruction-based methods directly accept textual commands, such as add dog, offering better editing flexibility and generalization. InstructPix2Pix [4] pioneered this paradigm by generating instruction-based editing data and modifying the conditions of T2I diffusion models. Building on this framework, subsequent work introduces vision-language models [12, 24, 33] or additional pre-training tasks for the denoising U-Net [14, 24, 43, 45] to enhance the understanding and reasoning of input conditions. However, these methods not only introduce substantial computational overhead but also overlook the fundamental noisy supervision issue. 2.2. Generating and Improving Editing Supervision Due to the difficulty of scaling instruction-based image editing data through manual collection, existing efforts [4, 13, 25, 58, 62] aim to automatically modify text descriptions of original images and generate edited images with T2I diffusion models. However, this approach often produces synthesized images that do not align with the editing instructions, as shown in Figure 3, resulting in noisy editing supervision signals [58, 62]. To address this, MagicBrush [58] manually filters out incorrect editing data, but it is hard to scale. Unlike existing methods focusing on edited image quality, we leverage diffusion prior and vision-language model (i.e., GPT4o [1]) to create better-aligned instructions with original-edited image pairs, providing more accurate supervision. 2.3. Alignment of Diffusion Models The success of alignment training in large language models (LLMs) [26, 32, 39] has been applied to diffusion models for better image generation. This is achieved by maximizing reward scores [8, 27, 54] or the generation probability of the winner image in pair [11, 52, 55]. In image editing, HIVE [60] and MultiReward [16] attempt to incorporate reward information into the text condition to align the editing model. In contrast, we guide the editing model by rectifying and constructing contrastive editing instructions, achieving more effective alignment. In this section, we first introduce the most general image editing framework in Sec. 3.1. Then, we explain how to use diffusion priors to rectify editing instructions with the multimodal model (i.e., GPT-4o) in Sec. 3.2, thereby enhancing the accuracy of supervision signals. Finally, we describe how to construct contrastive supervision with both correct and incorrect editing instructions and integrate it into the editing model training using triplet loss in Sec. 3.3. 3.1. Instruction-based Image Editing Framework InstructPix2Pix [4] pioneered instruction-based image editing, performing editing tasks by simultaneously taking the original image CI and editing instructions CT as input conditions to generate the edited image from random noise ϵ. Following the definition of DDPM [21], we randomly sample timestep during training, and then add corresponding noise ϵt to the edited image x: xt = αtx+ 1 αtϵt, ϵN (0,I), (1) where ϵ is noise map sampled from Gaussian distribution, and αt := (cid:81)t s=0αs, αt = 1βt is differentiable function of timestep t, which is determined by the denoising sampler such as DDPM [21]. Then the training objective of the editing model ϵθ is predicting the added noise at timestep t, which can be written as: Ltrain =Ext,t,CI,CT ,ϵ (cid:104)(cid:13) (cid:13)ϵθ (cid:0)concat(xt,CI),t,CT (cid:1)ϵt (cid:105) , (cid:13) 2 (cid:13) 2 (2) where concat refers to concatenating the image latents of noised edited image xt and original image cI in the channel dimension. 3.2. Rectifying Supervision with Diffusion Priors As shown in Fig. 3, existing image editing datasets [4, 13, 58] typically use only Steps 1 and 2: LLMs construct editing prompts and captions, and then text-to-image diffusion models synthesize edited images. However, diffusion models often fail to accurately follow prompts while maintaining image layout, creating mismatches between original-edited pairs and editing instructions, resulting in inaccurate supervision. While better supervision signals for text-to-image diffusion models are common in image generation [3, 50], this approach remains underexplored in image editing due to two challenges: (1) VLMs trained on single-image data struggle with multi-image inputs, and (2) editing instructions vary widely, making unified rectification guidelines difficult. To address these issues, we: (1) analyzed different VLMs capabilities with multi-image inputs, finding GPT-4o most effective, and (2) discovered that timestep-specific roles in image generation also apply to editing, providing foundation for unified rectification method across various instructions (Fig. 3 and 4). Due to page limitations, our VLM analysis is in the Supplementary Material, while this section focuses on Diffusion Prior and Editing Instruction Rectification. 3 Figure 3. (a) Existing work primarily uses LLMs and diffusion models to automatically generate edited images. However, current diffusion models often fail to accurately follow text prompts while maintaining the input images layout, resulting in mismatches between the original-edited image pairs and the editing instructions. (b) We perform instruction rectification (Step 3) based on the images constructed in Steps 1 and 2. We show VLMs can understand the differences between the images, enabling them to rectify editing instructions to be better aligned with original-edited image pairs. in the late stages of sampling. This finding inspires us to guide VLMs based on these four generation attributes, establishing unified rectification method for various editing instructions. We provide more analysis and details in the Supplementary Material. Editing Instruction Rectification. As demonstrated in Fig. 3, we extend the existing editing data generation pipeline by introducing our instruction rectification (Step 3). This process relies on the original edited image pairs obtained through Steps 1 and 2 from previous work. Specifically, we input original-edited image pairs into the vision-language model (i.e., GPT-4o) and instruct it to describe the changes in the edited image compared to the original image according to the above diffusion prior generation attributes. Finally, we use VLM to summarize the instructions and ensure that its length is less than the maximum length of CLIP text encoder, which is 77 tokens. 3.3. Facilitating Supervision with Contrastive"
        },
        {
            "title": "Instructions",
            "content": "Although using rectified editing instructions can significantly improve performance across various editing tasks, we find that editing models still struggle with closely related text instructions. For example, add cat on the left side of the image and add two cats on the right side of the image might produce the same edited image. This indicates that inherent biases in pre-trained text-to-image diffusion models [15, 22], such as difficulties in understanding quantity, position, and spatial relationships, persist in editing models. More importantly, our experiments show that training models with rectified editing instructions does not resolve these challenges. To further facilitate supervision signal effectiveness, we drew on successful alignment experiences from large language models [1, 32, 39] and text-to-image diffusion models [7, 52, 54]: constructing positive and negative sample pairs and guiding the model to assign higher generation probability to positive samples compared to negative ones. Figure 4. We show that the editing model follows consistent generation attributes at different sampling stages, independent of the editing instructions. The early, middle, and late sampling stages correspond to global, local, and detail changes, respectively, while style changes occur at all stages. All the generated images here are DDIM 30-step sampled final images. The orange progress bar and the grid progress bar represent the sampling stages with and without the editing instructions, respectively. Diffusion Generation Priors. Previous work has shown that different timesteps play distinct roles in image generation for text-to-image diffusion models, regardless of the text prompt [2, 17, 19, 36, 46, 56, 61]. We find that this phenomenon also exists in instruction-based editing models and present examples based on pre-trained InstructPix2Pix [4], as shown in Fig. 4. Specifically, diffusion models focus on global layout in the early stages, local object attributes in the mid stages, and image details 4 Figure 5. (a) Based on the rectified editing instruction and original-edited image pair, we utilize the Vision-Language Models (VLM) to generate various image-related wrong instructions. These involve random substitutions of quantities, spatial locations, and objects within the rectified editing instructions according to the original-edited images context; (b) During each training iteration, we randomly select one wrong instruction cT neg and input it along with the rectified instruction cT pos into the editing model to obtain predicted noises. The goal is to make the rectified instructions predicted noise ϵpos closer to the sampled training diffusion noise ϵ, while ensuring the noise from incorrect instructions ϵneg is farther. Best viewed in color. it Constructing Contrastive Instructions. Unlike the standard alignment process for large language models or text-to-image is challenging to generate different diffusion models, editing results from the same instruction to create positive and negative sample pairs for image editing tasks. To address this, we construct positive and negative editing instructions for alignment, thereby generating relatively positive and negative edited images. As shown in Fig. 5 (a), we use the original image, edited image, and rectified editing instruction as input. The VLM (i.e., GPT-4o) is used to modify attributes in the rectified editing instruction, such as quantity, spatial relationships, and object types, to create different wrong instructions. Here, we require VLM to modify only single attribute from the rectified editing instruction in each wrong instruction, keeping most of the editing text unchanged. Since only few words are replaced between the rectified instruction and the wrong instruction, the text embeddings produced by the CLIP text encoder that serve as input to the denoising model will also be similar. This ensures the tasks learning difficulty, helping the model understand how subtle differences between the two editing instructions result in significantly different editing results. Facilitating Editing Models with Contrastive Instructions. Our key insight is that enhancing the effectiveness of supervision signals can improve various editing tasks without introducing additional model architectures or pre-training tasks. Therefore, we adhere strictly to the InstructPix2Pix [4] model architecture and training pipeline. To be specific, the inputs including the original image cI, edited image x, the rectified instruction cT pos, and wrong editing instruction cT neg. During training, we will add sampled timestep to obtain the noised edited image xt with Equation 1. Both the rectified and wrong editing instructions are fed into the denoising model to predict the final noises ϵpos and ϵneg, which are then used to 5 construct positive and negative samples, respectively: ϵpos =ϵθ ϵneg =ϵθ (cid:0)concat(cid:0)xt,cI(cid:1),t,cT (cid:0)concat(cid:0)xt,cI(cid:1),t,cT pos neg (cid:1), (cid:1). (3) (4) After constructing the positive and negative sample pairs, we aim for the noise predicted by the positive editing instruction ϵpos to be closer to the true noise ϵt sampled during training, compared to the noise predicted by the wrong editing instruction ϵneg. This goal can be achieved through triplet loss function: Ltriplet =max{d(ϵt,ϵpos)d(ϵt,ϵneg)+m,0}, (5) where d(x,y) = xy2 2 and margin is hyper-parameter. The final training loss is the combination of the original diffusion training loss and the triplet loss: Ltotal =Ltrain+λLtriplet, where Ltrain =d(ϵt,ϵpos). (6) Please note that the contrastive supervision signals are only used during the training phase. During inference, the editing model only requires one single input editing instruction. 4. Experiment 4.1. Data Collection and Construction To build diverse dataset with various types of editing instructions, we need original and edited images from different data domains, as well as wide variety of editing instructions. To achieve this, we sampled data from different public editing datasets to construct rectified and contrastive supervision signals. Specifically, we extracted 10,177, 8,807, and 21,016 editing pairs from InstructPix2Pix [4], MagicBrush [58], and Seed-Data-Edit [13], respectively, resulting in total of 40,000 training samples. During extraction, we strive to ensure that the data for different types of editing tasks is as balanced as possible. Figure 6. Visual comparison with existing methods and the corresponding human-aligned GPT-4o evaluation scores (Following, Preserving, Quality Scores from left to right). We achieve better results while preserving the layout, quality, and details of the original image. Please note that we do not claim that our editing results are flawless. We provide more visual comparison results in the supplementary material. Preserving Quality Overall Method KOSMOS-G [33] MGIE [12] SmartEdit [24] MultiReward [16] InstructDiffusion [14] InstructPix2Pix [4] MagicBrush [58] HIVE [60] HQ-Edit [25] SuperEdit (Ours) Extra Module Pretrain Tasks Model Edit Size Data 9.0M 1.9B 1.0M 8.1B 1.2M 14.1B 1.2B 320K 1.1B 860K 1.1B 300K 310K 1.1B 1.1M 1.1B 1.1B 500K 40K 1.1B Following Acc 51% 2.82 40% 2.43 64% 3.50 63% 3.39 52% 2.87 52% 2.94 51% 2.90 54% 2.93 51% 2.84 67% 3.59 Score Acc Score Acc 9% 1.43 45% 2.79 66% 3.70 58% 3.43 54% 3.17 53% 3.31 70% 3.85 56% 3.36 16% 1.63 77% 4.14 Score 27% 3.20 38% 3.35 45% 3.56 54% 3.80 45% 3.58 50% 3.69 50% 3.67 53% 3.72 54% 3.84 65% 4.01 Score Acc 29.0% 2.48 41.0% 2.86 58.3% 3.59 58.3% 3.54 50.3% 3.21 51.7% 3.31 57.0% 3.47 54.3% 3.34 40.3% 2.77 69.7% 3. Table 1. Comparison with instruction-based image editing methods on Real-Edit benchmark [16]. Compared to existing work, our method achieves state-of-the-art performance across all metrics using small amount of high-quality editing data without introducing additional models or pre-training tasks. Please note that the scores range from 0 to 5. denotes higher result is better. All baseline results are cited from the MultiReward [16] paper. Figure 7. Human evaluation on three evaluation criteria for image editing effects. (a) Following: whether the edited image adhere to the editing instructions; (b) Preserving: whether the image structure outside of the editing instructions has been preserved; (c) Quality: whether the overall quality/aesthetics of the edited image has been degraded compared to the input image. Our SuperEdit achieves the best results on all of these metrics. InstructPix2Pix [4] SmartEdit-13B [24] SuperEdit Following 2.41 3.09 3.18+1.80% Preserving Quality 2.62 3.06 3.86+16.00% 2.44 2.63 3.37+14.80% 3.47+10.80% Overall 2.49 2.93 Table 2. Human evaluation results on Real-Edit [16] benchmark. All the human-evaluated scores range from 0 to 5. Overall represents the average score of Following, Preserving, and Quality scores. We then applied our proposed methods in Sec. 3 to rectify and construct contrastive editing instructions for these training samples. Since the MagicBrush data has been manually verified, we skip the rectification step for this dataset and directly construct contrastive supervision based on the original editing instructions. For Seed-Data-Edit dataset, we only sample images from the first part of data without human editing instructions. 4.2. Experimental Settings Evaluation Benchmarks and Metrics. To more accurately assess the effectiveness of various editing models, we conducted assessments on the Real-Edit benchmark [16], which is humanaligned evaluation benchmark with GPT-4o scoring. Specifically, MultiReward [16] uses high-resolution images from the Unsplash community as test dataset and combines them with GPT-4o [1] to create an automated evaluation method for singleturn editing. It assesses edited images in terms of accuracy (%) and scores (from 0 to 5), evaluating whether they adhere to the editing instructions (Following), whether the image structure outside of the editing instructions has been preserved (Preserving), and whether the overall quality/aesthetics of the edited image has been degraded compared to the original one (Quality). 4.3. Experimental Results Comparison on Real-Edit Benchmark. In Tab. 1, we present the quantitative results of editing effectiveness on the Real-Edit benchmark [16]. Without introducing additional parameters or pre-training stages, our method achieves the best results in the three GPT-4o automated evaluation metrics: Following, Preserving, and Quality, each of which includes percentage accuracy (Acc) and scores (from 0 to 5). For example, compared to SmartEdit [24], which introduces an additional 13B vision-language model (i.e., LLaVA [30]) to the 1.1B InstructPix2Pix [4] framework, we achieved improvements of 11.4% Overall Score. This suggests that given accurate and effective supervision signals, the trained editing model can understand and successfully execute the editing instructions, without the need for additional vision-language models. It is worth noting that unlike existing image editing methods, which often show improvement in single metric while others remain unchanged or worsen, our method achieves comprehensive and significant advancements across all three metrics. This indicates that improving the effectiveness of supervision signals can accurately execute editing instructions while reducing disruption to other non-edited parts of the image, and preserving the quality and aesthetics of the original images. Specifically, we surpassed the current best methods by 3%, 7%, and 11% Acc results in Following, Preserving, and Quality, respectively. Human Evaluation We also conduct comprehensive human evaluation on Real-Edit benchmarks [16] in Tab. 2 and Fig. 7. The assessment involved 15 experienced evaluators who rated edited images based on three critical metrics: instruction faithfulness (Following), preservation of irrelevant content (Preserving), and visual quality (Quality). The results of this manual evaluation demonstrate strong consistency with the GPT-4o scoring results shown in Tab. 1. This high alignment thoroughly validates that our proposed SuperEdit significantly outperforms existing methods across all evaluation criteria. Specifically, our SuperEdit surpasses the previous state-of-the-art method SmartEdit [24] by 1.8%, 16%, 14.8%, and 10.8% on Following, Preserving, Quality, and Overall scores, respectively. These substantial improvements not only confirm the effectiveness of our approach but also establish SuperEdit as new benchmark in instruction-guided image editing, achieving superior performance while requiring significantly less training data and cost. Visual Comparison with State-of-the-art Methods. We show the visual comparison with existing image editing methods in Fig. 6. Compared to existing instruction-based editing methods, our approach not only better understands and executes editing instructions but also preserves the original images layout and quality more effectively, thereby significantly outperforming previous methods. For example, with the instruction Replace the tiger with lion, maintaining the same position in the water our SuperEdit method achieved superior results (4.8/4.8/4.8) compared to SmartEdit (4.8/4.8/2.5) and other methods. Additionally, our method improves the models comprehension of editing instructions. For the instruction Change the background to sandy beach with the ocean in the distance our method received perfect scores (4.8/5.0/5.0) while SmartEdit only achieved (5.0/4.8/4.0). Similarly, for style transformation instructions like Change the image style to look like an impressionist painting style SuperEdit significantly outperformed SmartEdit with scores of (4.8/4.8/4.8) versus (1.0/4.8/4.8), demonstrating our methods superior ability to handle complex artistic transformations. Even more impressively, for scene transformation tasks like Transform the entire scene to winter setting with snow covering the houses, trees, and boat, our SuperEdit achieved (5.0/4.8/4.8) while SmartEdit only obtained (2.0/4.5/4.5). We provide more visual comparisons with other instruction-based image editing methods in the Supplementary Material. 4.4. Ablation Study Ablation on the Rectified and Contrastive Instructions. Considering that the Real-Edit [16] benchmark is evaluated by GPT-4o [1], and its evaluation results closely align with human ratings [16], we choose this benchmark to conduct ablation experiments in Tab. 3. Compared to the original 300K InstructPix2Pix training data, our 40K training data with rectified editing instructions significantly improves all the performance of the editing model. Specifically, our approach improves scores by 0.95, 0.79, and 0.11, and accuracy by 21%, 22%, and 4% in these three metrics, respectively. In addition, editing performance is further enhanced by incorporating contrastive supervision signals. Compared to using only rectified editing Rectified Instruction Contrastive Instructions Following Acc 41% 2.45 62% 3.40 67% 3.59 Preserving Quality Score Acc Score Acc 53% 3.27 75% 4.06 77% 4.14 Score 61% 3.90 65% 4.01 65% 4.01 Table 3. Ablation study on our methods. Both rectified and contrastive editing instructions achieved improvements across all metrics. instructions, the introduction of contrastive supervision signals improves the following and preserving scores by 0.19 and 0.08, and accuracy by 5% and 2%, while maintaining the quality accuracy and score. In summary, both the introduction of rectified editing instructions and contrastive editing instructions improve the overall performance of the editing model. Ablation on Data Scaling. We investigated the impact of training data volume on model performance by experimenting with datasets ranging from 5k to 40k samples. Tab. 4 shows consistent improvements across all metrics as training data increases. With just 5k samples, our model achieves reasonable performance (54.7% accuracy, 3.42 overall score), but scaling to 40k samples yields substantial gains (69.7% accuracy, 3.91 overall score). The most significant improvements appear in the Preserving and Quality metrics, with 10% and 15%, respectively. This upward trend across all data points demonstrates that SuperEdit effectively leverages additional training examples without performance saturation, suggesting potential for further gains with larger datasets. Quality Preserving Score Acc Score Acc Following Acc 49% 2.87 57% 3.26 64% 3.40 67% 3.59 Data Size Acc Score 5k 54.7% 3.42 10k 62.0% 3.63 20k 66.3% 3.79 69.7% 3.91 40k Table 4. Ablation study on data scaling results on Real-Edit [16]. Score 55% 3.69 58% 3.87 63% 3.94 65% 4.01 60% 3.71 71% 3.76 72% 4.02 77% 4.14 Overall 5. Conclusion In this paper, we re-examine image editing models from the perspective of enhancing supervision signals, finding that existing models have not adequately addressed this challenge, resulting in suboptimal performance. We introduce unified editing instruction rectification guideline based on diffusion priors that better aligns instructions with original-edited image pairs, thereby enhancing supervision effectiveness. We also construct contrastive editing instructions allowing models to learn from both positive and negative examples. Our data-oriented approach explores an important but overlooked research question: What level of performance can be achieved with minimal architectural modifications by primarily focusing on supervision quality and optimization? Remarkably, under both GPT-4o and human evaluation, our method outperforms existing approaches despite using less data and requiring no architectural modifications or additional pretraining. This shows high-quality supervision signals can effectively compensate for architectural simplicity, offering valuable new perspectives for image editing research."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 3, 4, 7, 8 [2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 2, 4 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2023. 3 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 2, 3, 4, 5, 7, 1 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In ICCV, 2023. 3 [6] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. In ICLR, 2023. 2, 3 [7] Yusuf Dalva and Pinar Yanardag. Noiseclr: contrastive learning approach for unsupervised discovery of interpretable directions in diffusion models. In CVPR, 2024. 4 [8] Fei Deng, Qifei Wang, Wei Wei, Tingbo Hou, and Matthias Grundmann. Prdp: Proximal reward difference prediction for In CVPR, large-scale reward finetuning of diffusion models. 2024. [9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. 1 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 1, 3 [11] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. NeurIPS, 2024. 3 [12] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. In ICLR, 2024. 2, 3, 7 [13] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 2, 3, 5 [14] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In CVPR, 2024. 2, 3, 7, 1 [15] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. NeurIPS, 2024. 2, [16] Xin Gu, Ming Li, Libo Zhang, Fan Chen, Longyin Wen, Tiejian Luo, and Sijie Zhu. Multi-reward as condition for instruction-based image editing. In ICLR, 2025. 3, 7, 8, 1 [17] Qin Guo and Tianwei Lin. Focus on your instruction: Fine-grained and multi-instruction image editing by attention modulation. In CVPR, 2024. 2, 4 [18] Qin Guo and Tianwei Lin. Focus on your instruction: Fine-grained and multi-instruction image editing by attention modulation. In CVPR, 2024. 2, 3 [19] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In ICLR, 2023. 2, 3, [20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 1, 3 [22] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. 2, 4 [23] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. NeurIPS, 2023. [24] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In CVPR, 2024. 2, 3, 7, 8, 1 [25] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 2, 3, [26] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023. 3 [27] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving In conditional controls with efficient consistency feedback. ECCV, 2024. 3 [28] Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, and Ming-Hsuan Yang. Text-driven image editing via learnable regions. In CVPR, 2024. 2, 3 [29] Bingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang. Towards understanding cross and self-attention in stable diffusion for text-guided image editing. In CVPR, 2024. 3 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2024. 7 [31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2, [32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022. 3, 4 [33] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. In ICLR, 2024. 3, 7 9 [51] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In CVPR, 2023. 3 [52] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. 3, 4 [53] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In CVPR, 2023. 2, 3 [54] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS, 2023. 3, 4 [55] Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, 2024. [56] Mingyang Yi, Aoxue Li, Yi Xin, and Zhenguo Li. Towards understanding the working mechanism of text-to-image diffusion model. arXiv preprint arXiv:2405.15330, 2024. 2, 4 [57] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023. 3 [58] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. NeurIPS, 2024. 2, 3, 5, 7, 1 [59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. 2 [60] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In CVPR, 2024. 3, 7 [61] Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jurgen Schmidhuber. Cross-attention makes inference cumbersome in text-to-image diffusion models. arXiv preprint arXiv:2404.02747, 2024. 2, 4 [62] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024. 2, [34] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH, 2023. 3 [35] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 2019. 1 [36] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In ICCV, 2023. 2, 4 [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2023. 1, 3 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 3 [39] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 2024. 3, 4 [40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 1, 3 [41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [42] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3 [43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 2, [44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 3 [45] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In CVPR, 2024. 2, 3, 1 [46] Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, and Federico Tombari. Lime: localized image editing via attention regularization in diffusion models. arXiv preprint arXiv:2312.09256, 2023. 2, 4 [47] Jaskirat Singh, Jianming Zhang, Qing Liu, Cameron Smith, Zhe Lin, and Liang Zheng. Smartmask: Context aware high-fidelity mask generation for fine-grained object insertion and layout control. In CVPR, 2024. 2, 3 [48] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. 1 [49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 1 [50] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 3 10 SuperEdit: Rectifying and Facilitating Supervision for Instruction-Based Image Editing (cid:128) Website Code"
        },
        {
            "title": "Data",
            "content": "6. Overview of Supplementary The supplementary material is organized into the following sections: Section 7: Implementation details. Section 8: More experiments and analysis. Section 9: More analysis on diffusion generation prior. Section 10: Detailed prompt for generation prior. Section 11: Discussion and limitation. Section 12: More visualization comparison and results. 7. Implementation Details We implemented our editing model training based on the InstructPix2Pix PyTorch [35] code from the Diffusers repository [48], using Stable Diffusion V1.5 [42] as the pre-trained weights for the editing model. Following InstructPix2Pixs implementation [4], we enable classifier-free diffusion guidance [20] for both the image condition and the text condition with 5% mask probability during training. The training batch size is 512 with learning rate of 1e-4, weight decay of 1e-2, and warm-up ratio of 100 steps. The training resolution is 512x512 by resizing input images without any crops. Margin m=5e3 and weight λ=1.0 is used for triplet loss Ltriplet. We train the edit model for 10,000 steps and use the triplet loss after the 2,000 training steps. During inference, we keep the original image ratio and resize the shorter side to 512, with DDIM [49] sampler and 50 sampling steps, following the default settings of Multi-Reward [16]. The text guidance scale and image guidance scale we used for inference are 10.0 and 1.5, respectively. 8. More Experiments and Analysis In this section, we provide more experiments and analysis. We first discuss limitations of current metrics in Sec. 8.1, then present the MagicBrush benchmark results in Sec. 8.2, and finally analyze GPT-4o cost and different VLMs in Sec. 8.3. 8.1. Limitations of Existing Metrics Here, we show an example from MagicBrush test set in Fig. 8 to illustrate that existing metrics (e.g., L1/L2/DINO) cannot reflect actual editing quality; that is, the results of these metrics do not match human judgment. This dilemma has also been noted in previous instruction-based image editing works, including SmartEdit [24], Emu-Edit [45], and MultiReward [16]. In addition, SmartEdits metrics (CLIP, DINO) in Tab. 5 are worse than MagicBrush, but its human evaluation shows better results in SmartEdit paper [24]. This discrepancy further shows the rationale for our comprehensive assessment using both GPT-4o-based evaluation (RealEdit) and human evaluation. Figure 8. Existing metrics cannot reliably indicate editing quality. 8.2. Evaluation on MagicBrush Benchmark In Tab. 5, we present quantitative comparison of various image editing methods evaluated on the MagicBrush single-turn benchmark. However, its important to note that these automated metrics (CLIP-I, CLIP-T, DINO, L1) should be interpreted with caution. As highlighted by previous works [16, 24, 45], such metrics often fail to fully capture human perceptual preferences, and can sometimes lead to misleading conclusions about actual editing quality. Several studies have demonstrated significant discrepancies between metric-based rankings and human evaluation results [16, 24, 45]. Our proposed method adopts data-oriented approach, contrasting with the model-oriented strategies prevalent in image editing. Remarkably, without requiring additional parameters, pretraining tasks, or extensive training data (using only 40K samples compared to 300K-1.2M in other methods), our approach achieves competitive performance across all metrics. The CLIP-T score of 30.3 is only 0.3 lower than the best results, and DINO score of 80.2 (second highest) is particularly noteworthy, suggesting strong preservation of both semantic and structural image features. Method InstructPix2Pix [4] InstructDiffusion [14] MagicBrush [58] SmartEdit [24] SuperEdit (Ours) Extra Module Pretrain Tasks Edit Data 300K 860K 310K 1.2M 40K CLIP-I CLIP-T DINO L1 85.4 89.2 90.7 90.4 90.5 29.2 30.2 30.6 30.3 30.3 69.8 77.7 80.6 79.7 80.2 0.112 - 0.062 0.081 0. Table 5. Quantitative comparison (L1/CLIP-I/CLIP-T/DINO-I) on the MagicBrush benchmark. Our SuperEdit achieves good performance with better efficiency, without extra modules or pretrain tasks. 8.3. GPT-4o Cost & Different VLMs Performance We respectfully emphasize that our core contribution is identifying and addressing noisy supervision in existing datasets, rather than focusing on cost-effective scaling strategies. Using GPT-4o for our method costs $0.02 per 512512 input-edited image pair, totaling $800 for 40K data, which is less expensive than existing works that require additional VLM fine-tuning or extra pre-training stages. For alternative ablation, we asked 5 annotators to evaluate rectified instructions from different VLMs. As shown in Tab. 6, existing open-source VLMs can partially substitute GPT-4o. These open-source models can be 1 Figure 9. We show the impact of incorporating the editing prompt at different inference timesteps on the edited image. (a) The global layout changes usually occur in the early stages of inference. Adding text editing instructions to modify the global layout at the mid or late stages does not effectively impact the global layout. (b) Local object attribute changes occur in the mid-stages of sampling. Adding text editing instructions in the early or late stages may result in incorrect editing outcomes. (c) The style changes happen across all inference stages, and the detail changes happen in the late stage (Please refer to the subtle differences between the last two images). Best viewed in color. further fine-tuned with GPT-4o data and then used for efficient scaling up, which we leave for future work. GPT-4o LLaVA-OV(72B) 76.2% Table 6. Instruction rectification success rate across 100 samples InternVL2(76B) Qwen2-VL(72B) 48.2% 50.4% 47.8% 9. Diffusion Generation Prior As discussed in Sec. 3.2 and Fig. 4 of the main paper, editing diffusion models focus on specific generation attributes during inference, independent of the different editing instructions. Specifically, editing models focus on global layout in the early stages, local object attributes in the mid stages, image details in the late stages, and style change across all sampling stages. In this section, we further demonstrate this generation prior in Fig.9. Fig.9 provides compelling visual evidence for the claims made in the main paper regarding how diffusion models process different aspects of image generation at specific timesteps. The experiments systematically demonstrate that this behavior is consistent across various editing tasks, reinforcing the observation that different timesteps play distinct roles in image generation for text-to-image diffusion models, regardless of the 2 text prompt as cited in previous works. Specifically, the figure illustrates three key patterns: (a) Global Layout Changes: The first row shows that changing the background to sky is most effective when prompts are introduced in the early stages (0-10 timesteps). When the same editing instruction is applied during mid (10-20) or late (20-30) stages, the model fails to properly modify the global layout, maintaining the original forest background despite the editing instructions. This validates our assertion that diffusion models focus on global layout in the early stages. (b) Local Object Attributes: The second row demonstrates that local attribute modifications, such as changing the teddy bears color to red, are optimally achieved during the mid-stages of sampling (10-20 timesteps). When the color change instruction is introduced too early or too late, the results show inconsistent or incomplete color transformation. This confirms that local object attributes are processed in the mid stages. (c) Style and Details: The third row reveals two important insights. First, style transformations (changing to ink painting style) can be effectively applied across all timesteps, indicating that style modifications have more flexible temporal window. Second, subtle detail refinements are predominantly processed in the late stages (20-30), as evidenced by the finer differences between the last two images in the bottom row. This supports our claim about image details in the late stages of sampling. These observations not only validate the theoretical framework presented in the main text but also provide practical insights for optimizing instruction-based image editing. The clear temporal division of editing capabilities suggests that more nuanced approach to prompt timing could significantly improve editing outcomes. This understanding directly supports our approach of guiding Vision-Language Models based on these four generation attributes (global layout, local attributes, style, and details), enabling us to establish unified rectification method applicable across various editing instructions as described in the main paper. 10. GPT-4o Prompts for Constructing Rectified and Contrastive Editing Instructions We show the detailed prompt for GPT-4o to construct the rectified and contrastive editing instructions in Fig. 10. As discussed in Sec. 9, we input the original image and the edited image into GPT-4o and ask it to return the differences in the following four attributes: Overall Image Layout Local Object Attributes, Image Details, and Style Change. When calling the GPT-4o API, we explicitly define Overall Image Layout as modifications to the major objects, characters, and background in the image. Local Object Attributes are defined as changes in the texture, motion, pose, and shape of the major objects, characters, and background. Additionally, we combine Style and Details into single category to reduce the number of tokens generated by GPT-4o, thus saving costs. We observed that this adjustment does not reduce GPT-4os understanding of the style and detail changes between the original-edited image pair. In the actual training of the editing model, acknowledging that CLIP [38] text encoder can accept maximum of 77 textual tokens as input, we ask GPT-4o to summarize and refine these rectified instructions. We then use the consolidated and refined editing instructions (Summarized Instruction in Fig. 10) to train the model. 11. Discussion and Limitation Discussion. Its important to emphasize that our data-oriented approach is not mutually exclusive with model-oriented methods like MultiReward or SmartEdit, nor is its purpose to surpass existing work across various benchmarks or diminish their excellent contributions. Instead, our work explores complementary yet important research question: What level of performance can be achieved with minimal architectural modifications by primarily focusing on supervision quality and optimization? Surprisingly, under both GPT-4o and human evaluation, our method significantly outperforms existing approaches despite using only small amount of data, without modifying the model architecture, and requiring no additional pretraining. This suggests that highquality data can substantially compensate for architectural simplicity, achieving results comparable to or even better than methods with considerably more parameters and pretraining requirements. We believe our approach and experimental results bring new insights and novelty to the field of image editing research. Furthermore, since our data-oriented approach is complementary and orthogonal to existing work, we can build upon current methods to further improve editing performance. Specifically, we follow the same setup as SmartEdit, retraining our model using InstructDiffusion as the pre-trained weights. The experimental results, as shown in Tab. 7, demonstrate that our method can complement existing work to achieve even better editing performance. When comparing SuperEdit with InstructDiffusion pre-trained weights against SmartEdit, we observe significant improvements across all metrics (71% vs. 64% in following instructions, 83% vs. 66% in preserving content, and 71% vs. 45% in image quality), despite using only 40K training samples compared to SmartEdits 1.2M. Method SmartEdit SuperEdit SuperEdit Pre-trained U-Net InstrutDiff SD1.5 InstrutDiff Following Acc Model Size Edit Data 14.1B/1.2M 64% 3.50 67% 3.59 1.1B/40K 71% 3.76 1.1B/40K Preserving Quality Score Acc Score Acc 66% 3.70 77% 4.14 83% 4.32 Score 45% 3.56 65% 4.01 71% 4.17 Table 7. SuperEdit outperforms previous SOTA SmartEdit and achieves further improvements with InstructDiffusion pre-trained weights. In addition, we also provide the results that trained with lower resolution (256 256), the results on Real-Edit benchmark still outperforms previous SOTA method SmartEdit [24]. Method SmartEdit SuperEdit Model Size Edit Data 14.1B/1.2M 1.1B/40K Training Resolution 256 256 Following Acc 64% 3.50 68% 3.56 Preserving Quality Score Acc Score Acc 66% 3.70 75% 4.02 Score 45% 3.56 66% 4.02 Table 8. SuperEdit results with lower training resolution. Both SmartEdit and SuperEdit are pre-trained with InstructDiffusion here. Limitation. Our method significantly enhances instructionbased image editing, but limitations still exist. The trained model still faces difficulties in understanding and executing complex instructions, especially with densely arranged objects and complicated spatial relationships. Although we used correction instructions and contrastive supervision signals, differences between editing results and editing instructions may still occur due to the inherent limitations of pre-trained Stable Diffusion and the challenges in fully capturing the nuances of natural language. Additionally, to fairly compare with existing methods, we chose Stable Diffusion v1.5 as the Base Model for building our editing model, which may result in worse image quality of edited images compared to state-of-the-art Text-to-Image models. Finally, ensuring the accuracy and effectiveness of correction and contrastive instructions requires the use of GPT-4o [1], which may incur additional costs as the amount of data increases. 12. More Visualization Comparison and Results We show more visual comparison with existing instructionbased image editing methods in Fig. 13 and Fig. 14. Compared to existing instruction-based editing methods, our approach not only better understands and executes editing instructions but also preserves the original images layout and quality more effectively, thereby significantly outperforming previous methods. 3 Figure 10. GPT-4o prompts for constructing rectified and contrastive editing instructions 4 Figure 11. Comparison of different vision-language models in rectifying editing instructions based on generation prior attributes. GPT-4o achieves more stable and accurate results in describing the differences between original-edited image pairs. Text in red represents incorrectly generated instructions. Figure 12. If the predefined four generation prior attributes are not used as templates for in-context learning, the GPT-4o rectified editing instructions will contain redundant information and lack the standardization needed for scalable processes. 5 Figure 13. More visual comparison with existing methods. 6 Figure 14. More visual comparison with existing methods."
        }
    ],
    "affiliations": [
        "ByteDance Intelligent Creation (USA)",
        "Center for Research in Computer Vision, University of Central Florida"
    ]
}