{
    "paper_title": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting",
    "authors": [
        "Seokhyun Youn",
        "Soohyun Lee",
        "Geonho Kim",
        "Weeyoung Kwon",
        "Sung-Ho Bae",
        "Jihyong Oh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D Gaussian Splatting (3DGS) has emerged as a powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation."
        },
        {
            "title": "Start",
            "content": "SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting SEOKHYUN YOUN, Chung-Ang University, South Korea SOOHYUN LEE, Kyung Hee University, South Korea GEONHO KIM, Chung-Ang University, South Korea WEEYOUNG KWON, Chung-Ang University, South Korea SUNG-HO BAE, Kyung Hee University, South Korea JIHYONG OH, Chung-Ang University, South Korea https://cmlab-korea.github.io/Awesome-Efficient-GS/ 3D Gaussian Splatting (3DGS) has emerged as powerful explicit representation enabling real-time, high-fidelity 3D reconstruction and novel view synthesis. However, its practical use is hindered by the massive memory and computational demands required to store and render millions of Gaussians. These challenges become even more severe in 4D dynamic scenes. To address these issues, the field of Efficient Gaussian Splatting has rapidly evolved, proposing methods that reduce redundancy while preserving reconstruction quality. This survey provides the first unified overview of efficient 3D and 4D Gaussian Splatting techniques. For both 3D and 4D settings, we systematically categorize existing methods into two major directions, Parameter Compression and Restructuring Compression, and comprehensively summarize the core ideas and methodological trends within each category. We further cover widely used datasets, evaluation metrics, and representative benchmark comparisons. Finally, we discuss current limitations and outline promising research directions toward scalable, compact, and real-time Gaussian Splatting for both static and dynamic 3D scene representation. 1 INTRODUCTION 3D scene representation and photorealistic novel view synthesis are crucial for wide range of applications, including virtual reality (VR) [17, 115], augmented reality (AR) [96], media generation [75], autonomous driving [110], and large-scale visualization [92, 98]. The evolution of these tasks has been dramatically accelerated by the emergence of Neural Radiance Fields (NeRF) [6]. By directly training implicit neural networks through differentiable volume rendering, NeRF has achieved high-quality 3D reconstruction from 2D images. However, despite its impressive visual quality, NeRF suffers from extremely slow training rendering speed, since it requires large number of ray samples per pixel to generate single image. [18, 37, 55, 70, 80, 89, 128] To overcome the limitations of such implicit representations, 3D Gaussian Splatting (3DGS) [42] revisits the idea of explicit 3D scene representation. Similar to traditional 3D representations such as point clouds [77, 78, 82] and meshes [24, 122], 3DGS explicitly represents scene using collection of primitives. Unlike these classical forms, however, 3DGS models the scene as set of anisotropic 3D Gaussians, each parameterized by set of learnable attributes including position, scale, rotation, opacity, and view-dependent color coefficients (see Sec. 2.1 for more details). By These authors contributed equally to this paper. Co-corresponding authors. Authors Contact Information: Seokhyun Youn, hisn16@cau.ac.kr, Chung-Ang University, South Korea; Soohyun Lee, nata1225@khu.ac.kr, Kyung Hee University, South Korea; Geonho Kim, joelkimgh@cau.ac.kr, Chung-Ang University, South Korea; Weeyoung Kwon, weeyoungkwon@cau.ac.kr, Chung-Ang University, South Korea; Sung-Ho Bae, Kyung Hee University, Seoul, South Korea, shbae@khu.ac.kr; Jihyong Oh, Chung-Ang University, Seoul, South Korea, jihyongoh@cau.ac.kr. 1 5 2 0 D 8 ] . [ 1 7 9 1 7 0 . 2 1 5 2 : r 2 Youn et al. learning these attributes directly from multi-view images, 3DGS not only achieves high-quality scene reconstruction, but also enables real-time rendering performance, which is infeasible for NeRF-based approaches. Further, recent methods can reconstruct 4D scene representation from monocular or multi-view video inputs. Dynamic 3D/4D Gaussian Splatting extends conventional static 3DGS into the temporal dimension, enabling the reconstruction of not only static scenes and objects but also dynamic ones exhibiting non-rigid motion. By modeling spatial and temporal variations within unified spatio-temporal Gaussian framework, 4DGS provides continuous motion representation and temporally coherent rendering (see Sec. 2.2 for more details). This capability opens up new possibilities for dynamic scene editing [46], motion capture [41], and realistic simulation in robotics and autonomous driving [65], where time-varying geometry and appearance are crucial. However, despite these advantages, the practical deployment of Gaussian Splattingbased methods remains challenging due to their massive memory footprint and computational overhead [73]. typical high-resolution static scene often contains millions of 3D Gaussians, which require significantly more memory than NeRF-based models [22]. Furthermore, 4D scene reconstruction with Gaussian Splatting demands even greater memory consumption, as each Gaussian must encode temporal information across multiple frames [125]. To overcome these bottlenecks, new research direction has emerged under the umbrella of Efficient Gaussian Splatting, aiming to reduce the redundancy of Gaussian primitives while preserving rendering quality. As various studies have been conducted, research on Efficient Gaussian Splatting can be categorized into the following groups: Parameter Compression (Sec. 3.1, Sec. 4.1): This approach directly compresses the attributes of Gaussians using techniques such as pruning, quantization, and entropy coding. Restructuring Compression (Sec. 3.2, Sec. 4.2): This approach performs compression by redesigning the structure of the original Gaussian Splatting framework through methods such as hierarchical anchors, neural integration, or geometry-aware organization. This survey provides the first comprehensive overview of Efficient 3D and 4D Gaussian Splatting (Sec. 3, Sec. 4). For both static 3DGS and dynamic 3D/4D Gaussian Splatting, we categorize existing methods into Parameter Compression (Sec. 3.1, Sec. 4.1) and Restructuring Compression (Sec. 3.2, Sec. 4.2), further organizing them into subcategories. For each class, we summarize the core methodology, mathematical formulation, and representative works. In addition, this survey provides systematic overview of widely used datasets and evaluation metrics in Gaussian Splatting research, and presents comparisons of recent methods to establish fair and consistent basis for performance evaluation (Sec. 5). Finally, we comprehensively discuss the limitations of existing approaches and propose key future directions that may guide the development of more efficient Gaussian Splatting techniques (Sec. 6). By establishing unified taxonomy and connecting isolated research threads, this survey aims to serve as foundation for the further development of scalable, efficient, and robust Gaussian Splatting frameworks for static and dynamic 3D scene representation. 2 PRELIMINARY All mathematical notations used in this survey are summarized in the supplementary material. 2.1 Static 3D Gaussian Splatting (3DGS) 3DGS represents 3D scene as set of anisotropic 3D Gaussians and directly projects them into 2D screen space for rendering. Unlike Implicit Neural Representations (e.g., NeRF [6]), 3DGS can be optimized without neural networks and SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 3 Fig. 1. Overview of the static 3D Gaussian Splatting (3DGS) pipeline. scene is represented as set of 3D Gaussians with attributes including position ğ, scale ğ’”, rotation ğ’“, spherical harmonics (SH) coefficients ğ’‰, and opacity ğœ (Sec. 2.1.1). Each Gaussian is projected into the camera coordinate system and rendered onto the image plane through differentiable tile rasterizer (Sec. 2.1.3, Sec. 2.1.4). The rendering is optimized by minimizing the difference between predicted and GT images (Sec. 2.1.5). To improve reconstruction quality, adaptive density control clones or splits Gaussians based on view-space gradient signals while pruning nearly transparent Gaussians (Sec. 2.1.6). In addition, efficiency-oriented strategies such as pruning, quantization, and entropy coding are applied to compress Gaussian attributes (Sec. 3.1). supports real-time rendering through CUDA-based tile rasterization pipeline. This section describes the initialization, rendering pipeline, and optimization process of 3DGS. An overall overview of the 3DGS framework is shown in Fig. 1. 2.1. Initialization and Attribute Definition. 3DGS requires multi-view RGB images of the same scene along with their corresponding camera pose information as input. Therefore, before the actual initialization of the 3D Gaussians, Structure-from-Motion (SfM) tool (e.g., COLMAP [85]) is used to initialize the sparse point cloud of the scene and recover the camera poses for each image. The sparse point cloud is then used for Gaussian initialization, where each point is initialized as single 3D Gaussian. The estimated camera poses are used later in the projection stage. Each Gaussian has the following attributes: Position ğ R3: It represents the center coordinates of the 3D Gaussian. This attribute is initialized using the world coordinates from the sparse point cloud. Scale ğ’” R3: It indicates the spatial extent occupied by the 3D Gaussian. Rotation ğ’“ R4: It represents the orientation of the 3D Gaussian, initialized as unit quaternion. View-dependent Spherical Harmonics (SH) coefficients ğ’‰ R(ğ‘‘+1)2 3: They represent colors that vary with the viewing direction. ğ‘‘ is parameter that denotes the degree of the SH, which determines the level of detail in representing view-dependent colors in the scene. It is commonly set to ğ‘‘ = 3 in practice. Since the spherical harmonics functions correspond to each RGB channel, the SH coefficients are also defined accordingly. 4 Youn et al. Opacity ğœ R: Each 3D Gaussian has an opacity value in the range [0, 1], stored with ğ‘€-bit precision. As the opacity value approaches 0, the 3D Gaussian becomes more transparent, while as it approaches 1, it becomes more opaque. In the original 3DGS paper [42], it uses ğ‘€ = 8 to store Gaussian opacity. Based on the above, in this survey, the set of ğ‘ 3D Gaussians at specific camera pose and their corresponding attributes are denoted as follows: = {ğºğ‘– = (ğğ‘–, ğ’”ğ‘–, ğ’“ğ‘–, ğ’‰ğ‘–, ğœğ‘– )}ğ‘ ğ‘–=1 . (1) Let ğºğ‘– denote the ğ‘–-th Gaussian among ğ‘ Gaussians, with position (ğğ‘– ), scale (ğ’”ğ‘– ), rotation (ğ’“ğ‘– ), SH coefficients (ğ’‰ğ‘– ), and opacity (ğœğ‘– ). Each attribute of the Gaussian is optimized through the training process. 2.1.2 Mathematical Definition of 3D Gaussian. When the center vector of the ğ‘–-th Gaussian in coordinate system is ğğ‘– , the Gaussian is defined as ğºğ‘– (ğ’™) = exp( (ğ’™ ğğ‘– )ğšº1 ğ‘– (ğ’™ ğğ‘– )), (2) 1 2 where ğ’™ is an arbitrary point in the world coordinate and ğšºğ‘– denotes the covariance matrix of the ğ‘–-th Gaussian. The covariance matrix is formulated as ğšºğ‘– = ğ‘¹ğ‘– ğ‘ºğ‘– ğ‘º ğ‘– ğ‘¹ ğ‘– , (3) where ğ‘¹ğ‘– and ğ‘ºğ‘– are the rotation and scaling matrices of the Gaussian, respectively, which are derived from the Gaussians attributes ğ’“ğ‘– and ğ’”ğ‘– . 2.1.3 Projection. Let ğ‘¾ be the coordinate transformation from the world coordinates to the camera coordinates, and ğ‘± be the Jacobian of the camera coordinate to screen coordinate projection. The covariance matrix is formulated as: ğšº ğ‘– = ğ‘± ğ‘¾ ğšºğ‘–ğ‘¾ ğ‘± . (4) This results in the representation of each 3D anisotropic Gaussian as 2D ellipse in the screen coordinate system. 2.1.4 Differentiable Tile Rasterization. The rendering pipeline proceeds as follows. First, the image is divided into ğ‘‡ ğ‘‡ pixel tiles. According to the original 3DGS paper, the tile size ğ‘‡ was set to 16. Next, for each projected Gaussian, the overlapping tiles are determined. Finally, per-pixel rasterization is performed to compute the contribution of each Gaussian to the pixels within the overlapping tiles. For each Gaussian ğºğ‘– , the alpha value at pixel ğ’‘ is given by ğ›¼ğ‘– (ğ’‘) = ğœğ‘– ğ‘”ğ‘– (ğ’‘) , where ğ‘”ğ‘– (ğ’‘) denotes the value of the projected 2D Gaussian at pixel ğ’‘, computed as ğ‘”ğ‘– (ğ’‘) = exp (cid:18) 1 2 (ğ’‘ ğ ğ‘– ) (ğšº ğ‘– ) 1 (ğ’‘ ğ ğ‘– ) (cid:19) . (5) (6) Here, ğ ğ‘– denotes the 2D projected center of the ğ‘–-th Gaussian after applying the camera projection, and ğšº covariance matrix obtained by projecting the 3D covariance ğšºğ‘– using the Jacobian of the projection function. ğ‘– is the 2D The final color of pixel ğ’‘ is obtained via alpha blending, which combines the color ğ’„ğ‘– predicted from the Gaussians SH coefficients with the corresponding depth information: ğ¶ (ğ’‘) = ğ’„ğ‘–ğ›¼ğ‘– (ğ’‘) ğ‘– ğ‘– 1 (cid:214) ğ‘—=1 (cid:0)1 ğ›¼ ğ‘— (ğ’‘)(cid:1) . (7) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 5 In this expression, denotes the total number of Gaussians projected onto the pixel. ğ’„ğ‘– is the view-dependent base color of the Gaussian, computed from its SH coefficients and the camera viewing direction. The rasterizer is fully differentiable, allowing gradients to be backpropagated to all Gaussian parameters. 2.1.5 Optimization. The reconstruction is optimized by minimizing the difference between the predicted 2D image ğ¼ğ‘ğ‘Ÿğ‘’ğ‘‘ and the ground truth (GT) ğ¼ğ‘”ğ‘¡ : ğ¿ = (1 ğœ†) ğ¿1 (ğ¼ğ‘ğ‘Ÿğ‘’ğ‘‘, ğ¼ğ‘”ğ‘¡ ) + ğœ† (1 ğ‘†ğ‘†ğ¼ğ‘€ (ğ¼ğ‘ğ‘Ÿğ‘’ğ‘‘, ğ¼ğ‘”ğ‘¡ )) , (8) where ğ¿1 () is the pixel-wise mean absolute error, and ğ‘†ğ‘†ğ¼ ğ‘€ [108] is the structural similarity index. ğœ† is weighting factor that controls the relative contribution of the ğ¿1 () and the ğ‘†ğ‘†ğ¼ğ‘€ loss in the total loss function. 2.1.6 Adaptive Density Control. Since the initial Gaussians are generated from sparse point cloud obtained via SfM, they are insufficient to represent the entire scene. Therefore, during training, the number of Gaussians is adaptively increased by duplicating or splitting existing Gaussians. To select Gaussians for densification, the view-space position gradients of the Gaussians are considered. large gradient indicates high reconstruction error in the corresponding region, implying that more Gaussians are required to represent the scene in greater detail. Specifically, if the view-space position gradients of Gaussian exceed certain threshold ğœğ‘ğ‘œğ‘  , which is set to 0.002 in the 3DGS paper, densification is performed. In this process, Gaussians with relatively small scales ğ’” are cloned, while Gaussians with larger scales are split into smaller Gaussians and distributed around the original position. This densification step is performed at fixed epoch intervals, which reduces unnecessary computational cost and ensures efficient training. In addition, if the opacity ğœğ‘– of Gaussian is less than certain threshold ğœ–ğœ , it is considered nearly transparent and thus negligible for scene reconstruction, and such Gaussians are pruned. This prevents an uncontrolled growth in the number of Gaussians. 2.2 Dynamic 3D/4D Gaussian Splatting In recent years, research interest has shifted beyond static scene reconstruction using 3D Gaussian Splatting toward the reconstruction of dynamic scenes that incorporate temporal dimension. As this field has gained momentum, various approaches have been proposed to extend 3D Gaussians for modeling dynamic scenes. In this section, we classify and describe how different dynamic 3DGS and 4DGS studies represent dynamic scenes. Representative approaches for dynamic 3D/4D Gaussian Splatting are illustrated in Fig. 2. It should be noted that many works use the terms dynamic 3DGS [63] and 4DGS [109, 119] interchangeably, and there is ongoing debate regarding which term to adopt. In this survey, we do not distinguish between the two terms. Some models adopt deformation modeling with canonical 3D Gaussian set. Deformable 3DGS [97] initializes all 3D Gaussians in canonical state and models their dynamic motion with deformation network. The deformation network is time-conditioned MLP that produces offsets for position, rotation, and scale: (Î”x, Î”r, Î”s) = ğ¹ğœƒ (xğ‘, ğ‘¡), (9) where xğ‘ is the canonical position and ğ‘¡ is the timestamp. This design provides flexibility in modeling non-rigid motion and allows monocular dynamic scene reconstruction. However, since it relies on dense MLP, the approach can suffer from temporal jitter, especially when supervision is limited. To mitigate this issue, Deformable 3DGS introduces Annealing Smooth Training (AST), which gradually regularizes the deformation field during optimization and effectively stabilizes the temporal dynamics. 6 Youn et al. Fig. 2. Overview of representative approaches for dynamic 3D/4D Gaussian Splatting. (a) Deformable 3DGS [97] initializes canonical set of 3D Gaussians and models temporal motion using time-conditioned MLP that predicts offsets in position, rotation, and scale. (b) Wu et al. 4DGS [109] adopts structured encoderdecoder design, where spatial-temporal structure encoder (HexPlane) organizes the 4D domain (ğ‘¥, ğ‘¦, ğ‘§, ğ‘¡ ) into multi-plane features, which are then decoded into Gaussian attribute offsets. (c) Yang et al. 4DGS [119] directly extends 3D Gaussians into the spatio-temporal domain by defining each Gaussian as 4D primitive with 4D mean, covariance, and appearance coefficients. During rendering, 4D Gaussians are converted into 3D Gaussians at target timestamps through temporal slicing. Wu et al. 4DGS [109] extends the canonical Gaussian formulation with structured encoder-decoder design. Instead of directly predicting offsets with simple MLP, spatial-temporal structure encoder organizes the 4D domain (ğ‘¥, ğ‘¦, ğ‘§, ğ‘¡) into multi-plane features. These features are interpolated to produce latent code: where ğ¸ğœ™ denotes the HexPlane [9] encoder. The latent feature is then decoded into Gaussian attribute offsets: (Î”x, Î”r, Î”s) = ğ·ğœ“ (f), = ğ¸ğœ™ (xğ‘, ğ‘¡), (10) (11) where ğ·ğœ“ is lightweight MLP decoder network. This grid-like representation improves expressiveness and motion fidelity compared to MLP-only approaches, but it requires more memory and results in slower rendering speed due to the cost of querying large spatio-temporal feature grids. Yang et al. 4DGS [119] directly extends 3DGS into the spatio-temporal domain by defining each Gaussian as four-dimensional primitive in (ğ‘¥, ğ‘¦, ğ‘§, ğ‘¡). In this formulation, time ğ‘¡ is not treated as conditional input but rather as an independent coordinate dimension. Each Gaussian is parameterized with 4D mean, 4D covariance, and 4D appearance coefficients: ğ4ğ· = (ğœ‡ğ‘¥, ğœ‡ğ‘¦, ğœ‡ğ‘§, ğœ‡ğ‘¡ ), ğšº4ğ· R44, h4ğ· = SH/Fourier(ğ‘¥, ğ‘¦, ğ‘§, ğ‘¡). (12) SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 7 Here, ğ4ğ· denotes the 4D position, ğšº4ğ· is the 4D covariance matrix (encoding both scale and rotation), and h4ğ· represents spherical harmonics or Fourier coefficients defined over the 4D coordinates. During rendering, 4D Gaussians must be converted into valid 3D Gaussians corresponding to the target timestamp ğ‘¡ğ‘– . This is achieved through temporal slicing, where each 4D Gaussian is projected onto the hyperplane ğ‘¡ = ğ‘¡ğ‘– . Unlike deformation network approaches, this method represents Gaussians as intrinsic 4D primitives that evolve continuously in time, allowing it to capture complex and periodic dynamics. For photorealistic rendering, Fourier basis functions are incorporated into the appearance coefficients to model high-frequency temporal variations. 3 STATIC Static 3DGS requires significant number of 3D Gaussians to maintain high fidelity. This leads to large memory and storage requirements (often over 1GB for large real-world scenes [49]). This makes deployment difficult in resourceconstrained environments like portable devices or head-mounted displays [2]. Therefore, reducing storage space and memory usage is essential to improve the scalability of 3DGS applications [69]. To this end, efficient Static 3DGS approaches can be categorized into two main directions: Parameter Compression: These approaches compress the 3D Gaussians without modifying the original 3DGS [42] model architecture. Restructuring Compression: These approaches fundamentally modify the original 3DGS [42] model architecture to obtain an efficient scene representation. 3.1 Parameter Compression Parameter Compression aims to reduce storage space and memory usage without modifying the 3DGS [42] model architecture. It can be applied to trained 3DGS models, making it flexible in various scenarios. This survey classifies Parameter Compression methods into five main strategies: Pruning: This strategy removes redundant or low-contribution 3D Gaussians based on various criteria. Attribute Pruning: This strategy simplifies Gaussian attributes by compressing specific components. Quantization: This strategy discretizes Gaussian attributes by reducing their bit precision. Entropy Coding: This strategy utilizes statistical redundancy in quantized attributes to minimize storage. Structured Compression: This strategy organizes 3D Gaussians by spatial relationships to improve efficiency. 3.1.1 Pruning. Pruning is the most fundamental approach to reducing redundant 3D Gaussians. To minimize the redundant 3D Gaussians, the original 3DGS [42] periodically resets the opacity of 3D Gaussians to low value at specific intervals and prunes Gaussians that remain below an opacity threshold after certain number of iterations (Sec. 2.1.6). However, according to Lee et al. [49], relying solely on this opacity-based control method still results in large number of redundant 3D Gaussians. To this end, the pruning methods use various criteria to eliminate these redundancies. Learnable mask-based Pruning uses learnable parameters to automatically select which 3D Gaussians to remove. Lee et al. [49] use this approach to enable end-to-end learning of pruning decisions. It considers both scale ğ’” and opacity ğœ together, avoiding the fixed thresholds used in the original 3DGS [42]. The binary mask Mğ‘› {0, 1} for the ğ‘›-th 3D Gaussian is computed as follows through the learnable mask parameter ğ‘šğ‘› [0, 1]: Mğ‘› = sg(1[Sig(ğ‘šğ‘›) > ğœ–] Sig(ğ‘šğ‘›)) + Sig(ğ‘šğ‘›) , (13) 8 Youn et al. Fig. 3. Overview of Parameter Compression strategies for Static 3DGS. These approaches reduce redundancy in the 3DGS representations without modifying the 3DGS [42] model architecture. (a) Pruning (Sec. 3.1.1) removes redundant 3D Gaussians. (b) Attribute Pruning (Sec. 3.1.2) compresses specific Gaussian attributes. (c) Quantization (Sec. 3.1.3) reduces Gaussian attribute bit precision. (d) Entropy Coding (Sec. 3.1.4) compresses quantized Gaussian attributes by exploiting statistical redundancy. (e) Structured Compression (Sec. 3.1.5) organizes 3D Gaussians by spatial relationships to improve compression efficiency. where sg() denotes the Stop Gradient operator, 1[] denotes the indicator function, and Sig() denotes the Sigmoid activation function. The scale Ë†ğ’” and opacity Ë†ğœ of the ğ‘›-th masked 3D Gaussian are calculated as follows: Ë†ğ’”ğ‘› = Mğ‘›ğ’”ğ‘› , Ë†ğœğ‘› = Mğ‘›ğœğ‘› . (14) Additionally, Lee et al. regularize the model to minimize the redundant 3D Gaussians ğ‘® through masking loss ğ¿ğ‘š: ğ¿ğ‘š = 1 ğ‘›=1 Sig(ğ‘šğ‘›) . (15) Significance Score-based Pruning methods introduce scoring functions to quantify the importance of each 3D Gaussian for scene reconstruction. LightGaussian [22] proposes Global Significance (GS) score that comprehensively SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 9 evaluates the contribution of each 3D Gaussian [26], inspired by the rendering equation Eq. (7). As shown in Fig. 3, the GS score for the ğ‘—-th 3D Gaussian is calculated as follows: ğºğ‘† ğ‘— = ğ¼ ğ‘ƒğ‘– ğ‘–=1 1[ğ‘® ğ‘— ğ’“ğ‘– ] ğœ ğ‘— ğ‘— 1 (cid:214) ğ‘˜= (1 ğœğ‘˜ ) ğ›¾ (ğšºğ‘— ) , (16) where ğ¼ denotes the total number of images in the training set, and ğ‘ƒğ‘– denotes the number of pixels in the ğ‘–-th image. The 1[ğ‘® ğ‘— ğ’“ğ‘– ] term represents whether the ray from the ğ‘–-th pixel ğ’“ğ‘– intersects the ğ‘—-th 3D Gaussian ğ‘® ğ‘— . The ğ›¾ (ğšºğ‘— ) term represents the normalized volume of the ğ‘—-th 3D Gaussian. EAGLES [26] introduces new criterion to identify inefficient 3D Gaussians. It calculates the influence ğ‘Šğ‘–,ğ‘ of the ğ‘–-th 3D Gaussian at specific pixel ğ‘, and the total influence ğ‘Šğ‘– as follows: ğ‘Šğ‘–,ğ‘ = ğ›¼ğ‘– Tğ‘– = ğ›¼ğ‘– (1 ğ›¼ ğ‘— ) , ğ‘Šğ‘– = ğ‘Šğ‘–,ğ‘ . ğ‘ (17) ğ‘– 1 (cid:214) ğ‘—=1 MesonGS [113] calculates the significance score for each 3D Gaussian by multiplying two components: view-dependent significance score ğ‘Šğ‘– from Eq. (17) and volume-based view-independent significance score [38, 112]. OMG [48] extends Eq. (17) by additionally considering the following Local Distinctiveness: Local Distinctiveness = (cid:169) (cid:173) (cid:173) (cid:171) 1 ğ¾ ğ‘— Nğ¾ ğ‘– ğ‘» ğ‘– ğ‘» ğ‘— 1 , ğœ† (cid:170) (cid:174) (cid:174) (cid:172) ğ‘– denotes the set of the ğ‘˜-Nearest Neighbors (KNN) of the ğ‘–-th 3D Gaussian and 1 ğ¾ where Nğ¾ the average difference in the appearance features ğ‘» . (18) (cid:205) ğ‘— Nğ¾ ğ‘– ğ‘» ğ‘– ğ‘» ğ‘— 1 represents Gradient-based Pruning methods calculate the importance of 3D Gaussians based on their gradient magnitudes. PUP 3DGS [33] measures 3D Gaussian significance using sensitivity score derived from the diagonal components of the Hessian. Speedy-Splat [32] reparameterizes the Hessian approximation of PUP 3DGS to reduce storage requirements. Trimming the Fat [3] and ELMGS [2] propose Gradient Aware Pruning (GAP), which uses both opacity and gradient to prune inefficient 3D Gaussians. 3.1.2 Attribute Pruning. Attribute pruning simplifies Gaussian attributes by selectively removing or compressing components that have minimal impact on visual fidelity. LightGaussian [22] uses knowledge distillation to reduce the dimension of SH coefficients ğ’‰ while maintaining visual fidelity. As shown in Fig. 3, the student model is trained to match the output of the teacher model under the same camera pose [ğ‘¹ğ’•]. Ldistill = 1 ğ‘ƒğ‘– ğ‘ƒğ‘– ğ‘–=1 ğ‘ª teacher (ğ’“ğ‘– ; [ğ‘¹ğ’•]) ğ‘ª student (ğ’“ğ‘– ; [ğ‘¹ğ’•])2 . (19) RDO-Gaussian [103] applies the masking strategy from Eq. (13) to the certain Gaussian attributes. 3.1.3 Quantization. Quantization discretizes Gaussian attributes by reducing their bit precision while maintaining visual fidelity [112]. Niedermayr et al. [71] define sensitivity ğ‘† (ğ‘) to quantify the impact of scalar attribute ğ‘ on total energy ğ¸ ğ‘— across the training images: ğ‘† (ğ‘) = 1 (cid:205)ğ¼ ğ‘–=1 ğ‘ƒğ‘– ğ¼ ğ‘—=1 ğ¸ ğ‘— ğ‘ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . (20) 10 Youn et al. Based on Eq. (20), they introduce sensitivity-aware ğ‘˜-means clustering to the covariance matrix ğšº and SH coefficients ğ’‰. Lee et al. [49] use Residual Vector Quantization (R-VQ) to quantize Gaussian attributes. LightGaussian [22, 69] applies Vector Quantization (VQ) to the SH coefficients ğ’‰ with low GS scores, as defined in Eq. (16). CompGS [69] compresses Gaussian attributes using VQ. RDO-Gaussian [103] modifies the codeword selection process in VQ to jointly minimize rate loss ğ‘Ÿğ‘–,ğ‘˜ and distortion loss ğ‘‘ğ‘–,ğ‘˜ for codeword index ğ‘˜. For example, the codeword selection for the scale parameter ğ’”ğ‘– of the ğ‘–-th 3D Gaussian is defined as follows: ğ‘— = arg min ğ‘˜ ğ‘Ÿ (ğ‘  ) ğ‘–,ğ‘˜ ğœ† (ğ‘  ) + ğ‘‘ (ğ‘  ) ğ‘–,ğ‘˜ = arg min ğ‘˜ log ğ‘ğ‘˜ ğœ† (ğ‘  ) (cid:16) ğ‘ ğ‘– CB(ğ‘  ) [ğ‘˜] (cid:17) + . (21) SizeGS [112] introduces size estimator which establishes relationship between hyperparameters and compressed file size. This relationship guides Hierarchical Mixed Precision Quantization (H-MPQ) for Gaussian attributes, which applies different quantization precision levels to each attribute. FlexGaussian [95] proposes training-free compression method that combines mixed-precision quantization and attribute-discriminative pruning. This approach compresses 3DGS models without retraining. 3.1.4 Entropy Coding. Entropy coding utilizes statistical redundancy in quantized Gaussian attributes to minimize storage requirements. This strategy adaptively compresses scene representation by utilizing statistical patterns. Subsequent methods are particularly effective for 3DGS compression as quantized Gaussian attributes often exhibit high statistical redundancy. Niedermayr et al. [71] use the DEFLATE algorithm [19] to compress codebook indices. RDO-Gaussian [103] compresses the indices obtained from Eq. (21) using arithmetic coding. HAC [14] models quantized anchor attributes Ë†ğ’‡ ğ‘ as Gaussian distribution ğœ™ğğ‘,ğˆğ‘ based on empirical observations. Specifically, as shown in Fig. 3, the probability of the quantized anchor features for the ğ‘–-th anchor ğ‘ ( Ë†ğ’‡ ğ‘– ğ‘) is calculated as follows: ğ‘ ( Ë†ğ’‡ ğ‘– ğ‘) = Ë†ğ’‡ ğ‘– ğ‘ + 1 ğ‘ğ‘– Ë†ğ’‡ ğ‘– ğ‘ 1 2 = Î¦ ğ‘,ğˆğ‘– ğğ‘– ğ‘ ğ‘ğ‘– (cid:18) Ë†ğ’‡ ğœ™ ğ‘,ğˆğ‘– ğğ‘– ğ‘ (ğ‘¥)ğ‘‘ğ‘¥ ğ‘– ğ‘ + (cid:19) ğ‘ğ‘– 1 2 Î¦ ğ‘,ğˆğ‘– ğğ‘– ğ‘ (cid:18) Ë†ğ’‡ ğ‘– ğ‘ (cid:19) , ğ‘ğ‘– 1 2 (22) ğ‘– ğ‘, ğˆ ğ‘– ğ‘ = MLPğ‘ (ğ’‡ ğ ğ‘– â„) , where ğ‘ğ‘– denotes the adaptive quantization step size for the attribute of the ğ‘–-th anchor, ğ’‡ ğ‘– â„ represents the interpolated hash features, and MLPğ‘ (ğ’‡ ğ‘– â„ to predict the mean ğğ‘– ğ‘ of the distribution of the quantized anchor attributes. FCGS [13] compresses 3DGS using single forward pass, avoiding the scene-specific optimization process. This framework utilizes Multi-path â„) is Multi-layer Perceptron (MLP) that utilizes the interpolated hash features ğ’‡ ğ‘– ğ‘ and standard deviation ğˆğ‘– Entropy Module (MEM) to compress Gaussian attributes, trading off size and visual fidelity. Additionally, it uses Interand Intra-Gaussian Context Models to effectively eliminate redundancy among the irregular Gaussian blobs. 3.1.5 Structured Compression. Structured Compression addresses the compression inefficiency of the original 3DGS [42], where existing compression techniques [1] achieve poor efficiency due to difficulty in identifying redundancy among unstructured 3D Gaussians. These methods organize 3D Gaussians based on spatial relationships to improve compression efficiency. Specifically, Morgenstern et al. [68] introduce the Parallel Linear Assignment Sorting (PLAS) algorithm to organize high-dimensional Gaussian attributes into 2D grid, grouping spatially adjacent 3D Gaussians expected to have similar attributes. Octree-GS [81] utilizes an Octree [66] structure to dynamically select Level-of-Detail (LoD) 3D Gaussians, enabling consistent real-time rendering for large-scale scenes. Papantonakis et al. [73] use adaptive SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 11 Fig. 4. Overview of Restructuring Compression strategies for Static 3DGS. These approaches fundamentally modify the 3DGS model architecture to achieve an efficient scene representation. (a) Anchor-based Hierarchical Structure methods (Sec. 3.2.1) introduce hierarchical representation to 3DGS [42] using sparse anchors. (b) Neural Network Integration methods (Sec. 3.2.2) replace 3DGS representations with neural networks. (c) Geometric Structure-aware methods (Sec. 3.2.3) exploit geometric properties. voxelization by computing the world-space volume of single pixel and using 3D Gaussian overlap frequency within this volume as spatial redundancy metric. MesonGS [113] utilizes an Octree structure to voxelize the 3D Gaussian positions ğ and uses RAHT (Region Adaptive Hierarchical Transform) to compress the Gaussian attributes in adjacent voxels. HGSC [38] compresses 3D Gaussian positions ğ using an Octree structure and uses multi-level attribute compression strategy. GoDe [84] proposes model-agnostic scalable compression framework that constructs multilevel 3D Gaussian hierarchy, enabling dynamic adjustment of detail and compression rates without retraining. Wang et al. [102] introduce an adaptive voxelization algorithm utilizing transform coding tools developed for point cloud compression [28]. GHAP [105] compresses Gaussian attributes through block-wise Gaussian Mixture Reduction (GMR) based on KD-tree [7] partitioning. HRGS [52] introduces hierarchical block-level optimization, enabling high-quality, high-resolution 3D reconstruction even with limited GPU memory. 3.2 Restructuring Compression Restructuring Compression aims to fundamentally modify the original 3DGS [42] model architecture to obtain an efficient scene representation. This survey classifies Restructuring Compression methods into three main strategies: Anchor-based Hierarchical Structure methods: This strategy addresses the lack of hierarchical representation in the original 3DGS [42] by utilizing sparse anchor representations. Neural Network Integration methods: This strategy achieves an efficient scene representation by replacing the original 3DGS representations with neural networks. Youn et al. Geometric Structure-aware methods: This strategy achieves an efficient scene representation by utilizing scene geometric properties. 3.2.1 Anchor-based Hierarchical Structure Methods. Anchor-based Hierarchical Structure methods utilize the sparse anchor representations derived from the voxelization process as the structural foundation. Scaffold-GS [62] proposes hierarchical scene representation based on anchors to address the lack of hierarchical structure in the original 3DGS [42], which results in excessive redundant 3D Gaussians. The framework uses the point clouds ğ‘· Rğ‘ 3 to voxelize the scene as follows: ğ‘½ = (cid:8)Floor (cid:0)(cid:12) (cid:1) (cid:9) ğœ–, where ğœ– is value that represents the size of the voxel ğ‘½ , and (cid:12) ğ‘· ğœ– Floor() is the floor function, which rounds down to the nearest smaller integer. Each voxel ğ‘½ serves as an anchor ğ’™ ğ‘£ ğ‘½ , which is assigned the following attributes: local context features ğ’‡ ğ‘£ R32, scaling factors ğ’ ğ‘£ R3, and ğ‘˜ learnable offsets ğ‘¶ ğ‘£ Rğ‘˜ 3. The framework produces ğ‘˜ neural Gaussians from each anchor ğ’™ ğ‘£ as follows: (cid:12) (cid:12) {ğ0 , . . . , ğğ‘˜ 1} = ğ’™ ğ‘£ + {O0, . . . , Oğ‘˜ 1} ğ’ ğ‘£ . (23) Subsequently, the anchor attributes {ğ’‚0, . . . , ğ’‚ğ‘˜ 1} of each neural Gaussian are predicted using an MLP ğ¹ğ’‚. The MLP takes the view-dependent features Ë†ğ’‡ ğ‘£, the relative camera-anchor distance ğ›¿ğ‘£ğ‘ , and the view direction ğ’…ğ‘£ğ‘ as input, as follows: {ğ’‚0, . . . , ğ’‚ğ‘˜ 1} = ğ¹ğ’‚ (cid:16) Ë†ğ’‡ ğ‘£, ğ›¿ğ‘£ğ‘, ğ’…ğ‘£ğ‘ (cid:17) . (24) The view-dependent features Ë†ğ’‡ ğ‘£ is weighted sum of multi-resolution local context features, where the weights depend on the relative distance ğ›¿ğ‘£ğ‘ and the view direction ğ’…ğ‘£ğ‘ . This approach enables the prediction of anchor attributes to be robust to changes in resolution and view direction. HAC [14] uses hash features derived from the interpolation of each anchor in hash grid as contexts to estimate the probability of each quantized anchor attribute, enabling efficient entropy coding. CompGS [60] uses hierarchical hybrid primitive structure that employs small set of anchor primitives to predict the other primitives. ContextGS [107] divides the anchors into multiple hierarchical levels using bottom-up voxelization strategy, as shown in Fig. 4. Subsequently, it introduces an autoregressive model that effectively predicts undecoded anchors by utilizing the already decoded anchors from the lower levels. HEMGS [58] achieves hybrid lossy-lossless compression of anchor attributes using Hybrid Entropy Model (HEM). The framework incorporates variable-rate predictor for lossy compression and combines hyperprior network with an autoregressive network for improved lossless entropy coding. CAT-3DGS [121] utilizes Triplane-based hyperprior and Spatial Autoregressive Model (SARM) to leverage the spatial inter-correlation among Gaussian primitives. It introduces Channel-wise Autoregressive Model (CARM) to utilize the channel-wise intra-correlation within individual primitives. PCGS [12] uses progressive masking to increase anchor quantity and progressive quantization with level-wise context modeling to refine anchor quality, enabling progressive bitstream generation. TC-GS [106] uses KNN to predict the distribution of Gaussian attributes based on the Tri-plane. SHTC [114] uses Karhunen-LoÃ¨ve Transform (KLT)-based layer for data decorrelation and sparsity-guided enhancement layer for residual compression. 3.2.2 Neural Network Integration Methods. Neural Network Integration methods modify the original 3DGS [42] model architecture by replacing the original 3DGS representations with neural networks. This strategy utilizes the generative capacity of neural networks to compress Gaussian attributes. EAGLES [26] introduces an MLP decoder, denoted as ğ· : Zğ‘™ Rğ‘˜ , which serves to decode the quantized latent vectors into actual attributes A. As shown in Fig. 4, the framework projects the uncompressed attributes ğ’‚ into the latent space. This projection applies the inverse decoder function ğ· 1 to obtain Ë†ğ’’ = ğ· 1 (ğ’‚). The framework rounds Ë†ğ’’ to the nearest integer ğ’’ and reconstructs SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting the attributes as ğ’‚ = ğ· ( ğ’’). NeuralGS [93] performs clustering based on the similarity of Gaussian attributes and assigns small MLP to each cluster to encode the Gaussian attributes. Liu et al. [59] introduce the Mixture of Priors (MoP) and Coarse-to-Fine Quantization (C2FQ) strategies. The MoP strategy uses multiple lightweight MLPs with gating mechanism to generate diverse prior features, improving conditional entropy modeling accuracy. 3.2.3 Geometric Structure-aware Methods. Geometric Structure-aware methods modify the original 3DGS [42] model architecture to utilize the geometric properties of the scenes. This approach achieves more efficient representation with high visual fidelity by incorporating geometric priors into scene representation. SAGS [100] applies curvature-aware densification to COLMAP [85] point clouds to populate under-represented areas. It then introduces structureaware encoder that processes these densified points, utilizing Graph Neural Networks (GNNs) on local-global graph representation to learn robust structural features. Mini-Splatting [23] reconstructs the spatial distribution of 3D Gaussians through densification with Blur Split and Depth Reinitialization. Specifically, Depth Reinitialization determines the Gaussian depth as the mid-point of the two intersection points between the ray of each pixel and 3D Gaussian, as shown in Fig. 4. These new points are reprojected into world space, contributing to the improvement of the inefficient spatial distribution. 3D-HGS [53] addresses the limitations of the original 3DGS [42] in modeling shape and color discontinuities by introducing novel 3D Half-Gaussian (3D-HGS) kernel. The kernel efficiently captures higher frequency information by using different opacity values for each half. 4 DYNAMIC As 3DGS [42] has shown remarkable success in static scene representation and real-time rendering, natural yet challenging extension lies in modeling dynamic scenes. Real-world environments often contain non-rigid motions, occlusions, and temporally varying geometry and appearance. However, most early 3DGS methods are designed under the static scene assumption, which makes them unsuitable for applications such as video-based rendering, AR/VR, or autonomous driving. To bridge this gap, recent approaches [57, 63] have extended the 3DGS framework to handle temporal dynamics. The primary challenge in this domain is achieving compact and efficient representation that captures temporal variations while keeping computational and memory overhead low. Specifically, dynamic scenes demand additional storage to represent temporal variations across time [47], which scales linearly or exponentially depending on the granularity and duration of motion being modeled. To address this, existing methods that address dynamic 3DGS typically fall into two broad categories. Following the same taxonomy introduced in the static setting, we analyze dynamic 3DGS methods through the lens of two complementary directions: (1) Parameter Compression and (2) Restructuring Compression. This dichotomy serves as unified framework in our survey to assess how efficiently different approaches represent both static and dynamic scenes under common goal of reducing redundancy: Parameter Compression: These approaches focus on reducing redundancy in the spatio-temporal domain using pruning, quantization, and entropy-based methods, independent of the model architecture. Restructuring Compression: These approaches reduce representation size by reusing shared structures across frames through architectural designs such as anchor-based modeling or learned deformation fields. Youn et al. Fig. 5. Overview of Parameter Compression strategies for Dynamic 3DGS. These approaches reduce redundancy in the Gaussian representation without modifying the rendering architecture. (a) Attribute Pruning (Sec. 4.1.2) removes temporally inactive components from 4D Gaussians, reducing them to 3D Gaussians that capture the invariant spatial attributes. The pruned 3D Gaussians are then combined with time-dependent 4D Gaussians at specific timestamps to reconstruct dynamic scenes while preserving spatial fidelity. (b) Gaussian Pruning (Sec. 4.1.1) discards less-contributing Gaussians based on temporal importance, followed by Quantization (Sec. 4.1.3), which discretizes Gaussian parameters to achieve compactness while preserving rendering quality. 4.1 Parameter Compression Parameter compression methods aim to reduce redundancy in the Gaussian representation without modifying the core rendering architecture. These techniques typically operate at the level of individual Gaussians or their attributes, making them lightweight and easily integrated into existing 3DGS pipelines. We categorize recent parameter compression approaches as the following subtypes: Gaussian Pruning: This strategy eliminates low-contributing Gaussians based on temporal activity. Attribute Pruning: This strategy removes low-impact Gaussian attributes. Quantization: This strategy discretizes Gaussian parameters. Entropy-based: This strategy exploits entropy in dynamic 3DGS. These methods serve as effective post-processing or training-time tools to trim over-parameterized models while maintaining high rendering quality, as summarized in Fig. 5. 4.1.1 Gaussian Pruning. Gaussian pruning strategies have been widely explored in the field of static 3DGS to remove Gaussians with minimal visual impact [22, 25, 49, 71]. These approaches typically assess importance based on spatial metrics or image-space contributions, which refers to the projected influence of 3D Gaussian on the 2D image plane from given camera viewpoint. However, extending such methods to dynamic scenes introduces additional SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 15 challenges [40, 50]. For instance, the pruning method, Lee et al. [49] described in static section utilizes Eq. (13) and (14) to prune unnecessary Gaussians. Subsequently, simple loss function (15) is employed to encourage the mask parameter ğ‘šğ‘› to be as small as possible, thereby enabling effective pruning. In dynamic scenes, as Gaussians move over time, the importance of individual Gaussians can also change. This method does not account for temporal variations, which can lead to Gaussian deemed important in one frame suddenly becoming unnecessary in the next, causing rendering quality degradation or unstable pruning. To address this, TC3DGS [40] introduces time attribute ğ‘¡ and defines an additional mask consistency loss function: Lğ‘šğ‘ = ğ‘ ğ‘›=1 ğ‘šğ‘›,ğ‘¡ sg(ğ‘šğ‘›,ğ‘¡ 1). (25) This Lğ‘šğ‘ forces ğ‘šğ‘›,ğ‘¡ to remain similar to ğ‘šğ‘›,ğ‘¡ 1, thereby ensuring that the importance of the Gaussians is learned in temporally stable manner. This loss is then added to the basic loss (15) to implement the final loss function. This allows the model to better grasp the global importance of Gaussians and effectively prune unnecessary ones, maximizing storage space and rendering efficiency. Building upon the foundation described in static section, Speedy-Splat [32] computes per-Gaussian sensitivity score by aggregating gradients across all static camera poses ğœ™. SpeeDe3DGS [97], designed for dynamic scenes, extends this idea with the Temporal Sensitivity Pruning Score. Since Gaussians deform and change their contributions over time ğ‘¡, it computes time-dependent gradients for the rendered image ğ¼ Gğ‘¡ (ğœ™) and aggregates them across both camera poses and timestamps, enabling more stable and effective pruning over the dynamic sequence. In this direction, 4DGS-1K [120] further advances pruning by jointly evaluating temporal and spatial scores. In particular, its temporal score quantifies the lifespan of each Gaussian along the time axis. By modeling the temporal opacity function ğ‘ğ‘– (ğ‘¡) and examining its second derivative, the method captures how steadily Gaussian persists over time or how abruptly it appears or disappears, enabling more informed temporal pruning. Also, Ex4DGS [47] introduces pruning strategy that leverages point backtracking, which traces image-space errors back to the responsible Gaussians. After computing pixel-wise errors by comparing rendered and GT images, the method exploits the backward pass to estimate each Gaussians contribution to the overall error. These contributions are weighted by opacity and accumulated transmittance to reflect their actual impact on pixels, and then averaged across all training views to obtain global error threshold Eğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ . Pruning is performed at predefined steps, where Gaussians with errors exceeding Eğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ are removed. Another line of pruning-based strategies focuses on handling newly appearing objects while maintaining memory efficiency. 3DGStream [90] assigns new Gaussians to emerging objects and applies Adaptive 3DG Quantity Control. When the view-space positional gradient, which measures how sensitively pixel colors change with respect to Gaussians 2D projection, exceeds threshold, new Gaussians are created, whereas those with gradients below the threshold are discarded. This allows the model to flexibly represent new content while preventing uncontrolled growth. Complementarily, Instant4D [64] reduces redundancy and mitigates self-occlusion by partitioning the world space into regular voxel grid and retaining only the centroid within each occupied voxel, keeping the representation compact during dynamic updates. 4.1.2 Attribute Pruning. Attribute pruning focuses on removing specific attributes or parameters of Gaussians that are redundant or have minimal impact on the scene representation. notable example is Hybrid 3D-4DGS [72], which prunes time attributes ğ‘¡ from the 4D Gaussians. The core idea of Hybrid 3D-4DGS is hybrid representation that models 16 Youn et al. scene by decomposing it into static and dynamic regions. It begins with complete 4D Gaussian representation and then identifies Gaussians that do not change over time. These static 4D Gaussians are converted into 3D Gaussians, which effectively removes their temporal dimension parameters. This significantly reduces the total number of parameters, leading to lower memory consumption and faster training. Unlike prior works that often identify static and dynamic content by analyzing the flow of Gaussians [47, 61, 69], Hybrid 3D-4DGS leverages 4D coordinate system. It uses time-axis scale parameter, exp(ğ‘ ğ‘¡,ğ‘– ), for each Gaussian. If this temporal scale exceeds predefined threshold ğœ, the Gaussian is classified as static and its time-attributes are pruned. 4.1.3 Quantization. Quantization is core parameter compression technique that converts continuous, high-precision values into discrete, low-precision representation. Vector Quantization (VQ) is specific and powerful form of quantization that exploits the inherent redundancy in 3DGS parameters. It groups similar parameters into few clusters, stores the cluster centers in codebook, and then replaces the original parameters with the index of their corresponding cluster. This process significantly reduces storage, as millions of Gaussians can be represented with just the codebook and set of indices. VQ is widely used parameter compression technique in various applications, including deep network compression [16, 27] and generative models [30, 79, 99]. This demonstrates its proven efficacy in handling large-scale, redundant data, which is precisely the challenge posed by 3DGS. While initially developed for static scenes [22, 49, 69, 71], VQ is crucial for dynamic 3DGS due to the massive storage requirements of temporal data. The core principle remains the same, but the challenge lies in applying it to dynamic parameters, such as temporal deformation fields. Not all Gaussian parameters equally affect rendering quality, making uniform quantization inefficient. Sensitivitybased quantization allocates higher bit precision to more critical parameters and lower precision to less important ones. In dynamic 3DGS, some parameters are more sensitive to temporal changes. TC3DGS [40] addresses this with gradientaware mixed-precision scheme that measures each parameters sensitivity via its gradient magnitude. Parameters with larger gradients receive higher precision, while less impactful ones are quantized with fewer bits, balancing compression and reconstruction accuracy. 4.1.4 Entropy-based. The concept of entropy is leveraged in dynamic 3DGS in two ways. The first is as compression codec used in the final compression stage after model optimization. The second is as regularization loss during the training process to induce model sparsity. These two approaches stem from fundamentally different philosophies, and its crucial to understand the distinct advantages and disadvantages of each. Entropy is first used as compression codec in the final compression stage. key challenge with 4D Gaussians is that each frame consumes storage comparable to keyframe, causing high memory usage for long sequences. HiFi4G [41] addresses this with residual compensation, quantization, and entropy encoding. It exploits the small differences between adjacent frames by computing residuals between each Gaussian and its keyframe attributes, which are often near zero. These residuals are quantized and encoded using Ranged Arithmetic Numerical System (RANS) [20], which efficiently compresses skewed frequency distributions. The encoded integer stream can be decoded to reconstruct the original attributes, enabling HiFi4G to achieve real-time compression and decompression even for long sequences. The use of entropy as regularization loss can be seen in MEGA [125], which introduces an entropy-constrained Gaussian deformation technique to enhance the utilization of each Gaussian and reduce the total number required. Conventional Yang et al.s 4DGS method [119] often assumes that 4D Gaussians exhibit only linear motion over time with constant covariance. Additionally, temporal decay opacity ensures that each Gaussian is only visible during specific time, with less than 10% of Gaussians participating in the rendering at any given moment. To overcome these SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 17 limitations, MEGA improves the flexibility of Gaussian motion and geometric structure. It then introduces spatial opacity-based entropy loss to encourage each Gaussians spatial opacity ğœ to be close to 0 or 1: Lğ‘œğ‘ğ‘ = 1 ğ‘ ğ‘ ğ‘—=1 (ğœ ğ‘— log(ğœ ğ‘— )). (26) Periodically during the optimization process, Gaussians with near-zero opacity are aggressively pruned, as they are considered to not be contributing to the scenes representation. This reduces the models memory footprint and increases computational efficiency. 4.2 Restructuring Compression While parameter compression methods reduce redundancy at the level of individual Gaussians or their attributes, restructuring compression techniques aim to achieve more efficient representation by modifying the underlying architecture itself. These methods are architecture-dependent in that they introduce additional structural priors or modules that enable more efficient modeling of dynamic scenes. Specifically, recent restructuring compression methods can be broadly categorized into three major approaches: Anchor-based Representation: This strategy compresses dynamic content using representative keyframes. Canonical Deformable Representation: This strategy models dynamic scenes by establishing canonical space and deformation field. LoD Representation: This strategy organizes Gaussians into multi-resolution hierarchies. These strategies reduce parameters, improve temporal consistency, and provide inductive biases that enhance generalization in dynamic environments, as summarized in Fig. 6. In the following subsections, we review representative methods in each category. While we group them for clarity, recent works [45, 127] often combine multiple techniques. Many state-of-the-art methods adopt hybrid designs that leverage the strengths of different paradigms, so these categories should be seen as conceptual guides rather than strict boundaries. 4.2.1 Anchor-based Representation. Recent studies [14, 60, 62, 107] have successfully introduced more compact 3DGS frameworks using anchor-based representations. This approach assigns implicit features to sparse anchor points, which are then used to predict the attributes for broader set of neighboring 3D Gaussians. Building on this natural progression, anchor-based frameworks for static scenes are being actively extended to dynamic environments to address the significant storage overhead caused by the temporal dimension. Anchor-based approach represents dynamic scenes complex deformations by using small number of hierarchical control points or anchors to represent large number of Gaussians. These control points increase in density in areas with complex motion, allowing the model to effectively capture even subtle non-rigid deformations [101]. MoSca [51] separates the 3D geometry and motion of dynamic scene using representation called 4D Motion Scaffolds. These scaffolds compactly encode the underlying deformations of the scene, while 3D Gaussians are anchored to them to represent the appearance. This enables the model to globally encode information from all video frames into single, consistent representation. EDGS [44] models dynamic scenes efficiently through sparse anchor-grid representation. This approach decomposes time-invariant and time-variant properties. In particular, it employs an unsupervised learning strategy to effectively filter out anchors corresponding to static regions, while querying time-variant attributes by feeding only anchors related to dynamic objects into an MLP. Another notable method is MoDec-GS [45], introducing Global-to-Local Motion Decomposition (GLMD) to capture both global and local movements. This approach extends 18 Youn et al. Fig. 6. Overview of Restructuring Compression strategies for Dynamic 3DGS. These approaches achieve compact yet expressive representations by restructuring the underlying 3DGS architecture to better encode temporal and spatial variations. (a) Anchor-based and Canonical Deformable Representation (Sec. 4.2.1, Sec. 4.2.2) combines anchor-based referencing with canonicaldeformable formulation. small set of anchor Gaussians serve as spatial references, where each anchor encodes local context attributes and deformation parameters that map the canonical static space to dynamic frames over time. (b) LoD Representation (Sec. 4.2.3) organizes Gaussians into multi-resolution hierarchy, progressively refining spatial detail through level-wise deformation and spawning static scaffold representations to dynamic video reconstruction by using Global Canonical Scaffolds and Local Canonical Scaffolds. HAIF-GS [11] employs sparse motion anchors as deformation units to reduce redundant computation and improve efficiency. Its Hierarchical Anchors Densification (HAD) adaptively refines anchor resolution in motioncomplex regions for fine-grained deformations. An Anchor Filter predicts dynamic confidence scores to suppress redundant updates in static areas, while the Induced Flow-Guided Deformation (IFGD) module aggregates multi-frame features to induce scene flow in self-supervised manner, regularizing anchor transformations. SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 19 4.2.2 Canonical Deformable Representation. This approach represents dynamic scene as set of static 3D Gaussians defined in canonical space and deformation field that maps points from this canonical space to the time-dependent real-world by implicitly or explicitly. Efficiency is achieved by inducing dynamic changes through the deformation field, rather than storing all Gaussians for every timestamp [63]. Implicit Deformation learns time-dependent deformations from the canonical space using MLP or grid-based architectures. Instead of defining rigid, explicit function, the deformation field is modeled as neural network that infers the transformation for each Gaussian at given time. This allows for the capture of complex, non-rigid movements with high fidelity. One of the early works in this field, Deformable 3DGS [118] introduces an implicit deformation field modeled by an MLP network, which, at each timestamp, takes 3D Gaussians position and the corresponding time ğ‘¡ as input to predict offsets for its position ğ, orientation r, and scale s. Wu et al. 4DGS [109] takes different approach by maintaining single set of canonical 3D Gaussians and transforming them at each timestamp via Gaussian deformation field. This field, comprising temporal-spatial structure encoder and multi-head Gaussian deformation decoder, leverages multi-resolution HexPlane [9], representation for 4D volumes that uses six planes of learned features to efficiently compute features for spatio-temporal points, thereby efficiently modeling Gaussian motion and shape changes. Building upon these, DeformGS [21] focuses on dynamic 3D tracking using multiple cameras, employing Neural-voxel Encoding combined with an MLP to infer Gaussians position, rotation, and even shadow scalar. This scalar is value between 0 and 1 that represents the shadow intensity. This approach further incorporates regularization terms based on conservation of momentum and isometry to reduce trajectory errors. Explicit Deformation models dynamic scenes using mathematically predefined functions to represent motion trajectories and temporal changes. Instead of relying on neural network to predict transformations, this approach directly parameterizes motion with set of explicit variables, which can include polynomial coefficients or keyframebased interpolators. This strategy often leads to more compact representations by avoiding the need for deep neural network. For example, Li et al. [57] model dynamic scenes using an explicit deformation approach that achieves compactness through low-order polynomial functions. The position ğ and orientation of each Gaussian are represented by these polynomials, with motion trajectories inspired by [31, 42] and rotations by [36, 39]. Representing time-varying motion with small set of polynomial coefficients greatly reduces parameters compared to per-frame modeling, making the parameter count proportional to the number of Gaussians rather than frames. While other techniques like splatted feature rendering aid compactness, the core efficiency of Li et al.s method lies in its explicit polynomial representation of dynamic motion. Another approach that uses explicit deformation is Ex4DGS [47], which efficiently represents the motion of dynamic 3DGS by employing keyframe-based interpolation. Instead of storing motion information for every frames, Ex4DGS explicitly stores the position and rotation of Gaussians only at small number of keyframes. The scenes motion at any given timestamp is then synthesized by interpolating between adjacent keyframes. For the Gaussians position ğ, it uses Cubic Hermite Spline (CHip) [5], which provides smooth and continuous interpolation ğœ‡ (ğ‘¡) = CHip(ğ‘ğ‘›, ğ‘šğ‘›, ğ‘ğ‘›+1, ğ‘šğ‘›+1; ğ‘¡ ). (27) Here, ğ‘ğ‘› is the position at the ğ‘›-th keyframe and ğ‘šğ‘› is its tangent vector, which is calculated based on the position change between keyframes to avoid extra storage. For the Gaussians rotation, it uses Spherical Linear Interpolation (Slerp) [87] to ensure consistent and bias-free interpolation of the quaternion ğ’’, ğ‘(ğ‘¡) = Slerp(ğ‘Ÿğ‘›, ğ‘Ÿğ‘›+1; ğ‘¡ ), (28) 20 Youn et al. where ğ‘Ÿğ‘› is the rotation at the ğ‘›-th keyframe. By using these explicit interpolators and keyframes, Ex4DGS avoids the need to store information for every single timestamp, which significantly reduces the model size. This approach effectively balances expressiveness for complex motion with the practicality of minimal memory overhead. 4.2.3 LoD Representation. As we described in the static section, LoD representation contributes to compact and efficient 3DGS frameworks by dynamically adjusting the level of detail of objects based on the viewers perspective, distance, and importance. This makes it valuable for various applications like gaming [8, 91] and virtual reality [86, 123]. Inspired by scalable video encoding [29, 67], Scale-GS [117] employs LoD techniques with deformation and spawning to build compact and efficient dynamic 3DGS framework. It introduces multi-scale Gaussian representation, where large Gaussians capture coarse structures and small ones refine high-frequency details, improving efficiency by avoiding unnecessary optimization. Scale-GS hierarchically partitions the scale space through recursive binary splitting. From the initial frame, it estimates each Gaussians scale and defines the maximum, minimum, and mean scales (ğ‘  (ğ‘™ ) ğ‘šğ‘’ğ‘ğ‘›) for each level ğ‘™. If finer level ğ‘™+1 is required, the scale range is divided as: ğ‘šğ‘–ğ‘›, ğ‘  (ğ‘™ ) ğ‘šğ‘ğ‘¥, ğ‘  (ğ‘™ ) max] [ğ‘  (ğ‘™ ) , ğ‘  (ğ‘™ ) max ] [ğ‘  (ğ‘™ ) , ğ‘  (ğ‘™+1) This hierarchy enables coarse-to-fine optimization, where large-scale Gaussians first approximate the global structure [ğ‘  (ğ‘™ ) min [ğ‘  (ğ‘™+1) min ğ‘šğ‘’ğ‘ğ‘›, ğ‘  (ğ‘™ ) ğ‘šğ‘–ğ‘›, ğ‘  (ğ‘™ ) ğ‘šğ‘’ğ‘ğ‘›]. max], (30) (29) using low-resolution views, and smaller ones are later activated to refine fine details, greatly reducing redundant computation and training time. similar hierarchical principle appears in 4DGCPro [126], which decouples motion into two levels: rigid transformation capturing large object movements, and residual deformation that models non-rigid shape changes and fine-grained local dynamics. This decomposition mirrors coarse-to-fine strategy, allowing the system to first account for dominant global motion and then refine complex deformations efficiently. 5 DATASETS AND EVALUATION Evaluating efficient 3DGS and its dynamic extensions requires efficiency-oriented metrics beyond conventional rendering quality measures. While earlier studies focused primarily on visual quality (PSNR, SSIM [108], LPIPS [124]), efficient approaches target quality-efficiency trade-off. We summarize commonly used datasets and metrics from the literature and analyze the balance between visual fidelity and efficiency. 5.1 Datasets 5.1.1 Static Scene Datasets. As static scene reconstruction and novel view synthesis have established themselves as fundamental research areas in computer vision and graphics, numerous high-quality datasets have been developed to support and evaluate various methodologies. In this section, we review five representative static scene datasets widely used for 3D reconstruction. Each dataset presents unique challenges for evaluating model performance. Tanks and Temples (TNT) [43] introduces benchmark dataset for evaluating large-scale 3D reconstruction techniques. The dataset comprises real-world captures acquired using an industrial laser scanner at submillimeter accuracy. The dataset captures videos primarily using gimbal-stabilized cameras (DJI Zenmuse X5R and Sony a7S II) at 4K resolution. TNT includes 14 diverse scenes with 100400 views each, categorized into intermediate (8 outdoor) and advanced (4 indoor, 2 outdoor) groups, with images extracted at 1920 1080 resolution. Deep Blending [34] introduces comprehensive dataset of 2, 630 real photographs from 19 scenes. The dataset comprises captures from Chaurasia et al. [10] (7 scenes), Hedman et al. [35] (4 scenes), the Eth3D benchmark (1 scene), SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 21 Venue Type Modality #Views #Scenes Resolution Environment Link TNT [43] ToG17 Real Multi-view 100400 14 (10+4) 1920 1080 Mixed 2 Deep Blending [34] ToG18 Real Multi-view 12418 19 (14+5) 12282592 8161944 Mixed 2 NeRF-Synthetic [6] BungeeNeRF [111] Mip-NeRF 360 [4] ECCV22 Mixed Multi-view 220463 12 (12+0) N/A Outdoor CVPR22 Real Multi-view 100330 9 (5+4) 4946 3286 Mixed 2 ECCV20 Synthetic Multi-view 300 8 (0+8) 800 800 Indoor 2 Table 1. Summary of representative static 3DGS datasets. Type indicates whether scenes are real-world or synthetic. Modality specifies the camera configuration (monocular vs. multi-view). #Views denotes the number of viewpoints per scene. #Scenes represents the total number of scenes, where red indicates outdoor and blue indicates indoor scenes. Resolution indicates image dimensions in pixels (width height). Environment describes the scene setting (indoor vs. outdoor). and newly captured scenes (7 scenes). Each scene contains 12418 images from different viewpoints, spanning 5 indoor and 14 outdoor environments with resolutions ranging from 1228 816 to 2592 1944 pixels. NeRF-Synthetic [6] presents synthetic dataset of eight path-traced objects designed to evaluate novel view synthesis on complex geometry and realistic non-Lambertian materials. The dataset employs the Blender Cycles renderer for high-fidelity physically-based rendering. Each scene provides 100 training views and 200 test views. The eight objects (Chair, Drums, Ficus, Hotdog, Lego, Materials, Microphone, and Ship) exhibit complex geometry and intricate appearance properties. All scenes feature object-centric indoor setups in controlled studio environments with neutral backgrounds, rendered at 800 800 pixel resolution. BungeeNeRF [111] presents multi-scale datasets for novel view synthesis in extreme multi-scale scenarios. The collection includes synthetic data from Google Earth Studio [6] and Blender, and real-world UAV captures. The dataset contains twelve outdoor cities (New York, San Francisco, Sydney, Seattle, Chicago, Quebec, Amsterdam, Barcelona, Rome, Los Angeles, Bilbao, and Paris), plus additional landscape and Blender-synthetic environments. Each city scene provides approximately 220463 multi-viewpoint images along the camera trajectory. Mip-NeRF360 [4] introduces real-world dataset for evaluating novel view synthesis in unbounded 360-degree scenes. The dataset captures 9 scenes with 100330 images per scene. The dataset comprises images captured using Sony NEX C-3 for 5 outdoor scenes (bicycle, flowers, garden, stump, treehill) and Fujifilm X100V for 4 indoor scenes (room, counter, kitchen, bonsai), with an average image resolution of 4946 3286 pixels. 5.1.2 Dynamic Scene Datasets. As dynamic 3DGS has become popular research area, wide variety of datasets have been introduced to support the rapid pace of development. In this section, we will review five representative datasets that are commonly used across the dynamic 3DGS literature previously discussed. These datasets are frequently cited as benchmarks for evaluating the performance of new methods. Technicolor Dataset [83] is multi-view light-field video collection of various dynamic scenes, including close-ups of human faces and animated objects. It is captured using synchronized 4 4 camera grid at 30 frames per second with high resolution of 2048 1088 pixels. The datasets precise synchronization and calibration make it strong benchmark for validating fine details and complex textures in dynamic 3DGS methods. However, key consideration is that each sequence has unique shift and calibration tables, requiring specific post-processing pipeline for proper use. D-NeRF [76] is synthetic extension of static NeRF [6] benchmark, designed for dynamic scenes. It features eight scenes with large deformation and non-Lambertian materials, which are surface whose color and brightness 22 Youn et al. Venue Type Modality #Views #Scenes #Frames Resolution Environment Link Technicolor [83] D-NeRF [76] HyperNeRF [74] SIGGRAPH Asia21 CVPR21 Synthetic Real Monocular Multi-view 100200 12 7 (0+7) 8 (2+6) 450900 50200 1980 1080 800 800 Indoor Mixed 2 2 CVPR17 Real Multi-view 16 11 (0+11) 150300 2048 1088 Indoor N3DV [56] NeRF-DS [116] CVPR22 Real Multi-view 1821 6 (0+6) 300 2704 2028 Indoor 2 CVPR23 Real Multi-view 2 8 (0+8) 500 480 270 Indoor 2 Table 2. Summary of representative dynamic 3DGS datasets. Type indicates whether the dataset contains real-world captured scenes or synthetically generated scenes (Real vs. Synthetic). Modality specifies the camera configuration used for data acquisition (Monocular vs. Multi-view). #Views denotes the number of camera viewpoints available per scene in the dataset. #Scenes represents the total number of distinct scenes included in the dataset, where red numbers indicate outdoor scenes and blue numbers indicate indoor scenes. #Frames represents the number of frames per scene. Since each dataset contains scenes of varying lengths, the frame count is expressed as an approximate range. Resolution indicates the video resolution in pixels (width height) used for each dataset. Environment describes the setting where the scenes are captured or created (Indoor vs. Outdoor). depend on the viewing direction as well as the lights direction. This enables the modeling of complex and realistic light interactions, such as those on glossy or reflective surfaces. The dataset is created by rendering 100200 multi-view still frames per scene at 800 800 resolution and arranging them into sequences over time. Its primary advantage for 3DGS is that it is clean dataset, free from real-world camera or geometric errors, making it ideal for evaluating an algorithms core dynamic deformation modeling capabilities. However, the lack of real-world noise and variations limits its use for validating generalization to real-world multi-view videos. HyperNeRF [74] is collection of video sequences captured with single moving camera, specifically designed to include scene with topology changes, such as fluids, contacts, and separation, which are lacking in prior public datasets. The videos, each lasting 3060 seconds, are subsampled to 15 fps and camera poses for all frames are estimated using COLMAP. For training, every fourth frame is used, with intermediate frames reserved for validation. From 3DGS perspective, this dataset is valuable for testing the robustness of methods that handle topology changes and non-rigid deformations. However, its performance is sensitive to failures or ambiguities in single-camera pose registration, meaning that COLMAPs alignment quality and the frame sampling strategy directly impact the final result. Neural 3D Video Dataset [56] is collection of real-world multi-camera videos that includes everyday indoor scenes, cooking, people, and challenging dynamic and optical effects like fire and steam. It also captures complex effects such as reflections, transparency, self-shadows, volumetric effects, and even topology changes like pouring liquids. This dataset is captured using 21 synchronized GoPro Black Hero 7 cameras, shooting at 2028 2704 resolution at 30 fps. Camera parameters are precisely estimated using COLMAP. Typically 18 views are used for training, and 1 view for qualitative and quantitative evaluations. From 3DGS perspective, this datasets strength lies in its high-resolution, multi-view nature, and the inclusion of challenging dynamic and optical phenomena. This makes it highly suitable for evaluating models real-world performance, as well as for comparing model compression and efficiency for long sequences. noted limitation is the potential for color inconsistencies between views due to differences in camera color correction, which may require post-processing. NeRF-DS [116] is real-world collection focused on dynamic specular objects, such as moving mirrors, metal, and glossy surfaces. It consists of eight scenes from everyday environments with various types of motion and deformation. This dataset is created specifically to address the scarcity of such dynamic specular cases in existing dynamic NeRF SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 23 datasets. The data is captured using two rigidly mounted forward-facing cameras shooting simultaneously, with each scene containing two videos of approximately 500 frames. One video is used for training, and the other is used for testing. This design avoids the unrealistic \"teleporting camera\" problem of alternating between cameras. Camera poses are registered using COLMAP, with the stability of the SfM process improved by pre-applying moving object mask obtained with MiVOS [15]. From 3DGS perspective, key advantage of this dataset is that it enables evaluation of real-world challenges such as dynamic specular highlights and reflections, foregroundbackground separation, and the role of masking moving foregrounds during pose registration. It has been quantitatively shown that omitting such masks leads to significant errors, underscoring pose sensitivity and reflection-induced breakdowns of multi-view consistency. 5.2 Evaluation Metrics To evaluate efficient 3DGS and its dynamic extensions, we consider both visual fidelity metrics and efficiency-oriented indicators. The following metrics are commonly adopted in the literature: PSNR (Peak Signal-to-Noise Ratio): PSNR quantifies the pixel-level accuracy between rendered image and the GT reference. Higher PSNR values indicate lower reconstruction error and better fidelity. SSIM (Structural Similarity Index Measure) [108]: SSIM measures perceptual similarity by considering luminance, contrast, and structural information between two images. higher SSIM score represents closer resemblance to the GT from perceptual perspective. LPIPS (Learned Perceptual Image Patch Similarity) [124]: LPIPS evaluates perceptual similarity using deep neural network features that approximate human visual judgment. Lower LPIPS values correspond to higher perceptual quality. Model Size: Model size is reported either in megabytes (MB) or in terms of the total number of Gaussians. smaller model size indicates more compact representations and better memory efficiency. Compression Ratio / Reduction Percentage: These metrics measure the degree of compactness achieved compared to the original model. higher compression ratio (or reduction percentage) reflects more effective elimination of redundancy while ideally preserving rendering quality. Training Time: It represents the total time required to optimize the model from initialization to convergence. Faster training time highlights the practicality of method, particularly for large-scale or dynamic scenes. Inference FPS (Frames Per Second): Inference speed is measured in FPS, indicating how efficiently model can render frames in real time. Higher FPS values are crucial for interactive applications such as AR/VR and robotics. 5.3 Quantitative and Visual Comparison of Gaussian Splatting Methods As shown in Fig. 7 and Fig. 8, we visualize representative static and dynamic 3DGS methods using bubble charts that jointly consider reconstruction quality (PSNR , LPIPS ), rendering speed (FPS ), and model size (bubble radius). For fair and comprehensive visualization, Mip-NeRF 360 [4] is adopted for static scenes and N3DV [56] for dynamic ones, as these are the most widely used benchmarks in each category. 5.3. Static 3DGS. As illustrated in Fig. 7, the performance differences among static 3DGS compression methods are closely associated with the key technical features adopted by each approach. MesonGS [113], which demonstrates superior performance in the upper-left region of the chart, effectively exploits spatial adjacency through voxelization 24 Youn et al. with an Octree structure [66] and RAHT-based attribute compression. Similarly, Octree-GS [81] achieves both high visual quality and efficiency in large-scale scenes due to its hierarchical structure that enables dynamic LoD selection. Scaffold-GS [62] and SAGS [100] provide the highest visual quality due to anchor-based hierarchical representation and structural feature learning via GNNs, respectively, although these complex structures require large model sizes. ELMGS [2] and Trimming the Fat [3] maintain competitive visual quality even with extremely small model sizes due to their adoption of gradient-based aggressive pruning strategies. ContextGS [107] and HAC [14] achieve an efficient quality-size tradeoff by exploiting spatial correlations through autoregressive models and hash-based entropy coding, respectively. HEMGS [58] achieves high visual quality at low bitrates through hybrid entropy models that enable lossylossless compression. CAT-3DGS [121] exploits inter-channel correlations via Triplane-based hyperpriors to achieve high visual quality. EAGLES [26] and NeuralGS [94] achieve smaller model sizes through MLP decoders and clusteringbased encoding, respectively, by directly utilizing the generative capabilities of neural networks for compression. Liu et al. [59] maintain relatively high visual quality by improving the accuracy of conditional entropy modeling through MoP strategy. CompGS [69] and Niedermayr et al. [71] apply sensitivity-based ğ‘˜-means clustering, while Lee et al. [49] apply Residual Vector Quantization, both achieving simple but effective compression. LightGaussian [22] achieves effective compression of SH coefficients through combination of knowledge distillation and VQ, enabling substantial size reduction while maintaining visual fidelity. Papantonakis et al.[73] and Morgenstern et al.[68] effectively eliminate spatial redundancy through adaptive voxelization and the PLAS algorithm, respectively, resulting in compact representations. CompGS [60] achieves efficient compression by exploiting predictive relationships between anchor and non-anchor primitives through hierarchical hybrid primitive structure. PUP 3DGS [33] attains smaller model sizes by selectively removing 3D Gaussians with low importance through Hessian-based sensitivity scores. OMG [48] improves pruning effectiveness by performing pruning that considers local distinctiveness. SizeGS [112] achieves predictable compression ratios by explicitly modeling the hyperparameter-size relationship via Size Estimator to guide mixed-precision quantization. GoDe [84] provides scalable quality control by enabling dynamic detail adjustment through progressive hierarchical structure construction via gradient-informed masking. PCGS [12] enables progressive decoding by supporting progressive bitstream generation through progressive masking and level-wise context modeling. SHTC [114] achieves efficient residual compression through the combination of KLT-based decorrelation and sparsityguided enhancement layer, maintaining high quality at reduced bitrates. FlexGaussian [95] demonstrates competitive performance without retraining due to the combination of training-free mixed-precision quantization and attribute-wise pruning, offering practical deployment advantages. RDO-Gaussian [103] maximizes compression efficiency by directly integrating rate-distortion optimization into the codeword selection process. 5.3.2 Dynamic 3DGS. As illustrated in Fig. 8, few NeRF-based methods [54, 88] and non-compact 4DGS [119] are also included for reference, where 4D Gaussian primitives achieve higher PSNR and FPS by modeling geometry explicitly. Deformable 3DGS [118] provides one of the earliest compression strategies by avoiding the need to store all Gaussian attributes for every frame. Instead, it adopts canonical space with learned deformation field, yielding substantial reduction in model size compared to per-frame representations. Building on this idea, 4DGS [109] further improves efficiency by incorporating HexPlane-based decomposed neural voxel encoding, which results in notably higher rendering speed. Subsequent works explore selective dynamic modeling. Ex4DGS [47] separates static and dynamic regions and leverages keyframe-based temporal interpolation with pruning to reduce memory footprint even further. Hybrid 3D4DGS [72] similarly performs staticdynamic decomposition, while retaining full 4D expressiveness for dynamic Gaussians to faithfully capture complex motions. Another line of research focuses on compressing spherical SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 25 Fig. 7. Performance comparison visualization graph. Bubble plot comparing static 3DGS methods on Mip-NeRF 360 [4]. The x-axis shows perceptual quality (LPIPS ), the y-axis shows reconstruction quality (PSNR in dB ), and the bubble radius is proportional to model storage size (MB ). For clarity, various compactness-oriented models are distinguished by color tones: Brick red for Pruning methods (Sec. 3.1.1), raspberry for Quantization methods (Sec. 3.1.3), vivid yellow for Structured Compression methods (Sec. 3.1.5), deep violet for Anchor-based Hierarchical Structure methods (Sec. 3.2.1), sky blue for Neural Network Integration methods (Sec. 3.2.2), and light gray for Geometric Structure-aware methods (Sec. 3.2.3). Several compact designs achieve improved PSNR and lower LPIPS with reasonable storage, demonstrating the shift toward lightweight yet high-fidelity representations. harmonics (SH), the dominant contributor to storage cost. STG [57] and MEGA [125] reduce SH complexity to obtain compact representations, with MEGA further benefiting from entropy-based regularization. 6 LIMITATIONS AND FUTURE DIRECTIONS Efficient static 3DGS and dynamic 3DGS have overcome the issue of slow training and rendering speed inherent in NeRF, enabling real-time rendering and high-quality scene reconstruction. However, their practical deployment remains challenging due to massive memory footprint and computational overhead. typical high-resolution static scene often contains millions of 3D Gaussians. Furthermore, 4D scene reconstruction demands even greater memory consumption because each 3D Gaussian must encode temporal information across multiple frames. To address these challenges, this survey categorizes and analyzes approaches for improving the efficiency of both static 3DGS and dynamic 3DGS. All research presented in this survey is categorized into two approaches: Parameter Compression and Restructuring Compression. 26 Youn et al. Fig. 8. Performance comparison visualization graph. Bubble plot comparing dynamic 3DGS methods on N3DV [56]. The x-axis shows rendering speed (FPS ), the y-axis shows reconstruction quality (PSNR in dB ), and the bubble radius is proportional to model storage size (MB ). For clarity, both compactness-oriented models (in blue tones) and non-compact baselines are shown. clear reduction in bubble size can be observed among compact models, indicating substantial decrease in storage cost. Moreover, several compact designs achieve improved PSNR while maintaining high rendering speed, revealing the direction of dynamic 3DGS research toward lightweight yet high-fidelity representations. Parameter Compression approach aims to reduce redundant 3D Gaussians or their attributes without modifying the original 3DGS [42] model architecture. This makes it flexible, as it can be applied to already trained models. For static 3DGS, parameter compression strategies can be categorized as follows: Pruning removes redundant or low-contribution 3D Gaussians. Attribute Pruning selectively removes specific components with minimal impact on rendering quality. Quantization reduces the bit precision of Gaussian attributes to minimize storage. Structured Compression organizes 3D Gaussians using spatial adjacency or hierarchical relationships to enhance compression efficiency. For dynamic 3DGS, parameter compression strategies can be categorized as follows: Gaussian Pruning eliminates low-contributing 3D Gaussians based on temporal activity or motion magnitude. Attribute Pruning eliminates temporal parameters, effectively converting some 4D Gaussians into 3D ones. Quantization applies sensitivity-based quantization methods to temporal parameters. Entropy-based Compression employs entropy-based codecs. Restructuring Compression fundamentally modifies the original 3DGS [42] model architecture to obtain an efficient scene representation. This category achieves compression through architectural redesign, such as hierarchical structures or alternative primitive representations. SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting For static 3DGS, restructuring compression strategies can be categorized as follows: Anchor-based Hierarchical Method uses sparse anchors to address the lack of hierarchical structure in the original 3DGS [42], reducing redundancy. Neural Network Integration utilizes neural networks to compress Gaussian attributes by learning compact latent representations. Geometric Structure-aware Method utilizes geometric properties of the scene to improve efficiency. For dynamic 3DGS, restructuring compression strategies can be categorized as follows: Anchor-based Representation models complex deformations through sparse anchors. Canonical Deformable Representation maps canonical static space to time-varying space using deformation field. LoD Representation builds multi-resolution hierarchies to adjust detail dynamically. Despite these advances in efficient static 3DGS and dynamic 3DGS, several critical challenges remain unresolved. This section discusses these limitations and proposes future research directions. 6.1 Hardware Optimization and Real-time Deployment For practical deployment of static and dynamic 3DGS, optimization across diverse hardware platforms is essential. Although most current efficient 3DGS methods for both static and dynamic scenes are developed and evaluated in high-performance GPU environments, real-world applications such as AR/VR headsets, mobile devices, and edge devices operate under constrained memory and computational resources. Therefore, compression techniques and rendering pipelines that account for hardware constraints must be developed. 6.2 Long-sequence Processing for Dynamic Scenes Most current dynamic 3DGS research targets relatively short video sequences. However, real-world applications require processing long sequences. In such long sequences, memory requirements grow exponentially, and maintaining temporal consistency becomes challenging. To this end, novel compression strategies that effectively exploit temporal redundancy are required. This can be addressed by extending keyframe-based interpolation methods to perform adaptive keyframe selection, or introducing hierarchical temporal encoding structures capable of modeling long-range temporal dependencies. 6.3 Semantically-aware Compression Current static and dynamic 3DGS compression techniques primarily focus on pixel-level reconstruction accuracy, without sufficiently exploiting the semantic properties of scenes. Real-world scenes contain various semantic categories such as objects, backgrounds, and materials, which can be utilized to achieve more efficient compression. Specifically, scenes can be semantically segmented to allocate higher bit budgets to important foreground objects while allocating lower bit budgets to less important backgrounds. Since 3D Gaussians belonging to the same semantic category are likely to share similar characteristics, category-specific codebooks can achieve more effective compression. In dynamic scenes, explicitly separating dynamic objects from static backgrounds and applying semantically-aware compression strategies is also promising. 28 6.4 Generalization Youn et al. Most current static and dynamic 3DGS methods employ per-scene optimization, requiring training from scratch for each individual scene. This results in substantial computational cost and training time for every new scene. In contrast, recent studies such as VGGT [104] can reconstruct 3D scenes from only few images without per-scene optimization. However, these foundation models are computationally expensive and memory-intensive, making them impractical for resource-constrained environments such as mobile devices. Future research should focus on developing lightweight compression models that preserve the generalization capability of foundation models while being compact enough for mobile deployment. In this context, single forward pass compression frameworks without per-scene optimization, such as FCGS [13], are also promising. 6.5 User-controllable Quality-efficiency Trade-offs Most current efficient static and dynamic 3DGS research targets fixed quality-efficiency trade-offs. However, real-world applications require dynamic adjustment of this trade-offs based on user requirements or execution environments. Therefore, flexible compression frameworks that enable diverse rate-distortion trade-offs without retraining are required. Recent studies such as GoDe [84] have begun exploring this direction, but more precise control over rate-distortion trade-offs remains challenging. 6.6 Reliability and Robustness Enhancement For efficient static and dynamic 3DGS models to be deployed in safety-critical applications, reliability and robustness must be ensured. Specifically, adaptive compression strategies that identify safety-critical regions and allocate higher bit budgets to 3D Gaussians in those areas are required. Furthermore, quantifying compression-induced uncertainty and applying LoD to enhance details in high-uncertainty areas can improve reliability. 7 CONCLUSION Although 3DGS enables real-time rendering through its explicit representation, its memory overhead remains one of the major challenges. Recent studies have therefore focused on designing compact and efficient representations that reduce memory usage while preserving high-fidelity scene reconstruction for both static and dynamic scenes. This survey reviews recent works that address this challenge by proposing compact and efficient 3DGS frameworks. We first introduce the core concepts of Gaussian splatting in the Preliminary section (Sec.2). Building on this foundation, the Static and Dynamic sections (Sec.3, Sec.4) systematically review recent methods that pursue compact and efficient representations under consistent criteria. We then summarize widely used datasets and evaluation metrics to support fair benchmarking across studies (Sec.5), and conclude by discussing current limitations and promising future directions in 3DGS research (Sec. 6). References [1] Jyrki Alakuijala, Ruud Van Asseldonk, Sami Boukortt, Martin Bruse, Iulia-Maria Coms, a, Moritz Firsching, Thomas Fischbacher, Evgenii Kliuchnikov, Sebastian Gomez, Robert Obryk, et al. 2019. JPEG XL Next-generation Image Compression Architecture and Coding Tools. In Applications of digital image processing XLII. SPIE. https://doi.org/10.1117/12.2529237 [2] Muhammad Salman Ali, Sung Ho Bae, and Enzo Tartaglione. 2025. Elmgs: Enhancing Memory and Computation Scalability through Compression for 3D Gaussian Splatting. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE. https://doi.org/10.1109/WACV61041. 2025.00257 SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting [3] Muhammad Salman Ali, Maryam Qamar, Sung Ho Bae, and Enzo Tartaglione. 2024. Trimming the Fat: Efficient Compression of 3D Gaussian [4] Splats through Pruning. In British Machine Vision Conference (BMVC). BMVA. https://arxiv.org/abs/2406.18214f Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. 2022. Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52688.2022. 00539 [5] Richard Bartels, John Beatty, , and Brian Barsky. 1987. An Introduction to Splines for Use in Computer Graphics and Geometric Modeling. Morgan Kaufmann Publishers Inc. https://doi.org/doi/10.5555/35072 [6] Matthew Tancik Jonathan T. Barron Ravi Ramamoorthi Ben Mildenhall, Pratul P. Srinivasan and Ren Ng. 2020. Nerf: Representing Scenes as Neural Radiance Fields for View Synthesis. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3030-58452-8_24 Jon Louis Bentley. 1975. Multidimensional Binary Search Trees Used for Associative Searching. Commun. ACM (1975). https://doi.org/10.1145/ 361002. [7] [8] David Brogan. 2000. Simulation Levels of Detail for Control and Animation. Georgia Institute of Technology. https://doi.org/doi/10.5555/931829 [9] Ang Cao and Justin Johnson. 2023. HexPlane: Fast Representation for Dynamic Scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52729.2023.00021 [10] Gaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung, and George Drettakis. 2013. Depth Synthesis and Local Warps for Plausible [11] Image-based Navigation. ACM Transactions on Graphics (ToG) (2013). https://doi.org/10.1145/2487228.2487238 Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, and Yucheng Zhang. 2025. HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://doi.org/10.48550/arXiv.2506. [12] Yihang Chen, Mengyao Li, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. 2025. PCGS: Progressive Compression of 3D Gaussian Splatting. AAAI Conference on Artificial Intelligence (AAAI) (2025). https://arxiv.org/abs/2503.08511 [13] Yihang Chen, Qianyi Wu, Mengyao Li, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. 2024. Fast Feedforward 3D Gaussian Splatting Compression. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2410.08017 [14] Yihang Chen, Qianyi Wu, Weiyao Lin, Mehrtash Harandi, and Jianfei Cai. 2024. HAC: Hash-grid Assisted Context for 3D Gaussian Splatting Compression. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-031-72667-5_ [15] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. 2021. Modular Interactive Video Object Segmentation: Interaction-to-Mask, Propagation and Difference-Aware Fusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/ CVPR46437.2021.00551 [16] Minsik Cho, Keivan A. Vahid, Qichen Fu, Saurabh Adya, Carlo Del Mundo, Mohammad Rastegari, Devang Naik, and Peter Zatloukal. 2024. IEEE Computer Architecture Letters (2024). eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. https://doi.org/10.1109/LCA.2024.3363492 Jianmei Dai, Zhilong Zhang, Shiwen Mao, and Danpu Liu. 2020. View Synthesis-Based 360 VR Caching System Over MEC-Enabled C-RAN. IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) (2020). https://doi.org/10.1109/TCSVT.2019.2946755 [17] [18] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. 2022. Depth-supervised NeRF: Fewer Views and Faster Training for Free. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52688.2022. [19] Peter Deutsch. 1996. DEFLATE Compressed Data Format Specification version 1.3. RFC 1951, RFC Editor. https://doi.org/10.17487/RFC1951 [20] Jarek Duda. 2013. Asymmetric Numeral Systems: Entropy Coding Combining Speed of Huffman Coding with Compression Rate of Arithmetic Coding. arXiv preprint arXiv: 1311.2540 (2013). https://arxiv.org/abs/1311.2540 [21] Bardienus P. Duisterhof, Zhao Mandi, Yunchao Yao, Jia-Wei Liu, Jenny Seidenschwarz, Mike Zheng Shou, Deva Ramanan, Shuran Song, Stan Birchfield, Bowen Wen, and Jeffrey Ichnowski. 2024. DeformGS: Scene Flow in Highly Deformable Scenes for Deformable Object Manipulation. In The 16th International Workshop on the Algorithmic Foundations of Robotics (WAFR). Springer. https://arxiv.org/abs/2312.00583 [22] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, and Zhangyang Wang. 2024. LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://proceedings. neurips.cc/paper_files/paper/2024/file/fd881d3b625437354d4421818f81058f-Paper-Conference.pdf [23] Guangchi Fang and Bing Wang. 2024. Mini-Splatting: Representing Scenes with Constrained Number of Gaussians. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-031-72751- [24] Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, and Yue Gao. 2019. MeshNet: mesh neural network for 3D shape representation. In AAAI Conference on Artificial Intelligence (AAAI). AAAI Press. https://doi.org/10.1609/aaai.v33i01.33018279 [25] Sara Fridovich-Keil, Giacomo Meanti, Frederik Warburg, Benjamin Recht, and Angjoo Kanazawa. 2023. K-Planes: Explicit Radiance Fields in Space, Time, and Appearance. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52729. 2023.01201 [26] Sharath Girish, Kamal Gupta, and Abhinav Shrivastava. 2024. Eagles: Efficient Accelerated 3D Gaussians with Lightweight Encodings. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-031-73036-8_4 [27] Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev. 2014. Compressing Deep Convolutional Networks Using Vector Quantization. arXiv preprint arXiv:1412.6115 (2014). https://doi.org/10.48550/arXiv.1412.6115 30 Youn et al. [28] Danillo Graziosi, Ohji Nakagami, Satoru Kuma, Alexandre Zaghetto, Teruhiko Suzuki, and Ali Tabatabai. 2020. An Overview of Ongoing Point Cloud Compression Standardization Activities: Video-based (V-PCC) and Geometry-based (G-PCC). APSIPA Transactions on Signal and Information Processing (APSIPA TSiP) (2020). https://doi.org/10.1017/ATSIP.2020.12 [29] Colin Groth, Sascha Fricke, and Marcus Magnor Susana Castillo. 2023. Wavelet-Based Fast Decoding of 360-Degree Videos. IEEE Transactions on Visualization and Computer Graphics (TVCG) (2023). https://doi.org/10.1109/TVCG.2023. [30] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. 2022. Vector Quantized Diffusion Model for Text-to-Image Synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/ CVPR52688.2022.01043 [31] hang Chen, Zhong Li, Liangchen Song, Lele Chen, Jingyi Yu, Junsong Yuan, , and Yi Xu. 2023. NeuRBF: Neural Fields Representation with Adaptive Radial Basis Functions. In IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. https://doi.org/10.1109/ICCV51070.2023.00386 [32] Alex Hanson, Allen Tu, Geng Lin, Vasu Singla, Matthias Zwicker, and Tom Goldstein. 2025. Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10. 1109/CVPR52734.2025.02006 [33] Alex Hanson, Allen Tu, Vasu Singla, Mayuka Jayawardhana, Matthias Zwicker, and Tom Goldstein. 2025. Pup 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/ CVPR52734.2025.00558 [34] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. 2018. Deep Blending For Free-viewpoint Image-based Rendering. ACM Transactions on Graphics (ToG) (2018). https://doi.org/10.1145/3272127.3275084 [35] Peter Hedman, Tobias Ritschel, George Drettakis, and Gabriel Brostow. 2016. Scalable Inside-out Image-based Rendering. ACM Transactions on Graphics (ToG) (2016). https://doi.org/10.1145/2980179.2982420 [36] Adam Houenou, Philippe Bonnifait, Veronique Cherfaoui, and Wen Yao. 2013. Vehicle trajectory prediction based on motion model and maneuver recognition. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE. https://doi.org/10.1109/IROS.2013.6696982 [37] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. 2022. EfficientNeRF - Efficient Neural Radiance Fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52688.2022.01256 [38] He Huang, Wenjie Huang, Qi Yang, Yiling Xu, and Zhu Li. 2025. Hierarchical Compression Technique for 3D Gaussian Splatting Compression. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. https://doi.org/10.1109/ICASSP49660.2025.10887742 iangji Fang, Qinhong Jiang, Jianping Shi, and Bolei Zhou. 2020. TPNet: Trajectory Proposal Network for Motion Prediction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR42600.2020. [39] [40] Saqib Javed, Ahmad Jarrar Khan, Corentin Dumery, Chen Zhao, and Mathieu Salzmann. 2025. Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes. In British Machine Vision Conference (BMVC). BMVA. https://arxiv.org/abs/2412.05700 [41] Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, and Lan Xu. 2024. HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.01866 [42] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics (ToG) (2023). https://doi.org/10.1145/ [43] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. 2017. Tanks and Temples: Benchmarking Large-scale Scene Reconstruction. ACM Transactions on Graphics (ToG) (2017). https://doi.org/10.1145/3072959.3073599 [44] Hanyang Kong, Xingyi Yang, and Xinchao Wang. 2025. Efficient Gaussian Splatting for Monocular Dynamic Scene Rendering via Sparse Time-Variant Attribute Modeling. In AAAI Conference on Artificial Intelligence (AAAI). AAAI press. https://doi.org/10.1609/aaai.v39i4.32460 [47] [46] [45] Sangwoon Kwak, Joonsoo Kim, Jun Young Jeong, Won-Sik Cheong, Jihyong Oh, and Munchurl Kim. 2025. MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52734.2025.01059 Joohyun Kwon, Hanbyel Cho, and Junmo Kim. 2025. Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52734.2025.02501 Junoh Lee, Chang-Yeon Won, Hyunjun Jung, Inhwan Bae, and Hae-Gon Jeon. 2024. Fully Explicit Dynamic Gaussian Splatting. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2024/file/ 09b47a77997b7dd7d2b26bd8ff769392-Paper-Conference.pdf Joo Chan Lee, Jong Hwan Ko, and Eunbyung Park. 2025. Optimized Minimal 3D Gaussian Splatting. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://doi.org/10.48550/arXiv.2503.16924 Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. 2024. Compact 3D Gaussian Representation for Radiance Field. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.02052 Joo Chan Lee, Daniel Rho, Xiangyu Sun, Jong Hwan Ko, and Eunbyung Park. 2024. Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields. arXiv preprint arXiv:2408.03822 (2024). https://doi.org/10.48550/arXiv.2408.03822 Jiahui Lei, Adam Harley Yijia Weng, Leonidas Guibas, and Kostas Daniilidis. 2025. MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. https://doi.org/10.1109/ CVPR52734.2025.00578 [49] [48] [51] [50] SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 31 [52] Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, and Baochang Zhang. 2025. HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction. arXiv preprint arXiv:2506.14229 (2025). https://arxiv.org/abs/2506.14229 [53] Haolin Li, Jinyang Liu, Mario Sznaier, and Octavia Camps. 2025. 3D-HGS: 3D Half-Gaussian Splatting. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://arxiv.org/abs/2406.02720 [54] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Ping Tan. 2025. Streaming Radiance Fields for 3D Video Synthesis. In Advances https://proceedings.neurips.cc/paper_files/paper/2022/file/ in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. 57c2cc952f388f6185db98f441351c96-Paper-Conference.pdf [55] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo Kanazawa. 2023. NerfAcc: Efficient Sampling Accelerates NeRFs. In IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. https://doi.org/10.1109/ICCV51070.2023.01699 [56] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. 2022. Neural 3D Video Synthesis from Multi-view Video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52688.2022.00544 [57] Zhan Li, Zhang Chen, Zhong Li, and Yi Xu. 2024. Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.00813 [58] Lei Liu, Zhenghao Chen, Wei Jiang, Wei Wang, and Dong Xu. 2024. HEMGS: Hybrid Entropy Model for 3D Gaussian Splatting Data Compression. arXiv preprint arXiv:2411.18473 (2024). https://arxiv.org/abs/2411.18473 [59] Lei Liu, Zhenghao Chen, and Dong Xu. 2025. 3D Gaussian Splatting Data Compression with Mixture of Priors. In ACM International Conference on Multimedia (ACM MM). https://doi.org/10.1145/3746027.3755432 [60] Xiangrui Liu, Xinju Wu, Pingping Zhang, Shiqi Wang, Zhu Li, and Sam Kwong. 2024. CompGS: Efficient 3D Scene Representation via Compressed Gaussian Splatting. In ACM International Conference on Multimedia (ACM MM). ACM. https://doi.org/10.1145/3664647 [61] Zhening Liu, Yingdong Hu, Xinjie Zhang, Rui Song, Jiawei Shao, Zehong Lin, and Jun Zhang. 2024. Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction. arXiv preprint arXiv:2411.14847 (2024). https://doi.org/10.48550/arXiv.2411.14847 [62] Tao Lu, Mulin Yu, Linning Xu, Yuanbo Xiangli, Limin Wang, Dahua Lin, and Bo Dai. 2024. Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.01952 Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. 2024. Dynamic 3D Gaussians: Tracking by Persistent Dynamic View Synthesis. In International Conference on 3D Vision (3DV). IEEE. https://doi.org/10.1109/3DV62453.2024.00044 [63] [64] Zhanpeng Luo, Haoxi Ran, and Li Lu. 2025. Instant4D: 4D Gaussian Splatting in Minutes. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://doi.org/10.48550/arXiv.2510.01119 [65] Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, and Andrew J. Davison. 2024. Gaussian Splatting SLAM. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.01708 [66] Donald Meagher. 1982. Octree Encoding: New Technique for the Representation, Manipulation and Display of Arbitrary 3-D Objects by Computer. Computer Graphics and Image Processing (1982). https://doi.org/10.1016/0146-664X(82)90104-6 [67] Takato Mizuho, Takuji Narumi, and Hideaki Kuzuoka. 2024. Reduction of Forgetting by Contextual Variation During Encoding Using 360IEEE Transactions on Visualization and Computer Graphics (TVCG) (2024). https: Degree Video-Based Immersive Virtual Environments. //doi.org/10.1109/TVCG.2024.3403885 [68] Wieland Morgenstern, Florian Barthel, Anna Hilsmann, and Peter Eisert. 2024. Compact 3D Scene Representation via Self-organizing Gaussian Grids. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-031-73013-9_2 [69] KL Navaneet, Kossar Pourahmadi Meibodi, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. 2024. CompGS: Smaller and Faster Gaussian Splatting with Vector Quantization. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-03173411-3_19 [70] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, Anton S. Kaplanyan, and Markus Steinberger. 2021. DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields using Depth Oracle Networks. Computer Graphics Forum (CGF) (2021). https://doi.org/10.1111/cgf.14340 [71] Simon Niedermayr, Josef Stumpfegger, and RÃ¼diger Westermann. 2024. Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.00985 [72] Seungjun Oh, Younggeun Lee, Hyejin Jeon, and Eunbyung Park. 2025. Hybrid 3D-4D Gaussian Splatting for Fast Dynamic Scene Representation. arXiv preprint arXiv:2505.13215 (2025). https://doi.org/10.48550/arXiv.2505.13215 [73] Panagiotis Papantonakis, Georgios Kopanas, Bernhard Kerbl, Alexandre Lanvin, and George Drettakis. 2024. Reducing the Memory Footprint of 3D Gaussian Splatting. ACM SIGGRAPH Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). https://doi.org/10.1145/3651282 [74] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien Bouaziz, Dan Goldman, Ricardo Martin-Brualla, and Steven M. Seitz. 2021. HyperNeRF: Higher-Dimensional Representation for Topologically Varying Neural Radiance Fields. ACM Transactions on Graphics (ToG) (2021). https://doi.org/10.1145/3478513.3480487 [75] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. Dreamfusion: Text-to-3D using 2D Diffusion. In International Conference on Learning Representations (ICLR). https://doi.org/10.48550/arXiv.2209.14988 32 Youn et al. [76] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. 2021. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR46437.2021.01018 [77] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017. PointNet++: Deep Hierarchical Feature Learning on Point Sets in Metric Space. In Advances in Neural Information Processing Systems. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2017/file/ d8bf84be3800d12f74d8b05e9b89836f-Paper.pdf [78] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. 2017. PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR.2017.16 [79] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. 2019. Generating Diverse High-Fidelity Images with VQ-VAE-2. In Advances https://proceedings.neurips.cc/paper_files/paper/2019/file/ in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. 5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper.pdf [80] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. 2021. KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. In IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. https://doi.org/10.1109/ICCV48922.2021.01407 [81] Kerui Ren, Lihan Jiang, Tao Lu, Mulin Yu, Linning Xu, Zhangkai Ni, and Bo Dai. 2025. Octree-GS: Towards Consistent Real-time Rendering with Lod-structured 3D Gaussians. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) (2025). https://doi.org/10.1109/TPAMI. 2025.3568201 [82] Gernot Riegler, Ali Osman Ulusoy, and Andreas Geiger. 2017. OctNet: Learning Deep 3D Representations at High Resolutions. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR.2017. [83] Neus Sabater, Guillaume Boisson, Benoit Vandame, Paul Kerbiriou, Frederic Babon, Matthieu Hog, Remy Gendrot, Tristan Langlois, Olivier Bureller, and Arno Schubert. 2017. Dataset and Pipeline for Multi-view Light-Field Video. In IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE. https://doi.org/10.1109/CVPRW.2017.221 [84] Francesco Di Sario, Riccardo Renzulli, Marco Grangetto, Akihiro Sugimoto, and Enzo Tartaglione. 2025. GoDe: Gaussians on Demand for Progressive [85] [86] Level of Detail and Scalable Compression. arXiv preprint arXiv:2501.13558 (2025). https://arxiv.org/abs/2501.13558 Johannes L. Schonberger and Jan-Michael Frahm. 2016. Structure-From-Motion Revisited. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR.2016.445 Jinseok Seo, Gerard Jounghyun Kim, and Kyo Chul Kang. 1999. Levels of Detail (LoD) Engineering of VR Objects. In ACM Symposium on Virtual Reality Software and Technology (VRST). ACM. https://doi.org/10.1145/323663.323680 [87] Ken Shoemake. 1985. Animating Rotation with Quaternion Curves. In ACM SIGGRAPH Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). ACM. https://doi.org/doi/10.1145/325165.325242 [88] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. 2023. NeRFPlayer: Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields. IEEE Transactions on Visualization and Computer Graphics (TVGC) (2023). https://doi.org/10.1109/TVCG.2023.3247082 [89] Cheng Sun, Min Sun, and Hwann-Tzong Chen. 2022. Direct Voxel Grid Optimization: Super-fast Convergence for Radiance Fields Reconstruction. [90] In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52688.2022.00538 Jiakai Sun, Han Jiao, Guangyuan Li, Zhanjie Zhang, Lei Zhao, and Wei Xing. 2024. 3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.01954 [91] Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, and Sanja Fidler. 2021. Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D Shapes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR46437.2021. [92] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P. Srinivasan, Jonathan T. Barron, and Henrik Kretzschmar. 2022. Block-NeRF: Scalable Large Scene Neural View Synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.48550/arXiv.2202.05263 [93] Zhenyu Tang, Chaoran Feng, Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liu, Xiaoxiao Long, Wenping Wang, and Li Yuan. 2025. NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations. AAAI Conference on Artificial Intelligence (AAAI) (2025). https://arxiv.org/abs/2503.23162 [94] Zhenyu Tang, Chaoran Feng, Xinhua Cheng, Wangbo Yu, Junwu Zhang, Yuan Liu, Xiaoxiao Long, Wenping Wang, and Li Yuan. 2025. NeuralGS: Bridging Neural Fields and 3D Gaussian Splatting for Compact 3D Representations. arXiv preprint arXiv:2503.23162 (2025). https://arxiv.org/abs/ 2503.23162 [95] Boyuan Tian, Qizhe Gao, Siran Xianyu, Xiaotong Cui, and Minjia Zhang. 2025. Flexgaussian: Flexible and Cost-effective Training-free Compression for 3D Gaussian Splatting. In ACM International Conference on Multimedia (ACM MM). https://dl.acm.org/doi/pdf/10.1145/3746027.3754744 [96] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf. 2023. Consistent View Synthesis with Pose-Guided Diffusion Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.48550/arXiv.2303.17598 [97] Allen Tu, Haiyang Ying, Alex Hanson, Yonghan Lee, Tom Goldstein, and Matthias Zwicker. 2025. Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes. arXiv preprint arXiv: 2506.07917 (2025). https://arxiv.org/abs/2506.07917 [98] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. 2022. Mega-NERF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.48550/arXiv.2112.10703 SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 33 [99] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. 7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf Neural Discrete Representation Learning. In Advances in https://proceedings.neurips.cc/paper_files/paper/2017/file/ [100] Evangelos Ververas, Rolandos Alexandros Potamias, Jifei Song, Jiankang Deng, and Stefanos Zafeiriou. 2024. Sags: Structure-aware 3D Gaussian Splatting. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-031-72655-2_13 [101] Diwen Wan, Ruijie Lu, and Gang Zeng. 2024. Superpoint Gaussian Splatting for Real-Time High-Fidelity Dynamic Scene Reconstruction. In International Conference on Machine Learning (ICML). PMLR. https://proceedings.mlr.press/v235/wan24f.html [102] Chenjunjie Wang, Shashank Sridhara, Eduardo Pavez, Antonio Ortega, and Cheng Chang. 2025. Adaptive Voxelization for Transform Coding of 3D Gaussian Splatting Data. In International Conference on Image Processing (ICIP). IEEE. https://doi.org/10.1109/ICIP55913.2025.11084522 [103] Henan Wang, Hanxin Zhu, Tianyu He, Runsen Feng, Jiajun Deng, Jiang Bian, and Zhibo Chen. 2024. End-to-end Rate-distortion Optimized 3D Gaussian Representation. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/978-3-031-73636-0_5 Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. 2025. VGGT: Visual Geometry Grounded Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). https://openaccess.thecvf.com/content/CVPR2025/ html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html [104] [105] Tao Wang, Mengyu Li, Geduo Zeng, Cheng Meng, and Qiong Zhang. 2025. Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https: //arxiv.org/abs/2506.09534 [106] Taorui Wang, Zitong Yu, and Yong Xu. 2025. TC-GS: Tri-plane Based Compression for 3D Gaussian Splatting. In IEEE International Conference on Multimedia and Expo (ICME). IEEE. https://arxiv.org/abs/2503.20221 [107] Yufei Wang, Zhihao Li, Lanqing Guo, Wenhan Yang, Alex Kot, and Bihan Wen. 2024. ContextGS: Compact 3D Gaussian Splatting with Anchor Level Context Model. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://proceedings.neurips.cc/ paper_files/paper/2024/file/5c20ca4b0b20b0bd2f1d839dc605e70f-Paper-Conference.pdf [108] Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simoncelli. 2004. Image Quality Assessment: from Error Visibility to Structural Similarity. IEEE Transactions on Image Processing (TIP) (2004). https://doi.org/10.1109/TIP.2003.819861 [109] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 2024. 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https: //doi.org/10.1109/CVPR52733.2024.01920 [110] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang Shi, Yiyi Liao, and Hao Zhao. 2024. MARS: An Instance-Aware, Modular and Realistic Simulator for Autonomous Driving. In International Conference on Artificial Intelligence (CICAI). Springer, Singapore. https://doi.org//10.1007/978-981-99-8850-1_1 [111] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. 2022. Bungeenerf: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. In European Conference on Computer Vision (ECCV). Springer, Cham. https: //doi.org/10.1007/978-3-031-19824-3_ [112] Shuzhao Xie, Jiahang Liu, Weixiang Zhang, Shijia Ge, Sicheng Pan, Chen Tang, Yunpeng Bai, and Zhi Wang. 2024. SizeGS: Size-aware Compression of 3D Gaussians with Hierarchical Mixed Precision Quantization. In ACM International Conference on Multimedia (ACM MM). ACM. https: //doi.org/10.1145/3746027.3755370 [113] Shuzhao Xie, Weixiang Zhang, Chen Tang, Yunpeng Bai, Rongwei Lu, Shijia Ge, and Zhi Wang. 2024. MesonGS: Post-training Compression of 3D Gaussians via Efficient Attribute Transformation. In European Conference on Computer Vision (ECCV). Springer, Cham. https://doi.org/10.1007/ 978-3-031-73414-4_25 [114] Hao Xu, Xiaolin Wu, and Xi Zhang. 2025. 3DGS Compression with Sparsity-guided Hierarchical Transform Coding. arXiv preprint arXiv:2505.22908 (2025). https://arxiv.org/abs/2505.22908 [115] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota BulÃ², Lorenzo Porzi, Peter Kontschieder, AljaÅ¾ BoÅ¾iÄ, Dahua Lin, Michael ZollhÃ¶fer, and Christian Richardt. 2023. VR-NeRF: High-Fidelity Virtualized Walkable Spaces. In SIGGRAPH Asia Conference Proceedings. ACM. https://doi.org/10.1145/3610548.3618139 [116] Zhiwen Yan, Chen Li, and Gim Hee Lee. 2023. NeRF-DS: Neural Radiance Fields for Dynamic Specular Objects. In IEEE/CVF Conference on [117] Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52729.2023.00801 Jiayu Yang, Weijian Su, Songqian Zhang, Yuqi Han, Jinli Suo, and Qiang Zhang. 2025. Scale-GS: Efficient Scalable Gaussian Splatting via Redundancy-filtering Training on Streaming Content. arXiv preprint arXiv: 2508.21444 (2025). https://arxiv.org/abs/2508.21444 [118] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. 2024. Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/ CVPR52733.2024.01922 [119] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. 2024. Real-Time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting. In International Conference on Learning Representations (ICLR). https://arxiv.org/pdf/2310.10642 [120] Yuheng Yuan, Qiuhong Shen, Xingyi Yang, and Xinchao Wang. 2025. 1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://doi.org/10.48550/arXiv.2503.16422 34 Youn et al. [121] Yu Ting Zhan, Cheng Yuan Ho, ]Hebi Yang, Yi Hsin Chen, Jui Chiu Chiang, Yu Lun Liu, and Wen Hsiao Peng. 2025. CAT-3DGS: Contextadaptive Triplane Approach to Rate-distortion-optimized 3DGS Compression. In International Conference on Learning Representations (ICLR). https://arxiv.org/abs/2503.00357 [122] Chen Zhang, Ganzhangqin Yuan, and Wenbing Tao. 2023. DMNet: Delaunay Meshing Network for 3D Shape Representation. In IEEE/CVF [123] International Conference on Computer Vision (ICCV). IEEE. https://doi.org/10.1109/ICCV51070.2023.01326 Jian Zhang, S. Payandeh, and J. Dill. 2003. Levels of Detail in Reducing Cost of Haptic Rendering: Preliminary User Study. In Symposium on Haptic Interfaces for Virtual Environment and Teleoperator System (HAPTICS). IEEE. https://doi.org/10.5555/795683.797571 [124] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR.2018.00068 [125] Xinjie Zhang, Zhening Liu, Yifan Zhang, Xingtong Ge, Dailan He, Tongda Xu, Yan Wang, Zehong Lin, and Jun Zhang Shuicheng Yan. 2025. MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes. In IEEE/CVF International Conference on Computer Vision (ICCV). IEEE. https://arxiv.org/abs/2410.13613 [126] Zihan Zheng, Zhenlong Wu, Houqiang Zhong, Yuan Tian, Ning Cao, Lan Xu, Jiangchao Yao, Xiaoyun Zhang, Qiang Hu, and Wenjun Zhang. 2025. 4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming. In Advances in Neural Information Processing Systems (NeurIPS). Curran Associates, Inc. https://doi.org/10.48550/arXiv.2509.17513 [127] Le Hui Tianrui Chen Min Yang Xiao Tang Feng Zhu Yuchao Dai Zhicheng Lu, Xiang Guo. 2024. 3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10. 1109/CVPR52733.2024.00850 [128] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. 2024. Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE. https://doi.org/10.1109/CVPR52733.2024.02048 SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 35 SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting Supplementary Material NOTATION TABLE All notations and symbols appearing in this survey are grouped into five categories according to their usage in Tab. 3. The table provides concise reference for readers unfamiliar with common 3DGS notation, including all frequently used variables and those mentioned at least once. Vector quantities are highlighted in boldface for clarity. Notation Explanation 1. Basic Elements of 3D Gaussians ğ ğ’” ğ’“ ğ’‰ ğ‘‘ ğšº ğ‘¹ ğ‘º ğœ ğ‘€ ğº ğ¶ Center coordinates of the 3D Gaussian Spatial extent occupied by the 3D Gaussian Orientation of the 3D Gaussian Colors that vary with the viewing direction Degree of the Spherical Harmonics Covariance matrix of the 3D Gaussian Rotation matrix of the 3D Gaussian Scale matrix of the 3D Gaussian Opacity value of each 3D Gaussian Bit width of the 3D Gaussian attributes Individual 3D Gaussian Set of 3D Gaussians at specific camera pose View-dependent RGB color of the Gaussian 2. Rendering and Projection ğ‘¾ ğ‘‡ ğ’‘ ğ ğšº ğ›¼ (ğ’‘) ğ‘”(ğ’‘) ğ‘ (ğ’‘) ğ¼ğ‘ğ‘Ÿğ‘’ğ‘‘ ğ¼ğ‘”ğ‘¡ ğœ™ Coordinate Transformation matrix from world coordinates to camera coordinates Tile size Pixel in 2D space Projected center of the 3D Gaussian in 2D screen coordinates system Projected 2D covariance matrix of the 3D Gaussian in screen coordinates Total number of Gaussians projected onto the pixel Alpha blending value at pixel ğ‘ Pixel value of the projected 2D Gaussian in screen coordinates Predicted pixel color value Rendered 2D image GT image Transformation matrix that defines the cameras viewpoint and orientation 36 3. Static Scene Modeling Mğ‘› ğ‘šğ‘› ğœ– sg() 1[] Sig() Ë†ğ’” Ë†ğœ ğ¿ğ‘š ğºğ‘† ğ‘— ğ›¾ (ğšºğ‘— ) ğ‘Šğ‘–,ğ‘ ğ‘Šğ‘– ğ›¼ğ‘– Tğ‘– Nğ‘² ğ’Š ğ‘» ğ‘– Ldistill ğ‘ª teacher ğ‘ª student ğ‘† (ğ‘) ğ¸ ğ‘— ğ‘ğ‘˜ ğ‘Ÿ (ğ‘  ) ğ‘–,ğ‘˜ ğ‘‘ (ğ‘  ) ğ‘–,ğ‘˜ CB(ğ’” ) [ğ‘˜] ğ‘· ğ‘½ ğ’‡ ğ‘£ ğ’ ğ‘£ ğ‘¶ ğ‘£ Ë†ğ’‡ ğ‘£ ğ›¿ğ‘£ğ‘ ğ’…ğ‘£ğ‘ ğ¹ğ’‚ ğ· Ë†ğ’’ ğ’’ Youn et al. Gaussian attributes Binary mask for n-th 3D Gaussian Learnable mask parameter for n-th 3D Gaussian Threshold value for masking Stop Gradient operator Indicator function Sigmoid activation function Masked scale of 3D Gaussian Masked opacity of 3D Gaussian Masking loss Global Significance score for j-th Gaussian Normalized volume of j-th Gaussian Influence of i-th Gaussian at pixel Total influence of i-th Gaussian on entire scene Alpha blending value of i-th Gaussian Transmittance value up to the i-th 3D Gaussian Set of K-nearest neighbors of i-th 3D Gaussian Appearance feature vector of i-th 3D Gaussian Knowledge distillation loss Rendered pixel values from teacher model Rendered pixel values from student model Sensitivity of parameter Total energy for j-th image Probability of k-th codebook vector Rate loss when k-th codeword is selected for the scale of i-th 3D Gaussian Distortion loss when k-th codeword is selected for the scale of i-th 3D Gaussian k-th codeword in scale codebook Point clouds from COLMAP Voxelized scene Local context features of anchor Scaling factor of anchor Learnable offsets of anchor View-dependent features Relative distance between camera and anchor View direction vector MLP for deriving attributes MLP decoder function Latent vector of the quantized attribute Rounded latent vector SUCCESS-GS: Survey of Compactness and Compression for Efficient Static and Dynamic Gaussian Splatting 37 ğ’‡ â„ ğ‘ğ‘– ğ‘– Ë†ğ’‡ ğ‘ ğœ™ Î¦ ğ‘,ğˆğ‘– ğğ‘– ğ‘ ğğ‘– ğ‘,ğˆğ‘– ğ‘ MLPğ‘ Hash feature Quantization step size for the i-th anchor Quantized i-th anchor attribute Gaussian distribution for i-th quantized anchor attributes Cumulative distribution function for i-th anchor attributes MLP for context modeling 4. Optimization and Training ğ‘¥ ğ¼ğ‘”ğ‘¡ ğ¿1 () ğœ† ğœğ‘ğ‘œğ‘  ğœ–ğœ Arbitrary point in world coordinates Ground truth image Pixel-wise mean absolute error Weighting factor controlling relative contribution of ğ¿1 and SSIM loss Threshold for view-space position gradients of Gaussian Opacity threshold for the Gaussian 5. Dynamic Scene Modeling Î”x Î”r Î”s xğ‘ ğ¹ğœƒ ğ¸ğœ™ ğ·ğœ“ ğ4ğ· ğšº4ğ· h4ğ· ğ’’ ğ‘¡ Offset for position produced by time-conditioned MLP Offset for rotation produced by time-conditioned MLP Offset for scale produced by time-conditioned MLP Position of canonical gaussian Time-conditioned MLP HexPlane encoder Latent feature Lightweight MLP decoder network 4D position 4D covariance matrix Spherical harmonics of Fourer coefficients of 4D coordinates Quaternion representing orientation of 3D Gaussian over time Time variable for modeling dynamic scenes Table 3. Notation and symbols used throughout this survey. The table organizes mathematical notation into five categories: basic elements of 3D Gaussians, rendering and projection, static scene modeling, optimization and training, and dynamic scene modeling."
        }
    ],
    "affiliations": [
        "Chung-Ang University",
        "Kyung Hee University"
    ]
}