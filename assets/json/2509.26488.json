{
    "paper_title": "dParallel: Learnable Parallel Decoding for dLLMs",
    "authors": [
        "Zigeng Chen",
        "Gongfan Fang",
        "Xinyin Ma",
        "Ruonan Yu",
        "Xinchao Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 8 8 4 6 2 . 9 0 5 2 : r Preprint. DPARALLEL: LEARNABLE PARALLEL DECODING FOR DLLMS Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang National University of Singapore zigeng99@u.nus.edu, xinchao@nus.edu.sg"
        },
        {
            "title": "ABSTRACT",
            "content": "Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5 speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in 10.5 speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel"
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion large language models (dLLMs) (Yu et al., 2025a; Zhang et al., 2025; Yi et al., 2024) have emerged as promising alternative to autoregressive LLMs (Achiam et al., 2023; Bai et al., 2023). By leveraging bidirectional attention, they overcome the sequential generation bottleneck and enable parallel, random-order text generation, offering the potential for substantial improvements in inference efficiency. This potential has already been demonstrated in proprietary models such as Mercury (Labs et al., 2025), Gemini-Diffusion, and Seed-Diffusion (Song et al., 2025). However, realizing this parallelism in existing open-source dLLMs remains challenging. Open implementations such as LLaDA (Nie et al., 2025; Zhu et al., 2025) and Dream (Ye et al., 2025), still require number of decoding steps proportional to the sequence length to maintain generation quality, resulting in limited inference efficiency. Many recent efforts have attempted to accelerate dLLMs. Some approaches (Ma et al., 2025; Liu et al., 2025; Wu et al., 2025; Hu et al., 2025) reduce the time cost per decoding step by enabling KV caching. Other works (Israel et al., 2025; Wei et al., 2025; Li et al., 2025a;b; Gwak et al., 2025; Ben-Hamu et al., 2025) focus on optimizing parallel sampling algorithms to accelerate inference by reducing the necessary decoding steps. Despite these advancements, existing methods have yet to fully unlock the parallel potential of dLLMs, as highly parallel decoding consistently leads to degraded performance. This paper focuses on training dLLMs to unleash their potential for parallel decoding. We identify the core bottleneck as their sequential certainty convergence. Although dLLMs predict all masked tokens in parallel at each step, the certainty of these predictions still converges in left-to-right sequential order. This sequential propagation of certainty prevents the model from reliably determining Correspoding Author 1 Preprint. Figure 1: Our method achieves highly parallel decoding. Compared to the original LLaDA Model, dParallel decodes over 8 tokens per step on GSM8K while preserving the accuracy. multiple tokens simultaneously, forming the key bottleneck to highly parallel decoding. Employing naive teacher forcing or diffusion forcing (Chen et al., 2024) training is insufficient to resolve this issue, as they solely focus on trajectory alignment. Consequently, new training paradigm centered on predictive certainty itself is needed for dLLMs to further unlock parallelism. Building on this insight, we present certainty-forcing distillation, simple and effective training strategy that directly leverages token certainty as training signal. The core idea is to convert dLLMs inherently sequential certainty propagation into more parallel convergence process. Concretely, we guide pretrained dLLM to self-distill along its original semi-autoregressive decoding trajectory to maintain trajectory consistency, while simultaneously minimizing its predictive entropy over correctly predicted masked tokens to enforce high certainty. Certainty-forcing enables more tokens to reach high certainty in parallel at each step, thereby significantly extending the boundary of parallel decoding in dLLMs. We evaluate the effectiveness of our method on two representative open-source dLLMs: LLaDA, native dLLM trained from scratch, and Dream, dLLM initialized from an autoregressive LLM. Comprehensive experiments across multiple benchmarks demonstrate that our approach significantly reduces the number of decoding steps in dLLMs, while maintaining comparable performance. For instance, when applied to the LLaDA-8B-Instruct model, our approach achieves an 88% reduction in decoding steps on GSM8K (Cobbe et al., 2021), yielding an 8.5 speedup without sacrificing accuracy  (Fig.1)  . On MBPP (Austin et al., 2021b), it further reduces decoding steps by 91%, delivering 10.5 acceleration while maintaining performance. Furthermore, the training process of our method is highly efficient and low-cost. Leveraging Low-Rank Adaptation (LoRA) (Hu et al., 2022), the training can be completed in just 10 hours on only eight A5000 GPUs with 24 GB memory each. In conclusion, we present dParallel, learnable approach that unleashes the potential of parallel decoding in dLLMs, drastically reducing the number of decoding steps. Our analysis identifies the core bottleneck as the sequential convergence of certainty across masked tokens. To address this, we introduce certainty-forcing distillation strategy that ensures consistency with the original generation trajectory while encouraging masked tokens to attain high certainty faster and more in parallel. Extensive experiments demonstrate the effectiveness of our method. This work establishes new baseline and provides foundation for future research on few-step and parallel dLLMs."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Diffusion Language Models. In recent years, diffusion models (Ho et al., 2020; Song et al., 2020) have established dominance in the field of visual generation (Rombach et al., 2022; Podell et al., 2023; Ruiz et al., 2023; Zhang et al., 2023). However, their application to text generation remains highly challenging. Masked diffusion models (Shi et al., 2024; Austin et al., 2021a; Sahoo et al., 2024; Zheng et al., 2024; Lou et al., 2023)have emerged as promising approach, modeling language in the discrete space by predicting masked tokens, thereby offering the potential for fast and parallel 2 Preprint. decoding. Building upon this idea, two representative dLLMs, LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025), have recently attracted significant attention from the community, demonstrating that dLLMs can achieve performance comparable to autoregressive LLMs at the billion-parameter scale. Beyond these developments, there is also growing interest in reasoning dLLMs (Zhao et al., 2025; Wang et al., 2025b; Zhu et al., 2025), multimodal dLLMs (You et al., 2025; Yu et al., 2025b; Yang et al., 2025; Li et al., 2025c), and code generation (Gong et al., 2025; Xie et al., 2025) dLLMs. Accelerating Diffusion Language Models. The potential of dLLMs in inference efficiency remains largely underexplored. Recent studies have increasingly focused on accelerating the decoding process of dLLMs. Some approaches (Ma et al., 2025; Liu et al., 2025; Wu et al., 2025; Hu et al., 2025; Chen et al., 2025) aim to reduce the time cost for each decoding step by enabling caching mechanisms and employing token dropping during inference. Other works (Israel et al., 2025; Wei et al., 2025; Li et al., 2025a;b; Gwak et al., 2025; Ben-Hamu et al., 2025) focus on reducing the total number of decoding steps by designing improved sampling strategies. In addition, hybrid methods (Wang et al., 2025a; Arriola et al., 2025)have been proposed that combine the generative paradigms of dLLMs and autoregressive LLMs, training models to realize more efficient inference pipelines. SDTT Deschenaux & Gulcehre (2024) employs progressive distillation to reduce the inference steps. Further effort Xu & Yang (2025) leverages quantization techniques to construct lightweight dLLMs."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "Masked Diffusion Language Models (MDLMs). Unlike AR-LLMs that predict tokens in strict left-to-right fashion, MDLMs (Shi et al., 2024; Austin et al., 2021a; Zheng et al., 2024)formulate generation as probabilistic process consisting of forward masking corruption and reverse denoising recovery. The forward process corrupts clean sequence x0 into xt at level [0, 1]: q(xt x0) = (cid:89) (cid:104) i=1 (1 t) δ(xi = xi 0) + δ(xi (cid:105) = [MASK]) . (1) The reverse process is parameterized by mask predictor pθ, which attempts to recover x0 from xt. At each step, the model predicts all masked tokens jointly: pθ(x0 xt) = (cid:89) pθ(xi 0 xt), i:xi t=[MASK] (2) The training objective, defined as the negative log-likelihood restricted to masked positions, has been shown to upper bound the models negative log-likelihood (Ou et al., 2024): L(θ) = Et,x0,xt (cid:34) 1 (cid:88) i=1 1[xi = [MASK]] log pθ(xi (cid:35) 0 xt) . (3) Sampling Process. Inference proceeds through discretized reverse process: at each step the model predicts distributions for all masked tokens in parallel, samples provisional tokens, and then applies dynamic remasking strategy to determine which positions remain masked for further refinement. Unlike autoregressive decoding, this procedure allows multiple tokens to be determined in parallel, thereby enabling more flexible and potentially faster generation."
        },
        {
            "title": "4 METHOD",
            "content": "4.1 THE BARRIERS TO PARALLEL DECODING Diffusion language models are designed, in principle, for highly parallel token prediction. Yet in practice, this theoretical promise breaks down. To understand this discrepancy, we analyze the certainty dynamics of token predictions in dLLMs, revealing why their potential for parallel decoding remains unrealized. Certainty Correlates with Prediction Accuracy. We first establish that token-level certainty is reliable indicator of prediction correctness. Using LLaDA-8B-Instruct on the GSM8K test set 3 Preprint. Figure 2: Empirical Studies: (a) The average confidence score exhibits positive correlation with generation accuracy. (b) Token confidence propagates sequentially during the decoding process. (c) Convergence trajectories of confidence for different tokens. (Cobbe et al., 2021), we adopt remasking strategy with varying confidence thresholds and record the average determined confidence of tokens. Fig 2 (a) shows strong positive correlation between token confidence and the generation correctness: tokens resolved at higher confidence consistently achieve higher accuracy, whereas low-confidence commitments lead to frequent errors. This result confirms that high certainty is necessary condition for accurate generation. Certainty Converges to Peak Sequentially. The high certainty is not achieved in parallel. Instead, it propagates sequentially through the sequence. At any given decoding step, the model predicts all masked tokens, but only small subset, typically those adjacent to already known context, attain high confidence. The vast majority of tokens remain in low-confidence regime until new context becomes available. Once confident token is committed, it provides new conditioning context that allows another subset to rise in certainty at the next step. This dynamic is illustrated in Fig 2 (b), which shows the average confidence of tokens progressing as left-to-right propagation over decoding steps. Fig 2 (c) further confirms this at the individual token level, showing confidence trajectories that converge to high certainty in staggered, sequential order. Together, these findings reveal that high certainty does not emerge in parallel but unfolds sequentially through iterative context enrichment. The Fundamental Bottleneck. The key bottleneck is the sequential convergence of certainty. While true parallelism requires committing many tokens in single step, dLLM gains high certainty only for few neighboring tokens per iteration. Forcing multiple commitments too early introduces lowconfidence predictions, causing cascading errors and performance degradation. Key to Unlocking Parallelism Potential. The above insight illuminates clear path forward: if we could guide the model to achieve peak confidence in parallel across multiple token positions, we could break the sequential bottleneck. However, traditional training strategies, such as teacher forcing and diffusion forcing (Chen et al., 2024), are inadequate for this purpose, as their focus on trajectory alignment overlooks the dynamics of predictive certainty. Consequently, unlocking greater parallelism in dLLMs requires new training paradigm that directly optimizes for certainty. We therefore propose certainty-forcing disillation, novel strategy that reshapes the models certainty dynamics by using token certainty itself as direct training signal. 4.2 CERTAINTY-FORCING DISTILLATION We propose certainty-forcing distillation, straightforward approach that enforces parallel certainty along the original trajectory without altering it. An overview is shown in Fig 3. Teacher Trajectory Generation. Let MθT be the teacher model (a pre-trained vanilla dLLM), and let MθS be the student model, initialized as an identical copy. We train on dataset = i=1, where each (i) is an instruction prompt. For each prompt, the teacher MθT generates {X (i)}K target response trajectory using semi-autoregressive remasking strategy with total length and block size Lb, producing sequence = (y1, y2, . . . , yL). This sequence is partitioned into contiguous blocks {B1, B2, . . . , BN } such that = Lb, where the n-th block is defined as Bn = (cid:0)y(n1)Lb+1, . . . , ynLb (cid:1) for {1, . . . , }. 4 Preprint. Figure 3: Overview of proposed certainty-forcing distillation. The dLLM is self-distilled along its original generation trajectory, ensuring consistency with the trajectory throughout training while encouraging token certainty to converge faster in parallel rather than sequentially. Semi-Autoregressive Forward Masking. To simulate the trajectory generation process for training, we perturb the clean trajectory and create noisy input sequence by applying semiautoregressive structural masking scheme. We first uniformly sample block index {0, . . . , 1}. The sequence is then divided into three distinct parts based on this index: (1) Context Blocks (i nLb): Tokens within the first blocks remain unmasked, serving as the models context. (2) Active Block (nLb < (n + 1)Lb): This is the block currently being generated, where its tokens are randomly replaced by the token [MASK] with masking probability pm = q. (3) Future Blocks (i > (n + 1)Lb): All tokens in subsequent blocks are fully masked, as they have not yet been generated. This procedure yields noisy token yi at each position i, defined as: yi = yi, (cid:26)yi with probability 1 [MASK] with probability [MASK], if nLb (Context) if nLb < (n + 1)Lb (Active Block) (4) if > (n + 1)Lb (Future) The resulting sequence simulates an intermediate state in the semi-autoregressive generative process, where the model is predicting the (n + 1)-th block given the context of the first blocks. Training Objective. Our objective differs from standard dLLM pre-training, which typically aims Instead, we restrict the learning signal to the to predict all masked tokens across the sequence. masked tokens within the active block Bn+1. Our training objective is for the student model not only to replicate the target sampling trajectory within the active block but also to parallel achieve maximal certainty in its predictions. To enforce consistency between the student models generated trajectory and that of the teacher, we apply standard Cross-Entropy (CE) loss on the masked tokens of the active block, denoted as Ma: LConsistency = 1 Ma (cid:88) iMa log pθ(yi ), (5) where pθ(yi ) denotes the probability assigned by the student model to the correct token yi at position i, conditioned on the noisy input sequence . However, conventional CE loss is insufficient for our certainty-maximizing target. It focuses solely on correctness, and once the correct token is predicted, the gradient quickly vanishes, offering no incentive to further increase confidence. To explicitly encourage highly confident predictions, we introduce term that directly minimizes the entropy of the models output distribution, incorporating temperature parameter . This loss is applied only to the masked tokens in the active block that the student model already predicts 5 Preprint. Algorithm 1 Certainty-Forcing Distillation (CFD) Require: Teacher MθT , student MθS ; target trajectory set = {Y (i)}K weight β 0; optimizer O; token length L; block length Lb; mask ratio (0, 1]. i=1; temperature > 0; vV p(v) log p(v) Notation: H(p) = (cid:80) 1: for = 1 . . . Iteration do 2: 3: 4: Sample D, {0, 1, . . . , L/Lb 1}; ( , Ma) SEMI-AR-FOWARDMASKING(Y, q, n, L, Lb); MθS ( ); pi(v) softmax(zi)v, LConsistency Ma1(cid:80) (v) softmax(zi/T )v, p(T ) 5: log pi(yi); 6: 7: Mc { Ma arg maxvV pi(v) = yi }; LCertainty 1[Mc > 0] Mc1(cid:80) 8: LCFD LConsistency + β LCertainty; 9: θS O(θS, θS LCFD); 10: 11: end for iMa iMc H(p(T ) ); V correctly. Formally, we define the set of correctly predicted tokens as (cid:26) Mc = Ma arg max vV pθ(v ) = yi (cid:12) (cid:12) (cid:12) (cid:12) (cid:27) , (6) where denotes the vocabulary. The certainty-forcing loss is then defined as the average entropy of the predictive distributions for these tokens: (cid:32) (cid:33) pθ(v ; ) log pθ(v ; ) , (7) LCertainty = 1 Mc (cid:88) (cid:88) iMc vV where pθ(v ; ) denotes the temperature-scaled softmax distribution. Minimizing this term encourages the student model to generate sharper, higher-certainty distributions over the correct tokens, where controls the strength of the certainty enforcement. The overall training objective is combination of consistency loss and the certainty-forcing loss: LCFD = LConsistency + βLCertainty, (8) where β is hyperparameter balancing the objective of matching the teachers trajectory with the objective of enforcing high certainty. We find that this simple distillation strategy significantly accelerates the parallel convergence of certainty in dLLMs, thereby unlocking their inherent potential for parallel decoding. The overall training pipeline is summarized in Algorithm"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETUP Implementation Details. We evaluate the effectiveness of our method on two representative opensource dLLMs: LLaDA-8B-Instruct (Nie et al., 2025) and Dream-7B-Instruct (Ye et al., 2025). The training is conducted using the LoRA technique (Hu et al., 2022). For semi-autoregressive masking, we set the block length to Lb = 32 for LLaDA and Lb = 256 for Dream, with fixed masking ratio of 50%. The certainty loss is applied with temperature of = 0.5. Full training configurations are provided in the appendix. During inference, our models adopt an entropy-threshold semi-autoregressive remasking strategy , which is inherently consistent with our training objective. Training Data. As self-distillation approach, we use prompts from publicly available training datasets and let the pretrained model generate its own responses as training data. For LLaDA-8BInstruct, we sample prompts from the GSM8K (Cobbe et al., 2021), PRM12K (Lightman et al., 2023) training set, and part of the Numina-Math dataset (Li et al., 2024). We generate target trajectories using semi-autoregressive strategy with sequence length of 256 and block length of 32. We further filter out responses containing incorrect answers and finally get about 92k samples. For 6 Preprint. Table 1: Evaluation results on LLaDA-8B-Instruct. For all methods, we adopt semi-autoregressive remasking strategy with total sequence length of 256 and block length of 32. For our approach, the entropy threshold is set to either 0.45 or 0.5 for different tasks. Latency Speedup Accuracy Benchmark Method #Steps GSM8K -CoT (0-shot) MATH (4-shot) HumanEval (0-shot) MBPP (3-shot) LLaDA-8B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) LLaDA-8B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) LLaDA-8B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) LLaDA-8B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) 256 256 64 72 64 256 256 64 97 64 46 256 256 64 77 64 33 256 256 64 68 64 24 18.6s 9.7s 4.7s 5.2s 4.7s 2.2s 50.9s 11.3s 12.7s 17.6s 12.7s 8.9s 23.5s 9.8s 5.9s 6.7s 5.9s 2.9s 50.1s 10.7s 12.5s 12.8s 12.5s 4.8s 1.0 1.9 4.0 3.6 4.0 8.5 1.0 4.5 4.0 2.9 4.0 5.7 1.0 2.4 4.0 3.5 4.0 8.2 1.0 4.7 4.0 3.9 4.0 10.5 75.7% 72.9% 68.6% 75.5% 69.9% 76.1% 33.5% 32.6% 26.3% 33.2% 28.0% 31.5% 38.4% 34.1% 19.5% 37.2% 19.5% 40.2% 42.4% 39.8% 19.6% 41.6% 25.0% 40.8% Dream-7B-Instruct, we adopt the same trajectory generation strategy, and additionally generate code data using prompts from subset of the AceCode dataset (about 10k) (Zeng et al., 2025). Importantly, all training tokens are generated by the model itself, without introducing any external data as targets. Evaluation Details. We evaluate our models across multiple benchmarks, including two mathematics datasets (GSM8K and MATH (Lewkowycz et al., 2022)) and two code generation datasets (HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021b)). For GSM8K, we append chain-of-thought (CoT) prompt (Wei et al., 2022) after each question. We report accuracy, the average number of decoding steps, latency, and speedup ratio to provide comprehensive evaluation. All efficiency evaluations are conducted on NVIDIA RTX 6000 Ada GPUs. Baselines. We evaluate the original dLLM under its official default inference setting, and further compare our approach with four baselines that seek to accelerate the generation: (1) Dual-Cache: enable KV-cache on both prefix tokens and suffix tokens (Wu et al., 2025). (2) Few-step Decoding: reducing the number of decoding steps used by the original dLLM. (3) Conf-threshold Decoding: apply adaptive remasking based on the models confidence in predicting masked tokens (Wu et al., 2025; Yu et al., 2025b), with the confidence threshold set as 0.90 or 0.95 depending on the task. (4) Consistency Distillation: training the dLLM to predict all remaining masked tokens from intermediate state along its own generation trajectory (Luo et al., 2023). The training data and LoRA configuration are the same as our method. 5.2 MAIN RESULTS Results on the Native LLaDA Model. As shown in Table 1, directly reducing the decoding steps of the original model leads to substantial drop in performance. Consistency distillation has only marginal effect on LLaDA, offering slight improvement over the original model under the same number of steps. The confidence-threshold remasking strategy preserves accuracy, but its parallelism is limited, averaging only 34 tokens decoded per step. In contrast, our method significantly pushes the boundaries of parallel inference in dLLMs, achieving more than 8 tokens decoded per 7 Preprint. Table 2: Evaluation results on Dream-8B-Instruct. The original model uses the official inference setting with sequence length of 256. Other methods adopt semi-autoregressive remasking with the same length and block size of 32. The entropy threshold for our method is set to either 0.45 or 0.5. #Steps Speedup Accuracy Benchmark Method Latency GSM8K -CoT (0-shot) MATH (0-shot) HumanEval -Instruct (0-shot) MBPP -Instruct (0-shot) Dream-7B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) Dream-7B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) Dream-7B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) Dream-7B-Instruct Dual-Cache Few-step Decoding Conf-threshold Decoding Consistency Distillation dParallel (Ours) 256 256 64 61 64 39 256 256 64 93 64 256 256 64 71 64 37 256 256 64 43 64 29 17.2s 8.2s 4.3s 4.0s 4.3s 2.5s 17.5s 8.2s 4.4s 6.1s 4.4s 4.1s 25.9s 8.4s 6.5s 7.3s 6.4s 3.8s 19.8s 8.9s 5.0s 3.3s 5.0s 2.2s 1.0 2.1 4.0 4.3 4.0 6.9 1.0 2.1 4.0 2.9 4.0 4.2 1.0 3.1 4.0 3.5 4.0 6.9 1.0 2.2 4.0 5.9 4.0 8.8 82.9% 79.5% 59.0% 81.9% 75.6% 82.1% 39.5% 38.8% 16.7% 38.9% 29.6% 38.3% 52.4% 47.0% 16.5% 53.1% 34.2% 54.3% 58.8% 52.8% 25.0% 56.4% 37.4% 56.2% Figure 4: Comparison of speedaccuracy trade-off curves between confidence-threshold decoding and our method. (a) and (b) show results on the LLaDA model for GSM8K and HumanEval, respectively. (c) and (d) present results on the Dream model for GSM8K and HumanEval benchmarks. step on average while still maintaining performance. Notably, for LLaDA, we trained using only prompts from mathematical tasks, yet the model still exhibited remarkable improvement in parallel decoding ability on code tasks. Results on the AR-initialized Dream Model. As shown in Table 2, our method also demonstrates superior performance on the Dream model, which is initialized from an AR-LLM. Compared to other approaches designed to reduce the number of decoding steps, dParallel achieves substantially higher speedup while maintaining accuracy, thereby greatly enhancing decoding parallelism. It is worth noting that we observed risk of degeneration toward the original AR LLM when training Dream with semi-autoregressive masking. To avoid this issue, we employed standard random masking over the entire sequence instead. Consequently, the acceleration gains of our method on Dream are slightly lower than those observed on LLaDA. Superior EfficiencyPerformance Trade-off. In Fig 4, we compare our method against the original model with confidence-threshold decoding in terms of the efficiencyperformance trade-off curve. Our approach achieves substantially better trade-off. On LLaDA with GSM8K, at the same 9.4 speedup, our method attains 16.5% higher accuracy than confidence-threshold decoding. On HumanEval, at the same 9.3 speedup, our method improves accuracy by 21.3%. Results on Dream 8 Preprint. Figure 5: Average token confidence at the 8th and 16th decoding steps for LLaDA-8B-Instruct Model on GSM8K. The proposed certainty-forcing strategy reshapes the original sequential certainty convergence into faster and more parallel convergence process. Table 3: Ablation study on different training strategies of our method using the LLaDA model. Consistency Loss Certainty Loss Semi-AR Masking GSM8K-CoT (0-shot) HumanEval (0-shot) #Steps Speed Acc #Steps Speed Acc 53 23 44 30 4.5 10.4 5.5 8.5 73.5% 57.8% 73.3% 76.1% 71 28 61 33 3.6 9.8 4.3 8.2 36.0% 30.5% 32.9% 40.2% exhibit similar curve. These findings strongly demonstrate that our method effectively broadens the boundary of parallel decoding in diffusion language models. Faster and Parallel Certainty Convergence. As illustrated in Fig 5, the original dLLM exhibits sequential convergence of token certainty, where each step produces high confidence only for small set of neighboring tokens, while the majority remain in low-confidence range. Confidencebased decoding can extend the boundary of token certainty but still follows sequential propagation pattern. In contrast, our dParallel, trained with certainty-forcing distillation, transforms this process into significantly faster and more parallel convergence of certainty. Such parallel convergence further unlocks the potential of dLLMs for highly efficient parallel decoding. 5.3 ABLATION STUDY Ablation Study on Training Strategy. We conducted an ablation study to validate the effectiveness of our proposed certainty-forcing distillation, with the results shown in Table 3. When the certaintyforcing loss is removed, the remaining consistency loss is insufficient to alter the sequential convergence pattern of the dLLM, resulting in speed and performance similar to the baseline model. Conversely, applying only the certainty loss without enforcing trajectory consistency achieves high decoding speed but leads to sharp performance drop. Finally, our use of semi-autoregressive forward masking effectively aligns the trajectory generation process with self-distillation, yielding superior efficiency and performance. These results collectively demonstrate that each component in the training process is essential. Ablation Study on Masking Ratio. We conduct an ablation study to determine the optimal masking ratio, training LLaDA for one epoch with various settings as shown in Table 4. We find that fixed masking ratio of 50% yields the best performance, offering significant acceleration while preserving accuracy. In contrast, both higher and lower fixed ratios, as well as random ratios, lead to noticeable accuracy degradation. This suggests that 50% ratio creates an optimal trade-off between the training signals for consistency and certainty by balancing masked and unmasked tokens. Importantly, training with this fixed ratio does not impair the models ability to handle variable ratios during inference. Table 4: Performance of our method applied to LLaDA-8B-Instruct on GSM8K with different masking ratios used in the forward process during training. #Steps Random 25% 75% 100% 50% 72.4% 69.9% 73.7% 71.1% 76.3% 7.1x 7.3x 6.7x 8.3x 7.1x Speed Acc Masking Ratio 36 35 38 31 9 Preprint."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we present dParallel, simple yet effective method that unleashes the parallel decoding potential of dLLMs. At the core of our approach is certainty-forcing distillation, novel training strategy that maintains trajectory consistency while compelling high-certainty predictions, thus overcoming the sequential certainty propagation issue. Extensive experiments across various benchmarks validate the effectiveness of our method. Our work establishes new baseline for parallel decoding in dLLMs and explores new avenue for dLLM training paradigms."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021a. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021b. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Heli Ben-Hamu, Itai Gat, Daniel Severo, Niklas Nolte, and Brian Karrer. Accelerated sampling from masked diffusion models via entropy bounded unmasking. arXiv preprint arXiv:2505.24857, 2025. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai Li, Yiran Chen, et al. Dpad: Efficient diffusion language models with suffix dropout. arXiv preprint arXiv:2508.14148, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Justin Deschenaux and Caglar Gulcehre. Beyond autoregression: Fast llms via self-distillation through time. arXiv preprint arXiv:2410.21035, 2024. 10 Preprint. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. Diffucoder: Understanding and improving masked diffusion models for code generation. arXiv preprint arXiv:2506.20639, 2025. Daehoon Gwak, Minseo Jung, Junwoo Park, Minho Park, ChaeHun Park, Junha Hyung, and Jaegul Choo. Reward-weighted sampling: Enhancing non-autoregressive characteristics in masked diffusion llms. arXiv preprint arXiv:2509.00707, 2025. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed Abdelfattah, Jae-sun Seo, Zhiru Zhang, and Udit Gupta. Accelerating diffusion language model inference via efficient kv caching and guided diffusion. arXiv preprint arXiv:2505.21467, 2025. Daniel Israel, Guy Van den Broeck, and Aditya Grover. Accelerating diffusion llms via adaptive parallel decoding. arXiv preprint arXiv:2506.00413, 2025. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, and Dahua Lin. Beyond fixed: Variable-length denoising for diffusion large language models. arXiv e-prints, pp. arXiv2508, 2025a. Pengxiang Li, Yefan Zhou, Dilxat Muhtar, Lu Yin, Shilin Yan, Li Shen, Yi Liang, Soroush Vosoughi, and Shiwei Liu. Diffusion language models know the answer before decoding. arXiv preprint arXiv:2508.19982, 2025b. Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, and Aditya Grover. Lavida: large diffusion language model for multimodal understanding. arXiv preprint arXiv:2505.16839, 2025c. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Zhiyuan Liu, Yicun Yang, Yaojie Zhang, Junjie Chen, Chang Zou, Qingyuan Wei, Shaobo Wang, and Linfeng Zhang. dllm-cache: Accelerating diffusion large language models with adaptive caching. arXiv preprint arXiv:2506.06295, 2025. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating the ratios of the data distribution. 2023. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Xinyin Ma, Runpeng Yu, Gongfan Fang, and Xinchao Wang. dkv-cache: The cache for diffusion language models. arXiv preprint arXiv:2505.15781, 2025. 11 Preprint. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai arXiv preprint Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, and Zhijie Deng. Diffusion llms can do faster-than-ar inference via discrete diffusion forcing. arXiv preprint arXiv:2508.09192, 2025a. Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, and Mengdi Wang. RevolutionizarXiv preprint ing reinforcement learning framework for diffusion large language models. arXiv:2509.06949, 2025b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, and Linfeng Zhang. Accelerating difarXiv preprint fusion large language models with slowfast: The three golden principles. arXiv:2506.10848, 2025. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding. arXiv preprint arXiv:2505.22618, 2025. Zhihui Xie, Jiacheng Ye, Lin Zheng, Jiahui Gao, Jingwei Dong, Zirui Wu, Xueliang Zhao, Shansan Gong, Xin Jiang, Zhenguo Li, et al. Dream-coder 7b: An open diffusion language model for code. arXiv preprint arXiv:2509.01142, 2025. Chen Xu and Dawei Yang. Dllmquant: Quantizing diffusion-based large language models. arXiv preprint arXiv:2508.14090, 2025. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. 12 Preprint. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Qiuhua Yi, Xiangfan Chen, Chenwei Zhang, Zehai Zhou, Linan Zhu, and Xiangjie Kong. Diffusion models in text generation: survey. PeerJ Computer Science, 10:e1905, 2024. Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, and Chongxuan Li. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Runpeng Yu, Qi Li, and Xinchao Wang. Discrete diffusion in large language and multimodal models: survey. arXiv preprint arXiv:2506.13759, 2025a. Runpeng Yu, Xinyin Ma, and Xinchao Wang. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025b. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. ArXiv, abs/2207.01780, 2025. Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip Yu, et al. survey on parallel text generation: From parallel decoding to diffusion language models. arXiv preprint arXiv:2508.08712, 2025. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38363847, 2023. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025. 13 Preprint."
        },
        {
            "title": "A MORE IMPLEMENTATION DETAILS",
            "content": "In Table 5, we present the training configuration used for the certainty-forcing distillation process. For data generated by LLaDA-8B-Instruct (Nie et al., 2025) and LLaDA-1.5 (Zhu et al., 2025), we standardized sequence lengths by padding or truncating with the end-of-sequence token to fixed length of 384 tokens. In contrast, for Dream-7B-Instruct (Ye et al., 2025), we preserved the original response length of 256 tokens per sample without modification. Additionally, we set the balance weight β = 2 for all training. We also used complementary mask training strategy to improve token utilization. Our training was conducted on two NVIDIA H100 GPUs, with per-GPU mini-batch size of 1 and gradient accumulation step of 32, resulting in an effective global batch size of 64. Notably, despite the relatively large model sizes, the adoption of parameter-efficient fine-tuning (PEFT) (Hu et al., 2022) and the use of shorter sequence lengths kept the memory footprint remarkably low. The entire training process required only 23 GB of GPU memory, meaning that it can be efficiently reproduced even on multiple consumer-grade GPUs with 24 GB of memory each. This efficiency highlights the practicality of our approach, as it enables large-scale distillation training to be carried out on widely accessible hardware rather than being restricted to specialized high-memory accelerators. Table 5: The training configuration for certainty-forcing distillation across three base models. Learning Rate Lr-Schedule Batchsize Epoch Base Model LoRA Rank LoRA Alpha LLaDA-8B-Instrcut LLaDA-1.5 Dream-7B-Instrcut 32 128 32 128 16 2e-5 2e-5 2e-5 constant constant cosine 64 64 64 6 4 3 Table 6: Evaluation results on LLaDA-1.5 Model across four benchmarks. Benchmark Method #Steps Latency Speedup Accuracy GSM8K (0-shot) MATH (4-shot) LLaDA-8B-Instruct dParallel (Ours) LLaDA-8B-Instruct dParallel (Ours) HumanEval (0-shot) LLaDA-8B-Instruct dParallel (Ours) MBPP (3-shot) LLaDA-8B-Instruct dParallel (Ours) 256 30 256 45 256 46 256 26 19.1s 2.3s 50.0s 8.7s 22.0s 4.0s 49.0s 5.1s 1.0 8.5 1.0 5.7 1.0 5.6 1.0 9.8 76.0% 76.3% 34.0% 32.1% 41.5% 40.2% 43.2% 41.6%"
        },
        {
            "title": "B MORE EXPERIMENTAL RESULTS",
            "content": "In Table 6, we report the performance of applying our method to the LLaDA-1.5 model. Extensive evaluations across four standard benchmarks demonstrate the strong effectiveness of our approach on this reinforcement learning based model. Specifically, we reduce the original 256 decoding steps required by the baseline model to only 2646 steps. This dramatic compression of the decoding steps delivers substantial acceleration in generation speed, while at the same time preserving accuracy and reliability across tasks. In Figure 6, we present the average token confidence of the LLaDA-8B-Instruct model on GSM8K, measured across the first 160 positions over the initial 16 decoding steps. The results reveal that the original dLLM exhibits clear sequential convergence of token certainty: each step yields high confidence for only narrow band of neighboring tokens, while the majority remain in low-confidence range. Although confidence-based decoding can extend the certainty frontier, it still follows this sequential propagation pattern. By contrast, our proposed dParallel, trained with certainty-forcing 14 Preprint. distillation, reshapes this process into substantially faster and more parallel convergence of certainty. This parallel convergence further unlocks the efficiency potential of dLLMs, enabling highly parallel decoding."
        },
        {
            "title": "C CASE STUDY",
            "content": "We also present additional case studies in Figure 7, Figure 8, and Figure 9. Our dParallel achieves significantly reduced decoding steps while maintaining the generation quality."
        },
        {
            "title": "D LIMITATIONS AND FUTURE WORK",
            "content": "The primary limitation of our method is its reliance on the performance of the pretrained dLLM. While our approach achieves substantial gains in inference efficiency by unleashing the potential of parallel decoding and maintains strong accuracy, it cannot significantly improve the performance if the base model itself is weak. As next step, we plan to extend our certainty-forcing strategy to the pretraining stage of dLLMs and substantially scale up the training data to explore the performance boundary of our approach. Currently, we have only used relatively small dataset of around 10k math problems. We believe that by dramatically increasing both the size and diversity of the training data, our method can yield further improvements: not only activating highly parallel decoding, but also enhancing overall model performance and demonstrating stronger generalization."
        },
        {
            "title": "E ETHICS STATEMENT",
            "content": "This work adheres to the ICLR Code of Ethics. Our study does not involve human subjects or sensitive personal data. All datasets used are publicly available and properly licensed. While our method focuses on improving the efficiency of diffusion language models, we recognize potential risks of misuse in harmful applications and encourage responsible use aligned with ethical and legal standards."
        },
        {
            "title": "F REPRODUCIBILITY STATEMENT",
            "content": "We have made significant efforts to ensure the reproducibility of our work. Detailed descriptions of model architectures, training objectives, and experimental settings are provided in the main text and Appendix. All datasets used are publicly available, and their preprocessing steps are documented in the main paper and appendix. Additionally, we include pseudocode and implementation details to facilitate replication, and source code is provided in the supplementary materials. 15 Preprint. Figure 6: Average token confidence over the first 160 tokens across the first 16 decoding steps of the LLaDA-8B-Instruct model on GSM8K. Our certainty-forcing strategy transforms the sequential certainty convergence of the baseline into faster and more parallel convergence process. 16 Preprint. Figure 7: Case study on LLaDA-8B-Instruct Model with chain-of-thought reasoning problem. 17 Preprint. Figure 8: Case study on LLaDA-8B-Instruct Model with naive code generation task. Figure 9: Case study on LLaDA-8B-Instruct Model with instruction-based code generation task."
        }
    ],
    "affiliations": [
        "National University of Singapore"
    ]
}