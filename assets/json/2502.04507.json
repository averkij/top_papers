{
    "paper_title": "Fast Video Generation with Sliding Tile Attention",
    "authors": [
        "Peiyuan Zhang",
        "Yongqi Chen",
        "Runlong Su",
        "Hangliang Ding",
        "Ion Stoica",
        "Zhenghong Liu",
        "Hao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost -- when generating just a 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with a novel hardware-aware sliding window design, preserving expressiveness while being hardware-efficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D sliding-window-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.8-17x over FlashAttention-2 (FA2) and 1.6-10x over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only a 0.09% drop on VBench."
        },
        {
            "title": "Start",
            "content": "Peiyuan Zhang 1 Yongqi Chen * 2 Runlong Su * 1 Hangliang Ding 3 Ion Stoica 4 Zhengzhong Liu 5 Hao Zhang 1 5 2 0 2 6 ] . [ 1 7 0 5 4 0 . 2 0 5 2 : r Abstract Diffusion Transformers (DiTs) with 3D full attention power state-of-the-art video generation, but suffer from prohibitive compute cost when generating just 5-second 720P video, attention alone takes 800 out of 945 seconds of total inference time. This paper introduces sliding tile attention (STA) to address this challenge. STA leverages the observation that attention scores in pretrained video diffusion models predominantly concentrate within localized 3D windows. By sliding and attending over the local spatial-temporal region, STA eliminates redundancy from full attention. Unlike traditional token-wise sliding window attention (SWA), STA operates tile-by-tile with novel hardware-aware sliding window design, preserving expressiveness while being hardwareefficient. With careful kernel-level optimizations, STA offers the first efficient 2D/3D slidingwindow-like attention implementation, achieving 58.79% MFU. Precisely, STA accelerates attention by 2.817 over FlashAttention-2 (FA2) and 1.610 over FlashAttention-3 (FA3). On the leading video DiT, HunyuanVideo, STA reduces end-to-end latency from 945s (FA3) to 685s without quality degradation, requiring no training. Enabling finetuning further lowers latency to 268s with only 0.09% drop on VBench. 1. Introduction Diffusion Transformers (DiTs) have emerged as the leading architecture for high-resolution video generation, capable of synthesizing long-duration, visually coherent outputs (Peebles & Xie, 2023; OpenAI, 2024). Central to their success is 3D attention mechanism, which models spatial and temporal dependencies by flatterning video frames as unified *Co-second authorship. 1University of California, San Diego 2University of Michigan, Ann Arbor 3Tsinghua University 4University of California, Berkeley 5Mohamed bin Zayed University of Artificial Intelligence. Correspondence to: Hao Zhang <haozhang@ucsd.edu>. Preprint. Figure 1. (a) Generating 5s 720P clip in Hunyuan involves processing 115K tokens, making attention the dominant cost. (b) Attention latency comparison: existing methods fail to translate FLOP reduction into wall-clock speedup; STA is hardwareefficient and achieves proportional speedup with sparsity. sequence of visual tokens (Yang et al., 2024b; Genmo-Team, 2024; HunyuanVideo-Team, 2025). However, this design introduces significant computational overhead due to the quadratic complexity of attention, making training and inference prohibitively slow as video resolutions and durations increase. As illustrated in Figure 1, attention computation dominates the overall inference cost. Even with FlashAttention 3 (FA3) (Shah et al., 2024) and high-end H100 GPU, HunyuanVideo (HunyuanVideo-Team, 2025) still requires 16 minutes to generate 5-seconds 720p video. This bottleneck severely limits the practical deployment of DiTs. Video data inherently exhibit high redundancy adjacent frames exhibit minimal differences, and spatially close pixels tend to have stronger correlations. This redundancy suggests that treating every token independently in 3D attention may be unnecessarily expensive. In this paper, we hypothesize that such redundancies are carried by 3D full attention in pretrained video diffusion models, which, if properly exploited, can drastically accelerate inference. To verify this, we visualize the attention scores of HunyuanVideo in Figure 2. The results reveal an intriguing 3D locality pattern: queries assign significantly higher attention scores to spatially and temporally nearby keys. To quantify this effect, we compute attention recall, measuring the fraction of total attention scores concentrated within local window. As shown in Figure 3 left, despite training with full 3D attention, HunyuanVideo exhibits strong locality: on average, local window covering only 15.52% of the total token space Fast Video Generation with SLIDING TILE ATTENTION forming spatial-temporal cube, with its size determined by the block size in FlashAttention.Instead of sliding over contiguous tokens, STA operates tile-by-tile, enabling efficient memory access and parallelism while effectively preserving the 3D locality. Inspired by FA3, STA adopts consumer-producer paradigm, where producer warpgroups asynchronously load data from HBM to SRAM while consumer warpgroups compute attention. Because STA slides over tiles instead of individual tokens, it eliminates the need for explicit attention masking at computation, significant overhead observed in other SWA implementations. The sparse attention mask is managed entirely by the producer warpgroups; hence, the computation on consumer wrap groups remains dense and hardware-efficient. As result, STA is the first higher-order sliding-window-like attention to achieve wallcock speedups proportional to sparsity (Figure 1 (b)). Besides efficient computation, selecting optimal window sizes is crucial to preserving generation quality. We find that different attention heads exhibit specialized locality patterns some heads focus on finer details in small area, yet others capture broader context at larger window which we term as head specialization. Importantly, this head specialization remains agonistic to prompts, as evidenced in Figure 3. Based on this property, we develop simple yet effective method to automatically configure the optimal window size per head via profiling, striking balance between efficiency and quality. With STA, HunyuanVideo can generate 5-second 720P video in 695s or 578s with no or minimal quality loss in plug-and-play manner. In comparison, HunyuanVideo with FlashAttention-2 takes 1496s, while FlashAttention-3 takes 945s. STA achieves end-to-end speedup of 2.152.59 over FlashAttention-2 and 1.351.63 over FlashAttention-3. Additionally, by fine-tuning diffusion models under more radical attention sparsity, we unlock even greater efficiency, delivering 2.43 - 3.53 end-to-end speedup compared with FlashAttention3. This paper makes the following contributions: (1) We identify and quantify 3D locality and head specialization in stateof-the-art video DiTs, revealing substantial redundancy in full 3D attention. (2) We introduce SLIDING TILE ATTENTION, tile-based sliding window attention mechanism. Our optimized kernel achieves minimum overhead compared to FlashAttention 3 with an MFU of 58.79%. (3) STA accelerates attention by > 10 and end-to-end video generation by up to 3.53 with no or minimum quality loss. 2. Problem In this section, we provide background on why sliding window attention (SWA) is inefficient in high-dimensional settings, particularly for Video Diffusion Transformers (PeeFigure 2. Visualization of attention locality. The green point means the query point and the magma-colored regions indicate areas of high attention values in response to the query. Instead of attending to the entire image, the querys attention forms concentrated local hotspot. accounts for 70% of the total attention score. Figure 3. Left: Fraction of attention scores within (12, 24, 24) local window across diffusion steps and 10 different prompts. Most heads show high recall, indicating local attention pattern. Right: Despite the different recall across heads, the standard deviation across prompts remains low. This observation seemingly suggests that sliding window attention (SWA) is an ideal alternative to retain attention expressiveness while reducing computational cost. However, existing 2D or 3D SWA implementations, such as NATTEN (Hassani et al., 2023) and CLEAR (Liu et al., 2024), fail to translate FLOP reductions into proportional wall-clock speedups, as shown in Figure 1b. Their inefficiency arises because higher-order (2D/3D) sliding window attention creates highly irregular attention mask, wasting many computations and generating significant masking overhead. The computation pattern for SWA is inherently GPU-unfriendly, resulting in poor hardware utilization (see 2.2). To overcome this, we develop SLIDING TILE ATTENTION (STA), hardware-aware attention mechanism that rethinks sliding window computation via system-algorithm co-design. We define tile as contiguous group of tokens 2 Fast Video Generation with SLIDING TILE ATTENTION bles & Xie, 2023). For clarity, our notation assumes cubic window, tile, and video sizes unless stated otherwise, though our approach extends to non-cubic configurations. through input tiling and kernel fusion, but its performance suffers from problems described next. 2.1. Attention in Video DiTs State-of-the-art Video DiTs employ 3D full attention to mix signals across tokens, allowing each token to attend to any other token. Given video latent of shape (L, L, L) (often encoded via VAE), this is achieved by flattening the 3D data into sequence of length L3 and applying full bidirectional attention. However, as the sequence length grows cubically with L, even small increase in resolution or duration leads to significant computational burden. As result, applying 3D attention to high-resolution, longduration videos becomes prohibitively expensive. Formally, let Q, K, RN represent the query, key, and value of input sequences for single attention head, where = L3, and is the dimension of each head. Let {, 0}N represents the attention mask. The attention operation is defined as: = QK dk , = Softmax(S + ), = AV (1) naive attention implementation constructs the full S, A, RN on GPU HBM, leading to both O(N 2) memory overhead and excessive data transfers between HBM and SRAM. FlashAttention mitigates this issue by tiling input sequences into smaller blocks. Through online softmax, each GPU SM loads block of queries, keys, and values into SRAM, performs computation, and writes the final result to HBM, avoiding the materialization of and S. To reduce computation cost, we can apply attention mask to control sparsity, with the mask computed on-chip per block to avoid the O(N 2) memory cost of global mask. This sparsity can reduce latency by skipping masked-out attention regions. However, as we show in the next section, this is not always effective. Sliding window attention (SWA) is widely used sparse attention method that reduces computation costs while preserving locality. In SWA, query attends only to keys within fixed window, and stacking multiple attention layers naturally expands the receptive field beyond the window size. SWA has been extensively studied in natural language processing (Beltagy et al., 2020; Jiang et al., 2023). As motivated in 1, state-of-the-art video diffusion models exhibit strong 3D locality pattern, making them natural candidate for applying 3D SWA. However, directly applying SWA to high-dimensional data fails to fully utilize GPU computation. Existing 2D/3D SWA kernels, such as Tiled NATTEN (Hassani et al., 2023), shift window centers at image and video boundaries to ensure each query attends to constant number of keys. It also improves kernel efficiency 3 2.2. Inefficiency of Sliding Window Attention Implementing 2D/3D SWA with FlashAttention requires defining its attention mask. As discussed in 2.1, FlashAttention calculates and applies masks at the block level. Based on different intra-block masking strategies, we categorize attention blocks into three types: dense (with all attention scores retained), empty (mask out all values), and mixed (with some scores removed), shown in Figure 4. Empty blocks can be entirely skipped during computation. Mixed blocks, while sparser than dense blocks, introduce significant overhead. First, they require full computation for the entire block before applying masks to retain or discard attention scores, introducing unnecessary computations. Second, to determine which position to retain or discard, the attention kernel needs to calculate the value of the mask based on the SWA pattern and the blocks position relative to the entire attention mask. The mask calculation introduces substantial overhead in the case of calculating the simple causal mask, FlexAttention (Dong et al., 2024) reports 15% overhead. We show in 4.1 that the mask overhead dominates the attention latency for more complex masking patterns such as sliding windows. In SWA, each query attends to distinct set of keys, resulting in zigzag pattern in the attention map and generating numerous mixed blocks, as shown in Figure 4 (a). Although the state-of-the-art sliding window attention implementation, Tiled NATTEN, aims to reorder the inputs to increase the number of dense blocks, significant portion of blocks remains as mixed blocks. In summary, SWA beyond 1D sequences introduces two major inefficiencies: (1) mixed blocks do not reduce FLOPs due to sparsity, and (2) they incur additional mask evaluation overhead, making them slower than dense blocks. Efficient sparse attention in 3D should minimize mixed blocks while maintaining locality. This insight drives the development of our novel STA method, which significantly reduces mixed blocks while preserving the locality property of SWA. 2.3. Alternative Methods for 3D Attention Other than sparsifying the attention maps, some approaches accelerate video diffusion by decomposing the 3D attention into alternating spatial and temporal components (Singer et al., 2022; Wang et al., 2023; Chen et al., 2023; Ma et al., 2024). However, these methods have been largely superseded by full 3D attention in state-of-the-art video models (Zheng et al., 2024; Lin et al., 2024; Genmo-Team, 2024; HunyuanVideo-Team, 2025). We hypothesize that this is because alternating spatial and temporal attention fails to capture interactions between tokens that are offset in both spatial and temporal dimensions. For instance, query at Fast Video Generation with SLIDING TILE ATTENTION Figure 4. The attention map of NATTEN, Tiled NATTEN, and STA. We plot with an image size 2424 and 1212 local window. The tile size is set to 44. (a) NATTEN creates many mixed blocks that are very inefficient for Flash Attention computation. (b) Tiled NATTEN increases the number of dense blocks, but the mixed blocks persist. (c) STA completely eliminates the mixed block, making the computation extremely friendly for GPU. Note that we mainly show STAs application in 3D scenarios for video generation in this paper, but for better illustration, we present the 2D scenario in this plot. (1, 1, 1) can attend to keys at (1, X, X) or (X, 1, 1), but not at (2, 2, 2), even though they are spatially close. This disrupts the 3D locality pattern, which we have shown as key characteristic of video diffusion models. 3. Methods 3.1. SLIDING TILE ATTENTION In vanilla sliding window attention, each query attends to local window centered around it, resulting in different queries attending to distinct key groups. This lack of shared attention key groups is the root cause of irregularities in SWAs attention map, creating mixed blocks. We propose SLIDING TILE ATTENTION (STA), novel sliding window attention variant that exclusively operates on dense blocks and empty blocks. As shown in Figure 5, STA organizes queries and keys into tiles. All queries in the same tile attend to the same set of keys within their common local window, ensuring more structured attention pattern. By setting the tile area equal to the block size in FlashAttention and arranging queries in tile with consecutive token indices, we form dense FlashAttention blocks. This design allows each query tile to attend densely to key tiles within the window, eliminating mixed blocks and improving compute efficiency. We illustrate STA attention mask in Figure 4 (c). Table 1. Ratio of dense and mixed blocks for tiled NATTEN and STA with tile size (4,4,4) and video size (48,48,48). STA generate only dense blocks, which is more computationally friendly than mixed blocks in GPU. Attention Window Size Dense Block Mixed Block Tiled NATTEN STA STA (11,11,11) (12, 12, 12) (20, 20, 20) 0.06% 1.56% 7.23% 7.17% 0.0% 0.0% Formally, for 3D STA, given video of dimension (L, L, L) and FlashAttention block size of (B, B), STA sets the tile size such that = 3. It further assumes that both the video size and window size are integer multiples of . The video is partitioned into non-overlapping tiles of size (T, T, ), and flattened into 1D sequence in way that tokens within the same tile have consecutive sequence indices. Conceptually, STA slides the window with step size of (T, T, ). For each step, it computes attention between the central query tiles and all key tiles within the window, producing (cid:0) dense attention blocks without mixed blocks. (cid:1)3 To demonstrate STA superiority in creating GPU-friendly compute pattern, we give the following formula to quantitatively measure the different types of blocks in 3D Tiled NATTEN and 3D STA. 4 Fast Video Generation with SLIDING TILE ATTENTION tionality to skip all empty blocks and avoid adding unnecessary intra-block mask on the dense blocks. We can further optimize the sparse attention masks by disaggregating the inter-block mask logic from the compute kernels. Thus, we implement our attention kernels based on ThunderKittens (Spector et al., 2024) and FlashAttention3 (Shah et al., 2024). Our implementation split the threadblock into compute warpgroups and data warpgroups, and the inter-block mask is completely managed by the data warpgroups. Each compute warpgroup is responsible for calculating one query block, which always resides in the SRAM (Split-Q (Dao, 2024)). The data warpgroup is responsible for asynchronously loading the KV blocks from HBM to SRAM. For each block of query, the data warpgroup needs to decide which key and value blocks the query block will attend to in STA and only load those blocks. Since the data warpgroups are asynchronous, the overhead of calculating the inter-block mask in STA and deciding which data to load can be hidden with overlapping. On the other hand, the compute worker is completely oblivious of the sparse attention pattern. It performs attention computation with the key value blocks in shared memory loaded by data workers, and once all data is consumed in the circular cache, the computation is finished. 3.2. Applying STA to Video Diffusion Model Algorithm 1 STA Mask Search Input: Transformer Model , total steps , Mask Pattern list P, threshold δ Output: Dictionary dict that stores mask pattern result Initialize dict for = 1 to do (output of original ) for each layer head combination (l, h) in do for each in (ordered by descending sparsity) do Mask head for layer using mask pattern (output of after masking) if SE(O, O) < δ then Record for (h, l, t) in dict break return dict We can either apply STA to directly replace the 3D attention in pretrained video DiTs without training, or with small amount of training which enables even greater sparsity. Training-free. As illustrated in Fig. 3, video diffusion models exhibit pronounced 3D locality and head specialization pattern. Different transformer heads have different levels of locality, but their pattern is largely consistent across different prompts. We can exploit this property to search for the optimal window size for each head on very small number of prompts, and expect the search result to work well on Figure 5. 2D SLIDING TILE ATTENTION with tile size (2, 2) and window size (6, 6). After attending to all the key tiles, each query tile will generate nine 4x4 dense blocks in the attention map. We showcase 2D STA for better illustration. 3D STA can be inferred similarly. Theorem 3.1. Consider tiled NATTEN configuration with tile size (T, T, ), window size (W, W, ), and video size (L, L, L). Let the FA block size be (B, B), where = 3. Ignoring boundary effects, the number of dense blocks is given by: Ndense = (cid:18) (cid:16) max (cid:107) (cid:106) + 1 2 2T (cid:17)(cid:19)3 1, (cid:18) (cid:19)3 . The number of mixed blocks in tiled NATTEN is: Nmix = (cid:25) (cid:18) 2 (cid:24) 1 2T (cid:19)3 + 1 (cid:19) (cid:18) Ndense. Intuitively, for block to be dense in NATTEN, the window size should be at least twice the size of the tile size, such that the left-most query in the tile can attend to the right-most query. On the other hand, the left-most query in tile can still attend to keys that are 1 2T tiles further left, creating mixed blocks. Theorem 3.2. With the same notation, if is an integer multiple of , the number of dense blocks in SLIDING TILE ATTENTION is: Sdense = (cid:19)3 (cid:18) (cid:18) (cid:19)3 . All remaining blocks are empty and there is no mixed blocks. Intuitively, each query tile will only attend to its local window in STA, which has (cid:0) tiles of keys, creating the same number of blocks in the attention map. We apply Theorem 3.1 and Theorem 3.2 to calculate the ratio of different blocks and report them in Table 1. (cid:1)3 Kernel-level optimization. STA can be efficiently implemented with FlexAttention, which provides enough funcFast Video Generation with SLIDING TILE ATTENTION Figure 6. Qualitative example of 720P 5-second videos. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by -DiT are generally less sharp than those generated by the original HunyuanVideo and STA. other prompts. We develop simple heuristics to find such configuration in Algorithm 1. By adjusting the hyperparameter threshold δ, we can balance the speedup and the quality loss. Finetuning. Beyond searching for the optimal mask per attention head without tuning, we can fix window size with high sparsity and fine-tune the model to adapt. Since STA follows the 3D locality property, this adaptation can be learned efficiently with minimal training overhead (in our experiments, 8 hours on 8 H100, which is minimal compared to the pretrain cost of video diffusion models). Although each attention layer is restricted to local window, the receptive field expands through stacked transformer layers, enabling the Diffusion Transformer to generate globally coherent videos in the end. We use three different loss terms during finetuning. The attention distillation loss directly supervises the intermediate attention patterns of our STA to match the original dense attention behaviors: Lattn = 1 (cid:88) i=1 (i) ϕ (xt, t, c) (i) ψ (xt, t, c)2 2, (2) 6 ϕ and (i) where (i) ψ denote the intermediate attention outputs from the i-th transformer layer of our sliding tile model and the original attention teacher. This loss ensures each sparse attention layer to approximate its corresponding dense attention teacher. We also add final layer loss to align the final output of the student and teacher: Lf inal = fϕ(xt, t, c) fψ(xt, t, c)2 2 (3) Additionally, we employ data loss following the flow matching formulation (Esser et al., 2024; Lipman et al., 2022): Ldata = (f x0) fϕ(xt, t, c)2 2, (4) where x0 represents the VAE latent of the input frame, xt is the noised latent at diffusion step t, and denotes the text embedding. The complete objective combines these terms: Exp(x),cN (0,1),t[αLdata + βLf inal + γLattn] (5) min ϕ The detailed training setup can be found in Appendix B. Fast Video Generation with SLIDING TILE ATTENTION Table 2. Forward speed of sparse attention kernels in setup aligned with HunyuanVideos inference configuration (bf16, 720P, 5s, 115.2K seq len, dhead = 128, # heads = 24). Config controls the window size of each sparse attention. Methods Implementation Config Sparsity TFLOPS Latency(ms) MFU Kernel Efficiency Speedup FA 3 FA 3 ThunderKittens CUDA - - CLEAR NATTEN Tiled NATTEN Tiled NATTEN Swin STA STA STA FlexAttention FlexAttention CUDA FlexAttention FlexAttention FlexAttention ThunderKittens ThunderKittens r=16 w=(19,25,25) w=(19,25,25) w=(19,25,25) w=(24,32,32) w=(18,24,24) w=(30,40,40) w=(18,24,24) 0.00% 0.00% 90.46% 89.69% 89.69% 89.69% 87.42% 91.00% 58.33% 91.00% 164.03 164.03 15.65 16.91 16.91 16.91 20. 14.76 68.35 14.76 265.28 256.59 307.44 313.92 458.36 208.36 47.90 36.36 111.73 25.38 62.49% 64.61% 5.15% 5.44% 3.73% 8.20% 43.55% 41.03% 61.82% 58.79% 100.00% 103.39% 8.24% 8.71% 5.97% 13.12% 69.69% 65.66% 98.93% 94.09% 1.00 1.03 0.86 0.85 0.58 1.27 5.54 7.30 2.37 10.45 4. Experiments We evaluate STA on HunyuanVideo, state-of-the-art open video DiT comparable to many proprietary ones1. We generate Hunyuan outputs with 117 frames at 1280768 resolution. After VAE compression and tokenization, this corresponds to latent video of shape (30, 48, 80). Beyond video, we also apply STA on the leading image diffusion model, FLUX (Black-Forest, 2023), to demonstrate its effectiveness in 2D. We evaluate both efficiency and video quality. STA kernels efficiency is measured using standard metrics such as MFU and latency, as detailed in 4.1. For end-to-end speedup on DiT, we report measured wall-clock latency, excluding time spent on VAE and text encoder. For generated video quality, we find existing automated metrics are often unreliable. Following Polyak et al. (2024), we emphasize human evaluation and present the results in 4.2. For completeness, we also report automated metrics, including VBench (Huang et al., 2024), SSIM, PSNR, and CD-FVD (Ge et al., 2024). We provide an example in Figure 6, with additional qualitative results available in Appendix Section F. Baseline methods. We compare STA to other sparse or window attention designed for image or video, including CLEAR (Liu et al., 2024), NATTEN (Hassani et al., 2023), and Swin (Liu et al., 2021b). Also, we adapt the cachingbased method, -DiT (Chen et al., 2024), for evaluation on HunyuanVideo. Further details on the baseline methods and their implementations can be found in Appendix C. 4.1. Efficiency of SLIDING TILE ATTENTION We benchmark the efficiency of various attention algorithms assuming generating 720P 5s videos using Hunyuan, shown 1We skip evaluating on other open models (Lin et al., 2024; Zheng et al., 2024; Ma et al., 2024) due to their significantly lower overall quality compared to Hunyuan. in Table 2. The configuration ensures that all sparse kernels maintain approximately 90% sparsity, with additional results for lower sparsity setting (56%) provided in table 6. Since STA builds on FA3 and ThunderKittens, we use ThunderKittens FA3 as the baseline and report the relative speedup of all sparse attention kernels. To quantify efficiency, We introduce kernel efficiency, defined as the ratio of sparse kernels MFU to that of full attention. This metric captures how well sparse kernels translate theoretical FLOP reductions into actual latency improvements. The results highlight the inefficiency of existing methods. Despite reducing TFLOPs to 15.65, CLEAR incurs 0.86 slowdown. Similarly, NATTEN variants, despite reaching 0.91 sparsity, still suffers from inefficiency: its vanilla version slows down by 0.85, while its optimized tiled variant in FlexAttention achieves only modest 1.27 speedup. Among existing methods, Swin (Liu et al., 2021a) is the only kernel with MFU exceeding 40% and kernel efficiency above 60%. However, Swin is not sliding-window-based attention, and we argue its efficiency comes at the cost of expressiveness in 4.4. Compared to Tiled NATTEN, one of the most optimized sliding window attention implementations, the key algorithmic difference in SLIDING TILE ATTENTION is changing the sliding unit from token to tile. Despite its simplicity, this modification significantly improves efficiency. To ensure direct comparison with tiled NATTEN, we also implement STA in FlexAttention STA improves MFU from 8.20% to 41.03%. Further, with our optimized kernel for asynchronous data loading and inter-block mask management in ThunderKittens, STA achieves 10.45 speedup over full attention. Additionally, we evaluate STA with 58.33% sparsity, where it achieves 2.37x speedup. This efficiency gain enables significantly larger window size while still outperforming NATTEN. To our knowledge, STA is the first sliding-window sparse attention that achieves both 3D 7 Fast Video Generation with SLIDING TILE ATTENTION Table 3. Training-free performance with varying sampling steps. -DiT shows consistently worse quality compared to STA, and that gap widens as the number of inference steps decrease. Model SSIM PSNR CD-FVD Latency Speedup steps = 50 -DiT STA 72.86% 76.21% steps = 25 -DiT STA 77.91% 82.47% steps = 10 -DiT STA 83.19% 87.15% 18.09 19.94 19.86 22. 21.20 24.04 122.74 97.03 196.25 95.86 201.24 80.41 693s 695s 352s 348s 144s 139s 1.36 1.36 1.34 1.36 1.32 1.36 inference following Hunyuans default settings. Our training-free STA consistently outperforms -DiT, with the quality gap widening as the step count decreases. At 50 steps, -DiTs CD-FVD score is 25.71 higher than STA (122.74 vs. 97.03, where lower is better). This gap grows to 100.39 at 25 steps and 120.83 at 10 steps. Qualitatively, -DiT produces structurally similar but visually degraded outputs, explaining the narrow SSIM gapSSIM captures structural similarity but not fine details. These results suggest that caching-based methods degrade in very low-step sampling, while STA maintains fidelity to the original model. Intuitively, caching methods exploit temporal redundancy, which diminish with fewer steps, whereas STA remains effective. This distinction suggests STA may complement step-reduction techniques like consistency distillation (Song et al., 2023), which we leave for future work. 4.4. Finetuning Results Fine-tuning on new data introduces slight distribution shifts, meaning the same prompt may yield different, yet highquality, video variants. Consequently, similarity metrics like PSNR become less suitable, and we instead rely on VBench(Huang et al., 2024), comprehensive benchmark for video generation. We first examine the impact of directly replacing full attention with sparse attention, without tuning, to evaluate how well each algorithm approximates full 3D attention. In Table 4, CLEAR and Tiled NATTEN retain reasonable video quality (VBench scores of 82.37% and 82.68%, respectively) compared to full attention (82.71%). However, despite sparsifying attention, these methods paradoxically increase end-to-end inference latency. Swin presents the opposite challenge: while it achieves moderate speedup (1.241.90), its rigid, nonoverlapping window partitions prevent local queries and keys from attending to each other if they fall into separate windows, violating the 3D locality property. This results in degraded video quality, and crucially, fine-tuning with Swin attention not only fails to recover performance but further Figure 7. Human evaluation on 200 prompts from the MovieGen Bench (Polyak et al., 2024). STA achieves 1.36 end-to-end speedup while maintaining performance comparable to the original HunyuanVideo. Additionally, STA consistently outperforms - DiT across different inference budgets. locality and hardware efficiency. 4.2. Human Evaluations We assess human preference across five models that achieve the best quality performance: (1) HunyuanVideo; (2) STA-tf1.36x: HunyuanVideo with 1.36 speedup via training-free mask search, (3) STA-t-2.43x: HunyuanVideo with 2.43 speedup via finetuning with STA, (4-5) two variants of - DiT (1.36 and 1.8 speedup). Other baselines such as CLEAR or Swin are either prohibitively slow or produce subpar quality. Following MovieGen (Polyak et al., 2024), we randomly sample 200 prompts from the MovieGen Bench and conduct pairwise comparisons between these models. Evaluators select the video with higher overall quality or mark both as tie. In Figure 7, STA-t-2.43x decisively outperforms -DiT1.8x, achieving dominant 69.8% win rate versus 11.1%, despite greater speedup. Similarly, STA-tf-1.36x surpasses -DiT-1.36x with 58.5% win rate against 11.0%. Compared to the original HunyuanVideo, STA maintains competitive quality, with STA-tf-1.36x achieving 74.5% tie rateindicating near-parity in most cases. Though it has 5.5 percentage point lower win rate than its loss rate, this tradeoff comes with 1.36 speedup, demonstrating strong quality preservation alongside significant efficiency gains. These results establish STA as achieving superior quality-efficiency tradeoff compared to -DiT. 4.3. Training-free Results In Table 3, we evaluate mask-search STA and -DiT on VBench prompts, testing robustness across different sampling steps. Our focus is on preserving video quality in low-step regime. For each diffusion step count, we report SSIM, PSNR, and CD-FVD, using HunyuanVideos outputs at the same step count as the reference. We set the scheduler shift to 17 for 10-step inference and 7 for 25and 50-step 8 Fast Video Generation with SLIDING TILE ATTENTION Table 4. Performance on VBench across different sparse attention patterns. STA achieves both high-quality video generation and significant speedup, while CLEAR and Tiled NATTEN suffer from efficiency issues and Swin suffers from quality degradation. Methods Config FA2 FA3 w.o training CLEAR Tiled NATTEN Swin Swin STA STA w. training Swin STA STA r=32 w=(30,41,41) w=(48,64,64) w=(30,40,40) w=(30,40,40) w=(18,24,24) w=(30,40,40) w=(30,24,40) w=(18,24,24) VBench Quality 85.34% 85.34% VBench Semantic 72.17% 72.17% VBench Total 82.71% 82.71% 84.41% 84.61% 80.91% 78.84% 84.63% 81.47% 77.50% 85.37% 84.76% 74.20% 75.00% 71.35% 72.28% 73.83% 77.03% 67.39% 73.52% 74.05% 82.37% 82.69% 79.00% 77.53% 82.46% 80.58% 75.48% 83.00% 82.62% Attn Sparsity PFLOPS Latency Speedup 0.00% 0.00% 56.23% 58.33% 55.81% 76.49% 58.33% 91.00% 55.81% 75.00% 91.00% 574.16 574.16 280.90 269.92 283.11 175.20 269.92 99.54 283.08 182.99 99. 1496s 945s 2567s 1858s 762s 497s 527s 268s 497s 388s 268s 0.63 1.00 0.37 0.51 1.24 1.90 1.79 3.53 1.90 2.44 3.53 lowers the VBench score. In contrast, STA addresses both quality and efficiency limitations. With window configuration of wt=(3,3,3), it achieves 91.00% attention sparsity, yielding 5.76 FLOPs reduction and 3.53 actual latency reduction.2 Importantly, this efficiency gain comes with minimal quality tradeoff: STA maintains an 80.58% VBench score in the training-free setting and improves to 82.62% with fine-tuning. 4.5. Results on Image Super-Resolution We also apply STA to speed up image superresolution with SDEdit (Meng et al., 2022). We find that FLUX with STA achieves comparable generation quality to CLEAR while offering significantly higher efficiency. Experiments for can be found in Appendix D. 5. Related Work We review additional related work in diffusion acceleration. Linear attention methods (Wang et al., 2020; Liu et al., 2021a; Arar et al., 2022; Yang et al., 2024a) can decompose the softmax operation in quadratic attention using kernel or gate functions to achieve linear complexity. However, these methods have not yet been successful in video DiTs. Another major bottleneck in diffusion models is the large number of diffusion steps. Several techniques have been proposed to accelerate sampling without sacrificing quality, including DDIM (Song et al., 2020) and faster ODE and PDE solvers using numerical methods (Song & Ermon, 2019; Lu et al., 2022a;b; Jolicoeur-Martineau et al., 2021). New methods have also emerged to further reduce the num2Other memory-bound operations, such as LayerNorm and modulation, likely contribute to inference overhead, preventing the full FLOPs reduction from translating directly into speedup. ber of sampling steps, such as consistency distillation (Kim et al., 2023; Song et al., 2023; Salimans et al., 2024; Xie et al., 2024), adversarial distillation (Sauer et al., 2023), and other distillation approaches (Li et al., 2024; Yin et al., 2023; 2024). STA is largely complementary to these methods. 6. Conclusion and Future Work We introduce SLIDING TILE ATTENTION to accelerate video diffusion models, with an optimized kernel for highorder sliding-window-like attention, enabling efficient GPU execution while preserving the locality property. Experiments demonstrate that SLIDING TILE ATTENTION accelerates video generation with minimal or no quality loss. Conceptually, our method is orthogonal to other acceleration techniques, such as caching and consistency distillation. We plan to explore their combined effectiveness for further efficiency gains in future work."
        },
        {
            "title": "Impact Statement",
            "content": "Our work addresses computational bottlenecks in Diffusion Transformers by introducing efficient attention kernels that reduce video generation time while maintaining output quality. The improved efficiency makes video generation more practical for researchers and developers working with limited computing resources, potentially benefiting AI-driven video applications across creative industries, education, and so on. While faster video generation could potentially enable misuse, existing content detection and watermarking techniques can help mitigate such risks. Overall, the benefits of more efficient video generation significantly outweigh potential concerns, representing meaningful step toward accessible video AI systems. 9 Fast Video Generation with SLIDING TILE ATTENTION"
        },
        {
            "title": "References",
            "content": "Arar, M., Shamir, A., and Bermano, A. H. Learned queries for efficient local attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1084110852, 2022. Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer, 2020. URL https: //arxiv.org/abs/2004.05150. Black-Forest. Flux. black-forest-labs/flux, 2023. https://github.com/ Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X., Weng, C., and Shan, Y. Videocrafter1: Open diffusion models for high-quality video generation, 2023. URL https://arxiv.org/ abs/2310.19512. Chen, P., Shen, M., Ye, P., Cao, J., Tu, C., Bouganis, C.- S., Zhao, Y., and Chen, T. Delta-dit: training-free acceleration method tailored for diffusion transformers. CoRR, abs/2406.01125, 2024. URL https://doi. org/10.48550/arXiv.2406.01125. Dao, T. Flashattention-2: Faster attention with better In The Twelfth Inparallelism and work partitioning. ternational Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=mZn2Xyh9Ec. Dong, J., Feng, B., Guessous, D., Liang, Y., and He, H. Flex attention: programming model for generating optimized attention kernels, 2024. URL https: //arxiv.org/abs/2412.05496. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Ge, S., Mahapatra, A., Parmar, G., Zhu, J.-Y., and Huang, J.-B. On the content bias in frechet video distance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 72777288, June 2024. Genmo-Team. Mochi 1. https://github.com/ genmoai/models, 2024. Hassani, A., Walton, S., Li, J., Li, S., and Shi, H. Neighborhood attention transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Huang, Z., He, Y., Yu, J., Zhang, F., Si, C., Jiang, Y., Zhang, Y., Wu, T., Jin, Q., Chanpaisit, N., et al. Vbench: Comprehensive benchmark suite for video generative models. In 10 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. HunyuanVideo-Team. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.- A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mistral 7b, 2023. URL https: //arxiv.org/abs/2310.06825. Jolicoeur-Martineau, A., Li, K., Piche-Taillefer, R., Kachman, T., and Mitliagkas, I. Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080, 2021. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Li, J., Feng, W., Fu, T.-J., Wang, X., Basu, S., Chen, W., and Wang, W. Y. T2v-turbo: Breaking the quality bottleneck of video consistency model with mixed reward feedback. arXiv preprint arXiv:2405.18750, 2024. Lin, B., Ge, Y., Cheng, X., Li, Z., Zhu, B., Wang, S., He, X., Ye, Y., Yuan, S., Chen, L., et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Dollar, P. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405. 0312. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, S., Tan, Z., and Wang, X. Clear: Conv-like linearization revs pre-trained diffusion transformers up, 2024. URL https://arxiv.org/abs/2412.16112. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021a. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows, 2021b. URL https: //arxiv.org/abs/2103.14030. Fast Video Generation with SLIDING TILE ATTENTION Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Spector, B. F., Arora, S., Singhal, A., Fu, D. Y., and Re, C. Thunderkittens: Simple, fast, and adorable ai kernels, 2024. URL https://arxiv.org/abs/2410. 20399. Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P., et al. Lavie: Highquality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. Xie, Q., Liao, Z., Deng, Z., Tang, S., Lu, H., et al. Mlcm: Multistep consistency distillation of latent diffusion model. arXiv preprint arXiv:2406.05768, 2024. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2401.00002, 2024a. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all, March 2024. URL https: //github.com/hpcaitech/Open-Sora. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.-F., Chen, C., and Qiao, Y. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image synthesis and editing with stochastic differential equations, 2022. URL https://arxiv.org/abs/2108.01073. OpenAI. Sora, 2024. URL https://openai.com/ index/sora/. Accessed: [2024]. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Polyak, A., Zohar, A., Brown, A., Tjandra, A., Sinha, A., Lee, A., Vyas, A., Shi, B., Ma, C.-Y., Chuang, C.-Y., et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. Salimans, T., Mensink, T., Heek, J., and Hoogeboom, E. Multistep distillation of diffusion models via moment matching. arXiv preprint arXiv:2406.04103, 2024. Sauer, A., Lorenz, D., Blattmann, A., and Rombach, R. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Shah, J., Bikshandi, G., Zhang, Y., Thakkar, V., Ramani, P., and Dao, T. Flashattention-3: Fast and accurate attention with asynchrony and low-precision, 2024. URL https: //arxiv.org/abs/2407.08608. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: Text-to-video generation without text-video data, 2022. URL https: //arxiv.org/abs/2209.14792. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. arXiv preprint arXiv:1907.05600, 2019. 11 A. Further Details of SLIDING TILE Algorithm 3 Mask Definition of 3D STA Fast Video Generation with SLIDING TILE ATTENTION"
        },
        {
            "title": "ATTENTION",
            "content": "Mask of 3D NATTEN Algorithm 2 defines the attention mask in 3D NATTEN. First, it computes the window center for each query token. If the query is near the video edges, the center shifts inward to stay within bounds. Next, it determines the querys attention window within the spatiotemporal neighborhood. Finally, the mask is constructed by enforcing spatiotemporal constraints on query-key distances. Algorithm 2 Mask Definition of 3D NATTEN Input: Query coordinates (qt, qh, qw), Key coordinates (kt, kh, kw), Video size (Lt, Lh, Lw), Window size (Wt, Wh, Ww) Compute window center: qct max (cid:0)min (cid:0)qt, Lt 1 Wt qch max (cid:0)min (cid:0)qh, Lh 1 Wh qcw max (cid:0)min (cid:0)qw, Lw 1 Ww Compute masks: time constraint qct kt Wt 2 hori constraint qch kh Wh 2 vert constraint qcw kw Ww 2 return time constraint hori constraint vert constraint (cid:1) , Wt (cid:1) (cid:1) , Wh (cid:1) (cid:1) , Ww (cid:1) 2 2 2 2 2 Input: Query coordinates (qt, qh, qw), key coordinates (kt, kh, kw), video size (Lt, Lh, Lw), kernel size (Wt, Wh, Ww), tile size (Tt, Th, Tw) Compute QK coordinates in: qt,tile qt//Tt qh,tile qh//Th qw,tile qw//Tw kt,tile kt//Tt kh,tile kh//Th kw,tile kw//Tw Compute window size in tiles: Wt,tile Wt//Tt Wh,tile Wh//Th Ww,tile Ww//Tw Compute window center: (cid:16) qct max (cid:16) (cid:17) (cid:17) qch max min (cid:16) min (cid:16) qt,tile, (Lt//Tt 1) Wt,tile (cid:16) qh,tile, (Lh//Th 1) Wh,tile (cid:16) qw,tile, (Lw//Tw 1) Ww,tile 2 2 (cid:17) , Wt,tile 2 (cid:17) , Wh,tile 2 (cid:17) , Ww,tile 2 (cid:17) qcw max min Compute masks: time constraint qct kt,tile Wt,tile hori constraint qch kh,tile Wh,tile vert constraint qcw kw,tile Ww,tile return time constraint hori constraint vert constraint 2 2 2 2 Mask of 3D STA Algorithm 3 defines the mask for STA, introducing tile-based coordinate framework that differs from 3D NATTEN. First, query and key coordinates are mapped to tile coordinates, where each QK pair is assigned tile ID, with queries and keys in the same tile sharing the same ID. STA also computes the window center within tile coordinates, ensuring queries remain within valid bounds. Finally, neighboring keys are selected based on their tile distance from the querys window center. Tiling in STA Figure 8 illustrates STAs token tiling and ordering mechanism in 2D scenario, which extends naturally to 3D. Unlike conventional approaches that flatten 2D/3D data into 1D sequences using zigzag pattern, STA organizes tokens into tiles, ensuring that tokens within tile maintain neighboring sequence IDs. This ordering strategy preserves locality, so when tile attends to another tile, the resulting attention map forms dense block, as all participating sequence IDs remain consecutive. Visialization of 2D SWA In Figure 9, we illustrate how query tokens attend to its window key tokens. In 2D-SWA, the window slides token by token. For each window, SWA calculates the attention between the center with all keys within the window. 12 B. Finetuning Details We train on 2,000 synthetically generated videos from HunyuanVideo at resolution of 1280768 with 117 frames. The prompts are sourced from the Mixkit dataset (Lin et al., 2024). To reduce memory usage and accelerate training, we precompute VAE-encoded latents and text encoder states. Training involves fine-tuning for 1,600 steps with batch size of 2 and learning rate of 2e-5. We optimize using the loss function from Eq. (5) with coefficients α = 1, β = 0.5, and γ = 0.5. To prevent overfitting on single guidance scale, we alternate between guidance scales of 1 and 6 at odd and even steps. The entire process runs on 8 H100 GPUs with FSDP and context parallelism for training (8 hours) and sequence parallelism for inference. C. Further Details of Baselines Swin Transformer (Liu et al., 2021b) introduces hierarchical vision transformer with shifted window-based attention mechanism. Instead of computing self-attention globally, it partitions the image into non-overlapping windows and applies attention locally, improving computational efficiency. key innovation is the alternating window partitioning strategy: one layer uses standard window partitioning, while the next shifts the windows to enable cross-window connections Fast Video Generation with SLIDING TILE ATTENTION -DiT implementation is unavailable, we reimplemented its method based on the paper to accelerate video generation. Given speedup budget, we vary Nc , , and to pick the best hyperparameters, ensuring fair evaluation of its effectiveness. For the 50-step 1.36 speedup, we set Nc = 24, = 3, and = 24. For 1.8 speedup, we set Nc = 28, = 6, and = 24. D. Results on Image Super-Resolution Table 5. Image superresolution results with FLUX (Black-Forest, 2023) on 1000 captions randomly sampled from COCO-2014 (Lin et al., 2015) validation dataset. Methods SSIM PSNR Sparsity Latency Speedup 1K 2K CLEAR r=16 CLEAR r=32 STA w=(48,72) 2K4K CLEAR r=16 CLEAR r=32 STA w=(48,72) 0.9291 0.9443 0. 28.1142 29.6722 29.1086 96.12% 85.94% 81.25% 0.9394 0.9455 0.9470 29.0463 30.0742 30.1939 98.98% 96.08% 95.31% 13s 15s 14s 67s 92s 57s 1.54 1.33 1.43 2.90 2.11 3.40 E. More Experiment Results E.1. Kernel Performance We additionally benchmark various sparse attention kernels at sparsity level of around 56% and present the results in Table 6. With lower sparsity, sparse kernels generally have higher MFU, but the findings in Table 2 remain unchanged. E.2. Detailed VBench Results In Tables 7 and 8, we present detailed comparisons of VBench scores across key dimensions originally summarized in Table 4. Our analysis reveals that STA surpasses swin attention in video quality metrics such as Imaging Quality and Multiple Objects, while achieving comparable or superior scores to CLEAR and Tiled NATTEN. For trainingfree models, we observe systematic degradation in qualityrelated metrics (e.g., temporal flickering, motion smoothness) as sparsity increases in the STA attention mechanism. Conversely, semantic-aligned dimensionsincluding Appearance Style, Color, and Spatial Relationshipsimprove under higher sparsity regimes, phenomenon driven by the text embeddings amplified role in attention computation when spatial-temporal attention is sparsified. Furthermore, the trained STA demonstrates significant gains in video quality metrics over its untrained counterpart, while maintaining semantic coherence at comparable levels which underscores the efficacy of training in refining low-level visual fidelity without compromising text-video alignment. Figure 8. Left: Conventional zigzag flattening strategy. Right: STA sequence flattening strategy. The plot is given assuming (9, 9) image with (3, 3) tile size. Figure 9. 2D Sliding Window Attention visualization. and better information exchange. Swin attention is typically used in train-from-scratch setting. limitation of this approach is that it disrupts local connectivity within single attention layer. Tokens in adjacent regions may not attend to each other if they fall into separate windows. In this paper, we apply Swin attention to HunyuanVideo and shift the window every other layer accordingly. CLEAR (Liu et al., 2024) achieves linear attention by replacing the original full attention with circular windowbased attention mechanism where each query token only attends to key-value tokens within radius r, maintaining the same scaled dot-product attention formula but restricting its computation to local windows. The authors implement CLEAR with FlexAttention. -DiT (Chen et al., 2024) optimizes inference speed by caching feature offsets instead of full feature maps. It employs staged caching strategy: residuals from later DiT blocks are stored for early-step sampling, while residuals from earlier blocks are cached for later steps. The key parameters in -DiT include the residual cache interval , the number of cached blocks Nc, and the timestep boundary b, which determines the cache position. Since the official 13 Fast Video Generation with SLIDING TILE ATTENTION Table 6. Speedup with sparse attention kernels on H100. Methods Implementation Config Sparsity TFLOPS Latency(ms) MFU Kernel Efficiency Speedup FA 3 FA 3 ThunderKittens CUDA - - CLEAR NATTEN Tiled NATTEN Tiled NATTEN Swin FlexAttention FlexAttention CUDA FlexAttention FlexAttention r=32 w=(30,41,41) w=(29,41,41) w=(30,41,41) w=(48,64,64) STA STA FlexAttention ThunderKittens w=(30,40,40) w=(30,40,40) 0.00% 0.00% 56.23% 56.22% 57.68% 56.22% 55.81% 58.33% 58.33% 164.03 164.03 71.80 71.81 69.41 71.81 72. 68.35 68.35 265.28 256.59 675.05 804.62 173.57 409.89 127.51 174.17 111.73 62.49% 64.61% 10.75% 9.02% 4.04% 17.70% 57.46% 39.66% 61.82% 100.00% 103.39% 17.20% 14.43% 6.47% 28.33% 91.95% 63.46% 98.93% 1.00 1.03 0.39 0.33 0.15x 0.65 2.08 1.52 2.37 Model FA3 w.o training CLEAR Tiled NATTEN Swin w=(48,64,64) Swin w=(30,40,40) STA w=(30,40,40) STA w=(18,24,24) w. training Swin w=(30,40,40) STA w=(30,24,40) STA w=(18,24,24) Appearance Style 18.43% 18.73% 18.79% 20.85% 20.62% 18.79% 21.25% 20.07% 18.90% 18.90% Table 7. Model Performance Comparison - Part 1 Temporal Flickering 99.21% Background Consistency 96.74% Motion Smoothness 99.15% Subject Consistency 94.22% Dynamic Degree 75.00% Aesthetic Quality 64.63% Imaging Quality 67.97% Overall Consistency 25.96% 93.63% 94.59% 91.74% 90.33% 94.75% 89.66% 89.78% 94.90% 94.64% 96.51% 96.61% 95.48% 93.09% 96.50% 91.64% 94.93% 97.60% 96.76% 98.99% 98.75% 98.67% 98.78% 98.82% 98.46% 98.86% 99.68% 99.22% 99.01% 98.85% 97.77% 96.53% 98.83% 97.27% 96.64% 99.23% 99.11% 68.06% 70.83% 77.78% 75.00% 69.44% 83.33% 70.83% 73.61% 69.44% 63.75% 63.79% 51.01% 48.10% 64.18% 59.75% 44.91% 63.77% 64.52% 68.35% 68.16% 62.22% 61.89% 68.39% 64.23% 55.99% 66.21% 66.67% 26.23% 26.53% 25.27% 25.62% 26.47% 26.61% 26.00% 26.58% 26.09% Model FA3 w.o training CLEAR Tiled NATTEN Swin w=(48,64,64) Swin w=(30,40,40) STA w=(30,40,40) STA w=(18,24,24) w. training Swin w=(30,40,40) STA w=(30,24,40) STA w=(18,24,24) Table 8. Model Performance Comparison - Part 2 Object Classification 85.76% Human Multiple Objects Action 70.12% 90.00% 88.66% Color Spatial Relationship 71.28% Scene Quality Score 35.25% 85.34% Final Semantic Score Score 72.17% 82.71% 88.13% 83.54% 78.16% 79.19% 80.54% 88.13% 77.14% 91.77% 92.96% 77.97% 88.00% 91.10% 72.18% 94.00% 92.28% 58.54% 87.00% 93.68% 60.44% 88.00% 93.68% 71.19% 93.00% 89.81% 75.46% 91.00% 91.61% 48.86% 73.00% 87.00% 68.45% 86.00% 89.59% 74.16% 93.00% 84.50% 77.49% 81.21% 77.45% 77.24% 79.25% 82.52% 63.38% 72.76% 73.41% 32.85% 84.41% 37.94% 84.61% 37.79% 80.91% 35.54% 78.84% 36.77% 84.63% 42.15% 81.47% 74.20% 82.37% 75.00% 82.69% 71.35% 79.00% 72.28% 77.53% 73.83% 82.47% 77.03% 80.58% 39.03% 77.50% 39.53% 85.37% 38.23% 84.76% 67.39% 75.48% 73.52% 83.00% 74.05% 82.62% F. Qualitative Examples We show qualitatively show videos generated by the original HunyuanVideo, STA, and -DiT in Figure 10 and Figure 11. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by -DiT are generally less sharp than those generated by the original HunyuanVideo and STA. 14 Fast Video Generation with SLIDING TILE ATTENTION Figure 10. Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by -DiT are generally less sharp than those generated by the original HunyuanVideo and STA. Fast Video Generation with SLIDING TILE ATTENTION Figure 11. Qualitative comparisons. While fine-tuning introduces minor shifts in the output distribution of STA-t-2.43x, the model still preserves high video generation quality. Videos generated by -DiT are generally less sharp than those generated by the original 16 HunyuanVideo and STA."
        }
    ],
    "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence",
        "Tsinghua University",
        "University of California, Berkeley",
        "University of California, San Diego",
        "University of Michigan, Ann Arbor"
    ]
}