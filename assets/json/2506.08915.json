{
    "paper_title": "Inherently Faithful Attention Maps for Vision Transformers",
    "authors": [
        "Ananthu Aniraj",
        "Cassio F. Dantas",
        "Dino Ienco",
        "Diego Marcos"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. At the same time, many image-level object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose a two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds."
        },
        {
            "title": "Start",
            "content": "Ananthu Aniraj1,3 Cassio F. Dantas1,2,3 Dino Ienco1,2,3 Diego Marcos1,3 {ananthu.aniraj, diego.marcos}@inria.fr {cassio.fraga-dantas, dino.ienco}@inrae.fr 1Inria 2Inrae 3University of Montpellier 5 2 0 2 0 1 ] . [ 1 5 1 9 8 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce an attention-based method that uses learned binary attention masks to ensure that only attended image regions influence the prediction. Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-ofdistribution backgrounds. At the same time, many imagelevel object-centric tasks require identifying relevant regions, often requiring context. To address this conundrum, we propose two-stage framework: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling focused analysis while filtering out potentially spurious information. Both stages are trained jointly, allowing stage 2 to refine stage 1. Extensive experiments across diverse benchmarks demonstrate that our approach significantly improves robustness against spurious correlations and outof-distribution backgrounds. Code is available here. 1. Introduction Deep Learning (DL) models often rely on contextual cues to learn object representations. While this can be beneficial for certain tasks, it can also introduce spurious correlations on which the model learns to rely, hampering generalization [10, 46, 66]. common example is when models prioritize background cues over intrinsic object properties, leading to failures in out-of-distribution (OOD) settings where such correlations no longer hold [2, 5]. It is therefore crucial to ensure that the model focuses on task-relevant image regions and that users can assess whether the attended regions are appropriate. To obtain these insights, many post hoc explainability methods [35] have been proposed, commonly categorized as eXplainable AI (XAI) tools, which generate explanations in the form of saliency maps, providing glimpse into the models decision-making process without altering its structure. While post hoc methods are appealing because they Figure 1. Previous attention-based approaches apply the attention mask to deep feature tensor, where all locations can be affected by the whole image due to large receptive fields (top). Our approach ensures that only the selected tokens contribute to the downstream task (bottom). do not affect model performance, this also means that they are unsuitable to prevent the model from latching onto spurious cues. Additionally, these methods offer no guarantee that the explanations are faithful to the models reasoning [1, 13, 14], making failures difficult to detect [6] and potentially misleading users [48]. In contrast, models that integrate spatial attention maps directly into their inference process can help guiding the model towards focusing on the correct image regions and have the potential to provide guarantees of faithfulness, as they reveal the reasoning of the model rather than relying on post hoc approximation. Among these, part discovery methods [3, 18, 58] have gained prominence for inherently highlighting relevant object parts through learned attention maps. These methods typically compute the similarity between learned prototypes and high-level feature representations, using the resulting soft attention maps to assign greater importance to specific regions when forming the final image representation. 1 However, we argue that the attention maps produced by such methods do not fully capture the models reasoning, leading to the same reliability issues as post hoc approaches. Specifically, (i) high-level feature representations at later stages aggregate information from the entire image due to their large receptive field, resulting in unintended background dependence; and (ii) soft attention masks, being non-binary, assign non-zero weights to all locations, allowing further unintended information leakage. To address these issues, we propose two-step framework that jointly learns region selector and Vision Transformer (ViT)-based classification model, where the latter relies solely on the selected image regions (Figure 1). Building on recent part discovery method [3], we use discretized attention mapsformed by merging discovered partsto explicitly select image regions for second-stage classifier. This classifier, which also takes the raw image as input, has only access to the selected regions, thus mitigating spurious correlations present in other regions. Our approach provides an end-to-end signal that jointly optimizes both stages. Thus, our core contribution is model that explicitly ignores image regions that do not contribute to its prediction, ensuring robustness against spurious correlations present in those regions. This design allows for systematic evaluation using established benchmarks for robustness against spurious correlations. 2. Related Works Spatial attention in computer vision. Attention mechanisms induce the model to focus on subset of the input that is deemed relevant to solve the task at hand. Originally devised as means to reduce computational load in image classification [36], spatial attention mechanisms started to gain popularity for tasks such as captioning [69], visual reasoning [19], and other tasks [16] where sharp focus on sequence of relevant image regions allows the model to decompose the complex task into multiple, simpler ones. Recent work on part discovery [3, 18, 58] also leverages attention mechanisms. These approaches assume that focusing the attention on the correct parts will lead to better classification results, and leverage this learning signal to discover the semantic parts that compose the objects of interest. However, all of these methods apply attention to deep feature representations, where large receptive fields allow regions outside the attended area to influence the attended regions. This can potentially reduce faithfulness, or how well the attention map actually coincides with the image regions that matter for the downstream task. This has led to work aiming at measuring the faithfulness of attention maps in ViTs [63], as well as to methods improving it [40, 62, 68] Unlike these works, our two-stage framework ensures that the attention maps are inherently faithful by explicitly constraining the predictors receptive field. Local object representations. Object-centric computer vision tasks require representations that remain invariant to changes in backgrounds and co-occurring objects. Previous works provide local object representations via maskinvariance losses [56], clustering-like losses [72] or directly altering the attention mechanism [21]. While some methods aim to align post-hoc explanations with segmentation maps [47], they do not guarantee that only attended areas contribute to the decision, with studies highlighting information contamination from outside the object attention masks due to large receptive fields [2]. Input attention maps for interpretability. Auxiliary mask predictors have been proposed to explain black-box classifiers by identifying minimal masks that preserve predictions without retraining [7, 38, 44, 55, 71, 74]. Others use post hoc attribution maps to guide training [22]. Closer to our approach, joint amortized explanation methods (JAMs) [9, 15, 70] jointly learn selector and predictor models but risk encoding class information through the selection pattern [24, 45]. Although more recent methods have proposed solutions to alleviate this drawback, they involve either unstructured selection masks [24] or simplistic ones parametrized as single spatial Gaussian [15]. COMET [74] takes step further and aims at finding the complete foreground, rather than sufficient mask. In contrast to these works, our approach introduces mechanism specifically developed for ViTs and leverages recent advances in part discovery to provide rich spatial representation to the predictor. Empirical results show this improves performance, particularly in the presence of spurious cues. Input attention maps for robustness. Joint learning of input masks has also been explored to enhance model robustness. [65] shows that limiting the receptive field and applying targeted patch masking improves adversarial robustness. Spurious correlations can be mitigated by isolating foreground regions and constructing image composites with mismatched backgrounds [8, 39, 67], encouraging the model to rely on foreground cues. [4] masks key image regions using attribution maps, forcing the model to identify alternative features and assess potential spurious correlations. Multiple spurious cues can coexist in dataset, and techniques designed to mitigate one may inadvertently amplify another [28]. In this work, we leverage the part discovery mechanism to simultaneously model several of these correlations. 3. Methodology iFAM (Inherently Faithful Attention Maps for vision transformers) depicted in Figure 2, consists of two stages: the first one has access to the whole image and predicts which image regions should be selected for the second stage. These selected regions then define the receptive field used by the second stage for solving the downstream task. This 2 Figure 2. Left: iFAM first discovers task-relevant regions (Stage 1) and then classifies using only the selected regions (Stage 2), preventing reliance on background cues. Right: At test time, we leverage the models inherently faithful region attribution to design (training-free) intervention strategies that further enhance robustness to spurious correlations. design ensures that the second stage can only pay attention to the selected image regions, guaranteeing that it cannot make use of any information outside the mask. within fpred. Since the masking happens at the input level, the receptive field is determined by the mask for any aggregation method. 3.1. Early vs Late Masking Existing attention-based methods learn two functions on the input: selector fsel, with = fsel(x), and feature extractor fpred, with = fpred(x). The input RDinN is set of elements, such as pixels or tokens, RDoutN is set of feature vectors and {0, 1}N is binary selection mask1. An image feature vector RDout , to be used for some downstream task, is then computed as: = m(fpred(x), fsel(x)), (1) where m(, ) is some masking and aggregator function. common choice is weighted average: ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 sihi. (2) With our approach, the image feature vector is computed by applying the selector (stage-1) and the feature extractor (stage-2) sequentially: = fpred(m(x, fsel(x))), (3) where m(, ) is now masking function applied to the input of fpred, and the aggregation is assumed to be performed 1s [0, 1]N in case of soft selection mask. Implementation on ViT with attention masks. In the case that the model fpred is based on self-attention [59], such as ViT, m(, ) can be implemented by modulating the selfattention in each layer with mask RN : Attention(Q, K, V) = softmax (cid:32) QK (cid:33) + V, (4) where the elements in are defined as: (cid:40) Mij = , 0, if si = 0 or sj = 0 otherwise. (5) This forces the attention from and towards the masked out tokens to be zero after the softmax, preventing them from having any influence on the resulting image representation. 3.2. Stage 1: Identifying Relevant Image Regions To identify relevant image regions for the downstream task, we leverage the PDiscoFormer part discovery method [3]. This approach, guided solely by image-level class labels and part-shaping priors, partitions the image into + 1 regions, where distinct foreground parts are identified, and the remaining region represents the background, which is discarded. The discovered parts are shared across classes. Each part is associated with learned prototype, encouraging semantic consistency across the dataset. The prototypes 3 are also trained to be mutually de-correlated, so that each part captures distinct aspect of the object. We use the original papers default settings. 3.3. Stage 2: Masked-input classification PDiscoFormer suffers from the same issues that we have identified as flaws in attention mechanisms: it uses soft attention masks that are applied to high-level representation. To address this drawback, we propose to make the masks binary, via discretization, and to use them to explicitly define the receptive field of the second stage model, using Eq. (4). Discrete masks. PDiscoFormer produces part attention maps that assign, for each image token, weight distribution across parts, with weights summing to one. These weights are designed to approach hard assignment via Gumbel softmax, where one part receives weight close to one, while the others are close to zero. However, we emphasize that these maps still remain soft distribution across parts. This may seem as subtlety, but we argue that only truly discrete attribution map can provide faithfulness guarantees by fully preventing information leakage. To tackle this issue, we introduce discretization step for the obtained part maps prior to the second stage. At this point, the foreground parts are merged together to obtain binary input mask for the second stage model. With the aim to allow gradient flow between the second and first stages, we employ the straight-through gradient trick used by Gumbel softmax [23], where the hard masks are used in the forward pass and the soft ones in the backward pass. Input image masks. An additional requirement in order to prevent information leakage, related to the receptive fields of modern computer vision architectures, is to adopt early masking [2]. That is, masking directly the input of the model instead of doing so at higher-level representation. In this way, only the unmasked tokens are considered by the ViT, thus eliminating any possible information contamination from the unattended regions. Part dropout. During training, we randomly drop out discovered image parts with probability p. This not only helps to promote robustness to missing parts in the second stage (which will be useful for the intervention functionality discussed in Sec. 3.4), but also makes sure that all parts have the opportunities to backpropagate useful learning signals to the first stage, as the stage-2 model cannot always rely on single informative part to perform classification. 3.4. Test-time Correction/Interventions Although the stage-1 training objective encourages foreground discovery, spurious objects or correlations may still be captured due to the weakly supervised nature of the task. Unlike standard DL models, our framework is locally interpretable, meaning it faithfully reveals the image regions responsible for solving the task. This property enables targeted test-time corrections to mitigate learned spurious correlations. Here, we propose two intervention methods. Drop part that captures spurious object. The original PDiscoFormer, due to the asymmetry in the treatment of the background part, exhibits bias toward assigning as much as possible of the image content to the background, the unattended image regions. This implies that the discovered parts are typically the most informative for the downstream task, often corresponding to the image regions that are causally related to the classification label. However, when the number of parts is set sufficiently high, some parts may begin to focus on spurious correlations. iFAM allows the users to select, at inference time, subset of the discovered parts to feed into the stage-2 classifier. Since the part discovery component encourages each part to capture semantically consistent content across the dataset, this operation can be performed globally. This allows for the manual inspection of few images (see Appendix D) to gain insights into what each part captures. If one of the parts is found to consistently capture an element associated with spurious correlation, it can be excluded from the input to the second stage. Drop tokens assigned to part with low confidence. In cases where OOD objects present at inference time lead to false positive part detections, it is possible to simply remove the low confidence tokens from any given part. This can be achieved by checking whether the assigned parts are unexpectedly distant from the corresponding prototype in the feature space, based on statistics drawn from the training set [31]. Specifically, distance-based threshold τ can be calibrated on the training set given large percentile q, such that is the proportion of tokens assigned to part that have distance to the corresponding part prototype smaller than τ . At inference, tokens assigned to part with distance exceeding τ are reclassified as background. Finally, since these two approaches are complementary, the first addressing part-level intervention while the second covers individual tokens from all parts, they can be adopted simultaneously. 4. Experimental Setup We aim to discover task-relevant image regions using only image-level class labels, applying attention masking to restrict the predictors receptive field and focus solely on these regions. To evaluate the effectiveness of our approach, we use datasets with known background-related biases or other spurious correlations. All models are implemented in PyTorch. We use ViTB [12] with publicly available DINOv2 weights [41] for initialization in all experiments, except on SIIM-ACR, where we use RAD-DINO [43]. Training and compute details are provided in Appendix and Appendix B. Datasets and Evaluation Metrics. We evaluate our ap4 proach on two binary classification tasks: MetaShift cat vs. dog [29, 64] and Waterbirds [51], with spurious background correlations. In MetaShift, dogs predominantly appear in outdoor settings (e.g., bench, bike) and cats in indoor environments (e.g., sofa, bed) during training, while the test set contains only indoor backgrounds (e.g., shelf ), making dogs harder to detect. In Waterbirds, derived from CUB [60], species are assigned to waterbird and landbird classes with controlled background replacement. During training, 95% of waterbirds appear on water and 95% of landbirds on land, with the hardest groups thus consisting of waterbirds on land and landbirds on water. Both datasets report average accuracy (AA), which can be inflated by leveraging background correlations, and worst group accuracy (WGA), which measures robustness under background shifts. We also train on CUB as 200-way classification task and evaluate on Waterbird200 (CUB with artificial backgrounds) to assess robustness in finegrained scenarios. Additionally, we assess our approach on SIIM-ACR [73], chest X-ray dataset for pneumothorax (collapsed lung) detection, where positive samples are often biased by visible chest tubes [50]; WGA is computed on curated subset without this artifact. Finally, we test the scalability to larger datasets on the ImageNet-9 (IN9) Backgrounds Challenge [66], which allows direct evaluation of models trained on ImageNet-1K (IN-1K) [49] for background robustness. We focus on three IN-9 variants: Original (unaltered), Mixed-Same (same-class backgrounds), and Mixed-Rand (random-class backgrounds). BG-GAP [66] measures the accuracy drop from MixedSame to Mixed-Rand. 5. Results and Discussion 5.1. Results on robustness benchmarks [3], We compare our method against several approaches including late-masking-based PDisfrom the literature, standard CNN/ViT models, and dedicoFormer cated de-biasing methods, across MetaShift, Waterbirds, CUBWaterbirds200, SIIM-ACR, and IN-9. For MetaShift and Waterbirds, we also evaluated early and late masking techniques based on the result of saliency-based foreground detection method [52]. For datasets with pixel-level annotations (e.g., masks or boxes), we additionally report results from models trained with this extra supervision as upper bounds (shaded rows). The results in Tables 1, and 2 demonstrate that our twostep approach, which explicitly limits the receptive field of the predictor to the discovered foreground regions, leads to significant improvements in robustness on datasets with spurious background correlations. Qualitative results are provided in Appendix D. Results on MetaShift and Waterbird."
        },
        {
            "title": "Results on",
            "content": "(a) Results on Metashift and Waterbird MetaShift AA WGA Waterbird Method Early maskgt Late maskgt ResNet50 ERM [64] ViT-B ERM ViT-B DinoV2 (cid:94) ViT-B DinoV2 PCA [11] ViT-B DinoV2 ResNet50 MaskTune [4] ResNet50 GroupDRO [51] ResNet50 DISC [64] PDiscoFormer [3] PDiscoFormer [3] PDiscoFormer [3] Late maskf [52] Early maskf [52] iFAM iFAM iFAM iFAM iFAM+ AA WGA 97.2 99.2 84.0 95.7 63.7 97.0 80.7 95.0 88.5 95.9 94.0 97.4 95.8 98.6 86.4 93.0 90.6 91.8 88.7 93.8 87.4 96.0 84.3 94.2 85.1 95.9 83.3 95.3 95.2 98.6 95.8 98.7 96.4 98.7 99.0 97.0 97.0 98.8 97.4 98.8 (b) Results on ImageNet-9 (IN-9) Backgrounds Challenge - - 72.9 75.8 83.2 - 84.7 - 73.6 75.5 86.9 83.2 88.7 82.3 84.5 88.5 89.1 88.7 84.5 84. - - 62.1 62.5 72.6 - 76.8 - 66.0 73.5 81.0 75.5 83.6 73.5 77.1 86.9 86.3 88.6 78.8 83.0 1 1 - - - - - - - - 4 8 16 1 1 1 4 8 16 16 - - - - - - - - - - 2 4 8 1 1 1 2 4 8 8 Method ResNet50 ERM [61] ResNet-152 ERM [61] ViT-B ERM [57] ViT-L ERM [57] ViT-B DinoV2 [12] ViT-L DinoV2 [12] ResNet50 MaskTune [4] ResNet50 LLE [28] ViT-B SWAG+LLE1[28] ViT-B MAE+LLE2[28] ViT-L MAE+LLE 2 [28] PDiscoFormer [3] iFAM iFAM + IN-1K IN-9O MS MR 84.6 96.4 - 87.4 97.3 - 87.9 97.9 - 89.4 98.0 - 87.1 98.1 - 90.2 98.3 - 78.6 95.6 - 83.4 95.5 - 87.9 98.0 - 88.3 97.4 - 89.8 97.4 - 98.4 88.6 1 91.1 97.5 1 91.6 97.3 1 81.2 83.5 83.8 84.8 84.6 86.7 - 76.3 85.2 83.7 85.8 83.3 84.3 83. 90.0 92.1 92.4 93.0 93.1 95.5 91.1 88.3 92.4 92.5 93.5 93.9 93.5 94.0 BG-GAP 5.4 4.7 4.6 3.6 6.0 5.3 12.5 4.9 4.5 4.2 3.6 5.3 2.4 2.4 Table 1. Results on MetaShift, Waterbird, IN-1K, and IN-9 (Original: IN9O; Mixed-Same: MS; Mixed-Rand: MR).BG-GAP = MS MR (lower is better). Shaded rows (performance upper bounds): models trained with extra supervision; larger-capacity models. K: number of foreground parts. LLE: Last Layer Ensemble [28], SWAG [53], MAE [17], (cid:94): Frozen backbone, : Intervention, gt: Ground Truth Masks, f: FOUND (Saliency detection) [52], 1: SWAG [53] pre-train + LLE [28], 2: MAE [17] pre-train + LLE [28] : Fine-tuned backbone, . MetaShift and Waterbird (Table 1-a) highlight the advantage of using pretrained DINOv2 backbone, as also noted by [11]. Notably, simply fine-tuning DINOv2 surpasses all prior OOD robustness methods, while the same ViTB pretrained on ImageNet does not, underscoring the impact of self-supervised pretraining. Additionally, early masking consistently outperforms late masking in robust accuracy, whether using ground-truth masks or saliencybased selection [52]. Our method significantly improves upon these baselines, improving WGA from 81.0% to 88.6% on MetaShift and from 94.0% to 97.0% on Waterbirdeffectively halving the error. Only early masking with ground-truth segmentation surpasses our results. How5 ever, for = 8 parts in MetaShift, performance drops sharply to 78.8% (from 88.6% at = 4), suggesting that larger number of parts leads the model to capture spurious regions. We posit that such errors can be corrected via testtime interventions, which we explore in the next section. Results on IN-9. Table 1-b presents background sensitivity using the BG-GAP metric, which quantifies the accuracy difference between the Mixed-Same and Mixed-Rand variants. Surprisingly, vision transformers (ViTs) with advanced pre-training, such as DINOv2 [12, 41], perform worse than standard CNNs and ViTs trained purely on IN1K following modern training protocols [57, 61], suggesting that such pre-training does not inherently improve background robustness. While ResNets incorporating de-biasing methods during training [4, 28] show minor improvements in BG-GAP, they perform significantly worse on individual IN-9 variants, and ViTs with post-pretraining de-biasing objectives [28] offer only marginal gains. In contrast, our iFAM model achieves the lowest BG-GAP of 2.4, outperforming its baseline (PDiscoFormer) and all other models, including larger architectures like ViT-L, demonstrating its effectiveness in mitigating spurious cues. Results on CUB and Waterbird200. Table 2-a shows that fine-tuning DINOv2 ViT-B backbone does not scale well to fine-grained tasks. The fine-tuned CUB baseline underperforms its frozen counterpart on Waterbird200, despite improving by 2% in-distribution, suggesting overfitting to background cues. All late-masking models, including PDiscoFormer, stabilize around 76% on Waterbird200, indicating that background biases persist even with an oracle late mask. Our method achieves 86.2%, closely matching earlymasked models from [2], which rely on supervised segmentation masks. Despite using only self-discovered masks, our approach is within 2.5% of their fully fine-tuned model. Results on SIIM-ACR. For SIIM-ACR (Table 2-b), training RAD-DINO or PDiscoFormer with late masking alone results in biased model that overly relies on spurious correlations, leading to WG AUC close to random performance. However, our method, with = 8, achieves 69.0% WG AUC after interventions (up from 65.9%), approaching the 72.0% obtained with ground-truth bounding boxes, despite not using such additional annotations. 5.2. Additional robustness via interventions In this experiment, we assess the impact of our intervention strategies on robustness to spurious correlations. Due to the weakly supervised nature of part discovery, our model may (i) identify spurious parts in datasets with stronger, more object-like spurious correlations (e.g., MetaShift, SIIMACR) or (ii) assign out-of-distribution (OOD) objects to the foreground (e.g., models trained on CUB and evaluated on Waterbird200). To address the first issue, we perform leave-one-out (LOO) evaluation at inference, measuring (a) Results on CUB and Waterbird200 Method Early maskseg [2] (cid:94) 1 Early maskseg [2] 1 Late maskseg [2] (cid:94) 1 Late maskseg [2] 1 ViT-B DinoV2 (cid:94) - ViT-B DinoV2 - 4 PDiscoFormer [3] 8 PDiscoFormer [3] 16 PDiscoFormer [3] 1 iFAM 4 iFAM 8 iFAM 16 iFAM 16 iFAM+ CUB in-distrib. 90.1 91.4 88.6 90.7 89.2 91.6 89.1 88.8 88.7 89.0 90.1 90.4 90.6 90.5 Waterbird200 OOD 86.9 88.8 76.6 74.8 76.6 68.4 76.0 76.8 75.8 84.2 86.1 86.2 86.2 87.3 (b) Results on SIIM-ACR Method BBox-ERM [50] Segmentation-ERM [50] ResNet50 [50] ResNet50 JTT [30] ResNet50 GEORGE [54] ViT-B RAD-DINO (cid:94) ViT-B RAD-DINO PDiscoFormer [3] iFAM iFAM+ A. AUC WG AUC - - - - - - - 8 8 8 92.4 93.3 90.9 92.6 92.0 90.6 92.6 92.6 92.1 91. 72.0 82.0 45.5 55.9 63.4 40.6 54.3 46.7 65.9 69.0 Table 2. Results on CUB, Waterbird200 (CUB with OOD backgrounds) models and SIIM-ACR. Shaded rows (performance upper bounds): trained with extra supervision . (cid:94) : Frozen backbone, : Fine-tuned : Intervention, AUC: Area Under the Curve. backbone, its effect on WGA. For OOD foreground assignments, we remove unconfident tokens and evaluate classification performance. Additionally, we analyze the complementarity of these approaches by applying token removal on top of LOO for the worst-performing variant (without any intervention), where spurious part is likely to have been discovered, in MetaShift and SIIM-ACR. For comparison, we apply the same interventions to PDiscoFormer. Part-Removal Intervention on MetaShift. Fig. 3 (left) presents part assignment maps in MetaShift, color-coded, alongside WGA results from leave-one-out (LOO) evaluation. Most parts consistently capture coherent semantics. However, the brown part is strongly biased toward indoor elements, likely due to correlations between indoor backgrounds and the cat class. Removing this part at inference improves WGA from 78.8% to 81.7%, whereas removing other parts either reduces performance or has no effect. Part-Removal Intervention on SIIM-ACR. Fig. 3 (right) shows SIIM-ACR results, where removing the red part increases WG AUC by nearly 1.5 points. This part predomi6 Parts WGA 78.8 All 64.7 75.8 75.8 78.8 75.5 81.7 77.1 69.9 WG Parts AUC All 65.9 65.0 59.9 63.7 67.3 65.2 65.6 66.7 65. Figure 3. Leave-one-out (LOO) part removal intervention results on MetaShift (left) and SIIM-ACR (right) for = 8. The bottom right image shows heatmap of the average pneumothorax location across the dataset. Method iFAM MetaShift (K=8) Waterbird (K=16) AA 84.5 = 97% +0.2 = 99% +0.2 WGA 78.8 +0.3 +1.3 WGA 97.0 -0.4 +0.4 AA 98.8 -0.1 0. SIIM-ACR (K=8) Waterbird200 (OOD) A. AUC WG AUC K=4 K=8 K=16 86.2 +1.1 +0.7 86.1 +0.7 +0.5 86.2 +0.5 +0.7 65.9 +0.1 +0.5 92.1 -0.1 +0. Table 3. Results of applying the token removal intervention on MetaShift, Waterbird, SIIM-ACR, and the OOD Waterbird200 dataset. nantly covers the central chest region, which has little overlap with common pneumothorax locations, as confirmed by the heatmap of average pneumothorax occurrence, but often contains spurious cues, such as drainage tubes. OOD Token Removal in Waterbird200. Fig. 4 illustrates OOD token removal for = 8. In CUB (second column), discovered parts align well with the bird. However, in Waterbird, background objects are often misassigned to foreground parts. Since these objects have representations farther away from part prototypes, applying 97th percentile threshold effectively removes them. This results in small but consistent improvement in Waterbird200 (Tab. 3), with over one-point gain at = 16. quantitative analysis of intervention effects on foreground and part discovery in OOD settings is provided in the supplementary material. Combining Intervention Strategies. Table 4 shows that test-time interventions provide notable gains for iFAM but only marginal improvements for PDiscoFormer. Specifically, applying both strategies improves iFAMs performance by over 4 and 3 points on MetaShift and SIIM-ACR, respectively, while PDiscoFormer sees only 1-point and 0.1-point increase in WGA. 5.3. Ablation Studies To understand the contribution of each component in our proposed method, we conduct an ablation study on the 200-way CUB/Waterbird200 benchmark and the binary"
        },
        {
            "title": "MetaShift",
            "content": "SIIM-ACR Method PDiscoFormer [3] AA WGA A. AUC WG AUC 83.2 +2.0 LOO LOO + = 97% +2.0 LOO + = 99% +2.2 84.5 LOO +0.2 LOO + = 97% +0.2 LOO + = 99% +0.3 75.5 +1.3 +0.3 +1.3 78.8 +2.9 +3.2 +4.2 48.1 0.0 +0.1 +0.1 65.9 +1.4 +2.8 +3.1 92.6 0.0 0.0 0.0 92.1 -1.5 -1.3 -1. iFAM Table 4. Results on MetaShift and SIIM-ACR using LOO and token removal, selecting the worst-performing variant without any . MetaShift task. The results are given in Tab. 5. Impact of the Second Stage. Removing the second stage of iFAM, reducing the model to PDiscoFormer, results in the steepest accuracy drop on both robustness metrics (Waterbird and MetaShift WGA). This highlights the importance of our two-stage approach in improving robustness. Effect of Soft Masks. Using soft masks, where all input tokens retain some non-zero level of attention, improves in-distribution accuracy on CUB and slightly degrades performance on in-distribution MetaShift. However, it significantly reduces performance in out-of-distribution settings. This suggests that soft input masks allow background regions to influence stage-2 classification, leading to weaker 7 CUB iFAM WB200 iFAM = 99% = 97% Figure 4. Qualitative results of part discovery of our model on the CUB dataset (K = 8), along with results on the corresponding out-of-distribution (OOD) images from the WB200 (WaterBirds200) dataset and the effect of the test-time intervention of thresholding on the OOD images. Full iFAM No second stage Soft masks = 1 w/o shaping No stage-1 classif. Frozen stage-2 Part Dropout = 0.5 Part Dropout = 0.3 Part Dropout = 0.1 Part Dropout = 0. CUB in-distrib. 90.1 89.1 90.6 90.3 88.9 89.1 89.8 90.1 89.8 89.9 Waterbird200 OOD 86.1 76.0 85.7 80.2 85.0 83.7 85.5 86.1 85.4 85.4 MetaShift AA WGA 88.6 88.7 75.5 83.2 86.3 88.0 79.1 85.4 82.3 86.9 85.0 85.0 84.3 87.1 88.6 88.7 82.0 84.1 86.0 86.5 Table 5. Ablation results with = 4. Rows with are identical. robustness to spurious correlations. Role of the first stage learning objective. Removing only the first stage classification loss or completely removing the PDiscoFormer part discovery losses both result in notable but non-catastrophic performance drops. This suggests that, although using PDiscoFormer as stage-1 contributes to the quality of the model, the stage-2 classification is still capable to drive the foreground discovery of stage-1. Importance of Fine-tuning Stage-2. Fully fine-tuning the second stage leads to consistent performance improvements, as the model cannot overfit to spurious correlations that are filtered out by stage-1. Part Dropout. sensitivity analysis on the part dropout rate in stage-2 reveals that value of 0.3 is appropriate. 6. Conclusion Limitations. The main limitation of our approach is the extra computational cost incurred by the use of two forward passes: one for part discovery and the second for the downstream task. While the straight-through gradient requires the entire image to be processed during training, the second pass only requires access to subset of the image at inference, allowing optimization via patch token pruning [27]. Conclusion. We investigated two-step framework where stage-1 processes the full image to discover task-relevant regions, while stage-2 operates exclusively on this binary selection. By guaranteeing the receptive field of the stage-2 predictor through attention masking, we ensure that only the regions identified by stage-1 influence its representations, thereby minimizing background-related biases. Empirically, we show that this approach significantly improves robustness on benchmarks designed to test resilience against such biases. Our findings highlight the importance of inherently faithful attention mechanisms for developing robust computer vision systems. 8 Acknowledgment. This work was supported in part by the ANR project OBTEA (ANR-22-CPJ1-0054-01) and granted access to the HPC resources of IDRIS under the allocation 2023-AD011014325 made by GENCI."
        },
        {
            "title": "References",
            "content": "[1] Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checks for saliency maps. In NeurIPS, 2018. 1 [2] Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, and Diego Marcos. Masking strategies for background bias removal in computer vision models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2023. 1, 2, 4, 6 [3] Ananthu Aniraj, Cassio F. Dantas, Dino Ienco, and Diego Marcos. PDiscoFormer: Relaxing part discovery constraints with vision transformers. In ECCV, 2024. 1, 2, 3, 5, 6, 7, 12 [4] Saeid Asgari, Aliasghar Khani, Fereshte Khani, Ali Gholami, Linh Tran, Ali Mahdavi Amiri, and Ghassan Hamarneh. Masktune: Mitigating spurious correlations by forcing to explore. In NeurIPS, 2022. 2, 5, 6 [5] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In ECCV, 2018. 1 [6] Clara Bove, Thibault Laugel, Marie-Jeanne Lesot, Charles Tijus, and Marcin Detyniecki. Why do explanations fail? typology and discussion on failures in xai. arXiv preprint arXiv:2405.13474, 2024. [7] Marc Brinner and Sina Zarrieß. Model interpretability and In ACL, rationale extraction by input mask optimization. 2023. 2 [8] Rwiddhi Chakraborty, Yinong Wang, Jialu Gao, Runkai Zheng, Cheng Zhang, and Fernando De la Torre. Visual data diagnosis and debiasing with concept graphs. arXiv preprint arXiv:2409.18055, 2024. 2 [9] Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In ICML. PMLR, 2018. 2 [10] Myung Jin Choi, Antonio Torralba, and Alan Willsky. Context models and out-of-context objects. Pattern Recognition Letters, 33(7):853862, 2012. 1 [11] Rafayel Darbinyan, Hrayr Harutyunyan, Aram Markosyan, and Hrant Khachatrian. Identifying and disentangling spurious features in pretrained image reparXiv preprint arXiv:2306.12673, 2023. resentations. 5 [12] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. 4, 5, [13] Shi Feng, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber. Pathologies of neural models make interpretations difficult. arXiv preprint arXiv:1804.07781, 2018. 1 [14] Dan Friedman, Andrew Kyle Lampinen, Lucas Dixon, Danqi Chen, and Asma Ghandeharioun. Interpretability illusions in the generalization of simplified models. In ICML, 2023. 1 [15] Alireza Ganjdanesh, Shangqian Gao, and Heng Huang. Interpretations steered network pruning via amortized inferred saliency maps. In ECCV, 2022. 2 [16] Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, ZhengNing Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai Zhang, Ralph Martin, Ming-Ming Cheng, and Shi-Min Hu. Attention mechanisms in computer vision: survey. Computational visual media, 8(3):331368, 2022. 2 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. 5 [18] Zixuan Huang and Yin Li. Interpretable and accurate finegrained recognition via region grouping. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86628672, 2020. 1, 2 [19] Drew Hudson and Christopher Manning. Compositional attention networks for machine reasoning. In ICLR, 2018. 2 [20] Wei-Chih Hung, Varun Jampani, Sifei Liu, Pavlo Scops: Molchanov, Ming-Hsuan Yang, and Jan Kautz. In Proceedings of Self-supervised co-part segmentation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 869878, 2019. [21] Nabil Ibtehaz, Ning Yan, Masood Mortazavi, and Daisuke Kihara. Fusion of regional and sparse attention in vision transformers. arXiv preprint arXiv:2406.08859, 2024. 2 [22] Aya Abdelsalam Ismail, Hector Corrada Bravo, and Soheil Feizi. Improving deep learning interpretability by saliency guided training. In NeurIPS, 2021. 2 [23] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparametrization with gumbel-softmax. In ICLR, 2017. 4 [24] Neil Jethani, Mukund Yindalon Sudarshan, Aphinyanaphongs, and Rajesh Ranganath. Have we learned to explain?: How interpretability methods can learn In AISTATS. to encode predictions in their interpretations. PMLR, 2021. 2 [25] Diederik Kingma. Adam: method for stochastic optimization. In ICLR, 2015. 12 [26] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997, 2014. 12 [27] Ling Li, David Thorsley, and Joseph Hassoun. Sait: Sparse vision transformers through adaptive token pruning. arXiv preprint arXiv:2210.05832, 2022. [28] Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, and Mark Ibrahim. whac-a-mole dilemma: Shortcuts come in multiples where mitigating one amplifies others. In CVPR, 2023. 2, 5, 6 [29] Weixin Liang, Xinyu Yang, and James Zou. Metashift: dataset of datasets for evaluating contextual distribution shifts. In ICML Shift Happens Workshop, 2022. 5, 12 [30] Evan Liu, Behzad Haghgoo, Annie Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In ICML, 2021. 6 9 [31] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. NeurIPS, 2020. 4 [45] Aahlad Manas Puli, Nhi Nguyen, and Rajesh Ranganath. Explanations that reveal all through the definition of encoding. In NeurIPS, 2024. 2 [32] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. [33] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2022. 12 [34] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In International Conference on Learning Representations, 2018. 12 [35] Dang Minh, Xiang Wang, Fen Li, and Tan Nguyen. Explainable artificial intelligence: comprehensive review. Artificial Intelligence Review, pages 166, 2022. 1 [36] Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. Advances in neural information processing systems, 27, 2014. 2 [37] Daniel Morales-Brotons, Thijs Vogels, and Hadrien Hendrikx. Exponential moving average of weights in deep learning: Dynamics and benefits. Transactions on Machine Learning Research, 2024. 12 [38] Angelos Nalmpantis, Apostolos Panagiotopoulos, John Gkountouras, Konstantinos Papakostas, and Wilker Aziz. Vision diffmask: Faithful interpretation of vision transformers with differentiable patch masking. In CVPR, 2023. 2 [39] Fahimeh Hosseini Noohdani, Parsa Hosseini, Aryan Yazdan Parast, Hamidreza Yaghoubi Araghi, and Mahdieh Soleymani Baghshah. Decompose-and-compose: compositional approach to mitigating spurious correlation. In CVPR, 2024. [40] Mariano Ntrougkas, Nikolaos Gkalelis, and Vasileios Mezaris. T-tame: trainable attention mechanism for explaining convolutional networks and vision transformers. IEEE Access, 2024. 2 [41] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 4, 6 [42] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 13101318. Pmlr, 2013. 12 [43] Fernando Perez-Garcıa, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Matthew Lungren, et al. Exploring scalable medical image encoders beyond text supervision. Nature Machine Intelligence, pages 112, 2025. 4 [44] Jason Phang, Jungkyu Park, and Krzysztof Geras. Investigating and simplifying masking-based saliency methods for model interpretability. arXiv preprint arXiv:2010.09750, 2020. [46] Amir Rosenfeld, Richard Zemel, and John Tsotsos. The arXiv preprint arXiv:1808.03305, elephant in the room. 2018. 1 [47] Andrew Slavin Ross, Michael Hughes, and Finale DoshiVelez. Right for the right reasons: training differentiable models by constraining their explanations. In IJCAI, 2017. 2 [48] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206215, 2019. 1 [49] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. 5, 12 [50] Khaled Saab, Sarah Hooper, Mayee Chen, Michael Zhang, Daniel Rubin, and Christopher Re. Reducing reliance on spurious features in medical image classification with spatial specificity. In Machine Learning for Healthcare Conference. PMLR, 2022. 5, 6, 12 [51] Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. Distributionally robust neural networks. In ICLR, 2020. 5, [52] Oriane Simeoni, Chloe Sekkat, Gilles Puy, Antonın Vobeck`y, Eloi Zablocki, and Patrick Perez. Unsupervised object localization: Observing the background to discover In Proceedings of the IEEE/CVF Conference on objects. Computer Vision and Pattern Recognition, pages 3176 3186, 2023. 5 [53] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Dollar, and Laurens Van Der Maaten. Revisiting weakly supervised pre-training of visual perception models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022. 5 [54] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No subclass left behind: Fine-grained In robustness in coarse-grained classification problems. NeurIPS, 2020. 6 [55] Steven Stalder, Nathanael Perraudin, Radhakrishna Achanta, Fernando Perez-Cruz, and Michele Volpi. What you see is what you classify: Black box attributions. In NeurIPS, 2022. 2 [56] Austin Stone, Huayan Wang, Michael Stark, Yi Liu, Scott Phoenix, and Dileep George. Teaching compositionality to cnns. In CVPR, 2017. 2 [57] Hugo Touvron, Matthieu Cord, and Herve Jegou. Deit iii: Revenge of the vit. In ECCV. Springer, 2022. 5, 6 [58] Robert van der Klis, Stephan Alaniz, Massimiliano Mancini, Cassio Dantas, Dino Ienco, Zeynep Akata, and Diego Marcos. PDiscoNet: Semantically consistent part discovery for fine-grained recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1866 1876, 2023. 1, 2, 12 10 [59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 3 [60] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 5, 12 [61] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet strikes back: An improved training procedure in timm. arXiv preprint arXiv:2110.00476, 2021. 5, 6 [62] Junyi Wu, Bin Duan, Weitai Kang, Hao Tang, and Yan Yan. Token transformation matters: Towards faithful post-hoc explanation for vision transformer. In CVPR, 2024. [63] Junyi Wu, Weitai Kang, Hao Tang, Yuan Hong, and Yan Yan. On the faithfulness of vision transformer explanations. In CVPR, 2024. 2 [64] Shirley Wu, Mert Yuksekgonul, Linjun Zhang, and James Zou. Discover and cure: Concept-aware mitigation of spurious correlation. In ICML, 2023. 5 [65] Chong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. PatchGuard: provably robust defense against adversarial patches via small receptive fields and masking. In USENIX Security Symposium, 2021. 2 [66] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. In ICLR, 2021. 1, 5 [67] Yao Xiao, Ziyi Tang, Pengxu Wei, Cong Liu, and Liang Lin. Masked images are counterfactual samples for robust finetuning. In CVPR, 2023. 2 [68] Weiyan Xie, Xiao-Hui Li, Caleb Chen Cao, and Nevin Zhang. Vit-cx: Causal explanation of vision transformers. arXiv preprint arXiv:2211.03064, 2022. [69] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015. 2 [70] Jinsung Yoon, James Jordon, and Mihaela Van der Schaar. Instance-wise variable selection using neural netInvase: works. In ICLR, 2018. 2 [71] Hao Yuan, Lei Cai, Xia Hu, Jie Wang, and Shuiwang Ji. Interpreting image classifiers by generating discrete masks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):20192030, 2020. 2 [72] Sukmin Yun, Hankook Lee, Jaehyung Kim, and Jinwoo Shin. Patch-level representation learning for self-supervised vision transformers. In CVPR, 2022. 2 [73] Anna Zawacki, Carol Wu, George Shih, Julia Elliott, Mikhail Fomitchev, Mohannad Hussain, Paras Lakhani, Phil Culliton, and Shunxing Bao. Siim-acr pneumothorax segmentation, 2019. Kaggle. 5, [74] Xianren Zhang, Dongwon Lee, and Suhang Wang. Comprehensive attribution: Inherently explainable vision model with feature detector. In ECCV, 2024."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Training Settings We trained all models for 90 epochs using the AdamW optimizer [32]. During the part discovery stage, we followed the procedure outlined in the original paper [3]. Specifically, the class token, position embedding, and register token were kept unfrozen, while the remaining ViT layers were frozen. In this stage, we trained these unfrozen tokens along with the randomly initialized layers, including the projection, modulation, and final classification layers. In the second stage, we fine-tuned all parameters of the model. To adjust the learning rate dynamically, we employed cosine annealing schedule [33]. The initial learning rates were set as follows: 106 for the fine-tuned tokens of the ViT backbone in both stages and for the layers of the second-stage ViT, 103 for the linear projection layer forming the part prototypes, and 102 for the modulation and final linear layers used for classification in both stages. We used variable batch size, with minimum of 16, depending on the available computational resources. To scale the learning rate appropriately, we applied the square root scaling rule [26]. Regularization was performed using gradient norm clipping [42] with constant value of 2 and normalized weight decay [32] set to 0.05. The PDiscoFormer losses were configured as in the original paper [3], with one exception for the biomedical dataset SIIM-ACR [73]. For this dataset, we disabled the background loss Lp0 by setting its weight to 0, as this loss assumes the background part is more likely to occur at the image boundaries an assumption that does not necessarily hold for pneumothorax occurrences. Finally, we used constant part dropout value of 0.3 for both stages of the model in all experiments. The dropout value for the first stage aligns with that used in the original PDiscoFormer paper [3], while the value for the second stage was ablated in Table 5 of our main paper. Scaling up to larger datasets. For larger datasets such as ImageNet1K [49], we adopted optimizations including Automatic Mixed Precision (AMP) [34] and temporal averaging using Exponential Moving Average (EMA) [25, 37] to accelerate and stabilize training. By leveraging these optimizations, we were able to double the batch size, leading to 3.5 reduction in training time, all while maintaining performance. Additionally, we found that larger datasets benefited from longer training, prompting us to increase the total number of epochs to 120. Baseline Training Settings. Wherever possible, we report results from cited papers or evaluate public weights; otherwise, we re-train baselines using the experimental setup"
        },
        {
            "title": "Method",
            "content": "K Kp Fg. MIoU iFAM = 97% = 99% iFAM = 97% = 99% iFAM 4 8 = 97% = 99% 16 10.3 8.4 9.2 9.3 6.7 7.3 8.0 6.2 6. 63.7 65.2 65.9 68.6 71.4 72.4 70.2 72.9 73.1 Top-1 Acc. 86.1 86.8 86.6 86.2 86.7 86.9 86.2 87.3 86.9 Table 6. Quantitative analysis of the effect of the token removal intervention on part assignment consistency using keypoint regression (Kp) and foreground discovery (Fg. MIoU) on the OOD Waterbird200 dataset. K: Number of foreground parts. from the original paper. B. Training Time and Inference Speed We use an input image size of 518 for the CUB [60], Waterbirds [51], SIIM-ACR [73] aligning with the default resolution of DINOV2. This higher resolution is consistent with prior works [3, 50, 58]. For the MetaShifts [29] and ImageNet1K datasets, we adopt reduced input size of 224, resulting in lower computational requirements. Training Time. On machine with 8 NVIDIA A100 GPUs, the training times are as follows: approximately 3 hours for CUB and Waterbirds, 5 hours for SIIM-ACR, 11 minutes for MetaShifts, and 34 hours for ImageNet-1K (with AMP and EMA optimizations). Inference Speed. On an RTX 3090, models trained on CUB (input size: 518) run at 43 images/second, while those trained on MetaShift (input size: 224) reach 151 images/second. These results are reported without any inference-time optimizations. We believe future work can further improve speed by leveraging the sparsity of secondstage inputs. C. Quantitative Analysis of Token Removal In Table 3 of our main paper, we demonstrated that the testtime intervention of OOD/Low-confidence token removal consistently improves classification accuracy for models trained on CUB when evaluated on the Out-of-Distribution this technique endataset WaterBird200. Additionally, hances qualitative foreground object discovery, as illusIn this section, we trated in Figure 4 of the main paper. 12 m 1 = 4 = 8 = 6 1 = Figure 5. Qualitative results for part discovery for the iFAM model (without any ) trained on the CUB dataset for different values of K, the number of foreground parts. provide detailed quantitative analysis of these results, focusing on the models part assignment consistency and foreground discovery capability under the intervention. Evaluation Metrics. The CUB dataset provides groundtruth annotations for parts in the form of keypoints, which denote the centroid locations of parts within each image, as well as foreground-background masks. Since the images in the Waterbird200 dataset are identical to those in CUB, differing only in their adversarial backgrounds, the CUB annotations can also be used for Waterbird200. We evaluate foreground discovery using mean Foreground Intersectionover-Union (Fg. mIoU) and part assignment consistency using Keypoint Regression (Kp). 1. Fg mIoU. This metric assesses the models ability to identify the foreground region relevant for downstream classification. We merge all detected foreground parts and compute the IoU between the merged parts and the ground-truth foreground-background masks from the CUB dataset. 2. Kp. Following [20], we measure part assignment consistency by deriving landmark locations through trained linear regression model. This model maps the 2D geometric centers of the part assignment maps to their corresponding ground-truth part landmarks. The predicted landmarks are then compared against ground-truth annotations on the test set, with the evaluation metric being the normalized mean L2 distance. Results on Foreground Discovery. The low-confidence token removal technique consistently improves Foreground MIoU across all values of on the OOD Waterbird200 dataset (see Tab. 6). However, increasing the threshold = 97%) leads to slight reduction in MIoU com- (e.g., = 99%. For instance, at = 8 (repared to using sults shown in Figure 4 of the main paper), the baseline model achieves Foreground MIoU of 68.6%, which imq = 99%, but drops to 71.4% with proves to 72.4% with = 97%, suggesting that stricter confidence threshold may inadvertently remove some foreground regions. Despite this, the drop in classification accuracy is minimal (from 86.9% to 86.7%), indicating that the model remains robust to removed foreground regions. Similar trends are observed across other values of K, where = 99% genq = 97% erally leads to the best Foreground MIoU, while provides slightly better classification performance. Results on Part Assignment Consistency. The intervention improves keypoint regression (Kp) values across all values, indicating that the centroids of part assignment maps align more closely with ground-truth annotations. For instance, at = 16, the Kp value improves from 8% (baseq = 97%), likely due to the removal of lowline) to 6.2% ( confidence tokens near part boundaries, as shown in Fig. 4. Overall, these results suggest that low-confidence token removal enhances both foreground discovery and part asq = 99% generally yielding signment consistency, with 13 m 1 = 4 = 8 = 6 1 = Figure 6. Qualitative results for part discovery for the iFAM model (without any ) trained on the Waterbirds dataset for different values of K, the number of foreground parts. Results on ImageNet-1K. Qualitative results on ImageNet1K for various animal classes, including birds, cats, dogs, and insects, are shown in Figures 8, 9, and 10 for = 1. At this setting, the model effectively performs foreground discovery, which appears to generalize well across the 1000 classes of ImageNet. This observation aligns with our quantitative results on background robustness in Table 1-b of the main paper. the best Foreground MIoU, while proves classification performance. = 97% slightly imD. Qualitative Results for Part Discovery To complement the quantitative evaluations in the main paper, we provide additional qualitative results in Figures 5 to 10. These results demonstrate our models ability to discover meaningful parts and accurately identify foreground regions, which are crucial for downstream classification tasks and improving model interpretability. Results on CUB and WaterBird. In datasets such as CUB and Waterbird, where all images belong to single superclass (birds), the granularity of the discovered parts improves as increases. The identified parts generally align well with the foreground regions, as shown in Fig. 5 and Fig. 6. Results on MetaShifts. For the binary classification task in MetaShifts (Cat vs. Dog), illustrated in Fig. 7, the model assigns single part (blue) to both cats and dogs when = 1. At = 2, the same part (orange) is assigned to both classes, while another part (blue) is allocated to objects that frequently co-occur with these animals in the training set. However, at higher values of K, such as = 8, the model begins to identify more non-causal or spurious parts, likely explaining the performance drop observed for this variant in Table 1-a of the main paper. 14 m 1 = 2 = 4 = 8 = Figure 7. Qualitative results for part discovery for the iFAM model (without any ) trained on the MetaShifts dataset for different values of K, the number of foreground parts. Figure 8. Qualitative Results on ImageNet-1K for Birds (without any ) for = 1. 16 Figure 9. Qualitative Results on ImageNet-1K for Cats and Dogs (without any ) for = 1. 17 Figure 10. Qualitative Results on ImageNet-1K for Insects (without any ) for = 1."
        }
    ],
    "affiliations": [
        "Inrae",
        "Inria",
        "University of Montpellier"
    ]
}