{
    "paper_title": "Protein Autoregressive Modeling via Multiscale Structure Generation",
    "authors": [
        "Yanru Qu",
        "Cheng-Yen Hsieh",
        "Zaixiang Zheng",
        "Ge Liu",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation."
        },
        {
            "title": "Start",
            "content": "Yanru Qu1,2,, Cheng-Yen Hsieh1,,, Zaixiang Zheng1, Ge Liu2, Quanquan Gu1, 1ByteDance Seed, 2University of Illinois Urbana-Champaign Equal Contributions, Project Lead, Corresponding Author"
        },
        {
            "title": "Abstract",
            "content": "We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting statue, forming coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as promising framework for protein structure generation. Date: February 5, 2026 Correspondence: Quanquan Gu quanquan.gu@bytedance.com Project Page: https://par-protein.github.io Note: Work was done during Yanru Qus internship at ByteDance Seed 6 2 0 2 4 ] . [ 1 3 8 8 4 0 . 2 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "Deep generative modeling of proteins has emerged as way to design and model novel structures with desired functions and properties, with broad applications in biomedicine and nanotechnology [21, 25]. widely adopted approach is to directly model the distribution of three-dimensional protein structures, which govern protein function. Typically, structure generative models produce protein backbones without sequences or side chains. Prior work in this area could be broadly categorized into methods that predict the SE(3) backbone frame representations [45, 47] and those that directly model atoms, e.g., Cα coordinates for simplicity and scalability [14, 31]. However, all these works are based on diffusion models and their variations (e.g., flow matching). 1 Figure 1 Overview of PAR. PAR comprises the autoregressive (AR) transformer Tθ and the flow-based backbone decoder vθ. During training, we downsample backbone RL3 into multi-scale representations {x1, . . . , x}. AR transformer performs next-scale prediction, producing conditional embeddings (z1, . . . , zn) from (bos, . . . , xn1). The shared flow-based decoder learns to denoise backbones xi at each scale conditioned on zi. At inference, PAR autoregressively generates xi until the final structure is constructed. On the other hand, autoregressive (AR) modeling has emerged as powerful paradigm for large language models [1, 41]. AR models employ next-token prediction to model the probability of each token based on prior ones, showing striking empirical behaviors such as scalability [24] and zero-shot generalization to unseen tasks [6]. Despite its success in other domains, AR modeling has received little attention in backbone modeling. We identify two main reasons. (i) Extending AR models to continuous data, e.g. atomic positions in 3D, often relies on data discretization [12], which can reduce structural fidelity and fine-grained details for proteins, limiting generative performance [19]. (ii) Protein residues exhibit strong bidirectional dependencies: residues distant in sequence may be spatially close and form hydrogen bonds or hydrophobic contacts. This mutual dependency conflicts with the unidirectional assumption of standard AR models, and thus limits the quality of previous attempts on autoregressive structure generation [13]. natural question therefore arises: can we apply AR modeling to protein backbone design? In this paper, we answer the above question affirmatively, and propose PAR, Protein AutoRegressive framework, to unlock the power of AR models for protein backbone generation. We take initiative from the hierarchical nature of proteins: their structures span multiple scales of granularity, from coarse 3D topology and tertiary fold arrangements, local secondary structures, to the finest atomic coordinates. PAR thus adopts multi-scale autoregressive framework via next-scale prediction, predicting each scale conditioned on prior coarser scales. This strategy, inspired by advances in image generation, enabled AR models to surpass strong diffusion models in image synthesis for the first time [40], and further allows multimodal LLMs to achieve unified text and image generation framework [28]. Building on this multi-scale framework, PAR includes three key components  (Fig. 1)  . The multi-scale downsampling creates coarse-to-fine structural representations to serve as structural context and targets during training. AR transformer, stack of non-equivariant attention layers [42], encodes all preceding scales to produce scale-wise conditional embedding following Li et al. [30]. The flow-based backbone decoder is conditioned on this embedding to model Cα backbone atoms directly. As result, PAR avoids both discretization of protein structures and residue-wise unidirectional autoregressive ordering, thereby overcoming the two aforementioned limitations that compromise structural fidelity and generative quality. Moreover, training on ground-truth structural context, AR models suffer from exposure bias [3], which is key challenge substantially reducing structure generation quality in our preliminary study. We effectively mitigate such issue via noisy context learning and scheduled sampling, allowing the model to learn from corrupted context. This multi-scale approach introduces several notable model behaviors. PAR generates backbones by establishing global topology and performing refinements, analogous to progressively sculpting statue into masterpiece  (Fig. 2)  . For unconditional generation, PAR exhibits favorable scaling behavior, yielding competitive results on 2 Figure 2 Samples generated by PAR over scales. We illustrate PARs generation process across five scales. Much like sculpting statue, the model first formulates the global structural layout at coarse scales and progressively refines the details at later scales. distributional metrics like Fréchet Protein Structure Distance (FPSD). Unlike diffusion models, which operate at single scale, PAR flexibly handles inputs at various granularities, and hence shows zero-shot generalization in tasks like prompt-based generation and motif scaffolding. The multi-scale formulation enables PAR to orchestrate sampling strategies, achieving 2.5x sampling speedup compared to single-scale baselines. Finally, PAR provides more general framework, incorporating flow-based models as special case when restricted to single scale, and thus remains compatible with techniques from flow-based models like self-conditioning [9]. Main contributions: (i) We present PAR, the first multi-scale AR model for protein backbone generation that addresses key limitations of existing AR methods. (ii) PAR comprises multi-scale downsampling, AR transformer, and flow-based decoder, to directly model Cα atom, avoiding discretization loss. (iii) We alleviate exposure bias through noisy context learning and scheduled sampling, effectively improving structure generation. (iv) Our model shows an interpretable generation process that forms coarse backbone topology and refines it progressively. (v) Benchmarking results show that PAR effectively captures protein data distributions, achieving FPSD score of 161.0 against PDB dataset that further scale with training compute. (vi) PAR exhibits efficient sampling and zero-shot generalization potential, reflecting the versatility of AR large language models."
        },
        {
            "title": "2 Background and Related Work",
            "content": "Flow and diffusion-based structure generative models. Flow-based and diffusion methods [18, 34] operate by transforming samples from prior distribution to the target data distribution, and have been widely applied to protein backbone generation. These methods either predict per-residue rotations and translations using frame-based Riemannian manifold representation [5, 22, 45, 47, 48] or directly model atom coordinates, such as Cα positions [14, 31, 32], with some approaches generating fully atomistic proteins including side chains [10, 37]. Discrete diffusion methods [15, 43], trained on structure tokens, often reduce structural fidelity and limit generation quality [19]. Unlike most diffusion approaches, which are single-scale, PAR models protein structures across multiple scales using parameterized upsampling autoregressive process from short to long, allowing flexible handling of different structural granularities and zero-shot generalization to tasks like prompt-based generation. In addition, PAR provides more general framework, as it naturally reduces to flow-based model when restricted to single scale. 3 Autoregressive modeling. Autoregressive (AR) modeling has been driving natural language processing and computer vision due to its strong scalability and zero-shot generalization [1, 40, 41]. The approach relies on next-token prediction that predicts the distribution of the next token based on prior ones in unidirectional sequence. However, adapting autoregressive models to continuous domains, like image generation, often involves tokenizers such as VQVAE [12], which discretizes the data for transformer training and may discard fine-grained details. Recently, Li et al. [30] used the AR model that produces conditioning for diffusion network (e.g., small MLP) to model image latents, unlocking the operations of AR models in continuousvalued space. In addition, defining appropriate autoregressive orders that preserve data properties is crucial. Since next-token prediction inherently discards spatial locality by flattening the 2D image feature map into 1D sequence, VAR [40] introduced next-scale prediction. Leveraging multi-scale VQVAE, the image feature map is quantized into multi-scale token maps that preserve the spatial and bidirectional correlations. To our knowledge, autoregressive modeling has not been widely applied to protein structure generation despite their success in other domains. The only exception is Gaujac et al. [13], which models structure tokens with causal transformer. In contrast, we design multi-scale autoregressive framework that operates directly in continuous backbone space using flow-based backbone decoder, thereby addressing the limitations of discrete token maps while respecting the bidirectional biophysical relations of protein structures."
        },
        {
            "title": "3 Protein Autoregressive Modeling",
            "content": "In this section, we introduce PAR, multi-scale autoregressive (AR) framework for protein backbone generation. Formally, we want to model protein backbone Cα structure with residues RL3 in an autoregressive manner as follows: pθ(x) = EXqdecompose(x) (cid:2)pθ(X = {x1, . . . , xn})(cid:3) = EXqdecompose(x) (cid:89) i= pθ(xi <i). (1) where qdecompose(x) defines decomposition of autoregressive order for protein structure into scales = {x1, . . . , xn} with xn = x, while pθ(xi <i) is the desired PAR model learning to generate via scale-wise autoregression. The design space of qdecompose and pθ under this formulation (Eqn. 1) can be flexible. Recall that our goal is to enable AR modeling to preserve spatial dependencies and avoid discretization, as discussed in 1. To this end, in 3.1, we devise non-parametric and deterministic qdecompose by multi-scale protein downsampling  (Fig. 1)  that represents protein backbones at multiple scales via hierarchical down-sampling (Eqn. 2), providing structural context and training targets. In 3.2, we parameterize PAR pθ as backbone autoregressive upsampling process via next-scale prediction and achieve direct Cα modeling in the continuous space (Eqn. 3). This comprises two key components: (i) an autoregressive transformer  (Fig. 1)  that produces scale-wise conditional embeddings informed by preceding scales to guide generation (Eqn. 4); and (ii) flow-based backbone decoder  (Fig. 1)  which samples Cα backbone coordinates conditioned on the learned embeddings (Eqn. 5). Finally, in 3.3, we dedicated strategies to mitigate exposure bias [3], mismatch between training on groundtruth data and inference on model predictions that leads to error accumulations and degrading generation quality in AR models. Together, these components enable PAR to robustly generate protein backbones in coarse-to-fine manner."
        },
        {
            "title": "3.1 Multi-scale Protein Downsampling",
            "content": "We construct the multi-scale representations of protein structures via hierarchical downsampling to serve as training context and targets for PAR  (Fig. 1)  . Given protein structure RL3, it produces hierarchy of coarse-to-fine scales by progressively downsampling into scales: qdecompose : (cid:55) = {x1, x2, . . . , xn} = {Down(x, size(1)), Down(x, size(2)), . . . , x}. (2) 4 where Down(x, size(i)) Rsize(i)3 denotes downsampling operation that interpolates along the sequence dimension, leading to size(i) 3D centroids that provide coarse structural layout. Since qdecompose is designed as deterministic mapping for every x, the likelihood of Eqn. 1 can be simplified without marginalization: pθ(x) = (cid:81)n i=1 pθ(xi <i). We show that this downsampling strategy properly preserves pairwise spatial relationships in C.8. Scale configurations. = {size(1), . . . , size(n)} could be defined in two ways. When defined by length, scales are chosen as hyperparameters, e.g., = {64, 128, 256}. In this case, if lies in (size(i), size(i + 1)], the protein could be generated with only i+1 autoregressive steps. When defined by ratio, scales are adaptively determined based on protein length, e.g., = {L/4, L/2, L}. Empirically, defining scales by length yields slightly better results in modeling data distributions. We adopt this as the default configuration. This design enables training PAR with flexible scale configurations. In the following sections, we describe how this hierarchy of representations are modeled using the autoregressive transformer and backbone decoder."
        },
        {
            "title": "3.2 Coarse-to-Fine Backbone Autoregressive Modeling",
            "content": "Preserving the inherent dependencies in data when defining the autoregressive order is crucial and affects generation performance [40]. Standard AR models assume unidirectional dependency, which conflicts with the strong bidirectional interactions in protein sequences, e.g., spatially close residues can form hydrophobic contacts or hydrogen bonds even if distant in sequence. PAR addresses this with multi-scale AR framework via next-scale prediction, capturing mutual structural dependency over each scale. Motivated by Li et al. [30], we propose to use an AR Transformer with diffusion/flow-based regression loss to enable modeling of Cα atoms directly in continuous space. That is, we could rewrite the likelihood as: pθ(X = {x1, . . . , xn}) = = (cid:89) i=1 (cid:89) i= pθ(xiX <i) pθ(xi zi = Tθ(X <i)), (3) where Tθ is an AR Transformer that produces scale-wise conditioning zi while pθ(xizi) is optimized with flow-based atomic decoder vθ with flow matching. This avoids discretizing protein structures into tokens, preserving structural details and generation fidelity. We describe each component below. Autoregressive transformer for scale-wise conditioning. To formulate the autoregressive order, we leverage the hierarchical nature of proteins, where protein structure could span various levels of representations from coarse tertiary topology to the finest atomic coordinates. We adopt the next-scale prediction to model per-scale distribution based on prior coarser scales, which further ensures that the bidirectional dependencies of residues are modeled over each scale. We train our autoregressive model  (Fig. 1)  , non-equivariant transformer Tθ, to produce scale-wise conditioning embedding zi for scale depending on prior scales <i = {x1, . . . , xi1}: zi = Tθ(X <i) = Tθ (cid:16)(cid:2)bos, Up(x1, size(2)), . . . , Up(xi1, size(i))(cid:3)(cid:17) . (4) where bos Rsize(1)3 is learnable embedding, and Up(xi1, size(i)) interpolates xi1 to size(i) 3D points. All inputs are concatenated along the sequence dimension before being fed into Tθ. The embedding zi is then used to condition the flow matching decoder to predict the backbone coordinates xi, detailed below. Flow-based atomic decoder. We enable PAR to directly model Cα positions x, wherein pθ(xzi) is parameterized by an atomic decoder vθ with flow matching [FM, 34], which maps standard normal distribution to the target data distribution. We condition the vθ with scale-wise conditioning zi predicted by the AR Transformer Tθ at each scale  (Fig. 1)  . During training, we sample the noise ϵi (0, I) and time variable ti [0, 1], and compute the interpolated sample as xi ti = ti xi + (1 ti) ϵi. As such, we can jointly train vθ and Tθ with an FM objective: (cid:20) 1 2 (cid:21) ti, ti, zi) (xi ϵi)(cid:13) . (cid:13) Etip(ti),ϵiN (0,I) L(θ) = ExpD 1 size(i) (cid:13) (cid:13)vθ(xi (cid:88) (5) i= 5 where pD(x) denotes the training data distribution and p(t) denotes the t-sampling distribution in Geffner et al. [14]. The conditioning embedding zi is injected into the atomic decoder network vθ through adaptive layer norms [36]. We further concatenate learnable scale embedding alongside zi to help the model identify different scales and incorporate self-conditioning input as an additional condition [9], though we omit them in the equation for simplicity. To formulate the indices for positional encoding pi at scale i, we uniformly sample size(i) numbers from the interval [1, L], i.e., pi = linspace(1, L, size(i)). At coarse scales, the wide spacing between adjacent indices encourages the model to capture global structural layout, while at finer scales the dense indices allow the model to focus on local details. For more details, please refer to A.1. Leveraging the learned flow network vθ, sampling could be performed at each scale through ordinary differential equation (ODE) dxt = vθ(xt, t) dt, with the scale superscript and condition omitted for simplicity. Moreover, we could define the stochastic differential equation (SDE) for sampling: dxt = vθ(xt, t) dt + g(t) sθ(xt, t) dt + (cid:112)2g(t)γ dWt, (6) where g(t) is time-dependent scaling function for the score function sθ(xt, t) [2, 35] and the noise term, γ is noise scaling parameter, and Wt is standard Wiener process. The score function, defined as the gradient of the log-probability of the noisy data distribution at time t, could be computed as sθ(xt, t) = vθ(xt,t)xt . Multi-scale structure generation. At inference, the autoregressive transformer first produces z1 at the coarsest scale, which conditions the flow matching decoder to generate x1 either via ODE or SDE sampling in Eqn. 6. We upsample x1 using Up(x1, size(2)) and send it back into the autoregressive transformer to predict the next scale embedding z2. This coarse-to-fine process iterates times until the flow-matching model generates the full-resolution backbone x. KV cache is applied throughout the autoregressive process for efficiency. 1t"
        },
        {
            "title": "3.3 Mitigating Exposure Bias",
            "content": "Training AR models typically uses teacher forcing [46], where ground-truth data are fed as context to stabilize learning. However, during inference the model is conditioned on its own predictions, creating traininginference mismatch known as exposure bias [3, 16]. Errors can then accumulate across autoregressive steps, degrading output quality. Our preliminary study shows that teacher forcing greatly reduces the designability of generated structures. To mitigate this, we adapt Noisy Context Learning (NCL) and Scheduled Sampling (SS), techniques from language and image AR modeling [4, 38], for PAR. Noisy context learning. We train PAR with noisy context, adding noise to the ground-truth prior-scale input during training. This encourages the model to learn the per-scale distribution without relying on perfectly ncl} [0, 1], and accurate context, improving robustness. We randomly sample noise weights {w1 draw noise samples {ϵ1 ncl xi + ncl = wi . This perturbation is applied to the input context only during training, which updates the (1 wi autoregressive step in Eqn. 4 as zi = Tθ ncl, , wn ncl} (0, I). Each input context xi is corrupted as xi (cid:16)(cid:2)bos, Up(x1 ncl, size(2)), . . . , Up(xi1 ncl , size(i))(cid:3)(cid:17) ncl, , ϵn ncl) ϵi ncl . Scheduled sampling. During training, we use scheduled sampling [4] by running the forward process iteratively across scales. At the i-th scale, the flow-based backbone decoder predicts the clean data xi pred = t, ti, zi). With probability of 0.5, we replace the ground truth context xi with this prediction xi + (1 ti)vθ(xi at later scales. This exposes the model to its own output and reduces the train-test gap. Notably, we xi pred could combine noisy context learning with this technique by adding noises to the model predicted context xi . pred"
        },
        {
            "title": "4 Experiments",
            "content": "We begin by evaluating PAR on unconditional backbone generation and compare it with existing structure generative methods in 4.1. Next, we examine its zero-shot generalization ability in 4.2. We then study the scaling behavior, efficient sampling, and propose strategies to mitigate exposure bias, along with additional ablations in 4.3. We include additional empirical analysis in C. 6 Table 1 Unconditional backbone generation performance. We follow Geffner et al. [14] in adopting FPSD and fS to evaluate the models ability to capture the data distribution. PARpdb denotes the 400M model finetuned on the PDB subset. Method FrameDiff (17M) RFDiffusion (60M) ESM3 (1.4B) Genie2 (16M) Proteina (200M) Proteina (400M) PAR (200M) PAR (400M) γ=0. PARpdb γ=0.45 Designability FPSD vs. (%) 65.4 94.4 22.0 95.2 92.8 92.6 sc-RMSD PDB AFDB 258.1 252.4 855.4 313.8 285.6 272. 194.2 253.7 933.9 350.0 282.3 271.3 - - - - 1.14 1.09 87.0 96.0 88.0 96.6 88.8 1.33 1.01 1.28 1.04 1. 252.0 313.9 231.5 161.0 176.6 237.9 296.4 211.8 228.4 256.4 fS (C / / T) 2.46/5.78/23.35 2.25/5.06/19.83 3.19/6.71/17.73 1.55/3.66/11.65 2.17/6.22/21.48 2.13/6.14/21.18 2.11/6.41/19.22 2.24/6.60/16.71 2.20/6.59/20.96 2.57/7.42/23.61 2.62/7.52/30. Diversity TM-Sc. 0.40 0.42 0.42 0.38 0.37 0.37 0.37 0.39 0.36 0.43 0.40 Sec. Struct. % (α/β) 64.9/11.2 64.3/17.2 64.5/8.5 72.7/4.8 66.3/9.2 65.1/9.5 64.3/8.8 66.3/8.9 63.2/9.7 50.2/16.7 50.2/16."
        },
        {
            "title": "4.1 Protein Backbone Generation\nGeneration over scales. We illustrate PAR’s backbone generation using a 5-scale model in Fig. 2 (i.e.,\nS = {L/16, L/8, L/4, L/2, L}), showing generated structures with target lengths of {50, 100, 200, 250} residues.\nGeneration proceeds in a coarse-to-fine manner, which resonates with statue sculpting: the coarser scales\nestablish a rough global layout, and finer scales progressively add local details. This multi-scale formulation\nyields a clear and interpretable generation process. We present the quantitative analysis on PAR’s backbone\ngeneration in the next paragraph.",
            "content": "Unconditional generation benchmark. We compare 3-scale PARs (S = {64, 128, 256}) backbone generation performance with other baselines in Tab. 1, following the evaluation protocol in Geffner et al. [14]. The baselines span three categories: frame-based diffusion methods [45, 48], multimodal protein language models [15], and diffusion/flow-based Cα generators [14, 32]. We disable the optional pair representations and trianglebased modules [23] in both PAR and Proteina to align architectural capacity and improve computational efficiency. We train PAR using two-stage procedure following Geffner et al. [14]: the model is first trained for 200K steps on the AFDB representative dataset and subsequently fine-tuned for 5K steps on PDB subset of 21K designable samples. Results for the remaining baselines are taken directly from Geffner et al. [14]. Evaluation metrics and baseline categories are detailed in A.2A.3. To better reflect the goal of unconditional protein generation as modeling the full data distribution, we adopt FPSD, which jointly measures quality and diversity by comparing generated and reference distributions, analogous to FID in image generation [17]. As shown in Tab. 1, PAR generates samples that closely match the reference data distribution and maintains competitive designability. On FPSD, PAR achieves scores of 211.8 against AFDB and 231.5 against PDB. By reducing the noise scaling parameter γ (Equation 6) from 0.45 to 0.3 in SDE sampling, we can reduce sampling stochasticity and further improve sample quality, improving the designability from 88.0% to 96.00%. After fine-tuning, PAR achieved 96.6% designability and 161.0 FPSD against the PDB, highlighting its superior distributional fidelity compared to pure diffusion-based baselines. We provide additional analysis on longer proteins in C.3."
        },
        {
            "title": "4.2 Zero-Shot Task Generalization\nGuiding backbone generation with human-specified prompt. Proteins possess hierarchical and complex\nstructures, which makes it challenging to directly specify a target shape and design proteins accordingly. By\nleveraging PAR’s coarse-to-fine generation, a simple prompt (e.g., 16 points) can specify a protein’s coarse\nlayout, from which the model generates the complete structure as shown in Fig. 3. In particular, we first\nobtain a 16-point input prompt either by downsampling a real protein structure from the test set, or by\nspecifying the points manually (the top row in Fig. 3). Using a 5-scale PAR (S = {16, 32, 64, 128, 256}), we\ninitialize the first-scale prediction with the 16-point prompt and autoregressively upsample until the full",
            "content": "7 Figure 3 Backbone generation with human prompt. Given small number of points (e.g., 16) as prompt, PAR can generate protein backbones that adhere to the global arrangements specified by these points, without any finetuning. For visualization, input points are interpolated to match the length of the generated structure. Figure 4 Zero-shot motif scaffolding. Given motif structure, PAR can generate diverse, plausible scaffold structures that accurately preserve the motif via teacher-forcing the motif coordinates at each scale, without additional conditioning or fine-tuning. protein structure is generated, as illustrated in the bottom row of Fig. 3. Following this process, PAR can generate new structure that preserves the coarse structural layout (first five examples), and explore entirely novel structures (last three examples). If desired, longer prompts (e.g., 32 points) could be specified to achieve more finer-grained control over backbone generation. As later shown in Tab. 5, we quantitatively evaluate the structural consistency (TM-score) between the prompted layout and the final generation. Motif scaffolding. Besides the point-based layout, PAR can preserve finer-grained prompts like atomic coordinates. Fig. 4 highlights the zero-shot motif scaffolding capabilities of PAR. Using 5-scale PAR, we downsample raw protein structure into five scales and teacher-force the ground-truth motif coordinates at each scale before propagating into the next scale. To avoid clashes or discontinuities, we superimpose the ground-truth motif residues and the generated motif segments before replacement. With no fine-tuning and no conditioning, PAR generates plausible scaffolds that preserve motif structures with high fidelity. This stands in contrast to diffusion or flow-based frameworks, which typically require fine-tuning on additional conditions such as masks or motif coordinates, or rely on decomposition strategies [14, 44, 45]. Moreover, the generated scaffolds differ substantially from the input structure, showing that PAR generates structurally diverse scaffolds rather than merely copying. For example, the leftmost example in Fig. 4 preserves the yellow motif helix while introducing new secondary structure elements like β-sheet and loops, in contrast to the original helices. We further benchmark zero-shot motif scaffolding in Tab. 10 in the appendix, following evaluation protocols in [14, 32]. Figure 5 Scaling effects of PAR. Performance of four metrics over varying training steps and model sizes, (a) FPSD vs. PDB, (b) FPSD vs. AFDB, (c) fS(T), (d) sc-RMSD. Table 2 Sampling efficiency. Combining SDE and ODE sampling across scales yields 2.5 inference speedup compared to the single-scale 400-step baseline, shown in the first and the last row. We generate 100 samples at each length. Sampling Steps Length 150 Length Time (s) Design. (%) Time (s) Design. (%) Proteina (SDE) All SDE All ODE S/S/O S/O/O 0/0/400 0/0/200 400/400/400 400/400/2 400/400/400 400/400/400 400/400/2 400/400/400 400/2/2 131 67 312 184 312 184 312 67 97% 89% 97% 0% 28% 98% 99% 96% 97% 170 351 - - - 186 - 68 92% 80% 94% - - - 91% - 94%"
        },
        {
            "title": "4.3 Empirical Analysis of Multiscale PAR\nScaling effects of PAR. We examine the model’s behaviors by varying the backbone decoder’s size and\nnumber of training steps in Fig. 5. We train PAR with 3 scales over three different model sizes with 60,200,400\nmillion parameters and three training durations over 200, 400, 600K steps. PAR demonstrates favorable\nbehavior when scaling both model size and training duration, effectively improving its ability to capture the\nprotein data distribution with FPSD scores of 187 against PDB and 170 against AFDB (first two columns in\nFig. 5). Further, the fS scores, which reflect quality and diversity, increase with larger model sizes and greater\ncomputational budgets. While extending training duration alone offers negligible gains, increasing model size\nsubstantially enhances designability, leading to lower sc-RMSD values. Meanwhile, we empirically observe\nthat scaling the autoregressive transformer has minimal impacts on the evaluation results. This allows us to\nreduce computational costs and prioritize increasing the backbone decoder’s model capacity that effectively\nimproves generation quality. We provide more discussion on varying model sizes in §C.7.",
            "content": "Efficient sampling with multi-scale orchestration of SDE/ODE. While Tab. 1 reports results using uniform number of sampling steps across scales, the multi-scale formulation of PAR actually offers advantages in sampling efficiency, as shown in Tab. 2. More specifically, (1) sampling at the coarser scale (e.g., first scale) is more efficient than sampling at finer scales (e.g., 2nd scale) due to shorter sequence length; (2) we can use less number of sampling steps at finer scales than coarser scales. As shown in Tab. 2, by using SDE sampling only at the first scale, and switching to ODE sampling for the remaining scales, PAR could dramatically reduce the diffusion steps from 400 to 2 steps at the last two scales without harming designability (97%), yielding 4.7x inference speedup. This is possible because high-quality coarse topology places the model near high-density regions, enabling efficient refinement with ODE sampling. Naively reducing the SDE sampling 9 steps significantly harms designability, dropping to 22% when reducing steps to 50, as shown in Fig. 7. This is consistent with the observation of single-scale models like Proteina, where designability degrades to 89% when reducing SDE sampling steps to 200 in Tab. 2. Crucially, SDE sampling at the first scale is necessary for establishing reliable global topology, given that ODE-only sampling exhibits poor designability. Compared to the single-scale 400-step baseline, PAR achieves 1.96x and 2.5x sampling speedup at length 150 and 200, respectively. This improvement is driven by speeding up the final scales, where the longer sequence lengths cause computational costs to grow quadratically in transformer architectures. Moreover, the computational costs remain constant at the first scale because it has fixed size 64, even when generating longer sequences. Table 3 Mitigating exposure bias for PAR. We adopted various training strategies to mitigate the exposure bias for multi-scale autoregressive modeling. These techniques are consistently effective effective in improving structure quality. NCL: Noisy Context Learning. SS: Schedule Sampling. Results are obtained with 60M PAR trained for 100K steps. Method sc-RMSD FPSD vs. (PDB/AFDB) Teacher Forcing + NCL + NCL & SS 2.20 1.58 1.48 99.66 / 37.64 89.70 / 23.69 90.66 / 24.59 fS-(C/A/T) 2.53 / 5.56 / 29.67 2.54 / 5.85 / 28.37 2.54 / 5.84 / 28.77 Mitigating exposure bias. To mitigate exposure bias, we adopted noisy context learning (NCL) and scheduled sampling (SS) as defined in 3.3. Noisy context learning encourages the model to infer structural guidance from corrupted context and boosts the structure generation quality. Tab. 3 shows that noisy context learning effectively improves the sc-RMSD of the generated structure from 2.20 to 1.58, and reduces FPSD against AFDB to 23.69 when using ODE sampling. The designability further improved to 1.48 along with scheduled sampling, which makes the training process more aligned with the inference scenario. Results are obtained with 60M PAR trained for 100K steps. Figure 6 Visualization of the average attention scores in PAR autoregressive transformer over 5 scales. Obtained from samples with lengths in (128, 256]. We provide attention map visualization for shorter proteins in C. Interpreting multi-scale PAR. We visualize the attention maps of the autoregressive transformer at each scale  (Fig. 6)  . We average the attention scores within each scale, normalize them such that the scores across scales sum to 1, and average them over 50 test samples to obtain the scale-level attention distribution during inference. We summarize three key observations: (i) Most scales barely attend to the first scale, since the input to this scale, bos token, carries little structural signal. (ii) Each scale primarily attends to the previous scale, which typically contains richer contextual and structural information. (iii) Despite focusing most heavily on the current scale, the model still retains non-negligible attention to earlier scales. This indicates that PAR effectively integrates information across multiple scales and maintains structural consistency during generation. This aligns with results in Tab. 5 where the autoregressive Transformer effectively improves consistency with the given prompt. We also observe similar patterns on shorter proteins, as shown by the attention maps in Fig. 10 in the appendix. Multi-scale formulation. We ablate the effect of defining scale by length versus ratio, as shown in 3.1. Tab. 4 shows that under comparable levels of upsampling ratio ({64, 128, 256} and {L/4, L/2, L}), the by-length strategy outperforms by-ratio. Meanwhile, PAR obtains better designability and FPSD when increasing from 10 Table 4 Multi-scale formulation. We ablate different strategies for scale configuration in downsampling. Results are obtained with 60M PAR. Define scale {64, 256} {64, 128, 256} {64, 128, 192, 256} {64, 96, 128, 192, 256} {L/4, L/2, L} Designability FPSD vs. (%) 83.0 85.0 77.8 81.0 86.4 (sc-RMSD) PDB AFDB 274.32 267.35 282.69 263.58 298.30 282.85 279.63 296.70 276.00 310.64 1.38 1.39 1.55 1.51 1. fS (C / / T) 2.14/6.58/20.66 2.15/6.52/20.35 2.05/6.04/18.69 2.17/6.31/20.65 2.00/5.87/18.91 two scales to three scales. Beyond this point, increasing the scale configurations to four and five scales results in degraded designability, potentially due to error accumulation and exposure bias. These results support our choice of adopting the 3-scale PAR as the default. All results are obtained using the 60M model. Table 5 Structural consistency for prompted generation. Using transformer to encode prior-scale structural conditions shows better prompt-following than direct input. Results are obtained with 60M PAR. Length RMSD vs. Reference (32.64] (64,128] (128,256] TM-score vs. Prompt (64,128] (128,256] (32.64] Reference Direct Input Trans. Encode - 2.13 1.45 - 3.38 2.72 - 6.51 5.75 0.60 0.58 0.60 0.61 0.61 0.64 0.59 0.59 0. AR Transformer improves structural consistency. We conduct an ablation study to evaluate the effectiveness of the autoregressive transformer in Tab. 5. We compare two different strategies for encoding prior-scale structural context, including (i) direct input, where the multi-scale structures are directly fed into the backbone decoder without any intermediate encoding; and (ii) transformer encoder, where all scales are processed autoregressively by Transformer encoder, and the resulting encoded representation is then passed to the backbone decoder. We train two 60M models and evaluate both models by downsampling 588 testing structures as prompts and re-upsamples them with PAR. As shown in Tab. 5, the transformer encoder demonstrates better structural consistency, indicating that autoregressive encoding across scales produces coherent structural guidance over scales, consistent with attention maps in Fig. 6."
        },
        {
            "title": "5 Discussion",
            "content": "PAR is the first multi-scale autoregressive model for protein backbone generation, offering general framework that includes flow-based methods as special case. PAR addressed limitations of standard autoregressive models, such as unidirectional dependency, discretization, and exposure bias. Our method robustly models structures over multiple granularities and in turn enables strong zero-shot generalization. This capability includes coarse-prompted conditional generation using points (e.g., 16 points) as structural layout and finergrained controls such as atomic-coordinate-based motif scaffolding. For unconditional backbone generation, PAR exhibits powerful distributional fidelity and generation quality. The analysis of scale-level attention map provides additional insights into how the multi-scale formulation operates. We hope that PAR unlocks the potential of autoregressive modeling for protein design. Some promising open directions include: (1) Conformational dynamics modeling. PAR can, in principle, perform zero-shot modeling of conformational distributions: we downsample structure and upsample it with PAR to mimic local molecular dynamics. We leave this exciting application for future research. (2) All-atom modeling. This work focuses on backbone Cα atoms to prioritize autoregressive design, but its natural to extend to full-atom representations [37]. The multi-scale framework offers an advantage for flexible zero-shot prompt-based all-atom designs."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Dr. Hang Li, Liang Hong, Xinyou Wang, Jiasheng Ye, Yi Zhou, Jing Yuan, Yilai Li, Zhenghua Wang, Yuning Shen, Huizhuo Yuan, as well as other colleagues at ByteDance Seed for their valuable comments and support."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. [3] Kushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias matters: An imitation learning perspective of error accumulation in language generation. arXiv preprint arXiv:2204.01171, 2022. [4] Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. Advances in neural information processing systems, 28, 2015. [5] Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym Korablyov, Michael Bronstein, and Alexander Tong. Se (3)-stochastic flow matching for protein backbone generation. arXiv preprint arXiv:2310.02391, 2023. [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [7] Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola. Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design. arXiv preprint arXiv:2402.04997, 2024. [8] Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. [9] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202, 2022. [10] Alexander Chu, Jinho Kim, Lucy Cheng, Gina El Nesr, Minkai Xu, Richard Shuai, and Po-Ssu Huang. An all-atom protein generative model. Proceedings of the National Academy of Sciences, 121(27):e2311500121, 2024. [11] Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert Ragotte, Lukas Milles, Basile IM Wicky, Alexis Courbet, Rob de Haas, Neville Bethel, et al. Robust deep learningbased protein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [13] Benoit Gaujac, Jérémie Donà, Liviu Copoiu, Timothy Atkinson, Thomas Pierrot, and Thomas Barrett. Learning the language of protein structure. arXiv preprint arXiv:2405.15840, 2024. [14] Tomas Geffner, Kieran Didi, Zuobai Zhang, Danny Reidenbach, Zhonglin Cao, Jason Yim, Mario Geiger, Christian Dallago, Emine Kucukbenli, Arash Vahdat, et al. Proteina: Scaling flow-based protein structure generative models. arXiv preprint arXiv:2503.00710, 2025. [15] Thomas Hayes, Roshan Rao, Halil Akin, Nicholas Sofroniew, Deniz Oktay, Zeming Lin, Robert Verkuil, Vincent Tran, Jonathan Deaton, Marius Wiggert, et al. Simulating 500 million years of evolution with language model. Science, 387(6736):850858, 2025. [16] Tianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass. Exposure bias versus self-recovery: Are distortions really incremental for autoregressive text generation? arXiv preprint arXiv:1905.10617, 2019. [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 13 [19] Cheng-Yen Hsieh, Xinyou Wang, Daiheng Zhang, Dongyu Xue, Fei Ye, Shujian Huang, Zaixiang Zheng, and Quanquan Gu. Elucidating the design space of multimodal protein language models. arXiv preprint arXiv:2504.11454, 2025. [20] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron Courville. Riemannian diffusion models. Advances in Neural Information Processing Systems, 35:27502761, 2022. [21] Po-Ssu Huang, Scott Boyken, and David Baker. The coming of age of de novo protein design. Nature, (7620):320327, 2016. [22] John Ingraham, Max Baranov, Zak Costello, Karl Barber, Wujie Wang, Ahmed Ismail, Vincent Frappier, Dana Lord, Christopher Ng-Thow-Hing, Erik Van Vlack, et al. Illuminating protein space with programmable generative model. Nature, 623(7989):10701078, 2023. [23] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. [24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [25] Brian Kuhlman and Philip Bradley. Advances in protein structure prediction and design. Nature reviews molecular cell biology, 20(11):681697, 2019. [26] Patrick Kunzmann and Kay Hamacher. Biotite: unifying open source computational biology framework in python. BMC bioinformatics, 19(1):346, 2018. [27] Gilles Labesse, Colloch, Joël Pothier, and J-P Mornon. P-sea: new efficient assignment of secondary structure from cα trace of proteins. Bioinformatics, 13(3):291295, 1997. [28] Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive model for unified understanding and generation. arXiv preprint arXiv:2509.03498, 2025. [29] Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025. [30] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [31] Yeqing Lin and Mohammed AlQuraishi. Generating novel, designable, and diverse protein structures by equivariantly diffusing oriented residue clouds. arXiv preprint arXiv:2301.12485, 2023. [32] Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed AlQuraishi. Out of many, one: Designing and scaffolding proteins at the scale of the structural universe with genie 2. arXiv preprint arXiv:2405.15489, 2024. [33] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. [34] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [35] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: In European Exploring flow and diffusion-based generative models with scalable interpolant transformers. Conference on Computer Vision, pages 2340. Springer, 2024. [36] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [37] Wei Qu, Jiawei Guan, Rui Ma, Ke Zhai, Weikun Wu, and Haobo Wang. (all-atom) is unlocking new path for protein design. bioRxiv, pages 202408, 2024. [38] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. 14 [39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. [40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [43] Xinyou Wang, Zaixiang Zheng, Fei Ye, Dongyu Xue, Shujian Huang, and Quanquan Gu. Dplm-2: multimodal diffusion protein language model. arXiv preprint arXiv:2410.13782, 2024. [44] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot image restoration using denoising diffusion null-space model. arXiv preprint arXiv:2212.00490, 2022. [45] Joseph Watson, David Juergens, Nathaniel Bennett, Brian Trippe, Jason Yim, Helen Eisenach, Woody Ahern, Andrew Borst, Robert Ragotte, Lukas Milles, et al. De novo design of protein structure and function with rfdiffusion. Nature, 620(7976):10891100, 2023. [46] Ronald Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270280, 1989. [47] Jason Yim, Andrew Campbell, Andrew YK Foong, Michael Gastegger, José Jiménez-Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan Veeling, Regina Barzilay, Tommi Jaakkola, et al. Fast protein backbone generation with se (3) flow matching. arXiv preprint arXiv:2310.05297, 2023. [48] Jason Yim, Brian Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud Doucet, Regina Barzilay, and Tommi Jaakkola. Se (3) diffusion model with application to protein backbone generation. In International Conference on Machine Learning, pages 4000140039. PMLR, 2023. [49] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025."
        },
        {
            "title": "A Implementation and Evaluation Details",
            "content": "We follow the implementation of Proteina [14] for training PAR, using the same architecture and hyperparameter setup. Training is conducted on 8 H100 GPUs, with batch size of 15 per GPU, for total of 200k steps. We train the flow-based backbone decoder with 60 M, 200 M, and 400 parameters, using the same non-equivariant transformer architecture as Proteina. For the autoregressive module, we adopt Proteinas smallest configuration (60 parameters), as we find that small AR module is enough to yield competitive generation quality, discussed in 4.3. For fair comparison, we trained Proteina from scratch under the same setting and achieved similar or even better performance than results reported in the original paper. For other baselines, we directly obtain the results from [14]. Model and training configurations can be found in Tab. 6. Note that we remove pair representations, triangle update as well as auxiliary loss for memory and training efficiency, and the additional trainable parameters come from the 60M autoregressive transformer encoder. Table 6 Hyperparameters for PAR models. Tθ: autoregressive transformer; vθ: flow-based atomic decoder. PAR Architecture initialization sequence repr dim sequence cond dim sinusoidal enc dim interpolated position enc dim # attention heads # transformer layers # trainable parameters Tθ 60M 60M vθ 200M 400M random random random random 512 128 196 196 12 12 60M 512 128 196 196 12 12 60M 768 512 256 128 12 15 200M 1024 512 256 128 16 18 400M A."
        },
        {
            "title": "Implementation Details",
            "content": "In 3.2 we briefly introduce two novel techniques for our autoregressive modeling: scale embedding and interpolated position embedding. Scale Embedding. Since we use shared decoder to train across all scales, we introduce scale embedding to distinguish data distributions at different scales. Each scale is assigned unique scale id, which is incorporated into the model to help disambiguate the varying statistical characteristics associated with different scales. Interpolated Position Embedding. Interpolated position embedding is natural extension to the standard position embedding for sequence representation. In the raw structure, each residue is associated with 3D coordinate and position ID ranging from 1 to L, where is the protein length. Our downsampled structure and interpolated position embeddings are derived from the raw structure and position IDs via interpolation, following the sequential order of residues. Each interpolated residue is computed by interpolating the coordinates of neighboring real residues, while each interpolated position ID is obtained by interpolating over the corresponding relative positions. This approach has the advantage that, across inputs of different lengths (i.e., different scales), the interpolated positions still reflect the relative location of each interpolated residue within the original structure, providing coarse-grained view of the real protein. A.2 Evaluation Metrics We evaluate the model from multiple perspectives, including quality and diversity, following evaluation protocols established in prior literature by [5, 48]. Specifically, we sample 100 structures for each of the five sequence lengths: 50, 100, 150, 200, and 250, resulting in total of 500 structures for evaluation. Designability. Following the procedure from Yim et al. [48], we generate 8 candidate sequences for each structure using ProteinMPNN [11] with temperature of 0.1. Each sequence is folded into predicted structure using ESMFold [33]. We compute the root-mean-square deviation (RMSD) between each predicted 16 structure and the original generated structure, and record the minimum RMSD across the 8 predictions. structure is considered designable if its minimum RMSD is less than 2 Å. We report the proportion of designable structures and the average minimum RMSD across all samples. Diversity. Following Bose et al. [5], we compute the average pairwise TM-score among all designable structures for each sequence length. The final diversity score is obtained by averaging these values across all five lengths. Secondary Structure. To analyze secondary structure characteristics, we annotate all designable structures using the P-SEA algorithm [27] as implemented in Biotite [26]. For each structure, we compute the proportion of alpha helices and beta sheets, and report the average proportions across all samples. To better assess the models overall structural fidelity at the distributional level, we adopt two metrics introduced in Geffner et al. [14]. We randomly sample 125 structures at each sequence length from 60 to 255 (with step size of 5), resulting in 5,000 structures in total. Importantly, no designability filtering is applied during this stage, and all samples are used for evaluation. Frchet Protein Structure Distance (FPSD). Analogous to the Fréchet Inception Distance (FID) [17], FPSD measures the Wasserstein distance between the distributions of generated and reference structures. Structures are embedded into feature space defined by fold class predictor, and the distance is computed based on the resulting Gaussian approximations. Protein Fold Score (fS). Inspired by the Inception Score (IS) [39], the fS metric encourages both diversity and sample-level quality. High-quality generations lead to confident fold class predictions, while diversity is captured by the entropy across the predicted fold distribution. A.3 Unconditional Backbone Generation We train 200M and 400M models for Proteina and PAR for 200k steps, using Adam optimizer with learning rate 1e-4, no warmup applied. For evaluation, we sample from Proteina and PAR with the same techniques below. We follow the optimal configuration and sample 400 steps for Proteina. For PAR, we find 1k steps show better results. Self conditioning. Self-conditioning has been widely employed in protein design. During sampling, the models own previous predictions ˆx(xt) = xt + (1 t)vθ (xt) (7) are fed back as conditions to guide subsequent generation. During training, the model is conditioned on its own predictions with probability of 50%. Sampling can be performed either with or without self-conditioning. Low temperature sampling. In Eqn. 6, the parameter γ is injected to control the scale of noise. When γ = 1, this SDE yields the same marginals as the ODE defined by flow model. In practice, it is common to use lower γ < 1 which empirically improves designability at the cost of diversity. In this paper, we use γ = 0.30 by default. Category of unconditional backbone generation baselines. We categorize each baseline based on their modeling types and frameworks in the table below. Table 7 Category of unconditional backbone generation baselines. Type Method FrameDiff Frame RFDiffusion Frame Token ESM3 Ca Genie2 Ca Proteina Ca PAR Framework Diffusion Diffusion PLM Diffusion FM PAR"
        },
        {
            "title": "B Datasets",
            "content": "The training data is derived from the curated AFDB representative dataset (denoted as DFS, containing 0.6M structures), as processed by Proteina. This dataset ensures both high quality (pLDDT > 80) and structural diversity, with sequence lengths ranging from 32 to 256 residues. We follow [14] and split it by 98:19:1 for training, validation and testing. For PDB finetuning, as the dataset used in [14] is not publicly available, we reproduce their filtering protocol and curate designable subset of 21K samples from PDB."
        },
        {
            "title": "C More Empirical Analysis",
            "content": "C.1 Efficient Sampling with SDE/ODE Orchestration Figure 7 Designability analysis of multi-scale SDE/ODE sampling methods. Naively reducing the SDE sampling steps substantially degrades the designability (red). Using ODE alone exhibits limited designability (purple). Orchestrating SDE and ODE sampling enables reduced sampling steps while retaining designability (blue and green). We report the designability over varying sampling steps in Fig. 7. Leveraging SDE sampling at the first scale and ODE for the remaining scales, PAR could effectively reduce diffusion steps without harming designability, highlighting the unique advantage of multi-scale design to orchestrate SDE and ODE sampling at different scales. In addition, aggressively reducing SDE steps or replacing SDE with ODE across all scales yields much worse designability, highlighting the necessity of combining both sampling methods. These results suggest that PAR decomposes backbone generation into coarse topology formation and efficient structure refinement at later scales. C.2 Ablation with Self-Conditioning Multi-scale autoregressive modeling and self-conditioning similarly guide the generation with coarse estimate of the structure. To evaluate the role of self-conditioning in our multiscale framework, we conducted an ablation study  (Fig. 8)  , where the results are from the same 60M model in the previous ablation study. Across all length ranges, the model with self-conditioning consistently generates higher-quality protein structures, in terms of sc-RMSD. Although self-conditioning also supplies an intermediate structural estimate during generation, it is complementary to the multi-scale formulation and yields further improvements in structural quality. C.3 Long Protein Generation Finetuning on longer protein chains. We follow Proteina to finetune our models on datasets with longer proteins. Since Proteina has not released its long-protein dataset, we cannot fully reproduce their experiment setups. Instead, we follow the filtering procedure described in their appendix on PDB structures to curate 18 Figure 8 Ablation with self-conditioning. Self-conditioning consistently improves backbone generation performance of PAR across varying protein lengths, showing that both methods are compatible. Results are obtained with 60M PAR. Figure 9 Protein length distribution for long protein finetuning. long-protein dataset. We filter PDB structures to lengths between 256 and 768 residues and keep only designable samples, resulting in 26k high-quality proteins. The length-distribution of this dataset  (Fig. 9)  exhibits long-tail shape with peaks around 300-400 residues. We then finetune the 400M PAR and Proteina models in Tab. 1 on this dataset for 10k steps. Long-protein generation. We generate 100 proteins for each length in {300, 400, 500, 600, 700}. PAR exhibits higher designability at lengths {300, 400}, consistent with the higher density of training samples in this range. At lengths between 500 to 700, both Proteina and PAR show degraded designability, while PAR demonstrating slightly better results. We attribute this to the long-tail nature of the training set, which includes far fewer samples in the length range between 500 and 700. The limited size of the training set (26K) also potentially hinders the model from reaching its full potential. We leave scaling up long-protein data as promising direction for future work. Table 8 Long protein generation. scR: sc-RMSD (Å) . DesA: Designability (%) . 300 400 500 600 700 scR DesA scR DesA scR DesA scR DesA scR DesA Proteina PAR 1.91 1.28 93 2.70 1.65 61 72 4. 3.19 49 52 7.90 6.80 29 13.32 11.29 4 10 C.4 Foldseek Cluster Diversity Table 9 Foldseek cluster diversity. γ 0.35 0.40 0.45 0.50 0.60 0.70 0.80 Designable Clusters 119 126 142 140 164 160 We investigated the foldseek cluster diversity of PAR-generated samples. larger γ increases sampling stochasticity and improves the diversity, reaching its peak value at γ=0.6. We generate 500 structures, with 100 samples for each length in {50, 100, 150, 200, 250}. We use the same foldseek command following Geffner et al. [14] with tmscore threshold of 0.5. The command is foldseek easy-cluster <path_samples> <path_tmp>/res <path_tmp> --alignment-type 1 --cov-mode 0 --min-seq-id 0 --tmscore-threshold 0.5 C.5 Zero-shot Motif Scaffold Benchmark Table 10 Zero-shot motif scaffold benchmark. PAR* indicates our zero-shot model, producing 100 samples, while other baselines require finetuning. Baseline results are taken directly from Geffner et al. [14], which reports results using 1000 samples. SR: success rate. Results are obtained with 60M PAR. Unique Solutions (%) PAR* Proteina Genie2 RFDiffusion FrameFlow 1PRW 1BCF 5TPN 5IUS 3IXT 5YUI 1QJG 1YCR 2KL8 7MRX.60 7MRX.85 7MRX.128 4JHW 4ZYP 5WN9 5TRV_short 5TRV_med 5TRV_long 6E6R_short 6E6R_med 6E6R_long 6EXZ_short 6EXZ_med 6EXZ_long # tasks (SR 1%) 0 0 0 0 6.0 0 1.0 4.0 4.0 0 1.0 1.0 0 0 0 0 1.0 0 6.0 3.0 3.0 3.0 8.0 10.0 13 0.3 0.1 0.4 0.1 0.8 0.5 0.3 24.9 0.1 0.2 3.1 5.1 0 1.1 0.2 0.1 2.2 17.9 5.6 41.7 71.3 0.3 4.3 29.0 11 0.2 0.1 0.8 0.1 1.4 0.3 0.5 13.4 0.1 0.5 2.3 2.7 0 0.3 0.1 0.3 2.3 9.7 2.6 27.2 41.5 0.2 5.4 32.6 11 0.1 0.1 0.5 0.1 0.3 0.1 0.1 0.7 0.1 0.1 1.3 6.6 0 0.6 0 0.1 1.0 2.3 2.3 15.1 38.1 0.1 2.5 16.7 9 0.3 0.1 0.6 0 0.8 0.1 1.8 14.9 0.1 0.1 2.2 3.5 0 0.4 0.3 0.1 2.1 7.7 2.5 9.9 11.0 0.3 11.0 40.3 11 We quantify the zero-shot motif scaffolding performance of PAR in Tab. 10. For other training-based methods, we directly quote the results reported in Proteina [14]. We use PAR to generate 100 backbone structures for each benchmark problem in Watson et al. [45]. Following Proteinas evaluation protocol, we produce 8 ProteinMPNN sequences with the motif residues fixed, and feed 20 each sequence to ESMFold. Using the predicted structure, we calculate ca-RMSD and MotifRMSD. design is considered success if any sequence achieves scRMSD 2Å, motifRMSD 1Å, pLDDT 70, and pAE 5. Note that our method is the only one evaluated in zero-shot setting, whereas all other baselines rely on training or finetuning with additional motif conditioning. C.6 Scale-Agnostic Inference Table 11 Inference with flexible scale configuration. Designability (%) 96.6 92.8 72.6 (sc-RMSD) 1.04 1.16 1.74 FPSD vs. PDB vs. AFDB fS (C/A/T) 160.99 175.09 177.01 228.44 246.34 246. 2.57/7.42/23.61 2.54/7.66/26.68 2.56/7.53/26.78 PAR (3 scale) w/o scale emb 5 scale inference In our original setup, we included learnable scale embedding vector as part of the AR modules conditioning. This embedding allows the model to identify the current scale and adjust its behavior (e.g., generating coarse vs. fine structures). However, since the dimensionality of this learnable embedding is fixed to the number of scales, the model cannot be applied to different scale configuration at inference. To explore flexible scale configurations, we finetune an alternative model that simply discards the learnable embedding on the PDB designable subset for 5k steps. This formulation cancels the embedding from fixed number of scales and enables inference across arbitrary scale settings. As shown in the Tab. 11, when inferring with five scales using this 3-scale model, FPSD remains stable, suggesting that the model still captures the underlying data distribution under altered scale configurations. However, the designability substantially drops, indicating that sampling with an unseen scale configuration fails to preserve structural detail, ultimately leading to lower-quality results. C.7 Ablating AR and Decoder Size Table 12 Effect of AR module and decoder size. Both AR and decoder utilize transformer-based architectures. AR 400M 60M 60M Decoder 60M 400M 60M sc-RMSD Designability (%) 1.26 1.01 1.19 87.80 96.00 92.60 We introduced an ablation study examining the AR encoder size, and discussed crucial design choices for both the AR encoder and flow-based decoder. We summarize key findings below. Per-token vs per-scale decoder. In our preliminary study, we implemented the model with 200M-parameter AR module and, following MAR [30], used 3-layer MLP ( 20M) as the diffusion head. However, this setup failed to generate reasonable structures, yielding an average sc-RMSD of 16. This likely occurs because per-token decoder is not expressive enough to capture the global correlations between atoms that is required to produce reliable coarse structure at the first scale, which is crucial for the subsequent coarse-to-fine refinement. These observations motivated our shift to per-scale transformer-based decoder. Large vs. small decoder. As shown in Tab. 12 and our scaling experiments in 4.3, using large decoder brings effective improvements to generation quality. Large AR vs small AR. With the decoder size fixed, increasing the AR transformer size from 60M to 400M does not offer improvements. One one hand, this is consistent with the modules role to generate scale-wise conditioning to guide the backbone generation, which does not require large model capacitya similar trend observed in image generation [8]. In addition, we believe this is due to exposure bias: the AR module overfits 21 to ground truth context to stabilize training, resulting in mismatch with inference, where the model relies on its predictions as context. This issue becomes more severe under several conditions: (1) Larger AR models tend to overfit the context more strongly, making exposure bias more severe. (2) Limited data increases overfitting risks: our 588K training structures (32256 residues each) provide far less coverage than datasets like ImageNet (1.28M 256x256 images). (3) High precision tasks like protein modeling are sensitive to small errors, making exposure bias more serious than in image generation, where the compressed VAE latents lie in smoother Gaussian space that is robust to small errors at the cost of some visual details [29, 49]. Our noisy context learning and scheduled sampling mitigate this issue for the 60M PAR, but scaling the AR transformer appears to intensify this issue. Exploring more training data is potential solution and we leave this for future work. C.8 Sequence-Based Downsampling Preserves Pairwise Spatial Relationships Table 13 RMSE and LDDT across different downsample sizes. Size(i) RMSE LDDT 16 32 64 0.362 1 0.275 1 0.217 1 0.170 1 We discuss whether 1D downsampling properly preserves pairwise spatial relationships. To study this, we attempt to investigate the difference between pairwise distances computed after downsampling the 1D coordinate sequence and those obtained by downsampling the full-resolution 2D distance map. We discuss details below. Spatial relationships in downsampled 1D sequence. We follow the process below to quantify the spatial relationships: 1. Downsample the coordinate sequence from RL3 to Rsize(i)3 for each scale i. 2. We compute pairwise distance maps using the downsampled sequence, leading to size(i) size(i) map. Spatial relationships in 3D space after downsampling. We quantify this using the pairwise distance map calculated from the full-resolution structure: 1. Calculate the pairwise distance map of the structure, producing map. 2. We downsample pairwise map this using the F.interpolate(mode=bicubic) operation, resulting in size(i) size(i) map. Does sequence-based downsampling preserve spatial relationships? We select all samples from the testing set, and calculate the RMSE and LDDT between the aforementioned two size(i) size(i) pairwise maps for each sample. As expected, rmse slightly increases as size(i) decreases, reflecting the loss of fine-grained details at coarser scales. However, lddt remains consistently at 1 and the rmse values remain low across all scales. Together, these results indicate that, despiste small information loss at the coarse scales, 1D sequence downsampling preserves the essential pairwise spatial correlations captured by the downsampled 2D distance map. C.9 Visualization of Attention Scores We provide attention score visualization for shorter proteins in Fig. 10. The pattern generally aligns with Fig. 6, where each scale primarily attends to its previous scale. 22 Figure 10 Visualization of the average attention scores in PAR autoregressive transformer over 3/4 scales. Left Length (32, 64]. Right Length (64, 128]."
        },
        {
            "title": "D Other Related Work",
            "content": "Flow and diffusion-based structure generative models. Flow-based and diffusion methods have been widely applied to protein backbone generation, with examples including RFDiffusion [45] and Chroma [22]. Subsequently, various protein representations have been proposed for protein structure generation. FrameDiff, FoldFlow and FrameFlow [5, 47, 48] model protein structures through per-residue rotation and translation predictions, employing frame-based Riemannian manifold representation [20, 23]. Building upon FrameFlow, Multiflow [7] jointly models sequence and structures. In contrast, Genie and Genie2 [31, 32] generate protein backbones by diffusing the Cα coordinates. Pallatom and Protpardelle [10, 37] further generate fully atomistic proteins that include side-chains. Meanwhile, Proteina [14] leverages non-equivariant transformer architecture to model the Cα backbone coordinates, exhibiting scalability and simplicity. In addition to continuous diffusion and flow-matching based approaches, discrete diffusion methods like ESM3 [15] and DPLM-2 [43] have been trained on structure tokens, which often reduce structural fidelity and thus limit structure generation quality [19]."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "University of Illinois Urbana-Champaign"
    ]
}