{
    "paper_title": "IA-T2I: Internet-Augmented Text-to-Image Generation",
    "authors": [
        "Chuanhao Li",
        "Jianwen Sun",
        "Yukang Feng",
        "Mingliang Zhai",
        "Yifan Chang",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, a T2I model released in February would struggle to generate a suitable poster for a movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether a reference image is needed based on the given text prompt; a hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; a self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed framework's performance, we collect a dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft a complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 7 7 5 1 . 5 0 5 2 : r IA-T2I: Internet-Augmented Text-to-Image Generation Chuanhao Li1*, Jianwen Sun2,5*, Yukang Feng2,5*, Mingliang Zhai3, Yifan Chang4,5, Kaipeng Zhang1,5(cid:0) 1Shanghai AI Laboratory 2Nankai University 3Beijing Institute of Technology 4University of Science and Technology of China 5Shanghai Innovation Institute"
        },
        {
            "title": "Abstract",
            "content": "Current text-to-image (T2I) generation models achieve promising results, but they fail on the scenarios where the knowledge implied in the text prompt is uncertain. For example, T2I model released in February would struggle to generate suitable poster for movie premiering in April, because the character designs and styles are uncertain to the model. To solve this problem, we propose an Internet-Augmented text-to-image generation (IA-T2I) framework to compel T2I models clear about such uncertain knowledge by providing them with reference images. Specifically, an active retrieval module is designed to determine whether reference image is needed based on the given text prompt; hierarchical image selection module is introduced to find the most suitable image returned by an image search engine to enhance the T2I model; self-reflection mechanism is presented to continuously evaluate and refine the generated image to ensure faithful alignment with the text prompt. To evaluate the proposed frameworks performance, we collect dataset named Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Moreover, we carefully craft complex prompt to guide GPT-4o in making preference evaluation, which has been shown to have an evaluation accuracy similar to that of human preference evaluation. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by about 30% in human evaluation."
        },
        {
            "title": "Introduction",
            "content": "Text-to-image (T2I) generation models, such as Stable Diffusion [1], ControlNet [2] and FLUX [3], have attracted considerable attention for their ability to generate highly realistic images based on text prompts that often encapsulate complex and context-dependent knowledge. However, knowledge is unevenly distributed across the world, constantly evolving, and often ambiguous. These characteristics make it challenging for T2I models to perform reliably in scenarios where the knowledge implied in the text prompt is uncertain. For example, T2I model released in February would likely struggle to generate an appropriate poster for movie premiering in April, as it lacks access to up-to-date information about the movie. Key visual elements such as character designs, costumes, and stylistic choices may not be publicly available or finalized at the time the model was trained, which can lead to inaccurate or overly generic results. To address this issue, we propose an Internet-Augmented Textto-Image (IA-T2I) generation framework, which augments T2I models understanding of uncertain knowledge by supplying them with relevant reference images retrieved from Internet. We first present our overall framework for augmenting T2I models with reference images, consists of six components: active retrieval module, query generator, search engine, hierarchical image * Equal contribution (cid:0) Corresponding author: kp_zhang@foxmail.com Figure 1: The proposed IA-T2I, framework for T2I models to refer to images. selection module, augmented T2I generation, and self-reflection mechanism, as illustrated in Figure 1. Specifically, We begin by exploring the knowledge boundaries of T2I model to determine whether generating an accurate image for given text prompt requires additional reference images. Next, we use large vision-language models (LVLMs) to extract queries from the text prompt and retrieve potentially useful reference images via search engines. However, directly augmenting T2I models with the retrieved images is impractical due to: (1) The number of retrieved images is typically large, making processing computationally expensive and time-consuming. (2) Although the images are ranked by relevance, those at the top are not necessarily the most suitable as reference images. To address this, we introduce hierarchical image selection module to identify the most helpful reference image for guiding image generation. It first performs an initial filtering based on diversity to form candidate set, and then re-ranks the candidates to select the most relevant and informative reference image. Once the T2I model generates an image augmented by the selected reference image, self-reflection mechanism is employed to evaluate the output and autonomously decide whether to reselect reference images for another generation attempt or to accept the result as final. We then construct dataset dubbed Img-Ref-T2I, where text prompts include three types of uncertain knowledge: (1) known but rare. (2) unknown. (3) ambiguous. Known but rare refers to knowledge that existed before the models release (set as March 26, 2025, the latest update date of GPT-4o [4] in this paper) but is not commonly encountered. For example, people are generally more familiar with checkers than with Polish draughts. Unknown refers to new knowledge that emerges after the models release, which is inaccessible to the model. Ambiguous refers to concept that has different meanings and visual representations depending on the context. For example, drawing map of EU member states requires specifying point in time, as the composition of the European Union has changed over the years. Our Img-Ref-T2I dataset is designed for two tasks: general text-to-image generation and text-conditioned image editing (TI2I). To assess the quality of generated images, we design complex prompt to perform automatic preference evaluation pipeline based on GPT-4o, which achieves evaluation accuracy comparable to human preference evaluation. Experimental results on the Img-Ref-T2I dataset validate the effectiveness of the proposed IA-T2I framework, outperforming GPT-4o by approximately 30% in human evaluations. Our contributions are summarized as follows. (1) We propose IA-T2I, the first framework that integrates reference images from the Internet into T2I models, effectively mitigating inaccurate image generation caused by uncertain knowledge in text prompts. (2) We collect Img-Ref-T2I, the first dataset for evaluating the performance of T2I models under three types of scenarios where the textual knowledge is uncertain. (3) We develop GPT-4o-based automatic preference evaluation method by prompt engineering, achieving results comparable to human preference evaluation."
        },
        {
            "title": "2.1 Text-to-Image Generation",
            "content": "With the continuous advancement of technologies such as generative adversarial networks [5, 6], diffusion models [7, 1, 2], and autoregressive models [8, 9], text-to-image generation has garnered 2 increasing attention. Works [2, 10, 11] emphasize the role of control signals in guiding the image generation process. Works [12, 13, 14, 15, 16] unify generation and understanding within single model. Works [17, 18, 19, 20, 21, 22, 23] explore the in-context learning capabilities of text-to-image models. In contrast, we focus on retrieving helpful reference images for the image generation process to mitigate the uncertain textual knowledge, which is orthogonal to existing works. Works [24, 25, 26, 27, 28] explore retrieving additional reference images for text-to-image generation, where the retrieval modules require model-specific training and typically retrieve from fixed, carefully curated local databases. In contrast, our proposed IA-T2I framework is training-free and retrieves reference images from the Internet, which is constantly evolving and highly noisy source. 2.2 Internet-Augmented Generation Recently Internet-augmented generation (IAG) attracted increasing attention of both the natural language processing (NLP) and vision-and-language (V&L). In NLP, Komeili et.al. [29] demonstrate that incorporating search engines into large language models (LLMs) can reduce the generation of factually incorrect content during human dialogues. Lazaridou et.al. [30] employs few-shot prompting to allow LLMs to leverage knowledge retrieved from search engine to respond to questions involving factual and up-to-date information. Tian et.al. [31] use IAG to build open-domain generative dialogue system for digital human. In V&L, Li et al. [32] propose SearchLVLMs, framework that enables existing LVLMs to access up-to-date knowledge during inference through IAG. Jiang et al. [33] present MMSearch to empower LVLMs for multimodal searching via IAG. Differently, we introduce IAG into the text-to-image generation task, to mitigate inaccurate image generation caused by uncertain knowledge in text prompts. 3 IA-T2I Framework In this section, we introduce IA-T2I, framework that addresses the issue brought by uncertain knowledge by integrating reference images from the Internet into them. The overview of IA-T2I is illustrated in Figure 1. For text prompt ğ‘‡ (for TI2I, the input information additionally includes an original image ğ¼0), we first use an active retrieval module to determine whether reference image is required. Then we extract queries for ğ‘‡ and fed them into search engine to obtain potential reference images. Next, hierarchical image selection module is used to identify the most helpful reference image, and the T2I model is augmented by the reference image to generate an output image. Finally, we employ self-reflection mechanism to evaluate the output and determine whether to reselect reference images for another generation attempt or accept the result as final."
        },
        {
            "title": "3.1 Active Retrieval",
            "content": "To determine whether reference image is required for ğ‘‡, it is essential to explore the knowledge boundaries of T2I model. We attempt two different lines: (1) Judge solely based on the input information (ğ‘‡ for T2I, ğ‘‡ and ğ¼0 for TI2I). (2) Judge based on both the input information and the image generated by the T2I model (without reference images). For the former, we prompt the model to analyze the input information and determine whether it contains uncertain knowledge. For the latter, we assess the instruction-following ability of the generated image to decide whether reference images are needed for regeneration. The effectiveness of active retrieval depends on the design of the prompt. Detailed experimental analysis and prompt choice are provided in Section 5.5."
        },
        {
            "title": "3.2 Query Generator",
            "content": "We leverage existing LVLMs, such as GPT-4o [4] and Qwen2.5-VL [34], to extract queries from ğ‘‡ to obtain queries that lead search engines to return potential helpful reference images. Thanks to their language understanding capabilities, LVLMs can infer the grammatical role of each word in ğ‘‡, even when the knowledge implied in certain words is uncertain. Since some knowledge is culture-specific, we translate ğ‘‡ into two additional languages including Chinese and Japanese, and extract queries accordingly to ensure higher recall of reference images returned by search engines. The prompt used for guiding LVLMs in generating queries can be found in the Appendix."
        },
        {
            "title": "3.3 Search Engine",
            "content": "The queries in three different languages are fed separately to the search engine. By invoking the image search function, the engine directly returns multiple images. However, directly using all the returned images to enhance the T2I model is impractical. Firstly, many of the images are noisy and may mislead the T2I generation process. Additionally, the sheer number of images makes it computationally expensive and time-consuming to process them all. Although the search engine ranks the returned images based on query relevance, the top-ranked images may only match the content of the webpage they are embedded in rather than the actual visual content. Therefore, further filtering of the returned images is necessary."
        },
        {
            "title": "3.4 Hierarchical Image Selection",
            "content": "To filter the returned images, we propose hierarchical image selection module that performs two-step filtering, consisting of diversity-based selection followed by re-ranking process. Diversity Selection. We first perform an initial filtering of the returned images by selecting those with the greatest diversity, aiming to ensure that the candidate set includes as many useful reference images as possible. Specifically, we extract CLIP features [35] from the returned images and apply k-means clustering [36] based on cosine similarity between these features to form clusters. The image closest to the center of each cluster is then selected as candidate reference image. Re-Rank. The purpose of re-rank is to sort the candidate reference images and select the most helpful one. We input multiple images into LVLMs and use the prompt (can be found in the Appendix) to guide them in ranking. Notably, due to the self-reflection mechanism (described in Section 3.6), the hierarchical image selection module may be executed in multiple rounds. Therefore, we use ğ¼ğ‘Ÿ ğ‘’ ğ‘“ to denote the top-1 reference image selected for round ğ‘–. ğ‘–"
        },
        {
            "title": "3.5 Augmented Generation",
            "content": "After obtaining the reference image ğ¼ğ‘Ÿ ğ‘’ ğ‘“ , We provide it during the image generation process of T2I models and indicate its role in the text prompt to perform augmented generation. The output image by augmented generation of round ğ‘– is denoted as ğ¼ ğ‘œ ğ‘– . ğ‘–"
        },
        {
            "title": "3.6 Self-Reflection",
            "content": "A self-reflection mechanism is employed to evaluate the accuracy and usability of the output image ğ¼ ğ‘œ ğ‘– generated in the current round. If the result is deemed unsatisfactory, new round is initiated to reselect reference images and attempt generation again. The evaluation of ğ¼ ğ‘œ is based on three key ğ‘– criteria: (1) Whether it faithfully follows the text prompt ğ‘‡. (2) Whether the reference image ğ¼ğ‘Ÿ ğ‘’ ğ‘“ is ğ‘– helpful. (3) Whether it effectively incorporates information from the reference image ğ¼ğ‘Ÿ ğ‘’ ğ‘“ selected in this round. (4) For ğ‘– > 1, whether it improves upon the output image from the previous round, i.e., ğ¼ ğ‘œ ğ‘–1. For these four criteria, we use different prompts (provided in the Appendix) to guide GPT-4o in scoring ğ¼ ğ‘œ is accepted as the final output image, denoted as ğ¼ğ‘œğ‘¢ğ‘¡ . ğ‘– . When the total score is greater than or equal to 8, ğ¼ ğ‘œ ğ‘– ğ‘– 4 Img-Ref-T2I Dataset To evaluate the performance of T2I models in scenarios with uncertain textual knowledge, we construct dataset named Img-Ref-T2I, which maintains high quality, as it is entirely curated and annotated by human experts. The dataset consists of total of 240 samples: 120 for T2I and another 120 for TI2I. We categorize uncertain knowledge into three types: (1) Known but rare, (2) Unknown, and (3) Ambiguous. For each tasks 120 samples, 30 samples correspond to each of the three uncertainty categories. In addition, we collect 30 samples that contain no uncertain knowledge, which are used to evaluate the accuracy of the active retrieval module. Known but rare refers to knowledge that existed before the release date of the model. In this paper, we define this as prior to GPT-4os most recent update, March 26, 2025. Unknown refers to new concepts or events that emerged after the models release. Ambiguous refers to concepts that exhibit different visual representations depending 4 Figure 2: The distribution and examples of the proposed Img-Ref-T2I dataset. on the context. For each sample containing uncertain knowledge, we manually collect ground-truth (GT) reference image to evaluate the performance bottleneck in image generation. The distribution and examples of the Img-Ref-T2I dataset are illustrated in Figure 2."
        },
        {
            "title": "5.1 Settings",
            "content": "Baselines. We select three categories of models as baselines for our experiments: (1) T2I models, including FLUX [3] and DDPM [37]. FLUX is commonly used T2I model, while DDPM is an inverse-based T2I model capable of injecting reference image information into the generation process. (2) TI2I models, represented by Step1X-Edit [38]. (3) Omnipotent (Omni.) models, including Gemini-2.0-flash [39] (represented by Gemini in the following text for convenience) and GPT-4o [4]. These commercial models support both T2I and TI2I, and can incorporate reference images as contextual input for image generation or editing. Implementation Details. For open-source models: FLUX, DDPM, and Step1X-Edit, we reimplement them via their official code repositories. For closed-source commercial models, GPT-4o and Gemini, we access the models via their official APIs. We incorporate GPT-4o and Gemini into IA-T2I, as they support reference images. We perform evaluations with single Nvidia A100 GPU."
        },
        {
            "title": "5.2 Generated Image Evaluation",
            "content": "We evaluate generated images on the proposed Img-Ref-T2I dataset via human evaluation. For text prompt ğ‘‡ in the T2I task, the image generated by model ğ‘‹ is denoted as ğ¼ğ‘œğ‘¢ğ‘¡ . Each record is represented as [ğ‘‡, ğ¼ ğ‘‹ ğ‘œğ‘¢ğ‘¡ ] (for the TI2I task, the original image ğ¼0 is additionally included in the record). We ask evaluators (co-authors of this paper) to score the outputs based on three aspects: (1) Aesthetic Quality (AQ). Evaluators are asked three questions: a. (Layout) Is the layout of ğ¼ ğ‘‹ ğ‘œğ‘¢ğ‘¡ harmonious? b. (Color) Are the colors in ğ¼ ğ‘‹ ğ‘œğ‘¢ğ‘¡ coordinated? c. (Clearness) Is ğ¼ ğ‘‹ ğ‘œğ‘¢ğ‘¡ visually clear? (2) Commonsense Consistency (CC). Evaluators are asked: Do the details in ğ¼ ğ‘‹ ğ‘œğ‘¢ğ‘¡ align with human commonsense? For example, an image of cat with two tails would violate commonsense. (3) Instruction Following 5 Table 1: Comparision with SOTA image generation models on Img-Ref-T2I, where Raw represents the model without our framework, Ours stands for incorporating the Raw baseline into our framework. N/A/GT/Search denote use nothing/ground-truth/our frames reference images. The value before/after /\" indicates the score on the T2I/TI2I task. Reference Image AQ Type Model Variant T2I FLUX [3] Raw DDPM [37] Raw TI2I Step1X-Edit [38] Raw Omni. Gemini GPT-4o Raw Raw Ours Raw Raw Ours - - - -/71.9 95.3/- 77.2/- N/A GT Search Layout - - - - - - - - - - - - - - - Color Clearness CC IF Overall 93.3/- 84.8/- 87.3/- 48.7/- 91.3/- 81.6/- 3.3/- 72.1/- 3.3/- 40.5/- -/85.0 -/48.8 -/80.6 -/5. -/3.1 96.3/91.2 97.5/96.0 88.2/89.3 91.9/91.9 12.9/9.4 11.1/6.7 93.7/93.0 96.2/95.5 81.8/87.3 92.5/95.0 67.3/31.2 56.6/26.1 95.8/87.7 97.0/96.7 90.4/83.2 94.6/89.7 52.1/13.5 45.5/10.3 98.0/96.3 99.3/100 98.0/98.1 97.4/94.4 20.4/33.3 19.1/32.7 99.3/97.4 100/100 98.5/96.8 96.3/96.8 69.9/76.0 65.4/72.7 98.6/98.6 99.3/100 93.7/95.7 95.8/96.4 55.2/61.2 48.3/57.6 Figure 3: Experimental results of preference evaluation. (IF). Evaluators are asked: Does ğ¼ ğ‘‹ ğ‘œğ‘¢ğ‘¡ follow ğ‘‡ (or ğ‘‡ and ğ¼0 in the case of TI2I)? All questions in the evaluation process are binary: answers must be either Yes or No. Yes earns 1 point; No earns 0. An image scoring 5 points is considered correct in overall. Note that throughout the human evaluation process, model ğ‘‹ remains hidden from evaluators to ensure fairness. Each record is evaluated by three different annotators to reduce human bias. For each model, we normalize the scores for each evaluation criterion by dividing the total score for that criterion by the number of samples. The resulting value is used as the final score for that evaluation criterion. Experimental results are listed in Table 1. We can observe that: (1) On our proposed Img-Ref-T2I dataset, existing models struggle to generate correct images, indicating that knowledge uncertainty significantly impacts the robustness of image generation. (2) Even when using ground-truth GT reference images as context, the generation accuracy of Gemini and GPT-4o still leaves room for improvement. (3) By leveraging the proposed IA-T2I framework, Gemini and GPT-4o achieve performance comparable to that with GT reference images, demonstrating the effectiveness of our framework in selecting appropriate reference images."
        },
        {
            "title": "5.3 Preference Evaluation\nPreference evaluation refers to the process of comparing two images, ğ¼ ğ‘‹1\nmodels ğ‘‹1 and ğ‘‹2 respectively for a given text prompt ğ‘‡. By assessing ğ¼ ğ‘‹1\ndimensions, we determine which model performs better.",
            "content": "ğ‘œğ‘¢ğ‘¡ and ğ¼ ğ‘‹2 ğ‘œğ‘¢ğ‘¡ and ğ¼ ğ‘‹2 ğ‘œğ‘¢ğ‘¡ , generated by ğ‘œğ‘¢ğ‘¡ from multiple Human Preference Evaluation. Similar to Section 5.2, we begin the human evaluation by comparing ğ‘œğ‘¢ğ‘¡ and ğ¼ ğ‘‹2 ğ¼ ğ‘‹1 ğ‘œğ‘¢ğ‘¡ from three aspects: aesthetic quality, commonsense consistency, and instruction 6 alignment. However, unlike Section 5.2, the evaluation questions shift from Is it . . . ? to Which model, ğ‘‹1 or ğ‘‹2, performs better? In addition, we introduce an extra question: Considering all the above aspects, the image generated by which model is more suitable as the final generated result? to evaluate the overall preference. All questions are designed to have binary answers, either ğ‘‹1 or ğ‘‹2, to prevent ambiguous or indecisive responses from evaluators. We conduct the human preference evaluation by comparing the baseline models with their counterparts integrated into our proposed framework, i.e., ğ‘‹1 and ğ‘‹2, respectively. Two sets of evaluations were performed, with the baseline models being Gemini and GPT-4o. The experimental results are presented in Figure 3. It can be observed that, for both the T2I and TI2I tasks, human evaluators consistently prefer the images generated using our framework. This indicates that the proposed framework significantly improves the performance of the baseline models. GPT-4o Preference Evaluation. Conducting large-scale human preference evaluations is extremely costly. To address this, we design complex prompt to guide GPT-4o in making preference evaluation based on GPT-4o, which enables automatic assessment and significantly reduces human labor costs. For the T2I task, GPT-4o preference evaluation relies on the input set [ğ‘‡, ğ¼1, ğ¼2, ğ¼ref]. For the TI2I task, it additionally considers the initial image ğ¼0. The prompt is provided in the Appedix. Experimental results of this automated preference evaluation are denoted as Ours and shown in Figure 3, demonstrating that the scores produced by our pipeline are comparable to those from the human preference evaluation, highlighting the potential of automatic preference evaluation. Table 2: Ablation studies of our framework on Img-Ref-T2I."
        },
        {
            "title": "Diversity",
            "content": "Re-Rank"
        },
        {
            "title": "Self",
            "content": "Acc. Ori. Qwen2.5-VL GPT-4o Selection Qwen2.5-VL GPT-4o Human Reflection T2I TI2I"
        },
        {
            "title": "Raw",
            "content": "o 4 - G"
        },
        {
            "title": "Ours",
            "content": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 19.1 32.7 51.4 61.2 56.1 67.6 58.0 69.7 25.8 39.8 26.9 40.2 40.8 49.4 48.3 57."
        },
        {
            "title": "5.4 Ablation Studies",
            "content": "The results of the ablation study are shown in Table 2, where Acc. denotes the overall score of the generated images as assessed by human evaluation. We incrementally integrate different components of our proposed framework on top of GPT-4o, and explored several implementation variants for specific modules. The following observations can be made: (1) The framework is not highly sensitive to the choice of query generator. Using the original text prompt as the query (Ori.) or extracting queries via Qwen2.5-VL [34] yields competitive performance. (2) When reference images are manually selected by humans (Human), the generated images are still not entirely accurate, indicating that T2I and TI2I models require further improvements in utilizing reference images effectively. (3) Performing an initial filtering of search engine results using image clustering significantly enhances generation accuracy by reducing the difficulty of the re-ranking process. (4) Incorporating selfreflection mechanism further improves generation accuracy. These findings demonstrate that the components of our framework are both effective and complementary. ğ‘– , ğ¼ ğ‘œ ğ‘–1 ğ‘– , ğ‘‡), (ğ¼ ğ‘œ We perform an ablation study on the criteria used in the self-reflection mechanism, with the ğ‘– , ğ¼ğ‘Ÿ ğ‘’ ğ‘“ (ğ¼ ğ‘œ ), results shown in Table 3. and (ğ¼ ğ‘œ ) correspond to criteria 1, 2, and 3 in Section 3.6, respectively. The results indicate that employing all three criteria together leads to higher image generation accuracy, demonstrating not only the complementarity of the criteria but also the rationality and effectiveness of the self-reflection mechanism. ğ‘– Model Variant (ğ¼ğ‘œ Table 3: Ablation studies of self-reflection. ğ‘– , ğ¼ğ‘Ÿ ğ‘’ ğ‘“ - ğ‘– , ğ¼ğ‘œ ğ‘–1 - ) (ğ¼ğ‘œ"
        },
        {
            "title": "Raw",
            "content": "ğ‘– ) T2I TI2I 19.1 32.7 ğ‘– , ğ‘‡) (ğ¼ğ‘œ - 4 - G"
        },
        {
            "title": "Ours",
            "content": "- - - 41.7 46.6 43.6 51.2 48.3 57.6 7 Figure 4: Prompt candidates for active retrieval."
        },
        {
            "title": "Prompt",
            "content": "Table 4: Analysis of active retrieval. For the two distinct lines of the active retrieval module mentioned in Section 5.5, we conduct experiments using different prompts, and the results are shown in Table 4. In this table, Dependency indicates which inputs given prompt relies on to determine whether to trigger active retrieval. For the specific information of each prompt, please refer to Figure 4. Specifically, ğ‘‡, ğ¼ ğ‘œ, and ğ¼0 represent the text prompt, the image output by the model without reference images, and the original image (only applicable to the TI2I task), respectively. Acc. refers to the accuracy of the active retrieval module, defined as the percentage of correct decisions: for samples containing uncertain knowledge, the module should output Y, and for those without, N. We observe the following: (1) For T2I, using more dependencies and providing more detailed prompts can lead to higher accuracy. (2) For TI2I, due to the increased complexity, relying solely on the textual input ğ‘‡ for judging active retrieval actually yields better results. This implies that dependencies and prompts should be adjusted according to the specific task. prompt1 - prompt2 - prompt3 prompt6 - 91.7 prompt4 74.2 prompt5 80.8 Dependency ğ¼ğ‘œ ğ¼0 ğ‘‡ - - - 90.0 80.8 95.8 Acc. TI2I T2I - -"
        },
        {
            "title": "5.6 Analysis of Diversity Selection",
            "content": "This section investigates the necessity of diversity selection. We compared the performance of using different numbers of clusters for diversity selection with baseline where all images retrieved are directly used for Re-Rank. The experimental results are shown in Figure 5. Acc. represents the ratio of cases where the Top-1 image obtained through Re-Rank is suitable reference image, as judged by humans. C(ğ‘¥) indicates the use of diversity selection with ğ‘¥ clusters, while NC means no diversity selection is applied. From the figure, the following observations can be made: (1) When the number of clusters is less than 10, as the number of clusters increases, the ratio of correctly ranked Top-1 images also increases. (2) When the number of clusters exceeds 10, the correct Re-Rank rate begins to decrease as the number of clusters increases. (3) When no diversity selection is used, the accuracy of Re-Rank is the lowest, because an excessive number of images introduces noise into the Re-Rank process. These observations demonstrate that diversity selection is both necessary and effective. Figure 5: Analysis of diversity selection."
        },
        {
            "title": "5.7 Qualitative Analysis",
            "content": "Figure 6 depicts several qualitative examples from the Img-Ref-T2I dataset of different models. The words in bold red in the text prompt denotes short description of the ground-truth reference image (GTRF). For both T2I and TI2I, we provide two qualitative examples respectively. We can observe the following: (1) GPT-4o (Ours) performs the best, consistently generating reasonable images across all four qualitative examples. (2) GPT-4o (with GTRF) performs slightly worse than GPT-4o (Ours). 8 Figure 6: Qualitative comparisons on the proposed Img-Ref-T2I dataset, where GTRF denotes the ground-truth reference image provided in Img-Ref-T2I by human selection. Due to the availability of only one GTRF and the lack of self-reflection process, the generated images often contain minor issues. For example, in the fourth example, the icon generated by GPT-4o (with GTRF) has noticeable flaws. (3) DDPM (with GTRF) uses the GTRF but merely imitates it without deeper understanding. (4) FLUX (w/o Reference) and Step1X-Edit (w/o Reference) cannot utilize any reference images and perform poorly on the T2I and TI2I tasks, respectively. These observations demonstrate that the proposed IA-T2I framework is effective and, in some scenarios, can even produce results that are better than the GTRF."
        },
        {
            "title": "6 Limitation",
            "content": "The Img-Ref-T2I dataset constructed in this paper relies on manual collection and annotation, making large-scale expansion extremely costly. In the future, we plan to explore automatic collection and annotation of T2I and TI2I samples with uncertain knowledge."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we have presented IA-T2I, framework that integrates reference images from the Internet into T2I/TI2I models, which can effectively mitigate inaccurate image generation caused by uncertain knowledge in text prompt. An Img-Ref-T2I dataset that includes three types of scenarios involving uncertain knowledge is curated by human experts. The dataset enables evaluate the ability of T2I/TI2I models to generate images when the textual knowledge is uncertain. Experimental results on Img-Ref-T2I demonstrate that our framework can significantly enhances the image generation performance of T2I/TI2I models in these challenging scenarios."
        },
        {
            "title": "References",
            "content": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. High-resolution image synthesis with latent diffusion models, 2021. [2] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [3] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [4] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139 144, 2020. [6] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13161324, 2018. [7] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [8] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [9] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [10] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. Controlnet++: Improving conditional controls with efficient consistency feedback: Project page: liming-ai. github. io/controlnet_plus_plus. In European Conference on Computer Vision, pages 129147. Springer, 2024. [11] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mgpt: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. [12] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [13] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [14] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024. [15] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [16] Jianwen Sun, Yukang Feng, Chuanhao Li, Fanrui Zhang, Zizhen Li, Jiaxin Ai, Sizhuo Zhou, Yu Dai, Shenglin Zhang, and Kaipeng Zhang. Armor v0. 1: Empowering autoregressive multimodal understanding model with interleaved multimodal generation via asymmetric synergy. arXiv preprint arXiv:2503.06542, 2025. [17] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Chen Liang, Tong Shen, Han Zhang, Huanzhang Dou, Yu Liu, and Jingren Zhou. Chatdit: training-free baseline for task-agnostic free-form chatting with diffusion transformers. arXiv preprint arXiv:2412.12571, 2024. [18] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. [19] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Towards segmenting everything in context. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11301140, 2023. [20] Zhendong Wang, Yifan Jiang, Yadong Lu, Pengcheng He, Weizhu Chen, Zhangyang Wang, Mingyuan Zhou, et al. In-context learning unlocked for diffusion models. Advances in Neural Information Processing Systems, 36:85428562, 2023. [21] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [22] Ivona Najdenkoska, Animesh Sinha, Abhimanyu Dubey, Dhruv Mahajan, Vignesh Ramanathan, and Filip Radenovic. Context diffusion: In-context aware image generation. In European Conference on Computer Vision, pages 375391. Springer, 2024. [23] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [24] Andreas Blattmann, Robin Rombach, Kaan Oktay, Jonas MÃ¼ller, and BjÃ¶rn Ommer. Retrieval-augmented diffusion models. Advances in Neural Information Processing Systems, 35:1530915324, 2022. [25] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and Yaniv Taigman. Knn-diffusion: Image generation via large-scale retrieval. arXiv preprint arXiv:2204.02849, 2022. [26] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William Cohen. Re-imagen: Retrieval-augmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022. [27] Huaying Yuan, Ziliang Zhao, Shuting Wang, Shitao Xiao, Minheng Ni, Zheng Liu, and Zhicheng Dou. Finerag: Fine-grained retrieval-augmented text-to-image generation. In Proceedings of the 31st International Conference on Computational Linguistics, pages 1119611205, 2025. [28] Yuanhuiyi Lyu, Xu Zheng, Lutao Jiang, Yibo Yan, Xin Zou, Huiyu Zhou, Linfeng Zhang, and Xuming Hu. Realrag: Retrieval-augmented realistic image generation via self-reflective contrastive learning. arXiv preprint arXiv:2502.00848, 2025. [29] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. arXiv preprint arXiv:2107.07566, 2021. [30] Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented language models through few-shot prompting for open-domain question answering. arXiv preprint arXiv:2203.05115, 2022. [31] Junfeng Tian, Hehong Chen, Guohai Xu, Ming Yan, Xing Gao, Jianhai Zhang, Chenliang Li, Jiayi Liu, Wenshen Xu, Haiyang Xu, et al. Chatplug: Open-domain generative dialogue system with internetaugmented instruction tuning for digital human. arXiv preprint arXiv:2304.07849, 2023. [32] Chuanhao Li, Zhen Li, Chenchen Jing, Shuo Liu, Wenqi Shao, Yuwei Wu, Ping Luo, Yu Qiao, and Kaipeng Zhang. Searchlvlms: plug-and-play framework for augmenting large vision-language models by searching up-to-date internet knowledge. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [33] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024. [34] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 87488763, 2021. [36] Anil Jain and Richard Dubes. Algorithms for clustering data. Prentice-Hall, Inc., 1988. [37] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1246912478, 2024. [38] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [39] Google Gemini2. Experiment with gemini 2.0 flash native image generation. 2025."
        },
        {
            "title": "A Prompts",
            "content": "In this section, we provide the prompts of this paper, as shown in the Figure 7. Figure 7: The prompts used in this paper."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology",
        "Nankai University",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "University of Science and Technology of China"
    ]
}