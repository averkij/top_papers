{
    "paper_title": "Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning",
    "authors": [
        "Jiazheng Liu",
        "Sipeng Zheng",
        "Börje F. Karlsson",
        "Zongqing Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, a multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as a strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chain-of-Thought and annotations respectively, throughout multi-turn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 2 0 0 7 0 . 3 0 5 2 : r Taking Notes Brings Focus? Towards Multi-Turn Multimodal Dialogue Learning Jiazheng Liu1* Sipeng Zheng2 Borje F. Karlsson2 Zongqing Lu1,2 1School of Computer Science, Peking University 2Beijing Academy of Artificial Intelligence"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs), built on large-scale pre-trained vision towers and language models, have shown great capabilities in multimodal understanding. However, most existing MLLMs are trained on single-turn vision question-answering tasks, which do not accurately reflect real-world human conversations. In this paper, we introduce MMDiag, multi-turn multimodal dialogue dataset. This dataset is collaboratively generated through deliberately designed rules and GPT assistance, featuring strong correlations between questions, between questions and images, and among different image regions; thus aligning more closely with real-world scenarios. MMDiag serves as strong benchmark for multi-turn multimodal dialogue learning and brings more challenges to the grounding and reasoning capabilities of MLLMs. Further, inspired by human vision processing, we present DiagNote, an MLLM equipped with multimodal grounding and reasoning capabilities. DiagNote consists of two modules (Deliberate and Gaze) interacting with each other to perform Chainof-Thought and annotations respectively, throughout multiturn dialogues. We empirically demonstrate the advantages of DiagNote in both grounding and jointly processing and reasoning with vision and language information over existing MLLMs. 1. Introduction In recent years, large language models (LLMs) have achieved remarkable advances in various natural language applications, including chatbots [1, 2, 35], programming assistants [12], and rhetorical aides [14]. The success has further spurred the development of multimodal large language models (MLLM) [25, 51]. However, most existing MLLMs are trained as single black-box systems to handle multimodal instructions, often struggling with inaccuracies *Work done as an intern at BAAI. Correspondence to <zongqing.lu@pku.edu.cn> and hallucinations, especially in complex multi-turn dialogues [42, 50]. We hypothesize such challenges arise from the MLLMs difficulty in maintaining focus on target regions throughout the conversation, especially for highIn this resolution images with overly long visual tokens. paper, we seek to address these issues by moving beyond black-box approach to an explicit target-grounding solution. Here, we summarize two key goals for multiturn multimodal dialogue learning: ❶ saliency tracking, where the MLLM must keep tracking different relevant regions over the course of the dialogue, and ❷ saliency recall, where the model needs to consistently retain focus on the same critical information across multiple questionanswering (QA) rounds. For example, in the dialogue illustrated in Figure 1, completing the Minigrid [10] task requires the MLLM to accurately locate both the agent (i.e. red triangle) and the target (i.e. purple key) to answer the initial question. The following question then builds upon this information, requiring the MLLM to reason about the agents starting position based on the previously identified location of the key. This example illustrates the need for sustained and explicit grounding to multiple specific visual details in multi-turn multimodal dialogue. To achieve these two goals, we draw inspiration from how humans maintain focus while studying. For instance, when working through documents, people may lose concentration, but can quickly refocus by using simple techniques such as jotting down notes or highlighting key points. Even basic marks, such as circling or underlining, can significantly enhance focus without requiring elaborate explanations. These visual cues guide attention, making it easier to track, recall, and revisit important information. In contrast, existing MLLMs lack such tracking capabilities, prompting us to ask: Can an MLLM be designed to equip similar attention-guiding abilities? If so, what would that model design entail? To answer this question, we first review existing tuning methods for MLLMs and identify critical gap: the lack of quality multi-turn multimodal dialogue datasets that adequately reason over both visual and text information. Exist1 processing to produce an answer accompanied by optional reasoning and grounding steps. Through this interactive mechanism, DiagNote can achieve more effective reasoning with multimodal information, resulting in accurate and context-aware responses throughout multi-turn dialogues. Our main contributions are summarized as follows: ❶ To address the need for robust multimodal grounding and reasoning, we build new large-scale multi-turn multimodal dialogue dataset MMDiag across several QA scenarios (e.g. daily life and tabular data), using rule-based searching and GPT-4o-mini [31] capabilities. ❷ Inspired by human visual processing, we propose DiagNote and its two key modules Deliberate and Gaze to enhance the models capacity for multimodal information integration and reasoning. ❸ We evaluate DiagNotes reasoning and grounding abilities on MMDiag and other benchmarks and the results demonstrate that the introduction of MMDiag and DiagNote significantly improves performance in multimodal conversations, while the MMDiag itself can also serve as more challenging benchmark for this area. 2. Related Work 2.1. Multimodal Large Language Models The introduction of Transformers [27, 46] and large-scale training has greatly enhanced model capabilities, leading to the development of advanced vision encoders [33] and large language models (LLMs) [11, 44]. Building on these advancements, multimodal large language models (MLLMs) [25, 50] have demonstrated impressive performance across wide range of multimodal tasks, and potential applications from VR/AR to game agents [16, 48]. An MLLM typically consists of three main components: modality encoders, modality interfaces, and LLMs [49]. Modality encoders and LLMs process modality information and language separately, and then modality interfaces align other modalities with the representations of the language. For modality interfaces, most approaches [21, 25] rely on learnable connectors. For modality encoders, research indicates that visual information processing (especially in terms of image resolution [30]) significantly affects the performance of MLLM. Additionally, certain models incorporate generators to produce other multimodal data, such as low-level actions [15] or images [50]. MLLM training commonly follows two-stage process. In the first stage, vision and language modalities are aligned with the modality interface, often through pre-training on large datasets of image-caption pairs [5, 25, 36]. The second stage involves fine-tuning with visual question-answering (VQA) tasks [25, 40] for better LLMs capabilities of instruction following. This two-stage process is widely used in MLLMs like PALI-X [8], Qwen-VL [3], and LLaVA [25], forming strong foundation for subsequent Figure 1. Multi-turn multimodal dialogue: (a) Saliency tracking. The MLLM needs to focus on both the red triangle agent and the purple key, which scatter on the image, to answer the question correctly. (b) Saliency recall. The MLLM needs to retain focus on the region where the agent will stop after the last question. ing datasets, such as MMDU [28] and SciGraphQA [22], primarily consist of single-turn QA pairs, where most questions can be answered independently without relying on prior context. To bridge this gap, we introduce novel dataset, MMDiag, designed as foundational benchmark for challenging multi-turn multimodal dialogue. This dataset offers visually detailed multi-turn dialogues across range of scenarios. Furthermore, recent studies have introduced various modules to help keep focus in multi-turn multimodal dialogues. However, these methods either zoom in to progressively narrow focus areas with the aid of external grounding and OCR tools [32], or identify single region of interest per question before generating an answer [38]. These approaches lead to severe limitations: the zoom-in method restricts the focus to smaller regions, potentially missing broader context, while the single-region method isolates specific areas, overlooking multiple relevant details that could enrich responses. To address these limitations, we propose DiagNote, model designed to enhance focus and reasoning in multi-turn multimodal dialogue. DiagNote comprises two main modules: Deliberate and Gaze. The Deliberate module guides the Gaze module in dynamically adjusting the region of visual focus, while the Gaze module highlights crucial areas for subsequent processing by the Deliberate module. These two modules interact across multiple dialogue turns, emulating human visual 2 MLLM advancements. 2.2. Grounding and Reasoning Benefit MLLMs MLLMs can perform in-context learning [4], enabling generalization to new tasks from few examples. The Chain-of-Thought (CoT) [47] reasoning mechanism also allows models to approach problem-solving step-by-step. However, when faced with unfamiliar tasks, MLLMs sometimes rely excessively on the generalization capabilities of the LLM component, leading to overlooking visual details and hallucinations. To address these limitations, models like CogCoM [32] introduce Chain of Manipulations, allowing MLLMs to perform CoT reasoning with external grounding and OCR models, which enable incremental task-solving. Although this approach improves performance, it is limited to zooming in on specific areas and may miss key scattered details. Similarly, Visual CoT [38] enhances performance by focusing on single region of interest per question, improving both answer accuracy and visual grounding. However, single grounding and reasoning round is often insufficient for complex, multistep problems. To overcome these challenges, we propose two modules: Deliberate for reasoning and Gaze for grounding, enabling multiple rounds of CoT reasoning. This iterative approach allows for better problem-solving by refining both grounding and reasoning across interactions, making it more effective in handling complex tasks, like multi-turn multimodal QAs. 2.3. Multi-Turn Multimodal Dialogue Multi-turn dialogue entails sustained interactions between human and an MLLM-based agent. These range from casual interactions [39], to cooperative tasks with shared objectives [6] and structured question-answering scenarios [23, 40]. Our focus is on structured questionanswering in these dialogues. In language-only multiturn dialogues, core challenge lies in managing question interdependence, where responses to earlier questions serve as contextual references in subsequent queries. To provide accurate responses, interpret both the the model must initial answer and the contextual references in follow-up the questions. When visual modality is introduced, it must ❶ supplement model faces added complexity: language information with visual context, ❷ synchronize and integrate visual and linguistic data, and ❸ manage reduction in visual focus over prolonged dialogues. the In dialogues where questions are independent, interdependence challenge is absent, simplifying the interaction to single-turn question answering. Existing multiturn datasets [13, 22, 28] generally feature QA pairs with minimal interconnection. The MNIST Dialog [37] dataset incorporates spatial reasoning for correlated QA pairs, but tasks remain relatively simple. ChatterBox [43] acknowledges the referential challenge but undermines coherence with rule-based substitutions, simply substituting words occurring repeatedly with it, introducing ambiguities. Our approach addresses these limitations by generating correlated question-answer drafts through rulebased methods, then refining them using GPT-4o-mini [31]. This produces more complex and realistic multimodal, multi-turn dialogue dataset. 3. MMDiag: New Benchmark for Multi-"
        },
        {
            "title": "Turn Multimodal Dialogue",
            "content": "In the following section, we first motivate the choice of scenarios. Next, we show details on how to construct the QA pairs for our MMDiag dataset. We then explain the evaluation process in Section 3.3. Finally, we compare MMDiag with existing multimodal dialogue datasets in Section 3.4. MMDiag contains three scenarios: everyday, tabular, and Minigrid. Examples of QA pairs are given in Section A.2. Both MMDiag and its generation code will be publicly released. 3.1. Chosen Scenarios The three selected scenarios Everyday, Tabular, and Minigrid are chosen to evaluate distinct yet complementary challenges in multimodal reasoning. Everyday scenes test common-sense understanding and multiturn interactions, reflecting real-world AI applications. Tabular scenarios require structured data comprehension and numerical reasoning, which many MLLMs struggle with. And Minigrid focuses on spatial reasoning and planning, essential for navigation and decision-making. This diverse selection ensures comprehensive assessment of multimodal understanding. Empirically, all three settings pose significant challenges even for state-of-the-art models like GPT-4o (Figure 3), with notable failures, such as Visual CoTs inability to generate positive grounding predictions in Tabular tasks  (Table 2)  . 3.2. Dataset Curation Everyday Scene Subset. The raw source dataset [19] for this subset contains 108K images, each with detailed annotations. This allows us to construct directed graph = (V, E) for each image, where represents the objects and denotes their relationships. Then, each QA pair for an image is created and represented as subgraph of G, i.e., Gqa = (Vqa, Eqa), with nodes and edges that belong to either the question or the answer. Note that if QA pair lacks shared nodes or edges with other subgraphs, we classify it as independent, as it does not contribute to the dialogues complexity and does not require information from other QAs for response. The created QA pairs are then extended into multi-turn QAs. We begin by constructing subgraph pattern = (cid:83)n qa, where i=1 Gi 3 each Gi qa represents subgraph of QA pair, ensuring qa = . This design guarantees i, = i, that answering any individual pair requires information from other QA pairs within the multi-turn dialogue. qa s.t. i"
        },
        {
            "title": "This subset",
            "content": "We then apply subgraph matching to locate instances of in the graph for each image, enabling us to create diverse multi-turn QAs. We employ GPT-4o-mini [31] to generate various, natural questions, answers, and reasoning steps, while also providing ground truth location data for key objects. The specific prompt used in this process is detailed in Section A.1. Tabular Scene Subset. is sourced from ChartQA [29], which contains 18K real-world charts and 23.1K human-authored QA pairs. As ChartQA consists only of single-turn QA, it does not meet our multi-turn dialogue requirements. To generate multi-turn question answering, we use GPT-4o-mini, primarily relying on chart images due to the questionable reliability of tabletype metadata. To ensure interrelated dialogues, where certain regions are referenced as pronouns to increase complexity, we explicitly emphasize this requirement in the prompt. However, GPT-4o-mini struggles with maintaining this structure, requiring supplementary prompts to guide generation more effectively. Details on the prompt design are provided in Section A.1. Finally, we use EasyOCR [17] to match keywords with corresponding chart regions, enabling generation of bounding boxes for relevant areas. Minigrid Scene Subset. Minigrid [10] is Gymnasiumbased [45] collection of 2D grid-world environments with goal-oriented tasks. The agent, represented as triangular figure with discrete action space, navigates maze-like maps and interacts with objects such as doors, keys, and boxes. These tasks test the models ability to focus on image details, spatial reasoning, and action planning, with some requiring numerous steps to complete, making them particularly challenging. To construct this subset, we use Minigrid and BabyAI [9] to generate grid worlds, tasks, and step-by-step action plans, which are formatted as prompts for GPT-4o-mini. Minigrid creates environments based on specific constraints, saving grid world data as both rendered images and lists of special objects with bounding boxes. BabyAI then identifies feasible solutions by analyzing the agents field of view and determining subgoal-aligned actions. To simplify QA generation, we make the entire grid world visible, allowing MLLMs to guide the agent from top-down perspective. GPT-4o-mini then generates natural questions, reasoning steps, key region queries, and concise final answers. Further details on environment generation and prompt design are in Section A.1. Common Visual-Text Subset. To enable MLLMs with robust capabilities to answer the question, we also add additional visual-text pairs with high quality from previous works [25] to enhance their instruction-following ability. 4 3.3. Multi-Turn Multimodal Dialogue Evaluation The answers in MMDiag consist of three main components: the reasoning process, the corresponding grounded key region, and the final answer. Accordingly, we evaluate these three components separately. For the reasoning process and final answer, both of which are expressed in natural language and may vary in phrasing, we pass the image, question, ground-truth answer, and generated answer to powerful MLLM for scoring, adhering to widely used evaluation practice. To mitigate the potential bias of using the same model for both dataset generation and evaluation, as MMDiag is generated using GPT-4o-mini [31], we instead use Gemini-1.5-Pro [35] in evaluation. Following prior studies [7, 20, 41], we evaluate the MLLMs through ad-hoc reasoning and scoring across five categories on 0-10 scale, for greater consistency and interpretability. The complete evaluation prompt is provided in Section A.3. Additionally, we use the key region queries and their bounding boxes to constitute grounding (GND) subset for evaluation. Since key region queries often involve detailed descriptions of objects or areas, including attributes and relationships, this GND subset can effectively measure grounding capability for complex queries. In this context, we use Intersection over Union (IoU) to evaluate the accuracy of grounding. 3.4. Multimodal Dialogue Datasets Comparison We compare MMDiag with prior datasets designed for vision-language understanding and reasoning. As shown in Table 1, MMDiag is the first to feature multi-turn, multiregion dialogues with strong QA dependencies, reinforced In contrast, datasets by thorough generation process. like CB-300k [43] and MMDU [28] lack mechanisms to enforce such dependencies, reducing multi-turn dialogues to mere concatenations of independent QA pairs. Although MMDiag has relatively short dialogues, the inherent dependence between turns presents significant challenges for MLLMs, including GPT-4o, as demonstrated in Figure 3. The grounding and QA test splits include 1,000 unseen images and QA pairs, respectively. 4. DiagNote In this section, we introduce our proposed DiagNote and its training process. Using two essential modules named Deliberate and Gaze, DiagNote is trained on the train split of MMDiag to meet the requirements for multiturn multimodal dialogue, which provides capabilities of stepwise reasoning and grounding corresponding salient visual regions for each dialogue. Dataset QA Scale GND Scale Generation Process Average Turns Multi-Turn Multi-Region Dialogue Correlation CB-300k [43] Visual CoT [38] CoM [32] MMDU [28] MMDiag 463k 438k 76k 410k 639k 254k 438k - - GPT-4/Rule-based GPT-4/OCR GPT-4/Tree-Search/Human LLM-filtered/GPT-4o 1139k Graph-search/OCR/GPT-4o-mini 5.49 1 1 9 2.19 Table 1. Comparison between MMDiag and other multimodal dialogue datasets. : Features are considered, but implemented weakly. Figure 2. Model architecture of DiagNote. Regions with blue backgrounds represent deliberation step and the interaction between the Deliberate and Gaze modules. At each turn, the Deliberate module processes the original image, dialogue context, and buffers from both modules. It produces two outputs: (1) Deliberate step, stored in the Deliberate buffer, and (2) Gaze query, which is processed by the Gaze module. The resulting bounding boxes are then stored in the Gaze buffer. 4.1. Model Architecture The overall framework of our model is illustrated in Figure 2. We use the same architecture (LLaVA-1.5 [24, 25]) for both Deliberate and Gaze modules, where the two modules do not share parameters. Considering the generalization capabilities of MLLMs, we choose not to use dedicated grounding model like Grounding DINO [26] for the Gaze module. Specifically, the Deliberate module consists of an LLM as backbone, pre-trained ViT [34] as vision encoder, and one MLP with projection matrix to serve as the visual-text connector. The same structure applies for the Gaze module, with distinct parameters. Given an input image Iv, we consider the entire dialogue contains turns of questions and answers, which can be represented as (cid:0)I1 and It a, , IT respectively denote the question and the answer in the t-th dialogue turn. (cid:1), where It , IT q, i1 1, , St = (cid:0)St and Gaze query Qt (cid:1) to generate the Deliberate buffer Bt Deliberate step St i. The Gaze module G, again, takes Gaze query Qt as input and outputs the annotation bounding box ot i. This process continues until the Deliberate module outputs END as the Gaze query Qk Fin1, indicating that the Deliberate and Gaze back-andforth process is complete. Fin (i.e., It a) and the Gaze query Qt Finally, the image, the dialogue context, and all the buffers are fed into the Deliberate module to produce the final answer St Fin. The Gaze module then provides the bounding box of the salient area ot Fin for the t-th dialogue turn. The final output is St Fin, along with the optional key region bounding box (cid:1), Fin, as well as the Deliberate process (cid:0)St ot if required. The final answer It is then appended to the dialogue context for the next dialogue turn. 1, , St Fin1 4.2. Model Training a, , It1 At each turn t, given question It q, DiagNote undergoes multiple interactive rounds between the Deliberate and Gaze modules for reasoning and to generate reliable response It a. To be specific, for the first interactive round, the Deliberate module takes as input the dialogue context Ct = (cid:0)I1 (cid:1) and image Iv and outputs , It q, I1 1. St 1 and Gaze query Qt Deliberate step St 1 is then stored d. The Gaze module takes Gaze in the Deliberate buffer Bt query Qt 1 as input and outputs the corresponding bounding box ot In each subsequent interactive round of Deliberate and Gaze, the Deliberate module takes as input the image Iv, the dialogue (cid:1), and context Ct, the Gaze buffer Bt 1, which is stored in the Gaze buffer Bt g. , It1 = (cid:0)ot 1, , ot 5 The training process of both Deliberate and Gaze modules follows that of LLaVA, and DiagNote provides two prompt templates pd and pg for Deliberate and Gaze respectively. At the i-th round of Deliberate and Gaze for Question It q, the instruction Rind for the Deliberate module is:"
        },
        {
            "title": "Rind",
            "content": "i = pd(Iv, Ct), pd(cid:0)Iv, Ct, Bt pd(cid:0)Iv, Ct, Bt = 1 (cid:1), g, Bt d, Fin(cid:1), g, Bt = (cid:0)Qt (cid:1) and Bt for the Gaze module is: 1, , St 1 < < Fin = Fin, = (cid:0)St where Bt The instruction Ring Ring = pg(cid:0)Iv, Qt (cid:1), Fin, = Fin 1. (2) 1, , Qt i1 (cid:1). (1) Model Train Data MMDiag GND Testset GND Dataset Everyday Tabular Minigrid MSCOCO RefCOCO Grounding DINO [26] - LLaVA [25] Visual CoT [38] LCS558K+Mixed665K VisCoT DiagNote DiagNote DiagNote DiagNote COCO MMDiag MMDiag + COCO MMDiag + COCO + VisCoT 0.384 0.237 0.220 0.307 0.369 0.399 0.433 0.001 0.006 0. 0.008 0.466 0.487 0.281 0.209 0.142 0.160 0.199 1.0 0.988 0.910 0.715 0.365 0.321 0.662 0.259 0.624 0.662 0.469 0.414 0. 0.765 0.257 0.742 0.837 Average 0.356 0.233 0.213 0.388 0.471 0.648 0.625 Table 2. Comparison results with existing MLLMs on Grounding benchmarks (GND) to demonstrate the challenging characteristics of our dataset MMDiag. We use Intersection over Union (IoU) as the evaluation metric. We fine-tune the LLM on the prediction tokens, utilizing the auto-regressive training objective to optimize. We compute the probability of the target output Routx with length at i-th round by: ground-truth outputs per round. During inference, the Gaze module signals reasoning completion by outputting END for turn Tx  (Table 4)  , with the round number dynamically determined by DiagNote. Additional training details are provided in the Sections and C. (Routx Rinx ) = (cid:89) l= pθx (cid:0)rl Rinx , Routx ,<l where {d, g}. (cid:1) , (3) 5.2. Results on MMDiag 5.2.1. Visual Grounding θx is the trainable parameters of the Deliberate and Gaze modules respectively, with {d, g}. Rinx are the input tokens of i-th round of the Deliberate and Gaze interaction process. Routx ,<l are the answer tokens before the current prediction token rl. Our Deliberate and Gaze modules take LLaVA-1.5 [24] as base model. For the Gaze module, since grounding such salient areas as words and objects with detailed descriptions is quite challenging, we can first fine-tune it with an additional grounding dataset, and then fine-tune Deliberate and Gaze modules together. We combine the fine-tuning dataset from LLaVA [25] and the grounding datasets of MSCOCO [23] and RefCOCO [18, 43] with the augmentation grounding split of MMDiag to generate the grounding dataset; and we also combine the finetuning dataset from LLaVA with the training split of the MMDiag dataset to generate the entire training dataset. For data points in LLaVA, DiagNote does not add Deliberate prompts for the Deliberate module, thus instructing the Deliberate module to maintain the ability to output answers in general format. 5. Experiments 5.1. Implementation Details We use LLaVA-1.5-7B [24] as the foundation model for both Deliberate and Gaze modules, with CLIP-ViT-LargePatch14-336 [34] as vision tower. Training is conducted on 8 A800 GPUs with learning rate of 2e-5. Deliberate and Gaze are optimized separately via supervised learning with In this section, we focus primarily on how the MMDiag dataset benefits the grounding performance of MLLMs. Grounding is crucial capability for MLLMs, enabling them to focus on relevant salient regions and reveal the visible reasoning process in dialogues, rather than functioning as black box. We evaluate our DiagNote on several general grounding (GND) benchmarks [18, 23, 43], as well as our MMDiag GND benchmark. We use the average Intersection over Union (IoU) scores as the metric to assess GND performance, with results summarized in Table 2. When comparing DiagNotes performance on established GND benchmarks like MSCOCO to its performance on MMDiag, we observe significant decline on MMDiag, highlighting its increased difficulty relative to existing benchmarks. Existing models, such as Visual CoT, incorporate regions of interest for multimodal dialogue learning, but perform unsatisfactorily on GND datasets. For instance, Visual CoT scores -0.394 compared to Grounding DINO on the MSCOCO benchmark and performs worse than LLaVA. These results indicate lack of robustness in explicitly grounding relevant areas in images. In contrast, DiagNote, trained with limited standard GND annotations provided by MMDiag and MSCOCO, shows significant improvements in both MSCOCO and RefCOCO benchmarks and performs better across the three subset scenarios of MMDiag. Notably, the MSCOCO data here is used solely to enhance grounding capability, and we intentionally limit the scale of GND data to prevent dataset size from influencing our conclusion. As shown in Row 4, DiagNote performs the poorest when trained exclusively on the MSCOCO dataset, underscoring the necessity and 6 Model Gaze Train Data Everyday MMDiag Tabular Minigrid Average reasoning answer reasoning answer reasoning answer LLaVA [25] CogCoM [32] Visual CoT [38] DiagNote DiagNote DiagNote DiagNote LCS558K+Mixed665K - VisCoT MMDiag MMDiag MMDiag+COCO GT 2.55 3.05 4. 4.25 5.82 6.35 6.85 4.85 5.45 4.90 4.95 6.15 5.97 5.80 1.00 0.50 1.23 3.61 3.95 3.95 6.32 1.28 1.25 1. 4.20 4.05 4.30 7.76 2.29 0.53 1.09 4.95 5.10 5.75 7.37 0.42 0.96 2.50 4.27 4.15 4.93 9.15 2.21 2.20 2. 4.32 4.92 5.18 7.00 Table 3. Comparison of the evaluation score with baselines to validate the Gaze module, we use Gemini-1.5-Pro to evaluate the performance of the reasoning process and the final answer. The evaluation process is detailed in Section 3.3. Model Reasoning Answer Tabular T1 T2 T3 T4 T1 T3 T4 CogCoM Visual CoT LLaVA w/o Gaze with Gaze 0.55 1.50 2.34 4.01 3. 0.91 1.05 0.35 3.05 3.34 1.15 1.33 1.00 2.15 2.31 0.67 1.02 0.58 1.66 2. 1.75 1.86 1.42 3.47 3.25 0.73 1.24 0.50 2.03 2.65 0.85 1.03 0.97 1.65 2. 0.35 0.88 0.50 1.63 1.98 Table 4. The Gemimi-1.5-Pro evaluation of the reasoning process and the final answer, scaling to 0-10, at turns 1 to 4 under the tabular scenario, where denotes the -th turn in the dialogue. benefits of our MMDiag dataset. 5.2.2. Multi-Turn Reasoning As expected, We also evaluate our models multi-turn reasoning capabilities using the MMDiag benchmark. Beyond evaluating the correctness of the final answers, the evaluator also assesses the coherence and logic of the reasoning process within the Deliberate module. Detailed results are presented in Table 3. GT denotes scenarios where the Deliberate module receives ground-truth inputs for the reasoning step, serving as an upper bound. Except for the GT results, Gaze queries are generated by DiagNote, preventing information leakage. the GT setting significantly outperforms other settings, highlighting considerable room for improvement. To validate the effectiveness of our proposed module, we observe that the Gaze module enhances performance in specific reasoning tasks. For instance, in the everyday scenario, models utilizing the Gaze module achieve notably higher accuracy than those without it, demonstrating its ability to enhance focus and accuracy in reasoning. When there are multiple things, of the same kind, with different locations and attributes, in the image, chances are that the model cannot tell which object is exactly the one mentioned in the question. If the specific target is annotated on the image, the model can regain focus on it easily and avoid such cases that it fails to locate the right target when the reasoning process moves on. To further evaluate model performance, we compare our DiagNote with CogCoM [32] and Visual CoT [38], both of which can focus on specific regions and manage multimodal dialogues. Results show that DiagNote has significant advantages, especially in the tabular and Minigrid scenarios, reflecting the complexity of the MMDiag dataset and the strengths of DiagNotes architecture, featured with the Deliberate and Gaze modules. To deepen the analysis, we show comparison of results in tabular scenes under different numbers of dialogue turns in Table 4. DiagNote consistently outperforms the other models in the second, third, and fourth rounds under the tabular scenario, underscoring its superior capability in handling longcontext scenes with contextual and pronoun references. Meanwhile, the Gaze module shows more significant improvement, especially for increasingly long dialogues (e.g. T3 or T4), which further validates its effectiveness and benefits in long-context multimodal understanding. It is important to note that MMDiags tabular scenes in Table 3 include QA pairs of varying lengths (24), while Table 4 focuses only on dialogues with exactly 4 QA pairs. 5.3. Qualitative Results. In this section, we provide additional examples of the visual grounding and reasoning capabilities of DiagNote. More visualization results can be found in Sections and F. Visual Grounding. The Gaze module offers both grounding and OCR capabilities across diverse scenarios. As illustrated in Figure 4b, Grounding DINO [26] struggles in complex scenes where multiple objects of the same category exist with different attributes or relationships, therefore often failing to locate the target object precisely. In contrast, DiagNotes Gaze module effectively manages such situations, as shown in Figure 4a. Additionally, when faced with tasks requiring text recognition, the Gaze module exhibits more robust OCR capabilities, accurately Figure 3. Comparison for an example of the Minigrid scenario, one of the subsets in MMDiag. We give DiagNote (green) and GPT-4o (orange) the same environmental description and question. DiagNote focuses on the key regions and gives the correct reasoning process and the final answer. In contrast, GPT-4o fails to locate the object and thus gives the wrong answer. Examples for the MMDiag subsets of everyday scenarios and tabular scenes can be found in Section F. inputs. Failure cases show that when dialogues reference tiny key regions (under 0.2% of the image), Gaze often produces inaccurate bounding boxes, confusing the Deliberate module. The CLIP-ViT-Large-Patch14-336 encoder further limits resolution, contributing to errors. On standard multimodal benchmarks, DiagNote performs comparably or slightly lower, as it targets complex multiregion dialogues without in-domain training data. Ablation details are in Section E. 6. Conclusion In this paper, we focus on key challenging task scenario for MLLMsmulti-turn multimodal dialogue. To address it, we first introduce specially designed dataset, MMDiag, where accomplishing tasks requires properly integrating visual information across different regions of an image and connecting multimodal information across various QA pairs. This setting closely resembles natural conversations and poses significant challenges to current MLLMs. To solve this, we construct the MMDiag dataset across three distinct scenarioseveryday, tabular, and Minigridusing combination of rule-based methods and GPT-4o-mini to ensure robustness and diversity. Experimental results highlight the challenges posed by MMDiag. Therefore, we propose DiagNote, an MLLM inspired by human visual processing, composed of two primary modules: Gaze and Deliberate. The Deliberate module performs CoT reasoning step by step, with the assistance of the Gaze module, which provides annotations of salient regions to focus on. Experiments show that this design enhances both grounding and reasoning capabilities, effectively addressing MMDiag challenges. We hope our work contributes to advancing the development of more intelligent MLLMs. (a) DiagNote (b) Grounding DINO Figure 4. grounding comparison between Grounding DINO and DiagNotes Gaze module , with the Gaze query pink and white sign. In (a), the red bounding box represents the ground-truth answer, while the blue one indicates the output generated by the Gaze module in DiagNote. In (b), the red bounding boxes show the outputs produced by Grounding DINO. identifying and localizing specific keywords. Multi-Turn Reasoning. With the incorporation of the Gaze module, our model can also more effectively focus on fine-grained details distributed across the image, offering clear advantage in tasks that demand cohesive reasoning across both visual and linguistic information. As shown in Figure 3, comparison between our DiagNote and GPT-4o within simple Minigrid environment highlights this benefit. Despite detailed descriptions provided in the prompt, GPT-4o struggles with completing shortrange, single-subgoal task, underscoring the strengths of our dataset and methodology. 5.4. Ablation Study We observe counterintuitive performance trend when comparing DiagNote with and without the Gaze module. To analyze its impact, we fine-tune DiagNote and Visual CoT on MMDiag and confirm Gazes effectiveness. However, its gains are limited, likely due to low-resolution image"
        },
        {
            "title": "Limitations",
            "content": "Although MMDiag contains diverse data, our methods can be expected to generate even more scenarios and complex questions, resulting in even more challenging datasets for multi-turn multimodal dialogue. While qualitative results and case studies demonstrate the effectiveness of our approach, there remains considerable room for improvement. The potential performance drops with the introduction of Gaze module may stem from failures in queries involving extremely tiny objects. Fine-tuning Gaze to abstain from answering when uncertain or replacing the vision encoder backbone may enhance its robustness. Further exploration of training paradigms and model architecture could also potentially lead to enhanced performance."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ilge Akkaya, Florencia Leoni Aleman, Diogo Janko Altenschmidt, Sam Altman, Shyamal arXiv preprint Ahmad, Almeida, Anadkat, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. 1 [2] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen Technical Report. arXiv preprint arXiv:2309.16609, 2023. 1 [3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond. arXiv preprint arXiv:2308.12966, 2023. 2 [4] Tom B. Brown. Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165, 2020. [5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts. In CVPR, pages 35583568, 2021. 2 [6] Guangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, Borje F. Karlsson, Jie Fu, and Yemin Shi. AutoAgents: Framework for Automatic Agent Generation. In IJCAI, 2024. 3 [7] Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669, 2024. 4 [8] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. PaLI-X: On Scaling up Multilingual Vision and Language Model. arXiv preprint arXiv:2305.18565, 2023. 2 [9] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. BabyAI: First Steps Towards Grounded Language Learning With Human In the Loop. In ICLR, 2019. 4, [10] Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo Samuel Castro, and Jordan Terry. Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks. CoRR, abs/2306.13831, 2023. 1, 4, 12 [11] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, 2023. 2 [12] Cursor. The AI Code Editor. https://www.cursor. com/, 2024. 1 [13] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi Parikh, and Dhruv Batra. Visual Dialog. In CVPR, 2017. 3 [14] DeepL. Better writing with DeepL Write. https://www. deepl.com/en/write, 2024. 1 [15] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An Embodied Multimodal Language Model. arXiv preprint arXiv:2303.03378, 2023. 2 [16] Yicheng Feng, Yuxuan Wang, Jiazheng Liu, Sipeng Zheng, and Zongqing Lu. LLaMA-Rider: Spurring Large Language Models to Explore the Open World. In NAACL, pages 4705 4724, 2024. 2 [17] JaidedAI. EasyOCR. JaidedAI/EasyOCR, 2024. 4, 15 https : / / github . com / [18] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to Objects in Photographs of Natural Scenes. In EMNLP, pages 787798, 2014. 6 [19] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. IJCV, 123:3273, 2017. 3, 12 [20] Noah Lee, Jiwoo Hong, and James Thorne. Evaluating arXiv preprint the Consistency of LLM Evaluators. arXiv:2412.00543, 2024. 4 [21] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML, pages 1973019742. PMLR, 2023. 2 [22] Shengzhi Li and Nima Tajbakhsh. SciGraphQA: LargeScale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs. arXiv preprint arXiv:2308.03349, 2023. 2, 9 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In ECCV, pages 740755. Springer, 2014. 3, 6 [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. In Improved Baselines with Visual Instruction Tuning. CVPR, pages 2629626306, 2024. 5, 6, 19 [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. NeurIPS, 36, 2024. 1, 2, 4, 5, 6, 7, 13 [26] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. arXiv preprint arXiv:2303.05499, 2023. 5, 6, 7, 15 [27] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows. In ICCV, pages 1001210022, 2021. [28] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, et al. MMDU: Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs. arXiv preprint arXiv:2406.11833, 2024. 2, 3, 4, 5 [29] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In ACL, 2022. 4, 12 [30] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. MM1: Methods, Analysis & Insights from Multimodal LLM Pretraining. arXiv preprint arXiv:2403.09611, 2024. 2 [31] OpenAI. GPT-4o mini: advancing cost-efficient intelligence. https : / / openai . com / index / gpt - 4o - mini - advancing - cost - efficient - intelligence/, 2024. 2, 3, 4, 12 [32] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. CogCoM: Train Large Vision-Language Models Diving into arXiv preprint Details through Chain of Manipulations. arXiv:2402.04236, 2024. 2, 3, 5, 7, 19 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In ICML, pages 87488763. PMLR, 2021. 2 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In ICML, pages 87488763. PMLR, 2021. 5, 6 [35] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian SchritGemini 1.5: Unlocking multimodal twieser, et al. understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 1, 4, 12 [36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. NeurIPS, 35: 2527825294, 2022. 2 [37] Paul Hongsuck Seo, Andreas Lehrmann, Bohyung Han, and Leonid Sigal. Visual Reference Resolution using Attention Memory for Visual Dialog. NeurIPS, 30, 2017. 3 [38] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual CoT: Advancing Multi-Modal Language Models with Comprehensive Dataset and Benchmark for Chainof-Thought Reasoning. arXiv preprint arXiv:2403.16999, 2024. 2, 3, 5, 6, 7, 19 [39] Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason Image Chat: Engaging Grounded Conversations. Weston. arXiv preprint arXiv:1811.00945, 2018. [40] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards VQA Models That Can Read. In CVPR, pages 83178326, 2019. 2, 3 [41] Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. Large language models are inconsistent and biased evaluators. arXiv preprint arXiv:2405.01724, 2024. 4 [42] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards General Computer Control: Multimodal Agent for Red Dead Redemption II as Case Study. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. 1 [43] Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi Tang, Yuan Zhang, Jianbin Jiao, Qi Tian, and Qixiang Ye. ChatterBox: Multi-round Multimodal Referring and Grounding. arXiv preprint arXiv:2401.13307, 2024. 3, 4, 5, 6 [44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. 2 [45] Mark Towers, Ariel Kwiatkowski, Jordan Terry, John Balis, Gianluca De Cola, Tristan Deleu, Manuel Goulao, Andreas Kallinteris, Markus Krimmel, Arjun KG, et al. Gymnasium: Standard Interface for Reinforcement Learning Environments. arXiv preprint arXiv:2407.17032, 2024. [46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. NeurIPS, 30, 2017. 2 [47] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS, 35:2482424837, 2022. 3 10 [48] Xinrun Xu, Yuxin Wang, Chaoyi Xu, Ziluo Ding, Jiechuan Jiang, Zhiming Ding, and Borje F. Karlsson. Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges. arXiv preprint arXiv:2403.10249, 2024. 2 [49] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. Survey on Multimodal Large Language Models. arXiv preprint arXiv:2306.13549, 2023. [50] Sipeng Zheng, Jiazheng Liu, Yicheng Feng, and Zongqing Lu. Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds. In ICLR, 2024. 1, 2, 13 [51] Sipeng Zheng, Bohan Zhou, Yicheng Feng, Ye Wang, and Zongqing Lu. UniCode: Learning Unified Codebook for Multimodal Large Language Models. In ECCV, pages 426 443. Springer, 2025. 1 11 A. Dataset We use GPT-4o-mini [31] to generate our MMDiag dataset. Our dataset mainly consists of three parts: everyday scenes, tabular scenes, and Minigrid settings. We adopt different prompts for the generation of datasets under different scenes. A.1. Dataset Collection We design prompts for different scenarios, and the same devising ideas can be used in other scenarios for data collection. Everyday Scenes. For everyday scenes, we generate our dataset from the Visual Genome dataset [19]. Since the original dataset has human-annotated attributes and relationship data, we extract the subsets that represent the QA pairs and feed them to GPT-4o-mini to generate Figures 5 to 7 show several corresponding dialogues. example prompts. Figure 6. The second example prompt for generating data samples in everyday scenes. Figure 5. The first example prompt for generating data samples in everyday scenes. Tabular Scenes. For tabular scenes, we generate our dataset from the ChartQA dataset [29]. In general, we use different types of graphs to capture various visualization intuitions, providing corresponding chart examples in the prompts. Figure 8 illustrates the main structure of the prompt, while Figures 9 to 11 show examples for line, pie, and bar charts, respectively. Minigrid Settings. For Minigrid settings, we generate our dataset from the Minigrid database [10]. Since we observe that GPT-4o-mini struggles to solve the mission without ground-truth planning, we first use BabyAI [9] to collect the plan needed to complete the mission for each environment generated by the Minigrid database. We then combine the positions of all objects with the mission and plan, as shown Figure 7. The third example prompt for generating data samples in everyday scenes. in Figure 12, and feed them to GPT-4o-mini. The prompt structure is illustrated in Figure 13. A.2. Dataset Format Examples of the final MMDiag dataset are shown in Figures 14 to 16. Figures 14a, 15a and 16a display the original images from the source datasets and environments, while Figures 14b, 15b and 16b show the data format of MMDiag generated by GPT-4o-mini and standardized according to specific rules. A.3. Evaluation Since GPT-4o-mini contributes to generating our datasets, we use Gemini-1.5-Pro [35] for evaluation. There are answer multiple reasons for choosing it for this task: 12 Figure 8. The prompt structure to generate samples in tabular scenes. Figure 10. The question-answer (QA) and Chain-of-Thought (CoT) examples for pie charts. B. DiagNote Our DiagNote consists of two MLLMs, one for Deliberate, and one for Gaze. For each input question, DiagNote appends buffer information and queries to the respective prompts for Deliberate and Gaze. For images from Minigrid, description of the Minigrid environment, as shown in Figure 20, is included in both training and testing. The remaining components of the Deliberate prompt and Gaze prompt are consistent across all three scenes. Deliberate Prompt. For deliberating, DiagNote provides the dialogue context and Chain of Thought (CoT) history for the current question in the prompt, as shown in Figure 21. When the END token appears in the latest Query from the Deliberate module, signaling the end of the CoT process, DiagNote provides new prompt, as shown in Figure 22, to the Deliberate module for generating the final answer. Gaze Prompt. For gazing, DiagNote extracts the Query from the output of the Deliberate module and provides it to Figure 9. The question-answer (QA) and Chain-of-Thought (CoT) examples for line charts. formatting and the Chain of Thought (CoT) processes may be diverse, making simple similarity score insufficient for evaluation. Additionally, recent works [25, 50] commonly apply LLMs for judgment. We provide the MLLM with images, ground-truth answers, and generated responses, and ask it to score the accuracy of the generated answers across five categories. We notice that the MLLM provides more reasonable rankings when asked to explain the ad-hoc reason before their final score. As result, we include this reasoning step in the prompt, as shown in Figure 17. Figure 11. The question-answer (QA) and Chain-of-Thought (CoT) examples for bar charts. hyper-parameters value deepspeed base model conversation template vision tower zero3 LLaVA-1.5-7B Vicuna v1 CLIP-ViT-LargePatch14-336 modality projector type mlp2x gelu image aspect ratio training epochs training batch size learning rate weight decay warm-up ratio model max length data loader workers pad 1 16 2e-5 0 0.03 2048 4 Figure 12. The mission and plan input example of Minigrid settings. Figure 13. The prompt structure to generate data samples in Minigrid settings. bounding box of the query, is then saved in the Deliberate buffer to support the next turn of Deliberating. Table 5. The implementation details of the Deliberate module. C. Implementation the Gaze module along with the prompt shown in Figure 23. The output from the Gaze module, which includes the The detailed parameters of implementation are shown in Tables 5 and 6. 14 (a) the original image (a) the original image (b) the sample format Figure 14. One example of the original image and the generated sample from Visual Genome in JSON format. (b) the sample format D. Qualitative Comparison of Grounding Figures 18 and 19 show comparison of grounding ability between DiagNote and Grounding DINO [26]. As illustrated in Figure 18b, Grounding DINO struggles with grounding tasks involving Optical Character Recognition (OCR). In contrast, DiagNote leverages the generalization capability of LLMs, enabling it to effectively locate the Figure 19b target words, as shown in Figure 18a. Figure 15. One example of the original image and the generated data point from ChartQA in JSON format. The bounding boxes of the queries are generated using EasyOCR [17] and thus are not shown in the example. illustrates that Grounding DINO fails to handle objects with attributes. Although the grey key has marginally higher confidence, accurately locating the grey key in the 15 (a) the original image (b) the sample format Figure 16. One example of the original image and the generated sample from Minigrid in JSON format. Figure 17. The evaluation prompt structure given to Gemini1.5-Pro. The content in [] is added when the CoT process is evaluated. hyper-parameters value deepspeed base model conversation template vision tower zero3 LLaVA-1.5-7B Vicuna v1 CLIP-ViT-LargePatch14-336 modality projector type layer selected for mlp2x gelu -2 fine-tuning vision tower image aspect ratio training epochs training batch size learning rate weight decay warm-up ratio model max length data loader workers fine-tune vision tower pad 1 32 2e-5 0 0.03 2048 4 True/False Table 6. The implementation details of the Gaze module. image confuses Grounding DINO. In contrast, DiagNote accurately identifies the grey key in Figure 19a, which aids the subsequent actions of the Deliberate module. E. Ablation Study We observe counterintuitive performance trend in Table 3 in the main paper: Gaze provides only limited performance gains and, in some cases, even reduces performance, particularly in tabular and Minigrid scenarios. As shown in Figure 24, Gaze incorrectly identifies the bounding box for critical but tiny piece of informationthe year 2019misleading Deliberate to focus on the wrong color 16 (a) DiagNote (b) Grounding DINO Figure 18. The grounding comparison between Grounding DINO and the Gaze module of DiagNote in Tabular Scene. The grounding query is Cyprus. The red bounding box in (a) is the ground-truth answer, while the blue one is the bounding box generated by our Gaze module. The red bounding box in (b) is the output of Grounding DINO. (a) DiagNote (b) Grounding DINO Figure 19. The grounding comparison between Grounding DINO and the Gaze module of DiagNote in Minigrid Scene. The grounding query is grey key. The blue bounding box in (a) is generated by the Gaze module of DiagNote, which overlaps the ground-truth red bounding box. Meanwhile, the red bounding box in (b) is the output of Grounding DINO. 17 Figure 20. The description of Minigrid Scene added to the prompts. Figure 21. The prompt structure of the Deliberate module when the last Query output of the Deliberate module is not END. Figure 22. The prompt structure of the Deliberate module when the last Query output of the Deliberate module is END. Figure 23. The prompt structure of the Gaze module. bar. This issue accounts for most failure cases. To further analyze this, we evaluate the proportion of tiny key regions across different scenarios in MMDiag In tabular and Minigrid scenes, nearly all key  (Table 9)  . regions occupy less than 3% of the total image area, making them particularly challenging for Gaze to detect accurately. To mitigate this, we curate an alternative test dataset for tabular scenes, excluding questions that require attention to extremely small regions. We then fine-tune Visual CoT and DiagNote with MMDiag and evaluate them on this revised tabular split. As shown in Table 7, Gazes impact becomes more pronounced. Table 8 demonstrates that DiagNote performs comparably or slightly lower on standard multimodal benchmarks, as it targets complex multi-region dialogues without in-domain training data. Figure 24. The second example of comparison between different MLLMs under everyday scenes. Model Fine-tuning Data Gaze T2 T3 T4 Visual CoT-13B DiagNote-14B DiagNote-14B MMDiag MMDiag MMDiag - 2.00 3.15 4.20 1.43 2.35 3.10 0.40 1.78 2.55 0.95 1.23 1.95 Table 7. Tabular scenes results of MLLMs fine-tuned on MMDiag, using the same evaluation metrics as the previous evaluation. Benchmark MMBench MM-Vet RefCOCO+ RefCOCOg DiagNote-14B 63.7 28.5 0.834 0. Table 8. DiagNote performance on general datasets. Scenario 0.2% 1% 3% 5% 10% Everyday Tabular Minigrid 7.57% 27.62% 47.99% 57.49% 69.91% 87.17% 99.24% 99.80% 99.92% 100% 6.98% 66.61% 96.99% 99.41% 100% Table 9. MMDiag tiny key regions percentage. F. Qualitative Comparison of Multi-Turn Multimodal Dialogue We present several cases comparing models in everyday scenarios and tabular scenes. Figures 25 and 26 show In Figure 25, examples from unseen everyday scenarios. 18 Figure 26. The second example of comparison between different MLLMs under everyday scenes. Figure 25. The first example of comparison between different MLLMs under everyday scenes. CogCoM [32] completely fails to answer the two-turn questions correctly. Despite the assistance of the counting expert, CogCoM is unable to answer the first counting Although LLaVA-1.5-13B [24] and Visual question. CoT [38] can answer the first questions accurately, both encounter hallucinations when responding to the second question, mistakenly identifying white plates as cups and In contrast, our DiagNote performs bowls, respectively. well on both questions, demonstrating the effectiveness of the Gaze module in ensuring DiagNote stays grounded in In Figure 26, CogCoM fails to provide visual details. clear answer to the first question, instead offering confusing single word jean. Again, LLaVA-1.5-13B and Visual CoT answer the first question correctly, but imagine the man was holding frisbee. Both CogCoM and DiagNote understand the context, with DiagNote accurately describing the can based on the visual details. In contrast, CogCoM mistakenly assumes it is can of beer, which may not be the case. Figure 27. One example of comparison between different MLLMs under tabular scenes. Figure 27 presents examples of unseen tabular scenes. All models answer the first question correctly. However, Visual CoT provides completely incorrect answer to the second question, while CogCoM introduces an unfounded 50%. LLaVA-1.5-13B correctly identifies the visual detail 34%, but overlooks the keyword change in the question, which requires calculation between two percentages. Only DiagNote answers the question precisely. The final 19 question requires the models to understand the entire pie chart. The model should compare the sum of two parts on the right side of the pie chart with the left part to obtain the final answer yes. Visual CoT fails to provide this correct answer, and LLaVA-1.5-13B misinterprets the unaffiliated percentage and derives an incorrect affiliated percentage. Both CogCoM and DiagNote reach the right conclusion. Overall, DiagNote performs well on all questions, demonstrating its ability to focus on both visual and language details and to comprehend the full picture the chart conveys. This strong ability can be attributed to the Gaze and Deliberate structure, which enables it to zoom in on specific details while integrating multimodal information for holistic understanding."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "School of Computer Science, Peking University"
    ]
}