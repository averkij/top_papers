{
    "paper_title": "GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces",
    "authors": [
        "Melis Ocal",
        "Xiaoyan Xing",
        "Yue Li",
        "Ngo Anh Vien",
        "Sezer Karaoglu",
        "Theo Gevers"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typically distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, a pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. A latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 3 8 6 3 0 . 2 1 5 2 : r GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces Melis Ocal1* Xiaoyan Xing1 Yue Li1 Ngo Anh Vien2 Sezer Karaoglu1 Theo Gevers1 1University of Amsterdam 2Bosch Center for AI https://gaussianblender.github.io/ Figure 1. Given 3D Gaussian splat asset and an edit prompt, GaussianBlender - diffusion-based feed-forward style editor - generates modified assets instantly, fully eliminating per-asset test-time optimization. GaussianBlender delivers high-fidelity, geometry-preserving, multi-view consistent stylizations and supports interactive appearance editing - unlocking practical, democratized 3D stylization at scale."
        },
        {
            "title": "Abstract",
            "content": "3D stylization is central to game development, virtual reality, and digital arts, where the demand for diverse assets calls for scalable methods that support fast, high-fidelity manipulation. Existing text-to-3D stylization methods typi- *Correspondence to b.m.ocal@uva.nl cally distill from 2D image editors, requiring time-intensive per-asset optimization and exhibiting multi-view inconsistency due to the limitations of current text-to-image models, which makes them impractical for large-scale production. In this paper, we introduce GaussianBlender, pioneering feed-forward framework for text-driven 3D stylization that performs edits instantly at inference. Our method 1 learns structured, disentangled latent spaces with controlled information sharing for geometry and appearance from spatially-grouped 3D Gaussians. latent diffusion model then applies text-conditioned edits on these learned representations. Comprehensive evaluations show that GaussianBlender not only delivers instant, high-fidelity, geometry-preserving, multi-view consistent stylization, but also surpasses methods that require per-instance test-time optimization - unlocking practical, democratized 3D stylization at scale. 1. Introduction 3D style editing is core requirement in gaming, AR/VR, and digital arts, where immersive experiences rely on large libraries of high-quality, customizable assets. Workflows increasingly require instant editing: 1) offline to rapidly explore diverse styles during content creation and 2) online for interactive, user-driven customization. This dual need calls for scalable, high-fidelity, and multi-view-consistent stylization. Conventionally, 3D stylization has been laborious, expert-driven, and time-consuming process, relying on professional tools and limiting scalability. With recent breakthroughs in text-to-image diffusion [24, 53], 2D editing has advanced substantially [73], laying groundwork for 3D. IN2N [20] takes key step by introducing an iterative dataset update paradigm where pretrained image editor edits rendered views while the 3D representation is progressively re-optimized under the updated supervision. Another line of work directly optimizes the 3D representations via score-distillation [21, 36, 84]. Despite progress, both iterative dataset update and score distillation approaches remain bottlenecked by slow per-asset optimization, making them unsuitable for large-scale production and interactive use. Moreover, test-time optimization often introduces inconsistencies due to the lack of strict global prior over appearance or geometry. Recent approaches [6, 29] offer zero-shot or feed-forward alternatives that reduce editing time, but they typically operate on shared geometryappearance representation without explicit constraints, which limits editing control and can lead to blurry results or unintended changes to the objects geometric identity. To address these shortcomings - slow per-scene optimization and representational constraints - we introduce GaussianBlender, diffusion-based feed-forward 3D Gaussian splat (3DGS) style editor that edits in the latent space of 3D Gaussian VAE. Once trained, GaussianBlender generates modified, high-quality, 3D-consistent assets from text prompts in single feed-forward pass instantly, fully eliminating test-time optimization. This design raises three challenges: 1) The unstructured nature of 3DGS hinders recovery of spatial structure when learning latent represen2 tations. Our approach tackles this by grouping Gaussians by spatial proximity and encoding them into latent spaces that retain global 3D structure for effective diffusion prior learning. 2) Geometry and appearance parameters of Gaussians are unevenly distributed, creating mismatched learning dynamics that makes joint optimization non-trivial. 3) Effective 3D style editing should primarily decouple appearance from geometry to preserve shape while modifying materials, textures, and colors, yet still benefit from controlled information exchange so that appearance can leverage geometric context. To address this, GaussianBlender adopts dual-branch architecture with disentangled latent spaces for appearance and geometry, and lightweight cross-branch feature-sharing module that exchanges only necessary signals, simplifying Gaussian-parameter learning, and enabling targeted, controllable edits. We divide the learning process of GaussianBlender into three stages: group-structured disentangled latent space learning, latent diffusion pre-training and latent editing. 1) Given 3DGS asset, GaussianBlender groups Gaussians by spatial proximity and encodes them with dual-branch 3D VAE into disentangled geometry and appearance latents, using cross-branch feature-sharing module for controlled information exchange. 2) latent diffusion model is then pre-trained to denoise the appearance latent, conditioned on the geometry latent and the text prompt, under reconstruction objective. 3) Once trained, the pre-trained denoiser is adapted to perform latent editing driven by edit prompts, while preserving shape via the geometry latent. Supervision comes from distillation of pretrained 2D editor. The Gaussian group-structured latent representation handles the discrete nature of Gaussian splats and ensures 3D consistency, while disentanglement enables controllable edits. Extensive quantitative and qualitative evaluations show that GaussianBlender delivers instant, highfidelity, geometry-preserving, multi-viewconsistent stylizations, outperforming methods that require per-instance test-time optimization. In brief, our contributions are: diffusion-based, feed-forward 3D Gaussian splat editor that delivers high-quality stylization instantly at inference, fully eliminates per-asset test-time optimization, generalizes to out-of-domain inputs, supports practical interactive use and large-scale production. An effective strategy for learning 3D diffusion priors in structured, disentangled latent spaces - decoupling appearance from geometry with controlled information sharing - to enable controllable edits and simplify Gaussian parameter learning. evaluations show that GaussianBlender achieves high-fidelity 3D stylizations, with stronger 3D consistency and better geometry preservation than state-of-the-art baselines. Extensive 2. Related Work 2.1. Image Editing with Diffusion Models Recent advances in image generation models [24, 53] have paved the way for diffusion-based 2D image editing with direct guidance. Personalization methods [19, 32, 54] preserve identity while controlling appearance/pose, dragbased [47, 57] and layout-guided models [3, 16, 35] enforce spatial control. GLIDE [48] guides image generation using CLIP features, and ControlNet [79] incorporates diverse conditioning signals such as pose or normal maps. More relevant to our work, several studies [1, 23, 30, 43, 46, 50, 63] have reframed image editing as an image-to-image translation task guided by text or image conditions. Building on Stable Diffusion [53], InstructPix2Pix and variants [2, 45, 78, 82] improve edit quality and localization via joint imagetext conditioning. 2.2. 3D Generation Inspired by advances in text-to-image diffusion, DreamFusion [51] first leveraged pretrained diffusion via SDS to optimize 3D content. Follow-up work improved fidelity [7, 34, 38] and speed by pairing 3DGS with 2D priors [31, 59, 75]. Despite improvements, SDS approaches still suffer from slow per-asset optimization and prone to geometric ambiguity. Recent approaches fine-tune 2D diffusion for multi-view image generation [3941, 56, 61], followed by 3D reconstruction. Two-stage pipelines first generate consistent views and then reconstruct 3D geometry [33, 60], and others diffuse over encoded multi-view splat images [37]. However, these methods depend on multiview inputs and lack unified 3D latent representation, limiting geometric consistency and precision. Recent approaches like GVGEN [22] adapt diffusion to 3D Gaussians via volumetric structures, but introduce costly preprocessing and training. To improve scalability, several methods operate in 3D latent spaces [15, 25, 72, 74, 77, 80]. However, operating directly on 3D Gaussians remains underexplored due to their unstructured, discrete nature. GaussianBlender addresses this by encoding spatially-grouped Gaussians into latent spaces that retain global 3D structure for effective diffusion prior learning. This design handles discreteness, simplifies Gaussian-parameter learning, and yields 3D-consistent stylization. 2.3. Text-driven 3D Editing Recent 3D editing methods start from existing representations (e.g., NeRF or 3DGS) and typically distill from 2D image editors or vision-language models. CLIP-based approaches [8, 44, 52, 66, 67, 83] align appearance or geometry with text or reference images. Built on an image translation model [2], IN2N [20] pioneered the iterative dataset update paradigm, employing pre-trained image diffusion model to edit rendered views of 3D model and iteratively optimize the underlying representation using the updated images. This line of work has since been extended to 3DGS models [9, 64] and point clouds [73]. Subsequent research [4, 14, 68] has enhanced multi-view consistency through attention-based latent code alignment [69], feature injection with epipolar constraints [5], depth-guided feature sharing [71]. Orthogonally, SDS-based methods directly optimize 3D [21, 28, 36, 49, 55, 84]. Despite progress, iterative dataset update and score distillation approaches continue to suffer from slow per-asset optimization, preventing their use in interactive applications and large-scale production. ProteusNeRF [65] and Edit-DiffNeRF [76] accelerate optimization, while FreeEditor [29] offers zero-shot alternative. More related to our work, Shap-Editor [6] learns feed-forward network eliminating test-time optimization. However, Shap-Editor edits shared appearancegeometry representation, limiting control and often causing blur or In contrast, GaussianBlender pergeometric distortion. forms instant feed-forward edits and explicitly disentangles appearance and geometry with controlled cross-branch information sharing, enabling geometry-preserving, multiview-consistent style edits. 3. Method We introduce GaussianBlender, diffusion-based feedforward 3D Gaussian splat style editor that learns 3D priors directly from spatially-grouped Gaussians encoded into disentangled latent spaces for appearance and geometry via 3D Gaussian VAE. These priors are then used for learning an editing function g(). The training pipeline is divided into three stages: 1. Latent space learning: Dual-encoders Eg θ and Ecγ map the spatially-grouped input Gaussians G, into latents zs and zs c. Dual-decoders Dg ϕ and Dcψ, together with cross-branch feature sharing module fϑ, then reconstruct the Gaussian parameters (Sec. 3.1). 2. Latent diffusion pre-training: latent diffusion model dϱ learns denoising the noisy latent zs conditioned on text embedding C, with the objective of reconstructing the input Gaussian representation (Sec. 3.2). 3. Latent editing: Once 3D priors are captured, dϱ is further trained to learn an editing function g() that maps latent zs conditioned on embedding Ce. to modified latent ze Supervision is provided by distilling from pre-trained 2D image editor (Sec. 3.3). The diffusion-related stages are guided by the geometry latent zs through cross-branch feature sharing module fϑ to provide structural context. At inference, GaussianBlender generates modified high-quality, 3D-consistent assets from text prompts in single feed-forward pass, without any testtime optimization. The scheme of the framework is illustrated in Fig. 2. 3 Figure 2. Overview of our method. (1) Latent space learning: Given input Gaussians, our method first groups them based on spatial proximity and encodes into group-structured disentangled latent spaces, with controlled cross-branch feature sharing. (2) Latent diffusion pre-training: denoiser dϱ then learns to denoise the noisy appearance latent zs conditioned on embedding C. (3) Latent editing: Once 3D priors are captured, dϱ is further trained to learn an editing function g() that maps latent zs based on embedding Ce, guided by the geometry latent zs g. At inference, GaussianBlender generates modified high-quality, 3D-consistent assets from text prompts in single feed-forward pass instantly, fully eliminating test-time optimization. Trainable models at each stage are denoted. to modified latent ze 3.1. Stage 1: Gaussian VAE for Group-structured"
        },
        {
            "title": "Disentangled Latent Space Learning",
            "content": "Learning latent representations of 3D Gaussians introduces two key challenges: (1) The unstructured nature of 3DGS data hinders spatial structure recovery, and (2) jointly learning Gaussian parameters is difficult due to their uneven distribution across geometry and appearance, as pointed out by [42]. GaussianBlender addresses the former by grouping input Gaussians by spatial proximity and encoding them into latent spaces that retain global 3D structure for effective diffusion prior learning. To tackle the second, GaussianBlender promotes controlled information sharing rather than complete separation so that appearance can benefit from geometric context. It adopts dual-branch architecture to learn disentangled latents for appearance and geometry, facilitating Gaussian-parameter learning and enabling geometry-preserving edits. Controlled sharing is achieved via lightweight cross-branch feature-sharing module that passes only the necessary signals between branches. Encoding. More formally, the input is set of Gaussian splats = {Si}N i=1, where after stacking the attributes we have = [µ, r, s, c, o] RN 14, with each splat parameterized by its centroid µ RN 3, scale RN 3, quaternion vector RN 4, opacity RN 1 and color RN 3, representing 3D asset. First is randomly downsampled to fix number of splats, yielding ˆS ˆN 14. Gaussians are then grouped by their centroids, following the encoding strategy of GaussianMAE [42]. Group centers µgroup = FPS(µ) Rp3 are computed via farthest point sampling, where is the number of 4 groups. For each µgroup, the nearest splats are obtained as = KNN( ˆS, µgroup) Rpk14. The resulting Gaussian groups are then partitioned into Gg Rpk10 and Gc Rpk4, separating geometry (i.e., mean, quaternion vector, scale) and appearance (i.e., color, opacity) parameters. Gg and Gc are fed into tokenizers to obtain group tokens: Tg = Tokenizerg(Gg), Tc = Tokenizerc(Gc) (1) Both Tg and Tc lie in R10241024. Finally, the tokens are processed by the geometry and appearance encoders Eg θ and Ecγ. We adopt the transformer-based encoder architecture introduced in [42]: g, zs g, zs zs zs = Ecγ(Tc), zs = Eg θ(Tg), R10241024, (2) are Gaussian group-structured latents reprewhere zs senting the geometry and appearance of 3D input. Decoding. To reconstruct parameters from zs c, we employ dual-decoders with lightweight cross-branch feature sharing module fϑ. The geometry decoder Dg ϕ predicts geometry tokens for centroids, scales and rotation quaternions, while the appearance decoder Dcψ produces appearance tokens for color and opacity. The transformer-based decoders are conditioned on positional embeddings φ(µgroup) derived from group centers to facilitate 3D structure recovery: and zs = Dg ϕ(zs g, φ(µgroup)), = Dcψ(zs c, φ(µgroup)) (3) These tokens are then passed to the cross-branch feature sharing module fϑ, which uses bidirectional cross-attention to produce fused tokens (T , ) = fϑ(T ). The fused tokens are injected via residual update: g, where denotes the time steps, and ϵ is noise level sampled from (0, I). Classifier-free guidance is also employed. The diffusion loss is complemented by the parameter reconstruction and render losses. Tg = + αT , Tc = + αT (4) 3.3. Stage 3: 3D Editing in Latent Space Finally, projection module Φ predicts the Gaussian attributes for the reconstructed groups G: µ, r, s, c, = Φ( Tg, Tc) (5) Training. To capture 3D priors, the Gaussian VAE is supervised by an L1 parameter reconstruction loss between input groups and reconstructions G, denoted Lparam. However, because the mapping from image supervision to 3D Gaussian parameters is many-to-one, an L1 loss alone can be overly restrictive. Incorporating render loss Lrender relaxes this constraint and makes it possible to capture broader range of valid solutions. The L1 render loss is computed over random views sampled from the training images used to optimize the input Gaussians and is complemented by an LPIPS loss [81]. Lrender = Lrgb + τ LLP IP (6) Controlled sharing without entanglement is further enforced via latent similarity loss LLS. Latents are averaged over spatial positions and normalized. Latent similarity loss is computed as the mean cosine similarity across the batch B: LLS = 1 (cid:88) i=1 (cid:10)zs g(i), zs c(i)(cid:11) (7) The VAE is trained end-to-end with the following objective: LGV AE = λ1Lparam+λ2Lrender+λ3LLS +λ4LKL, (8) where LKL = DKL(qg p) + DKL(qc p) is the sum of KL divergence terms from the geometry and appearance posteriors to the Gaussian prior p(z) = (0, I), scaled by β-annealing schedule to stabilize latent-space learning. 3.2. Stage 2: Latent Diffusion Pre-training Our Gaussian VAE maps any 3DGS into group-structured latent preserving global 3D structure. To train diffusion model in this space, we employ the tranformer-based text-conditioned denoiser of Shap-E [27], and its pretrained weights for initialization as Shap-E is trained on 3D assets encoded into latent space, and already carries rich priors. Given Gaussian group-structured appearance latent zs c, encoded by Ecγ and text condition (obtained by encoding text via CLIP [52]), the forward process gradually adds Gaussian noise, corrupting zs over time steps. The denoiser dϱ is then tasked with recovering the latent distribution from zs into zs T. The training objective is: c)2 , t, C, t) zs c,t,ϵN (0,I) dϱ(zs Ezs (9) 5 cCe, zs = g(zs to an edited latent ze Once the latent denoiser dϱ is trained with the reconstruction objective, it is further adapted to learn an editing function ze g) that maps the source appearance latent zs c, by distilling from the pretrained image editor InstructPix2Pix [2]. To this end, random views are sampled from the images used to optimize the input Gaussians. Following SIGNeRF [13], the views are arranged into grid, as grid-based editing yields more 3D-consistent results than editing individual images. Editing grid of images is then formulated as: i+1 Eθ(Vgrid Vgrid , Ce), , t; Vgrid (10) where Ce is the conditioning text embedding, is the noise level, Vgrid is the grid of source images, Vgrid is the noised 0 grid, and Vgrid i+1 is the edited grid. Eθ denotes the sampling process. Similar to the computation of Lrgb, the edit loss Ledit is computed over edited images and their corresponding rendered counterparts. This stage is trained using only Ledit. 4. Experiments Dataset. We use the TRELLIS-500K [72] split of Objaverse [12], filtered by aesthetic scores. From this split, we sample 140K 3D assets, render 72 views per asset, and generate 3DGS with LightGaussian [17]. Assets with PSNR below 30 dB are discarded from the dataset. The final filtered dataset contains approximately 123K 3D assets, of which 3K are randomly held out for testing purposes, and the remaining 120K are used for training. For latent diffusion pre-training, we use the splits provided text prompts, for the editing stage, we curate set of edit prompts. Evaluation Setup and Baselines. Our approach is evaluated against five state-of-the-art text-conditioned 3D editing methods. IN2N [20] represents assets with NeRFs, while IGS2GS [64], GaussianEditor [9] and GaussCtrl [69] are 3DGS-based. All optimize the 3D representation by distilling from 2D image editor. Shap-Editor [6] is feedforward editing approach and, like ours, requires no testtime optimization. We evaluate on 30 test assets with 10 edit instructions against test-time optimization baselines, each instruction is applied to 3 assets. Since the training code for Shap-Editor is not publicly available, we evaluate it separately using its four released pre-trained models (one per edit instruction), applying each to 500 test assets (2000 total) sampled from our test split. Metrics. Following prior work [2, 6, 9, 69], we report the CLIP [52] similarity (CLIPsim) and CLIP directional simFigure 3. Qualitative comparison with state-of-the-art methods. Unlike baselines that yield over-saturated, dramatic edits that alter 3D structure (e.g., blurred boundaries; Make it pop-art neon duotone), minimal edits that are barely perceptible (Make its colors look like rainbow), or severe geometric distortions, GaussianBlender delivers high-fidelity, text-aligned 3D stylizations with strong geometry preservation instantly, with single feed-forward pass. ilarity (CLIPsim) to evaluate alignment between the edit prompt and the resulting 3D edit. Following Shap-Editor [6], we also report Structure Distance (Structure Dist.) [62] metric to quantify structural consistency between input and edited assets. We additionally conduct user study to capture perceptual preferences. GaussianBlender is impleImplementation Details. mented in PyTorch [58], and trained on 4 NVIDIA H100 GPUs with AdamW optimizer. The VAE uses LR 1e3, weight decay 5e2, CosLR scheduler with 10-epoch 6 warm-up, for 200 epochs. The diffusion model uses LR 1e5, the same weight decay and cosine annealing (200 epochs pre-training, 15 epochs editing). 4.1. Experimental Results Qualitative Comparison. Visual comparison of GaussianBlender with state-of-the-art 3D editing approaches is provided in Fig. 3. GaussianBlender produces high-fidelity 3D stylizations that surpass baseline methods in terms of textalignment and geometry preservation. GaussianEditor often yields noisy, sketch-like artifacts and alters geometry (e.g., blurred boundaries, structural loss; Make it Starry Night Van Gogh style). IN2N, IGS2GS, and GaussCtrl tend to produce either over-saturated, dramatic edits that change also the 3D structure (Make it pop-art neon duotone) or minimal edits that are barely perceptible (Make its colors look like rainbow). This is because optimization-based methods are sensitive to the 2D editors guidance scale and typically require careful per-asset tuning to avoid these extremes. In contrast, GaussianBlender is trained with guidance scales sampled from reasonable range across many samples, yielding more realistic, geometry-preserving edits without tuning. Shap-Editor, by contrast, yields textaligned 3D stylizations with uniform edit strength across inputs, but often fails to preserve geometry, causing extreme smoothness, and structural distortion. This stems from editing shared appearancegeometry representation, which limits control and entangles shape with style. Quantitative Comparison. To evaluate text-alignment and structural consistency, we compute the average CLIPsim, CLIPdir and Structure Dist. metric. As shown in Tab. 1, GaussianBlender achieves the highest overall textalignment and structure consistency. While IGS2GS [64] and GaussianEditor [9] report CLIP scores close to ours, these are inflated on scenes with dramatic, over-saturated color changes that exhibit severe multi-view inconsistency and geometric distortion. IN2N [20] achieves closer Structure Dist. by producing minimal edits, as reflected in its CLIP scores. Shap-Editor [6] achieves the lowest structural consistency as it introduces severe distortions. Cross-data Generalization. To demonstrate the generalization ability of GaussianBlender to unseen objects, we present additional qualitative results in Fig. 4 on the OmniObject3D [70] dataset, whose 3DGS representations are similarly generated using LightGaussian [17]. Although assets in OmniObject3D exhibit significantly richer textures than those in Objaverse, on which our model is trained, placing them outside our models typical training distribution, our editor still generalizes well to this dataset. User Study. We conducted user study with 50 participants to capture perceptual preferences. For each task, participants viewed outputs from GaussianBlender and the baselines alongside the edit prompt and the input 3D asFigure 4. Cross-dataset generalization on OmniObject3D [70]. Our framework demonstrates strong style editing performance on out-of-distribution 3D assets. set. Shap-Editor [6] was excluded because public checkpoints do not cover studys edit range. For each criterion, participants selected the output that (i) best aligns with the text prompt, (ii) best preserves the input assets 3D structure, and (iii) exhibits the highest visual quality. We also asked, conditioned on their preferred outputs, how long they would be willing to wait for single asset. As summarized in Tab. 2, GaussianBlender was consistently preferred across all three criteria. In terms of latency, 69.6% favored real-time (< 1 s); the remaining choices were 210 mins (30.4%), 1020 mins (13.0%), and 2060 mins (0%), indicating clear preference for fast editing approaches. Inference Time. Inference time denotes end-to-end latency from input receipt to final output. As shown in Tab. 1, GaussianBlender produces edits in 0.26 second while achieving higher quality than baselines, enabling interactive use and large-scale production. 4.2. Ablation Study Effect of Disentanglement on 3DGS Parameter Reconstruction. We conduct an ablation study by isolating the Gaussian VAE and evaluating its standalone 3DGS parameter reconstruction performance. We compare singlebranch variant (w/o disent.), which encodes appearance and geometry into shared latent, against our dual-branch model with disentangled latents (w/ disent.) Both are trained only with the Stage 1 reconstruction objective. Tab. 3 reports PSNR between ground-truth renders and input/reconstructed 3DGS renders, on 3K test samples. All assets are represented using 16384 Gaussians. Input 3DGS yields lower PSNR because of subsampling the original 50K Gaussians (sparser representations), and LightGaussian [17] artifacts (Fig. 5a). However, trained with multi-view render loss, both variants of Gaussian VAE recover ground-truth renders, mitigating spar7 Table 1. Quantitative comparison with state-of-the-art methods. Our method achieves the best promptedit alignment and structural consistency while delivering edits instantly. Method Optimization-free Evaluation Metric Inference Time IN2N [20] IGS2GS [64] GaussCtrl [69] GausssianEditor [9] Shap-Editor [6] Ours CLIPsim CLIPdir 0.211 0.245 0.231 0.246 0.238 0.251 0.087 0.193 0.159 0.189 0.128 0.210 Structure Dist. 0.0085 0.0356 0.0229 0.0412 0.0457 0. 35 mins 6 mins 8 mins 7 mins 1 sec 0.26 sec Table 2. User study results. GaussianBlender is consistently preferred over the baselines across the three criteria. Method IN2N IGS2GS GaussCtrl GaussianEditor Ours Text Alignment Structural Consistency Visual Quality 16.30% 14.67% 20.10% 15.21% 33.69% 20.24% 17.48% 15.84% 4.37% 42.05% 21.68% 20.21% 17.48% 6.01% 34.60% sity and artifacts. The disentangled Gaussian VAE produces sharper reconstructions (Fig. 5b), higher geometric fidelity (Fig. 5a), and better appearance recovery than the non-disentangled variant, which tends to wash out colors (Fig. 5b). Figure 5. Visual assessment of Gaussian VAE reconstructions. The proposed dual-branch Gaussian VAE yields sharper reconstructions with better geometric fidelity and color accuracy. Table 4. Assessment of framework components. We report CLIPsim and CLIPdir to compare different variants. Table 3. Effect of disentanglement on 3DGS parameter reconstruction. GT: ground-truth renders; Input 3DGS: constructed with LightGaussian [17]; w/o disent.: single-branch variant; w/ disent.: dual-branch Gaussian VAE. Input 3DGS w/o disent. w/ disent. (Ours) PSNR 32.7136 33. 34.3299 Assessment of Framework Components. We compare our full model against two variants (1) w/o LLS, and (2) w/o feat. sharing, which decodes Gaussian parameters directly from the two disentangled latents, omitting the cross-branch feature-sharing module. The quantitative results in Tab. 4 confirm the effectiveness of LLS and the feature-sharing module. 4.3. Application Scene Editing. Given 3DGS scene and an edit prompt, we locate the target object using dataset-provided 3D annotated bounding boxes (3D-FRONT [18]), crop its 3DGS, and stylize it with GaussianBlender. Results (Fig. 1, Fig. 6) show our method can be adopted for interactive object-level scene editing. CLIPsim CLIPdir (1) w/o LLS (2) w/o feat. sharing (3) Ours (full) 0.223 0.218 0.251 0.171 0.165 0.210 Figure 6. GaussianBlender can be adopted for interactive objectlevel scene editing. Style Transfer via Appearance-latent Exchange. In Fig. 7, we transfer appearance latent codes from source assets to target assets. These results demonstrate that our disentangled representation enables controlled, geometrypreserving appearance transfer across assets while - by de8 sign - restricting cross-branch communication to necessary signals. We perform direct latent swapping, without post-processing or region-specific control, yet obtain highquality results. Figure 7. Style transfer via appearance latent exchange. Transferring the appearance code from source to target asset modifies appearance while preserving the geometric structure. 4.4. Limitations Similar to prior approaches that distill from 2D editors, our method inherits the limitations of the underlying pre-trained components. Currently, we support limited set of edit prompts (see the supplementary for multi-prompt results), as training an editor that generalizes to broader range of prompts is significantly GPU-intensive. 5. Conclusion We introduce diffusion-based, feed-forward 3D Gaussiansplat stylizer that delivers high-fidelity edits instantly at inference, without test-time optimization. By learning diffusion priors within structured latent space, our method effectively models discrete Gaussians. Disentangling geometry from appearance improves control and consistency, while also simplifying the training process. Experiments show that GaussianBlender surpasses baselines in textalignment and geometry preservation."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank members of the Bosch-UvA Delta Lab for helpful discussions and feedback. This project was generously supported by the Bosch Center for Artificial Intelligence."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image In European Conference on Computer and video editing. Vision (ECCV), pages 707723. Springer, 2022. 3 [2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instruc9 tions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18392 18402, 2023. 3, 5, 2 [3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. 3 [4] Jun-Kun Chen, Samuel Rota Bul`o, Norman Muller, Lorenzo Porzi, Peter Kontschieder, and Yu-Xiong Wang. Consistdreamer: 3d-consistent 2d diffusion for high-fidelity scene In Proceedings of the IEEE/CVF Conference on editing. Computer Vision and Pattern Recognition (CVPR), pages 2107121080, 2024. 3 [5] Minghao Chen, Iro Laina, and Andrea Vedaldi. Dge: Direct gaussian 3d editing by consistent multi-view editing. In European Conference on Computer Vision (ECCV), pages 74 92. Springer, 2024. [6] Minghao Chen, Junyu Xie, Iro Laina, and Andrea Vedaldi. Instruction-guided latent 3d editing in secShap-editor: onds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 26456 26466, 2024. 2, 3, 5, 6, 7, 8, 1 [7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for highIn Proceedings of the quality text-to-3d content creation. IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 3 [8] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. Tango: Text-driven photorealistic and robust 3d stylization via lighting decomposition. Advances in Neural Information Processing Systems (NeurIPS), 35:3092330936, 2022. 3 [9] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2147621485, 2024. 3, 5, 7, 8, 2 [10] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. 2 [11] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In Proceedings of universe of annotated 3d objects. the IEEE/CVF conference on computer vision and pattern recognition, pages 1314213153, 2023. 5, 1 [13] Jan-Niklas Dihlmann, Andreas Engelhardt, and Hendrik Lensch. Signerf: Scene integrated generation for neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 66796688, 2024. 5 [14] Jiahua Dong and Yu-Xiong Wang. Vica-nerf: Viewconsistency-aware 3d editing of neural radiance fields. Advances in Neural Information Processing Systems (NeurIPS), 36:6146661477, 2023. 3 [15] Yuan Dong, Qi Zuo, Xiaodong Gu, Weihao Yuan, Zhengyi Zhao, Zilong Dong, Liefeng Bo, and Qixing Huang. Gpld3d: Latent diffusion of 3d shape generative models by enforcIn Proceedings of the ing geometric and physical priors. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5666, 2024. 3 [16] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems (NeurIPS), 36:1622216239, 2023. [17] Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang, et al. Lightgaussian: Unbounded 3d gaussian compression with 15x reduction and 200+ fps. Advances in neural information processing systems, 37: 140138140158, 2025. 5, 7, 8, 1 [18] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1093310942, 2021. 8 [19] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [20] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: EditIn Proceedings of the ing 3d scenes with instructions. IEEE/CVF International Conference on Computer Vision (ICCV), pages 1974019750, 2023. 2, 3, 5, 7, 8 [21] Runze He, Shaofei Huang, Xuecheng Nie, Tianrui Hui, Luoqi Liu, Jiao Dai, Jizhong Han, Guanbin Li, and Si Liu. Customize your nerf: Adaptive source driven 3d scene editIn Proceedings of ing via local-global iterative training. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 69666975, 2024. 2, 3 [22] Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang, and Tong He. Gvgen: Text-to-3d generation with volumetric repIn European Conference on Computer Vision, resentation. pages 463479. Springer, 2024. [23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3 [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems (NeurIPS), 33:68406851, 2020. 2, 3 [25] Fangzhou Hong, Jiaxiang Tang, Ziang Cao, Min Shi, Tong Wu, Zhaoxi Chen, Shuai Yang, Tengfei Wang, Liang Pan, 3dtopia: Large text-to-3d generaDahua Lin, et al. arXiv preprint tion model with hybrid diffusion priors. arXiv:2403.02234, 2024. 3 [26] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1 [27] Heewoo Jun and Alex Nichol. ing conditional 3d implicit functions. arXiv:2305.02463, 2023. 5, 1, 2 Shap-e: GeneratarXiv preprint [28] Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Instruct 3d-to-3d: Text inarXiv preprint Ishii, and Takuya Narihira. struction guided 3d-to-3d conversion. arXiv:2303.15780, 2023. 3 [29] Nazmul Karim, Hasan Iqbal, Umar Khalid, Chen Chen, and Jing Hua. Free-editor: zero-shot text-driven 3d scene editing. In European Conference on Computer Visio (ECCV), pages 436453. Springer, 2024. 2, 3 [30] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60076017, 2023. 3 [31] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42 (4), 2023. 3 [32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19311941, 2023. 3 [33] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Instant3d: Fast text-to-3d Shakhnarovich, and Sai Bi. with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. [34] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023. 3 [35] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2251122521, 2023. 3 [36] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi Zhang, Peng Zhou, and Bingbing Ni. Focaldreamer: TextIn Proceeddriven 3d editing via focal-fusion assembly. ings of the AAAI conference on artificial intelligence, pages 32793287, 2024. 2, 3 [37] Chenguo Lin, Panwang Pan, Bangbang Yang, Zeming Li, and Yadong Mu. Diffsplat: Repurposing image diffusion models for scalable 3d gaussian splat generation. In International Conference on Learning Representations (ICLR), 2025. 3 [38] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 300309, 2023. 3 [39] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 92989309, 2023. 3 [40] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [41] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 99709980, 2024. 3 [42] Qi Ma, Yue Li, Bin Ren, Nicu Sebe, Ender Konukoglu, Theo Gevers, Luc Van Gool, and Danda Pani Paudel. Shapesplat: large-scale dataset of gaussian splats and their selfsupervised pretraining. arXiv preprint arXiv:2408.10906, 2024. 4, 1 [43] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. ICLR, 2022. 3 [44] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and Rana Hanocka. Text2mesh: Text-driven neural stylization In Proceedings of the IEEE/CVF Conference for meshes. on Computer Vision and Pattern Recognition (CVPR), pages 1349213502, 2022. [45] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus Brubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos Derpanis, and Igor Gilitschenski. Watch your steps: Local In European image and scene editing by text instructions. Conference on Computer Vision (ECCV), pages 111129. Springer, 2024. 3 [46] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real imIn Proceedings of ages using guided diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60386047, 2023. 3 [47] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. 3 [48] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [49] Jangho Park, Gihyun Kwon, and Jong Chul Ye. Ed-nerf: Efficient text-guided editing of 3d scene with latent space nerf. arXiv preprint arXiv:2310.02712, 2023. 3 [50] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 conference proceedings, pages 111, 2023. 3 [51] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben MildenIn The hall. Dreamfusion: Text-to-3d using 2d diffusion. Eleventh International Conference on Learning Representations (ICLR), 2023. 3 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International Conference on Machine Learning vision. (ICML), pages 87488763. PmLR, 2021. 3, 5, 2 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2, 3 [54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (CVPR), pages 22500 22510, 2023. 3 [55] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. Vox-e: Text-guided voxel editing of 3d objects. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 430440, 2023. 3 [56] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [57] Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent Y. F. Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 88398849, 2024. 3 [58] PyTorch An Imperative Style. High-performance deep learning library. Advances in Neural, 2024. 6, 1 [59] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 3 [60] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision (ECCV), pages 118. Springer, 2024. 3 [61] Shitao Tang, Fuyang Zhang, Jiacheng Chen, Peng Wang, and Yasutaka Furukawa. Mvdiffusion: Enabling holistic multiview image generation with correspondence-aware diffusion. arXiv, 2023. [62] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10748 10757, 2022. 6, 2 11 ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 67966807, 2024. 3 [76] Lu Yu, Wei Xiang, and Kang Han. Edit-diffnerf: Editing 3d neural radiance fields using 2d diffusion model. arXiv preprint arXiv:2306.09551, 2023. 3 [77] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. 3 [78] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems (NeurIPS), 36:3142831449, 2023. 3 [79] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF international Conference on Computer Vision (ICCV), pages 38363847, 2023. 3 [80] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. 3 [81] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [82] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 90269036, 2024. 3 [83] Xuying Zhang, Bo-Wen Yin, Yuming Chen, Zheng Lin, Yunheng Li, Qibin Hou, and Ming-Ming Cheng. Temo: Towards In Protext-driven 3d stylization for multi-object meshes. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1953119540, 2024. 3 [84] Jingyu Zhuang, Chen Wang, Liang Lin, Lingjie Liu, and Guanbin Li. Dreameditor: Text-driven 3d scene editing with neural fields. In SIGGRAPH Asia 2023 Conference Papers, pages 110, 2023. 2, 3 [63] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 19211930, 2023. 3 [64] Cyrus Vachha and Ayaan Haque. Instruct-gs2gs: Editing 3d gaussian splats with instructions, 2024. 3, 5, 7, 8, [65] Binglun Wang, Niladri Shekhar Dutt, and Niloy Mitra. Proteusnerf: Fast lightweight nerf editing using 3d-aware image context. Proceedings of the ACM on Computer Graphics and Interactive Techniques, 7(1):117, 2024. 3 [66] Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image driven manipIn Proceedings of the ulation of neural radiance fields. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 38353844, 2022. 3 [67] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions on Visualization and Computer Graphics, 2023. 3 [68] Yuxuan Wang, Xuanyu Yi, Zike Wu, Na Zhao, Long Chen, and Hanwang Zhang. View-consistent 3d editing with gaussian splatting. In European Conference on Computer Vision (ECCV), pages 404420. Springer, 2024. 3 [69] Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, and Victor Adrian Prisacariu. Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing. In European Conference on Computer Vision (ECCV), pages 5571. Springer, 2024. 3, 5, 8, 2 [70] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 803814, 2023. 7, [71] Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, and Mohamed Sayed. Morpheus: Text-driven 3d gausIn Proceedings of sian splat shape and color stylization. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 3 [72] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pages 2146921480, 2025. 3, 5, 1 [73] Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying Instructp2p: Learning to edit arXiv preprint instructions. Shan, and Shenghua Gao. 3d point clouds with text arXiv:2306.07154, 2023. 2, 3 [74] Haitao Yang, Yuan Dong, Hanwen Jiang, Dejia Xu, Georgios Pavlakos, and Qixing Huang. Atlas gaussians diffusion for 3d generation. arXiv preprint arXiv:2408.13055, 2024. 3 [75] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Pro12 GaussianBlender: Instant Stylization of 3D Gaussians with Disentangled Latent Spaces"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we first present details of the dataset (Sec. 6) and implementation (Sec. 7). Details of the user study are provided in Sec. 8. Additional experimental results are shown in Sec. 9, followed by further ablation studies in Sec. 10. Lastly, related work (Sec. 11) and failure cases of our framework (Sec. 12) are further discussed. Please refer to the output models folder in the supplementary material to access the 3D editing results. 6. Additional Dataset Details In this section, we provide details of the curated dataset used for training, as well as the evaluation benchmark. Edit prompts used for evaluation and for training single-prompt models are listed in Tab. 5. Training Dataset. For training our framework, we employ the TRELLIS-500K [72] split, curated collection of 500K 3D assets primarily from Objaverse (XL) [12], filtered based on aesthetic scores to support 3D generation tasks. After filtering, the Objaverse portion includes 168K objects with text captions generated using GPT-4o [26], from which we sample 140K assets. Each asset is rendered from 72 viewpoints, at resolution of 400 400 using Blender [11], and converted into 3D Gaussian splatting representations consisting of 50K Gaussians using LightGaussian [17]. To initialize the Gaussian centroids, we uniformly sample point cloud of 5K points from the 3D asset surfaces following ShapeSplat [42]. Assets with PSNR below 30 dB are excluded, resulting in final dataset of approximately 123K high-quality assets, with 3K reserved for testing and 120K used for training. For latent diffusion pre-training (Stage 2), we use the GPT-generated prompts included in the dataset split. Custom-designed set of edit prompts is used for training the latent editing stage (Stage 3, see Tab. 5). We will release this refined dataset, which includes multi-view renderings and corresponding 3D Gaussian splatting representations for approximately 123K highquality assets, to support future research in 3D learning. Evaluation Benchmark. We sample 30 assets from our unseen test split and apply 10 editing instructions, with each instruction used on 3 assets to compare with test-time optimization methods. Since the training code for Shap-Editor [6] is not publicly available, we evaluate it separately using its four released pre-trained models (one per edit instruction), applying each to 500 test assets (2000 total) sampled from our test split. For the qualitative results of our generalization experiment, we use the OmniObject3D dataset [70], which comprises 6K objects across 190 categories. We render 100 random views per object at resolution of 400400 using Blender [11] and similarly generate their 3DGS representations using LightGaussian [17]. Table 5. Edit prompts used for evaluation and for training singleprompt models."
        },
        {
            "title": "Evaluation\nBenchmark",
            "content": "Training (singleprompt) Make its colours look like rainbow, Make it Starry Night Van Gogh painting style, Make it wooden, Make it in cyberpunk style, Make it pop art neon duotone, Make it marble, Make it Barbie style, Make it golden, Make it botanical themed, Make it Halloween themed Each prompt in the evaluation benchmark is used to train separate model. 7. Additional Implementation Details GaussianBlender is implemented in PyTorch [58], and all trainings are performed on 4 NVIDIA H100 GPUs. 7.1. Architecture Details Gaussian VAE. For the Gaussian VAE architecture, we adopt the transformer-based encoder and decoder design from GaussianMAE [42]. Originally, GaussianMAE is masked autoencoder with mask ratio of 0.6, producing latent representation of size 64 384 from input assets containing 1024 Gaussians. To adapt GaussianMAE for our VAE, we remove the masking mechanism and increase the latent dimensions to 1024 1024 to match the latent space of Shap-E [27], which we use as the denoiser architecture. Since using only 1024 Gaussians would be too sparse for extracting high-dimensional latents, we instead use 16384 Gaussians as input to our model. These are randomly subsampled from each 3DGS asset, which originally contains 50K Gaussians. The encoder consists of 12 layers with 6 attention heads, while the decoder comprises 4 layers and 6 heads. drop path rate of 0.1 is applied. The cross-branch feature sharing module consists of symmetric multi-head cross-attention block with 4 heads, followed by learnable residual scaling and layer normalization, and aggregates geometry and appearance tokens to yield fused tokens. Denoiser. textthe transformer-based, conditioned denoiser from Shap-E [27], using its pretrained weights for initialization, as it is trained on 3D assets encoded into latent space, and thus carries rich 3D priors. We adopt 1 The model expects input latents of shape 1024 1024. For text-conditioned generation, we prepend single token containing the CLIP [52] text embedding. Rasterizer. We employ the parallelizable rasterizer introduced in MVSplat [10] in our framework. During training, images are rendered at resolution of 200 200 using the camera parameters of the input views provided in the dataset. 2D Image Editor. Pre-trained InstrucPix2Pix [2] is used as the image editor in our implementation. 7.2. Training Details (cid:16) Stage 1 - Latent Space Learning. We train the Gaussian VAE using the AdamW optimizer with an initial learning rate of 1e3 and weight decay of 5e2. cosine learning rate scheduler (CosLR) is employed over 200 epochs with 10-epoch warm-up phase. The model operates on 16384 input Gaussians, grouped into = 1024 clusters of size = 32 using soft KNN strategy based on Gaussian centroids. Weight of the fused tokens during injection is set to α = 0.5. Training is performed with batch size of 92. The render loss Lrender is computed over = 6 random views of the images from which the input Gaussians are optimized. Loss weights {λ1, λ2, λ3} are set to (cid:8)0.01, 1.0, 1e4(cid:9). Weight of LLP IP is set to τ = 0.4. λ4 follows beta-annealing schedule defined as λ4 = min , which gradually increases the LKL weight to stabilize latent space learning. During training of Stage 2, parameters of the Gaussian VAE are kept fixed and are not updated. In Stage 3, the encoder remains frozen while the decoder is further trained. Stage 2 - Latent Diffusion Pre-training. For the latent diffusion model pre-training, we use lower initial learning rate of 1e5 and the same weight decay of 5e2, along with cosine annealing learning rate scheduler (CosAnLR) over 200 epochs. Batch size of 120 is employed. The diffusion process is applied only to the appearance latent, while the geometry latent is directly passed to the geometry decoder. To support classifier-free guidance, conditioning is zeroed out with 0.1 probability during training similar to Shap-E [27]. The full training objective is: (cid:17) 100 , 1.0 2.0 epoch = λ1Lparam + λ2Lrender + λ5Ldif , (11) where {λ1, λ2, λ5} are set to {0.01, 1.0, 1.0}, and Ldif is the diffusion loss from Equation 9 of the main paper. Stage 3 - 3DGS Editing within the Latent Space. Similarly, this stage is trained using lower initial learning rate of 1e5 and the same weight decay of 5e2, using CosAnLR scheduler over 15 epochs. batch size of 120 is used. The diffusion time step is sampled from the range [0.02, 0.98]. For InstructPix2Pix [2], we apply classifierfree guidance with text scales sT [5.5, 9.5] and an image 2 guidance scale of sI = 2.0, where the text scale is randomly sampled for each batch. This stage is trained solely with the edit loss Ledit, using = 6 views arranged in grid. 7.3. Evaluation Details To evaluate how well the 3D edits align with the edit prompts, we report CLIP [52] similarity (CLIPsim) and directional similarity (CLIPdir) scores. Following ShapEditor [6], we also report Structure Distance (Structure Dist.) [62] metric to quantify structural consistency between input and edited assets. To compute the scores, outputs from our framework and the baselines are rendered at resolution of 200 200 from 8 viewpoints using the camera parameters provided in the dataset. 8. Details of the User Study We conducted user study with 50 participants to assess perceptual preferences between GaussianBlender and optimization-based methods IN2N [20], IGS2GS [64], GaussCtrl [69] and GaussianEditor [9]. Shap-Editor [6] was excluded because publicly available models do not cover the studys edit range. For each of the 8 tasks, participants viewed outputs from GaussianBlender and the baselines from multiple viewpoints, alongside the edit prompt and the input 3D asset. For each criterion, participants were requested to select the output with the best (i) textalignment, (ii) structural consistency, and (iii) visual quality, with detailed definitions provided in Tab. 6. We further asked the acceptable per-asset wait time conditioned on their preferred results. The results in Tab. 2 of the main paper show that GaussianBlender was consistently preferred across all three criteria, confirming its strength in text-to-3D style editing versus test-time optimization methods. For latency, 69.6% favored real-time (< 1 s); the remaining choices were 210 mins (30.4%), 1020 mins (13.0%), and 2060 mins (0%), indicating clear preference for fast editing. 9. Additional Experimental Results 9.1. More Visual Results We provide additional visual results of our method in Fig. 8 and Fig. 9. GaussianBlender consistently delivers highquality, text-aligned 3D stylization across large set of 3D assets through single forward pass instantly, eliminating the need for test-time optimization. By decoupling appearance from geometry, it achieves better geometry preservation, enabling direct stylization of locked 3D assets for large-scale production across diverse styles. 9.2. Scaling to Multiple Prompts We explore the capacity of single editor to learn and generalize across more edit prompts by training 20-prompt Table 6. The criteria used in the user study. The definitions listed here are also provided to participants. Criterion"
        },
        {
            "title": "Visual Quality",
            "content": "Which edited result follows the prompt most closely? Consider whether the style change is too subtle (almost no visible change), or too drastic (over-saturated). Which edited result best preserves the original assets 3D structure while applying only the requested edit? The objects shape should not change after stylization (e.g., no changes to the objects physical structure; for characters, the face, identity and features, must remain unchanged). Which edited result looks best overall? Figure 8. Additional visual results of our method. GaussianBlender delivers high-fidelity, text-aligned 3D stylization with strong geometry preservation instantly, with single feed-forward pass. model. The full set of edit instructions is given in Tab. 7. Visual comparisons between the single-prompt and 20-prompt variants of GaussianBlender in Fig. 10 show that the number of prompts can be scaled while maintaining visual quality and geometric fidelity. 10. Additional Ablation Studies Effect of Guidance Scale. GaussianBlender distills from InstructPix2Pix [2] in Stage 3, using classifier-free guidance with text scale sT [5.5, 9.5] and image scale sI = 2.0. We Figure 9. Additional visual results of our method. GaussianBlender delivers high-fidelity, text-aligned 3D stylization with strong geometry preservation instantly, with single feed-forward pass. Table 7. Edit prompts used for training the multi-prompt model. Training (20prompt) Make its colours look like rainbow, Make it Starry Night Van Gogh painting style, Make it wooden, Make it in cyberpunk style, Make it pop art neon duotone, Make it marble, Make it Barbie style, Make it golden, Make it botanical themed, Make it Halloween themed, Make it Hello Kitty themed, Make it look like made of steel, Make it The Scream Edvard Munch painting, Make it in sunset palette, Make it Pirates of the Caribbean style, Make it look like statue, Make it Garfield themed, Make it look like made of bronze, Make it Dalmatian style, Turn it into orange Table 8. Effect of guidance scale. Higher text guidance scales yield improved CLIP metrics with minimal loss in structural consistency. CLIPsim CLIPdir (1) sT [5.5, 9.5] (2) sT [9.5, 12.5] (3) sT [12.5, 15.5] 0.251 0.253 0.257 0.210 0.211 0.218 Structure Dist. 0.0064 0.0102 0.0185 compare our main model against higher text-scale variants: (2) sT [9.5, 12.5] and (3) sT [12.5, 15.5]. In all settings, sT is sampled uniformly per batch within the specified interval. (1) sT [5.5, 9.5] denotes the main model used throughout the paper. As shown in Tab. 8, higher text guidance scales yield improved CLIP metrics with minimal loss in structural consistency. 4 information exchange. This focuses edits on appearance while effectively preserving the underlying geometry. As shown in our ablations, removing disentanglement degrades reconstruction quality, and removing the feature-sharing In contrast, Shapmodule harms editing performance. Editor [6], while also avoiding test-time optimization, operates on shared appearancegeometry representation encoded from NeRFs using frozen Shap-E [27] autoencoder. As evident from Shap-Editors results, this shared representation is ill-suited for global appearance editing; it limits control and often produces extreme smoothness, loss of fine details, and severe structural distortion. Our disentangled formulation is therefore better aligned with geometrypreserving, large-scale stylization. 3) Structured latent representation for 3D Gaussians: As discussed, operating directly on 3D Gaussians is challenging due to their unstructured nature and the uneven distribution of geometry and appearance parameters, which complicates joint optimization. GaussianBlender makes this feasible by grouping Gaussians by spatial proximity and encoding them into disentangled latent spaces that retain global 3D structure. These structured latent representations provide promising starting point for downstream 3D applications and future research. 12. Failure Cases Figure 11. Failure cases. Our model struggles to represent complex scenes with fine-grained details using 16384 Gaussians, leading to lower-quality results. This can be addressed by increasing the number of Gaussians to better capture such details. The failure cases of our model are shown in Figure 11. Since our model represents assets using 16384 Gaussians, it may be insufficient for large complex assets such as full scenes, leading to lower quality reconstructions and blurry edits. These cases can be addressed by increasing the number of Gaussians to better capture fine-grained details in complex scenes. Figure 10. Visual results of scaling to multiple prompts. GaussianBlender is able to scale to multiple prompts while maintaining the 3D stylization quality and geometric fidelity. 11. Further Discussion on Prior Work GaussianBlender departs from prior state-of-the-art methods in several key aspects. 1) Feed-forward design: Our method is feed-forward 3DGS style editor that operates in the latent space of 3D Gaussian VAE, fully eliminating test-time optimization. Optimization-based methods often require minutes to tens of minutes per asset, while GaussianBlender achieves 0.26 second latency, enabling both large-scale production of diverse assets and truly interactive editing, both of which are crucial for practical 3D content creation workflows. Furthermore, GaussianBlender is distilled from 2D editor with guidance scales sampled from reasonable range across many samples. As result, it is far less sensitive to the 2D editors guidance scale than optimization-based baselines, which typically demand careful tuning to avoid overly drastic or barely perceptible edits. 2) Disentangled appearancegeometry with controlled sharing: GaussianBlender learns separate latent spaces for appearance and geometry, coupled with lightweight feature-sharing module that enables controlled"
        }
    ],
    "affiliations": [
        "Bosch Center for AI",
        "University of Amsterdam"
    ]
}