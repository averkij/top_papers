{
    "paper_title": "Name That Part: 3D Part Segmentation and Naming",
    "authors": [
        "Soumava Paul",
        "Prakhar Kaushik",
        "Ankit Vaidya",
        "Anand Bhattad",
        "Alan Yuille"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as a direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling a theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D part segmentation and naming method finds applications in several downstream tasks, including serving as a scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create a unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task."
        },
        {
            "title": "Start",
            "content": "Name That Part: 3D Part Segmentation and Naming Soumava Paul Prakhar Kaushik"
        },
        {
            "title": "Ankit Vaidya Anand Bhattad Alan Yuille",
            "content": "Equal Contribution Project Lead Johns Hopkins University Baltimore, MD, USA {spaul27,pkaushi1,avaidya7,bhattad,ayuille1}@jh.edu 5 2 0 2 9 1 ] . [ 1 3 0 0 8 1 . 2 1 5 2 : r Figure 1: ALIGN-Parts is the first large-scale method to be able to efficiently and semantically segment and name 3D parts of an object, unlike previous methods, which could only do class-agnostic part segmentation. Our method is also feed-forward, and defines 3D parts according to human-oriented, object-specific affordance descriptions. Abstract We address semantic 3D part segmentation: decomposing objects into parts with meaningful names. While datasets exist with part annotations, their definitions are inconsistent across datasets, limiting robust training. Previous methods produce unlabeled decompositions or retrieve single parts without complete shape annotations. We propose ALIGN-Parts, which formulates part naming as direct set alignment task. Our method decomposes shapes into partlets - implicit 3D part representations - matched to part descriptions via bipartite assignment. We combine geometric cues from 3D part fields, appearance from multi-view vision features, and semantic knowledge from language-model-generated affordance descriptions. Text-alignment loss ensures partlets share embedding space with text, enabling theoretically open-vocabulary matching setup, given sufficient data. Our efficient and novel, one-shot, 3D Project page: https://name-that-part.github.io. part segmentation and naming method finds applications in several downstream tasks, including serving as scalable annotation engine. As our model supports zero-shot matching to arbitrary descriptions and confidence-calibrated predictions for known categories, with human verification, we create unified ontology that aligns PartNet, 3DCoMPaT++, and Find3D, consisting of 1,794 unique 3D parts. We also show examples from our newly created Tex-Parts dataset. We also introduce 2 novel metrics appropriate for the named 3D part segmentation task. Keywords 3D part semantic segmentation, geometric deep learning"
        },
        {
            "title": "1 Introduction\nMany vision tasks require 3D parts, not just whole-object labels. Ex-\namples include robots grasping handles and creators editing assets.\nThis requires solving two problems simultaneously: geometrically",
            "content": "1 Paul and Kaushik et al. Method Complete Decomposition Named Parts Open Vocabulary Permutation Invariant Part-level Features Feed Forward PartField SAMPart3D Find3D Ours - Table 1: Comparison with 3D part methods. specifies the part. We align this set of partlets to set of candidate part descriptions via bipartite matching in single forward pass. Each partlet inherits name from its matched description. null class allows the model to discard unused partlets, enabling the number of parts to adapt per shape while ensuring permutation consistency: each predicted part receives at most one name, and each name is used at most once. Our key contribution is applying set-level matching to 3D part fields. This enables lower computational complexity: we match handful of partlets to descriptions instead of all points to all labels. It provides permutation consistency: each predicted part receives at most one name, and each name is used at most once. Because of this, ALIGN-Parts is 100 faster at generating these segments along with names. To ensure partlets are both geometrically separable and semantically meaningful, we combine three signals. (1) Geometry features from 3D part-field backbone [Liu et al. 2025] capture shape structure. (2) Appearance features from multi-view image encoders [Oquab et al. 2023] lifted onto 3D geometry provide texture cues. (3) Semantic knowledge comes from affordance-aware part descriptions that encode form-and-function relationships [Connell and Brady 1987]. For example, chair seat becomes the horizontal surface where person sits, linking its flat, horizontal geometry to its sitting affordance. For text grounding, we generate these descriptions using large language model [Comanici et al. 2025] and embed them using MPNet sentence transformers [Song et al. 2020]. By representing part descriptions as embeddings in continuous space, our approach supports scenarios where the model can match partlets to any user-provided set of part descriptions without retraining. Our approach provides tool to address the data bottleneck. We construct unified part ontology using hybrid LLM-andhuman process that normalizes labels and verifies geometric consistency across PartNet, 3DCoMPaT++, and Find3D. We then apply ALIGN-Parts to bootstrap annotations from unlabeled TexVerse assets, creating Tex-Parts: dataset with 8450 objects spanning 14k part categories. ALIGN-Parts serves as scalable annotation engine that proposes named parts for human verification, converting raw meshes into training data with minimal effort and enabling the construction of large, consistently-annotated 3D datasets. In summary, our contributions are: Direct 3D parts alignment for open-world part naming. We introduce partlets - shape-conditioned part proposals with text embeddings - and match them to candidate descriptions via bipartite assignment, enabling efficient labeling of 3D part segmentation and naming. Figure 2: ALIGN-Parts segments and names 3D parts, unlike PartField [Liu et al. 2025], which only segments. Our method is 100 (post data pre-processing) faster at generating these segments along with names compared to PartField. segmenting the parts and semantically naming them. While large datasets of 3D objects exist, only few provide part annotations, and these annotations are often inconsistent across datasets [Ma et al. 2024; Mo et al. 2019; Slim et al. 2023]. An algorithm that can provide accurate and consistent annotations of named parts on any 3D object would enable scalable training data and support human-in-the-loop annotation pipelines. Existing methods address only one aspect of this problem. Part segmentation models can identify geometric boundaries but produce unnamed regions [Liu et al. 2025]. Language-grounded systems can retrieve single part from text query but fail to produce complete set of names for all parts of an object [Ma et al. 2024]. Classical unsupervised part discovery lacks the semantic grounding to derive consistent definitions. This gap has created bottleneck: the absence of large-scale, consistently-annotated 3D part data. Inducing consistent parts from unlabeled web assets requires coupling geometric features and semantic knowledge with human verification. We propose ALIGN-Parts, which formulates 3D part naming as direct set alignment problem. Rather than deciding per-point which text label to assign, we decompose the shape into small set of shape-conditioned partlets. Each partlet consists of set of points (a segmentation mask) and text description (embedding) corresponding to one part. These partlets aggregate information across all points belonging to part: single point on chair seat contains limited information, but the set of all points on the seat 2 Name That Part Geometry-appearance-semantic fusion. We combine geometric structure, appearance features, and affordance-aware LLM-generated descriptions to produce semantically grounded, visually coherent partlets. Metrics for evaluating 3D part semantic segmentation We introduce 2 metrics suitable for our 3D part segmentation and naming task, which evaluate ALIGN-Parts and related baselines for both part segmentation accuracy and semantic correctness of the predicted part label. Unified ontology and scalable annotation engine. We harmonize part taxonomies across PartNet, 3DCoMPaT++, and Find3D, and demonstrate human-in-the-loop pipeline that bootstraps TexParts, verified benchmark of 8450 objects spanning 14k categories derived from Texverse [Zhang et al. 2025]. ALIGN-Parts converts raw meshes into training data with 5-8 less human effort than manual annotation."
        },
        {
            "title": "2 Related Work\n3D Part Segmentation. Traditional methods operate in a purely\ngeometric regime. PartField [Liu et al. 2025], the current state-of-the-\nart, learns dense per-point feature fields but produces unlabeled re-\ngions. Recent works lift 2D foundation models into 3D: SAM-based\napproaches [Ma et al. 2025; Tang et al. 2024; Yang et al. 2024] adapt\nSegment-Anything via multi-view projection but require prompts\nand lack semantic names. PartSTAD [Kim and Sung 2024] adapts\nGLIP and SAM with 3D-aware objectives, while Diff3F [Dutt et al.\n2024] exploits diffusion features, but both operate as multi-stage\npipelines without producing complete, non-overlapping named\nparts.",
            "content": "To our knowledge, [Kalogerakis et al. 2010, 2017] are the only previous works that do simultaneous 3D part segmentation and labeling. [Kalogerakis et al. 2010] defines the task by assigning fixed labels to mesh faces using statistical consistency using Conditional Random Field Model, which is hard to scale up. Further, the model lacks any semantic knowledge given its the reliance on hand engineering features and geometric consistency. Simultaneous 3D part segmentation and recognition has remained complex task with little progress since then as it requires incorporation of human contextual knowledge of parts. Therefore, most subsequent works have focused on amodal segmentation, which is relatively easier task. Other important works in the area of 3D parts - which are not directly relevant to our work - but involve relevant 3D analysis, understanding or retrieval, include [Golovinskiy and Funkhouser 2008; Hanocka et al. 2019; Jones et al. 2020; Shapira et al. 2008; Stelzner et al. 2021; Takmaz et al. 2023; Tulsiani et al. 2017]. Language-Grounded 3D Understanding. PartSLIP [Liu et al. 2023], PartSLIP++ [Zhou et al. 2023], and PartDistill [Umam et al. 2024] use image-language models for part segmentation but require per-category fine-tuning with predefined vocabularies. Find3D [Ma et al. 2024] is most related: it trains point transformer in text embedding space for text-to-part retrieval. However, Find3D operates query-by-query (given \"wing, head,\" it returns masks) rather than producing complete decompositions, and works on individual point features rather than part-level aggregations. ALIGN-Parts differs fundamentally. We formulate part segmentation and naming as direct set alignment: Partlets aggregate point features into part-level representations matched to text via optimal transport, enabling (1) complete, non-overlapping decompositions in one pass, (2) permutation-consistent naming, (3) dynamic part counts, and (4) zero-shot generalization  (Table 1)  . Unlike previous methods, ALIGN-Parts jointly learns segmentation and alignment in single feed-forward pass. Part count emerges automatically from activated Partlets, eliminating dependence on prediction or part-name inputs. Part-Based Generation and Datasets. Generative methods [Chen et al. 2024, 2025; Lin et al. 2025; Tang et al. 2025; Yang et al. 2025] discover part structure through synthesis but lack open-vocabulary naming mechanisms. For datasets, PartNet [Mo et al. 2019], 3DCoMPaT++ [Slim et al. 2023], and GAPartNet [Geng et al. 2023] provide part annotations but use inconsistent taxonomies. ALIGN-Parts constructs unified ontology across these datasets and creates unified dataset for cross-dataset evaluation."
        },
        {
            "title": "3 Method\nOverview. We propose ALIGN-Parts, a framework that treats au-\ntomatic semantic 3D part segmentation as a direct set alignment\nproblem, analogous to DETR [Carion et al. 2020] in 2D detection.\nThe pipeline consists of three components: (1) a dense feature fu-\nsion module combining geometry and appearance via localized\nbi-directional 3D-aware cross-attention, (2) a Partlets module that\nlearns K adaptive part-level representations by aggregating fused\npoint features, and (3) a semantic grounding module that aligns\nPartlets to part names via text descriptions of part affordances. At\ninference, given a 3D shape and candidate part descriptions, our\nmethod produces named part segments without specifying part\ncounts, names, or point prompts. Training uses contrastive align-\nment and differentiable optimal transport matching between pre-\ndicted Partlets and ground-truth parts, enabling end-to-end learning\nof both segmentation and semantics.\nProblem Formulation. Given a 3D point cloud P = {pğ‘– }ğ‘\nğ‘–=1, each\nğ‘”\nğ‘– âˆˆ Rğ‘‘ğ‘” , appearance-first fea-\npoint has geometry-first features f\nğ‘– âˆˆ Rğ‘‘ğ‘ , and coordinates xğ‘– âˆˆ R3. During training, we have\ntures fğ‘\nground-truth part masks Mgt âˆˆ {0, 1}ğ´Ã—ğ‘ and corresponding text\nembeddings {tğ‘ }ğ´\nğ‘=1 where tğ‘ âˆˆ Rğ‘‘ğ‘¡ is an MPNet embedding of a\nlanguage-model-generated part description.",
            "content": "ğ‘˜=1 Our model learns ğ¾ = 32 instance-specific Partlet embeddings {sğ‘˜ Rğ‘‘ğ‘¡ }ğ¾ that reside in the same semantic space as text embeddings (we use ğ‘‘ğ‘¡ = 768). Each Partlet predicts three outputs: (i) soft mask mğ‘˜ [0, 1]ğ‘ over points, (ii) partness score partğ‘˜ indicating whether it represents an actual part, and (iii) its embedding sğ‘˜ which serves as the prototype for semantic alignment. We select ğ¾ = 32 to accommodate variable part configurations (most shapes have 28 parts) while allowing future extensibility. Point labels are obtained by matching Partlets to ground-truth parts via optimal transport."
        },
        {
            "title": "3.1 Architecture\nğ‘”\nDense Feature Fusion Module The raw geometric features f\nğ‘–\nand appearance features fğ‘\nğ‘– capture complementary information:\ngeometry encodes shape characteristics, while appearance provides",
            "content": "3 Paul and Kaushik et al. Figure 3: ALIGN-Parts. Overview of the ALIGN-Parts framework for language-grounded 3D part segmentation and naming. Top: training. Given 3D from our semantically unified 3D parts data, geometry features are extracted with PartField and appearance features with DINOv2 from multi-view renderings; these are fused by the BiCo Fusion module using efficient bi-directional cross-attention on local ğ‘˜=16 nearest-neighbor graphs in 3D space, reducing complexity from (ğ‘ 2) to (ğ‘ğ‘˜) and yielding enriched point features. decoder then learns ğ¾ part-level Partlet representations that aggregate the fused features, with segmentation supervision provided at the Partlet level. To semantically ground Partlets, an LLM generates affordance-aware descriptions for each possible part (e.g., structural supports that elevate the sofa for sofa legs), which are embedded by pretrained MPNet encoder; Sinkhorn matching establishes bipartite assignment between Partlet and text embeddings, and an InfoNCE loss further aligns them in shared representation space while classifier predicts the object category. Bottom: inference. At test time, ALIGN-Parts operates in both closed-vocabulary (object categories similar to those seen in training) and open-vocabulary (novel object categories) settings: in the closed-vocabulary case, the trained 3D classifier predicts the object class and retrieves its candidate part list, whereas in the open-vocabulary case an LLM proposes an overcomplete set of plausible parts for the queried object. Given these candidate part texts, their MPNet embeddings are bipartite-matched to the predicted Partlets, which jointly produce 3D part segmentation masks and corresponding part names. texture and visual cues. We fuse these modalities through bidirectional cross-attention module that operates on ğ‘˜-nearest neighbor (KNN) graph to maintain computational tractability. Given the KNN graph structure with indices Nğ‘– denoting the ğ‘˜nearest neighbors of point ğ‘–, we compute: Relative Positional Bias. To incorporate 3D spatial structure through Fourier-encoded relative positional biases, for each neighbor pair (ğ‘–, ğ‘—) where ğ‘— Nğ‘– , we compute: dğ‘– ğ‘— = xğ‘— xğ‘– R3 fğ‘– ğ‘— = dğ‘– ğ‘— ğœ” R3ğ¹ hğ‘– ğ‘— = [sin(fğ‘– ğ‘— ), cos(fğ‘– ğ‘— )] R32ğ¹ bğ‘– ğ‘— = MLP([dğ‘– ğ‘—, flatten(hğ‘– ğ‘— )]) Rğ» (1) (2) (3) (4) where ğœ” = [20, 21, . . . , 2ğ¹ 1] with F=6 are logarithmically-spaced frequencies, and the MLP consists of two layers: R39 R64 Rğ» with ReLU activation. Bi-Directional Cross-Attention. Let ğ» = 8 be the number of attention heads and ğ‘‘â„ = ğ‘‘ğ‘š/ğ» = 96 be the head dimension where ğ‘‘ğ‘š = 768 is the model dimension. Name That Part Geometric-to-Appearance Direction: For each point ğ‘– with neighbors Nğ‘– , geometric features attend to appearance features of neighbors, producing cross-modal features that capture appearance information. instance through refinement layers. At each layer â„“, each Partlet undergoes three operations: Partlet-to-Partlet Interaction: Partlets interact to model part co-occurrence (e.g., chairs have seats and backs): ğ‘ ğ‘– ğ‘ = ğ‘ ğ‘– ğ‘— ğ‘ ğ‘ = ğ‘˜ ğ‘– ğ‘— ğ‘ ğ‘ = ğ‘£ V ğ‘” ğ‘– Rğ» ğ‘‘â„ ğ‘ ğ‘— Rğ» ğ‘‘â„, ğ‘— Nğ‘– ğ‘ ğ‘— Rğ» ğ‘‘â„, ğ‘— Nğ‘– ğ‘– ğ‘— exp((Qğ‘– ğ‘ [â„])/ ğ‘ [â„] ğ‘– ğ‘— ğ‘ [â„])/ ğ‘ [â„] ğ›¼ğ‘–,â„,ğ‘— ğ‘ğ‘ = ğ‘–,â„ ğ‘ = (cid:205)ğ‘— Nğ‘– exp((Qğ‘– ğ‘– ğ‘— ğ‘ [â„] ğ›¼ğ‘–,â„,ğ‘— ğ‘ğ‘ ğ‘‘â„ + bğ‘– ğ‘— [â„]) ğ‘‘â„ + bğ‘– ğ‘— [â„]) ğ‘— Nğ‘– ğ‘– ğ‘ = Wğ‘ğ‘concatâ„ [z ğ‘–,â„ ğ‘ ] Rğ‘‘ğ‘” (5) (6) (7) (8) (9) (10) Appearance-to-Geometric Direction: Symmetrically, appearance features attend to geometric features of neighbors, capturing geometric information. This produces rğ‘– ğ‘ Rğ‘‘ğ‘ . Gated Fusion. Learned sigmoid gates control how much of this cross-modal information to incorporate into each original feature, based on both the original feature and the attended information. ğ‘ ğ‘– ğ‘ = ğœ (W ğ‘” [f ğ‘– ğ‘ ğ‘ = ğœ (W ğ‘” [f ğ‘” ğ‘– = LayerNorm(f ğ‘ ğ‘– = LayerNorm(f ğ‘” ğ‘– ğ‘ ]) Rğ‘‘ğ‘” ğ‘– ; ğ‘– ğ‘ ğ‘]) Rğ‘‘ğ‘ ğ‘– ; ğ‘” ğ‘– ğ‘– ğ‘– + ğ‘ ğ‘ ) ğ‘– ğ‘ ğ‘– ğ‘– + ğ‘ ğ‘) (11) (12) (13) (14) Final Projection. After gated addition and layer normalization, we concatenate both modalities and project through two-layer MLP to produce fused features hğ‘– R1216 for each point. hğ‘– = W2GELU(W1LayerNorm([ ğ‘– ; ğ‘ ğ‘– ])) (15) ğ‘” where W1 Rğ‘‘ğ‘“ (ğ‘‘ğ‘”+ğ‘‘ğ‘ ) and W2 Rğ‘‘ğ‘“ ğ‘‘ğ‘“ with ğ‘‘ğ‘” = 448, ğ‘‘ğ‘ = 768, and ğ‘‘ğ‘“ = 256. BiCo Fusion operates on local ğ‘˜=16 nearest neighbor graphs in 3D coordinate space, reducing complexity to (ğ‘ ğ‘˜). Points Partlets: Learning Part-Level Representations. We learn Partlet embeddings that aggregate point-level information into part-level representations. Partlet is defined by three components: (i) soft segmentation mask [0, 1]ğ‘ representing membership scores, (ii) Partlet embedding Rğ‘‘ğ‘¡ in the learned semantic space, and (iii) text embedding Rğ‘‘ğ‘¡ representing the part description. Formally, we learn parameterized function ğ‘“ğœƒ : Rğ‘‘â„ ğ‘ ğ‘–=1 to (Rğ‘ Rğ‘‘ğ‘¡ )ğ¾ that maps fused point features = {hğ‘– }ğ‘ Partlets: (mğ‘˜, sğ‘˜ ) = ğ‘“ğœƒ (H), ğ‘˜ = 1, . . . , ğ¾ (16) The parameters ğœƒ include all weight matrices and biases in the refinement network described below. The motivation for Partlets is: individual point features cannot reliably map to semantic part descriptions (e.g., single point on chair seat lacks context to predict \"seat\"), but aggregating features across all points in part enables robust semantic grounding. We initialize learnable Partlet embeddings {s(0) ğ‘˜ ğ‘˜=1 sampled from (0, I), shared across all shapes but adapted per Rğ‘‘ğ‘¡ }ğ¾ s(â„“,1) ğ‘˜ = s(â„“ 1) ğ‘˜ + SelfAttn(s(â„“ 1) Point-to-Partlet Aggregation: Partlets gather shape-specific evidence from BiCo-fused point features: , {sğ‘˜ }ğ¾ ğ‘˜=1) (17) ğ‘˜ s(â„“,2) ğ‘˜ = s(â„“,1) ğ‘˜ + CrossAttn(s(â„“,1) ğ‘˜ , {hğ‘– }ğ‘ ğ‘–=1) Non-linear Transformation: Two-layer MLP with GELU: ğ‘˜ = s(â„“,2) s(â„“ ) ğ‘˜ + MLP(s(â„“,2) ğ‘˜ ) (18) (19) After layers, we obtain refined Partlet embeddings sğ‘˜ = s(ğ¿) . ğ‘˜ Mask Prediction. Each Partlet predicts point membership via scaled dot-product: ğ‘šğ‘˜ğ‘– = (Wğ‘sğ‘˜ )ğ‘‡ (Wğ‘˜ hğ‘– ) ğ‘‘ğ‘¡ (20) During training, we apply sigmoid activation; at inference, softmax across Partlets yields soft assignments. Partness Prediction. Each Partlet predicts whether it represents an actual part: ğ‘‡ partğ‘˜ = partsğ‘˜ + ğ‘part (21) Higher values indicate active Partlets; lower values signify no-part, enabling dynamic part count adaptation per shape. Semantic Alignment: Partlet Part Names. Each Partlets refined embedding sğ‘˜ serves directly as its prototype zğ‘˜ = sğ‘˜ Rğ‘‘ğ‘¡ . After normalization (Ë†zğ‘˜ = zğ‘˜ /zğ‘˜ 2), we compute cosine similarity with text embeddings Ë†tğ‘: sim(ğ‘˜, ğ‘) = Ë†zğ‘˜ Ë†tğ‘ (22) Importantly, Partlet and text embeddings share the same semantic space (Rğ‘‘ğ‘¡ ) without intermediate projections - this alignment is driven by the text alignment loss (Section 3.2), enabling openvocabulary matching at inference without retraining. However, part names are often ambiguous handle could refer to door handle, mug handle, or wheelchair handle, each with distinct geometry and function. To disambiguate, we use affordance-based descriptions (e.g., the part of door grasped to open it for door handle, the horizontal surface of chair where person sits for chair seat) generated by Gemini 2.5 Flash [Comanici et al. 2025]. These are embedded with sentence transformer model, such as MPNet (all-mpnet-base-v2) [Song et al. 2020], for capturing long-form semantic affordances, particularly for similar part names."
        },
        {
            "title": "3.2 Training: Partlet â†” Part Names\nWe establish correspondences between predicted Partlets and ground-\ntruth parts via differentiable optimal transport. The cost matrix\nC âˆˆ Rğ¾ Ã—ğ´ combines mask overlap and semantic similarity:",
            "content": "ğ¶ğ‘˜ğ‘ = (ğ‘˜,ğ‘) mask + (1 sim(Ë†zğ‘˜, Ë†tğ‘)) (23) where (ğ‘˜,ğ‘) gt ğ‘ ) (equal weighting ğ›¼ = ğ›½ = 1.0). mask = 1 Dice(ğœ (mğ‘˜ ), Sinkhorn-Knopp iterations produce soft assignment matrix [0, 1]ğ¾ ğ´. Thresholding yields hard assignments ğœ‹ : {1, . . . , ğ¾ } {1, . . . , ğ´} {}, where ğœ‹ (ğ‘˜) = ğ‘ matches Partlet ğ‘˜ to part ğ‘ and ğœ‹ (ğ‘˜) = indicates no match. Losses. Let = {ğ‘˜ : ğœ‹ (ğ‘˜) } denote matched Partlets. Our training objective combines several losses: Text Alignment Loss. This loss is essential for open-vocabulary grounding. Without it, Partlet embeddings remain geometrically meaningful but semantically ambiguous. We apply InfoNCE contrastive loss to align matched Partlets with their text embeddings: Ltext = 1 ğ‘˜ log exp(Ë†zğ‘˜ Ë†tğœ‹ (ğ‘˜ ) /ğœ) ğ‘=1 exp(Ë†zğ‘˜ Ë†tğ‘/ğœ) (cid:205)ğ´ (24) with ğœ = 0.07. Operating over Partlets rather than individual points makes this optimization tractable and stable. Mask and Partness Losses. For matched Partlets, binary crossentropy and Dice loss supervise masks: Lmask = ğ‘˜ gt [BCE(mğ‘˜, ğœ‹ (ğ‘˜ ) ) gt +(1 Dice(ğœ (mğ‘˜ ), ğœ‹ (ğ‘˜ ) ))] (25) Binary classification loss supervises partness, teaching Partlets to predict whether they are active (matched) or inactive (no-part): Lpart = 1 ğ¾ ğ¾ ğ‘˜=1 BCE(partğ‘˜, 1[ğœ‹ (ğ‘˜) ]) (26) Auxiliary Regularizers. Coverage loss prevents over / undersegmentation by penalizing mask size disparities: Lcov = 1 ğ‘˜ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:205)ğ‘– ğœ (ğ‘šğ‘˜ğ‘– ) (cid:205)ğ‘– ğ‘šgt ğ‘ ğœ‹ (ğ‘˜ )ğ‘– (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (27) Overlap loss enforces mutual exclusivityeach point should belong to at most one part: Loverlap = 1 ğ‘ ğ‘ (cid:32) ğ¾ ğ‘–=1 ğ‘˜=1 (cid:33) 2 ğœ (ğ‘šğ‘˜ğ‘– ) 1 (28) Global Alignment Loss. Symmetric InfoNCE aligns the global shape representation with class-level text embeddings, providing object-level semantic context: Lglobal = 1 2 (cid:2)LCE (S, I) + LCE (S, I)(cid:3) (29) Ë†Zglobal where = 1 ğœ Total Loss. The complete training objective as function of model parameters ğœƒ is: Ë†T class. Paul and Kaushik et al."
        },
        {
            "title": "3.3 Inference Modes and Use Cases\nOur model supports three primary inference modes for different\ndeployment scenarios.",
            "content": "Mode 1: Closed-Vocabulary with Confidence Calibration. We use ALIGN-Parts to scalably annotate large datasets from known categories (e.g., labeling millions of airplane meshes). This is the most practical scenario, enabling efficient 3D part segmentation and labeling with minimal oversight. We utilize this capability in building the TexParts dataset 4.7. For training categories C, we predict the object category via global shape-text alignment: ğ‘ = arg max ğ‘ sim(zglobal, tğ‘ ) (31) then filter Partlets by partness score (ğœ (partğ‘˜ )>0.5) to obtain active set Kactive. We construct cost matrix Kactive Lğ‘ : ğ¶ğ‘˜ğ‘ = 1 sim(Ë†zğ‘˜, Ë†tğ‘) At inference, we use the Jonker-Volgenant algorithm [Crouse 2016] for exact optimal assignment, which is more efficient than Sinkhorn since gradients are not required. (32) Mahalanobis Score Estimation. The Mahalanobis confidence (Eq. 35) requires class-conditional statistics (mean and covariance) that are estimated from the training set. After training, we perform single forward pass over the entire training dataset. For every partlet ğ‘˜ that is successfully matched to ground-truth part label â„“ (i.e., ğœ‹ (ğ‘˜) = â„“), we extract its prototype embedding zğ‘˜ . We then compute the empirical mean ğâ„“ for each part label â„“ in our known training vocabulary C: ğâ„“ = E[zğ‘˜ ğœ‹ (ğ‘˜) = â„“] For robustness, we compute single, shared covariance matrix ğšº by pooling the embeddings from all part classes: (33) ğšº = Cov({zğ‘˜ }ğ‘˜,â„“ s.t. ğœ‹ (ğ‘˜ )=â„“ ) We apply regularization (e.g., adding small value ğœ–I to the diagonal) before computing the inverse ğšº1 to ensure numerical stability. These pre-computed ğâ„“ and ğšº1 are stored and used at inference time to compute the following Mahalanobis-distance based confidence score: (34) confmaha (ğ‘˜) = exp((zğ‘˜ ğğ‘ )ğ‘‡ ğšº1 ğ‘ ğ‘˜ where ğğ‘ are estimated from training embeddings. Predictions with confmaha >= 0.8 are auto-accepted; lower-confidence predictions are flagged for human verification, dramatically reducing annotation cost. (zğ‘˜ ğğ‘ ğ‘˜ and ğšºğ‘ ğ‘˜ (35) )) ğ‘˜ ğ‘˜ Ltotal (ğœƒ ) = ğœ†maskLmask (ğœƒ, ğœ‹) + ğœ†partLpart (ğœƒ, ğœ‹) +ğœ†textLtext (ğœƒ, ğœ‹) + ğœ†covLcov (ğœƒ, ğœ‹) + ğœ†ovLoverlap (ğœƒ, ğœ‹) +ğœ†globalLglobal (ğœƒ ) (30) where ğœƒ includes all learnable parameters (BiCo fusion weights, Partlet decoder layers, prediction heads), ğœ‹ is the assignment from Partlets to ground-truth parts, computed via Sinkhorn matching given the current model predictions. All hyperparameters are in the supplement. Fused Confidence Formulation. The final confidence score conf(ğ‘˜) for matched query ğ‘˜ is fusion of the softmax confidence (confsoft) and the Mahalanobis confidence (confmaha). We combine them as follows: conf(ğ‘˜) = ğ›¼ confsoft (ğ‘˜) + (1 ğ›¼) ğœ (ğ›½ (confmaha (ğ‘˜) 0.5)) (36) where ğœ () is the sigmoid function. confsoft (ğ‘˜) is the temperature-calibrated softmax score (Eq. 38). confmaha (ğ‘˜) is the Mahalanobis confidence (Eq. 35). 6 Name That Part ğ›¼ and ğ›½ are hyperparameters that balance the two scores. We set ğ›¼ = 0.5 and ğ›½ = 1.0 based on calibration on heldout validation set. Annotations with conf(ğ‘˜) < ğœconf (where ğœconf = 0.5) are flagged as low-confidence and routed to human annotator for manual review. which confirms that 32 accommodates the majority of objects without excessive over-segmentation or loss of fine granularity. Optimization. Loss weights: ğœ†mask = 1.0, ğœ†part = 0.5, ğœ†text = 1.0, ğœ†cov = 0.5, ğœ†overlap = 0.1, ğœ†global = 1.0. We use AdamW with an initial learning rate of 3e-4 and cosine annealing to minimum of 5e-6. Mode 2: Open-Vocabulary Grounding. For novel categories, users provide candidate part descriptions {tğ‘ }ğ´ ğ‘=1 (or generate via LLM from image/hint). After filtering inactive Partlets, each active Partlet matches to the best description: ğ‘ ğ‘˜ = arg max ğ‘ {1,...,ğ´} sim(Ë†zğ‘˜, Ë†tğ‘) Confidence is computed via temperature-calibrated softmax: confsoft (ğ‘˜) = max ğ‘ exp(Ë†zğ‘˜ Ë†tğ‘/ğœ) ğ‘=1 exp(Ë†zğ‘˜ Ë†t (cid:205)ğ´ ğ‘/ğœ) (37) (38) with ğœ = 0.07. This is less calibrated than the Mahalanobis distance (Mode 1) due to the lack of training statistics for novel categories. Mode 3: Text-Conditioned Part Retrieval. For compatibility with Find3D [Ma et al. 2024], we retrieve single part for query tğ‘: Figure 4: Pairwise cosine similarity heatmaps between text embeddings for MPNet (left) and SigLiP (right). (Zoom in for labels) ğ‘˜ = arg max ğ‘˜ Kactive sim(Ë†zğ‘˜, Ë†tğ‘) (39) returning mask mğ‘˜ . This mode is primarily for benchmark comparison. Point Label Assignment. Points are assigned to the highestscoring Partlets label: labelğ‘– = ğ‘ ğ‘˜ where ğ‘˜ = arg max ğ‘˜ Kactive ğœ (ğ‘šğ‘˜ğ‘– ) (40) Points with maxğ‘˜ ğœ (ğ‘šğ‘˜ğ‘– ) < 0.5 remain unlabeled."
        },
        {
            "title": "3.4 Implementation Details\nTraining Setup. ALIGN-Parts is trained on 3 NVIDIA A6000 GPUs\nfor 2 days (batch size 16). Due to academic compute constraints,\nwe sample 10k points per shape (vs. 100k in PartField) while\nmaintaining strong performance. Models are normalized to [âˆ’1, 1]3\nduring training.",
            "content": "Architecture. The model has 34M parameters total: 5.7M for feature Fusion, 26.8M for Partlets, and 1.5M for the global classifier. Feature interactions (Partlet-to-points and Partlet-to-Partlet) use 3 transformer [Vaswani et al. 2017] blocks with multi-head crossattention, LayerNorm, residual connections, and feedforward layers. The BiCo fusion employs sparse 16-NN attention with 3D relative positional bias, computed via learned MLP over Fourier-encoded (F=6 frequencies) displacement vectors, which provides geometric context while maintaining (ğ‘ ğ‘˜) complexity. We also note that for calculating the runtime, we do not include data preprocessing time, as it varies depending on parallelization and system capabilities. We set the number of Partlets to 32, as this value provides reasonable estimate for the typical number of semantic parts found in most objects in our unified dataset. This choice is further validated by analyzing the statistics of part counts across the full dataset, 7 Choice of Text Encoder We adopt MPNet [Song et al. 2020] as our text encoder for part descriptions rather than CLIP / SigLIP due to its superior structure-preserving properties for sentencelevel embeddings. SigLIP, optimized for short image captions (e.g., \"A photo of dog\"), exhibits pathologically high cosine similarities across semantically distinct part descriptions generated by Gemini, undermining the discriminative structure necessary for partlet-based learning. In Fig. 4 for instance, MPNet correctly assigns high similarities (>0.8) to functionally equivalent parts across classes - such as wheels (airplane, car, bicycle, wheelchair), doors (airplane, car), and handles (scooter, bicycle, wheelchair), while maintaining low similarities (<0.3) between parts with different affordances, such as tires vs. doors/windows or pedals vs. airplane components. In contrast, SigLIP assigns uniformly high similarities to both sets, collapsing the semantic space and preventing our partlets from learning meaningful text-conditioned part alignment during training. Why affordance descriptions? key motivation for incorporating affordance information into part annotations is rooted in the cognitive science understanding that humans interpret and define object parts not just by geometry, but by their function, context, and description. Short or generic part names (e.g., leg, handle) are often ambiguous across different objects, lacking any semantic detail regarding the role or meaning of part within specific context. For example, legs fulfill distinct structural functions and take on different forms for chairs, tables, or sofas, distinction that arises from their object-specific affordances. Prior work demonstrates that affordance-based cues and descriptive information guide human part recognition, reducing label ambiguity and supporting more robust reasoning and communication. Thus, by situating part annotations within functional and contextual descriptions, our approach enables higher-quality, less ambiguous labeling, consistent with cognitive models of human object understanding. Datasets. We train on 40,982 shapes from three datasets: 3DCoMPaT++ (8,627), PartNet (32,141), and Find3D (124). All use finegrained part labels. For evaluation, we hold out 206 shapes: 126 objects (42 categories) from 3DCoMPaT++, 72 objects (24 categories) from PartNet, and 8 novel objects (8 categories) from Find3D."
        },
        {
            "title": "4 Experiments\nIn this section, we discuss our methodology for unifying 3D part\nbenchmarks with inconsistent part naming conventions for the\nsame object classes ( Section 4.1), introduce our baselines and met-\nrics for the named 3D part segmentation task ( Section 4.2), and\nshow qualitative and quantitative comparison of ALIGN-Parts with\nrelated baselines ( Section 4.3). We ablate different components and\ninference modes of ALIGN-Parts in Section 4.4 and Section 4.5. We\nend the section with 2 applications of our method in Section 4.6\nand Section 4.7.",
            "content": "4.1 3D Part Annotation Alignment &"
        },
        {
            "title": "Vocabulary Compression",
            "content": "Our annotation alignment and vocabulary compression system employs two-stage pipeline that combines MPNet embeddings for candidate generation and Gemini LLM verification to reject spurious matches as well as identify and merge duplicate classes and parts across unified 3D object taxonomy. For confirmed matches, the system successfully identifies semantically equivalent entities with high MPNet similarity scores that Gemini validates as identical: for example, \"laptop_computer\" and \"laptop\" (similarity: 0.944) are merged because Gemini recognizes that \"both candidate names refer to the exact same physical device... consistently define it as portable personal computer designed for mobile use\". Similarly, microwave_oven (3DCoMPaT) and microwave (PartNet) with similarity 0.902 are merged after Gemini confirmed they \"describe the same kitchen appliance.\" Within the \"microwave_oven\" class, \"door_glass\" and \"glass\" (similarity: 0.865) are unified because Gemini concludes \"both descriptions refer to the transparent panel integrated into the door... that allows viewing food and contains radiation. The secondary part name glass is concise reference to the \"door_glass\". Similarly, \"bed_footboard\" and \"footboard\" (similarity: 0.953) are merged as Gemini states, \"bed_footboard\" is more explicit naming of footboard, and their descriptions are semantically identical, describing panel at the foot of the bed opposite the headboard.\" For rejected pairs, the system correctly distinguishes semantically distinct parts despite high embedding similarity: \"car_front_bumper\" and \"car_rear_bumper\" (similarity: 0.879) are kept separate because Gemini determines \"while both parts are bumpers with the same protective function, their specified locations (front vs. rear) make them distinct semantic parts for 3D car object,\" and within the \"chair\" class, \"back_frame_horizontal_rod\" and \"back_frame_vertical_rod\" (similarity: 0.943) remain separate because Gemini explains \"the parts are distinct based on their orientation within the back frame: one is explicitly described as horizontal rod providing reinforcement for the backrest, while the other is vertical rod providing structural support.\" The compressed vocabulary output maintains canonical names (choosing Paul and Kaushik et al. more verbose/descriptive variants), aggregates part counts across merged entities, and produces mapping log that records every alias resolution for downstream lookup when legacy names are encountered during inference. This compressed vocabulary enables training with unified part semantics."
        },
        {
            "title": "Segmentation",
            "content": "Baselines. While no prior work addresses named 3D part segmentation end-to-end, we compare against two recent baselines: PartField [Liu et al. 2025] and Find3D [Ma et al. 2024]. PartField achieves state-of-the-art class-agnostic segmentation but cannot name parts. It clusters learned feature field (requiring groundtruth part count K). Find3D matches per-point features to SigLIP embeddings of provided part queries, needing the list of groundtruth part names as input. PartField+MPNet (Our Strong Baseline; w/o partlets). We extend PartField with semantic alignment: linear head maps PartField features to shared space with MPNet text embeddings, trained via InfoNCE loss. However, it still requires predicting via an auxiliary classifier, making it brittle to over/under-segmentation from prediction errors. Further architecture details are provided in Section A.1. Metrics. We evaluate our method using three complementary metrics that progressively incorporate semantic label correctness. (1) Class-agnostic mIoU: Following prior work [Liu et al. 2025; Yang et al. 2024], for each ground-truth part, we compute the maximum IoU across all predicted segments and average these values, ignoring semantic labels entirely - this captures pure geometric segmentation quality. (2) Label-Aware mIoU (strict, LA-mIoU): For each groundtruth part, we identify the predicted segment with the highest geometric overlap (as in class-agnostic mIoU), then assign credit only if its semantic label exactly matches the ground truth; otherwise, the part contributes 0.0 - this measures joint geometry-semantic accuracy with strict label matching. (3) Relaxed Label-Aware mIoU (rLAmIoU): Identical segment selection to strict LA-mIoU, but instead of binary label matching, we weight the IoU by the cosine similarity between MPNet text embeddings of predicted and ground-truth labels, giving partial credit to semantically related predictions (e.g., \"screen\" vs. \"monitor\") - this captures semantic near-misses that strict matching penalizes. By construction, we have class-agnostic mIoU rLA-mIoU LA-mIoU (strict), where equality holds only when all predictions have perfect label agreement. The gap between class-agnostic and label-aware metrics reveals semantic prediction errors, while the gap between strict and relaxed variants quantifies label confusion on semantically similar parts. Correlation plots (Fig 6 and 7) empirically validate that relaxed scoring recovers significant semantic credit on near-miss predictions."
        },
        {
            "title": "4.3 Results\nIn Figures 5 and 8 and Table 2, we show that ALIGN-Parts compre-\nhensively outperforms all baselines on both class-agnostic segmen-\ntation (mIoU) and named part segmentation (LA-mIoU). On average\nmIoU, we outperform PartField by 15.8%, whereas on LA-mIoU and\nrLA-mIoU, we improve over PartField+MPNet by 58.8% and 43.8%,",
            "content": "8 Name That Part Figure 5: Qualitative Results. ALIGN-Parts segments and names 3D parts robustly in single feed-forward pass (rightmost column). Find3D [Ma et al. 2024] (first column) fails despite ground-truth part names, unable to segment the laptop in Figure 5 (bottom row). PartField [Liu et al. 2025] (second column) also fails: it requires ground-truth part counts for clustering, missegments bed bunks (top row), and misses refrigerator handles (second-to-last row). Our strong baseline without Partlets (third column) exhibits similar errors. In contrast, ALIGN-Parts correctly segments tiny parts, such as handles, and groups semantically similar instances (e.g., all ceiling fan blades into single cluster). Table 2: Evaluation of ALIGN-Parts and related baselines on our test set. In addition to the usual mIoU metric for evaluating class-agnostic part segmentation, we introduce 2 metrics more suited for our named part-segmentation task, namely, labelaware mIoU (LA-mIoU) and relaxed label-aware mIoU (rLA-mIoU). Variant 3DCoMPaT Find3D PartNet Average mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU PartField Find3D PartField+MPNet ALIGN-Parts 0.371 0.239 0.316 0.453 n/a 0.072 0.185 0.268 n/a 0.178 0.259 0.391 0.662 0.379 0.590 0.595 n/a 0.232 0.137 0. n/a 0.324 0.451 0.466 0.521 0.354 0.446 0.753 n/a 0.204 0.276 0.546 n/a 0.298 0.394 0.729 0.518 0.324 0.451 0.600 n/a 0.169 0.199 0. n/a 0.267 0.368 0.529 respectively, while being 100 faster because we do not require running K-means clustering algorithm. Qualitatively, our baselines show several weaknesses. PartField often fragments instances of the same part into multiple segments after clustering, contradicting human labeling conventions in 3DCoMPaT++ and PartNet. This likely stems from its use of SAM [Kirillov et al. 2023] to extract unlabeled parts from 2D renderings. In contrast, ALIGN-Parts, trained on human annotations, correctly groups instances: e.g., all four bed posts of double bed and all three wheels of an airplane are identified as single parts (Figure 8). Find3D learns per-point semantic vectors without considering shape geometry, resulting in noisy and overlapping segmentations. PartField+MPNets reliance on predicted cluster counts leads to under-segmentation, as fine parts such as the refrigerator handle and laptop screen frame are missed (Figure 8). Both Find3D and PartField fail to segment even simple structures, such as fan blades, whereas ALIGN-Parts accurately segments and annotates fine-grained parts in complex shapes. Runtime Comparison. In terms of runtime, our method compares favorably against all baselines. Find3D runs in 0.25ğ‘ , whereas both PartField and PartField+MPNet require 4ğ‘ , where the majority of the runtime is consumed by K-means clustering. ALIGN-Parts, being one-shot feedforward method, needs only 0.05ğ‘  (barring feature pre-processing) to produce labelled 3D parts. Fine-Part Localization Despite using only 10k sampled points, ALIGN-Parts segments fine parts like the screw of scissors (Fig 8) - structures that PartField, trained with 100k points, cannot localize. Paul and Kaushik et al. Table 3: Ablation study on 3DCoMPaT, Find3D, and PartNet. We report mIoU, label-aware mIoU (LA-mIoU), and relaxed label-aware mIoU (rLA-mIoU). Variant 3DCoMPaT Find3D PartNet Average mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU Base Model No Lcov, Lov, Ltxt No Lcov, Lov Geo Input Only Feature Concat PartField+MPNet ALIGN-Parts 0.228 0.233 0.422 0.224 0.221 0.316 0. 0.021 0.019 0.193 0.014 0.017 0.185 0.268 0.141 0.140 0.338 0.134 0.134 0.259 0.391 0.374 0.382 0.499 0.384 0.367 0.590 0.595 0.041 0.003 0.110 0.027 0.058 0.137 0.133 0.239 0.198 0.384 0.187 0.239 0.451 0.466 0.335 0.357 0.664 0.332 0.317 0.446 0. 0.027 0.041 0.414 0.040 0.033 0.276 0.546 0.201 0.223 0.609 0.210 0.194 0.394 0.729 0.312 0.324 0.528 0.313 0.302 0.451 0.600 0.030 0.021 0.239 0.027 0.036 0.199 0.316 0.194 0.187 0.443 0.177 0.189 0.368 0.529 Figure 6: Proposed Metrics Correlation Analysis. Correlation analysis between our proposed label-aware mIoU metrics (strict/relaxed) and class-agnostic mIoU, computed on segmentation results from our ALIGN-Parts model. The strict label-aware metric (left) shows moderate agreement with class-agnostic mIoU (Pearson ğ‘Ÿ = 0.739, Spearman ğœŒ = 0.730, ğ‘ = 206), while the relaxed variant (right) demonstrates nearperfect correlation (Pearson ğ‘Ÿ = 0.978, Spearman ğœŒ = 0.974, ğ‘ = 206). These findings indicate that our model achieves strong semantic and quantitative consistency, further supporting the use of the relaxed metric as robust evaluation protocol for semantic 3D part segmentation."
        },
        {
            "title": "4.4 Ablations\nTable 3 and Figure 9 evaluate ALIGN-Partsâ€™s design choices, show-\ning quantitative and qualitative improvements from each compo-\nnent.\nBaseline Comparisons. Using only PartField geometric features,\nour base model (Lmask, Lpart, Lglobal) performs slightly better than\nnaive concatenation of PartField and DINOv2 features, indicating\nthat simple multi-modal fusion can be counterproductive. A variant\nusing raw geometry without PartField shows no improvement,\nconfirming that learned geometric features are essential.\nProgressive Component Addition. Adding DINOv2 appearance\nfeatures yields modest gains. Incorporating the InfoNCE text align-\nment loss (Ltxt) significantly improves both LA-mIoU and rLA-\nmIoU; this variant is the only one, besides our full model, that\ncorrectly segments fine-grained parts, such as plants in vases or\nshelves in storage furniture. Finally, adding auxiliary regulariz-\ners (Lcov, Loverlap) yields the complete ALIGN-Parts model, which\nachieves peak performance both qualitatively and quantitatively.",
            "content": "Figure 7: Label Mismatches and Metric Robustness. Distribution of test objects according to the number of label mismatches (x-axis) and the resulting increase in mIoU from strict to relaxed label-aware matching (y-axis), colored by class-agnostic mIoU. Most objects with low mismatches exhibit small or moderate increases in mIoU, while objects with higher mismatch counts still do not show large outliers in metric difference, supporting the robustness of our evaluation protocol. The absence of extreme discrepancies suggests that the relaxed metric yields stable and meaningful improvement, even for challenging cases, confirming its reliability for assessing semantic part segmentation on our test set. Figure 8: ALIGN-Parts correctly segments the tiny screw despite training with sparser points (10k vs. 100k)."
        },
        {
            "title": "4.5 Inference-time Ablations\nWe evaluate two additional inference modes of our ALIGN-Parts\nmodel, extending beyond the primary dynamic part activation ap-\nproach to better understand the contributions of part cardinality",
            "content": "10 Name That Part Figure 9: Ablation (Qualitative results). Results improve from left to right as components are added sequentially, with ground truth in the final column. The first five columns result in significant misalignments and segmentation leakage. Neither geometric-only features nor naive DINO concatenation improves performance. Major gains arise from Partlets (see sixth and seventh columns) and the coverage loss, which refine fine details and are consistent with quantitative metrics in Table 3. Table 4: Evaluation of different inference modes of ALIGN-Parts on our test set, using mean IoU (mIoU), label-aware mIoU (LA-mIoU), and relaxed label-aware mIoU (rLA-mIoU). Providing additional ground-truth part count information only slightly improves the models performance on rLA-mIoU showing that ALIGN-Parts often estimates accurate part cardinality based on just the input geometric and appearance features of 3D shape. Variant Clustering + Part Number +Part Number No Part Number 3DCoMPaT (126) Find3D (8) PartNet (72) Average mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU mIoU LA-mIoU rLA-mIoU 0.370 0.452 0. n/a 0.268 0.268 n/a 0.389 0.391 0.528 0.625 0.595 n/a 0.138 0.133 n/a 0.473 0.466 0.537 0.757 0. n/a 0.559 0.546 n/a 0.737 0.729 0.478 0.611 0.600 n/a 0.322 0.316 n/a 0.533 0.529 and label information in our segmentation pipeline. The first alternative mode, which we term the clustering+part number setting, completely forgoes the use of any part vocabularies or text labels during inference. Instead, it relies solely on the fused geometric and appearance features output by the model, upon which we run k-means clustering to produce purely class-agnostic instance clusters. This setup rigorously probes the ability of the learned feature embeddings, untethered to semantic labels, to support coherent part decompositions across diverse objects, essentially isolating the impact of visual and geometric cues alone. The second mode, called +Part number, examines whether providing the model with the exact ground-truth part count for each input shape improves segmentation quality compared to the default setting, where the model dynamically infers the number of parts to activate. After producing all candidate partlet masks and calculating their partness scores, this mode ranks the partlets by saliency score, which is composite measure combining the confidence that partlet corresponds to an actual part (i.e., partness) and the average mask coverage over the point cloud (mean mask probability mass over points). From this ranking, the top ğ‘€ partlets are retained, where ğ‘€ is the true number of parts for the target shape, and every point in the shape is assigned the best matching mask among these selected partlets to yield hard ğ¾-way partition. These inference ablation modes and their qualitative and quantitative outcomes are detailed and visualized in Fig 10 and Tab 4, demonstrating that ALIGN-Parts is able to robustly estimate accurate part cardinality and segmentation even without explicit part label or count guidance, and that the fused multimodal features alone provide meaningful cues towards coherent part delineation. This analysis not only highlights the flexibility and robustness of ALIGN-Parts at inference time, but also emphasizes the benefits of its design choices in learning and leveraging rich feature representations supporting both semantic and instance-aware part segmentation. 4.6 3D Shape Co-Segmentation and Part Label"
        },
        {
            "title": "Transfer",
            "content": "Fig. 11 shows results and analysis of 3D Shape Co-Segmentation using ALIGN-Parts (and the BiCo features) as compared to PartField. classical approach to 3D part segmentation operates in 11 Paul and Kaushik et al. Figure 10: Inference-time Ablations. We assess two additional inference modes in our ALIGN-Parts model. The Clustering+Part number mode disregards text vocabularies and applies k-means to the fused geometric and appearance features, generating class-agnostic clusters and probing how well raw feature representations alone support meaningful part segmentation. The +Part number mode uses the true part count for each shape, activating exactly ğ‘€ partlets with the highest saliency scores - computed by combining partness confidence and average mask coverage - and assigning every point to its best matching mask; this tests whether supplying ground-truth cardinality adds value compared to dynamic, data-driven part discovery. Both setups are compared to the default dynamic activation (No Part number) and the ground truth, highlighting that ALIGN-Parts robustly estimates part cardinality and produces accurate segmentations even without explicit part label or count supervision. co-segmentation setting, where multiple shapes from the same category are jointly analyzed to establish consistent part correspondence. Prior methods, including PartField, employ what we call dependent clustering: they first segment source shape using feature clustering, then initialize k-means clustering on target shape using the source cluster centroids, implicitly enforcing part correspondence. While this strategy can succeed on geometrically similar shapes, it proves fragile when target shapes exhibit different part counts or topologies. For example, in Figure 11, dependent clustering on moderately similar target chair (Target A) produces reasonable results, but fails dramatically on targets with substantial part variation (Target B), causing geometric boundaries to blur (e.g., the backrest merging incorrectly with the seat). An alternative, independent clustering approach segments each target shape autonomously and then matches clusters post hoc by comparing source and target cluster centers. As shown in Target C, this mode is more robust to topological differences, though it forgoes any direct geometric correspondence to the source. In contrast to both clustering-based paradigms, our proposed ALIGN-Parts adopts fully feedforward, discriminative approach that predicts part segmentation masks and semantic labels jointly, without requiring source shape initialization or explicit co-segmentation. This design eliminates brittleness to part count variation and geometric mismatches, enabling robust generalization across shapes with diverse part structures and semantics. As demonstrated in Figure 11 (middle and right panels), ALIGN-Parts consistently produces accurate, semantically grounded part segmentations regardless of target shape complexity."
        },
        {
            "title": "4.7 TexParts Dataset\nA central aim of our approach is to enable the construction of\na high-quality 3D part annotation dataset with minimal manual\nintervention, ensuring both unified and comprehensive part la-\nbeling at scale. For this purpose, we select the TexVerse dataset\nas our unannotated 3D source corpus, leveraging its exceptional\nquality, high-resolution textures, and extensive diversity of 3D as-\nsets [Zhang et al. 2025]. TexVerse consists of over 850,000 unique\n3D models with physically based rendering (PBR) materials and\nrich metadata, making it an ideal foundation for large-scale part\nsegmentation.",
            "content": "Our pipeline begins with the automated filtering of TexVerse models: using Gemini-Flash LLM, we combine thumbnail images and other metadata to preselect high-quality objects and exclude inadequate or malformed models. Next, we apply our ALIGN-Parts model and save, for each shape, its predicted part masks, part names, and both semantic and segmentation confidence scores. To prioritize downstream annotation effort, we sort objects by their average confidence score (in descending order) so that annotators see the 12 Name That Part Figure 11: 3D Shape Co-Segmentation Analysis. Left: clustering-based co-segmentation. Prior methods such as PartField perform dependent clustering by first segmenting source shape via feature clustering and then using the resulting cluster means to initialize K-means on target shape, implicitly enforcing part correspondence; this can break when the target has different part count or geometry, causing errors such as the red backrest region bleeding into the seat on the target chair. Using the same dependent co-segmentation strategy with our fused BiCo features (BiCo Feature Dependent Clustering) yields improved transfers on moderately similar targets (Target A), but performance degrades on more challenging targets with greater variation in part structure (Target B). As an alternative, we apply independent clustering to Target C, where the target is segmented with source initialization and clusters are matched post hoc by comparing source and target cluster centers, which proves more reliable for difficult co-segmentation cases. Middle and right: feedforward ALIGN-Parts. In contrast to all clustering-based variants, the proposed feedforward ALIGN-Parts model (middle) directly predicts part segmentation and names, achieving robust results across shapes with differing part counts and topologies, and eliminating any dependence on source shapes or explicit co-segmentation. most reliable candidates first. Selected objects are then routed to human annotators for validation and correction. During annotation, annotators use several aids: part name prompting tool for searching or extending the active part vocabulary, and (optionally) the ability to reference unlabeled geometric part masks generated by PartField. Our annotation process is explicitly bilevel; Phase One focuses on validating and making minor edits to ALIGN-Parts predictions, while Phase Two addresses new or missing parts that require more substantive manual annotation. By the time of submitting this work, the first phase has covered approximately 8,000 objects, comprising around 14,000 unique part categories. Examples from the current dataset are shown in Figure 12. key observation from our annotation workflow is the dramatic reduction in manual effort enabled by our methodology: annotating 3D objects from scratch typically takes anywhere from 15 to 25 minutes per shape, while our model-assisted pipeline reduces annotation time to just 3 to 5 minutes on average - time saving of approximately 58 without sacrificing annotation quality. Importantly, and in clear contrast to recent approaches that keep their Objaverse-derived part annotations closed-source, we commit to releasing TexParts as public resource upon publication, with the aim of advancing large-scale open research in semantic 3D part understanding."
        },
        {
            "title": "5 Limitations and Future Work\nThe primary limitations of our work stem from the relatively re-\nstricted set of objects and parts on which ALIGN-Parts has been",
            "content": "13 Paul and Kaushik et al. Figure 12: TexParts Dataset. We demonstrate human-in-the-loop annotation of Texverse [Zhang et al. 2025] using ALIGN-Parts, enabling scalable dense 3D part segmentation. learn geometry and semantics without part count supervision. Unlike Find3Ds per-point alignments or PartFields brittle clustering, our end-to-end matching produces fast, coherent, named parts directly. LLM-generated affordance descriptions (\"door handle is grasped to open door\") are important for disambiguating finegrained parts that confuse simple part names, and MPNets handling of long-form text outperforms CLIP/SigLIP. Despite using 10 fewer points than PartField, we achieve superior fine-part localization (e.g., scissors screw), confirming that semantic part-level representations are more data-efficient than dense per-point features. Key limitations are: noisy real-world scans challenge our manifold assumptions, Mahalanobis confidence degrades under distribution shift, and open-vocabulary generalization is limited to categories similar to the training data. Future work should extend this to articulated objects and integrate part-level alignments into foundation 3D models for manipulation and generation. By bridging dense geometry and structured language, ALIGN-Parts enables scalable, semantically rich 3D asset creation. trained, compared to the vast (though finite) variety of parts that occur in the real world. This gap is largely due to the scarcity of large-scale 3D datasets with dense part annotations and unified, operational definition of what constitutes part. In effect, this creates chicken-and-egg problem: ALIGN-Parts was designed to enable robust 3D part annotation at scale, yet the robustness and coverage of the model itself are constrained by the limited annotated data available for training. Future work will focus on mitigating this dependency by exploring self-supervised or weakly supervised formulations and by incorporating stronger 3D priors, for example from generative models or skeletal/medial representations. Another important direction is to reduce the current reliance on frozen PartField features by enabling full end-to-end training of the geometric feature extractor, which was not pursued here primarily due to computational constraints rather than methodological ones. Despite these limitations, our framework is immediately usable by organizations and research labs with abundant compute and proprietary 3D assets, who can scale ALIGN-Parts to richer, closed-source datasets and drive progress towards truly large-scale 3D scene understanding at the part level."
        },
        {
            "title": "6 Discussion\nALIGN-Parts reframes semantic 3D part segmentation as a set align-\nment problem, where Partlets trained via optimal transport jointly",
            "content": "14 compositional recognition. arXiv preprint, 2023. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-training for language understanding. Advances in neural information processing systems, 33:1685716867, 2020. Karl Stelzner, Kristian Kersting, and Adam R. Kosiorek. Decomposing 3d scenes into objects via unsupervised volume segmentation. ArXiv, abs/2104.01148, 2021. AyÃ§a Takmaz, Elisabetta Fedele, Robert W. Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. In Advances in Neural Information Processing Systems (NeurIPS), 2023. George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh. arXiv preprint arXiv:2408.13679, 2024. Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing. arXiv preprint arXiv:2506.09980, 2025. Shubham Tulsiani, Hao Su, Leonidas J. Guibas, Alexei A. Efros, and Jitendra Malik. Learning shape abstractions by assembling volumetric primitives. In Computer Vision and Pattern Regognition (CVPR), 2017. Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, and Yen-Yu Lin. Partdistill: 3d shape part segmentation by vision-language model distillation. In IEEE/CVF International Conference on Computer Vision (CVPR), 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025. Yibo Zhang, Li Zhang, Rui Ma, and Nan Cao. Texverse: universe of 3d objects with high-resolution textures. arXiv preprint arXiv:2508.10868, 2025. Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. Partslip++: Enhancing low-shot 3d part segmentation via multi-view instance segmentation and maximum likelihood estimation. arXiv preprint arXiv:2312.03015, 2023. Name That Part References Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with multi-view diffusion models. arXiv preprint arXiv:2412.18608, 2024. Minghao Chen, Jianyuan Wang, Roman Shapovalov, Tom Monnier, Hyunyoung Jung, Dilin Wang, Rakesh Ranjan, Iro Laina, and Andrea Vedaldi. Autopartgen: Autogressive 3d part generation and discovery. arXiv preprint arXiv:2507.13346, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Jonathan Connell and Michael Brady. Generating and generalizing models of visual objects. Artificial Intelligence, 31(2):159183, 1987. David F. Crouse. On implementing 2d rectangular assignment algorithms. Transactions on Aerospace and Electronic Systems, 52(4):16791696, 2016. Niladri Shekhar Dutt, Sanjeev Muralikrishnan, and Niloy Mitra. Diffusion 3d features (diff3f): Decorating untextured shapes with distilled semantic features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44944504, 2024. IEEE Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, and He Wang. Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70817091, 2023. Aleksey Golovinskiy and Thomas Funkhouser. Randomized cuts for 3d mesh analysis. ACM Trans. Graph., 27(5), 2008. Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman, and Daniel Cohen-Or. Meshcnn: network with an edge. ACM Transactions on Graphics (TOG), 38(4):90:190:12, 2019. R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy Mitra, and Daniel Ritchie. Shapeassembly: Learning to generate programs for 3d shape structure synthesis. ACM Transactions on Graphics (TOG), Siggraph Asia 2020, 39(6):Article 234, 2020. Evangelos Kalogerakis, Aaron Hertzmann, and Karan Singh. Learning 3D Mesh Segmentation and Labeling. ACM Transactions on Graphics, 29(3), 2010. Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, and Siddhartha Chaudhuri. 3D Shape Segmentation with Projective Convolutional Networks . In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 66306639, Los Alamitos, CA, USA, 2017. IEEE Computer Society. Hyunjin Kim and Minhyuk Sung. Partstad: 2d-to-3d part segmentation task adaptation. In ECCV, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr DollÃ¡r, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers, 2025. Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2173621746, 2023. Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. arXiv preprint arXiv:2504.11451, 2025. Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation, 2025. Ziqi Ma, Yisong Yue, and Georgia Gkioxari. Find any part in 3d. arXiv preprint arXiv:2411.13550, 2024. Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. Maxime Oquab, TimothÃ©e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Lior Shapira, Ariel Shamir, and Daniel Cohen-Or. Consistent mesh partitioning and skeletonisation using the shape diameter function. Vis. Comput., 24(4):249259, 2008. Habib Slim, Xiang Li, Yuchen Li, Mahmoud Ahmed, Mohamed Ayman, Ujjwal Upadhyay, Ahmed Abdelreheem, Arpit Prajapati, Suhail Pothigara, Peter Wonka, and Mohamed Elhoseiny. 3DCoMPaT++: An improved large-scale 3D vision dataset for 15 Paul and Kaushik et al. attention mechanisms utilize dropout regularization (p = 0.1) to prevent overfitting during training. During inference, PartField + MPNet first predicts object category by comparing the projected global feature against all class embeddings, then performs soft kmeans clustering (ğ‘˜ from the part count head) on fused point-level features with Hungarian matching to assign semantic labels by computing cosine similarity between projected cluster centroids and MPNet embeddings of candidate part names. A.2 Part-Retrieval Comparison with Find3D Beyond semantic segmentation, ALIGN-Parts also supports textdriven part retrievalthe task of localizing and retrieving point cloud regions corresponding to natural language part queries. This capability, introduced by Find3D, enables flexible, open-vocabulary part discovery directly from unstructured text descriptions. Our approach performs retrieval by constraining the candidate label vocabulary to only those parts known to be present in the target object class, rather than the full semantic vocabulary. Additionally, we set the number of active partlets to match the ground-truth part count for the object, which serves as an oracle constraint. While this restriction reduces the search space and assignment ambiguity, allowing the model to match predicted part slots to small, object-specific set of valid labels rather than choosing from dozens of candidates, it also enables fairer and more interpretable comparisons. This constrained retrieval setup typically yields higher segmentation accuracy by minimizing false positive label assignments and focusing the models attention on semantically coherent parts. The key advantage of our approach lies in the compositional three-level hierarchy: point cloud partlet part label. This formulation naturally encourages the discovery of connected point groups with consistent semantic meaning, whereas alternatives may suffer from fragmentation or over-segmentation. We present qualitative comparisons with Find3D on two representative 3D objects from the airplane and motorbike object classes in the Objaverse-General benchmark (part of our closed-vocabulary evaluation set), released by the Find3D authors. As shown in Figure 13, ALIGN-Parts consistently retrieves more spatially coherent and semantically meaningful part groups, demonstrating the effectiveness of our partlet-based design for part localization and retrieval tasks. Experiments and Analysis Given the challenges inherent in semantic 3D part segmentation, we find that no current published work is directly comparable to our method. To enable rigorous evaluation, we introduce our own strong baseline detailed in Section A.1. While we do include comparisons against class-agnostic 3D part segmentation methods in this manuscript, it is important to note that these do not constitute an entirely fair benchmark for our approach. Most prior methods have been trained using proprietary, closed-source Objaverse-scale datasets, with specific data details and part annotations rarely disclosed publicly. In contrast, our experiments are conducted on fully open, publicly available datasets, and our methodology itself improves upon these resources, making our results more easily reproducible and comparable for future researchers. Furthermore, key emphasis of our approach is efficiency: we process only 10,000 input points per shape, in stark contrast to the 100,000 points typically used by class-agnostic segmentation baselines. This restriction stems from the academic compute limitations we faced, while prior works often benefit from corporate-scale GPU resources. Despite these constraints, our method achieves competitive or superior performance relative to existing baselines. It is reasonable to expect that - if provided with similar data volume and computational resources - ALIGN-Parts would further extend its advantage on standard metrics and benchmarks. Our design choices thus not only democratize research in 3D part segmentation but also highlight the promise of reproducibility, accessibility, and efficiency for large-scale semantic understanding in open 3D datasets. A.1 PartField+MPNet baseline Given that our task of semantic part segmentation (in contrast to the relatively easier and more prominent class agnostic part segmentation, we create our own baseline - PartField + MPNet, which assigns labels to parts obtained by KMeans clustering on per-point features. We experimented with two variants of this model, in terms of input features: PartField and PartField + DINOV2, and found that the latter usually yields much better performance. So, without loss of generality, our baseline PartField + MPNet refers to the model where we have per-part PartField + DINOv2 features fused through cross-attention. Specifically, we employ cross-attention fusion to combine per-part geometric (448-D) and appearance (768-D) features, projecting them through 512-D hidden layers into shared 256-D latent space. The architecture consists of dense feature fuser (2.8M parameters) with 4 attention heads operating at 512-D, followed by dedicated MLP projectors for local part features (0.39M), semantic text embeddings (0.52M), and global shape descriptors (0.75M), totaling approximately 5.1M parameters. Training optimizes three objectives: symmetric InfoNCE loss for local part-text alignment, global-level contrastive loss between shape and class embeddings, and cross-entropy clustering loss that predicts part counts with equal weighting (ğœ†=1.0) across all terms. The model is trained for 100 epochs using AdamW with learning rate 3e-4, weight decay 1e-5, and cosine annealing schedule (ğœ‚min=5e-6) with batch size 64. The part count prediction head (0.63M parameters) uses two-layer MLP with GELU activation to classify the number of semantic parts from fused global features. All projectors and Name That Part Figure 13: Part Retrieval Comparison with Find3D. We demonstrate text-driven part retrieval on two representative objects (airplane and motorbike) from Objaverse-General. Given natural language part queries (e.g., body, wing, gas tank, wheel), ALIGN-Parts identifies and retrieves spatially coherent point groups corresponding to each part. Compared to Find3D (left), our method produces more semantically and spatially consistent part retrievals by leveraging the hierarchical point partlet part label decomposition. This design encourages the discovery of well-connected, semantically meaningful regions rather than fragmented point clusters. Ground truth part segmentations (right) show the target labels. ALIGN-Parts achieves results that closely align with ground truth, validating the effectiveness of our partlet-based formulation for open-vocabulary part localization and retrieval."
        }
    ],
    "affiliations": [
        "Johns Hopkins University"
    ]
}