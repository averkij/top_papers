{
    "paper_title": "Improving Autoregressive Image Generation through Coarse-to-Fine Token Prediction",
    "authors": [
        "Ziyao Guo",
        "Kaipeng Zhang",
        "Michael Qizhe Shieh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quantization errors that existed in VQ-VAE, recent works tend to use larger codebooks. However, this will accordingly expand vocabulary size, complicating the autoregressive modeling task. This paper aims to find a way to enjoy the benefits of large codebooks without making autoregressive modeling more difficult. Through empirical investigation, we discover that tokens with similar codeword representations produce similar effects on the final generated image, revealing significant redundancy in large codebooks. Based on this insight, we propose to predict tokens from coarse to fine (CTF), realized by assigning the same coarse label for similar tokens. Our framework consists of two stages: (1) an autoregressive model that sequentially predicts coarse labels for each token in the sequence, and (2) an auxiliary model that simultaneously predicts fine-grained labels for all tokens conditioned on their coarse labels. Experiments on ImageNet demonstrate our method's superior performance, achieving an average improvement of 59 points in Inception Score compared to baselines. Notably, despite adding an inference step, our approach achieves faster sampling speeds."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 4 9 1 6 1 . 3 0 5 2 : r Improving Autoregressive Image Generation through Coarse-to-Fine Token Prediction Ziyao Guo1 Kaipeng Zhang2, Michael Qizhe Shieh1, 1National University of Singapore 2Shanghai AI Lab"
        },
        {
            "title": "Abstract",
            "content": "marks [8, 13, 20, 29, 31]. Autoregressive models have shown remarkable success in image generation by adapting sequential prediction techniques from language modeling. However, applying these approaches to images requires discretizing continuous pixel data through vector quantization methods like VQ-VAE. To alleviate the quantization errors that existed in VQ-VAE, recent works tend to use larger codebooks. However, this will accordingly expand vocabulary size, complicating the autoregressive modeling task. This paper aims to find way to enjoy the benefits of large codebooks without making autoregressive modeling more difficult. Through empirical investigation, we discover that tokens with similar codeword representations produce similar effects on the final generated image, revealing significant redundancy in large codebooks. Based on this insight, we propose to predict tokens from coarse to fine (CTF), realized by assigning the same coarse label for similar tokens. Our framework consists of two stages: (1) an autoregressive model that sequentially predicts coarse labels for each token in the sequence, and (2) an auxiliary model that simultaneously predicts fine-grained labels for all tokens conditioned on their coarse labels. Experiments on ImageNet demonstrate our methods superior performance, achieving an average improvement of 59 points in Inception Score compared to baselines. Notably, despite adding an inference step, our approach achieves faster sampling speeds. Code and model weights will be released in https://github.com/ GzyAftermath/CTF. 1. Introduction Autoregressive models have demonstrated remarkable performance in language modeling by sequentially predicting the next token in sequence [2, 25, 34]. Recently, this next token prediction approach has been successfully extended to image generation tasks [19, 32, 33], achieving results that surpass diffusion-based methods on several bench- *Corresponding authors. The application of autoregressive modeling to images presents fundamental challenge: unlike language data, which is inherently discrete, images consist of continuous pixel values. Vector quantization techniques such as VQVAE [37] address this challenge through two-step process: first, an encoder network compresses images into lowerdimensional feature maps; second, vector quantization discretizes these features by mapping each continuous feature vector to the nearest codeword in learned codebook. This enables representing continuous images as discrete token sequences, which are suitable for autoregressive generation. Despite the effectiveness of VQ-VAE, its discretization process introduces quantization errors that limit reconstruction quality and, consequently, the fidelity of generated images. To alleviate this issue, recent approaches have dramatically increased codebook sizesfrom 1,024 entries [28, 37] to 16,384 [32], and even up to 262,144 [21, 41]. While larger codebooks enhance reconstruction accuracy, they significantly complicate the autoregressive generation process since the vocabulary size is expanded accordingly, making the prediction task considerably more difficult. As result, overall generation quality may not improve proportionally and can sometimes deteriorate [41]. This creates fundamental tension: can we enjoy the benefits of large codebooks for high-quality reconstruction while keeping the complexity of autoregressive modeling tasks manageable? To address this question, we first investigate whether exact token-level predictions are as essential in image generation as they are in language modeling. Unlike language, where precise token predictions are critical for coherence, we observe that substituting image tokens with others corresponding to similar codewords results in only minor visual differences (Figure 1(b)). This insight reveals significant redundancy in large codebookstokens with similar codeword representations produce similar effects on the generated image, yet current methods treat these closely related tokens as entirely distinct classes. This unnecessary complexity explains why even state-of-the-art autoregressive image generation methods [33] with billions of parameters achieve less than 7% accuracy in token predic1 tion on the ImageNet validation set. Based on our findings, we propose novel coarse-to-fine (CTF) generation approach that leverages the visual similarity between tokens. Rather than discriminating among tens of thousands of distinct tokens, we first group tokens with similar codewords using k-means clustering and assign each cluster corresponding coarse label. Our generation process is then divided into two stages: (1) an autoregressive model sequentially predicts the coarse label (cluster index) for each token, and (2) conditioned on these coarse labels, an auxiliary model simultaneously predicts the fine label (original codebook index) for all tokens. This coarse-to-fine strategy effectively solves our central challenge by maintaining large codebook sizes for quality reconstruction while simplifying the autoregressive modeling task. The effective vocabulary size is reduced from the total number of tokens to much smaller number of clusters, streamlining the learning process. Furthermore, our experiments demonstrate that once coarse labels are determined, predicting fine labels is relatively straightforwardthe network can generate fine labels for all tokens in single step, enabling our architecture to achieve both improved performance and high efficiency. Extensive experiments confirm that our approach markedly outperforms baseline methods, achieving up to 1-point reduction in FID scores while improving the Inception Score by an average of 59 points. Notably, despite introducing an auxiliary network and an additional inference step, our method achieves faster sampling speeds in practice due to the reduced vocabulary space. Overall, our main contributions are: coarse-to-fine token prediction framework that alleviates vocabulary redundancy in autoregressive image generation. systematic method for assigning coarse labels through k-means clustering of codebook vectors. Empirical evidence that fine labels for all tokens in sequence can be efficiently predicted in single step when conditioned on coarse labels. two-stage generation algorithm that can be seamlessly integrated with various autoregressive image generation methods, yielding both improved performance and faster sampling speed. 2. Preliminary 2.1. VQ-VAE Vector Quantized Variational Autoencoder (VQ-VAE) provides framework for mapping continuous images into discrete latent space. Given an image RHW 3, the encoder transforms it into latent feature map: = E(x) Rhwd (1) Each feature vector zij Rd is then quantized by mapping it to its closest codeword in learned codebook = {ek}K k=1, with the quantization defined as: min k{1,2,...,K} zq ij = ek , where = arg zij ek2 2. (2) The quantized feature map zq is then flattened into sequence of discrete tokens {T1, T2, . . . , TN }, with = w, where each token Ti {1, 2, . . . , K} represents the index of the corresponding codeword in the codebook B. To reconstruct an image from this token sequence, tokens are first mapped to their corresponding codewords {eT1, eT2, . . . , eTN }, which are then reshaped to recover the quantized latent feature map zq. Finally, the decoder reconstructs the image: ˆx = D(zq). 2.2. Autoregressive Image Generation Autoregressive image generation leverages the discrete token representation produced by the VQ-VAE. After an image is encoded and quantized into token sequence {T1, T2, . . . , TN }, an autoregressive model fθ (typically based on transformer architecture) is trained to model the conditional distribution of each token given all previous tokens in the sequence: Pθ(Ti T<i) = fθ(T<i). (3) The training objective is to maximize the likelihood of the observed token sequences, which is equivalent to minimizing the negative log-likelihood: LAR = (cid:88) i=1 log Pθ(Ti T<i). (4) During inference, the model generates the token sequence autoregressively by sampling each token from the conditional distribution Pθ(Ti T<i). Once the complete token sequence is generated, it is transformed back into an image using the VQ-VAE decoder as described in Section 2.1. 3. Motivation and Findings As we have introduced above, autoregressive image generation typically requires using VQ-VAE to convert images into token sequences. To minimize the quantization error that existed in VQ-VAE, recent approaches have dramatically scaled up codebook sizesfrom the original 1,024 entries [28, 37] to 16,384 [32], and even reaching 262,144 [21, 41]. While these larger codebooks successfully reduce quantization errors and improve reconstruction quality, they simultaneously complicate autoregressive modeling since the vocabulary size is expanded accordingly. This tension raises crucial question: Can we maintain the benefits of 2 Figure 1. (a) The codeword clustering process, where token indices are grouped based on the similarity of their corresponding feature vectors in the codebook. (b) Visual demonstration of token redundancy: replacing each token with another randomly sampled from the same cluster produces images with only minor variations in detail, preserving the overall structure and content. (c) Illustration of our two-stage generation process: in the first stage, the model autoregressively predicts coarse labels (cluster indices) for each token in the sequence; then the second stage model predicts fine labels (indices in the codebook) for all tokens in single step. larger codebooks while keeping the vocabulary complexity manageable for autoregressive models? To answer this question, we first investigate whether redundancy exists in the expanded vocabulary space. We hypothesize that within large codebooks, multiple distinct tokens may represent similar visual information, suggesting an opportunity for vocabulary optimization without sacrificing generation quality. To verify our hypothesis, we performed experiments based on LlamaGen [32], which uses 16,384-entry codebook. Firstly, we clustered the codewords into 2,048 distinct clusters using K-means based on their vector representations in the embedding space. Then, after the autoregressive model generates all tokens, we replace them with random alternatives from the same cluster and use the VQVAE to convert them into images. As can be observed in Figure 1(b), the resulting images maintained their overall structure, content, and quality, showing only minimal variations in visual details. This demonstrates that tokens with similar vector representations contribute comparable visual information to the final image. This observation helps explain why even state-of-theart autoregressive image generation methods struggle with token prediction accuracy. For example, VAR [33], despite employing two-billion-parameter model, achieves less than 7% accuracy in token prediction on ImageNet test set. The models difficulty in discriminating between visually similar tokens severely complicates the learning process without proportionally improving generation quality. Based on these insights, we propose to predict tokens from coarse to fine (CTF), which enables scaling codebook size while maintaining manageable vocabulary for autoregressive modeling. The general workflow of our approach can be summarized as follows: 1. Token Clustering: We systematically group tokens with similar codeword representations using k-means clustering, assigning each cluster coarse label. This reduces the effective vocabulary size from tens of thousands of distinct tokens to much smaller number of semantically meaningful clusters. 2. Coarse Label Prediction: Use an autoregressive model to sequentially predict the coarse labels (cluster indices) for tokens in the sequence, significantly simplifying the prediction task while preserving the essential visual information needed for high-quality generation. 3. Fine Label Prediction: Conditioned on the predicted 3 coarse labels, an auxiliary model is employed to predict the fine labels (original codebook indices) for all tokens in single step. By applying CTF, we can enjoy the benefits brought by large codebook while keeping the autoregressive modeling task tractable by focusing on reduced set of meaningful visual clusters. 4. Method In this section, we first describe the process of clustering fine labels to obtain coarse labels, then introduce our twostage prediction framework. 4.1. Codeword Clustering Figure 1(a) illustrates our codeword clustering pipeline, which we formally describe below. Given VQ-VAE codebook = {e1, e2, . . . , eK} with codewords, we apply the k-means algorithm to partition these codewords into distinct clusters (M K): = {G1, G2, . . . , GM }, (5) where each cluster Gm contains codewords with similar features based on their Euclidean distance in the embedding space. This clustering naturally induces mapping function: ϕ : {1, 2, . . . , K} {1, 2, . . . , }, such that: ϕ(k) = if ek Gm. (6) (7) This mapping function ϕ assigns each fine-grained token to its respective cluster, effectively creating coarse-grained representation where visually similar tokens share the same cluster label. This process substantially reduces the vocabulary size from to , making the autoregressive modeling task more tractable. In our implementation, we use default configuration that maps 16,384 fine-grained token categories to 512 coarse-grained clusters, yielding 32-fold reduction in vocabulary size. 4.2. Coarse-to-Fine Prediction Framework As illustrated in Figure 1(c), our generation process consists of two sequential stages, predicting coarse labels and fine labels for tokens in the sequence respectively. 4.2.1. Stage 1: Autoregressive Coarse Label Prediction In the first stage, we train an autoregressive model to predict coarse labels for tokens in sequence to capture the overall structure and content of the image. For each token position i, we define the coarse label as Ci = ϕ(Ti), where Ti is the original fine-grained token. The autoregressive model learns to predict: θ (CiC<i) = c θ (C<i). (8) The training objective is to maximize the likelihood of the correct coarse labels in the sequence: Lcoarse = (cid:88) i=1 log θ (CiC<i). (9) This formulation significantly simplifies the autoregressive modeling task, allowing the model to focus on capturing meaningful structural relationships rather than discriminating among highly similar tokens. 4.2.2. Stage 2: Parallel Fine Label Prediction After stage 1, we obtain the complete sequence of coarse labels = {C1, C2, . . . , CN } for all tokens. Next, we deploy full-attention transformer model to simultaneously predict the fine-grained labels for the entire sequence. This model takes the complete coarse sequence as input and outputs probability distributions for each fine-grained token {T1, T2, . . . , TN }. The training objective for this stage is to maximize the likelihood of the actual tokens conditioned on the coarse sequence: Lfine = (cid:88) i=1 log pθ(TiC). (10) Importantly, our experiments demonstrate that this finelabel prediction task is substantially simpler compared with the autoregressive token prediction problem. Since each coarse label Ci restricts the possible fine labels to much smaller subset (those within the corresponding cluster), the model only needs to discriminate among tokens within the same cluster rather than across the entire vocabulary. Furthermore, by predicting all fine labels simultaneously with full-attention mechanism, the model can leverage global context from the entire coarse sequence, enabling more coherent refinement. 4.3. Details in Training and Inference key advantage of our approach is that the Stage 1 and Stage 2 models are independent in training, thus they can be trained in parallel to improve training efficiency. Also, our framework is agnostic to the specific architecture of the Stage 1 modelany autoregressive model can be employed, as long as its outputs are token sequences rather than continuous representations [19]. This flexibility makes our method compatible with most autoregressive architectures. For inference, we first use the Stage 1 model to autoregressively generate the coarse label sequence. In this process, techniques such as temperature control and classifierfree guidance [12] can be applied, just as in conventional autoregressive models. Once the coarse sequence is complete, we pass it to the Stage 2 model, which produces all 4 Type Model #Para. FID IS Pr. Re. 5. Experiments GAN Diff. BigGAN [1] GigaGAN [16] StyleGAN-XL [30] ADM [8] CDM [14] LDM-4 [29] DiT-XL/2 [24] Mask. MaskGIT [3] MaskGIT-re [3] VAR VAR-d16 [33] VAR-d20 [33] VAR-d24 [33] AR AR AR VQGAN [9] VQGAN [9] VQGAN-re [9] ViT-VQGAN [39] ViT-VQGAN-re [39] RQTran. [18] RQTran.-re [18] IAR-B [15] IAR-L [15] IAR-XL [15] LlamaGen-B [32] LlamaGen-L [32] LlamaGen-XL [32] LlamaGen-B [32] + CTF LlamaGen-L [32] + CTF LlamaGen-XL [32] + CTF 112M 6.95 569M 3.45 166M 2.30 - 554M 10.94 4.88 400M 3.60 675M 2. 227M 6.18 227M 4.02 310M 3.30 600M 2.57 2.09 1.0B 227M 18.65 15.78 1.4B 5.20 1.4B 4.17 1.7B 3.48 1.7B 7.55 3.8B 3.80 3.8B 111M 5.14 343M 3.18 775M 2.52 111M 6.09 343M 3.07 775M 2.62 111M 5.46 87M 4.15 343M 3.80 310M 2.97 775M 3.39 734M 2.76 224.5 225.5 265. 101.0 158.7 247.7 278.2 182.1 355.6 274.4 302.6 312.9 80.4 74.3 280.3 175.1 175.1 134.0 323.7 202.00 234.80 248.10 182.54 256.06 244.08 193.61 254.99 248.28 291.53 227.08 299. 0.89 0.84 0.78 0.69 - - 0.83 0.80 - 0.84 0.83 0.82 0.78 - - - - - - 0.85 0.82 0.82 0.85 0.83 0. 0.83 0.86 0.83 0.84 0.81 0.84 0.38 0.61 0.53 0.63 - - 0.57 0.51 - 0.51 0.56 0.59 0.26 - - - - - - 0.45 0.53 0.58 0.42 0.52 0.57 0.45 0.48 0.52 0.53 0.54 0.55 Table 1. Performance comparison on class-conditional ImageNet at 256256 resolution. Models are evaluated using FID, Inception Score (IS), precision (Pr.), and recall (Re.) metrics. The background is colorized for convenient comparison with baseline. Models with were trained at 384384 resolution and downsampled to 256256 for evaluation. : Our autoregressive models have fewer parameters due to the reduced vocabulary size, complemented by an auxiliary network for fine-grained prediction, see Section 5.3 for detailed efficiency analysis. fine labels in single step. In this process, we find that performing top-k sampling also helps to improve performance, largely by increasing output diversity (see Section 5.5.6). The resulting fine-grained token sequence is then processed by the VQ-VAE decoder to generate the final image. This two-stage framework effectively addresses the vocabulary redundancy problem in autoregressive image generation. By first capturing the essential structure using reduced set of semantically meaningful labels and then recovering detailed visual information within each cluster, our approach simplifies the most challenging aspect of autoregressive prediction while preserving high reconstruction fidelity. 5.1. Setup Our work tackles the redundancy in token labels for autoregressive image generation, thereby enabling the use of any AR method that produces token sequences [3, 32, 33]. We adopt LlamaGen [32] as our backbone, owing to its simplicitytraining both the VQ-VAE and autoregressive model solely on ImageNet without using complex training or inference strategy. Model Architectures. We employ the VQ-VAE trained by LlamaGen [32] on ImageNet-1K as our image tokenizer. The tokenizer has codebook size of 16,384 and downsamples the input image by factor of 1616. For both stage 1 and stage 2, we adopt the same architecture as LlamaGen [32], with only one modification: the fine stage model uses full attention instead of causal attention. Benchmark. We compare our method with previous work on the class-conditional image generation task using the ImageNet-1K benchmark [5]. For fairness, all results of previous work are obtained from [32, 33]. Training Settings. Following LlamaGen [32], we train all models for 300 epochs using batch size of 256. We employ the AdamW optimizer with parameters β1 = 0.9, β2 = 0.95, and weight decay of 0.05 while applying gradient clipping at 1.0 to stabilize training. To enable classifier-free guidance, we apply dropout rate of 0.1 on the class token embeddings. The learning rate is initialized at 1 104 and decays to 1 105 following cosine annealing schedule. Sampling Settings. For simplicity, we adopt consistent set of sampling hyperparameters across all model configurations. Specifically, we set the classifier-free guidance scale to 2.0, temperature to 1.1, and use unrestricted sampling with top-k=0. 5.2. Main Results We evaluate our approach against baseline methods on the class-conditional ImageNet 256256 benchmark, as shown in Table 1. Our method consistently achieves substantial performance gains across model sizes ranging from 100M to 775M parameters. Compared to LlamaGen-B, our approach reduces the FID score by over 1.0 while significantly improving the Inception Score. Additionally, LlamaGen-L with our method surpasses LlamaGen-XL across all metrics by large margin, highlighting the effectiveness of our coarse-to-fine prediction strategy. Notably, despite being trained at 256256 resolution, our models outperform baseline models trained at higher 384384 resolution in most cases. Overall, our approach substantially enhances image generation qualityimproving the Inception Score by an average of 59 Model Total Param images/sec FID IS 5.4. Training Efficiency Step LlamaGen-B + CTF LlamaGen-L + CTF 111M 87M+343M 256+1 343M 310M+343M 256+1 LlamaGen-XL + CTF 775M 256 734M+343M 256+1 13.75 12. 7.50 8.71 5.32 6.26 5.46 4.15 3.80 2.97 3.39 2.76 193.61 254. 248.28 291.53 227.08 299.69 Table 2. Sampling speed comparison, measured on single A100 GPU with batch size of 64. Although our method employs an auxiliary network with 343M parameters, our method achieved better sampling efficiency in most cases. Figure 2. Model performance comparison on different epochs. When our method is applied, models achieve significantly better performance. pointswhile also offering faster sampling speed (see Section 5.3). 5.3. Sampling Speed In Table 2, we compare the sampling speeds of our proposed method with the baseline approach. Despite introducing an auxiliary network for detailed label predictionwhich requires an additional inference stepour method achieves faster sampling speeds in most test cases. This performance improvement stems primarily from the efficiency of our autoregressive model architecture. Unlike the baseline, our models fully connected layer contains fewer parameters, as it only needs to predict coarse labels with significantly fewer categories than detailed labels. This advantage becomes particularly significant considering that the autoregressive model executes 256 times during the sampling process, effectively offsetting the computational cost of the auxiliary network. Furthermore, our approachs speed advantage becomes increasingly pronounced as the autoregressive model scales up in size. The reduction in parameter count in the fully connected layer creates compounding efficiency benefits that outweigh the overhead of the additional network component. 6 To evaluate whether our coarse-to-fine prediction strategy enables more efficient learning of image token distributions, we compared model performance across different training durations. Figure 2 presents the FID and IS metrics for both our approach and the baseline method at various training epochs. The results demonstrate that our method consistently outperforms the baseline by substantial margin throughout the training process. Most notably, our model achieves an FID of 5.43 and an IS of 193.34 after just 100 epochs, comparable to the baseline models performance after 300 epochs (FID: 5.46, IS: 193.61). This represents 3 reduction in required training time to reach equivalent generation quality. These findings confirm that our coarse-to-fine approach not only accelerates convergence but also enhances the models capacity to learn meaningful token distributions. By reducing vocabulary redundancy and focusing on hierarchical relationships between tokens, our method improves both training efficiency and the quality of generated images. 5.5. Ablation In this section, we conduct ablation studies to investigate the impact of various hyperparameters involved in training and inference processes. The results are reported in Table 3. 5.5.1. Number of Clusters. According to the results reported in Table 3a, performance degrades significantly when the number of clusters is low. This occurs because each cluster contains too many fine labels when clustering is sparse. For instance, with 128 clusters, each cluster contains approximately 128 fine labels on average, compared to just 32 labels per cluster when using 512 clusters. When single cluster encompasses numerous fine labels, the auxiliary model struggles to accurately predict fine labels based on the given coarse labels in single step. However, simply increasing the cluster number does not always yield improvements, as the task approaches standard autoregressive image generation process when the cluster number approaches the total number of detailed labels. Our experiments reveal clear trade-off in this parameter. Based on experimental results, we established 512 as the optimal number of clusters for our default configuration. 5.5.2. K-means. To evaluate the importance of clustering based on codeword similarity in our method, we conducted an experiment replacing K-means with random clustering. The results demonstrate that the auxiliary model fails to accurately predict fine labels from coarse labels when clustering is performed randomly. This failure can be attributed to the weak correlation between tokens within the same randomly asNum. FID IS Pr. Re. Para. FID IS Pr. Re. CFG FID IS Pr. Re. 128 512 1024 11.23 5.51 5. 98.41 0.68 0.39 273.45 0.88 0.44 269.10 0.89 0.44 111M 5.51 273.45 0.88 0.44 343M 5.33 285.15 0.89 0.43 775M 5.27 289.10 0.89 0.46 1.75 2.00 2.25 4.24 247.02 0.86 0.48 5.33 285.15 0.89 0.43 6.71 313.06 0.90 0.41 (a) Number of Clusters (b) Model Size (stage 2) (c) CFG Top-k FID IS Pr. Re. τ1 FID IS Pr. Re. τ2 FID IS Pr. Re. 1 32 All 6.95 210.21 0.87 0.39 5.35 286.73 0.89 0.43 5.33 285.15 0.89 0. 0.9 1.0 1.1 7.30 292.73 0.90 0.39 5.33 285.15 0.89 0.43 4.16 259.45 0.86 0.47 0.9 1.0 1.1 5.55 288.01 0.89 0.43 4.16 259.45 0.86 0.47 4.15 254.99 0.86 0.48 (d) Top-k (stage 2) (e) Temperature (stage 1) (f) Temperature (stage 2) Table 3. Ablation Studies. Hyper-parameters with grey background are selected as the default settings. signed cluster, which significantly impairs the models ability to capture inter-token relationships. These findings confirm the effectiveness of our algorithm design, highlighting that meaningful clustering based on codeword similarity is essential for the hierarchical prediction mechanism to function properly. The structured grouping provided by K-means creates coherent clusters that enable the auxiliary model to learn meaningful patterns between coarse and fine labels. 5.5.3. Auxiliary Model Size. As shown in Table 3b, increasing the auxiliary model size leads to consistent performance improvements. However, we observe that this benefit becomes marginal once the parameter count approaches 343M. Balancing performance gains against computational efficiency, we selected the 343M parameter auxiliary model as our default configuration. 5.5.4. Classifier Free Guidance. As observed in Table 3c, our experiments reveal an interesting trade-off: lower CFG (Classifier-Free Guidance) factors improve FID scores, while higher CFG factors enhance Inception Scores. For the consideration of balanced performance, we selected CFG factor of 2 as our default setting, which performed well across various settings. 5.5.5. Temperature. According to the results reported in Table 3e and Table 1, our models achieve significantly better IS scores than the baseline method, demonstrating the effectiveness of our coarse-to-fine prediction strategy. However, our methods FID is relatively inferior when temperature control is not applied. We attribute this performance gap to two key factors: (1) during the autoregressive generation process, our models can sample from only 512 classes while the baseline method can sample from 16,384 classes, and (2) our regressive model exhibits higher confidence in its predictions, as our coarse-to-fine strategy simplifies the task (our models training loss is approximately 5 compared to the baselines loss of around 7). Although these factors help to improve the quality of the generated images, they reduce the diversity of the generation. However, this can be effectively addressed by using temperature control to make the models predictions less confident. As demonstrated in Table 3e and Table 3f, using larger temperature effectively improves the diversity of the generation (reflected in the better FID score). The temperature control plays crucial role in balancing the trade-off between generation quality and diversity in our approach. 5.5.6. Top-k. As demonstrated in Table 3d, restricting the sampling to only the highest probability class (Top-k=1) in stage 2 leads to significantly worse performance on both FID and Inception Score metrics. This degradation occurs because these evaluation metrics assess not only image quality but also diversity. When Top-k is set to 1, the generation process can only select the token with the highest probability, which substantially limits output variability. To enhance diversity in our generated images, we follow Sun et al. [32]s recommendation and adopt Top-k=0 (allowing sampling across the entire probability distribution) as our default configuration. 5.6. Visualization We visualize the images generated by our method (based on LlamaGen-XL) in Figure 3. As shown, our models coarse-to-fine prediction strategy produces images with remarkably detailed textures and structures. The progressive refinement successfully preserves global composition while enhancing local features, demonstrating the effectiveness of our hierarchical generation approach. 6. Related Work 6.1. Large Language Models. Large language models (LLMs) [7, 11, 26, 34] have revolutionized the field of natural language processing (NLP). The transformer architecture [38], with its self-attention mechanism, serves as the foundation for these models. Early approaches employed the encoder-decoder structure [7], but recent advancements have demonstrated the effectiveness of decoder-only architectures. Models such as GPT-2 [26] and GPT-3 [2] utilize next-token prediction as their core training objective, where the model learns to predict the subsequent token given sequence of preceding tokens. This autoregressive approach has proven remarkably effective, enabling models to generate coherent and contextually appropriate text across diverse domains. Following these successes, wave of open-source models including LLaMA [34], Mistral [2], and deepseek [11] have adopted similar training paradigms while introducing architectural refinements to enhance efficiency and performance. The next-token prediction objective has emerged as surprisingly powerful learning signal, enabling these models to capture complex linguistic patterns, factual knowledge, and even rudimentary reasoning capabilities without explicit supervision for these skills. 6.2. Image Generation. Generative adversarial networks (GANs) [1, 10, 16, 17] are the pioneering method for visual generation in the deep learning era, focusing on learning to generate realistic images through adversarial training. More recently, Diffusion models [8, 13, 20, 29, 31] introduce novel approach, treating visual generation as reverse diffusion process, where images are gradually denoised from Gaussian noise through series of steps. These models have demonstrated exceptional capabilities in image synthesis, particularly for highresolution and photorealistic outputs. Autoregressive Image Generation. In autoregressive image generation, 2D visual data is transformed into 1D sequences of either pixels or tokens, with each element generated sequentially following predetermined ordering. Pioneer studies [4, 23, 35, 36] have successfully validated the efficacy of autoregressive frameworks for RGB pixel generation, yielding results that rival those of GANs [1, 10, 16, 17]. The introduction of VQVAE [37] and its hierarchical extension VQVAE-2 [27, 28] established methods for representing images as discrete token sequences in latent space. Building on this foundation, VQ-GAN [9] incorporated adversarial training to enhance perceptual quality, while RQ-Transformer [18] further refined autoregressive learning within these discrete representation spaces. Furthermore, drawing inspiration from BERT [6], models incorporating masked prediction techniques [3, 40, 40, 41] Figure 3. Generation results of our method (based on LlamaGenXL) on ImageNet 256256 benchmark. have emerged, enabling parallel prediction of multiple tokens per iteration, thereby substantially reducing computational demands during deployment. Recently, by expanding vocabulary size [32, 41] and improving training strategy [22, 42], autoregressive image generation methods [19, 32, 33] have shown promising performance, exceeding methods based on diffusion [8, 13, 20, 29, 31] on several benchmarks. Moreover, similar to our findings, concurrent work [15] also discovers that similar codewords have similar effects on the resulting images. Based on this observation, they propose an additional training loss to guide the prediction toward the correct token class cluster. Different from their approach, we propose to predict tokens from coarse to fine, which not only brings more performance improvement but also helps to improve the sampling speed. 7. Conclustion In this work, we present novel coarse-to-fine token prediction framework for autoregressive image generation, which helps to alleviate the vocabulary redundancy problem in large VQ-VAE codebooks. Extensive experiments demonstrated that our method substantially outperforms baseline approaches, improving Inception Scores by an average of 59 points. Moreover, despite introducing an additional prediction step, our method accelerates the sampling process, making it both more effective and more efficient than conventional approaches."
        },
        {
            "title": "References",
            "content": "[1] Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. 5, 8 [2] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 1, 8 [3] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. 5, 8 [4] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. 8 [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [6] Jacob Devlin. Bert: Pre-training of deep bidirectional arXiv preprint transformers for language understanding. arXiv:1810.04805, 2018. 8 [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171 4186, 2019. 8 [8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. 1, 5, 8 [9] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 5, 8 [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 8 [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 4 [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 1, 8 [14] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 5 [15] Teng Hu, Jiangning Zhang, Ran Yi, Jieyu Weng, Yabiao Wang, Xianfang Zeng, Zhucun Xue, and Lizhuang Ma. Improving autoregressive visual generation with clusteroriented token prediction. arXiv preprint arXiv:2501.00880, 2025. 5, 8 [16] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1012410134, 2023. 5, 8 [17] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 8 [18] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. 5, [19] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. 1, 4, 8 [20] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022. 1, 8 [21] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 1, 2 [22] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024. 8 [23] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. ImIn International conference on machine age transformer. learning, pages 40554064. PMLR, 2018. 8 [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [25] Alec Radford. Improving language understanding by generative pre-training. 2018. 1 [26] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 8 [27] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 8 [28] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 1, 2, 8 [29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 1, 5, 8 [30] Axel Sauer, Katja Schwarz, and Andreas Geiger. Styleganxl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. 5 [31] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 1, 8 [32] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 2, 3, 5, 7, 8 [33] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable imarXiv preprint age generation via next-scale prediction. arXiv:2404.02905, 2024. 1, 3, 5, 8 [34] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 8 [35] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. 8 [36] Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pages 17471756. PMLR, 2016. [37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. 1, 2, 8 [38] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 8 [39] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. 5 [40] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 8 [41] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 1, 2, 8 [42] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Shanghai AI Lab"
    ]
}