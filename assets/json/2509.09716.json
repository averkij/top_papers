{
    "paper_title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions",
    "authors": [
        "Jun Zhan",
        "Mingyang Han",
        "Yuxuan Xie",
        "Chen Wang",
        "Dong Zhang",
        "Kexin Huang",
        "Haoxiang Shi",
        "DongXiao Wang",
        "Tengtao Song",
        "Qinyuan Cheng",
        "Shimin Li",
        "Jun Song",
        "Xipeng Qiu",
        "Bo Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Spoken language models (SLMs) have emerged as a unified paradigm for speech understanding and generation, enabling natural human machine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), a new task that examines whether SLMs can modify their speaking style, such as timbre, prosody, or persona following natural language spoken commands. To study this task, we present VStyle, a bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as a Judge (LALM as a Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with a foundation for advancing human centered spoken interaction. The dataset and code are publicly available at \\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 1 7 9 0 . 9 0 5 2 : r VSTYLE: BENCHMARK FOR VOICE STYLE ADAPTATION WITH SPOKEN INSTRUCTIONS Jun Zhan1,2,, Mingyang Han2,, Yuxuan Xie1,, Chen Wang2, Dong Zhang1, Kexin Huang1, Haoxiang Shi2, DongXiao Wang2, Tengtao Song2, Qinyuan Cheng1, Shimin Li1, Jun Song2,, Xipeng Qiu1,, Bo Zheng1 1Fudan University 2Alibaba Group jzhan24@m.fudan.edu.cn, xpqiu@fudan.edu.cn {hanmingyang.hmy, jsong.sj}@alibaba-inc.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Spoken language models (SLMs) have emerged as unified paradigm for speech understanding and generation, enabling natural humanmachine interaction. However, while most progress has focused on semantic accuracy and instruction following, the ability of SLMs to adapt their speaking style based on spoken instructions has received limited attention. We introduce Voice Style Adaptation (VSA), new task that examines whether SLMs can modify their speaking stylesuch as timbre, prosody, or personafollowing natural language spoken commands. To study this task, we present VStyle, bilingual (Chinese & English) benchmark covering four categories of speech generation: acoustic attributes, natural language instruction, role play, and implicit empathy. We also introduce the Large Audio Language Model as Judge (LALM-as-a-Judge) framework, which progressively evaluates outputs along textual faithfulness, style adherence, and naturalness, ensuring reproducible and objective assessment. Experiments on commercial systems and open-source SLMs demonstrate that current models face clear limitations in controllable style adaptation, highlighting both the novelty and challenge of this task. By releasing VStyle and its evaluation toolkit, we aim to provide the community with foundation for advancing human-centered spoken interaction. The dataset and code are publicly available at projects homepage. Index Terms Spoken Language Models, Voice Style Adaptation, Benchmark, LALM-as-a-Judge 1. INTRODUCTION Spoken language models (SLMs)[1, 2, 3] have recently gained wide attention. Compared with traditional cascaded pipelines, they offer more natural and realistic interactions. However, most research focuses on what the model says * Equal contribution. Corresponding authors. (semantic accuracy) rather than how the model says it (expressiveness). Non-verbal cuessuch as speaker identity, emotion, and paralinguistic signalsare crucial to natural dialog and user experience, but comprehensive frameworks for evaluating these expressive dimensions remain lacking. Conventional TTS metrics[4] like Word Error Rate (WER) and Speaker Similarity (SIM) fail to adequately capture the diverse requirements of dialogue systems. Existing benchmarks have limitations: Some[5, 6] consider only content accuracy; VocalBench[7] targets conversational style, not speech style; S2S-Arena[8] relies on costly, unscalable human evaluations; WavReward[9] lacks coverage of realistic dialog scenarios; and Kimi-Audio[10], Step-Audio[11, 12] provide only small-scale test sets with limited reproducibility. To address these gaps, we first formalize the task of Voice Style Adaptation (VSA): determining whether an SLM can modify its speaking style in response to spoken instructions. To study this task, we introduce VStyle, bilingual (Chinese/English) benchmark covering four categories of speech generation: acoustic attributes, natural-language instructions, role-play, and implicit empathy, comprising 1,523 prompts designed around realistic interaction needs. key challenge is reliable quantitative evaluation, since human assessment is costly and variable. We therefore introduce the LALM-as-a-Judge framework[13, 14, 15, 16], which leverages Large Audio Language Models to progressively assess outputs across three dimensions: content faithfulness, style consistency, and overall naturalness. This enables scalable and reproducible automatic evaluation pipeline. Applying VStyle to commercial and open-source SLMs, we show it effectively distinguishes voice style adaptation performance and reveals significant gap across systems. We release the dataset and toolkit to support progress toward more expressive, controllable, and human-centered spoken interaction. Fig. 1. Instruction examples from the VStyle dataset across four categories: Acoustic Attributes, Natural Language Instruction, Role-Playing, and Implicit Empathy. 2. VSTYLE 2.1. Overview VStyle is bilingual benchmark for evaluating voice style adaptation (VSA) in spoken language models. Each instance provides spoken instruction that may specify content, assign task, or set up an interaction, but also the desired speaking style, expressed explicitly or implicitly. The model must generate spoken response that aligns with both intent and style. To reflect realistic interaction needs, VStyle covers four categories: acoustic attributes, natural language instruction, role-play, and implicit empathy, with examples shown in Figure 1. Acoustic Attributes. In this category, the instruction explicitly constrains one or more acoustic attributes of the generated speech, including age, gender, speaking rate, pitch, loudness, and emotion. Each attribute is defined within finite closed set, enabling direct evaluation of models capacity for fine-grained yet essential control over speech acoustics. Natural-Language Instructions. This category uses open-ended natural language instructions to guide speaking style generation. It includes three subtypes: emotion, referring to unconstrained descriptions of affective states; style, allowing free-form specification of global speech style; and variation, which entails temporal variation between emotions and styles within single utterance, thereby showcasing models fine-grained controllability. Role-Play. Role-play tasks are categorized into two subtypes: scene-based and character-based. The former requires the model to assume role within given scenario, while the latter involves imitating personas characterized by distinctive vocal traits. Success in both depends on the models ability to infer and produce appropriate timbre, emotion, and speaking style from contextual cues. Implicit Empathy. Emotional companionship is key In this cateapplication of conversational speech systems. gory, instructions do not specify speaking style; instead, they prompt the model to interact as friend while conveying strong emotion. The model must infer the users affective state and deliver supportive response that integrates both linguistic content and prosodic expression. Four representative affective contexts are considered: anger, anxiety and fear, sadness and disappointment, and joy and excitement. 2.2. Data Construction We constructed the instruction dataset using hybrid humanLLM approach. Seed instructions were first manually designed, then expanded and iteratively refined with LLMs. To minimize ambiguity in spoken language, we adopted fixed patterns such as *Please say this sentence* (content specification), *Please speak in an appropriate tone* (role-play), and *Please act as my friend and talk to me* (empathy). After preparing the text corpus, we synthesized corresponding audio instructions with commercial voice cloning models. For the first three categories that do not require distinct emotional expression, we selected audio prompts from Seed-TTS with DNSMOS scores above 3.2 and generated diverse audio instructions through commercial voice cloning system. For empathy-related data, Gemini-TTS was employed, leveraging its instruction control capabilities to produce speech aligned with the intended emotional states. Following filtering of erroneous and ambiguous samples, the final dataset comprised 1,523 bilingual (ChineseEnglish) speech instructions. 2.3. Large Audio-Language Model as Judge Automatic and quantitative evaluation of speech generation quality remains fundamental challenge. Recent studies[14, 15, 16] indicate that Large Audio-Language Models exhibit strong capabilities in audio assessment, which we adapt for spoken dialogue evaluation. We structure the evaluation along three hierarchical dimensions: textual adherence, stylistic adherence, and overall naturalness.A 5-point Mean Opinion Score (MOS) scale is employed, with the evaluation procedure illustrated in Figure 2. If the generated speech fails to satisfy textual adherence, it is directly assigned score of 1. Otherwise, the proFig. 2. Evaluation framework using Large Audio-Language Models (LALMs) as judge. Given instruction and generated voice answer, the LALM evaluator conducts hierarchical assessment: (1) Content evaluation, where failure leads to score of 1; (2) Style evaluation, assigning score of 2 for non-compliance, 3 for partial compliance, and progressing if satisfied; and (3) Naturalness evaluation, yielding score of 4 for unnatural and 5 for highly natural speech. This staged process underlies the 5-point MOS scoring scheme. cess proceeds to stylistic adherence: score of 2 is given for complete non-compliance, 3 for partial but imperfect compliance, and full compliance advances the evaluation to naturalness. When the generated speech demonstrates high level of naturalness, it is awarded the maximum score of 5. This progressive framework reflects the incremental requirements of speech models: first correctness of content, then stylistic appropriateness, and finally natural, human-like speech capable of sustaining realistic interaction. Leveraging LALMs perceptual and reasoning abilities, our chain-of-thought evaluation achieves consistent and stable scoring. Final benchmark scores are obtained by averaging across categories with equal weights. 3. EXPERIMENT 3.1. Experimental Setup We evaluate GPT-4o Audio (snapshot: gpt-4o-audio-preview2025-06-03) [17], GPT-4o-Mini Audio (snapshot: gpt-4omini-audio-preview-2024-12-17) [17], and Doubao [18] as representative commercial voice-based dialogue systems. In addition, we include four open-source end-toend speechlanguage models noted for their strong speech generation capabilities: Step-Audio [11], Kimi-Audio [10], Baichuan-Audio [19], and Qwen-2.5 Omni [20]. For models supporting multiple fixed speakers, we ranFor domly assigned one speaker per dialogue session. Baichuan-Audio, we removed the voice-prompt to allow more diverse speech generation. Full-duplex systems such as Doubao autonomously manage response timing. To ensure outputs were produced only after complete input reception, we avoided inserting long pauses in the speech input. For evaluation, we employed Gemini-2.5-pro, the strongest audio evaluation model validated in prior work[14, 15], with inference parameters set to maximum token length of 4096, temperature of 1.0, and top-p of 0.7. 3.2. Results and Analysis Overall Results. Table 1 presents the comprehensive evaluation results across all models and evaluation dimensions. Several key findings emerge from our analysis: Significant performance gap between open-source and commercial models. Commercial models clearly outperform open-source ones. In terms of overall performance, GPT-4o achieved the best results in English tasks (4.05), while Doubao ranked highest in Chinese tasks (4.10). By contrast, open-source models generally scored between 2 and 3 points. Among open-source systems, Kimi-Audio (3.11) performed best in Chinese, while Step-Audio (2.77) led in English. These results highlight that commercial models remain substantially stronger in speech generation. The performance gap arises mainly from two factors. From technical perspective, open-source models emphasize content correctness but lack robustness in expressive speech generation, as they mostly rely on semantic-level representations and insufficiently model acoustic features. An exception is Baichuan-Audio, which uses unified codec to better capture vocal attributes, achieving strong results in age control and showing some timbre-control ability. From resource perspective, commercial models benefit from larger training corpora and stronger computation, yielding greater stability, while open-source systems often struggle with instructionfollowing, leading to frequent low scores that lower overall performance. Variation across task categories. Models show clear differences across task types. In acoustic attributes, composite tasks requiring control of multiple dimensions are more difficult than single-attribute tasks and score lower overall. In instruction-following, GPT-4o demonstrates strong English ability across all subtasks, including the hardest variation type, and shows robust style adaptation. Other models perform notably worse in style variation than in the other subtasks, indicating this remains difficult. In role-play, GPT-4o performs well in scene and character subtasks, while Doubao is strong in Chinese scene tasks. In implicit empathy, several Table 1. Evaluation Results on VStyle. Bold indicates the best score in each category for each language. Abbreviations: Gend. = Gender, Emot. = Emotion, Vol. = Volume, Comp. = Composite, Vari. = Variation, Scen. = Scenario, Char. = Character, Sad. = Sadness/Disappointment, Anx. = Anxiety/Fear, Joy = Joy/Excitement. Model Lang Overall Acoustic Attributes Instruction Role-Play Empathy Age Speed Gend. Emot. Pitch Vol. Comp. Emot. Style Vari. Scen. Char. Anger Sad. Anx. Joy Baichuan-Audio Step-Audio Qwen2.5-Omni Kimi-Audio Doubao GPT-4o-Mini GPT-4o en zh en zh en zh en zh en zh en zh en zh 2.50 2.25 2.77 2.92 2.46 2.97 2.54 3.11 3.63 4.10 3.88 3. 4.05 3.84 2.71 2.67 2.71 2.88 2.58 3.21 2.79 3.33 3.75 3. 2.83 3.25 3.67 3.42 2.20 2.45 2.40 2.60 2.25 2.45 2.45 3. 3.55 4.35 3.75 3.75 3.45 3.10 3.83 3.08 2.38 2.58 1.92 3. 2.54 2.25 3.46 3.25 3.50 3.50 2.79 3.50 2.58 2.29 2.46 3. 3.04 2.62 3.04 3.75 3.38 4.65 3.79 3.75 4.00 3.83 2.05 2.05 2.00 2. 2.20 3.15 2.80 2.75 1.95 2.05 2.35 2.55 1.55 3.00 2.95 3.25 3.25 4.05 4.35 4.70 3.10 4.15 3.30 3.70 3.60 4.10 3.35 3. 2.55 2.58 2.28 2.57 2.30 2.30 2.33 3.17 3.13 3.77 3.05 3. 3.27 3.22 2.23 1.71 2.59 2.39 2.67 2.53 2.19 2.66 3.52 3. 3.72 3.46 3.93 3.37 2.21 1.88 1.72 1.69 2.89 2.56 2.32 2.07 2.87 2.36 2.38 2.07 2.41 2.33 2.74 2. 3.67 2.90 3.96 2.88 4.00 3.47 3.47 2.98 4.23 4.07 3.51 3.11 2.08 2.29 1.65 2.93 1.80 2. 1.73 3.01 3.27 4.45 3.23 3.48 3.89 3.89 2.33 1.95 2.11 2. 1.68 2.24 1.72 2.23 2.56 3.79 3.82 3.84 3.83 3.90 2.41 2. 3.95 3.59 2.95 4.64 3.59 3.86 4.89 4.59 4.98 4.30 4.95 4. 3.43 2.74 3.91 2.55 2.20 3.51 4.37 3.87 4.29 4.52 3.20 4.26 2.73 3.55 3.43 4.28 4.77 4.91 3.97 3.65 3.46 3.86 3.80 4.57 5.00 4.81 4.94 4.72 4.80 4.83 5.00 4.87 5.00 4.52 4.73 4. 4.90 5.00 4.54 4.83 4.67 4.80 models handle emotions effectively, with both positive and negative emotions generally well processed. Language preference among models. Performance varies significantly across languages. For example, Doubao, Kimi-Audio, and Qwen2.5-Omni perform much better in Chinese than in English, whereas the GPT-4o series shows the opposite pattern. This discrepancy may stem from imbalanced distributions of training data across languages, or from substantial differences in pronunciation habits, which make cross-lingual transfer in speech generation far less effective than in text generation. 3.3. Evaluation Consistency Analysis To assess the consistency between model scores and human evaluations, we randomly sampled approximately half of the instructions for human assessment. Prior to the formal study, all annotators were required to complete trial task and pass qualification test; only those who met the standard were eligible to participate. Each instance was independently rated by five expert annotators. To ensure consistency in evaluation criteria, annotators followed the same set of instructions and guidelines as those used in the model-based assessment. To measure consistency, we adopted Spearmans rank correlation coefficient. Specifically: To assess the reliability of human evaluations, we calculated the average Spearman correlation coefficient across different annotators (Inter-Human Agreement Table 2. Spearman Correlation Between Human and Model Evaluations (%)"
        },
        {
            "title": "Metric",
            "content": "English (%) Chinese (%) Inter-Human Model-Individual Human Model-Consensus Human 78.58 70.61 77.01 70.54 64.53 73.03 (Average)). To assess the alignment between the model and human judgments, we calculated the average Spearman correlation coefficient between the model and each individual annotator (ModelIndividual Human Agreement (Average)), as well as the correlation between the model and the mean score of all annotators (ModelConsensus Human Agreement). The results are presented in Table 2. The correlations between model scores and human judgments are generally close to inter-human agreement levels. Notably, the correlation with the average human score reached 77.01% for English and 73.03% for Chinese, comparable to the human agreement benchmark. These findings indicate that the model can reliably approximate human evaluation patterns with cross-lingual stability. Overall, model-based evaluation achieves near human-level consistency and can serve as an efficient and scalable alternative to manual assessment. 4. LIMITATION AND CONCLUSION While VStyle introduces novel benchmark for voice style adaptation, several limitations remain. First, the instruction dataset is built from manually designed seeds and LLM-based expansion, so its distribution reflects annotator preferences and model-driven patterns, which may diverge from real user demands. Second, although Large Audio Language Models (LALMs) show strong potential in audio evaluation, they still face issues such as hallucinations. To address this, we adopt step-by-step prompts and explicit guidelines, which help improve consistency and reliability. Future advances in reasoning and auditory perception will further enhance the evaluation pipeline. Despite these constraints, VStyle establishes an essential foundation for progress in voice style adaptation. On one hand, its bilingual and multi-category design covers realistic interaction needs ranging from acoustic attribute control to implicit empathy, thus complementing existing benchmarks that often overlook expressive aspects. On the other hand, our experiments reveal strong correlations between LALM-as-a-Judge and human assessment, confirming the scalability and practicality of our framework. We hope VStyle will serve not only as diagnostic tool for identifying model shortcomings, but also as catalyst for more natural, controllable, and human-centered speech generation systems. 5. REFERENCES [1] Siddhant Arora, Kai-Wei Chang, Chung-Ming Chien, Yifan Peng, Haibin Wu, Yossi Adi, Emmanuel Dupoux, Hung-Yi Lee, Karen Livescu, and Shinji Watanabe, On the landscape of spoken language models: comprehensive survey, arXiv preprint arXiv:2504.08528, 2025. [2] Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, and Irwin King, Recent advances in speech language models: survey, arXiv preprint arXiv:2410.03751, 2024. [3] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu, Speechgpt: Empowering large language models with intrinsic arXiv preprint cross-modal conversational abilities, arXiv:2305.11000, 2023. [4] Xu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu, arXiv preprint survey on neural speech synthesis, arXiv:2106.15561, 2021. [5] Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et al., Textually pretrained speech language models, Advances in Neural Information Processing Systems, vol. 36, pp. 6348363501, 2023. [6] Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby Tan, and Haizhou Li, Voicebench: BencharXiv preprint marking llm-based voice assistants, arXiv:2410.17196, 2024. [7] Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, and Yu Wang, Vocalbench: Benchmarking the vocal conversational abilarXiv preprint ities for speech interaction models, arXiv:2505.15727, 2025. and Haizhou Li, [8] Feng Jiang, Zhiyu Lin, Fan Bu, Yuhao Du, Benyou Wang, evaluating speech2speech protocols on instruction followarXiv preprint ing with paralinguistic information, arXiv:2503.05085, 2025. S2s-arena, [9] Shengpeng Ji, Tianle Liang, Yangzhuo Li, Jialong Zuo, Minghui Fang, Jinzheng He, Yifu Chen, Zhengqing Liu, Wavreward: SpoZiyue Jiang, Xize Cheng, et al., ken dialogue models with generalist reward evaluators, arXiv preprint arXiv:2505.09558, 2025. [10] Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu, Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan, Heyi Tang, et al., Kimi-audio technical report, arXiv preprint arXiv:2504.18425, 2025. [11] Ailin Huang, Boyong Wu, Bruce Wang, Chao Yan, Chen Hu, Chengli Feng, Fei Tian, Feiyu Shen, Jingbei Li, Mingrui Chen, et al., Step-audio: Unified understanding and generation in intelligent speech interaction, arXiv preprint arXiv:2502.11946, 2025. [12] Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, et al., Step-audio 2 technical report, arXiv preprint arXiv:2507.16632, 2025. [13] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al., survey on llmas-a-judge, arXiv preprint arXiv:2411.15594, 2024. [14] Cheng-Han Chiang, Xiaofei Wang, Chung-Ching Lin, Kevin Lin, Linjie Li, Radu Kopetz, Yao Qian, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, et al., Audioaware large language models as judges for speaking styles, arXiv preprint arXiv:2506.05984, 2025. [15] Ruskin Raj Manku, Yuzhi Tang, Xingjian Shi, Mu Li, Emergenttts-eval: Evaluating tts and Alex Smola, models on complex prosodic, expressiveness, and linarXiv guistic challenges using model-as-a-judge, preprint arXiv:2505.23009, 2025. [16] Kexin Huang, Qian Tu, Liwei Fan, Chenchen Yang, Dong Zhang, Shimin Li, Zhaoye Fei, Qinyuan Cheng, and Xipeng Qiu, Instructttseval: Benchmarking complex natural-language instruction following in text-toarXiv preprint arXiv:2506.16381, speech systems, 2025. [17] OpenAI, Gpt-4o audio preview model doc- [Online]. Available: June 2025, umentation, https://platform.openai.com/docs/models/gpt-4oaudio-preview. [Accessed: Aug. 28, 2025]. [18] Ltd. Beijing Volcano Engine Technology Co., Endto-end real-time speech large model product introduction, [Online]. Available: https://www.volcengine.com/docs/6561/1594356. [Accessed: Aug. 28, 2025]. (in Chinese). June 2025, [19] Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, et al., Baichuan-audio: unified framework for end-to-end speech interaction, arXiv preprint arXiv:2502.17239, 2025. [20] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al., Qwen2. 5-omni technical report, arXiv preprint arXiv:2503.20215, 2025."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Fudan University"
    ]
}