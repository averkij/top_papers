{
    "paper_title": "Discovering Hierarchical Latent Capabilities of Language Models via Causal Representation Learning",
    "authors": [
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade",
        "Hanlin Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose a causal representation learning framework wherein observed benchmark performance is modeled as a linear transformation of a few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as a common confounder. Applying this approach to a comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify a concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal a clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, a step critical to accurately uncovering the underlying causal relationships among latent model capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 1 8 7 3 0 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Discovering Hierarchical Latent Capabilities of\nLanguage Models via Causal Representation Learning",
            "content": "Jikai Jin1, Vasilis Syrgkanis1, Sham M. Kakade2, and Hanlin Zhang2 1Stanford University 2Harvard University Abstract Faithful evaluation of language model capabilities is crucial for deriving actionable insights that can inform model development. However, rigorous causal evaluations in this domain face significant methodological challenges, including complex confounding effects and prohibitive computational costs associated with extensive retraining. To tackle these challenges, we propose causal representation learning framework wherein observed benchmark performance is modeled as linear transformation of few latent capability factors. Crucially, these latent factors are identified as causally interrelated after appropriately controlling for the base model as common confounder. Applying this approach to comprehensive dataset encompassing over 1500 models evaluated across six benchmarks from the Open LLM Leaderboard, we identify concise three-node linear causal structure that reliably explains the observed performance variations. Further interpretation of this causal structure provides substantial scientific insights beyond simple numerical rankings: specifically, we reveal clear causal direction starting from general problem-solving capabilities, advancing through instruction-following proficiency, and culminating in mathematical reasoning ability. Our results underscore the essential role of carefully controlling base model variations during evaluation, step critical to accurately uncovering the underlying causal relationships among latent model capabilities."
        },
        {
            "title": "1 Introduction",
            "content": "State-of-the-art large language models (LMs) have exhibited exceptional proficiency across wide spectrum of intricate natural language processing tasks, encompassing text generation, summarization, question answering, and creative language synthesis (BMR+20, AAA+23, GDJ+24, Ant24, AAA+24, YYZ+24, GYZ+25). These billion-parameter models are often pre-trained extensively on diverse web corpora and undergoes various post-training stages including supervised fine-tuning (SFT), reinforcement learning with human feedback (RLHF) (OWJ+22, BJN+22) to enable downstream model deployment. These complicated system engineerings make it hard to evaluate how models acquire capabilities and derive scientific claims thereafter. In particular, rigorous evaluation of post-training presents notable difficulties: (i) Costs and heterogenity in pre-training: implementation details such as data mixture, model architecture, etc, are often proprietary and vary greatly across institutions. For example, models might be subject to contamination on benchmark data (GS23); the heterogeneity of base models leads to evidence that the benefits of post-training on reasoning abilities can differ substantially even between models of comparable size (GCS+25, ZMK+25, HBU+25). Even with transparent pre-training recipes, training from scratch to control for these confounders through rigorous controlled studies implies prohibitive 1Blog post: https://hanlin-zhang.com/causal-capabilities. 1 costs (CRF+24, QNA+25). (ii) Intricate interdependencies among distinct capabilities such as reasoning, few-shot learning, and instruction-following further complicates evaluation. For example, fine-tuning on instruction data might not improve knowledge-intensive question answering capabilities (GEK+24, GWS+23). Although various capabilities often seem to co-emerge or interact synergistically as model scale increases (OWJ+22, ZCY+25, CSDC25, LBS+25), rigorous theoretical frameworks can aid in understanding which capability to target in post-training. Our research pioneers novel approach to these persistent challenges by introducing the first framework for modeling capability factors through an explicit structured representation. The cornerstone of our methodology rests on crucial insight: the heterogeneity observed across diverse domains (HZZ+20, JS24, ZXNZ24) rather than being merely an obstacle actually provides valuable \"multi-view\" perspectives into the shared latent capability structures across different base models. This lens enables us to identify and characterize these underlying structures with strong guarantees. To establish the theoretical foundation for our investigation, we propose two hypotheses: 1. Capability-Performance Invariance: small, distinguishable set of latent capability factors governs benchmark performance, maintaining consistent relationships across diverse base models. 2. Hierarchical Capability Structure: Within any individual base model, these capabilities organize themselves into hierarchical framework representable as directed acyclic graph (DAG) (Pea95). In this structure, an edge signifies that interventions targeting capability can propagate through the models internal mechanisms to influence capability B, revealing causal pathways of skill development. Base Model Capability Capability Capability Fine-tuning (Intervention) Hierarchical Structure of Model Capabilities Pretraining Effect Capability Effect Capability Effect Direct Fine-tuning Indirect Fine-tuning Capability Benchmark While the first hypothesis has been explored in series of recent studies (RMH24, RBK+25, PSC+24), the second represents novel contribution to the literature, although the hierarchical structure of human capabilities has been an active and influential research area in philosophy (Sim12), cognitive science (Car93, And96, ABB+04, KCB09, TKGG11) and neuroscience (KOK03, BD07). To illustrate this idea, consider language model fine-tuned on instructionfollowing data: such tuning may indirectly improve its ability to solve mathematical problems, since successful solutions often require adhering to precise formatting and logical sequencingskills closely tied to instruction-following. This hypothesis formalizes common intuition: like instruction-following, some capabilities, serve as foundational building blocks, while others, such as math problem-solving, emerge as higher-level skills that depend on these core abilities. In this context, it is essential to control for the base model being used, as it influences all downstream capabilities. We formalize this hierarchical capability structure within Pearls structural causal model framework (Pea95), treating the base model as shared latent parent that influences all capability factors and fine-tuning as an intervention on these latent factors, as illustrated in Figure 1. Under this structural hypothesis, existing unstructured factorization approaches (such as PCA) for analyzing latent capaFigure 1: Example of Hierarchical model of capabilities influencing benchmark performance (top) and hypothesized mechanism (bottom). Capability Capability Capability Source variable 2 Source variable 1 Source variable 3 Benchmark 1 Benchmark 2 Benchmark 2 bilities (RMH24, RBK+25, PSC+24) may fail to disentangle hierarchical latent factors due to their lack of causal constraints. Probabilistic latent-variable approaches, such as Item Response Theory (IRT) models (TTL+25) and Bayesian latent factor models (PN22), require full likelihood specifications and hand-crafted modeling assumptions. Moreover, all these approaches fail to account for the base models overarching influence on all latent capabilities. Drawing inspiration from the causal representation learning (CRL) literature, we propose Hierarchical Component Analysis (HCA) that exploits heterogeneity across base models to recover hierarchical latent capabilities with provable identifiability guarantees under mild conditions. We apply HCA to the open LLM leaderboard data2 and show that models fine-tuned from four base models: Qwen2.5-7B, Qwen2.5-14B, Llama-3-8B, and Llama-3.1-8B can be well-explained by linear SCM. We further assign meaningful semantic interpretations to these factors, allowing practitioners clear understanding of which capabilities to target during fine-tuning. Indeed, establishing explicit alignments between learned latent factors and human-interpretable concepts has remained both significant and underexplored area within the CRL literature. To address this gap, we systematically explore correlations between latent factors, established benchmarks, and the effectiveness of prevalent leaderboard interventions. Moreover, performance on the generalreasoning parent nodeencompassing benchmarks such as MMLU (HBB+20) and BIG-Bench-Hard (SSS+22)correlates more strongly with base-model FLOPs, underscoring the importance of scaling up pre-training compute for downstream problem solving. The remaining sections are organized as follows. In Section 1.1, we introduce necessary notations for subsequent sections. In Section 2, we revisit existing PCA approaches and discuss the ignored heterogeneity caused by the base model being used. We show that this finding can help us impute missing model performance data with higher accuracy then the naive matrix completion performed on the whole leaderboard. In Section 3, we present our key hypothesis and formulate the capability discovery task as an instance of causal representation learning, and introduce the HCA algorithm with theoretically optimal identifiability guarantee. In Section 4, we present experimental results by applying our algorithm to the leaderboard data, and discuss hypothetical interpretations of the capabilities factors that we learn."
        },
        {
            "title": "1.1 Notation\nMost analysis of this work is based on the open LLM leaderboard, which contains the accuracy of\nN0 = 4576 LMs on d = 6 benchmarks. The leaderboard does not directly provide information on\nbase model,3 so we develop a principled approach to determine the base model from information\nin other columns. In this way, we get a subset of models on the original leaderboard that contains\nN = 3360 LMs with known base models, which we denote by Θ. The eight most frequently-used\nbase models includes Llama-3-8B, LLama-3.1-8B, (GDJ+24), Qwen2.5-0.5/7/14B (YYZ+24), Qwen2-\n7B (YYZ+24), Mistral-7B (JSM+23) and Gemma-2-9B (TRP+24). Some parts of our analysis also\ninclude other base models into study, which we will explicitly describe. We will denote these eight\nbase models by θ∗\nk as\nbase model.\nFor any LM θ and benchmark B, we use xθ,B to denote the accuracy of θ on B, if observed. In our\nsetting, we observe xθi,k,Bj for all k ∈ [K], i ∈ [Nk] and j ∈ [d], and we will simply denote this by\n(cid:1)d\nx(k,i)\nj=1 is the observed performance vector for model θi,k. The set of all data,\nj\n(cid:8)x(k,i) : k ∈ [K], i ∈ [Nk](cid:9), is denoted by X. We also define X (k) = (cid:8)x(k,i) : i ∈ [Nk](cid:9). In what",
            "content": "8. We let Θk = {θ1,k, , θNk,k} Θ be the set of models using θ . Then x(k,i) = (cid:0)x(k,i) 2, , θ 1, θ 2https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/ 3There is column named Base model in the leaderboard, but usually that base model is itself fine-tuned version of, say, Llama-3-8B. 3 follows, we will sometimes abuse notation and view as matrix, where each row is performance vector xj ."
        },
        {
            "title": "2 The Latent Capability Hypothesis",
            "content": "Recently, line of works developed observational scaling laws (RMH24, RBK+25, PSC+24). The key hypothesis that make their analyses possible is that the observed benchmark performance is some linear transformation of low-dimensional latent capability vectors. Hypothesis 0. There exists some latent capability vector Rd0, d0 < and some matrix Rdd0, such that = Gz. exhibits (a) Low-rank structure of the leaderboard data. (b) Distance distributions of domain data to the principal component subspace. Figure 2: PCA analysis showing low-rank structure and domain heterogeneity in leaderboard data. component Principal (PCA) proanalysis vides systematic approach to validate by hypothesis this whether examining model-accuracy the matrix an approximate low-rank structure. Applying PCA to the leaderboard data, we find that the performance matrix is approximately rank-3 (as shown in Figure 2a), aligning closely with previous findings reported by existing works. We next examine whether these patterns persist when we vary the model subset used for PCA. To this end, we isolate all Qwen2.5 variants (across scales) and, separately, the Qwen2.514B model. As shown in Figure 2b, the distributions of their distances from the full-leaderboard rank-3 PC subspace diverge substantially. This divergence implies that one-size-fits-all PCAapplied indiscriminately to every modelcan obscure meaningful heterogeneity unique to particular model families. To further examine this heterogeneity, we choose the eight most commonly used base models on the leaderboard as listed in Section 1.1, and examine the PC subspaces of their corresponding domains. Specifically, for each = 1, 2, , 12, we apply PCA to the domain data Xk to obtain the rank-3 principal component subspace, and then measure the cosine distances between these subspaces. The similarity matrix is shown in Figure 3, revealing striking pattern: five domains (with base models Llama-3-8B, Llama-3.1-8B, Qwen2-7B, Qwen2.5-7/14B) have roughly the same PC subspaces, whereas the other three lie distinctly apart. We define Sinv = {1, 2, 4, 5, 6} to be the index set of these seven models and Xinv = kSinvXk to be the corresponding benchmark performance data. Notably, this heterogeneity persists under ICA (HHH+09), another popular factor analysis method, since PCA and ICA span the same component Figure 3: Principal component subspace similarity across domains. 4 subspace, differing only in how they parametrize the independent sources within it. Overall, we have found that: Performance data exhibit distinct heterogeneity among models fine-tuned from different base models. This underscores the necessity of controlling for base models when interpreting statistical results from benchmark evaluations. particular subset of base models emerges, wherein their fine-tuned derivatives consistently reveal similar latent performance patterns across benchmarks. In what follows, we build on these observations and introduce novel latent factor model for LM capabilities. (a). Random missing pattern. (b). Block missing pattern. Figure 4: RMSE of global and local matrix completion approaches for two types of missing patterns. Remark 1. As brief detour, we show how our findings can help us more accurately impute missing benchmark performance in the leaderboard. We focus on two distinct missing patterns, illustrated in Figure 4, and we are interested in the missing performances of models fine-tuned on Qwen2.5-14B. Motivated by the heterogeneity we observed in the PC subspaces, we apply matrix completion with nuclear norm regularization (MC-NNR) both within the Qwen2.5-14B domain and across the entire leaderboard. We find that the former approach yields notably lower reconstruction error. Additional details are provided in Figure 16."
        },
        {
            "title": "2.1 A Refined Hypothesis",
            "content": "In view of the limitation of Hypothesis 0, we propose the following modification, restricting it to domains with identical PC subspaces the we identify in Figure 3: Hypothesis 1. The observed benchmark performance xi Xinv is governed by set of latent capability factors zi Rd0, where d0 d. Moreover, there exists linear and injective relationship between zi and xi, meaning that there exists some matrix Rdd0 with full column rank such that xi Gzi, Xinv. In the remainder of this work, we will focus on the base models in Sinv and their corresponding benchmark performance data Xinv. 3 Learning Hierarchical Language Model Capabilities In this section, built upon the initial observations in Section 2, we introduce new approach to capture invariant laws underlying different domains. Our approach leverages latent hierarchical structure among different capability components in zi. To formally describe this latent structure, we introduce the following definition of linear structural causal models (SCMs) (Pea95). Definition 1. Given directed acyclic graph (DAG) = (V, E) with node set = [d0] and edge set E, linear SCM is data-generating process of d0 random variables z1, z2, , zd0 with zi = (cid:80) jpaG(i) wjizj + ϵi, [d] with independent source variables ϵi wuth unit variance, where wij are weights and σ1/2 paG(i) is the parent set of in G. Intuitively, latent factors earlier in the topological ordering of the DAG are primitive, while later factors are progressively less primitive, as they inherits the variability in their ancestors. In practice, assuming exact SCMs is often too restrictive. We define inexact SCMs below, which allows the source variables to be entangled with each other: ε1 z1 ε z2 ε3 z3 Definition 2. linear α-inexact SCM is data generating process of z1, z2, , zd0 with ˆϵ = ϵ, zi = (cid:80) ˆϵi, [d0] for some independent source variables ϵi wuth unit variance and some matrix = [u1, , ud0] Rd0d0 with ui2 = 1 and 1 ij α. Finally, d0 for collection of αi-inexact linear SCMs sharing the same causal graph G, we define α = maxi αi to be the maximum inexactness coefficient (MIC) of C. jpaG(i) wjizj + σ1/2 i=j (cid:80)d0 When α = 0, an α-inexact SCM becomes an exact SCM. Hence, the MIC measures the extent of violating the independence assumption on the source variables. Given this definition, we are ready to state our second hypothesis. graphical illustration of exact and inexact SCMs is given in Figure 5. (a) standard SCM with zis being the causal factors. ε2 z2 ε z3 ε1 z1 (b) An inexact SCM where the ϵis can be dependent. Figure 5: Illustration of Definition 1. Hypothesis 2. There exists subset of domain indices in Sinv, such that for all S, the capability factors z(k,i) associated with x(k,i) Xk are generated from linear inexact SCMs with some small MIC, and the causal graph is invariant across all ks, while the weights and errors can be domain specific and denoted with w(k) ˆϵ(k) . ij and Different from all existing works that are restricted to correlation-based analysis, Hypothesis 2 characterizes causal generative mechanism underlying an LMs capabilities. Specifically, given base model Bk, each independent factor ϵi directly influences exactly one capability zi, while other capabilities are either unaffected by ϵi or affected only indirectly through zi. Since Hypothesis 1 and Hypothesis 2 need not hold for every data distribution, it is necessary to develop diagnostic that empirically tests their validity in our context. Once this is done, we pursue two objectives: (1) recover the latent capability factors that drive observed benchmark performance, and (2) characterize precisely how those capabilities map to performance outcomes. It turns out that these questions are closely related to recent advances in multi-domain causal representation learning (CRL) (JS24, ZXNZ24). In that setting, one assumes domains = {Ek : [K]} and dataset (k) = {x(k,i)}Nk i=1 associated with the k-th domain generated from the structural equations z(k,i) = Akz(k,i) + Ω1/2 ϵ(k,i), x(k,i) = Gz(k,i), [K], (1) where (Ak)ij = w(k) if there exists direct causal edge zj zi in the latent graph and otherwise it ij is zero, Ωk is diagonal matrix encoding the variances of source variables. is the shared mixing matrix. For convenience, we assume that the nodes of is sorted in topological order, i.e., zj zi implies < i. CRL seeks to uncover both the causal graph and the mixing map G. Table 1 6 M1 M3 Mk = Bk ε ε Common unmixing matrix B1 1 B1 2 B1 3 x (a) ICA mapping to source variables ε. (b) Decomposition of Mk that we need to recover. (c) The whole data generating process, where = (HH )1 is the mixing matrix. M1 M2 M3 Orthogonalized h2 h3 Principal Component (d) Finding orthogonalized matrices and extracting principal components. Figure 6: Illustration of our setting and the key row-residual extraction step in our algorithm summarizes the parallels and distinctions between this CRL framework and our LM capability model. Causal representation learning Our context: learning latent LM capabilities Domain set Observed dataset (k) = {x(k,i)}Nk Causal factors (k) = {z(k,i)}Nk i=1, [K] i=1, [K] Mixing matrix (invariant across different domains) Identification of exact causal models as base model. Each domain Ek is defined by base model Mk and the observed dataset Xk that contains the performance of all LM {θi,k}Nk i=1 that use θ x(k,i) Rd contains the known benchmark accuracies of the θi,k. z(k,i) is the unobserved d0-dimensional capability vector of θi,k that possesses some causal structure. We assume that d0 d. The observed benchmark performance is linear transformation of the underlying capability factors. This linear dependency does not change no matter what base model is chosen. We define the notion of inexact causal models, and the objective is to minimize the inexactness. Table 1: comparison between linear CRL and some key elements in our context. Prior work of (JS24) showed that for exact linear SCMs, assuming that the domains Ek satisfy richness assumption, the latent causal factors are identifiable up to benign ambiguity set, which for instance implies that one can recover the mixing matrix up to left multiplication of lowertriangular matrix for the causal model in Figure 5. However, the identification algorithm presented in (JS24) is sensitive to the exactness assumption. We propose novel identification algorithm, which we call Hierarchical Component Analysis (HCA), that is more robust to the inexactness of the SCM. The main ideas of HCA are discussed in Section 3.1, and more details can be found in Appendix C."
        },
        {
            "title": "3.1 Hierarchical Component Analysis (HCA)",
            "content": "In this section, we introduce the main ideas behind HCA, an algorithm for recovering hierarchical latent factors. 1. ICA-based unmixing. As first step, we apply Independent Component Analysis (ICA, (HHH+09)) separately to each domain [K] to obtain an unmixing matrix Mk that maps independent source variables to observed benchmark data x, as shown in Figure 6a. Under standard non-Gaussian assumptions in the ICA literature, these source varisbles are uniquely identified as ϵ(k) up to permutations, implying that Mk = Pk Bk H, where Pk is an unknown permutation matrix, Bk = Ω1/2 Our goal is to recover the matrices Bk and from Mk, as they allow us to recover the whole DGP as shown in Figure 6c. In partcular, the latent factors are recovered via = Hx. (I Ak) is lower-triangular, and = (GG)1G is the right inverse of G. 2. Row-residual extraction. For any matrices Mk, [K], we derive testable equivalent condition for admitting the decomposition Mk = BkH. specifically, for each component index [d0], we can compute the residual rk,i of projecting the i-th row of onto the span of its first (i 1) rows. Then such decomposition exists if and only if [rk,i]K k=1 is rank 1 for all i, and hi can be recovered (up to scale) as its principal singular vector. This process is visualized in Figure 6d. 3. Permutation alignment and factor refinement. Since each is known only up to row permutation, we search over all permutations of the rows of Mk. For each case, we apply the previous step to obtain an estimate of H, and then refine each domains weight matrix by solving minBk lower-triangular Mk Bk H2 , thereby fitting the best hierarchical structure to the observed unmixing matrices. Finally, we choose the set of permutations that induces minimal MIC. The full description HCA appears in Algorithm 2, and Appendix C.2 proves that, under an exact SCM, HCA is guaranteed to identify the underlying causal factors up to some benign ambiguities. Specifically, for the causal graph in Figure 5, is recovered up to left multiplication of lowertriangular matrix. Equivalently, each identified latent causal factor for zi is mixture of zj, 1 i. As shown in (JS24), this ambiguity is not limitation but rather an intrinsic property reflecting equivalent models that generate identical distributional outcomes. When the SCM is inexact, HCA recovers data generating process z(k,i) = ˆB1 ˆϵ(k,i), x(k,i) = ˆGz(k,i), [K], so that ˆϵ(k,i) = ˆBk source components, so one can see that ˆϵ(k,i) = Jkϵ(k,i) where Jk = ˆBk provides guarantee on the MIC (introduced in in Definition 2): ˆHx(k,i). On the other hand, the ICA recovers ϵ(k,i) = Mkx(k,i) with independent )1. This (MkM ˆHM Proposition 1. Suppose that the ICA step is exact, then HCA recovers linear αk-inexact SCM for the k-th ij, ( Jk)i = (Jk)i/(Jk)i2. It follows that α = maxk[K] αk is valid domain, where αk = 1 d0 MIC. i=j( Jk)2 (cid:80) Proposition 1 provides quantitative measure of how well the recovered causal model can explain the variations in the observed benchmark data (k), [K]. ε1 z1 -5.69 +2.76 ε2 +24 +0.27 +0.06 ε3 z3 ε +4.08 +6.10 +1.52 z1 ε2 + z2 -0.00 +0.31 ε3 z3 +5. ε1 z1 ε2 +18 +7.04 +2. z2 -0.20 +0.59 ε3 z3 -6. ε1 z1 ε2 -16 +6.00 +1. z2 -0.22 +0.54 ε3 z3 +6. (a) Llama-3-8B (b) Llama-3.1-8B (c) Qwen2.5-7B (d) Qwen2.5-14B Figure 7: The causal graphs that we recover for each domain. The numbers represent the weights of each causal edge. For instance, in the Llama-3-8B domain, z2 = 2.76z1 + 24ϵ2."
        },
        {
            "title": "4.1 Recovery of the Data Generating Process",
            "content": "Given its theoretical justification in the previous section, we now use HCA to recover causal model with d0 = 3 nodes that explains the observed benchmark performance of models within domains in Sinv. We observe that running our algorithm on the subset of {1, 2, 4, 5}, with Qwen2-7B excluded, achieves minimal MIC of 0.04. This likely indicates that Qwen2-7B may deviate from the shared causal pattern of the other four base models (Llama-3-8B, Llama-3.1-8B, Qwen2.5-7B, Qwen2.5-14B). Moreover, in view of the ambiguity discussed in Section 3.1, we run an OLS zi (cid:80) j<i ajzj +γBxB +c where xB represents the performance on benchmark B. For each i, we pick that maximizes the R2 and replace zi with zi (cid:80) j<i ajzj to attain best-possible alignment between the recovered latent factors and their most indicative benchmarks. (a) Adjusted unmixing matrix. (b) Regressing z1 on BBH. (c) Regressing z2 on IFEval. (d) Regressing z3 on MATH. Figure 8: The unmixing matrix and the alignment between benchmarks and capabilities via OLS. We also compare the fitted OLS with the latent factor values of other base models. The causal graphs that we recover are shown in Figure 7. The source factors ϵis are normalized to have unit variance. In Figure 8a, we present the unmixing matrix (i.e., linear mapping from benchmarks to latent capabilities), from which interesting patterns can be observed: z1 is mixture of all five benchmarks except IFEval with BBH and MMLU-Pro contributing the most, z2 is mixture of IFEval and MATH Lvl 5, and z3 is almost identical to MATH Lvl 5. We will revisit these observations in the next subsection. Figure 8 further shows the results of OLS, where z1, z2, z3 are observed to correlate strongly with BBH, IFEval and MATH Lvl 5, respectively. It is also important to notice that the causal conclusions we draw only apply to the four base models being considered: Figure 8 shows that the fitted OLS can have poor performance on some other base models."
        },
        {
            "title": "4.2 Towards a Causal Hierarchy of Interpretable Capabilities",
            "content": "In the previous subsection, we explored the correlation between benchmark performance and the inferred latent factors. However, practically interpreting what causal intervention entails within this framework remains unclear. The broader challenge of interpreting and intervening on 9 Figure 9: Sigmoid scaling law for BBH performance. latent factors is longstanding and unresolved issue within causal representation learning, with no universal methodology currently available. By further analyzing the Llama-3 and Qwen2.5 models included in leaderboards, we propose hypotheses regarding these latent capabilities, supported by reliable empirical evidence. Interpreting z1 (Foundational General Capability). As root node in the causal graph, z1 root node in our causal graph, likely represents foundational, generalized capability. This interpretation is supported by its positive influence across nearly all benchmarks (see mixing matrix in Figure 8a), consistent with the expectation that enhancing general capability should broadly improve downstream task performance. Interestingly, we find that model performances on benchmarks well-aligned with general capabilities, such as BBH, roughly follows sigmoid scaling law described by: L/(1 + exp(k(log log C0)) + τ + b, where L, k, C0, b, τ are unknown parameters, is the pretraining compute and is binary variable distinguishing fine-tuned models (T = 1) from from pretrained-only models (T = 0) as shown in Section 4.2. This relationship suggests that LMs general capabilities are predominantly determined by pretraining compute resources and experience comparatively modest enhancements during subsequent post-training procedures. Interpreting z2 (Instruction Following). z2 strongly correlates with IFEval, suggesting it embodies instruction-following capability. We extract instruction-tuned models from the Open LLM leaderboard, excluding those specific for math reasoning, which act as proxies of intervention on z2. These models show minimal changes on BBH, MMLU-Pro, GPQA and MUSR for the first three base models, aligning with its mixing pattern shown in Figure 8a. We also conduct supervised fine-tuning (SFT) directly on IFEval and observe similar patterns. Interpreting z3 (Advanced Mathematical Reasoning). z3 highly correlates with the MATH Lvl 5 benchmark, suggesting it represents advanced mathematical reasoning capability. Isolating this capability through fine-tuning is challenging; targeted mathematical finetuning often causes catastrophic forgetting (ZTL+23), reducing performance on other tasks and likely affecting z1 and z2. Identifying fine-tuning strategies that selectively enhance mathematical reasoning (z3) without negatively impacting other core capabilities Table 2: Performance comparison of language models before and after fine-tuning with IFEval SFT. BASE and INSTRUCT model results are sourced from the open LM leaderboard. Reported values represent averaged performance across all INSTRUCT models, excluding specialized math reasoning variants. BBH IFEval MATH GPQA MUSR MMLU-PRO Base IFEval SFT Instruct Base IFEval SFT Instruct Base IFEval SFT Instruct Base IFEval SFT Instruct 0.05 0.05 0.11 0.37 0.38 0. 0.33 0.32 0.30 0.23 0.28 0.33 0.44 0.43 0.42 0.32 0.33 0.30 0.40 0.36 0.32 0.45 0.43 0. 0.29 0.32 0.55 0.13 0.12 0.17 0.44 0.42 0.42 0.36 0.33 0.34 0.33 0.33 0.35 0.44 0.44 0. 0.46 0.49 0.51 0.12 0.50 0.53 0.54 0.55 0.52 0.33 0.50 0.61 0.53 0.52 0.49 0.36 0.55 0. 0.61 0.63 0.64 0.41 0.40 0.41 0.53 0.52 0.59 0.16 0.63 0.57 Qwen2.5-14B Gemma-2-9B Qwen2.5-7B Llama-3-8B Config Model 10 (z1, z2) remains critical open question. Furthermore, both observational instruct model performance and interventional SFT demonstrate substantial improvement on MATH, supporting the hypothesized causal link from z2 (instruction-following) to z3 (mathematical reasoning). This influence likely occurs because mathematical tasks demand precise adherence to instructions for correct formatting and problem interpretation, where misunderstandings severely impact accuracy. Additionally, the causal effect of z2 on z3 is larger for Qwen models than for Llama models, consistent with the weights of the causal graphs shown in Figure 7."
        },
        {
            "title": "5 Discussions",
            "content": "In this work, we initiate the study of causal relationships between LM capabilities. We conclude this work with few takeaways and remarks that may be useful to practitioners. For post-training evaluation: Our results demonstrate that the impact of any fine-tuning intervention can differ substantially across base models. Evaluation studies therefore are expected to specify exactly which pre-trained checkpoints their methodology applies to. To quantify these heterogeneous effects, one can employ standard causal-inference tools such as estimating the conditional average treatment effect (CATE) to measure, with statistical rigor, how fine-tuning impacts performance on each base models. For model developers: The directed, hierarchical structure of capabilities we uncover suggests clear development priority: given sufficient compute budget, one can focus on scaling up pretraining FLOPs which are more correlated with upstream parent node z1 performance and can, and gains there cascade to more specialized abilities. That said, not every capability is equally malleable. Some like instruction-following correlate less with model scale (i.e., FLOPs) and exhibit large noise-factor variances (the z2 node in Figure 7), indicating they respond more readily to post-training. On the other hand, given limited budgets or for small models, our noise-weight estimates suggest that we may need other interventions like instruction tuning to further improve downstream performance. For model evaluators: Due to the inherent hierarchical structure of evaluation suites, it is important to examine fine-grained performance beyond aggregate numerical scores. For example, gains on the MATH benchmark may partly stem from improved instruction-following, which, while related to, is not equivalent to the mathematical reasoning the benchmark aims to evaluate. Secondly, as specialized abilities are causally affected by upstream ones, evaluators can therefore prioritize designing benchmarks that evaluate general, foundational capabilities, such as BBH and MMLU-Pro. These benchmarks reflect more substantive improvements rather than artifacts of limited domain adaptation. We leave as future work the use of tapestry of tools in causal inference, such as matching (Stu10), stratification (FR02), doubly robust estimation (BR05), etc, to derive more scientific insights from observational language model benchmark data."
        },
        {
            "title": "Acknowledgement",
            "content": "VS acknowledges the support from the NSF Award IIS-2337916. SK acknowledges the support from the National Science Foundation Grant under award IIS 2229881; SK and HZ acknowledge the Chan Zuckerberg Initiative Foundation for establishing the Kempner Institute for the Study of Natural and Artificial Intelligence. 11 References [AAA+23] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [AAA+24] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [ABB+04] John Anderson, Daniel Bothell, Michael Byrne, Scott Douglass, Christian Lebiere, and Yulin Qin. An integrated theory of the mind. Psychological review, 111(4):1036, 2004. [ABGLP19] Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019. [AG23] Sanjeev Arora and Anirudh Goyal. theory for emergence of complex skills in language models. arXiv preprint arXiv:2307.15936, 2023. [And96] John Anderson. Act: simple theory of complex cognition. American psychologist, 51(4):355, 1996. [Ant24] Anthropic. Claude 3.5 sonnet. https://www.anthropic.com/news/ claude-3-5-sonnet, 2024. [ASR+21] Kartik Ahuja, Shiori Sagawa, Harikrishnan Ramaswamy, Edward Kung, Yair Carmon, David Krüger, Amy Zhang, and Percy Liang. Invariant risk minimization games. In 38th International Conference on Machine Learning (ICML), volume 139 of Proceedings of Machine Learning Research, pages 145159, 2021. [AVST24] Emre Acartürk, Burak Varıcı, Karthikeyan Shanmugam, and Ali Tajer. Sample complexity of interventional causal representation learning. Advances in Neural Information Processing Systems, 37:3935039385, 2024. [Bar24] Matthew Barnett. An empirical study of scaling laws for transfer. arXiv preprint arXiv:2408.16947, 2024. [BCE+23] Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [BD07] David Badre and Mark DEsposito. Functional magnetic resonance imaging evidence for hierarchical organization of the prefrontal cortex. Journal of cognitive neuroscience, 19(12):20822099, 2007. [BdHLC22] Johann Brehmer, Pim de Haan, Phillip Lippe, and Taco S. Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems 35 (NeurIPS), pages 1706617079, 2022. [BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [BR05] Heejung Bang and James Robins. Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962973, 2005. [BRR+23] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Schölkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. Advances in Neural Information Processing Systems, 36:4541945462, 2023. [BSB+23] Ryan Burnell, Wout Schellaert, John Burden, Tomer Ullman, Fernando MartinezPlumed, Joshua Tenenbaum, Danaja Rutar, Lucy Cheke, Jascha Sohl-Dickstein, Melanie Mitchell, et al. Rethink reporting of evaluation results in ai. Science, 380(6641):136138, 2023. [Car93] John Bissell Carroll. Human cognitive abilities: survey of factor-analytic studies. Cambridge university press, 1993. [CCZ16] Tianxi Cai, Tony Cai, and Anru Zhang. Structured matrix completion with applications to genomic data integration. Journal of the American Statistical Association, 111(514):621633, 2016. [CHL+24] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):1 53, 2024. [CPY+23] Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Skills-in-context prompting: Unlocking compositionality in large language models. arXiv preprint arXiv:2308.00304, 2023. [CRF+24] Ben Cottier, Robi Rahman, Loredana Fattorini, Nestor Maslej, and David Owen. The rising costs of training frontier ai models, 2024. [CSDC25] Pin-Yu Chen, Han Shen, Payel Das, and Tianyi Chen. Fundamental safety-capability trade-offs in fine-tuning large language models. arXiv preprint arXiv:2503.20807, 2025. [DODH25] Ricardo Dominguez-Olmedo, Florian E. Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. In The Thirteenth International Conference on Learning Representations, 2025. [EK04] Jan Eriksson and Visa Koivunen. Identifiability, separability, and uniqueness of linear ica models. IEEE signal processing letters, 11(7):601604, 2004. [FR02] Constantine Frangakis and Donald Rubin. Principal stratification in causal inference. Biometrics, 58(1):2129, 2002. [GCS+25] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. 13 [GDJ+24] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [GEK+24] Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Dinesh Manocha, et al. closer look at the limitations of instruction tuning. arXiv preprint arXiv:2402.05119, 2024. [GS23] Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493, 2023. [GWS+23] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [HBB+20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [HBU+25] Andreas Hochlehnert, Hardik Bhatnagar, Vishaal Udandarao, Samuel Albanie, Ameya Prabhu, and Matthias Bethge. sober look at progress in language model reasoning: Pitfalls and paths to reproducibility. arXiv preprint arXiv:2504.07086, 2025. [HDN+24] Zhenyu Hou, Pengfan Du, Yilin Niu, Zhengxiao Du, Aohan Zeng, Xiao Liu, Minlie Huang, Hongning Wang, Jie Tang, and Yuxiao Dong. Does rlhf scale? exploring the impacts from data, model, and method. arXiv preprint arXiv:2412.06000, 2024. [HHH+09] Aapo Hyvärinen, Jarmo Hurri, Patrik Hoyer, Aapo Hyvärinen, Jarmo Hurri, and Patrik Hoyer. Independent component analysis. Springer, 2009. [HZZ+20] Biwei Huang, Kun Zhang, Jiji Zhang, Joseph Ramsey, Ruben Sanchez-Romero, Clark Glymour, and Bernhard Schölkopf. Causal discovery from heterogeneous/nonstationary data. Journal of Machine Learning Research, 21(89):153, 2020. [JS24] Jikai Jin and Vasilis Syrgkanis. Learning linear causal representations from general In The Thirty-eighth Annual environments: Identifiability and intrinsic ambiguity. Conference on Neural Information Processing Systems, 2024. [JSM+23] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 14 [KCB09] James Kaufman, Jason Cole, and John Baer. The construct of creativity: Structural model for self-reported creativity ratings. The Journal of Creative Behavior, 43(2):119134, 2009. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [KOK03] Etienne Koechlin, Chrystele Ody, and Frédérique Kouneiher. The architecture of cognitive control in the human prefrontal cortex. Science, 302(5648):11811185, 2003. [LAD+22] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [LBL+23] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. [LBS+25] Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, et al. Not-just-scaling laws: Towards better understanding of the downstream impact of language model design decisions. arXiv preprint arXiv:2503.03862, 2025. [LPR+20] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In 37th International Conference on Machine Learning (ICML), volume 119 of Proceedings of Machine Learning Research, pages 63486359, 2020. [MLH+22] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1104811064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. [MLP+23] Ian R. McKenzie, Alexander Lyzhov, Michael Martin Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Xudong Shen, Joe Cavanagh, Andrew George Gritsevskiy, Derik Kauffman, Aaron T. Kirtland, Zhengping Zhou, Yuhui Zhang, Sicong Huang, Daniel Wurgaft, Max Weiss, Alexis Ross, Gabriel Recchia, Alisa Liu, Jiacheng Liu, Tom Tseng, Tomasz Korbak, Najoung Kim, Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isnt better. Transactions on Machine Learning Research, 2023. [OWJ+22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 15 [Pea95] Judea Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669688, 1995. [PN22] Panagiotis Papastamoulis and Ioannis Ntzoufras. On the identifiability of bayesian factor analytic models. Statistics and Computing, 32(2):23, 2022. [PSC+24] Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Yuekai Sun, and Mikhail Yurochkin. Sloth: scaling laws for llm skills to predict multi-benchmark performance across families. arXiv preprint arXiv:2412.06540, 2024. [QNA+25] Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric Xing, Sham Kakade, and Hanlin Zhang. Evolm: In search of lost language model training dynamics, 2025. Manuscript. [RBK+25] Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan Kim, et al. Safetywashing: Do ai safety benchmarks actually measure safety progress? Advances in Neural Information Processing Systems, 37:6855968594, 2025. [Rec11] Benjamin Recht. simpler approach to matrix completion. Journal of Machine Learning Research, 12(12), 2011. [RMH24] Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. Observational scaling laws and the predictability of langauge model performance. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. [Sim12] Herbert Simon. The architecture of complexity. In The Roots of Logistics, pages 335361. Springer, 2012. [SLB+21] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612634, 2021. [SMK23] Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage? Advances in Neural Information Processing Systems, 36:55565 55581, 2023. [SSBU23] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In International conference on machine learning, pages 3254032560. PMLR, 2023. [SSS+22] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. [Stu10] Elizabeth Stuart. Matching methods for causal inference: review and look forward. Statistical science: review journal of the Institute of Mathematical Statistics, 25(1):1, 2010. [SWR+22] Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian 16 Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. [TKGG11] Joshua Tenenbaum, Charles Kemp, Thomas Griffiths, and Noah Goodman. How to grow mind: Statistics, structure, and abstraction. science, 331(6022):12791285, 2011. [TRP+24] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [TTL+25] Sang Truong, Yuheng Tu, Percy Liang, Bo Li, and Sanmi Koyejo. Reliable and efficient amortized model-based evaluation. arXiv preprint arXiv:2503.13335, 2025. [vKBW+23] Julius von Kügelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Kekic, Elias Bareinboim, David Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal representations from unknown interventions. Advances in Neural Information Processing Systems, 36:4860348638, 2023. [WJ22] Yixin Wang and Michael I. Jordan. Desiderata for representation learning: causal perspective. arXiv, 2022. [WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [YKG+23] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix: flexible and expandable family of evaluations for ai models. arXiv preprint arXiv:2310.17567, 2023. [YYZ+24] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [ZCY+25] Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, and Linda Ruth Petzold. Unveiling the impact of coding data instruction finetuning on large language models reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2594925957, 2025. [ZGS+23] Jiaqi Zhang, Kristjan Greenewald, Chandler Squires, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. Advances in Neural Information Processing Systems, 36:50254 50292, 2023. [ZLCF24] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning: The effect of data, model and finetuning method. In The Twelfth International Conference on Learning Representations, 2024. 17 [ZMK+25] Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining. arXiv preprint arXiv:2504.07912, 2025. [ZTL+23] Yuexiang Zhai, Shengbang Tong, Xiao Li, Mu Cai, Qing Qu, Yong Jae Lee, and Yi Ma. Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313, 2023. [ZXNZ24] Kun Zhang, Shaoan Xie, Ignavier Ng, and Yujia Zheng. Causal representation learning from multiple distributions: general setting. In Forty-first International Conference on Machine Learning, 2024. 18 20 21 22 22 24 26 28 30 31 31 33 34 36"
        },
        {
            "title": "Table of Contents",
            "content": "A Related Work Implementation Details Details of HCA and Its Theoretical Guarantee C.1 HCA: Hierarchical Component Analysis . . C.2 Identifiability Guarantee for HCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional PCA Analyses Scaling Laws and The Effect of Fine-tuning . E.1 Heterogeneity of Fine-tuning Effects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Details for Matrix Completion Additional Experiment Results G.1 Details for the HCA Recovery in Section 4 . . G.2 Complementary Results for Section 4 . . . G.3 Filtering Out \"badly\" Fine-tuned models . . . G.4 Using Open LM Leaderboard v1 . . . . G.5 MMLU by Task Leaderboard . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Sensitivity Analysis of Causal Graph Recovery"
        },
        {
            "title": "A Related Work",
            "content": "Benchmark-driven LM capability studies. Benchmarks give researchers shared scoreboard, letting everyone check claims about better language models instead of relying on hype. Early scaling-law studies showed that test loss falls in smooth power curve as model size, data, and compute grow, setting baseline for how capability should rise (KMH+20). Later work found many frontier models were under-trained for their size and mapped out compute-efficient path that the Chinchilla model follows (HBM+22). Instead of running new model sweeps, (RMH24) proposed observational scaling laws involving latent capability factors, that depend on the model famility and are obtained by PCA. They showed that benchmark performances are inherently low-rank and 3 principal components are sufficient to obtain good fitting performance. This approach is also adopted by some follow-up works on new tasks (RBK+25) and larger sets of models (PSC+24). (DODH25) further proposed an adjustment of the scaling law based on the model release time, given the fact that later models are more likely to be \"trained on test tasks\". While pretrained LMs exhibit predictable scaling laws post-training presents more complex picture regarding such predictive capabilities. For fine-tuning, performance generally scales with model size and fine-tuning data (as suggested by (ZLCF24)), but the \"transfer gap\" between pre-training and downstream tasks is key variable (Bar24), and pre-training metrics arent always reliable predictors of post-tuning success. Instruction tuning demonstrates clear benefits from scaling model size and the number/diversity of instructional tasks, as shown by work on FLAN (WTB+22), T0 (SWR+22), and FLAN-PaLM (CHL+24). RLHF, crucial for aligning models with human preferences (OWJ+22), shows performance gains with larger models and more feedback. However, recent work (HDN+24) indicates RLHF might scale less efficiently than pre-training, with potential diminishing returns from increased data or reward model size under fixed conditions. Recently, evaluation of post-training has been shown to be unreliable (BSB+23, HBU+25). While the Open LLM Leaderboard provides truthful evaluation of models across different benchmarks, the contamination issue may still prevent us to obtain reliable assessment of model capabilities. Connections between LM capabilities. Research increasingly shows that LM capabilities are not isolated but form complex, interconnected system. Studies reveal strong synergies, such as the bidirectional enhancement between coding and reasoning abilities (ZCY+25, BCE+23), and how strong reasoning underpins mathematical problem-solving (LAD+22). Complex skills often arise from compositionality, where LMs combine simpler, foundational skills in novel ways (YKG+23, AG23, CPY+23). Evidence also points towards latent abilities or general factors influencing performance across diverse tasks (LBL+23, PSC+24). The nature of emergent abilities skills appearing in larger models is debated, with some questioning if they are genuinely novel or byproducts of other mechanisms (WTB+22, SMK23). Also, there are significant trade-offs: efforts to enhance safety can sometimes reduce raw capability (CSDC25), and fine-tuning for one skill can lead to catastrophic forgetting of others (ZTL+23). Phenomena like inverse scaling further highlight these complex interactions (MLP+23). Finally, successful task transfer and in-context learning demonstrate that LMs leverage shared underlying mechanisms and representations across different tasks (MLH+22, BMR+20), underscoring the deep interrelations among their varied skills. Causal representation learning. Causal representation learning (CRL) aims to recover latent variables and mechanisms that remain stable under interventions and distribution shifts, thereby enabling robust prediction, reasoning, and control. Foundational position papers argue that learning 20 disentangled causal factors is essential for machine intelligence rather than merely desirable for interpretability (SLB+21, WJ22). Most existing works are devoted to establishing identifiability of causal representations in realistic scenarios. Weakly supervised disentanglement shows that paired samples before/after unknown interventions are sufficient to identify factors without compromising downstream utility (LPR+20, BdHLC22). (vKBW+23) showed that pair of single-node hard interventions on each latent factor is sufficient for full identifiability of the latent causal factors. Subsequent works generalize this to the case of single-node soft interventions (ZGS+23, SSBU23, BRR+23). Recently, there has been surge of interest in studying identifiability under multi-node interventions, which is much more practical (JS24, ZXNZ24). Closely related to CRL, invariant Risk Minimization (IRM) and its game-theoretic variants formalize how multiple training environments can pin down causal predictors (ABGLP19, ASR+21)."
        },
        {
            "title": "B Implementation Details",
            "content": "We release our code at https://github.com/hlzhang109/causal-eval. Supervised Fine-tuning. We use lm-eval-hardness to evaluate models before and after fine-tuning. We first test base model performance and observe that it can match the performance in Open LM Leaderboard. We train all models with standard hyper-parameters for SFT - 3 epochs, learning rate 2e-5. Moreover, noticing that the IFEval dataset lacks ground truth responses followed by the instructions, we query GPT-4 to generate responses with the prompt \"You are helpful assistant evaluating instruction-following ability. For each prompt, provide ONLY direct response to the specific instruction, prefixed with Response: . Keep your response concise, clear, and strictly follow the instruction without adding explanations or unnecessary information. Your response (excluding the Response: prefix) should strictly satisfy the length requirement.\" Moreover, we also SFT on z1 BBH. But we observe marginal improvement over the same BBH test sets. We hypothesize that parent nodes like z1 are more dependent on base model FLOPs thus maybe hard to improve through fine-tuning alone. Matching models on the leaderboard with the base models. Our algorithm for mapping LLMs to their pretraining token counts implements hierarchical, multi-layered identification strategy with progressively decreasing confidence levels. The approach consists of four distinct identification layers: 1. Explicit Base Model Detection: We first parse the model name for explicit references to base models with size specifications (e.g., Llama-3.1-8B). This is implemented through specialized regular expression patterns tailored to each model familys naming conventions. For instance, Gemma-2-9B is unambiguously matched to the Gemma-2-9B model trained on 8 trillion tokens. 2. Model Name Pattern Inference: For models lacking explicit base references, we perform broader pattern matching on model names, scanning for family indicators (e.g., mistral, qwen2.5) and version numbers. This layer identifies the model family but may not precisely determine the variant, necessitating parameter-based disambiguation in some cases. For example, detecting llama-3 in the name identifies the family but requires parameter count verification to distinguish between 8B and 70B variants. 3. Architecture-based Attribution: Lastly, we leverage architecture information combined with parameter counts. This approach varies by model family: For Llama models, we employ stringent parameter matching (e.g., 7.8-8.3B for Llama-3-8B) to prevent false positives, as many models adopt the Llama architecture without using Llama weights. 21 For other architectures (e.g., Mistral, Qwen), we implement more generous parameter ranges and higher confidence attribution, as architecture adoption typically indicates weight inheritance. Size-variant mapping is crucial for families like Gemma-2, where pretraining compute differs by size (2B: 2T tokens, 9B: 8T tokens, 27B: 13T tokens). The algorithm traverses these layers sequentially, defaulting to the highest-confidence identification available. When all layers fail to produce sufficient confidence match, the algorithm returns null rather than making low-confidence attributions. This ensures precision over recall, maintaining the reliability of identified mappings. Upon successful model identification, we retrieve the corresponding pretraining token count from our comprehensive knowledge base, which consolidates information from research papers, technical reports, and official documentation. This multi-layered approach balances completeness with accuracy, addressing the inherent ambiguity in model naming and metadata across the diverse landscape of contemporary LLMs."
        },
        {
            "title": "C Details of HCA and Its Theoretical Guarantee",
            "content": "C.1 HCA: Hierarchical Component Analysis In (JS24), the authors introduced the LiNGCReL algorithm identfiability guarantees in Theorem 1 for exact SCMs. Here we introduce hierarchical Component Analysis (HCA) that is equivalent to LiNGCReL in the exact setting, but with several modifications to make it fit into our context. The first step, same as (JS24), is to apply linear ICA to each individual domain. Recall that ICAs goal is to the independent signals; in our setting, it recovers the ICA unmixing matrix Mk that maps observed to the source variables ϵ(k) defined in Equation (1). This shall be carefully distincted from = (GG)1G which is the unmixing matrix for CRL. When the SCM is exact, we would have PkMk = BkH, where Pk is some permutation matrix. The main challenge of CRL is that we only know Mk = BkH, [K] but each Pk is unknown. The second and main part of our algorithm is presented in Algorithm 2. The algorithm is motivated by the observation that, since the unmixing matrix is the same across all domains, the structure of any row spaces of Bk, [K], which are unknown, is captured by the row structures of the known ICA unmixing matrices BkH. Moreover, given an already-recovered subgraph G1 of G, one can discover some / G1 such that paG(v) G1, if the corresponding rows in each Bk, after projecting onto the row spaces corresponding to the the orthogonal complement of the row space of already-recovered nodes, is rank-1. This is because this rank captures the \"remaining degree of freedom\" of conditioned on G1, which equals one if and only if all its parents are in G1. While this idea is close to the original LiNGCReL, some key differences are worth-noticing: 1. Compared with LiNGCReL, HCA only recovers transitive closure of the true graph G4. It is still possible to infer whether each edge in indeed exists in (see appendix). For simplicity and due to the fundamental inexactness of our model, we do not perform this step here. Equivalently, we are only imposing the constraint that each Bk is upper-triangular, without assuming that any other entries are also zero. 2. The identifiability guarantee of LiNGCReL makes the restrictive assumption that the distribution ϵ(k) does not depend on k. This assumption is indeed unnecessary; the price to pay is more 4The transitive closure of directed acyclic graph (DAG) is obtained by drawing an edge for any and such that is an ancestor in j, i.e., there is path = i0 i1 ik = in G. 22 complicated approach to identify the \"correct matching\" between the components of ϵ(k). This step could be computationally expensive, but works well in our context where is small. 3. We determine the matrices Bk, [K] by explicitly optimizing the distance between the recovered unmixing matrix and the target unmixing matrix. Compared with LiNGCReL that sets the entries of Bks as the projection coefficients in Algorithm 1, which is theoretically equivalent for exact causal models, this extra step provides additional flexibility that optimizes the fitting quality in the presence of inexactness. Algorithm 1: Ortho-proj(S, {Ak}K k=1) Input: Ordered set = {s1, s2, . . . , sm} [d], index / S, Ak Rdn for [K]. Output: Residual matrices {Rk}K for 1 to do k=1. span{(Ak)s : S}; // (Ak)s is the s-th row of Ak Rk projW (Ak);// Row-wise orthogonal projection end Algorithm 2: Hierarchical component analysis Input: Matrices Mk Rdn, [K]. Output: The optimal unmixing matrix ˆH and weight matrices { ˆB k=1. Let Sd be the set of all permutations of {1, 2, . . . , d}. min_mic_score ; ˆH null; { ˆB for each permutation combination π = (π1, . . . , πK ) (Sd)K do k=1 null; k}K k}K Apply the current permutation to each matrix // 1. Let // 2. for 0 to 1 do be the matrix Mk with rows permuted according to πk, for = 1, . . . , K. ; Generate candidate ˆHπ based on permuted matrices k}K k=1 Ortho-proj(Sortho, {M Sortho {j + 1, + 2, . . . , d}; {R // Extract principal direction from the (j+1)-th rows of residuals [(R j+1 v1( R); // Top right singular vector j+1]; // Stack the (j+1)-th rows j+1, . . . , (R k=1); ) k}K 1) end ˆHπ [h // 3. Let (d) be the set of upper-triangular matrices. ; for 1 to do 1, . . . , Compute optimal upper-triangular ˆBk,π d]; // Construct candidate H. Optionally: Gram-Schmidt(h 1, . . . , d) ˆBk,π arg minBT (d) ˆHπ2 ; // Best upper-triangular estimate Compute the MIC score for this permutation using Proposition 1 Update if this is the best score found so far k}K k=1, { ˆBk,π}K k=1, ˆHπ) ; end // 4. current_mic_score ComputeMIC({M // 5. if current_mic_score < min_mic_score then min_mic_score current_mic_score ; k=1 ˆHπ, { ˆBk,π}K ˆH , { ˆB k}K k=1 ; end end return ˆH , { ˆB k}K k=1 23 C."
        },
        {
            "title": "Identifiability Guarantee for HCA",
            "content": "In this subsection, we provide our main identifiability result for HCA in the special case when the graph is known to be complate DAG with for all < j. Equivalently, this means that each Ak is lower-triangular. Assumption 1. (Node-level non-degeneracy, adapted from (JS24, Assumption 5)) We assume that the matrices {Bk}K k=1 are node-level non-degenerate, i.e., for all node [d], we have dim span (Bk)i : [K] = (cid:12)paG(i)(cid:12) (cid:12) (cid:12) + 1, where (Bk)i is the i-th row of Bk. As shown in (JS24), this assumption holds as long as the weight vectors at node across domains do not lie in low-dimensional vector space, which generally holds. To ensure identifiability, we also require that the components of noise variables are non-Gaussian and have different distributions. Assumption 2. For all [K], each component of ϵ(k) follows different distribution, and all of them are non-Gaussian. Remark 2. With more involved procedure, (JS24) showed that one can identify zi, [d] up to \"surrounding node ambiguity\" in the case when is unknown. Specifically, this means that the can be fully recovered and the identified factor is some linear combination of zjs with surG(i) := {i} {i paG(i) : chG(i) chG(i)}. Moreover, this ambiguity is intrinsic in this setting. Our main result is stated below: Theorem 1. Suppose that d, then if the ICA step is exact, one can recover the mixing matrix up to left multiplication of lower-triangular matrix. Equivalently, it recovers latent factors is linear mixture of the true latent factors zj, < i. where 1, , The remaining part of this subsection is devoted to proving Theorem 1. By our assumption of the causal model, we know that in the k-th domain, the observations and the noise variables are related via ϵ(k) = BkHx. Since we assume that the ICA is exact, the uniqueness of ICA in the non-Gaussian setting (EK04) implies that the umixing matrix that leads to independent source variables must be unique up to row permutations. In other words, there exists some permutation matrix Mk = BkH, [K]. Without loss of generality, we also assume that is orthonormal, since otherwise one can always consider QR factorization = where is lower-triangular and is orthonormal, and one can treat BkU as the new Bk. Recall that our algorithm goes through all possibilities of permutations Pk, [K] and pick one with the smallest MIC. To begin with, it is not hard to see the following fact: Proposition 2. Suppose that would give zero MIC. Mk, then running the subroutine in Algorithm 2 on , such that k, [K] = = BkH. We will prove by induction that each row Proof. Recall that ˆHπ in Algorithm 2 is parallel to hi (*). For = 1, since Bk is lower-triangular, and its diagonal entries Ω1/2 of BkH, [K] is nonzero multiple of h1. By definition, rows, which is obviously parallel to h1. Suppose the conclusion holds for all < j0, we now prove it for = j0. Since Bk is lower-triangular, the induction hypothesis implies that for each [K], span(M k)i : < = spanhi : < j. By are nonzero, so the last rows 1 is the principal component of these of the recovered matrix 24 definition, (Rk)j is the orthogonal projection of (Mk)j onto this subspace. Notice that (Mk)j spanhi : j, is then easy to see that this projection is nothing but constant multiple of hj, since this is the unique direction in spanhi : that is orthogonal to spanhi : j. Hence by definition, we have From (*), it is easy to see that the best lower-triangular estimate ˆBk,π is equal to up to some row-wise scaling, and that ˆBk,π = αjhj for some scalar αj. This concludes the proof of (*). k. Hence the MIC is zero by definition. ˆHπ = To complete the proof of Theorem 1, we need to show that any permutation that achieves zero MIC successfully recovers the causal graph up to transitive closure. Specifically, suppose that some permutation matrices Pk, [K] leads to zero MIC, let Qk = PkP , then Mk = QkBkH. We show that 1. Q1 = Q2 = = Qd, and 2. Suppose that the j-th row of Q1 is eij , then i1, i2, , id is topological ordering of the graph G, meaning that paG(ij) {i1, , ij1}. We say that row index is \"good\" if the j-th row of Qk, [K] are equal and the second condition above is satisfied up to (i.e. i1, , ij is an ancestral set of G), and is \"bad\" otherwise. Then it suffices to show that all [d] are good. Suppose the contrary holds, let = j0 be the smallest bad index. zero MIC implies that [K], (MkM )1 = λkjej. (cid:35) ( ˆBk)jiˆhi (cid:34) (cid:88) i=1 (cid:35) Hence, (cid:34) (cid:88) i= ( ˆBk)jiˆhi λkj(Mk)j (MkM )1 = 0 (cid:88) i= ( ˆBk)jiˆhi λkj(Mk)j , (2) where is the row space of H. The last step holds since the row space of Mk is also . However, by induction hypothesis, the first (j 1) rows of Mk are exactly the i1, i2, , ij1-th rows of BkH. The construction of ˆH and ˆBk imply that the s-th (s < j) row of ˆH is equal to the is-th row of H, and the first (j 1) rows of ˆBk are equal to the i1, i2, , ij1-th rows of Bk. Let Vi = spanˆh1, , ˆhi, then we have that projV j1 ( ˆBk)jiˆhi λkj(Mk)j (cid:32) (cid:88) i=1 (cid:33) (cid:16) = projV j1 ( ˆBk)jj ˆhj λkj(Mk)j (cid:17) = ( ˆBk)jjprojV j1 (ˆhj) λkjprojV j1 (Mk)j. However, equation 2 implies that this quantity is equal to zero. As result, we have (cid:68) rank projV j1 (Mk)j : [K] (cid:69) = 1. Let the j-th row of Qk be ejk, [K]. Then the above equation becomes (cid:68) rank projV j1 (BkH)jk : [K] (cid:69) = 1. In the following, we show that this property can only hold when is good. Note that projV j1 (BkH)jk = (cid:88) (Bk)jk,iprojV j1 (hi), paG(jk){i1, ,ij1} 25 (3) (4) (5) (a) Qwen2-7B (b) Gemma-2-9B (c) Llama-3-8B (d) Mistral-7B (e) Qwen2.5-7B (f) Qwen2.5-14B (g) Llama-3.1-8B (h) Qwen2.5-0.5B Figure 10: Results for running PCA on individual domains. (BkH)jl where paG(i) = paG(i) {i}. For = l, equation 4 implies that projV (hi), [d] {i1, , ij1} are independent, are colinear, but since has full row rank, projV so we must have paG(jk) {i1, , ij1} = paG(jl) {i1, , ij1}. In particular, jk paG(jl) and jl paG(jk), so we must have jk = jl. Thus j1 = j2 = = jK. By Assumption 2, (BkH)jl and projV j1 j1 j1 rank (BkH)j1 : [K] = (cid:12) (cid:12)paG(j1)(cid:12) (cid:12) + 1, so that (cid:68) rank projV j1 (BkH)jk : [K] (cid:69) (cid:12) (cid:12)paG(j1)(cid:12) (cid:12) + 1 (cid:12) (cid:12)paG(j1) {i1, , ij1}(cid:12) (cid:12) . Hence it mus be the case that paG(j1) {i1, , ij1}, concluding the proof."
        },
        {
            "title": "D Additional PCA Analyses",
            "content": "In this section, we examine different ways to choose the domains from the open LM leaderboard and discuss our findings. Domain-specific PCA. While Figure 2a indicates that the complete leaderboard dataset is approximately low-rank, this global characteristic does not inherently imply similar low-rank structure for benchmark performance data within individual domains. It is plausible that some domains possess full-rank data, but these higher-rank properties are obscured or averaged out when the entire leaderboard is considered. To investigate this, we performed PCA on the eight domains containing the largest number of model entries. As illustrated by the analysis of their leading principal components in Figure 10, all examined domains are effectively rank-3, with the exception of Gemma-2-9B, which exhibits an approximate rank of 2. PCA for more base models. We first provide an extended version of Figure 3 for 20 most frequently used base models of the open LM leaderboard. Mixture of Experts (MoE). We investigate the MoE architecture, which is used by Mixtral, and more recently, by Deepseek. The information of whether model uses the MoE architecture is directly available from our leaderboard. In Figure 12a, we plot the principal component subspace distances 26 Figure 11: Pairwise cosine distance matrix for 20 base models. between MoE models and non-MoE models. We also include two architectures upon which vast majority of MoE models are built. We can see that there is little difference in the principal component subspaces. Different relative sizes of and D. While the pretraining compute 6N is well-known to directly affect the model performance, the precise roles of and remain unclear. Our PCA results in Figure 12b considers four domains of data that contain models with small and large D, large and small D, small and small D, large and large respectively. The finding is intriguing it shows that the small , large domain has principal component subspace that is quite different from the other domains. Further investigation by controlling for base models show that this is just coincidence, as shown in Figure 12c. In this figure, we consider domains corresponding to the two most frequent base models for each domain used in Figure 12b. We find that while three principal component subspaces in Figure 12b look similar, they are actually the mixture of domains with very different principal component subspaces. This further hightlights the importance of controlling for the base model in causal analysis, as in the approach of our main work. 27 (a) MoE v.s. non-MoE. (b) Different and Ds. (c) Different model types. time. Figure 12: PCA Results comparing principal component subspaces for different criteria. (d) Model uploaded Different uploaded time. Lastly, we define domains according to which year the model is uploaded. In Figure 12d, we find that there is clear watershed between 2023 and 2024. Similar findings are also made in (DODH25), where the authors argue that after November 2023, \"training on test task\" becomes more prevalent. It should be noticed that similarity of PC subspaces is necessary but not sufficient conditions for our causal analysis. Domains with similar PC subspaces may not be explained by linear causal model. Moreover, we apply causal analysis to domains defined by base models primarily because this helps us remove all confounders related to the pretraining stage. On this other hand, difference in PC subspaces likely indicate some heterogeneous causal patterns. We leave the analyses of these patterns to future work. Scaling Laws and The Effect of Fine-tuning Existing predictive models for language model performances are typically restricted to pretrained models. This is not unexpected, since it is hard to characterize the performance gains in post training in terms of the relevant factors. In this section, we point out some of the key challenges in understanding the effect of fine-tuning. Figure 13: The average benchmark performance of fine-tuned models on the open LM leaderboard with three base models in different sizes. As illustrated in Figure 13, models fine-tuned on more powerful base models tend to exhibit uniformly better performance across all benchmarks. In other words, base model is common confounder of all benchmark performances. We observe that base model also confounds the amount of improvment one can achieve on all benchmarks. To illustrate this point, we estimate the average treatment effect (ATE) of on all six benchmarks of the open LM leaderboard using the backdoor adjustment formula E[Y do(T )] = (cid:82) E[Y T, = x]pX (x)dx, where = log(C) is the log 28 pretraining compute and pX () its density. As illustrated in Figure 14, fine-tuning yields substantial gains on math reasoning and instruction-following benchmarks, while producing little to negative change on general reasoning and QA-based tasks. Examining Llamaversus Qwen-based variants separately, we observe that Qwen models gain more from fine-tuning on math reasoning and instruction-following, yet incur larger drops on general reasoning. Remark 3. Caution is warranted when interpreting the causal implications of the estimates presented in Figure 14. These values represent true Average Treatment Effects (ATEs) only when the conditional ignorability assumptionfundamental to causal inferenceis satisfied. In our context, this assumption requires that different base models experience equivalent \"distributions of interventions\" across tasks. For example, if Qwen demonstrates superior performance gains compared to Llama on mathematics-related tasks, conditional ignorability would be violated if researchers strategically selected Qwen models more frequently for mathematical applications to maximize performance outcomes. We contend that this assumption is difficult to substantiate in practice. An important open research question remains: how might we circumvent this methodological challenge when limited to observational performance data? Developing robust approaches that account for such selection biases represents significant opportunity for future work in this domain. Figure 14: Estimates of the average effect of fine-tuning. E.1 Heterogeneity of Fine-tuning Effects Scaling law (KMH+20) has been widely adopted to predict the benchmark performance from pretraining compute. Later, (RMH24, RBK+25) used sigmoid scaling laws to fit the principal components of performance data from multiple benchmarks. Could scaling law alone explain the leaderboard data? To investigate this question, we let 6 be the pretraining compute, and fit sigmoid regression equation 1+exp(k(log Clog C0) + τ + b, where L, k, C0, b, τ are unknown parameters, and is binary treatment variable indicating whether model is fine-tuned or pretrained. We fitted sigmoid curve to the benchmark results of all officially released models on the leaderboard (see Figure 15). Our findings indicate that scaling laws more faithfully describe trends on BBH, MMLU-Pro and GPQAthan the remaining benchmarks. Our conjecture is that the former three benchmarks are more \"knowledge-driven\", in the sense that many questions in these benchmarks merely test whether the model possesses cetain knowledge. As result, fine-tuning, mainly focusing on reasoning and alignment, can being negligible effect. By contrast, performances on tasks requiring other proficiencies (e.g. instruction following in IFEval, mathematical reasoning in MATH or multistep soft reasoning in MUSR) are much easier to improve by fine-tuning. Figure 15: Sigmoid scaling laws of benchmark accuracies for pretrained and fine-tuned models. Top row: all pretrained and fine-tuned models. Middle row: Llama-based models only. Bottom row: Qwen-based models only."
        },
        {
            "title": "F Details for Matrix Completion",
            "content": "In this subsection, we provide detailed description of the experimental setup in Remark 1. Specifically, our goal is to show how to accurately impute missing leaderboard data when the benchmark performances of LMs are only partially observed. Indeed, this task can be naturally viewed as an instance of matrix completion, where RN is the performance matrix for models and = 6 benchmarks, with missing entries. Restricting ourselves to the missing entries in one particular domain the group of models fine-tuned on Qwen2.5-14B we consider \"global\" and \"local\" approach to perform matrix completion. In the global approach, we apply nuclear norm regularization (NNR, (Rec11)) to the whole leaderboard data RN d, while the local approach only runs NNR on the submatrix D2 that only contains rows in I2, following the notation in Section 1.1. We conduct synthetic experiments to simulate two different scenarios. First, for the case when the benchmark accuracies are missing at random, we remove each entry of independently with probability = 0.8, as visualized in Figure 4 (a). Second, we consider \"block\" missing pattern as visualized in Figure 4 (b), where performance on two benchmarks are fully observed, while for the ramaining four benchmarks, the performance data for = 0.1, 0.2, , 0.9 fraction of models is missing. We repeat the experiment 1000 times for the first case, and for all (cid:0)6 (cid:1) = 20 possible sets of fully observed benchmarks of size 3 for the second case. Since standard NNR does not perform well on block missing entries, we use structured matrix completion that is designed specifically for handling this case (CCZ16). The RMSEs of the global and local approaches for these two cases are plotted in Figure 4. We can see that for the first case, the local approach is significantly more accurate than the global approach, despite the fact that it relies on fewer rows. For the second case, the local approach also performs better on average. 3 30 In Figure 16 we further plot the RMSEs for all 20 possible choices of fully observed columns. The remaining 6 3 = 3 columns have rows that are missing with = 0.5 probability. We observe that the local approach is always no worse than the global one, and in most cases, the local aproach leads to significant improvements. (a) {1, 2, 3} (b) {1, 2, 4} (c) {1, 2, 5} (d) {1, 2, 6} (e) {1, 3, 4} (f) {1, 3, 5} (g) {1, 3, 6} (h) {1, 4, 5} (i) {1, 4, 6} (j) {1, 5, 6} (k) {2, 3, 4} (l) {2, 3, 5} (m) {2, 3, 6} (n) {2, 4, 5} (o) {2, 4, 6} (p) {2, 5, 6} (q) {3, 4, 5} (r) {3, 4, 6} (s) {3, 5, 6} (t) {4, 5, 6} Figure 16: RMSEs of global v.s. local matrix completion for each possible set of fully observed columns. The caption below each figure indicates the columns that are fully observed. Each number representing benchmark on the leaderboard, with 1, 2, , 6 standing for IFEval, BBH, MATH Lvl 5, GPQA, MUSR and MMLU-Pro respectively."
        },
        {
            "title": "G Additional Experiment Results",
            "content": "G.1 Details for the HCA Recovery in Section 4 In this subsection, we provide more details and results for the recovery of the causal model in Section 4. First, we provide the visualization of the full DGP recovered by HCA before the OLS adjustment in Figure 17. The OLS adjustment essentially operates on the columns of the mixing matrix in Figure 17 by subtracting from the i-th column some linear combination of the j, < columns. We report the R2 for aligning all six benchmarks with the three capability factors in Table 3a. The findings are 31 0.03 0.03 0. 0.04 Inexactness coefficient Correlation matrices of source variables Nearly independent source variables ϵ Rd Causal weights (I Ak)Ω1/2 Latent causal factors Rd Mixing matrix Rnd Benchmark accuracies Rn5 Figure 17: HCAs recovery of the DGP, including the linear SCM (second column) and mixing matrix (fourth column) on four domains (base models): Llama-3-8B, Llama-3.1-8B, Qwen2.5-7B and Qwen2.5-14B. Here, we have = 6 benchmarks, = 3 latent factors and = 6 domains. particularly interesting if we consider what each benchmark is supposed to measure. Specifically, BBH and MMLU-PRO both contain tasks across different domains and are both related to language understanding and general reasoning, which, intuitively, are more fundamental capabilities. IFEval tests models ability of answering questions in correct formats, which is built on top of the language understanding ability. Finally, the MATH Lvl 5 benchmark requires models to answer math questions correctly and in the correct format, which is the most ad-hoc capability built on all the previous ones. These intuitions precisely align with the hierarchical structure of capability factors that we recover. More discussions can be found in Section 4.2. caveat is that this causal structure is only guaranteed to hold for the four base models we consider. As shown in Table 3b, the fitted OLS model can have poor performance on other base models. IFEval BBH MATH GPQA MUSR MMLU-PRO z1 0.36 0.96 z2 0.92 0.53 z3 1.00 0. 0.56 0.66 1.00 0.73 0.57 0.18 0.57 0.58 0.14 0.96 0.54 0.16 (a) The R2 of running OLS on zi using zj, > and the benchmark performance as controls. In Sample Gemma-2-9B Mistral-7B Qwen2.5-0.5B Qwen2.5-3B Llama-2-7B Llama-2-13B Llama-3.2-1B z1 z2 z3 0.96 0.92 1 0.76 0.94 1 0.71 0.74 1 0.2 1.18 0.99 0.89 0.73 0.97 0.98 1 0.66 0.45 1 0.01 0.9 1 (b) The R2 of the fitted OLS on out-of-sample performance data with different base models. Table 3: The precise alignment of underlying factors with established benchmarks, coupled with their ability to extend effectively across diverse model domains. Figure 18: Overview of the MIC obtained by difference choices of domain indices. Here, as we indicated in Section 1.1, indices 1, 2, 4, 5, 7 correspond to base models Llama-3-8B, Qwen2.5-14B, Llama-3.1-8B, Qwen2-7B and Qwen2.5-7B respectively. G.2 Complementary Results for Section 4 MIC for all other domain subsets. In Figure 18, we report the corresponding MIC for all possible choices of domains in Sinv. We observe that the four subsets with smallest MIC are achieved by excluding Qwen2-7B. Additional metrics for the recovery results. Note that one potential limitation of MIC is that it is insensitive to the orthogonal complement component of each row in ˆB ˆH relative to Mk. Therefore, we present two additional metrics indicating how well our causal model fits the observed data, as shown in Table 4. We introduce these metrics since they directly measure how close ˆBk ˆH is to the true ICA mixing matrix Mk. Node z1 z3 Rank-1 error 0.05 0.13 0.02 (a) The amount of variation in defined in Algorithm 2 uncaptured by rank-1 matrix in each iteration. Domain Llama-3-8B Llama-3.1-8B Qwen2.5-7B Qwen2.5-14B Unmixing error 0.17 0.20 0.16 0.23 (b) The relative recovery error of the unmixing matrix of ICA for each domain, calculated from Mk BkHF /MkF , where Mk is the ICA unmixing matrix of the k-th domain, Bk is the inverse of the recovered weight matrix, and is the unmixing matrix of CRL. Table 4: Additional metrics on how good our causal model explains the observed data. Low-rank approximation error of our causal model. Recall that we hypothesize that the data is generated from linear causal model with 3 nodes. This necessarily requires that the performance data across all 6 benchmarks is matrix with rank at most 3. While we have seen in Figure 2a that 33 (a) IFEval (b) BBH (c) MATH Lvl 5 (d) GPQA (e) MUSR (f) MMLU-Pro Figure 19: Approximation error of the low-rank latent factor space for the observed benchmark performances. this is approximately the case, here we revisit this assumption and see investigate the error induced by this assumption. In Figure 19, we plot the approximation errors of the subspace spanned by the values of three latent factors zi, = 1, 2, 3 learned via our algorithm. One can see that the low rank subspace approximates 4 out of 6 benchmarks nearly perfectly. The relatively poor fitting for the remaining two, namely GPQA and MUSR, is partially due to the fact that the models accuracies on them are systematically lower than the remaining ones. As result, they would be ignored to some extent when picking the principle components. In terms of the MSE, the error of fitting GPQA is comparable to the remaining four, while that of MUSR is significantly higher. This highlights limitation in our current methodology: although we introduce the notion of inexact causal graph for more flexibility, the assumption that each two latent factors have causal relationship is still restrictive. For instance, it is possible that z1, z2 are correlated but there exists no causal relationship between them, and both of them causally affect z3. It will be an interesting future direction to investigate how to identify the latent factors in these cases. G.3 Filtering Out \"badly\" Fine-tuned models We notice that some models on the leaderboard are badly fine-tuned, so that their benhmark performances are even worse than the pretrained model. In this subsection, we provide results of our causal analysis with these bad models removed. Removing the bad models allow us to characterize the hierarchical relationship between capabilties that is restricted to \"good\" fine-tuning strategies. The recovered DGP is shown in Figure 20. After adjusting for the ambiguity as we did in Section 4, we obtain the causal graphs shown in Figure 21. Finally, in Figure 22, we plot the unmixing matrix after adjustment and the relationship between each latent factor and the most indicative benchmark. 34 The overall pattern that our algorithm discovers is the same as the unfiltered approach. However, we notice that in the filtered case, the MIC is much larger, indicating that the causal model is less well-fitted. This is likely due to the fact that after filtering, the variance of performances on BBH and MMLU-Pro becomes significantly smaller, so that the weights of GPQA and MUSR in z1 are larger compared with the unfiltered case (see Figure 8a). These two benchmarks are relatively not well-explained by our linear causal model, as we discussed in Appendix G.2. 0.05 0.07 0.10 0. Inexactness coefficient Correlation matrices of source variables Nearly independent source variables ϵ Rd Causal weights (I Ak)Ω1/2 Latent causal factors Rd Mixing matrix Rnd Benchmark accuracies Rn Figure 20: HCAs recovery of the DGP after removing badly fine-tuned models that have average performance lower than the pretrained model. ε1 z1 +7.59 +0. ε2 z2 ε3 ε1 +13 +5. +25 -0.36 +2.01 +0.31 z3 ε2 z2 -14 +0.27 ε3 ε +7.43 +11 -0.33 +2.35 z3 ε2 z2 +13 +0.32 +0.12 ε ε1 +7.97 +5.22 -0.18 z3 ε2 z2 -14 +0.06 -0.51 ε z3 -8.50 (a) Llama-3-8B (b) Llama-3.1-8B (c) Qwen2.5-7B (d) Qwen2.5-14B Figure 21: The causal graphs recovered for different models. The numbers represent the weights of each causal edge. For instance, in the Llama-3-8B model, z2 = 0.59z1 + 13ε2 (representing direct influences shown). (a) Adjusted unmixing matrix. (b) Regressing z1 on BBH. (c) Regressing z2 on IFEval. (d) Regressing z3 on MATH. Figure 22: The unmixing matrix and the alignment between benchmarks and capabilities via OLS. 35 G.4 Using Open LM Leaderboard v1 We also apply our method to analyze the hierarchical structure underlying the six benchmarks used in the old version of open LM leaderboard. We choose the following six base models that are most commonly used there: Mistral-7B, Llama-2-13B, Llama-3-8B, Llama-2-7B, Llama-2-70B and Mixtral-8x7B. Similar to our previous case, we plot the pairwise cosine distance between domains in Figure 23b. We denote these models by M1, , M6. We observe that except for Llama-2-7B, the principle component subspaces of all remaining domains are pretty close to each other, so that the invariant domain is Sinv = {1, 2, 3, 5, 6}. This is quite interesting, since Mixtral-8x7B uses MoE architecture, which is fundamental difference compared with the other base models. We then run HCA on all subsets of Sinv of size 3 and plot the corresponding MIC in in Figure 23c. We observe that choosing all domains in Sinv would still lead to small error. The corresponding recovered DGP is presented in Figure 24. We further adjust for the ambiguity as in Section 4, and obtain the causal graphs shown in Figure 25. Finally, the adjusted unmixing matrix and the alignment between latent factors and benchmarks are presented in Figure 26. From Figure 26, we can see hierarchical relationship from truthfulness to general reasoning capability, and math reasoning capability. The hierarchical relationship between the latter two is consistent with our findings on the Open LLM Leaderboard v2. By looking at the causal graphs, one can observe that the weight of the edge z2 z3 for the Llama-3-8B domain is much larger that that of the remaining ones, which indicates that models fine-tuned on Llama-3-8B could have more performance gains on math problem solving when fine-tuned to enhance general reasoning. (a) Low-rankness of the old Open LM leaderboard data. (b) The PCA similarity matrix for Open LM leaderboard (old version). (c) domains defined via pretraining compute. Figure 23: figures for our analysis of Open LLM leaderboard v1. 36 0.01 0.00 0.03 0. 0.02 Inexactness coefficient Correlation matrices of source variables Nearly independent source variables ϵ Rd Causal weights (I Ak)Ω1/2 Latent causal factors Rd Mixing matrix Rnd Benchmark accuracies Rn Figure 24: Results for applying our method to open LM leaderboard v1. ε1 z1 +6. +0.31 ε2 z2 +7.57 +0.42 -0. ε3 z3 +9.12 ε1 z1 +8. +0.44 ε2 z2 ε3 ε1 +4. +17 +6.70 +0.94 +0.46 ε2 ε3 ε1 +4.46 +18 +12 +2. +0.34 ε2 z2 ε3 ε1 -5. -15 +8.59 +1.40 +0.34 +0.56 z1 -0.01 z3 z1 +0.42 z1 ε2 z2 -13 +1.39 -0. ε3 z3 -17 (a) Llama-2-13B (b) Llama-2-70B (c) Llama-3-8B (d) Mistral-7B (e) Mixtral-87B Figure 25: The causal graphs recovered for different models. The numbers represent the weights of each causal edge. For instance, in the Llama-2-13B model, z2 = 0.31z1 + 7.57ε2 (representing direct influences shown). (a) Adjusted unmixing matrix. (b) Regressing z1 on TruthfulQA. (c) Regressing z2 on ARC. (d) Regressing z3 on GSM8K. Figure 26: The unmixing matrix and the alignment between benchmarks and capabilities via OLS. 37 Figure 27: Correlation matrix for the tasks in the MMLU benchmark. G.5 MMLU by Task Leaderboard The MMLU benchmark has total of 57 subtasks, each corresponding to distinct subject. It therefore makes sense to apply our methodology to these subjects and investigate their latent causal structure. To begin with, we first investigate the correlation between the performance of different tasks in MMLU, which is plotted in Figure 27. We observe that majority of tasks have highlycorrelated performance, although they seemingly focus on unrelated fields. This is likely due to the fact that MMLU primarily contains knowledge-based tasks, and crucially depends on the quality of the training dataset. Larger datasets likely contain more data in all disciplines and can hence lead to improvement on all tasks. In terms of causality, this means that there exists single \"knowledge\" node for the MMLU benchmark as whole. Math-related subjects. We first select subjects that correspond to mathematics, including: MMLU_college_mathematics, MMLU_elementary_mathematics, MMLU_high_school_mathematics. In this setting, we choose the set of base models to be Mistral-7B, Llama-2-7B and Llama-2-70B, which induces minimal MIC of 0.02. The result of HCA is summarized in Figure 28c. Counterintuitively, it shows that z1 is close to college math, while z2, z3 likely represent elementary and high-school math. 38 (a) Correlations of the performance of math-related subtasks. (b) Low-rank structure of the subtasks performance data. (c) The learned mixing matrix. Figure 28: HCA analysis of the MMLU by Task Leaderboard data of math-related subjects. Physics-related subjects. We conduct similar analysis for Physics-related subjects. We choose the set of base models to be Mistral-7B, Mistral-8x7B, Llama-2-13B, Llama-2-70B, which induce minimal MIC of 0.05 among all domain subsets with size 4. The result of HCA is summarized in Figure 29c. We can see that z1 is conceptual physics while z2 and z3 are both linear cominations of high-school and college physics. (a) Correlations of the performance of physics-related subtasks. (b) Low-rank structure of the subtasks performance data. (c) The learned mixing matrix. Figure 29: HCA analysis of the MMLU by Task Leaderboard data of physics-related subjects. Cross-subject domains. It would also be interesting to explore how different subWe choose MMLU_college_mathematics, MMLU_college_physics and jects are related. MMLU_college_electrical_engineering and run HCA on these subjects. We found the hierarchical relationship exists in the order of math, electrical engineering and physics."
        },
        {
            "title": "H Sensitivity Analysis of Causal Graph Recovery",
            "content": "While very few works in the literature study finite sample guarantees for causal representation learning (AVST24), the accuracy of the recovered causal graph is crucial for downstream scientific studies. In this section, we conduct sensitivity analysis of the causal model we discover, and discuss its implications. Specifically, The causal model could be sensitive to the choice of base models. We discuss possible reasons for this. We find that the causal link between instruction following and math reasoning is stable. 39 (a) Correlations of the performance of tasks under different subjects. (b) Low-rank structure of the subtasks performance data. (c) The learned mixing matrix. Figure 30: HCA analysis of the MMLU by Task Leaderboard data of three different subjects Adding Two Base Models. We add two more base models, Qwen2.5-3B and Llama-3.2-3B, into our causal analysis. From Figure 11 we can see that these two domains have roughly the same PC subspace as the four domains we previously used in the main paper. Therefore, Hypothesis 1 would not be violated. That said, one caveat is that these two domains only include 65 and 94 models respectively, while the four previously used domains all include more than 150 models. We run the same algorithms on these six domains, and the results are summarized in Figure 32 and Figure 33. One can see that the pattern of the unmixing matrix, as shown in Figure 32a, is the same as Figure 8a in z2 ( IFEval) and z3 ( MATH Lvl 5) but different in z1. In particular, rather than being closely aligned with general reasoning benchmarks like BBH and MMLU-Pro, it represents some sort of tradeoff between general reasoning and specialized capabilities. Recall that given infinite samples, we show that the causal model is identifiable up to mixtures with ancestors, wich does not include the case of Figure 32a. Recall that for picking z1, the key idea was to find combination of rows in Mk that are colinear. In the finite sample regime and/or our causal assumptions are not exact, the error induced by this step could be hard to control. In Figure 31 we present the ICA unmixing matrices Mk. We find that among the domains chosen to conduct the causal analysis, there are two notable patterns: 1. There is row that is approximately weighted combination of BBH and MMLU-Pro, e.g. third row in the matrix of Llama-3-8B. Recall that these two benchmarks are highly correlated, so this is close to the general reasoning capability discovered in Figure 8a as z1. 2. There is another row that has positive weights on BBH and MMLU-Pro and negative weights on IFEval and MATH (or vice versa), e.g. the second row of Llama-3-8B. This row is aligned with the z1 that we discover here in Figure 32a. These observations indicate that one possible cause for the sensitivity of causal graphs is the environment non-degeneracy assumption (Assumption 2). even when the data comes from an exact causal graph, in the finite sample case, when there exists two is such that span(Bk)i, [K] is approximately rank-1, the corresponding causal factor can be hard to identify. Note that for base models that are not used in our causal analysis, namely Mistral-7B, Qwen27B, Qwen2.5-0.5B, Gemma-2-9B, Llama-2-7B and Llama-3.2-1B, the unmixing matrices do not demonstrate these patterns. This is paritcularly interesting observation that we left for future studies, since it likely reveals different latent knowledges learned by different base models. 40 Figure 31: The unmixing matrices of ICA for individual domains. Since algorithm might pick either one of the above as z1, and the induced errors are comparable, it is hard to argure which one makes more sense. Nonetheless, we find that the subspace spanned by the first two rows of Figure 8a and Figure 32a are roughly the same, and IFEval approximately lies in this subspace. Moreover, z3 always represents math reasoning. So, the causal effect from instruction following to math reasoning is stable across different choices of domains. Moreover, viewing the weight of the causal edge z2 z3 as the causal effect of fine-tuning on instruction following, we can see that this effect is generally much higher for Qwen models compared with Llama models, phenomenon that we also observe in Figure 7. We also notice that the estimated effect is roughly the same for Qwen models in these two approaches, while for Llama models, the weights are smaller here compared with the ones in Figure 7. Overall, further investigating the effect of finite-sample error and violations of causal assumptions is an important direction for future research. (a) Adjusted unmixing matrix. (b) Regressing z2 on IFEval. (c) Regressing z3 on MATH. Figure 32: The unmixing matrix and the alignment between benchmarks and capabilities via OLS. 41 ε2 z2 +5. +0.12 -0.37 +24 +1.26 (a) Llama-3-8B ε z2 +6.07 +0.41 -0.52 +24 +1. ε1 z1 ε1 z1 ε3 ε3 z3 +4.48 +7.58 ε2 +7.76 +0.14 -0.37 +33 +1.36 (b) Llama-3.1-8B ε2 z2 +9.37 +0.55 -0.64 + +1.39 ε1 z1 ε1 z1 ε z3 ε3 z3 +6.64 -8.60 ε z2 +6.04 +0.19 -0.45 +19 +1. (c) Llama-3.2-3B ε2 z2 +9.52 0.57 -0. +20 +1.36 ε1 z1 ε1 ε3 z3 ε3 z3 +2.74 +4. (d) Qwen2.5-14B (e) Qwen2.5-7B (f) Qwen-2.5-3B Figure 33: Causal graphs recovered from each domain."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Stanford University"
    ]
}