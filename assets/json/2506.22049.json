{
    "paper_title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling",
    "authors": [
        "Tianhao Chen",
        "Xin Xu",
        "Zijing Liu",
        "Pengxiang Li",
        "Xinyuan Song",
        "Ajay Kumar Jaiswal",
        "Fan Zhang",
        "Jishan Hu",
        "Yang Wang",
        "Hao Chen",
        "Shizhe Diao",
        "Shiwei Liu",
        "Yu Li",
        "Yin Lu",
        "Can Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 9 4 0 2 2 . 6 0 5 2 : r GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling Tianhao Chen1, Xin Xu1, Zijing Liu2, Pengxiang Li3, Xinyuan Song4, Ajay Kumar Jaiswal5, Fan Zhang1, Jishan Hu1, Yang Wang1, Hao Chen1, Shizhe Diao6, Shiwei Liu7, Yu Li2, Yin Lu8, Can Yang1 1The Hong Kong University of Science and Technology 3Dalian University of Technology 4Emory University 2International Digital Economy Academy 5University of Texas at Austin 6NVIDIA 7University of Oxford 8University of Surrey {tchenbb, xxuca}@connect.ust.hk, l.yin@surrey.ac.uk, macyang@ust.hk"
        },
        {
            "title": "Abstract",
            "content": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the residual path to dominate over sub-layer outputs and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as SandwichLN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in wide range of settings."
        },
        {
            "title": "Introduction",
            "content": "The pursuit of more cost-effective and performant architectures is central theme in the development of Large Language Models (LLMs). The introduction of the Transformer architecture [1] laid solid foundation for modern language modeling and has since become the backbone of most state-ofthe-art LLMs. To facilitate stable and scalable training, recent models such as the LLaMA, Qwen and DeepSeek series [2, 3, 4] adopt the Pre-LayerNorm (Pre-LN) Transformer architecture [5, 6, 7], enabling training models with up to hundreds of billions of parameters. Although Pre-LN Transformers offer strong scalability and stability, they remain suboptimal in terms of convergence speed and parameter efficiency. Prior studies [8, 9, 10] have shown that deeper layers of Pre-LN Transformers can often be pruned or removed entirely with minimal impact on performance. While this creates opportunities for reducing inference costs, it also highlights significant inefficiency in the training processwhere most of the computational budget is spent. One of the core reasons for the underutilization of deep layers lies in the accumulation of activation variance in Pre-LN Transformers. While the original Transformer architecture places Layer Normalization [11] after the residual connection (commonly referred to as Post-LN), Pre-LN places it * Equal contribution. Corresponding author. Preprint. Under review. before the attention and FFN modules, leaving the residual branch unnormalized (Figure 1a). As result, activation variance tends to accumulate with layer depthoften at an exponential rate. Recent works [12, 13] have shown that this variance growth causes the signal from attention and FFN modules to be increasingly overpowered by the residual branch, limiting the effectiveness of deeper layers in transforming hidden states. (a) Pre-LN (b) Pre + GPAS (c) GPAS block (d) GPAS backprop Figure 1: Architectures of Pre-LN, Pre + GPAS, and the GPAS block. Red lines indicate gradient flow. Red dotted line means the scale parameter can be either learnable or fixed. In this work, we propose Gradient-Preserving Activation Scaling (GPAS), simple yet effective solution to mitigate variance growth in Pre-LN Transformers. The idea of GPAS is straightforward: reduce variance growth by directly scaling down intermediate activations. However, naively scaling down activations in the forward pass leads to downscaled gradients during backpropagation, which could lead to vanishing gradients and slow down the learning process. As shown in Figure 1, GPAS works around this issue by scaling down forward activations while preserving the magnitude of backward gradients. Experiments on models ranging from 71M to 1B parameters show that Pre-LN with GPAS achieves faster convergence and improved downstream performance. Detailed analysis reveals that GPAS-augmented models exhibit more compact distribution of activation variance across layers and more uniform layerwise importance. Beyond enhancing Pre-LN Transformers, GPAS also serves as general-purpose plugin for other architectural variants. We apply GPAS to architectures including DeepNorm [14], Sandwich-LN [15], Mix-LN [12] and LayerNorm Scaling [13], and observe consistent improvements in convergence speed and performance. Our main contributions are summarized as follows: We propose Gradient-Preserving Activation Scaling to alleviate activation variance growth in Pre-LN Transformers while preserving gradient magnitudes. We validate the effectiveness of GPAS through pretraining experiments across various model sizes, and show that its benefits carry over to downstream supervised finetuning tasks. We further apply GPAS to other architectures such as DeepNorm, Sandwich-LN, Mix-LN and LayerNorm Scaling, and observe consistent performance gains. We conduct thorough analysis of the training dynamics and model properties of models with and without GPAS."
        },
        {
            "title": "2 Related Work",
            "content": "Normalization layers. Normalization is crucial for training deep neural networks, especially for modern LLMs with increasing depth and parameter count. Layer Normalization [11] plays vital role in the success of the original Transformer [1]. Despite its effectiveness, improving normalization remains an active area of research. GroupNorm [16] divides channels into subgroups and normalizes within each group. RMSNorm [17], which omits mean centering and uses root mean square instead of variance, offers simpler and more efficient alternative to LayerNorm, and has gained popularity in recent LLMs [2, 3, 4]. AdaNorm [18] proposed new transformation function to replace the gain and bias in LayerNorm. Dynamic Tanh [19] eliminates normalization altogether by replacing it with non-linear activation. 2 LLM normalization schemes. Modern LLM architectures can be broadly characterized by three key components: normalization schemes, attention mechanisms, and FFN designs. Among these, normalization is most relevant to our work, while attention and FFN variants are largely orthogonal. Transformer normalization has evolved around two main paradigmsPost-LN and Pre-LN, which have inspired numerous refinements to address their respective limitations. Post-LN was proposed in the vanilla Transformer [1], while Pre-LN was proposed in several works [5, 6, 7] to address the optimization issue of Post-LN. theoretical perspective from [20] showed that Post-LN leads to larger gradients near the output layer, necessitating warm-up to avoid divergence. Conversely, Pre-LN yields smaller gradients in deeper layers, mitigating instability at initialization but also potentially curtailing the effectiveness of late-stage parameters. B2T [21] bypasses normalization in early layers to combat gradient decay of Post-LN. DeepNorm [14] modifies Post-LN by scaling up the residual connection before normalization and downscaling layer parameters during initialization. Sandwich-LN [15, 22] applies normalization both before and after the attention and FFN modules to further stabilize Pre-LN. Mix-LN [12] combines Post-LN and Pre-LN layers in the same model to alleviate both the variance growth of Pre-LN and the gradient vanishing issue of Post-LN. LayerNorm Scaling [13] scales output of normalization layers by inverse square root of layer depth, which facilitates linear variance growth in Pre-LN. As for attention and FFN architectures, existing variants mainly focus on expressiveness, scalability and hardware efficiency [23, 24, 25, 26]. While these directions are orthogonal to our work, we briefly mention them here to provide more complete picture."
        },
        {
            "title": "3 Gradient-Preserving Activation Scaling",
            "content": "3.1 Pre-LN with GPAS We first demonstrate how to incorporate GPAS into the prevailing Pre-LN architecture, before extending to other baselines. Equation (1) shows typical forward pass of Pre-LN sub-layer (of layer l). GPAS introduces simple modification after each sub-layer as Equation (2). Each layer now has an additional learnable scalar αl R. SiLU() is the Sigmoid Linear Unit [27]. sg() is the stop gradient operator, which acts as an identity mapping during forward pass, but gives zero gradient during backpropagation. visualization of this modification is shown in Figure 1. The learnable gate αl controls the amount of scaling that is applied to the intermediate activation l+1. Moreover, by scaling l+1, αl controls the mixing ratio between residual xl+1 and the output of the next sub-layer (LN(xl+1)), effectively balancing information from these 2 variables. SiLU activation is used to encourage positive αl values to reduce activation variance, while also allowing the network to learn negative values when necessary. Notice that the input to the next sub-layers attention or FFN is not scaled regardless of the value of αl. This is because Pre-LN architecture applies LayerNorm right before the attention or FFN module, which cancels out any scaling applied to l+1. Overall, Equation (2) scales activation by (1 SiLU(α)), while the Jacobian xl+1/x l+1 = I. l+1 = xl + (LN(xl)), {Attn, FFN}, (1) l+1 SiLU(αl) sg(x l+1). (2) Pre-LN: + GPAS: xl+1 = Preserving gradient with stop gradient. We analyze the gradient preservation of GPAS using the first layers gradient as example. For brevity only one sub-layer per layer is considered. Let βl = 1 SiLU(αl) be the scaling factor for each layer. If we replace sg() with identity, Equation (2) becomes naive scaling: xl+1 = l+1 βl. Suppose we have transformer layers in total, with = L(xL, y) being the loss. Then the gradient L/x1 is: L1 (cid:89) L1 (cid:89) naive scaling: x1 = xL GPAS: x1 = xL βl, l=1 l+1 xl l+1 xl . l=1 L1 (cid:89) l=1 Since βl < 1, the product (cid:81)L1 l=1 βl decays exponentially with depth, causing gradient vanishing problem in early layers. Applying GPAS eliminates the scaling terms βl during backpropagation and preserves gradient magnitude. Detailed derivations are provided in Appendix B. 3.2 Apply GPAS to Other Normalization Schemes LayerNorm Scaling (LNS) [13] scales normalized output by inverse square root of layer depth with Equation (3). Since the scaling factor can be absorbed into the affine transformation inside LayerNorm, LNS is essentially Pre-LN with different initialization of LayerNorm weights. Thus, we apply GPAS to LNS in the same way as Pre + GPAS. LNS: Sandwich-LN: l+1 = xl + (LN(xl)/ l), l+1 = xl + LN(f (LN(xl))), {Attn, FFN}. {Attn, FFN}. (3) (4) For Sandwich-LN [15], GPAS also applies after the residual connection with Equation (2). The only difference from Pre-LN is that the sub-layer forward has an additional LayerNorm as shown in Equation (4). For Post-LN and DeepNorm [1, 14], however, the scenarios are quite different. As shown in Equation (5), Post-LN breaks the residual connection by wrapping xl + (xl) with LayerNorm. DeepNorm inherits this architecture and introduces scaling factor α to the residual xl and another scaling factor β to the sub-layer weights in , where both α and β are predefined and fixed during training. Empirically, we found applying GPAS after LayerNorm does not bring performance gain. Instead, we hypothesize that the scaling factor α is not optimal since it does not account for layerwise variation and training dynamics. Therefore, we apply GPAS to the scaled residual α xl, while keeping the input to attention and FFN unscaled. Ultimately, we found applying GPAS as Equation (7) and (8) boosts pretrain performance. Post-LN: xl+1 = LN(xl + (xl)), DeepNorm: xl+1 = LN(α xl + (xl)), {Attn, FFN}. {Attn, FFN}. DeepNorm + GPAS: = xl SiLU(αl) sg(xl), xl+1 = LN(α + (xl)), {Attn, FFN}. (5) (6) (7) (8) Mix-LN [12] combines Pre-LN and Post-LN layers in the same model, so we apply GPAS differently to its Preand Post-LN layers. For the Post-LN layers, we apply the same strategy as Equation (7). For the Pre-LN layers, we use Equation (2)."
        },
        {
            "title": "4 Experiments and Results",
            "content": "4.1 Experiment Setup Model architectures. Based on our proposed architectural modifications in Section 3, we conduct extensive experiments to verify their effectiveness. Following [28] and [29], we employ LLaMAbased model architectures for implementing Post-LN [1], DeepNorm [14], Pre-LN [30], SandwichLN [31], Mix-LN [12] and LNS [13]. All models share the same attention and FFN architectures as well as normalization layers except for normalization scheme. Specifically, all baseline architectures (from Post-LN to LNS) utilize RMSNorm [17], LLaMA attention and LLaMA MLP [32] with SwiGLU activation [33]. Moreover, they share the same initialization except for DeepNorm and LNS, which add additional scaling to certain parameters. We then apply GPAS to these baseline architectures according to Section 3, and refer to the modified architectures as DeepNorm + GPAS, Pre + GPAS, Sandwich + GPAS, Mix + GPAS, and LNS + GPAS, respectively. Notice that we did not conduct experiment on Post + GPAS, since we found DeepNorm to be better starting point in preliminary study, and utilized our compute budget on DeepNorm + GPAS instead. Pretraining. We perform pretraining experiments across all architectures and at five model scales: 71M, 130M, 250M, 350M, and 1B. For the larger 7B configuration, we only pretrain Pre-LN and Pre + GPAS due to limited computational resources. Following [28], we adopt the Adam [34] optimizer and train on the C4 dataset [35, 36]. We tokenize the pretraining corpus with T5 tokenizer [35] since it was trained on the C4 dataset. For models with GPAS, we initialize all learnable gates αl = 0. When αl = 0, GPAS does not scale the activations or alter the gradients, meaning the very first forward and backward passes during training are identical to those of the baseline model. We did not explore alternative initialization schemes for αl. We also use the same gate value for both attention and FFN sub-layers within the same layer. 4 All experiments are carried out on 4 NVIDIA H800 GPUs. Models below 350M can be trained in under 1 to 8 hours, while each 1B model took 2 to 3 days of training. More detailed configurations are listed in Table 1. Table 1: Pretraining configurations for models of different sizes. Model Size Learning Rate Warmup Steps Training Steps Batch Size Train Tokens Eval Tokens 71M 130M 250M 350M 1B 1 103 1 103 1 103 5 104 5 104 1K 2K 4K 6K 10K 10K 20K 40K 60K 100K 512 512 512 512 512 1B 2B 4B 6B 10B 10M 10M 10M 10M 10M Supervised finetuning. To determine whether the improvements observed during pretraining persist in subsequent training stages, we perform SFT on the pretrained models with 1B parameters from Section 4.2, and then evaluate their performance on common reasoning benchmarks. Following [12], we finetune the models on the Commonsense170K dataset [37] and evaluate the models on seven downstream tasks. The learnable gates αl are frozen during SFT to avoid disturbing pretrained knowledge. We use learning rate of 3 104 and train for 4 epochs using LISA [38]. As for evaluation, we adopt the widely used LM Evaluation Harness [39]. 4.2 Pretrain Results Table 2: Perplexity () of pretrained models on evaluation set. Numbers in parentheses show improvement over the base method. Method Post-LN DeepNorm Pre-LN Sandwich-LN Mix-LN LNS 71M 33.97 35.89 34.23 32.82 34.00 34.70 130M 26.62 26.96 26.75 25.57 26.36 26.03 250M 1412.13 22.47 21.77 20.52 21.90 20. 350M 22.78 22.06 21.35 21.54 21.41 20.43 1B 1404.98 1404.22 16.92 16.87 16.73 15.65 DeepNorm + GPAS Pre + GPAS Sandwich + GPAS Mix + GPAS LNS + GPAS 34.97 (-0.92) 33.49 (-0.74) 32.34 (-0.48) 33.48 (-0.52) 32.93 (-1.77) 26.65 (-0.31) 26.34 (-0.41) 25.05 (-0.52) 26.22 (-0.14) 25.07 (-0.96) 21.81 (-0.66) 21.42 (-0.35) 20.45 (-0.07) 21.78 (-0.12) 19.93 (-0.72) 21.03 (-1.03) 20.35 (-1.00) 19.79 (-1.75) 20.51 (-0.90) 19.44 (-0.99) 18.10 (-1386) 16.25 (-0.67) 15.90 (-0.97) 16.49 (-0.24) 14.88 (-0.77) Table 2 summarizes the pretraining results across all model sizes and architectures. Results for 7Bparameter Pre-LN and Pre + GPAS are provided in Appendix A. We have the following observations: ❶ Consistent Gains from GPAS. For nearly all architectures and model sizes, applying GPAS leads to lower perplexities (numbers in parentheses), confirming that preserving gradient magnitudes while adjusting activation scales is beneficial. ❷ Best Baselines at Different Scales. At smaller scale (71M), Sandwich-LN is the strongest baseline (32.82), and adding GPAS further reduces perplexity to 32.34. At larger scale (1B), LNS already outperforms other baselines (15.65), but GPAS still offers notable improvement (14.88). ❸ Magnitude of Improvement. The improvements (in parentheses) range from moderate ( 0.30.7 drop in perplexity) to quite substantial (e.g., 1.75 for Sandwich-LN at 350M). These consistent gains highlight that GPAS can be seamlessly integrated with various normalization schemes and model sizes, providing an effective and lightweight solution to boost convergence and performance in LLM pertaining. For detailed analysis on how GPAS enhances performance, please refer to Section 5. 4.3 SFT Results We present the finetuning results in Table 3 and notice the following observations: ❶ Gains from GPAS Across Architectures. When GPAS is applied (bottom half), every architecture sees measurable boost in average accuracy. For example, Pre + GPAS achieved notable 2.49% increase in 5 (a) Pretrain loss (b) Evaluation loss Figure 2: Pretrain and evaluation loss of Pre-LN and Pre + GPAS, 1B parameters Table 3: 0-shot performance () on various benchmarks (1B-parameter models). All numbers are accuracy in %. Numbers in parentheses show improvement over the base method. Method Post-LN DeepNorm Pre-LN Sandwich Mix-LN LNS DeepNorm + GPAS Pre + GPAS Sandwich + GPAS Mix + GPAS LNS + GPAS MMLU WinoG PIQA OBQA HellaSwag BoolQ ARC-e Average 22.95 22.95 23.35 23.27 23.22 23.16 23.71 23.46 23.14 23.22 23.34 49.49 49.64 50.99 51.07 52.88 52.09 50.67 52.25 50.36 52.88 52.01 52.88 52.61 68.72 67.63 68.82 69.37 67.95 68.99 69.10 70.18 70. 11.60 11.60 17.80 21.20 20.80 20.00 21.20 22.20 22.20 21.00 24.40 26.24 26.20 32.29 32.70 33.11 34.68 31.43 33.72 34.62 33.52 36.09 37.83 37.83 50.37 61.74 61.90 62.08 53.82 59.76 61.96 61.93 61. 27.44 27.23 49.33 47.43 48.91 51.18 47.22 49.87 50.97 49.79 52.40 32.63 32.58 41.83 43.58 44.23 44.65 42.29 (+9.71) 44.32 (+2.49) 44.62 (+1.04) 44.65 (+0.42) 45.83 (+1.18) average accuracy. DeepNorm + GPAS achieves the most dramatic improvement since vanilla DeepNorm diverged during pretraining, while incorporating GPAS successfully avoided this catastrophic behavior. This suggests that DeepNorms fixed residual scaling may not be optimal yet for network stability, and GPAS automatically learns to increase stability and convergence speed. ❷ Task-Specific Improvements. Breaking down the gains by individual datasets shows that the enhancements are typically consistent across tasks but vary in magnitude. For instance, Pre + GPASs performance on BoolQ reached 9.39% increase in accuracy, OBQA also has notable gain of 4.4%. Meanwhile, LNS + GPAS attains improvements in both OBQA (+4.40) and HellaSwag (+1.41), pushing its overall average to 45.83% (+1.18). Overall, by adaptively controlling the scale of the activations while preserving gradient magnitudes, GPAS consistently unlocks additional gains in downstream tasks."
        },
        {
            "title": "5 Analysis",
            "content": "In this section, we provide thorough analysis of GPAS by examining its training dynamics and model properties. We base our analysis primarily on Pre-LN and its GPAS-augmented version with 1B parameters. 5.1 Learned Layerwise Gate Values GPAS introduces learnable gate αl for each layer. We visualize the learned values of αl in Figure 3. All models are the 1B-parameter models trained in Section 4.2. Generally, Pre-LN and Sandwich-LN layers tend to learn positive gates, while Post-LN layers tend to learn negative gates. Notably, for the Mix + GPAS model, the first 6 Post-LN layers learned negative gates, and the remaining Pre-LN layers learned positive values. Across all model variants, the first layer tend to learn negative gate values. This means the network scales up the activation in the first layer. We hypothesize that this is due to the low variance of the initial word embeddings, and the network automatically learns to scale up that embedding to match the variance of subsequent layers. This hypothesis is also supported by Figure 4b, where the input 6 activation to the first layer (which is the initial word embedding) has much lower variance than the remaining layers. Throughout training, the gates αl exhibit the most variation during the first 10 20% of steps. After that, they tend to remain relatively stable. One likely reason for this behavior is the use of halfprecision training with BFloat16, which may suppress small updates to the gate values due to limited numerical precision. (a) Learned gate values αl for different models (b) Activated gate values SiLU(αl) across training steps of Pre + GPAS Figure 3: Learned layerwise gate values 5.2 Activation Variance Comparison We compare the activation variance of Pre-LN and Pre + GPAS, across all pretraining steps and layers. Experiments are conducted on 1B parameter models. We only plot every 2 layers including the first and last layers for brevity. Figure 4 shows that Pre-LN has an exponential increase in activation variance across layers, while Pre + GPAS offers near 50% decrease in highest activation variance, with more uniform and compact activation variance distribution across layers. (a) Pre-LN (b) Pre + GPAS Figure 4: Layerwise activation variance with and without GPAS 5.3 Gradient Norm Comparison We verify whether GPAS preserves gradient while scaling down activations. Figure 5 shows the layerwise gradient norms across across training steps. Models are the 1B parameter variants we trained in Section 4. Its evident that Pre + GPAS has much larger gradient compared to its baseline method. Most layers in Pre-LN have gradient norm around 0.05 0.1, while Pre + GPASs are scaled to 0.05 0.5. Although the first layers gradient of Pre + GPAS seemed overly aggressive, the model was able to adjust to that gradient scale and achieve faster convergence. However, we did find that the gradient spike around step 10K in Figure 5b disturbed the pretraining process. The gate values at step 10K went through an aggressive fluctuation (see Figure 3b), which caused the model to adjust to that change at steps 10K 30K. This is also observed in the loss curves in Figure 2, where both pretrain and evaluation loss of Pre + GPAS temporarily went above that of Pre-LN around step 10K. One simple way to fix this issue is to use gradient clipping on the gate values αl. However, we did not use gradient clipping in this case as we found the model was eventually able to recover from this aggressive gradient spike and achieve better pretrain and downstream 7 performance. We also do not use gradient clipping on gate values for all other GPAS-augmented baselines at all model scales. The only exception is Sandwich + GPAS with 1B parameters, where we clipped gradient norm of gates to 0.01 to avoid crashing. For other GPAS-augmented baselines, we did not encounter such fluctuation in gate values which disturbs training. (a) Pre-LN (b) Pre + GPAS Figure 5: Layerwise gradient norm with and without GPAS 5.4 Weight Norm Comparison Since Pre + GPAS scales up activations in the early layers according to Figure 4b, we suspect that its early layer weights also have larger norms. Specifically, we compare the norm of weights in the attention and FFN modules. Figure 6 shows that Pre + GPAS has larger weights in almost all layers compared to Pre-LN, especially in the early layers where activation variances are relatively small. This effectively scales up the output variance of attention and FFN modules, such that early layers and deeper layers share similar activation variance. We believe that the combination of larger weights and stable activation variance across layers enables the network to tolerate larger gradients without crashing, which in turn speeds up convergence. (a) Attention weight norm ratio (b) FFN weight norm ratio Figure 6: Attention and FFN weight norm ratios of Pre + GPAS over Pre-LN. 5.5 Layer Importance Comparison We compare the layerwise importance of models with and without GPAS. The importance of given layer is measured as the performance drop after removing that layer. We take the finetuned models in Section 4.3 as baseline, and measure the difference in average score across those benchmarks listed in Table 3 after removing each one of the layers. Figure 7a shows that Pre + GPAS increases layer importance for most and especially the deeper layers, while some layers in vanilla Pre-LN are even harmful to performance. This indicates that GPAS enables more efficient utilization of model parameters, while Pre-LNs exponential variance growth limited the learning capacity of deeper layers. We also show layer importance for LNS + GPAS in Figure 7b. LNS mitigates variance growth in Pre-LN by downscaling LayerNorm output with square root of layer depth. In this case, GPAS still amplifies the importance of each layer by small but noticeable amount, leading to higher overall performance. 8 (a) Pre-LN vs. Pre + GPAS (b) LNS vs. LNS + GPAS Figure 7: Layer importance measured by performance drop after removal. 5.6 Gradient Analysis Based on our experimental findings, we provide brief and preliminary analysis to the activation variance and gradient profiles of Pre-LN and Pre + GPAS. By the result of [13] Theorem 1 and Lemma 1, and the work of [40], we have the following result for Pre-LN transformers: Lemma 1 (Variance and Gradient Growth in Pre-LN). For Pre-LN Transformer with layers, using Equations (1), and let Wℓ be the model parameter matrix at layer ℓ. Assume that for all layers, xℓ, ℓ, and Wℓ are mutually independent and follow Gaussian distributions with mean zero. Let yL . Let the upper bound for this be the output of the L-th layer, and consider the gradient norm (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13)2 norm denoted by UP(). Then it satisfies: UP (cid:19) (cid:18)(cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 O(L), (9) When we add the GPAS to the Pre-LN transformers, we have the following theorem Theorem 1 (Variance and Gradient Growth in Pre-LN with GPAS). Using Equations (1) and (2), and under the same assumptions as in Lemma 1, assume that each αℓ is bounded and varies slowly across layers. Let L(α) and (α) denote the layerwise lower and upper bounds of log(1 SiLU(αℓ)), respectively. Let σ denote the variance of xℓ. Then the upper bound of the gradient norm, denoted as UP , satisfies the following inequality: (cid:17) (cid:32) 1 σ 1 1 e( 1 σ+1 +L(α))/2 (cid:33)(cid:41) +1 UP (cid:19) (cid:18)(cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:40) (cid:32) exp (cid:110) exp 1 2 (M (α)) (cid:111) min{L, σ} σ (cid:33)(cid:41) (10) yL x1 (cid:13) (cid:13) (cid:13)2 (cid:16)(cid:13) (cid:13) (cid:13) (cid:40) exp The detailed description as well as the complete proof, are provided in Appendix C. From Theorem 1, By comparing Lemma 1 (before GPAS) with Theorem 1 (after GPAS), For the lower bound of the gradient norm upper estimate UP( ), the term L(α) effectively acts as compensatory factor that offsets the influence of σ, thereby accelerating the growth of the bound beyond constant rate. This leads to strictly non-constant lower bound, which in turn ensures that the gradient magnitude retains sufficient variability across layers. In particular, this precludes the possibility of vanishing gradients and encourages meaningful gradient flow during backpropagation. (cid:13) (cid:13) (cid:13)2 yL x1 (cid:13) (cid:13) (cid:13) In contrast, for the upper bound, the term (α) serves as multiplicative scaling factor that exponentially suppresses the bound. This results in significantly lower gradient norm ceiling compared to the O(L) upper bound of standard Pre-LN Transformers. Consequently, the proposed modification mitigates gradient explosion and reduces the occurrence of large loss spikes, thereby enhancing training stability. Notably, the improved upper bound grows more slowly than L, further dampening excessive gradient amplification and promoting smoother optimization dynamics. 5.7 Ablation Study We perform series of ablation studies on GPAS to elucidate the rationale behind its architectural design. In particular, we investigate the impact of four key factors: the Activation function, the 9 Application location of GPAS, the Necessity of stop gradient operator, and the choice between Learnable vs. predefined gate value. All ablations are conducted on 350M-parameter models, chosen because this scale typically serves as reliable proxy for larger models while remaining feasible within our computational budget. We also limit the baseline architecture to Pre-LN only, and leave explorations of other architectures to future research. Activation function. GPAS uses the SiLU activation by default. To enable more customizable control over activated gate values, we generalize GPAS to Equation (11), where Act() can be any activation function. We then examine how different activation functions affect performance, using 350M-parameter Pre-LN model with GPAS. Table 4 shows that Identity and Tanh achieve slightly lower perplexities than SiLU in the 350M setting. However, this advantage does not persist at larger scale: in the 1B setting, Identity and SiLU achieve perplexities of 16.49 and 16.25, respectively. We hypothesize that these activations, particularly Identity, permit overly aggressive gate updates during training; by contrast, SiLU imposes smoother gradient profile, constraining updates to more stable range. xl+1 = l+1 Act(αl) sg(x l+1). (11) Table 4: Ablations on activation function (left) and GPAS insertion position (right). Based on 350M Pre-LN + GPAS. SiLU (β = 8) refers to scaled SiLU: sigmoid(βx). Activation Perplexity Identity ReLU LeakyReLU Tanh SiLU (β = 8) SiLU (default) 20.08 (-1.27) 21.17 (-0.18) 21.17 (-0.18) 20.09 (-1.26) 20.26 (-1.09) 20.35 (-1.00) No GPAS 21.35 GPAS Position After sub-layer (default) Before sub-layer After LayerNorm After Attn / FFN No GPAS Perplexity 20.35 (-1.00) 20.73 (-0.62) 21.33 (-0.02) 21.23 (-0.12) 21.35 Where to apply GPAS. Since GPAS can, in principle, be integrated at any point where intermediate activations arise, we investigate the impact of inserting it at different locations within the transformer In Table 4, we compare four variants against 350M-parameter Pre-LN baseline: (1) block. our default setting, which applies GPAS right after the sub-layer (residual + module output); (2) applying GPAS before the sub-layer; (3) placing it immediately after the LayerNorm; and (4) after each Attn/FFN module. While each insertion strategy outperforms the no-GPAS baseline, the default approach (inserting GPAS after the sub-layer) offers the largest perplexity reduction (1.00), indicating that modulating the combined residual and module output is the most effective choice. Necessity of stop gradient operator. We conduct control experiment to show the necessity of the stop gradient operator in GPAS. For the control experiment, we use Equation (12) to replace Equation (2): xl+1 = l+1 SiLU(αl) l+1. (12) We found that the stop gradient operator sg() is crucial for preserving gradients. The gradient norm of GPAS without sg() looks very similar to that of Pre-LN in Figure 5a. Although it still manages to scale down the activation variance by around 50% across all layers, GPAS without sg() does not offer meaningful improvement in perplexity compared to Pre-LN (see Table 5). This highlights the importance of the stop gradient operator to prevent gradient vanishing issue associated with gradient downscaling. Table 5: Ablations on stop-gradient usage (left) and gating strategy (right) in GPAS. Method Perplexity w/ sg() w/o sg() 20.35 (-1.00) 21.34 (-0.01) No GPAS 21.35 Method Perplexity Learnable gate Predefined gate 20.35 (-1.00) 22.46 (+1.11) No GPAS 21.35 10 Learnable vs. predefined gate value. GPAS adopts learnable gates by default. We investigate whether predefined gates can also be used, which could potentially offer more predictability and consistency. To determine reasonable set of predefined gate values, we extract the gates from pretrained Pre + GPAS model, and use those values to initialize the gates for the control experiment, where the gate values will be fixed during training. Results are shown in Table 5. We found that fixing the gates in this way led to substantial drop in performance, especially in early stages when GPAS with learnable gates converges much quicker. This result confirms the importance of learnable gates to account for training dynamics. Another potential way of using predefined gate value is to introduce warmup stage for the gates, which we leave to future work."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed Gradient-Preserving Activation Scaling, simple method that mitigates the exponential growth of activation variance in Pre-LN Transformers, thereby accelerating convergence and enhancing parameter efficiency. Beyond improving Pre-LN Transformers, GPAS also proves to be viable plugin for alternative normalization schemes such as DeepNorm, Sandwich-LN, Mix-LN, and LNS. Limitations. While GPAS showed consistent gains in pretraining performance, our experiments are limited to 1B-parameter models due to computational constraints. Additionally, the current use of the SiLU activation may still permit unstable gate updates that disrupt training dynamics although gradient clipping partially mitigates this issue. Nevertheless, the learnable gates introduce uncertainty during pretraining, and predefined gate schedule with theoretical guarantees might be more preferable, especially at larger scales. deeper understanding of how GPAS affects training stability and convergence remains an open question. Finally, GPAS is intended for training LLMs from scratch, and applying it to models pretrained without GPAS will likely be suboptimal. Broader Impact. GPAS offers lightweight solution to improve the convergence speed and parameter efficiency of LLM pretraining. This may contribute to more accessible and sustainable development of foundation models, with potential downstream benefits across applications such as education and scientific research."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partially supported by grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project Reference Number: AoE/E-601/24-N)."
        },
        {
            "title": "References",
            "content": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. [2] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [3] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [4] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [5] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. ICLR, 2019. [6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [7] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek Wong, and Lidia Chao. Learning deep transformer models for machine translation. ACL, 2019. [8] Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed layerwise sparsity (owl): missing secret sauce for pruning llms to high sparsity. ICML, 2024. [9] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel Roberts. The unreasonable ineffectiveness of the deeper layers. arXiv preprint arXiv:2403.17887, 2024. [10] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853, 2024. [11] Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [12] Pengxiang Li, Lu Yin, and Shiwei Liu. Mix-ln: Unleashing the power of deeper layers by combining pre-ln and post-ln. arXiv preprint arXiv:2412.13795, 2024. [13] Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, and Shiwei Liu. The curse of depth in large language models. arXiv preprint arXiv:2502.05795, 2025. [14] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. TPAMI, 2024. [15] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 34:1982219835, 2021. [16] Yuxin Wu and Kaiming He. Group normalization. In ECCV, pages 319, 2018. [17] Biao Zhang and Rico Sennrich. Root mean square layer normalization. NeurIPS, 32, 2019. [18] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. arXiv preprint arXiv:1911.07013, 2019. [19] Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, and Zhuang Liu. Transformers without normalization, 2025. [20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In ICML, pages 1052410533. PMLR, 2020. [21] Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. B2t connection: Serving stability and performance in deep transformers. ACL, 2023. [22] Jeonghoon Kim, Byeongchan Lee, Cheonbok Park, Yeontaek Oh, Beomjun Kim, Taehwan Yoo, Seongjin Shin, Dongyoon Han, Jinwoo Shin, and Kang Min Yoo. Peri-ln: Revisiting normalization layer in the transformer architecture. arXiv preprint arXiv:2502.02732, 2025. [23] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness. arXiv preprint arXiv:2205.14135, 2022. [24] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. [25] Damai Dai, Chengqi Deng, Chenggang Zhao, R. X. Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y. Wu, Zhenda Xie, Y. K. Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, and Wenfeng Liang. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [26] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [27] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2017. [28] Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky. Relora: Highrank training through low-rank updates. In ICLR, 2023. [29] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. Galore: Memory-efficient llm training by gradient low-rank projection. ICML, 2024. [30] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. ACL, 2019. [31] Xinyu Gong, Wuyang Chen, Tianlong Chen, and Zhangyang Wang. Sandwich batch normalization: drop-in replacement for feature distribution heterogeneity. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 24942504, 2022. [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [33] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [34] Diederik Kingma. Adam: method for stochastic optimization. ICLR, 2015. [35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [36] Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: case study on the colossal clean crawled corpus, 2021. [37] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. EMNLP, 2023. [38] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient large language model fine-tuning. 2024. [39] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. 13 [40] Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models, 2023. [42] E. T. Whittaker and G. N. Watson. Course of Modern Analysis. Cambridge Mathematical Library. Cambridge University Press, Cambridge, 4 edition, 1996. 14 Pretrain Results on 7B-Parameter Models To further verify the effectiveness of GPAS on larger scale models, we perform pretraining experiments on Pre-LN and Pre + GPAS with 7B parameters. We follow [41] and use learning rate of 3 104 with 10K warmup steps and cosine decay. Batch size is set to 2048 and scheduled to train for 150K steps on 60B tokens. We use gradient clipping of 0.01 on gate parameters αl and 1.0 on other parameters to stabilize training. Due to constraints on computational resources, we only train the models up to 40K steps with 16B tokens. After 40K training steps, Pre-LN and Pre + GPAS reached evaluation perplexity of 15.27 and 13.82, respectively. As shown in Figure 8b, Pre + GPAS achieves much faster convergence than vanilla Pre-LN. (a) Pretrain loss (b) Evaluation loss Figure 8: Pretrain and evaluation loss of Pre-LN and Pre + GPAS, 7B parameters"
        },
        {
            "title": "B Gradient Preservation with GPAS",
            "content": "Following Section 3.1, the gradient L/x1 is: naive scaling: x1 GPAS: x1 = = = = = = = xL xL xL xL xL xL xL L1 (cid:89) l=1 L1 (cid:89) l=1 L1 (cid:89) l=1 L1 (cid:89) l=1 L1 (cid:89) l=1 L1 (cid:89) l=1 L1 (cid:89) l=1 xl+1 xl (cid:18) xl+1 l+ l+1 xl (cid:19) l+1 xl l+1 xl l+1 xl l+1 xl xl+1 l+1 βl, xl+1 l+1 L1 (cid:89) l= L1 (cid:89) l=1 L1 (cid:89) l=1 L1 (cid:89) l= l+1 xl . Proof of Theorem 1 of GPAS Proof. By equation (1) and 2, we have the following: = xℓ+1 = ℓ + FFN( 1 ℓ LN(x ℓ)), ℓ = xℓ + Attn( LN(xℓ)). 1 ℓ 15 (13) Pre-LN: + GPAS: xl+1 = l+1 = xl + (LN(xl)), {Attn, FFN} l+1 SiLU(αl) sg(x l+1) (14) (15) Following the variance analysis in [13], both the FFN and Attn modules contribute equally to variance accumulation. For simplicity, we have: σ2 xℓ+1 = σ2 xℓ (1 + 1 σxℓ ) (1 SiLU(αl)). The variance with regard to σx1 : σ2 xℓ = σ2 Θ (cid:16) ℓ1 (cid:89) (cid:16) 1 + k=1 (cid:17)(cid:16) 1 σxk 1 SiLU(αl) (cid:17)(cid:17) , For the parameter αℓ we have 1 SiLU(αℓ) = 1 αℓ 1 + eαℓ = (1 + eαℓ ) αℓ 1 + eαℓ . log σ2 xℓ = log σ2 + ℓ1 (cid:88) k=1 (cid:16) log 1 + (cid:16) + log (cid:17) 1 σxk 1 SiLU(αℓ) (cid:17) + C, where is an (unspecified) constant. Using the original definition for SiLU we have (16) (17) (18) (19) (cid:16) log 1 SiLU(αℓ) (cid:17) = log (cid:16) 1 + eαℓ αℓ 1 + eαℓ (cid:17) = log(cid:0)1 + eαℓ αℓ (cid:1) log(cid:0)1 + eαℓ(cid:1). (20) Next, denote L(αℓ) = log (cid:16) 1+eαℓ αℓ 1+eαℓ (cid:17) , which is function of αℓ. Thus the inequality becomes log σ2 xℓ log σ2 x1 + ℓ1 (cid:88) (cid:18) k=1 1 σxk + 1 (cid:19) + L(αℓ) + C. (21) For fixed σ2 can reach 0. To establish upper bound for σ2 xℓ xℓ, log σ2 xℓ (C + L(αℓ)), if αl < 0, there is no lower bound for the variance. So it (cid:16) log 1 + ℓ1 (cid:88) k=1 (cid:17) 1 σxk ℓ1 (cid:88) k=1 1 σxk . (22) The SiLU value of this term is less than or equal to zero (since typically 1 SiLU(αℓ) 1) or it can be bounded by an appropriate constant (αℓ). For our derivation, we denote Putting these bounds together we have (cid:16) log 1 SiLU(αℓ) (cid:17) (αℓ). log σ2 xℓ log σ2 x1 + ℓ1 (cid:88) k=1 1 σxk + (αℓ) + C. (23) (24) Next, we analyze the gradient stability of GPAS. Following Equation (38) in [13], and applying the formalization techniques from [42], we derive the result under the consideration that stopgrad is used, thereby eliminating any gradient contributions at that point. Consequently, we obtain:"
        },
        {
            "title": "U P",
            "content": "(cid:19) (cid:18)(cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 = L1 (cid:89) (cid:18) 1 + l=1 1 σxℓ + (cid:19) , 1 σ2 xℓ (25) Substitute our lower bound in Equation 21 on σ2 xl into the above product. Notice that if (cid:40) (cid:41) σ2 xl σ2 x1 exp Sl with Sl = l1 (cid:88) (cid:18) k=1 1 σxk + 1 (cid:19) + L(αℓ) + C, (26) Thus, we have the upper bounds on the inverses: 1 σxl 1 σx1 (cid:110) exp (cid:111) Sl and 1 σ2 xl 1 σ2 x1 (cid:110) Sl (cid:111) . exp"
        },
        {
            "title": "Thus a valid lower bound for the UP norm is",
            "content": "U (cid:19) (cid:18)(cid:13) (cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13) (cid:13)2 L1 (cid:89) (cid:32) 1 + σx1 (cid:110) exp 1 2 (cid:104) l1 (cid:88) (cid:18) k=1 1 σxk + (cid:19) + L(αℓ) + (cid:105)(cid:111) l=1 σ2 + (cid:110) exp (cid:104) l1 (cid:88) (cid:18) k=1 1 σxk + 1 (cid:19) + L(αℓ) + (cid:105)(cid:111) (cid:33) . Similar to above, assuming that for each layer and based on Equation (24), we have: So we have: σ2 xℓ σ2 x1 (cid:40)ℓ1 (cid:88) exp k=1 1 σxk (cid:41) + (αℓ) + . 1 σxℓ 1 σx1 (cid:40) exp 1 2 (cid:104)ℓ1 (cid:88) k=1 1 σxk + (αℓ) + (cid:41) (cid:105) , Hence the UP product is upper bounded by (cid:17) (cid:16)(cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13)2 + L1 (cid:89) ℓ=1 σ2 C.1 Upper bound of UP() (cid:32) 1 + (cid:40) exp (cid:40) exp σx1 1 2 (cid:16)ℓ1 (cid:88) k=1 1 σxk (cid:41) (cid:17) + (αℓ) + (cid:16)ℓ1 (cid:88) k=1 1 σxk + (αℓ) + (cid:17) (cid:41)(cid:35) ). (27) (28) (29) (30) (31) Assume that the (αl) is bounded and will not change so much. Inserting this into (31), thus, in the case of constant σ and layer-independent (α), we obtain the explicit bound (cid:17) (cid:16)(cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13)2 (cid:40) exp σ (cid:110) exp 1 (M (α) + C) (cid:111) 1 exp 1 exp (cid:110) (cid:17)(cid:111) 1 exp (cid:111) (32) (cid:41) . (cid:110) (cid:111) L1 2σ (cid:111) (cid:110) 1 2σ L1 σ (cid:111) (cid:110) 1 σ + σ2 exp (cid:110) (cid:16) (α) + 1 exp 17 It is easy to verify: 1 exp (cid:110) and similarly 1 exp (cid:110) (cid:111) (cid:111) L1 2σ (cid:110) 1 2σ L1 σ (cid:110) 1 σ 1 exp 1 exp (cid:111) = (cid:16) min{L, σ} (cid:17) (cid:111) = (cid:16) min{L, σ} (cid:17) . (33) (34) Plug Equations (33) and (34) into the bound. We deduce"
        },
        {
            "title": "U P",
            "content": "(cid:16)(cid:13) (cid:13) (cid:13) exp yL x1 (cid:40) σ (cid:17) (cid:13) (cid:13) (cid:13)2 (cid:110) exp 1 2 (M (α) + C) (cid:111) O(cid:0)min{L, σ}(cid:1) + (cid:40) (cid:32) = exp σ (cid:110) exp 1 2 (M (α) + C) (cid:111) min{L, σ} + (M (α) + C) (cid:110) σ2 exp (cid:110) (M (α) + C) (cid:111) O(cid:0)min{L, σ}(cid:1) (cid:41) σ2 exp (cid:32) (cid:33) (cid:40) (cid:32) = exp exp (cid:110) 1 2 (M (α)) (cid:111) min{L, σ} σ (cid:33)(cid:41) C.2 Lower bound for UP() (cid:111) min{L, σ} (cid:33)(cid:41) . (35) Assume that the L(αl) is bounded and will not change so much. Inserting this into (28), thus, in the case of constant σ and layer-independent (α), we can obtain the explicit bound. Assume with = 1 σ+1 + L(α), For each term in the product (with index l), define (l) = 1 + σ (cid:110) exp (cid:104) 1 2 (l 1)D + (cid:105)(cid:111) + σ2 exp (cid:110) (cid:104) (l 1)D + (cid:105)(cid:111) . (36) Taking the logarithm of the whole product, we get (cid:40) L2 (cid:88) s=0 σ (cid:110) exp 1 2 (cid:111) (sD + C) + (cid:110) (cid:0)sD + C(cid:1)(cid:111) σ2 exp (cid:16) + exp{sD C} (cid:17) (cid:41) . (37) Each of the sums over is geometric series. For instance, L2 (cid:88) s=0 (cid:110) exp 1 2 (cid:111) (sD + C) = eC/2 L2 (cid:88) s=0 (cid:16) eD/2(cid:17)s = eC/2 1 e(L1)D/2 1 eD/2 , (38) and similarly for the second term. Notice that when eD/2 < 1 the whole sum (even as ) is bounded by constant. Exponentiating, we can write: (cid:17) (cid:16)(cid:13) (cid:13) (cid:13) yL (cid:13) (cid:13) (cid:13)2 exp (cid:40) σ eC/2 1 1 eD/ + σ2 eC 1 (cid:41) . 1 eD + O(1) similar to the above process, we can get: (cid:17) (cid:16)(cid:13) (cid:13) (cid:13) yL x1 (cid:13) (cid:13) (cid:13)2 (cid:40) (cid:32) exp 1 σ 1 1 e( 1 σ+1 +L(α))/2 (cid:33)(cid:41) . + 1 (39) (40)"
        },
        {
            "title": "D License",
            "content": "We provide licenses of assets used in our paper, including model, dataset and code. C4 dataset: This is the dataset we used in our pretraining experiments. Its released under the Open Data Commons License Attribution family License. Commonsense 170K dataset: This is the dataset we used for supervised fine-tuning, released under MIT License."
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "Emory University",
        "International Digital Economy Academy",
        "NVIDIA",
        "The Hong Kong University of Science and Technology",
        "University of Oxford",
        "University of Surrey",
        "University of Texas at Austin"
    ]
}