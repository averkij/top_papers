{
    "paper_title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers",
    "authors": [
        "Yusuf Dalva",
        "Hidir Yesiltepe",
        "Pinar Yanardag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce LoRAShop, the first framework for multi-concept image editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration."
        },
        {
            "title": "Start",
            "content": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers Yusuf Dalva Hidir Yesiltepe Virginia Tech https://lorashop.github.io/"
        },
        {
            "title": "Pinar Yanardag",
            "content": "5 2 0 2 9 2 ] . [ 1 8 5 7 3 2 . 5 0 5 2 : r Figure 1. LoRAShop. We present LoRAShop, training-free framework enabling the simultaneous use of multiple LoRA adapters for generation and editing. By identifying the coarse boundaries of personalized concepts as subject priors, we allow the use of multiple LoRA adapters by eliminating the cross-talk between different adapters."
        },
        {
            "title": "Abstract",
            "content": "We introduce LoRAShop, the first framework for multiconcept image editing with LoRA models. LoRAShop builds on key observation about the feature interaction patterns inside Flux-style diffusion transformers: conceptspecific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive disentangled latent mask for each concept in prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into practical photoshop-with-LoRAs tool and opens new avenues for compositional visual storytelling and rapid creative iteration. 1. Introduction The rapid progress in Text-to-image (T2I) generative models [27, 28, 32] has opened new creative avenues such as content generation [4, 21, 37] and editing [3, 5, 6, 22, 38, 48, 49], but users often desire customized outputs with specific topics or styles not present in the original training data [51]. Personalization techniques that fine-tune pretrained generative model on small set of user-provided images have emerged to meet this need. Notably, methods like DreamBooth [29] and Low-Rank Adaptation (LoRA) [12] allow T2I models to be customized, capturing userspecific concepts (e.g. particular pet, unique face, or distinct art style) and regenerating them in new contexts with high fidelity. While single-concept personalization is relatively simple task, multi-concept generation is challenging problem: Given multiple fine-tuned concept models (e.g. several LoRAs trained on different subjects), how can we compose them to synthesize coherent image containing all the custom concepts? Achieving such compositions is challenging because independently trained LoRAs can interfere with each other when combined, leading to identity distortions or one concept dominating the other phenomenon sometimes called LoRA crosstalk [10, 20, 24, 35]. Simply merging or applying multiple LoRAs naively often causes one concept to vanish or entangle attributes with the other [10]. Recent research indeed highlights that multi-concept generation remains nontrivial: combining personalized models typically degrades individual concept quality unless special measures are taken [10, 24, 35]. However, these methods still require training new combined model or fine-tuning process (e.g., imposing constraints during each LoRAs training or running post hoc alignment optimization). While existing techniques can achieve multi-concept generation i.e. producing new image containing several personalized concepts none of these methods addresses the task of multi-concept editing: modifying given image to insert multiple new concepts. Multi-concept image editing presents different set of challenges. Here, the goal is not to generate scene from scratch, but to start from an input image and seamlessly blend in additional personalized elements (each defined by LoRA model) into that image. naive approach to this problem might be to apply iterative inpainting: for example, masking region in the image and prompting the diffusion model (with the LoRA loaded) to generate the new concept in that area. Unfortunately, off-the-shelf inpainting with personalized diffusion models often yields artifacts and inconsistencies. The injected object or character may not blend naturally with the lighting and context of the original image, or the model may unintentionally alter the surrounding content. Another approach could be face-swapping or identity transfer, where persons face in the image is replaced with personalized face (using LoRA of that person). Although this can handle single face, it often does not preserve the full appearance of the person, such as body features, and can produce unrealistic results. In this paper, we propose LoRAShop, novel framework that enables multi-concept image editing with LoRA models, without requiring any additional training, special auxiliary inputs, or external segmentation. Given an input image and set of LoRA modules (each encoding different concept), LoRAShop allows the user to insert each concept into the image at desired location in disentangled way. One of our key observations is disentangled mask extraction technique that leverages the internal representations of the rectified-flow model to localize the influence of each subject to be personalized. In essence, as each LoRA is applied during the denoising process, our method extracts coarse mask that delineates the regions where that concept significantly contributes to the image. By combining these masks with the users concept specifications, LoRAShop is able to blend multiple concepts directly into the diffusion latent in controlled manner (see Fig. 1). Our experiments show that LoRA subjects blend naturally into the original scene, and their identities/styles match the LoRA concepts with high fidelity. Our approach does not require training of any new model or ensemble; it directly utilizes existing LoRAs and the base rectified-flow model at inference time, making it efficient and user-friendly. We believe that LoRAShop fills an important gap between personalized generation and image editing, opening the door to new creative workflows (such as LoRAshopping with generative models) that were previously impractical. 2 Figure 2. LoRAShop Framework. LoRAShop enables multi-subject generation and editing over two-stage training-free pipeline. First, we extract the subject prior Ë†Mc , which gives coarse-level prior on where the concept of interest, c, is located. Following, we introduce blending mechanism over the transformer block residuals, which both enables seamless blending of customized features and bounds the region-of-interest for the LoRA adapter utilized. 2. Related Work Personalized Image Generation. Personalized image generation aims to inject user-defined concept, typically face, style, or object, into text-to-image model so it can be used in future generations. Early work relied on Textual Inversion (TI) [9], which learns single embedding that reproduces users concept. TI is lightweight, but struggles to learn concepts involve high level of detail, where it learns to reconstruct the target concept with diffusion loss. DreamBooth (DB) [29] improves fidelity by fine-tuning selected model weights and reserving rare token for the new concept, though at higher compute cost. Later methods seek better qualityefficiency trade-offs: P+ [40] extends TI with richer token representation; Custom Diffusion [19] trains only cross-attention layers; and DB-LoRA [31] applies low-rank adaptation [12] to store each concept in small rank-limited update. Recent encoder-based systems such as StyleDrop [36], HyperDreamBooth [30], Taming Encoder [15], IP-Adapter [46], MS-Diffusion [42], MIPAdapter[13], InfiniteYou [16], OmniGen [45] and UNO [43] predict adapter features directly from reference images, enabling near-instant personalization but often with some loss of identity fidelity compared with full DreamBooth tuning. Merging Multiple Concepts. Combining LoRAs for style and subject control remains as challenging tasks, as combined adapters usually optimize overlapping representaIn achieving such combination of personalized tions. concepts, current work still faces certain challenges. Simple weight averaging [31] is fast but quickly causes interference. Mix-of-Show [10] trains special embeddingdecomposed LoRAs that avoid this clash, yet it needs the original data and cannot use community models, such as those available on platforms like civit.ai [2]. ZipLoRA [33] merges one style and one content adapter but breaks down with more than one content LoRA. On the other hand, OMG [18] is based on an external segmenter to apply separate concepts, whose errors propagate to the result. Orthogonal Adaptation [24] keeps LoRAs in separate subspaces with additional constraints introduced, reducing cross-talk, but adds training overhead and likewise assumes data access. Our proposed approach differs from existing multi-concept generation methods since our main goal is editing as opposed to generation. Moreover, our method does not require any input conditions such as keypoints or segmentation masks. 3. Method We propose LoRAShop, new training-free pipeline that enables the use of multiple LoRA adapters through targeted feature blending scheme for multi-subject generation and editing. Our method, Multi-Subject Residual Blending (MSRB), consists of two fundamental stages: 1) the extraction of subject prior that effectively highlights the spatial regions where each subject is intended to appear, and 2) the application of residual feature blending scheme within the diffusion transformer that selectively merges the outputs of different LoRA adapters. This allows us to spatially combine features corresponding to distinct concepts, enabling coherent and disentangled multi-subject genera3 Figure 3. Editing Generated & Real Images with LoRAShop. We provide qualitative editing results with different human concepts. LoRAShop can achieve both edits on real and generated images. Due to non-intersecting subject prior extraction scheme of our framework, LoRAShop can perform edits with multiple concepts in one denoising pass. tion and editing without any additional training. 3.1. Preliminaries Multi-Modal Diffusion Transformers. Multi-modal diffusion transformers (MM-DiT) [8] extend the DiT architecture by processing text and image tokens in two tightly coupled streams, enabling end-to-end text-to-image generation. Rectified-flow models such as FLUX adopt this design and alternate between two transformer block types. We denote blocks that keep separate parameter sets for the text and image streams as double-stream blocks, and those that apply shared transformation to both streams as single-stream blocks. During the denoising trajectory, the network first aligns textual and visual features within the double-stream blocks and subsequently refines the fused representation in the single-stream blocks. All feature updates propagate through residual connections, an architectural property that our generation and editing protocol leverages directly. Personalization via Low-Rank Adaptation. Low-Rank Adaptation (LoRA) [12] was originally introduced as lightweight fine-tuning method for large language models. Instead of updating the full weight matrix W0 Rdk, LoRA learns low-rank increment, formulating the finetuned weights as = W0 + = W0 + BA with Rdr, Rrk, and an intrinsic rank min(d, k). Because only and are trained, the additional parameter count and memory footprint scale linearly with r, making LoRA especially attractive for large backbones. Following its success in NLP, LoRA has been 4 adopted for text-to-image diffusion models and, more recently, for rectified-flow transformers such as FLUX. We leverage single-subject LoRA adapters trained for rectifiedflow transformers and introduce training-free mechanism that allows multiple adapters, each corresponding to different subject, to be used simultaneously without any additional optimization. 3.2. Self-Supervised Subject Prior Extraction Training several LoRA adapters so they can be applied simultaneously is costly and often infeasible for large-scale denoisers: Every additional adapter consumes optimization memory, and jointly fine-tuning many of them tends to introduce interference and distribution drift. To bypass this bottleneck, we first predict, in inference time, where each personalized subject will emerge in latent space and then confine every adapters effect to the pixels assigned to that subject. The binary masks that delimit these regions are our subject priors. We extract each prior once in short pseudo-denoising run that proceeds only until timestep Î³, when latents are still close to noise, yet crossattention already carries strong spatial cues [5, 11]. Rectified flow transformers such as FLUX provide well-localized cross-attention maps. In particular, the map from the last block that still keeps the text and image streams separate (the double-stream block) gives the sharpest separation. For prompt and the token subset naming one subject we compute Mc = softmax(cid:0)QiK c/ d(cid:1), (1) Figure 4. Ablation Study. Ablation on transformer blocks, where Block 19 shows superior ability for separation between subjects. where Qi are the image queries, Kc the keys of c, and the key dimension. Because raw attention may fragment, we iteratively blur Mc with 3 3 Gaussian kernel and renormalize until the super-threshold area forms single connected component. Thresholding at the Ï„ posterior quantile then produces the final binary mask, which we denote by Ë†Mc, for the subject c. When multiple subjects are present, these masks can intersect, leading to undesirable LoRA cross-talk. To obtain non-overlapping maps, we stack the smoothed attention maps { (cid:102)Mu}N u=1, determine for every spatial position (i, j) the subject with the strongest response, k(i, j) = arg max , (cid:102)Mu(i, j) (cid:102)Mmax(i, j) = (cid:102)Mk(i,j)(i, j) (2) and finally define one-hot priors Ë†Mu(i, j) = 1!(cid:2)u = k(i, j)(cid:3). (3) The set Ë†Mu partitions the latent canvas without overlap and serves as the spatial guide for adapter mixing during generation and editing. 3.3. Prior-Guided Blending of Residual Features The diffusion transformer proceeds as usual, but at every block we overwrite the residual feature tensors wherever subject prior is active. At block â„“ the frozen backbone proâ„“,r RSC with duces collection of residual tensors, Fbase = 1, . . . , R, corresponding to the outputs of multi-modal attention, MLP, and any other sublayer that feeds skip connection. In parallel, the k-th LoRA adapter contributes its counterparts F(k) â„“,r . The binary priors Ë†Mc {0, 1}S indicate which latent tokens belong to subject c. For each token position we turn the priors into weights, so the weights sum to one on the subject tokens and to zero on background tokens. , u=1 (cid:80)N Îµ 1, Î±c(p) = Ë†Mk(p) Ë†Mu(p) + Îµ Whether the block is double-stream (text and image kept separate) or single-stream, we treat it the same way: only image tokens are blended; prompt tokens keep their backbone residuals. For every image token and every residual index we substitute (4) Fâ„“,r(p) = (cid:88) k=1 Î±c(p) F(k) â„“,r (p), (5) and feed Fâ„“,r back through the blocks skip connection. If no subject claims token ((cid:80) Ë†Mu(p) = 0), we leave Fbase â„“,r (p) unchanged. Blending is disabled during the first until timestep t, letting the backbone establish the overall layout of the scene before subject-specific features are inserted. Because we mix the residual outputs of every sublayer rather than changing any weights, all adapters remain independent, and each subject influences exactly the tokens selected by its prior across the entire depth of the transformer. 3.4. Editing with LoRAShop LoRAShop intervenes only in the feature space of rectified flow transformer: it neither modifies the noise schedule nor alters any model weights. During the reverse diffusion process, we overwrite residual features solely at token positions indicated by the subject priors, leaving all other tokens unchanged. Because this operation is local and linear, the global denoising trajectory, and thus the overall scene layout, remains intact. The same mechanism integrates seamlessly with inversion. We adopt the RF-Solver pipeline of [41], which uses second-order solver to recover the latent noise corresponding to target image. After reconstructing the latent, we utilize LoRAShop to edit the inverted latent. As illustrated in Fig. 1 and Fig. 3, this enables regioncontrolled insertion of multiple personalized concepts into 5 Figure 5. Qualitative Comparisons. We provide qualitative comparisons on three mainstream tasks: single-subject generation, multisubject generation and face swapping. Over all of the benchmarked tasks, LoRAShop provides superior performance against competing approaches. real images while faithfully preserving the properties of the input. 4. Experiments We evaluate LoRAShop on both image generation and image editing tasks. For generation, we measure how well the method renders single personalized subject and how reliably it composes multiple personalized subjects in one scene. For editing, we evaluate identity transfer on real images, replacing persons appearance with that encoded by LoRA adapter. We provide the details of our experimental protocol along with the results in this section. Experimental Setup We use FLUX.1-dev, as the rectified-flow transformer on which we build our approach. Our approach is based on utilizing pre-trained LoRA adapters for tasks such as single/multi-concept generation and editing. In all of our experiments, we use the LoRAs available at diffusers [39] library. We provide complete list of LoRAs used in our experiments in the supplementary material, along with visual representations of these concepts for ease of understanding. Unless otherwise mentioned, we set the editing timestep = 0.90, Î³ = 0.94 and Ï„ = 0.7, where we apply the proposed blending scheme (Sec. 3.3) onward timestep during the reverse process. Our approach requires no training over the pre-trained adapters and can perform the aforementioned personalization task in inference time. We conduct our experiments using one NVIDIA L40S GPU. LoRAShop can generate images using two concepts approximately in 50 seconds, as opposed to the manual inference time of FLUX.1-dev which requires 30 seconds per image. Furthermore, since LoRAShop can apply each concept sequentially, we introduce no memory constraints on how many concepts that can be applied to given image. See Fig. 2 for 4 subject generation results. 4.1. Qualitative Results We qualitatively assess the effectiveness of our method in single & multiple subjects and in generated & real images. To assess the visual performance of our framework, we demonstrate its capabilities in experiments on human subjects. Although LoRAShop can also perform edits on variety of types of subject, we perform our experiments on human subjects due to the high level of details such concepts involve, and their wide-usage in customization tasks. Since 6 Table 1. Quantitative Comparisons on Single-Subject Generation. We provide quantitative comparisons on single-subject generation. Our method outperforms the competing FLUX-based approaches in the overall performance, measured over identity similarity, prompt alignment and visual quality. Method ID CLIP-T HPS Aesthetics 0.755 0.089 DreamBooth IP-Adapter-FLUX 0.309 0.077 0.683 0.068 InfiniteYou 0.657 0.066 Omni-Gen 0.486 0.137 UNO 0.740 0.066 Ours 0.429 0.055 0.330 0.053 0.439 0.039 0.434 0.043 0.415 0.051 0.439 0.047 0.305 0.030 0.272 0.026 0.307 0.026 0.311 0.030 0.289 0.030 0.321 0. 6.311 0.505 6.340 0.408 6.490 0.459 6.514 0.448 6.303 0.527 6.499 0.529 our method requires no fine-tuning for LoRA-adapters, we can use any adapter trained for our base model. Furthermore, since our approach does not focus on specific type of residuals (e.g. attention layer outputs), but operates on the overall representation space, we can also use LoRAs with different ranks and different sets of fine-tuned parameters together. Editing on Generated and Real Images. We provide editing results with LoRAShop on both male and female subjects, where these LoRAs are trained with different sets of combinations, which involve different sets of weights, ranks, and presence of trigger word. Presented in Fig. 3, LoRAShop can both perform edits on real & generated images, without altering any subject-independent details. Note that, since LoRA adapters offer us way to utilize the rich semantics in the weight space of the denoiser, our approach can also perform changes to the body of the edited subject (Fig. 3, row 1), which exceeds the limits of the face swapping task and provide us an advanced way of editing images with customized concepts. Furthermore, as our subject prior extraction algorithm provides non-intersecting masks, our approach facilitates performing multiple edits with distinct LoRA adapters in single denoising pass. Qualitative Comparisons. We provide qualitative comparisons of our approach with competing methods on singlesubject, multi-subject and face swapping tasks. Since our proposed approach performs generation by editing, we enable blending of the residual features. While vanilla approaches such as DreamBooth [29] achieve subject-based generation results, since they fine-tune the weights of the original denoiser, they result in reduced prompt alignment and visual coherence. On the other end, encoder-based approaches such as IP-Adapter [46], InfiniteYou [16], UNO [43] and OmniGen [45] struggle to encode the identity features that are effectively captured by DreamBooth. In this regard, our approach offers the best of both worlds (Fig. 5 (a)), where we personalize only the regions related to the identity, which both achieves superior prompt alignment and personalization performance. For multi-subject generation, we provide comparisons with FLUX-based approaches such as UNO [43], OmniGen [45], DreamBooth [29] and SDXL[25]-based approaches MS-Diffusion [42] and MIP-Adapter [13]. We use federated averaging for DreamBooth, as baseline towards multi-subject personalization. As we demonstrate Fig. 5 (b), our subject priors mitigate the confusion between similar concepts effectively, where the remaining approaches either attempt to merge the two identities into one, or fail to capture the identity accurately. In this regard, our approach outperforms the competing methods for multi-subject generation as training-free solution, which can effectively reflect multiple concepts effectively, mitigating the crosstalk effect between the concepts. Additionally, we provide comparisons with methods combining multiple LoRA adapters in Fig. 7, where our method offers compositions with high quality, without any pose input. To benchmark our method in terms of editing, we select the face swapping task, where we use identity LoRAs to represent the identity to be inserted into the original image. As we qualitatively benchmark in Fig. 5 (c), our approach extends the limitations of identity swapping, which was task that is limited with swapping the faces until today. As LoRA adapters are capable of capturing physical features in addition to facial features, LoRAShop enables the transfer of physical features in addition to the face of the source identity, in addition to superior fidelity against methods based on inpainting such as ReFace [1]. 4.2. Quantitative Results Extending our benchmark in qualitative experiments, we benchmark the editing and generation performance of LoRAShop on three mainstream tasks. Specifically, we benchmark the performance of LoRAShop for single & multi concept generation along with face swapping task. We provide the details of each constructed benchmark below. Single-Subject Generation. Following previous work, we populate set of varying identities and generation prompts to benchmark our generation results. Among publicly available LoRA adapters, we select 15 identity LoRAs and generate total of 520 images where 15 generation prompts 7 Table 2. Quantitative Comparisons on Multi-Subject Generation. We benchmark our approach against FLUX and SDXL based methods. LoRAShop achieves superior identity preservation over multiple subjects, while maintaining the prompt alignment and visual quality of the base model. Method ID CLIP-T HPS Aesthetics OMG S b F a MS-Diffusion MIP-Adapter"
        },
        {
            "title": "DreamBooth\nOmniGen\nUNO\nOurs",
            "content": "0.305 0.14 0.206 0.05 0.209 0.06 0.444 0.08 0.453 0.09 0.270 0.07 0.532 0.12 0.217 0.09 0.251 0.08 0.243 0.07 0.248 0.08 0.256 0.08 0.252 0.08 0.252 0.08 0.212 0.05 0.253 0.03 0.236 0.03 0.259 0.04 0.258 0.04 0.255 0.04 0.260 0. 6.017 0.35 6.119 0.24 6.111 0.30 6.113 0.30 6.264 0.26 6.113 0.36 6.124 0.29 Table 3. User Study. We present user study results on identity preservation (Q1), and prompt alignment (Q2) for multi-subject generation task."
        },
        {
            "title": "Method",
            "content": "User Study - Q1 User Study - Q2 OMG S b F a MS-Diffusion MIP-Adapter DreamBooth OmniGen UNO Ours 2.591 0.25 2.596 0.27 2.889 0.26 3.196 0.10 3.340 0.32 2.711 0.23 3.762 0.25 3.332 0.55 2.753 0.19 3.123 0.50 4.060 0.14 4.012 0.26 3.587 0.44 4.230 0. were applied to each identity separately. To adequately assess both the personalization, prompt alignment and visual coherence of the generated outputs, we construct our benchmark prompts with themes such as artistic creations, contexts defined by activities and superficial concepts (see Fig. 5 (a)). We provide the complete list of prompts we use for our benchmark in the supplementary material. To assess both identity preservation, text alignment and visual coherence of the generated images, we utilize ArcFace embeddings [7], CLIP-T similarity [26], HPS score [44] and Aesthetics score1. We present the quantitative results in Table 1. As quantitative metrics also show, our approach leads to sweet spot between identity preservation, prompt alignment, and visual coherence, as we utilize the generative priors in our residual blending scheme. Multi-Subject Generation. In addition to our benchmark for single-subject generation, we also benchmark our approach against multi-subject generation methods. Using the 15 subjects that we used in our benchmark for single-subject generation, we initially generate random pairs of identities with corresponding prompts to create benchmark for the two-subject generation task. In our evaluations, we compare our method with both FLUX-based methods UNO 1https : / / github . com / christophschuhmann / improved-aesthetic-predictor [43], OmniGen [45] and DreamBooth (FedAvg) [29] and SDXL-based methods OMG [18], MS-Diffusion [42] and MIP-Adapter [13]. As we present in the results in Table 2, our approach achieves both superior prompt alignment, visual coherence, and identity preservation. User Study. Supplementary to our benchmark on multisubject generation, we also conducted user study to perceptually evaluate the generation quality of our approach. We conducted our study on 50 participants over Prolific.com crowdsourcing platform, where each participant is asked to assess 70 images involving multiple subjects. In our study, we evaluated the generation performance in which users are asked to rate the images in two aspects on Likert scale (1: poor, 5: excellent): (Q1) alignment with the target identities and (Q2) alignment with the generation prompt. We provide the result of our study in Tab. 3. As our results also demonstrate, LoRAShop outperforms the competing approaches in both prompt alignment and identity preservation. Please see Appendix for additional details about the user study. Face Swapping. We also benchmark our approach in the face swapping task. We compare our method with an inpainting-based swapping approach ReFace [1]. Although our approach does not involve any hard constraints for content preservation such as inpainting masks that restrict the regions to be edited, our method still achieves competitive 8 Table 4. Quantitative Comparisons on Face Swapping. We benchmark LoRAShop against REFace [1]. While performing on-par in input preservation, LoRAShop introduces significant improvements in identity preservation. Method"
        },
        {
            "title": "ReFace\nOurs",
            "content": "ID DINO CLIP-I LPIPS 0.330 0.091 0.709 0.101 0.982 0.012 0.970 0. 0.940 0.038 0.926 0.037 0.031 0.033 0.050 0.019 Figure 6. Ablation Study. (a) Ablations on hyperparameters time step t, subjects prior extraction step Î³, and the posterior threshold for binarization of the subjects prior masks Ï„ . (b) Ablation on transformer blocks, where Block 19 shows superior ability for separation between subjects. performance in terms of input preservation, which we measure using DINO [23], CLIP-I [26], and LPIPS [50] metrics. Furthermore, LoRAShop leads to significant improvements in identity preservation properties. Note that our approach extends the bounds of the face swapping task and can perform full identity transfer by editing the physical appearance, in comparison to inpainting-based swapping approaches. 4.3. Ablation Studies Ablations on Transformer Blocks. To further justify the use of the last double-stream block for subject prior extraction, and to provide an investigation over the roles of different transformer blocks, we provide ablations over the masks extracted from different transformer blocks in Fig. 4. As shown by the attention masks extracted for the subject (e.g. woman), we observe that through the double-stream blocks (blocks 0-19), FLUX constructs the semantic context and is able to perform the separation between different In the single-stream concepts at the end of these blocks. blocks, we observe that the model attempts to focus more on the visual details, which results in maps spread out over different entities. Building up on this observation, we build our subject prior extraction scheme on the attention maps produced by the last double-stream block (e.g. Block 19). Ablations on Editing Parameters. Complementary to the block selection, LoRAShop includes three additional hyperparameters for editing, which are the editing time step t, the subjects prior extraction step Î³, and the posterior threshold for binarization of the subjects prior masks Ï„ . We provide ablations on these hyperparameters in Fig. 6. Similarly to the trend observed in diffusion-based editing methods, LoRAShop is able to preserve the adapter-irrelevant features of the input image better when the edit is performed in later timesteps. Considering that the effect should be effective enough and preserve certain features of the input image, we achieve good balance for the timestep t. Regarding the subject priors extracted prior to the denoising 9 Figure 7. Qualitative Comparisons with Multi Composition Methods. We compare our method with multi-composition methods operating on multiple LoRA adapters, LoRAShop outperforms the competing approaches while not relying on pose input, and thus generate compositions with diverse settings. steps, we recognize that the introduced parameters have significant impact on the quality of the mask. In general, we find Î³ = 0.94 and Ï„ = 0.7 as suitable hyperparameters, which we utilize in all of our experiments for complete and accurate enough masks. 5. Discussion Limitations and Broader Impact. Because the extracted masks inherit the latent biases of the underlying diffusion model (e.g., greater attention to faces, stereotypical gender features, or saturated colors) [17, 47], they can sometimes mislocate or underrepresent certain regions, leading to less coherent or unbalanced edits, particularly for concepts underrepresented in the models pretraining data. Our mask extraction leverages attention patterns unique to the Flux architecture; other diffusion backbones (e.g., SDXL-Turbo) may require re-tuning of threshold parameters or yield less coherent masks. This limits immediate portability across all T2I models. Like other powerful editing tools, LoRAShop can be used to create non-consensual content. We encourage deployment within responsible-AI guardrails, but broader ethical safeguards remain necessary. Nevertheless, LoRAShop demonstratesfor the first timetraining-free, region-controlled multi-concept editing with LoRAs, unlocking new creative workflows and research directions in compositional image manipulation. Conclusion. We presented LoRAShop, the first trainingregion-controlled multifree framework that enables concept image editing with off-the-shelf LoRA modules. By uncovering, and exploiting, spatially coherent activation patterns inside Flux diffusion transformers, we devised disentangled latent-mask extraction procedure that lets each LoRA act only where it is intended, eliminating crossconcept interference. Without any extra optimization, segmentation, or auxiliary guidance, LoRAShop seamlessly blends multiple personalized subjects or styles into an input image, preserving both global context and fine local detail. Beyond advancing the state of the art in personalized image editing, LoRAShop turns diffusion models into an intuitive photoshop-with-LoRAs, opening new possibilities for collaborative storytelling, product visualization, and rapid creative iteration."
        },
        {
            "title": "References",
            "content": "[1] Sanoojan Baliah, Qinliang Lin, Shengcai Liao, Xiaodan Liang, and Muhammad Haris Khan. Realistic and efficient face swapping: unified approach with diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 10621071. IEEE, 2025. 7, 8, 9 [2] Civitai. https://civitai.com, 2020. 3 [3] Yusuf Dalva and Pinar Yanardag. Noiseclr: contrastive learning approach for unsupervised discovery of interpretable directions in diffusion models. arXiv preprint arXiv:2312.05390, 2023. 2 [4] Yusuf Dalva, Yijun Li, Qing Liu, Nanxuan Zhao, Jianming Zhang, Zhe Lin, and Pinar Yanardag. Layerfusion: Harmonized multi-layer text-to-image generation with generative priors. arXiv preprint arXiv:2412.04460, 2024. 2 [5] Yusuf Dalva, Kavana Venkatesh, and Pinar Yanardag. Fluxspace: Disentangled semantic editing in rectified flow transformers, 2024. 2, 4 [6] Yusuf Dalva, Hidir Yesiltepe, and Pinar Yanardag. Gantastic: Gan-based transfer of interpretable directions for disentangled image editing in text-to-image diffusion models. arXiv preprint arXiv:2403.19645, 2024. 2 [7] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, pages 46904699, 2019. 8, 15 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 4 [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 3 [10] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023. 2, 3, 13 [11] Alec Helbling, Tuna Han Salih Meral, Ben Hoover, Pinar Yanardag, and Duen Horng Chau. Conceptattention: Diffusion transformers learn highly interpretable features. arXiv preprint arXiv:2502.04320, 2025. 4 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 3, 4 [13] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion for finetuning-free personalized image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 37073714, 2025. 3, 7, [14] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 15 [15] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023. 3 [16] Liming Jiang, Qing Yan, Yumin Jia, Zichuan Liu, Hao Kang, InfiniteYou: Flexible photo recrafting while and Xin Lu. preserving your identity. arXiv preprint, arXiv:2503.16418, 2025. 3, 7 [17] Tahira Kazimi, Ritika Allada, and Pinar Yanardag. Explaining in diffusion: Explaining classifier through hierarchical semantics with text-to-image diffusion models. arXiv preprint arXiv:2412.18604, 2024. 10 [18] Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, and Wenhan Luo. Omg: Occlusion-friendly personalized multiarXiv preprint concept generation in diffusion models. arXiv:2403.10983, 2024. 3, 8 [19] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. [20] Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. Clora: contrastive approach to compose multiple lora models. arXiv preprint arXiv:2403.19776, 2024. 2 [21] Tuna Han Salih Meral, Enis Simsar, Federico Tombari, and Pinar Yanardag. Conform: Contrast is all you need for highIn Proceedings of fidelity text-to-image diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90059014, 2024. 2 [22] Tuna Han Salih Meral, Hidir Yesiltepe, Connor Dunlop, and Pinar Yanardag. Motionflow: Attention-driven moarXiv preprint tion transfer in video diffusion models. arXiv:2412.05275, 2024. 2 [23] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 9 [24] Ryan Po, Guandao Yang, Kfir Aberman, and Gordon Wetzstein. Orthogonal adaptation for modular customization of In Proceedings of the IEEE/CVF Condiffusion models. ference on Computer Vision and Pattern Recognition, pages 79647973, 2024. 2, 3, 13 [25] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. [26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 8, 9, 15 [27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 22500 22510, 2023. 2, 3, 7, 8, 16 [30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for [44] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 8 [45] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 3, 7, 8 [46] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 7 [47] Hidir Yesiltepe, Kiymet Akdemir, and Pinar Yanardag. Mist: Mitigating intersectional bias with disentangled crossattention editing in text-to-image diffusion models. arXiv preprint arXiv:2403.19738, 2024. 10 [48] Hidir Yesiltepe, Yusuf Dalva, and Pinar Yanardag. The curious case of end token: zero-shot disentangled image editing using clip. arXiv preprint arXiv:2406.00457, 2024. 2 [49] Hidir Yesiltepe, Tuna Han Salih Meral, Connor Dunlop, and Pinar Yanardag. Motionshop: Zero-shot motion transfer in video diffusion models with mixture of score guidance. arXiv preprint arXiv:2412.05355, 2024. [50] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 9, 15 [51] Matthew Zheng, Enis Simsar, Hidir Yesiltepe, Federico Tombari, Joel Simon, and Pinar Yanardag Delul. Stylebreeder: Exploring and democratizing artistic styles through text-to-image models. Advances in Neural Information Processing Systems, 37:3409834122, 2024. 2 fast personalization of text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65276536, 2024. 3 [31] Simo Ryu. Low-rank adaptation for fast text-to-image diffusion fine-tuning, 2023. 3 [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 2 [33] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023. [34] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 15 [35] Enis Simsar, Thomas Hofmann, Federico Tombari, and Pinar Yanardag. Loraclr: Contrastive adaptation for customization of diffusion models, 2024. 2, 13 [36] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 3 [37] Kavana Venkatesh, Yusuf Dalva, Ismini Lourentzou, and Pinar Yanardag. Context canvas: Enhancing text-to-image diffusion models with knowledge graph-based rag. arXiv preprint arXiv:2412.09614, 2024. 2 [38] Kavana Venkatesh, Connor Dunlop, and Pinar Yanardag. Crea: collaborative multi-agent framework for creative content generation with diffusion models. arXiv preprint arXiv:2504.05306, 2025. 2 [39] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. [40] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual conditioning in text-toimage generation. arXiv preprint arXiv:2303.09522, 2023. 3, 13 [41] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 5 [42] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. MS-diffusion: Multi-subject zero-shot image personalization with layout guidance. In The Thirteenth International Conference on Learning Representations, 2025. 3, 7, 8 [43] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. 3, 7, 8 12 LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers"
        },
        {
            "title": "Table of Contents",
            "content": "B. Supplementary Generation and Editing ExA. Details of User Study 13 B. Supplementary Generation and Editing Examples 13 C. Additional Comparisons D. Detailed Masking and Blending Algorithm E. Experiment Details F. List of LoRA Adapters A. Details of User Study 13 13 15 We provide sample question for the user study conducted in Fig. 8. To assess both the identity preservation and prompt alignment capabilities of our approach, we direct two questions to the participants of our study. The users are also provided representative examples of the personalized subjects, where these images are outsourced from assets available for public use. Then, the users are asked to rate the provided image on Likert scale, where 1 corresponds to an unsuccessful generation and 5 corresponds to successful generation. Figure 8. User Interface of our User Study. 13 amples Supplementary to the editing and generation examples provided in the main paper, we provide supplementary results from LoRAShop in this section. Specifically, we provide examples of four subject generation in Fig. 9, three subject generation in Fig. 10, two subject generation in 11, and combination of human and non-human adapters in Fig. 12. As we demonstrate qualitatively, our approach can both handle multiple instances of the same type of entities (e.g. woman) and different type of entities (e.g. man, sunglasses, clothing). C. Additional Comparisons We compare our method against multi-concept LoRA composition approaches, including Mix-of-Show [10], LoRACLR [35], Orthogonal Adaptation [24], and Prompt+ [40] in Fig. 15. Notably, the first three methods require pose condition for generating compositions, and the first two depend on specialized LoRA models such as EdLoRA, which limits their applicability when using community LoRAs from platforms like Civit.ai. In contrast, our method operates without pose conditions, retraining, or model merging, enabling successful composition using arbitrary LoRA models out of the box. Additionally, our method can compose LoRAs with different characteristics (e.g. different ranks and different sets of parameters), by operating on output space only. We also highlight that other methods do not support Flux, thus we visually compare with their Stable Diffusion-based generations. We also note that LoRACLR [35], Orthogonal Adaptation [24], and Prompt+ [40] do not have publicly available implementations, which prevents us from conducting quantitative comparison. D. Detailed Masking and Blending Algorithm To further clarify the details of our method, we provide detailed descriptions of subject prior extraction and residual blending scheme introduced in the main paper. For the subject prior extraction, we provide the details of blob construction algorithm in 1. In addition, to further clarify the blending process, we describe the blending process for given residual (from transformer block) in Alg. 2. Note that this blending operation is applicable for all residual features outputted inside the transformer blocks. Figure 9. Multi-subject composition results on four human subjects. As our approach does not rely on any other external conditioning like pose conditioning, LoRAShop can utilize the generative capabilities of FLUX, and thus generate outputs with high fidelity and superior prompt alignment. In the provided examples, we utilize the concepts <Margot>, <Gal>, <Kiernan> and <Beer>. Algorithm 1 HOMOGENEOUSBLOB Require: Soft mask [0, 1]BHW 1, image size (H, ), Gaussian size k, variance Ïƒ, threshold t, maximum passes , mode flatten, distance parameter Î» 01 scaling if every batch sample has 1 connected component above then break 1: reshape(M, B, 1, H, ) 2: GaussianKernel(k, Ïƒ) 3: renorm(M) 4: for = 1 to do 5: renorm(cid:0)conv2d(M, G)(cid:1) 6: 7: 8: 9: end for 10: 11: 1{M=max(M)} 12: morph reconstruct(cid:0)P, M(cid:1) 13: renorm(M) 14: Ë†M reshape(M, B, W, 1) 15: return Ë†M end if Homogenise the blob Use the global peak as single-pixel marker Flood-fill outward until original mask intensity is reached Rescale result to [0, 1]; yields flat, uniform blob E. Experiment Details In this section, we provide supplementary details on our quantitative evaluations and provide the specifics of the 14 Figure 10. Multi-subject composition results for three subjects. We provide generation results for the subjects <Pitt>, <Elon> and <DiCaprio>. We provide generation results on three different generation prompts, with different compositions of the subjects. metrics utilized and the prompt sets used. We provide the prompts that we use for the evaluation of the single-subject and multi-subject generation tasks in Table 8 and Table 9, where we generate the prompt set with GPT-4o[14]. In the following, we provide the details for each of the metrics that we use in our evaluations. ID: We use the InsightFace2 codebase for the ID similarity metric. Specifically, we use ArcFace [7] embeddings provided in their implementation, using the buffalo variant. CLIP: To assess text-to-image similarity for single/multi subject generation tasks, and image-to-image similarity for face swapping benchmark, we utilize CLIP [26] as our feature extractor. In all of our experiments, we use the big-G variant of the model3. HPS: As secondary metric to quantify text-to-image alignment, we utilize the Human Preference Score (HPS), which is fine-tuned with user preferences. In our experiments, we use the HPSv24 variant. Aesthetics: To assess the quality of the generated images, 2https://github.com/deepinsight/insightface 3https : / / huggingface . co / laion / CLIP - ViT - bigG - we utilize the aesthetics score for single and multi subject generation tasks. We use the second version of the predictor in all of our experiments5. DINO: As secondary metric to assess the input preservation for the face swapping task, we use DINO for our benchmark. We use the checkpoints from https: //huggingface.co/facebook/dinov2-base. LPIPS: Following the common practice from image editing tasks, we utilize LPIPS [50] score with VGG [34] backbone. For all of the competing approaches, we use the default hyperparameter setups and their corresponding official implementations. F. List of LoRA Adapters We provide complete list of LoRA adapters used in this section. Specifically, we provide the list of the LoRA adapters for woman subjects in Tab. 5, man subjects in Tab. 6 and non-human subjects in Tab. 7. For each of the adapters, we provide representative images for each, to help readers identify the subjects. Note that, we provide this 14-laion2B-39B-b160k 4https://github.com/tgxs002/HPSv2 5https : / / huggingface . co / shunk031 / aesthetics - predictor-v2-sac-logos-ava1-l14-linearMSE 15 Figure 11. Multi-subject composition results for two subjects. We provide generation results for the concepts <Armas>, <Sabrina>, <Pitt>, <DiCaprio>. As we demonstrate in the examples, LoRAShop can perform compositions between the same type (e.g. womanwoman) and different type (e.g. man-woman) of identity concepts. list as legend, where the adpater icons are in match with the ones used in the main paper. As exceptions, we train the LoRA adapters for <Gosling> and <Lebron> using Dreambooth [29]. 16 Figure 12. Multi-subject composition results generated by our method on different types of objects. As can be seen in the examples LoRAShop can perform combinations between different types of concepts. 17 Figure 13. Face Swapping results with LoRAShop. As we demonstrate in the provided examples, our editing approach offers seamless blending between the input subject and the target identity, while preserving the input characteristics. 18 Figure 14. Face Swapping results with LoRAShop. 19 Figure 15. Comparison with state-of-the-art multi composition methods, on two subject generation task. 20 Algorithm 2 RESIDUALBLENDING Require: Rbase RSC R(k) RSC for = 1, . . . , Ë†Mk {0, 1}S for = 1, . . . , {1, . . . , S} Îµ small constant Ensure: (cid:101)R RSC 1: for each token index = 1, . . . , do 2: 3: 4: (cid:101)R(p) Rbase(p) continue if / then 5: 6: 7: 8: 9: 10: end if sumMask (cid:80)N if sumMask = 0 then (cid:101)R(p) Rbase(p) u=1 Ë†Mu(p) else for = 1 to do Î±k Ë†Mk(p)/(sumMask + Îµ) end for (cid:101)R(p) (cid:80)N k=1 Î±k R(k)(p) 11: 12: 13: 14: 15: end for 16: return (cid:101)R end if residual from frozen backbone residuals from LoRA adapters token-wise subject priors indices of image tokens avoids divide-by-zero blended residual tensor prompt token: no blending background token token claimed by subject normalise prior to weight blend adapter residuals according to weights ready for the blocks skip connection 21 Adapter Icon Adapter Tag URL of the Adapter <Armas> https://huggingface.co/Trenddwdw/Ana_de_Armas <Billie> https://huggingface.co/punzel/flux_billie_eilish <Watson> https://huggingface.co/punzel/flux_emma_watson <Gal> https://huggingface.co/punzel/flux_gal_gadot <Kiernan> https://huggingface.co/punzel/flux_kiernan_shipka <Margot> https://huggingface.co/punzel/flux_margot_robbie <Margot> https://huggingface.co/punzel/flux_emma_stone <Beer> https://huggingface.co/punzel/flux_madison_beer <Sabrina> https : / / huggingface . co / mmaluchnick / sabrina - carpenter-flux-model <Taylor> https : / / huggingface . co / DeZoomer / TaylorSwift - FluxLora Table 5. Image-and-text comparison table. 22 Adapter Icon Adapter Tag URL of the Adapter <DiCaprio> https://huggingface.co/openfree/leonardo-dicaprio <Pitt> https://huggingface.co/Trenddwdw/Brad_Pitt <Lee> https://huggingface.co/openfree/bruce-lee <Elon> https : / / huggingface . co / roelfrenkema / flux1 . lora . elonmusk <Messi> https://huggingface.co/namita2991/messi Table 6. Image-and-text comparison table. 23 Adapter Icon Adapter Tag URL of the Adapter <Lumiva> https://huggingface.co/Litqecko/lumiva-glasses <Jacket> https : / / huggingface . co / Oscar2384 / Loewe _ Hybrid _ bomber_jacket_in_nappa <Dress> https://huggingface.co/martintomov/moncler-dress1000-v1 <Tower> https://huggingface.co/seawolf2357/ntower <Cat> https://huggingface.co/ginipick/fluxloraericcat <Royce> https://huggingface.co/seawolf2357/flux-lora-carrolls-royce Table 7. Image-and-text comparison table. 24 ID 2 3 4 5 6 8 9 10 11 12 14 15 Prompt woman/man rendered in stylized manner is centered in the image, standing in front of backdrop of expressive brushstrokes and vibrant color blocks. woman/man illustrated in pencil is centered in the frame, with fine shading and linework defining her/his face, placed against softly sketched background. woman/man illustrated with smooth digital brushwork is centered in the image, with soft ambient lighting and clean gradient background behind her/him. woman/man rendered in art deco style is centered in the scene, framed by angular gold patterns and symmetrical borders in an ornate composition. woman/man is centered in the image, rendered in cyberpunk painting style with neon reflections casting pink and blue highlights across her/his face, glowing circuitry traced along her/his cheekbones, and blurred futuristic cityscape of holograms and rain-soaked signs behind her/him. woman/man with happy expression, sitting near tall window with natural light falling across her/his face, while shadows from nearby plants frame the soft background. beautiful woman/man is centered in cozy room filled with bookshelves and warm lighting, her/his face lit by glowing screen as she/he laughs during video call. woman/man with nervous expression on misty morning trail, the background gently blurs into distant trees and dew-covered grass. woman/man with happy expression in warmly lit kitchen, preparing meal with relaxed expression, surrounded by ingredients and subtle reflections from the counter. woman/man is centered at cafe table, sketching in notebook with soft light falling on her/his face, as the background softly fades into rustic textures and furniture. woman/man knight with fierce expression, wearing intricately detailed medieval armor, standing on battlefield at sunset as orange light reflects off her/his head and the silhouettes of fallen weapons surround her/him. woman/man sorcerer is centered in the image, casting glowing spell with both hands, her/his face illuminated by swirling magical energy, while runes float in the air and faint aura pulses around her/him in the twilight mist. futuristic cyborg woman/man is centered in the image, with metallic faceplate, cybernetic implants across her/his jaw and temple, and glowing blue circuitry along her/his neck, standing in front of neon-lit skyline under starless night sky. woman/man dragon rider is centered in the image, her/his face framed by windswept hair and dark leather hood, with the neck of black-scaled dragon behind her/him and storm clouds swirling in the sky, her/his expression fierce and focused as wind lifts her/his cloak around her/his shoulders. happy woman/man elf in portrait photo setting, with long silver hair and pointed ears, cloaked in forestgreen robes, standing beneath ancient glowing trees in an enchanted forest where magical particles float in the air and moonlight streams through twisted branches. Table 8. Prompt list for single-subject generation. ID 1 2 3 4 6 7 8 9 10 12 13 14 15 Prompt close-up profile photo of woman in red suit with slicked-back hair and defined brows, next to woman in green suit with soft curls and warm smile, both standing side by side under office hallway lighting, posing to the camera. headshot-style image of woman in white lab coat with glasses and sharp jawline, beside woman in navy scrubs with tied-back hair and round face, both facing forward in hospital corridor. portrait-style image of woman in floral dress with curly blonde hair and bright eyes, and woman in denim jacket with straight black hair and neutral expression, both seated on park bench, looking at the camera. profile photo of woman in black business suit with confident expression, next to woman in beige blazer with composed look, both looking directly at the camera in modern office setting. woman in crisp chefs uniform with her hair neatly tied back and confident expression, and woman in barista apron with short bangs and friendly smile, both posed for professional headshots in warmly lit cafe interior. head-and-shoulders photo of woman in athletic wear with her hair tied up and serious look, next to woman in hoodie with loose strands and light smile, both standing on track field at sunrise. portrait-style image of woman in yellow raincoat with damp bangs and composed face, and woman under black umbrella coat with cheerful smile, both captured walking side by side on rainy city street. softly lit bridal portrait of bride in white wedding dress with glowing makeup, alongside bridesmaid in navy gown with calm expression, both facing forward in bridal room setting. posed gala portrait of woman in red evening gown with defined features and dramatic makeup, beside woman in silver sequin dress with soft curls and neutral expression, both under spotlight lighting. construction site ID photo of woman in safety vest and hard hat with firm gaze, next to woman holding blueprints with glasses and composed face, both framed in the foreground. portrait of woman holding camera in casual wear with focused look, and woman with soft gaze in long white dress, both photographed at golden hour in field. greenhouse portrait of woman in green apron with tied-back hair and relaxed expression, next to woman in plaid shirt with gentle smile, both facing the camera with greenery in the background. coastal roadside profile photo of woman in black motorcycle jacket with bold lipstick, and woman in sundress holding an ice cream cone with cheerful expression, both posing beside scooter. woman with calm expression sits at cafe table, her face softly lit and clearly framed in the image; and woman beside her with gentle smile turns slightly toward her, both captured from the shoulders up in warm, relaxed atmosphere with the background softly out of focus. college campus profile photo of graduate woman in black gown and cap with proud smile, and woman in floral dress holding bouquet with joyful expression, posing for graduation photo. Table 9. Multi-subject prompts used in our evaluation."
        }
    ],
    "affiliations": [
        "Virginia Tech"
    ]
}