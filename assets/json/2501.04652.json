{
    "paper_title": "Multi-task retriever fine-tuning for domain-specific and efficient RAG",
    "authors": [
        "Patrice Béchard",
        "Orlando Marquez Ayala"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction fine-tune a small retriever encoder on a variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-of-domain settings as well as to an unseen retrieval task on real-world enterprise use cases."
        },
        {
            "title": "Start",
            "content": "Multi-task retriever fine-tuning for domain-specific and efficient RAG Patrice Béchard ServiceNow patrice.bechard@servicenow.com Orlando Marquez Ayala ServiceNow orlando.marquez@servicenow.com 5 2 0 2 ] . [ 1 2 5 6 4 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has become ubiquitous when deploying Large Language Models (LLMs), as it can address typical limitations such as generating hallucinated or outdated information. However, when building real-world RAG applications, practical issues arise. First, the retrieved information is generally domain-specific. Since it is computationally expensive to fine-tune LLMs, it is more feasible to fine-tune the retriever to improve the quality of the data included in the LLM input. Second, as more applications are deployed in the same real-world system, one cannot afford to deploy separate retrievers. Moreover, these RAG applications normally retrieve different kinds of data. Our solution is to instruction finetune small retriever encoder on variety of domain-specific tasks to allow us to deploy one encoder that can serve many use cases, thereby achieving low-cost, scalability, and speed. We show how this encoder generalizes to out-ofdomain settings as well as to an unseen retrieval task on real-world enterprise use cases."
        },
        {
            "title": "Introduction",
            "content": "As more and more Generative AI (GenAI) applications are integrated into real-world production systems, Retrieval-Augmented Generation (RAG) has been adopted in industry as common technique to improve the output of Large Language Models (LLMs). RAG alleviates inherent LLM pitfalls such as propensity to hallucinate, generating outdated knowledge, and lack of traceability to data sources (Fan et al., 2024; Gao et al., 2024). Introducing retrieval step into the generation process introduces, however, several practical challenges. While an LLM with large number of parameters, such as GPT-4 (OpenAI et al., 2024), can be prompted to work with any kind of input and generate any kind of textual output, the retriever needs to be small, fast, and perform well with data sources that tend to be domain-specific. 1 Figure 1: Given an ecosystem of RAG applications, how do we build retriever that can adapt to specific domain and to variety of retrieval tasks? Off-the-shelf retrievers of different sizes are available to AI practitioners. Embedding services such as Voyage1 perform well on open-source benchmarks but they do not necessarily generalize to the kind of data seen in real-world settings, especially when this data is structured and comes from existing databases. Another practical challenge is achieving scalability and generalization across different GenAI use cases that depend on retrieval. crucial advantage of LLMs compared to traditional machine learning models is that they can generalize to myriad of tasks due to vast amounts of pretraining data and instruction fine-tuning (Wei et al., 2022; Zhang et al., 2024a; Ouyang et al., 2022). But if the retriever does not perform well and fast across many 1https://docs.voyageai.com/docs/embeddings retrieval tasks, the downstream generation will be negatively affected. The problem we are trying to solve is then: how to adapt the retrieval step to specific domain and to variety of retrieval tasks? In this work, we are not interested in the choice of LLM, assuming that improvements in the retrieved results translate into improvements in the downstream generation task. The context is an enterprise company that deploys several GenAI applications that currently rely or will rely on RAG: Flow Generation (Bechard and Ayala, 2024), Playbook Generation2, and Code Generation. Workflows are step-by-step processes that automate one or more goals while playbooks contain workflows and other UI components such as forms. The objective of these applications is to generate domain-specific workflows, playbooks, and code from textual input. Our solution is to instruction fine-tune small retriever on variety of tasks. This retriever is deployed in the ecosystem of GenAI applications, which prompt it to retrieve desired structured data Information such as workflow from databases. step names, table names, and field names are then passed to LLMs to generate workflows or playbooks, leading to higher output quality. To build multi-task retriever dataset, we extracted data from internal databases and reused the Flow Generation training set. For our solution, we fine-tune mGTE (Zhang et al., 2024c) because of its large context length (8,192 tokens), allowing it to receive long instructions, and because of its multilingual capabilities. We compare it with BM25, simple yet powerful term-frequency method (Robertson and Walker, 1994), and with recent open-source multilingual embedding models: mE5 (Wang et al., 2024) and mGTE. We perform several evaluations. First, we evaluate on the tasks that the retriever was trained on, but on out-of-domain (OOD) settings. The internal training datasets come from the IT domain, but the OOD splits come from diverse domains such as HR and finance. We then evaluate on related but different retrieval task to test the generalization ability of the model. For example, while the model was trained to retrieve step names based on text, we also want to retrieve relevant workflow structures based on text. This is related task because workflows are made up of steps. Lastly, to see whether the 2www.servicenow.com/docs/bundle/xanaduintelligent-experiences/page/administer/now-assistplatform/concept/now-assist-playbook-generation-skill.html multilingual abilities of mGTE are preserved after our multi-task fine-tuning, we evaluate on input from different languages even though our retrieval training dataset is only in English. Our contributions are the following: We provide case study for how to build domain-specific and efficient retriever for realworld RAG. We demonstrate that multi-task retriever finetuning can lead to generalization to out-ofdomain datasets and to related but different retrieval tasks."
        },
        {
            "title": "2 Related Work",
            "content": "There have been several efforts to achieve domainspecific RAG. While more complex, jointly training the retriever and the generator has been shown to work well for question-answering (Siriwardhana et al., 2023). Others encode the domain-specific information in knowledge graphs in addition to vector databases (Barron et al., 2024). Retrieval Augmented Fine Tuning (Zhang et al., 2024b) helps the LLM adapt to the domain by modifying how the retrieved information is present in the LLM training dataset. In contrast, our approach relies solely on training better retriever, which can be used with any LLM, thereby reducing coupling. Multi-task retriever models (Maillard et al., 2021; Wang et al., 2022) enhance embedding versatility by simultaneously training on multiple tasks or by utilizing large dataset spanning diverse topics. Building upon this concept, instructionfollowing embedding models (Asai et al., 2023; Su et al., 2022; BehnamGhader et al., 2024) integrate explicit instructions into the embedding process, thereby enabling the generation of more nuanced and task-specific embeddings. We leverage this kind of instruction fine-tuning to train retriever on many datasets containing structured data extracted from databases. Lastly, generating workflows is structured output task, similar to code generation, that requires specialized embeddings. Code embedding models (Feng et al., 2020; Li et al., 2022; Guo et al., 2020; Neelakantan et al., 2022; Zhang* et al., 2024) adapt common training techniques to the domain of code representation learning and retrieval. parallel research trajectory focuses on structured data embedding models, including Synchromesh (Poesia et al., 2022), which fine-tunes model to retrieve relevant samples for few-shot prompting 2 using similarity metric derived from tree edit distance, and SANTA (Li et al., 2023), which implements modified pretraining objective to generate structure-aware representations. Prior work already uses RAG for similar but more limited structured output task (Bechard and Ayala, 2024). This paper is natural extension to increase reusability of the retriever in an expanding ecosystem of GenAI applications."
        },
        {
            "title": "3 Methodology",
            "content": "To build mult-task retriever, we first had to define the tasks and then build their datasets, keeping in mind that we could not do any labeling for these tasks. The starting point was to define the data that the existing RAG applications would need. The GenAI applications currently deployed in our system require diverse types of retrieved data to generate output acceptable to users. As these applications become more sophisticated, the extent and type of data retrieved can only increase. But currently we focus mostly on steps, which are used by workflows and playbooks, and tables, which are used by workflows and playbooks, and can be used in code generation. Steps are building blocks of processes; they can be actions, subflows, or activities. When it comes to tables, the crucial information are table names and table field names. For instance, user may want to generate code that makes database call, but without grounding the LLM on actual table information, the LLM may refer to non-existent table name. We extracted data from two sources to create the multi-task retrieval dataset. Complex and semantically diverse examples were created from the Flow Generation training set. Figure 2 shows an example from this set in YAML format, which includes step names (definition), table names, and table field names (part of conditions and values). The other source are database tables that include other elements we are interested in, besides steps and tables, that contain text field such as description."
        },
        {
            "title": "3.1 Tasks",
            "content": "From the Flow generation training set, we constructed three categories of tasks, constituting around half of all retrieval examples: 1. Retrieve steps that can be used in workflows or playbooks. 2. Retrieve tables that can be used in step inputs or in code. Figure 2: Dataset example of Flow Generation. We constructed retrieval multi-task examples from them. 3. Retrieve table fields that can be used in step inputs or in code, given table name. To add diversity to each of the tasks, we included several permutations of the input, such as: Retrieve all the steps used in the workIn the exflow given the requirement. this means to reample in Figure 2, trieve look_up_records, FOREACH, and update_record given the requirement Every day, look up incident tasks that do not have assignees and close them. Retrieve specific step from an annotation. Given the example in Figure 2, we can ask retriever to get the step look_up_records given the text Look up incident tasks that do not have assignees. Retrieve table name given workflow context. We would want the table name incident_task given all the previous steps (all YAML lines up to definition: update_record in Figure 2). Retrieve table field name from text. Given the annotation Look up incident tasks that do not have assignees, retrieve assigned_to. The remaining half of retrieval examples come from database tables, such as catalog items. This 3 table represents items such as laptops that can be requested in the system. There is field description along the catalog item name, yielding task where the item name is retrieved given description."
        },
        {
            "title": "3.2 Dataset Generation",
            "content": "Once we defined the tasks to train the retriever, we proceeded to create set of pairs consisting of text and objects, which can be steps, tables, table field names, catalog items, etc. The text portion of the pair is an instruction describing what has to be retrieved. Below we instruct the retriever to find steps given only requirement."
        },
        {
            "title": "Represent this requirement for searching",
            "content": "relevant steps: requirement: Every day, look up incident tasks that do not have assignees and close them. Given that there may be more metadata available to find steps, we can add extra information to the instruction. Below we tell the retriever that this flow is part of the teams scope. Represent this flow for searching relevant steps: Following established practices in sentence embedding model training (Reimers and Gurevych, 2019), we create both positive and negative pairs, enabling the model to effectively discriminate and retrieve relevant elements. Positive samples are extracted from the labeled workflows as described above. To mine negative samples, we use two sampling strategies: Random negative sampling simply takes an unrelated step, table, or field and matches it to piece of text used in another positive sample. Hard negative sampling uses the structure of the data to find harder negative examFor example, as steps tend to be ples. grouped in scopes, we sample another step coming from the same scope as the positive step. Both post_response_to_slack and post_a_message are in the slack scope, but they are used in different scenarios."
        },
        {
            "title": "4 Experiments",
            "content": "type: flow scope: sn_ms_teams_ah requirement: Every day at 9am, notify me of pending tasks in teams msg. We describe the datasets used in training and evaluation, the baselines we compare against, as well as the retrieval metrics we used. Because we instruction fine-tune the retriever, we can even add more details. For instance, the instruction can contain the workflow context. This is useful when we want the retriever to be influenced by what is already included in the workflow in addition to the last annotation."
        },
        {
            "title": "Represent this flow for searching relevant steps",
            "content": "for the last step: type: flow rcope: sn_ms_teams_ah requirement: Every day, look up incident tasks that do not have assignees and close them. trigger: annotation: every day type: daily inputs: - name: time - value: '1970-01-02 00:00:00' steps: - annotation: look up incident tasks that do not have assignees We can use similar instructions to retrieve any element. Given annotated workflows and extracts from database tables, we can therefore create large number of samples for text/step, text/table, text/field, text/catalog item, etc. In total, we use 15 instruction templates to add variety to the input, and we plan to add more."
        },
        {
            "title": "4.1 Datasets",
            "content": "For training, we use all pairs we could extract from the Flow generation training set and dataset tables, but for our development set, we evaluate only on step, table name, and table field retrieval, as these are the most important retrieval tasks. The development examples come only from the Flow Generation development set. Table 1 shows the number of examples for steps, tables, and fields in the multitask development set. Train Pairs 172,658 Dev Dev Dev Steps Tables Fields 355 307 279 Table 1: Number of training pairs and examples in the development dataset. Although there are close to 5K unique steps in the datasets, there are around 20 that are used most frequently. This creates large imbalance in the step retrieval task. For example, the look_up_record step is used much more often than step in the slack scope, since retrieving database records is frequent operation in workflows. We then experimented with downsampling very frequent components based on their frequency in an exponential fashion (e.g., downsample steps occurring 50 times in our dataset by 4x and downsample steps occuring 500 times by 16x). similar procedure is applied to tables and fields although their imbalance is less drastic. For evaluation, we use 10 splits from deployments of the enterprise system, thereby simulating evaluating on real-world customer settings. While the labeled dataset is from our internal IT domain, these other splits come from different domains and include steps and tables not present in the IT domain. Their statistics are shown in Table 2. Dataset ID Flows OOD1 OOD2 OOD3 OOD4 OOD5 OOD6 OOD7 OOD8 OOD9 OOD10 TOTAL 103 100 100 100 100 100 98 100 100 171 1,072 Steps samples 166 94 119 136 141 179 140 145 207 134 1, Tables samples 58 209 261 188 206 255 329 188 158 477 2,329 Fields samples 107 392 450 275 305 529 665 246 301 690 3,960 Table 2: Number of retrieval examples in OOD splits. To test whether the multilingual abilities of the base model (mGTE) are preserved after our multi-task fine-tuning, we translate the development dataset using the Google Translate API. Lastly, our aim is to generalize to similar retrieval tasks, allowing the retriever to be used in any RAG application that needs similar kind of data. To this end, we create task called workflow retrieval, consisting of retrieving workflows (in YAML format) from text. This could be useful for few-shot prompting in Flow Generation. For this task, we evaluate on flows from OOD1, OOD8, and OOD9, resulting in 303 examples."
        },
        {
            "title": "4.2 Models",
            "content": "The simplest baseline we compare against is BM25, the simplest in terms of RAG deployment. Stronger baselines are open-source multilingual models. We select the small (118M), base (278M), and large (560M) versions of mE5 (Wang et al., 2024) to see whether more parameters help retrieval. Since mE5 has context length of 512, we also pick mGTEbase (Zhang et al., 2024c), which has 8,192 context length and 305M parameters. We prefer retriever with large context as, once deployed, we do not control the length of instructions/context that it will receive. We also prefer small model to allow the retrieval part of RAG to scale to many LLM concurrent calls. Appendix provides details of our training process, including hyperparameters."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "We use Recall@K (Manning, 2008) as our metric across all tasks. While we briefly explored more sophisticated metrics such as Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2002) and Mean Reciprocal Rank (MRR) (Voorhees et al., 1999), we found that these metrics highly correlate with recall in our use case. We report results on the most important retrieval tasks. When generating workflow or playbook with many steps, the LLM needs to be grounded on the customer installation of the enterprise system. In these cases, as we suggest to show many steps to the LLM, we evaluate Step@15 (K=15). When the LLM needs to include table or field name in its generation, we suggest fewer choices, as there is typically one or two tables that are needed; hence, here we use Table@5 and Field@5 (K=5)."
        },
        {
            "title": "5 Results",
            "content": "We first show how we arrived at our best multi-task retriever using the development set, comparing it to models fine-tuned on each task separately. We then show the OOD, multilingual, and workflow retrieval results."
        },
        {
            "title": "5.1 Multi-task Retriever",
            "content": "We expect multi-task fine-tuning to be better than single task fine-tuning due to larger dataset of related tasks. However, we did not expect the imbalance in the steps distribution to cause such degradation. Table 3 shows that downsampling the data causes an improvement of 8% in step retrieval with small loss in field retrieval, on fine-tuned mGTE-base model. Setup Single Task Multi-Task + Downsampled data Step@15 0.78 0.77 0.86 Table@5 0.82 0.86 0.88 Field@5 0.71 0.73 0.71 Table 3: Single task vs Multi-task and effect of balanced dataset. Evaluation on development split. Our solution is to fine-tune mGTE-base using contrastive loss objective (Hadsell et al., 2006). These findings suggest that when fine-tuning retriever for RAG, one needs to be careful on the 5 dataset make-up. In real-world settings, there typically is large data imbalance that needs to be handled according to the domain."
        },
        {
            "title": "5.2 Comparison to Baselines on OOD Splits",
            "content": "Table 4 shows average results on all OOD splits, weighted by dataset size. Model BM25 mE5-small mE5-base mE5-large mGTE-base Ours Step@15 Table@5 Field@5 0.79 0.54 0.64 0.59 0.63 0.90 0.82 0.72 0.74 0.72 0.72 0.90 0.26 0.15 0.15 0.13 0.08 0.60 Table 4: Performance of BM25, open source models, and multi-task instruction fine-tuned mGTE-base (ours). Results are weighted averages across all OOD splits. On our domain-specific retrieval tasks, our multitask fine-tuned model surpasses BM25, and the mE5 and mGTE open source alternatives. It is notable that BM25 beats all the considered open source models. Scaling the base retriever from mE5-small to mE5-large did not improve results. We also see that field retrieval is the most difficult task given that there is greater variety of field names compared to tables and steps. The retriever seems to generalize well across domains as the recall numbers do not vary much from the development set (shown in Table 3)."
        },
        {
            "title": "5.3 Effect on Multilingual Capabilities",
            "content": "We picked mGTE as our base model not only for its long context length but also because it supports multiple languages. Many open source and commercial LLMs already support multiple languages. To make RAG work properly, the retriever also needs to work well in non-English languages. We translated the development dataset into German (DE), Spanish (ES), French (FR), Japanese (JA), and Hebrew (HE) using the Google Translate API. Table 5 shows the results comparing mGTEbase and our fine-tuned version. Our multi-task fine-tuning on English datasets allows the retriever to work better than the base model on these non-English datasets. The only instance where the base model performs better is with Hebrew step retrieval. However, the average results across all three retrieval tasks are significantly lower than the English results shown in Table 4: step retrieval goes down from 0.90 to 0.64, Model Step@15 mGTE-base Ours Table@5 mGTE-base Ours Field@5 mGTE-base Ours DE ES FR JA HE Avg. 0.53 0.65 0.34 0. 0.15 0.49 0.66 0.75 0.38 0.76 0.17 0.58 0.66 0.76 0.42 0. 0.16 0.57 0.60 0.64 0.36 0.62 0.15 0.41 0.47 0.42 0.39 0. 0.17 0.36 0.58 0.64 0.38 0.66 0.16 0.48 Table 5: Results on multilingual development datasets across step, table, and field retrieval. table retrieval from 0.90 to 0.66, and field retrieval from 0.60 to 0.48. This suggests that we may need to add some multilingual domain-specific data to the multi-task dataset."
        },
        {
            "title": "5.4 Generalization to Workflow Retrieval",
            "content": "Once our retriever is deployed, it can be used by any other RAG application. We therefore evaluate it on similar task that entails retrieving workflows given text. We use recall@5 where we want to find single workflow for specific text. We picked three OOD splits to reduce evaluation time. Table 6 shows that the multi-task fine-tuned retriever performs better than both BM25 and the base model. But this may be too easy task as the base model obtains 0.87 recall@5 on average. Nevertheless, our fine-tuning improved performance, showing that the multi-task dataset can transfer knowledge to similar retrieval tasks. Model BM25 mGTE-base Ours OOD1 OOD8 OOD9 Avg. 0.66 0.73 0.87 0.94 0.94 0.98 0.66 0.90 0.90 0.60 0.76 0. Table 6: Generalization on workflow retrieval task. Metric is recall@5."
        },
        {
            "title": "6 Conclusion",
            "content": "We present an approach to build small retriever for domain-specific RAG. Via multi-task training and instruction fine-tuning, we can deploy retriever of only 305M parameters for many applications, so that hardware costs and latency are minimized. Moreover, this retriever performs well on multilingual datasets in domain-specific use cases and shows promising results generalizing to similar retrieval tasks. Future work includes expanding the retrieval tasks and improving the multilingual capabilities of the fine-tuned model."
        },
        {
            "title": "References",
            "content": "Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi, and Wentau Yih. 2023. Task-aware retrieval with instructions. In Findings of the Association for Computational Linguistics: ACL 2023, pages 36503675, Toronto, Canada. Association for Computational Linguistics. Ryan C. Barron, Ves Grantcharov, Selma Wanna, Maksim E. Eren, Manish Bhattarai, Nicholas Solovyev, George Tompkins, Charles Nicholas, Kim Ø. Rasmussen, Cynthia Matuszek, and Boian S. Alexandrov. 2024. Domainspecific retrieval-augmented generation using vector stores, knowledge graphs, and tensor factorization. Preprint, arXiv:2410.02721. Patrice Bechard and Orlando Ayala. 2024. Reducing hallucination in structured outputs via retrieval-augmented generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 228238, Mexico City, Mexico. Association for Computational Linguistics. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. Llm2vec: Large language models are secretly powerful text encoders. arXiv preprint arXiv:2404.05961. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. 2024. survey on rag meeting llms: Towards retrievalaugmented large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 64916501, New York, NY, USA. Association for Computing Machinery. Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert: pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155. Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. 2021. Scaling deep contrastive learning batch size under memory limited setup. arXiv preprint arXiv:2101.06983. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-augmented generation for large language models: survey. Preprint, arXiv:2312.10997. Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, et al. 2020. Graphcodebert: Pretraining code representations with data flow. arXiv preprint arXiv:2009.08366. Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR06), volume 2, pages 17351742. IEEE. Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), 20(4):422446. Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022. Coderetriever: large scale contrastive pre-training method for code search. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 28982910. Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, and Zhiyuan Liu. 2023. Ge yu. structure-aware language model pretraining improves dense retrieval on structured data. arXiv preprint arXiv:2305.19912. Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas Oguz, Veselin Stoyanov, and Gargi Ghosh. 2021. Multi-task retrieval for knowledge-intensive tasks. arXiv preprint arXiv:2101.00117. Christopher Manning. 2008. Introduction to information retrieval. Syngress Publishing,. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al. 2022. Text and code embeddings by contrastive pre-training. arXiv preprint arXiv:2201.10005. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, 7 Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc. Gabriel Poesia, Oleksandr Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and Sumit Gulwani. 2022. Synchromesh: Reliable code generation from pre-trained language models. arXiv preprint arXiv:2201.11227. Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. S. E. Robertson and S. Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic In Proceedings of the 17th Annual weighted retrieval. International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 94, page 232241, Berlin, Heidelberg. Springer-Verlag. Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive In Internalearning rates with sublinear memory cost. tional Conference on Machine Learning, pages 45964604. PMLR. Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Tharindu Kaluarachchi, Rajib Rana, and Suranga Nanayakkara. 2023. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering. Transactions of the Association for Computational Linguistics, 11:117. Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah Smith, Luke Zettlemoyer, and Tao Yu. 2022. One embedder, any task: Instruction-finetuned text embeddings. arXiv preprint arXiv:2212.09741. Ellen Voorhees, Dawn Tice, et al. 1999. The trec-8 question answering track evaluation. In TREC, volume 1999, page 82. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Multilingual e5 text embeddings: technical report. Preprint, arXiv:2402.05672. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations. Dejiao Zhang*, Wasi Ahmad*, Ming Tan, Hantian Ding, Ramesh Nallapati, Dan Roth, Xiaofei Ma, and Bing Xiang. 2024. Codesage: Code representation learning at scale. In The Twelfth International Conference on Learning Representations. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Instruction Zhang, Fei Wu, and Guoyin Wang. 2024a. tuning for large language models: survey. Preprint, arXiv:2308.10792. Tianjun Zhang, Shishir Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion Stoica, and Joseph E. Gonzalez. 2024b. RAFT: Adapting language model to domain specific RAG. In First Conference on Language Modeling. Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. 2024c. mGTE: Generalized long-context text representation and reranking models for multilingual text retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 13931412, Miami, Florida, US. Association for Computational Linguistics."
        },
        {
            "title": "A Training Details",
            "content": "We fine-tuned mGTE-base for 5,000 steps, equivalent to approximately 2 epochs on the dataset. Training hyperparameters were as follows: batch size of 32 was employed, with per-device batch size of 2 and 16 gradient accumulation steps. Since we are not using in-batch negatives, we do not need to employ techniques such as GradCache (Gao 8 et al., 2021) to account for larger batch sizes. The learning rate was set to 5e-5, with weight decay of 0.01. warmup period of 500 steps was implemented, followed by cosine learning rate scheduler. To optimize memory usage, gradient checkpointing was utilized (Chen et al., 2016). The Adafactor optimizer (Shazeer and Stern, 2018) was chosen for limited memory consumption."
        }
    ],
    "affiliations": [
        "ServiceNow"
    ]
}