{
    "paper_title": "SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder",
    "authors": [
        "Minglei Shi",
        "Haolin Wang",
        "Borui Zhang",
        "Wenzhao Zheng",
        "Bohan Zeng",
        "Ziyang Yuan",
        "Xiaoshi Wu",
        "Yuanxing Zhang",
        "Huan Yang",
        "Xintao Wang",
        "Pengfei Wan",
        "Kun Gai",
        "Jie Zhou",
        "Jiwen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation."
        },
        {
            "title": "Start",
            "content": "SVG-T2I: SCALING UP TEXT-TO-IMAGE LATENT DIFFUSION MODEL WITHOUT VARIATIONAL AUTOENCODER Minglei Shi1 Haolin Wang1 Borui Zhang1 Wenzhao Zheng1 Bohan Zeng2 Ziyang Yuan2 Xiaoshi Wu2 Yuanxing Zhang2 Huan Yang2 Xintao Wang2 Pengfei Wan2 Kun Gai2 1Department of Automation, Tsinghua University 2Kling Team, Kuaishou Technology Jiwen Lu1 Jie Zhou1 5 2 0 D 2 1 ] . [ 1 9 4 7 1 1 . 2 1 5 2 : r Code Repository: Model Weights: https://github.com/KlingTeam/SVG-T2I https://huggingface.co/KlingTeam/SVG-T2I"
        },
        {
            "title": "ABSTRACT",
            "content": "Visual generation grounded in Visual Foundation Model (VFM) representations offers highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully opensource the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generative modeling, particularly text-to-image synthesis (Rombach et al., 2021; Esser et al., 2024; Labs, 2024), has advanced rapidly in recent years, with diffusion models (Rombach et al., 2021; Ho et al., 2020; Song et al., 2021b; Lipman et al., 2023; Esser et al., 2024) becoming the prevailing solution. To accelerate VAE (Kingma & Welling, 2022)-based latent diffusion models (LDM) (Rombach et al., 2021), existing works (Yu et al., 2025; Leng et al., 2025; Yao et al., 2025; Kouzelis et al., 2025b; Wu et al., 2025) incorporate pretrained Visual Foundation Model (VFM) features through feature alignment or joint generation. SVG (Shi et al., 2025) and RAE (Zheng et al., 2025) take further step by training diffusion models directly in the high-dimensional VFM feature space, achieving improved generation quality as well as higher efficiency during both training and inference. In addition to performance gains, this paradigm opens the possibility of unifying the encoder infrastructure across tasks, leveraging single encoder in place of traditional choices such as SigLIP (Zhai et al., 2023; Tschannen et al., 2025) for understanding, VAE (Kingma & Welling, 2022) for generation, and VGGT (Wang et al., 2025) for geometry perception and reasoning. However, two key challenges remain unresolved: 1. Can unified feature space support visual reconstruction, perception, high-fidelity generation, and semantic understanding without compromising performance on any of these tasks? 2. Are VFM-derived representations inherently compatible with large-scale, high-resolution text-toimage diffusion training, which is essential for real-world applications? Equal contribution. Corresponding author"
        },
        {
            "title": "Technical Report",
            "content": "Figure 1: Showcases of SVG-T2I with diverse text prompts. Examples are generated at 7201280 (left), 10801080 (middle), and 1440720 (right) resolution. All corresponding prompts are listed in Appendix B."
        },
        {
            "title": "Technical Report",
            "content": "Addressing the first challenge is essential for realizing native, unified general vision model. principled representation space would eliminate the need for fragmented task-specific encoders, enabling single architecture to jointly support low-level perceptual fidelity, accurate reconstruction, and high-level semantic understanding and generation. Encouraging progress highlights the feasibility of this goal. Large-scale self-supervised learning (Kouzelis et al., 2025c) has demonstrated that expanding model capacity and training data yields strong perceptual robustness together with competitive performance in understanding tasks. Moreover, frameworks such as SVG (Shi et al., 2025), UniFlow (Yue et al., 2025), and UniLiP (Tang et al., 2025) show that VFM representations can be lightly adapted to support precise reconstruction and synthesis while retaining their discriminative semantics. These advances collectively suggest that unified representation space capable of aligning perception, reconstruction, understanding, and generation is an achievable next step for the field. Regarding the second challenge, existing approaches (Shi et al., 2025; Zheng et al., 2025; Yue et al., 2025) have already demonstrated strong performance on class-conditioned ImageNet (Deng et al., 2009) generation. However, large-scale and systematic investigations on text-to-image generation remain largely absent. Given that ImageNet has limited scale and category diversity, and most vision encoders are already pre-trained on it, such results cannot validate the generalization ability of these methods. As result, there remain open questions regarding their feasibility and performance potential in realistic text-to-image scenarios. In this paper, we mainly focus on the second challenge and conduct the first large-scale study on training text-to-image diffusion model directly within the VFM feature space. We summarize the primary contributions of this paper as follows: We provide the first large-scale validation of the text-to-image latent diffusion model on the VFM feature space. We open-source the complete training and inference pipeline of the SVG-T2I model, together with pre-trained weights of multiple sizes, to facilitate future research."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Visual Generative Models. Visual generative models aim to approximate the underlying data distribution of real-world visual content and synthesize new samples drawn from it. Over their development, several major paradigms have emerged. Generative adversarial networks (GANs) (Goodfellow et al., 2014; Arjovsky et al., 2017; Gulrajani et al., 2017; Karras et al., 2019; Zhu et al., 2017; Radford et al., 2016; Karras et al., 2018; Sauer et al., 2022) generate high-fidelity images via adversarial training but often suffer unstable training, model collapse, and poor interpretability. Autoregressive models (Salimans et al., 2017; Vaswani et al., 2018; Chen et al., 2020; Sun et al., 2024; Tian et al., 2024) frame generation as sequential prediction task, achieving strong distribution modeling and stability through likelihood maximization, yet they incur high inference latency due to their token-by-token nature. Masked generative models (Chang et al., 2022; Zheng et al., 2024; Yu et al., 2024; Li et al., 2024a) predict missing tokens given visible context, analogous to masked language models in NLP, also achieving strong performance. Recently, Diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021; Song et al., 2021a;b) have been established as the leading paradigm by learning to invert progressive noising process. They offer robust optimization objectives, superior mode coverage. An enhanced approach, the latent diffusion model (LDM) (Rombach et al., 2021; Peebles & Xie, 2022; Ma et al., 2024; Liu et al., 2022), incorporates VAE (Kingma & Welling, 2022) into the diffusion pipeline to operate within lower-dimensional latent space, thereby reducing computational cost while preserving generation quality. However, the latent space of VAEs lacks coherent semantic structure, making it suboptimal for downstream understanding and perception tasks. Representation for Generative Modeling. Recent works have increasingly focused on improving the representation of generative modeling. One direction enhances the VAE latent space through structural regularization (Burgess et al., 2018; Xu & Durrett, 2018; Kouzelis et al., 2025a; Skorokhodov et al., 2025) or diffusion-guided reconstruction (Preechakul et al., 2022; Pandey et al., 2021; Vahdat et al., 2021). Another line aligns generative latents with external semantic embeddings (Yu et al., 2025; Leng et al., 2025; Yao et al., 2025; Li et al., 2023) or incorporates discriminative knowledge in generation (Li et al., 2023; Wu et al., 2025; Kouzelis et al., 2025b). More recently, SVG (Shi et al., 2025) and RAE (Zheng et al., 2025) demonstrate strong performance by training diffusion"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Architecture of the proposed SVG-T2I. (Left) Single-Stream DiT architecture. (Right) We optionally augment the DINO encoder with Residual Encoder to achieve high-quality reconstruction and preserve transferability. models directly in the VFM feature space. Concurrent works such as UniLiP (Tang et al., 2025) and UniFlow (Yue et al., 2025) demonstrate that self-distilled VFMs, when coupled with pixel decoder and trained under MAR (Li et al., 2024a) or MetaQueries-style paradigms (Pan et al., 2025), can effectively support both high-quality reconstruction and generation."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "3.1 PRELIMINARIES Diffusion models. Diffusion Models (Ho et al., 2020; Rombach et al., 2021; Song et al., 2021b) have been the dominant generative modeling for continuous feature space, which can transform the Gaussian distribution to the data distribution through iterative inference. In this paper, we adopt the widely used flow matching method (Liu et al., 2022; Lipman et al., 2023; Esser et al., 2024) for training SVG-T2I. Flow matching constructs velocity field that interpolates between Gaussian distribution and the data distribution: xt = (1 t)x0 + tϵ, [0, 1], ϵ (0, I), The flow matching objective is then formulated as LFM = Ex0p0(x),ϵp1(x)[λ(t)vθ(xt, t) (ϵ x0)]. (1) (2) Sampling from flow-based model can be achieved by solving the probability flow ODE. 3.2 SELF-SUPERVISED REPRESENTATIONS FOR VISUAL GENERATION SVG (Shi et al., 2025) demonstrated the feasibility of achieving high-quality image reconstruction and class-to-image generation within high-dimensional VFM feature spaces. Building on this foundation, SVG-T2I extends the approach to large-scale text-to-image training, enabling effective generation directly in the VFM feature domain. The overall architecture of SVG-T2I is shown in Figure 2. SVG-T2I Autoencoder. Inheriting the architectural design from SVG (Shi et al., 2025) and RAE (Zheng et al., 2025), we release two autoencoder configurations to facilitate community research. The first, autoencoder-P (Pure), utilizes frozen DINOv3 features directly. The second, autoencoder-R (Residual), retains the residual branch design from SVG as an optional choice. This residual module, built on Vision Transformer (Dosovitskiy et al., 2021), is designed to compensate for high-frequency details and color cast artifacts when higher fidelity is required. Both variants utilize the same decoder design to map features back to pixel space."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Visualization of SVG reconstruction. For each reconstructed image, we also present the PCA visualization of its DINOv3 features, and DINOv2 features are shown in the right three columns. The numbers indicate the resolution of the input images. SVG-T2I DiT. We use the Unified Next-DiT (Qin et al., 2025) architecture as our backbone, which treats text and image tokens as joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. The Unified Next-DiT architecture is scalable single-stream variant similar to that used in the state-of-the-art open-source VAE-based text-to-image model ZImage (Team et al., 2025). We adopt this single-stream design to achieve greater parameter efficiency and to jointly process text and DINO features. We train the backbone directly on high-dimensional VFM(DINOv3) feature space, using the flow matching objective defined in Equation (2). In our framework, we use the DINOv3-ViT-S/16+ encoder, which maps an 3 image to (H/16) (W/16) 384 feature representation. SVG-T2I Training Pipeline. Training proceeds in two stages. In the first stage, we train autoencoderP, autoencoder-R separately from scratch. Specifically, autoencoder-R is optimized with both reconstruction losses and distribution-matching strategy on its residual branch and decoder following (Shi et al., 2025). In the second stage, we train SVG-T2I DiT equipped with autoencoder-P, following progressive schedule (see training details). 3.3 SCALING SVG TO HIGHER RESOLUTION SVG (Shi et al., 2025) and RAE (Zheng et al., 2025) primarily focused on learning generative diffusion models within VFM representation spaces under low-resolution settings. In this work, we extend this line of inquiry by examining the behavior and effectiveness of SVG for high-resolution generation. We observe distinct resolution-dependent behaviors when reconstructing images from DINOv3 features, as illustrated in Figure 3. While reconstructions from low-resolution inputs suffer from degradation in fine structures, high-resolution inputs yield substantially more detailed and faithful results. This indicates that DINOv3 representations inherently preserve detailed visual cues effectively at higher resolutions. Crucially, this capability suggests that the DINOv3 encoder alone is relatively sufficient for high-resolution reconstruction, obviating the need for an auxiliary residual encoder. Furthermore, relying exclusively on VFM representations offers more generalized and reusable paradigm compared to hybrid architectures. Motivated by both the representation sufficiency and the desire for streamlined, versatile framework, we configure the residual encoder in the original SVG Autoencoder as optional, omitting it during high-resolution reconstruction or generation."
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Comparison of VAE and DINO features. We visualize the PCA projections of features extracted by DINOv3 (Left), DINOv2 (Middle), and the VAE (Right). Cosine similarity across different resolutions is computed by downsampling higher-resolution features to match the spatial size of the lower ones. Overall, VAE features exhibit limited semantic structure, yet demonstrate stronger scale invariance compared to DINO features. Table 1: Overview of the datasets used across different training stages. Cap. Sample Ratio denotes the probability of sampling short, middle, and long captions during training for each dataset, which controls the caption-length distribution seen by the model. Data Type Number Description Caption Length Cap. Sample Ratio Reconstruction Generation D 1.2M 3M 60M 15M 1M ImageNet General Data High-quality Realistic Data - - - - High-quality General Data High-quality Realistic Data High-aesthetic Data Short, Middle, Long Short, Middle, Long Short, Middle, Long (0.10, 0.35, 0.55) (0.10, 0.35, 0.55) (0.00, 0.00, 1.00)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we describe the training recipe of SVG-T2I, then validate the feasibility and effectiveness of the proposed SVG-T2I through extensive experiments. 4.1 MODEL TRAINING Training Details of SVG-T2I Autoencoder. The autoencoder is trained with progressive strategy. We first pre-train the model on ImageNet (Data A) for 40 epochs at fixed resolution of 256256. Then, during the multi-resolution fine-tuning stage, we continue training using native-resolution images from 3M-sample dataset (Data B). In this phase, the model is trained at an anchor resolution of 512512 for 10M seen images, followed by 10241024 for an additional 6M seen images. Training Details of SVG-T2I DiT. We adopt the Unified Next-DiT architecture from Lumina-Image-2.0 (Qin et al., 2025) as the backbone of our diffusion transformer. For text conditioning, we utilize the Gemma2-2B large language model, which possesses strong multilingual capabilities, to extract rich textual embeddings. We set the maximum text token length to 256 to balance long-caption modeling capability and training efficiency at first three stages. In high quality data tuning state, the maximum text token length is set to 512. Each image in Data (C, D, E) is annotated with bilingual captions (Chinese and English) in three lengths: short, middle, and long. During training, we adopt mixed sampling strategy that selects both the caption language and its length. The sampling probabilities for short, middle, and long captions are provided in Table 1, and the language sampling ratio is fixed to 0.2 for Chinese and 0.8 for English. We train SVG-T2I which is equipped with autoencoder-P using multi-stage progressive training strategy. In the first two stage, the model is trained at low resolution and middle resolution on 60M"
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Comparison of SVG-T2I four-stage results. The generated images become progressively more detailed and aesthetically refined as the stages advance. samples (Data C) to establish robust textimage alignment and capture low-frequency structures. In the third stage, we transfer the learned knowledge to higher resolutions, enabling the model to refine fine-grained visual details using 15M samples (Data D). In the final stage, SVG-T2I is fine-tuned on 1M high-quality aesthetic samples (Data E) to further enhance its ability to synthesize realistic and visually appealing outputs. As shown in Figure Figure 5, the visual quality improves steadily across stages. 4.2 MAIN RESULTS Evaluation. We evaluated SVG-T2I through both quantitative and qualitative metrics. We report performance on GenEval (Ghosh et al., 2023) and DPG-Bench (Hu et al., 2024) to evaluate allaround capabilities of SVG-T2I following their official protocol. All images used for evaluation were generated at high resolution of 1024 1024. Our SVG-T2I model successfully scales the VFM representation paradigm for large-scale T2I generation, achieving competitive performance across these two benchmarks. On GenEval Ghosh et al. (2023)  (Table 5)  , our final model, SVG-T2I, attains an overall score of 0.74, matching the performance of models like SD3-Medium (Esser et al., 2024) and significantly surpassing SDXL (Podell et al., 2023) and DALL-E 2 (Betker et al., 2023). Furthermore, on DPG-Bench  (Table 6)  , SVG-T2I achieves an overall score of 85.78, placing it statistically comparable to top VAE-based diffusion models such as FLUX.1 (Labs, 2024) and HiDream-I1-Full (Cai et al., 2025). Table 2: Model configuration of the proposed SVG-T2I DiT architecture. Model Params Patch Size Dimension Head KV Heads Layers Pos. Emb. SVG-T2I 2.6B 1 2304 8 26 M-RoPE Table 3: Training configuration of SVG-T2I Decoder across different stages. Stage Fixed Low Res. Multi Middle Res. Multi High Res. Anchor Resolution 2562 5122 10242 #Images Training Steps (K) Batch Size #Seen Samples 1.2M 3M 3M 94K 78K 190K 512 128 48M 10M 6M 4.3 ANALYSIS Limitations of current VFM features. Existing self-supervised learning methods produce representations that capture both high-level semantic context and fine-grained visual detail, offering"
        },
        {
            "title": "Technical Report",
            "content": "Table 4: Training configuration of SVG-T2I DiT across different stages. Stage Multi Low Res. Multi Middle Res. Multi High Res. HQ Tuning. Anchor Resolution 2562 5122 10242 10242 #Images Training Steps (K) Batch Size #Seen Samples 60M 60M 15M 1M 91K 90K 44K 40K 1536 768 768 768 140M 70M 34M 30M Table 5: Evaluation of text-to-image generation ability on GenEval (Ghosh et al., 2023) benchmark. refer to the methods using LLM rewriter. Model Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Chameleon (Team, 2025) Emu3-Gen Wang et al. (2024) Show-o Xie et al. (2025) Janus-Pro-7B (Chen et al., 2025) Discrete Generation (Autoregressive) - 0.99 0.98 0.99 - 0.81 0.80 0.89 - 0.42 0.66 0. VAE-based Generation (Diffusion) PixArt-α (Chen et al., 2023a) SDv2.1 (Rombach et al., 2021) DALL-E 2 (Betker et al., 2023) SDXL (Podell et al., 2023) DALL-E 3 (Betker et al., 2023) Lumina-Image-2.0 (Qin et al., 2025) SD3-Medium (Esser et al., 2024) FLUX.1-dev (Labs, 2024) 0.98 0.98 0.94 0.98 0.96 - 0.99 0.98 0.50 0.51 0.66 0.74 0.87 0.87 0.94 0.93 0.44 0.44 0.49 0.39 0.47 0.67 0.72 0.75 Representation Generation (Diffusion) - 0.80 0.84 0.90 0.80 0.85 0.77 0.85 0.83 - 0.89 0.93 - 0.49 0.31 0.79 0.08 0.07 0.10 0.15 0.43 - 0.33 0.68 - 0.45 0.50 0.66 0.07 0.17 0.19 0.23 0.45 0.62 0.60 0. 0.39 0.66 0.68 0.80 0.48 0.50 0.52 0.55 0.67 0.73 0.74 0.82 SVG-T2I 0.94 0.89 0. 0.89 0.69 0.62 0.75 strong basis for downstream reconstruction and generation. In principle, these representations are largely self-sufficient. However, this self-sufficiency is critically challenged when the training paradigm involves multiple input resolutions. As represented in Figure 4, VAE features exhibit nearly resolution-invariant behavior. Their cross-resolution consine similarity is close to 1.0, whereas DINOv3 and DINOv2 features vary more substantially. This observation indicates that VFM-derived features undergo non-negligible shifts across scales. When VFM encoder uses fixed patch or receptive-field size (e.g., 1616) across inputs of different absolute resolution, the semantic granularity and effective compression ratio of each patch vary systematically with scale: patch on low-resolution image aggregates much larger portion of the scene, producing strongly compressed, detail-poor features; the identical patch size on highresolution image captures finer, predominantly local texture and structural detail. Because VFM encoders are typically optimized to produce semantically discriminative tokens rather than to preserve uniform local detail, they are particularly sensitive to this scale-dependent shift in the semantic/texture balance. By contrast, reconstruction-oriented encoders (e.g., VAEs) do not explicitly account for the semantic content present in each encoded region; instead, they primarily aim to capture sufficient local information for pixel-level reconstruction, leading to more uniform and resolution-stable allocation of representation capacity. Accordingly, for semantic visual encoders used for diffusion modeling, maintaining stable crossresolution behavior emerges as an important optimization goal. The training pipeline may need to incorporate mechanisms that encourage consistent feature geometry and help preserve the fidelity of fine-grained details across scales. Limitations of SVG-T2I. While SVG-T2I demonstrates strong generation capability across diverse scenarios, several limitations remain. As shown in Figure 6, the model occasionally struggles to produce highly detailed human faces, particularly in regions requiring fine-grained spatial consistency, such as eyes, eyebrows. Similarly, the generation of anatomically accurate fingers continues to be challenging, common failure mode in generative models, often resulting in distorted shapes or incorrect topology when pose complexity increases. SVG-T2I also exhibits limited reliability in text rendering. These shortcomings largely stem from the insufficient coverage of such fine-grained"
        },
        {
            "title": "Technical Report",
            "content": "Table 6: Evaluation of text-to-image generation ability on DPG (Hu et al., 2024) benchmark. Model Global Entity Attribute Relation Other Overall Discrete Generation (Autoregressive) Janus (Wu et al., 2024) Emu3-Gen (Wang et al., 2024) Janus-Pro-1B (Chen et al., 2025) Janus-Pro-7B (Chen et al., 2025) 82.33 85.21 87.58 86. 87.38 86.68 88.63 88.90 87.70 86.84 88.17 89.40 VAE-based Generation (Diffusion) SD1.5 (Rombach et al., 2021) PixArt-α (Chen et al., 2023b) Lumina-Next (Zhuo et al., 2024) SDXL (Podell et al., 2023) Hunyuan-DiT (Li et al., 2024b) PixArt-Σ (Chen et al., 2024) DALL-E 3 (Betker et al., 2023) FLUX.1 [Dev] (Labs, 2024) SD3 Medium (Esser et al., 2024) HiDream-I1-Full (Cai et al., 2025) Lumina-Image 2.0 (Qin et al., 2025) 74.63 74.97 82.82 83.27 84.59 86.89 90.97 74.35 87.90 76.44 - 74.23 79.32 88.65 82.43 80.59 82.89 89.61 90.00 91.01 90.22 91. 75.39 78.60 86.44 80.91 88.01 88.94 88.39 88.96 88.83 89.48 90.20 85.46 90.22 88.98 89.32 73.49 82.57 80.53 86.76 74.36 86.59 90.58 90.87 80.70 93.74 94.85 86.41 83.15 88.30 89.48 67.81 76.96 81.82 80.41 86.41 87.68 89.83 88.33 88.68 91.83 - 79.68 80.60 82.63 84. 63.18 71.11 74.63 74.65 78.87 80.54 83.50 83.84 84.08 85.89 87.20 Representation Generation (Diffusion) SVG-T2I 88.50 91.00 91. 92.21 91.86 85.78 Figure 6: Failure cases of SVG-T2I. These examples illustrate that SVG-T2I may struggle with generating highly detailed human faces, accurate finger structures, and reliable text rendering, which typically require more specialized data and larger training compute. cases in the training corpus, as well as the substantial computational demand required to model high-frequency patterns and precise geometric relationships. Addressing these limitations will likely require more specialized datasets and additional training compute."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we successfully extended the original SVG framework to large-scale, high-resolution text-to-image synthesis, culminating in the SVG-T2I model. Our work validates the feasibility of training high-quality T2I model from scratch based on VFM representations, achieving generative metrics comparable to modern advanced methods and demonstrating the potential of the VFM semantic space as an effective latent manifold for high-resolution synthesis. To foster further research and ensure reproducibility, we have fully open-sourced the training, inference, and evaluation code, along with the model weights, hoping to benefit the academic community. However, in the course of this research, we also identified critical challenge: existing VFM encoders (such as"
        },
        {
            "title": "Technical Report",
            "content": "DINOv2 (Oquab et al., 2023) and DINOv3 (Simeoni et al., 2025)) produce representations with poor internal consistency when encoding the same image at different input resolutions. This resolutiondependent feature instability directly compromises the T2I models ability to generalize across various sizes and maintain generation quality, underscoring the necessity for future research to focus on scale-invariance. Overall, we believe that the strategic use and refinement of powerful VFM latent space, as demonstrated in this work, present highly promising avenue toward achieving truly unified representation for diverse visual tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan, 2017. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, and Yunxin Jiao. Improving image generation with better captions, 2023. Preprint. Available at OpenAI Papers. Christopher P. Burgess, I. Higgins, Arka Pal, L. Matthey, Nicholas Watters, Guillaume Desjardins, and Alexander Lerchner. Understanding disentangling in beta-vae. ArXiv, abs/1804.03599, 2018. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. In CVPR, June 2022. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis, 2023a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023b. Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever. Generative pretraining from pixels. 2020. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling, 2025. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, pp. 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023."
        },
        {
            "title": "Technical Report",
            "content": "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), NeurIPS, volume 27. Curran Associates, Inc., 2014. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), NeurIPS, volume 33, pp. 68406851. Curran Associates, Inc., 2020. Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment, 2024. Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. In ICLR, 2018. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks, 2019. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. Theodoros Kouzelis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. EQ-VAE: Equivariance regularized latent space for improved generative image modeling. In ICML, 2025a. Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Boosting generative image modeling via joint image-feature synthesis. In NeurIPS, 2025b. Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Boosting generative image modeling via joint image-feature synthesis. 2025c. Black Forest Labs. Flux: powerful tool for text generation, 2024. Accessed: 2024-09-26. Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method. arXiv:2312.03701, 2023. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization, 2024a. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024b. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, pp. 81628171. PMLR, 2021. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. VAEs meet diffusion models: Efficient and high-fidelity generation. In NeurIPS, 2021. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis, 2023. Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion autoencoders: Toward meaningful and decodable representation. In CVPR, 2022. Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wenhai Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu Qiao, Chang Xu, and Peng Gao. Lumina-image 2.0: unified and efficient image generative framework, 2025. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks, 2016. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: pixelcnn implementation with discretized logistic mixture likelihood and other modifications. In ICLR, 2017. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder, 2025. Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. Ivan Skorokhodov, Sharath Girish, Benran Hu, Willi Menapace, Yanyu Li, Rameen Abdal, Sergey Tulyakov, and Aliaksandr Siarohin. Improving the diffusability of autoencoders. In ICML, 2025. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ICLR, 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021b."
        },
        {
            "title": "Technical Report",
            "content": "Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, and Liwei Wang. Unilip: Adapting clip for unified multimodal understanding, generation and editing. arXiv preprint arXiv:2507.23278, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2025. Image Team, Huanqia Cai, Sihan Cao, Ruoyi Du, Peng Gao, Steven Hoi, Zhaohui Hou, Shijie Huang, Dengyang Jiang, Xin Jin, Liangchen Li, Zhen Li, Zhong-Yu Li, David Liu, Dongyang Liu, Junhan Shi, Qilong Wu, Feng Yu, Chi Zhang, Shifeng Zhang, and Shilin Zhou. Z-image: An efficient image generation foundation model with single-stream diffusion transformer, 2025. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Henaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, 2025. Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In NeurIPS, 2021. Ashish Vaswani, Niki Parmar, Jakob Uszkoreit, Noam Shazeer, and Lukasz Kaiser. Image transformer, 2018. Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer, 2025. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Liangdong Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need, 2024. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal understanding and generation, 2024. Ge Wu, Shen Zhang, Ruijing Shi, Shanghua Gao, Zhenyuan Chen, Lei Wang, Zhaowei Chen, Hongcheng Gao, Yao Tang, Jian Yang, et al. Representation entanglement for generation: Training diffusion transformers is much easier than you think. arXiv preprint arXiv:2507.01467, 2025. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation, 2025. Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. In EMNLP, 2018. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In CVPR, 2025. Lijun Yu, Jose Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion tokenizer is key to visual generation, 2024. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025."
        },
        {
            "title": "Technical Report",
            "content": "Zhengrong Yue, Haiyu Zhang, Xiangyu Zeng, Boyu Chen, Chenting Wang, Shaobin Zhuang, Lu Dong, KunPeng Du, Yi Wang, Limin Wang, and Yali Wang. Uniflow: unified pixel flow tokenizer for visual understanding and generation, 2025. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders, 2025. Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. In TMLR, 2024. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycle-consistent adversarial networkss. In ICCV, 2017. Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-next: Making lumina-t2x stronger and faster with next-dit, 2024."
        },
        {
            "title": "Technical Report",
            "content": "Table 7: Hyperparameter setup of SVG-T2I Autoencoder. Encoder Base model Downsample Ratio Latent dim. Residual branch Training mode Params Decoder Channels dim. Out channels channels Params Optimization Optimizer lr (β1, β2) Autoencoder-P Autoencoder-R DINOv3-s16p 16 16 384 / Frozen 29M DINOv3-s16p 16 16 392 ViT-S-RoPE Frozen 51M [512, 256, 256, 128, 128] 3 384 43M 3 392 43M Adam 1e-4 (0.5, 0.9) Adam 1e-4 (0.5, 0.9) Table 8: Hyperparameter setup of SVG-T2I DiT. SVG-T2I-L Lumina-Image-2.0 Architecture Downsample Ratio Latent dim. Num. layers Hidden dim. Num. heads Params Base-encoder Optimization Optimizer lr (β1, β2) Interpolants αt σt Training objective Sampler 16 16 384 26 2304 24 2.6B Autoencoder-P AdamW 2e-4 (0.9, 0.95) 1 v-prediction Euler 8 8 16 26 2304 24 2.6B Flux-VAE AdamW 2e-4 (0.9, 0.95) 1 v-prediction Euler"
        },
        {
            "title": "A MORE QUALITATIVE RESULTS",
            "content": "We provide additional qualitative results of SVG-T2I with 1080 1080 resolution. These results further demonstrate the diversity and visual quality of the proposed approach."
        },
        {
            "title": "Technical Report",
            "content": "Figure 7: Visualization of SVG-T2I at 10801080 resolution. These examples highlight the models ability to produce high-quality, text-aligned visual content with fine-grained details and strong spatial consistency."
        },
        {
            "title": "Technical Report",
            "content": "B PROMPTS USED IN FIGURE 1 Here we summarize the prompts/instructions used in Figure 1, which can be directly input into the corresponding model to reproduce our generation results. Column #1: Case #1: Realistic, high-resolution photography with commercial still-life style. The image emphasizes clarity, texture, and color accuracy, suitable for food and beverage advertising or editorial use., Two clear wine glasses filled with white wine are placed on rustic wooden table, surrounded by green grapes, bread, wine corks, and bottle of white wine in the background., The main subjects are two transparent wine glasses, each filled with pale yellow-white wine. The glasses are made of clear glass with long stems and round bowls. The wine is clear and slightly golden, with visible reflections on the glass surfaces. The glasses are positioned close together, with the left glass slightly in front of the right. Both glasses are upright and stable on the table. In the foreground, there are clusters of green grapes with smooth, round shapes and translucent skin. Several loose grapes are scattered near the base of the glasses. To the right of the glasses, there is piece of rustic brown bread with rough texture and visible crust. In the background, wire basket contains multiple cylindrical beige wine corks, and glass bottle of white wine is partially visible., The scene is set indoors on rustic wooden table with natural, weathered texture and light brown color. The foreground features green grapes in sharp focus, with some grapes and corks scattered around the base of the glasses. The midground contains the wine glasses, bread, and wire basket of corks. The background is slightly blurred and includes glass bottle of white wine and wooden wall with warm, neutral tone. The overall color palette consists of natural wood, green, beige, and pale yellow tones, creating cozy and inviting atmosphere., Realistic, The lighting is natural and soft, with warm color temperature. The main light source comes from the left side, casting gentle highlights on the glass surfaces and creating soft reflections. The lighting is even, with subtle shadows under the glasses and grapes, contributing to bright and inviting tone., The composition is centered, with the two wine glasses occupying the central area of the frame and taking up about 50% of the image. The shot is close-up, taken at slightly high angle, focusing on the glasses and surrounding elements. The camera is positioned at table height, angled slightly downward. standard lens with wide aperture is used, resulting in shallow depth of field that keeps the foreground and main subjects sharp while softly blurring the background. Case #2: The subject is young East Asian woman with fair skin and slender, proportionate build. Her long, wavy, ash-blonde hair falls over her shoulders and down her chest. She wears decorative headpiece adorned with silver, gold, and blue gemstones and metallic leaf motifs. Her gown is light lavender color with sheer long sleeves and intricate floral embroidery in silver and white, featuring fitted bodice and voluminous tulle skirt. Her posture is upright, with both hands gently placed on her lap, fingers relaxed. Her body faces slightly to the left of the frame, and her head is oriented forward., The scene is an indoor setting with soft, dreamy atmosphere. The background consists of white curtains with subtle lace pattern, creating delicate and airy backdrop. Green leafy vines are draped along the upper left and right edges of the background, adding natural element. The foreground is clear, showing the subjects gown and hands in detail. The background is softly blurred, with the greenery and curtain textures providing gentle contrast., Centered composition; the subject occupies the central portion of the frame and approximately 70% of the image. Medium close-up shot, straight-on angle at eye level. The camera is positioned at medium distance, using standard focal length lens with wide aperture, resulting in shallow depth of field that keeps the subject in sharp focus while softly blurring the background., Realistic, high-resolution photography with soft, romantic, and slightly dreamy aesthetic. The image emphasizes delicate textures and fine details, suitable for fashion or portrait editorial use., young woman with long wavy hair sits indoors, wearing light lavender gown and an ornate jeweled headpiece, with her hands resting on her lap., Realistic, Soft, diffused lighting with cool to neutral color temperature. The light source appears to be natural or simulated daylight, coming from the front and slightly above, evenly illuminating the subject and"
        },
        {
            "title": "Technical Report",
            "content": "minimizing harsh shadows. The lighting creates gentle, ethereal effect on the gown and hair. Case #3: Realistic, high-resolution landscape photography, snow-covered mountain range rises behind calm lake, with the peaks and surrounding forest reflected clearly in the water under bright blue sky., The subject is group of rugged mountain peaks covered in white snow, with sharp ridges and rocky outcrops. The mountains are flanked by steep, forested slopes with dark green coniferous trees. The lower slopes and the area around the lake are dusted with snow. The reflection of the mountains and trees is visible in the still, pale greenish water of the lake, forming near-symmetrical image., The scene is an outdoor alpine landscape during daytime in winter. The foreground consists of the shallow, clear lake water with sandy and slightly muddy bottom, reflecting the mountains and sky. The middle ground features the lakes edge lined with snow and dense band of dark green coniferous trees. The background is dominated by the snow-capped mountain range, with rocky faces and patches of snow, set against cloudless, vivid blue sky. The foreground water is clear and detailed, while the background mountains are sharp and well-defined., Realistic, Natural sunlight, cool color temperature, hard light. The light source is from above and slightly to the left, illuminating the snow and casting subtle shadows on the mountain slopes. The lighting is even and high-key, enhancing the clarity of the snow and the reflection in the water., Symmetrical composition with the mountain range centered in the frame and its reflection forming vertical axis. Wide landscape shot, straight-on angle at eye level. The mountains occupy the upper half of the frame, while the lake and its reflection fill the lower half. Wide-angle lens, small aperture, deep depth of field. Case #4: photorealistic, The image features three cupcakes adorned with colorful sprinkles and encased in checkered wrappers. The cupcakes are positioned on pink surface with scattered sprinkles around them. In the background, there is small green leaf, possibly mint leaf, adding hint of freshness to the composition., The foreground of the image showcases three cupcakes. Each cupcake is generously topped with colorful assortment of sprinkles that include shades of pink, white, red, green, and blue. The cupcakes are wrapped in checkered brown and black paper, providing neat and structured appearance. The upper surface of the cupcakes is golden brown, indicating they are well-baked, with soft, crumbly texture visible. They are compactly arranged, filling the bottom half of the image., The background of the image consists of smooth pink surface, which enhances the vibrant colors of the sprinkles. Among the scattered sprinkles, there is small green leaf, likely mint leaf, offering contrast to the otherwise pink and colorful visuals. The setting suggests modern and bright environment, focused on accentuating the cupcakes., solid color, close-up, standard lens, natural light, central composition, frontal view Case #5: realistic photography, frontal view, blurred, central composition, vibrant red rose is prominently featured in the foreground, with droplets of water on its petals. The background shows green foliage and bushes, likely indicating garden setting. The image captures the delicacy and color of the rose against blurred backdrop of greenery., natural light, close-up, macro lens. Case #6: realistic, high-resolution photography, Bengal cat with green eyes lies on brown surface, facing the camera with its head slightly tilted., The subject is an adult Bengal cat. It has muscular build and short, dense fur with golden-brown base and dark brown rosette and stripe patterns. The cats face is broad with pinkish-brown nose, white muzzle, and long white whiskers. Its ears are upright and pointed, with pinkish inner fur. The cats eyes are large, round, and bright green, with vertical slit pupils. Its front legs are tucked under its body, and its head is slightly tilted to the left. The cats gaze is directed straight at the camera, and its expression appears calm and alert., The scene is indoors. The foreground consists of brown fabric surface, likely couch or cushion, which is in sharp focus. The background is smooth, dark brown wall, softly blurred. The overall color palette is warm, dominated by shades of brown and gold., Realistic, Natural light, soft and diffused, coming from the front left. The lighting is warm, evenly illuminating the cats face and fur, with gentle"
        },
        {
            "title": "Technical Report",
            "content": "shadows under the chin and on the right side of the face., Centered composition; the cats face occupies the central area of the frame, filling about 70% of the image. Medium close-up shot, eye-level angle. The camera is positioned close to the subject, using standard or short telephoto lens. Shallow depth of field, with the cat in sharp focus and the background blurred. Column #2: Case #1: Realistic, high-resolution photography with classic portrait style. The image emphasizes texture and detail in clothing and accessories, with focus on elegance and sophistication., woman with curly brown hair is dressed in fur-collared coat and black hat with ornate embroidery, standing against dark, softly lit background., The subject is woman, likely young adult, with light skin and voluminous, curly brown hair that frames her head and falls around her ears. She wears black hat adorned with intricate gold and silver embroidery featuring floral and leaf patterns. Her ears are visible, and she wears dangling, multi-stone earrings. She is dressed in coat with thick, grayish fur collar that frames her neck and shoulders. Her posture is upright, with her head and body facing directly forward., The scene is indoors with dark, softly blurred background. The background features deep brown and reddish hues, creating warm and subdued atmosphere. The foreground is occupied by the subjects upper body and clothing, which are in sharp focus. The background is out of focus, providing sense of depth and isolating the subject., Realistic, Soft, warm lighting, likely artificial, coming from the front and slightly above the subject. The light gently illuminates the subjects hair, hat, and fur collar, creating subtle highlights and soft shadows. The background remains darker, enhancing the subjects prominence., Centered composition; the subject occupies the central portion of the frame and fills most of the vertical space. Medium close-up shot, straight-on angle at eye level. The camera is positioned close to the subject, using standard or short telephoto lens. Shallow depth of field, with the subject in sharp focus and the background blurred. Case #2: Realistic style, photography, finely detailed, high color saturation, rich in details, overall conveying healthy lifestyle theme. bowl of colorful fruit salad placed on burlap cloth, surrounded by fresh fruits such as bananas, kiwis, apples, grapes, and strawberries, the scene full of natural and healthy life vibes. The main subject is bowl of mixed fruit salad, including sliced bananas, strawberries, kiwis, grapes, mangoes, and apples, with vivid colors, evenly cut pieces, fresh and glossy fruit surfaces, arranged generously, served in light-colored ceramic bowl with rounded rim and smooth body. The scene is indoors, with the main subject positioned slightly lower in the center of the frame; the foreground features burlap and wooden tabletop with clear details. In the lower-left corner, there are two red strawberries and light wooden spoon; in the upper-right corner, red-yellow gradient apple; the background contains bunch of reddish-purple grapes and whole kiwi, with three yellow bananas in the upper-left corner. Background objects are slightly blurred, the overall color tone is warm, and the atmosphere is natural and fresh. Natural warm soft light, front lighting from the upper-left side, overall high-key lighting, bright scene, with soft highlights reflecting off the fruit surfaces. Case #3: vibrant watercolor botanical illustration featuring red and yellow tulip flowers, depicted in close-up pattern style with abstract blue and green backgrounds., The image is high-quality, semi-realistic botanical illustration created in watercolor. The style combines realistic floral forms with expressive, abstract background elements. The technique features wet-on-wet blending, visible brushstrokes, and splatter effects, characteristic of contemporary botanical art. The overall effect is lively, decorative, and artistic, suitable for use in textiles, wallpapers, or stationery., The main subjects are several tulip flowers, shown in various stages of bloom. The tulips have elongated, slightly curved petals with mix of red, yellow, and orange hues, blending softly at the edges. The petals are rendered with visible watercolor gradients and subtle textural variations, giving sense of translucency and freshness. The green stems and leaves are painted with loose, expressive brushstrokes, adding to the lively aesthetic. The flowers are depicted in natural, upright orientation, with"
        },
        {
            "title": "Technical Report",
            "content": "some petals overlapping and others angled differently, creating dynamic arrangement., The lighting is implied through the use of watercolor techniques, with soft, diffused highlights and gentle shading that suggest natural daylight. The color temperature is warm, with the red and yellow petals contrasting against the cooler blue and green background. The overall tone is high key, with bright, luminous colors and minimal shadow, enhancing the fresh and uplifting mood., The environment is an abstract, artistic background composed of overlapping washes of blue, green, and yellow watercolor, with splashes and drips that evoke sense of spontaneity and movement. The background is mostly clear, with the floral elements standing out against the colorful, painterly backdrop. The overall atmosphere is bright, cheerful, and energetic, reminiscent of spring or early summer setting., Non-realistic, The composition is close-up, pattern-like arrangement with repeating motif. The tulip flowers are distributed diagonally and vertically across the frame, filling most of the image space. The perspective is flat and frontal, with the flowers overlapping and intersecting, creating sense of depth through layering. The image uses centered and balanced composition, with the flowers occupying the majority of the visual field. The depth of field is shallow, with all elements rendered in sharp focus due to the illustrative nature. Case #4: The scene is indoors with dark, neutral background. The foreground features soft, out-offocus shapes in the lower left and right corners, possibly fabric or shadows, which frame the subject. The background is uniformly dark, providing contrast to the subjects lighter clothing and hair. The overall atmosphere is subdued and formal., Centered composition; the subject occupies the central portion of the frame, filling approximately 60% of the image. Medium close-up shot, straight-on angle at eye level. The camera is positioned close to the subject. Shallow depth of field, with the foreground and background softly blurred, focusing attention on the subjects upper body and hair., Realistic, black-and-white photography with formal portrait style. The image has high-resolution, professional finish, emphasizing texture and contrast., Soft, diffused lighting with cool tone. The light source appears to come from the front and slightly above, illuminating the subjects hair and shirt evenly. The lighting creates gentle shadows and smooth gradient across the background., Realistic, The subject is young Caucasian man with fair skin and proportionate build. His hair is short, dark, and neatly combed to the side, with smooth texture. He is wearing white collared shirt and light-colored tie, suggesting formal attire. His posture is upright, with shoulders squared and body facing forward. The subjects head is oriented straight ahead., young Caucasian man with neatly styled hair is pictured in formal setting, wearing collared shirt and tie, with his upper body visible against dark background. Column #3: Case #1: Realistic, high-resolution wildlife photography, An adult Bengal tiger lies on the ground in grassy outdoor setting, facing the camera with its body partially visible and its gaze directed forward., The subject is an adult Bengal tiger. It has large, muscular build with orange fur and prominent black stripes running along its body and face. The tigers face is broad with pink nose, white fur around the mouth and chin, and white whiskers. Its ears are rounded with black backs and white inner fur. The tigers eyes are yellow-green, and it is gazing directly at the camera. Its body is reclined on the ground, with the front legs extended forward and the head held upright. The tigers expression is calm and alert., The scene is outdoors during daytime. The foreground contains pile of pale beige straw or dried grass, which is in sharp focus. Behind the tiger, there is patch of green grass and horizontal log with rough brown texture. The background consists of more green grass and blurred beige ground, creating natural habitat atmosphere. The foreground is clear, while the background is softly blurred., Natural daylight, soft and diffused, with even illumination across the tigers face and body. The light source appears to be from above and slightly to the front, producing gentle shadows and highlighting the tigers fur texture., Centered composition; the tigers head and upper body occupy the central area of the frame, filling about 60% of the image. Medium close-up shot, eye-level angle. The camera is positioned at the tigers eye height, straight-on. Medium focal length, moderate aperture, shallow depth of field., Realistic."
        },
        {
            "title": "Technical Report",
            "content": "Case #2: The scene is outdoors in garden or natural setting during daytime. The foreground features the sharply focused cosmos flowers and few green stems and buds. The background is blurred, displaying additional purple cosmos flowers and green foliage, with circular bokeh highlights in shades of purple, green, and yellow. The overall color palette includes purples, greens, and warm yellow tones, creating vibrant and lively atmosphere., Realistic, photography, high-resolution macro style with focus on natural detail and color. The image emphasizes clarity and vibrancy, typical of botanical or nature photography., Realistic, Several blooming purple cosmos flowers are clustered together outdoors, with sunlight illuminating their petals and casting soft shadows., The subjects are cosmos flowers (Cosmos bipinnatus) in full bloom. The flowers have broad, delicate petals in soft purple hue with subtle gradients and slightly ruffled edges. The centers are bright yellow-orange, surrounded by ring of small, dark-tipped stamens. The petals are thin and semi-translucent, catching the light. The flowers are at various angles, with the central flower facing forward and slightly upward, while others are angled to the left or right. The stems are slender and green, with some unopened buds visible below the flowers., Centered composition with the main flower group occupying the middle of the frame. Medium close-up shot, straight-on angle at flower height. The main flowers fill about 60% of the frame. Shallow depth of field achieved with wide aperture, rendering the background and some foreground elements softly blurred. Likely taken with DSLR camera and macro or standard lens., Natural sunlight, warm color temperature, soft light quality. The light source comes from the upper left, creating gentle highlights on the petals and subtle shadows on the lower right sides of the flowers. The lighting emphasizes the translucency and texture of the petals. Case #3: The image features close-up of two glass cups filled with Dalgona coffee. The foreground shows cup with whipped coffee foam on top of milk layer. The background includes blurred cup of the same drink and stone-like surface with scattered coffee beans., realistic, blurred, close-up, natural light, frontal view, standard lens, central composition."
        }
    ],
    "affiliations": [
        "Department of Automation, Tsinghua University",
        "Kling Team, Kuaishou Technology"
    ]
}