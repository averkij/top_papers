{
    "paper_title": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution",
    "authors": [
        "Bo Peng",
        "Ruichong Zhang",
        "Daniel Goldstein",
        "Eric Alcaide",
        "Haowen Hou",
        "Janna Lu",
        "William Merrill",
        "Guangyu Song",
        "Kaifeng Tan",
        "Saiteja Utpala",
        "Nathan Wilce",
        "Johan S. Wind",
        "Tianyi Wu",
        "Daniel Wuttke",
        "Christian Zhou-Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present RWKV-7 \"Goose\", a new sequence modeling architecture, along with pre-trained language models that establish a new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces a newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as a relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to $\\mathsf{TC}^0$. To demonstrate RWKV-7's language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset. To foster openness, reproduction, and adoption, we release our models and dataset component listing at https://huggingface.co/RWKV, and our training and inference code at https://github.com/RWKV/RWKV-LM all under the Apache 2.0 License."
        },
        {
            "title": "Start",
            "content": "RWKV-7 \"Goose\" with Expressive Dynamic State Evolution Bo Peng1,2, , Ruichong Zhang3,*, Daniel Goldstein2,4,*, Eric Alcaide2,5, Haowen Hou6, Janna Lu4,7, William Merrill8, Guangyu Song2,9, Kaifeng Tan10, Saiteja Utpala2, Nathan Wilce2,4, Johan S. Wind11, Tianyi Wu12, Daniel Wuttke2,13, and Christian Zhou-Zheng2 1RWKV Project (under Linux Foundation AI & Data), 2EleutherAI, 3Tsinghua University, 4Recursal AI, 5Dalle Molle Institute for Artificial Intelligence USI-SUPSI, 6Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), 7George Mason University, 8New York University, 9Tano Labs, 10Shenzhen University, 11University of Oslo, 12Beijing Normal University, 13Denigma"
        },
        {
            "title": "Abstract",
            "content": "We present RWKV-7 \"Goose\", new sequence modeling architecture, along with pre-trained language models that establish new state-of-the-art in downstream performance at the 3 billion parameter scale on multilingual tasks, and match current SoTA English language performance despite being trained on dramatically fewer tokens than other top 3B models. Nevertheless, RWKV-7 models require only constant memory usage and constant inference time per token. RWKV-7 introduces newly generalized formulation of the delta rule with vector-valued gating and in-context learning rates, as well as relaxed value replacement rule. We show that RWKV-7 can perform state tracking and recognize all regular languages, while retaining parallelizability of training. This exceeds the capabilities of Transformers under standard complexity conjectures, which are limited to TC0. To demonstrate RWKV-7s language modeling capability, we also present an extended open source 3.1 trillion token multilingual corpus, and train four RWKV-7 models ranging from 0.19 billion to 2.9 billion parameters on this dataset. To foster openness, reproduction, and adoption, we release our models1 and dataset component listing2 on Hugging Face, and our training and inference code3 on GitHub; all under the Apache 2.0 License. 5 2 0 2 8 1 ] . [ 1 6 5 4 4 1 . 3 0 5 2 : r Equal first authorship. Others listed alphabetically. 1Model weights at https://huggingface.co/RWKV 2Dataset components listed at https://huggingface.co/RWKV 3Source code at: https://github.com/RWKV/RWKV-LM"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Background 3 Architecture 4 Method 4.1 Time Mixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 MLP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 RWKV World v3 Dataset 6 Pre-Trained Models 7 Language Modeling Experiments 3 4 5 6 9 9 9 10 7.1 LM Evaluation Harness Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 7.2 Recent Internet Data Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 7.3 Associative Recall . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 7.4 Mechanistic Architecture Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 7.5 Long Context Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 7.6 Evaluating State Tracking Using Group Multiplication . . . . . . . . . . . . . . . . . . 15 8 Speed and Memory Usage 9 Multimodal Experiments 10 Conclusions 15 18 19 10.1 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 10.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Author Contributions Training Dataset Details Transition Matrix Eigenvalues and Stability Expressivity of RWKV-7 30 32 33 D.1 Warmup: Expressivity Beyond TC0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 D.2 Main Result: RWKV-7 Can Recognize Any Regular Language . . . . . . . . . . . . . . . 34 D.3 Detailed Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Additional Architecture Discussion",
            "content": "G Pseudocode For RWKV-7 PyTorch code For Naive WKV7 Kernel (Forward and Backward)"
        },
        {
            "title": "M Initial Token Sensitivity",
            "content": "2 38 41 44 45 48 51 52 52 Scalars LS FD DD GE Name RWKVRetNet RWKV-5 Mamba RWKV-6 & GLA HGRN-2 Mamba-2 TTT Longhorn Gated DeltaNet Titans Generalized Rule RWKV-7 (ours) kt + ekt kt State Evolution t 1 + ekt vt ; = = 1 = wS 1 + t = 1diag(w) + t = 1 exp((w = 1diag(wt ) + t = 1diag(wt ) + t = wt 1 + t = 1 at (S 1, kt , vt ) = 1 (I aT k2 = wt 1(I at kT = (1 αt )M 1 + t = wt 1 at (M 1, kt , vt ) bt ) + t = 1(diag(wt ) zT (at ˆκt )) + t = 1(diag(wt ) ˆκT ) + (at )T kt kt ) + at kt kt (1 wt ) kt kt 1) exp(A)) + (wt vt )T kt wt wt , at wt , at kt Table 1: Recent RNN architectures used for language modeling. LS (Large State): matrix-valued states, or state size at least 4 times larger than the model dimension. FD (Flexible Decay): the dimension of the decay term or wt is not smaller than the model dimension. DD (Dynamic Dependence): the decay term wt is function over the input xt . GE (Generalized Eigenvalue): evolution matrix admits eigenvalues outside of the interval [0, 1]. Shown with mini batch size 1 for simplicity."
        },
        {
            "title": "1 Introduction",
            "content": "Autoregressive Transformers (Vaswani et al., 2023) have recently dominated sequence modeling tasks, enjoying excellent in-context processing and highly parallelizable training due to their use of softmax attention. However, softmax attention comes with quadratic computational complexity and memory usage with respect to sequence length due to its linearly growing key-value cache. For short sequences, much of this cost can be covered by modern GPU parallelism techniques, but Transformer inference becomes increasingly costly as sequence lengths grow. As result, significant research has been conducted into the design of recurrent neural network (RNN) architectures with compressive states that afford linear computational complexity and constant memory usage, while still allowing highly parallel training. Linear attention variants (Katharopoulos et al., 2020b; Sun et al., 2023; Peng et al., 2024; Yang et al., 2023a) and State Space Models (Gu & Dao, 2023) are two of the most commonly proposed replacements satisfying these requirements. These architectures have grown more advanced, and recently many such proposals have involved some form of the delta rule, as embodied by parallelized DeltaNet (Schlag et al., 2021; Yang et al., 2024c). Such models have achieved impressive downstream performance results: ever since RWKV-4 (Peng et al., 2023), RNN models have shown increasing potential to rival Transformers given the same model size and training compute, while dramatically reducing inference costs. We present new architecture, RWKV-7 \"Goose\", that generalizes the delta rule for use in sequence modeling. First, we add vector-valued state gating mechanism, enhancing expressivity and providing implicit positional encoding. Second, we expand the in-context learning rate from scalar to become vector-valued, allowing the model to selectively replace state data on channelwise basis. Third, we decouple the keys at which the delta rule removes from and adds to the state. Finally, we layer these innovations on top of modified RWKV-6 architecture, inheriting important features such as token-shift, bonus, and ReLU 2 feedforward network. We also introduce an expanded 3.1 trillion token RWKV World v3 corpus designed to provide excellent English, code, and multilingual capabilities. We use this architecture and corpus to train new state-of-the-art open-source language models, upgraded from pre-existing RWKV-5/RWKV-6 checkpoints. 3 Our main contributions are: The RWKV-7 \"Goose\" architecture, which dramatically improves downstream benchmark performance over RWKV-6 and demonstrates state-of-the-art multilingual performance at 3B scale and near SoTA English language performance, despite being trained on many fewer tokens than the top models in its class. The RWKV World v3 public dataset, comprised of 3.1 trillion tokens of publicly available multilingual data. Public release of four pre-trained RWKV-7 World v3 language models, ranging from 0.19 to 2.9 billion parameters trained on 1.6 to 5.6 trillion tokens. Public release of three pre-trained RWKV-7 Pile language models, using the GPT-NeoX tokenizer (Black et al., 2022), ranging from 0.17 to 1.47 billion parameters, useful for comparative study with other architectures. Proofs that the generalized delta rule employed in RWKV-7 can solve problems outside of TC0 under the widely held complexity conjecture that TC0 = NC1. This includes solving an S5 state tracking problem known to be in NC1 using only single layer, and recognizing all regular languages using only constant number of layers. method for upgrading the RWKV architecture without pre-training from scratch, producing increasingly competitive trained models at reduced computational expense. Larger datasets and RWKV-7 models are under active preparation and construction and will be released under the Apache 2 license whenever practical."
        },
        {
            "title": "2 Background",
            "content": "Linear attentions major advantage over softmax attention is that it can be formulated as RNN with constant running time per token and constant memory usage (Katharopoulos et al., 2020a), while softmax attention takes O(N ) time per token and O(N ) memory with regard to sequence length. Despite this dramatic efficiency improvement, linear attention has its own significant drawbacks (Schlag et al., 2021; Han et al., 2024; Fan et al., 2025). One such issue is that linear attention numerically adds to the fixed-size state at every time-step: older state contents are never removed, only reduced by becoming smaller proportion of the numerically increasing state. Due to limitations on the state size, eventually such system must mix values together and muddy the outputs retrieved for given key (Schlag et al., 2021; Yang et al., 2024b). Modern linear attention architectures like RWKV-6 (Peng et al., 2024), RetNet (Sun et al., 2023), Gated Linear Attention (Yang et al., 2023a), and Mamba 2 (Dao & Gu, 2024) use per time-step decay to remove some portion of such older values from the state in data-dependent manner. However, decay is blunt tool that cannot remove only the values stored at specific keys. Delta Rule. DeltaNet (Schlag et al., 2021) sidesteps the problem of numerically increasing state by partially replacing the value stored at the current key with the same amount of new value, allowing the model to both take away old memories and add new ones on per-key basis. It reformulates the state update as an explicit online learning problem where the goal is to retrieve the correct value as output for given key as input. DeltaNet was the first to apply the foundational Error Correcting Delta Rule (Widrow et al., 1960) to key-value compressive states, akin to those stored in the RNN formulation of linear attention. This update rule is equivalent to single step of stochastic gradient descent, training the state at test time to output the desired values for the keys as inputs using loss = 1 k, leading to recurrent 2 update formula of = 1(I βkT kt . The ideas behind this internal state update can be traced back to fast weights (Schmidhuber, 1992) and Hebbian learning (Hebb, 1949). (St kt vt )2 and gradient kt ) + βv = Sk S There has been significant recent interest in improvements to DeltaNet, in order to bring its efficiency and downstream performance in line with Transformers while still capturing the speed and memory benefits of Linear Attention. Parallelizing DeltaNet (Yang et al., 2024c) showed that DeltaNet used diagonal plus low-rank (DPLR) state evolution like S4 (Gu et al., 2022), and could be parallelized across the time dimension, creating path to efficiently train such models. Our work further extends that parallelization to cover the generalized delta rule formulation introduced herein, as well as the specific formula of RWKV-7. 4 Concurrent Work. Concurrent work with our own has focused on architectural improvements beyond DeltaNet while still using the delta rule or variations thereof. Longhorn (Liu et al., 2024) employs an update rule that approximates closed-form solution to globally optimal update objective, applied on an otherwise unchanged Mamba architecture. Gated Delta Networks (Yang et al., 2024a) applies gating to the DeltaNet state, essentially multiplying the transition matrix by data-dependent scalar per head. This combines the DeltaNet update rule with the scalar decay found in some modern RNNs like RetNet and Mamba-2. The delta rule gradient descent formula kt diag(at )(cid:162)+ with dynamic weight decay wt and learning rate at becomes St = St 1 kt diag(at ). (cid:161)diag(wt ) TTT (Test-Time Training) (Sun et al., 2024) and Titans (Behrouz et al., 2024) also both apply scalar decay, but eschew per-step gradient descent update rules in favor of batched multi-timestep approach. Titans also adds momentum to the otherwise classical SGD update applied to the state. Another concurrent work with our own, Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues (Grazzi et al., 2024), has demonstrated the potential for increased expressiveness that comes from allowing the state transition matrix to contain negative eigenvalues. We show result significantly beyond this, proving that RWKV-7 and our generalized delta rule can recognize all regular languages using only small constant number of layers."
        },
        {
            "title": "3 Architecture",
            "content": "t bt ) + Unlike the other work described above, RWKV-7 generalizes the delta update rule into an extended formula St = St 1(diag(wt ) + zT kt to increase expressivity (see Table 1). This is still diagonal plus rank one update rule, which admits efficient forms of parallelization (Yang et al., 2024c). Here, wt is more expressive data-dependent vector-valued decay, unlike the scalar decays featured in the other works previously described. Our use of zt and bt in this extended formula permits flexible approach to state update, while retaining the important reduction in non-SRAM memory bandwidth usage that comes from using small data-dependent vectors instead of large matrices. One example of this flexibility is the ability to use different removal key than replacement key. This extended delta rule is flexible enough that there may be other useful formulations that fit within it beyond the formulas we have chosen for calculating zt and bt in RWKV-7. The original delta rule allowed fixed scalar fraction of value to be replaced from the state via its in-context learning rate parameter. RWKV-7s extended delta rule instead replaces data-dependent vector-valued amounts of the state, allowing each key channel in the state to vary independently. We parameterize zt = ˆκt and bt = ˆκt at where at has elements in (0, 1). This keeps the update stable (see Appendix C), while maintaining increased expressivity. We demonstrate the improved performance of these design choices via ablations in Appendix K. Previous research (Merrill et al., 2024) pointed out that Transformers and RNNs with diagonal transition matrix could only represent functions in TC0. RWKV-7, however, has non-diagonal and input-dependent transition matrix, allowing it to represent more complex functions than its predecessors. In fact, we demonstrate that RWKV-7 possesses expressive power surpassing that of TC0 under standard complexity conjectures and can recognize all regular languages. One new component of this power, not present in the original delta rule, is the ability to represent the \"copy\" state transition (Lemma 3). This is key element in our proof that RWKV-7 can recognize all regular languages with constant number of layers. See Appendix for proof and details. We replace the main RWKV-6 (Peng et al., 2024) diagonal transition matrix with our extended delta rule and make several other changes to the RWKV architecture, observing significant modeling improvements. These include updates to the channel mixing module and the token shift module. We remove the data dependency of token-shift and the receptance gating of channel mixing, both of which contribute to faster training and inference. We increase the use of low-rank projections to generate more of our intermediate calculations, striking balance between the total number of model parameters, training and inference speed, and downstream performance. 4For now, we choose to allow only part of the range of possible negative eigenvalues in our pre-trained large language models due to experimentally observed training instabilities. 5 Figure 1 presents the overall architecture of RWKV-7. Please refer to Appendix for more details. Figure 1: RWKV-7s overall architecture."
        },
        {
            "title": "4 Method",
            "content": "In this section, we use to denote the model dimension. Bold capital letters represent trainable matrices, and vectors without subscript are trainable parameters. The first subscript denotes sequence position and second subscript denotes layer index, where necessary. We use the convention that all vectors are row vectors unless explicitly transposed, so all matrices operate on the right side, therefore aT is an outer product and abT is an inner one. We use the square subscript to denote placeholder for variable names and use the (cid:81) sign for cumulative matrix multiplication. See Appendix for pseudocode implementation of these formulas."
        },
        {
            "title": "4.1 Time Mixing",
            "content": "Weight Preparation Along the lines of (Peng et al., 2024), we introduce the following notation templates for common operators in the model, using the square subscript to denote variable: lerp(a, b, x) = + (b a) x, loramlp( , x, bias) = (x A)B + (λ if bias else 0), (1) (2) Unless explicitly stated, all vectors appearing in this section are dimension D. We extend the use of low-rank MLP (a 2-layer MLP with small hidden dimension compared to input and output), abbreviated as loramlp, to implement data dependency using minimal parameters. 6 The replacement key k, value v, decay w, removal key κ, in-context learning rate a, receptance , and rwkv gate parameters are computed as follows (outputs annotated with ): , bias=True)), = lerp(xt , xt 1, µ) {r , k, v, , a, }, at = sigmoid(loramlpa(Identy, kt = xk , κt = kt ξ, kt = kt lerp(1, at , α), νt = sigmoid(loramlpν(Identity, t ,l , bias=True)), = W , (cid:40)v ,0, lerp(v vt = ,0, ,l , νt ), dt = loramlpd (tanh, xd wt = exp(e rt = xr , = loramlpg (sigmoid, 0.5sigmoid(dt )), layerl = 0 layerl 1 , , bias=True), , bias=False) token shifted inputs in-context learning rate key precursor removal key replacement key value residual gate value precursor (3) (4) (5) (6) (7) (8) (9) value (10) decay precursor decay receptance rwkv gate (11) (12) (13) (14) ξ is learned parameter representing the removal key multiplier, which transforms the original key into version to be removed from the state. In practice, ξ lies in range of approximately [5.3, 9.4]. α is learned parameter representing the replacement rate booster, which adjusts the amount added back to the state after the transition matrix is applied. Unlike , and which are the main carriers of information, , , ν and act like gates which control the amount of information allowed to pass. For comprehensive statistics of ξ, α and biases of dt observed in the released RWKV-7 model, including extremum values, mean measurements, and distribution trends, see Appendix L. For the computation of improve training speed. , we removed data dependency of linear interpolation from RWKV-6 to We adapted the idea of Value Residual Learning Zhou et al. (2024) for the computation of vt , which has shown to improve the final language modeling loss. νt represents the value residual mix, which interpolates between the layer zero and current layer value precursors: vt ,0 and vt ,l . 0.5), 1) in We also updated the formula for computation of wt , restricting all entries in (exp(e favor of smaller condition number for diag(wt ), which maintains better training stability, and was beneficial to accuracy of the backward pass. The kt in the formula can be regarded as \"normalized key\", design to ensure that the state of contains columns of O(1) size. Normally, we expect kt = kt (1 wt ), as employed in RWKV-6c (see Appendix F), so that t rows are linear interpolations between t 1 and kt controlled by wt . However, to further enhance expressivity, we decide to decouple wt and at . We further decouple at from the amount actually added to the state, allowing the replacement rate booster α to interpolate the amount added between the normal in-context learning rate and 1.0. Importantly, all of these modifications operate on per-channel basis. The numerical range of RWKV-7s entries are generally stable as in RWKV-6c, unlike RWKV-6, where entries of the states can accumulate to thousands (see Appendix for state visualization). The Weighted Key Value State Evolution After weight preparation, we reshape (r , w, k, v, κ, a)t , splitting them to heads, with each head sized D/h. We always assume that is factor of and heads are equally split. All operations in this section are shown per-head. Before mixing in the time dimension, κt is normalized per head: ˆκt = κt /κt 2 7 (15) The (Weighted Key Value) is multi-headed matrix-valued state of fast weights that undergoes dynamic evolution. The evolution of is crucial for encoding context information by learning at test time to map keys to values. We start by defining the WKV time mixing as the recurrence relation 0 = 0, t = t 1 (cid:161)diag(wt ) ˆκT Compared to RWKV-5 and RWKV-6, the in this paper is transposed to ensure consistency with RWKV-7s code. (at ˆκt )(cid:162) + kt (17) (16) Figure 2: simple illustration of the update mechanism of single head of RWKV-7s state. Note that the actual state size is 64 64 per head, not 4 4. The t attention calculation can alternatively be written in parallel manner: t = (cid:195) (cid:89) (cid:88) =1 =i +1 (cid:179) diag(w ) ˆκT (a ˆκ ) (cid:33) (cid:180) ki R(D/h)(D/h) (18) The recurrent transition design has parallels with Schlag et al. (2021), but crucially the transition matrix Gt = diag(wt ) ˆκT (at ˆκt ) = (cid:181) ˆκT ( at wt (cid:182) ˆκt ) diag(wt ) (cid:161)I 2 ˆκT ˆκt (cid:162) diag(wt ) (19) is no longer Householder matrix but scaled approximation of it, as κt = at κt . This mimics wt Householder matrix but with expanded dynamics, while still having all eigenvalues in stable range of [1, 1] and allows the network to decay information in all subspaces if necessary. It contrasts with the case of Householder-like matrix with learning rate (I av v), [0, 1], as used in Schlag et al. (2021); Yang et al. (2024c) where all eigenvalues are one except for the last one corresponding to 1 a. Given these properties, we refer to wt as \"in-context weight decay\" and to at as \"in-context learning rate\" (ICLR). The RWKV-7 transition matrix, therefore, allows for both dynamic state evolution and approximation to forget gate at the same time. See Appendix for the details on the eigenvalue of the transition matrix, and when the transition matrix is guaranteed to be stable. The original delta rule in Schlag et al. (2021) allows partial or full removal of pre-existing values from the state at each time-step, with the amount removed being equal to the scalar a. Our formulation extends this ability by making vector, allowing for different removal amount per state column. WKV Bonus and Output All operations in this section are shown per-head unless otherwise specified. Receptance, which acts like the query found in transformers, is applied to the WKV state, and the result is normalized. An added bonus, the amount of which is weighted by ρ, allows the model to place extra attention on the current shifted input token without requiring it to store that token in the state. ut = (cid:161)rt (ρ kt )T (cid:162) vt pt = LayerNorm(rt T ) + ut (bonus) (attention result) Finally, the heads are recombined via reshaping so that pt RD , gated, and transformed into the output as follows: ot = (g pt )W RD (20) 8 4.2 MLP The MLP module of RWKV-7 is no longer identical to the Channel Mixing module of previous RWKV-4,5,6 architectures (Peng et al., 2024). We remove the gating matrix , making it twolayer MLP. In compensation for the removed gating parameters to satisfy the equi-parameter condition, we set the hidden dimension to be 4 times the size of model dimension. o"
        },
        {
            "title": "5 RWKV World v3 Dataset",
            "content": "= lerp(x = ReLU(k 1, µ , )2W RD )W R4D (21) (22) We train our models on the new RWKV World v3 Dataset, new multilingual 3.119 trillion token dataset drawn from wide variety of publicly available data sources. This dataset aims to help close the gap with the amount of data used to train modern LLMs, which may consume as many as 15 - 18 trillion tokens (Qwen et al., 2025; Grattafiori et al., 2024). We select the data to approximate the distribution of our previous World datasets, including English, multilingual, and code, while slightly enhancing Chinese novels.We describe the composition of our dataset in Appendix B."
        },
        {
            "title": "6 Pre-Trained Models",
            "content": "We have pre-trained and publicly released seven Apache 2.0 licensed RWKV-7 models: 1. Trained on Pile: RWKV7-Pile of sizes 0.1B, 0.4B, and 1.4B 2. Trained on RWKV World V3: RWKV7-World-3 of sizes 0.1B, 0.4B, 1.5B, and 2.9B See Appendix for detailed configurations. The RWKV-7 Pile models all use the GPT-NeoX-20B tokenizer (Black et al., 2022), and were all trained from scratch on the Pile dataset, which has 332 billion tokens. All RWKV World dataset models use the RWKV World Tokenizer. Due to compute budget constraints, the Goose World 3 0.1B and 0.4B models were trained from pre-existing RWKV-5 World v1 and v2 checkpoints, and the Goose World 3 1.5B and 2.9B models were trained from pre-existing RWKV-6 World v2.1 checkpoints. These checkpoints parameters were then converted to the RWKV-7 format via process described below. Once in the new format, the models are trained on either the additional full 3.1 trillion tokens of the World v3 corpus, or an equally weighted sub-sampling of it. Under this methodology, some documents were seen two or even three times. The World v1, v2, v2.1, and v3 corpora contain 0.6, 1.1, 1.4, and 3.1 trillion tokens, respectively. The amounts of training in each stage at with each successive model architecture and corpus are shown in Table 2. Model World v1 World v2 World v2.1 World v3 Total RWKV7-World3-0.1B RWKV7-World3-0.4B RWKV7-World3-1.5B RWKV7-World3-2.9B 0.6 (RWKV-5) 1.1 (RWKV-5) 1.1 (RWKV-6) 1.1 (RWKV-6) 1.4 (RWKV-6) 1.4 (RWKV-6) 1.0 (RWKV-7) 2.0 (RWKV-7) 3.1 (RWKV-7) 3.1 (RWKV-7) 1.6 3.1 5.6 5. Table 2: Total trillions of tokens trained for all RWKV-7 World 3 models Our model format conversion process involves removing the token-shift low-rank MLPs, rescaling by half the embeddings, wkv receptance, wkv output matrix weights, and Layernorm and Groupnorm bias values. Layernorm and Groupnorm weights are clamped above zero and square 3) uniform rooted. We widen the FFN MLP from 3.5x (in RWKV-6) to 4x and add new small (1 10 initalizations in the new regions, removing the RWKV-6 FFN receptance weights. We widen the 4) uniform initializations in the new regions. time decay Low-rank MLP and add new small (1 10 We replace the gate weights with LoRA obtained through singular value decomposition and rescaling by half. 9 (a) FLOPS vs. Average Benchmark Accuracy (b) Active Parameters vs. Average Benchmark Accuracy Figure 3: Model Comparisons across Multilingual Benchmarks"
        },
        {
            "title": "7 Language Modeling Experiments",
            "content": "7.1 LM Evaluation Harness Benchmarks RWKV-7 models are evaluated on series of common English-focused and multilingual benchmarks using LM Evaluation Harness (Gao et al., 2023) as shown in Tables 3 and 4. We benchmarked RWKV-7 along with several new open models which are state-of-the-art in their parameter count ranges. All numbers are evaluated under fp32 precision with lm-eval v0.4.3 using 0-shot, except for MMLU in which case 5-shot was used. We find that RWKV-7 is generally able to match the English performance of Qwen2.5 (Qwen et al., 2025) with less than one third as many training tokens. Interestingly, we found that RWKV-7 models have shown giant leaps in MMLU performance compared to RWKV-6. We also find that RWKV-7-World models expand upon RWKV-6-World models already strong capabilities on multilingual benchmarks, outperforming SmolLM2 (Allal et al., 2025), Llama-3.2 (Grattafiori et al., 2024), and Qwen-2.5 (Qwen et al., 2025) by significant margin. In Figures 3a and 4a we plot FLOPs used to train several open models versus average accuracy across the same sets of common english and multi-lingual benchmarks. The multilingual evals show very dramatic Pareto improvement versus the transformer models. Also note the similar english-language eval scores, but dramatically lower total FLOPs usage of RWKV7-World models versus other highly trained open transformer models. We theorize that if we were less constrained by compute and were able to train these models from scratch with the same amount of total tokens instead of from pre-trained checkpoints of earlier RWKV versions, the difference would be even more dramatic. Note that we did not plot the Llama 3.2 series of models, as they have no corresponding FLOPs amounts due to having been created via pruning and distillation from larger models."
        },
        {
            "title": "7.2 Recent Internet Data Evaluation",
            "content": "Modern large language models are trained on massive datasets. Despite careful data cleaning, benchmark data leakage remains challenge, compromising the validity of these evaluations. To complement traditional benchmarks, we evaluated RWKV-7 Goose and other leading open-source models using temporally novel internet data, generated after the models training periods; this data could not have appeared in the training sets, removing data leakage concerns. Specifically, we collected new data created after January 2025, including: newly submitted computer science and physics papers on arXiv, newly created Python/C++ open-source repositories on 10 (a) FLOPS vs. Average Benchmark Accuracy (b) Active Parameters vs. Average Benchmark Accuracy Figure 4: Model Comparisons across English-Language Benchmarks Model (Name) Tokens (T) lmb.o acc RWKV5-World1-0.1B SmolLM2-135M RWKV7-World2.8-0.1B RWKV5-World2-0.4B SmolLM2-360M Qwen2.5-0.5B RWKV7-World2.9-0.4B RWKV6-World2.1-1.6B Llama3.2-1B SmolLM2-1.7B Qwen2.5-1.5B RWKV7-World3-1.5B 0.6 2.0 1.6 1.1 4.0 18.0 3.1 2.5 b15.0 11.0 18.0 5.6 38.4 42.9 48.1 54.0 53.8 52.5 58.6 67.4 63.0 67.7 63.0 69. hella piqa arcE arcC acc acc acc acc_n glue WG sciq mmlu acc aacc acc acc 31.9 43.1 42.1 40.9 56.4 52.1 56.8 61.1 63.7 71.5 67.7 70.8 61.4 68.4 67.3 66.5 72.1 70.2 72. 74.4 74.5 77.0 75.8 77.1 44.2 64.4 59.3 54.0 70.4 64.6 68.7 64.3 65.5 77.7 75.5 78.1 19.9 28.1 25.5 24.0 36.5 29.5 31. 31.0 31.3 44.7 41.2 44.5 45.5 49.0 48.1 50.0 50.7 54.7 49.4 51.0 49.7 51.5 65.0 62.4 52.9 53.0 52.7 53.2 59.0 56.4 59. 60.7 60.7 66.1 63.4 68.2 76.3 84.0 86.3 86.9 91.2 93.1 89.7 89.5 91.4 93.3 94.2 94.3 23.1 25.8 25.4 23.8 26.3 47.8 26. 25.1 32.1 50.3 61.0 43.3 RWKV6-World2.1-3B Llama3.2-3B Qwen2.5-3B RWKV7-World3-2.9B 76.4 76.7 78.6 79.7 glue is the average accuracy of 8 subtasks: mnli, mnli_mismatch, mrpc, qnli, qqp, rte, sst2 and wnli Llama3.2-1B and 3B were pruned and distilled from Llama3.1-8B (Grattafiori et al., 2024) 2.5 b15.0 18.0 5.6 35.6 42.2 45.0 48.7 71.2 74.5 77.4 81. 68.4 73.6 73.5 76.4 66.3 69.9 68.5 72.8 56.3 50.7 70.2 61.8 92.2 95.7 96.2 95.0 71.7 70.5 67.1 73.4 28.3 56.5 65.7 55. avg acc 43.7 51.0 50.5 50.4 57.4 57.9 57.1 58.3 59.1 66.6 67.4 67.6 62.9 67.8 71.4 71.5 Table 3: English Focused Benchmarks, including LAMBADA (lmb.o) (Paperno et al., 2016), Hellswag (hella) (Hampel, 1974), PIQA (Bisk et al., 2020), AI2 ARC (arcE, arcC) (Bhakthavatsalam et al., 2021), GLUE (Wang et al., 2018), Winogrande (WG) (Sakaguchi et al., 2021), SciQ (Welbl et al., 2017), MMLU (Hendrycks et al., 2021). GitHub, recently published Wikipedia entries, new fiction on Archive of Our Own (Various, 2025), and recent news articles. Inspired by Delétang et al. (2024); Li et al. (2024b), we used compression rate as our evaluation metric. See Table 5 for details. Remarkably, despite being trained on significantly less data than other top models, RWKV-7 Goose showed competitive performance on this temporally novel data."
        },
        {
            "title": "7.3 Associative Recall",
            "content": "Associative recall (AR) (Arora et al., 2023) evaluates the ability of the model to recall previously encountered information within given context. Research indicates that models capacity for AR can reflect its effectiveness in learning from context (Elhage et al., 2021; Olsson et al., 2022). 11 Model (Name) Tokens (T) lmb.m lmb.m pawsx acc appl acc xcopa acc RWKV5-World1-0.1B SmolLM2-135M RWKV7-0.1B RWKV5-World2-0.4B SmolLM2-360M Qwen2.5-0.5B RWKV7-World3-0.4B RWKV6-World2.1-1.6B Llama3.2-1B SmolLM2-1.7B Qwen2.5-1.5B RWKV7-World3-1.5B 0.6 2.0 1.6 1.1 4.0 18.0 3. 2.5 b15.0 11.0 18.0 5.6 270 1514 114 66 389 108 52 28 52 85 49 25 22.0 18.6 31.6 36.8 25.8 32.9 39. 47.2 39.0 37.1 40.0 48.4 48.6 51.2 46.1 49.5 51.4 52.6 48.7 52.5 53.9 56.5 55.3 54.8 53.0 52.2 53.3 54.0 51.7 54.4 55. 58.1 55.3 53.1 57.4 59.7 xnli acc 36.1 34.9 37.6 38.5 36.0 38.6 40.3 41.4 41.2 38.1 40.6 43.7 xsClz acc xwin acc 51.7 50.6 52.6 54.1 51.2 53.9 55.3 58.2 56.6 54.1 57.7 61.4 59.5 61.7 64.1 65.6 67.8 67.8 72. 76.5 72.2 72.8 75.8 79.8 RWKV6-World2.1-3B Llama3.2-3B Qwen2.5-3B RWKV7-World3-2.9B 51.0 45.9 43.5 52.9 The perplexity is the geometric mean, rather than arithmetic average, across 5 languages Llama3.2-1B and 3B were pruned and distilled from Llama3.1-8B (Grattafiori et al., 2024) 2.5 b15.0 18.0 5.6 53.4 59.9 53.3 58.2 61.3 60.6 59.6 64. 42.7 44.2 38.5 45.4 60.2 58.5 59.0 63.1 21 30 36 18 78.8 79.2 79.8 82.4 avg acc 45.1 44.9 47. 49.8 47.3 50.0 52.0 55.7 53.0 52.0 54.5 58.0 57.9 58.1 55.6 61.1 Table 4: Multilingual Benchmarks, including LAMBADA Multilingual (lmb.m) (Gao et al., 2023), XCOPA (Ponti et al., 2020), XNLI (Conneau et al., 2018),XStoryCloze (xsClz) (Lin et al., 2022), xWinogrande (xwin) (Tikhonov & Ryabinin, 2021). Consequently, AR has become standard benchmark for developing new architectural designs in language models (Fu et al., 2023; Poli et al., 2023; Lutati et al., 2023). However, training for multi-query associative recall (MQAR) is highly unstable and strongly dependent on initialization and hyperparameter settings. We observe significant variability in performance under identical configurations across different studies (Peng et al., 2024; Yang et al., 2024c; Beck et al., 2024). We train two-layer RWKV-7 with MQAR and increased the difficulty by scaling the sequence length 18 to to as long as 2048. We use the RWKV-7 specific initialization, and set the ϵ of AdamW to 110 stabilize learning in later stages. Weight decay of 0.1 is only applied to weight matrices, preventing the degeneration of certain modules (such as weights and biases of LayerNorm). Interestingly, with only WKV size of 8192, RWKV-7 is able to recall 72.93% at the setting of 256 Key-value pairs. This suggests that total of roughly 256 0.7293 log2(number of key tokens number of value tokens) = 186.2 2 log2(4096) = 4480.8 bits of information, is stored in 8192 dimensional state, yielding an information density of 0.547 bits per dimension."
        },
        {
            "title": "7.4 Mechanistic Architecture Design",
            "content": "We evaluate RWKV-7 on the Mechanistic Architecture Design (MAD) benchmark (Poli et al., 2024), suite of synthetic token manipulation tasks designed to probe architectural capabilities in sequence modeling, as shown in Table 7. RWKV-7 achieves the highest average score across all six tasks, outperforming previous architectures. It demonstrates perfect accuracy on In-Context and Noisy Recall tasks, matching DeltaNet while setting new state-of-the-art for Fuzzy Recall. RWKV-7 also shows strong performance in memorization and selective copying, suggesting effective combination of attention-based and recurrent model strengths."
        },
        {
            "title": "7.5 Long Context Experiments",
            "content": "To evaluate the ability of RWKV models to retain information over long sequences, we measured loss versus sequence position (we select tokens in range [L/2 16384, L/2 + 16384) for document length L) on the PG19 test set (Rae et al., 2019) for two types of RWKV7 models and their predeces12 Model Qwen2.5-1.5B RWKV-7 1.5B Llama-3.2-1B SmolLM2-1.7B Index-1.9B stablelm-2-1.6b RWKV-6 1.5B RWKV-5 1.5B mamba2-1.3b MobileLLM-1.5B mamba-1.4b-hf Zamba2-1.2B SmolLM-1.7B MobileLLM-1B RWKV-4 1.5B pythia-1.4b-v0 Falcon3-1B-Base Llama-3.2-3B Qwen2.5-3B RWKV-7 2.9B stablelm-3b-4e1t Minitron-4B-Base recurrentgemma-2b RWKV-6 3B gemma-2-2b mamba2attn-2.7b RWKV-5 3B mamba2-2.7b Zamba2-2.7B mamba-2.8b-hf RWKV-4 3B pythia-2.8b-v0 arXiv CS arXiv Phys. Github Python Github C++ 8.12 8.25 8.37 8.38 8.34 8.58 8.62 8.77 8.74 8.82 8.88 8.57 8.38 9.03 9.34 9.12 8.60 7.78 7.79 7.90 8.15 8.09 8.24 8.27 8.39 8.33 8.42 8.43 8.17 8.57 8.90 8.72 8.65 8.77 8.76 9.04 8.59 9.08 9.00 9.11 8.74 9.29 8.86 9.21 9.02 9.57 9.80 9.20 9. 8.10 8.25 8.34 8.50 8.70 8.52 8.58 8.81 8.29 8.70 8.37 8.70 8.52 9.27 8.73 4.42 5.57 5.18 5.17 5.65 5.54 6.06 6.20 6.32 6.79 6.43 6.91 5.76 7.03 6.54 6.79 6.92 4.15 4.15 5.16 5.28 5.13 5.22 5.66 5.36 5.78 5.78 5.93 6.30 6.03 6.07 6.29 4.40 5.29 5.16 4.94 5.29 5.45 5.80 5.92 5.71 6.29 5.81 7.08 6.55 6.53 6.16 6.15 7.16 4.59 4.12 4.88 4.85 4.74 4.80 5.39 5.01 5.22 5.51 5.34 6.39 5.46 5.67 5.71 AO3 Eng 11.76 10.93 11.69 11.20 11.49 11.42 11.09 11.25 11.63 11.59 11.70 11.39 12.68 11.86 11.33 12.19 13.04 10.90 11.23 10.48 10.89 11.05 11.30 10.67 11.35 11.13 10.83 11.21 10.97 11.31 10.90 11.66 BBC news Wiki Eng average 9.58 9.34 9.34 9.40 9.51 9.24 9.57 9.75 9.74 9.15 9.83 9.38 9.85 9.35 10.00 10.20 10. 8.70 9.15 8.92 8.82 9.08 8.94 9.17 8.90 9.28 9.36 9.37 8.95 9.49 9.57 9.74 9.49 8.97 9.07 9.46 9.23 9.06 9.30 9.50 9.86 9.22 9.97 9.26 9.89 9.43 9.82 10.43 10.75 8.28 8.96 8.47 8.51 8.90 8.88 8.82 9.03 9.26 9.00 9.38 8.74 9.53 9.30 9.82 8.06 8.16 8.23 8.23 8.30 8.34 8.49 8.64 8.68 8.73 8.78 8.83 8.88 8.97 9.00 9.15 9.45 7.57 7.66 7.74 7.86 7.96 7.99 8.08 8.12 8.18 8.23 8.29 8.32 8.41 8.53 8.67 Table 5: Compression rate (unit: %) compared across different language models on various data sources, including arXiv papers, GitHub repositories, AO3 fiction, and news articles created after January 2025. Dim WKV state dim (64, 4) 64 128 256 512 8192 16384 32768 65536 (128, 8) (256, 16) (512, 64) (1024, 128) (2048, 256) 98.43 95.01 72.93 94.97 98.97 Table 6: RWKV-7 MQAR test results. We use (a, b) to denote the sequence length and number of KV pairs respectively. check mark indicates that the model achieves over 99% accuracy. Results are maxed over 3 different learning rate settings. sors trained on either The Pile dataset or World dataset. Despite sharing the same architecture and being pretrained on 4k context windows, models trained on different datasets exhibited different behaviors. The Pile-trained RWKV7 showed more significant loss reduction on long contexts compared to its predecessors, demonstrating effective long-context extrapolation (see Figure 5). Surprisingly, for RWKV7 trained on the World dataset, when processing contexts longer than 10k, the loss began to show an increasing trend (see Figure 6). We speculate this is because the larger dataset and model size created inductive biases that caused overfitting to specific context lengths. Further experiments showed that fine-tuning on long contexts can restore its long context capabilities. To further test RWKV-7 long-context retrieval abilities, we conduct pass-key retrieval evaluation following the approach of (Chen et al., 2024b) and plot the results in Figure 7. In this evaluation, single sentence is repeated multiple times within long context window, with key phrase 13 Model Compress Fuzzy Recall In-Context Memorize Noisy Recall Recall Selective Copy RWKV-7 Transformer Multihead Hyena DeltaNet Mamba Hyena GLA 44.5 51.6 44.8 42.2 52.7 45.2 38.8 43.2 29.8 14.4 35.7 6.7 7.9 6.9 100 94.1 99.0 100 90.4 81.7 80. Results for comparison models from Yang et al. (2024c) 89.1 85.2 89.4 52.8 89.5 89.5 63.3 100 86.8 98.6 100 90.1 78.8 81.6 98. 99.6 93.0 100 86.3 93.1 88.6 Avg 79.3 74.5 73.2 71.8 69.3 66.0 60.0 Table 7: Results on the MAD benchmark Figure 5: PG19 loss versus sequence position for RWKV and Mamba models trained on The Pile datasets. embedded at different positions. RWKV7-World3-1.5B achieves perfect accuracy up to context length of 19600 tokens but exhibits degradation beyond 20600 tokens. The larger RWKV7-World32.9B extends perfect retrieval up to 35000 tokens, highlighting the benefits of scaling. However, performance begins to degrade beyond this point. To explore potential improvements, we fine-tuned RWKV7-World3-1.5B and RWKV7-World3-2.9B on packed training sequences of length 128k tokens from specially constructed dataset, which leads to further improvements in retrieval accuracy. With this fine-tuning, RWKV-7 (1.5B) reliably retrieves key phrases up to 29k tokens, and degradation is observed only around 40k tokens. RWKV-7 (2.9B) reliably retrieves the pass key up to 30k tokens, and degrades around 50k tokens. Our context length extension dataset is comprised of both public and custom sources listed in Table 8. We employed document length-based weighting scheme to prioritize longer contexts during training. To approximate document lengths of 128,000 tokens, we used character counts of 512,000. Documents of less than 32,768 characters were assigned weight of 1.0, while longer documents were assigned linearly increasing weights between 2.0 and 3.0, with cap of 3.0 beyond 512,000. This method increases the inclusion of longer documents to bolster the models handling of extended contexts while retaining shorter documents for diversity. 14 Figure 6: PG19 loss versus sequence position for RWKV7 models and predecessors trained on the World dataset. Dataset Type Amount dclm-baseline-1.0 fineweb-edu fineweb codeparrot/github-code arXiv-CC0-v0.5 SuperWikiNEXT-32B public domain books the-stack (filtered) Public Public Public Public Custom Custom Custom Custom 25% 15% 5% 10% 10% 10% 15% 10% Table 8: Context Length Extension Dataset Components 7.6 Evaluating State Tracking Using Group Multiplication We adopt the experimental setting from Merrill et al. (2024) to evaluate the state-tracking capabilities of RWKV7 in comparison to Transformer, Mamba, S4, and classical RNN models. Given sequence g0, g1, g2, . . . , gn drawn from A5, A4Z5, or Z60, each step is labeled with the cumulative product of the first elements. We plot the minimum number of layers required to achieve over 95% validation accuracy on group multiplication tasks, as function of sequence length and group structure. The results are shown in Figure 8. Our findings indicate that RWKV-7 exhibits stronger state-tracking capabilities than Transformers, Mamba, and S4, though slightly weaker than classical RNNs. Figure 8 also aligns with our theory from Appendix D.2, which predicts that RWKV-7 can perform state tracking and recognize any regular language with constant number of layers. RWKV-7 is not more expressive than classical RNNs, because classical RNNs can recognize any regular language in single layer. Classical RNNs, while being theoretically expressive, typically suffer from gradient vanishing and memorization problems. (Zucchet & Orvieto, 2024)"
        },
        {
            "title": "8 Speed and Memory Usage",
            "content": "We compare the training speed and memory usage of the RWKV-7 attention-like kernel with the RWKV-6 kernel and Flash Attention v3 (Shah et al., 2024). The \"RWKV-7\" kernel accelerates bfloat16 matrix multiplications with modern CUDA instructions. We also include the \"RWKV-7 15 (a) 1.5B (b) 3B (c) 1.5B extended (d) 3B extended Figure 7: RWKV7-World3 pass-key retrieval evaluation fp32\" kernel, which is simpler and performs all its internal calculations using float32. Although the bfloat16 kernel is faster, to maximize precision, the RWKV-7 fp32 kernel was used to train the RWKV-7 World models. Our CUDA kernels are tuned for head dimension 64, as used in the RWKV-7-World models. The kernels still perform well for head dimension 128, but their efficiency drops off at larger head dimensions. There exist other RWKV-7 implementations which focus on head dimensions greater than 128. key example is the Flash Linear Attention library (Yang & Zhang, 2024), which offers Triton-based implementation designed for these larger configurations. Speed In Figure 9, we time the forward + backward pass of each kernel for batch size 8, head dimension 64 and model dimension 4096 (64 wkv heads) on an H100 SXM GPU, for varying sequence lengths. Although Flash Attention v3 is heavily optimized for the H100 GPU, it scales quadratically with sequence length, while the RWKV models scale linearly. This makes the RWKV models faster than attention for large sequence lengths. Furthermore, the optimized RWKV-7 kernel is about three times faster than the official RWKV-6 kernel. 16 Figure 8: Minimum number of layers (lower is better) required to attain > 95% validation accuracy on group multiplication problems by sequence length and group. Figure 9: Time vs. Sequence Length (H100) The forward pass of RWKV-7 is about twice as fast as the backward pass. For inference, the forward pass does not need to store the wkv state, making it faster. For example, for sequence length 16k, the forward pass without storing state takes 7.9 ms, while the forward pass with storing state takes 11.2 ms, the backward pass takes 22.5 ms, and the Flash Attention v3 forward pass takes 33.9 ms. Memory The peak training memory usages of the tested models are well described by the formulas derived below. For example, the runs in Figure 9 required peak memory within 2% of the estimates. In the tested kernels, the memory required per stored variable (e.g. q, k, or in attention) is batch size model dimension sequence length 2 bytes for bfloat16. For sequence length 1024, this is 64MB per variable. To calculate memory usage, we may use Flash Attention v3: 10 variables, RWKV-6: 10 variables, RWKV-7: 18 variables, RWKV-7 fp32: 24 variables. 17 Flash Attention v3 requires 4 variables q, k, and output for the forward pass, and the corresponding 4 gradients. Finally, the backward pass uses temporary variable to accumulate the gradient of in float32, yielding 2 variables worth of addition memory, for total of 4+4+2 = 10. RWKV-6 requires 5 variables r, w, k, v, and output for the forward pass and also the corresponding 5 gradients for the backward pass, for total of 10. RWKV-7 uses 7 variables in the forward pass (r, w, k, v, ˆκ, ˆκa, and output), and the corresponding 7 gradients. Additionally, it stores the wkv state every 16 timesteps. At head size 64, the state contributes the equivalent of 4 variables, for total of 18. \"RWKV-7 fp32\" has the same 14 forward variables and gradients, but uses more memory to store the states in float32, for total of 24 variable equivalents. Memory usage is constant for single token inference and follows the formulas above, minus the gradients and state storage. Pre-fill can easily be accomplished in chunked manner, with memory usage growing linearly with regard to chunk size. This allows an easy trade-off for fast parallelized pre-fill, with user selectable maximum memory usage."
        },
        {
            "title": "9 Multimodal Experiments",
            "content": "In this section, we explore the capabilities of Goose when extended to handle multimodal tasks, where the model processes and integrates textual inputs with inputs from different domain. Figure 10: The architecture of VisualRWKV-7. The input image is processed by three vision encoders, and the obtained features are concatenated. Afterward, they are projected through an MLP with context gating to align with the dimensions of the RWKV-7 block. Finally, the image features are concatenated with the text embeddings and fed into the RWKV-7 LLM. RWKV for Image Understanding To demonstrate the modeling capabilities of RWKV-7, we constructed VisualRWKV-7 (Figure10), visual language model based on the RWKV-7 block, to evaluate the image understanding capabilities of RWKV-7. VisualRWKV-6 (Hou et al., 2024) used the CLIP encoder, which focused on processing low-resolution images and achieved good results. VisualRWKV-7 replaces the CLIP encoder with SigLIP and DINO visual encoders, and introduced new high-resolution SAM vision encoder, which enhances the models supported resolution to 1024 1024. The experimental results of VisualRWKV-7 are shown in Table 9. The vision encoders used in both VisualRWKV-7 and VisualRWKV-6 are identical, and the training data remains consistent, aligned with the training data of LLaVA-1.5. The first stage consists of 558k alignment data, while the second stage includes 665k SFT data. VisualRWKV-7 0.1B and 0.4B outperform VisualRWKV-6 1.6B on the in-domain benchmarks VQAv2 and GQA and rapidly approach VisualRWKV-6 1.6B on two other benchmarks. The experimental results are highly compelling. With only 1/4 of the parameters (1.6B vs. 0.4B), VisualRWKV7 surpasses VisualRWKV-6 on the VQAv2 and GQA benchmarks, demonstrating the powerful modeling capabilities of RWKV-7. Method Vision Encoder LLM VQA SQA TQA GQA VisualRWKV-6 VisualRWKV-6 SigLIP+DINOv2+SAM RWKV6-1.6B SigLIP+DINOv2+SAM RWKV6-3.1B VisualRWKV-7 VisualRWKV-7 VisualRWKV-7 VisualRWKV-7 SigLIP+DINOv2+SAM RWKV7-0.1B SigLIP+DINOv2+SAM RWKV7-0.4B SigLIP+DINOv2+SAM RWKV7-1.5B SigLIP+DINOv2+SAM RWKV7-2.9B 73.6 79.1 75.2 77.9 79.8 80. 57.0 62.9 50.6 55.0 59.7 63.4 48.7 52.7 37.9 41.1 49.5 58.0 58.2 61.0 59.9 62.3 63.2 63. Table 9: comparison of VisualRWKV-7 to other Visual Language Models across 4 distinct benchmarks. We evaluate these models on benchmarks: GQA(Hudson & Manning, 2019), SQA(Lu et al., 2022), TQA(Singh et al., 2019) and VQA(Li et al., 2023b). On the out-of-domain benchmark SQA, VisualRWKV-7 2.9B also outperforms VisualRWKV-6 3.1B, indicating that VisualRWKV-7 possesses strong generalization ability. In the TextQA (TQA) benchmark, which assesses models associative recall, VisualRWKV-7 2.9B achieves 5.3-point improvement over VisualRWKV-6 3.1B, further proving its superior associative recall capabilities."
        },
        {
            "title": "10 Conclusions",
            "content": "We introduced RWKV-7, novel RNN architecture that pushes the boundaries of recurrent neural networks to new heights. RWKV-7 achieves state-of-the-art performance for its size across wide range of benchmarks, demonstrating its potential to rival even highly optimized models such as Qwen2.5 despite being trained on many fewer tokens. As an RNN, RWKV-7 maintains high parameter efficiency, linear time complexity, and constant memory usage, offering compelling alternative to traditional Transformer-based architectures. 10.1 Limitations Despite its strengths, the RWKV-7 architecture and models face certain limitations yet to be mitigated in future work. Numerical Precision. We observed that some operators, particularly the WKV7 kernel, are sensitive to the numerical precision of the implementation. This highlights the need for careful handling of numerical precision during model deployment. We also observed differences in training dynamics when using different kernels, which implies that the correct handling of precision while calculating and applying state updates is of utmost importance in this architecture. Lack of Instruction Tuning and Alignment. All RWKV-7 models presented in this work are pretrained base models and have not undergone the phase of Supervised Fine-Tuning (SFT) for instruction following nor alignment with human preferences (RLHF). Future efforts should focus on incorporating these capabilities to enhance the models usability in real-world applications. Prompt Sensitivity. We found that the absence of the special token <endoftext> results in degraded performance of RWKV-7 models, e.g. inability to remember the first token of the input. See Appendix for details. Compute Resources. Due to computational budget constraints, our training was limited on at most 12 8 = 96 Nvidia H800 GPUs. This falls short of the resources required for recent large-scale training efforts, such as DeepSeek-V3 (DeepSeek-AI et al., 2025). Additionally, we are forced to continue training from pre-existing checkpoints of earlier RWKV architectures and therefore re-use some parts of our dataset. This may limit the capabilities of our models versus pre-training from scratch. Scaling up RWKV-7 to larger sizes and datasets will require additional computational resources. 19 10.2 Future Work In addition to training larger RWKV-7 models with more tokens in the future, we also aim to explore several promising directions to further enhance the architecture and its capabilities. Speedup Techniques. variety of speed optimization techniques were highlighted in the technical report of DeepSeek-V3 (DeepSeek-AI et al., 2025), including Dual Pipelining Mechanism, Mixture-of-Experts, Multi-Token Prediction, and FP8 Training. We are aware that many of these techniques are orthogonal to RWKV-7s architectural optimizations, therefore could be integrated to further accelerate training in later RWKV models. However, RWKV-7, like its predecessors, has been trained completely without pipeline parallelism. We also noticed that there is room for speed optimization of RWKV-7 kernels and operators. We will explore both kernel-level optimizations and distributed training strategies in the future. Incorporating Chain-of-Thought Reasoning. We believe that RWKV-7, as linear RNN, is wellsuited for efficient Chain-of-Thought reasoning (Wei et al., 2022). However, this capability has been barely explored due to the lack of suitable reinforcement learning pipelines. In future work, we plan to incorporate deep thinking abilities into RWKV-7, enabling it to excel in tasks requiring multi-step logical reasoning and complex problem-solving. Acknowledgments We extend our gratitude to Shenzhen Yuanshi Intelligent Co. Ltd. and Shanghai Yuanwo Intelligent Co. Ltd. for providing computational resources and their dedication to promoting and commercializing RWKV. We thank Featherless AI for their extensive experimentation with the RWKV architecture and their contributions to this paper. We are grateful to the members of the RWKV and EleutherAI Discord communities for their collaborative efforts in extending the applicability of RWKV to diverse domains. We extend special thank you to Stella Biderman, Songlin Yang and Yu Zhang."
        },
        {
            "title": "References",
            "content": "Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models, 2023. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4WnqRR915j. David A. Barrington. Bounded-width polynomial-size branching programs recognize exactly those languages in nc1. Journal of Computer and System Sciences, 38(1):150164, 1989. URL https: //www.sciencedirect.com/science/article/pii/0022000089900378. Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory, 2024. URL https://arxiv.org/abs/2405.04517. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time, 2024. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Cosmopedia, February 2024a. URL https://huggingface.co/datasets/HuggingF aceTB/cosmopedia. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, July 2024b. URL https://huggingface.co/datasets/HuggingFac eTB/smollm-corpus. Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question answering? try arc-da, the direct-answer ai2 reasoning challenge. arXiv preprint arXiv:2102.03315, 2021. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPTNeoX-20B: An open-source autoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gallé (eds.), Proceedings of BigScience Episode #5 Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.bigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9. Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, and Maosong Sun. Stuffed mamba: State collapse and state capacity of rnn-based long-context modeling, 2024a. URL https://arxiv.org/abs/2410.07145. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models, 2024b. URL https://arxiv. org/abs/2309.12307. 21 Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. Xnli: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 24752485, 2018. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL https://arxiv.org/abs/2405.210 60. Dao AI Lab. Causal conv1d. https://github.com/Dao-AILab/causal-conv1d, 2023. Accessed: 2025-02-26. DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. URL https://arxiv.org/abs/2412.19437. Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression, 2024. URL https://arxiv.or g/abs/2309.10668. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. Qihang Fan, Huaibo Huang, and Ran He. Breaking the low-rank dilemma of linear attention, 2025. URL https://arxiv.org/abs/2411.07635. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. 22 Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Riccardo Grazzi, Julien Siems, Jörg K. H. Franke, Arber Zela, Frank Hutter, and Massimiliano Pontil. Unlocking state-tracking in linear rnns through negative eigenvalues, 2024. URL https: //arxiv.org/abs/2411.12537. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022. Frank Hampel. The influence curve and its role in robust estimation. Journal of the american statistical association, 69(346):383393, 1974. Dongchen Han, Yifan Pu, Zhuofan Xia, Yizeng Han, Xuran Pan, Xiu Li, Jiwen Lu, Shiji Song, and Gao Huang. Bridging the divide: Reconsidering softmax and linear attention, 2024. URL https://arxiv.org/abs/2412.06590. Donald Olding Hebb. The organization of behavior: neuropsychological theory. John Wiley & Sons, 1949. 24 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https://ar xiv.org/abs/2009.03300. Roger Horn and Charles Johnson. Matrix analysis. Cambridge university press, 2012. Haowen Hou, Peigen Zeng, Fei Ma, and Fei Richard Yu. Visualrwkv: Exploring recurrent neural networks for visual language models. ArXiv, abs/2406.13362, 2024. URL https://api.se manticscholar.org/CorpusID:270620870. Drew A. Hudson and Christopher D. Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 66936702, 2019. URL https://api.semanticscholar. org/CorpusID:152282269. Jean Kaddour. The minipile challenge for data-efficient language models, 2023. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020a. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. Proceedings of the 37 th International Conference on Machine Learning, 2020b. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. Datacomp-lm: In search of the next generation of training sets for language models, 2024a. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023a. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji rong Wen. Evaluating object hallucination in large vision-language models. In Conference on Empirical Methods in Natural Language Processing, 2023b. URL https://api.semanticscholar.org/CorpusID: 258740697. Yucheng Li, Yunhao Guo, Frank Guerin, and Chenghua Lin. Evaluating large language models for generalization and robustness via data compression, 2024b. URL https://arxiv.org/ab s/2402.00861. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 90199052, 2022. Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. Longhorn: State space models are amortized online learners, 2024. URL https://arxiv.org/abs/2407.142 07. Anton Lozhkov, Loubna Ben Allal, Leandro von Werra, and Thomas Wolf. Fineweb-edu: the finest collection of educational content, 2024. URL https://huggingface.co/datasets/Hu ggingFaceFW/fineweb-edu. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. ArXiv, abs/2209.09513, 2022. URL https://api.semantic scholar.org/CorpusID:252383606. Shahar Lutati, Itamar Zimerman, and Lior Wolf. Focus your attention (with adaptive iir filters), 2023. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. ArXiv, abs/1812.06162, 2018. URL https://api.semanticschola r.org/CorpusID:56262183. William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. ArXiv, abs/2404.08819, 2024. URL https://api.semanticscholar.org/CorpusID: 269149086. Igor Molybog, Peter Albert, Moya Chen, Zachary DeVito, David Esiobu, Naman Goyal, Punit Singh Koura, Sharan Narang, Andrew Poulton, Ruan Silva, Binh Tang, Diana Liskovich, Puxin Xu, Yuchen Zhang, Melanie Kambadur, Stephen Roller, and Susan Zhang. theory on adam instability in large-scale machine learning, 2023. URL https://arxiv.org/abs/2304.0 9871. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads, 2022. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset: Word prediction requiring broad discourse context. arXiv preprint arXiv:1606.06031, 2016. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wozniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1404814077, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.936. URL https://aclanthology.org/2023.find ings-emnlp.936. Bo Peng, Daniel Goldstein, Quentin Gregory Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Kranthi Kiran GV, Haowen Hou, Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Jian Zhu, and Rui-Jie Zhu. Eagle and finch: RWKV with matrix-valued states and dynamic recurrence. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=soz1SEiPeq. 26 Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Ré. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 2804328078. PMLR, 2023. Michael Poli, Armin Thomas, Eric Nguyen, Pragaash Ponnusamy, Björn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher Ré, Ce Zhang, and Stefano Massaroli. Mechanistic design and scaling of hybrid architectures, 2024. URL https: //arxiv.org/abs/2403.17844. Edoardo Maria Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: multilingual dataset for causal commonsense reasoning. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 23622376, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlpmain.185. URL https: //aclanthology.org/2020.emnlp-main.185. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. URL https://arxiv.org/abs/ 1911.05507. Mark Rudelson and Roman Vershynin. Sampling from large matrices: An approach through geometric functional analysis. J. ACM, 54(4):21es, July 2007. ISSN 0004-5411. doi: 10.1145/12 55443.1255449. URL https://doi.org/10.1145/1255443.1255449. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers, 2021. URL https://arxiv.org/abs/2102.11174. Jürgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131139, 1992. doi: 10.1162/neco.1992.4.1.131. John Schultz, Jakub Adamek, Matej Jusup, Marc Lanctot, Michael Kaisers, Sarah Perrin, Daniel Hennes, Jeremy Shar, Cannada Lewis, Anian Ruoss, Tom Zahavy, Petar Veliˇckovic, Laurel Prince, Satinder Singh, Eric Malmi, and Nenad Tomašev. Mastering board games by external and internal planning with language models, 2024. URL https://arxiv.org/abs/2412.1 2119. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 10, 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read, 2019. Samuel L. Smith, Pieter-Jan Kindermans, and Quoc V. Le. Dont decay the learning rate, increase the batch size. In International Conference on Learning Representations, 2018. URL https: //openreview.net/forum?id=B1Yy1BxCZ. Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https: //www.cerebras.net/blog/slimpajama-a-627b-token-cleaned-and-ded uplicated-version-of-redpajama, June 2023. URL https://huggingface.co /datasets/cerebras/SlimPajama-627B. 27 Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint, 2024. Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, and Carlos Guestrin. Learning to (learn at test time): Rnns with expressive hidden states, 2024. Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: successor to transformer for large language models, 2023. Alexey Tikhonov and Max Ryabinin. Its all in the heads: Using attention heads as baseline for cross-lingual transfer in commonsense reasoning. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 35343546, 2021. Oguzhan Topsakal, Colby Jacob Edell, and Jackson Bailey Harper. Evaluating large language models with grid-based game competitions: An extensible llm benchmark and leaderboard, 2024. URL https://arxiv.org/abs/2407.07796. Various. Archive of our own, 2025. URL https://archiveofourown.org. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355, 2018. Jason Wei, Maarten Bosma, Vincent Y. Zhao abd Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum ?id=_VjQlMeSB_J. Johannes Welbl, Nelson Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pp. 94106, 2017. Bernard Widrow, Marcian Hoff, et al. Adaptive switching circuits. In IRE WESCON convention record, volume 4, pp. 96104. New York, 1960. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing, 2024. URL https://arxiv.org/abs/2406.08464. Takuto Yamana. Egaroucid, January 2025. URL https://www.egaroucid.nyanyan.dev/. Songlin Yang and Yu Zhang. Fla: triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/fla-org/fla sh-linear-attention. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2023a. Songlin Yang, Jan Kautz, and Ali Hatamizadeh. Gated delta networks: Improving mamba2 with delta rule, 2024a. 28 Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2024b. URL https://arxiv.org/abs/23 12.06635. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length, 2024c. URL https://arxiv.org/abs/2406.0 6484. Xiaocong Yang, James Y. Huang, Wenxuan Zhou, and Muhao Chen. Parameter-efficient tuning with special token adaptation, 2023b. URL https://arxiv.org/abs/2210.04382. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024. Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web. Advances in Neural Information Processing Systems, 2024. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew Chi-Chih Yao. Training and evaluating language models with template-based data generation. arXiv preprint arXiv:2411.18104, 2024. Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, and Zhenzhong Lan. Value residual learning for alleviating attention concentration in transformers, 2024. URL https://arxiv.org/abs/2410 .17897. Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, and Wenhu Chen. Structlm: Towards building generalist models for structured knowledge grounding, 2024. Nicolas Zucchet and Antonio Orvieto. Recurrent neural networks: vanishing and exploding gradients are not the end of the story, 2024. URL https://arxiv.org/abs/2405.21064."
        },
        {
            "title": "A Author Contributions",
            "content": "Bo Peng Original RWKV-7 ideas, original code, performance optimizations, original experiments, dataset composition, and trained models from 0.1B to 2.9B. Ruichong Zhang Personnel organization, wrote Sections 3, 4, 7.1, 7.3, 10 and Appendices C, E, H, J, K, M, Figures 1, 2, 11, 12, 15, 16 and Tables 1, 3, 4, 6, 13, 14, 15, 16, 17, 19. Additional contributions on implementing RWKV-7 for Flash-linear-attention and converting RWKV-7 models on HuggingFace. Daniel Goldstein Manuscript organization, initial draft sections 1, 2, 3, 4, 5, FLOPS portion of 7.1, figures 3a and 4a, and appendices B, F, G. Proofreading and revisions of full manuscript. Oversaw and chose experiments for appendix and for pass-key in subsection 7.5. Assistance with appendix revisions and ideas. Developed and tested initial RWKV-7 Hugging Face model code. Eric Alcaide Section 3, validation of CUDA kernels for scalable training, and manuscript proofreading. Haowen Hou Wrote Section 9, covering architectural design, coding, model training, experimental evaluation, as well as figure (Figure 10), table  (Table 9)  , and text writing. Janna Lu Needle-in-haystack evaluations for Section 7.5, experiments for figure 4a, figure 3a, and table 17. William Merrill Developmental discussion, proofreading and revisions for appendix D. Guangyu Song Section 7.4. Experiments for 7.4 Kaifeng Tan Section 7.2, Figures 5, 6. Appendix on board game modeling with Othello/Reversi, including training data design, model implementation, experiments, and result analysis. Saiteja Utpala Section 7.6 and state tracking experiments for Figure 8. Nathan Wilce Extended context-length dataset development and extended-length model training. Description of extended context-length dataset in 7.5 Johan S. Wind Main author of RWKV-7 CUDA kernel implementations. Experiments for Figure 9. Section 8 and Appendix D. Contributions to Appendix C. Tianyi Wu Appendix and Appendix L. Contributions to section 4 and Appendix (proofreading and revisions). Daniel Wuttke Constructed an itemized table of v2-v3 world datasets. Proofreading and revision of the manuscript. Contributed to Abstract, Sections 1 and B. Contributed to Tables 10, 11, 12, and 15 Performed evaluations of RWKV-7 world models and reference base models (Table 3 and 4). Christian Zhou-Zheng Experiments and writing for Appendix and Table 18. Contributions to Sections 1 and 2. Proofreading and revisions of full manuscript."
        },
        {
            "title": "B Training Dataset Details",
            "content": "The RWKV World v3 corpus builds upon the RWKV World v2 corpus (Peng et al., 2024) in two steps that we describe separately here for the purposes of reproducibility for the Goose training runs: World v2.1 adds the entries listed in Table 10 to sum to total of approximately 1.4 trillion RWKV World Tokenizer tokens. World v3 adds more entries, listed in Table 11, to sum to total of 30 Dataset Domain Dataset Domain slimpajama C4 dolma v1.6 (reddit only)a glaive-code-assistant-v3 m-a-p_Code-Feedback cosmopedia-v0.1 SystemChat-2.0 Tess-v1.5 UltraInteract_sft Web Forums Code Code Synthetic Instruct Instruct Instruct Llama-3-Magpie-Pro-1M-v0.1 Magpie-Pro-MT-300K-v0.1 Magpie-Air-MT-300K-v0.1 Magpie-Qwen2-Pro-1M-v0.1 Magpie-Phi3-Pro-300K-Filteredv1 Magpie-Gemma2-Pro-200KFiltered-v0.1 Align Align Align Align Align Align Table 10: Components added into the RWKV World v2.1 dataset, their source links, and their domains. We added only the reddit datasets from dolma v1.6 DM_math as part of The Pile was in World v2 but missed being mentioned explicitly in (Peng et al., 2024) Dataset REMOVED slimpajama partsa dclm-baseline-10-of-10b ccnews fineweb-edu TemplateGSM open-web-math algebraic-stack Domain Web Web Web Web Edu Math Math Math Dataset StarCoderc python-edu cosmopedia-v0.2 WebInstructSub Buzz-v1.2 SKGInstruct FLAN Domain Code Code Synthetic Forums Instruct Instruct Instruct Table 11: Components added into the RWKV World v3 dataset, their source links, and their domains. We removed the CC and C4 components of SlimPajama from the corpus for World v3 For DCLM-baseline, we include only global-shard_10_of_10 For StarCoder, we now include all datasets, instead of just those datasets with at least 10 stars approximately 3.1 trillion tokens. In the combined corpora, all tokens are given equal weighting unless otherwise noted. SlimPajama StarCoder Cosmopedia Dolma UltraInteract Magpie FineWeb DataComp LM (DCLM) WebInstructSub StructLM TemplateGSM SmolLM Corpus FLAN OpenWebMath Algebraic-Stack Soboleva et al. (2023) Li et al. (2023a) Ben Allal et al. (2024a) Soldaini et al. (2024) Yuan et al. (2024) Xu et al. (2024) Lozhkov et al. (2024) Li et al. (2024a) Yue et al. (2024) Zhuang et al. (2024) Zhang et al. (2024) Ben Allal et al. (2024b) Wei et al. (2021) Paster et al. (2023) Azerbayev et al. (2024) Table 12: RWKV World v3 dataset component citations Most of the component data sources for the RWKV World v3 dataset are used intact, with no upor down-sampling done so that all tokens are given equal weighting. Some sub-sampling is done for over-represented languages within few data sources in the original World v2 corpus. All newly added tokens in v2.1 and v3 are given equal weighting. 31 Category Tokens (B) Web Books Code Science & Wiki Fiction Chat & QA & Instruction Math Law & Government Poetry & Lyrics Total 1945.2 337.2 258.4 222.7 192.6 110.0 32.3 19.0 1. 3119.2 Table 13: RWKV World v3 dataset categories"
        },
        {
            "title": "C Transition Matrix Eigenvalues and Stability",
            "content": "We are interested in all the eigenvalues of the transition matrix At = diag(wt ) ˆκT (at ˆκt ), and when it can be stable. We drop the subscript for statements which hold at all timesteps, to avoid clutter. Theorem 1. Let = diag(w) ˆκT (a ˆκ) Mm(R) be matrix, where all entries of belong to 1/2) = 0.5452 is the clamping lower bound. Also, all entries of are (u, 1), where = exp(e located in (0, 1), and ˆκ is unit row vector. When (0, 1 + u), the following holds: 1. The matrix is similar to symmetric matrix, hence similar to diagonal matrix. 2. All eigenvalues of lie in the interval (1, 1). 3. The matrix admits at most one negative eigenvalue. 4. If further assumed that at is time-independent, then the update formula is guaranteed to be stable, i.e. there exists time-independent constant such that (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) (cid:89) = At (cid:176) (cid:176) (cid:176) (cid:176) (cid:176)2 , where 2 denotes the spectral norm. This hyperparameter in the formula can be regarded as \"Global ICLR Multiplier\". This hyperparameter is set to 1 in the current implementations of RWKV-7 language modeling. Proof. 1. We notice that = diag(w) ˆκT (a ˆκ) = diag(w) ˆκT ˆκdiag(a). The matrix itself is not necessarily symmetric matrix. However, we can use the fact that diag(a) is positive definite, so we can compute its square root. This allows us to rewrite diag(a)1/2 diag(a) 1/2 = diag(a)1/2diag(w)diag(a) 1/2 diag(a)1/2 ˆκT ˆκ diag(a)diag(a) 1/2 = diag(w) (cid:161) ˆκdiag(a)1/2(cid:162)T (cid:161) ˆκdiag(a)1/2(cid:162) , which is symmetric matrix. We denote this matrix by . It has exactly the same eigenvalues as A, since it is formed by similarity transformation of A. 1/2 is symmetric, all eigenvalues of (and hence A) are real and 2. Since = diag(a)1/2 Adiag(a) located on the interval of (cid:183) min ˆs=1 ˆsB ˆsT , max ˆs=1 ˆsB ˆsT (cid:184) . It suffices to show that ˆsB ˆsT (1, 1). This can be proved via direct expansion of by ˆsB ˆsT = ˆsdiag(w) ˆsT ˆs( ˆκdiag(a)1/2)T ( ˆκdiag(a)1/2) ˆsT (cid:175)( ˆκdiag(a)1/2) ˆsT (cid:175) (cid:175) (cid:175) 2 ˆs ˆsT > 1. Similarly, ˆsB ˆsT = ˆsdiag(w) ˆsT ˆs( ˆκdiag(a)1/2)T ( ˆκdiag(a)1/2) ˆsT (cid:175) (cid:175)( ˆκdiag(a)1/2) ˆsT (cid:175) (cid:175) 2 < ˆs ˆsT ˆs ˆsT = 1, which completes this part. 3. Recall = diag(w) (cid:161) ˆκ diag(a)1/2(cid:162)T (cid:161) ˆκ diag(a)1/2(cid:162). Then is congruent with ˆB = diag(w) 1/2B diag(w) 1/2 = uT u, (cid:112) ˆκ diag(a)1/2diag(w) where = Clearly, ˆB has at most one negative eigenvalue with value 1 u2, and all other eigenvalues equal to 1. By Sylvesters law of inertia (Horn & Johnson, 2012), congruency preserves the number of negative eigenvalues. Hence, also has at most one negative eigenvalue. 1/2. 4. We drop the subscript in the time-invariant at . 1/2 is symmetric, the spectral norm of Bt is equal to the largest Since Bt = diag(a)1/2 At diag(a) absolute value for eigenvalues of Bt . We proved previously that the eigenvalues lie in (1, 1), so Bt 2 Furthermore, we have 1. That is, Bt is contraction matrix. (cid:89) =1 At = (cid:89) =t diag(a) 1/2Bt diag(a)1/2 = diag(a) 1/ (cid:33) Bt diag(a)1/2. (cid:195) (cid:89) =1 Then (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) (cid:89) =1 At (cid:176) (cid:176) (cid:176) (cid:176) (cid:176)2 (cid:176) (cid:176)diag(a) min(a) 1/2(cid:176) (cid:176)2 (cid:176) (cid:176) (cid:176) (cid:176) (cid:176) 1/2 1 1. (cid:89) = Bt (cid:176) (cid:176) (cid:176) (cid:176) (cid:176)2 (cid:176) (cid:176)diag(a)1/2(cid:176) (cid:176)2 We can set = min(a) 1/2 < , which is time-independent. While our stability proof only holds for time-independent at , we do not observe any problems with time-varying at in practice. We therefore put greater emphasis on the expressivity of RWKV-7, and include time-varying at . Expressivity of RWKV-7 We show that the RWKV-7 architecture can express NC1-complete state tracking problems that cannot be expressed by transformers or other recurrent architectures like S4 and Mamba. We first show particular NC1-complete problem that can be expressed by RWKV-7 in Section and then generalize the argument to show that any regular language can be recognized by RWKV-7 in Section D.2. As regular language recognition can be understood to formalize finite state tracking problems, this suggests an expressivity advantage of RWKV-7 on state-tracking problems. 33 D.1 Warmup: Expressivity Beyond TC0 Recall that the RWKV-7 wkv state is, at each token, updated by multiplication with At = diag(wt ) ˆκT (at ˆκt ), where = 1. In the following, we will consider = 2, and show that this yields expressivity beyond TC0 (unless TC0 = NC1). TC0 is the complexity class which includes transformers as well as all non-gated or diagonal SSMs (Merrill et al., 2024; Barrington, 1989). Theorem 2. RWKV-7 can solve problem which is NC1-complete under AC0 reductions. Proof. RWKV-7 can, by Lemma 2, solve the problem of tracking swaps on five elements. This problem is NC1-complete under AC0 reductions (Merrill et al., 2024). Lemma 1. The RWKV-7 transition matrix can represent an arbitrary swap matrix, where swap matrix is an identity matrix with two of its rows swapped. Proof. Given indices and y, let wt = 1, = 2, (cid:112) ˆκt = (ex )/ 2, and at = 1. Here ei denotes the vector with 1 at position and 0 elsewhere. Then the transition matrix becomes At = eT matrix which swaps indices and y. ex eT + eT + eT ex , which is the permutation Lemma 2 (RWKV-7 can track swaps on 5 elements). Let sequence of swaps on 5 elements be encoded in the format #[x1 y1][x2 yn] . . . , where # is special beginning-of-sequence token, and [xi yi ] is token which denotes swap between elements xi and yi . Then there exists one-layer RWKV-7 model which outputs 1 if the sequence of swaps encode the identity permutation, and outputs 0 otherwise. Proof. Let the RWKV-7 model have 5 wkv heads of head dimension 5. The embedding weights and \"weight preparation\" part of the time mixing layer are set such that the following two properties hold: Firstly, when the model sees the special beginning-of-sequence token, the th wkv head receives = 1, = 2, ˆκ = ei , = 0, and = = ei . Here ei denotes the vector with 1 at position and 0 elsewhere. This sets the state of the th wkv head to wkv = eT Secondly, when token represents swap between tokens 1 < 5. In this case, using Lemma 1, all wkv heads receive ei , which represents state . = 1, = 2, ˆκ = (ex )/ 2, = 1, and = = 0. (cid:112) This changes the state to if it was x, or if it was y, or keeps it unchanged otherwise. To calculate the output, the th wkv head checks whether it represents state , by applying receptance = ei . Finally, the MLP layer combines these outputs to check if all 5 heads agree with the identity permutation, and the output head outputs 1 if this is true, and 0 otherwise. D.2 Main Result: RWKV-7 Can Recognize Any Regular Language Moreover, we are able to demonstrate that RWKV-7 has the capability to recognize any regular language. The regular languages are precisely those which can be recognized by deterministic finite automaton (DFA). It is therefore sufficient to show that RWKV-7 can simulate any DFA. We define DFAs in the usual way: 34 Definition 1. Classically, DFA is tuple = (Q, Σ, δ, q0, ) where is finite set of states, Σ is finite vocabulary of tokens, δσ : is transition function for each token σ Σ, q0 is the initial state, and is set of accepting states. Equivalently, the DFAs computation on Σ δσ can be represented by boolean matrix Mσ {0, 1} initial state q0 can be represented as one-hot vector α {0, 1} can be represented as multi-hot vector ω {0, 1} For given string w1 wT , the DFA computes α Mw1 can be represented by matrix computations. Each , where Mw (i , ) = 1 iff δw (q ) = qi . The , and the set of accepting states MwT QQ ω Q . We say that if and only if this expression evaluates to 1. For convenience, we will let = Q. Having defined regular languages, we are in position to show our main result: Theorem 3. For any regular language, there exists 4-layer RWKV-7 model which recognizes it. To prove Theorem 3, it is sufficient to evaluate to simulate the DFA, i.e. evaluate the function αMw1 . . . MwT ωT . Furthermore, each DFA transition matrix can be factored into elementary transition matrices, that the wkv heads can directly implement. However, wkv head can only implement one elementary transition matrix per input token, but naively expanding each DFA transition gives more elementary transition matrices than tokens. Our construction therefore uses the initial RWKV-7 layers to compress blocks of DFA transitions, such that each block yields single, combined DFA transition matrix. This allows the wkv heads in the final layer to spend tokens on implementing each combined DFA transition matrix. Remaining DFA transitions are handled by large lookup table based on the last (up to) 2n tokens. Our constructions RWKV-7 layers can be summarized as follows: 1. Check whether the current token is first, and find the parity of the current position. These preliminary quantities will be used later by the construction. 2. Calculate the current position modulo 2n. 3. Memorize the last 2n tokens, and apply lookup tables based on these tokens and the current position modulo 2n. 4. Accumulate elementary transition matrices, and evaluate (1) based on the current wkv state and lookup from layer 3. In aggregate, this allows us to compute representation of α Mw1 means we can simulate any DFA (and thus recognize any regular language). MwT ω at token , which D.3 Detailed Proof of Theorem 3 The proof for Theorem 3 will be broken up across many different lemmas. single RWKV-7 wkv state transition cannot directly implement an arbitrary DFA transition. However, DFA transition matrices can be decomposed into product product of elementary transition matrices that can be directly simulated by wkv state transitions (cf. Lemma: 1): Lemma 3. Let be DFA transition matrix. I.e., has shape n, and contains single 1 in each column. Then can be factored into product of elementary transition matrices G1, . . . ,Gn. Specifically, = G1G2 . . .Gn, where each of G1, . . . ,Gn has one of the following forms: 1. Identity matrix. 2. Swap matrix y; an identity matrix with rows and swapped. 3. Copy matrix y; an identity matrix with column replaced by copy of column x. Proof. We will greedily build from the identity matrix by right-multiplying elementary transition matrices. Right-multiplying by the identity matrix does nothing, right-multiplying with swap matrix swaps columns and y, and right-multiplying with copy matrix replaces column by copy of column y. We use to denote the current partial product of elementary transition matrices. Initially, is the identity matrix, and the goal is to apply transitions to make = . We proceed greedily in three stages. 1. Find column of which differs from column of , but which matches different column of . If no such position exists, proceed to the next stage. Otherwise, right-multiply by swap matrix which swaps columns and Note that and differed in columns and swap. before the swap, but match in column after the . 2. Find column where and differ. In no such column exists, proceed to the next stage. of , necessarily different Otherwise, the previous stage has ensured that there exists column from c, which contains column identical to column of . Right-multiply by copy matrix which replaces column of with column and differed in column before the move, while they agree afterwards. of . Note that 3. Right-multiply by identity matrices until is the product of elementary transition matrices. Initially, and differed in at most columns. Each subsequent right-multiplication by an elementary transition matrix reduced the number of differing columns by at least one. Thus, stages 1 and 2 required at most multiplications. Recall that the RWKV-7 wkv state is at token index updated by multiplication with At = diag(wt ) ˆκT (at ˆκt ), with = 1. To simplify the presentation of the core idea, we will instead present construction with = 2, and then show how to remove this assumption in Section D.3. Lemma 4. For any elementary transition matrix (in the sense of Lemma 3), there exist ndimensional vectors ˆκ and a, where ˆκ = 1, has elements in {0, 1}, and where = 2 and = 1. = diag(w) ˆκT (a ˆκ), Proof. We use ei to denote the n-dimensional vector with 1 at position and 0 elsewhere. 1. Identity matrix: Select any length one vector ˆκ, for example ˆκ = e1, and = 0. 2. Swap matrix; an identity matrix with rows and swapped: Select ˆκ = (ex )/ 3. Copy matrix; an identity matrix with column replaced by copy of column x: Select ˆκ = 2 and = 1. (cid:112) (cid:112) (ex )/ 2 and = ex . Lemma 5. There is 1-layer RWKV-7 which outputs whether the current position is first, and whether the current position is even or odd. Proof. RWKV-7 performs token-shift operation before the wkv heads are reached. This token shift takes as input the last token, and can therefore detect whether previous token exists. The first layers wkv state can track position parity by selecting k1 = v1 = e1 for the first position = 1, and subsequently letting = 2, wt = at = 1, ˆκt = e1 and kt = vt = 0 for 2. This leads to wkv1 = eT 1 e1 for odd and wkvt = eT Receptance = e1 can then be used to read out the sign of the wkv state, which encodes the current positions parity. 1 e1 and subsequently wkvt = wkvt 1(I 2eT 1 e1) for 2. Then wkvt = eT 1 e1 for even . 36 Lemma 6. For any positive integer n, there is 2-layer RWKV-7 which outputs the position modulo 2n. Proof. We use Lemma 5 for the first layer, which tells the second layer whether the current position is first, and the parity of the current position. At the first position, set k1 = v1 = e1, such that wkv1 = eT 1 e1. For all subsequent positions 2, set = 2, wt = at = 1 and kt = vt = 0. Furthermore, set ˆκt = e1 for even and ˆκt = cos(π/n)e1 + ˆκT ). Note that for even 2, the sin(π/n)e2 at odd . Then wkvT = eT ˆκt )(I 2 ˆκT matrix (I 2 ˆκT (cid:40) +1 ˆκt +1) rotates the first two coordinates by an angle 2π/n. Thus, 2 ˆκ2) . . . (I 2 ˆκT 1 e1(I 2 ˆκT wkvt = 1 (cos(π(t 1)/n)e1 + sin(π(t 1)/n)e2) , eT 1 ( cos(π(t 2)/n)e1 + sin(π(t 2)/n)e2) , eT if is odd if is even . The wkv heads are immediately followed by group normalization, which discards information about magnitudes. We therefore use 2n wkv heads of the type above, where the kth head applies receptance = cos(πk/n)e1 + sin(πk/n)e2. The signs of these readouts, along with the parity from the first layer, can then be combined in the subsequent MLP layer to deduce the current position module 2n. Lemma 7. Consider lookup table Ξ which takes as key the current position modulo 2n and the 2n most recent tokens (padded with special token for before-sequence tokens). There is 3-layer RWKV-7 which produces outputs matching Ξ. Proof. We use Lemma 6 for the first two layers, such that if the current position is , we know 1 2n such that modulo 2n. Recall that the wkv state is initialized to all zeros. Call the current token wt . We apply the wkv state update wt . t ) + eT wkvt = wkvt 1(I eT 1, kt = ˆκt = and = ewt . . In words, the This can be achieved by selecting 1, wt = 1, at = 1 state update replaces the th column of the wkv state with ewt . Hence, the wkv state stores the last 2n tokens. We make such wkv heads, where the th wkv head applies receptance = ei . This reads out the full state, which contains the last 2n tokens. The model also knows the position modulo 2n, , from the second layer. The state and are fed into the subsequent MLP layer, which performs the lookup into Ξ. Removing the assumption = 2 Some of our constructions use = 2, while the actual model uses = 1. However, since the transition matrix is At = diag(wt ) ˆκT (at ˆκt ), halving both and wt simply causes At to be halved. This causes the wkv state to halve in magnitude at each token. However, since the wkv heads are immediately followed by group normalizations, the magnitude of the wkv state does not affect subsequent calculations. Additionally, since floating point numbers store separate exponent, this rescaling only requires log-precision. The shrinking state could in principle be mismatched with the scales of vt and kt , but our constructions always satisfy vt = kt = 0 whenever = 1 is required. We now move to completing proof of the main theorem. Proof of Theorem 3. We must demonstrate that the RWKV-7 architecture can recognize strings in some regular language L. Consider string Σ and its membership in L. For any 1 , the current position can be split into = + ˆt , for integers 0 and 1 ˆt n. We view the input sequence as blocks of length n, indexed by . We make the first three layers as in Lemma 7, and we focus on the fourth layer. Consider the wkv state update. At the first token, set v1 = e1 and k1 = α (recall that α was the initial state of the DFA). All subsequent transitions 2 set vt = kt = 0, and use implement elementary 37 α for 1 n. transition matrices from Lemma 4. When = 0 and ˆt 2, apply the identity elementary transition matrix. Then wkvt = eT 1 Next, we describe 1. Consider the product of DFA transitions Ml = Mw(l 1)n+1 . . . Mw(l 1)n+n . This product is also DFA transition matrix (i.e., it has single 1 per column). Hence, Lemma 3 allows us to factor Ml = Gl ,1Gl ,2 . . .Gl ,n, where Gl ,1, . . . ,Gl ,n are elementary transition matrices. Fix one such factorization for each possible DFA transition matrix. At position = + ˆt , the wkv state is right-multiplied by the elementary transition matrix Gl ,ˆt . Since Gl ,ˆt is uniquely defined by the last 2n tokens and position modulo 2n, it can be computed in layer 3 using Lemma 7. With this construction, 1 xt , where xt = αMw1 Mw2 . . . Mw(l 1)n Gl ,1Gl ,2 . . .Gl ,ˆt , where as usual, empty products (such as for 1) evaluate to identity matrices. wkvt = eT Duplicate this construction into wkv heads, where the th applies receptance ei . In combination, the wkv heads read out the whole vector xt . To match the DFA evaluation formula from (1), xt can be multiplied by ωT . = Gl ,ˆt +1 . . .Gl ,n Mwl n+1 Mwl n+2 . . . Mwl n+ˆt Fortunately, this expression is fixed function of the current position modulo 2n and the last 2n tokens, and can hence be found by lookup in layer 3 by Lemma 7. The final MLP can thus output the scalar product xt which is equal to (1). Hence, the model can simulate an arbitrary DFA. Thus, we have established that for any regular language Σ , there exists an RWKV-7 that recognizes L."
        },
        {
            "title": "E Additional Architectural and Training Details",
            "content": "Architecture Diagram We provide comprehensive architecture diagram (Figure 11) in order to help readers thoroughly understand our architecture design. Parameters and Dimensions Throughout this section, we denote by the model dimension, the number of layers, = D/Dh the number of heads, and the vocabulary size. All models are trained with head size Dh = 64, i.e., each time-mixing has = D/64 heads with dimension 64 64. Pile models are trained with = 50304 with the GPT-NeoX 20B tokenizer. World models are trained with = 65536 with RWKV World tokenizer. Model Name RWKV7-World3-0.1B RWKV7-World3-0.4B RWKV7-World3-1.5B RWKV7-World3-2.9B 12 24 24 State Size (WKV + Shift) Parameters 768 1024 2048 2560 589 824 + 18 432 1 572 864 + 49 152 3 145 728 + 98 304 5 242 880 + 163 840 191 034 624 450 767 872 1 527 404 544 2 947 735 Table 14: Released Goose models with parameters and state size. RWKV-7 uses four low-rank MLPs for decay w, value residual v, in-context learning rate and gate respectively. The intermediate dimensions are listed in Table 15. These values are based on our mere speculation of how much information can be passed through. The number of parameters for all RWKV-7 models can be computed by the formula: #(Params) = 2DV + 4D + LD (cid:161)12D + 2 (cid:161)dw + da + dv + dg (cid:162) + 19(cid:162) (2Ddv + D). (23) Where: The weights of the embeddings and head, and the Layernorms beside them, yield 2DV + 4D parameters; 38 Figure 11: The architecture of RWKV-7, drawn in detail. Dimension (D) 768 1024 2048 2560 4096 6144 dw 64 64 96 96 128 128 da 64 64 96 96 128 128 dv 32 32 64 64 96 96 dg 128 128 256 320 480 640 Table 15: Suggested Intermediate Dimensions for the low-rank MLPs for RWKV-7 models The weights of each layer yield (cid:161)12D + 2 (cid:161)dw + da + dv + dg (cid:162) + 19(cid:162) parameters, except for the first layer; The low-rank MLP for the value residual is not present in the first layer, subtracting (2Ddv + D) parameters. Parameter Initializations Proper parameter initialization is crucial for ensuring training stability and achieving optimal performance for language models. RWKV-7 employs carefully designed initialization strategy tailored to its architecture. The detailed initialization scheme is beyond the scope here but can be found in the official code repository. We emphasize that using the recommended initialization is essential for replicating the results in this paper. Deviations from the prescribed initialization may lead to performance degradation. Dataset Loading The dataset used for pretraining consists of 3 119 194 079 123 tokens stored on disk, which are memory-mapped using the mmap mechanism. To ensure diverse and pseudorandom sampling of training sequences, we employ custom data loading strategy based on mathematical function with desirable properties. Specifically, we utilize pseudo-random number generator defined by the function (x) = ax3 over the finite field Z/pZ, where is prime number of the form 3n + 2. This function is chosen because it is bijection (full map) in Z/pZ, ensuring that all possible indices are eventually accessed exactly once within one epoch. For pretraining with sequence length of 4096, the relative address of the k-th sample is determined as: start_address = 4096 (ak3 mod p), end_address = start_address + 4097. Here, is chosen as the largest prime of the form 3n +2 smaller than dataset_size/4096, yielding = 761 521 949. The parameter is set to an integer close to 0.618p that ensures good mixing properties of the generated addresses. This approach guarantees both simple calculation and uniform access to the dataset while maintaining pseudo-randomness. By leveraging the properties of modular arithmetic and cubic mappings, we achieve balance between computational efficiency and data diversity during pretraining. Training Details All RWKV-7 models were trained under bfloat16 format on nodes of 8 18, and Nvidia H800. The AdamW optimizer was configured with β1 = 0.9, β2 = 0.99, ϵ = 1 10 weight decay of 0.1 applied exclusively to linear layers and embedding weights. The choice of such small ϵ value is motivated by the theory proposed by Molybog et al. (2023), which suggests that reducing ϵ can help stabilize training in large-scale models by ensuring that intermediate layers remain in regime of active updates, thus mitigating sudden loss spikes and promoting smoother convergence. The context length for pretraining was 4096 tokens. The base decay rate w0 parameters are placed into special 2x learning rate multiplier grouping. Besides the traditional cosine learning rate decay schedule, we used phased dynamic batch size scaling strategy inspired by the concept of critical batch size proposed by McCandlish et al. (2018) and similar to the approaches in Smith et al. (2018). Our strategy involves progressively increasing the batch size during training, accompanied by corresponding adjustments to the learning rate. 40 Model Phase Nodes RWKV7-World3-0.1B RWKV7-World3-0.4B RWKV7-World3-1.5B RWKV7-World3-2.9B 1 1 1 2 3 1 2 3 4 1 1 2 3 4 6 4 6 7 Batch size 240 4096 240 4096 480 4096 480 4096 672 4096 1152 4096 640 4096 1008 4096 1120 4096 2016 4096 Proposed Initial LR Final Loss 4 4 4 6 10 5 10 6 10 4 4 10 4 4.5 10 4 6.1 10 4 4 10 4 5 10 4 5.4 10 4 8 10 2. 2.2580 1.9970 1.8745 Table 16: Training Schedules and Batch Sizes The detailed training schedules for different model sizes are listed in Table 16. The learning rate undergoes cosine decay schedule from the proposed initial learning rate at the 5 at the end of the beginning of the entire training run to the expected final learning rate of 1 10 entire run, but the implied initial rate varies across phases. This approach not only enhances training efficiency but also utilizes GPU resources economically. After smaller models complete their training, additional GPU resources become available for the later stages of training larger models. This cascading resource allocation ensures that computational power is dynamically reallocated, maximizing hardware utilization and reducing idle time. We observe extremely stable training without any loss spikes in all four runs, indicating that the likelihood of encountering such spikes during the training of very large RWKV-7 model is minimal. See Figure 12 for the resulting learning rates and observed loss curves. Despite the general stability of our loss curves, we did sometimes observe NaN loss across single training step, which we theorize may be due to our use of such an extremely low AdamW ϵ. When this occurs, we rewind the training to the prior checkpoint, clear optimizer states, and continue from that point."
        },
        {
            "title": "F Additional Architecture Discussion",
            "content": "The following is general overview of the RWKV-7 architecture and selected rationale for its design choices: The RWKV-7 Time Mixer begins with feature that has been in RWKV since its inception: token shift. Token shift is variety of 1D short convolution that is intended to allow the model to create induction heads within single layer. At its core, token shift is just linear interpolation between the current and prior tokens on per channel basis, where the amount per channel is learned parameter. Many other modern models (e.g. Mamba) have begun including short convolutions before attention replacements (See also Dao AI Lab (2023)). RWKV-6 introduced an advanced new form of token shift that was data dependent. While we found that this was beneficial in terms of loss decrease per step, we made the judgement call that the improvement in training and inference efficiency was not worthwhile. Therefore, RWKV-7 includes only the simple token shift found in RWKV-4 and RWKV-5. RWKV-7 time mixing follows the overall form of delta rule, applying an SGD-like mechanism to train the state at test time, such that when presented with key it will respond with an appropriately matching value, like those it has been shown previously. This can be conceptualized as simple order of operations: 1) decay the state 2) remove part of the old value located at the current key 3) add part of the new value into the current key. Then, query (we call it receptance) is applied to 41 (a) 0.1B (b) 0.4b (c) 1.5B (d) 2.9B Figure 12: Training loss curve for RWKV-7 World models. Blue line: loss; Red line: smoothed loss; Green line: actual learning rate; Dotted red line: learning rate schedule. the state in order to request the value located at specific key. After that, the remaining operations normalize the returned values to keep numerical sizing consistent, apply gating, and recombine the heads to obtain the output. In SGD terms, the decay found in models like RWKV-7 and Gated DeltaNet can be thought of as similar to weight decay. RWKV models since RWKV-4 have employed some form of vector-valued decay on the state in place of positional encoding. We continue this tradition with RWKV-7. While vector valued decay is harder to compute efficiently than its scalar valued equivalent, it brings significant improvement in loss per step. Many other models, like Mamba-2, make the choice to maximize training efficiency and simplify their kernels by using scalar decay, purposely trading quality for speed. We are forced to limit the maximum decay that can occur in given time-step both in order to maintain training stability and to assist the creation of fast but numerically stable kernel implementations. We have many varieties of kernel available today, and are still working on new designs with enhanced efficiency and accuracy. Please see Parallelizing Linear Transformers with the Delta Rule over Sequence Length (Yang et al., 2024c) and its accompanying blog post series for insightful details on many components of such algorithms. We hope to be able to reduce the decay limit further in future revisions. 0.5σ(dt )) formula for decay. This lower bound on the decay multiplier is expressed by the exp(e It is rearrangement of an original source formula exp(exp(0.5 softplus(dt ))). The outer exp(exp(x)) in the original is nearly flipped version of sigmoid, but with better gradient. The 0.5 softplus(dt ) is way of limiting the inputs to this sigmoid-like function to be less than -0.5, which results in the final decay being greater than 0.545. This means that decay can remove at most 45.5% of the pre-existing values in the wkv state from one timestep to the next. More can be then removed by the delta rule mechanism. 42 The in-context learning rate is usually equivalent to the learning rate in SGD. Ours is bit less restrictive than the traditional delta rule or SGD would allow. We also make this data dependent and extend it to be vector-valued instead of being merely scalar. This is allows each key channel to have its own learning rate, dependent on the current token. Note that in TTT, Gated DeltaNet, and Titans although the in-context learning rates are data dependent, they are only scalar valued. Some of the design of RWKV-7 with regard to the in-context learning rate is theoretically motivated but may not be apparent from the equations. RWKV-6c, later v6 sub-version with no trained models but which worked well experimentally, kept its state somewhat normalized using new design. As mentioned early in this paper and consistent with the observations in Yang et al. (2024b); Chen et al. (2024a), there is fundamental issue with linear attention numerically growing in an unbounded fashion, and the base RWKV-6 revision has this problem. RWKV-6c, however, defeats this issue by ensuring that the amount of key added to the state is never more than the amount removed by decay. It accomplishes this by pre-multiplying the key by one minus the decay before adding it into the wkv state. Early versions of RWKV-7 attempted to use similar mathematical formulations to keep everything normalized. But experimentally we found that the model both did best and was most efficient when we allowed it enough mathematical leeway to make these decisions on its own, rather than enforcing them. So instead of pre-multiplying the key, we give the replacement key latitude in its learning rate via the replacement rate booster. Similarly, the removal key is decoupled from the replacement key. Note that the removal key is normalized. This is important in delta rule variations, because otherwise the outer product of removal key with itself would cause unwanted changes in the amount removed due to the implicit squaring of its length during that process. Another point that may not be clear upon first examination is the importance of RWKV-7 being outside of the TC0 complexity class in which transformers, linear attention, and many varieties of SSMs lie. Consider single layer of the Key Value Cache of transformer. It is appended to upon each new token processed, and can never be changed. RWKV-7, however, can and does edit its state at each subsequent token. This can include simple operations like replacement, which can be viewed as functioning similarly to transformer losing KV Cache entry via some sparsity mechanism e.g. sliding window. But it can also include complex operations like swapping two entries in the state; operations transformer cannot do within its immutable KV Cache. These kinds of operations can be used to enact computations on the state itself, which lends greater computational abilities to RWKV-7 when processing fixed set of inputs. You might think of the RWKV-7 state as being like an internal scratchpad. There are easy problems that simply cannot be solved by transformers on fixed inputs because they lack this ability. One example is that if you give transformer an ordered set of items and list of which ones swap places, it will not be able to tell you which item ends up in which position at the end. Both architectures gain even more power when they are allowed extra outputs (e.g. chain of thought), becoming Turing complete. But this requires costly additional inference-time computation, whereas the RWKV-7 state does not. possible way to interpret the designation of RWKV-7 is training model to learn (how to train an internal model). WKV head can be viewed as linear transformation that takes the input (receptance) and outputs o. Every WKV head can be regarded as linear model that can update its weights as the context progresses. The entries of WKV states are exactly the parameters of these models. After receptance is applied to the WKV state, we normalize the result on per head basis. This has become common across many linear attention and post-linear-attention architectures. It is way of ensuring that change in the numerical size of the state over time does not impact the models ability to use the state. The original formulations of RWKV-4 used denominator in place of normalization to achieve similar effect, but it is costly, harder to code, and uses more memory. The readout part of RWKV-7 differ from RWKV-6 by the addition of the per-head (cid:161)rt (ρ kt )T (cid:162) scalar gating of vt . The trainable parameter ρ resembles the design of \"time-first\" term existing from RWKV-4 to RWKV-6, under the belief that the information of the current token deserves special treatment. The \"time-first\" term was fused inside the WKV kernel in previous architectures 43 of RWKV, but we decide to extract this term out of the kernel to simplify the implementation of RWKV-7. Pseudocode For RWKV1 import torch as th 2 import torch.nn.functional as 3 4 5 def rwkv_timemix(params, x, v0, shift_state, wkv_state): 6 # batch_size, sequence_length, d_model B, T, = x.shape = wkv_state.shape[-1] # head size = // # head count # weight preparation x_shifted shift_state = x[:, -1:, :] = th.cat([shift_state, x[:, :-1, :]], dim=1) x_receptance = th.lerp(x, x_shifted, params.mu_r) = th.lerp(x, x_shifted, params.mu_d) x_decay = th.lerp(x, x_shifted, params.mu_k) x_key = th.lerp(x, x_shifted, params.mu_v) x_value = th.lerp(x, x_shifted, params.mu_a) x_iclr = th.lerp(x, x_shifted, params.mu_g) x_gate receptance = x_receptance @ params.W_receptance # BTC # BTC proto_decay = params.decay_lora(x_decay) # BTC proto_key # BTC proto_value = x_value @ params.W_value = params.gate_lora(x_gate) gate # BTC = params.iclr_lora(x_iclr).sigmoid() # BTC iclr = x_key @ params.W_key # 1st layer: return value to use in later layers, no interpolation if layer_id == 0: v0 = proto_value else: = v0 vn = th.sigmoid(params.v0_mix_amt_lora(x_value)) = th.lerp(proto_value, v0, vn) decay = th.exp(-math.exp(-0.5) * proto_decay.to(th.float).sigmoid()) removal_k = proto_key * params.removal_key_multiplier removal_k = F.normalize(removal_k.view(B,T,H,-1), dim=-1).view(B,T,C) replacement_k = th.lerp(proto_k, proto_k * iclr, params.iclr_mix_amt) # recurrence relation out = torch.empty_like(x).view(B,T,C) for in range(T): # single step of wkv state transition decay_t iclr_t removal_k_t replacement_k_t = replacement_k[:, t].view(B, H, N) v_t = decay[:, t].view(B, H, N) = iclr[:, t].view(B, H, N) = removal_k[:, t].view(B, H, N) = v[:, t].view(B, H, N) wkv_state = wkv_state * decay_t.mT wkv_state = wkv_state - wkv_state @ removal_k_t @ (iclr_t * removal_k_t).mT wkv_state = wkv_state + v_t @ replacement_k_t.mT = wkv_state @ out[:,t] = y.view(B,C) # recombine heads # BHVK @ BHK1 = BHV # normalization 44 7 8 9 11 12 13 14 15 17 18 19 20 21 23 24 25 26 27 29 30 31 32 33 35 36 37 38 39 41 42 43 44 45 47 48 49 50 51 53 54 55 56 57 59 60 61 62 63 65 66 67 71 72 74 75 76 = F.group_norm(y.view(B * T, -1), num_groups=H, params.ln_x.weight, params.ln_x.bias, eps = * 1e-5). view(B, T, -1) # bonus and output bonus = ((r*k*params.bonus_multiplier).sum(dim=-1, keepdim=True) * v) bonus = bonus.view(B,T,C) out = out + bonus out = (out * gate) @ W_output # BTC # recombine heads return out, v0, shift_state, wkv_state 68 69 def rwkv_channelmix(x, shift_state): 70 x_shifted = th.cat([shift_state, x[:, :-1, :]], dim=1) shift_state = x[:, -1:, :] xk = th.lerp(x, x_shifted, params.mu_x) = params.W_k @ xk = params.W_v @ th.relu(k).square() return v, shift_state 77 78 def rwkv_model(params, input_ids, state): = params.embedding(input_ids) 79 = params.layer_norm_pre(x) 80 81 82 83 84 86 87 88 89 90 92 93 94 95 96 98 99 100 101 v0 = None for layer in params.layers: dx, v0, state.timemix_shiftstate, state.timemix_wkvstate = rwkv_timemix( layer.time_mix, layer.layer_norm_timemix(x), v0, state.timemix_shiftstate, state.timemix_wkvstate ) = + dx dx, state.chanmix_shiftstate = rwkv_chanmix( layer.channel_mix, layer.layer_norm_chanmix(x), state.chanmix_shiftstate ) = + dx = params.layer_norm_out(x) logits = params.head(x) return logits, state PyTorch code For Naive WKV7 Kernel (Forward and Backward) 1 2 class WKV7_Kernel(nn.Module): def __init__(self): 3 4 5 6 8 9 10 11 12 super().__init__() def forward(self, r, w, k, v, a, b, state): = r.view(B, T, H, N) = k.view(B, T, H, N) = v.view(B, T, H, N) = a.view(B, T, H, N) = b.view(B, T, H, N) self.state_cache = torch.zeros((B, T+1, H, N, N)) self.state_cache[:, 0, :] = state 45 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 56 58 59 60 61 62 = torch.exp(-torch.exp(w.view(B, T, H, N))) out = torch.zeros((B, T, H, N)) for in range(T): kk = k[:, t, :] rr = r[:, t, :] vv = v[:, t, :] aa = a[:, t, :] bb = b[:, t, :] state = ( state * w[: , t, :, None, :] + torch.einsum('bhik,bhk,bhj->bhij', state, aa, bb) + torch.einsum('bhj,bhi->bhij', kk, vv) ) self.state_cache[:, t+1, :] = state out[:, t, :] = torch.einsum('bhj,bhij->bhi', rr, state) return out, state def backward(self, r, w0, k, v, a, b, gout, gstate): gout = gout.view(B, T, H, N) gr = torch.zeros((B, T, H, N)) gw = torch.zeros((B, T, H, N)) gk = torch.zeros((B, T, H, N)) gv = torch.zeros((B, T, H, N)) ga = torch.zeros((B, T, H, N)) gb = torch.zeros((B, T, H, N)) = torch.exp(-torch.exp(w0.view(B, T, H, N))) for in range(T-1, -1, -1): gr[:, t, :] = torch.matmul( gout[:,t,:,None,:], self.state_cache[:,t+1,:]).squeeze(-2) gstate.add_(torch.matmul( gout[:,t,:,:,None], r[:,t,:,None,:])) gk[:, t, :] = torch.matmul( v[:,t,:,None,:], gstate).squeeze(-2) gv[:, t, :] = torch.matmul( gstate, k[:,t,:,:,None]).squeeze(-1) ga[:, t, :] = torch.einsum( 'bhik,bhj,bhij->bhk', self.state_cache[:, t, :], b[:, t, :], gstate) gb[:, t, :] = torch.einsum( 'bhik,bhk,bhij->bhj', self.state_cache[:, t, :], a[:, t, :], gstate) gw[:, t, :] = torch.einsum( 'bhij,bhij->bhj', self.state_cache[:, t, :], gstate) gstate = torch.einsum( 'bhj,bhij->bhij', w[:, t, :], gstate) + torch.einsum( 'bhk,bhj,bhij->bhik', a[:, t, :], b[:, t, :], gstate) gw = -torch.exp(w0-torch.exp(w0)) * gw return gr, gw, gk, gv, ga, gb, gstate"
        },
        {
            "title": "I Board Game Modeling",
            "content": "Previous research (Schultz et al. (2024); Topsakal et al. (2024)) has shown that board games can serve as potentially effective tools for evaluating models capabilities. As an RNN with powerful state tracking abilities, RWKV-v7 is highly suitable for board game modeling and conducting extended searches directly within context to find better strategies. As an early exploration, we tested RWKV-v7s board game modeling capabilities and its ability to perform in-context search on the game of Othello (Reversi). Data We designed training data that guides the model to predict legal moves, evaluate these moves, and perform Alpha-Beta pruning. Each training sample consists of three components: 46 Input section: Captures the game state, including the current board position, active player, and search parameters (tree depth and width settings). Reasoning section: Varies with search settings: Without search (depth or width = 1): Lists legal moves and their evaluations With search (depth and width > 1): Performs Alpha-Beta pruning to find optimal moves All move evaluations in the reasoning section were generated using the Egaroucid engine (Yamana, 2025). Output section: This contains the final move decision and displays the resulting board position after implementing the move. An example of our training data is shown in sample 1. 1 <input> 2 . . * * . . 3 * . * * . 4 * * * * * 5 * * * * * 6 * * * 7 * * * . 8 * . o . . 9 . . . . . . 10 NEXT 11 MAX_WIDTH-2 12 MAX_DEPTH-2 13 </input> 14 <reasoning> 15 Possible moves and score: g1 -19 h1 -01 b2 -08 h2 -23 b7 -12 g7 -09 16 <stack> 17 Remaining_Depth:2 18 Max_Node Alpha: -in Beta: +in Best: -- Current: h1 -01 Unexplored: b2 -08 19 </stack> 20 => Search next node 21 [Depth limit not reached] 22 <board> 23 . . * * . 24 * . * . 25 * * * * 26 * * * * o 27 * * * 28 * * * . 29 * . o . . 30 . . . . . . 31 </board> 32 NEXT * 33 Possible moves and score: b1 +02 b2 +05 h2 +10 h6 +03 h7 +08 c8 +06 d8 +01 g8 +09 34 [Current player has legal moves] 35 [Internal node - expand] 36 <stack> 37 Remaining_Depth:1 38 Min_Node Alpha: -in Beta: +in Best: -- Current: d8 +01 Unexplored: b1 +02 39 Max_Node Alpha: -in Beta: +in Best: -- Current: h1 -01 Unexplored: b2 -08 40 </stack> 41 => Search next node 42 [Depth limit reached - evaluate all leaves] 43 [Updated stack] 44 <stack> 45 Remaining_Depth:1 46 Min_Node Alpha: -in Beta: +01 Best: d8 Current: -- --- Unexplored: 47 Max_Node Alpha: +01 Beta: +in Best: h1 Current: b2 -08 Unexplored: 48 </stack> 49 => Search next node 50 [Depth limit not reached] 47 51 <board> 52 . . * * . . 53 * o * * . 54 * * * 55 * * * 56 * * * 57 * * * . 58 * . o . . 59 . . . . . . 60 </board> 61 NEXT * 62 Possible moves and score: a1 -07 b1 +13 h6 -01 b7 -08 g7 +08 h7 -02 c8 +01 d8 -03 g8 + 63 [Current player has legal moves] 64 [Internal node - expand] 65 <stack> 66 Remaining_Depth:1 67 Min_Node Alpha: +01 Beta: +in Best: -- Current: b7 -08 Unexplored: a1 -07 68 Max_Node Alpha: +01 Beta: +in Best: h1 Current: b2 -08 Unexplored: 69 </stack> 70 => Search next node 71 [Depth limit reached - evaluate all leaves] 72 [Updated stack] 73 <stack> 74 Remaining_Depth:1 75 Min_Node Alpha: +01 Beta: -08 Best: b7 Current: -- --- Unexplored: 76 Max_Node Alpha: +01 Beta: +in Best: h1 Current: -- --- Unexplored: 77 </stack> 78 [End of search] 79 > Playing h1 80 </reasoning> 81 <output> 82 h1 83 . . * * . 84 * . * . 85 * * * * 86 * * * * o 87 * * * 88 * * * . 89 * . o . . 90 . . . . . . 91 </output> Code Listing 1: Training Data Example Training We trained RWKV-7 models with 9M and 26M parameters respectively on 6 million samples. By tracking the loss across different token types during training (figure 13), we noticed that the model first mastered output formatting, then developed board state tracking capability, and continuously improved its evaluation accuracy throughout training. Evaluation We control the models thinking budget by setting the width and depth of Alpha-beta pruning, and test the win rate against baseline model (depth=1, width=1) under different budgets. As shown in figure 14, by increasing the testing budget, RWKV-7 can effectively search for better strategies, demonstrating positive test-time scaling law on this task."
        },
        {
            "title": "J State Inspections",
            "content": "We inspected the WKV matrix states of RWKV-7 and compared them with those of RWKV-5 and RWKV-6. We analyzed the following aspects: 1. Visualization example of WKV state matrices, to better understand the structure and behavior of the matrices. 48 Figure 13: Reversi Training loss for different token types Figure 14: Reversi Token consumption and win rates under different search configurations 2. Root Mean Square (RMS) of matrix elements, to assess the numerical stability of those WKV matrices. 3. Stable Rank (SR) of the matrices (Rudelson & Vershynin, 2007), defined as the square of (the Frobenius norm divided by the spectral norm): SR(A) := (cid:181) AF (cid:182)2 . This serves as rough measure to the effective amount of information of the states. 49 For this analysis, we selected 10 samples from the validation set of the PG19 dataset (Rae et al., 2019), ensuring that each sample had sequence length of at least 8192 tokens. We tested the 1.5B parameter versions of RWKV-5, RWKV-6, and RWKV-7, plotting on the appearance, stable rank and RMS values of their WKV matrices. (a) RWKV-5 1.5B (b) RWKV-6 1.6B (c) RWKV-7 1.5B Figure 15: Visualization example of RWKVs WKV matrices. We observed that the WKV states of RWKV-7 had significantly smaller RMS values compared to RWKV-5 and RWKV-6. The entries of the WKV matrix in RWKV-7 were consistently of order O(1) (i.e., no outliers, and does not grow over context length), whereas RWKV-5 and RWKV-6 constantly produced outliers on the order of thousands (see Figures 15 and 16). This indicates that RWKV-7 has better numerical stability during training and inference. Interestingly, the stable rank of the WKV matrix in RWKV-7 has shown to be lower than that of RWKV-5 and RWKV-6 for context longer than 32. lower stable rank typically suggests that the matrix contains less information or has more compressed representation. However, this observation appears to contradict the experimental results showing that RWKV-7 performs better on tasks requiring long-term memory. We hypothesize that this contradiction can be explained by RWKV7s enhanced state evolution mechanism, which enables RWKV-7 to achieve stronger information compression and utilization capabilities and allowing it to maintain important information in more compact form. This dual capability likely contributes to both the reduced stable rank and the improved performance on memory-intensive tasks. Figure 16: The global RMS and average stable rank of WKV matrices, plotted over sequence length."
        },
        {
            "title": "K Ablation Experiments",
            "content": "The Pile To demonstrate the architectural advantages of RWKV-7, we conducted ablation experiments by training models of three different sizes168M, 421M, and 1.47B parameterson the full Pile dataset (Gao et al., 2020). Model Tokens (B) lmb.o lmb.o acc ppl hella acc_n piqa acc arcE arcC glue WG acc acc acc acc RWKV4-169M-Pile Pythia-160M Mamba-130M Mamba2-130M RWKV6-173M-Pile RWKV7-168M-Pile RWKV4-430M-Pile Pythia-410M Mamba-370M Mamba2-370M RWKV7-421M-Pile RWKV4-1.5B-Pile Pythia-1.4B Mamba-1.4B Mamba2-1.3B RWKV7-1.47B-Pile 332 300 300 300 332 332 332 300 300 300 332 332 300 300 300 332 29.2 37.3 16.0 16.8 16.0 14.2 13.1 10.8 8.1 8.0 7.2 7.1 6.1 5.0 5.0 4. 33.2 35.4 44.3 43.9 44.5 45.7 45.1 51.6 55.6 55.9 57.9 56.4 61.7 65.0 65.6 67.0 32.2 30.3 35.3 35.3 34.9 36.9 40.8 40.6 46.5 46.9 48.0 52.8 52.0 59.1 59.9 61. 64.8 62.3 64.5 64.9 64.4 65.5 67.7 66.7 69.5 70.5 69.3 72.2 70.8 74.2 73.2 73.6 47.1 43.6 48.0 47.4 48.3 47.9 52.8 51.9 55.0 54.8 56.3 60.7 60.5 65.5 64.2 64. 19.9 19.5 19.7 20.9 19.7 19.7 24.1 21.4 25.0 25.1 23.5 24.9 26.1 29.8 29.9 30.2 47.6 46.5 48.5 45.8 48.9 49.1 49.4 44.1 46.8 48.1 50.3 50.5 47.7 46.2 46.1 48. 51.2 51.3 52.1 52.6 51.9 52.4 52.0 53.3 55.5 55.4 56.4 54.3 57.5 61.4 61.0 64.4 sciq acc avg acc 77.6 75.4 78.2 81.0 80.6 81. 80.7 81.5 84.5 85.3 85.9 85.8 86.6 87.3 89.8 91.1 46.7 45.5 48.8 49.0 49.2 49.8 51.6 51.4 54.8 55.2 56.0 57.2 57.9 61.1 61.2 62.6 Table 17: English Focused Benchmarks, including LAMBADA (lmb.o) (Paperno et al., 2016), Hellaswag (hella) (Zellers et al., 2019), PIQA (Bisk et al., 2020), AI2 ARC (arcE, arcC) (Bhakthavatsalam et al., 2021), GLUE (Wang et al., 2018), Winogrande (WG) (Sakaguchi et al., 2021), SciQ (Welbl et al., 2017). These results highlight the consistent improvements brought by the RWKV-7 architecture over earlier RWKV models, even when trained on the same dataset. As all RWKV models shown were trained under identical configurations and dataset, this underscores the inherent architectural advantages of RWKV-7 over its predecessors. Notably, the performance gap sustains as the model size increases, suggesting that RWKV-7 may scale more effectively than its predecessors. Architecture Choice Ablations We ran series of ablation experiments to validate the various improvements made in RWKV-7 versus some of the more restrictive choices seen in other DeltaNet and post-DeltaNet related work. These improvements were: using vector-valued decay instead of scalar-valued decay; using vector-valued in-context learning rate instead of scalar-valued rate; using different removal κ and replacement keys, instead of the same for both; and 51 adding the bonus term ut , vt , to the output step (Equation bonus). We trained small 6-layer, dmod el = 768 Goose model on the 1.6B token minipile (Kaddour, 2023) dataset at context length 512 and obtained the loss results shown in Table 18. Model Training Loss Validation Loss Goose Goose, scalar decay Goose, scalar in-context learning rate Goose, same removal/replacement keys Goose, no bonus term 2.834 2.873 2.843 2.840 2.841 2.541 2.609 2.591 2.560 2.588 Table 18: Ablation Results for 6 layer 768 dimension Goose model"
        },
        {
            "title": "L Parameters Statistics",
            "content": "In Section 4, the actual ranges and statistical metrics of the parameters within the trained model are not specified. To facilitate better understanding of the role of these parameters in the practical models, this appendix provides empirical statistical metrics of selected parameters from the released RWKV-7 model. Figure 17: Box Plots of ξ and α Across Models"
        },
        {
            "title": "M Initial Token Sensitivity",
            "content": "In our evaluation of LAMBADA (Paperno et al., 2016), we found that RWKV-7s performance varies significantly under different settings. After ruling out precision issues, we investigated the consistency of the input and discovered that omitting the special token <endoftext> at the beginning of the input caused substantial and statistically significant differences in perplexity (PPL) and accuracy (ACC) for some RWKV-7 models. Previous research highlights that Transformer models are sensitive to special tokens, and finetuning these tokens can yield notable improvements (Yang et al., 2023b). However, to our knowledge, no quantitative study has examined this effect systematically. We analyzed the cases with the largest performance discrepancies, and identified key pattern: 52 Figure 18: Mean ξ and α Across Layers for Different Models Figure 19: Maximum and Minimum of ξ Across Layers in Different Models The answer appears as the first word of the paragraph. This first word does not reappear elsewhere in the text. This suggests that the model may struggle to retain the first token in memory. In the LAMBADA test set, we identified 142 such examples out of total of 5153 questions. One example is: Beth smoothed her wiry half-black, half-gray hair from her makeup-free face. In New Mexico, the natural look was common. Standing next to Cindy Fanucci, she felt like disaster. She hid her ragged nails under the sleeves of her sweatshirt. \"Hi, Im Cindy. Its so nice to meet you, Beth.\" 53 Figure 20: Maximum and Minimum of α Across Layers in Different Models Figure 21: Box Plots of biases of dt Across Models The following table summarizes the performance of different models with and without the padding token <endoftext>: The results indicate that the inclusion of the <endoftext> token improves the performance of RWKV-7 very significantly, especially for small models (e.g., RWKV-7 0.1B). This finding highlights the importance of proper state initialization and context setting for RNN-based architectures like RWKV. Unexpectedly, we also found that two consecutive <endoftext> tokens at the beginning can further improve the performance of RWKV-7, despite that <endoftext> never appears consecutively in the training corpus. However, for Transformer-based models such as Qwen2.5-0.5B, we observe that the impact of the <endoftext> token is less pronounced, suggesting that these models may have better mechanisms for attending to the initial token. 54 Figure 22: Mean biases of dt Across Layers for Different Models Figure 23: Maximum and Minimum of biases of dt Across Layers in Different Models As result, for optimal performance when prompting RWKV-7, we recommend always including the <endoftext> token at the beginning of the prompt. For example, if you plan to use RWKV-7 as chat assistant, consider the following structured prompt format: <endoftext>User: <Your Question> Assistant: <Assistant Answer> User: <Another Question> Assistant: 55 Model EOS padding PPL ACC (%) RWKV7 World 0.1B RWKV7 World 0.4B SmolLM2 360M Qwen2.5 0.5B 0 1 0 1 0 0 1 357 16.4 42.7 7.25 21.1 9.17 12.2 7.97 9.2 36. 28.9 48.6 39.4 49.3 47.9 54.9 Significance NS Table 19: Performance comparison of different models with and without the <endoftext> < 0.01, token in the partial set of 142 samples from LAMBADA. Significance levels: < 0.05, < 0.001, NS = Not Significant."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "Dalle Molle Institute for Artificial Intelligence USI-SUPSI",
        "Denigma",
        "EleutherAI",
        "George Mason University",
        "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
        "New York University",
        "RWKV Project (under Linux Foundation AI & Data)",
        "Recursal AI",
        "Shenzhen University",
        "Tano Labs",
        "Tsinghua University",
        "University of Oslo"
    ]
}