{
    "paper_title": "REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding",
    "authors": [
        "Yan Tai",
        "Luhao Zhu",
        "Zhiqiang Chen",
        "Ynan Ding",
        "Yiying Dong",
        "Xiaohong Liu",
        "Guodong Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through a triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across a variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 1 4 7 0 . 3 0 5 2 : r REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding Yan Tai1, Luhao Zhu2, Zhiqiang Chen4, Yunan Ding3, Yiying Dong3 Xiaohong Liu1, Guodong Guo4 1Shanghai Jiao Tong University, 2Zhejiang University, 3Hong Kong Polytechnic University 4Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China yan.tai@sjtu.edu.cn, aaronzhu@zju.edu.cn Figure 1. Overview of Visual Tasks Supported by REF-VLM. REF-VLM supports wide range of visual tasks with user-provided visual inputs such as points, boxes, scribbles, and masks, while enabling the decoding of visual contents into formats like points, boxes and masks. For better visualization, details such as TRP and part of special tokens are hidden in the models responses."
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. Simultaneously, current MLLMs utilizing latent embeddings for visual task decoding generally demonstrate limited adaptability to both multi-task learning and multi-granularity scenarios. In this work, we present REF-VLM, an end-to-end framework for unified training of various visual decoding tasks. To address complex visual decoding scenarios, we introduce the Triplet-Based Referring Paradigm (TRP), which explicitly decouples three critical dimensions in visual decoding tasks through triplet structure: concepts, decoding types, and targets. TRP employs symbolic delimiters to enforce structured representation learning, enhancing the parsability and interpretability of model outputs. Additionally, we construct Visual-Task Instruction Following Dataset (VTInstruct), large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like box, keypoint, depth and mask. The combination of different visual prompts and visual units generates wide variety of task types, expanding the applicability of REF-VLM significantly. Both qualitative and 1 Table 1. Comparisons of recent MLLMs and their capabilities in performing downstream tasks. Visual Understanding Referring Expression Interactive Grounding (IG) Grounded Conversation Generation (GCG) Open Vocabulary Identification Model End-to-End Extensible VQA Caption RES REC REG Mask LLaVA [34] MM-REACT [64] Visual ChatGPT [54] HuggingGPT [48] BuboGPT [74] Kosmos-2 [41] Shikra [9] MiniGPT-v2 [8] NExT-Chat [70] Ferret [66] SHPINX [31] LLaVA-Plus [35] LISA [28] Osprey [68] GLaMM [45] PixelLM [61] PSALM [73] GroundHOG [72] F-LLM [58] VITRON [19] VisionLLM [52] VisionLLMv2 [56] REF-VLM (Ours) (cid:34) - - - - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) - - - - - - (cid:34) (cid:34) - - - - - - - (cid:34) - - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - - (cid:34) (cid:34) - - - - (cid:34) - - (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - - - - - - (cid:34) - - (cid:34) (cid:34) (cid:34) - - (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - - - (cid:34) (cid:34) - (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) (cid:34) - - - - - - - - - - - - - - - - (cid:34) - - - - (cid:34) (cid:34) Box - - - - - - - - - (cid:34) - - - - - - (cid:34) - - - - (cid:34) (cid:34) Mask - - - - - - - - (cid:34) - (cid:34) - - - (cid:34) (cid:34) - (cid:34) (cid:34) - (cid:34) (cid:34) (cid:34) Box - (cid:34) - - (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) - - - - - - - - - (cid:34) (cid:34) (cid:34) OVS OVD FOVS - - (cid:34) (cid:34) - - (cid:34) (cid:34) - - (cid:34) (cid:34) - - - - - - - (cid:34) - - - - (cid:34) - - - (cid:34) (cid:34) (cid:34) - - - - - - - - - - - - - - - - (cid:34) (cid:34) (cid:34) - - - - - - - - - - - - - - - - - - (cid:34) FOVD - - (cid:34) (cid:34) - - - - - - - - - - - - - - - - - - (cid:34) quantitative experiments demonstrate that our REF-VLM outperforms other MLLMs across variety of standard benchmarks. The code, dataset, and demo available at https://github.com/MacavityT/REF-VLM . 1. Introduction Multimodal Large Language Models (MLLMs) demonstrate excellent performance in tasks such as visual question answering and scene understanding [2, 34, 50]. Despite these achievements, typical MLLMs primarily understand input and generate responses with text, which limits their ability to perform fine-grained visual localization. As result, they struggle to make significant contributions in real-world applications such as autonomous driving, robotics, and medical diagnosis. In this work, we present REF-VLM, an end-to-end framework that enables unified multi-task training, contrasting with existing models that require separate stage-wise training for different tasks, thereby enhancing semantic consistency. To encode diverse user interactions, we introduce novel parameter-free Mask-Guided Aggregation scheme. Additionally, we propose Latent Embeddings Router and Parallel Group Hungarian Matching to handle multi-task and multi-granularity decoding scenarios effectively. As illustrated in Figure 2 (b), conventional Vision Language Models (VLMs) [28, 45, 56] typically generate responses in simplistic Visual Concept + Referring token format, which proves inadequate for complex multi-granularity scenarios. To address even more challenging visual decoding tasks as shown in Figure 2 (c), we introduce the Triplet-Based Referring Paradigm (TRP), which enforces explicit generation of three core components: (1) visual concept, (2) decoding type, and (3) referring tokens, organized through structured special token framework. The inherent compositional nature of the triplet structure endows TRP with one-fits-all capability, effectively handling complex multi-task and multi-granularity scenarios. To further enhance the precision of structured text generation, we introduce Visual Decoding Chain-of-Thought (VD-CoT), which requires the model to first overview the image and summarize task-relevant information before generating TPR-compliant responses. The synergistic combination of VD-CoT and TRP significantly improves model performance and accuracy across multi-task visual decoding scenarios. To enhance the diversity of vision-language tasks, we propose Visual-Task Instruction Following Dataset (VTInstruct), multimodal dataset specifically designed to support wide range of tasks, including Visual Understanding [34], Referring Expressions [9, 66, 70, 73], Interactive Grounding (IG) [56, 73], Open-Vocabulary Identification [48, 54, 56, 73], Grounded Conversation Generation (GCG) [45], Keypoint Detection [31, 56] and Depth Estimation [31]. VT-Instruct consists of more than 100 million high-quality multimodal dialogue samples, primarily derived from pubFigure 2. Comparison of Visual Unit Decoding Methods. Benefiting from the Triplet-Based Referring Paradigm, REF-VLM can adapt to more complex granularity scenarios and visual decoding tasks, enhancing the interpretability and accuracy of the MLLMs responses. licly available datasets such as LAION-5B [47], SA-1B [26], COCO [30], GRIT [41], etc. Each sample is enhanced with thoughtfully crafted prompt templates with multimodal inputs (e.g. images, texts, points, boxes, scribbles and masks) to facilitate instruction following and diverse outputs (e.g. texts, boxes, keypoints, depth and masks) for different downstream tasks. Our contributions can be summarized as follows: We propose REF-VLM, an end-to-end framework for unified visual decoding tasks, integrating novel components like Mask-Guided Aggregation, Latent Embeddings Router, and Parallel Group Hungarian Matching to boost multi-task performance and adaptability. We design Unified Instruction Pipeline with the TripletBased Referring Paradigm and VD-CoT for precise referring in multi-task and multi-granularity scenarios. We also introduce VT-Instruct, large-scale dataset with 100M multimodal dialogue samples spanning 25 task types, enabling robust understanding and decoding of diverse visual units. Extensive experiments show that REF-VLM outperforms existing MLLMs on tasks including Visual Understanding, Referring Expression, Grounded Conversational Generation, Open-Vocabulary Identification, and Interactive Grounding. 2. Related Works MLLMs often lack the capability to output visual units such as boxes, keypoints, and masks. To expand their applicability in real-world visual tasks, it is typically necessary to implement targeted designs for different visual tasks. Decode Visual Units with Agent Tools. Another approach involves using the MLLM as an agent to coordinate taskspecific models, enabling localization of visual targets [35]. In this case, MLLM outputs textual descriptions of recognized content and scheduling results, which can be utilized by downstream visual tools. LLaVa-Plus [35] constructs an instruction-following dataset that includes large number of samples for using task-specific models as tools. VITRON [19] incorporates sketch encoder to process user-provided visual prompts and supports direct generation of bounding box coordinates by the model. However, since the final visual units is derived from the tool models, there may be gap between the MLLMs understanding and the final output. Moreover, the model is unable to effectively leverage previous visual recognition results during prediction, requiring repeated input of tool model outputs, leading to issues such as insufficient robustness and accuracy in multi-task and multi-target applications. Decode Visual Units with Latent Embeddings. Using the tokens output by the MLLM as learnable queries input into task-specific decoders is the most widely adopted visual decoding strategy [28, 45]. LISA [28] adopts SAM [26] as the mask decoder, where MLLM generates learnable special tokens as prompts for SAM, producing fine-grained segmentation results. PSALM [73] divides the input for openvocabulary segmentation tasks into instruction prompts, condition prompts, and discrete mask tokens, decoding the output mask tokens to obtain segmentation results aligned with the prompt content. GLAMM [45] integrates text-based dialogue with segmentation tasks, utilizing SAM to simultaneously generate detailed descriptions and mask results from the model. VisionLLM v2 [56] introduces the super-link technique, where the MLLM generates task-specific special tokens to serve as routing tokens. These tokens are followed by additional learnable queries appended to them, facilitating visual decoding tasks. 3. Unified Instruction Pipeline 3.1. Triplet-Based Referring Paradigm Section 2 describes two dense prediction decoding approaches: Agent Tools and Latent Embeddings. As shown in Figure 2(a), MLLMs act as agents, generating structured text (e.g., JSON) to invoke external decoders like Grounding DINO [36] and SAM [26]. Meanwhile, Figure 2(b) and (c) depict the Latent Embeddings approach, 3 Prompt Share your perspective of the scene and give the bounding box for each object you discuss. Please give me detailed rundown. VD-CoT <Task> Unit decode (True). Class name, target unit and number: - Name: electric boat Unit: box Num: 1 - Name: two men Unit: box Num: 2 - Name: baseball cap Unit: box Num: 2 - Name: red apple Unit: box Num: 1 </Task> Answer with Triplets <Phrase>Two men</Phrase> (<Unit>box</Unit>[0] <REF>[1]<REF>) wearing <Phrase>baseball caps </Phrase> (<Unit>box </Unit>[0] <REF>[1]<REF>) stand on an <Phrase>electric boat</Phrase>(<Unit>box</Unit>[0]<REF>), with one holding <Phrase>red apple</Phrase>(<Unit>box</Unit>[0]<REF>) . Table 2. An Example of VD-CoT Applied to the Grounded Conversation Generation (GCG) Task. VD-CoT analyzes the image and outputs the structured visual decoding information required by TRP. The answer is generated synchronously with the triplet, and the special tokens have been simplified in the example. where MLLM outputs serve as learnable queries for visual task decoders. Here, the special token <REF> facilitates referential learning. Our REF-VLM adopts the Latent Embeddings framework, offering greater flexibility and adaptability for diverse visual decoding tasks. For clarity, we omit some special tokens used for assisting referential tasks. In Figure 2(b), we illustrate the conventional referring paradigm [28, 45, 46, 56], where the <REF> token is typically introduced after visual concepts to enable single decoding process. However, discrete labels suffer from semantic ambiguity. For instance, the phrase People are crossing the street can refer to visual concepts such as the entire scene (single target), the people (multiple targets) and the street (single target). Existing reference schemes struggle to effectively handle visual concept references at different granularities, ultimately impacting the interpretability and accuracy of MLLM responses. Moreover, since different visual tasks require varying decoding granularities, extending the Latent Embeddings decoding approach to multi-task scenarios necessitates more effective referring and embedding framework. ambiguity and tasks. (i) Visual Concepts, We propose the Triplet-Based Referring Paradigm (TRP), mechanism for multi-granularity visual concept decoding. As shown in Figure 2(c), TRP resolves supports multi-granularity semantic TRP comprises three referencing in visual components: encapsulated in <Phrase> tags (e.g., <Phrase>dogs</Phrase>); (ii) Decoding Types, specified in <Unit> tags (e.g., <Unit>box</Unit>); and (iii) References, denoted by <REF> to link concepts to instances (e.g., [0]<REF> for the first detected dog). Consequently, the example in Figure 2(c) corresponds to the full representa- <Phrase>capybaras</Phrase>((<Unit> tion: box</Unit>[0]<REF>[1]<REF>), <Unit>box </Unit>[1]<REF>). The TRP framework demonstrates one-fits-all advantage across diverse visual tasks. This is achieved through two key design strengths: (1) Syntactic Scalability: The triplet-based structure inherently supTRP can represent ports compositional expansion. complex scene descriptions through hierarchical nestsuch as <Phrase><Phrase>dog</Phrase> ing, in <Phrase>park</Phrase></Phrase>. Additionally, TRP can specify composite tasks by combining multiple <Unit> tags (e.g., <Unit>box, keypoint</Unit>). (2) Task Extensibility: By predefining the semantic space of <Unit> tags (e.g., introducing <Unit>depth</Unit>), TRP enables zeroshot task extension, allowing seamless adaptation to new visual tasks without additional training [50]. These design strengths ensure that TRP is not only versatile but also futureproof, making it robust solution for multi-granularity visual concept decoding. 3.2. Visual Decoding Chain-of-Thought TRP enforces structured representation learning through symbolic delimiters, enhancing the parsability and interpretability of the output. To further improve the accuracy of the triplet-structured output, we introduce the Visual Decoding Chain-of-Thought (VD-CoT), an instruction-tuning approach designed to guide TRP generation. As shown in Table 2, the VD-CoT process is encapsulated within <Task> and </Task> tags. When executing visual decoding tasks, VD-CoT requires the MLLM to: (1) Identify the visual concepts to be decoded. (2) Specify the type of decoding required for the current task. (3) Determine the number of instances to be decoded. When no decoding task is needed, VD-CoT simply outputs Unit decode (False). This structured approach ensures precise and context-aware generation, further enhancing TRPs effectiveness in multi-granularity visual concept decoding. 4 Figure 3. The Framework of REF-VLM. REF-VLM employs dual-architecture visual encoders to jointly encode images into feature pyramid, enhancing visual unit decoder performance. Additionally, visual prompts are fused with global features and share projector, enabling parameter-free encoding of image interactions. Training samples adhere to the Triplet-Based Referring Paradigm, ensuring one-to-one mapping between REF-VLMs latent embeddings and decoding targets. Definitions and roles of all special tokens used in RE-VLM are detailed in Appendix Table 10. 3.3. Visual-Task Instruction Following Dataset Following the Triplet-Based Referring Paradigm, We present Visual-Task Instruction Following Dataset (VT-Instruct), large-scale visual multi-task dataset that combines different visual prompts as inputs and visual units as outputs. VT-Instruct comprises over 100 million dialogue samples featuring multimodal input-output pairs. These pairs encompass various combinations of output units, ranging from low to high visual density, including point, box, keypoint, depth and mask, combined with either low or high text complexity. For each downstream task, we (i) first construct specific system instruction and (ii) generate over 150 task-specific prompt templates using GPT-4, randomly selecting them to construct user prompts, then (iii) we modify existing dataset annotations to construct unified answering format following the rule of TRP (Section 3.1), creating multi-turn conversations featuring system-prompt-answer combination. Due to efficiency and computational resource considerations, REF-VLM utilizes only small subset of VTInstruct as its training set. As visual decoding task framework, REF-VLMs training samples are generally comparable to or fewer than those in similar studies [2, 5, 11, 45, 46, 56, 68, 73] for each individual task. For an analysis of VT-Instruct dataset usage, please refer to the Appendix Table 16. The details of definition for each task will be presented in Appendix B. 4. End-to-End Decoding Framework Unlike existing Latent Embeddings decoding methods that require task-specific fine-tuning in separate stages [56], REF-VLM achieves unified end-to-end training for all tasks, including conventional QA, VQA, and various visual decoding tasks. We will illustrate the training process using an example based on the Referring GCG-Segmentation task and discuss the core components of the framework in the following subsections. 4.1. Unified Training Workflow The example training task in Figure 3 requires the MLLM to describe user-specified region based on mask and prompt input, and generate precise segmentation results. REF-VLM supports image and visual prompt (VPT) inputs, where VPT includes point, box, scribble, and mask. For image encoding, we follow LLaVA [34] and use CLIP-ViT [44] to extract global features mapped to the text embedding space. To address CLIP-ViTs limitations [28, 45, 73] in dense prediction tasks, we additionally employ CLIPConvNeXt [14], Conv-based architecture pre-trained on large-scale image-text pairs, to capture multi-scale local features. The outputs of both encoders are concatenated to improve visual task decoding. For VPT encoding, we propose parameter-free Mask-Guided Aggregation method (see Section 4.2) to fuse image and VPT features, enabling precise region understanding. REF-VLM employs Vicuna-v1.5-7B [75] as the base LLM for processing the text modality. Figure 3(b), (c), and (d) illustrate the parallel decoding and supervision process of training sample. In the (b) Training Sample Text Encoding process, the input training text adheres to the TRP specification outlined in Section 3.1, requiring the LLM to generate sequence of latent embeddings of equal length for the final visual decoding target, represented as <REF> tokens. In pipeline (c) Image-Text Interleaved Embedding, the image global features and aggregated visual prompt features output from pipeline (a) Visual Encoding are substituted into the reserved placeholders (i.e., <image> and <region>) respectively, forming complete sequence fed into the LLM Decoder. 5 Figure 3(d) and (e) illustrates the supervision process of the training sequence. The gray area corresponds to the prompt part, which does not require loss calculation during the supervised fine-tuning. The blue area corresponds to the answer part, which is mapped to the vocabulary and included in the loss computation. The red area corresponds to the <REF> tokens, which not only undergo the same LLM loss calculation as the blue area but also serve as inputs to the visual unit decoders for further decoding. This aspect will be detailed in Section 4.3. REF-VLM follows two-stage training process. In the first stage, similar to Shikra [9], only the global visual encoder (CLIP-ViT), the projector, and the LLM participate in computation. During this phase, the weights of CLIPViT and the LLM remain fixed, with only the projectors parameters being updated. In the second stage, REF-VLM is trained in unified manner across all tasks. Beyond the modules in the (a) Visual Encoding pipeline, all other components, including the projector, LLM, and visual unit decoders are updated. The Unified Training Workflow of REFVLM offers better semantic consistency compared to similar approaches that rely on pre-trained visual task decoders [28, 45, 46, 56, 68], such as SAM [26] or Grounding-DINO [36]. REF-VLM eliminates the need to repeatedly use visual encoders for each visual task, significantly reducing the overall model parameters. This aspect will be discussed in detail in Appendix C. 4.2. Mask-Guided Aggregation REF-VLM achieves diverse referencing tasks through visual prompt support (points, boxes, scribbles, masks). Unlike existing methods requiring additional parameters [19, 68], we introduce parameter-free Mask-Guided Aggregation. irst, we perform two steps: (1) converting prompts into normalized masks whose sizes are aligned with global features; (2) partitioning features and masks into grids for patch-wise fusion. Let the input image global features be denoted as RCN HW , where is the number of channels, indicates the count of spatial patches, and define the spatial resolution of each patch. Given mask RQN HW with representing the embedding length used to encode the features of visual prompts, our aggregation operation computes an Hadamard product between channel-aligned features (X ) and query-specific masks (M) at each spatial position (h, w), followed by summation over the dimensions. Vq,n,c = (cid:88) (cid:88) h=1 w=1 Xc,n,h,w Mq,n,h,w (1) Let the aggregated output tensor be RQN C. To inject spatial awareness, we augment it with cosine positional encodings. For position index and channel c, the encoding is defined as: PE(n, c) = cos (cid:16) s2c/C (cid:17) , (2) where is temperature hyperparameter. The refined features (cid:101)V are obtained by: (cid:101)Vq,n,c = Vq,n,c + α PE(n, c), (3) where α could be learnable scalar, but in our setup, α is set to constant value of 1. Finally, the visual prompt features and image global features share the projector layer, achieving efficient feature fusion without introducing additional parameters. 4.3. Visual Unit Decoders In unified multi-task training, each batch instance undergoes task-specific decoding processes. Figure 4 illustrates REFVLMs workflow, showing single sample requiring dualtask decoding. For clarity, we omit components detailed in Section 4.1. Figure 4(a) shows REF-VLMs training pipeline: (1) visual data passes through dual encoders to build feature pyramid, aligning with Section 4.1; (2) the LLM processes prompts, generating TRP-structured responses (Section 3.1) with <REF> tokens for one-to-one referring between latent embeddings and visual instances. The LLM embeddings are categorized as: gray (text tokens), red (box targets embeddings), and blue (mask targets embeddings). We leverage the Latent Embeddings Router to filter task-specific embeddings, which are then padded to fixed length before being fed into their respective visual decoders. The TRPs one-toone mapping enables exclusion of padding tokens from loss computation. For both box and mask decoders, we adopt streamlined yet effective architectures: DETR [6] for box prediction and MaskFormer [12] for mask segmentation. As illustrated in Figure 4(a) and (b), the image feature pyramid undergoes fusion and flattening operations before serving as input to the transformer decoder, which jointly processes the features with latent embeddings for visual unit decoding. However, given the distinctive properties of the TRP mechanism - particularly its inherent one-to-one correspondence between embeddings and targets - this introduces two significant modifications to the decoding process: (1) Each prediction is uniquely associated with decoding target, thereby eliminating the need for classification head; (2) The TRP inherently partitions predictions into distinct groups, rendering the standard Hungarian matching algorithm [6] inadequate. critical challenge arises when multiple targets exist within single group, as exemplified by the capybaras group containing two bounding boxes: how to determine the optimal matching configuration? To address issue (1), we eliminate 6 Figure 4. Architecture of Visual Unit Decoders. We propose Latent Embeddings Router to facilitate unified multi-task training in REF-VLM, and enhance the Hungarian matching algorithm for the TRP-based one-to-one referring decoding scheme. the classification layers from both the box and mask decoders. For issue (2), we propose novel Parallel Grouped Hungarian Matching strategy to ensure precise alignment between predictions and ground truth annotations. 1 , ..., p(i) Let denote batch containing independent groups. For the i-th group: (i) P(i) = {p(i) }: Set of Ni predicted boxes. (ii) T(i) = {t(i) }: Set of Mi target boxes. (iii) C(i) RNiMi: Cost matrix where element c(i) nm represents the matching cost between p(i) . (iv) σ(i) : {1, ..., Ni} {1, ..., Mi} {}: Injective matching function for the i-th group. 1 , ..., t(i) and t(i) Mi Ni The pairwise cost combines geometric measures: , t(i) ) + λGIoULGIoU(p(i) nm = λL1LL1(p(i) c(i) where λL1 and λGIoU are weighting coefficients, LL1 denotes normalized coordinate differences, and LGIoU represents the generalized IoU loss. , t(i) ) (4) To enable parallel computation, we construct padded tensor: (cid:101)C RBNmaxMmax (5) where Nmax = max(Ni) and Mmax = max(Mi). Invalid entries are masked using: M(i) nm = (cid:40) 0 otherwise Ni Mi (6) For each group i, find optimal permutation matrix (cid:101)A(i) {0, 1}NmaxMmax that minimizes: Nmax(cid:88) Mmax(cid:88) n=1 m=1 nm (cid:101)A(i) (cid:101)C(i) nm (7) 7 subject to: Mmax(cid:88) m=1 (cid:101)A(i) nm 1 n, Nmax(cid:88) n=1 (cid:101)A(i) nm 1 (8) 5. Experiments We conduct quantitative evaluations of our REF-VLM in Section 5.1 and Appendix F.1: (i) Visual Understanding, (ii) Grounded Conversation Generation (GCG), (iii) Referring Expression, (iv) Freeform Open-Vocabulary Identification, (v) Interactive Grounding. Then, we perform ablation studies to evaluate the effectiveness of the key elements in our approach in Section 5.2 and Appendix F.2. 5.1. Quantitative Results Visual Understanding. We begin by presenting quantitative comparisons on zero-shot image captioning tasks using the Flickr30k [42] and NoCaps [1] validation datasets, as well as VQA tasks on the VQAv2 [3] and OK-VQA [39] test datasets. For image captioning, we report the CIDEr score, while for VQA tasks, overall accuracy is provided. The summarized results in Table 3 demonstrate that our REF-VLM model achieves the highest performance on the image captioning task, with CIDEr score of 96.0 on the Flickr30k dataset and 122.4 on the NoCaps dataset. For VQA tasks, REF-VLM outperforms other models, achieving 62.39% accuracy on the OK-VQA test dataset and 81.6% on the VQAv2 test dataset, comparable to VisionLLMv2-Chat [56]. Furthermore, we utilize the POPE benchmark [29] to assess hallucination performance in REF-VLM, as shown in Table 5, where REF-VLM attains the highest F1 score, surpassing other MLLMs in every case. Grounded Conversation Generation (GCG). The Table 3. Evaluation on Image Captioning and VQA for MLLMs. Model Flamingo-80B [2] InstructBLIP [16] LLaVA-1.5-7B [34] Shikra-13B [9] InternVL-G [11] Qwen-VL [5] VisionLLMv2-Chat [56] VisionLLMv2 [56] GLaMM [45] REF-VLM Image Captioning Flickr30K NoCaps VQAv2 OKVQA VQA 67.2 82.8 - 73.9 79.2 85.8 88.7 90.2 95.3 96.0 - 121.9 - - 113.7 121.4 118.1 116.9 106.8 122.4 56.3 - 78.5 77.4 80.2 78.2 81.4 80.8 - 81.6 50.6 - 54.40 47.16 - - - - - 62. Table 4. REG Comparison on RefCOCOg. Model GRiT [57] Kosmos-2 [41] ASM [53] RegionGPT [23] PixelLLM [61] GLaMM [45] Osprey [68] Groma [38] VisionLLMv2-Chat [56] VisionLLMv2 [56] REF-VLM CIDEr Meteor 71.6 62.3 103.0 109.9 82.3 106.0 108.3 107.3 118.5 111.6 119.1 15.2 14.1 20.8 16.9 14.3 16.2 16.6 16.8 21.2 20.4 21.6 Table 5. Object hallucination benchmark in three POPE [29] evaluation settings. Sampling Metrics REF-VLM GroundHOG [72] LION [7] Osprey [68] Ferret [66] Shikra [9] Random Popular Adversarial Accuracy Precision Recall F1 Score Accuracy Precision Recall F1 Score Accuracy Precision Recall F1 Score 92.44 90.00 96.00 92.90 90.10 85.78 96.13 90.66 87.30 91.54 82.20 86.62 91.03 85.80 96.40 90.79 90.13 85.93 93.81 89.70 86.33 85.93 86.63 86.28 88.97 97.12 81.00 88.33 86.77 91.69 80.87 85.94 85.37 88.69 81.07 84.71 89.47 93.40 84.93 88.97 87.83 89.94 85.20 87.50 85.33 85.43 85.20 85. 90.24 97.72 83.00 89.76 84.90 88.24 80.53 84.21 82.36 83.60 80.53 82.00 86.90 94.40 79.26 86.19 83.97 87.55 79.20 83.16 83.10 85.60 59.60 82.49 LLaVA -1.5 [34] 88.73 88.89 88.53 88.71 85.83 83.91 88.67 86.22 72.10 74.69 88.34 80.94 Instruct -BLIP [16] 88.57 84.09 95.13 89.27 82.77 76.27 95.13 84.66 65.17 65.13 95.13 77.32 MiniGPT4 [8] 79.67 78.24 82.20 80.17 69.73 65.86 81.93 73.02 79.20 61.19 82.93 70. mPLUG -Owl [65] 53.97 52.07 99.60 68.39 50.90 50.46 99.40 66.94 50.67 50.34 90.33 66.82 Table 6. REF-VLM performance on Grounding Conversation Generation (GCG) task. Evaluation Metrics for GCG Tasks: CIDEr, Meteor, AP50, mIoU, and Recall. Scratch indicates whether the decoder is trained from scratch or utilizes pretrained visual decoder (e.g., SAM [26], Mask2Former [12]). (cid:34)symbol indicates that the visual decoder in REF-VLM was trained from scratch, showcasing that REF-VLM outperforms models using pretrained decoders for generating boxes or masks. Model Dataset Type Scratch BuboGPT [74] Kosmos-2 [41] LISA [28] GLaMM [45] REF-VLM REF-VLM Mask Mask Mask Mask Mask Box GranDf Flickr30k (cid:34) (cid:34) Val Test CIDEr Meteor AP50 mIoU Recall CIDEr Meteor AP50 mIoU Recall 27.0 29.0 35.5 38.0 45.3 47.7 54.1 56.8 61.7 64.6 56.6 66.1 17.3 17.2 24.8 27.2 27.7 35.4 17.1 15.8 12.9 14.6 21.7 26.0 3.5 27.2 32.2 37.9 53.2 82. 29.4 28.3 36.3 41.8 50.0 - 54.0 55.6 62.0 66.3 57.9 - 17.2 16.1 13.0 16.2 18.4 - 19.1 17.1 25.2 30.8 26.2 - 3.6 27.6 33.9 47.2 56.9 - Grounded Conversation Generation (GCG) task consists of two components: GCG-mask and GCG-box. For the GCG-mask task, we further finetune our REF-VLM on the GranDf [45] training dataset and evaluate its performance on the GranDf validation and test splits, following the process outlined by [45]. The results presented in Table 6 demonstrate that our REF-VLM, trained from the scratch outperforms current baseline methods which applied pretrained visual decoders, such as GLaMM [45], across metrics including CIDEr, Meteor, AP50, and Recall. Additionally, we assess the GCG-box task using the Flickr30k [42] test set, due to the lack of available MLLMs for the GCG-box task, we only report our zero-shot performance on this dataset. Referring Expression. We evaluate Referring Expression Generation (REG) on the RefCOCOg test dataset [25], using Meteor and CIDEr as evaluation metrics. The results, shown in Table 4, indicate that our REF-VLM outperforms the State-of-Art (SOTA) MLLM VisionLLMv2-Chat [56]. Additionally, we assess both Referring Expression Comprehension (REC) and Referring Expression Segmentation (RES) tasks in Appendix F.1, demonstrating that REF-VLM achieves the highest performance across these tasks. Freeform Open-Vocabulary Identification. Compared to existing MLLMs, which generally require category prompts to be added in open-vocabulary tasks [56, 73], enabling the model to understand potential categories in the image before performing open-set detection or segmentation, our 8 Table 7. Evaluation on zero-shot open-vocabulary tasks. Evaluation of Open-Vocabulary Instance Segmentation on the ADE20k Dataset and Object Detection on COCO2017. Since no existing MLLM can perform zero-shot open-vocabulary object detection using straightforward template without prompting any class hint, we report only our models performance on this dataset. Model Type Scratch MaskCLIP [18] ODISE [60] SAN [62] PSALM [73] PSALM+LVIS [73] REF-VLM (mAPS) SEG SEG SEG SEG SEG SEG/DET (cid:34) (cid:34) (cid:34) (cid:34) Mask Decoder MaskCLIP Diffusion UNet CLIP+SAN Mask2Former Mask2Former MaskFormer Box Decoder - - - - - DETR ADE20k COCO 6.0 14.4 10.6 9.0 13.9 16.7 - - - - - 26.7 Table 8. Comparison across visual encoder and ablation study on group matchers. [0,1,2,3] means we choose all four feature layers from CLIP-ConvNeXt model, -2 means we only choose CLIP-ViT second last layer, [0,1,2,4] means we concatenate the first three feature layers from CLIP-ConvNeXt and the output feature map from CLIP-ViT. (cid:34)means we applied the Group Hungarian Matcher when we trained the decoder. Visual Encoder Size ConvNeXt-L ConvNeXt-L CLIP-ViT ConvNeXt-L + CLIP-ViT ConvNeXt-L + CLIP-ViT ConvNeXt-L + CLIP-ViT 320 336 336 320 512 512 Group Matcher (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) Feature Dimension [0,1,2,3] [0,1,2,3] -2 [0,1,2,4] [0,1,2,4] [0,1,2,4] cIoU 41.94 46.08 60.44 60.02 61.83 62.49 Table 9. CoT ablation study on SA-1B subsets. The accuracy of the LLMs output categories and special tokens was assessed under both conditions (with and without CoT) using CIDEr and Meteor as evaluation metrics. FOVS FOVD GCG Box GCG Mask No CoT CoT CIDEr Meteor CIDEr Meteor CIDEr Meteor CIDEr Meteor 47.77 25.60 48.23 27.57 197.88 200.72 204.78 205.80 21.30 24.90 47.88 47. 31.66 27.14 34.45 34.22 REF adopts more flexible approach. With REF-VLM, there is no need to predefine categories, we simply input the prompt like Please detect bounding boxes (segment objects) in the image<image>. With this prompt, REF-VLM can autonomously identify potential categories within the image through its LLM, handing off decoding tasks to the subordinate visual decoder. We refer to this more adaptable task as Freeform OpenVocabulary Identification (detail in Appendix E.1). Unlike other MLLMs [56, 73], REF-VLM allows the LLM to directly output categories instead of having the downstream decoder output category names and confidence scores. Accordingly, we introduce mAPS as metric (detailed in Appendix E.2) for evaluating REF-VLMs performance on this task and use it for comparison with other MLLMs. We evaluate REF-VLM on the ADE20k test dataset for 9 zero-shot freeform open-vocabulary segmentation task and COCO2017 validation dataset for object detection. The results for both tasks are presented in Table 7. Notably, REF-VLM achieves strong performance without any specialized design, outperforming other MLLMs (e.g., PSALM [73]) and specialist models (e.g., SAN [62]). Additionally, REF-VLM also demonstrates the capability to perform open-vocabulary object detection. 5.2. Ablation Study To assess our frameworks effectiveness, we analyze the impact of group matcher configuration, visual encoder selection, and CoT implementation. We also evaluate various mask token configurations on Mask-Guided Aggregation and test the Group Matcher on more challenging freeform object detection tasks in Appendix F.2. Choose of Group Matchers. To validate the effectiveness of our Group Hungarian Matcher, we perform an ablation study on its usage in the mask decoder for the RES task, using the RefCOCOg test dataset and cIoU as the evaluation metric. As shown in Table 8, applying the Group Hungarian Matcher for loss computation yields significantly better performance compared to configurations without it, demonstrating its substantial impact on improving the overall accuracy. Different Configuration of Visual Encoders. To investigate the effect of different configurations of CLIP vision encoders, including CLIP-ConvNeXt and CLIP-ViT, along with variations in image size and feature selection layers, we conduct experiments on the RES task using the RefCOCOg test dataset. As shown in Table 8, REF-VLM achieves the highest performance when concatenating the CLIP-ConvNeXt and CLIP-ViT encoders with the setting of image size as 512512. Choose of CoT. Our ablation study of CoT vs. non-CoT models (1,000 SA-1B images [26]) in Table 9 showed CoT significantly improved performance across all dense prediction tasks, demonstrating enhanced contextual understanding and object recognition. 6. Limitations and Conclusion In conclusion, this paper introduces REF-VLM, powerful and extensible open-ended visual multi-task learning framework; TRP, compositional and extensible onefits-all referring paradigm for visual decoding tasks; VT-Instruct, large-scale multimodal instruction-tuning dataset. Extensive experiments validate the effectiveness of our REF-VLM. However, despite TRPs theoretical support for multiple tasks, the actual implemented tasks remain limited. Additionally, performance degradation in multi-turn dialogues and the ratio of data used in training are areas that require further analysis and refinement. Given the aforementioned limitations, our future work will focus on expanding task coverage, enhancing multiturn dialogue robustness, and optimizing data utilization."
        },
        {
            "title": "References",
            "content": "[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the IEEE/CVF international conference on computer vision, pages 89488957, 2019. 7 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2, 5, 8 [3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In International Conference on Computer Vision (ICCV), 2015. 7, 3, 4 [4] Bruno Artacho and Andreas Savakis. Unipose: Unified human pose estimation in single images and videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 5 [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. 5, 8, 9 [6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 6, 5 [7] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2654026550, 2024. 8, [8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 2, 8, 9 [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 2, 6, 8, 5, 9 [10] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104120. Springer, 2020. 9 [11] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Internvl: Scaling up vision foundation models and Dai. aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 5, 8 10 [12] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. 2021. 6, [13] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time openvocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. 3 [14] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 28182829, 2023. 5 [15] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. 3, 4, 7 [16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning, 2023. 8 [17] Henghui Ding, Chang Liu, Suchen Wang, and Xudong Jiang. Vision-language transformer and query generation for referring segmentation. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 10 [18] Xiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, et al. Maskclip: Masked self-distillation advances contrastive language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1099511005, 2023. [19] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: unified pixel-level vision llm for understanding, generating, segmenting, editing, 2024. 2, 3, 6, 5, 10 [20] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for visionand-language representation learning. Advances in Neural Information Processing Systems, 33:66166628, 2020. 9 [21] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. International Journal of Robotics Research (IJRR), 2013. 4 [22] Cristina Gonzalez, Nicolas Ayobi, Isabela Hernandez, Jose Hernandez, Jordi Pont-Tuset, and Pablo Arbelaez. Panoptic narrative grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13641373, 2021. 4, 7 [23] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regiongpt: Towards region understanding vision language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1379613806, 2024. 8 [24] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17801790, 2021. 9 [25] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 8, 3, 4, [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3, 6, 8, 9, 5, 7, 10 [27] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. 3, 4, 7 [28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 2, 3, 4, 5, 6, 8, 10 [29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Evaluating object hallucinaarXiv preprint Zhao, and Ji-Rong Wen. tion in large vision-language models. arXiv:2305.10355, 2023. 7, 8 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 3, 4, [31] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 2 [32] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized referring expression segmentation. In CVPR, 2023. 8, 10 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 3, 4, 7 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2, 5, 8 [35] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023. 2, 3, [36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 3, 6, 5, 9 [37] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 10 [38] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. arXiv preprint arXiv:2404.13013, 2024. 8 [39] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. 7 [40] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 4 [41] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2, 3, 8, 4, 7, [42] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In Proceedings of the IEEE international conference on computer vision, pages 26412649, 2015. 7, 8, 4 [43] Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, and Amjad Almahairi. Jack of all tasks master of many: Designing general-purpose coarse-to-fine vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1407614088, 2024. 9 [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [45] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. 2, 3, 4, 5, 6, 8, 7, 10 [46] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. 4, 5, 6, 7, 8, 10 [47] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. 3 11 [48] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36, 2024. [49] Shweta Singh, Aayan Yadav, Jitesh Jain, Humphrey Shi, Justin Johnson, and Karan Desai. Benchmarking object detectors with coco: new path forward, 2024. 3, 4, 7 [50] Yan Tai, Weichen Fan, Zhao Zhang, and Ziwei Liu. LinkIn Proceedings of context learning for multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2717627185, 2024. 2, 4 [51] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through simple sequence-to-sequence learning framework. In International conference on machine learning, pages 2331823340. PMLR, 2022. 9 [52] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, and Jifeng Dai. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [53] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. arXiv preprint arXiv:2308.01907, 2023. 8 [54] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. CoRR, abs/2303.04671, 2023. [55] Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, and Song Bai. General object foundation model for images and videos at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 37833795, 2024. 10 [56] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, and Jifeng Dai. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. CoRR, abs/2406.08394, 2024. 2, 3, 4, 5, 6, 7, 8, 9, 1, 10 [57] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. In European Conference on Computer Vision, pages 207224. Springer, 2025. 8 [58] Size Wu, Sheng Jin, Wenwei Zhang, Lumin Xu, Wentao Liu, Wei Li, and Chen Change Loy. F-lmm: Grounding frozen large multimodal models. arXiv preprint arXiv:2406.05821, 2024. 2, 8 [59] Ke Xian, Jianming Zhang, Oliver Wang, Long Mai, Zhe Lin, and Zhiguo Cao. Structure-guided ranking loss for single image depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 611620, 2020. [60] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models. arXiv preprint arXiv:2303.04803, 2023. 9 [61] Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, and Cordelia Schmid. Pixelaligned language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1303013039, 2024. 2, 8, 7, 10 [62] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. San: Side adapter network for open-vocabulary semantic segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 9 [63] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance percepIn Proceedings of tion as object discovery and retrieval. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1532515336, 2023. 10 [64] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023. 2 [65] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [66] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 2, 8, 9 [67] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13071315, 2018. 9 [68] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2820228211, 2024. 2, 5, 6, 8, 3, 4, 7, 10 [69] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 4, 7 [70] Ao Zhang, Liming Zhao, Chen-Wei Xie, Yun Zheng, Wei Ji, and Tat-Seng Chua. Next-chat: An lmm for chat, detection and segmentation. arXiv preprint arXiv:2311.04498, 2023. 2, 9, 10 [71] Hao Zhang, Hongyang Li, Feng Li, Tianhe Ren, Xueyan Zou, Shilong Liu, Shijia Huang, Jianfeng Gao, Lei Zhang, Chunyuan Li, et al. Llava-grounding: Grounded visual chat with large multimodal models. arXiv preprint arXiv:2312.02949, 1, 2023. 4, 7 [72] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1422714238, 2024. 2, 8, 10 [73] Zheng Zhang, Yeyao Ma, Enming Zhang, and Xiang Bai. Psalm: Pixelwise segmentation with large multi-modal model. arXiv preprint arXiv:2403.14598, 2024. 2, 3, 5, 8, 9, 4, 7, 10 [74] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling visual grounding in multi-modal llms. arXiv preprint arXiv:2307.08581, 2023. 2, 8 [75] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 5 [76] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. 3, 4, 7 [77] Zijian Zhou, Zheng Zhu, Holger Caesar, and Miaojing Shi. Openpsg: Open-set panoptic scene graph generation via large multimodal models. arXiv preprint arXiv:2407.11213, 2024. 4, 7 [78] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 49955004, 2016. 4, 7 [79] Xueyan Zou*, Zi-Yi Dou*, Jianwei Yang*, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee*, and Jianfeng Gao*. Generalized decoding for pixel, image and language. 2022. [80] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. Advances in Neural Information Processing Systems, 36, 2024. 9, 10 13 REF-VLM: Triplet-Based Referring Paradigm for Unified Visual Decoding"
        },
        {
            "title": "Contents",
            "content": "1. Introduction 2. Related Works 3. Unified Instruction Pipeline . . . . 3.1. Triplet-Based Referring Paradigm . 3.2. Visual Decoding Chain-of-Thought . . . . 3.3. Visual-Task Instruction Following Dataset . . . . 4. End-to-End Decoding Framework . 4.1. Unified Training Workflow . . 4.2. Mask-Guided Aggregation . . . 4.3. Visual Unit Decoders . . . 5. Experiments 5.1. Quantitative Results . 5.2. Ablation Study . . . . . . . . . . . . 6. Limitations and Conclusion A. VD-CoT Details B. VT-Instruct Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.1. Definition of Each Downstream Task . . . . . . . . B.2. Dataset Construction Details . . . . . C. Implementation Details C.1. Group Hungarian Matcher . . C.2. Extend to More Plugins . . . . . . . . . . . . . . . . . . . D. Training Details E. Freeform Open-Vocabulary Identification . . . . E.1. Task Definition . E.2. mAP Similarity (mAPS) . . . . . . . . . . . . . . . . . . . . . . F. More Results F.1. More Experimental Results . F.2. More Ablation Results . . . . . . . . . . . . . . . . . . . . G. More Visualization Results A. VD-CoT Details In this section, we provide additional details about VD-CoT, including its functionality, design rationale, and advantages over existing approaches. Unified Instruction Tuning for Visual Decoding Tasks As mentioned in Section 3.2, classical approaches based on learnable queries typically require the model to provide an additional special token (referred to as <REF> in our setting) for each visual entity (referred to as Phrase in our setting). This token is used to represent the visual entity and serves as learnable query for downstream task decoders. However, in the vanilla Phrase + <REF> approach, although the <REF> may have length greater than 1, it always refers to the same instance and decodes into single target. This design limits the flexibility of instruction-tuning methods, when the instance represented by the phrase has multiple occurrences, this approach fails to adapt effectively. For example, the Decoding Triplets process in VD-CoT supports both one-to-one decoding (e.g., There are two capybaras, <Phrase>a big one</Phrase>[0]<REF> and <Phrase>a small one</Phrase>[0]<REF>.) and one-tomany decoding (e.g., There are <Phrase>two capybaras</Phrase>[0]<REF>[1]<REF>.), where all content enclosed in angle brackets represents special tokens. All special token definitions can be found in Table 10, and the VD-CoT processes for different tasks are detailed in Appendix F. Decoding Triplets for Prevent Task Conflicts In visual multi-task training schemes for MLLMs, task conflict is typically mitigated by introducing different special tokens for each task. For instance, VisionLLM v2 [56] incorporates eight distinct special tokens. However, as the number of tasks increases, the required special tokens also grow, and introducing each new token necessitates retraining the MLLM. In our VD-CoT, we employ the <REF> token to refer to visual entities for any task. To prevent potential task conflicts, we leverage the next-token prediction mechanism of MLLMs. Before generating the <REF> token, the model is required to first output the decoding unit of the current task, forming Phrase-Unit-<REF> triplets. As shown in Figure 5, the two curves represent the response magnitudes of two <REF> tokens to the MLLMs answers, with the shaded regions indicating unit contents. During the generation process, <REF> exhibits high response to the Unit, 3 3 3 4 5 5 5 6 6 7 7 9 9 3 3 3 5 5 5 5 8 8 8 8 8 10 1 Figure 5. Attention Significance of <REF>Tokens for MLLM Output Tokens. We selected general segmentation task with the prompt please segment objects in the image <image>. The model outputs two classification results: bride and giant illuminated heart, using our designed VD-CoT format. We used attention visualization techniques to generate attention significance maps for the REF token with respect to each output token of the MLLM. The attention values from each layer were averaged and visualized, as shown in the figure above. The figure demonstrates that each <REF>token exhibits high attention response values to the preceding <Phrase>, <Unit>, and output numbers. Table 10. Special token lists for training REF-VLM. During REF-VLM training, the special tokens mentioned above are predefined and added to the LLMs vocabulary for training. Speical Token Lists Visual Prompt Token Visual Reference Token Phrase Start Phrase End Default Pad Token Token Names [VPT] <REF> <Phrase> </Phrase> [PAD] Speical Token Lists Begin of Task Token End of Task Token Begin of Unit Token End of Unit Token Image Placeholder Token Names <Task> </Task> <Unit> </Unit> <image> ensuring that the representation of <REF> varies across tasks, thereby avoiding task conflicts. Link-Context Learning for Unknown Tasks LinkContext Learning (LCL) [50] enables MLLMs to acquire the knowledge required for unknown tasks from few-shot prompts. In our VD-CoT setting, since the MLLM acts as router and outputs the type of decoding content in textual form, we can leverage LCL to handle unknown tasks that do not appear in the training set using few-shot approach. Specifically, we augment the instruction-tuning data with random samples where user inputs are matched with Unit decoding. For example, the input prompts include You are performing new task, the unit of the task is [unit name], please [task-specific command], and in the corresponding answer, the unit name is replaced with the content from the prompt. Through this approach, VD-CoT requires the MLLM to determine the current decoding task based on the input prompt and form the corresponding triplets, eliminating the need to introduce new special token for each task. This provides REF-VLM with incremental learning capabilities. 2 VD-CoT for High-Accuracy Triplets REF-VLM supports wide range of visual decoding tasks, with differences in how Phrase-Unit-<REF> triplets are constructed for each task. Complex instruction-tuning schemes can lead to reduced generation accuracy by the MLLM. For instance, in the one-to-many decoding setting, the <REF> token may fail to match the expected number of instances in the image, instead entering an infinite generation loop. Similarly, incorrect unit predictions for the current task can degrade the decoding performance of the <REF> token. In the first step of VD-CoT, the MLLM performs stepby-step reasoning over the image, including identifying the visual entities, decoding type, and number of instances. During the Decoding Triplets step, since the necessary decoding information has already been established, the triplets are organized according to predefined paradigm, improving the accuracy of the generated content. As shown in Table 9, the introduction of VD-CoT significantly enhances the models reasoning accuracy. B. VT-Instruct Construction Figure 6. Data Distribution Map. VT-Instruct comprises four output unitsbox, keypoint, depth, and maskpaired with either low (phrases) or high (sentences) text complexity, with different visual prompts unified under the same task for clarity. B.1. Definition of Each Downstream Task VT-Instruct includes 7 different downstream tasks, as shown in Figure 7. The Visual Understanding task includes Image Captioning and Visual Question Answering (VQA), involving image-text inputs and text-only outputs. Referring Expression tasks cover Referring Expression Comprehension (REC), Referring Expression Segmentation (RES), and Referring Expression Generation (REG). While REC and RES require models to predict bounding boxes or masks in response to query about specific region in an image, REG involves generating descriptive text from visual inputs like points, boxes, scribbles, or masks. Interactive Grounding (IG) enables users to provide prompts via both text and interactive inputs (e.g., points, boxes, masks), allowing MLLMs to interpret and generate corresponding outputs. Open-Vocabulary Identification focuses on localizing and segmenting objects from descriptive text, even if the object categories were not part of the training data. For traditional Open-Vocabulary tasks performed by current MLLM, it typically requires user inputs specific class names, Grounded Conversation Generation (GCG) produces natural language responses interwoven with bounding boxes or masks, with the GCG task further divided into GCG-box (bounding box outputs) and GCG-mask (mask outputs). B.2. Dataset Construction Details For each task, we select unique prompt-unit pair to develop task-specific instructions. For example, visual understanding task encompasses Image Captioning and Visual Question Answering (VQA), with image-text inputs and pure text outputs. To facilitate MLLMs in comprehending image-level information and addressing diverse questions, we construct conversations for visual understanding tasks using our proposed pipeline with the COCO [30], GranD [45], GRIT [41], VQAv2 [3], and LLaVA-instruct [33] datasets, which collectively comprise over 15 million image-text pairs featuring multi-turn conversations. Referring expression tasks include Referring Expression Comprehension (REC), Referring Expression Segmentation (RES), and Referring Expression Generation (REG). The REC and RES tasks require the model to respond to question or description regarding specific area in an image, predicting bounding boxes or masks. In contrast, the REG task involves inputs such as points, boxes, scribbles, and masks, with the model expected to generate descriptive response based on the visual prompts. We construct conversations for referring expression task from refCOCO [25], refCOCO+ [25], refCOCOg [25], GranD [45], GRIT [30], Osprey [68], Visual Genome [27] datasets with more than 22 million samples. Interactive grounding allows users to provide prompts through both text and interactive elements, such as points, boxes, masks, or scribbles, enabling MLLMs to interpret these inputs and generate corresponding outputs, including bounding boxes or masks. We constructed interactive grounding samples using the COCO-interactive [73] dataset , which contains over 64 million examples. The open-vocabulary identification task focuses on localizing and segmenting objects in an image based on descriptive text prompts, even if the specific object categories were not included in the models training data. To equip REF-VLM with zero-shot capabilities for object detection and segmentationsimilar to traditional open-vocabulary detection models (e.g., YOLO-World [13]) and segmentation models (e.g., SAM [26]) we designed multimodal conversation system using bounding boxes and masks annotations from the GRIT [41], GranD [45], COCOREM [49], ADE20k [76], and Cityscapes [15] datasets, resulting in corpus of over 20 million examples. Grounded conversation generation (GCG) aims to produce natural language responses interwoven with bounding boxes or object segmentation masks. The GCG task is divided into GCGbox, which outputs bounding boxes, and GCG-mask, which outputs masks. We developed these tasks using datasets that Figure 7. Example of VT-Instruct Dataset by Using the Automated Data Construction Pipeline. Our VT-Instruct dataset contains seven distinct downstream tasks, including Visual Understanding, Referring Expression, Interactive Grounding, Grounded Conversation Generation, Open-Vocabulary Identification and Depth Estimation. Table 11. Data statistics of VT-Instruct and actual use of dataset in the training process. Multiple datasets were utilized to train REF-VLM, with some supporting multiple tasks. For instance, GranD [45] enables tasks such as captioning, REC, RES, REG, GCG, and open-vocabulary identification. Most datasets within VT-Instruct were employed as subsets in our training process. Task Visual Understanding Sub-Task Caption VQA Referring Expression REC RES REG Interactive Grounding Grounded Conversation Generation Open-Vocabulary Identification IG-Box IG-Mask IG-Keypoint GCG-box GCG-mask OVD/FOVD OVS/FOVS Keypoint Detection Depth Estimation - - Original Dataset COCO [30], GranD [45], GRIT [41] VQAv2 [3], LLaVA-Instruct [33], VCR [69] RefCOCO [25], RefCOCO+ [25], RefCOCOg [25], GranD [45], GRIT [41] RefCOCO [25], RefCOCO+ [25], RefCOCOg [25], GranD [45] RefCOCO [25], RefCOCO+ [25], RefCOCOg [25], GranD [45], GRIT [41], COCO-Interactive [73], Osprey [68], Visual Genome [27], Visual7W [78] COCO-Interactive [73] COCO-Interactive [73] COCO [30] GRIT [41], GranD [45], Flickr30k-Entities [42] GranD [45], LLaVA-Grounding [71] , PNG [22], OpenPSG [77] GranD [45], GRIT [41], COCO-REM [49] GranD [45], COCO-REM [49], ADE20k [76], Cityscapes [15] COCO [30] Kitti [21] , HRWSI [59], NYU [40] Construction Number Actual Use 15,980,000 1,310,000 22,880,000 780,000 1,310, 880,000 3,880,000 680,000 22,750,000 1,200,000 3,200,000 3,200,000 500,000 15,630, 4,000,000 15,770,000 3,795,000 140,000 150,000 120,000 120,000 140,000 540,000 450, 600,000 600,000 140,000 - be seen in Figure 6 and Table 11. include captions and phrases associated with bounding box or mask annotations, such as Flickr30k-entities [42], GranD [45], GRIT [41], LLaVA-grounding [71], OpenPSG [77], and PNG [22], collectively comprising over 18 million annotations. The example of our dataset can be seen in Figure 7. The total distribution and overall statistics of our dataset can generated by the MLLM in the answer is strictly equal to the number of target instances. When the number of predictions equals the number of targets , the result of the Hungarian Matcher is perfect match, meaning each prediction is paired with unique target. Therefore, we group all <REF> according to different triplets, perform Hungarian matching within each group, and ultimately obtain one-to-one matching result to calculate the loss. C.2. Extend to More Plugins Our model offers high customizability and scalability. It not only supports the use of custom-designed meta plugins as downstream visual decoders to handle dense prediction tasks but also allows integration with existing pre-trained models by loading their weights and fine-tuning with the VT-Instruct dataset to perform variety of vision tasks. Additionally, compared to agent-based MLLMs [19, 35], our model is an end-to-end system, enabling end-to-end training and fine-tuning. In our design of the model, we not only design meta plugins, but also replace our meta plugins with pretrained models such as SAM [26], GroundingDINO [36] and UniPose [4] as detailed in Table 12. Additionally, to train these visual decoders simultaneously, we introduce multimodal, multi-task training paradigm that ensures consistent computational graph across backward passes when heterogeneous input data from multiple tasks are directed to different decoders. During each forward pass, we construct so-called fake tensors, which are essentially null inputs directed to inactive decoders. This approach ensures that every decoder shares the same computational graph, effectively preventing hang-ups during multi-task training. The overall loss of our model mainly come from two components: cross entroy loss from LLM (denoted as LLLM ) and visual decoder loss from each specific downstream decoder (denoted as LDecoder). The overall loss is (λ is the hyperparameter for each loss.): LREF LM = λ0LLLM + (cid:88) i= λiLDecoderi. (9) D. Training Details The training process of REF-VLM is conducted in three stages, during which both CLIP-ViT and CLIP-ConvNeXt are frozen, with no parameter updates. We use eight NVIDIA A800-80GB GPUs in all of our training processes and pick Vicuna-7B as our LLM, CLIP-large-14-336 and CLIPConvNeXt-512 as our visual encoder. The details parameters and datasets of each stage is shown in Table 14 and Table 15. Stage 1. In the first stage, we train the projector to equip the LLM with the capability to understand visual information. REF-VLM adopts the same setup as Shikra [9], freezing all model parameters except for the projector to focus Figure 8. The Comparison of parameter numbers. Our Metaplugins uses 248M parameters, while the External plugins method requires 1028M. C. Implementation Details C.1. Group Hungarian Matcher Hungarian Matcher DETR [6] uses combined cost function to compute the matching cost between predictions and targets. This cost function includes: 1. Box loss: measure of the similarity between predicted and target bounding boxes, typically using GIoU or L1 distance. 2. Classification loss: The cross-entropy loss between the predicted class distribution and the true class labels. The Hungarian algorithm always produces exactly matching pairs, corresponding to the ground truth targets because: The algorithm is designed to match all target boxes to predictions. If there are more predictions (N > ), the algorithm will match the remaining predictions to the no-object class. Thus, the number of matching pairs is always equal to the number of target boxes . Extra predictions are treated as no-object and are not counted in the matching pairs. Group Hungarian Matcher All visual unit decoding plugins in REF-VLM are built on the DETR architecture and benefit from the Phrase-Unit-<REF> triplets structure. Each <REF> token uniquely corresponds to specific phrase and unit, ensuring consistency between visual entity recognition and the visual unit decoding process. In Phrase-Unit-<REF> triplet (the number of <REF> token may be greater than 1), the Phrase represents the category of the <REF> tokens within the group, so there is no need to consider classifying instances (e.g., boxes, masks, etc.) during the decoding process of the reference tokens. In visual decoding tasks, the number of <REF> tokens 5 Table 12. Implementation Details of Each Component in REFVLM. We provide detailed overview of all components used in REF-VLMs actual training, including visual encoders, the LLM, the VPT encoder, and visual decoders. Here, visual decoders (meta) refers to custom-built decoder architectures, while visual decoders (model) refers to existing pretrained architectures. The column weight init specifies the initialization method: scratch indicates that the component is trained from scratch, and pretrained indicates that the component is initialized with pretrained weights. Modules Weight Init Visual Encoder pretrained VPT Encoder scratch Projector scratch LLM pretrained Box Decoder (Meta) scratch Config CLIP-ViT-L CLIP-ConvNeXt-L strategy=pooling patch size=8 num patches=9 use mask token=True use projector=True depth=2 bias=True activation=gelu output dim=4096 Vicuna-1.5-7B use group matcher=True num queries=100 queries input dim=4096 encoder input index=[0,1,2,4] decoder layers=6 model=256 dropout=0.1 bbox loss coef = 5 giou loss coef= Box Decoder (GroundingDINOv2) pretrained weight=dino swint ogc Mask Decoder (Meta) scratch use group matcher=True num queries=30 queries input dim=4096 encoder input index=[0,1,2,4] decoder layers=6 model=256 dropout=0.1 fpn feature size=256 mask feature size=256 mask loss coef=20 dice loss coef= Mask Decoder (SAM) pretrained weight=sam vit Keypoint Decoder (Meta) scratch num queries=100 decoder layers=6 model=256 dropout=0.1 num body points=17 aux loss coef=0.5 oks loss coef=2 cls loss coef= Keypoint Decoder (UniPose) pretrained weight=unipose swin on aligning multimodal data. This stage primarily involves training on visual understanding tasks, such as image captioning and VQA, to improve the projectors performance in 6 visual-text alignment. For object detection tasks like REC and GCG-box, we did not incorporate specific downstream visual decoders at this stage. Instead, following the approach in [9], we treated these numeric box inputs as strings to provide the model with sense of spatial awareness. The first stage was trained for approximately two days with learning rate of 1e 5 using the AdamW optimizer on 64 A800 GPUs, with batch size of 16 per GPU. Stage 2. In the second stage, We train fundamental MLLM combined with different visual plugins. We introduce VPT encoder as our encoder visual plugin, box decoder and mask decoder as our decoder visual plugin. REF-VLM is trained using the VT-Instruct data that we constructed as shown in Section 3.3, updating parameters for all modules except the keypoint decoder. The goal of this stage is to train the LLM and various visual plugins using large-scale data, while the keypoint decoder is excluded from training due to its strong correlation with the box decoder. In this stage, we jointly trained both the encoder and decoder visual plugins to enable the REF-VLM has the ability of predicting boxes and masks with different types of interactive inputs to perform various kinds of downstream tasks. For the VPT encoder, we randomly initialize its parameters and train it together with the LLM and decoder. Additionally, we use projector to align the output dimensions of the VPT encoder with the input dimensions of the LLM. The parameters of this projector are shared with the projector used between the visual encoder and the LLM. For the box decoder and mask decoder, we applied DETR-like architecture for box decoder and MaskFormer-like architecture for mask decoder. We trained all these decoders from scratch without using pretrained models, as we aimed to ensure greater cohesiveness in our model. This approach allows for the use of unified visual encoder, making deployment more convenient and avoiding excessive computational overhead. We set the learning rate to 2e 6 in the second stage and trained the model on A800 GPUs with batch size of 16 per GPU for about 9 days. Stage 3. In the third stage, we train additional visual plugins based on our pretrained foundational MLLM from stage 2 to demonstrate the extensibility of our REF-VLM. During this stage, REF-VLM continued training on the VTInstruct dataset, updating all modules with newly appended keypoint decoder. The keypoint decoder was initialized with the weights of the box decoder from the second stage. We set the learning rate to 2e 5 and trained the keypoint decoder for 5 epochs, which took approximately 10 hours using batch size of 16 per GPU. Furthermore, after completing all training processes, we replaced the box decoder with GroundingDINO, the mask decoder with SAM, and the keypoint decoder with UniPose. We then repeated the stage 3 training process to finetune the entire MLLM along with the newly integrated visual Table 13. Comparison of different plugin backbones without decoders. We compared PixelLM [61], GLaMM [45], VisionLLMv2 [56], and our REF-VLM in terms of the parameter count of the backbone used for feature extraction in the visual decoder module and whether it is frozen during training. The backbone refers to the image encoder and prompt encoder components within the models decoder module. Freeze backbone indicates whether this modules parameters are frozen during the training process of the entire MLLM. Model Visual Backbone Backbone Parameters (M) Total Parameters (M) Freeze Backbone PixelLM [46] GLaMM [45] SAM-ViT-L CLIP-ViT-H 308M 986M 312M 1006M False True VisionLLMv2 [56] REF-VLM DINOSwin-T+UniPoseSwin-T ConvNeXt-L 319M 387M False 199M 248M True Table 14. Training details of REF-VLM in 3 stages. For input resolution, 336 means the input resolution for CLIP-ViT is 336 336, 336+512 means the input resolution for CLIP-ViT is 336336, and the input resolution for CLIP-ConvNeXt is 512512. In Stage 3, we applied multiple extensiv visual plugins such as SAM [26], Grounding DINO and UniPose to demonstrate that our REF-VLM is extensible to more common architectures. Config visual encoder vpt encoder projector LLM box decoder (meta) mask decoder (meta) keypoint decoder (meta) box decoder (gDINO) mask decoder (SAM) keypoint decoder (UniPose) learning rate optimizer warmup ratio weight decay max norm input resolution batch size per GPU numerical precision GPUs for training Stage1 frozen - unfreeze frozen - - - - - - 1e-5 AdamW 0.03 0 1 3362 16 bfloat16 64A800 (80G) Stage2 frozen unfreeze unfreeze unfreeze unfreeze unfreeze - - - - 2e-6 AdamW 0.03 0 1 3362+5122 16 bfloat16 64A800 (80G) Stage3-Keypoint frozen unfreeze unfreeze unfreeze - - unfreeze - - - 2e-5 AdamW 0.03 0 1 3362+5122 16 bfloat16 8A800 (80G) Stage3-gDINO frozen unfreeze unfreeze unfreeze - - - unfreeze - - 2e-5 AdamW 0.03 0 1 Stage3-SAM Stage3-UniPose frozen unfreeze unfreeze unfreeze - - - - unfreeze - 2e-5 AdamW 0.03 0 frozen unfreeze unfreeze unfreeze - - - - - unfreeze 2e-5 AdamW 0.03 0 1 16 bfloat16 8A800 (80G) 16 bfloat16 8A800 (80G) 16 bfloat16 8A800 (80G) Table 15. Summary of datasets used in the whole training process. Stage-1 datasets focus on visual understanding tasks, while in Stage-2 and Stage-3 we not only train the visual understanding tasks such as image captioning, VQA, but also target specific downstream density prediction tasks. Training Stage Stage Stage2 Datasets Visual Genome [27], Visual7W [78], llava-Instruct [33],VQAv2 [3], COCO [30], Flickr30k-Entities [42], RefCOCO [25], RefCOCO+ [25], RefCOCOg [25], VCR [69] COCO [30],VQAv2 [3], GranD [45], GRIT [41], RefCOCO [25], RefCOCO+ [25], RefCOCOg [25], COCO-Interactive [73], Osprey [68], Visual Genome [27], Visual7W [78], Flickr30k-Entities [42], LLaVA-Grounding [71], PNG [22], OpenPSG [77] Stage3-Keypoint COCO-Keypoint [30] Stage3-gDINO Stage3-SAM GranD [45], GRIT [41], RefCOCO [25], RefCOCO+ [25], RefCOCOg [25] GranD [45], RefCOCO [25], RefCOCO+ [25], RefCOCOg [25], COCO-REM [49], ADE20k [76], CityScapes [15] COCO-Keypoint [30] Stage3-UniPose plugins separately. This demonstrated that our model could not only accommodate custom-designed decoders trained from scratch but also effectively leverage state-of-the-art (SOTA) visual decoders. For each separate training process for different visual decoders, We set the learning rate to 2e 5 and trained the decoder for 5 epochs, using batch 7 Table 16. Total Data Volume and Data Ratios Across Different Training Stages. We present the data ratios used in the three distinct training stages. In each stage, the model undergoes joint training on these datasets to develop multi-task capabilities. Task Visual Understanding Referring Expression Grounded Conversation Generation (GCG) Interactive Grounding Open-Vocabulary Identification Keypoint Detection Total Number Stage1 55.85% 25.58% 20.29% - - - 4,241,680 Stage2 26.22% 40.57% 14.01% 2.90% 16.30% - 8,091, Stage3-Keypoint - - - - - 100% 109,289 Stage3-gDINO Stage3-SAM Stage3-UniPose - 32.91% 23.03% - 44.06% - 2,165,892 - 33.41% 26.51% - 40.08% - 1,545,982 - - - - - 100% 109,289 size of 16 per GPU. E. Freeform Open-Vocabulary Identification Compared to existing MLLMs [56, 58, 73], our REF-VLM offers greater flexibility and freedom in Open-Vocabulary Identification tasks. We therefore propose new, more flexible task format called Freeform Open-Vocabulary Identification and introduce corresponding evaluation metric, mAP Similarity (mAPS), specifically designed for this task. E.1. Task Definition In many open-vocabulary identification tasks that current MLLMs can perform [56, 58, 73], users are typically required to input relevant category information about the image in the prompt. This category information serves as input features for the prompt encoder, allowing the downstream visual decoder to execute detection and segmentation tasks based on the prompt. However, the VT-Instruct dataset adheres to the flexibility of the REF-VLM model when constructing such tasks. Users only need to provide simple prompt like Please segment/detect the objects in the image <image>. to perform downstream detection and segmentation tasks. This approach eliminates the need for specific category information in the prompt, offering greater freedom and flexibility for these tasks. Therefore, we propose new type of Open-Vocabulary Identification task called the Freeform Open-Vocabulary Identification task. In this task, open-set detection is carried out with enhanced flexibility, allowing users to omit specific category details in the prompt. E.2. mAP Similarity (mAPS) Instead of the calculating mAP as our evaluation metric for Open-Vocabulary Identification tasks, we propose new metric called mAP Similarity (mAPS) to evaluate our REFVLM performance. For traditional open-vocabulary models, they typically predict classes with logit score by their classification head. However, instead of applying classification head for each task, our REF-VLM leverages large language model (LLM) to predict classes without generating any class logits. We therefore compute the similarity score between REF-VLMs class predictions and all ground truth class names. We then assign the class label based on the highest similarity, using this similarity score in place of the traditional confidence score. For the implementation of mAPS, we define the phrases predicted by the LLM as pi p1, p2, p3, . . . , pk, where denotes the number of LLM predictions. The ground truth classes are denoted as ci c1, c2, c3, . . . , cn, where is the total number of ground truth classes for the dataset. We first use the CLIP-Large-14-336 model to compute the text embeddings e, as shown in Equation (10). Next, we compute the cosine similarity score between each pi and ci as in Equation (11). The class of our predicted phrase is assigned based on the maximum similarity score and its corresponding index, which also serves as the logit score for the prediction. epi = CLIP(pi), eci = CLIP(ci). (10) smax, idmax = max (Cosine Similarity(epi, eci)). (11) F. More Results F.1. More Experimental Results Referring Expression For the Referring Expression Segmentation (RES) task, we evaluate REF-VLM on the RefCOCO, RefCOCO+, and RefCOCOg test and validation datasets by calculating the cumulative IOU (cIOU) as proposed by [32]. REF-VLM with meta plugins, trained from scratch, achieves results in both zero-shot and fine-tuned settings that are comparable to recent methods like LISA [28], which utilized pretrained backbones such as SAM (see Table 20). The results show that the performance of our REF-VLM trained from scratch is slightly lower than that of PixelLM [46], primarily due to PixelLMs use of the CLIP-ViT-H visual encoder, which has significantly more parameters than ours (see Table 13). Additionally, to demonstrate that REF-VLM is not only capable of using customdesigned components but also can extend to current VGMs, we employed SAM as an external plugin for our mask decoder. By loading the pretrained weights from SAM and 8 Table 17. Prompt template for evaluating different kind of tasks. For different evaluation tasks, we utilized distinct prompt templates. During the actual training process, to ensure the models generalization across tasks, we constructed at least 100 templates for each subtask. Task Caption VQA GCG-Mask GCG-Box REC RES REG IG-Mask FOVD FOVS Keypoint Detection"
        },
        {
            "title": "Template",
            "content": "<image>Please describe the image in detail. Please take look at the image <image>and promptly provide an answer for <question>. Describe the setting of the image <image>and offer masks for each visible object. Please describe the image <image>and detect relevant bounding boxes. What are the coordinates of <referring expression>in the image<image>? Provide segmentation mask for <referring expression>in the picture <image>. For the given image <image>, can you provide unique description of the area <mask>? Please generate mask based on the region <region>in the image <image>. Please detect bounding boxes in the image<image>. Please segment objects in the image<image>. Please detect all the people and visualize all the keypoints in the image<image>. Table 18. Comparison of interactive grounding performance on segmentation task. The task is evaluated on the COCO-Interactive [73] validation dataset. The evaluation metrics for interactive grounding are mIoU and cIoU. Model Decoder Scratch Type SAM-B [26] SAM-L [26] SEEM-B [80] PSALM [73] VisionLLMv2 [56] REF-VLM (meta) REF-VLM (external) - - - Mask2Former GroundingDINO Mask Decoder SAM (cid:34) (cid:34) (cid:34) (cid:34) VGM MLLM Point Scribble Box Mask mIoU cIoU mIoU cIoU mIoU cIoU mIoU cIoU 48.7 51.8 47.8 64.3 65.4 62.8 65.6 73.7 76.6 44.9 67.3 74.2 71.2 74. - - 48.4 67.6 67.9 66.3 68.2 68.7 71.6 42.1 80.9 83.2 73.7 84.6 - - 43.0 66.9 66.8 59.8 68.3 33.6 37.7 57.8 74.0 70.9 70.4 75.2 - - 65.0 82.4 83.8 77.5 83.7 - - 44.0 80.0 77.2 60.2 79. Table 19. Comparison of Referring Expression Comprehension (REC) performance. REC is evaluated by IOU@0.5 Accuracy. VGM represents vision generalist model and MLLM represents multimodal large language model. Model Type OFA-L [51] MAttNet [67] UNITER [10] VILLA [20] MDETR [24] GroundingDINO-T [36] GroundingDINO-L [36] Kosmos-2 [41] Shikra [9] Ferret [66] NeXT-Chat [70] MiniGPTv2-7B [8] Qwen-VL-7B [5] VistaLLM [43] VisionLLMv2 [56] LION-12B [7] REF-VLM VGM MLLM RefCOCO Test-A Test-B Val 76.4 83.7 80.0 80.4 81.4 87.0 82.4 87.5 86.8 89.6 89.2 91.9 90.6 93.2 52.3 57.4 87.0 90.6 87.5 91.4 85.5 90.0 88.1 91.3 88.6 92.3 88.1 91.5 90.0 93.1 89.8 93.0 90.8 93.7 76.4 69.3 74.2 74.8 81.4 86.0 88.2 47.3 80.2 82.5 77.9 84.3 84.5 83.0 87.1 85.6 89.1 RefCOCO+ Test-A Test-B Val 68.3 76.0 64.9 70.3 75.9 81.5 76.2 81.5 79.5 84.1 81.1 87.4 82.8 89.0 45.5 50.7 81.6 87.4 80.8 87.4 77.2 84.5 79.6 85.5 82.8 88.6 82.9 89.8 81.1 87.3 84.0 89.2 81.7 88.3 61.8 56.0 66.7 66,8 70.6 74.7 75.9 42.2 72.1 73.1 68.0 73.3 76.8 74.8 74.5 78.1 77.6 RefCOCOg Val Test 80.0 67.6 76.4 67.0 74.0 68.7 76.2 76.7 81.6 80.9 85.2 84.9 86.1 87.0 60.6 61.7 82.3 82.2 83.9 84.8 80.1 79.8 84.2 84.3 86.0 86.3 83.6 84.4 83.9 84.8 85.5 85.7 87.0 87.1 Table 20. Comparison of Referring Expression Segmentation (RES) performance. The Decoder column refers to the visual decoder utilized by the MLLM for performing RES tasks. indicates that the visual decoder is custom-designed and trained from scratch. The performance of RES is evaluated using cumulative IoU (cIoU) as proposed by [32]. Model Decoder Sractch Type MCN [37] VLT [17] RELA [32] X-Decoder [79] SEEM-L [80] UNINEXT-H [63] GLEE-Pro [55] LISA-7B [28] PixelLM [46] PixelLLM [61] AnyRef NExT-Chat [70] VITRON [19] GroundHOG [72] GLaMM [45] VisionLLMv2 [56] REF-VLM (meta) REF-VLM (external) - - - - - - - SAM Mask Decoder SAM SAM SAM SEEM Mask2Former SAM GroundingDINO Mask Decoder SAM (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) (cid:34) VGM MLLM RefCOCO Test-A Test-B Val 62.4 64.2 67.5 70.5 73.8 76.5 - - - - 82.2 83.4 80.0 - 74.9 72.3 73.0 76.5 76.9 78.5 76.9 79.9 74.7 78.9 74.4 78.7 78.5 79.9 79.5 83.2 79.2 82.3 69.0 73.4 81.2 82.9 59.7 65.2 70.2 - - 81.3 - 79.1 68.2 74.4 74.2 69.5 71.6 75.7 76.9 77.0 63.9 76.8 RefCOCO+ Test-A Test-B Val 50.6 55.0 56.3 61.0 66.0 71.0 - - - - 72.5 76.4 69.6 - 65.1 70.8 66.3 71.7 69.2 72.1 70.3 73.5 65.1 71.9 66.3 72.1 70.5 75.0 78.7 72.6 68.9 75.8 62.3 70.8 73.1 77.6 44.7 50.1 57.7 - - 66.2 - 58.1 58.3 64.5 61.8 56.7 57.8 64.9 64.6 61.8 56.2 63.4 RefCOCOg Val Test 49.2 49.4 55.0 57.7 65.0 66.0 64.6 - 65.7 - 74.7 76.4 72.9 - 67.9 70.6 69.3 70.5 70.7 72.4 70.0 70.7 67.0 67.0 67.2 67.3 74.1 74.6 74.2 74.9 73.3 74.8 65.0 65.8 74.6 75.0 fine-tuning it on our VT-Instruct datasets, we found that REFVLM with the external plugin outperforms current MLLMs such as VisionLLMv2 [56], GLaMM [45], and demonstrates comparable performance to the Generalist Model such as UNINEXT-H [63]. For the Referring Expression Comprehension (REC) task, we compare our REF-VLM with current MLLMs capable of generating referring boxes based on specific prompts in both zero-shot and fine-tuned settings. The metric used for REC evaluation is IoU@0.5. As shown in Table 19, REFVLM demonstrates superior performance in the REC task compared to other MLLMs. Table 21. Comparison of different configurations of VPT encoders on RefCOCOg validation dataset. Method Osprey [68] Use Mask Token No Mask Token CIDEr Meteor 78.5 77.9 78.8 12.0 12.0 12.1 Params 6.55M 1024 Table 22. Comparison of different configurations of Matching Strategy on COCOREM Test dataset. Method No-Matcher Use Matcher AP50 AP75 AP[50:95] AR[50:95] 12.4 12.7 16.4 19.3 1.4 2.6 4.0 4. Interactive Grounding For this task, we evaluate using Please generate mask based the prompt, on the region <region> in the image <image>. where <region> is replaced with visual prompts such as points, scribbles, boxes, or masks. The results presented in Table 18 show that our REF-VLM with meta plugins outperforms both SAM [26] and SEEM-B [80] across point, scribble, box, and mask settings, achieving performance comparable to PSALM, which utilizes pretrained Swin-T and Mask2Former weights in these configurations. Furthermore, our REF-VLM with external plugins achieves superior performance compared to VisionLLMv2 [56] and PSALM [73]. F.2. More Ablation Results Different Configuration of VPT Encoding Strategy. Our parameter-free VPT encoder configuration outperforms alternatives as shown in Table Table 21. Without mask tokens, our approach achieves the best CIDEr (78.8) and Meteor (12.1) scores with zero additional parameters, using blockwise dot products between masks and visual features with position encoding and shared projectors. Conversely, Osprey [68] uses unblocked dot products (N=1) and separate projections, requiring 6.55M parameters while yielding inferior results. Our efficient design delivers both superior 10 performance and exceptional cross-architecture adaptability. Ablation of Group Hugrain Matcher We conduct experiments with Group Hungarian Matching on the more challenging task of freeform object detection. Both experiments are trained for 5 epochs on the COCO-REM dataset, with comparative results shown in Table 1. The Group Matcher demonstrates superior performance across AP and AR metrics, particularly showing significant improvement in AP75, indicating its effectiveness in enhancing box regression accuracy. G. More Visualization Results 11 Prompt Provide simple description of this photograph. would like to hear more thorough description. VD-CoT <Task> Unit decode (False). </Task> Answer with Triplets group of people, including children, are gathered on brick wall, some holding umbrellas and backpacks while others wear dresses, sunhats, and shirts. man wearing hat and pink-shirted person are also visible. In the background, large brick building can be seen, and the scene outside provides view. Table 23. An Example of VD-CoT Applied to the Visual Caption Task.In this task, only textual responses are required, and no operations on image information are needed. Therefore, in the generation of VD-CoT, Unit decode is set to False, and no visual-related special tokens are generated. Similarly, in the result of Answer with Triplets, there are no decoded triplets related to visual content. Prompt Please identify each object in the image. VD-CoT <Task> Unit decode (True). Class name, target unit and number: - Name: car Unit: box Num: 10 - Name: skyscraper Unit: box Num: 1 - Name: barricade Unit: box Num: 1 - Name: city Unit: box Num: 1 - Name: street Unit: box Num: 1 </Task> Answer with Triplets <Phrase>car</Phrase> (<Unit>box</Unit>[0]<REF>[1]<REF>[2]<REF>[3]<REF> [4]<REF> [5]<REF>[6]<REF>[7]<REF>[8]<REF>[9]<REF>), <Phrase>skyscraper</Phrase> (<Unit>box</Unit>[0]<REF>), <Phrase>barricade</Phrase> (<Unit>box</Unit>[0]<REF>), <Phrase>city</Phrase> (<Unit>box</Unit>[0]<REF>), <Phrase>street</Phrase> (<Unit>box</Unit>[0]<REF>). Table 24. An Example of VD-CoT Applied to the Object Detection Task. VD-CoT analyzes the visual content and generates decoding triplets to identify objects in the image. The special tokens and references have been included for clarity. 12 Prompt Delineate areas of the image into semantic segments. VD-CoT <Task> Unit decode (True). Class name, target unit and number: - Name: couch Unit: mask Num: 1 - Name: computer keyboard Unit: mask Num: 1 - Name: mask Unit: mask Num: 2 - Name: person Unit: mask Num: 2 - Name: sofa Unit: mask Num: 1 - Name: monitor (computer equipment) computer monitor Unit: mask Num: 1 </Task> Answer with Triplets <Phrase>couch</Phrase> (<Unit>mask</Unit>[0]<REF>), <Phrase>computer keyboard</Phrase> (<Unit>mask</Unit>[0]<REF>), <Phrase>mask</Phrase> (<Unit>mask</Unit>[0]<REF>[1]<REF>), <Phrase>person</Phrase> (<Unit>mask</Unit>[0]<REF>[1]<REF>), <Phrase>sofa</Phrase> (<Unit>mask</Unit>[0]<REF>), <Phrase>monitor (computer equipment) computer monitor</Phrase> (<Unit>mask</Unit>[0]<REF>). Table 25. An Example of VD-CoT Applied to the Open-Vocabulary Segmentation Task. VD-CoT analyzes the visual content and generates decoding triplets for semantic segmentation of the image. Special tokens and references are provided for clarity. Prompt Please pinpoint the egg yolk in the image and provide the dimensions of its bounding box. VD-CoT <Task> Unit decode (True). Class name, target unit and number: - Name: target Unit: box Num: 1 </Task> Answer with Triplets <Phrase>target</Phrase> (<Unit>box</Unit>[0]<REF>). Table 26. An Example of VD-CoT Applied to the Referring Expression Comprehension Task. VD-CoT analyzes the visual content and generates decoding triplets to identify the target (egg yolk) and its bounding box dimensions in the image. 13 Prompt How do you create segmentation mask for white boat in the water in the image? VD-CoT <Task> Unit decode (True). Class name, target unit and number: - Name: target Unit: mask Num: 1 </Task> Answer with Triplets <Phrase>target</Phrase> (<Unit>mask</Unit>[0]<REF>). Table 27. An Example of VD-CoT Applied to the Referring Expression Segmentation Task. VD-CoT analyzes the visual content and generates decoding triplets to identify the target (white boat) and creates its segmentation mask in the image. Prompt What defining features can you describe for the area [VPT] in the image? VD-CoT <Task> Unit decode (False). </Task> Answer with Triplets The cabinet is closed. Table 28. An Example of VD-CoT Applied to the Referring Expression Generation Task. VD-CoT analyzes the visual content and generates descriptive features for the given area [VPT] in the image. Prompt Segment the region based on the region [VPT] in the image, and illustrate the resulting mask. VD-CoT <Task> Unit decode (True). Class name, target unit and number: - Name: boat Unit: mask Num: 1 </Task> Answer with Triplets <Phrase>boat</Phrase> (<Unit>mask</Unit>[0]<REF>). Table 29. An Example of VD-CoT Applied to the Interactive Grounding Task. VD-CoT analyzes the visual content, segments the region [VPT], and generates mask for the identified object (boat) in the image. Figure 9. The Visual Understanding Results of REF-VLM. 15 Figure 10. The Detection Results of REF-VLM. The text color in the models responses corresponds to the bounding box colors of the detected objects in the images. For example, in the top-right image, the model detects two categories: person and dress. The person category contains two instances, represented by [0]<REF> and [1]<REF>, while the dress category contains one instance, represented by [0]<REF>. 16 Figure 11. The Segmentation Results of REF-VLM. The figure illustrates the segmentation and GCG segmentation outputs generated by REF-VLM. The text corresponds to the mask colors of the segmented objects in the images. For example, in the top-left image, the model segments three categories: stool, person, and grassy field. The person category contains two instances, represented by [0]<REF> and [1]<REF>, while the stool category contains one instance, represented by [0]<REF>. The background, labeled as grassy field, is also represented by [0]<REF>. Figure 12. The Grounding Detection Results of REF-VLM. We use the textual prompts and visual box prompts for grounding detection tasks. In the first row (left), textual prompt instructs the model to locate the object bear mentioned in the sentence. The second row displays the result, where the model successfully detects the location of the bear using bounding box. In the first row (right), bounding box around bird is provided as visual prompt. The textual query uses the special token <area> to refer the boxed region in the image. The second row (right) shows the models output, correctly identifying the object in the boxed region as bird. 18 Figure 13. The Grounding Segmentation Results of REF-VLM. We use the textual prompts and visual box prompts for grounding segmentation tasks. In the first row (left), textual prompt instructs the model to segment the middle bird in the center of the image. The second row (left) shows the segmentation result for the middle bird produced by the model based on the textual prompt. In the first row (right), bounding boxes around two birds are provided as visual prompts, and the textual prompt includes special symbol <area> to indicate the region for segmentation. The second row (right) displays the segmentation results for the left bird and right bird based on two types of prompts."
        }
    ],
    "affiliations": [
        "Hong Kong Polytechnic University",
        "Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China",
        "Shanghai Jiao Tong University",
        "Zhejiang University"
    ]
}