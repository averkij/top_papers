{
    "paper_title": "ConfQA: Answer Only If You Are Confident",
    "authors": [
        "Yin Huang",
        "Yifan Ethan Xu",
        "Kai Sun",
        "Vera Yan",
        "Alicia Sun",
        "Haidar Khan",
        "Jimmy Nguyen",
        "Mohammad Kachuee",
        "Zhaojiang Lin",
        "Yue Liu",
        "Aaron Colak",
        "Anuj Kumar",
        "Wen-tau Yih",
        "Xin Luna Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present a fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers a question correctly, it is trained to continue with the answer; otherwise, it is trained to admit \"I am unsure\". But there are two key factors that make the training highly effective. First, we introduce a dampening prompt \"answer only if you are confident\" to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQA's confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 0 3 7 0 . 6 0 5 2 : r ConfQA: Answer Only If You Are Confident Yin Huang1,, Yifan Ethan Xu1, Kai Sun1, Vera Yan1, Alicia Sun2, Haidar Khan1, Jimmy Nguyen1, Mohammad Kachuee1, Zhaojiang Lin1, Yue Liu1, Aaron Colak1, Anuj Kumar1, Wen-tau Yih2, Xin Luna Dong1 1Meta Reality Labs, 2FAIR at Meta Work done at Meta Can we teach Large Language Models (LLMs) to refrain from hallucinating factual statements? In this paper we present fine-tuning strategy that we call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across multiple factuality benchmarks. The core idea is simple: when the LLM answers question correctly, it is trained to continue with the answer; otherwise, it is trained to admit \"I am unsure\". But there are two key factors that make the training highly effective. First, we introduce dampening prompt\"answer only if you are confident\"to explicitly guide the behavior, without which hallucination remains high as 15%-25%. Second, we leverage simple factual statements, specifically attribute values from knowledge graphs, to help LLMs calibrate the confidence, resulting in robust generalization across domains and question types. Building on this insight, we propose the Dual Neural Knowledge framework, which seamlessly select between internally parameterized neural knowledge and externally recorded symbolic knowledge based on ConfQAs confidence. The framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%. Date: June 2, 2025 Correspondence: Yin Huang at maggiehuang@meta.com"
        },
        {
            "title": "1 Introduction",
            "content": "Despite the remarkable capabilities that Large Language Models (LLMs) have demonstrated, hallucination of factual statements remains challenge (Maynez et al., 2020; Zhou et al., 2021; Ji et al., 2023a). It has been broadly realized that factual information shall not be fabricated or generalized, instead shall be anchored in internally parameterized neural knowledge or externally recorded symbolic content (stored in knowledge graphs, webpages, or other repositories). Significant progress has been made in both knowledge internalization through pre-training (Grattafiori et al., 2024) and external knowledge utilization via Retrieval-Augmented Generation (RAG) (Wei et al., 2021; Yu et al., 2022; Gao et al., 2024; Fan et al., 2024; Huang and Huang, 2024). However, critical question remains: when should LLMs rely on parameterized knowledge versus external sources? In the absence of external data, we wish the LLM to state only reliably parameterized knowledge and eliminate hallucinations; in the presence of external data, we wish the LLM to conduct retrieval wisely to reduce latency, save resources, and avoid distractions (Jiang et al., 2023). In this paper, we study this problem through three questions: Q1. Does an LLM know what it knows? Q2. Can we teach LLMs to refrain from hallucinations? Q3. What is the optimal strategy for RAG triggering? We conducted extensive experimental studies to answer these questions and made three contributions. Our first contribution is to demonstrate that an LLM does possess sense of what it knows, but it tends to be over-confident (Q1). Whereas higher confidence generally correlates with higher answer accuracy, the confidence is poorly calibrated: when Llama-3.1-70B predicts confidence score above 80% on CRAG (Yang et al., 2024), the real accuracy is only 33%. In contrast, answer consistency across multiple trials aligns much Figure 1 Overall factuality improvement of our Dual Neural Knowledge framework. Our fine-tuned model ConfQA reduces hallucination to under 5%; when combined with RAG, we increase accuracy by 45% on average while cutting latency by selective RAG triggering. more closely with accuracy, but the prohibitively expensive computation and latency limit its practicality (Section 4). Our second contribution is an effective fine-tuning method for hallucination suppression, called ConfQA (Q2). Our training data checks the LLMs inherent answer to question, and teaches it to state am unsure if the answer is incorrect. Though seemingly simple, there are two key ingredients to make the training effective. First, we discover an important dampener promptAnswer only if you are confident, which plays crucial role in guiding the LLM behavior. Second, the training data comprise exclusively of simple questions that inquire about an attribute of an entity; by concentrating the regularization on simple factual statements, which serves as building blocks to more complicated factual statements, the training teaches LLMs to generalize this behavior. ConfQA is able to reduce hallucinations to under 5% on various benchmarks (see Figure 1). Our method exhibits strong transferability: although trained solely on questions derived from DBPedia, similar improvements are observed across other domains (e.g., IMDb), benchmarks (CRAG (Yang et al., 2024), SimpleQA (Wei et al., 2024a)), and long-form answers (LongFact (Wei et al., 2024b), Biography (Min et al., 2023)). Additionally, the fine-tuned model maintains its performance on general benchmarks such as MMLU (Hendrycks et al., 2021)(Section 5). Our final contribution is an information-providing framework that we call the Dual Neural Knowledge (DualKnowl) framework (Q3). The framework simultaneously invokes both ConfQA and RAG pipeline in parallel, but it halts the RAG pipeline early unless one of the two conditions is met: (1) when question requests dynamic informationfacts that may change over minutes, days, or years (Vu et al., 2023), or (2) when the ConfQA model responses with am unsure to factual question. Our empirical study shows that DualKnowl achieves QA accuracy comparable to always invoking RAG, while reducing latency by over 600ms on CRAG (Yang et al., 2024) (Section 6)."
        },
        {
            "title": "2 Related Work",
            "content": "Hallucination, with systematic taxonomy introduced in Tonmoy et al. (2024), has been listed as top open problem for Generative AI (Cao et al., 2023). There are two bodies of work on hallucination reduction: LLM post-training and RAG. We next discuss each in detail. RAG with external knowledge: Retrieval-augmented generation (RAG) (Lewis et al., 2020; Shi et al., 2024) has been extensively researched in academia and widely applied in industry to enhance LLMs capability to answer factual questions accurately; there have been numerous surveys on RAG, such as Wei et al. (2021), Yu et al. (2022), Gao et al. (2024), Fan et al. (2024), Huang and Huang (2024). What is closely related to our 2 Benchmark Category Question types Head-to-Tail (Sun et al., 2023a) SimpleQA (Wei et al., 2024a) CRAG (Yang et al., 2024) LongFact (Wei et al., 2024b) AlpacaFact (Lin et al., 2024) Biography (Min et al., 2023) MMLU 5-shot (Hendrycks et al., 2021) MMLU pro (Wang et al., 2024) short-form short-form short-form long-form long-form long-form general knowl. multi-choice questions general knowl. multi-choice questions simple questions (attribute of entities) general fact-seeking questions simple questions, reasoning questions general questions fack-seeking instruction-following biography questions # Domain dbpedia, imdb multiple domains 5 domains 38 domains multiple domains celebrity 57 domains multiple domains Size 1,200 4,326 642 250 241 183 14,042 12,032 Table 1 The overall statistics of evaluation datasets. work is RAG triggering strategies. Su et al. (2024) propose strategy to trigger RAG when the generated token has high entropy or high self-reported attention. Similarly, Jiang et al. (2023) propose to use LLM internalized knowledge for generation, and apply RAG when the confidence of the next token is low. Peng et al. (2023) proposes system that conducts retrieval and answer generation iteratively, revising the LLM prompts to improve model responses using the factuality score from an automatic verifier. These methods all use token-level confidence, and often need multiple retrievals sequentially. Our work focuses on fact-level confidence, can apply in situations when token-level confidence is unavailable, and typically require single retrieval. LLM fine-tuning: Training-based hallucination mitigation has been surveyed in Tonmoy et al. (2024). There are two directions for training: enriching the parameterized knowledge, and suppressing hallucinations. lot of study focuses on knowledge enrichment during post-training, such as using ground truths in QA data sets (Zhang et al., 2024), leveraging trustworthy sources and especially knowledge graphs (Bayat et al., 2023; Ji et al., 2023b), constructing factuality preference pairs for DPO (Rafailov et al., 2024) based on long form generation, and distilling from larger models (Elaraby et al., 2023). Recently it was discovered that feeding knowledge in post training would encourage the LLM to ignore what it learns during pre-training, thus leading to even more hallucinations (Lin et al., 2024; Gekhman et al., 2024). This is confirmed in our experiments. On the other hand, lot of research focuses fine-tuning on hallucination suppression. Sun et al. (2023b) teaches LLMs to recite factual passages to avoid hallucination. Dhuliawala et al. (2023) verifies responses with internalized knowledge before final generation. Tian et al. (2023) generates factuality preference rankings to favor factual statements consistent with external sources or internal knowledge. Xie et al. (2025) trains factuality evaluator to provide LLM generators with claim-level factuality feedback. few works focused on teaching LLMs about its confidence on factual statements, including pre-LLM work (Mielke et al., 2022; Kadavath et al., 2022) and recent work based on local intrinsic dimension (Yin et al., 2024). The works that are closest to ours are Cheng et al. (2024) and Zhang et al. (2024), both instructing the LLM to refuse to answer questions where it has low confidence about the answer. R-Tuning (Zhang et al., 2024) identifies such questions according to answer correctness and pads sure or unsure to the end of the answer. Cheng et al. (2024) in addition requires providing the correct answer consistently. Our ConfQA training is tremendously different in two ways: first, we use the dampener prompt, which reduces hallucination further by 5-11% in our empirical study; second, we focus on simple factual questions from the DBPedia knowledge graph, which increases factuality by up to 30%. Our experiments also show that requiring consistency in addition can cause large correctness regression. We present the experimental comparison in detail in Section 5."
        },
        {
            "title": "3 Experiment Setup",
            "content": "Data sets: As we focus on factuality, we wish to reduce hallucination on factual statements, without regressing performance on general knowledge and problem-solving tasks. We thus consider benchmarks in three categories: 1) Short-form factuality benchmarks, where the answers are mostly short; the question include both simple questions regarding an attribute of an entity, and complex ones that require comparison, aggregation, reasoning, and post-processing; 2) Long-form factuality benchmarks, where answers are expected to be long and contain multiple factual statements; 3) General knowledge benchmarks, which focuses on general knowledge and 3 reasoning in diverse disciplines. Under these three categories we considered seven benchmarks, where Table 1 summarizes the benchmarks and Appendix gives details. Metrics and evaluation: We use slightly different metrics to measure the factuality of answers generated by LLMs. For short-form factuality benchmarks, we compute the percentage of correct, incorrect, and missing (not attempted) answers, and then define two top-line metrics. Factuality-score: Following the CRAG benchmark (Yang et al., 2024), we compute Factuality = correct% - incorrect%, which ranges from -1 to 1. F1-score: Following the SimpleQA benchmark (Wei et al., 2024a), we compute F1-score as harmonic mean of precision and recall, where Precision = correct% / (correct% + incorrect%), and Recall = correct%. F1-score ranges from 0 to 1. Between the two metrics, F1-score is more lenient for incorrect answers (hallucinations), but factuality strongly prefers missing answers to hallucinations (see examples in Section B). We decide the correctness of an answer by prompting Llama-3.1-405B to compare generated answer with the ground truth. We use SimpleQA evaluation prompt 1 for the SimpleQA benchmark, and use Prompt 2 in Appendix for other benchmarks. Similar to Yang et al. (2024), we observe 99% accuracy in the judgment. For long-form responses we use the automatic evaluation metric, VeriScore (Song et al., 2024), which computes precision, recall, and F1-score. The main difference from short-form questions is that we set the minimum number of facts required for perfect recall based on the median number of extracted claims per dataset, using their fine-tuned models for claim extraction and verification. For general knowledge benchmarks, MMLU provides ground truths for the multi-choice questions. The score is computed as the percentage of correctly answered questions, as weighted average among the 57 diverse subjects. LLM Models and implementations: We conduct experiments using six well-known LLMs: Llama3.1-8B, Llama3.1-70B (Touvron et al., 2023), GPT-4o-mini and GPT-4o (OpenAI, 2023), Claude3.5-Sonnet2 and Claude3.5-Haiku3. We conducted experiments on Nvidia H100 96GB HBM2e GPUs with different configurations based on different model sizes. For Llama3.1-70B models, we did fine tuning on 32 H100 GPUs and inference on 8 H100 GPUs."
        },
        {
            "title": "4 Q1. Does an LLM Know What It Knows?",
            "content": "We first investigate whether an LLM can accurately estimate its own confidence in factual statement. If so, we can rely on this estimation to choose between internalized neural knowledge and external symbolic knowledge. We measure confidence in two ways: self-reported confidence, and consistency of answers. We focus on the three short-form benchmarks: Head-to-Tail, SimpleQA, and CRAG. We investigate the calibration between confidence and QA accuracy; that is, when the model has confidence of 0.8, is the QA accuracy close to 80%? Confidence vs. Accuracy: To obtain self-reported confidence, we prompt the LLM to directly provide confidence score between 0 and 1 along with its answer (prompt template in Appendix 3). We remove missing answers, divide the reported confidences by equal-sized quantile bins, and plot the average accuracy within each bin. The top panels in Figure 2 show the calibration, leading to four observations. (We in addition compare calibration for head, torso, tail entities in Appendix C). 1. The self-reported confidence is mostly positively correlated with QA accuracy, but LLMs tend to be over-confident (the correlation curves are below the ideal calibration dashed line). For example, when Llama-3.1-70B predicts confidence of 80% on CRAG, the real accuracy is only 33%. 1 https://github.com/openai/simple-evals 2https://www.anthropic.com/news/claude-3-5-sonnet 3https://www.anthropic.com/claude/haiku 4 Figure 2 LLMs self-reported confidences is correlated with QA accuracy, but often over-confident. The answer consistency is often better calibrated with QA accuracy. 2. Notably, for the same model series, the smaller model is often more confident than the larger model (with an exception of Claude3.5 on CRAG), demonstrating the interesting correlation between ignorance and self-assurance. 3. Finally, the overconfidence is more pronounced when answering SimpleQA questions than on other benchmarks. sample of 50 questions from SimpleQA shows that the questions are often nuanced for fairly popular entities (e.g. What was the first line after the salutation in the letter sent to Sardar Patel by Abhay Charan De?\", In which month and year was Service Pack 3 for Windows Fundamentals for Legacy PCs released?), possibly causing LLMs to be over-confident. Consistency vs. Accuracy: To measure consistency, we ask LLM the same question 20 times with the temperature set to 1.0, select the most frequent response as the final answer, and calculate its frequency among the 20 times as the consistency score. To be robust against minor differences, we determine the \"most frequent\" answer based on semantic similarity rather than exact string match. The bottom panels in Figure 2 shows that consistency is mostly better calibrated than self-reported confidence, and largely overlays with the ideal calibration on CRAG. In addition, the calibration curve is more linear compared to self-reported confidence. Ideally, an LLM would know what it knows, give facts it has high confidence about, and refrain Summary: from including low-confidence facts to avoid hallucinations. Unfortunately, as this analysis shows, self-reported confidence tends to be over-confident, whereas answer consistency requires invoking LLMs multiple times and thus is impractical. We thus need to find different strategy."
        },
        {
            "title": "5 Q2. Can We Teach LLMs to Refrain from Hallucinating?",
            "content": "In this section, we shift gears and study whether we can fine tune an LLM to suppress hallucinations, by only answering question that it has high confidence about, and admitting am unsure otherwise. We have three key intuitions for this fine-tuning. First, the calibration study shows that LLMs self-reported confidence is associated with answer accuracy but not calibrated, we thus teach it to calibrate its confidence by showing the ground truth. Second, we introduce dampener prompt Answer only if you are confident, to explicitly guide LLMs behavior. Third, as we wish to regularize behavior only for factual statements, we focus the teaching on atomic facts (attributes of entities) to avoid distractions of other factors."
        },
        {
            "title": "5.1 Fine-tuning setup\nTraining: We conducted supervised fine-tuning (SFT) and constructed the training data as a set of question-\nlabel pairs, where the label teaches the model the answer it shall generate.",
            "content": "The questions ask for atomic facts, and are generated from DBPedia, which covers diverse set of domains (Intuition #3). We used the open-sourced script from Sun et al. (2023a) to generate question-answer pairs from DBPedia (exclusive from those in the Head-to-Tail benchmark), evenly distributed across head, torso, and tail entities. We generated labels as follows. First, we prompted Llama-3.1-70B model to answer the DBPedia-based questions (Prompt 1 in Appendix A). Then, we prompt Llama-3.1-405B to judge if the answer is consistent with the ground truth (Prompt 2 in Appendix A). If the answer is correct, the label is the ground truth answer; otherwise, the label is am unsure about the answer (Intuition #1). Inferences: We call our fine-tuned model ConfQA. At inference time, we prompt QA in two ways: without the dampener, and with the dampener, to examine the dampening effect. (Intuition #2). Other solutions: We compare our solution with two state-of-the-art solutions. R-Tuning (Zhang et al., 2024) generates its training data by adding prompt Are you sure you accurately answered the question based on your internal knowledge? in the question, and padding am sure or am unsure based on correctness of the generated answer. In the inference, it again appends the prompt and applies post-processing by removing answers with the suffix of am unsure. We used both MMLU (proposed in the paper) and DBPedia for training. IDK (Cheng et al., 2024) requires answer consistency in addition to answer correctness, and we add consistency requirement of at least four out of five times. We used DBPedia for its fine-tuning for more direct comparison. Additionally, we considered the following alternatives for ablation study; more alternatives are discussed in Appendix F. No-dampener: Remove the dampener from training prompt. MMLU-as-source: Use MMLU, instead of DBPedia, to generate training data. GT-as-label: Use ground truth everywhere as the label. Fact-feeding: Feed additional knowledge by mixing 10K instances from Tulu3 data (Lambert et al., 2025)4. Implementation and hyper-parameters: We experimented with Llama-3.1-70B (we observed similar trend on Llama-3.1-8B). We conducted simple scaling-law study (see Appendix D) and decided to run one epoch on 3K high quality training data, with learning rate of 1e-6 and batch size of 1. 6 Model Corr (Rec) Miss Hall. Fac. Prec F1 Corr (Rec) Miss Hall. Fact. Prec F1 Llama-3.1 Llama-3.1 (D) R-tuning (MMLU) R-tuning (DBPedia) IDK (DBPedia) ConfQA ConfQA (D) Llama3.1 Llama3.1 (D) R-tuning (MMLU) R-tuning (DBPedia) IDK (DBPedia) ConfQA ConfQA (D) 52.0 47.0 24.3 24.5 17.0 49.9 31.5 20.0 16.8 20.3 3.7 0. 17.3 4.9 DBpedia (in-domain) 26.0 26.0 20.8 26.2 22.0 26.8 67.3 67.8 81.5 8.3 7.7 16.0 16.8 15. 33.5 63.3 1.5 17.5 5.2 SimpleQA (out-of-domain) 35.9 35.2 -15.8 -18.4 31.5 26.3 44.1 48.0 38.0 83.3 99. 55.8 93.1 41.7 13.0 0.2 26.8 2.1 -21.4 -9.3 0.4 -9.5 2.8 66.7 64. 74.5 76.2 91.9 74.0 85.9 35.8 32.3 32.8 22.0 73.0 39. 70.2 58.4 54.3 36.6 37.1 28.7 59.0 46.1 25.7 22.1 25.1 6.3 1. 24.0 9.1 44.8 40.7 28.2 25.3 22.0 43.0 32.5 58.7 57.5 57.8 31.6 20. 57.0 39.4 34.2 36.2 60.5 70.2 77.0 IMDB (out-of-domain) 23.8 21.0 17.5 23.2 11.3 4.5 16.9 20.8 21.0 42.0 63. 1.0 16.0 27.0 4.2 28.3 CRAG (out-of-domain) 25.7 33.0 20.2 15.6 22.3 17.1 55.0 78.2 19.6 56.2 25.1 13.4 1.1 23.4 4. 37.2 32.7 18.2 19.6 33.6 35.0 68.1 63.7 71.3 84.9 95.7 73.0 88. 69.6 73.9 69.7 70.2 95.0 70.9 90.0 54.1 49.6 40.4 39.0 35. 54.0 47.6 63.7 64.7 63.2 43.6 34.0 63.0 54.8 Table 2 Overall factuality improvement on short-form benchmarks; ConfQA can reduce hallucination to below 5% with the dampener prompt. All numbers are in percentage (%)."
        },
        {
            "title": "5.2 Results on short-form question answers\nOverall results: Table 2 presents our overall results, and shows interesting effect of the dampener prompt.\nFirst, without fine-tuning, the effect of the dampener is inconsistent. For all benchmarks, the dampener\nincreased the percentage of missing answers by 2-7%. However, it (correctly) dampens hallucinations on\nCRAG, but dampens correct answers and thus reduced the factuality on Head-to-tail and SimpleQA. This is\nnot surprising since the LLM confidence is not well calibrated.",
            "content": "Second, after the fine-tuning, ConfQA improves precision by up to 40% and improves factuality by up to 20%. Without the dampener, we observe mild reduction of the hallucinations, accompanied with minor sacrifice on correct answers, leading to factuality improvement up to 6%. With the dampener, the LLM is much more conservative and the hallucination rate drops to below 5% on all benchmarks. As side effect, correctness also drops significantly; for example, since SimpleQA focuses on nuanced facts, after the finetuning we observe nearly zero correctness. Still, the factuality increases by up to 18% compared to the baseline. Third, R-tuning mostly has higher precision and lower hallucination, especially if trained on DBPedia. However, we also observe much lower correctness. We suspect this is because when the model gives wrong answer, it feeds ground truths as additional knowledge and causes confusion; as we will discuss soon, we observe similar trend for Fact-feeding, with similar flavor. We also observe better results when trained on DBPedia than on MMLU, which mixes factual statements and skills and can cause confusion. Fourth, IDK obtains the lowest hallucination rate (below 1.5% for all benchmarks), as it requires in addition the consistency signal and thus is stricter. However, the correctness also drops significantly, reducing overall factuality. Fifth, even though the training data are generated only from DBPedia, ConfQA behavior changes on the other datasets as well, showing amazing generalization. Finally, we note that although the factuality metric increases most of the time, the F1-metric stays flat or drops, as the latter is more lenient to hallucinations as discussed in Appendix B. Ablation study We now compare ConfQA with the several alternatives, as shown in Table 3 (full comparison in Table 8 in Appendix F). MMLU-as-source obtains low hallucination (below 2%) but also significantly lower 4https://huggingface.co/datasets/allenai/tulu-3-sft-mixture. The 10K instances are composed of 2K randomly sampled from subset No Robots (https://huggingface.co/datasets/HuggingFaceH4/no_robots), 1K from TableGPT (https://huggingface.co/ datasets/LipengCS/Table-GPT) Li et al. (2023), 1K from SciRIFF 8192 (https://huggingface.co/datasets/allenai/SciRIFF), and 6K from Tulu 3 Persona IF (https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following). 7 Model Llama-3.1 ConfQA * No-dampener * MMLU-as-source * GT-as-label * Fact-feeding DBpedia (in-domain) IMDB (out-of-domain) SimpleQA (out-of-domain) CRAG (out-of-domain) Corr Miss Hall Fac Corr Miss Hall Fac Corr Miss Hall Fac Corr Miss Hall Fac 47.0 31.5 36.0 8.2 48.0 20.7 26.8 63.3 50.2 89. 2.8 76.7 26.2 5.2 13.8 2.0 49.2 2.7 20.8 26.3 22.2 6.2 -1.2 18.0 40. 32.5 34.2 16.7 41.2 25.5 36.2 63.3 50.7 82.0 4.3 70.7 23. 4.2 15.2 1.3 54.5 3.8 17.5 16.8 48.0 35.2 -18. 28.3 19.0 15.4 -13.3 21.7 4.9 5.8 0.6 17.8 2.5 93.1 87.5 98.8 13.7 94.7 2.1 6.7 0.5 68.4 2. 2.8 -0.9 0.1 -50.6 -0.3 57.5 39.4 46.0 7.0 53.7 22.4 22.3 56.2 44.2 92. 14.2 74.5 20.2 37.2 4.4 9.8 0.3 32.1 3.1 35. 36.2 6.7 21.6 19.3 Table 3 Ablation study when applying dampening in inference, showing effectiveness of our fine-tuned model. All numbers are in percentage (%) and full results in Table 8. (a) DBPedia (b) IMDB (c) CRAG Figure 3 ConfQA suppresses more on long-tail facts than popular facts. Model Long Fact Alpaca Fact Biography Prec Rec F1 Miss Prec Rec F1 Miss Prec Rec F1 Miss Llama3.1 RAG (Llama3.1) (Yu et al., 2022) ConfQA 64.5 71.7 67. 65.4 74.6 67.7 64.3 72.7 66.7 0 0 0.8 62.3 65.8 62.2 71.0 74.3 71.1 63.8 66.0 63. 0 0 0.4 35.4 44.9 42.0 40.3 48.1 46.5 37.1 43.8 42.6 0 0 12.6 Table 4 ConfQA improves precision and recall for long-form answer generation. correctness. We suspect this is because MMLU contains diverse set of tasks, reducing overall confidence of the model. On the other hand, No-dampener increases correctness and reduces missing rate, but can increase hallucinations, showing the important role of the dampener in training as well. GT-as-label achieves the highest correctness and lowest missing rate, but becomes over-confident to hallucinate (hallucination rate can reach 70%). This is consistent with observations in previous work (Lin et al., 2024; Gekhman et al., 2024) that feeding facts in the SFT-stage can teach LLMs to hallucinate. Interestingly, Fact-feeding combines our ConfQA fine-tuning data with Tulu facts, drops hallucinations but also correctness, similar to R-tuning. We suspect this is because what our training data teach the LLM (saying unsure) is of different purpose from what the extra Tulu facts teach the LLM (feeding knowledge), when mixed together can offset each other and cause confusion. Answer distributions: Finally, we show in Figure 3 the distribution of correct, missing, and incorrect answers for entities of different popularity, before and after fine-tuning, with and without dampening. It confirms that fine-tuning suppresses hallucinations, and the dampener prompt further reduces hallucinations. Additionally, it shows ConfQA suppresses more on long-tail facts, where it lacks confidence."
        },
        {
            "title": "5.3 Results on other benchmarks",
            "content": "On the long-form benchmarks, we do not apply the dampener prompt, and instead we retrieve 10 passages using the input prompts as queries and append Provide as many specific details and examples as possible (such as names of people, numbers, events, locations, dates, times, etc.) to the end of the prompt to encourage the model to provide as much confident information as possible. Table 4 shows that ConfQA achieves higher or comparable precision and recall, except for 13% biography questions where it has low confidence about and 8 Model SimpleQA CRAG Upper Corr Miss Hall Fac L-P50 L-P90 Upper Corr Miss Hall Fac L-P50 L-P90 LLM-only RAG-everywhere ConfQA-based 20.0 100.0 95.1 20.0 78.1 77. 44.1 11.5 11.4 35.9 10.5 11.5 -15.8 67.6 65. 480 1,900 1,802 896 2,780 2,650 58.7 100.0 95.6 58.7 61.1 62. 15.6 15.1 14.2 25.7 23.8 23.5 33.0 37.3 38.8 480 1,900 1, 896 2,780 1,955 Table 6 ConfQA-based RAG invocation achieves similar quality to RAG-everywhere, while cutting latency. does not answer. For RAG, we use Contriever (Izacard et al., 2022) to retrieve passages from C4 (Raffel et al., 2020) and Wikipedia, following the setting in MassiveDS (Shao et al., 2024)."
        },
        {
            "title": "Model",
            "content": "Llama3.1 ConfQA MMLU (5-shot) MMLU-Pro 66.3 65.4 82.7 82.8 Table 5 ConfQA does not regress on MMLU. Figure 4 RAG invocation architecture. We also evaluate ConfQA on the standard MMLU benchmark. Table 5 shows that the scores are mostly similar to the baseline. Summary: Our experiments validated the effectiveness of our ConfQA fine-tuning strategy. If one aims to eliminate hallucinations, we recommend applying ConfQA with the dampener; if one aims to maintain the correctness and meanwhile reduce hallucinations as much as possible, we recommend ConfQA without the dampener."
        },
        {
            "title": "6 Q3. What Is the Optimal Strategy for Triggering RAG?",
            "content": "Since our fine-tuned model can reduce hallucinations to nearly zero, we can invoke the RAG pipeline when it says unsure, and rely on the LLM-generated answer otherwise. Figure 4 depicts RAG invocation architecture. The system always answers dynamic question that inquires changing information through the RAG pipeline. When static question arrives, the system invokes LLM generation and the RAG pipeline in parallel. When LLM generates non-missing answer, the system early-stops the RAG pipeline and outputs the answer; otherwise, the system waits and outputs the answer from the RAG pipeline. Calculations based on Table 2 shows that our framework enables potential accuracy gains to beyond 95%, while reducing unnecessary external retrievals by over 30%. We next evaluate QA quality and latency through real RAG implementation, which invokes search APIs (Bing API and Knowledge Graph API) for retrieval, and passes the retrieved content to Llama-3.1-70B to generate the responses. Table 6 reports the end-to-end QA accuracy and latency for our proposed RAG architecture, and compares it with not invoking RAG and invoking RAG everywhere. ConfQA-triggered RAG obtains similar quality to RAG-everywhere, but cut latency by 300ms P50 and 400ms P90 for CRAG. The latency-cut is less pronounced on SimpleQA since RAG should be triggered for the majority of the questions. The quality improvement is small on CRAG because it contains lot of complex questions requiring reasoning over retrieval results, which our simple RAG implementation does not excel."
        },
        {
            "title": "7 Limitations",
            "content": "Our experiments focus on SFT, and we leave DPO-based fine-tuning for future work. We conducted training on the Llama-3.1 model of different sizes; it would be interesting to experiment with other open sourced models and later versions of models. We empirically compared DBPedia and MMLU, where the former 9 contains only simple factual questions, and the latter contains questions ranging from factual to reasoning. comprehensive study regarding the effect of sources with different coverage in this spectrum would improve the understanding. We can extend our learnings beyond factual statements, for math, coding, reasoning etc. Lastly, SFT requires the access to LLM itself for fine tuning, and thus restrict the application of the proposed framework to proprietary LLMs only accessible through APIs."
        },
        {
            "title": "8 Conclusion",
            "content": "Recent studies have shown that LLMs acquire substantial knowledge during pre-training, and that introducing new knowledge during post-training can often increase hallucinations (Lin et al., 2024; Gekhman et al., 2024). Rather than injecting additional knowledge, the Dual Neural Knowledge (DualKnowl) framework we propose trains LLMs to identify and withhold low-confidence factual claims, deferring to RAG in such cases to improve factuality. Through comprehensive set of experiments, we show that 1) LLMs tend to be over-confident on what they know; 2) we can fine-tune LLM to refrain from generating inconfident factual statements and thus reducing the hallucination rate to below 5%; and 3) using this fine-tuned model for RAG-triggering can reach similar answer accuracy as RAG-everywhere, whereas reducing RAG retrievals by over 30% to save cost. Our proposed framework naturally integrates internal neural knowledge with external symbolic knowledge, allowing for improvements in both factuality and latency as LLMs develop richer internal knowledge and more effective RAG capabilities."
        },
        {
            "title": "References",
            "content": "Farima Fatahi Bayat, Kun Qian, Benjamin Han, Yisi Sang, Anton Belyi, Samira Khorshidi, Fei Wu, Ihab F. Ilyas, and Yunyao Li. Fleek: Factual error detection and correction with evidence retrieved from external knowledge, 2023. https://arxiv.org/abs/2310.17119. Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S. Yu, and Lichao Sun. comprehensive survey of ai-generated content (aigc): history of generative ai from gan to chatgpt, 2023. https://arxiv.org/abs/2303.04226. Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, and Xipeng Qiu. Can ai assistants know what they dont know?, 2024. https://arxiv.org/abs/2401.13275. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models, 2023. https://arxiv.org/abs/2309.11495. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpacafarm: simulation framework for methods that learn from human feedback. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 3003930069. Curran Associates, Inc., 2023. https://proceedings. neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf. Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang, and Yuxuan Wang. Halo: Estimation and reduction of hallucinations in open-source weak large language models, 2023. https://arxiv.org/abs/2308.11764. Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. survey on rag meeting llms: Towards retrieval-augmented large language models, 2024. https://arxiv.org/abs/2405.06211. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, 2024. https://arxiv.org/abs/2312.10997. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. Does fine-tuning llms on new knowledge encourage hallucinations?, 2024. https://arxiv.org/abs/2405.05904. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. https://arxiv.org/abs/2407.21783. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. https://arxiv.org/abs/2009.03300. Yizheng Huang and Jimmy Huang. survey on retrieval-augmented text generation for large language models, 2024. https://arxiv.org/abs/2404.10981. Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning, 2022. https://arxiv.org/abs/2112.09118. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and 12 Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, March 2023a. ISSN 1557-7341. doi: 10.1145/3571730. http://dx.doi.org/10.1145/3571730. Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and Pascale Fung. RHO: Reducing hallucination in open-domain dialogues with knowledge grounding. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 45044522, Toronto, Canada, July 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.275. https://aclanthology. org/2023.findings-acl.275/. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation, 2023. https://arxiv.org/abs/2305.06983. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. 2022. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James V. Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, Yuling Gu, Saumya Malik, Victoria Graf, Jena D. Hwang, Jiangjiang Yang, Ronan Le Bras, Oyvind Tafjord, Chris Wilhelm, Luca Soldaini, Noah A. Smith, Yizhong Wang, Pradeep Dasigi, and Hannaneh Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. https://arxiv.org/abs/2411.15124. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 94599474. Curran Associates, Inc., 2020. https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf. Peng Li, Yeye He, Dror Yashar, Weiwei Cui, Song Ge, Haidong Zhang, Danielle Rifinski Fainman, Dongmei Zhang, and Surajit Chaudhuri. Table-gpt: Table-tuned gpt for diverse table tasks, 2023. https://arxiv.org/abs/2310.09263. Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Wen-tau Yih, and Xilun Chen. Flame : Factuality-aware alignment for large language models, 2024. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization, 2020. https://arxiv.org/abs/2005.00661. Sabrina J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857872, 2022. doi: 10.1162/tacl_a_00494. https://aclanthology.org/2022.tacl-1.50. Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation, 2023. https://arxiv.org/abs/2305.14251. OpenAI. Gpt-4 technical report. 2023. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback, 2023. https://arxiv.org/abs/2302.12813. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. https://arxiv.org/abs/2305.18290. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. http://jmlr.org/papers/v21/20-074.html. Rulin Shao, Jacqueline He, Akari Asai, Weijia Shi, Tim Dettmers, Sewon Min, Luke Zettlemoyer, and Pang Wei Koh. Scaling retrieval-based language models with trillion-token datastore, 2024. https://arxiv.org/abs/2407.12854. Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. REPLUG: Retrieval-augmented black-box language models. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association 13 for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83718384, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.463. https: //aclanthology.org/2024.naacl-long.463/. Yixiao Song, Yekyung Kim, and Mohit Iyyer. VeriScore: Evaluating the factuality of verifiable claims in long-form text generation. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 94479474, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.552. https://aclanthology.org/2024.findings-emnlp.552/. Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. Dragin: Dynamic retrieval augmented generation based on the information needs of large language models, 2024. https://arxiv.org/abs/2403.10081. Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs?, 2023a. Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and Denny Zhou. Recitation-augmented language models, 2023b. https://arxiv.org/abs/2210.01296. Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D. Manning, and Chelsea Finn. Fine-tuning language models for factuality, 2023. https://arxiv.org/abs/2311.08401. S. Towhidul Islam Tonmoy, Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. comprehensive survey of hallucination mitigation techniques in large language models, 2024. https://arxiv.org/abs/2401.01313. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. 2023. Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, and Thang Luong. Freshllms: Refreshing large language models with search engine augmentation, 2023. https://arxiv.org/abs/2310.03214. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark, 2024. https://arxiv.org/abs/2406.01574. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. Measuring short-form factuality in large language models, 2024a. Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8075680827. Curran Associates, Inc., 2024b. https://proceedings.neurips.cc/paper_ files/paper/2024/file/937ae0e83eb08d2cb8627fe1def8c751-Paper-Conference.pdf. Xiaokai Wei, Shen Wang, Dejiao Zhang, Parminder Bhatia, and Andrew Arnold. Knowledge enhanced pretrained language models: compreshensive survey, 2021. https://arxiv.org/abs/2110.08455. Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, Carolyn Rose, Daniel Fried, and Hejia Zhang. Improving model factuality with fine-grained critique-based evaluator, 2025. https://arxiv.org/abs/2410.18359. Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, and Xin Luna Dong. Cragcomprehensive rag benchmark, 2024. Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang. Characterizing truthfulness in large language model generations with local intrinsic dimension, 2024. https://arxiv.org/abs/2402.18048. Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. survey of knowledge-enhanced text generation. ACM Computing Surveys, 54(11s):138, January 2022. ISSN 1557-7341. doi: 10.1145/3512467. http://dx.doi.org/10.1145/3512467. Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. R-tuning: Instructing large language models to say dont know, 2024. https://arxiv.org/abs/2311.09677. Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Paco Guzman, Luke Zettlemoyer, and Marjan Ghazvininejad. Detecting hallucinated content in conditional neural sequence generation, 2021. https://arxiv.org/abs/2011.02593."
        },
        {
            "title": "A Prompts",
            "content": "We add list of prompts we used in experiments for reference. Prompt 1 Simple question answer generation prompt. As Assistant AI, you help answer factual questions. Please keep your responses short and concise and directly provide the answer to the user question without reasoning. Answer only if you are confident; otherwise, respond with am unsure about the answer. Prompt 2 simple question answer grading prompt. You need to check whether the prediction of question-answering system to question is correct. You should make the judgment based on list of ground truth answers provided to you. Your response should be \"correct\" if the prediction is correct or \"incorrect\" if the prediction is wrong. Example 1: Question: Who authored The Taming of the Shrew (published in 2002)? Ground truth: [\"William Shakespeare\", \"Roma Gill\"] Prediction: Shakespeare Correctness: correct Example 2: Question: Who authored The Taming of the Shrew (published in 2002)? Ground truth: [\"William Shakespeare\", \"Roma Gill\"] Prediction: Roma Gill and Shakespeare Correctness: correct Example 3: Question: Who authored The Taming of the Shrew (published in 2002)? Ground truth: [\"William Shakespeare\", \"Roma Gill\"] Prediction: Roma Shakespeare Correctness: incorrect Example 4: Question: What country is Maharashtra Metro Rail Corporation Limited located in? Ground truth: [\"India\"] Prediction: Maharashtra Correctness: incorrect Example 5: Question: Whats the job of Song Kang-ho in Parasite (2019)? Ground truth: [\"actor\"] Prediction: He plays the role of Kim Ki-taek, the patriarch of the Kim family. Correctness: correct Prompt 3 Self-reported confidence prompt. Please answer the following question truthfully to your best knowledge. Provide confidence score between 0 to 1 representing your confidence in the answer. Question: {{{question}}} Your response must be in the following JSON format. {answer: Your answer here, confidence_score: float number} Remember, your answer MUST be valid json string with two keys: answer, confidence_score."
        },
        {
            "title": "B Experiment Setup Details",
            "content": "We consider 7 benchmarks. Head-to-Tail (short-form with simple questions) (Sun et al., 2023a): We leveraged the data scraping scripts from the Head-to-Tail benchmark, and generated simple questions and their answers according to content from DBPedia 5 (general knowledge graph) and IMDb 6 (data in the Movie domain). From 5dbpedia.org 6imdb.com 16 each dataset we randomly sampled 200 entities for head entities, 200 for torso entities, and 200 for tail entities. Here we follow the definition in Sun et al. (2023a) for head, torso and tail: we rank all entities by their traffic; head entities are top-popular entities that together account for 1/3 of traffic, tail entities are unpopular entities that together account for 1/3 of traffic, and torso entities are the remaining medium-popular entities. Together, we have 1200 question-answer pairs, 600 from each source. SimpleQA (short-form with simple questions) (Wei et al., 2024a): SimpleQA is benchmark released by OpenAI to measure LLM factuality. It contains 4326 manually crafted short, fact-seeking questions, covering diverse topics such as science, technology, history, and entertainment. CRAG (short-form with simple and complex questions) (Yang et al., 2024): CRAG is benchmark to test RAG capabilities. It contains 4,409 training and 1335 evaluation questions covering five domains (general, finance, sports, music, movie), entities of different popularities (head, torso, tail), facts of different dynamisms (static, slow-changing, fast-changing, real-time), and eight question types (simple, condition, set, comparison, aggregation, multi-hop, post-processing, false premise). We selected the 642 static questions from the evaluation data set, with 97 questions for head entities, 99 for torso, 90 for tail entities and 356 for facts from the web (mostly popular); we excluded false-premise and dynamic questions from the sampling as it presents different challenges. LongFact (long-form) (Wei et al., 2024b): Aiming to measure of the factuality of long-form responses consisting of at least several paragraphs, LongFact has 2,280 factual questions covering 38 topics, generated by prompting GPT-4. Following Wei et al. (2024b), we use the 250 prompts from the LongFact-Objects dataset in our experiments. AlpacaFact (long-form) (Lin et al., 2024): Initially sourced from diverse interactions with real-world users, the 805 instructions in AlpacaFarm (Dubois et al., 2023) served as benchmark for evaluating the ability of different LLMs to follow instructions. Following Lin et al. (2024), we used subset of 241 fact-seeking instructions in this work. Biography (long-form) (Min et al., 2023): To validate the effectiveness of FActScore, Min et al. (2023) created collection of prompts named Biography by applying the template Tell me bio of [Person Name] to 183 notable individuals listed on Wikipedia. Given its extensive use in recent literature, we have included this prompt set for our experiments as well. MMLU (General knowledge): The MMLU (Hendrycks et al., 2021) dataset covers 57 subjects, including areas such as mathematics, history, law, and medicine. It contains two subsets: the MMLU 5-shots dataset contains 14,042 multi-choice questions to evaluate general knowledge and problem-solving tasks; the MMLU-Pro (Wang et al., 2024) dataset contains 12,082 multi-choice questions to stress-test reasoning, disambiguation, and factual accuracy. For short-form questions we consider two set of metrics, where F1-score is more lenient for incorrect answers (hallucinations), but factuality strongly prefers missing answers to hallucinations. For example, consider model that answers 10% questions correctly (correct% = 10%) and the rest of the questions incorrectly (incorrect% = 90%); the F1-score is 10% (not punishing hallucinations much) while the factuality is -80%. Now consider another models that answers 10% questions correctly and admits \"I an unsure about the answer\" for the rest of the questions; the F1-score is 18.2%, only slightly higher than 10%, but the factuality is 10%, significantly higher than -80%. For long-form responses we use the automatic evaluation metric, VeriScore Song et al. (2024), for measuring the factuality. Following FActScore (Min et al., 2023) and SAFE (Wei et al., 2024b), VeriScore extracts more sensible and verifiable claims from each sentence and uses Google search snippet instead of Wikipedia as the source of knowledge. This approach allows VeriScore to be applied to more diverse topics and requires fewer but more meaningful claims to be checked. We report the F1 score from VeriScore, which represents the harmonic mean of precision and recall. In line with Song et al. (2024), we set the minimum number of facts required for perfect recall based on the median number of extracted claims per dataset, using their fine-tuned models for claim extraction and verification."
        },
        {
            "title": "C Influence of entity popularity on confidence",
            "content": "In this section, we study the calibration versus popularity of the entities. Figure 5 show the calibration on the Head-to-Tail and CRAG benchmarks, where questions are categorized by entity popularity into Head, Torso, Tail (plus Web for CRAG). Figure 5 Correlation between LLMs self-reported confidences and average accuracies on the CRAG dataset and the Head-to-Tail dataset, categorized by question types. Interestingly, we found for simple questions on Head-to-Tail, models are better calibrated for head entities than torso or tail entities (Figure 5 bottom panels). However, on more complex questions on CRAG, models are better calibrated for tail entities than torso or head entities (Figure 5 top panels). This shows two different dimensions that can affect the model confidence: entity popularity and question nuances."
        },
        {
            "title": "D Fine tuning implementation",
            "content": "In order to determine how many data instances and how many fine-tuning steps are necessary to achieve optimal performance, we conducted simple scaling-law study. We prepared 27K question-answer pairs from DBPedia, ran total of 10K steps with 4 hosts, 8 processes per host, and batch size of 1. We noticed that around 100 steps gives the best performance, and more steps can cause over-fitting. With this setting, 100 steps could run one epoch for 3200 samples. We thus selected 3K high quality instances for simplicity, 1K each for head, torso and tail entities, and run fine tuning for one epoch. The final setup for fine-tuning the Llama-3.1-70B instruction tuning model is as follows: Epoch: 1, Learning Rate: 1e-6, Batch Size: 1. This configuration utilizes 32 Nvidia H100 96GB HBM2e GPUs to achieve optimal performance. p-Value of ConfQA models We compute p-Values for ConfQA model on the hallucination reduction metrics comparing with baseline Llama-3.1 and Llama3.1 (D), and report in Table 7. The results show that the improvements on Hallucination reduction shown in Table 2 are statistically significant on all benchmarks. Model Hall (p-value) Fac (p-Value) Hall (p-Value) Fac (p-Value) DBpedia (in-domain) IMDB (out-of-domain) Llama-3.1 Llama-3.1 (D) 26.0 26.2 26.0 20.8 21.0 23.2 23.8 17.5 Fine-tuning Fine-tuning (D) 17.5 (9.31E-03) 5.2 (4.88E-08) 31.5 (4.45E-02) 26.3 (4.45E-02) 16.0 (5.14E-02) 4.2 (6.23E-07) 27.0 (7.69E-02) 28.3 (1.98E-03) Llama3.1 Llama3.1 (D) ConfQA ConfQA (D) SimpleQA (out-of-domain) CRAG (out-of-domain) 35.9 35.2 -15.8 -18.4 25.7 20.2 33.0 37. 26.8 (5.61E-10) 2.1 (0.00E+00) -9.5 (8.24E-06) 2.8 (0.00E+00) 23.4 (1.38E-01) 4.4 (1.62E-05) 33.6 (2.26E-01) 35.0 (1.38E-01) Table 7 Factuality improvement on short-form benchmarks with p-Value; our fine-tuned models can reduce hallucination to nearly zero with the dampener prompt with significant difference. All numbers are in percentage (%)."
        },
        {
            "title": "F Full ablation study",
            "content": "We compare ConfQA with the more alternatives options than in the main content, as shown in Table 8. Gen-as-label: the same strategy to choose questions the model can answer as ConfQA, but use model generation as the true label, rather than the ground truth. IDK: the same as the IDK (DBPedia) in the main part of the paper. No-dampener: the same as ConfQA, but only use the question, excluding the dampener in the training input data. GT-as-label: feed in the 3k rows of raw DBPedia data into the SFT without processing to change labels. Fact-feeding: rather than using only the DBPedia data, mixed 10k samples from Tulu3 data as discribed in the main paper. R-tuning (DBPedia): using our DBPedia training set and following R-tuning paper to generate labels for SFT. R-tuning (MMLU): using randomly sampled 3k MMLU samples to generate training set following R-tuning labeling strategy. MMLU-as-source: the same strategy as ConfQA, but use MMLU as data source. We use the same 3k samples from R-tuning (MMLU). Table 8 reports results in two rows. Results on the top are evaluated using no system prompt. Only pass in the original questions to the models. The bottom rows are results with dampener as the system prompt when doing model evaluation. 19 Model Llama-3.1 ConfQA * Gen-as-label * IDK (no-dampener) * No-dampener * GT-as-label * Fact-feeding * R-tuning (DBPedia) * R-tuning(MMLU) * MMLU-as-source Llama-3.1 (D) ConfQA (D) * Gen-as-label (D) * IDK * No-dampener (D) * GT-as-label (D) * Fact-feeding (D) * R-tuning (DBPedia) * R-tuning(MMLU) * MMLU-as-source (D) DBpedia (in-domain) IMDB (out-of-domain) SimpleQA (out-of-domain) CRAG (out-of-domain) Corr Miss Hall Fac Corr Miss Hall Fac Corr Miss Hall Fac Corr Miss Hall Fac 52.0 49.0 48.7 44.5 42.0 48.7 50.0 53.7 50.2 50. 47.0 31.5 28.5 17.0 36.0 48.0 20.7 24.5 24.3 8.2 22.0 33.5 31.7 40.3 34.7 1.5 26.8 6.7 19.2 21.8 26.8 63.3 65.7 81.5 50.2 2.8 76.7 67.8 67.3 89.8 26.0 17.5 19.7 15.2 23.3 49.8 23.2 39.7 30.7 27.7 26.2 5.2 5.8 1.5 13.8 49.2 2.7 7.7 8.3 2.0 26.0 31.5 29 29.3 18.7 -1.1 26.8 14.0 19.5 22. 20.8 26.3 22.7 15.5 22.2 -1.2 18.0 16.8 16.0 6.2 44.8 43.0 42.5 40.7 40.2 42.0 43.3 44.5 45.5 44.2 40.7 32.5 27.7 22.0 34.2 41.2 25.5 25.3 28.2 16.7 34.2 42.0 39.5 45.8 38.0 0.2 35.5 11.3 28.2 32.7 36.2 63.3 69.8 77.0 50.7 4.3 70.7 70.2 60.5 82.0 21.0 16.0 18 13.5 21.8 57.8 21.2 44.2 26.3 23. 23.2 4.2 2.5 1.0 15.2 54.5 3.8 4.5 11.3 1.3 23.8 27.0 24.5 27.2 18.4 -15.8 22.1 0.3 19.2 21.0 17.5 28.3 25.2 21.0 19.0 -13.3 21.7 20.8 16.9 15.4 20.0 17.3 17.7 14.4 12.0 18.9 18.1 22.5 20.3 20.4 16.8 4.9 3.1 0.6 5.8 17.8 2.5 3.7 5.8 0.6 44.1 55.8 52.4 65.0 66.4 2.7 41.2 13.5 38.0 39. 48.0 93.1 96 99.1 87.5 13.7 94.7 83.3 85.1 98.8 35.9 26.8 29.9 20.6 21.6 78.5 40.7 64.0 41.7 39.8 35.2 2.1 1.9 0.2 6.7 68.4 2.8 13.0 9.1 0.5 -15.8 -9.5 -12.3 -6.2 -9.6 -59.6 -22.6 -41.5 -21.4 -19.4 -18.4 2.8 1.2 0.4 -0.9 -50.6 -0.3 -9.3 -3.3 0.1 58.7 57.0 57.6 56.9 52.6 58.1 56.9 58.7 57.8 56. 57.5 39.4 32.7 20.7 46.0 53.7 22.4 31.6 31.3 7.0 15.6 19.6 18.4 21.5 31.2 5.3 16.5 8.7 17.1 18.8 22.3 56.2 64 78.2 44.2 14.2 74.5 55.0 56.5 92.7 25.7 23.4 24.0 21.7 16.2 36.6 26.6 32.6 25.1 25.1 20.2 4.4 3.3 1.1 9.8 32.1 3.1 13.4 12.1 0.3 33.0 33.6 33.6 35.2 36.4 21.5 30.3 26.1 32.7 31. 37.2 35.0 29.4 19.6 36.2 21.6 19.3 18.2 19.2 6.7 Table 8 Ablation study, showing effectiveness of our fine tuned model and its alternative No-consistency. All numbers are in percentage (%)."
        }
    ],
    "affiliations": [
        "FAIR at Meta",
        "Meta Reality Labs"
    ]
}