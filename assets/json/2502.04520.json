{
    "paper_title": "Linear Correlation in LM's Compositional Generalization and Hallucination",
    "authors": [
        "Letian Peng",
        "Chenyang An",
        "Shibo Hao",
        "Chengyu Dong",
        "Jingbo Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists a linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., \"X lives in the city of\" $\\rightarrow$ \"X lives in the country of\" for every given X. This mirrors the linearity in human knowledge composition, such as Paris $\\rightarrow$ France. Our findings indicate that the linear transformation is resilient to large-scale fine-tuning, generalizing updated knowledge when aligned with real-world relationships, but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as a potential identifier of LM's generalization. Finally, we show such linear correlations can be learned with a single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 0 2 5 4 0 . 2 0 5 2 : r Linear Correlation in LMs Compositional Generalization and Hallucination Letian Peng 1 Chenyang An 1 Shibo Hao 1 Chengyu Dong 1 Jingbo Shang"
        },
        {
            "title": "Abstract",
            "content": "The generalization of language models (LMs) is undergoing active debates, contrasting their potential for general intelligence with their struggles with basic knowledge composition (e.g., reverse/transition curse). This paper uncovers the phenomenon of linear correlations in LMs during knowledge composition. For explanation, there exists linear transformation between certain related knowledge that maps the next token prediction logits from one prompt to another, e.g., lives in the city of lives in the country of for every given X. This mirrors the linearity in human knowledge composition, such as Paris France. Our findings indicate that the linear transformation is 1) resilient to large-scale fine-tuning, 2) generalizing updated knowledge when aligned with real-world relationships, 3) but causing hallucinations when it deviates. Empirical results suggest that linear correlation can serve as potential identifier of LMs generalization. Finally, we show such linear correlations can be learned with single feedforward network and pre-trained vocabulary representations, indicating LM generalization heavily relies on the latter. 1 1. Introduction What knowledge do language models (LMs) learn beyond memorizing the training data? The generalization ability of LMs is undergoing an active debate. Optimists claim that LMs might have the capability in entirely novel tasks with their emergent behavior (Wei et al., 2022) by scaling-up parameters, while pessimists argue that LMs struggle with composing simple knowledge (Peng et al., 2024a; Thomm et al., 2024), such as reverse or transition curses claiming that LMs cannot even simply compose knowledge by reversing or transiting (Berglund et al., 2024; Zhu et al., 2024). 1University of California, San Diego. Correspondence to: Jingbo Shang <jshang@ucsd.edu>. 1Code: https://github.com/KomeijiForce/LinCorr 1 While macroscopically investigating how skills emerge in language models remains challenging, we can gain microscopical insight from the generalization behavior on the smallest learning unit, next token prediction (NTP). We unveil an interesting linear correlation between logits of related NTPs, such as CityCountry, from the source knowledge like logits of FCity(X) = NTP(X lives in the city of) to the target knowledge like logits of FCountry(X) = NTP(X lives in the country of). Between logits in knowledge subdomains (e.g., {Paris, Shanghai, } for FCity(X)), we can fit linear transformation (W, b) that well approximates FCountry(X) = FCity(X) + for any as the input. To fit the transformation, we sample numerous output logits from prompts with arbitrary inputs Xs as shown in Figure 1. Then, (W, b) is fitted with partial logit pairs and tested on the rest. The Pearson correlation coefficients for evaluation reflects the inherent relations of knowledge in the real world, with high correlations in cases like CityCountry and low correlations in cases like CityGender. Examining , we find that its weights mirror the linearity in the knowledge composition of humans. In the CityCountry case, the assigns high weights to realworld (City, Country) pairs such as ParisFrance. In other words, probability (FCountry(X) = France) is correlated with (FCity(X) = Paris). However, there also exists counterfactual weights learned in , for instance, the weight fit in for (Indianapolis, India) is much higher than the correct (Indianapolis, USA). We say is precise when assigns high weights for the correct knowledge pairs. precision is generally low for knowledge pairs with low correlations, but high linear correlation also does not guarantee high precision. This motivates us to explore the connection between 1) such linear correlations, 2) precision, and 3) LMs compositional generalization. Importantly, if the same and also fit the parameter updates after gradient propagation, then learning source knowledge will simultaneously update the target knowledge. We begin with one-step parameter updates, fine-tune the LM with piece of source knowledge, and then check the gradients on the source and target knowledge. When the linear correlation between the source and target knowledge is high, we find capable of estimating the gradients on the target knowledge based on the source gradient. We then extend the comparison to LMs before and after large-scale Linear Correlation in LMs Compositional Generalization and Hallucination Figure 1. Demonstration of our main discoveries. 1) We can fit linear transformation between the output of source and target knowledge prompts, which is resilient against fine-tuning. 2) Updating the source knowledge will generalize to the target one via resilient linearity, causing compositional generalization/hallucination. post-training, which shows fitted before post-training to retain the estimation ability for the LM after post-training. Thus, between highly correlated knowledge is found resilient against gradient propagation, which consistently plays an important role in generalization. To validate the important role of linear correlation in LM generalization, we test the generalization effect between source and target knowledge with different levels of correlation intensity and precision. Our study shows that successful generalization for simultaneous knowledge update between source and target requires high correlation intensity and precision. This implies that LMs struggle to generalize their predictions in non-linear manner, explaining why simple fine-tuning cannot efficiently edit LMs (Cohen et al., 2024). When the Pearson coefficient is high and is imprecise, the resilient linear correlation will consequently lead to compositional hallucination. For instance, learning (City(X) = Indianapolis) unfortunately generalizes to (Country(X) = India). Our linear correlation reflects the occurrence of such hallucinations before fine-tuning, demonstrating its utility in diagnosing potential faults in the knowledge composition of LMs. Finally, we explore the linear correlations origin and hypothesize that vocabulary representations are key. Even when we remove the LMs complex internals (position embeddings, self-attention, etc.) and use only mean-pooling layer plus single feedforward network, the model still learns to compose knowledge from few paired texts (e.g., FCity = Paris paired with FCountry = France). The simplified archecture shows similar generalization performance as the original Transformer. However, altering lexical mappings (e.g., ParisJapan) disrupts this ability, underscoring the critical role of vocabulary representations. Our contributions are presented as follows, We unveil the linear correlation between the LMs output logits for related knowledge. We find such linear correlation existing between gradients and resilient against training, which connects it to compositional generalization and hallucination of LMs. We attribute the formation of the linear correlation between NTPs to the vocabulary representations. 2. Related Works 2.1. Language Model Interpretation Language models (LMs) (Achiam et al., 2023; Team et al., 2024; Groeneveld et al., 2024; Dubey et al., 2024) are gaining widespread attention across various fields due to their strong performance on variety of tasks, like reasoning and knowledge retrieval. However, the black-box nature of (neural) LMs hinders humans understanding of their working mechanism. Various methods have been developed to interpret LM behavior by analyzing its parameters and intermediate representations. Several works suggest that LMs store knowledge inside the feedforward layers (Geva et al., 2021; Dai et al., 2022; Meng et al., 2022a), which are used in key-value matching manner to map inputs into related knowledge (Geva et al., 2022). Some parameters are also found to perform certain relational transformations for the LM (Todd et al., 2024; Zhang et al., 2024), known as the task representations (Lampinen & McClelland, 2020). For certain subsets of relations, LMs have been unexpectedly found to encode knowledge in linear manner (Hernandez et al., 2024), suggesting potential role of linearity in their understanding of relational structures. However, it remains unknown how the LM understands the transformation between relations. Our work shows the linearity between the output from several relation pairs given the same input. Linear Correlation in LMs Compositional Generalization and Hallucination 2.2. Model Generalization The power of modern deep neural networks lies in their remarkable ability to generalize effectively to unseen inputs. However, the exact mechanisms through which these models achieve generalization remain poorly understood. For instance, in the context of knowledge editing, numerous research studies have observed that standard fine-tuning methods for updating knowledge often struggle to meet critical objectives simultaneously (Onoe et al., 2023; HoelscherObermaier et al., 2023; Meng et al., 2022b; Gupta et al., 2023). On one hand, they fail to prevent unintended modifications to unrelated knowledge. On the other hand, they frequently fall short of ensuring that logical deductions based on the updated knowledge are properly incorporated (Cohen et al., 2024; Zhong et al., 2023). Previous research has proposed various metrics and methods to measure and predict generalization in deep neural networks. However, these approaches dont cover the perspective of correlation in model generalization proposed in our work (Yu et al., 2022; Garg et al., 2022; Kang et al., 2024). 2.3. Hallucination Detection Hallucination remains one of the most significant challenges in the deployment of language models (LMs) (Zhang et al., 2023; Huang et al., 2024). Numerous studies have explored approaches to predict and mitigate this issue. For instance, some prior works utilize trained classifiers to identify hallucinations (Jiang et al., 2024; Quevedo et al., 2024; Chen et al., 2024). Another method involves detecting hallucinations by clustering semantically similar responses and calculating entropy across these clusters (Farquhar et al., 2024). Additionally, the MIND framework has been proposed to exploit the internal states of LMs during inference, enabling real-time hallucination detection (Su et al., 2024). Moreover, formal methods guided by iterative prompting have been employed to dehallucinate LM outputs (Jha et al., 2023). RAG has also been used to detect and correct hallucinations in LM (Mishra et al., 2024). Our study presents an innovative approach to predicting hallucinations, different from existing methodologies, by leveraging the correlation. 3. Discovering Linear Correlation 3.1. Preliminary and Motivation Next Token Prediction. Neural language models have been scaled up to numerous parameters but can still be understood as mapping function among vocabulary representations R#Vocabd. We denote the embedding of the word as VX Rd. For an input word sequence, such as lives in the city of , the embeddings of the involved words will be processed with other components in the LM θV (positional embedding, self-attention networks, etc.) to encode the input context as = ([VX, , Vof]) Rd. Most2, if not all, LMs tie the input and output vocabulary embeddings together (Press & Wolf, 2017) to use the dot product VY as the logit of for the next token prediction. Finally, the vocabulary-wise dot products are normalized by softmax layer to represent the probability of certain token (Y for example).3 PθV (Y[VX, Vlives, , Vof]) = eCVY ZVocab eCVZ (cid:80) (1) For subset of all possible sequences that follow the template lives in the city of and takes arbitrary as the input, we can view template representations [Vlives, , Vof] as constant to map variable (VX ) with the City relation. PθV (Y[VX, Vlives, , Vof]) = PθV ,[Vlives, ,Vof](YVX) (2) Here, the encoding function FCity = ([Vlives, , Vof]) (subscript City denotes the semantics of constant representations) affects the final probabilistic distribution by mapping VX to near vocabulary embeddings of cities, such as VParis, VShanghai, VTokyo. Motivation: Linearity in Relation. Some knowledge like FCityToCountry (X is city in the country of ) are found linear (Hernandez et al., 2024) between vocabulary representations, which means can be well approximated by (W, b) s.t. = + b. While not all mappings have such an interesting property, this phenomenon indicates the potential for LMs to compose knowledge in their parameters. Knowledge Composition. There exists compositional relations between knowledge such as FCountry (X lives in the country of ) can be composed by other relations as FCityToCountry(FCity) since ones residential city (source knowledge) indicates ones residential country (target knowledge). Suppose the LM applies FCity(VX ) to map VX close to city embedding like VParis), then may the LM learn (W, b) inside parameters and perform FCountry(VX ) = FCityToCountry(FCity(VX )) = VParis + = VFrance? While the hypothesis can be made for non-linear relations in the composition as well, we emphasize the linearity as it corresponds to the key-value matching (Geva et al., 2021) behavior of Transformers. The linear transformation can be simply performed by feedforward network activated by self-attention. Motivated by the potential role of linearity in compositional knowledge, we conduct experiments to validate the hypoth2In Appendix F, we empirically show our conclusion also holds for an parameter untied LM - Mistral (Jiang et al., 2023) 3We omit the discussion of potential bias terms, multiple token input for simplification and reading fluency. 3 Linear Correlation in LMs Compositional Generalization and Hallucination between such output subdomains. The specific procedure to build such subdomains is presented in Appendix E. Based on the prior discussion above, we propose method to search for the linear transformation. We first build comprehensive input set by enumerating large number of words in the LMs vocabulary. While some words might indicate clear answers for certain knowledge (e.g., Obama as for FCountry), most of them do not (e.g., Lit as for FCountry). We feed all inputs to different prompts and collect the output logits such as LogPCity and LogPCountry. For each logit, we only keep dimensions for words falling inside the corresponding output vocabulary domain such as DCity and DCountry. By collecting numerous (10K in our experiments) logit pairs, we fit the linearity transformation (W, b) with half of those pairs (LogPCity, X, LogPCountry, X), Train and then evaluate the transformation on other half of pairs (LogPCity, X, LogPCountry, X), Test. Evaluation. With (W, b), we make predictions on the test pairs, LogPCountry, = LogPCity, + b, Test. We compare the predictions with the test references using the correlation metric, Pearson correlation, to evaluate how similar the logits are distributed. The evaluation is applied by both instance-wise (averaged over instance-wise logits on x1, x2, X) and label-wise (averaged over label-wise logits across instances on d1, d2, D). Our main content focuses on the label-wise Pearson correlation as we find that the global bias plays an important role in the instancewise predictions as shown in Appendix D. The label-wise evaluation eliminates the effect of b, which concentrates on the logit correlation matrix . Another advantage of instance-wise correlation is that the metric is calculated based on distributions with the same dimensions. Besides, the correlation weights on different labels also reflects how well each label is approximated by the linear transformation. 3.3. Experiment Setup While numerous compositional knowledge pairs exist in natural language, we focus on large families of knowledge composition that share commonality. Specifically, we include four large families, attribute, cross-language, simile, and math. We include 111 prompts in our experiments to cover broad knowledge fields as listed in Appendix C. Attribute. Updating one attribute of subject will affect other attributes as well. The CityCountry example illustrated before shows such compositional relation in the spatial attribute. Another example is shown as follows, FCEO FCompany Cross-language. Knowledge update is expected to be propagated to other languages, like the English French example, Figure 2. Our hypothesis and questions about how LMs compose knowledge by learning (W, b). esis that LMs learn such linear transformation inside the parameters to compose knowledge. The roadmap of our exploration is presented in Figure 2, with questions we will answer in the following sections. We will demonstrate that Such (W, b) exists for logits prompted from certain related knowledge pairs, which is applicable to arbitrary inputs, not necessarily indicating known output ( 3.4). Such linearity stays resilient against large-scale finetuning, which guarantees the LMs generalization to compositional knowledge ( 4). Such linearity can be highly attributed to the vocabulary representations. ( 6). 3.2. Method and Evaluation We search for the potential linear transformation between pairs of source and target knowledge. Continuing with the (FCity, FCountry) example, the transformation will be established between CCity,X = FCity(VX) and CCountry,X = FCountry(VX). We then decode the two representations by the LM head to produce logits LogPCity,X and LogPCountry,X both in shape R#Vocab. LogPCity,X = CCity,X ; LogPCountry,X = CCountry,X (3) As the dot product with is linear, the potential linearity holds after the transformation. We can calculate R#Vocab#Vocab and R#Vocab for the transformation between logits. We learn (W, b) for logit transformation (rather than hidden state) to improve the interpretability of the fitted . For example, high weight in W(France,Paris) indicates correct understanding of knowledge composition. In practice, only subdomain4 of the LMs large vocabulary is meaningful for the predicted logits, such as DCity = {Paris, Shanghai, Tokyo, } for LogPCity and DCountry = {France, China, Japan, } for LogPCountry. Thus, we are more interested in the submatrix of for these meaningful words. Our main experiments will focus on those values in representing the linear transformation W(DCity,DCountry) 4General subdomain size is 100, as listed in Appendix C. FCity FVille 4 Linear Correlation in LMs Compositional Generalization and Hallucination Family Prompt Domain Examples Attribute lives in the city of lives in the country of Paris, Vienna France, Austria X-Lang. vit dans la ville de lebt in der Stadt von Paris, Vienne Paris, Wien Simile Math has the same color as Apple, Banana Xs color is Red, Yellow X+1= X*2= 1, 2, 3, 4, 5 2, 4, 6, 8, 10 Table 1. Examples of prompts and domains in different families of knowledge composition. Simile. Simile builds equivalence among the attributes between objects. Thus, updating simile to subject will result in updating the corresponding attribute. An example is as follows, FSameColorAsFruit FColor Math. Numbers have denser compositional relations with each other, such as X+1=2X+2=3. We involve the four basic arithmetic operations in experiments to explore the knowledge composition in math. An example is, FX+1 FX+2 For each family, we include the results on 10 20 knowledge prompts in the main content to save the length and place the others in Appendix F. Table 1 showcases some examples of prompts and domains. We include different LLaMA-3 (Dubey et al., 2024) models in our experiments with parameter numbers of 1B, 3B, 8B, and 70B. We include the before and after post-training LMs for the evaluation of linear correlations resilience against fine-tuning. The variance in the model scale allows us to explore the generality and scaling law of the linear correlation inside different models. We include LMs from the same family to ensure consistency in tokenization and training data, allowing for more controlled and convenient discussion. Results on other LMs for broader generality are also included in Appendix F. 3.4. Experiment Results The main results of the linear correlation between NTP logits are presented in Figure 3, where we put subset of results. The whole massive results are plotted in Figures of Appendix F, which are referred to in the main discussions. Attribute The correlation pattern between attributes reflects prominent semantic factor behind the correlation. For instance, the spatial attributes {city, country, continent} are highly correlated with each other. Other attributes such as ethical attributes like language also show high correlations with spatial attributes. Besides spatial attributes, there are also highly correlated attribute clusters including job and family-related clusters. On the other hand, we can observe much weaker correlation between unrelated attributes in the real world, indicating that LMs disentangle the correlation. The gender attribute is good example, which cannot be identified by any other attribute, showing the effort of the LM to avoid gender bias (except the job attribute as some jobs like policeman and policewoman can identify the gender). These correlations reflect how knowledge is organized inside the parameters of LMs, which shows high consistency with the real world. There also exist related knowledge with poor correlation like LanguageContinent and CEOCompany, reflecting the limitation of LMs in comprehending all knowledge composition. Cross-language The results demonstrate some crosslingual correlation in LMs, which suggests that the knowledge is shared across languages to some degree. However, the correlation between the same concept in different languages is not as strong as related attributes, especially for languages in different families (e.g. EnglishChinese). The relatively weak correlation can be attributed to the dominance of English in LLaMA-3 training (Dubey et al., 2024), which we provide further insights using multilingual LM, Aya ( Ustun et al., 2024), in Appendix H. Simile As shown in Table 8 of Appendix A, the correlation between the attribute and the object in the simile also shows moderate linear correlation. This indicates that LMs bridge an object in similes with its attributes, which is another evidence that LMs can implicitly transfer knowledge. Math The results from the same math operator shows strong correlation with one another in Figure 7 of Appendix A. While this indicates strong mutual influences between calculations, we will show in the next subsection that the correlation in math is imprecise. 3.5. can Reflect Real-world Knowledge The weight matrix can reflect compositional relations between source and target domains. Thus, we check whether the weights reflect real-world knowledge. Specifically, for each token in the source (target) domain, we check whether the top-influenced (influencing) outputs (inputs), i,e. have the highest weights, are consistent with the real world. We use Hit@Top-N (N = 1, 3, 5) metric to evaluate whether there is correct influenced (influencing) token with top weight. In experiments that require closed reference, we test subset of knowledge pairs with clear causal relations (e.g., CityCountry rather than MotherFather). The experiment scale is relatively small due to the sparsity of knowledge composition with clear references. We analyze the precision of 2 cases from each family with the results presented in Table 2. We find the LM have relatively precise understanding of the cor5 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 3. The linear correlation between NTP logits of llama-3-8b. Hit@Top-N Relation Pair Influenced Target Influencing Source CityCountry CEOCompany CityenCityes CityenCityzh FruitColor FoodTaste X+1X+2 X+1X*2 1 0.42 0.09 0.91 0. 0.25 0.28 0.00 0.10 3 0.45 0.09 0.91 0.13 0.38 0. 0.50 0.40 5 0.48 0.14 0.92 0.16 0.47 0.62 0.60 0. 1 0.67 0.05 0.67 0.09 0.38 0.14 0.10 0.10 0.74 0.05 0.74 0.11 0.50 0.36 0.30 0.30 5 0.78 0. 0.78 0.15 0.54 0.43 0.50 0.70 If City = Shanghai NYC Oslo Seattle Indianapolis If + 1 = 1 2 3 4 5 Then Country = China, Italia, Albania, USSR, Korea USA, USSR, UAE, China, CCCP CCCP, Norway, Kosovo, Israel, Oman Uruguay, Serbia, Kosovo, Romania, Slovenia India, Indonesia, France, Iraq, Netherlands Then + 2 = 1, 2, 4, 6, 3 2, 3, 4, 5, 7 3, 6, 5, 4, 7 4, 0, 2, 1, 10 5, 6, 8, 7, 9 Table 2. The precision of compositional relations built up in . Table 3. Cases of top-influenced tokens pairs in target knowledge. relation between certain highly correlated attributes like CityCountry. In transformation matrix , 42% cities learn the top-1 weight with their influenced countries and 67% countries have correct top-1 influencing city. For less correlated CEOCompany attributes, is also imprecise, suggesting the failure to reflect the real-world causal relation. This phenomenon is also observed in the cross-language family for the strongly correlated EnglishSpanish and the weakly correlated EnglishChinese. However, strong correlation does not necessarily guarantee precise as shown in the math cases. In Table 3, we showcase some top-influenced tokens in the attribute and math correlations to visualize how reflects real-world correlations. In the CityCountry case, some cities like Shanghai and NYC are matched with the correct countries while some others like Oslo, Seattle, and Indonesia are not. The IndonesiaIndia case indicates bias introduced by superficial similarity into the weights in . The math cases show the correlation is dominated by identical mapping. While the LM tries to model correct correlation as many secondly influenced numbers are correct, the domination of identical mapping hinders the precision of to reflect real-world correlation. More cases in Appendix further support our observation and extend it to non-causal correlation like parent name correlation. 3.6. Is More Accurate in Larger LMs? Our discovery indicates that reflects real-world correlations between knowledge. We check whether the weights of are more in line with the real world knowledge for larger LMs. Thus, we plot the Top-N metric of correlations in LLaMA-3 of different model sizes in Figure 4. In the CityCountry case, we can view clear scaling-up of precision, showing that larger LMs also better organize their knowledge. However, CEOCompany is shown to be hard causal relation, whose precision is not successfully scaled up by larger model size. 4. Resilient Correlation against Training 4.1. Gradient Correlation As many weights in reflect the real-world correlation, we hypothesize that they are resilient against gradient propagation because they capture inherent patterns that resist change. 6 Linear Correlation in LMs Compositional Generalization and Hallucination Corr. Prec. Relation Pair Generalization (Random) High High High Low Low Low CityCountry CountryContinent CityenCityes X+1X+2 X+1X*2 FruitColor FoodTaste CEOCompany LanguageContinent CityenCityzh CityenCityja 53.70% (0.78%) 50.93% (20.00%) 39.10% (0.41%) 0.00% (9.09%) 8.18% (9.09%) 11.60% (6.67%) 19.44% (10.00%) 4.34% (1.00%) 23.65% (20.00%) 2.49% (0.41%) 4.60% (0.41%) Figure 4. The scaling-up of the precision of with model size. Relation Pair Logit Correlation Grad. Correlation Table 5. The ratio of successful generalization in relation pairs with different linear correlation and precision. CityCountry CEOCompany CityenCityes CityenCityzh FruitColor FoodTaste X+1X+2 X+1X*2 0.89 0.55 0.70 0. 0.48 0.47 0.93 0.73 0.79 0.47 0.79 0.46 0.46 0.47 0.87 0. Table 4. Correlation between gradients on related knowledge. Thus, we check whether the gradients on related knowledge prompts are also linearly correlated. We choose to train llama-3.2-3b5 with common setup for large LMs (AdamW (Loshchilov & Hutter, 2019) with 5 106 learning rate). We train the LM with different source knowledge and check whether there is gradient correlation existing between source and target knowledge. The gradient correlation results are presented in Table 4, demonstrating correlation between the gradients on different NTP logits. Specifically, with the gradient LogP on logit, we can estimate the gradient on correlated logit by LogP. If is precise one, the learned knowledge will also be correctly synchronized by knowledge composition caused by , such as ShanghaiChina. Thus, the correlation between gradients indicates potential mechanism behind how LMs compose learned knowledge. 4.2. Correlation after Large-scale Post-training We further extend our investigation from single update to the large-scale post-training of LMs. We check whether the linear correlation is still resilient to largescale post-training. Thus, we apply the linear transformation (W, b) fitted from an LM before post-training (e.g., llama-3-8b) to its corresponding LM after post-training (e.g., llama-3-8b-instruct). We run the same evaluation as in 3.4 and plot results in Figure 8 of Appendix A. Based on the comparison between the two correlation ma5We select the smaller 3B LM for fine-tuning efficiency, which shows similar correlation behavior as the 8B LM in Appendix F. trices in Figures 3 (before post-training) and 8 (after posttraining), we find the linear transformation still working after numerous optimization steps, indicating to be resilient against large-scale post-training. This further validates the role of linear correlation in the generalization of LMs, as further discussed in the next section. Another finding in Appendix is that the correlation resilience becomes stronger in larger LMs. 5. Correlation is Double-edged Sword The potential role of the linear correlation in knowledge composition inspires us to investigate how implicates the generalization of LMs. We anticipate the resilient correlation to be two-edged sword, which propagates knowledge with precise but also exacerbates hallucination with imprecise . For validation, we continue to fine-tune the llama-3.2-3b model. We first explore how the generalization is affected by the correlation and precision. In Table 5, we select relation pair representing high or low in correlation intensity and precision except for the unfounded low correlation and high precision situation. The results show the generalization is only significant when both correlation intensity and precision are high. We enumerate more knowledge pairs with low linear correlation than other situation to confirm their poor generalization. This implicates the linear correlation to be an indicator of generalization behavior. When the correlation intensity is high but the quality, the LM shows an expectable hallucination. In the X+1X+2 case, learning on any for X+1=N will generalize to high X+2=N as discussed in the case study in Figure 3. For further explanation, we check the weight of groundtruth pairs in the generalized and hallucinated cases of CityCountry. As shown in Figure 5, we find the weight to be an underlying factor in deciding whether the knowledge can be composed. Generally, higher weight on the ground-truth pair results in higher probability to generalization as the gradient will be more efficiently propagated, considering the observed gradient correlation in Table 4. 7 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 5. The effect of weights on generalization. City Reference Generalized Wref Wgen Wmax Shanghai NYC Copenhagen Karnataka Indianapolis Dresden Canberra Helsinki China USA Denmark India USA Germany Australia Finland China USA Denmark India India Israel Canada Sweden 0.50 0.58 0.47 0.34 0.05 0.04 0.04 0.42 0.50 0.58 0.47 0.34 0.15 0.13 0.10 0.11 0.50 0.58 0.47 0.56 0.17 0.15 0.10 0.42 Table 6. Generalization and hallucination in CityCountry. However, Figure 5 also shows that high weight does not guarantee successful generalization. To investigate the underlying reason, we make several case studies in Table 6. 1) The first 3 cases illustrate correct generalization with top weight. 2) The fourth case (KarnatakaIndia) shows generalization without top weight because India has high prior probability (bias) for its high frequency. In contrast, the top-influenced country Rwanda has low prior probability, making the hallucination in the gradient not explicitly propagated into the prediction. The hallucinated cases can also be divided into two categories. 1) Wrong weight, major reason of compositional hallucination. The fifth to seventh cases show low ground-truth weights, consequently leading to unsuccessful generalization. These cases also show relatively low maximal weight in , which is potentially an indicator of imprecise weights. 2) Low prior probability. The last case shows high weight between Helsinki and Finland but the prior probability of Finland is much lower than Sweden, which results in compositional hallucination. This is mirror case of the KarnatakaIndia generalization. Figure 6. We replace the deep intermediate layers of LMs with an initialized shallow bag-of-word network. Mapping Generalization (CityCountry) Shanghai, Tokyo, ParisChina, Japan, France Shanghai, Tokyo, ParisJapan, France, China S, T, PC, J, (CountryContinent) China, France, CanadaAsia, Europe, North (CEOCompany) Elon, Andy, TimTesla, Amazon, Apple (+1+2) 1, 2, 33, 4, 5 97.66% 22.66% 36.72% 78.12% 58.59% 9.38% Table 7. Generalization with Different Vocabulary Mappings. ing layer and single initialized feedforward network as shown in the Figure 6. To imitate the distribution causing the correlation, the feedforward network is then tuned with 1024 paired texts such as (X lives in the city of Shanghai, lives in the city of China) for 1000 epochs to learn the knowledge composition relations. For evaluation, the LM is tuned with 128 source knowledge such as lives in the city of Shanghai (Z different from any in training) for 2000 epochs. Then we check whether the LMs can predict composed knowledge, such as lives in the city of China. Several test results are presented in Table 7, showing consistent generalization performance as the initial deep Transformer model. When we switch the correspondence between cities and countries or keep only the first letter, the generalization behavior disappears, which highly attributes the generalization ability to the vocabulary representations. 6. What Causes the Correlation? 7. Conclusion Finally, we investigate the cause behind such linear correlation. Besides the pre-training data distribution, we hypothesize that vocabulary representations play crucial role in causing such correlations. This is because LMs with different intermediate architectures all show similar correlation behavior in Appendix F. To support our hypothesis, we launch simple ablation study by replacing the complex intermediate architectures (position embedding, self-attention, layer normalization, etc.) of LLaMA-3 with mean poolThis work reveals new perspective on how LMs generalize by knowledge composition. We detect linear correlations between related NTP logits, which are resilient to training. Such correlations are found to propagate updates on knowledge to one another, leading to compositional generalization and hallucination. We attribute the correlation to vocabulary representations with an ablation study. Future topics include further investigating the formation of such linear correlation and utilizing it for generalizable learning. 8 Linear Correlation in LMs Compositional Generalization and Hallucination"
        },
        {
            "title": "Impact Statement",
            "content": "This paper investigates the generalization mechanism behind LMs, which will not explicitly introduce any negative ethical or social impacts. Furthermore, our work have positive impact on detecting the potential compositional bias caused by unintended correlation with attributes like gender. Fortunately, no current popular LMs show significant compositional bias in gender according to our results."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A. C., Korbak, T., and Evans, O. The reversal curse: Llms trained on is fail to learn is a. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=GPKTIktA0k. Chen, Y., Fu, Q., Yuan, Y., Wen, Z., Fan, G., Liu, D., Zhang, D., Li, Z., and Xiao, Y. Hallucination detection: Robustly discerning reliable answers in large language models, 2024. URL https://arxiv.org/abs/2407. 04121. Cohen, R., Biran, E., Yoran, O., Globerson, A., and Geva, M. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 12:283298, 2024. Dai, D., Dong, L., Hao, Y., Sui, Z., Chang, B., and Wei, F. Knowledge neurons in pretrained transformers. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 84938502. Association for Computational Linguistics, 2022. doi: 10. 18653/V1/2022.ACL-LONG.581. URL https://doi. org/10.18653/v1/2022.acl-long.581. Demeter, D., Kimmel, G., and Downey, D. Stolen probability: structural weakness of neural language models. arXiv preprint arXiv:2005.02433, 2020. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Farquhar, S., Kossen, J., Kuhn, L., and Gal, Y. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Garg, S., Balakrishnan, S., Lipton, Z. C., Neyshabur, B., and Sedghi, H. Leveraging unlabeled data to predict out-of-distribution performance, 2022. URL https: //arxiv.org/abs/2201.04234. Geva, M., Schuster, R., Berant, J., and Levy, O. Transformer feed-forward layers are key-value memories. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 54845495. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021. EMNLP-MAIN.446. URL https://doi.org/10. 18653/v1/2021.emnlp-main.446. Geva, M., Caciularu, A., Wang, K. R., and Goldberg, Y. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 3045. Association for Computational Linguistics, 2022. doi: 10.18653/ URL https://doi. V1/2022.EMNLP-MAIN.3. org/10.18653/v1/2022.emnlp-main.3. Groeneveld, D., Beltagy, I., Walsh, E. P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A. H., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models. In Ku, L., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1578915809. Association for Computational Linguistics, 2024. doi: 10. 18653/V1/2024.ACL-LONG.841. URL https://doi. org/10.18653/v1/2024.acl-long.841. Gupta, A., Mondal, D., Sheshadri, A. K., Zhao, W., Li, X. L., Wiegreffe, S., and Tandon, N. Editing common sense in transformers. arXiv preprint arXiv:2305.14956, 2023. Hernandez, E., Sharma, A. S., Haklay, T., Meng, K., Wattenberg, M., Andreas, J., Belinkov, Y., and Bau, D. Linearity of relation decoding in transformer language In The Twelfth International Conference on models. Learning Representations, ICLR 2024, Vienna, Austria, 9 Linear Correlation in LMs Compositional Generalization and Hallucination May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=w7LU2s14kE. Hoelscher-Obermaier, J., Persson, J., Kran, E., Konstas, I., and Barez, F. Detecting edit failures in large language models: An improved specificity benchmark. arXiv preprint arXiv:2305.17553, 2023. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., and Liu, T. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems, November 2024. ISSN 1558-2868. doi: 10.1145/3703155. URL http://dx.doi.org/10.1145/3703155. Jha, S., Jha, S. K., Lincoln, P., Bastian, N. D., Velasquez, A., and Neema, S. Dehallucinating large language models using formal methods guided iterative prompting. In 2023 IEEE International Conference on Assured Autonomy (ICAA), pp. 149152. IEEE, 2023. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jiang, C., Qi, B., Hong, X., Fu, D., Cheng, Y., Meng, F., Yu, M., Zhou, B., and Zhou, J. On large language models hallucination with regard to known facts, 2024. URL https://arxiv.org/abs/2403.20009. Kang, K., Setlur, A., Ghosh, D., Steinhardt, J., Tomlin, C., Levine, S., and Kumar, A. What do learning dynamics reveal about generalization in llm reasoning?, 2024. URL https://arxiv.org/abs/2411.07681. Lampinen, A. K. and McClelland, J. L. Transforming task representations to perform novel tasks. Proc. Natl. Acad. Sci. USA, 117(52):3297032981, 2020. doi: 10.1073/ PNAS.2008852117. URL https://doi.org/10. 1073/pnas.2008852117. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https: //openreview.net/forum?id=Bkg6RiCqY7. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in GPT. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022a. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022b. Mishra, A., Asai, A., Balachandran, V., Wang, Y., Neubig, G., Tsvetkov, Y., and Hajishirzi, H. Fine-grained hallucination detection and editing for language models, 2024. URL https://arxiv.org/abs/2401.06855. Onoe, Y., Zhang, M. J., Padmanabhan, S., Durrett, G., and Choi, E. Can lms learn new entities from descriptions? challenges in propagating injected knowledge. arXiv preprint arXiv:2305.01651, 2023. Peng, B., Narayanan, S., and Papadimitriou, C. H. On CoRR, limitations of the transformer architecture. abs/2402.08164, 2024a. 10.48550/ARXIV. 2402.08164. URL https://doi.org/10.48550/ arXiv.2402.08164. doi: Peng, L., An, C., and Shang, J. Correlation and navigation in the vocabulary key representation space of language models. CoRR, abs/2410.02284, 2024b. doi: 10.48550/ ARXIV.2410.02284. URL https://doi.org/10. 48550/arXiv.2410.02284. Press, O. and Wolf, L. Using the output embedding to improve language models. In Lapata, M., Blunsom, P., and Koller, A. (eds.), Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017, Valencia, Spain, April 3-7, 2017, Volume 2: Short Papers, pp. 157163. Association for Computational Linguistics, 2017. doi: 10.18653/V1/E17-2025. URL https://doi.org/ 10.18653/v1/e17-2025. Quevedo, E., Yero, J., Koerner, R., Rivas, P., and Cerny, T. Detecting hallucinations in large language model generation: token probability approach, 2024. URL https://arxiv.org/abs/2405.19648. Su, W., Wang, C., Ai, Q., HU, Y., Wu, Z., Zhou, Y., and Liu, Y. Unsupervised real-time hallucination detection based on the internal states of large language models, 2024. URL https://arxiv.org/abs/2403.06448. Team, G., Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Thomm, J., Camposampiero, G., Terzic, A., Hersche, M., Scholkopf, B., and Rahimi, A. Limits of transformer language models on learning to compose algorithms. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 10 Linear Correlation in LMs Compositional Generalization and Hallucination standing of the reversal curse via training dynamics. CoRR, abs/2405.04669, 2024. doi: 10.48550/ARXIV. 2405.04669. URL https://doi.org/10.48550/ arXiv.2405.04669. Todd, E., Li, M. L., Sharma, A. S., Mueller, A., Wallace, B. C., and Bau, D. Function vectors in large language In The Twelfth International Conference on models. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=AwyxtyMwaG. Ustun, A., Aryabumi, V., Yong, Z. X., Ko, W., Dsouza, D., Onilude, G., Bhandari, N., Singh, S., Ooi, H., Kayid, A., Vargus, F., Blunsom, P., Longpre, S., Muennighoff, N., Fadaee, M., Kreutzer, J., and Hooker, S. Aya model: An instruction finetuned open-access multilingual language model. In Ku, L., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1589415939. Association for Computational Linguistics, 2024. doi: 10.18653/V1/ 2024.ACL-LONG.845. URL https://doi.org/ 10.18653/v1/2024.acl-long.845. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022, 2022. URL https://openreview.net/forum? id=yzkSU5zdwD. Yu, Y., Yang, Z., Wei, A., Ma, Y., and Steinhardt, J. Predicting out-of-distribution error with the projection norm, 2022. URL https://arxiv.org/abs/ 2202.05834. Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y., Chen, Y., Wang, L., Luu, A. T., Bi, W., Shi, F., and Shi, S. Sirens song in the ai ocean: survey on hallucination in large language models, 2023. URL https://arxiv.org/abs/2309.01219. Zhang, Z., Zhao, J., Zhang, Q., Gui, T., and Huang, X. Unveiling linguistic regions in large language models. In Ku, L., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 62286247. Association for Computational Linguistics, 2024. doi: 10. 18653/V1/2024.ACL-LONG.338. URL https://doi. org/10.18653/v1/2024.acl-long.338. Zhong, Z., Wu, Z., Manning, C. D., Potts, C., and Chen, D. Mquake: Assessing knowledge editing in language models via multi-hop questions. arXiv preprint arXiv:2305.14795, 2023. Zhu, H., Huang, B., Zhang, S., Jordan, M. I., Jiao, J., Tian, Y., and Russell, S. Towards theoretical under11 Linear Correlation in LMs Compositional Generalization and Hallucination Table 8. Correlation between gradients on simile objects and attributes. Relation Pair Fruit-Color Food-Taste Gem-Color Name-Country Animal-Size Correlation 48.42 46.68 27. 67.35 59.59 Relation Pair Object-Genre Object-Heat Object-Size Object-Price Object-Color Correlation 77.68 73.11 71.41 72.87 70.87 Figure 7. The linear correlation between NTP logits of llama-3-8b in math operations. A. Results for Main Content In Table 8, Figures 7 and 8, we illustrate the experiment results for the main content because of the length limitation. Table 8 demonstrates the correlation between simile objects and attributes. Figure 7 shows high correlation between math calculation results. Figure 8 presents the linear correlation between logits from knowledge before and after large-scale post-training, which is compared with the results in Figure 3 to conclude resilient linear correlation against fine-tuning. The cross-tuning results for simile and math families are presented in Table 9 and Figure 9, which validate resilient correlation against post-training for highly correlated knowledge pairs. Note that the concepts in Object (apple, t-shirt, laptop, chair, washing machine, etc.) for simile relations do not directly indicate attributes, so they are not used for evaluation when reference is required. 12 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 8. The linear correlation between NTP logits of llama-3-8b before and after large-scale post-training. Table 9. Correlation between logits on simile objects and attributes before and after large-scale post-training. Relation Pair Fruit-Color Food-Taste Gem-Color Name-Country Animal-Size Correlation 44.11 37.06 33.66 67.30 49.65 Relation Pair Object-Genre Object-Heat Object-Size Object-Price Object-Color Correlation 72.03 63.75 66. 71.09 66.27 Figure 9. The linear correlation between NTP logits in math operations before and after large-scale post-training. 13 Linear Correlation in LMs Compositional Generalization and Hallucination Template Attribute Cross-language Simile Math Total Domain Size 23 11 5 = 55 17 4 4 = 16 111 Table 10. The statistics of prompts in different families. Knowledge Template Domain Size b t birthplace city country continent language company landmark ceo mother father job personality pet sport food drink gender vehicle color music hobby flower vacation {} was born in the city of {} lives in the city of {} lives in the country of {} lives in the continent of {} speaks the language of {} works for the company of {} lives near the landmark of {} works for the CEO called {}s mothers name is {}s fathers name is {}s job is {}s personality is {}s pet is {}s favorite sport is {}s favorite food is {}s favorite drink is {}s gender is {}s preferred mode of transportation is {}s favorite color is {}s favorite music genre is {}s favorite hobby is {}s favorite flower is {}s favorite vacation spot is 242 242 128 6 217 100 100 101 100 100 105 100 100 102 104 102 3 51 15 100 101 97 101 Table 11. Templates used in our experiments (Part 1: Attribute). B. Limitation and Future Works As pioneering study, our work focuses on uncovering the phenomenon of linear correlations in language models but leaves several key aspects for future research: Theoretical Explanation We do not provide formal theory explaining why resilient linear correlations emerge. Future work can explore the underlying model architectures, optimization dynamics, and linguistic structures that drive this phenomenon. Data Distribution Effects Our study does not systematically analyze how training data influences the formation of these correlations. Investigating which data properties contribute to their emergence could provide deeper insights. Identifying Correlated Knowledge Pairs While we observe linear correlations in specific cases (e.g., citycountry), we do not establish general method to predict what knowledge pairs exhibit this property. Future work can develop theoretical or empirical criteria for identifying such relationships. Due to content limitations, we focus on describing the phenomenon rather than fully explaining its origins. We hope our findings serve as foundation for further research into the mechanisms and implications of linear correlations in LMs. C. Prompts and Setups Table 10 shows the statistics of the prompts used in our experiments. Tables 11, 12, 13 further list all the specific prompts used in our experiments. The domain size of most prompts is around 100 expect for some domains with limited valid outputs like Continent and Color. 14 Linear Correlation in LMs Compositional Generalization and Hallucination Knowledge Template Domain Size n S n n e e C n J birthplace city country continent language company ceo job mother father gender birthplace city country continent language company ceo job mother father gender birthplace city country continent language company ceo job mother father gender birthplace city country continent language company ceo job mother father gender birthplace city country continent language company ceo job mother father gender {} nacio en la ciudad de {} vive en la ciudad de {} vive en el paıs de {} vive en el continente de {} habla el idioma de {} trabaja para la empresa de {} trabaja para el CEO llamado El trabajo de {} es El nombre de la madre de {} es {} el nombre del padre es El genero de {} es {} est ne dans la ville de {} vit dans la ville de {} vit dans le pays de {} vit sur le continent de {} parle la langue de {} travaille pour lentreprise de {} travaille pour le PDG appele {} travaille comme Le nom de la m`ere de {} est Le nom du p`ere de {} est {} est de sexe {} wurde in der Stadt geboren {} lebt in der Stadt {} lebt im Land {} lebt auf dem Kontinent {} spricht die Sprache von {} arbeitet fur das Unternehmen von {} arbeitet fur den CEO namens Der Beruf von {} ist Der Name von {}s Mutter ist Der Name von {}s Vater ist Das Geschlecht von {} ist {}所出生的城市是 {}所居住的城市是 {}所居住的国家是 {}所居住的大陆是 {}说的语言是 {}工作的公司是 {}工作的公司的CEO是 {}的工作是 {}的母亲的名字是 {}的父亲的名字是 {}的性别是 {}が生まれた都市は {}が住んでいる都市は {}が住んでいる国は {}が住んでいる大陸は {}が話している言語は {}が働いている会社は {}が働いている会社のCEOは {}の仕事は {}の母の名前は {}の父の名前は {}の性別は 242 242 128 6 217 100 101 105 100 100 3 242 242 128 6 217 100 101 105 100 100 3 242 242 128 6 217 100 101 105 100 100 3 242 242 128 6 217 100 101 105 100 100 3 242 242 128 6 217 100 101 105 100 100 3 Table 12. Templates used in our experiments (Part 2: Cross Language). 15 Linear Correlation in LMs Compositional Generalization and Hallucination Knowledge Template Domain Size object color object price object heat object genre object size simile color simile price simile heat simile genre simile size simile taste name country gem color animal size food taste fruit color X+N X-N X*N X/N m t The color of {} is the same as The size of {} is the same as The heat of {} is the same as The genre of {} is the same as The size of {} is the same as The color of {} is The size of {} is The heat of {} is The genre of {} is The size of {} is The taste of {} is {} lives in the same country as The color of {} is the same as the gem called The size of {} is the same as the animal called {} has the same taste as the food: {} has the same color as the fruit: {}+N= {}-N= {}*N= {}/N= 85 85 85 85 85 15 2 4 22 3 3 128 50 100 95 99 11 11 11 11 Table 13. Templates used in our experiments (Part 3: Simile and Math). D. Instance-wise Correlation Figure 10 shows the instance-wise Pearson correlation evaluation results on different knowledge pairs. We use attribute correlation as an example to show that the target knowledge of each instance can be well approximated by linear transformation on the source knowledge. In the main content, we demonstrate the label-wise correlation because we find the bias term to dominate the prediction on many knowledge pairs that are poorly linear correlated (especially in gradient). Some target knowledge is predictable with only the prior probability from bias even without any linear indicator. Thus, the label-wise correlation is more challenging metric by eliminating the effect of with better reflection of how the source knowledge influences the target knowledge. 16 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 10. The instance-wise correlation between NTP logits of llama3-8b (attribute as an example). 17 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 11. The attribute correlation between NTP logits of gpt2-medium. E. Subdomain Building Procedure To build the subdomains, we do not simply collect the top predictions from the next token predictions because many predictions are introduced by the frequency and similarity bias (e.g., stop words like the) in the next token representation space (Demeter et al., 2020; Peng et al., 2024b). Instead, we enumerate the common answers by gpt-4o (Achiam et al., 2023) and search engines. Then we keep the first tokens of the tokenization for these answers which are not subwords. For example,China will be represented by China, South Korean will be represented by South, and Brunei will be dropped because it is tokenized into [Br, unei]. We exclude subwords because they cannot identify complete semantics without tokens after them. The discussion for subword cases is included in Appendix K. F. Whole Attribute Results and Extra Discussion From Figure 11 to Figure 19, we present the whole correlation matrices inside all kinds of LMs for different prompts. We can observe the existence of correlation behavior among different LMs. While the correlation in different LMs behaves differently, some common pairs like CityCountry hold for all different LMs. Also, models from the same LLaMA-3 family tend to behave in similar way. We can also observe many spurious correlations such as HobbyMother, which generally have low causal relations in the real world. Larger LMs tend to be better at disentangling such kind of spurious correlations as the smallest GPT2-Medium model shows much stronger correlation. In Figures 18 and 19, Table 14, we illustrate that the 3B model has similar correlation behavior as the 8B one. 18 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 12. The attribute correlation between NTP logits of llama-3.2-1b. 19 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 13. The attribute correlation between NTP logits of llama-3.2-3b. 20 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 14. The attribute correlation between NTP logits of llama-3-8b. 21 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 15. The attribute correlation between NTP logits of llama-3-70b. 22 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 16. The attribute correlation between NTP logits of deepseek-r1-distll-qwen-7B. 23 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 17. The attribute correlation between NTP logits of mistral-7b-v0.3. 24 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 18. The linear correlation between NTP logits of llama-3.2-3b. Relation Pair Fruit-Color Food-Taste Gem-Color Name-Country Animal-Size Correlation 48.37 46.95 50.48 78.83 69. Relation Pair Object-Genre Object-Heat Object-Size Object-Price Object-Color Correlation 81.92 76. 84.23 84.23 81.08 Table 14. Correlation between logits of llama-3.2-3b on simile objects and attributes. 25 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 19. The linear correlation between NTP logits of llama-3.2-3b before and after large-scale post-training. G. More Resilient Correlation in Larger LMs In Figure 20, we find the linear correlation is more resilient against fine-tuning by plotting the correlation before and after post-training in 1B, 3B, 8B LLaMA-3 LMs as we find more strong correlations in larger LMs. In Figure 21, we also plot the correlation matrix between logits from mistral-7b-v0.3 before and after post-training, which supports the existence of resilient linear correlation in LMs with vocabulary representation untied. 26 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 20. The correlation becomes more resilient in larger LMs. 27 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 21. The correlation between logits from mistral-7b-v0.3 before and after post-training. 28 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 22. The comparison between Aya and LLaMA in cross-lingual correlation. H. Multilingual LM Figure demonstrates the cross-lingual correlation of the multilingual LM, aya-expanse-8b, which outperforms LLaMA-3 in multilingual tasks but still lags behind in English ( Ustun et al., 2024). The results show Aya to have stronger cross-lingual correlation between knowledge pairs, especially in Chinese and Japanese. On Latin language, Ayas advantage becomes smaller because these languages share quite lot entity names with English and LLaMA-3 can benefit from its English ability to complement the weakness in multi-lingual ability. 29 Linear Correlation in LMs Compositional Generalization and Hallucination Country Sweden Cuba Switzerland Ghana Poland Turkey Sudan Romania Samoa Iceland Nigeria Iraq Laos USSR Kosovo China Guatemala Tunisia Denmark Nicaragua Turkiye Bosnia Netherlands Malaysia Venezuela Sri Ireland Liberia Afghanistan America Austria Scotland Libya Uruguay Bangladesh Bahrain Pakistan Fiji Cambodia Singapore Macedonia Mongolia Peru Myanmar Trinidad Colombia Maurit Iran India Spain Honduras USA Influencing Cities Stockholm, Brisbane, Johannesburg, Cardiff, Chicago, Hyderabad, Aleppo, Lima, Rochester, Salem Havana, Chicago, Columbus, stockholm, Rochester, Hyderabad, Scarborough, Johannesburg, singapore, Hamburg Columbus, Stuttgart, Cardiff, Leicester, Chicago, Brisbane, Saras, stockholm, vegas, Bethlehem Winnipeg, Nairobi, Johannesburg, Leicester, Atlanta, Tulsa, Maharashtra, Greenville, Brisbane, Lima Warsaw, Cardiff, Liverpool, Maharashtra, stockholm, Amsterdam, Atlanta, Kashmir, Perth, Aleppo Istanbul, Chicago, Toronto, Maharashtra, stockholm, Johannesburg, Cardiff, Lima, Columbus, Ankara Nairobi, stockholm, Lima, Tulsa, Johannesburg, Maharashtra, Winnipeg, Hyderabad, Wilmington, Kashmir Cardiff, Rochester, Johannesburg, Budapest, Seattle, Rajasthan, Hyderabad, Chicago, Kyoto, Lima Maharashtra, Leicester, Winnipeg, Chicago, Honolulu, Brisbane, Nairobi, Hyderabad, Lima, Cardiff Cardiff, Leicester, Chicago, Amsterdam, Wilmington, Islamabad, Winnipeg, Kyoto, Hyderabad, stockholm Winnipeg, Nairobi, Maharashtra, Lagos, Johannesburg, Stuttgart, Leicester, Abu, Chicago, Tulsa Chicago, Hyderabad, Wilmington, Lima, Baghdad, stockholm, Kashmir, Tulsa, Belfast, singapore Bangkok, Leicester, Chicago, Kashmir, Tulsa, stockholm, Winnipeg, Lima, Rajasthan, Johannesburg Moscow, NYC, Midlands, stockholm, Chicago, Cardiff, Maharashtra, Pyongyang, Boulder, Columbus Kashmir, Seattle, Leicester, stockholm, Tulsa, Belfast, Mosul, vegas, Rochester, Buenos Beijing, Shanghai, Hyderabad, Brisbane, Columbus, stockholm, Maharashtra, Amsterdam, Leicester, Hamburg Greenville, Tulsa, Leicester, Buenos, Johannesburg, Kashmir, Wilmington, Lima, Chicago, Rochester Johannesburg, stockholm, Hamburg, Columbus, Leicester, Tulsa, Stuttgart, Winnipeg, Cardiff, Maharashtra Copenhagen, Cardiff, Leicester, Brisbane, Hyderabad, Atlanta, Saras, Chicago, Hamburg, Salem Nairobi, Bangkok, Rochester, Leicester, Amsterdam, Kerala, Maharashtra, Belfast, Winnipeg, Chicago Maharashtra, Munchen, Seattle, Istanbul, stockholm, Jakarta, Istanbul, Toronto, Milwaukee, Kyoto Hyderabad, Islamabad, Belfast, Johannesburg, Jakarta, Cardiff, Rochester, Kashmir, Leicester, Lima Amsterdam, Cardiff, Midlands, Columbus, Karachi, stockholm, Nottingham, Maharashtra, Saras, Wilmington Leicester, Kuala, Cardiff, Hamburg, Maharashtra, Baltimore, Chicago, Columbus, Johannesburg, Hyderabad Wilmington, vegas, Cardiff, Maharashtra, Rochester, Brisbane, stockholm, Buenos, Lima, Tulsa Leicester, Atlanta, Kashmir, Rajasthan, Nairobi, Cardiff, stockholm, Lima, Maharashtra, Islamabad Dublin, Cardiff, Belfast, Leicester, Tehran, Johannesburg, Stuttgart, Aleppo, Bethlehem, Hyderabad Leicester, Winnipeg, Nairobi, Johannesburg, Chicago, Kerala, Rochester, Maharashtra, Atlanta, Greenville Kabul, Cardiff, Islamabad, stockholm, Tulsa, Chicago, Maharashtra, Kashmir, Rajasthan, Leicester Columbus, Chicago, Belfast, Sofia, Hyderabad, Seattle, Cardiff, Johannesburg, Maharashtra, Moscow Cardiff, Vienna, Hamburg, Hyderabad, Leicester, Bethlehem, Stuttgart, stockholm, Columbus, Rajasthan Cardiff, Glasgow, Edinburgh, Stuttgart, stockholm, Belfast, Leicester, Columbus, Maharashtra, Lima Chicago, stockholm, Columbus, Leicester, Aleppo, Cardiff, Mosul, Lima, Wilmington, Johannesburg Buenos, Seattle, Hyderabad, Maharashtra, Hamburg, Johannesburg, Wilmington, Leicester, Columbus, Cardiff Winnipeg, Cardiff, Leicester, Maharashtra, Tulsa, Atlanta, Chicago, Bangalore, Islamabad, Kashmir Leicester, Chicago, Brisbane, Kashmir, Lima, Riyadh, Dubai, Wilmington, Atlanta, Saras Islamabad, Cardiff, Jakarta, Karachi, Tulsa, Leicester, Winnipeg, Atlanta, Maharashtra, Wilmington Lima, Leicester, Fargo, Kashmir, Brisbane, Winnipeg, Johannesburg, Cardiff, Tulsa, Edinburgh Bangkok, Tulsa, Leicester, Cardiff, stockholm, Kashmir, Johannesburg, Wilmington, Kabul, Lima singapore, Chicago, Leicester, Brisbane, Hamburg, Columbus, Atlanta, Kashmir, Johannesburg, Cardiff Leicester, Stuttgart, Winnipeg, Rochester, Kashmir, Johannesburg, Jakarta, Maharashtra, Budapest, Lima Winnipeg, Chattanooga, Leicester, Lima, Cardiff, Kyoto, Maharashtra, Johannesburg, Rajasthan, Hamburg Lima, Perth, Maharashtra, Winnipeg, Leicester, Chattanooga, Seattle, Hyderabad, Nairobi, Chicago Bangkok, Cardiff, Tulsa, Leicester, Winnipeg, Kashmir, Maharashtra, Kyoto, Lima, Chicago Leicester, Cardiff, Maharashtra, Brisbane, Rochester, Tulsa, Winnipeg, Abu, vegas, Johannesburg Maharashtra, Columbus, Lima, Seattle, Rochester, Wilmington, Johannesburg, Stuttgart, Amsterdam, Hyderabad Winnipeg, Leicester, Johannesburg, Edinburgh, Cardiff, Chicago, Stuttgart, stockholm, Moscow, Wilmington Tehran, Cardiff, Lima, Kashmir, Hyderabad, Leicester, Aleppo, Chicago, Stuttgart, Hamburg Indianapolis, Cardiff, Maharashtra, Chicago, Hyderabad, Leicester, Lima, Columbus, Winnipeg, stockholm Madrid, Hyderabad, stockholm, Spokane, Cardiff, Amsterdam, Rome, Barcelona, Dallas, Johannesburg Wilmington, Winnipeg, Buenos, Hamburg, Nairobi, stockholm, Johannesburg, Amsterdam, Columbus, Lima NYC, Moscow, Columbus, Midlands, Chicago, Sofia, Karnataka, Karachi, Cardiff, Sevilla Table 15. The most influencing cities of counties in the CityCountry correlation. I. Extra Case Study We provide extra cases for analysis in this section. In Table 15, we provide massive cases on the influencing cities in the CityCountry knowledge composition, which shows that the LM establishes correlation between many (City, Country) pairs such as (Edinburgh, Scotland), (Islamabad, Pakistan), and (Afghanistan, Kabul). Tables 16 and 17 showcase the correlation between knowledge pairs that do not have clear reference. Taking parent correlation as an example, Table 16 shows correlation of parent names from the same ethnicity like (Chen, Mei) and (Santiago, Sofia). Linear Correlation in LMs Compositional Generalization and Hallucination Father Omar Victor Andre Julio Enrique Amir Xavier Javier Vlad Roberto Lars Min James Giovanni Ivan Diego Fernando Ethan Chen Gabriel Boris Jean Dmitry Ahmed Wei Ibrahim Liam Mustafa Jorge Leonardo Luca Carlos Pedro Michel Kai Benjamin Noah Ali Levi Antonio Rafael Marco Stefan Chung Abdul Muhammad Hugo Axel Lucas Mason Hassan Pablo Raphael Elijah Louis Ricardo Samuel William Salman Oliver Angelo Hans Jamal Santiago Influencing Mothers Olivia, Nora, Sara, Sofia, Naomi, Diana, Uma, Rosa, Eden, Jade Victoria, Sofia, Maria, Savannah, Sophie, Uma, Sonia, Angela, Grace, Ivy Angela, Sofia, Sophie, Savannah, Maria, Rebecca, Ivy, Clara, Chloe, Nina Sofia, Chloe, Maria, Carmen, Rebecca, Ivy, Rosa, Olivia, Sonia, Savannah Carmen, Chloe, Rosa, Clara, Sofia, Emma, Maria, Rebecca, Fiona, Olivia Sara, Sofia, Amelia, Eden, Mei, Nora, Uma, Bella, Victoria, Diana Sophie, Maria, Sonia, Olivia, Emma, Leah, Clara, Uma, Jasmine, Carmen Carmen, Chloe, Sofia, Ivy, Maria, Jasmine, Olivia, Rosa, Fiona, Jennifer Elena, Sofia, Chloe, Mia, Nina, Angela, Diana, Naomi, Savannah, Clara Chloe, Sofia, Rosa, Carmen, Lucia, Olivia, Clara, Mei, Maria, Elena Sophie, Clara, Maria, Nina, Ella, Sara, Harper, Savannah, Rebecca, Fiona Sonia, Mei, Angela, Eden, Clara, Chloe, Grace, Maria, Harper, Savannah Grace, Fiona, Ella, Savannah, Emma, Angela, Chloe, Harper, Leah, Maria Lucia, Fiona, Sofia, Savannah, Rosa, Diana, Bella, Chloe, Carmen, Mei Ivy, Elena, Sofia, Nina, Maria, Ada, Emma, Sophie, Savannah, Sakura Chloe, Sofia, Maria, Rosa, Angela, Carmen, Savannah, Diana, Clara, Mei Maria, Rosa, Fiona, Savannah, Carmen, Angela, Sofia, Luna, Clara, Ada Elena, Leah, Jennifer, Emma, Jasmine, Chloe, Clara, Mei, Ada, Serena Mei, Chloe, Grace, Nina, Eden, Harper, Sofia, Rebecca, Sakura, Sonia Maria, Sophie, Eden, Leah, Sara, Grace, Chloe, Rebecca, Elena, Luna Bella, Elena, Angela, Fiona, Nina, Ada, Sofia, Sophie, Nora, Leah Sophie, Angela, Chloe, Maria, Naomi, Carmen, Savannah, Nina, Rebecca, Lucia Sofia, Elena, Chloe, Diana, Nina, Savannah, Mia, Clara, Sakura, Ivy Sara, Sofia, Sophie, Nora, Uma, Victoria, Eden, Sonia, Jennifer, Mei Mei, Chloe, Grace, Rebecca, Mia, Sofia, Ada, Nina, Angela, Harper Sofia, Sara, Eden, Uma, Victoria, Nora, Bella, Ada, Sophie, Elena Fiona, Emma, Mia, Chloe, Nora, Leah, Grace, Jasmine, Jade, Angela Sara, Sofia, Nora, Victoria, Ada, Uma, Eden, Jade, Rosa, Elena Maria, Carmen, Rosa, Chloe, Sofia, Diana, Elena, Fiona, Angela, Nora Clara, Sofia, Jennifer, Olivia, Chloe, Jasmine, Fiona, Rosa, Lucia, Diana Fiona, Lucia, Sofia, Angela, Maria, Savannah, Emma, Clara, Sakura, Leah Carmen, Maria, Rosa, Olivia, Chloe, Sofia, Clara, Sakura, Savannah, Fiona Maria, Rosa, Carmen, Chloe, Olivia, Clara, Sakura, Sofia, Ivy, Ada Sophie, Lucia, Nina, Maria, Leah, Eden, Elena, Sara, Sonia, Carmen Mei, Maria, Nina, Angela, Chloe, Eden, Jade, Uma, Sakura, Ada Leah, Eden, Bella, Rebecca, Sophie, Grace, Nina, Harper, Lucia, Victoria Rebecca, Chloe, Nina, Nora, Eden, Naomi, Sara, Grace, Leah, Ada Sara, Nora, Eden, Victoria, Uma, Sofia, Mei, Jade, Bella, Sonia Chloe, Leah, Eden, Sara, Nina, Elena, Harper, Bella, Rosa, Rebecca Rosa, Maria, Angela, Lucia, Sofia, Chloe, Savannah, Olivia, Carmen, Fiona Sofia, Rosa, Carmen, Maria, Clara, Leah, Ivy, Chloe, Naomi, Lucia Maria, Sofia, Jasmine, Lucia, Clara, Angela, Chloe, Mei, Rebecca, Carmen Elena, Fiona, Angela, Savannah, Clara, Sophie, Mei, Maria, Eden, Rebecca Mei, Chloe, Grace, Maria, Angela, Sonia, Harper, Clara, Savannah, Mia Uma, Sara, Sofia, Nora, Jennifer, Ada, Rosa, Victoria, Eden, Bella Sofia, Sara, Victoria, Mei, Emily, Jennifer, Nora, Uma, Eden, Naomi Maria, Sophie, Chloe, Clara, Fiona, Emma, Savannah, Angela, Carmen, Ivy Sophie, Angela, Rebecca, Nina, Ada, Emma, Fiona, Ivy, Eden, Savannah Lucia, Maria, Clara, Fiona, Uma, Chloe, Harper, Savannah, Sophie, Jasmine Harper, Leah, Jasmine, Chloe, Angela, Nina, Ada, Sofia, Ella, Emma Sara, Eden, Nora, Victoria, Bella, Sofia, Naomi, Savannah, Mei, Diana Maria, Chloe, Sofia, Rosa, Savannah, Rebecca, Carmen, Elena, Fiona, Luna Rebecca, Sophie, Elena, Leah, Rosa, Grace, Eden, Fiona, Clara, Sonia Elena, Eden, Rebecca, Chloe, Savannah, Ella, Leah, Emily, Grace, Uma Sophie, Nina, Savannah, Grace, Rosa, Maria, Rebecca, Fiona, Leah, Sonia Chloe, Carmen, Sofia, Rosa, Jennifer, Clara, Rebecca, Sakura, Mei, Olivia Sonia, Savannah, Leah, Eden, Rebecca, Sophie, Grace, Ada, Emma, Clara Grace, Emma, Emily, Leah, Ada, Harper, Angela, Victoria, Fiona, Diana Sonia, Sofia, Nora, Uma, Sara, Bella, Eden, Jennifer, Victoria, Leah Olivia, Sophie, Harper, Elena, Nina, Maria, Grace, Diana, Emma, Nora Angela, Sofia, Fiona, Clara, Chloe, Rosa, Carmen, Savannah, Lucia, Nina Sophie, Rebecca, Angela, Savannah, Eden, Ella, Clara, Maria, Uma, Mei Sofia, Jasmine, Uma, Sara, Mei, Eden, Naomi, Victoria, Bella, Diana Sofia, Maria, Rosa, Carmen, Chloe, Savannah, Mei, Olivia, Ivy, Luna Table 16. The most influencing fathers of mothers in the MotherFather correlation. 31 Linear Correlation in LMs Compositional Generalization and Hallucination Attribute toys transport kitchen furniture decor accessories sports travel art fitness outdoors bags electronics clothing food photography literature appliances home music warm hot neutral cold large medium small black green blue beige gold natural silver orange red gray brown yellow purple white high low e e i l e P Influencing Objects toy, puzzle, drum, shoes, sweater, electric, fridge, gloves, chair, jeans headphones, pen, plate, drum, electric, car, couch, smartphone, rug, suitcase drum, jeans, pen, plate, toy, backpack, rug, fridge, chair, grill drum, chair, fridge, electric, rug, camera, puzzle, shoes, sweater, plate drum, rug, vase, pen, sweater, jeans, smartphone, backpack, washing, speaker drum, shoes, plate, laptop, electric, oven, gloves, curtains, jeans, chair basketball, pen, drum, jeans, plate, skateboard, tennis, rug, charger, puzzle pen, drum, water, yoga, suitcase, sunglasses, watch, plate, jeans, fridge drum, puzzle, pen, scarf, water, camera, couch, toy, chair, jeans yoga, puzzle, drum, pen, couch, electric, sweater, scarf, rug, camera drum, plate, pen, fishing, electric, water, couch, camera, toy, puzzle drum, fridge, sweater, gloves, jeans, backpack, pen, rug, electric, umbrella electric, drum, headphones, plate, toy, pen, laptop, jeans, sweater, couch drum, sweater, electric, shoes, skateboard, pen, jeans, camera, rug, fridge fridge, drum, pen, water, scarf, couch, plate, smartphone, sweater, speaker camera, water, drum, puzzle, scarf, skateboard, yoga, headphones, rug, couch book, iron, pen, drum, yoga, couch, water, speaker, scarf, fan electric, sweater, jeans, plate, shoes, fridge, drum, chair, oven, laptop electric, oven, drum, smartphone, pen, backpack, rug, jeans, fridge, puzzle guitar, drum, headphones, scarf, basketball, pen, toy, puzzle, suitcase, water hoodie, sweater, clock, lamp, drum, earrings, yoga, apple, tennis, oven hoodie, puzzle, tennis, drum, oven, jeans, car, lamp, earrings, fan jeans, speaker, blanket, sofa, car, puzzle, earrings, hoodie, tennis, rug hoodie, car, earrings, fan, lamp, curtains, couch, clock, puzzle, sweater smartphone, jeans, drum, puzzle, hoodie, umbrella, pencil, clock, car, backpack hoodie, tripod, car, keyboard, drum, suitcase, smartphone, basketball, curtains, bottle smartphone, hoodie, car, drum, pencil, jeans, backpack, keyboard, puzzle, toy jeans, iron, fan, umbrella, hoodie, suitcase, puzzle, bowl, printer, electric backpack, plate, puzzle, jeans, couch, umbrella, drum, soap, car, sweater jeans, electric, puzzle, plate, backpack, fishing, bottle, chair, car, umbrella jeans, soap, hoodie, drum, puzzle, bottle, suitcase, oven, bed, speaker puzzle, backpack, car, earrings, iron, bottle, drum, jeans, plate, fan jeans, bottle, puzzle, earrings, car, plate, oven, yoga, suitcase, drum bottle, jeans, puzzle, iron, drum, mirror, soap, electric, backpack, earrings puzzle, car, drum, backpack, jeans, umbrella, bottle, electric, oven, plate car, drum, earrings, puzzle, microwave, pen, umbrella, bowl, electric, backpack jeans, soap, mouse, puzzle, plate, sweater, umbrella, printer, bed, backpack soap, iron, puzzle, sweater, umbrella, backpack, speaker, drum, hoodie, couch plate, yoga, car, backpack, umbrella, soap, drum, puzzle, sweater, fan puzzle, drum, electric, hoodie, backpack, jeans, microwave, mouse, bottle, bowl plate, suitcase, fan, jeans, puzzle, backpack, soap, umbrella, sweater, drum smartphone, drum, air, car, hoodie, jeans, backpack, umbrella, puzzle, electric drum, jeans, backpack, smartphone, car, hoodie, air, umbrella, puzzle, electric Table 17. The most influencing objects of attributes in the simile correlation. 32 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 23. The std of correlation distribution between logits. J. Low Dispersion in Label-wise Correlation potential concern on the correlation metric is whether the correlation reflects the majority property of different labels or some highly correlated cast bias into the evaluation. We plot the std of label-wise correlation distributions of llama-3-8b in Figures 23 (on the same model) and 24 (before and after post-training). The result shows the distributions to be concentrated with std generally lower than 0.05, which addresses the misrepresentation concern. 33 Linear Correlation in LMs Compositional Generalization and Hallucination Figure 24. The std of correlation distribution between logits before and after large-scale post-training. 34 Linear Correlation in LMs Compositional Generalization and Hallucination Completeness Correlation Precision (Hit@Top-5) Generalization Whole Semantics Word in Phrase Subword 0.85 0.86 0.87 0.49 0.10 0. 55.67% 2.00% 0.00% Table 18. The correlation and precision of tokens with different levels of semantic completeness. K. Subword Issue Finally, we show the precision of is highly affected by the semantics of the input and output tokens. We first categorize the tokens into 3 categories, 1) Subword, token being part of word, such as prefix like Br in Brunei, 2) Word in phrase, token is whole word but also part of phrase like North in North America, 3) Whole semantics, the rest of tokens with full meaning in itself like USA. The results in Table 18 show the semantic completeness to be an important factor in whether knowledge can be generalized. With higher semantic completeness (Whole Semantics > Word in Phrase > Subword), the precision also rise as the token indicates clearer entity. Consequently, it can be better updated by the generalization behavior caused by the linear correlation. The only precise mapping (and successful) generalization for Word in Phrase is RiyadhSaudi Arabia, where the first token Saudi has strong indication of the country."
        }
    ],
    "affiliations": [
        "University of California, San Diego"
    ]
}