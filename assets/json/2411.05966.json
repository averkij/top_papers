{
    "paper_title": "Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation",
    "authors": [
        "Aayush Shah",
        "Shankar Jayaratnam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.75, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve a remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using a subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved a comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by a factor of 3."
        },
        {
            "title": "Start",
            "content": "Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation 4 2 0 2 8 ] . - [ 1 6 6 9 5 0 . 1 1 4 2 : r Aayush Shah Esperanto Technologies aayush.shah@esperantotech.com Shankar Jayaratnam Esperanto Technologies shankar.jayaratnam@esperantotech.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated significant success in natural language processing (NLP) tasks and have shown promising results in other domains such as protein sequence generation. However, there remain salient differences between LLMs used for NLP, which effectively handle multiple tasks and are available in small sizes, and protein language models that are often specialized for specific tasks and only exist in larger sizes. In this work, we introduce two small protein language models, based on Llama-3-8B and Phi-3-mini, that are capable of both uncontrollable and controllable protein generation. For the uncontrollable generation task, our best model achieves an average pLDDT score of 69.7512.74, demonstrating robust performance in generating viable protein structures. For the controllable generation task, in which the model generates proteins according to properties specified in the prompt, we achieve remarkable average TM-Score of 0.84, indicating high structural similarity to target proteins. We chose 10 properties, including six classes of enzymes, to extend the capabilities of prior protein language models. Our approach utilizes the Low-Rank Adaptor (LoRA) technique, reducing trainable parameters to just 4% of the original model size, lowering computational requirements. By using subset of the UniRef50 dataset and small models, we reduced the overall training time by 70% without compromising performance. Notably, Phi-3-mini reduced trainable parameters by 60%, decreasing training cost by 30% compared to Llama 3. Consequently, Phi-3 achieved comparable TM-Score of 0.81, demonstrating that smaller models can match the performance of larger ones, like Llama 3. We also demonstrate the deployment of our models on the energy efficient ET-SoC-1 chip, significantly improving the TPS/W by factor of 3. The models are available at https://huggingface.co/Esperanto/Protein-Llama-3-8B and https://huggingface.co/Esperanto/Protein-Phi-3-mini, encouraging further research and development in the field of protein language models. Keywords Protein language model Low Rank Adaptor Energy efficiency"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have revolutionized natural language processing (NLP) by demonstrating exceptional capabilities in wide range of tasks, from text generation to translation and sentiment analysis 18,2931. Models such as GPT-4 and Llama 3 have been termed as foundational models due to their ability to generalize to multiple tasks by the process of fine-tuning on specialized datasets. 3,24 Protein sequences are analogous to natural language because both are composed of set of basic building blocksamino acids for proteins and words for languagethat combine to form meaningful structures. 20,25,28 In proteins, the sequence of amino acids determines the three-dimensional structure and function, whereas in natural language sentences, the sequence of letters and words determine the semantic meaning and local context. These similarities have prompted the application of large language models to various tasks like structure prediction from protein sequence 21, de-novo protein sequence generation 10, also termed as uncontrollable generation, and generation of proteins which possess certain properties or are specialized for certain task, termed as controllable generation. Protein design finds its application in drug development and in the development of efficient artificial enzymes that break down industrial waste or plastics, contributing to carbon neutrality 13. Figure 1: Training pipeline - Llama 3 and Phi 3 models were trained in two stages. First, they were trained on protein sequences and then on protein-property pairs. LoRA was used to lower the training cost. Recently there has been drive towards the development of unified protein foundational models, similar to its counterparts in natural language, which possess an understanding of protein structure, sequence and they way they relate to each other. Such models have demonstrated the creation of novel proteins, effectively speeding up the evolutionary process 11. However, current protein foundational models are often extremely large in size and are generally closed source, which presents several challenges. Their vast size necessitates substantial computational resources for both training and inference, curbing their accessibility for many researchers and organizations. Additionally, the extensive computational power required for these large models leads to increased energy consumption, contributing to higher environmental impact. As result, there is growing need for more efficient protein models that maintain high performance while being computationally economical. This could entail the usage of smaller language models, energy efficient hardware, or combination of both. Lv et al. developed ProLLaMA 16, which demonstrated progress towards unified protein foundational model by fine-tuning Llama 2 using LoRA 12 for the tasks of uncontrollable generation, controllable generation and property prediction. The usage of LoRA, which is parameter efficient fine-tuning technique, drastically reduces the training requirements and allows wide adoption of the model. However, it remains to be seen whether employing these fine-tuning techniques on newer state-of-the-art LLMs like Llama 3 and Phi 3 1 attain better performance with fewer training costs and energy consumption. In this work, we introduce two protein language models based on the Llama 3 and Phi 3 architectures. Using LoRA, we reduce the number of trainable parameters to just 4% of the original model size, demonstrating the usefulness of our model in lowering the training cost. Considering the comparatively small size of Phi 3, we also reduce the training cost and inference time by imbuing Phi 3 with protein generation capability. Our major innovation is reducing the energy consumed during inference by 60% by utilizing Esperanto Technologies energy efficient chip - ET-SoC-1 9. ET-SoC-1 is based on the open-source RISC-V ISA and is designed for efficient, low-power computation. It is tailored for deploying generative AI models. We demonstrate the effectiveness of protein generation on ET-SoC-1 and prove 2 that it is viable alternative to GPUs for this task by assessing the quality of generated proteins. Following Lv et al., we utilize two stage training process involving continual learning using LoRA on protein sequences for uncontrollable generation and subsequently fine-tuning it on an instruction dataset consisting of proteins linked with their properties for controllable generation. summary of our contributions is as follows: Developed small protein language models based on the latest SOTA architectures, namely, Llama 3 and Phi 3. Demonstrated 70% reduction in the fine-tuning cost through the usage of LoRA and small language models. Extended the capabilities of protein language models to the task of controllable generation of enzymes. Reduced the energy consumption for inference by deploying the model on Esperantos ET-SoC-1 chip. Created user-friendly web interface which displays key performance and inference metrics."
        },
        {
            "title": "2 Related Work",
            "content": "Sequence to structure: The three-dimensional structure of protein is linked to its function, making its determination essential for comprehending biological processes and understanding of medicine and life sciences. Among the landmark developments is AlphaFold, developed by Google DeepMind, which revolutionized the field with its unprecedented accuracy in predicting protein structures, as demonstrated in the CASP13 competition. 4 AlphaFold uses neural network based architecture that predicts the 3D coordinates of all non-hydrogen atoms for given protein using the protein sequence and sequence homology. Subsequently, AlphaFold 2 and AlphaFold 3 have been developed, each extending the capabilities of its predecessor with improvements in the performance. 2,14 AlphaFold 2 makes use of multiple sequence alignments (MSA) that identifies similar sequences that have been found in living organisms. AlphaFold 3, while being more accurate than its predecessor, makes predictions not just about the protein structure, but also the interactions between proteins and biological molecules such as DNA, RNA and ligands. RoseTTAFold is another structure prediction model which uses multiple neural networks to quickly and accurately predict protein structures based on amino acid sequences. 5 It utilizes one, two and three dimensional information to collectively reason about the relationship between proteins chemical parts and its folded structure. OmegaFold predicts high-resolution protein structure from single primary sequence alone through geometry-inspired transformer model trained on protein structures. 27 ESMFold, from the Fundamental AI Research team at Meta (FAIR), uses large protein language model as backbone and generates structure prediction using only one sequence as input by leveraging the internal representations of the language model, making the inference much faster than other state-of-the-art models. 15 Protein language modeling: Natural language processing techniques have been modified to process protein language due to the similarities between protein sequences, which can be seen as concatenation of one letter representation of amino acids, and natural language. In the realm of building embedding based representations, ProteinBERT is pre-trained on protein sequences using masked language modeling objective. It is able to generate embeddings from protein sequence that captures its important biophysical properties. 6 For the task of protein sequence generation through causal language modeling, ProtGPT2 is decoder-only model based on the GPT-2 architecture which has learned the protein language by training on 50 million protein sequences. 10 PeptideGPT generates proteins conditionally based on user-defined property. 22 ProGen, from Salesforce, aims to generate novel protein sequences in controllable fashion using transformer-based architecture trained on 280 million proteins. 17,19 ProteinMPNN uses message passing neural network comprising of 3 encoder and 3 decoder layers and 128 hidden dimensions which predicts protein sequences in an autoregressive manner for fixed backbone of interest. 8 Unification of protein language models: Recently, there has been shift in research to combine multiple tasks pertaining to proteins in single model. This allows the model to gain deeper understanding of protein language rather than being specialized for niche task. ProLLaMA combined uncontrollable generation, controllable generation and property prediction tasks by fine-tuning Llama 2 model on dataset consisting of user instructions. ESM 3 is 98 billion parameter model that has been trained on three modalities corresponding to proteins - sequence, structure and function, through masked language modeling objective. This makes the model highly flexible and allows for chain-of-thought prompting to generate new proteins. 11 Taking this area of research forward, we aim to unify the task of controllable and uncontrollable generation via latest state-of-the-art LLMs, namely Llama 3 and Phi 3."
        },
        {
            "title": "3 Methods",
            "content": "3.1 Dataset preparation For the stage one of training, which aims to make the model generate proteins unconditionally, we sourced 2 million sequences from the 2024_03 release of the UniRef50 dataset. 23 To demonstrate attainment of comparable performance with fewer training resources, we did not utilize the full dataset. For stage two of training, involving the instruction based fine-tuning, we chose basket of 10 properties to illustrate the effectiveness of our model on controllable generation pertaining to these properties. The composition of the instruction dataset is shown in 1. We tackle the task of enzyme generation given the class of an enzyme, which has not been explored before. Six classes of enzymes have been considered, namely, Oxidoreductase, Lyase, Ligase, Transferase, Isomerase and Hydrolase. Creation of new enzymes is essential to medicine since engineered enzymes can be used in diagnostics, enzyme replacement therapies, and as potential treatments for diseases by targeting and breaking down harmful molecules. Moreover, new enzymes can help address environmental challenges by enabling the breakdown of pollutants and waste, contributing to more sustainable future. The sequences pertaining to these enzyme classes were sourced from the ECPred40 dataset created by Buton et al. 7 The remaining four classes were chosen as the S-adenosyl-L-methioninedependent methyltransferase superfamily (SAM-MT), the Tetratricopeptide-like helical domain superfamily (TPHD), the Thioredoxin-like superfamily (Trx), and the CheY-like superfamily (CheY). The performance on these classes have been reported by Lv et al., hence to establish common ground for the purposes of comparison, we chose to include these four classes in our instruction dataset. These sequences have been taken from the instruction dataset open sourced by Lv et al. Table 1: Protein classes distribution and sequence lengths Protein Type Number of Sequences Average Sequence Length SAM-MT TrX TPHD CheY Oxidoreductase Transferase Hydrolase Lyase Isomerase Ligase Total 195,820 129,686 141,034 133,126 23,901 65,899 35,758 18,550 12,151 24,010 779,935 188 159 170 152 343 336 323 325 342 465 3.2 Training pipeline To endow base LLM model with protein language understanding, it is necessary to train them on dataset of protein sequences. However, completely re-initializing the parameters for pre-training faces two disadvantages: (i) the huge amount of computational resources required, and (ii) elimination of its prior understanding of natural language, which is pertinent for multi-task capabilities by understanding user input. To circumvent this, we employed continual learning using LoRa based on similar approach by Lv et al. This reduces the number of trainable parameters and helps retain the prior natural language knowledge and allow the model to interpret user instructions. We employed this approach for Llama-3-8b and Phi-3-mini-4k-instruct models. The models were trained for the task of causal language modelling, where the loss function employed is cross-entropy between the output of the model and the ground truth, which is just the inputs shifted by one token to the left. This can be written as: = (cid:88) t=1 log (wt w1, w2, . . . , wt1) where is the loss, is the total number of words in the sequence, is the predicted probability of word wt, and w1, w2, . . . , wt1 are the preceding words in the sequence. For Llama 3, LoRA adaptors were added to Wq, Wk, Wv, Wo, Wup, Wgate and Wdown weights whereas for Phi 3 they were added to the Wq, Wk, Wv, Wo, Wup and Wdown weights. An overview of the training pipeline can be seen in Figure 1. learning rate of 5e-5 with warmup ratio of 0.03 was used. The models were trained on single NVIDIA A100 80GB GPU, with the per device Figure 2: Web interface for protein sequence generation and visualization. The user enters the starting amino acids, sequence length, and property for initiating the generation. The generated sequence and its structure shows up in the corresponding sections along with the performance metrics during inference. batch size being 8 for Llama 3 and 16 for Phi 3. LoRA rank was set to 128 and alpha parameter to 256 for both stages of training. 3.3 Model Inference The generation process from trained model is affected by several key parameters, including repetition penalty, top-p, top-k, and maximum sequence length. The repetition penalty discourages the model from repeatedly generating the same token, encouraging more diverse outputs. Top-p (nucleus sampling) ensures that the model selects tokens from the smallest possible set whose cumulative probability reaches at least p, thus choosing from dynamically adjusted subset of likely candidates. Top-k restricts the next token selection to the top most probable tokens, focusing on the most likely continuations. The maximum sequence length defines the upper limit on the number of tokens the model can generate in single sequence, thereby controlling the overall output length. The value of top was kept at 40, top at 0.9 and repetition penalty at 1.2 according to the best performing parameters found by Lv et al. The length of generation was approximately kept at the average sequence length of the respective class being generated to increase the alignment between the ground truth protein distribution and generated outputs. 3.4 Structure Evaluation Unlike natural language generated samples which can be evaluated on the basis of their grammatical correctness, coherence and relevance to the prompt by human evaluators, evaluation of protein sequences requires dedicated model 5 Figure 3: Evaluation pipeline - The generated sequence is passed to ESMFold for predicting its structure. Foldseek aligns this structure with known proteins and gives the TM-Score and RMSD value. to predict the structural stability and quality of the generated proteins. This is quantified by the pLDDT score (predicted Local Distance Difference Test), which is measure of the models confidence in its structure prediction and has been known to correlate with orderness of the structure. We used ESMFold model to predict the structures and the pLDDT scores of generated proteins. ESMFold is 15B parameter Transformer model and produces similar outputs compared to other state-of-the-art models like AlphaFold, with around 60 times faster inference. Regions with pLDDT score between 60 and 90 are expected to be modeled accurately and exhibit an ordered structure, while lower scores (pLDDT less than 50) are typically found in disordered regions. 3.5 Controllable Generation To assess if the model is generating proteins according to the class mentioned in the input prompt, 100 sequences were generated per class. These were passed to ESMFold model to obtain their pLDDT scores, and sequences with score lower than 60 were filtered out to ensure only sequences corresponding to stable structures were used in evaluation. The predicted protein structures were aligned with ground truth protein structures using Foldseek 26, and they were compared on the basis of TM-Score and homologous probability. TM-Score ranges from 0 to 1 and is measure of the structural similarity between two protein structures, with higher scores indicating better alignment whereas homologous probability is the likelihood that two protein sequences share common evolutionary ancestor. These scores tell us how similar given protein is to set of proteins belonging to common class or property."
        },
        {
            "title": "4 Results",
            "content": "4.1 Unconditional Generation The generation of sequences without providing the class of proteins as the context is termed as unconditional generation. We generated 100 sequences and evaluated them on the basis of their structural stability, as quantified through their 6 Table 2: Performance comparison of selected methods."
        },
        {
            "title": "Method",
            "content": "pLDDT"
        },
        {
            "title": "PDB",
            "content": "TM-score RMSD TM-score RMSD ProtGPT2 ProGen2 ProLLaMA Llama 3 (A100) Llama 3 (ET-SoC) Phi 3 (A100) Phi 3 (ET-SoC) 56.3216.05 61.0718.45 66.4912.61 69.7512.74 63.71 6.78 60.41 4.56 60.72 6.58 0.44 0.43 0.49 0.60 0.45 0.56 0.48 12.60 15.52 9.50 4.88 6.49 3.87 7.14 0.43 0.44 0.48 0.38 0.34 0.48 0. 9.19 11.02 7.63 7.29 4.11 4.38 4.62 pLDDT scores, and the measure of their alignment with two protein structure databases - AlphaFold Protein Structure Database (AFDB) and Protein Data Bank (PDB). The Swiss-Prot section of AFDB and the entire PDB was used from Foldseek to calculate the TM-Score and RMSD value in order to quantify the alignment. Table 2 gives the results for our models compared with other state-of-the-art approaches. Our Llama 3 model obtains pLDDT score of 69.75 12.74, performing better than ProLLaMas score of 66.49 12.61. Llama 3 has higher TM-Score and lower RMSD for AFDB database and lower RMSD on the PDB database, while being sub-optimal on TM-Score. Phi 3 model obtained an average pLDDT score of 51.30 6.90 but it secured higher RMSD scores for both AFDB and PDB. Considering the complexity of unconditional generation, our model has satisfactory performance and highlights the utility of LLM based methods in de-novo protein design. (a) SAM-MT (b) TPHD (c) Trx (d) CheY Figure 4: Structural alignment of proteins generated by Llama 3 for four different classes with their closest natural counterpart ranked according to TM-Score. 4.2 Controllable Generation Generating proteins according to some user-defined conditions is important to design proteins useful for different applications. We evaluated the performance of our models to generate proteins belonging to 10 different classes as highlighted in Table 3 on the basis of their TM-Score and RMSD with ground truth proteins. The high TM-Scores obtained by our models for these properties suggest that it is effectively capturing the underlying protein distribution pertaining to each of the classes. In the case of Oxidoreductase, the TM-Scores are below 75, implying sub-optimal modeling, however that can be ascribed to the inherent complexity associated with this class and the lesser data available in comparison. Llama 3 generally obtains higher score compared to Phi 3, with the exceptions being Transferase, Isomerase and Hydrolase, however the difference is not significant. This can be ascribed to the small size of Phi 3, which allowed it to be trained for longer period. We also show the top structural alignments for four of the properties as given by Foldseek in Figure 4. Comparing the performance of our model against ProLLaMA for the four properties in Figure 5, we can see our model performs better against these benchmarks. This proves the capability of our approach in controllable generation and paves the way for integration of more properties. 7 Table 3: Average TM-Score for 10 protein classes obtained by Llama 3 and Phi 3 on A100 GPU and ET-SoC-"
        },
        {
            "title": "Class",
            "content": "TM-Score Llama 3 Phi 3 A100 ET-SoC-1 A100 ET-SoC-1 SAM-MT TPHD TRX CheY Ligase Hydrolase Lyase Oxidoreductase Transferase Isomerase Average 0.83 0.95 0.97 0.96 0.85 0.79 0.86 0.73 0.73 0.81 0.84 0.82 0.94 0.84 0.91 0.85 0.70 0.85 0.74 0.73 0.84 0.82 0.79 0.79 0.86 0.95 0.77 0.81 0.84 0.69 0.80 0.86 0. 0.74 0.80 0.85 0.95 0.73 0.84 0.80 0.61 0.75 0.88 0.81 Figure 5: Comparison of best TM-Scores obtained by Llama 3 and Phi 3 on four properties 4.3 Inference metrics Measuring inference metrics such as tokens per second, energy consumption, and memory usage is crucial for evaluating the performance and efficiency of machine learning models, particularly in the context of large language models and other computationally intensive tasks. Tokens per second provides measure of processing speed, indicating how quickly model can generate or process text. Energy consumption reflects the power efficiency of hardware during inference, which is vital for assessing the sustainability and operational costs of deploying models at scale. Memory usage helps in understanding the resource requirements and limitations of the hardware, ensuring that models can be executed without running into memory bottlenecks. Comparing different hardware platforms is essential for optimizing these metrics. Traditional GPUs, while powerful, can be energy-intensive and may not be the most efficient choice for all applications. In contrast, ET-SoC-1 offers more energy-efficient alternative, potentially reducing power consumption while maintaining performance. ET-SoC-1 has an architectural design that focuses on minimizing energy usage and maximizing computational efficiency, which can lead to significant cost savings and lower environmental impact. By evaluating and comparing the performance of different hardware, developers and researchers can make informed decisions about the most suitable platform for their specific needs, balancing speed, efficiency, and operational costs. We compare the performance during inference for Llama 3 and Phi 3 when deployed on NVIDIA A100 and ET-SoC-1 in Table 4. The models are in FP16 quantization, and we can see the significant reduction in power when for ET-SoC-1, dropping from 300 in A100 to just 25 in ET-SoC-1. The tokens-per-second also see drop, however for TPS/W, 8 Table 4: Inference metrics comparison for NVIDIA A100 GPU and ET-SoC 1 Llama 3 Phi"
        },
        {
            "title": "Metric",
            "content": "A100 ET-SoC 1 A100 ET-SoC 1 Power (W) Tokens/sec (TPS) Memory (GB) TPS/W 300 36 15.35 0.12 25 5 15.35 0.20 300 36 7.36 0.12 25 10 7.36 0. more energy conscious performance metric, we see 60% increase in Llama 3 and more than three-fold increase for Phi 3. Hence, ET-SoC-1 consumes less energy than A100 GPU when generating the same number of tokens. These results highlight the importance of hardware for environmentally sustainable inference of protein language models. To enable intuitive interaction with our models and subsequently visualize the structure of the generated protein sequences, we created Gradio based user interface as shown in Figure 2. The user can input starting amino acids for generation, specify the length and the property, and get important inference metrics along with the generated output."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we have introduced two compact protein language models, based on LLaMa-3-8B and Phi-3-mini, capable of both uncontrollable and controllable protein generation tasks. Our models demonstrate good performance in generating viable protein structures and synthesizing proteins with specific properties, effectively bridging the gap between large language models in natural language processing and specialized protein language models. For the uncontrollable generation task, our LLaMa-3-8B model achieved an average pLDDT value of 69.7512.74, outperforming existing state-of-the-art models and showing robust performance comparable to naturally occurring proteins. This indicates our models capability to generate high-quality protein sequences that align well with structural databases such as AFDB and PDB. We also explore the theme of energy efficient training and inference by reducing the number of trainable parameters through the Low-Rank Adaptation (LoRA) technique and improving the TPS/W by factor of three for Phi 3 and 60% for Llama 3 by deploying the model on the energy efficient ET-SoC-1 chip. Future work could involve expanding the range of controllable properties that the models can generate and including multiple modalities into the model like structure prediction, allowing for the design of proteins tailored to more specific and diverse biotechnological applications. Collaborations with experimental scientists to validate and refine the generated proteins in laboratory settings will be essential for translating computational models into tangible outcomes. Our work paves the way for the development of small protein language models and marks significant step towards more sustainable AI practices in the field of protein language modeling."
        },
        {
            "title": "References",
            "content": "[1] M. Abdin, S. A. Jacobs, A. A. Awan, J. Aneja, A. Awadallah, H. Awadalla, N. Bach, A. Bahree, A. Bakhtiari, H. Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. [2] J. Abramson, J. Adler, J. Dunger, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, 630:493500, 2024. doi:10.1038/s41586-024-07487-w. URL https://doi.org/10.1038/ s41586-024-07487-w. [3] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [4] M. AlQuraishi. AlphaFold at CASP13. Bioinformatics, 35(22):48624865, 05 2019. ISSN 1367-4803. doi:10.1093/bioinformatics/btz422. URL https://doi.org/10.1093/bioinformatics/btz422. [5] M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. D. Schaeffer, C. Millán, H. Park, C. Adams, C. R. Glassman, A. DeGiovanni, J. H. Pereira, A. V. Rodrigues, A. A. van Dijk, A. C. Ebrecht, D. J. Opperman, T. Sagmeister, C. Buhlheller, T. Pavkov-Keller, M. K. Rathinaswamy, U. Dalwadi, C. K. Yip, J. E. Burke, K. C. Garcia, N. V. Grishin, P. D. Adams, R. J. Read, and D. Baker. Accurate prediction of protein structures and interactions using three-track neural network. Science, 373(6557):871 876, 2021. doi:10.1126/science.abj8754. URL https://www.science.org/doi/abs/10.1126/science. abj8754. 9 [6] N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial. ProteinBERT: universal deep-learning ISSN 1367-4803. model of protein sequence and function. Bioinformatics, 38(8):21022110, 02 2022. doi:10.1093/bioinformatics/btac020. URL https://doi.org/10.1093/bioinformatics/btac020. [7] N. Buton, F. Coste, and Y. L. Cunff. Predicting enzymatic function of protein sequences with attention. Bioinformatics, 39(10):btad620, October 2023. doi:10.1093/bioinformatics/btad620. URL https://doi.org/10. 1093/bioinformatics/btad620. [8] J. Dauparas, I. Anishchenko, N. Bennett, H. Bai, R. J. Ragotte, L. F. Milles, B. I. Wicky, A. Courbet, R. J. de Haas, N. Bethel, P. J. Leung, T. F. Huddy, S. Pellock, D. Tischer, F. Chan, B. Koepnick, H. Nguyen, A. Kang, B. Sankaran, A. K. Bera, N. P. King, and D. Baker. Robust deep learning-based protein sequence design using proteinmpnn. Science, 378(6615):4956, Oct 2022. doi:10.1126/science.add2187. [9] Esperanto Technologies. Esperanto technologies - ai hardware solutions. https://www.esperanto.ai/, 2024. Accessed: 2024-07-30. [10] N. Ferruz, S. Schmidt, and B. Höcker. Protgpt2 is deep unsupervised language model for protein design. Nature Communications, 13(1):4348, 2022. doi:10.1038/s41467-022-32007-7. URL https://doi.org/10.1038/ s41467-022-32007-7. [11] T. Hayes, R. Rao, H. Akin, N. J. Sofroniew, D. Oktay, Z. Lin, R. Verkuil, V. Q. Tran, J. Deaton, M. Wiggert, R. Badkundri, I. Shafkat, J. Gong, A. Derry, R. S. Molina, N. Thomas, Y. Khan, C. Mishra, C. Kim, L. J. Bartie, M. Nemeth, P. D. Hsu, T. Sercu, S. Candido, and A. Rives. Simulating 500 million years of evolution with language model. bioRxiv, 2024. doi:10.1101/2024.07.01.600583. URL https://www.biorxiv.org/content/ early/2024/07/02/2024.07.01.600583. [12] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank adaptation of large language models, 2021. URL https://arxiv.org/abs/2106.09685. [13] T. Huang and Y. Li. Current progress, challenges, and future perspectives of language models for protein representation and protein design. ISSN 26666758. doi:https://doi.org/10.1016/j.xinn.2023.100446. URL https://www.sciencedirect.com/science/ article/pii/S2666675823000747. The Innovation, 4(4):100446, 2023. [14] J. Jumper, R. Evans, A. Pritzel, et al. Highly accurate protein structure prediction with alphafold. Nature, 596: 583589, 2021. doi:10.1038/s41586-021-03819-2. URL https://doi.org/10.1038/s41586-021-03819-2. [15] Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. d. Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido, and A. Rives. Language models of protein sequences at the scale of evolution enable accurate structure prediction. bioRxiv, 2022. doi:10.1101/2022.07.20.500902. URL https://www.biorxiv.org/content/early/2022/ 07/21/2022.07.20.500902. [16] L. Lv, Z. Lin, H. Li, Y. Liu, J. Cui, C. Y.-C. Chen, L. Yuan, and Y. Tian. Prollama: protein large language model for multi-task protein language processing. arXiv preprint arXiv:2402.16445, 2024. [17] A. Madani, B. Krause, E. R. Greene, et al. Large language models generate functional protein sequences across diverse families. Nature Biotechnology, 41:10991106, 2023. doi:10.1038/s41587-022-01618-2. URL https://doi.org/10.1038/s41587-022-01618-2. [18] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar, N. Barnes, and A. Mian. comprehensive overview of large language models, 2024. URL https://arxiv.org/abs/2307.06435. [19] E. Nijkamp, J. Ruffolo, E. N. Weinstein, N. Naik, and A. Madani. Progen2: Exploring the boundaries of protein language models, 2022. URL https://arxiv.org/abs/2206.13517. [20] D. Ofer, N. Brandes, and M. Linial. The language of proteins: Nlp, machine learning and protein sequences. Computational and Structural Biotechnology Journal, 19:17501758, 2021. [21] A. W. Senior, R. Evans, J. Jumper, et al. Improved protein structure prediction using potentials from deep learning. Nature, 577:706710, 2020. doi:10.1038/s41586-019-1923-7. URL https://doi.org/10.1038/ s41586-019-1923-7. [22] A. Shah, C. Guntuboina, and A. B. Farimani. Peptide-gpt: Generative design of peptides using generative pre-trained transformers and bio-informatic supervision, 2024. URL https://arxiv.org/abs/2410.19222. [23] B. E. Suzek, Y. Wang, H. Huang, P. B. McGarvey, C. H. Wu, and U. Consortium. Uniref clusters: comprehensive and scalable alternative for improving sequence similarity searches. Bioinformatics, 31(6):926932, 2015. doi:10.1093/bioinformatics/btu739. [24] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. 10 [25] G. Valentini, D. Malchiodi, G. Gliozzo, M. Mesiti, M. Soto-Gomez, A. Cabri, J. Reese, E. Casiraghi, and P. N. Robinson. The promises of large language models for protein design and modeling. Frontiers in Bioinformatics, 3:1304099, 2023. doi:10.3389/fbinf.2023.1304099. [26] M. van Kempen, S. S. Kim, C. Tumescheit, et al. Fast and accurate protein structure search with foldseek. Nature Biotechnology, 42:243246, 2024. doi:10.1038/s41587-023-01773-0. URL https://doi.org/10. 1038/s41587-023-01773-0. [27] R. Wu, F. Ding, R. Wang, R. Shen, X. Zhang, S. Luo, C. Su, Z. Wu, Q. Xie, B. Berger, J. Ma, and J. Peng. Highresolution de novo structure prediction from primary sequence. bioRxiv, 2022. doi:10.1101/2022.07.21.500999. URL https://www.biorxiv.org/content/early/2022/07/22/2022.07.21.500999. [28] K. K. Yang, Z. Wu, and F. H. Arnold. Machine-learning-guided directed evolution for protein engineering. Nature Methods, 16:687694, 2019. doi:10.1038/s41592-019-0468-8. [29] B. Zhang, H. Yang, and X.-Y. Liu. Instruct-fingpt: Financial sentiment analysis by instruction tuning of generalpurpose large language models, 2023. URL https://arxiv.org/abs/2306.12659. [30] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. URL https://arxiv.org/abs/2303.18223. [31] W. Zhu, H. Liu, Q. Dong, J. Xu, S. Huang, L. Kong, J. Chen, and L. Li. Multilingual machine translation with large language models: Empirical results and analysis, 2024. URL https://arxiv.org/abs/2304.04675."
        },
        {
            "title": "6 Appendix",
            "content": "6.1 Training details The models used in this work were Llama-3-8b and Phi-3-mini-4k-instruct downloaded from Hugging Face. For training, LoRA was employed with rank of 128 and alpha parameter of 256. The rank and alpha were kept high due to the significant difference in the nature of natural language and protein language modeling. Learning rate was kept at 5e-5 for both the models. batch size of 8 was used for Llama 3 and 16 for Phi 3. The models were trained on single NVIDIA A100 80GB GPU. The training details are summarised in Table 5. We can see the benefit of using Phi 3, smaller language model compared to Llama 3, in reducing the training cost incurred. Phi 3 reduced the convergence time by 30% during training, highlighting the importance of using small language models for the task of protein sequence generation. Figure 6 shows the loss convergence for Llama 3 and Phi 3 during stage one of training with losses not fully converging for stage two, indicating further room for improvement. Table 5: Training parameters Training Parameter Llama 3 Phi Learning Rate Batch Size Training Time - Stage 1 Training Time - Stage 2 LoRA rank LoRA alpha 5e-5 8 37 hours 48 hours 128 256 5e-5 16 26 hours 35 hours 128 256 Table 6: Generation parameters Generation Parameter Llama 3 Phi Top Top Reptition penalty Max tokens - Unconditional Max tokens - Conditional 40 0.9 1.2 70 100 40 0.9 1.2 30 100 6.2 Generation Parameters We kept top k, top and repetition penalty of 40, 0.9 and 1.2 respectively since these parameters were reported to perform the best by Lv et al. For uncontrollable generation, we set the maximum number of generated tokens to 70 (a) Llama 3 - Stage 1 (b) Llama 3 - Stage 2 (c) Phi 3 - Stage 1 (d) Phi 3 - Stage 2 Figure 6: Training curves for Llama 3 and Phi 3 during both stages of training. for Llama 3 and 30 for Phi 3, since the latter was the best at generating shorter sequences. On an average there are two amino acids per token, hence this corresponds to protein length of 140 and 60 respectively. For controllable generation, since the ground truth proteins were themselves long, we set the number of generated tokens to 100 for both the models to facilitate maximum alignment with the ground truth proteins. summary of generation parameters used can be found in Table 6."
        }
    ],
    "affiliations": [
        "Esperanto Technologies"
    ]
}