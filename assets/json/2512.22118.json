{
    "paper_title": "ProEdit: Inversion-based Editing From Prompts Done Right",
    "authors": [
        "Zhi Ouyang",
        "Dian Zheng",
        "Xiao-Ming Wu",
        "Jian-Jian Jiang",
        "Kun-Yu Lin",
        "Jingke Meng",
        "Wei-Shi Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject's atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit."
        },
        {
            "title": "Start",
            "content": "ProEdit: Inversion-based Editing From Prompts Done Right Zhi Ouyang1, Dian Zheng2, Xiao-Ming Wu3 Jian-Jian Jiang1 Kun-Yu Lin4 Jingke Meng1,(cid:66) Wei-Shi Zheng1,5,(cid:66) 5 2 0 D 6 2 ] . [ 1 8 1 1 2 2 . 2 1 5 2 : r 1Sun Yat-sen University 2CUHK MMLab 3College of Computing and Data Science, Nanyang Technological University 4The University of Hong Kong 5Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China https://isee-laboratory.github.io/ProEdit/ Figure 1. ProEdit for image and video editing. We propose highly accurate, plug-and-play editing method for flow inversion that addresses the problem of excessive source image information injection, which prevents proper modification of attributes such as pose, number, and color. Our method has demonstrated impressive performance in both image editing and video editing tasks."
        },
        {
            "title": "Abstract",
            "content": "Inversion-based visual editing provides an effective and training-free way to edit an image or video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subjects atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention asequal contributions. (cid:66) corresponding authors. Code is available 1 pect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RFSolver, FireFlow and UniEdit. negative impact of the source image from both the attention and the latent aspects. For the attention aspect, we introduce KV-mix. We first identify the edited regions based on source and target prompts, then mix the and features in these regions while fully injecting source KV features in non-edited areas to preserve background consistency. This mixing mechanism applies to all attention operations without manual adjustment of heads, layers, or blocks. For the latent aspect, we propose LatentsShift. Inspired by AdaIN from style transfer, which performs structure-preserving distribution transformations, we inject random noise into the source distribution of edited regions. This reduces the influence of source image attributes while maintaining structural and background consistency. As shown in Fig 2, our method successfully eliminates the negative effects of source image information from inversion noise and attention injection mechanisms, accurately modifying the subjects attribute while maintaining the consistency of background and non-editing content. Through extensive experiments, we demonstrate that 1) Effectiveness: ProEdit can eliminate the negative impact of the source image/video on the editing content, while preserving the non-edited content, achieving state-of-the-art (SOTA) performance in various editing tasks; 2) Plug-andplay: Our method is plug-and-play, enabling its seamless integration into wide range of existing inversion and editing methods; 3) Attribute Correction: In attribute editing, where existing methods perform poorly, our approach showcases unprecedented performance. Our contributions can be summarized as follows: We investigate the issue of excessive source image information injection in inversion-based editing and identify that this problem stems from both latent initialization and attention injection mechanisms, leading to failures. We propose ProEdit, novel training-free approach designed to solve the above problem, which can eliminate the negative impact from the source while maintaining background consistency. Through extensive experiments, we proof that ProEdit is effective, plug-and-play and can be used in various types of image and video editing. Our code will be open-source to boost the developing of the generative community. 2. Related Work 2.1. Text-to-Visual generation Diffusion models have achieved significant success in the fields of text-to-visual generation, leading to the development of series of outstanding foundational models [19, 37, 41, 44, 58]. Recently, the text-guided generation paradigm for both images and videos has been shifting from diffusion models based on U-Net [42] architecture to flow models based on DiT [37] architecture. Flow-based models, Figure 2. Framework comparison between (a) previous methods and (b) our method. To address the issue of excessive source image information injection, we introduce the Shift module for inverted noise and the Mix module for the attention injection, alleviating the editing failures caused by these issues. 1. Introduction Inversion-based visual editing [7, 11, 21, 3335, 48] has emerged as highly effective and valuable research direction, offering powerful, training-free paradigm for modifying images and videos according to user instructions, especially the flow-based editing methods, which provides better generative abilities with fewer sampling steps. Most established inversion-based methods [8, 51] operate by first leveraging the inverted latents from the source image as the starting point. Then, they redo the sampling process by using the target prompt to guide the sampling process towards the target image or video. To maintain fidelity to the source content, they mostly employ the source injection strategy to re-introduce source-specific information or nuggets during the sampling process. However, as shown in Fig 1, this sampling strategy overly relies on source information, no matter in the latent aspect or the attention aspect, which negatively affects the edits in the target image, especially regarding subject attributes such as color, pose, and number. In this work, we first conduct an in-depth investigation into the above problems and conclude that completely relying on the inverted latents for sampling and applying global attention feature injection introduces excessive source image information, leading to editing failure. Specifically, in the attention aspect, we find that the global attention feature injection strategy introduces excessive attribute-related information from the source attention, causing the model to overly focus on source information while neglecting text guidance. For the latent aspect, starting from the source image distribution creates an overly strong prior that easily leads the sampling process to reconstruct the source distribution. Based on the above observation, we propose novel inversion-based editing method, ProEdit, to eliminate the such as FLUX [26] and HunyuanVideo [25], utilize the MMDiT [10] architecture and simulate straight path between two distributions through probability flow ordinary differential equation(ODE), enabling faster and better generation with fewer sampling steps. These T2I and T2V models also facilitate the editing of images and videos, where target images are generated based on source images and modified text prompts. 2.2. Text-driven Editing For visual editing tasks, early works focused on trainingbased methods [3, 20, 22, 23, 27, 29, 60]. These methods leverage generative models to achieve controllable image editing. As generative models have advanced, attention has shifted towards training-free editing methods, which offer greater flexibility and efficiency. Among them, inversionbased methods [49] have become an important research direction for applying diffusion models to image editing tasks. DDIM inversion [44], as representative method, marked significant advancement in inversion-based image editing within diffusion models, inspiring series of high-precision solvers [32, 50, 57] aimed at minimizing inversion errors and improving sampling efficiency. Sampling-based methods introduce controlled randomness to enable more flexible editing [9, 21, 36, 53]. On the other hand, attentionbased methods achieve controllable image editing by altering the role of attention tokens [5, 24, 28, 46, 48, 55], and these methods have gradually expanded to video editing [4]. Following the trajectory of diffusion models, recent inversion methods based on flow models have mainly focused on improving inversion solvers [8, 18, 51, 54] and the joint attention mechanism [2, 56] in MM-DiT [10]. Although they have achieve good editing performance, these methods still overlook the negative impact of inversion strategies on the editing content. In this work, we reveal the negative impact of inversion on editing and propose ProEdit to eliminate this negative influence from both the attention and latent distribution perspectives. Notably, existing methods rely on selecting specific attention heads, layers, or block types when modifying the attention mechanism, which limits their alignment with the source image. Our method is the first to achieve this without requiring the selection of specific layers, heads, or block types. 3. Method In this section, we first introduce the preliminary to understand our method in Section 3.1, then we conduct an investigation in Section 3.2, analyzing the reasons why the inversion-sampling paradigm faces challenges in removing the influence of the source image on the target images edited contents. Next, we introduce our proposed ProEdit method in Section 3.3 and 3.4. Our method eliminates the influence of the source image on the target images edited contents from both the attention guidance and the initial latents in the sampling process, while maintaining the consistency of the background structure. Finally, we summarize our methods editing process in Section 3.5. 3.1. Preliminaries First, we introduce the preliminary knowledge to better understand our method, including the training objective of flow-based generative models and the ODE solving process. Then, we derive the inversion ODE solving process based on flow models. Generative models [19] aim to generate data X1 that follows the real data distribution π1 from noise X0 that follows Gaussian distribution π0. Recently, flow matching [1, 30, 31] has emerged as method that learns velocity field vθ to transform noise into data along straight trajectory. The training objective is to solve the following optimization problem: EZ0,Z1,t min θ (cid:104) (Z1 Z0) vθ(Zt, t)2(cid:105) , Zt = tZ1 + (1 t)Z0, [0, 1], (1) where Z0 π0 is initialized from the source distribution, and Z1 π1 is generated at the end of the trajectory. The term Z1 Z0 represents the target velocity. The model learns velocity field to deterministically transform random samples of Gaussian noise into target data via an Ordinary Differential Equation (ODE) defined over the continuous time interval: dZt = vθ(Zt, t)dt, [0, 1] (2) This ODE can be discretized and numerically solved by solvers as follows: Zti+1 = Zti + (ti+1 ti)vθ(Zti, ti), (3) where {0, . . . , }, with t0 = 0 and tN = 1. Flow matching has deterministic trajectories. Its reverse process is obtained by reversing the learned flow trajectory. Starting from Z1 π1, the reverse ODE is given by reverse the velocity field: dZt = vθ(Zt, t)dt, [1, 0] (4) Correspondingly, this ODE is discretized and solved using numerical solver as follows: Zti1 = Zti (ti1 ti)vθ(Zti, ti), (5) where {N, . . . , 0}, with tN = 1 and t0 = 0. This inverse process generates Z0 π0 by utilizing the symmetry of the velocity field to ensure consistency with the forward process. Naturally, this inversion method is applied in visual reconstruction and visual editing. 3 Figure 3. Excessive source image information injection phenomenon in RF-Solver. We validate it by visualizing the attention from source and target text tokens to the visual tokens during initial and sampling stage. In RF-Solver, the attention from the source text token to the visual tokens remains higher than that from the target text token. However, after removing attention injection, the attention from black and orange to visual tokens returns to similar levels, but some subject attributes (e.g., pose) change accordingly. 3.2. Rethinking the Inversion-Sampling Paradigm In this subsection, we conduct an investigation of the challenge in visual editing, and we conclude that previous works mostly rely on sampling with inverted noise and use source attention injection mechanism to maintain background and structural consistency. This design often injects excessive source image information, leading to editing failure. The analysis is as follows. Attention Injection Problem. To maintain the overall structural consistency between the target and source images, current methods [8, 51] globally inject the value attention features into specific time steps during the sampling process, as described by the following equation: tg(l + 1) = Attn(Qt zt tg, Kt tg, ), (6) where denotes the attention features corresponding to the source prompt, while tg denotes the attention features corresponding to the target prompt. However, this global attention feature injection mechanism has negative impact on the editing process. As shown in Fig 3, after adding the attention injection mechanism, although overall consistency is enhanced, the model focus far less on prompt black than on orange in the image, representing that the subjects attributes are also forcibly injected into the target image. This issue increases the difficulty of attribute editing. Latent Distribution Injection Problem. As shown in Fig 3, the attention from orange to visual tokens is significantly higher than from black, indicating that although 4 the image is inverted back to noise, it still retains substantial source image attributes. This causes editing to fail when the gap between target and source prompts is too large. Summary. The negative impact of the source image on the editing process can be attributed to two factors: global attention feature injection and the latent distribution injection. Therefore, this paper proposes ProEdit to address all these issues from corresponding aspects. 3.3. KV-mix Motivation. As analyzed before, previous methods use global injection mechanism of visual and textual attention features to maintain consistency, but the excessive injection of source attention features negatively impacts the editing quality. Towards this end, our method aims to mitigate the problem with the insight that mixing source visual attention and target visual attention helps align with the target prompt while maintaining the consistency of non-edited content. Method. Based on the above observations, we execute attention control on the visual components across all blocks, while consistently using the attention features of the target prompt for text attention to achieve effective editing guidance. To distinguish between the editing and non-editing regions, we obtain mask by processing the attention map to separate the editing region, for the detailed implementation please see the Supplementary File. For the non-editing region, we apply full injection of visual attention features to maintain background consistency. For the editing region, we use mix of source and target visual attention features Figure 4. Pipeline of our ProEdit. The mask extraction module identifies the edited region based on source and target prompts during the first inversion step. After obtaining the inverted noise, we apply Latents-Shift to perturb the initial distribution in the edited region, reducing source image information. In selected sampling steps, we fuse source and target attention features in the edited region while directly injecting source features in non-edited regions to achieve accurate attribute editing and background preservation simultaneously. to preserve the consistency of non-editing content and improve the editing quality. After extensively exploring all plausible combinations of Q, K, and , we found that the configuration shown in Eq.7 is most conducive to achieving consistent editing. Formally, our KV-mix design is as follows: color and texture distributions while preserving structural consistency, we adapt this approach to image editing. Method. As our goal is to eliminate the influence of source image information, we directly use random noise as the style image to shift the distribution of the inverted noise. We improve its formula to implement the shift of the latent distribution for the editing region as follows: Kl ˆKl tg = δKl ˆV tg = δV tg = ˆKl tg = ˆV zt(l + 1) = Attn tg + (1 δ)K s, tg + (1 δ)V , tg + (1 ) s, tg + (1 ) , (cid:17) tg, Kl tg, l Ql (cid:16) tg , where denotes the edited region that is extracted from attention map and applied only to the visual branch. To enable controllable editing strength and preservation of non-edited content, we define the mixing strength δ as ratio of mix for applying attention control in the edited region, which determines the level of non-edited content preservation during editing. This attention mechanism enables precise text control for consistent editing. Since we perform the KV-mix operation only within the visual tokens, KV-mix is applied in both Double and Single Attention blocks. 3.4. Latents-Shift Motivation. Here we aim to mitigate the problem of distribution injection while preserving the structure consistency. Inspired by AdaIN [14] in style transfer, which transfers 5 zT = σ(zr ) (cid:18) zT µ(zT ) σ(zT ) (cid:19) + µ(zr ), (8) ˆzT = (β zT + (1 β)zT ) + (1 ) zT , (7) where β denotes the fusion ratio between the inverted noise and pure noise, controlling the level of shift in the inverted noise distribution. denotes the edited region, which is inherited from KV-mix to achieve the shifted inverted noise distribution for the editing region. 3.5. Overall The complete process of our pipeline can be summarized as Fig 4: During the inversion stage, the source image and source prompt are input into the model to conduct the inversion process, and Kl are cached in the fly. Then, the attention map is processed to obtain the mask of the editing region, and the inverted noise is output as the initial input for the sampling stage. and In the sampling stage, the inverted noise first passes through the Latents-Shift module to obtain the fusion noise, which is then input into the model along with the target prompt for sampling. During the sampling process, the Table 1. Text-driven image editing comparison on PIE-Bench. We report the peer-reviewed results of each baseline, and evaluate our proposed method using flow-based inversion methods RF-Solver, FireFlow, and UniEdit to demonstrate the effectiveness. The best and second-best results are shown in bold and underline respectively. Method P2P [13] PnP [48] PnP-Inversion [21] EditFriendly [16] MasaCtrl [5] InfEdit [55] RF-Inversion [43] RF-Solver [51] RF-Solver+Ours FireFlow [8] FireFlow+Ours UniEdit [18](α=0.6) UniEdit(α=0.6)+Ours UniEdit [18](α=0.8) UniEdit(α=0.8)+Ours Model Diffusion Diffusion Diffusion Diffusion Diffusion Diffusion Flow Flow Flow Flow Flow Flow Flow Flow Flow Structure Distance (103) BG Preservation CLIP Sim. PSNR SSIM (102) Whole Edited NFE 69.43 28.22 24.29 28.38 13.78 40.60 31.10 27. 28.30 27.51 10.14 9.22 26.85 24.27 17.87 22.28 22.46 24.55 22.17 28.51 20.82 22.90 24.77 23.28 24. 29.54 30.08 24.10 24.82 71.14 79.05 79.68 81.57 79.67 85.66 71.92 81.90 84.78 82.82 85.19 90.42 90. 84.86 85.87 25.01 25.41 25.41 23.97 23.96 25.03 25.20 26.00 26.28 25.98 26.28 25.80 25.78 26.97 27. 22.44 22.55 22.62 21.03 21.16 22.22 22.11 22.88 23.25 22.94 23.24 22.33 22.30 23.51 23.64 100 100 100 90 100 72 60 60 32 32 28 28 37 37 Table 2. Quantitative comparison on Color Editing. The best and second-best results are shown in bold and underline."
        },
        {
            "title": "Method",
            "content": "RF-Solver RF-Solver+Ours FireFlow FireFlow+Ours UniEdit UniEdit+Ours BG Preservation CLIP Sim. SSIM (102) Whole Edited 80.21 86.63 80.14 86. 85.39 85.21 25.61 27.30 26.03 27.32 26.81 27.32 20.86 22.88 21.02 22. 21.74 22.56 source visual attention features , obtained from the inversion, are injected through the KV-mix module, and the final model outputs the target image by multiple steps of sampling. and 4. Experiments 4.1. Setup Baseline. We mainly compare our methods with preturning-free visual editing methvious state-of-the-art ods. text-driven image editing, we compare For our ProEdit with diffusion-based methods: P2P [13], PnP [48], PnP-Inversion [21], EditFriendly [16], MasaCtrl [5], and InfEdit [55], along with flow-based methods: RF-Inversion [43], RF-Solver [51], Fire-Flow [8], and UniEdit [18]. For text-driven video editing, we compare our ProEdit with FateZero [39], Flatten [6], Tokenflow [12], and RF-Solver [51]. Datasets. For text-driven image editing, we evaluate our method based on the PIE-Bench [21], which contains 700 images with 10 different editing types. For text-driven video editing, we collected 55 text-video editing pairs with resolution of 480480, 540960 or 960540, consist of 40 to 120 frames, including the videos sourced from DAVIS dataset [38] and online platforms. The prompts are derived from ChatGPT or contributed by the authors. Metrics. to evaluate For text-driven image editing, edit-irrelevant context preservation, we use structure distance [47], PSNR [17] and SSIM [52] for annotated unedited regions. The performance of the edits is assessed using CLIP similarity [40] for both the whole image and the edited regions. For text-driven video editing, we follow the metrics proposed in VBench [15, 59], including Subject Consistency, Motion Smoothness, Aesthetic Quality, and Imaging Quality. Implementation. We primarily product experiments using FLUX.1-[dev] [26] for image editing and HunyuanVideo720p [25] for video editing. For image editing, we have made ProEdit plug-and-play for flow-based inversion methods: RF-Solver, FireFlow, and UniEdit. Notably, UniEdit uses α to denote the delay injection rate, and experiments were conducted with α = 0.6 and α = 0.8. In our experiments, unless otherwise specified, the delay rate α in UniEdit is set to 0.8. We set the sampling step for image editing to 15. For video editing, we have made ProEdit 6 Figure 5. Qualitative comparison on image editing. With our method, various flow-based inversion methods achieve more appropriate editing while preserving the consistency of background and non-editing content. plug-and-play for RF-Solver. We set the sampling step for video editing to 25. 4.2. Text-driven Image Editing Quantitative Evaluation. Table 1 presents the quantitative results for text-driven image editing. The results in the table show that with our ProEdit, flow-based inversion methods achieved superior results in image editing. Notably, ProEdit with the UniEdit inversion method achieves state-of-theart performance in both source content preservation and editing quality. Qualitative Evaluation. We compare the performance of our method with several baselines across different types of editing requirements in Fig 5. The baseline methods often fail to maintain the consistency of non-editing attributes such as background and posture, or fail to achieve satisfactory editing results. In contrast, our method achieves highquality editing results while maintaining the consistency of non-editing content. Color Editing. To validate that our method addresses the latent distribution injection issue overlooked by previous methods, We conducted experiments on the color editing task in PIE-Bench, which is significantly affected by the latent distribution. Table 2 shows the quantitative results for color editing. With our ProEdit, all flow-based inversion methods achieved impressive results. This supports the motivation behind our proposed Latents-Shift module. The AdaIN-based Latents-Shift helps the editing process break free from the constraints imposed by the source image distribution. We further validate it by visualizing the attention map after adding Latents-Shift in Fig 6. 4.3. Text-driven Video Editing Quantitative Evaluation. Table 3 presents the quantitative results for text-driven video editing. For each metric, we report the average score of all videos. The results in the table show that with our ProEdit, flow-based inversion methods achieved superior results in video editing. This proves the versatility of our method for flow-based models, demonstrating its applicability to video editing tasks and its ability to improve editing performance. Qualitative Evaluation. We compare the performance of our method with several methods in Fig 7. The baseline methods often fail to maintain the consistency of nonediting attributes such as background and posture, or fail to achieve satisfactory editing results. In contrast, ProEdit achieves high-quality editing while maintaining spatial and temporal consistency. 4.4. Ablation Study We conduct ablation study on PIE-Bench. First, we evaluate the effectiveness of each module we proposed and validate their synergistic effect. Then, we explore the best combinations within the attention mixing injecting mechanism. The Synergistic Effect Analysis. We evaluate the effectiveness of the proposed KV-mix and Latents-Shift in TaFigure 6. Visualization of attention map after performing ProEdit. The initial distribution is shifted to target prompt and during the sampling, the model can accurately edit the image while maintaining non-editing attribute and background concsistent. Table 4. Quantitative comparison for the ablation study. KVm, LS mean KV-mix, Latents-Shift in our method. The best and second-best results are shown in bold and underline respectively. Method KV-m LS CLIP Sim. Whole Edited RF-Solver FireFlow UniEdit 26.00 26.21 26. 25.98 26.22 26.28 26.97 27.02 27.08 22.88 23.21 23.25 22.94 23.18 23.24 23.51 23.54 23.64 serve significant improvement in CLIP similarity due to the reduced influence of source features in the attention. After incorporating the Latents-Shift module, the CLIP similarity is further enhanced as the influence of the source image on the inversion noise latent distribution is eliminated. In summary, the various modules of ProEdit work synergistically to improve the editing results. The Attention Feature Combination Effect Analysis. We evaluated the effectiveness of different attention feature combinations applied in the fusion injection mechanism using the RF-Solver inversion method on PIE-Bench to verify the superiority of our proposed KV-mix module. The quantitative results for different attention feature combinations is shown in Supplementary File. Note that the attention feature is the most important attention feature for editing quality, so all the attention feature combinations we evaluated include V. Among the four combinations we evaluate, the KV combination achieved the best performance in both background consistency preservation and editing quality. Therefore, we adopted the KV fusion injection mechanism and designed the KV-mix module. Figure 7. Qualitative comparison on video editing. The video comprises 48 frames with resolution of 540 960. Table 3. Text-driven video editing comparison. We report the peer-reviewed results of each baseline, and evaluate our proposed method using flow-based inversion method RF-Solver. The best and second-best results are shown in bold and underline. Method SC MS AQ IQ FateZero [39] Flatten [6] TokenFlow [12] RF-Solver [51] RF-Solver+Ours 0.9612 0.9690 0.9697 0.9708 0. 0.9740 0.9830 0.9897 0.9906 0.9920 0.6004 0.6318 0.6436 0.6497 0.6518 0.6556 0.6678 0.6817 0.6866 0.6936 ble 4. Note that without the modules we proposed, each method is evaluated using its original source code setup. When the KV-mix feature injection mechanism is applied to replace the original feature injection mechanism, we ob8 5. Conclusion In this work, we identified the issue of excessive injection of source image information caused by the inverted latent with global injection strategy used in existing flowbased inversion editing methods, which leads to sacrificing editing quality in order to maintain background consistency with the source image during the editing process. We introduce ProEdit, novel, training-free method that addresses this issue by proposing the KV-mix and Latents-Shift modules from both the attention and latent perspectives, aiming to eliminate the negative impact of excessive source image information injection on editing quality. Extensive experiments show that ProEdit can be seamlessly integrated into existing flow-based inversion methods, achieving high background consistency and excellent editing quality simultaneously."
        },
        {
            "title": "References",
            "content": "[1] Michael S. Albergo, Nicholas M. Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. CoRR, 2023. 3 [2] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel Cohen-Or. Stable flow: Vital layers for training-free image editing. In CVPR, 2025. 3 [3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. 3 [4] Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. Ditctrl: Exploring attention control in multi-modal diffusion transformer for tuning-free multi-prompt longer video generation. In CVPR, 2025. 3, 1 [5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In CVPR, 2023. 3, 6 [6] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flowguided attention for consistent text-to-video editing. In ICLR, 2024. 6, [7] Yusuf Dalva, Kavana Venkatesh, and Pinar Yanardag. Fluxspace: Disentangled semantic editing in rectified flow transformers. CoRR, 2024. 2 [8] Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. Fireflow: Fast inversion of rectified flow for image semantic editing. In ICML, 2025. 2, 3, 4, 6, 1 [9] Wenkai Dong, Song Xue, Xiaoyue Duan, and Shumin Han. Prompt tuning inversion for text-driven image editing using diffusion models. In ICCV, 2023. 3 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 3 [11] Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising. In ECCV, 2024. 2 [12] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2024. 6, 8 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. [14] Xun Huang and Serge Belongie. Arbitrary style transfer in In ICCV, real-time with adaptive instance normalization. 2017. 5 [15] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, 2024. 6 [16] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In CVPR, 2024. 6 [17] Quan Huynh-Thu and Mohammed Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics letters, 2008. 6 [18] Guanlong Jiao, Biqing Huang, Kuan-Chieh Wang, and Renjie Liao. Uniedit-flow: Unleashing inversion and editing in the era of flow models. arXiv preprint arXiv:2504.13109, 2025. 3, 6, [19] Ajay Jain Jonathan Ho and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 2, 3 [20] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting In ECCV, model with decomposed dual-branch diffusion. 2024. 3 [21] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. In ICLR, 2024. 2, 3, 6, 1 [22] Phillip Isola Jun-Yan Zhu, Taesung Park and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017. 3 [23] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. TPAMI, 2021. [24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, 2023. 3 [25] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 3, 6 [26] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 3, 6 [27] Hongyu Li, Manyuan Zhang, Dian Zheng, Ziyu Guo, Yimeng Jia, Kaituo Feng, Hao Yu, Yexin Liu, Yan Feng, Peng Pei, et al. Editthinker: Unlocking iterative reasoning for any image editor. arXiv preprint arXiv:2512.05965, 2025. 3 [28] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang, and Jian Yang. Stylediffusion: Prompt-embedding inversion for text-based editing. CoRR, 2023. 3 [29] Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Ying Shan, Yuexian Zou, and Qiang Xu. Brushedit: All-in-one image inpainting and editing. CoRR, 2024. 3 [30] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [31] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learfning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, 2025. 3 [33] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 2 [34] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models. In WACV, 2025. [35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In CVPR, 2023. [36] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In SIGGRAPH, 2023. 3 [37] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, 2023. 2 [38] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 6 [39] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: FusIn ing attentions for zero-shot text-based video editing. CVPR, 2023. 6, 8 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2 [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 2015. 2 [43] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differential equations. In ICLR, 2025. 6 [44] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, 3 and Stefano Ermon. arXiv preprint [45] Qwen Team. Qwen3 technical report, 2025. 2 [46] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object inserIn ICLR, tion in images with pretrained diffusion models. 2025. 3 [47] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In CVPR, 2022. 6 [48] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Plug-and-play diffusion features for text-driven Dekel. image-to-image translation. In CVPR, 2023. 2, 3, 6, 1 [49] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: exact diffusion inversion via coupled transformations. In CVPR, 2023. 3 [50] Fangyikang Wang, Hubery Yin, Yuejiang Dong, Huminhao Zhu, Zhang Chao, Hanbin Zhao, Hui Qian, and Chen Li. BELM: bidirectional explicit linear multi-step sampler for exact inversion in diffusion models. In NeurIPS, 2024. 3 [51] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. In ICML, 2025. 2, 3, 4, 6, 8, 1 [52] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 2004. [53] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in texttoimage diffusion models. In CVPR, 2023. 3 [54] Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, Charles Ling, and Boyu Wang. Unveil inversion and invariance in flow transformer for versatile image editing. In CVPR, 2025. 3 [55] Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. In CVPR, 2024. 3, 6 [56] Yu Xu, Fan Tang, Juan Cao, Yuxin Zhang, Xiaoyu Kong, Jintao Li, Oliver Deussen, and Tong-Yee Lee. Headrouter: training-free image editing framework for mmdits by adaptively routing attention heads. arXiv preprint arXiv:2411.15034, 2024. 3 [57] Guoqiang Zhang, John P. Lewis, and W. Bastiaan Kleijn. Exact diffusion inversion via bidirectional integration approximation. In ECCV, 2024. 3 [58] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [59] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Lulu Gu, Yuanhan Zhang, Jingwen He, WeiShi Zheng, et al. Vbench-2.0: Advancing video generation 10 benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025. 6 [60] Dian Zheng, Cheng Zhang, Xiao-Ming Wu, Cao Li, Chengfei Lv, Jian-Fang Hu, and Wei-Shi Zheng. Panorama generation from nfov image done right. In CVPR, 2025. 3 11 ProEdit: Inversion-based Editing From Prompts Done Right"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Extracting Mask From Attention Map When extracting the mask from the attention map, we adopted strategy different from DitCtrl[4]. In our observation, the attention map of the last Double block effectively associates the relevant regions of text and image. Fig 8 presents an example showing that both the editing region mask extracted from the last Double blocks attention map and the mask extracted from the average of all attention maps successfully segment the editing and non-editing regions. Additionally, this mask extraction method reduces memory consumption. Therefore, we extract our editing region mask from the attention map of the last Double block. Notably, we always extract the mask from the first step of inversion or the last step of sampling, as the images at these time steps are least affected by noise and have the best text-to-image correlation. Due to the downsampling operation performed in the feature space relative to the pixel space, the extracted mask has coarser granularity and cannot fully cover the boundary regions of the editing target in the pixel space. Therefore, we apply diffusion operation to the mask, expanding it outward by one step to obtain coarser mask that can fully cover the editing area. Given the strong global adaptability of generative models, this relatively coarse masking is sufficient for semantic alignment. The boundary coverage between the editing and non-editing regions helps smooth the edges and avoid image artifacts. The target object of the mask extraction can be identified by the noun of the editing object or through an externally provided mask for more flexible control. B. Implementation Details For image and video editing, we set the mix strength δ = 0.9 to balance source content preservation and editing performance. The fusion ratio β is set to 0.25 to achieve the best editing results. At each timestep, the feature fusion injection mechanism is applied to all Double and Single blocks. We fine-tune the hyperparameters of the attention feature fusion injection steps to obtain better image and video editing results. We use the official implementations of all baseline methods and adjust their hyperparameters to achieve satisfactory performance. C. Quantitative Results of Attention Feature"
        },
        {
            "title": "Combination Effect",
            "content": "Table 5 shows the quantitative results of different attention feature combinations in the fusion injection mechanism. Figure 8. visual comparison of the editing region mask extracted from the last Double block and all blocks. Using orange as the editing target, the editing region masks extracted from both the last Double block and all blocks effectively segment the editing region. Table 5. Quantitative experiments on different attention feature combinations. The best and second-best results are shown in bold and underline respectively. Method Q&V Q&K&V K&V BG Preservation CLIP Sim. PSNR SSIM (102) Whole Edited 24.04 24.51 23.69 24.77 82.24 83.04 81.68 84.78 26.16 26.20 26.26 26. 23.04 22.97 23.15 23.25 Among the four combinations evaluated, the KV combination demonstrated satisfactory results in both background consistency and editing quality. Therefore, we adopted the KV fusion injection mechanism and designed the KV-mix module. D. More Qualitative Results for Image Editing Here we provide more qualitative results for image editIn cases where other inversion-based editing in Fig 9. ing methods[8, 18, 21, 48, 51] fail, result in insufficient edits, or fail to maintain consistency, our method successfully achieves semantically consistent editing and demonstrates impressive performance. It is worth noting that in our qualitative results (3rd, 5th, and 6th rows in Fig 9), our method is able to effectively preserve human characteristics when editing human-centric images. E. More Qualitative Results for Video Editing Here we provide more qualitative results for video editing in Fig 10. Our method demonstrates impressive performance across wide range of video editing tasks, while maintaining temporal consistency and preserving the original motion patterns. 1 F. Editing by Instruction To lower the barrier for using our method and make it more user-friendly, we introduce large language model Qwen38B[45] to enable editing based on editing instructions. Fig 11 shows the qualitative results of our method based on editing instruction. With the assistance of large language model, our method can directly perform edits guided by editing instructions. 2 Figure 9. More qualitative comparison of image editing on PIE-Bench[21]. Figure 10. More video editing results. Figure 11. Qualitative results of image editing based on editing instruction. The actual input editing instruction are shown above each source image and its corresponding edited image."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "College of Computing and Data Science, Nanyang Technological University",
        "Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China",
        "Sun Yat-sen University",
        "The University of Hong Kong"
    ]
}