{
    "paper_title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers",
    "authors": [
        "Xiaotong Ji",
        "Rasul Tutunov",
        "Matthieu Zimmer",
        "Haitham Bou-Ammar"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures."
        },
        {
            "title": "Start",
            "content": "Decoding as Optimisation 6 2 0 2 0 2 ] . [ 1 2 9 2 8 1 . 2 0 6 2 : r Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers Xiaotong Ji1,, Rasul Tutunov1, Matthieu Zimmer1, Haitham Bou-Ammar1,2, 1 Huawei Noahs Ark 2 AI Centre, Department of Computer Science, UCL Equal contributions Abstract: Decoding sits between language model and everything we do with it, yet it is still treated as heuristic knob-tuning exercise. We argue decoding should be understood as principled optimisation layer: at each token, we solve regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures. Our message is simple: Decoding is not hack; it is optimisation!"
        },
        {
            "title": "1 Introduction",
            "content": "Large language model (LLM) decoding is usually taught like cookbook: Top-K [10, 28], temperature [13, 16], Top-P [14, 27], greedy [35, 32], beam search [38, 11]; shelf of tricks you pick from depending on the application[15, 19, 22, 25, 36, 37, 44, 48], whether you want more determinism [12, 34], more diversity [39, 41, 46, 47], or fewer hallucinations [9, 40, 43]. The problem is that this framing makes decoding feel like bag of heuristics: useful, but conceptually disconnected from the rest of machine learning. This paper argues the opposite: many decoding strategies are not heuristics at all. They are solutions to explicit optimisation problems, often the same optimisation problem, with different regularisers and constraints. Once you see decoding through that lens, familiar algorithms stop looking like folklore and start looking like principled design choices. Greedy decoding becomes limiting case of an objective with no regularisation. Softmax sampling becomes the unique optimum of score-maximisation problem regularised by (negative) Shannon entropy. Sparsity-inducing decoders arise from alternative convex penalties. In short: decoders differ less by how they sample and more by what objective they are implicitly optimising. Our starting point is small, but surprisingly powerful, shift in perspective. decoder 1 Decoding as Optimisation does not have to immediately choose token. At each step, it can first choose distribution over tokens, and only then sample (or take the mode). This turns decoding into clean optimisation problem over the probability simplex: pick distribution that (i) puts mass on high-scoring tokens while (ii) satisfying desirable structural preferences such as smoothness, sparsity, or staying close to reference distribution. This distribution-first view is general enough to cover deterministic and stochastic decoding in one line, and it exposes what decoding algorithms do: they are trading off score against regularisation under simplex constraints. With that formulation in place, we do something that decoder discussions rarely do: we derive the optimality conditions carefully. Because the variable is probability distribution, constraints are not decorative; they shape the solution. The simplex geometry introduces the familiar active vs inactive behaviour: tokens assigned nonzero probability satisfy an equality condition, while tokens pushed to zero satisfy an inequality. These KKT-style conditions act like master key: once derived, you can plug in choice of regulariser and immediately recover the structure of the decoder it implies. We then use this master key to re-derive canonical decoding rules as special cases. Setting the regulariser weight to zero collapses the optimisation to linear objective on the simplex, recovering greedy decoding. Choosing negative entropy yields smooth interior optimum and produces the softmax distribution as closed-form solution, where temperature sampling is not hack; it is the optimiser of maximum-entropy-regularised objective. More broadly, this paper builds the intuition that decoding methods are best understood as regularisation families, rather than as unrelated procedures. The takeaway is simple: decoding is an optimisation layer sitting on top of the models scores. More precisely, decoding is convex optimisation problem whose geometry is determined by the choice of regulariser. Once we treat it that way, we gain principled vocabulary for designing new decoders: decide the behaviour (diversity, sparsity, conservatism, stability), encode it as regulariser or constraint, and let the resulting optimisation problem define the algorithm. Beyond theoretical unification, our framework serves as generative tool for designing next-generation decoding objectives. We exemplify this by addressing the inefficiencies in current multi-sample pipelines through the design of Best-of-K (BoK). Unlike traditional methods that rely on empirical folklore, BoK utilises KL-anchored coverage objective to maximise the likelihood of capturing high-quality candidates within constrained Ksample budget. Our results demonstrate that by optimising for coverage, BoK not only provides more principled decoding path but also achieves improved practical performance. Across two 7B Qwen models (a math-specialised variant and general-purpose variant) and three complementary benchmarks, MATH500, GPQA-diamond, and HumanEval, BoK samplers act as practical decoding-time regulariser that improves multi-sample generation without any extra training or external verifiers. Sweeping temperature from neardeterministic (τ=0.1) to highly stochastic (τ=0.9), BoK samplers consistently match or outperform standard sampling and Top-K, with the largest gains precisely where vanilla sampling is most diverse but least reliable. For example, on the math-specialised model at τ=0.9 on MATH500, BoK raises accuracy from 53.0% (Base) to 71.6% (+18.6%), exceeding TopK (56.2%) by +15.4%; similar high-temperature gains appear on GPQA (+6.06%) and HumanEval (+14.64%). These improvements are robust across range of (β, λ) choices (coverage vs. KL anchoring), indicating stable operating region rather than brittle tuning. Finally, BoKs benefits come with only modest compute overhead: using 5 mirror-ascent steps 2 Decoding as Optimisation per token adds about 1s on MATH500 (16.88s vs. 15.84s), while even 2 steps already yield sizeable jump (64.4%69.6%) with negligible runtime increasesuggesting fast solver convergence and making BoK viable as lightweight drop-in for practical decoding. In short, our contributions can be stated as: Summary of Contributions 1. Unified Theory of Decoding: We unify decoding strategies into single framework, proving they are closed-form optima of objectives on the simplex. 2. Generative \"Master Key\" for Decoder Design: general framework for automatically deriving iterative algorithms for any decoding behaviour expressed as regulariser, enabling optimisation beyond closed-form solutions. 3. Case Study - Best-of-K (BoK) Decoding: We introduce Best-of-K (BoK), coverage-based objective for self-consistency and reranking. It replaces heuristic sampling with mathematically grounded approach that improves performance."
        },
        {
            "title": "2 Decoding, Sampling and Optimisation",
            "content": "Our central claim is that decoding strategies are not heuristics, but rather implicit solutions to well-defined optimisation problems. Sampling-based methods do not introduce randomness arbitrarily; instead, they correspond to soft or regularised forms of optimisation. Conversely, deterministic methods such as greedy arise as limiting or approximate solutions to hard optimisation objectives. From this perspective, decoding algorithms differ not in kind, but in what they optimise and the constraints they impose. MASTER OBJECTIVE max q( V) [q, st λΩ(q)] subject to: Ct MODE λ = 0 Greedy closed form MODE 02 MODE 03 Ω = H(q) Support Ct MODE 04 Ω q2 2 Softmax closed form Top-K/P closed form Sparsemax closed form MODE Ω(BoK) BoK mirror ascent Figure 1. Framework of Decoding as Optimisation: The master objective generalises standard LLM decoding strategies. By choosing appropriate λ, Ω(q) and Ct, we can recover current decoding strategies as special cases. 3 Decoding as Optimisation The goal of this section is to make this connection explicit. We begin by formalising decoding as the problem of selecting distribution over next tokens, rather than directly selecting token. This shift allows us to express decoding as an optimisation problem over distributions, balancing model score and regularisation. We then show that common decoding strategies emerge naturally as special cases."
        },
        {
            "title": "2.1 Decoding as a Decision Over Distributions\nTo make the optimisation perspective precise, we start by formalising what a decoder actu-\nally does at a single generation step. Consider a language model with parameters θ. Given a\nprefix x<t = (x1, . . . , xt−1), the model produces a real-valued score for every token v in the\nvocabulary V. In practice, these scores are the logits or log-probabilities produced by the\nmodel. We denote them by: st(v) ∈ R, for v ∈ V.",
            "content": "At this point, most decoding descriptions implicitly assume that we must immediately choose single token xt. Instead, in this work, we take small but crucial conceptual step back. Rather than asking which token to select, we ask: Which distribution over tokens should we use to make the selection? Formally, we introduce an auxiliary distribution qt() ( V) with ( V) denoting the probability simplex over the vocabulary V. Once qt() is chosen, decoding proceeds by either: sampling xt qt(), or deterministically selecting xt arg maxv qt(v). Importantly, this viewpoint is general enough to cover both stochastic and deterministic decoding, whereby i) greedy decoding corresponds to degenerate distribution qt() that places all its mass on single token 1, while ii) sampling-based decoding corresponds to non-degenerate qt() with positive entropy. To illustrate this distinction, consider simple example with vocabulary of three tokens: = {a, b, c}. Suppose that, for given prefix xt, the language model scores each token as follows: Now, greedy decoding selects the highest-scoring token: st(a) = 3, st(b) = 2, st(c) = 0. arg max In our framework, this corresponds to choosing degenerate distribution qt() that st(v). places all its probability mass on the maximiser of the score function: (cid:40) qt(v) = if arg max 1, 0, otherwise. st(u), In the example above, since st(a) > st(b) > st(c), this reduces to: 1Please note we assume rule that breaks ties. qt(a) = 1, qt(b) = 0, qt(c) = 0. Decoding as Optimisation By contrast, sampling-based decoding selects non-degenerate distribution that assigns nonzero probability to multiple tokens, while still favouring higher-scoring ones. In our framework, such distribution is also obtained as function of the score vector st. Concretely, for given temperature parameter τ > 0, sampling-based decoding chooses: qt(v) = exp(st(v)/τ) exp(st(u)/τ) , which assigns higher probability to tokens with larger scores, while maintaining positive entropy. In the example above, with scores st(a) = 3, st(b) = 2, and st(c) = 0, this construction yields distribution of the form: qt(a) 0.6, qt(b) 0.3, for an appropriate choice of τ. As such, sampling from qt() will most often select the highest-scoring token a, but will occasionally produce or c. This stochasticity allows the decoder to explore alternative continuations while remaining aligned with the models scoring function. qt(c) 0, Up to this point, common decoding strategies such as greedy decoding and temperature-based sampling may still appear as largely heuristic choices: practical rules that users select and tune based on empirical behaviour or trial-and-error. From this perspective, the decoder is seen as sampler whose properties, e.g., determinism, diversity, or randomness, are controlled by externally chosen parameters such as temperature. In the next section, we show that this view is incomplete if not misleading. In fact, these decoding strategies are not heuristic procedures, but rather exact solutions to well-defined optimisation problems. Greedy decoding and temperature-based sampling emerge naturally as optimisers of common objective that trades off expected model score against regularisation. The apparent differences between decoding methods are therefore not algorithmic accidents, but reflect differences in the underlying objectives being optimised. This perspective enables shift in how decoding strategies are designed and understood. Rather than treating samplers as ad-hoc mechanisms, we can view them as solutions to optimisation problems over distributions. Under this lens, designing decoder with desired properties such as increased diversity, robustness, or constraint satisfaction reduces to specifying the appropriate optimisation objective and constraints. We now make this connection explicit."
        },
        {
            "title": "2.2 Decoding as Distributional Optimisation\nThe discussion so far suggests that decoding can be viewed as a decision over distributions\nrather than individual tokens. We now make this perspective explicit by introducing a gen-\neral optimisation formulation that subsumes a wide range of decoding strategies.",
            "content": "At given decoding step t, recall that the model assigns score st(v) to each token V. We seek to choose distribution qt() ( V) from which the next token will be drawn (or whose mode will be selected). We define decoding as the solution to the following optimisation problem: MASTER PROBLEM = arg max q( V) (cid:104) q, st λ Ω(q) (cid:105) s.t. Ct. (1) 5 Decoding as Optimisation In the above equation, q, st = q(v) st(v), as the expected models score under the distribution q(), and Ω(q) is regularisation functional that encodes preferences such as diversity, sparsity, or stability. Moreover, λ 0 controls the strength of regularisation, and Ct denotes constraints on the support of q(). From geometric perspective, each component of the formulation plays structural role. The simplex constraints induce boundary behaviour: tokens may become active or inactive depending on whether the optimiser settles in the interior or on face of the feasible set. Different choices of regulariser shape this geometry in different ways: some encourage interior solutions, while others allow the optimiser to concentrate mass on lowerdimensional faces. Likewise, hard support constraints restrict the feasible region itself, carving out sub-simplices before optimisation even begins. As we will see in Section 3, classical decoding rules correspond to particular geometric choices within this framework. Interpreting Equation (1). The objective above consists of two competing terms. The first term, q, st, encourages the decoder to place probability mass on tokens with high model scores. If this term were optimised alone, the resulting distribution would collapse onto the highest-scoring token. The second term, Ω(q), acts as regulariser that penalises certain properties of the distribution q(). Its role is to control the shape of the decoding distribution, for example, by encouraging diversity or limiting deviation from reference distribution. The scalar λ determines the trade-off between strictly optimising model score and satisfying these additional preferences. Finally, the constraint set Ct allows the decoder to enforce hard restrictions, such as limiting the support to subset of tokens or excluding invalid continuations. Under this view, the design of decoding strategy reduces to: 1. What notion of quality is being optimised (through st(v)); 2. What properties are desired in the decoding distribution (through Ω(q)); and 3. What hard constraints must be respected (through Ct). Deriving Solution Conditions for Equation (1). Deriving the closed-form solution conditions for our general optimisation problem is relatively involved. We present it in steps that make various assumptions about Ct, (q) and Ω(q). In the first case, we consider the absence of Ct. Here, our optimisation problem becomes the following: = arg max q( V) (q, st λΩ(q)) , for λ 0, (2) (cid:110) (cid:111) where ( V) is defined as: ( V) = . It is easy to see that maximising the objective in Equation (2) can be equivalently rewritten as minimisation problem, such that: V : q(v) 0, q(v) = = arg min q( V) (λΩ(q) q, st). (3) Assuming that Ω(q) is convex2, the minimisation problem we just derived (Equation (3)) is simply convex - linear minimisation objective for which clean optimality conditions 2The convexity of Ω(q) is typical. As we show later, many special cases of LLM decoding strategies stem from different convex choices of Ω(q). 6 Decoding as Optimisation are easily attainable. What makes solving the problem in Equation (3) hard is the existence of the constraint set ( V). If we were to ignore those constraints, we could just set the derivative to zero, like in unconstrained calculus, and proceed. However, the existence of those constraints means we must be more careful: we are minimising over restricted set, so we should take those into account. But what do those constraints actually mean? Our constraints in ( V) are not fancy at all. They are pretty simple! All they do is make q() valid probability distribution. In detail, when we say V, we really mean the following two properties should hold: q(v) = 1 Forcing q() to be probability distribution that sums to 1 across all realisations of the random variable; and q(v) 0 for all You cant have negative q() values on any V. As is tradition in mathematical derivations, we begin by pretending the inconvenient part does not exist, ignore the second condition (q(v) 0), and come back to it later. So pretend for moment that q(v) can be any real numbers as long as they sum to 1. Here, our optimisation problem becomes: min J(q) λΩ(q) q(v)st(v), s.t. q(v) = 1. Now, we really, really want to take derivatives and set them to zero. Sadly, there is constraint! standard trick is: add the constraint into the objective with price η. This gives us what every constrained optimisation problem eventually produces, the Lagrangian: L(q, η) = λΩ(q)(1) q(v)st(v)(2) + η (3) (cid:33) q(v) 1 . (cid:32) (cid:125) (cid:123)(cid:122) (cid:124) The Constraint Penalty We now have an unconstrained objective, which allows us to deploy one of the great discoveries of the last century: the gradient. Let us do that! Let us take the partial derivatives of L(q, η) with respect to q(v). Doing so, term-by-term, we get: (1) q(v) λΩ(q) = λ q(v) Ω(q)(v), (2) q(v) (cid:32) (cid:33) q(v)st(v) = st(v), (3) q(v) η( q(v) 1) = η. So on the stationary point , we get this condition: λ q(v) Ω q(v)(q )(v) st(v) + η = 0 = st(v) λ q(v) Ω(q )(v) = η, V. (4) It is easyish to interpret the condition in Equation (4). It simply says that for every token )(v) must be the same constant η. This is balancing Ω(q v, the quantity st(v) λ condition created by the fact that probabilities must sum to 1. q(v) 7 Decoding as Optimisation Getting the Inequality Back. At this stage, we have derived general condition, but we should not forget about our previous promise! We still have to enforce q(v) 0 because some solutions to Equation (4) can give negative (v). One Dimensional Example for Handling Inequality Constraints For now, let us forget about tokens and consider single-variable objective: (x). min x0 An optimal means that no feasible small move from can reduce the function value. move is feasible if it respects the constraint 0. Thus: i) if > 0 (an interior point), we can move small amount both to the right and to the left, i.e., to + δ and δ, for sufficiently small δ > 0; ii) if = 0 (a boundary point), we can only move to the right (to 0 + δ), since 0 δ would violate 0. To make this precise, we use first-order Taylor approximation. For small δ > 0, (x + δ) (x) + δ (x), (x δ) (x) δ (x). Since is optimal, any feasible move must not decrease , i.e., (x + δ) (x) for all sufficiently small feasible δ > 0, and, when > 0 (so that δ is also feasible), (x δ) (x) 0 for all sufficiently small δ > 0. Combining these inequalities with the Taylor expansions gives the two cases below: Case (Interior optimum > 0): both + δ and δ are feasible. Hence δ (x) 0 and δ (x) 0 δ > 0, which implies (x) = 0. Case II (Boundary optimum = 0): only 0 + δ is feasible. Hence which implies (0) 0. δ (0) 0 δ > 0, In other words, these conditions reflect whether lies in the interior or on the boundary of the feasible set. If > 0, we can perturb both to the right and to the left while remaining feasible, and optimality therefore forces the slope to vanish, i.e., (x) = 0. If = 0, we can only move into the feasible region (to the right), so optimality only requires that rightward perturbations do not decrease (), equivalently (0) 0 (a one-sided slope condition). 8 Decoding as Optimisation Having understood what conditions we would need when having non-negativity feasibility set, we can now go back to our tokens and apply the same reasoning coordinate-wise to each component q(v) 0. In our problem, the role of (x) is played by the partial derivative of the Lagrangian with respect to the coordinate q(v). Recall the Lagrangian: L(q, η) = λΩ(q) q, st + η q(u) 1 (cid:17) , (cid:16) where the scalar η enforces the normalisation constraint q(u) = 1. Taking the partial derivative with respect to q(v) yields: q(v) L(q, η) = λ q(v) Ω(q)(v) st(v) + η. By the same one-dimensional argument as above, optimality under the constraint q(v) 0 implies two-case condition: q(v) > 0 q(v) = 0 q(v) L(q, η) = 0, q(v) L(q, η) 0. Substituting the expression for optimality of the solution, which we state below. q(v) L(, ) and rearranging gives us two conditions for the Take Home Message: KKT Optimality Conditions From the above derivations, the take-home message is that we can characterise an optimal t using the following identities: GLOBAL q (v) = 1, (v) 0 V. ACTIVE (v) > 0 = st(v) λ INACTIVE (v) = 0 = st(v) λ q(v) q(v) Ω(q )(v) = η, Ω(q )(v) η. (5)"
        },
        {
            "title": "3 LLM Decoding Strategies are Different Regularisers",
            "content": "At this point, the reader may reasonably wonder what all of the preceding mathematics was for. We now reap what we have sown (Galatians 6:7). In this section, we show that many widely used decoding strategies arise as simple special cases of our general formulation. Those will correspond to different choices of regularisation Ω(q), λ and Ct."
        },
        {
            "title": "3.1 Greedy Decoding: The Boring but Necessary Case\nWe begin with the simplest possible case, which serves mainly to reassure us that nothing\nhas gone terribly wrong. To recover greedy decoding, we assume there is no regulariser by",
            "content": "9 Decoding as Optimisation setting λ = 0. We also ignore Ct for the time being. As such, our Master objective from Equation (1) quietly collapses into far more modest one: = arg max q( V) q, st arg max q( V) q(v)st(v). We now cash in the take-home KKT-style conditions from Equation (5). In this greedy case, we have for some scalar η: ACTIVE (v) > 0 st(v) = η, INACTIVE t (v) = 0 st(v) η. These two lines already characterise the solution: every token that receives nonzero probability must have exactly the same score, and every token with zero probability must have score no larger. Let := maxu st(u) and define the argmax set := arg max st(u) = {v : st(v) = M}. The inactive condition forces η st(v) for all v, hence η M. But the active condition says there exists at least one active token (since q (v) = 1), and any active token must satisfy st(v) = η, hence η must equal the maximum score: η = M. Therefore, the only tokens that can be active are those in V, i.e., (v) > 0 V, equivalently supp(q ) V. In words: an optimal solution places all probability mass on the highest-scoring token(s). If the maximiser is unique, = {v}, then the unique optimum is the degenerate distribution (v) = δv(v), where arg max st(u). If there are ties (i.e., > 1), then any distribution supported on is optimal. In other words, greedy decoding corresponds to selecting vertex of this optimal face, i.e., picking one V and using = δv."
        },
        {
            "title": "3.2 From Negative Entropy to Softmax: A Predictable Ending",
            "content": "Let us now consider the case where, in move that will surprise no one, Ω(q) is chosen to be the negative entropy of q, such that: Ω(q) = q(v) log q(v). This will make our optimisation objective from Equation (1) look like the following: = arg max q( V) (cid:34) q(v)st(v) λ (cid:35) q(v) log q(v) . Returning to the conditions in Equation (5), the suspense is, of course, which case applies: active or inactive? This hinges on single question: does () live comfortably in the interior of the simplex, or does it press up against the boundary? 10 Decoding as Optimisation The Key Mathematical Fact: The Derivative Blows Up at Zero To understand this, let us take step back and look at the derivative of (x) = log x. Why this (x)? That is how the entropy looks! For > 0, we can see that: dx (x log x) = dx log + dx log = 1 log + = 1 + log x. 1 We now examine what happens as approaches the mysterious value 0+: 1 + log , lim x0+ because as 0+, log , and thus 1 + log . In other words, as our variable approaches 0+, the gradients blowup. Therefore, if we were optimising for x, every time we get closer to zero, the gradient politely but firmly tells us to move. Equipped with this realisation, we return to our case that contains the negative entropy regulariser Ω(q), which strongly discourages zero probabilities. Conveniently, the gradients make this preference explicit: which blows up as we q(v) approaches zero. Hence, our winner is ... the ACTIVE case: q(v) Ω(q) = 1 + log q(v), (6) ACTIVE (v) > 0 = st(v) λ q(v) Ω(q )(v) = η. Let us do some algebra! Replacing the derivative from Equation 6 in our condition, we get: (v)) η = 0 = λ(1 + log st(v) λ(1 + log (v)) = st(v) η st(v) η λ (cid:18) st(v) η λ 1 1 (cid:19) = log (v) = = = exp (v) = exp (cid:19) (cid:18) st(v) λ , λ . We know that with the constant = e1e η and valid probability distributions insist on summing to one. Let us indulge them: (cid:18) st(u) λ (u) = 1 = u = 1 = (cid:18) st(u) λ (v) must be valid probability distribution, C exp = exp (cid:19) (cid:19) = = 1 exp (cid:17) . (cid:16) st(u) λ 11 Decoding as Optimisation Therefore, our optimal distribution lariser, ends up being: () if we picked Ω(q) to be the negative entropy reguSOFTMAX DECODERS: (v) = exp (st(v)/λ) exp (st(u)/λ) . We thus recover the classical softmax decoder directly from our master optimisation problem. In particular, choosing the (negative) Shannon entropy as the regulariser produces the familiar temperature-controlled distribution, with λ playing exactly the role of the temperature τ in the LLM literature. This demonstrates that softmax decoding is not an independent heuristic, but the optimiser of our abstract objective under an entropic geometry. In this sense, our framework elevates decoding to higher-level problem definition: different decoders arise from different regularisers. Decoder design becomes regulariser design."
        },
        {
            "title": "3.3 Trimming the Vocabulary: Top-K Samplers\nFortunately, extending the results derived above to Top-K samplers requires only minimal\nadditional work. Until now, we have been pretending that the constraint Ct in Equation 1\ndoes not exist. For Top-K, this pretence is no longer sustainable. First, we define the indices\nof the k highest-scoring tokens. We define the set Vk ⊂ V such that: i) | Vk| = k, and ii)\n, st(v) ≥ st(u). This allows the constraint set C(k)\nto be a subset of the\n∀v ∈ Vk\nsimplex ∆( V) where tokens outside the top-k are forced to have zero probability:",
            "content": "and / Vk C(k) = {q ( V) : q(v) = 0, / Vk}. To ensure we end up with Softmax over the Top-K candidates, as before, we use the (negative) Shannon entropy as our regulariser Ω(q) = q(v) log q(v). With this, the objective becomes: (cid:34) = arg max (k) t V q(v)st(v) λ (cid:35) q(v) log q(v) . Because q(v) = 0 for all / Vk : the indices in Vk per our constraint, the summation effectively collapses to (cid:34) = arg max q( Vk) Vk q(v)st(v) λ Vk (cid:35) q(v) log q(v) . Following similar derivation to that in Section 3.2, we arrive at Top-K samplers giving us: TOP-K DECODERS: (v) = exp(st(v)/λ) exp(st(u)/λ) Vk 0 if Vk, otherwise. 12 Decoding as Optimisation"
        },
        {
            "title": "3.4 Mind the Mass: Top-P Sampling\nTop-P (nucleus) sampling adapts the size of the active support set based on the model’s\nconfidence in the current context. When the model’s distribution is flat (high uncertainty),\nthe nucleus expands to preserve diversity; when it is peaky (high confidence), the nucleus\ncontracts, filtering out low-probability noise in the long tail.",
            "content": "In our optimisation view, Top-P is obtained by replacing the fixed-cardinality support constraint of Top-K with cumulative-mass support constraint. Rather than restricting the decoder to exactly tokens irrespective of confidence, Top-P restricts the decoder to the smallest set of tokens whose model-assigned probability mass exceeds threshold (0, 1]. Defining the nucleus. Let pt() denote the model distribution over next tokens induced by the scores st. Sort tokens in descending order of pt: pt(v(1)) pt(v(2)) . Define to be the smallest index such that the cumulative mass reaches p, i.e., p, and set the nucleus to be3: v(i)(cid:17) i=1 pt (cid:16) Vp := {v(1), . . . , v(m)}. (p) This induces the context-dependent constraint set := {q ( V) : q(v) = 0, / Vp}, which is sub-simplex of ( V) supported on Vp. To recover the standard Top-P sampler, we again use the (negative) Shannon entropy regulariser, and solve the master objec- (p) tive restricted to : = arg max (p) t (cid:104) q, st λΩ(q) (cid:105) . Because q(v) = 0 for all / Vp, this optimisation is equivalent to optimising over the simplex ( Vp): = arg max q( Vp) (cid:104) Vp q(v)st(v) λ Vp q(v) log q(v) (cid:105) . The derivation in Section 3.2 therefore applies verbatim, yielding softmax distribution renormalised over the nucleus: TOP-P DECODERS: (v) = exp(st(u)/λ) exp(st(v)/λ) Vp 0 if Vp, otherwise. In words, Top-P sampling first selects context-dependent nucleus Vp based on cumulative model probability mass, and then samples from temperature-controlled softmax restricted to that nucleus."
        },
        {
            "title": "3.5 Letting Probabilities Go to Zero: Sparsemax Decoding\nWhile standard Softmax sampling is the standard choice for text generation, it suffers from\nthe “heavy tail” problem: because the derivative of Shannon entropy approaches −∞ as\nany probability q(v) approaches zero, the optimiser is strictly forbidden from reaching the",
            "content": "3If ties occur at the cutoff, we may break them arbitrarily. 13 Decoding as Optimisation boundary of the simplex. This forces the model to assign non-zero, albeit small, probability to every single token in the vocabulary, which can lead to the tail risk of sampling nonsensical or hallucinatory tokens. To solve this without the ad-hoc truncation rules of Top-K or Top-P, we look toward Sparsemax [23]. By replacing the logarithmic penalty of entropy with quadratic penalty, we allow the optimisation to reach the simplex boundary, effectively performing an automated, adaptive truncation that assigns exactly zero probability to low-scoring candidates. We again consider single decoding step with an empty Ct and solve the Master problem again. In this special case, we choose the quadratic regulariser: Ω(q) = 1 2 q2 2 = 1 2 vV q(v)2, = arg max q( V) (cid:20) q, st (cid:21) . q2 2 λ 2 (7) Please note that in our problem, both conditions in Equation (5) need to be considered. allows solutions that are positive but can also equate to This is the case since Ω(q) = 1 zero. Of course, the latter condition is what enables sparsity. Let us first consider the active (v) > 0. For token V, the active condition can be written as: tokens case, i.e., when 2 q2 2 ACTIVE st(v) λ q(v) Ω(q )(v) = η = st(v) λ (cid:35) (v) = η (cid:34) 1 q(v) 2 (v) = η. = st(v) λq Solving for q(v), we get: we can, thus, clearly see that st(v) > η. (v) = 1 λ (st(v) η). Also notice that since we are in the (v) > 0, Moving on to the second condition, we observe the following: INACTIVE st(v) λ q(v) Ω(q )(v) η = st(v) λq (v) η. Since this condition holds for token in which η. So, essentially, from those two conditions we learned that: (v) = 0, then we can conclude that: st(v) (v) = st(v) η λ 0 if st(v) > η if st(v) η. = (v) = 1 λ [st(v) η]+, with [x]+ = max(0, x) = ReLU(x). The last remaining ingredient needed to finalise our derivation is determining η. To do so, we make use of the constraint that q (v) = 1: t (v) = 1 = v 1 λ [st(v) η]+ = 1 = = [st(v) η]+ = λ max(0, st η) = λ. In other words, we need η such that the sum of the positive parts equals λ. As we see, it is challenging to acquire closed-form solution for η. Therefore, we develop an algorithmic solution. The key idea is that we will assume we know which tokens are active by defining 14 Decoding as Optimisation an active set S(η) = {v : st(v) > η}. Then, we have: i) If S(η), then [st(v) η]+ = st(v) η; and ii) If / S(η), then [st(v) η]+ = 0. Therefore, those conditions over η simplify to sum over the active set S(η), giving us: V(st(v) η) = λ. Unfortunately, this result is still implicit because S(η) depends on η itself. So, we apply the standard trick of guessing the active set size k. The key observation is that the active tokens are exactly those satisfying st(v) > η. Consequently, if token with some score st(v) is active, then every token with larger score must also be active. Therefore, the support of () must be Top-K set for some k. Let (1) (2) (n) denote the scores sorted in descending order, where := V. Assume that the active set has size and is given by the top-k indices: Under this hypothesis, the normalisation condition becomes: Sk(η) = {(1), (2), . . . , (k)}. i=1 since tokens outside Sk(η) contribute zero after the []+ clipping. Defining the prefix sum: Ak := η(cid:1) = 1, vS(η) , we obtain: (v) = 1 λ (cid:0)s (i) i=1 (i) = 1 = Ak kη = λ 1 (cid:0)Ak kη(cid:1) λ is not an arbitrary choice: it is the unique threshold value that would make Importantly, ηk (v) = 1 if the active set were exactly the top tokens. The remaining question is which q is self-consistent. The Top-K hypothesis holds precisely when the computed threshold ηk (k) separates the top scores from the rest, i.e., , with the convent > ηk tion = . We select any satisfying the above inequalities, set η = ηk, and then recover the optimal distribution via: and (n+1) (k+1) ηk := ηk Ak λ = . SPARSEMAX-STYLE DECODERS: (v) = 1 λ [st(v) η]+."
        },
        {
            "title": "4 Going Beyond Current Decoders",
            "content": "So far, our story has been pleasantly closed-form. Once we write decoding as regularised optimisation problem on the simplex, the KKT conditions act like master key, and classical decoders fall out as special cases. In complementary direction, prior work has shown that search-based decoding methods (e.g., beam search) can also be reinterpreted as optimising an explicit regularised objective [24]. But this view raises an immediate practical question: what do we do when the optimiser is no longer solvable in one line? The moment we introduce richer regularisers, coupling terms, coverage objectives, or more structured constraints, the distribution () is still well-definedbut we cannot always write it down analytically. This section switches from deriving decoders to computing them. We introduce mirror descent (and mirror ascent) as principled method tailored to simplex geometry. It preserves non-negativity and normalisation by construction and provides an algorithmic template that solves our master problem even when closed-form solutions are unavailable. 15 Decoding as Optimisation"
        },
        {
            "title": "4.1 Why not just run Vanilla (projected) Gradient Ascent on q(·)?\nIf we look back at our master problem, it is natural to try solving it with a standard tool from\nconstrained optimisation: projected gradient ascent [3, 4]. After all, we are maximising a\ndifferentiable objective over a convex set (the simplex, possibly intersected with additional\nconstraints). One could take a gradient step and then project back onto ∆( V), repeating un-\ntil convergence. In practice, this approach is indeed feasible, and it is often the first method\npeople reach for. To make this explicit, we write the following optimisation problem and\nupdate rule4:",
            "content": "MASTER PROBLEM max q( V) (q), with (q) = q, st λ Ω(q), PROJECTED GRADIENTS qj+1 = ( V) (cid:0)qj + η (qj)(cid:1) , with ( V) denoting the projection onto the simplex. What do projected gradients solve? We note that the projected ascent step above is also geometry that does not somewhat misguided as it implicitly equips the simplex with an L2 match the way probability distributions behave, especially near the boundary where many tokens should receive exactly zero (or near-zero) mass. This mismatch can lead to unstable updates, overly aggressive redistribution of mass, and fighting the constraints at every iteration. This is not philosophy! We can explicitly characterise this property by understanding that projected gradients are the solution to the following optimisation problem: qj+1 = arg max q( V) (cid:20) (qj), qj 1 2η (cid:12) (cid:12) (cid:12) (cid:12)q qj (cid:12) (cid:12) (cid:12) (cid:12) 2 2 (cid:21) . (8) Let us see why! We begin by expanding the terms in Equation (8): qj+1 = arg max q( V) = arg max q( V) = arg max q( V) (cid:20) (cid:20) (cid:20) (qj)T(q qj) (cid:16) 1 2η q2 2 2q, qj + qj2 2 (cid:17)(cid:21) (qj)Tq (qj)Tqj (qj)Tq (qj)Tqj (cid:16) (cid:16) 1 2η 1 2η q2 2 2q, qj + qj2 2 (cid:17)(cid:21) 2 2q, qj (cid:17) + 1 2η (cid:21) . qj2 2 Notice from the above that (qj)Tqj variable q. Removing those and multiplying by η, we get: and qj2 2 are independent from the optimisation (cid:20) (cid:20) qj+1 = arg max q( V) = arg max q( V) η (qj)Tq 1 2 q2 q, qj + η (qj) (cid:21) 2 + q, qj (cid:21) q2 2 . 1 2 4Please notice that we have ignored Ct for now. We can easily incorporate it again by defining sub-simplex depending on the ( V) Ct. 16 Decoding as Optimisation If we call = qj + η (qj), we get the following optimisation problem: qj+1 = arg min q( V) 2 q, (cid:21) . (cid:20) 1 2 Since is independent of q, we can add 1 the squares, giving us: 2 y2 to our minimisation problem to complete qj+1 = arg min q( V) (cid:20) 1 2 q2 2 q, + (cid:21) 1 2 y2 2 = arg min q( V) y2 2 (cid:105) . (cid:104) 1 2 From the last line, we see that the projected-gradient update is exactly Euclidean projection: qj+1 = ( V) (y), where ( V) (y) = arg min q( V) 1 2 y2 2. In other words, projected gradient ascent produces the next iterate by choosing the distribution on the simplex that is closest to the unconstrained step in squared L2 distance. This reveals an important (and often overlooked) fact: projected gradient ascent is not geometry neutral: it is the solution to proximal subproblem with an L2 regulariser, i.e., it implicitly assumes Euclidean geometry. While this is reasonable choice in Rn, it is poor match for probability distributions on the simplex, which form constrained manifold whose natural notion of distance is typically divergence-like (e.g., KL) rather than Euclidean. This mismatch is precisely what mirror descent corrects by replacing the squared L2 proximity term with Bregman divergence that respects the simplex geometry."
        },
        {
            "title": "4.2 Bregman Divergences & Mirror Ascent\nIf decoding lives on the simplex, then using L2\ngeometry is like doing navigation on the\nEarth with a flat map: it works locally, but it distorts what “small” moves mean. Mirror as-\ncent [2, 26, 31] replaces this distortion with the right geometry. Instead of measuring steps\nby squared distance, it measures them by a Bregman divergence Dψ(q, qj), induced by a\nconvex potential ψ [5]. The Bregman divergence is defined as:",
            "content": "Dψ(q, qj) = ψ(q) ψ(qj) ψ(qj), qj, for strictly convex and differentiable function ψ : ( V) R, which is called the distancegenerating (or potential) function. Given Dψ(q, qj), we rewrite the problem in (8) as Bregman regularised one: qj+1 = arg max q( V) (cid:20) (qj), qj Dψ(q, qj) (cid:21) . 1 η (9) 17 Decoding as Optimisation The L2 Special Case It is interesting to see that if we choose ψ(q) to be the Euclidean norm, i.e., ψ(q) = , we would recover Equation (8) as special case of the generalised problem 1 2 q2 2 in Equation (9). To show that, we begin by understanding the Bregman divergence: Dψ(q, qj) = 1 2 q2 2 1 2 qj2 2 qj, qj = 1 2 qj2 2. If we replace this last result into Equation (9), we exactly obtain Equation (8), which we have shown corresponds to the optimisation problem solved with projected gradient ascent. This goes to say that the problem in Equation (8) allows us to use more general notion of divergences defined through ψ, giving us the ability to consider manifolds (e.g., simplexes) that go beyond Euclidean spaces. Entropic Distance-Generating Functions over the Simplex. When optimising over simplex [18], the standard choice for the distance-generating function is the entropic poi=1 q(i) log q(i). In here, the resulting Bregman divergence becomes the tential: ψ(q) = Kullback-Leibler (KL) divergence. Therefore, the problem in Equation (9) becomes: qj+1 = arg max q( V) (cid:2)η (qj), KL(qqj)(cid:3) . (10) Now, let us solve the problem in Equation (10) to get the final form of our update. Noticing i=1 q(i) = 1 and ignoring the positivity constraint 5 on q(i), the that we have constraint Lagrangian is defined as: L(q, ν) = η (cid:19) i=1 (cid:18) (qj) q(i) q(i) i=1 q(i) log (cid:32) + ν 1 (cid:33) q(i) , i=1 q(i) (i) with ν being the Lagrange multipliers. Consider the partial derivative of L(q, ν) for single component q(i) and set it to zero: q(i) = η (cid:18) (q) q(i) = log (cid:19) (log q(i) + 1 log (i) ) ν = = η q(i) (i) (cid:18) (qj) q(i) (cid:18) (qj) q(i) (cid:19)(cid:19) (cid:18) η = exp = q(i) (i) (cid:19) (1 + ν) exp ((1 + ν)) = q(i) = Cq (i) exp (cid:18) η (cid:18) (qj) q(i) (cid:19)(cid:19) , 5The constraint has been ignored since the log function naturally enforces positivity. 18 Decoding as Optimisation with = exp ((1 + ν)). To find this constant, we use the constraint that the sum i=1 q(i) = 1: i=1 (i) exp (cid:19)(cid:19) (cid:18) η (cid:18) (qj) q(i) = 1 = = 1 (cid:16) (l) exp (cid:17)(cid:17) . (cid:16) (qj) q(l) η l=1 Hence, the overall update for the ith component of at round + 1 amounts to: (i) j+1 = (cid:16) η (i) exp (l) exp (cid:17)(cid:17) (cid:16) (qj) q(i) (cid:16) (qj) q(l) η (cid:16) l=1 (cid:17)(cid:17) . (11) To write in vectorised form that is amenable to implementation, we define the following as the vector of partial derivatives: gj = (cid:20) (qj) q(1) , . . . , (cid:21)T . (qj) q(n) (12) The numerator of Equation (11) is thus: Numerator = qj exp(ηgj), with being the Hadamard or element-wise product. The denominator is simply the 1 norm of the numerator: Denominator = qj exp(ηgj)1 . Therefore: MIRROR ASCENT UPDATE qj+1 = Numerator Denominator = qj exp(ηgj) qj exp(ηgj) . (13) In practice, computing exp(η q(i) ) directly can lead to numerical overflow if the gradients are large. To prevent this, we use the Log-Sum-Exp trick by subtracting the maximum value = max(ηgj) from the exponents. Because the sum normalises the update, this shift cancels out mathematically but ensures the computer always handles values 1."
        },
        {
            "title": "4.3 Use Case: Best-of-K Samplers\nWe have done a lot of groundwork: we reframed decoding as optimisation on the simplex,\nre-derived several classical decoders as closed-form special cases, and introduced mirror\nascent as a practical solver when closed forms are unavailable. The point of all this was not\njust unification for its own sake. It was to argue that an optimisation view gives us a princi-\npled design language for decoding: samplers are best understood as regularisers (and con-\nstraints), and new decoding behaviour should be obtained by writing down the objective\nwe actually want to optimise. To make good on this claim, we now present a concrete use\ncase. We introduce a new regulariser, Best-of-K (BoK), designed for settings where we draw\nmultiple samples and care about coverage of high-quality alternatives, not just the prop-\nerties of a single draw. BoK drops into our master problem as a plug-in term, and mirror\nascent gives an immediate, implementable algorithm.",
            "content": "19 Decoding as Optimisation The motivation is simple: many modern pipelines do not sample once; they sample times (self-consistency, rejection sampling, reranking, verifier-based selection, etc.). In that regime, the decoder is no longer judged by the quality of single draw, but by what the set of draws contains. Standard decoders were not designed for this: they often waste budget by repeatedly sampling the same high-probability continuation, while allocating too little mass to plausible alternatives that are individually unlikely but collectively valuable when we have multiple tries. What we want instead is decoding distribution () that makes good options show up at least once within samples, i.e., that explicitly trades off model score against multi-sample coverage. This is exactly the behaviour our framework is built to express: we define coverage utility for draws, convert it into regulariser, and plug it into the master objective to obtain the Best-of-K (BoK) sampler. The hit probability. Fix token and suppose we draw i.i.d. samples from decoding distribution q(). The probability that we never sample is (1 q(v))K. Therefore, the probability that appears at least once among the samples is: Pr[v appears at least once in samples] = 1 (1 q(v))K. (14) This quantity captures what single-sample decoding objectives ignore: in multi-sample regime, even moderately small probabilities can become valuable if they increase the chance of seeing useful alternative at least once. Summing Equation (14) over all tokens yields notion of total coverage. However, we do not want to reward covering junk tokens equally with high-quality ones, so we introduce nonnegative importance weights wt(v) 0 and define the weighted K-coverage: UK,t(q) := wt(v) (cid:16) 1 (1 q(v))K(cid:17) . (15) Intuitively, UK,t(q) is large when: i) We allocate mass to many tokens that we care about (large wt(v)), and ii) this mass is sufficient for them to be hit at least once within samples. In practice, wt(v) can be any nonnegative proxy for how much we would like to see token among draws (e.g., monotone function of the model score st(v), Top-M indicator, or softened rank-based weight). useful property of the hit probability 1 (1 q(v))K is that it exhibits diminishing returns: its marginal gain decreases as q(v) grows. Indeed: q(v) (cid:16) 1 (1 q(v))K(cid:17) = K(1 q(v))K1, which is strictly decreasing in q(v) for > 1. Thus, BoK has an inherent anti-collapse bias: it is more rewarding to allocate probability mass to under-covered but valuable tokens than to keep increasing the mass of tokens that are already likely to appear. This is exactly the multi-sample behaviour we want. BoK as regulariser. Maximising UK,t(q) alone would encourage spreading probability mass too widely, including onto implausible tokens. To keep the decoder anchored to the Decoding as Optimisation model, we combine coverage with KL trust region around the model distribution pt() and define the BoK regulariser Ω(BoK) (q) := KL(qpt) β UK,t(q), (16) where β 0 controls how strongly we reward coverage. Plugging Equation (16) into our master problem yields the BoK decoding objective: = arg max q( V) = arg max q( V) (cid:104) (cid:104) q, st λ Ω(BoK) (q) (cid:105) q, st λ KL(qpt) + β UK,t(q) (cid:105) , (17) with β := λβ for convenience. Mirror-Ascent Update for Ω(BoK). Unfortunately, exact closed-forms are hard to come by for Ω(BoK). Thus, we make use of our mirror ascent update rule in Equation 13. For single decoding step, BoK-regularised objective is defined as: (q) = q, st λ KL(qpt) + β UK,t(q), where ( V), pt ( V) is the reference/model distribution, and UK,t(q) = V wt(v(i)) (cid:16) 1 (1 q(i))K(cid:17) . Define the vector of partial derivatives at iterate qj as (18) (19) q(1) , . . . , For each coordinate {1, . . . , n}, the partial derivative admits the closed form (we denote (i) = wt(v(i))) . gj = (20) (cid:20) (qj) (cid:21)T (qj) q( V) = (i) λ log (qj) q(i) (i) (i) p + β + 1 (i) (1 (i) )K1. (21) Substituting Equation (21) into the mirror-ascent update from Equation (11) yields the explicit BoK update: (i) j+1 = (cid:32) (i) exp (cid:104) η (i) λ (cid:16) log (i) (i) q l=1 (l) exp (cid:32) (cid:104) η (l) λ (cid:16) log 21 (cid:17) + 1 + β (i) K(1 (i) )K1(cid:105) (cid:33) (cid:17) + 1 + β (l) K(1 (l) )K1 (22) (cid:33) . (cid:105) (l) (l) p Decoding as Optimisation Algorithm 1 BoK Decoder via Mirror Ascent (one decoding step) Require: scores st Rn, reference distribution pt n, weights wt Rn 0 Require: hyperparameters N, λ 0, β 0, stepsize η > 0, iterations 1: Initialise q0 pt 2: for = 0, 1, . . . , 1 do for = 1, 2, . . . , do 3: (cid:32) (cid:33) 4: (i) (i) λ log + 1 + β (i) K(1 (i) )K1 (i) (i) end for max(ηgj) qj+1 qj exp(ηgj M1) qj+1 qj+1/ qj+11 5: 6: 7: 8: 9: end for 10: return qJ Log-Sum-Exp stabilisation Of course, with gj as in Equation (20), the update can be written compactly as: qj+1 = qj exp(ηgj) qj exp(ηgj)1 . (23) Algorithm 1 summarises how to compute the BoK decoding distribution at single time step. We initialise the iterate with the model distribution q0 = pt (a natural warm-start), then perform mirror-ascent steps. Each step computes the vector of partial derivatives gj using the closed form in Equation (21), and applies the multiplicative update in Equation (23), and renormalises to stay on the simplex. In practice, we implement the update with Log-Sum-Exp stabilisation by subtracting = max(ηgj) before exponentiating."
        },
        {
            "title": "4.4 Use Case Evaluation: How good are BoK Samplers?\nWe evaluate the BoK Sampler as a practical, decoding-time regulariser for improving multi-\nsample generation without additional training or external verifiers. The evaluation is de-\nsigned to answer three questions: (i) whether BoK improves solution quality over standard\nsampling baselines; (ii) whether the gains are robust across decoding temperatures and\ndifferent hyperparameter choices in the underlying optimisation; and (iii) what compute\noverhead is incurred when solving the BoK objective with mirror ascent. Our main finding\nis that BoK consistently improves or matches standard decoding across tasks and models,\nwith the largest gains appearing in higher-temperature regimes where vanilla sampling is\nmore diverse but less reliable. BoK leverages the regularised objective to retain diversity\nwhile increasing the chance of sampling high-quality alternatives with only a small com-\nputational overhead.",
            "content": "Experimental setup. We evaluate BoK on math-specialised model Qwen2.5-Math-7B and general-purpose model Qwen2.5-7B, across three complementary benchmarks spanning math, QA, and code: MATH500 [21], GPQA-diamond [29], and HumanEval [7]. We compare BoK against standard autoregressive sampling (Base) from the model distribution 22 Decoding as Optimisation Method τ=0.10 τ=0.25 τ=0.50 τ=0.70 τ=0.90 MATH500 (Qwen2.5-Math-7B) Base Top-K BoK (Ours) β=0.01, λ=0.1 BoK (Ours) β=0.02, λ=0.2 BoK (Ours) β=0.05, λ=0.5 72.2 72.8 74.2 72.6 72.8 GPQA (Qwen2.5-Math-7B) Base Top-K BoK (Ours) β=0.01, λ=0.1 BoK (Ours) β=0.02, λ=0.2 BoK (Ours) β=0.05, λ=0.5 32. 33.84 30.81 31.31 33.84 HumanEval (Qwen2.5-Math-7B) Base Top-K BoK (Ours) β=0.01, λ=0.1 BoK (Ours) β=0.02, λ=0.2 BoK (Ours) β=0.05, λ=0.5 56.71 54.88 54.27 54.88 56.01 72. 73.4 72.8 71.8 71.6 33.84 35.35 33.84 36.36 32.83 53.05 51.83 54.88 54.88 51.83 69.4 69.4 71.0 72. 72.8 26.26 28.79 31.82 33.33 35.35 48.78 51.83 50.0 51.83 51. 64.4 65.0 73.0 72.4 72.8 31.31 31.31 33.33 34.85 31.82 49.39 46.34 54.27 53.05 52. 53.0 56.2 71.2 71.6 70.8 30.30 31.82 36.36 31.82 31.82 32.93 37.80 52.44 52.44 51. Table 1. Accuracy across temperatures for Qwen2.5-Math-7B on MATH500, GPQA, and HumanEval. We report Base, Top-K (K=50), and BoK with three representative (β, λ) settings, illustrating robustness across both temperature and hyperparameters. at temperature τ, and Top-K, sampling restricted to the top-K tokens per step with renormalisation (we fix K=50 across tasks and temperatures). All methods use the same prompts (Qwen default prompts [45]) and evaluation scripts, the same maximum generation length Tmax=3072 with early stopping on EOS, and the same random-seed protocol. Results and Analysis. We implement the BoK sampler following Algorithm 1, using mirror-ascent updates to compute per-step sampling distribution () that increases exploration via the coverage utility while remaining anchored to the base model through KL regularisation (Eq. (17)). In particular, β controls the strength of the coverage reward, and λ controls the strength of KL anchoring toward the reference distribution pt. Across tasks and models, BoK matches or improves upon Base and Top-K sampling in most settings, as shown in Tables 1 and 2. The gains are most pronounced at higher temperatures, where vanilla sampling becomes more diverse but less reliable. For Qwen2.5-Math-7B on MATH500 at τ=0.9, BoK increases accuracy from 53.0% to 71.6% (+18.6%) and exceeds Top-K at 56.2% by +15.4%. We observe similar pattern on GPQA and HumanEval, where BoK improves over the baselines at τ=0.9 by +6.06% and +14.64%. Similar improvements hold for Qwen2.5-7B, where BoK substan23 Decoding as Optimisation Method τ=0.10 τ=0.25 τ=0.50 τ=0.70 τ=0.90 MATH500 (Qwen2.5-7B) Base Top-K BoK (Ours) β=0.01, λ=0.1 BoK (Ours) β=0.02, λ=0.2 BoK (Ours) β=0.05, λ=0.5 GPQA (Qwen2.5-7B) Base Top-K BoK (Ours) β=0.01, λ=0.1 BoK (Ours) β=0.02, λ=0.2 BoK (Ours) β=0.05, λ=0.5 HumanEval (Qwen2.5-7B) Base Top-K BoK (Ours) β=0.01, λ=0.1 BoK (Ours) β=0.02, λ=0.2 BoK (Ours) β=0.05, λ=0.5 57.6 59.1 57.6 58.8 59.1 32.83 28.28 31.82 31.82 30.30 70.13 71. 72.56 71.59 72.56 60.4 59.4 59.6 59.2 61.0 27.78 29.80 29.80 29.29 30. 67.68 70.73 71.95 73.17 72.56 56.6 52.8 60.6 61.8 59.4 21.72 27.78 29.80 30.30 29. 71.34 71.95 71.95 74.78 69.51 50.2 51.6 60.0 60.4 59.4 25.76 29.80 29.29 27.78 29. 71.95 65.24 72.56 71.51 69.51 44.2 41.0 58.0 60.2 59.4 24.24 32.32 32.32 29.29 24. 45.12 57.93 71.34 69.51 66.46 Table 2. Accuracy across temperatures for Qwen2.5-7B on MATH500, GPQA, and HumanEval (same baselines and BoK settings as Table 1). tially mitigates the accuracy drop at high temperatures. At lower temperatures, improvements are naturally smaller; in few near-deterministic regimes, Base or Top-K sampling can remain slightly stronger or comparable to BoK, which is consistent with the reduced need for exploration when the model distribution is already sharply peaked. The results also indicate that BoK is not overly sensitive to hyperparameter choice: multiple (β, λ) settings yield competitive performance across temperatures, with the best pair varying by task and regime. Overall, the consistent improvements across all three tested configurations suggest stable operating region. BoK thus provides practical and robust way to trade off diversity and reliability at decoding time without exhaustive hyperparameter search, with particularly strong benefits in high-entropy sampling settings. In practice, BoK introduces only small runtime Efficiency and practical deployment. requires only few mirroroverhead because computing the BoK-optimal distribution ascent updates per token. All BoK results in Tables 1 and 2 use 5 mirror-ascent steps per token. Across the three benchmarks, the additional cost remains modest relative to base decoding: on MATH500, BoK runs in 16.88s vs. 15.84s; on GPQA, 17.60s vs. 15.43s; and on HumanEval, BoK is slightly faster in our implementation (8.65s vs. 9.74s), which we attribute to shorter generations under BoK. This suggests that the mirror-ascent solver converges . quickly, so only small number of optimisation steps are required to obtain an effective Decoding as Optimisation Gradient steps MATH500 acc. (%) Runtime (s) Base 64.4 15.84 5 10 15 20 69.6 15.87 73.0 16. 71.6 17.70 71.2 17.91 72.8 18.26 Table 3. Effect of the number of mirror-gradient steps per token on MATH500 accuracy for Qwen2.5-MATH-7B (BoK) using τ = 0.7, β = 0.01 and λ = 0.1. To make this explicit, we further study the effect of the number of mirror-gradient steps per token on the MATH500 task, as shown in Table 3. There is an improvement compared with the base decoding, even with small number of steps: using only 2 steps improves accuracy from 64.4% to 69.6% with negligible runtime increase (15.87s), and 5 steps reach 73.0% with still modest overhead (16.88s). Increasing the number of steps beyond 5 yields only marginal changes in accuracy while steadily increasing runtime. Overall, these results indicate that BoK remains effective with small, fixed number of mirror-ascent steps, making it suitable for practical decoding-time use."
        },
        {
            "title": "5 Conclusion and Future Work",
            "content": "We argued that decoding should not be viewed as collection of disconnected heuristics. Instead, many widely used decoding rules arise as exact solutions to single optimisation template on the probability simplex, where different decoding behaviours correspond to different regularisers and feasible sets. This perspective unifies classical procedures (e.g., greedy, Softmax, Top-K/Top-P, and sparse decoders) under one Master problem, and makes explicit the role of optimality conditions in determining which tokens become active or inactive. When closed-form solutions are unavailable, we showed that mirror ascent provides principled, simplex-native solver whose updates preserve valid distributions by construction. Finally, to demonstrate that the framework is not merely explanatory, we introduced concrete use case, the Best-of-K (BoK) regulariser, which directly targets multisample coverage and can be implemented with the same mirror-ascent machinery. The optimisation view suggests several promising directions. First, while we focused on per-step decoding, it would be natural to extend the framework to sequence-level objectives that couple decisions across time [16, 17], e.g., enforcing coverage, length, or style constraints globally rather than locally. Second, BoK illustrates how multi-sample utilities can be encoded as regularisers; broader family of compute-aware objectives could be explored, including utilities that model downstream reranking, verifier selection, or selfconsistency more explicitly [1, 8, 42, 49]. Third, mirror ascent opens the door to richer constraint sets beyond the simplex, such as structured sparsity, group constraints, or dynamic support sets that depend on external tools or retrieval modules [6, 20, 30, 33]. We hope this work helps shift decoder design from folklore to first-principles objective design. In short: Decoding is not hack; it is optimisation! 25 Decoding as Optimisation"
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal et al. Lets Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs. 2023. arXiv: 2305.11860 [cs.CL]. [2] Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimization. In: Operations Research Letters 31.3 (2003), pp. 167175. [3] Dimitri P. Bertsekas. Nonlinear Programming. 2nd. Belmont, MA: Athena Scientific, 1999. [4] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004. [5] Lev Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. In: USSR computational mathematics and mathematical physics 7.3 (1967), pp. 200217. [6] Andrew Brown, Muhammad Roman, and Barry Devereux. Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges. 2025. arXiv: 2508.06401 [cs.DL]. [7] Mark Chen et al. Evaluating Large Language Models Trained on Code. In: arXiv preprint arXiv:2107.03374 (2021). [8] Xinyun Chen et al. Universal Self-Consistency for Large Language Model Generation. 2023. arXiv: 2311.17311 [cs.CL]. [9] Yung-Sung Chuang et al. DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. 2024. arXiv: 2309.03883 [cs.CL]. [10] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical Neural Story Generation. 2018. arXiv: 1805.04833 [cs.CL]. [11] Giorgio Franceschelli and Mirco Musolesi. Creative Beam Search: LLM-as-a-Judge For Improving Response Generation. 2024. arXiv: 2405.00099 [cs.AI]. [12] Raja Gond et al. LLM-42: Enabling Determinism in LLM Inference with Verified Speculation. 2026. arXiv: 2601.17768 [cs.LG]. [13] Chuan Guo et al. On Calibration of Modern Neural Networks. 2017. arXiv: 1706 . 04599 [cs.LG]. [14] Ari Holtzman et al. The Curious Case of Neural Text Degeneration. 2020. arXiv: 1904. 09751 [cs.CL]. [15] Xiaotong Ji et al. On Almost Surely Safe Alignment of Large Language Models at Inference-Time. 2025. arXiv: 2502.01208 [cs.LG]. [16] Xiaotong Ji et al. Scalable Power Sampling: Unlocking Efficient, Training-Free Reasoning for LLMs via Distribution Sharpening. 2026. arXiv: 2601.21590 [cs.LG]. [17] Aayush Karan and Yilun Du. Reasoning with Sampling: Your Base Model is Smarter Than You Think. 2025. arXiv: 2510.14901 [cs.LG]. Decoding as Optimisation [18] Jyrki Kivinen and Manfred Warmuth. Exponentiated gradient versus gradient descent for linear predictors. In: Information and computation 132.1 (1997), pp. 163. [19] Ehsan Latif, Ramviyas Parasuraman, and Xiaoming Zhai. PhysicsAssistant: An LLMPowered Interactive Learning Robot for Physics Lab Investigations. In: 2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN). 2024, pp. 864871. DOI: 10.1109/RO-MAN60168.2024.10731312. [20] Patrick Lewis et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. 2021. arXiv: 2005.11401 [cs.CL]. [21] Hunter Lightman et al. Lets Verify Step by Step. In: arXiv preprint arXiv:2305.20050 (2023). [22] Fang Liu et al. Beyond Functional Correctness: Exploring Hallucinations in LLMGenerated Code. 2026. arXiv: 2404.00971 [cs.SE]. [23] Andre Martins and Ramon Astudillo. From softmax to sparsemax: sparse model of attention and multi-label classification. In: International conference on machine learning. PMLR. 2016, pp. 16141623. [24] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the question? In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020, pp. 21732185. [25] Ludovico Mitchener et al. BixBench: Comprehensive Benchmark for LLM-based Agents in Computational Biology. 2025. arXiv: 2503.00096 [q-bio.QM]. [26] Arkadij Semenovič Nemirovskij and David Borisovich Yudin. Problem complexity and method efficiency in optimization. In: (1983). [27] Minh Nhat Nguyen et al. Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs. 2025. arXiv: 2407.01082 [cs.CL]. [28] Georgy Noarov et al. Foundations of Top-k Decoding For Language Models. 2025. arXiv: 2505.19371 [cs.AI]. [29] David Rein et al. Gpqa: graduate-level google-proof q&a benchmark. In: First Conference on Language Modeling. 2024. [30] Timo Schick et al. Toolformer: Language Models Can Teach Themselves to Use Tools. 2023. arXiv: 2302.04761 [cs.CL]. [31] Shai Shalev-Shwartz. Online Learning and Online Convex Optimization. In: Foundations and Trends in Machine Learning 4.2 (2012), pp. 107194. [32] Chufan Shi et al. Thorough Examination of Decoding Methods in the Era of LLMs. 2024. arXiv: 2402.06925 [cs.CL]. [33] Zhengliang Shi et al. Tool Learning in the Wild: Empowering Language Models as Automatic Tool Agents. 2025. arXiv: 2405.16533 [cs.CL]. [34] Tarun Suresh et al. BEAVER: An Efficient Deterministic LLM Verifier. 2025. arXiv: [35] 2512.05439 [cs.AI]. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to Sequence Learning with Neural Networks. In: Advances in Neural Information Processing Systems. Ed. by Z. Ghahramani et al. Vol. 27. Curran Associates, Inc., 2014. 27 Decoding as Optimisation [36] Rasul Tutunov et al. Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing. 2025. arXiv: 2512.04829 [cs.AI]. [37] Rasul Tutunov et al. Why Can Large Language Models Generate Correct Chain-ofThoughts? 2024. arXiv: 2310.13571 [cs.CL]. [38] Ashwin Vijayakumar et al. Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models. 2018. arXiv: 1610.02424 [cs.AI]. [39] Luke Vilnis et al. Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models. 2023. arXiv: 2210.15458 [cs.CL]. [40] Chenxi Wang et al. MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation. 2025. arXiv: 2410.11779 [cs.CL]. [41] Tianchun Wang et al. On the Effect of Sampling Diversity in Scaling LLM Inference. 2025. arXiv: 2502.11027 [cs.LG]. [42] Xuezhi Wang et al. Self-Consistency Improves Chain of Thought Reasoning in Lan- [43] guage Models. 2023. arXiv: 2203.11171 [cs.CL]. Jiaheng Wei et al. Measuring and Reducing LLM Hallucination without GoldStandard Answers. 2024. arXiv: 2402.10412 [cs.CL]. [44] David P. Woodruff et al. Accelerating Scientific Research with Gemini: Case Studies and Common Techniques. 2026. arXiv: 2602.03837 [cs.CL]. [45] An Yang et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. In: arXiv preprint arXiv:2409.12122 (2024). [46] Jiayi Zhang et al. Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity. 2025. arXiv: 2510.01171 [cs.CL]. [47] Yuxuan Zhou, Margret Keuper, and Mario Fritz. Balancing Diversity and Risk in LLM Sampling: How to Select Your Method and Parameter for Open-Ended Text Generation. 2025. arXiv: 2408.13586 [cs.CL]. [48] Matthieu Zimmer et al. Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving. 2025. arXiv: 2507.02726 [cs.AI]. [49] Matthieu Zimmer et al. Rethinking Large Language Model Distillation: Constrained Markov Decision Process Perspective. 2025. arXiv: 2509.22921 [cs.LG]."
        }
    ],
    "affiliations": [
        "AI Centre, Department of Computer Science, UCL",
        "Huawei Noahs Ark"
    ]
}