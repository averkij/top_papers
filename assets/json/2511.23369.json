{
    "paper_title": "SimScale: Learning to Drive via Real-World Simulation at Scale",
    "authors": [
        "Haochen Tian",
        "Tianyu Li",
        "Haochen Liu",
        "Jiazhi Yang",
        "Yihang Qiu",
        "Guang Li",
        "Junli Wang",
        "Yinfeng Gao",
        "Zhang Zhang",
        "Liang Wang",
        "Hangjun Ye",
        "Tieniu Tan",
        "Long Chen",
        "Hongyang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving fully autonomous driving systems requires learning rational decisions in a wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce a novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with a reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we develop a pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that a simple co-training strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such a sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released."
        },
        {
            "title": "Start",
            "content": "SIMSCALE: Learning to Drive via Real-World Simulation at Scale Haochen Tian1,2,3 Tianyu Li2 Haochen Liu3 Jiazhi Yang2 Yihang Qiu2,3 Guang Li3 Junli Wang1,3 Yinfeng Gao1, Tieniu Tan1 Zhang Zhang1 Long Chen3 Hongyang Li2 Liang Wang1 Hangjun Ye3 5 2 0 2 8 2 ] . [ 1 9 6 3 3 2 . 1 1 5 2 : r a"
        },
        {
            "title": "1 MAIS, Institute of Automation, Chinese Academy of Sciences\n2 OpenDriveLab at The University of Hong Kong 3 Xiaomi EV\nhttps://opendrivelab.com/SimScale",
            "content": "Figure 1. Scaling up end-to-end planners by simulation. (a) We construct large-scale simulation data by perturbing ego trajectories, generating corresponding pseudo-expert demonstrations, and rendering multi-view observations in reactive environments. Combined with real-world data, this enables broad coverage of out-of-distribution states and supports simreal co-training for any end-to-end planner. (b) Across three representative planner families, including regression, diffusion, and vocabulary scoring, sim-real co-training consistently produces synergistic improvements in robustness and generalization, demonstrating clear and predictable simulation scaling trends."
        },
        {
            "title": "Abstract",
            "content": "Achieving fully autonomous driving systems requires learning rational decisions in wide span of scenarios, including safety-critical and out-of-distribution ones. However, such cases are underrepresented in real-world corpus collected by human experts. To complement for the lack of data diversity, we introduce novel and scalable simulation framework capable of synthesizing massive unseen states upon existing driving logs. Our pipeline utilizes advanced neural rendering with reactive environment to generate high-fidelity multi-view observations controlled by the perturbed ego trajectory. Furthermore, we Work done while interning at Xiaomi Embodied Intelligence Team. Primary contact to Haochen Tian tianhaochen2023@ia.ac.cn 1 develop pseudo-expert trajectory generation mechanism for these newly simulated states to provide action supervision. Upon the synthesized data, we find that simple cotraining strategy on both real-world and simulated samples can lead to significant improvements in both robustness and generalization for various planning methods on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 on navtest. More importantly, such policy improvement scales smoothly by increasing simulation data only, even without extra real-world data streaming in. We further reveal several crucial findings of such sim-real learning system, which we term SimScale, including the design of pseudo-experts and the scaling properties for different policy architectures. Our simulation data and code would be released. 1. Introduction Data scaling is recognized as foundational principle in modern deep learning across various domains, including language, vision, and multimodal modeling, underpinning steady performance improvements as data sizes increase [6, 37, 67, 85]. In autonomous driving, end-to-end (E2E) planning learns to map raw observations to actions, offering promising way to leverage large-scale driving data to enable the emergence of fully autonomous systems [29, 64, 88]. Nevertheless, real-world driving data from human expert demonstrations are dominated by common scenarios, while non-trivial cases, e.g., safety-critical, are underrepresented [10, 58, 77, 81]. Moreover, planners trained on such data are confined to human driving distribution and struggle to generalize to rare or unseen situations, leading to distribution shift and causal confusion in deployment [22, 52]. Consequently, scaling real-world data only is inefficient for achieving deploy-ready autonomous driving. Simulation via neural rendering [39] can generate highfidelity driving scenarios and thus has the potential to produce out-of-distribution (OOD) states deviated from human demonstrations at scale, which is essential for closedloop planning [23, 66]. Therefore, scaling simulation data presents an attractive alternative to solely relying on realworld data. However, planners require feasible corresponding demonstrations to learn how to handle OOD states, while current simulation methods fail to generate such demonstrations effectively. Moreover, the impact of scaling simulation data lacks in-depth analysis. In this work, we aim to derive systematic recipe for scaling simulation data from limited real-world scenarios in end-to-end planning. To conduct comprehensive experiments and analyses, this study is structured along three key questions: (1) what constitutes effective simulation data, (2) how well planners benefit from it, and (3) whether this system scales predictably upon fixed real-world corpus. To this end, we formulate scalable simulation data generation framework that extends the expert distribution from existing real-world training data to bootstrap end-toend autonomous driving systems. We develop 3D Gaussian Splatting (3DGS) [39] simulation data engine, which allows controlling the temporal ego and other agent states and rendering multi-view videos from the egos perspective. Concretely, we first sample diverse set of plausible perturbations to the ego trajectory, maximizing coverage of the state space, e.g., off-center lane drifts, close interactions, among others. Then, we take the final state of each perturbed trajectory as the perturbed state and generate corresponding demonstrations using pseudo-experts of two forms in comparison. The first, recovery-based expert retrieves trajectories that steer the policy within the human trajectory manifold, resulting in human-like yet cautious behaviors. The second, privileged planner-based expert [18] generates the trajectory that maximizes optimality, representing an exploratory strategy with lower realism. To enhance scalability and plausibility, the entire pipeline is executed in reactive environment [70], where surrounding agents interact with the ego vehicle responsively. To thoroughly assess the effect of simulation data, we consider three types of end-to-end planners with various model scales,i.e., LTF [14] for regression methods, DiffusionDrive [56] for diffusion-based planners, and GTRSDense [55] for vocabulary scoring. We employ simple yet effective sim-real co-training strategy [63, 65] to maintain human driving distribution while mitigating visual domain degradation. Furthermore, by keeping constant amount of real data and progressively adding simulation data via non-overlapping samples, we investigate how diverse planners can benefit from simulation data and the general scaling property. We utilize two real-world closed-loop benchmarks to evaluate the planners from multiple perspectives. navhard [7] focuses on unseen, challenging scenarios to assess the impact of OOD states on planners, while navtest [19] offers broad set of diverse scenarios to test planners ability to handle varying situations. The complete sim-real learning system, which comprises scalable simulation data construction pipeline and an effective simreal co-training strategy as shown in Fig. 1, is termed SimScale. Rigorous experiments uncover crucial findings enabled by SimScaleincluding, but not limited to, the following: Scalable simulation with pseudo-expert unlocks the inherent potential of available real-world driving data. Sim-real co-training improves both robustness and generalization synergistically in diverse end-to-end planners. Exploratory expert and interaction environments improve the effectiveness of simulation data. Planners with multimodal modeling capabilities exhibit more encouraging data scaling properties. 2. Methodology We outline SimScaleas follows. In Sec. 2.2, we briefly introduce our 3GDS simulation data engine supporting controllable multi-view video rendering. In Sec. 2.3, we propose pseudo-expert scene simulation pipeline to generate diverse simulation data containing OOD states with feasible demonstrations. In Sec. 2.4, we demonstrate scalable simreal co-training method with different end-to-end planners. 2.1. Preliminary End-to-end planning models take observations within history window as input and output predicted future trajectory. Each training scenario starts at selected timestep and includes history horizon of length and planning horizon of length H. The model processes the past frames to Figure 2. Pseudo-expert scene simulation pipeline. (a) Trajectory perturbation on to + H, (b) reactive environment rollout, and pseudo-expert trajectory generation from + to + 2H under recovery-based and planner-based strategies. predict the future frames, resulting in complete training sample spanning + timesteps. 2.2. 3DGS Simulation Data Engine To reduce the domain gap between real-world data and generated observations from novel views in simulated scenarios, photorealistic data engine is required. Built upon 3DGS [39] assets reconstructed from real-world datasets, our data engine Φ(Kt, Et, {xi,t, yi,t, θi,t}N i=1) takes as input the camera intrinsics Kt and extrinsics Et at timestep t, along with the positions and yaw angles (x, y, θ) of nonego vehicle at the same timestep, and renders the corresponding RGB observations from the camera. The camera extrinsics Et can easily be obtained from the ego vehicles position and heading (x0,t, y0,t, θ0,t) with the ego-tocamera transformation, while the other camera parameters are directly adopted from the raw dataset. Preprocessing. Following prior work [47], we apply exposure alignment across images captured simultaneously by multi-view cameras at time t, using projected LiDAR points at as guidance. Moreover, with help of 3D bounding boxes provided by NAVSIM annotations, colored LiDAR points are divided into several groups, i.e., the static background and multiple vehicles, and further used as initialization for Gaussians to improve the reconstruction performance. Block-wise Reconstruction. During scene reconstruction, increasing the number of images significantly raises computational cost and runtime. Thus, we perform reconstruction in block-wise manner, where each block corresponds to spatio-temporal range. Following previous work [47, 79], backgrounds and foregrounds are reconstructed as separate models using the per-timestep locations and orientations of 3D bounding boxes, resulting in static background asset and multiple movable vehicle assets. These assets could then be placed at desired locations and headings specified by inputs, rendering sensor data at novel views. Blocks with low average PSNR in novel view synthesis are excluded to ensure the quality of generated simulation data. 2.3. Pseudo-Expert Scene Simulation Based on the data engine, we design pseudo-expert scene simulation pipeline to generate diverse simulation data from existing real-world data, as shown in Alg. 1. The pipeline aims to produce feasible demonstrations from perturbed states paired with expert trajectories, as illustrated in Fig. 2. Reactive Scene Reconstruction. For each training clip d, we perform two simulations of duration H: one for exploring perturbed states at = , and the other for generating expert trajectories at = + H. In each simulation, ego trajectory at:t+H is simulated by LQR [41], while other agents are modeled using the IDM [70] to interact with the ego, producing the corresponding future states st:t+H . For the valid expert trajectories, the simulated egoagent trajectories cross two stage (aH:T +2H , sH:T +2H ) are then rendered into multi-view videos oH:T +2H using the data engine Φ. This decoupling of behavior simulation from sensor rendering enables reactive environments where other agents respond plausibly to the egos behaviors, thereby enhancing both the realism and diversity of the simulation data. Trajectory Perturbation. At the first simulation step (t = ), we perturb the ego trajectory at:t+H so that the ego reaches new terminal state at = + H, which then serves as the start state for the next rollout (Fig. 2 (a)). Our objective is to sample diverse yet plausible states. For diversity, perturbations are drawn from clustered humantrajectory vocabulary that densely covers the action space. For plausibility, we restrict perturbations to remain near human behavior by thresholding longitudinal/lateral shifts (rlon, rlat) and heading change θ, and by removing trajectories that are physically invalid (collisions, off-road, unstable renderings). We further apply spatially sparse sampling on trajectory endpoints using an interleaved grid with steps (δlon, δlat) to promote uniform coverage. Since reactive simulation is costly, we first filter infeasible trajectories 3 Algorithm 1 Pseudo-Expert Scene Simulation Require: Training clip = (ot, st, at)T +2H t=0 D, perturb action sets Aper, sensor engine Φ, pseudo policy πexp, reward functions , reactive simulation . 1: Initialize simulation scene dataset Dsim 2: for each perturbed action aper Aper do for timestep {T, + H} do 3: 4: 5: 6: 7: 1.Pseudo Action Generation: if = then at:t+H aper; else at:t+H πexp(ot, st). end if 2.Reactive State Simulation: st:t+H (at:t+H , st). 3.Sensor Simulation: ot:t+H Φ(st:t+H ). 4.Reward Filtering / Labeling: rt:t+H (at:t+H , st:t+H ). if Filtered(rt:t+H , ot:t+H ) then 8: 9: 10: 11: 12:"
        },
        {
            "title": "Break",
            "content": "13: 14: 15: 16: 17: 18: 19: end for 20: return Dsim end if end for Append (ot, st, at, rt)T +2H t=0 to Dsim non-reactively and run reactive checks only on the sparsed set. This produces set of dynamically and physically feasible perturbations aper Aper. Pseudo-Expert Trajectory Generation. At the second simulation = + H, for each perturbed state, we employ non-human expert, referring as the pseudo-expert πexp, to generate feasible corresponding trajectory at:t+H . Since the pseudo-expert is not perfect and at:t+H will be used as supervision, we apply stricter filtering in the second stage simulation. In addition to physical constraints, traffic rules and vehicle kinematic limits are also enforced, following Eq. 6. To investigate which strategy best serves as supervision for end-to-end planners, we compared two strategies: conservative recovery-based expert, and an exploratory planner-based expert, as detailed below. (1) Recovery-based Expert. Recovery steers the policy within the human manifold after perturbation. To ensure robustness, our recovery expert πexp retrieves human trajectory from large vocabulary Vh that best matches the ego vehicles logged state at time = + 2H (Fig. 2 (b1)). For each candidate trajectory of horizon H, we summarize its initial and final poses with compact matching vector: = [vx , vy , θt, xt+H1, yt+H1, θt+H1]. (1) Given the egos perturbed state with target vector mr, πexp 4 retrieves the closest human maneuver by: at:t+H = arg min aVh m(a) mr1. (2) This yields human-like yet conservative fallback behavior, stabilizing under distributional drifts. (2) Planner-based Expert. Following prior works [4, 87], we use privileged planner that leverages ground-truth states to generate reactive and optimized trajectory rollouts in simulation, as shown in Fig. 2 (b2). The planner-based expert πexp is defined as: at:t+H = P(st:t+H ). Compared with the recovery policy, planner-based expert relies on rules or cost heuristics, trading human-likeness and realism of its behavior occasionally. Still, it offers strong optimization and diverse exploratory rollouts, enriching expert supervision beyond human data. 2.4. Scalable Sim-Real Co-training Strategies for Co-training. Sim-real co-training serves as simple yet effective strategy [5, 63, 65, 81] that enables the integration of both real and simulated data for planning. In our approach, we randomly sample from mixture of real-world data and simulation data Dsim during training, aiming to preserve the human driving distribution while mitigating visual domain degradation caused by potential simulation artifacts, such as subtle rendering inconsistencies, temporal jitter, or unrealistic lighting and shadows [59, 60]. Our fully automated, scalable simulation data generation framework enables scaling the total training data by progressively adding non-overlapping simulation samples, while keeping the real data amount fixed. Planners for Co-training. We aim to comprehensively evaluate the effectiveness of simulation data for end-toend planners. Modern end-to-end planning approaches can be broadly categorized into three representative paradigms: regression-based planners [14, 27], diffusion-based planners [56, 76], and vocabulary scoring-based planners [11, 52]. Accordingly, we consider representative models from each paradigm in our co-training experiments. (1) Co-training with Pseudo-Expert Trajectory. Regression and diffusion-based planners rely on expert demonstrations. The co-training process can thus be formulated as: arg min θ E(a,o)(D Dsim) (cid:104) Lim (cid:0)a, πθ(ˆao)(cid:1)(cid:105) , (3) here, Lim denotes the imitation loss; and Dsim represent the real-world and generated simulation datasets, respectively; and refers to expert trajectories, i.e., human-expert trajectories in and pseudo-expert trajectories in Dsim. As for the vocabulary scoring-based planner, the learning objective has additional prediction of reward signals that distill the evaluation metrics, e.g. EPDMS as Eq. 6: arg min θ E(a,o,r)(DDsim) (cid:104) λLim + Lr (cid:0)r, πθ(ˆao)(cid:1)(cid:105) , (4) Closed [18] as the expert. Only pseudo-expert trajectories with decent sub-metrics of EPDMS are considered valid. Due to computational limits, for each type of pseudo-expert, we perform up to five random samplings of valid trajectories with no overlaps from navtrain split for rendering. Through multiple samplings, expert trajectory demonstrations are progressively accumulated. As shown in Fig. 3, the two pseudo-expert methods exhibit different success rates, with the planner-based expert achieving higher efficiency. In total, we generate 140K recovery-based simulation scenes and 185K planner-based simulation scenes. For more details, please refer to the supplementary. Benchmark and Metrics. We utilize two benchmarks in NAVSIMv2 [7] for end-to-end model evaluation, including navhard and navtest. navhard is the official twostage evaluation benchmark, which contains 244 challenging real-world scenarios in the first stage and corresponding 4,164 synthetic scenarios generated by 3DGS in the second stage. navtest is one-stage evaluation benchmark, containing large number of 12,146 real-world scenarios. navhard focuses on assessing the models closed-loop performance in safety-critical situations, while navtest emphasizes generalization across diverse driving conditions. The two benchmarks share rule-based planning metric, EPDMS [44], with several sub-metrics: EPDMS = (cid:89) Sm (cid:32) (cid:80) mMpen (cid:123)(cid:122) penalties (cid:124) (cid:124) (cid:125) wmSm wm mMavg (cid:80) mMavg (cid:123)(cid:122) weighted average , (6) (cid:33) (cid:125) where Sm is the sub-metric: penalty terms set Mpen includes No-at-fault Collisions (NC), Drivable Area Compliance (DAC), Driving Direction Compliance (DDC), and Traffic Light Compliance (TLC); weighted average terms set Mavg includes Time-to-Collision (TTC), Ego Progress (EP), Lane Keeping (LK), History Comfort (HC), and extended comfort (EC). Note that EPDMS in navhard further incorporates several modifications, e.g., two-stage aggregation, reactive traffic simulation, and the exclusion of penalties in cases where the human expert driver also fails. Models and Training. To thoroughly validate our modelagnostic simulation data, we select one representative opensource model for each paradigm discussed in Sec. 2.4: the regression-based LTF [14], the diffusion-based DiffusionDrive [56], and the scoring-based GTRS-Dense [55]. We adopt their official implementations with two modifications: the input image resolution is unified to 2048 512, and the LiDAR inputs are removed to align with the navhard evaluation setting. All models are trained from scratch on NVIDIA H20-3e GPUs. To ensure fair comparison, each model follows the same training strategy, whether using only the navtrain split or augmented with simulation data. Figure 3. Simulation data statistics across multiple sampling rounds. (a) Recovery-based expert impose stronger constraints, leading to slower data accumulation than (b) Planner-based expert. here, Lr denotes reward loss, λ is weighting factor. (2) Co-training with Rewards Only. For the vocabulary scoring-based planner, when the reward signal is wellaligned, the expert is theoretically unnecessary, as shown in Eq. 4. The planner can explore directions to increase the reward without being restricted to single expert trajectory. The co-training process can be formulated as: arg min θ E(a,o,r)D[λLim + Lr] + E(o,r)Dsim[Lr]. (5) We thus investigate purely reward-driven optimization for the scoring-based planner in simulation, to assess its potential for fully leveraging simulation data, and assess its potential for maximizing the utility of simulated data. 3. Experiments 3.1. Setup and Protocols. Real-world Datasets. We use the navtrain split from NAVSIM [19], built upon the largest publicly available annotated driving dataset nuPlan [38], and comprises 100K interactive real-world scenarios. Simulation Data Curation. We construct simulated scenarios on navtrain split. 3DGS models of blocks with PSNR < 27 in novel-view results are removed ensure reconstruction quality. For ego trajectory perturbation, we choose the plausible candidates from the vocabulary of 16,384 trajectories from [55], with EPDMS 0.8 and relative heading θ 20, where EPDMS is detailed in Eq. 6. During spatial sampling by trajectory endpoint, the longitudinal and lateral perturbation ranges are set as rlon = 20m, rlat = 2.0m, with step sizes δlon = 5m, δlat = 0.5m. For recovery-based trajectory generation, we use all human expert trajectories in navtrain split as the retrieval vocabulary. For planner-based trajectory generation, we select the SOTA rule-based planner in nuPlan [38], PDMTable 1. Performance on the NAVSIM-v2 navhard Leaderboard. PDM-Closed uses ground-truth symbolic inputs for planning, while other methods rely on sensor data. (: pseudo-expert supervision; : reward scoring; S.: per-stage EPDM score.) Method Backbone Sim. Stage NC DAC DDC TLC EP TTC LK HC PDM-Closed [18] - - LTF [14] ResNet34 DiffusionDrive [56] ResNet34 GTRS-Dense [55] ResNet34 V2w/o w/ w/o w/ w/o w/ w/o w/ 1 2 1 2 1 2 1 1 2 1 2 1 2 1 2 1 2 94.4 88. 97.3 79.4 98.0 83.0 96.8 80.1 97.1 81.6 99.3 92.8 99.1 93. 98.9 89.9 99.1 92.3 98.8 90.6 100 96.3 99.5 98.5 100 Regeression-based Planner 80.2 69.0 84.4 69.4 97.8 85.6 98.9 90.8 99.3 98. 99.6 98.7 Diffusion-based Planner 86.0 72.8 98.8 84.4 99.3 98.4 90.9 69. 99.3 99.0 92.9 98.2 Scoring-based Planner 100 99.6 99.4 95.5 96.6 88.6 95.3 89.4 94.9 90.5 98.2 93.8 99.7 95. 99.1 94.1 99.8 94.9 100 99.0 100 99.3 100 99.3 83.4 83. 83.6 91.4 84.0 85.9 83.8 90.3 57.4 55.9 74.3 78.0 76.1 77. 71.9 75.4 93.5 83.1 96.2 76.7 97.8 78.1 95.8 76.6 96.0 76. 99.5 91.3 98.2 92.0 98.4 88.5 99.3 90.7 99.3 73.7 92.9 47. 96.2 59.4 96.7 46.4 96.7 62.8 92.6 55.7 94.7 57.3 93.8 56. 95.6 60.3 87.7 91.5 97.8 97.0 97.8 95.2 97.6 96.3 97.6 94. 89.5 91.1 97.6 92.2 94.9 92.0 93.8 95.9 EC 36.0 25. 71.1 70.6 80.9 43.1 79.6 72.8 80.4 38.0 16.4 35.7 32.9 35. 37.8 30.2 28.0 37.4 S. - - 61.3 39.2 67.5+6.2 42.4+3. 66.7 40.5 71.5+4.8 43.5+3.0 67.1 55.8 71.7+4.6 61.5+5.3 70.4 58.5 73.2+2.8 64.0+5. EPDMS 51.3 24.4 30.2 24% 27.5 32.8 20% 38.3 45.1 18% 41.9 47.2 13% Table 2. Performance on the NAVSIM-v2 navtest Leaderboard. (: pseudo-expert supervision; : reward scoring.)"
        },
        {
            "title": "Backbone",
            "content": "Sim. NC DAC DDC TLC EP TTC LK"
        },
        {
            "title": "Human Agent",
            "content": "- - 100 100 99.8 87.4 100 100 LTF [14] ResNet34 DiffusionDrive [56] ResNet34 GTRS-Dense [55] ResNet34 V2-99 w/o w/ w/o w/ w/o w/ w/o w/ 97.7 98.1 98.4 98.2 97.6 97. 97.6 98.4 Regression-based Planner 94.0 99.8 99.3 95.8 99.7 Diffusion-based Planner 99.5 95. 99.9 99.8 96.8 99.9 99.7 Scoring-based Planner 99.9 99.0 97.5 98.4 98.5 98. 99.5 99.5 99.4 99.9 99.9 99. 87.2 87.3 87.5 87.6 87.9 89. 89.5 87.9 96.7 97.3 97.5 97. 97.0 97.1 97.2 98.1 95.5 96. 96.9 97.9 95.9 97.4 96.8 96. HC 98.1 EC 90.1 98.3 98. 98.4 98.3 97.5 97.1 97.2 97. 82.9 88.2 87.7 87.3 55.9 57. 57.2 58.8 EPDMS 90.3 81.5 84.4 +2. 84.2 85.7 +1.5 82.3 84.0 +1.7 84.0 84.6 +0. (1) LTF [14] employs pretrained ResNet34 [25] as image encoder and is trained for 100 epochs on 8 GPUs with total batch size of 512 and learning rate of 2.83 104. (2) DiffusionDrive [56] employs pretrained ResNet34 [25] as image encoder and applies truncated diffusion with 20 clustered anchors. It is trained for 100 epochs on 8 GPUs with total batch size of 512 and learning rate of 6104. (3) GTRS-Dense [55] employs pretrained ResNet34 [25] or V2-99 [40] as image encoder with an action space of 16,384 trajectories. It is trained for 50 epochs on 32 GPUs with total batch size of 352 and learning rate of 4 104. We use vocabulary of 16,384 trajectories for training. At inference time, 8,192 trajectories are used for navhard, while the full 16,384 trajectories are used for navtest. 3.2. Leaderboard Results Tab. 1 and Tab. 2 present the leaderboard results of SimScale sim-real co-training for the three planner paradigms on navhard and navtest, respectively. Navhard Leaderboard. All models exhibit significant improvements in both Stage 1 and Stage 2. Notably, GTRSDense (V2-99) achieves score of 47.2, establishing new SOTA on the navhard. These results demonstrate that incorporating simulation data with an extended distribution substantially enhances model robustness in challenging and unseen environments, without suffering from potential visual degradation caused by simulated data. Notably, weaker baseline models, i.e., LTF and DiffusionDrive, benefit the most, with gains exceeding 20%, indicating that sim6 Figure 4. Scaling dynamics with different planners and pseudo-expert trajectories. We visualize how simulation data scale and supervision signals influence the driving performance of various planners, where the infection point indicates learning plateau. Figure 5. Qualitative results of the simulation scenes on navtrain. Four representative simulation scenarios are shown, each mirroring typical real-world OOD scene, with synthetic front-view and auxiliary key-view images provided. real co-training with our simulation data effectively enables models to exploit dataset knowledge better and unlock their latent learning potential. Navtest Leaderboard. All models show consistent improvements of up to +2.9 points, demonstrating stronger performance under large-scale and diverse conditions. The quantitative results above highlight that our simulation data is model-agnostic, and that our general sim-real co-training achieves synergistic optimization of robustness and generalization, which is essential for reliable closed-loop deployment in the real world [10, 58]. 3.3. Ablation and Data Scaling Analysis Data Scaling Curves of Different Planners. Due to the lack of prior work studying the scaling behavior of simulation data under fixed amount of real-world data, we model the relationship between performance and the total data size (simulation + real) using log-quadratic function: S(N ) = log2(N ) + log(N ) + c, (7) where S(N ) denotes the planner performance with total data size , and a, b, are parameters fitted via nonlinear least squares: (a, b, c) = arg min a,b,c (cid:88) i=1 (cid:0)Si S(Ni; a, b, c)(cid:1)2 , (8) where Si is the observed performance at total data size Ni, and is the number of data points. If the data scaling trend exists, the quadratic coefficient approaches zero, degenerating the model to linear log relation; otherwise, the curve exhibits parabolic shape with visible saturation point. We evaluate four planners under 2 pseudoexperts following simulation data scaling in Fig. 3 and select EPDMS in navhard as S. The scaling curves and fitted log-quadratic functions are shown in Fig. 4, with the residual standard deviation depicted as an error band. Additionally, we conduct extra experiments using reward scoring only in planner-based simulation data for GTRS-Dense, proposed in Sec. 2.4. Some classical trends can be observed in Fig. 4. For instance, comparing planner-based and reward-scoring settings in Fig. 4 (c) and (d), larger models exhibit more pronounced data scaling trends under the same amount of data. Additional interesting and meaningful observations are highlight below. Table 3. The effect of expert with simulated reward scoring on navhard using GTRS-Dense.(S1/2:Per-stage EPDM scores.) Table 4. The effect between non-reactive vs. reactive data simulation on navhard using GTRS-Dense, across sampling rounds. Backbone Real Expert Sim. Expert S1 S2 EPDMS ResNet34 V2-99 67.1 55.8 66.8 54.9 71.6 61.9 71.7 61.5 70.4 58.5 68.1 55.6 71.8 64.1 73.2 64.0 38.3 37.6 44.8 45.1 41.9 38. 46.6 47.2 Pseudo-Expert Should Be Exploratory. For all planners, the scaling curves under recovery-based setting converge earlier and achieve lower performance compared to plannerbased. The recovery-based expert always steers to the human log, limiting diversity as the simulation data grows from the same real scenarios. In contrast, the planner-based expert explores broader range of possibilities and even produces feasible solutions in challenging situations. Consequently, the recovery-based expert only exhibits advantages in small-data regimes (Fig. 4 (d)), likely because its trajectory distribution is human-like and better aligned with real-world data, making it easier to learn. In most cases, data scaling yields diminishing returns for recovery-based compared to planner-based. These observations highlight the importance of exploratory behavior for pseudo-experts, which enhances the value of simulation data under scaling. Multi-modality Modeling Sparks the Scaling. Although the regression-based LTF and diffusion-based DiffusionDrive have comparable model sizes (56M vs 61M), they exhibit markedly different scaling properties for the plannerbased in Fig. 4 (a) and (b). For LTF, performance saturates and starts to degrade when the simulation-to-real ratio reaches 1:1, whereas DiffusionDrive exhibits an approximately linear improvement. This is due to the gradually increasing diversity of demonstrations from the same real scenario introduces an effectively multi-modal supervision problem. Single-mode regression struggles to model multi-peak distributions, leading to mode confusion and performance degradation, while diffusion models can capture multimodality, making them amenable to optimization under diverse supervision [13]. Since real-world autonomous driving is inherently multi-peak problem [11], our simulation-scaling results underscore the importance of multimodal modeling for scalable real-world end-to-end autonomous driving. Reward is All You Need. In Fig. 4 (c) and (d), for the scoring-based GTRS-Dense planner, reward signals alone, without expert trajectories in simulation, yields even better"
        },
        {
            "title": "Type",
            "content": "#Round #Sim. EPDMS ResNet34 V2-99 Non-Reactive Reactive Reactive 2 2 3 141K 87K 124K 43.7 43.4 44. 45.6 45.5 46.6 performance. To further analyze this, we conduct rewardonly training on real-world data only, which instead leads to performance degradation, as shown in Tab. 3. These results indicate that with sufficient expert supervision to stabilize the optimization direction, the reward guidance is better. The model benefits from the feedback from rewards during its exploration and interaction within the environment [17]. Effect of Reactive Simulation. To isolate the effect of reactive traffic, we compare reward-scoring GTRS-Dense on navhard using non-reactive vs. reactive simulation data (Tab. 4). Two rounds of non-reactive sampling yield 141K trajectories, i.e., 54K more valid samples due to fewer collision rates, yet provide no performance improvement in EPDMS. When reactive simulation reaches the third round, it produces 124K samples (18K fewer) yet delivers consistent and significant EPDMS gains across both model sizes. These results indicate that reactive agent dynamics enhance the realism and diversity of traffic interactions, thereby increasing the effectiveness of simulation data. 3.4. Qualitative Results of the Simulation Scenes We present qualitative visualizations from the simulation data in Fig. 5, showcasing four representative OOD scenarios used to train the policy. These scenarios mirror typical real-world driving challenges where learned policies tend to struggle, including (a) off-center lane drift, (b) near collision, (c) departure, and (d) cutting in cases. Each scenario is illustrated with top-down view showing the PseudoExpert Trajectory as the supervision, and the deviating Perturbed Trajectory as history actions, alongside the synthetic front-view as sensory input to the policy. For instance, scenario (b) requires the policy to adaptively avoid the collision in short horizon. 4. Conclusion In this paper, we introduce SimScale, simreal learning system that reveals how scalable simulation can amplify the value of real-world datasets for end-to-end autonomy. For simulation data pipeline, we first generate pseudo-expert demonstrations from potential OOD states by ego perturbation within reactive environments. Toward real-world simulation, the associated high-fidelity multi-view observations are rendered with our 3DGS engine. Upon the simulated 8 data, sim-real co-training produces synergistic improvements in robustness and generalization for various planners on challenging real-world benchmarks, up to +6.8 EPDMS on navhard and +2.9 navtest. Remarkably, the simreal system scales clearly and predictably with increased simulation while keeping the real-world corpus fixed. We further uncover that exploration and interaction contribute to more effective simulation, and that multi-modal planners strengthen its scaling behavior. We hope SimScale will inspire the community to further explore real-world simulation for data scaling. We provide additional experiments in the supplementary material, such as multi-expert ensemble and scaling with varying real data. It also provides extended discussions on related work, limitations, and future directions."
        },
        {
            "title": "Acknowledgments",
            "content": "This study is supported by National Key R&D Program of China (2022ZD0117901) and National Natural Science Foundation of China (Grant No. 62373355, 62236010). This work is in part supported by the JC STEM Lab of Autonomous Intelligent Systems funded by The Hong Kong Jockey Club Charities Trust. We also appreciate the generous research sponsor from Xiaomi. We extend our gratitude to Naiyan Wang, Yunsong Zhou, Hanxue Zhang, and the rest of the members from OpenDriveLab and Xiaomi Embodied Intelligence Team for their profound support."
        },
        {
            "title": "References",
            "content": "[1] Kingma DP Ba Adam et al. method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 2 [2] Hassan Abu Alhaija, Jose Alvarez, Maciej Bala, Tiffany Cai, Tianshi Cao, Liz Cha, Joshua Chen, Mike Chen, Francesco Ferroni, Sanja Fidler, et al. Cosmos-transfer1: Conditional world generation with adaptive multimodal control. arXiv preprint arXiv:2503.14492, 2025. 1, 4 [3] Mustafa Baniodeh, Kratarth Goel, Scott Ettinger, Carlos Fuertes, Ari Seff, Tim Shen, Cole Gulino, Chenjie Yang, Ghassen Jerfel, Dokook Choe, et al. Scaling laws of motion forecasting and planninga technical report. arXiv preprint arXiv:2506.08228, 2025. 1 [4] Jens Beißwenger. Pdm-lite: rule-based planner for carla leaderboard 2.0. Univ. Tubingen, 2024. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 4 [6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020. 2 [7] Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco PseudoPavone, Andreas Geiger, and Kashyap Chitta. In CoRL, 2025. 2, 5, simulation for autonomous driving. 1 [8] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: unified model to map, perceive, predict and plan. In CVPR, 2021. 1 [9] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In CVPR, 2024. 4 [10] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. TPAMI, 2024. 2, 7, 1 [11] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. 4, 8, 1 [12] Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, Li Song, and Yue Wang. Omnire: Omni urban scene reconstruction. In ICLR, 2025. 1 [13] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. RSS, 2025. [14] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. TPAMI, 2022. 2, 4, 5, 6, 1, 3 [15] OpenScene Contributors. OpenScene: The largest up-todate 3d occupancy prediction benchmark in autonomous driving. https://github.com/OpenDriveLab/ OpenScene, 2023. 5 [16] Daphne Cornelisse, Aarav Pandya, Kevin Joseph, Joseph Suarez, and Eugene Vinitsky. Building reliable sim driving agents by scaling self-play, 2025. 1, 4 [17] Marco Cusumano-Towner, David Hafner, Alex Hertzberg, Brody Huval, Aleksei Petrenko, Eugene Vinitsky, Erik Wijmans, Taylor Killian, Stuart Bowers, Ozan Sener, et al. Robust autonomy emerges from self-play. arXiv preprint arXiv:2502.03349, 2025. 8, 1, 4 [18] Daniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta. Parting with misconceptions about learningbased vehicle motion planning. In CoRL, 2023. 2, 5, 6, 3 [19] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. Navsim: Data-driven nonreactive autonomous vehicle simulation and benchmarking. In NeurIPS, 2024. 2, 5 [20] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In CoRL, 2017. 1 [21] Lan Feng, Yang Gao, Eloi Zablocki, Quanyi Li, Wuyang Li, Sichao Liu, Matthieu Cord, and Alexandre Alahi. Rap: 3d rasterization augmented end-to-end planning. arXiv preprint arXiv:2510.04333, 2025. 1 [22] Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, Ying Zhang, Wenyu Liu, Qian Zhang, and Xinggang Wang. Rad: Training an end-to-end driving policy via large-scale 3dgs-based reinforcement learning. In NeurIPS, 2025. 2, 1, 4 [23] Mitchell Goff, Greg Hogan, George Hotz, Armand du Parc Locmaria, Kacper Raczy, Harald Schafer, Adeeb Shihadeh, Weixing Zhang, and Yassine Yousfi. Learning to drive from world model. In CVPR, 2025. 2 [24] Ke Guo, Haochen Liu, Xiaojun Wu, Jia Pan, and Chen Lv. ipad: Iterative proposal-centric end-to-end autonomous driving. arXiv preprint arXiv:2505.15111, 2025. [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 6, 2 [26] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In ECCV, 2022. 1 [27] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In CVPR, 2023. 4, 1 [28] Xin Huang, Eric Wolff, Paul Vernaza, Tung Phan-Minh, Hongge Chen, David Hayden, Mark Edmonds, Brian Pierce, Xinxin Chen, Pratik Elias Jacob, et al. Drivegpt: Scaling autoregressive behavior models for driving. arXiv preprint arXiv:2412.14415, 2024. 1 [29] Jyh-Jing Hwang, Runsheng Xu, Hubert Lin, Wei-Chih Hung, Jingwei Ji, Kristy Choi, Di Huang, Tong He, Paul Covington, Benjamin Sapp, et al. Emma: End-to-end multimodal model for autonomous driving. arXiv preprint arXiv:2410.23262, 2024. 2, [30] Bernhard Jaeger, Daniel Dauner, Jens Beißwenger, Simon Gerstenecker, Kashyap Chitta, and Andreas Geiger. Carl: Learning scalable planning policies with simple rewards. In CoRL, 2025. 4 [31] Xiaosong Jia, Penghao Wu, Li Chen, Jiangwei Xie, Conghui He, Junchi Yan, and Hongyang Li. Think twice before driving: Towards scalable decoders for end-to-end autonomous driving. In CVPR, 2023. 1 [32] Xiaosong Jia, Junqi You, Zhiyuan Zhang, and Junchi Yan. Drivetransformer: Unified transformer for scalable end-toend autonomous driving. In ICLR, 2025. 1 [33] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In ICCV, 2023. 1 [34] Junzhe Jiang, Nan Song, Jingyu Li, Xiatian Zhu, and Li Zhang. Realengine: Simulating autonomous driving in realistic context. arXiv preprint arXiv:2505.16902, 2025. 1 [35] Max Jiang, Yijing Bai, Andre Cornman, Christopher Davis, Xiukun Huang, Hong Jeon, Sakshum Kulshrestha, John Lambert, Shuangyu Li, Xuanyu Zhou, et al. Scenediffuser: Efficient and controllable driving simulation initialization and rollout. In NeurIPS, 2024. 1, [36] Siwen Jiao, Kangan Qian, Hao Ye, Yang Zhong, Ziang Luo, Sicong Jiang, Zilin Huang, Yangyi Fang, Jinyu Miao, Zheng Fu, et al. Evadrive: Evolutionary adversarial policy optimization for end-to-end autonomous driving. arXiv preprint arXiv:2508.09158, 2025. 1 [37] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2, 1 [38] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, et al. Towards learning-based planning: The nuplan benchmark for real-world autonomous driving. In ICRA, 2024. 5, 2 [39] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. TOG, 2023. 2, 3 [40] Youngwan Lee and Jongyoul Park. Centermask: Real-time anchor-free instance segmentation. In CVPR, 2020. 6, 2 [41] Norman Lehtomaki, NJAM Sandell, and Michael Athans. Robustness results in linear-quadratic gaussian based multivariable control designs. TAC, 2003. 3 [42] Bohan Li, Jiazhe Guo, Hongsi Liu, Yingshuang Zou, Yikang Ding, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, et al. Uniscene: Unified occupancy-centric driving scene generation. In CVPR, 2025. [43] Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, et al. Finetuning generative trajectory model with reinforcement learning from human feedback. arXiv preprint arXiv:2503.10434, 2025. 1 [44] Kailin Li, Zhenxin Li, Shiyi Lan, Yuan Xie, Zhizhong Zhang, Jiayi Liu, Zuxuan Wu, Zhiding Yu, and Jose Hydra-mdp++: Advancing end-to-end drivAlvarez. arXiv preprint ing via expert-guided hydra-distillation. arXiv:2503.12820, 2025. 5 [45] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. TPAMI, 2022. 1 [46] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2drive: Efficient reinforcement learning by thinking with latent world model for autonomous driving (in carlav2). In ECCV, 2024. 4 10 [47] Tianyu Li, Yihang Qiu, Zhenhua Wu, Carl Lindstrom, Peng Su, Matthias Nießner, and Hongyang Li. arXiv preprint Mtgs: Multi-traversal gaussian splatting. arXiv:2503.12552, 2025. 3, [48] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: layout-guided multi-view driving scenarios video generation with latent diffusion model. In ECCV, 2024. 1 [49] Yingyan Li, Shuyao Shang, Weisong Liu, Bing Zhan, Haochen Wang, Yuqi Wang, Yuntao Chen, Xiaoman Wang, Yasong An, Chufeng Tang, et al. Drivevla-w0: World models amplify data scaling law in autonomous driving. arXiv preprint arXiv:2510.12796, 2025. 1 [50] Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, and Zhaoxiang Zhang. End-to-end driving with online traarXiv preprint jectory evaluation via bev world model. arXiv:2504.01941, 2025. 1 [51] Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, et al. Recogdrive: reinforced cognitive framework for end-to-end autonomous driving. arXiv preprint arXiv:2506.08052, 2025. 1 [52] Zhenxin Li, Kailin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Yishen Ji, Zhiqi Li, Ziyue Zhu, Jan Kautz, Zuxuan Wu, et al. Hydra-mdp: End-to-end multimodal planning with multitarget hydra-distillation. arXiv preprint arXiv:2406.06978, 2024. 2, 4 [53] Zhenxin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Zuxuan Wu, and Jose Alvarez. Hydra-next: Robust closed-loop driving with open-loop training. arXiv preprint arXiv:2503.12030, 2025. [54] Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Jingde Chen, Nadine Chang, Maying Shen, Jingyu Song, Zuxuan Wu, Shiyi Lan, et al. Ztrs: Zero-imitation end-to-end autonomous driving with trajectory scoring. arXiv preprint arXiv:2510.24108, 2025. 1 [55] Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Joshua Chen, Nadine Chang, Maying Shen, Zuxuan Wu, Shiyi Lan, and Jose Alvarez. Generalized trajectory scorarXiv preprint ing for end-to-end multimodal planning. arXiv:2506.06664, 2025. 2, 5, 6, 1, 3 [56] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, et al. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. In CVPR, 2025. 2, 4, 5, 6, 1, 3 [57] Haochen Liu, Zhiyu Huang, Wenhui Huang, Haohan Yang, Xiaoyu Mo, and Chen Lv. Hybrid-prediction integrated planning for autonomous driving. TPAMI, 2025. 1 [58] Haochen Liu, Tianyu Li, Haohan Yang, Li Chen, Caojun Wang, Ke Guo, Haochen Tian, Hongchen Li, Hongyang Li, and Chen Lv. Reinforced refinement with self-aware expansion for end-to-end autonomous driving. arXiv preprint arXiv:2506.09800, 2025. 2, 7, 1 [60] Xi Liu, Chaoyi Zhou, and Siyu Huang. 3dgs-enhancer: Enhancing unbounded 3d gaussian splatting with viewconsistent 2d diffusion priors. In NeurIPS, 2024. 4 [61] William Ljungbergh, Adam Tonderski, Joakim Johnander, Holger Caesar, Kalle Astrom, Michael Felsberg, and Christoffer Petersson. Neuroncap: Photorealistic closedloop safety testing for autonomous driving. In ECCV, 2024. 1 [62] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 2 [63] Abhiram Maddukuri, Zhenyu Jiang, Lawrence Yunliang Chen, Soroush Nasiriany, Yuqi Xie, Yu Fang, Wenqi Huang, Zu Wang, Zhenjia Xu, Nikita Chernyadev, Scott Reed, Ken Goldberg, Ajay Mandlekar, Linxi Fan, and Yuke Zhu. Sim-and-real co-training: simple recipe for vision-based robotic manipulation. In RSS, 2025. 2, 4 [64] Alexander Naumann, Xunjiang Gu, Tolga Dimlioglu, Mariusz Bojarski, Alperen Degirmenci, Alexander Popov, Devansh Bisla, Marco Pavone, Urs Muller, and Boris Ivanovic. Data scaling laws for end-to-end autonomous driving. In CVPR, 2025. 2, [65] NVIDIA, Nikita Cherniadev Johan Bjorck andFernando Castaneda, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An open foundation model for generalist humanoid robots. arxiv preprint arxiv:2503.14734, 2025. 2, 4 [66] Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, Bertrand Douillard, David Nister, Urs Muller, Ruchi Bhargava, et al. Mitigating covariate shift in imitation learning for autonomous vehicles using latent space generative world models. arXiv preprint arXiv:2409.16663, 2024. 2 [67] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 1 [68] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, Haoran Wu, and Sifa Zheng. Sparsedrive: End-to-end autonomous driving via sparse scene representation. In ICRA, 2025. 1 [69] Tianyi Tan, Yinan Zheng, Ruiming Liang, Zexu Wang, Kexin Zheng, Jinliang Zheng, Jianxiong Li, Xianyuan Zhan, and Jingjing Liu. Flow matching-based autonomous driving planning with advanced interactive behavior modeling. In NeurIPS, 2025. 4 [59] Kunhao Liu, Ling Shao, and Shijian Lu. Novel view extrapolation with video diffusion priors. arXiv preprint arXiv:2411.14208, 2024. [70] Martin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical observations and microscopic simulations. Physical review E, 2000. 2, 3, 1, 4 11 [85] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In CVPR, 2022. 2, 1 [86] Chris Zhang, Sourav Biswas, Kelvin Wong, Kion Fallah, Lunjun Zhang, Dian Chen, Sergio Casas, and Raquel Urtasun. Learning to drive via asymmetric self-play. In ECCV, 2024. 1 [87] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating reinforcement learning coach. In ICCV, 2021. 4 [88] Yupeng Zheng, Zhongpu Xia, Qichao Zhang, Teng Zhang, Ben Lu, Xiaochuang Huo, Chao Han, Yixian Li, Mengjie Yu, Bu Jin, et al. Preliminary investigation into data scaling laws for imitation learning-based end-to-end autonomous driving. arXiv preprint arXiv:2412.02689, 2024. 2, [89] Yinan Zheng, Ruiming Liang, Kexin ZHENG, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, and Jingjing Liu. Diffusion-based planning for autonomous driving with flexible guidance. In ICLR, 2025. 4 [90] Hongyu Zhou, Longzhong Lin, Jiabao Wang, Yichong Lu, Dongfeng Bai, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. Hugsim: real-time, photo-realistic and closed-loop simulator for autonomous driving. arXiv preprint arXiv:2412.01718, 2024. 1 [91] Jiawei Zhou, Linye Lyu, Zhuotao Tian, Cheng Zhuo, and Yu Li. Safemvdrive: Multi-view safety-critical driving video synthesis in the real world domain. arXiv preprint arXiv:2505.17727, 2025. 1 [92] Yunsong Zhou, Michael Simon, Zhenghao Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, and Bolei Zhou. Simgen: Simulator-conditioned driving scene generation. In NeurIPS, 2024. 1 [93] Yunsong Zhou, Naisheng Ye, William Ljungbergh, Tianyu Li, Jiazhi Yang, Zetong Yang, Hongzi Zhu, Christoffer Petersson, and Hongyang Li. Decoupled diffusion sparks adaparXiv preprint arXiv:2504.10485, tive scene generation. 2025. 1, 4 [71] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-worlddrive world models for autonomous driving. In ECCV, 2024. [72] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for realtime autonomous driving. In CVPR, 2024. 1 [73] Penghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: simple yet strong baseline. In NeurIPS, 2022. 1 [74] Yanhao Wu, Haoyang Zhang, Tianwei Lin, Lichao Huang, Shujie Luo, Rui Wu, Congpei Qiu, Wei Ke, and Tong Zhang. Generating multimodal driving scenes via next-scene prediction. In CVPR, 2025. 4 [75] Ziyang Xie, Zhizheng Liu, Zhenghao Peng, Wayne Wu, and Bolei Zhou. Vid2sim: Realistic and interactive simulation from video for urban navigation. In CVPR, 2025. 1 [76] Zebin Xing, Xingyu Zhang, Yang Hu, Bo Jiang, Tong He, Qian Zhang, Xiaoxiao Long, and Wei Yin. Goalflow: Goaldriven flow matching for multimodal trajectories generation in end-to-end autonomous driving. In CVPR, 2025. 4, 1 [77] Runsheng Xu, Hubert Lin, Wonseok Jeon, Hao Feng, Yuliang Zou, Liting Sun, John Gorman, Kate Tolstaya, Sarah Tang, Brandyn White, et al. Wod-e2e: Waymo open dataset for end-to-end driving in challenging long-tail scenarios. arXiv preprint arXiv:2510.26125, 2025. 2 [78] Zhiyuan Xu, Bohan Li, Huan-ang Gao, Mingju Gao, Yong Chen, Ming Liu, Chenxu Yan, Hang Zhao, Shuo Feng, and Hao Zhao. Challenger: Affordable adversarial driving video generation. arXiv preprint arXiv:2505.15880, 2025. 1 [79] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In ECCV, 2024. 3, [80] Jiawei Yang, Jiahui Huang, Yuxiao Chen, Yan Wang, Boyi Li, Yurong You, Apoorva Sharma, Maximilian Igl, Peter Karkus, Danfei Xu, et al. Storm: Spatio-temporal reconstruction model for large-scale outdoor scenes. arXiv preprint arXiv:2501.00602, 2024. 4 [81] Jiazhi Yang, Kashyap Chitta, Shenyuan Gao, Long Chen, Yuqian Shao, Xiaosong Jia, Hongyang Li, Andreas Geiger, Xiangyu Yue, and Li Chen. Resim: Reliable world simulation for autonomous driving. In NeurIPS, 2025. 2, 4 [82] Yan Yang, Haochen Tian, Yang Shi, Wulin Xie, Yi-Fan Zhang, Yuhao Dong, Yibo Hu, Liang Wang, Ran He, Caifeng Shan, et al. survey of unified multimodal understanding and generation: Advances and challenges. Authorea Preprints, 2025. 4 [83] Yifan Ye, Jun Cen, Jing Chen, and Zhihe Lu. Selfevolved imitation learning in simulated world. arXiv preprint arXiv:2509.19460, 2025. 4 [84] Junqi You, Xiaosong Jia, Zhiyuan Zhang, Yutao Zhu, and Junchi Yan. Bench2drive-r: Turning real world data into reactive closed-loop autonomous driving benchmark by generative model. arXiv preprint arXiv:2412.09647, 2024. 1 SIMSCALE: Learning to Drive via Real-World Simulation at Scale"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Related Work A.1. End-to-End Autonomous Driving . . . . . . . . A.2. Scene Simulation for Driving. . . . . . . . . . . A.3. Data Scaling for Driving . . . . . . . . . . . . . . B. Additional Implementation Details B.1. Simulation Data Curation . . . . . . . . . . . . . B.2. Models and Training . . . . . . . . . . . . . . . . C. Additional Experimental Results C.1. Detailed Leaderboard Results . . . . . . . . . . C.2. Multi-Expert Ensemble. . . . . . . . . . . . . . . C.3. Scaling with Varying Scales of Real Data. . . C.4. Qualitative Results of Simulation . . . . . . . . D. Limitations and Broader Impact D.1. Pseudo-Expert . . . . . . . . . . . . . . . . . . . . D.2. Scene Simulation. . . . . . . . . . . . . . . . . . . D.3. Online RL and Self-Play . . . . . . . . . . . . . . D.4. Societal Impact . . . . . . . . . . . . . . . . . . . . E. License of Assets 1 1 1 1 2 2 2 2 2 2 4 4 4 4 4 5 5 In We outline the supplementary material as follows. Sec. A, we first provide additional discussions of related work. Sec. presents further implementation details regarding data curation and models. In Sec. C, we report extended experimental analyses and qualitative results. Finally, Sec. discusses the limitations and broader impacts, and Sec. lists the licenses of all utilized assets. A. Related Work A.1. End-to-End Autonomous Driving End-to-end system maps directly from raw sensor inputs to planning [10]. Early works adopt regression-based planning and gradually shift from extra task branches [14, 31, 73] to unifying perception, prediction, and planning under joint supervision [27, 32, 33, 57, 72]. Recent work has moved toward generative approaches. Diffusion-based systems [43, 51, 56, 76] are framed as conditional denoising process, enabling diverse and high-fidelity trajectories. Concurrently, trajectory scoring has emerged as an efficient alternative, ranking candidate trajectories under spline curves [8, 26], discretized tokens [11, 22], clustered human trajectories [53, 55], or predicted proposals [24, 50, 68]. Moreover, it offers natural interface for rewardor costbased optimization [22, 36, 51, 54, 58], enabling reinforced improvements. Still, supervision on logged end-to-end data limits training to the experts open-loop distribution, causing compromised learning in the drifted state of sensory data. We address this by introducing scalable simulation framework that reactively generates end-to-end pairs using pseudo-experts or rewards, allowing end-to-end systems across any above paradigms to bootstrap extra supervision from existing training data. A.2. Scene Simulation for Driving Scene simulation, including traffic behavior simulation and sensor simulation, has long been key topic in autonomous driving research. For traffic simulation, existing works [7, 22, 90] leverage rule-based planners like IDM [70], or diffusion-based generators [35, 93] to simulate plausible interaction of traffic agents. For sensor simulation, traditional graphics-based simulators [20, 45] suffer from significant sim-to-real gap, which limits the real-world deployment of trained planners. Recent data-driven efforts follow two directions: some approaches [2, 48, 71, 78, 84, 91, 92] attempt to generate sensor data in unseen scenarios via video generation models conditioned on 3D bounding boxes, HD maps or BEV maps. Other works focus on scene reconstruction [7, 12, 34, 61, 75, 79, 90] to build photorealistic simulators for novel-view synthesis, using techniques like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). Although these methods achieve impressive visual results, most are primarily designed for closed-loop evaluation or visual augmentation. Our work aims to explore how to use traffic and sensor simulation to generate realistic scenes with feasible demonstrations for planner training, and its impacts on planner performances. A.3. Data Scaling for Driving Recent data scaling laws have driven major advances in foundation models [37, 67, 85], but their impact on autonomous driving planning remains underexplored. Prior researches [29, 64, 88] demonstrate that increasing realworld data from thousands to millions of driving logs improves end-to-end planner performance, though improvements diminish. Dense supervision from video predictions improves this data scaling efficiency [49]. For planners operating on abstract BEV representations, industry efforts [3, 28] demonstrate clear benefits from scaling real-world data and model capacity, while self-play [16, 17, 86] scales reinforcement learning via massive simulation to achieve strong zero-shot sim-to-real transfer. However, most existing approaches rely on costly real-world collection or traffic simulation, which expands only in abstract state spaces rather than raw sensory domains. 3D rasterization provides lightweight remedy, but suffers from information loss [21]. In contrast, we investigate scaling properties of Table 5. Simulation Data Curation Pipeline Configurations."
        },
        {
            "title": "Notation Value",
            "content": "3DGS Data Engine Peak signal-to-noise ratio of GS"
        },
        {
            "title": "PSNR",
            "content": "< 27 Vc θ rlon rlat δlon δlat Rper 16, 384 20 20m 2m 5m 0.5m EPDMS 0.8 Pseudo-Expert Trajectory Generation Rexp Reward filterer Recovery-based Whole human traj vocabulary Planner-based Privileged planner Vh EPDMS Sm=1,m=EP SEP>0.5 103, 288 PDM-Closed [18] the main paper. We follow the NAVSIM [19] default setup and use only the train logs [38] of navtrain for both real-data training and simreal co-training. All planners share identical settings across the real or sim-real training setups, and each model is trained to saturation to ensure the validity of the subsequent scaling analyses. LTF. [14] regression-based planner that directly regresses future waypoints upon fused sensory latents. DiffusionDrive. [56] multi-modal, DETR-style generative planner that iteratively denoises diverse trajectories using anchor-conditioned truncated diffusion. Each anchor adaptively queries the encoded image features. GTRS-Dense. [55] scoring-based, DETR-style planner that ranks dense vocabulary of clustered human trajectories using multiple supervised scoring heads. Inputs are augmented with random spatial shifts. Table 6. Model and Training Hyperparameters. C. Additional Experimental Results Hyperpara. LTF [14] DiffusionDrive [56] GTRS-Dense [55] C.1. Detailed Leaderboard Results Model Configuration Sensors Resolution Horizon Frequency Backbone Parameters Aux. Tasks 3 Cam. 2048 512 4s 2Hz R34 [25] 56M Det. Seg."
        },
        {
            "title": "Training Configuration\nGPUs\nEpochs\nTotal BS\nInitial LR\nSchedule\nOptimizer",
            "content": "8 H20 100 512 2.83 104 Constant Adam [1] 3 Cam. 2048 512 4s 2Hz R34 [25] 61M Det. Seg. 8 H20 100 512 6 104 Cosine Decay AdamW [62] 3 Cam. 2048 512 4s 10Hz R34 [25] / V99 [40] 67M / 83M None 32 H20 50 352 4 104 Cosine Decay AdamW [62] planning directly through large-scale 3DGS sensory simulation, which bridges abstract traffic simulation and realworld perception, offering scalable and realistic alternative to real-world data collection. B. Additional Implementation Details B.1. Simulation Data Curation Tab. 5 summarizes the detailed terms with notation and value used in the simulation data curation process to supplement Sec. 3.1 curation pipeline in the main paper. For pseudo-expert trajectory generation, we discard infeasible candidates to ensure valid supervision. Specifically, all submetrics of EPDMS must be satisfied, with SEP relaxed: EP 0.5, preventing biased driving styles. Qualitative results during curation are shown in Sec. C.4 B.2. Models and Training Tab. 6 provides the detailed model and training hyperparameters that complement Sec. 3.1 implementation protocol in In the main paper, Tab. 1 and Tab. 2 report only the best results in navhard and navtest of each planner co-training (highlighted in blue rows ). Supplementary Tab. 7 and Tab. 8 provide full results across different pseudo-experts and the reward-only scoring method. The best results remain highlighted in blue rows , allowing direct comparison with the main paper. Planner-based experts generally outperform recovery-based experts, while the scoring-based planner benefits most from the reward-only method. C.2. Multi-Expert Ensemble Different pseudo-experts exhibit distinct behavioral characteristics, e.g., the recovery-based expert is more conservative, whereas the planner-based expert is more exploratory. This raises the question of whether these complementary properties can be leveraged jointly. Therefore, we conduct simple ensemble study, multi-expert enseble as we call it, to examine their potential complementarity on navhard, as shown in Tab. 9. The scoring-based GTRS-Dense planner is chosen because it enables straightforward ensemble: the predicted sub-scores of each trajectory in the vocabulary can be directly averaged across the three models (recoverybased, planner-based, reward-only scoring). Ensemble Gains from Expert Diversity. As shown in Tab. 9, this simple multi-expert ensemble yields consisimprovements for both backbones, achieving +3.4 tent and +2.2 EPDMS on ResNet34 and V2-99, respectively, for navhard. Further enlarging the ensemble to all six models offers only marginal additional gain of +0.8 EPDMS. These indicate that, for end-to-end planning, different pseudo-experts provide strong complementary benefitsoften exceeding the gains brought by different back2 Table 7. Detailed Results on navhard. PDM-Closed uses ground-truth symbolic inputs for planning, while other methods rely on sensor data. (recovery / planner: recovery-based / planner-based expert; reward: reward-only scoring; S.: per-stage EPDM score.)"
        },
        {
            "title": "Backbone",
            "content": "Sim."
        },
        {
            "title": "Stage",
            "content": "NC DAC DDC TLC EP TTC LK HC PDM-Closed [18] - - w/o LTF [14] ResNet34 recovery planner w/o DiffusionDrive [56] ResNet34 recovery planner w/o recovery planner reward w/o recovery planner reward ResNet34 V2-99 GTRS-Dense [55] 1 1 2 1 2 1 2 1 2 1 2 1 1 2 1 2 1 2 1 2 1 2 1 1 2 1 2 94.4 88.1 97.3 79.4 96.4 82.3 98.0 83. 96.8 80.1 97.2 83.8 97.1 81.6 99.3 92.8 99.6 94.4 98.2 94. 99.1 93.4 98.9 89.9 99.0 91.7 99.1 94.3 99.1 92.3 98.8 90. 100 96.3 99.5 98.5 Regeression-based Planner 97.8 85.6 80.2 69.0 99.3 98.5 79.6 73. 98.1 89.2 99.6 98.7 84.4 69.4 98.9 90.8 99.6 98.7 Diffusion-based Planner 99.3 98.4 98.8 84. 86.0 72.8 89.1 69.0 90.9 69.3 98.9 90.2 99.0 92.9 99.3 98. 99.3 98.2 Scoring-based Planner 96.6 88.6 96.9 87.8 97.1 91.3 95.3 89. 94.9 90.5 97.3 91.3 97.8 91.9 98.2 93.8 99.6 95.5 99.9 94. 99.1 94.8 99.7 95.2 99.1 94.1 99.0 94.8 99.9 95.5 99.8 94. 100 99.4 99.8 99.5 100 98.5 100 99.0 100 99.3 100 99. 100 99.4 100 99.3 100 100 83.4 83.8 82.7 88.2 83.6 91. 84.0 85.9 83.2 87.7 83.8 90.3 57.4 55.9 58.5 69.0 70.6 76. 74.3 78.0 76.1 77.6 73.0 78.5 68.8 74.4 71.9 75.4 93.5 83. 96.2 76.7 95.6 79.3 97.8 78.1 95.8 76.6 95.8 78.6 96.0 76. 99.5 91.3 99.6 92.6 98.7 92.5 98.2 92.0 98.4 88.5 99.1 89. 98.9 91.5 99.3 90.7 99.3 73.7 92.9 47.9 92.7 49.4 96.2 59. 96.7 46.4 96.2 53.9 96.7 62.8 92.6 55.7 93.8 60.4 94.0 58. 94.7 57.3 93.8 56.0 95.1 60.8 95.3 61.0 95.6 60.3 87.7 91. 97.8 97.0 97.8 96.3 97.8 95.2 97.6 96.3 97.8 96.6 97.6 94. 89.5 91.1 93.6 94.0 95.1 92.6 97.6 92.2 94.9 92.0 96.0 94. 95.8 93.4 93.8 95.9 EC 36.0 25.4 71.1 70.6 79.1 52. 80.9 43.1 79.6 72.8 69.8 38.2 80.4 38.0 16.4 35.7 17.3 41. 33.8 31.1 32.9 35.8 37.8 30.2 28.9 37.0 26.7 39.0 28.0 37. S. EPDMS - - 61.3 39.2 61.5 43.5 67.5 42. 66.7 40.5 69.2 42.7 71.5 43.5 67.1 55.8 69.0 60.3 71.6 61. 71.7 61.5 70.4 58.5 72.2 62.1 71.8 64.1 73.2 64.0 51. 24.4 29.3 30.2 27.5 30.5 32. 38.3 42.5 44.8 45.1 41.9 45. 46.6 47.2 Table 8. Detailed Results on navtest. (recovery / planner: recovery-based / planner-based expert; reward: rewardonly scoring)"
        },
        {
            "title": "Backbone",
            "content": "Sim. NC DAC DDC TLC EP TTC LK"
        },
        {
            "title": "Human Agent",
            "content": "- - 100 100 99.8 87.4 100 100 HC 98.1 EC 90.1 LTF [14] ResNet34 recovery w/o planner w/o DiffusionDrive [56] ResNet34 recovery GTRS-Dense [55] planner w/o recovery planner reward w/o recovery planner reward ResNet34 V2-99 97.7 98. 98.1 98.4 98.3 98.2 97.6 98. 97.8 97.6 97.6 98.4 98.3 98. Regression-based Planner 99.3 94.0 99.8 95.1 99.5 99.8 95. 99.9 99.7 Diffusion-based Planner 99.8 99.5 95.5 96.5 96.8 99.6 99. 99.8 99.9 Scoring-based Planner 99.9 99.9 99. 99.9 99.9 99.9 99.9 99.9 97. 98.1 98.3 98.4 98.5 98.8 98. 98.4 99.0 99.3 99.4 99.5 99. 99.4 99.5 99.5 3 87.2 87. 87.3 87.5 87.6 87.6 87.9 88. 89.1 89.7 89.5 87.9 88.4 87. 96.7 97.5 97.3 97.5 97.5 97. 97.0 97.5 97.2 97.1 97.2 98. 98.0 98.2 95.5 96.7 96.9 96. 97.3 97.9 95.9 96.5 96.4 97. 96.8 96.4 96.7 95.8 98.3 98. 98.3 98.4 98.3 98.3 97.5 97. 97.8 97.1 97.2 97.6 97.5 97. 82.9 87.3 88.2 87.7 87.5 87. 55.9 56.2 56.6 57.3 57.2 58. 56.4 59.4 EPDMS 90.3 81.5 83. 84.4 84.2 85.2 85.7 82.3 83. 83.9 84.0 84.0 84.6 84.3 83. Table 9. The effect of multi-expert ensemble on navhard using GTRS-Dense. (S1/2:Per-stage EPDM scores; recovery / planner: recovery-based / planner-based expert; reward: reward scoring only; :multi-expert ensemble)"
        },
        {
            "title": "Expert",
            "content": "S1 S2 EPDMS recovery planner reward recovery planner reward 67.1 69.0 71.6 71.7 73.5 70.4 69.0 71.8 73. 74.0 73.8 55.8 60.3 61.9 61.5 65.4 58. 60.3 64.1 64.0 65.7 67.2 ResNet34 V2-99 38.3 42.5 44.8 45.1 48.5 41.9 42.5 46.6 47.2 49. 50.2 bone structures. C.3. Scaling with Varying Scales of Real Data To assess the practical utility of our approach, we study how simulation data affects performance as the amount of available real-world data scales up. Based on GTRS-Dense with reward-only scoring sim-real co-training, we conduct experiments with different real data from navtrain (10K, 20K, 50K, 100K). Inspired by [2], simulation data are generated from identical scenarios of corresponding real data tokens, and we fix the sim-real data ratio (five sampling rounds) for each experiment. Sustained Simulation Gains Across Real Data Scales. As shown in Fig. 6, our simulation data provides the most significant gains when real data is scarce (10K), achieving +22.4% EPDMS on ResNet34 and +12.1% EPDMS on V2-99 for navhard. As the amount of real data increases (from 10K to 100K), these gains remain consistently high, without noticeable narrowing. This demonstrates that our simulation data can effectively complement limited realworld data and still offers substantial potential to further exploit and amplify the value of real data even when it is abundant. C.4. Qualitative Results of Simulation We show additional visualizations of our simulated data as in Fig. 8 and Fig. 7, which demonstrate the high fidelity and pseudo-expert generation of our data generation pipeline. More OOD simulation scenes are shown in Fig 9. Recovery-based Expert. As shown in Fig. 7, all simulation trajectories successfully converge back to the human experts final waypoint. Figure 6. Scaling simulation with varying real data. Simulation data are scaled by corresponding real data scenario tokens and fixed sim-real data ratio Planner-based Expert As shown in Fig. 8, the simulation trajectories exhibit clear variations and more diverse distribution. D. Limitations and Broader Impact D.1. Pseudo-Expert Despite its effectiveness, our pseudo-expert generation pipeline has limitations. Current rule-based trajectory perturbations are static. potential enhancement is the selfevolving approach [83], using pretrained planners to iteratively explore the simulator and collect recovery data. Additionally, the privileged BEV planner we use is rulebased with limited performance, causing some degradation in comfort metrics (HC, EC in Tab. 7) and failing in extremely corner cases. More advanced learning-based BEV planners [30, 69, 89] could further improve generation efficiency and realism. D.2. Scene Simulation For traffic behavior simulation in our decoupled scene simulation paradigm, other agents are controlled by IDM [70], limits scenario diversity. which enables interaction but Diffusion-based traffic generators [35, 93] offer promising improvement. For sensor simulation, feedforward GS [9, 80] can improve reconstruction efficiency, while adding modalities like LiDAR [42] provides complemenFurthermore, unified world tary modality information. models [74, 82] can serve as substitute for both behavior and sensor simulation. D.3. Online RL and Self-Play Online reinforcement learning (RL) [30, 46] and selfplay [16, 17] in traffic simulators learn through exploration and feedback. Sensor simulation serves as bridge to real-world environments, enabling real-world RL for autonomous driving [22]. In contrast, the proposed SimScaleintroduces an alternative framework: by generating expert demonstrations, real-world online simulations can be leveraged directly for imitation learning (IL). Otherwise, we also study the effect of learning from feedback through reward optimization. D.4. Societal Impact Despite promising improvements, pseudo-expert scene simulation still has room for quality and efficiency enhancements for real-world deployment. Co-training may be affected by unrealistic simulation visuals and distributional differences in real-world corner cases, which could lead to potential risks. We hope SimScale will inspire both academia and industry in driving and robotics to leverage real-world simulation to address rare corner cases in data scaling, advancing fully autonomous systems that are robust and generalizable. In addition, we will provide the community with fully open-source simulation dataset and training framework to promote academic research on autonomous driving in simulation. E. License of Assets All training and evaluation are performed on data from publicly licensed datasets [7, 15, 38]. The real-world data engine is based on the MTGS codebase [47] under the Apache-2.0 license. We adopt publicly available end-to-end planners, including GTRS [55] (Apache-2.0) and DiffusionDrive [56] and LTF [14] (MIT). Figure 7. Qualitative results of recovery-based expert with real and simulation data. Figure 8. Qualitative results of planner-based expert with real and simulation data. 6 Figure 9. Additional qualitative results of the simulation scenes on navtrain."
        }
    ],
    "affiliations": [
        "Huawei Noah's Ark Lab",
        "Huawei Technologies",
        "Institute of Automation, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
    ]
}