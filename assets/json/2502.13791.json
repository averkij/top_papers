{
    "paper_title": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions",
    "authors": [
        "Nathanaël Carraz Rakotonirina",
        "Mohammed Hamdy",
        "Jon Ander Campos",
        "Lucas Weber",
        "Alberto Testoni",
        "Marzieh Fadaee",
        "Sandro Pezzelle",
        "Marco Del Tredici"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are increasingly used in working environments for a wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over long-term interactions? To investigate this, we introduce MemoryCode, a synthetic multi-session dataset designed to test LLMs' ability to track and execute simple coding instructions amid irrelevant information, simulating a realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight a fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions."
        },
        {
            "title": "Start",
            "content": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding Interactions Nathanaël Carraz Rakotonirina1, Mohammed Hamdy5, Jon Ander Campos3, Lucas Weber1, Alberto Testoni2, Marzieh Fadaee4, Sandro Pezzelle2, Marco Del Tredici 1Universitat Pompeu Fabra, 2University of Amsterdam, 3Cohere, 4Cohere For AI, 5Cohere For AI Community 5 2 0 2 9 1 ] . [ 1 1 9 7 3 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are increasingly used in working environments for wide range of tasks, excelling at solving individual problems in isolation. However, are they also able to effectively collaborate over longterm interactions? To investigate this, we introduce MEMORYCODE, synthetic multisession dataset designed to test LLMs ability to track and execute simple coding instructions amid irrelevant information, simulating realistic setting. While all the models we tested handle isolated instructions well, even the performance of state-of-the-art models like GPT-4o deteriorates when instructions are spread across sessions. Our analysis suggests this is due to their failure to retrieve and integrate information over long instruction chains. Our results highlight fundamental limitation of current LLMs, restricting their ability to collaborate effectively in long interactions."
        },
        {
            "title": "Introduction",
            "content": "Current efforts to improve the performance of large language models (LLMs) mostly focus on their ability to solve increasingly harder tasks autonomously. Examples of this research include solving complex math (Wang et al.; Gao et al., 2024; Trinh and Luong, 2024), coding (Chen et al., 2021; Austin et al., 2021; Tao et al., 2024; Puerto et al., 2024), or reasoning problems (Hao et al., 2023; Wang and Zhou, 2024; Renze and Guven, 2024). Since many of these tasks are relevant to real-world applications, LLMs are widely adopted in industry, where they have been reported to significantly enhance productivity (Weber et al., 2024; Cambon et al., 2023). This extensive adoption of LLM assistants into the daily working routine is effectively Correspondence: nathanael.rakotonirina@upf.edu Marco Del Tredici started this project while working at Cohere. Lucas Weber is now at Fraunhofer IIS and Alberto Testoni at Amsterdam UMC. The code and data are available at https://github.com/ for-ai/MemoryCode. 1 Figure 1: simplified but realistic example of longterm interaction between human and an LLM-based teammate. In this example, each day represents single session. The LLM teammate must remember piece of informationin redlearned during the session on Day 1 to correctly perform task on Day 20, while also receiving irrelevant informationin blueon Day 5. converting them from mere tools to fully-fledged teammates. For LLMs to behave as such, though, complementary skills related to collaboration and interaction are needed. One such ability is retaining relevant information from multiple interactions with human users and leveraging it for future tasks. In this paper, we investigate this challenge by introducing MEMORYCODE, synthetic dataset of multi-session dialogue histories designed to evaluate models ability to track simple coding instructions provided amid irrelevant information, and execute them in future coding tasks. Each dialogue history is chronological sequence of dialogues, or sessions, between mentor and mentee. Throughout the sessions, the mentor passes critical information for solving task to mentee. Crucially, this information is interspersed with substantial amount of unrelated content, thus reflecting the real-life scenario of working in an office. Furthermore, the information needed to perform task can be updated multiple times throughout the dialogue history. MEMORYCODE mimics natural interactions between coworkers. Figure 1 shows an example of such interactions, where various coding conventions and rules arise (Convertino et al., 2008; Chumg et al., 2022) that are passed on to new team members (Day 1), often among other pieces of information irrelevant to coding tasks (Day 5). Newcomers are expected to comply with such rules when performing future tasks (Day 20), unless rules are deprecated or changed. MEMORYCODE tests whether current models behave like new human teammates by consistently adhering to such rules across many sessions. Similar to previous work (Nelson et al., 2024; Epstein et al., 2024; Maharana et al., 2024), the primary goal of our benchmark is to retrieve important information from long conversational history. In contrast to previous datasets, MEMORYCODE requires to use retrieved information in practical tasks while not being explicitly cued to do so. This is more challenging than cued retrieval of static information, as it requires prospective memory and spontaneous retrieval (McDaniel and Einstein, 2007; Brandimonte et al., 2014). Additionally, MEMORYCODE requires an integration of information retrieved from different parts of the dialogue history, as rules can be updated, with only the last update being eventually relevant. At the same time, the rules in it (e.g., adding date at the start of every new code) are simple to execute, which allows for disentangling models retrieval capabilities from other complex skills. To the best of our knowledge, MEMORYCODE is the first multi-session dataset that tests this practically highly relevant skill. We test several proprietary and open-source SotA models on MEMORYCODE, and show that: (i) Even small models succeed in executing the single coding instructions in MEMORYCODE when prompted without additional complex context, indicating that such instructions are well within the reach of current LLMs; (ii) As we increase the complexity and provide full mentor-mentee session, including several instructions and irrelevant information, only larger models continue to perform well, while the performance of smaller models drops significantly; (iii)When we provide the full dialogue history, even strong proprietary models struggle to follow our simple instructions, with GPT-4o showing dramatic 67% drop in accuracy compared to its performance with instructions alone. This reveals that MEMORYCODE is challenging benchmark even for the best available models, that struggle to retrieve and incrementally update relevant information. We argue that solving MEMORYCODE requires more than simply scaling models even further. Instead, our results indicate pressing need to develop dedicated mechanisms to enhance LLMs abilities, such as improved long-term memory retention strategies, prospective memory, or additional reasoning mechanisms. We release the dataset under the Apache 2.0 license."
        },
        {
            "title": "2.1 Long-Context Evaluation",
            "content": "Early approaches to evaluating long-context understanding date back to the pre-LLM era. One such example is LAMBADA (Paperno et al., 2016), which includes high-quality human-annotated samples with an average length of 75 tokens. As context lengths increased, new datasets were created by repurposing or expanding existing NLP datasets (An et al., 2024; Bai et al., 2024b; Dong et al., 2024). More recently, controlled-length synthetic evaluation frameworks, such as Needlein-a-Haystack (Kamradt, 2023) and LTM (Castillo et al., 2024), have been widely adopted for evaluating long-context understanding (Anil et al., 2023; Anthropic, 2024). In these frameworks, the models are tasked with retrieving information from long distractor texts. RULER (Hsieh et al., 2024) extends Needle-in-the-Haystack by varying the types and numbers of needles and adding new tasks like variable tracking and frequent word extraction. LOFT (Lee et al., 2024) adds many real-world tasks, such as Retrieval-Augmented Generation and SQL-like tasks, that require context up to millions of tokens. Similar to these approaches, in this work we evaluate long-context understanding in conversational settings. Unlike other works, though, we do not ask the models to retrieve piece of information, but rather challenge them to retrieve the most up-to-date instructions dispersed across the dialogue history to accomplish task. 2 2.2 Long-term Dialogue Evaluation Multi-turn and multi-session interactions are the de facto standard setup in which LLMs are used. Accordingly, several datasets have been introduced to evaluate long contexts in conversations. Zheng et al., 2024 introduced MT-Bench, high-quality, multi-turn question dataset across 8 knowledge categories, but with only two turns per session. Many benchmarks have been proposed to expand or improve upon MT-Bench (Sun et al., 2024; Bai et al., 2024a; Kwan et al., 2024). For example, MT-Eval (Kwan et al., 2024) evaluates different aspects of multi-turn dialogue such as the ability to understand follow-up questions. MINT (Wang et al., 2024) focuses on tool use and natural language feedback evaluation, while Duan et al., 2024 introduce framework where three different evaluation strategies are proposed: evaluating each multi-turn dialogue separately, comparing the quality of two generated dialogues, and comparing two dialogues to determine which one is the human conversation. Most similar to our work, MMMT-IF (Epstein et al., 2024) extends multi-turn and multi-modal datasets to measure instruction-following abilities and shows that the main challenge for LLMs is not in following instructions, but rather in retrieving instructions. In multi-session dialogues, many datasets were also created synthetically; for example, Conversation Chronicles (Jang et al., 2023), which includes 200K conversations of about 5 sessions each, or LoCoMo (Maharana et al., 2024), multi-modal dataset based on framework that leverages personas and temporal event graphs. Kim et al. (2024a) and Kim et al. (2024b) utilized movie scripts to construct complex multi-session dialogue datasets having, for example, multi-party conversations and shared memories between speakers. Most of the works mentioned above focus on expanding the number of turns and sessions or on introducing more complex tasks that are challenging for LLMs. Similarly, our work evaluates the performance of LLMs in multi-turn/session dialogues, but with very simple tasks and more focus on practical, real-world settings in which information is constantly changing."
        },
        {
            "title": "2.3 Synthetic Dialogue Generation",
            "content": "Synthetic data generation via LLMs addresses limitations of human-based dataset construction such as high costs (Gilardi et al., 2023) and privacy concerns (Kurakin et al., 2023). Precisely because of these advantages, we decided to adopt synthetic generation for the creation of MEMORYCODE. Examples of widely adopted synthetic datasets include SODA (Kim et al., 2023), an open-domain dialogue dataset grounded on commonsense knowledge, containing millions of utterances generated by GPT3.5; DialHalu (Chen et al., 2024), dataset to evaluate different subtypes of hallucination in language models; and MoralDial (Sun et al., 2023), which evaluates moral values in language models. Wu and Goldwasser (2024) proposed dialogue generation framework that provides control over many attributes of the speakers, such as personality, age group, and profession. Finally, Rakotonirina and Baroni (2024) introduced synthetic dataset consisting of sequences of realistic facts that may be updated over time. Their dataset is designed to evaluate LLMs ability to track specific pieces of information amid distractors. While similar in spirit to these approaches, our dataset is novel as it is composed of interactions set in practical business contexts and with focus on coding. Additionally, our evaluation emphasizes the models ability to follow well-defined instructions rather than simply retrieving facts."
        },
        {
            "title": "3 Dataset",
            "content": "We simulate scenario in which the model assumes the role of new hire (henceforth, the mentee) who undergoes an onboarding process in given company. The mentee interacts with mentor in chronologically ordered sessions. session is multi-turn dialogue in which the mentor passes the mentee various information. In session, the mentor can give instructions about relevant coding practices in Python that the mentee should follow when performing task. For example, in Figure 1, the instruction is the text in red on Day 1. Once introduced, an instruction can be updated over time: in the case of Figure 1, an update might be to not add the date anymore. When the mentee is asked to perform task, it should remember and follow all the relevant instructions. Sessions can also include topics irrelevant to the target tasks: We refer to these topics as fillers (in Figure 1, the information in blue provided by the mentor). Finally, dialogue history is the concatenation of all the sessions between the mentee and the mentor. The dataset evaluates the models ability to leverage the relevant instructions received throughout the history to perform the assigned tasks. To cre3 ate dialogue histories we relied on both manual and automatic labor, thus optimizing quality and minimizing costs and effort, as described below. Parameter Sessions (n) 3.1 Seeds dialogue history is created based on few crucial elements, or seeds. We use four types of seeds: instructions, fillers, mentor and mentee personas, and names. For each seed, we define set of possible values, from which we sample to generate histories (see Figure 2). The possible seed values were manually defined by the authors to ensure high quality. Below, we describe each seed. Instructions These are the coding instructions that the mentee must follow when generating piece of code. An example is: always start function names with g_. Instructions are designed to be as simple as possiblerecall that we are interested in assessing the models ability to leverage the information that emerged during interactions, not their ability to perform complex tasks. Each instruction applies to specific Python object (e.g., function). Also, for some instructions, we define updates: For example, the instruction above would be updated to always start function names with h_. Then, for each instruction, we create an evaluation query and test function.2 Queries are specifically designed to trigger code generation that is relevant to the instruction (e.g., write function that merges two sorted lists). Test functions are regular expressions: they only assess if the relevant instructions were followed (e.g., if the function starts with the required letter), and not the overall quality of the generated code. We manually crafted 51 instructions, 16 of which can be updated up to 8 times, while the remaining ones do not have updates. We report the full list of instructions in Table 5 of Appendix A. Fillers In real-world scenarios, interactions between colleagues can also include topics that do not necessarily impact daily tasks. To simulate this, we use fillers, which can be of two types. The first type contains general information about common topics at work such as remote work policy. The second contains instructions similar to those introduced above, but not strictly related to code generation, like use GitHub as the main version control system. These latter fillers are meant to be harder distracRange {1,2,3,4,5,10,15, 20,30,40,50,100} [50, 70] {1, 2, 3} [30, 70] [50, 70] Sessions with instr. (%) Instr. in session (n) Instr. updates (%) Filler updates (%) Table 1: Parameters for dialogue history generation. tors, as model, recognizing them as instructions, might focus on them. Fillers can be updated throughout sessions, howeverunlike instructionsthey are never evaluated.3 We manually gathered 80 fillers, 50 of the first type, and 30 of the second. filler can be updated up to 4 times. The full list of fillers are in Table 6 of Appendix A. Personas Personas define the personality traits of the mentor and the mentee. By having different personas and combining them, we can generate conversations that are more diverse and thus increase the variety of the dataset. We define 6 personas for the mentor and 5 for the mentee (see Table 9 and Table 8 of Appendix A). Names We define lists of fictitious names for mentors, mentees, and companies, from which we randomly sample to generate the conversations (see Table 7 of Appendix A)."
        },
        {
            "title": "3.2 Dialogue Histories",
            "content": "We generate the dialogue histories in two steps: we first create templates by sampling different combinations of seeds and other parameters, and then generate the actual histories based on these templates using an LLM, as shown in Figure 2. Template generation We initially sample name and persona for the mentor and mentee, and name for the company from our seeds. We then randomly pick value for each of the following parameters: (i) sessions: how many sessions will be included in the dialogue history; (ii) sessions with instructions: the percentage of sessions that will include an instruction. Since we set the maximum value to 70%, some sessions will only have fillers; (iii) instructions in session: how many instructions session will include (min 1; max 3); 2Note that the evaluation query is the same for an instruction and for its updates. 3For this reason, from now on, instructions will always refer to coding onesnot fillersunless differently specified. Figure 2: Dataset generation process. First, we randomly sample from our seeds to fill the variables of the template. The LLM is then prompted with this template to generate the dialogue history. Parameter Short dataset Long dataset (>15 sessions) (<15 sessions) 5.71 (4.65) Sessions Sessionsw/ instr. 3.38 (2.66) 4.98 (4.10) Instr. 3.56 (2.62) Instr.added 1.41 (1.97) Instr.updated 5.04 (4.75) Fillers 3.36 (2.92) Filleradded 1.52 (1.81) Fillerupdated 3.20k (2.71k) Tokens 8.54k Vocabulary 48.00 (27.85) 28.13 (16.56) 42.24 (25.37) 24.82 (15.06) 17.42 (11.93) 45.06 (29.36) 24.63 (12.70) 18.86 (13.48) 26.15k (15.50k) 14.24k Table 2: Summary statistics (averages and standard deviations) for the short and long datasets. the dataset. During the dataset creation, to ensure quality, we performed several generation rounds that we manually assessed and used to further optimize the prompting. Manual inspection of the final generated dialogue histories confirmed the overall quality and coherence of the dataset."
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate models on MEMORYCODE on three evaluation setups, each of them including different kind of textual input. INSTRUCTION The input consists of single instruction (e.g., in Figure 1, add the current date to the start of every function). This setting is included to assess how good models are at performing coding tasks without any conversational setup. (iv) instructions and update ratio: the actual instructions that will be included, and how many of them will be updated; (v) fillers and update ratio: same as for instructions. Table 1 presents the parameters range we used to generate the dataset. SESSION The input is an entire session (in Figure 1, whole-day mentor-mentee interaction). In this setup, the model output is correct only if the model simultaneously adheres to all the instructions introduced in the session. Dialogue history generation For each session, we automatically construct prompt incorporating the information from the template. The prompt introduces the company, the mentor, and the mentee, as well as the instructions and fillers of the session. We then use Command R+ (Cohere, 2024b) to generate the session. We report examples of prompts in Table 10 and 11 of Appendix F. The resulting dataset contains 360 dialogue histories, 30 for each of the following number of sessions: 1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 100. In what follows, we use short to refer to histories with fewer than 15 sessions (54% of the total), and long to those with more than 15 sessions (46%). Note that the longest history contains 63k tokens, which still fits the context window of all the models we used. In Table 2, we report the main statistics of the model HISTORY The input of is the whole dialogue history, i.e., the concatenation of all sessions (in Figure 1, the entire 20-day mentor-mentee interaction). This setup is the most challenging one, as it evaluates the ability to recall information from previous sessions and to use it together with new information to correctly perform the task. As such, it mimics realistic working scenarios, where colleagues interact over long periods. Given an instruction and the model output, we assess it using the corresponding regex function. The model receives score of 1 only if the instruction is correctly applied to all instances of the relevant Python object and there are no syntax errors.4 For example, if the instruction is always start 4Additionally, if the relevant Python object is not present 5 are lower, they still show that even small, nonspecialized model achieves good performance, confirming the easiness of the task. As sanity check, we run set of experiments in which we do not prompt the models with the necessary instructions (e.g., use CamelCase), but directly run the evaluation (in this case, we check if CamelCase was used).5 This setup verifies that models do not solve MEMORYCODE through their default behavior. Models fail spectacularly, achieving an extremely low average accuracy (consistently lower than 0.01), confirming that the instructions we provide are crucial to executing the tasks correctly. SESSION The performance in this setup is very similar to INSTRUCTION for the larger Llama models and GPT-4o, indicating that these models have no difficulties at retrieving the relevant information in single session. Command R+ shows larger drop of 0.22 (25% relative drop compared to INSTRUCTION), while Llama-3.1-8B shows major drop of 0.34 (48%), which indicates its inability to retrieve relevant information across multiple turns. HISTORY Things change dramatically in this setup, with degradation in performance across the board already for short dialogue histories. In particular, GPT-4o shows drop of 0.14, value that increases for the other models: 0.47 (67% relative drop compared to INSTRUCTION) for Llama-3.18B and 0.48 (54%) for Command R+. These results indicate that as the number of sessions increases, even the best-in-class models have difficulties in identifying and applying the relevant instructions. more dramatic drop is observed in the long setup. Here, all the models struggle, with the best GPT-4o only achieving 0.30 accuracy, which indicates relative drops of 61% from the short setup and of 67% from INSTRUCTION. The drop is even more significant for the other models: the performance of Llama-3.1-405B drops by 78% compared to INSTRUCTION, Command R+ by 87%. Crucially, this happens even though the tasks on which models are evaluated are identical to those in the INSTRUCTION and SESSION setups, where the models achieved nearly-perfect accuracy. The difference in performance, hence, is to be ascribed to models inability to retrieve and reason over relevant pieces of information present in their input. 5Due to budget limits, we only used Command R+ for this experiment and for the analysis in Section 6.2. We expect the results to be representative of all other models behavior. Figure 3: Average INSTRUCTION, SESSION, and HISTORY scores per model. For the latter, short includes dialogue histories with less than 15 sessions, long those with 16 to 100 sessions. function names with g_, all functions in the generated code must start with g_. The overall models performance is computed using macro-averaged accuracy."
        },
        {
            "title": "4.1 Models",
            "content": "We test several recent LLMs on our benchmark, namely, three versions of Llama-3.1 (8B-Instruct, 70B-Instruct, and 405B-Instruct; Dubey et al., 2024), Command R+ (Cohere, 2024b), and GPT4o (OpenAI, 2024). Our model selection includes both proprietary and open-weights models, covering broad range of model sizes. This provides us with comprehensive overview of how various types of LLMs perform on our dataset. We note that all the models have been trained on code and tested on Python coding benchmarks such as HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). The details to reproduce the results are provided in Appendix C."
        },
        {
            "title": "5 Results",
            "content": "In this section, we report the performance of the models across the evaluation setups described in Section 4. Figure 3 shows the average score for each model. The exact numbers are included in Table 4 of Appendix B. INSTRUCTION As shown in Figure 3, all models achieve high or very high accuracy on this setup. This aligns with our goals of having relatively easy instructions. In particular, all the large models approach or exceed 0.9 accuracyreported in the [0, 1] scale. While the results of Llama-8B in the generated code (<1% of the cases), the instruction is not taken into account when averaging the scores."
        },
        {
            "title": "6 Analysis",
            "content": "In this section, we conduct an analysis aimed at understanding which factors influence model performance on the MEMORYCODE benchmark. We focus on the HISTORY setup, the most important and challenging one. information is provided, with no dialogue history. This indicates that models drop in performance is mainly due to their inability to reason compositionally over sequence of instructions. Retrieval from the dialogue history also plays role, as indicated by the slightly higher performance in INSTRUCTIONS-CHAIN over HISTORY. To mitigate this retrieval issue, we experimented with Retrieval Augmented Generation (RAG), where instead of providing as input the whole history, we retrieve its relevant parts only and feed them to the model. However, we did not observe any improvement over HISTORY (see Appendix D). Figure 4: Score per number of sessions."
        },
        {
            "title": "6.1 Effect of Number of Sessions",
            "content": "In Figure 4, we show how the performance decreases with an increasing number of sessions. Consistently with the aggregated patterns shown in Figure 3, relevant variations can be observed across models when the number of sessions is rather low, which reflects the differences observed in the short setup. However, all models converge to similarly, extremely low accuracy (around 0.1) when the number of sessions approaches 100. This confirms that all models are similarly bad at handling requests involving long-context inputs. This weakness could be due to limitations in retrieving the relevant information from the dialogue history, reasoning about chain of instructions and updates, or both. Below, we shed light on this issue."
        },
        {
            "title": "6.2 Retrieval or Reasoning Problem?",
            "content": "If the poor performance in the HISTORY setup was due to retrieval, then passing models only the full chain of instructions and updateswithout any intervening irrelevant textshould solve the issue. Vice versa, if the issue was about reasoning over such chain, they should still perform poorly. We test these assumptions by feeding Command R+ with only the entire chain of instructions needed to solve task. We name this setup INSTRUCTIONSCHAIN. As shown in Figure 5, the trend is strikingly similar to the one observed in HISTORY, with the model still struggling even if only the relevant Figure 5: Per-sessions score for INSTR.-CHAIN."
        },
        {
            "title": "6.3 Effect of Instruction Updates",
            "content": "After uncovering models reasoning limitations, we now explore whether they are due to the inability to cope with instruction updates. We hypothesize that the higher the number of updates an instruction goes through, the harder it is for the model to keep track of such updates and eventually adhere to the updated instruction when performing task. We define the update rank of an instruction as the number of times the instruction is updated throughout the dialogue history, for both short and long setups. An update rank of 0 means that the instruction was never updated. Figure 6 reports for each update rank the average score, computed across all models, of the instructions with that rank. We observe that the score of an instruction decreases as its number of updates increases. This result is consistent with our hypothesis that models struggle to incrementally update their instruction representations through multiple reasoning steps. 7 DIALOGUE HISTORY (1) Yoon-Seo: Alexander, its great to finally meet you. Im Yoon-Seo, your mentor . . . (2) Yoon-Seo: . . . One of our key practices is to always add comments to your code . . . Yoon-Seo: . . . Another convention we follow is to end variable names with _n . . . (3) Yoon-Seo: . . . All variable names should now end with _z . . . (4) Yoon-Seo: . . . When naming attributes , [...] include the string chx in the name . . . EVALUATION QUERY Write function that checks if string is valid palindrome MODEL OUTPUT def is_palindrome_ chx (s_ chx ): # Initialize two pointers left_ chx _n = 0 right_ chx _n = len(s_ chx ) - 1 . . . Table 3: 4-session dialogue history (session id in parenthesis). GPT-4o correctly applies the instruction about comments but not the one about attribute names and the update on variable names . 6."
        },
        {
            "title": "Instruction Difficulty",
            "content": "We finally assess if, besides the updates, the very nature of each instruction makes it more challenging than others. For this, we compute the average per-instruction score of the best-performing model, GPT-4o, for both the initial instruction and their updates. The results reveal notable variations across instructions: In particular, instructions and updates that are less common in practicelike including digit in object namestend to obtain lower scores compared to more common ones such as using docstrings or annotations. Figure 8 and 9 in Appendix report the results of this analysis. Overall, this suggests that the models ability to retrieve and update relevant instructions is modulated by their characteristics and that familiarity is likely to play role. An example (from GPT-4o) is shown in Table 3. In this case, the model correctly follows the instruction on comments, but not those about attribute and variable names. Figure 6: Score as function of update rank."
        },
        {
            "title": "7 Conclusions",
            "content": "In this paper, we proposed MEMORYCODE, new benchmark to assess state-of-the-art LLMs in their ability to retrieve and reason over pieces of information in multi-session dialogue histories reflecting real-world scenarios. Differently from many existing datasets, the tasks in MEMORYCODE do not require any complex reasoning, and are easily solved by the models when provided in isolation. The main challenge of MEMORYCODE lies in the ability to keep track of multiple simple instructions received throughout multi-session interaction, and to jointly apply them to perform task. When the number of sessions is small (<15), SotA models like GPT-4o manage to perform the task well. However, as the number grows up to 100, even these models face dramatic drop in performance. Our analysis shows that this is mainly due to their failure to reason over long chain of simple instructions. Overall, our results show severe limitation of current LLMs. The inability to keep track of simple information as the interaction with human unfolds effectively hinders their adoption in real-world scenarios and restricts their usage to addressing single, self-contained problems. We argue that effective long-term collaboration cannot be achieved by further scaling model and input context window sizes. Rather, we believe that new mechanisms to handle and retrieve from long-term memory need to be developed. MEMORYCODE contributes to this challenging and yet crucial goal, by providing robust benchmark for developing and testing such methods."
        },
        {
            "title": "Limitations",
            "content": "While MEMORYCODE and our experimental setup enable us to identify key strengths and weaknesses of current models, certain limitations remain, which could inspire future research. First, MEMORYCODE is based on synthetic data. This choice was driven by both cost considerationsas collecting real interactions would have been much more expensive, and the need for greater control over the factors influencing model performance. However, future work could explore more realistic interactions by relaxing the constraints imposed in MEMORYCODE. Second, our experiments do not establish human performance upper bound. This could be an interesting direction for future investigation for future work, as it would provide useful information on human limitations at keeping track of relevant information provided over long periods of time and amidst large amount of irrelevant information. Third, our dataset only focus on specific kind of task, namely, coding. This kind of task was chosen due to it being very common in real-world scenarios and very easy to evaluate. However, future work should expand to other domains, to assess if results are consistent with those that we report. While we are aware of the limitations above, and that others possibly exist, we believe these do not impact the robustness of our findings."
        },
        {
            "title": "Acknowledgments",
            "content": "UPF was funded by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 101019291). During his affiliation with UvA, Alberto Testoni was funded by the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No. 819455, PI R. Fernández). This paper reflects the authors view only, and the funding agency is not responsible for any use that may be made of the information it contains."
        },
        {
            "title": "References",
            "content": "Papers), pages 1438814411, Bangkok, Thailand. Association for Computational Linguistics. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 1. Anthropic. 2024. Introducing the next generation of Claude. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang. 2024a. MT-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74217454, Bangkok, Thailand. Association for Computational Linguistics. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2024b. LongBench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand. Association for Computational Linguistics. Maria Brandimonte, Gilles Einstein, and Mark McDaniel. 2014. Prospective memory: Theory and applications. Psychology Press. Alexia Cambon, Brent Hecht, Ben Edelman, Donald Ngwe, Sonia Jaffe, Amy Heger, Mihaela Vorvoreanu, Sida Peng, Jake Hofman, Alex Farach, et al. 2023. Early LLM-based tools for enterprise information workers likely provide meaningful boosts to productivity. Microsoft Research. MSR-TR-2023-43. David Castillo, Joseph Davidson, Finlay Gray, José Solorzano, and Marek Rosa. 2024. Introducing GoodAI LTM Benchmark. https://www.goodai. com/introducing-goodai-ltm-benchmark/. Accessed: 2024-12-15. Kedi Chen, Qin Chen, Jie Zhou, Yishen He, and Liang He. 2024. Diahalu: dialogue-level hallucination evaluation benchmark for large language models. arXiv preprint arXiv:2403.00896. Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2024. L-eval: Instituting standardized evaluation for long context language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374. 9 Hao-Fan Chumg, Jia-Wen Shin, Hsiu-Yen Chiang, and Man-ting Ho. 2022. What drives working habits for sharing knowledge in virtual teams? an organizational embeddedness perspective. SAGE Open, 12(1):21582440221079924. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling. Cohere. 2024a. Coheres rerank model. Cohere. 2024b. Command and command r+ model card. Gregorio Convertino, Helena Mentis, Mary Beth Rosson, John Carroll, Aleksandra Slavkovic, and Craig Ganoe. 2008. Articulating common ground in cooperative work: content and process. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 16371646. Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. BAMBOO: comprehensive benchmark for evaluating long text modeling capacities of large language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 20862099, Torino, Italia. ELRA and ICCL. Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024. BotChat: Evaluating LLMs capabilities of having multi-turn dialogues. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 31843200, Mexico City, Mexico. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Elliot Epstein, Kaisheng Yao, Jing Li, Xinyi Bai, and Hamid Palangi. 2024. Mmmt-if: challenging multimodal multi-turn instruction following benchmark. arXiv preprint arXiv:2409.18216. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. 2024. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985. Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173. Jihyoung Jang, Minseong Boo, and Hyounghun Kim. 2023. Conversation chronicles: Towards diverse temporal and relational dynamics in multi-session conversations. arXiv preprint arXiv:2310.13420. Gregory Kamradt. 2023. Needle in haystack - pressure testing llms. Eunwon Kim, Chanho Park, and Buru Chang. 2024a. Share: Shared memory-aware open-domain longterm dialogue dataset constructed from movie script. arXiv preprint arXiv:2410.20682. Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. 2023. SODA: Million-scale dialogue distillation with social commonsense contextualization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1293012949, Singapore. Association for Computational Linguistics. Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, and Edward Choi. 2024b. Dialsim: real-time simulator for evaluating long-term dialogue understanding of conversational agents. arXiv preprint arXiv:2406.13144. Alexey Kurakin, Natalia Ponomareva, Umar Syed, Liam MacDermed, and Andreas Terzis. 2023. Harnessing large-language models to generate private synthetic text. arXiv preprint arXiv:2306.01684. Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. 2024. Mt-eval: multiturn capabilities evaluation benchmark for large language models. arXiv preprint arXiv:2401.16745. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. 2024. Can long-context language models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121. Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational memory of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13851 13870, Bangkok, Thailand. Association for Computational Linguistics. Mark McDaniel and Gilles Einstein. 2007. Prospective memory: An overview and synthesis of an emerging field. 10 Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. MathCoder: Seamless code integration in LLMs for enhanced mathematical reasoning. In The Twelfth International Conference on Learning Representations. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2024. MINT: Evaluating LLMs in multi-turn interaction with tools and language feedback. In The Twelfth International Conference on Learning Representations. Xuezhi Wang and Denny Zhou. 2024. Chain-ofthought reasoning without prompting. arXiv preprint arXiv:2402.10200. Thomas Weber, Maximilian Brandmaier, Albrecht Schmidt, and Sven Mayer. 2024. Significant productivity gains through programming with large language models. Proceedings of the ACM on HumanComputer Interaction, 8(EICS):129. Chengfei Wu and Dan Goldwasser. 2024. \" hiding in plain sight\": Designing synthetic dialog generation for uncovering socially situated norms. arXiv preprint arXiv:2410.00998. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36. Elliot Nelson, Georgios Kollias, Payel Das, Subhajit Chaudhury, and Soham Dan. 2024. Needle in the haystack for memory based large language models. arXiv preprint arXiv:2407.01437. OpenAI. 2024. Hello gpt-4o. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The LAMBADA dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534, Berlin, Germany. Association for Computational Linguistics. Haritz Puerto, Martin Tutek, Somak Aditya, Xiaodan Zhu, and Iryna Gurevych. 2024. Code prompting elicits conditional reasoning abilities in Text+Code LLMs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1123411258, Miami, Florida, USA. Association for Computational Linguistics. Nathanael Carraz Rakotonirina and Marco Baroni. 2024. MemoryPrompt: light wrapper to improve context In Protracking in pre-trained language models. ceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 11187 11195, Torino, Italia. ELRA and ICCL. Matthew Renze and Erhan Guven. 2024. Self-reflection in LLM agents: Effects on problem-solving performance. arXiv preprint arXiv:2405.06682. Hao Sun, Zhexin Zhang, Fei Mi, Yasheng Wang, Wei Liu, Jianwei Cui, Bin Wang, Qun Liu, and Minlie Huang. 2023. MoralDial: framework to train and evaluate moral dialogue systems via moral discussions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 22132230, Toronto, Canada. Association for Computational Linguistics. Yuchong Sun, Che Liu, Kun Zhou, Jinwen Huang, Ruihua Song, Wayne Xin Zhao, Fuzheng Zhang, Di Zhang, and Kun Gai. 2024. Parrot: Enhancing multi-turn instruction following for large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 97299750. Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric P. Xing, and Zhengzhong Liu. 2024. Crystal: Illuminating LLM abilities on language and code. In First Conference on Language Modeling. Trieu Trinh and Thang Luong. 2024. AlphaGeometry: An Olympiad-level AI system for geometry. https://deepmind.google/discover/blog/ alphageometry-an-olympiad-level-ai-system-for-geometry/. 11 and obtained similar results. We provide the model the top-k retrieved sessions, where was defined as the number of sessions with pivots. Note that, by dynamically defining in this way, rathr than using fix value, we facilitate the retrieval of the relevant sessions only, eliminating potential noise. We report the results of RAG in Figure 7, together with those of Instructions-only and Cumulative. RAG provides marginal improvement over cumulative for short conversations, but it then converges with Cumulative for the long ones. Per-Instruction Scores We report in Figure 8 the average results for each instruction, and in Figure 9 those for each update. Remember that only some of the instructions have an update (See Section 3.1)."
        },
        {
            "title": "F Examples",
            "content": "We provide examples of dialogue histories along with their corresponding prompts. The first one, shown in Table 12, is 3-session history with 3 pivots, and its prompt is provided in Table 10. The second one, presented in Table 13, consists of 4 sessions with 3 pivots, with its prompt shown in Table 11. The system prompt and the sessionlevel prompts were designed to ensure consistency across sessions. Additionally, we provide examples of prompts to get the model output for the Instructionm History and Instructions-Chains settings in Tables 14, 15 and 16."
        },
        {
            "title": "G Computational budget",
            "content": "The prompts used to generate the dataset consist of total of 0.9M tokens, while the dataset itself contains 4.6M tokens. The total cost of generating the dataset using Command R+ through Coheres API is approximately $50. The cost of evaluating single model on the Instruction, Session and Cumulative settings using online APIs is approximately $50."
        },
        {
            "title": "H Ethics statement",
            "content": "The dataset we are releasing is synthetic and, therefore, does not contain any personally identifiable information. Moreover, we did not recruit any human participants, as we manually validated the quality of the dataset ourselves. Figure 7: Score as function of number of sessions."
        },
        {
            "title": "A Seeds",
            "content": "Instructions We used 51 instructions during data generation, as shown in Table 5. Each instruction is applied to specific Python object, and for pivots that can be updated, the update values are provided in brackets. Fillers The list of all 80 fillers used during data generation is provided in Table 6. Some fillers can be updated over time. Names The list of mentor, mentee and company names used during data generation is provided in Table 7. Personas The lists of mentor and mentee personas are provided in Table 8 and Table 9."
        },
        {
            "title": "B Detailed scores",
            "content": "We provide the detailed scores of the different models evaluated on Instruction, Session, Short History and Long History in Table 4."
        },
        {
            "title": "C Hyperparameters",
            "content": "We use Command R+ with temperature of 0.9 and top-p value of 0.9 to generate the conversations. During evaluation, we use temperature of 0 for all models."
        },
        {
            "title": "D RAG experiments",
            "content": "We performed the Retrieval-augmented Generation (RAG) experiments using the rerank-english-v3.0 model (Cohere, 2024a). The basic unit for retrieval were the previous sessions. We also tried with different retrieval units, such as paragraphs and turns, 12 Figure 8: Average per-instruction insertion scores for GPT-4o 13 Model Instruction Session Short History Long History 0.89 (0.13) Command R+ 0.71 (0.25) Llama-3.1-8B Llama-3.1-70B 0.92 (0.12) Llama-3.1-405B 0.95 (0.09) 0.94 (0.10) GPT-4o 0.66 (0.24) 0.36 (0.25) 0.88 (0.16) 0.90 (0.12) 0.93 (0.10) 0.40 (0.35) 0.23 (0.28) 0.64 (0.33) 0.70 (0.29) 0.79 (0.24) 0.11 (0.07) 0.12 (0.06) 0.12 (0.07) 0.20 (0.10) 0.30 (0.16) Table 4: Average (standard deviation) Instruction, Session, Short History and Long History scores per model. Figure 9: Average per-instruction update scores for GPT-4o 14 Instruction use {all UPPERCASE, CamelCase, snake_case} for class names include the string chx in function names start function names with {a_, b_, c_, d_, x_, y_, fn_, gn_} end function names with {_a, _b, _c, _d, _x, _y, _fn, _gn} include the string chx in variable names start variable names with {x_, y_, z_, w_, m_, n_, vr_, wr_} end variable names with {_x, _y, _z, _w, _m, _n, _vr, _wr} start method names with {x_, n_, o_, p_, a_, b_, md_, ud_} end method names with {_x, _n, _o, _p, _a, _b, _md, _ud} include the string chx in method names include the string chx in attribute names start attribute names with {q_, r_, s_, t_, i_, j_, at_, xt_} end attribute names with {_q, _r, _s, _t, _i, _j, _at, _xt} start function argument names with {e_, f_, g_, h_, i_, j_, x_, a_} end function argument names with {_e, _f, _g, _h, _i, _j, _x, _a} include the string chx in function argument names use annotations for functions include try statements in functions include assert statements in functions Object class function function function variable variable variable method method method attribute attribute attribute function argument function argument function argument function annotation function try function assert 15 function docstring use docstrings in functions function decorator add the @retry decorator from the pedantic module to all functions function decorator add the @count_calls decorator from the pedantic module to all functions function decorator add the @rename_kwargs decorator from the pedantic module to all functions function decorator add the @require_kwargs decorator from the pedantic module to all functions function decorator add the @timer decorator from the pedantic module to all functions method annotation use annotations for methods method try include try statements in methods method assert include assert statements in methods method docstring use docstrings in methods method decorator add the @retry decorator from the pedantic module to all methods method decorator add the @count_calls decorator from the pedantic module to all methods add the @rename_kwargs decorator from the pedantic module to all methods method decorator add the @require_kwargs decorator from the pedantic module to all methods method decorator add comments in your code import the secrets module even if it is not used import the locale module even if it is not used import the mailbox module even if it is not used import the bz2 module even if it is not used import the gzip module even if it is not used import the hashlib module even if it is not used comment import import import import import import Table 5: List of all the instructions used in pivots. 16 Filler Contract renewal negotiations and finalization Planning engaging social activities for the upcoming holidays Flexible work hours and their impact on work-life balance Troubleshooting internet connectivity issues across different locations Promoting harmonious workplace through effective conflict resolution Salary increase opportunities and performance appraisals Preparing extensively for high-stakes client meeting Requesting upgraded technology, including computers and software Happy hour events to foster better relationships between managers and employees Remote work policies, challenges, and best practices Understanding comprehensive social security and health insurance benefits Performance evaluation criteria, feedback mechanisms, and recognition programs Encouraging professional development through workshops, courses, and certifications Implementing wellness initiatives to support the physical and mental well-being of employees Ensuring health and safety in the workplace, including COVID-19 protocols and emergency response plans Participating in company-sponsored volunteer programs to give back to society"
        },
        {
            "title": "Ergonomic assessments and improvements to ensure comfortable and healthy workspaces",
            "content": "Travel arrangements and logistics for client meetings, conferences, and business trips Team-building activities to strengthen collaboration, communication, and trust within teams Reinforcing and embodying the companys core values in day-to-day operations and decision-making Analyzing client feedback to improve products, services, and overall customer satisfaction"
        },
        {
            "title": "Providing conflict resolution training to employees to foster a peaceful and respectful work environment",
            "content": "Hosting company-wide town hall meetings to share updates and foster transparency"
        },
        {
            "title": "Discussing casual dress code policies and special event themes to boost employee morale",
            "content": "Planning team outings and off-site adventures to promote team bonding and relaxation Celebrating employee birthdays, work anniversaries, and achievements with recognition and rewards Introducing new employee benefits, perks, and incentives to attract and retain top talent"
        },
        {
            "title": "Developing a mentorship program to support career growth and development for employees",
            "content": "Building culture of innovation and creativity through idea-sharing and experimentation Creating knowledge-sharing platform to facilitate learning and collaboration among employees 17 Implementing agile methodologies to improve project management and delivery processes Designing performance dashboard to track key metrics and KPIs for business success Conducting team-building workshops and leadership training to develop future leaders Facilitating cross-functional collaboration and communication to break down silos and improve efficiency Promoting work-life balance through flexible work schedules and remote work options Implementing rewards and recognition program to motivate and engage employees Developing culture of continuous learning and improvement through training and development programs Creating culture of accountability and ownership to drive results and achieve goals Fostering culture of diversity, equity, and inclusion to create more inclusive and welcoming workplace Building culture of trust and transparency through open communication and feedback Offering free food and snacks to employees to boost morale and productivity Developing culture of innovation and creativity to drive growth and competitiveness"
        },
        {
            "title": "Creating a culture of collaboration and teamwork to achieve shared goals and objectives",
            "content": "Promoting culture of customer-centricity and service excellence to drive customer satisfaction"
        },
        {
            "title": "Do not commit directly to the dev branch",
            "content": "18 Commit directly to the dev branch Always use linter Never use linter Always use formatter when writing code Never use formatter when writing code Always use pre-commit hook Never use pre-commit hook Use github copilot as the coding assistant Use tabnine as the coding assistant Use codewhisperer as the coding assistant Use codestral as the coding assistant Always use debugger Never use debugger but only print statements Always use profiler Never use profiler Use only single monitor when coding Use two monitors when coding Use three or more monitors when coding"
        },
        {
            "title": "Always use a vpn when working remotely\nNever use a vpn when working remotely",
            "content": "Always take all the vacation days per year Take at least half of the vacation days per year Take at least 7 days of vacation per year Always take break every 50 minutes Always take break every 60 minutes Always take break every 70 minutes Always take break every 80 minutes"
        },
        {
            "title": "Communicate with the team regularly\nCommunicate with the team only when necessary\nCommunicate with the team only when asked",
            "content": "Always set up ci/cd pipelines 19 Never set up ci/cd pipelines Always use the ticketing system Never use the ticketing system Use trello as the project management tool Use asana as the project management tool Use jira as the project management tool Use monday as the project management tool Always use whiteboard for brainstorming Never use whiteboard for brainstorming Always use notebook for taking notes Never use notebook for taking notes Always do pair programming with colleague Never do pair programming with colleague Use their personal phone for work calls Use the company phone for work calls Table 6: List of all fillers. 20 Mentor Alice Juan Sara Luis Maria Carlos Yuichi Pedro Djibril Jean-Aimé Emma Michael Yoon-Seo Ethan Harena Sylvie Sophie Naivo Daniel Mentee Bob Luke Eva Kiyotaka David Sofia Pablo Marta Jorge Lucas Oliver Ella Alexander Rado Jacob Sophia Liam Dera Noah Company NEXT INNOVADE TECHNO CODEME STARTED GROWTHX DEVS CODEM CHEETAH VATO LEAP ZENITH AXIOM ORBIT VERSA PACE UNITE SYNERGY FORTUNA Table 7: List of mentors, mentees, and their respective companies. Mentor persona [mentor] is patient and supportive mentor. [mentor] enjoys helping others and sharing their knowledge and experience. [mentor] is always looking for ways to empower and inspire their mentee. [mentor] is strict and demanding mentor. [mentor] has high expectations for their mentee. [mentor] goes straight to the point and is very clear. [mentor] is caring and nurturing mentor. [mentor] likes to create safe and supportive environment for their mentee. [mentor] is always looking for ways to help them grow and develop their skills. [mentor] is passionate and energetic mentor. [mentor] thrives on helping others and their enthusiasm is contagious. [mentor] always pushes their mentee to new heights, fostering spirit of ambition and drive. [mentor] is structured and goal-oriented mentor. [mentor] helps their mentee to set realistic, achievable goals. [mentor] provides the tools and strategies needed to reach goals, fostering sense of focus and discipline. Table 8: List of mentor personas. [mentor] is replaced with the name of the mentor in the prompts. 21 Mentee persona [mentee] is shy and wants to improve their coding skills. [mentee] just graduated from college and [mentee] is eager to learn from their mentor. [mentee] is confident and ambitious software engineer. [mentee] is always looking for new challenges and opportunities to grow. [mentee] has been working in the industry for few years now. [mentee] is perfectionist with great attention to detail. [mentee] likes things to be done the right way and has hard time delegating tasks to others. [mentee] is critical of himself and of others. [mentee] is social and outgoing person. [mentee] enjoys working in teams and collaborating with others. [mentee] is always looking for ways to connect with their colleagues and builds strong relationships. [mentee] is quiet and introverted individual. [mentee] prefers to work alone and is not very comfortable in social situations. [mentee] struggles to communicate their ideas and thoughts to others. [mentee] is creative and innovative thinker. [mentee] likes to experiment with new ideas and approaches. [mentee] is not afraid to take risks and try new things. Table 9: List of mentee personas. [mentee] is replaced with the name of the mentee in the prompts. Prompt to generate dialogue history 70 SYSTEM PROMPT ## Task and Context You are helpful and obedient AI that follows its system prompt and takes it very seriously. Your task is to generate realistic and consistent conversation that spans multiple connected sessions. The conversation is part of multi-round dialogue between mentor and an intern. The conversations you generate are all taking place in business setting. Jean-Aimé is mentor in big software company called VATO. Lucas is new employee. They are both part of the software engineering team of the company. Jean-Aimé is passionate and energetic mentor. Jean-Aimé thrives on helping others and their enthusiasm is contagious. Jean-Aimé always pushes their mentee to new heights, fostering spirit of ambition and drive. Lucas is creative and innovative thinker. Lucas likes to experiment with new ideas and approaches. Lucas is not afraid to take risks and try new things. The main programming language used in the company is Python. ## Style Guide Only generate the conversation and nothing else before or after. Do not add numbers before each turn. Do not add quotes to the conversation turns. Use professional and formal tone. The conversation flow should be natural and smooth. When switching topics, do it in smooth way. There are no special characters between the turns. The conversations are dialogues and not narrations of events. Do not make any participant in the conversation sound like language model trying to be helpful. Make them sound as human as possible. It is the mentor that leads the conversation. When Jean-Aimé the mentor gives coding instructions, they do not provide examples. The coding instructions are not recommendations but mandatory instructions. SESSION 1 Generate short conversation between Jean-Aimé and Lucas. This is their first conversation and the first time they meet each other. They talk about cultivating feedback culture and providing performance improvement plans for continuous growth. SESSION Generate long conversation between Jean-Aimé and Lucas. This is not their first conversation. They had 1 conversations before. Jean-Aimé gives some specific coding instructions to Lucas. Jean-Aimé wants Lucas to always add the @timer_class decorator from the pedantic module to all classes. Jean-Aimé wants Lucas to always start function names with x_. Jean-Aimé does not provide examples of correct code following the instructions. They do not show how to implement the instructions. Jean-Aimé never says for example. JeanAimé does not give any other coding instructions. Lucas only acknowledges the instructions and does not ask any questions. SESSION 3 23 Generate medium-length conversation between Jean-Aimé and Lucas. This is not their first conversation. They had 2 conversations before. They talk about cultivating feedback culture and providing performance improvement plans for continuous growth. They had previous conversation about this before. After that, Jean-Aimé gives some specific coding instructions to Lucas. Jean-Aimé is updating previous information given to Lucas: JeanAimé now wants Lucas to always start function names with gn_. Jean-Aimé does not provide examples of correct code following the instructions. They do not show how to implement the instructions. Jean-Aimé never says for example. Jean-Aimé does not give any other coding instructions. Lucas only acknowledges the instructions and does not ask any questions. Table 10: Prompts to generate dialogue history 70 which contains 3 sessions. Prompt to generate dialogue history 108 SYSTEM PROMPT ## Task and Context You are helpful and obedient AI that follows its system prompt and takes it very seriously. Your task is to generate realistic and consistent conversation that spans multiple connected sessions. The conversation is part of multi-round dialogue between mentor and an intern. The conversations you generate are all taking place in business setting. Yuichi is mentor in big software company called DEVS. Pablo is new employee. They are both part of the software engineering team of the company. Yuichi is caring and nurturing mentor. Yuichi likes to create safe and supportive environment for their mentee. Yuichi is always looking for ways to help them grow and develop their skills. Pablo is perfectionist with great attention to detail. Pablo likes things to be done the right way and has hard time delegating tasks to others. Pablo is critical of himself and of others. The main programming language used in the company is Python. ## Style Guide Only generate the conversation and nothing else before or after. Do not add numbers before each turn. Do not add quotes to the conversation turns. Use professional and formal tone. The conversation flow should be natural and smooth. When switching topics, do it in smooth way. There are no special characters between the turns. The conversations are dialogues and not narrations of events. Do not make any participant in the conversation sound like language model trying to be helpful. Make them sound as human as possible. It is the mentor that leads the conversation. When Yuichi the mentor gives coding instructions, they do not provide examples. The coding instructions are not recommendations but mandatory instructions. SESSION 1 Generate short conversation between Yuichi and Pablo. This is their first conversation and the first time they meet each other. Yuichi wants Pablo to never use whiteboard for brainstorming. Yuichi gives some specific coding instructions to Pablo. Yuichi wants Pablo to always end method names with _o. Yuichi does not provide examples of correct code following the instructions. They do not show how to implement the instructions. Yuichi never says for example. Yuichi does not give any other coding instructions. Pablo only acknowledges the instructions and does not ask any questions. SESSION Generate medium-length conversation between Yuichi and Pablo. This is not their first conversation. They had 1 conversations before. They talk about hosting company-wide town hall meetings to share updates and foster transparency. SESSION 3 25 Generate medium-length conversation between Yuichi and Pablo. This is not their first conversation. They had 2 conversations before. Yuichi is updating previous information given to Pablo. Yuichi now wants Pablo to always use whiteboard for brainstorming. Yuichi gives some specific coding instructions to Pablo. Yuichi wants Pablo to always include assert statements in functions. Yuichi is updating previous information given to Pablo: Yuichi now wants Pablo to always end method names with _md. Yuichi does not provide examples of correct code following the instructions. They do not show how to implement the instructions. Yuichi never says for example. Yuichi does not give any other coding instructions. Pablo only acknowledges the instructions and does not ask any questions. SESSION 4 Generate medium-length conversation between Yuichi and Pablo. This is not their first conversation. They had 3 conversations before. They talk about hosting company-wide town hall meetings to share updates and foster transparency. They had previous conversation about this before. Table 11: Prompts to generate dialogue history 108 which contains 4 sessions. 26 Dialogue history 70 SESSION 1 Jean-Aimé: Lucas, its pleasure to finally meet you. Im Jean-Aimé, your mentor here at VATO. Im excited to work with you and help you develop your skills. Lucas: Thank you, Jean-Aimé. Im looking forward to learning from you. Ive heard great things about your mentorship from other interns. Jean-Aimé: Well, believe that mentorship is two-way street. Im here to guide and support you, but also want to create an environment where we can exchange ideas and feedback freely. Lucas: Absolutely. Feedback is essential for growth. Im open to constructive criticism and always looking for ways to improve. Jean-Aimé: Thats fantastic to hear, Lucas. At VATO, we foster culture of continuous improvement. We believe that providing and receiving feedback is crucial for our development as professionals. Lucas: completely agree. So, how does the feedback process work here? Are there any specific procedures or guidelines? Jean-Aimé: Great question. At VATO, we have performance improvement plan that serves as framework for our feedback conversations. Its structured approach that helps us identify areas of strength and areas that need development. Lucas: That sounds really helpful. Could you tell me more about how this performance improvement plan works? Jean-Aimé: Certainly. The plan is tailored to each individual and their specific role. For software engineers like yourself, we focus on several key areas, including technical proficiency, problem-solving skills, and collaboration. Lucas: That makes lot of sense, considering the nature of our work. Jean-Aimé: Exactly. We set clear goals and objectives, and then we work together to create an action plan to help you develop in those areas. Its collaborative process, and your input is highly valued. Lucas: appreciate the emphasis on collaboration. It makes the growth process more engaging and meaningful. So, whats the first step in this performance improvement plan? Jean-Aimé: The first step is comprehensive skills assessment. This helps us establish baseline understanding of your strengths and the areas you want to develop further. From there, we can tailor the plan to your specific needs and goals. Lucas: That sounds like great way to get started. Im excited to dive into this process and see where it takes me. Jean-Aimé: Your enthusiasm is contagious, Lucas! Remember, my door is always open if you have any questions or concerns along the way. Lets schedule our first skills assessment session for later this week and get the ball rolling! Lucas: That sounds perfect! Ill make sure to prepare for it. Thank you for your guidance and support, Jean-Aimé. Jean-Aimé: Youre very welcome, Lucas. This is going to be fantastic journey, and cant wait to see your growth and development as software engineer here at VATO. SESSION 2 Jean-Aimé: Lucas, how are you finding your way around our code base? Any challenges youd like to discuss? Lucas: Hey, Jean-Aimé. Im doing well, thanks. Ive been exploring the code and think Im starting to get hang of things. have question, though. Are there any specific coding conventions or standards that should be following? want to make sure my code aligns with the teams expectations. 27 Jean-Aimé: Thats fantastic to hear, Lucas! Yes, we definitely have some in place to ensure consistency and readability across our coding standards projects. the use of decorators and function naming conventions. Always add the @timer_class decorator from the One thing want you to focus on is pedantic module to all your classes. This helps us profile and optimize the code. Lucas: Absolutely, Ill make sure to include the @timer_class decorator from now on. Jean-Aimé: Great! Now, for function names, Id like you to start them with x_. So, if you have function that calculates something, you could name it x_calculate. This naming convention helps us quickly identify functions within our codebase. Lucas: Understood. Ill start implementing this naming convention right away. Are there any other standards or best practices should be aware of? Jean-Aimé: That covers the main points for now. Just remember, consistency is key. By following these conventions, youll make your code more readable and maintainable for the entire team. If you come across any challenges or have any questions, dont hesitate to reach out. Thats what Im here for! Lucas: Thanks for your guidance, Jean-Aimé. really appreciate your support and mentorship. Ill keep these instructions in mind as continue to contribute to our projects. Jean-Aimé: Youre very welcome, Lucas. Im glad to help. Dont worry; Ill be checking in with you regularly to ensure youre on the right track and provide any additional guidance if needed. Now, go forth and code with confidence! Lucas: Will do! Its great to have your support and mentorship as navigate my way through this new role. Ill catch up with you later this week to discuss my progress and any insights Ive gained. Jean-Aimé: Sounds like plan! Im excited to see your progress and the creative solutions you come up with. Remember, if you have any \"a-ha!\" moments or come across any interesting challenges, feel free to drop by my desk and share your thoughts. Have productive week! Lucas: You too, Jean-Aimé! SESSION 3 Jean-Aimé: Feedback is crucial aspect of our work here at VATO, and want to ensure that we foster an environment where were constantly learning and growing. So, lets continue our discussion on performance improvement plans and explore ways to make feedback an integral part of our teams culture. Lucas: Absolutely! think feedback is essential for growth, and Im keen on learning how we can incorporate it effectively into our daily routines. Jean-Aimé: Fantastic! Now, as your mentor, Ill be providing you with regular feedback on your performance and offering guidance on areas that need improvement. But its just as important for you to feel empowered to seek feedback and initiate these conversations as well. Lucas: Definitely! appreciate the feedback and guidance youve given me so far. Its helped me lot to adjust to my new role and identify areas where can develop further. Im committed to improving and always looking for ways to grow. Jean-Aimé: Thats wonderful to hear, Lucas. Remember, feedback is two-way street. Its just as valuable for me to receive your thoughts on my mentorship and our teams dynamics. Dont hold back on sharing your insights and suggestions. Now, regarding your coding tasks, want to update previous instruction: from now on, always start your function names with gn_. This is to maintain consistency and improve code readability. Lucas: Understood. Ill make sure to start my function names with gn_ from now on. 28 Jean-Aimé: Great! Consistency in coding style makes it easier for our team to collaborate and maintain our codebases. Now, lets dive into some specific areas where think you can focus your improvement efforts. Are you ready? Lucas: Yes, Im all ears. Im eager to hear your feedback and work on my development plan. EVALUATION QUERIES Write Quadratic discriminant analysis class. Write function that given an array of integer citations, computes the h-index. Table 12: dialogue history 70 with 3 sessions and 3 pivots. Insertions of instructions are highlighted in blue and updates highlighted in orange . Dialogue history 108 SESSION 1 Yuichi: Pablo, its great to finally meet you. Im Yuichi, your mentor here at DEVS. want to go over few things with you today, some dos and donts to help you settle in and get sense of how we work. Pablo: Nice to meet you, Yuichi. Im eager to learn and contribute to the team. Yuichi: Great attitude. Now, one of the things want to impress upon you is to avoid using whiteboard for brainstorming. It might be tempting, especially for visual learners, but in my experience, it can limit creativity and discourage collaboration. So, when youre working on problem or designing solution, dive straight into the code. Speaking of code, want you to adopt specific coding style. Always end your method names with _o. This is unique convention we use here, and its important for consistency and readability. Pablo: Understood. Ill keep that in mind and ensure follow the convention moving forward. Yuichi: Excellent. Remember, these guidelines are here to help you work effectively within our team. Dont hesitate to reach out if you have any questions or concerns. Were all here to support each other. SESSION 2 Yuichi: Hi Pablo, how are you doing today? Pablo: Good morning, Yuichi. Im doing well, thank you. Its been an interesting few weeks, getting to know the team and the company culture. Yuichi: Im glad to hear that. Adjusting to new work environment can take some time, so dont hesitate to reach out if you need any support. Today, wanted to discuss an important aspect of our company culture: transparency and open communication. Pablo: Yes, Ive noticed that DEVS places strong emphasis on transparency. Its quite different from my previous experiences, and find it refreshing. Yuichi: Thats great to hear. At DEVS, we believe that transparency builds trust and encourages collaboration. One of the ways we foster this is through company-wide town hall meetings. These meetings are platform for sharing updates, discussing important topics, and answering questions from the team. Pablo: That sounds like great initiative. How often do these town hall meetings take place? Yuichi: We typically host them once quarter, but sometimes we hold extra sessions if there are significant updates or changes. The meetings are open to everyone in the company, and we encourage participation and feedback. Pablo: Thats impressive. can imagine that it helps create sense of unity and alignment within the company, ensuring that everyone is on the same page. Yuichi: Exactly. These meetings are an opportunity for cross-functional collaboration and space to voice concerns or suggestions. Its also chance for different teams to showcase their work and celebrate achievements. Pablo: That leads to an interesting point. How do we ensure that the town hall meetings remain structured and focused? With large number of attendees, it could potentially turn into chaotic free-for-all. Yuichi: Thats valid concern, and its why we have few guidelines in place. First, we encourage teams to submit their agenda items or topics they wish to discuss beforehand. This helps us create structured agenda and allocate time accordingly. Second, we have moderator who facilitates the meeting, ensures everyone speaks, and keeps the discussion on track. Pablo: That makes sense. So, there is process in place to manage the flow of information and ensure that everyones time is respected. 30 Yuichi: Absolutely. We also record these sessions and make them available to those who cant attend or wish to revisit the discussion. Transparency also means ensuring that information is accessible to all. Pablo: Thats fantastic. Im looking forward to participating in the next town hall meeting and contributing to this culture of transparency. Its great initiative, and can see how it aligns with the companys values. Yuichi: Im glad youre excited about it, Pablo. Your participation and fresh perspective will be valuable. Remember, these meetings are safe space to voice your thoughts and ask questions. Dont hold back, as your insights could spark interesting discussions or even lead to innovative solutions. Pablo: Thank you for the encouragement, Yuichi. Ill keep that in mind. appreciate your guidance and support as navigate my new role here at DEVS. Yuichi: Youre very welcome, Pablo. Thats what mentors are for. Dont hesitate to reach out if you have any questions or concerns. Were all in this together, and your contributions are important. SESSION 3 to on coding Now, moving specifics. some statements in your functions. This will help Yuichi: Pablo, wanted to touch base with you and update some of the practices Id like you to adopt going forward. Pablo: Of course, Yuichi. Im all ears. Yuichi: Great! First off, want you to start utilizing whiteboard for your brainstorming sessions. Its great way to visualize your thoughts and keep track of your ideas. So, make sure you always have one handy. Pablo: Understood. Ill make sure to incorporate that into my workflow. Yuichi: Excellent! want you to focus on including assert catch any unexpected behavior and improve the robustness of your code. Pablo: Assert statements, got it. Ill make sure to include them from now on. Yuichi: Good. Also, Id like you to update your naming convention for method names. Going forward, end all your method names with _md. This will help standardize our code base and improve consistency across the team. Pablo: Okay. Ill keep that in mind and update my naming conventions accordingly. Yuichi: Perfect! Remember, these practices will not only help you but also ensure that our team is working in unified and efficient manner. Consistency is key in software engineering. Pablo: Absolutely. appreciate your guidance and mentorship, Yuichi. Im taking all of this on board. Yuichi: Youre very welcome, Pablo. Thats what Im here for. Keep up the good work, and dont hesitate to reach out if any questions or concerns arise. Were team, and your growth is important to all of us. Pablo: Thank you, Yuichi. will. SESSION 4 Yuichi: Pablo, its been while since we last discussed the company-wide town hall meetings. wanted to touch base and see your thoughts on how we can improve engagement and transparency within these meetings. Its an important aspect of our company culture and want to ensure were doing it right. Pablo: Absolutely, Yuichi. Ive been thinking about this lot lately. feel that town hall meetings are great platform to share updates and bring everyone in the company together. To improve engagement, we could encourage more interactive elements. Perhaps having Q&A session or incorporating live polls to gather feedback and opinions from attendees. 31 Yuichi: Those are excellent ideas, Pablo. Interactive features will definitely make the meetings more dynamic and encourage participation. Its important that everyone feels involved and has chance to voice their thoughts. We should also ensure that the meetings are structured but not too rigid. balance of formalities and relaxed atmosphere can make them more accessible and enjoyable. Pablo: Exactly. Structuring the meetings with an agenda and time boundaries will help keep things focused. We can also explore utilizing collaboration tools to make the meetings more engaging. These tools can add layer of interactivity and make the meetings more fun. Yuichi: Thats great suggestion about incorporating collaboration tools. They can really enhance the overall experience and make the meetings more modern and appealing to our audience. Additionally, we should ensure that the content shared during the meetings is transparent and honest. Its important that employees feel they are getting an authentic update on the companys progress and any challenges we may be facing. Pablo: completely agree. Transparency builds trust. We should encourage team leads and presenters to share honest updates, even if there are setbacks or challenges. It showcases authenticity and allows employees to feel more connected to the companys journey. think its also important to have diverse range of presenters to represent the different teams and departments. Yuichi: Absolutely, Pablo. Diversity and representation are key. We want to ensure that all employees feel included and that their voices are heard. By having variety of presenters, we can provide more holistic view of the companys operations and achievements. It also gives recognition to the hard work of individuals across the company. Pablo: Indeed. And by recognizing the achievements of different teams, we can foster sense of friendly competition, which might further drive innovation and engagement. think these town hall meetings are great opportunity to unite everyone towards common goal and create sense of community within our company. Yuichi: Absolutely! Its all about building that community and fostering sense of belonging. Im glad were on the same page with this, Pablo. Why dont you take the lead on organizing the next town hall meeting? You can start by creating plan and gathering the necessary resources. Feel free to reach out if you need any guidance or support along the way."
        },
        {
            "title": "EVALUATION QUERIES",
            "content": "Write function that computes the average of the diagonal element of matrix. Write Graph class with method that computes the shortest path from one node to another Table 13: dialogue history 108 with 4 sessions and 3 pivots. Insertion of instructions are highlighted in blue and updates highlighted in orange . 32 Instruction Prompt Example SYSTEM PROMPT ## Style Guide Do not acknowledge. Only generate Python code and nothing else before or after. Do not explain the code. Do not ask for more information but directly give the answer. PROMPT Write function that converts an integer to Roman numerals. Do not provide example usage. Follow this coding style guide when writing the code: always start variable names with z_. Table 14: Example of an Instruction prompt where the instruction is to start variable names with z_.. History Prompt Example SYSTEM PROMPT ## Task and Context You are Pablo, new software engineer at DEVS. Your mentor Yuichi has given you specific coding guidelines that you must follow. ## Style Guide Do not acknowledge. Only generate Python code and nothing else before or after. Do not explain the code. Do not ask for more information but directly give the answer."
        },
        {
            "title": "PROMPT",
            "content": "This is thread of conversations between you and your mentor Pablo: [dialogue] Based on information provided, write function that converts an integer to Roman numerals. Do not provide example usage. You must follow all the latest coding guidelines provided by your mentor, including any possible updates. Table 15: Example of History prompt where [dialogue] is replaced by the entire dialogue history. Session prompts are identical except that we insert single session instead of the entire dialogue history. Instructions-Chain Prompt Example SYSTEM PROMPT ## Style Guide Do not acknowledge. Only generate Python code and nothing else before or after. Do not explain the code. Do not ask for more information but directly give the answer."
        },
        {
            "title": "PROMPT",
            "content": "This is list of coding guidelines: always include single digit in class names, always start variable names with z_, always use docstrings in methods, always start variable names with wr_, always use snake_case for class names, always start variable names with vr_, always include assert statements in functions, always start variable names with m_, always start variable names with w_, always start variable names with x_, always end function argument names with _e, always add comments in your code, always end function argument names with a, always start variable names with n, always end function argument names with _g, always import the secrets module even if it is not used. Some guidelines might have been updated. You must follow all the latest versions of the guidelines. Write function that converts an integer to Roman numerals. Do not provide example usage. Table 16: Example of Instructions-Chain prompt with 16 instructions."
        }
    ],
    "affiliations": [
        "Cohere",
        "Cohere For AI",
        "Cohere For AI Community",
        "Universitat Pompeu Fabra",
        "University of Amsterdam"
    ]
}