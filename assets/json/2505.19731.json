{
    "paper_title": "Accelerating Nash Learning from Human Feedback via Mirror Prox",
    "authors": [
        "Daniil Tiapkin",
        "Daniele Calandriello",
        "Denis Belomestny",
        "Eric Moulines",
        "Alexey Naumov",
        "Kashif Rasul",
        "Michal Valko",
        "Pierre Menard"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF) offers a more direct alternative by framing the problem as finding a Nash equilibrium of a game defined by these preferences. In this work, we introduce Nash Mirror Prox ($\\mathtt{Nash-MP}$), an online NLHF algorithm that leverages the Mirror Prox optimization scheme to achieve fast and stable convergence to the Nash equilibrium. Our theoretical analysis establishes that Nash-MP exhibits last-iterate linear convergence towards the $\\beta$-regularized Nash equilibrium. Specifically, we prove that the KL-divergence to the optimal policy decreases at a rate of order $(1+2\\beta)^{-N/2}$, where $N$ is a number of preference queries. We further demonstrate last-iterate linear convergence for the exploitability gap and uniformly for the span semi-norm of log-probabilities, with all these rates being independent of the size of the action space. Furthermore, we propose and analyze an approximate version of Nash-MP where proximal steps are estimated using stochastic policy gradients, making the algorithm closer to applications. Finally, we detail a practical implementation strategy for fine-tuning large language models and present experiments that demonstrate its competitive performance and compatibility with existing methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . s [ 1 1 3 7 9 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Accelerating Nash Learning from Human Feedback\nvia Mirror Prox",
            "content": "Daniil Tiapkin1,2 Daniele Calandriello3 Denis Belomestny4,5 Éric Moulines1,6 Alexey Naumov5 Kashif Rasul7 Michal Valko8 Pierre Ménard9 1CMAP, CNRS, École Polytechnique 2LMO, Université Paris-Saclay 3Google DeepMind 4Duisburg-Essen University 7Hugging Face 5HSE University 8Stealth Startup / Inria / ENS 6Mohamed Bin Zayed University of AI 9ENS Lyon {daniil.tiapkin, eric.moulines}@polytechnique.edu dcalandriello@google.com denis.belomestny@uni-due.de anaumov@hse.ru kashif.rasul@gmail.com michal.valko@inria.fr pierre.menard@ens-lyon.fr"
        },
        {
            "title": "Abstract",
            "content": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on reward models, frequently assuming preference structures like the Bradley-Terry model, which may not accurately capture the complexities of real human preferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF) offers more direct alternative by framing the problem as finding Nash equilibrium of game defined by these preferences. In this work, we introduce Nash Mirror Prox (NashMP), an online NLHF algorithm that leverages the Mirror Prox optimization scheme to achieve fast and stable convergence to the Nash equilibrium. Our theoretical analysis establishes that Nash-MP exhibits last-iterate linear convergence towards the β-regularized Nash equilibrium. Specifically, we prove that the KLdivergence to the optimal policy decreases at rate of order (1 + 2β)N/2, where is number of preference queries. We further demonstrate last-iterate linear convergence for the exploitability gap and uniformly for the span semi-norm of log-probabilities, with all these rates being independent of the size of the action space. Furthermore, we propose and analyze an approximate version of Nash-MP where proximal steps are estimated using stochastic policy gradients, making the algorithm closer to applications. Finally, we detail practical implementation strategy for fine-tuning large language models and present experiments that demonstrate its competitive performance and compatibility with existing methods."
        },
        {
            "title": "Introduction",
            "content": "Aligning powerful pre-trained Large Language Models (LLMs) with complex, often subjective, human preferences and values is critical challenge for ensuring safe and beneficial AI. Reinforcement Learning from Human Feedback (RLHF) [Christiano et al., 2017] has emerged as leading paradigm for this task, enabling agents to learn desired behaviors from human preference signals rather than sparse or hand-engineered reward functions. RLHF has been successfully applied to fine-tune LLMs for various tasks such as text summarization [Stiennon et al., 2020], dialogue generation, and question answering [Ziegler et al., 2019, Stiennon et al., 2020, Ouyang et al., 2022, Bai et al., 2022]. common approach within RLHF, rooted in the literature on contextual dueling bandits [Yue et al., 2012, Zoghi et al., 2014, Bengs et al., 2021], is to posit an underlying reward model. The most prevalent choice is the Bradley-Terry (BT) model [Bradley and Terry, 1952, Zermelo, 1929], which assigns scalar reward to each action and assumes that preference between two actions is probabilistically determined by their reward difference. Under this model, the agents goal simplifies Preprint. Under review. to learning the reward function and selecting actions that maximize the inferred reward. Such an action, maximizing the average preference over all others, corresponds to Condorcet winner in social choice theory. However, relying on reward model, particularly the Bradley-Terry model, comes with significant limitations [Dudík et al., 2015, Munos et al., 2023]. fundamental issue is the inherent assumption of transitive preferences: if action is preferred to b, and to c, then must be preferred to c. This assumption is often violated in real-world human judgments, which frequently exhibit intransitivity [Gardner, 1970, Tversky, 1969, Klimenko, 2015]. Moreover, even if individual preferences were transitive, aggregating preferences across group can result in collective intransitivity [May, 1954, Kreweras, 1965]. Such non-transitive preferences preclude the existence of Condorcet winner or consistent scalar reward function that aligns with all comparisons. Nash Learning from Human Feedback (NLHF). To circumvent the restrictive assumptions of reward models, particularly the existence of consistent reward or Condorcet winner, Dudík et al. [2015] proposed preference-based approach for dueling bandits, recently coined Nash Learning from Human Feedback (NLHF) by Munos et al. [2023]. This framework directly models pairwise preferences and formulates the problem as symmetric two-player game where each player proposes an action. The natural objective is to find symmetric Nash Equilibrium (NE, von Neumann 1928, Nash Jr 1950) of this preference game, known as von Neumann winner (VNW) in the dueling bandits literature [Dudík et al., 2015]. Unlike Condorcet winner (a single best action), VNW is generally distribution over actions (a mixed policy), representing stable outcome in the face of potentially intransitive preferences. Regularized NLHF. In practical RLHF settings, especially when fine-tuning pre-trained LLMs, it is crucial to learn policy that aligns with human preferences while remaining close to the original reference policy (e.g., the pre-trained model). To satisfy this constraint, we consider finding the NE of regularized preference game. This is achieved by adding penalization term proportional to the Kullback-Leibler (KL) divergence from the current policy to the reference policy. This regularization encourages similarity to the reference policy and can also offer theoretical benefits for optimization, such as uniqueness of the NE. Efficiently Finding the Regularized NE. Finding the Nash Equilibrium of such game can be challenging. Munos et al. [2023] proposed the NashMD algorithm, an adaptation of Mirror Descent, to approximate the VNW of the regularized preference game. NashMD proceeds by first regularizing the current policy by mixing it with the reference policy, and then performing mirror descent step against this regularized policy. They showed that the last iterate of NashMD converges to the regularized NE at rate of O((β2N )1), measured by the KL divergence to the NE, where is the number of preference queries and β is the regularization parameter. Mirror Prox and the Research Question. While NashMD provides foundational algorithm, related optimization methods for finding Nash Equilibria, such as the Proximal Point (PP) method [Martinet, 1970] and its approximation Mirror Prox [Nemirovski, 2004], are known to achieve faster convergence rates, often linear, under certain conditions like strong concave-convexity. This raises natural question: Can we develop an algorithm for NLHF, inspired by the powerful principles of Mirror Prox, that achieves faster convergence rate than NashMD for the regularized preference game? Contributions. We answer this question affirmatively and make the following contributions: We propose the Nash Mirror Prox (NashMP) algorithm, novel method for finding the NE of the regularized preference game in NLHF. Inspired by the two-step structure of Mirror Prox, NashMP first computes an \"improved\" opponent policy via mirror descent step and then updates the current policy by performing another mirror descent step against this improved opponent. We provide rigorous theoretical analysis demonstrating that the last iterate of NashMP converges to the regularized NE at linear rate of O((1 + 2β)N/2/β), where is the number of preference queries and β is the regularization parameter. As shown in Table 1, this represents significant improvement over the O((β2N )1) rate of NashMD. Crucially, this linear convergence holds for the last iterate, which is highly desirable in practical deep learning settings where computing or storing policy averages can be challenging [McAleer et al., 2023]. We also analyze the relationship between the regularized NE found by NashMP and the VNW of the original unregularized game, providing an upper bound on the sub-optimality gap. Our analysis shows NashMP can find an 2 Algorithm KL to β-regularized VNW Original ε-VNW complexity NashMD [Munos et al., 2023] Online IPO [Calandriello et al., 2024] SPO [Swamy et al., 2024] ONPO [Zhang et al., 2025a] INPO [Zhang et al., 2025b] MMD [Wang et al., 2025] EGPO [Zhou et al., 2025] O((β2N )1) Asymptotic Not provided Not provided O((β2N )1) O((1 + β2)N /β) O((1 β/(1 + β + 2Y ))N ) NashMP (this paper) O((1 + 2β)N/2/β) Not provided Not provided (cid:101)O(1/ε2) (cid:101)O(1/ε) Not provided Asymptotic (cid:101)O(Y /ε) (cid:101)O(1/ε) Table 1: Comparison of theoretical guarantees for finding von Neumann winner (VNW) from human preference data. The first column shows the convergence rate of the KL divergence to the regularized VNW for the last iterate, where is the number of calls to comparison oracle and β is the regularization parameter. The second column shows the sample complexity (number of preference queries) required to find an ε-VNW in the original (unregularized) game (see Section 3 for definitions). NashMP achieves faster rate than its competitor at finding the regularized NE and matches the state-of-the-art query complexity for finding an ε-VNW, with the added benefit of last-iterate guarantee for the regularized problem. ε-VNW of the original game with query complexity (cid:101)O(1/ε), matching recent state-of-the-art methods while offering last-iterate convergence guarantees (see Table 1). For the important case of parametrized policies, we provide an in-depth analysis of approximating NashMPs steps using policy gradient methods. Specifically, we derive an improved analysis for softmax policy gradients in entropy-regularized multi-armed bandits, demonstrating final complexity that depends only on the optimal policy and initial parameters, not on the number of actions or the scale of the reward function. We develop practical variant of NashMP tailored for deep learning architectures, where the required mirror descent steps are approximated using policy gradients. This variant utilizes an exponential moving average of parameters to stabilize training and mimic the two-step structure. We present empirical results on synthetic preference game and on fine-tuning large language models, showing competitive performance."
        },
        {
            "title": "2 Related work",
            "content": "The field of Nash Learning from Human Feedback (NLHF) has rapidly evolved, drawing upon foundational game theory and modern optimization techniques to address the limitations of traditional reward modeling. The NLHF framework was formally introduced by Munos et al. [2023], who built upon the reformulation of contextual dueling bandits as two-player symmetric game by Dudík et al. [2015]. They proposed the NashMD algorithm, demonstrating that its last iterate converges to the von Neumann Winner (VNW), or Nash Equilibrium (NE), of regularized preference game at polynomial rate. Subsequently, Calandriello et al. [2024] showed that an online version of the IPO algorithm [Gheshlaghi Azar et al., 2024] also converges to the VNW, though without providing explicit convergence rates. These approaches, like much of the subsequent work, typically assume preferences are provided between individual actions or responses at single decision point. Broadening the scope of preference feedback, Shani et al. [2025] addresses the limitations of single-turn preference emulation in settings requiring multi-turn planning via self-play Mirror Descent-based policy optimization algorithm and proves its convergence to Nash Equilibrium. significant line of research has focused on directly approximating VNW in the original (unregularized) preference game. Swamy et al. [2024] (see also Wu et al. 2025) leveraged classical regret minimization tools for matrix games to develop the Self-Play Preference Optimization (SPO) algorithm. SPO requires (cid:101)O(1/ε2) calls to the preference model to find policy with an ε-suboptimality 3 gap1, demonstrating the feasibility of finding approximate VNWs. Building on this, Zhang et al. [2025a] introduced the Optimistic Online Mirror Descent for NLHF (ONPO ) algorithm, inspired by optimistic mirror descent [Rakhlin and Sridharan, 2013]. ONPO improved the complexity to (cid:101)O(1/ε) for finding an ε-VNW in the original game, though without convergence guarantees for the regularized game setting that is often critical for LLM alignment. Efforts have also been directed towards improving convergence for the regularized NLHF game, which is central to our work. Zhang et al. [2025b] presented the Iterative Nash Policy Optimization (INPO) algorithm, which, similar to NashMD, achieves O((β2N )1) last-iterate convergence rate in KL divergence to the NE of the regularized game. notable advancement came from Wang et al. [2025], who, adapting techniques from Sokota et al. [2023], introduced Magnetic Mirror Descent (MMD). MMD achieved linear last-iterate convergence rate of O((1 + β2)N /β) towards the NE of the regularized game. However, no guarantees were provided for convergence in the original, unregularized game. Our algorithm, NashMP, also achieves linear last-iterate rate for the regularized game, but with potentially better dependence on β parameter (O((1 + 2β)N/2/β) vs O((1 + β2)N /β)) and, importantly, we also provide guarantees for the original game complexity. Most recently, and concurrently with our work, Zhou et al. [2025] proposed the Extragradient Preference Optimization (EGPO) algorithm. EGPO also draws inspiration from the Extragradient method (an alternative name for Mirror Prox, which also motivates NashMP) and achieves linear convergence rate of O((1 β/(1 + β + 2Y ))N ) for exact updates in the regularized game. They also provide (cid:101)O(Y /ε) complexity for finding an ε-VNW in the original game. While EGPO offers strong results, its convergence rates and original game complexity exhibit dependence on the number of actions . In contrast, NashMP achieves linear rate for the regularized game and an (cid:101)O(1/ε) complexity for the original game that are independent of , which can be significant advantage in settings with large action spaces, such as LLM fine-tuning. Table 1 provides detailed rate and complexity comparisons."
        },
        {
            "title": "3 Setting",
            "content": "We consider contextual dueling bandit setting (X , Y, P), where is context space, is finite action space, and P(y x) [0, 1] is the probability that action is preferred to given context , which satisfies symmetry: P(y x) = 1 P(y x) for all x, y, y. policy π : maps contexts to probability distributions over actions, where is the probability simplex over Y. Let Π be the space of all such policies. For context , we define the expected preference of an action over policy π Π, and of policy π Π over π, as: P(y πx) Eyπ(x)[P(y yx)], and P(π πx) Eyπ(x)[P(y πx)] . For each context , we define preference matrix Px [0, 1]YY with entries (Px)y,y = P(y yx). Then, the expected preference P(π πx) can be expressed as bilinear form: P(π πx) = π(x)TPxπ(x) , (1) where π(x) and π(x) are the vector representations of the policies distributions at context x. In the context-free setting, we omit the dependence on x. We may abuse notation and write P(v ux) for vTPxu, where v, RY are arbitrary vectors, not necessarily probability distributions. Preference game. Given context distribution ρ over , we consider the following two-player zero-sum symmetric game: 1. context ρ is sampled and revealed to both players; 2. The max-player chooses an action Y, and simultaneously, the min-player chooses Y; 3. An outcome Ber(P(y x)) is sampled. If = 1, wins; if = 0, wins. The max-player receives reward of o, and the min-player receives 1o. For pair of policies (π, π), the value of this game (expected reward for the max-player) is: P(π π) Exρ[P(π πx)]. 1See Section 3 for formal definition of the suboptimality gap. The (cid:101)O notation hides logarithmic factors in problem parameters like the number of actions or 1/ε. 4 Von Neumann winner. The preference game is symmetric, so it admits symmetric Nash Equilibrium (NE) (π, π) [von Neumann, 1928, Nash Jr, 1950]. The policy π is called von Neumann winner (VNW) [Dudík et al., 2015] and satisfies π arg maxπΠ minπΠ P(π π) . The suboptimality of policy π against π is SubOpt(π, π) 1 suboptimality of π, also known as its exploitability gap, is: 2 P(π π). The worst-case SubOpt(π) maxπΠ SubOpt(π, π) = 1 2 minπΠ P(π π) . (2) Due to game symmetry, P(π π) = 1/2 for any π Π, implying SubOpt(π, π) = 0. Thus, SubOpt(π) 0, and SubOpt(π) = 0 if and only if π is VNW. policy π is an ε-VNW if SubOpt(π) ε. Regularized preference game. Given reference policy πref Π and regularization parameter β > 0, the β-regularized preference of π over π is defined as: Pβ(π π) P(π π) β KLρ(ππref ) + β KLρ(ππref ) , (3) (cid:2) KL (cid:0)π(x)πref (x)(cid:1)(cid:3) is an expected Kullback-Leibler (KL) divergence. where KLρ(ππref ) Exρ Similar to the preference game introduced earlier, one can define β-regularized preference game [Munos et al., 2023] where the expected payoff of the max-player is the regularized preference while the min-players payoff is the opposite of the regularized preference. This game is symmetric, and strongly convex-concave, and thus admits unique symmetric NE (π β is the β-regularized VNW and satisfies π β, π β = arg maxπΠ minπΠ Pβ(π π). The β-regularized suboptimality of π against π is SubOptβ(π, π) 1 worst-case β-regularized suboptimality of π is: 2 Pβ(π π). The β). The policy π SubOptβ(π) maxπΠ SubOptβ(π, π) = 2 minπΠ Pβ(π π) . (4) policy π is an ε-VNW in the β-regularized game if SubOptβ(π) ε. Additional notations. For vector Rd we define span semi-norm of as xsp = inf cRx+ c1, where 1 = (1, . . . , 1)T is vector of all ones."
        },
        {
            "title": "4 Nash Mirror Prox",
            "content": "In this section, we introduce Nash Mirror Prox (NashMP), an algorithm designed to solve the regularized preference game that enjoys last-iterate linear convergence to β-regularized NE. 4.1 Algorithm Description Let us recall the problem we are aiming to solve, which is the following convex-concave saddle point optimization problem: max πΠ (cid:8)P(π π) β KLρ(ππref ) + β KLρ(ππref )(cid:9) , min πΠ (5) where Π is the space of policies. Our algorithm, NashMP, is an adaptation of Mirror Prox [Nemirovski, 2004] with specific design choices tailored to this problem: 1) we directly incorporate the KL regularization towards πref , leveraging its prox-friendliness without additional linearization, and 2) we exploit the games symmetry to simplify analysis and computations. The NashMP iterates are: πk+ 1 2 = arg min πΠ πk+1 = arg min πΠ (cid:8)P(πk π) + β KLρ(ππref ) + (β/η) KLρ(ππk)(cid:9) , (cid:110) π) + β KLρ(ππref ) + (β/η) KLρ(ππk) P(πk+ 1 (cid:111) , (6) where η > 0 is learning rate. It performs two iterations of self-improvement with respect to the current policy, while being regularized towards both reference policy πref and the target policy πk. In particular, NashMP calls the preference model two times per step. We compare NashMP to existing methods in Appendix C. 5 Connection with Proximal Point method. The initial motivation of Mirror Prox, by Nemirovski [2004], was to approximate the proximal point (PP) method [Martinet, 1970, Rockafellar, 1976]. The iterations of PP method for our problem would be implicitly defined as: πk+1 = arg min πΠ (cid:8)P(πk+1 π) + β KLρ(ππref ) + (β/η) KLρ(ππk)(cid:9) . (7) This PP perspective is foundational and informs our practical implementation (see Section 5.3). Notably, as the learning rate η + (implying the proximal term (β/η) KLρ(ππk) vanishes), the solution to (7) converges to the β-regularized VNW π β (see Lemma 2 in Appendix). This is because π . In practice, since the proximal step (7) is only approximated, finite learning rates η > 0 are necessary. more accurate approximation of the proximal step generally permits larger learning rates. (cid:111) β π) + β KLρ(ππref ) β = arg minπΠ β satisfies: π P(π (cid:110) 4.2 Theoretical Guarantees We now present the theoretical guarantees for the NashMP algorithm. The main result is linear convergence to the β-regularized Nash equilibrium (NE) in KL-divergence, suboptimality gap, and uniformly in log-probabilities. For simplicity of exposition, we consider context-free version of the β-regularized preference game, i.e., = {x0} and ρ(x0) = 1. For brevity, we omit dependence on x0 in the discussions on theoretical results. Additionally, it is worth mentioning that since the contextual problem is an expectation of separate problems over the possible contexts, one may just run the algorithm above for any possible context separately (see Munos et al. [2023] for an additional discussion). Theorem 1. Assume β 1/2. After iterations of NashMP with learning rate η 2β an initial policy π0 = πref , the suboptimality gap and KL-divergence to the optimal solution satisfy (cid:113) 1 ηβ (1 + η)K , KL(π βπK) 1 2β (1 + η)K . ) 1 2η (1 + η)K + SubOptβ(πK+ 1 2 At the same time, the algorithm enjoys linear rates to the optimal solution in the span semi-norm max{log πK log π βsp, log πK+ 1 log π βsp} 1 β (1 + η)K + 3 2β (cid:113) 1 ηβ (1 + η)K . We refer to Appendix for full proof. In essence, we tailor standard proof of Mirror Prox (see, e.g, Beznosikov et al. [2023]) to achieve convergence in KL-divergence and afterwards heavily use the properties of the game and optimal solutions to achieve convergence in suboptimality and strong uniform notion in span semi-norm. Next, we provide iteration and oracle complexity results. Corollary 1. Assume β 1/2. Then, for ε > 0, the final policy of NashMP with η = 2β is ε-VNW (cid:108) 1+β in β-regularized preference game after = iterates and = 2K preference oracle calls. Additionally, for uniform reference policy πref (y) = 1/Y for all Y, specifically chosen β = β(ε) and an initial policy π0 = πref , the final policy πK+1/2 is ε-VNW in the original preference game after = β log iterates. (cid:16) 2 εβ log (cid:17)(cid:109) (cid:17)(cid:109) (cid:108) 8 log(Y ) ε (cid:16) 8 log(Y ) ε The proof is presented in Appendix and essentially directly follows from Theorem 1. In practice, we recommend using reference policy that is not uniform over all actions but uniform over support of some von Neumann winner π. In this case, the convergence guarantee will persist. Comparison to existing results. NashMP achieves linear convergence rate to the VNW of the regularized preference game, which is in sharp contrast to the results obtained for NashMD or INPO. Furthermore, NashMP also outperforms its competitors, achieving linear convergence either with respect to β, the regularization parameter, or the number of actions . Refer to Table 1 for details."
        },
        {
            "title": "5 Approximate Nash Mirror Prox",
            "content": "This section introduces more practical algorithm that approximates the iterates of NashMP (defined in (6)) using stochastic policy gradients. For clarity, we focus on context-free setting, noting that these approaches readily generalize to the contextual setting. 6 5.1 Algorithm In practice, computing the exact updates from (6) is infeasible due to the need to solve highdimensional optimization problems over parameterized policy classes. To overcome this, we propose an approximate algorithm where iterates {1, 2} are found via inexact updates: (cid:98)πk+p/2 arg min πΠ (cid:8)P((cid:98)πk+(p1)/2 π) + β KL(ππref ) + (β/η) KL(π(cid:98)πk)(cid:9) , (8) where (cid:98)πk+p/2 denote the approximate solutions to the corresponding subproblems at iteration k. To solve these subproblems, we adopt the following common strategy: (1) parameterize the policy π as πθ using softmax function: πθ(y) exp(θy)/ (cid:80) yY exp(θy), and (2) optimize objectives over θ using stochastic gradient descent. The objective for subproblem steps = {1, 2} at outer iteration k: Jk+p/2(θ) Eyπθ [P((cid:98)πk+(p1)/2 y)] + β KL(πθπref ) + (β/η) KL(πθ(cid:98)πk) . Each of these optimization problems can be viewed as regularized multi-armed bandit problem. This perspective allows the application of well-studied theory of softmax policy gradient methods [Agarwal et al., 2020, Mei et al., 2020]. Specifically, the update of parameters θk+p/2,t are defined as: (9) θk+ 2 ,t+1 = θk+ 2 ,t γ ˆJk+ 2 (θk+ 2 ,t) , (10) where ˆJk+ tion is provided in Algorithm 1 in Appendix B.2. 2 is REINFORCE-style estimator [Williams, 1992]. complete algorithmic descrip5.2 Theoretical Guarantees We now analyze the convergence properties of this approximate algorithm. First, we establish what the guarantees of approximation of (8) are needed to guarantee overall convergence of the algorithm. Theorem 2. Assume β 1/2 and assume that log (cid:98)πk+p/2 log πk+p/2sp ε for ε (0, 1) for all {0, . . . , 1}, {1, 2}, where πk+p/2 denotes the exact minimizer to the corresponding (cid:1)(cid:109) ε objective in (8). Then after = β -VNW in βregularized game. iterates, policy (cid:98)πK+1/2 is 4 2β log(cid:0) (cid:108) 1+β ε We refer to Appendix B.1 for proof. This theorem shows that if we can approximate the true subproblem solution πk+p/2 with an accuracy ε = O(ε2β2) for ε (0, 1), the final policy of the approximation NashMP is ε-VNW in β-regularized game. We emphasize that achieving an ε-optimal solution in terms of the objective function value Jk+ might be insufficient. Instead, we require the approximate policy to be close to the true subproblem minimizer πk+p/2 in span semi-norm, which is stronger notion of convergence. 2 Our next goal is to achieve ε-solution for an arbitrary ε (0, 1) in terms of span semi-norm log (cid:98)πk+p/2 log πk+p/2sp ε, using policy gradient method. Lemma 1. Assume ε < 1/3 and assume that the stochastic gradient ˆJk+ is estimated using batch size of size = (cid:101)O((c β is defined in Appendix in (33) and depends only on β, η and optimal policy π β)1 log(1/(β ε))) steps of stochastic gradient descent with step size γ = η/(β(1 + η)), it holds log (cid:98)πk+p/2 log πk+p/2sp ε for all {0, . . . , 1} and {1, 2} with probability at least 1 δ. β ε)2), where the constant β. Then, after = O((c 2 We refer to Appendix B.2 for proof and additional details. This result relies on two key technical elements: (1) establishing dimension-free convergence rates for the stochastic policy gradient method in this regularized bandit setting, and (2) ensuring convergence in span semi-norm for the iterates of NashMP. Notably, our policy gradient analysis yields an improvement over Mei et al. [2020] in dependence of constant ). We refer to Appendix for further details. β on by factor of exp( 5.3 Practical Deep Learning Implementation In this section, we consider contextual setting in full generality. To propose more practical algorithm, we first notice that the approximate version of NashMP presented in Section 5.1 performs 7 gradient steps to approximate each of the two global mirror steps. However, as discussed in Section 4, Mirror Prox also serves as an approximation to the Proximal Point (PP) method, and, therefore, we may want to rebalance the outer and inner approximation steps. We consider the following policies: online policy πt with parameters θt, target πtarget parameters θtarget arg minθΘ LNashMP(θ; θt, θtarget with , and fixed reference policy πref . We introduce the parameter update θt+1 = , πref ), for the loss function t LNashMP(θ; θ, θtarget) xρ yπθ (x) yπθ (x) (cid:20) P(y yx) + β log πθ(yx) πref (yx) + β η log πθ(yx) πθtarget (yx) (cid:21) . (11) To obtain NashMP one should update the target policy parameter θtarget Remark that if we update it every steps, we obtain an algorithm that is closer to the PP method. with θt every two steps. The previous approximation approach with inner gradient steps to optimize (11) till convergence might be very impractical. Instead, we find it more practical and elegant to update the online parameter with one (or few) gradient update with the loss LNashMP and slowly update the target with an exponential moving average: θt+1 = θt αθLNashMP(θt; θt, θtarget ) , t+1 = κθt + (1 κ)θtarget θtarget , where α is some learning rate and the parameter κ [0, 1] controls implicitly the number of steps for one proximal update. Thus, we approximate the resolution of one proximal subproblem with 1/κ gradient steps. Also, we would like to note that this strategy is very common in deep reinforcement learning [Mnih et al., 2015, Lillicrap et al., 2016]. i=1 for xi ρ sampled from an offline prompt dataset, yi and Gradient estimation. Instead of using an exact gradient, we estimate it with batch of observations (xi, yi, i)B sampled from the online network πt. We notice that both and are sampled from the same distribution; therefore, we can symmetrize standard REINFORCE estimate with baseline 1/2 and get the following form of gradient estimator (cid:98)θLNashMP(θt; θt, θtarget ) 1 (cid:88) (cid:0)θ log πθt (yixi) θ log πθt (y ixi)(cid:1) i=1 (cid:34) + θ log πθt (yixi) β log + θ log πθt (y ixi) β log (cid:34) (cid:18) πθt (yixi) πref (yixi) (cid:19) (cid:19) (cid:18) πθt (y πref (y ixi) ixi) + + β η β η (cid:18) 1 2 (cid:32) log (cid:32) log P(yi ixi) (cid:19) πθt (yixi) πθtarget (yixi) πθt (y πθtarget ixi) (y ixi) (cid:33)(cid:35) (cid:33)(cid:35) . The first term of this gradient expression resembles the gradient that appears in DPO [Rafailov et al., 2024] in an even more contrastive nature: if two responses yi and are the same from the perspective of the preference model P, then P(yi 2 and they do not give any gradient signal. But if one of the responses is better than the other, we increase its likelihood and decrease the likelihood of worse answer. Notice that if we have only duel feedback, we can replace the preference model above with the duel results. The other two terms in the gradient expression represent the regularization to reference and target models. ixi)"
        },
        {
            "title": "6 Experiments",
            "content": "6.1 Matrix Games We use the simple contextual dueling bandit problem as an initial experiment to study the deep learning implementation of Nash Mirror Prox. Game definition. Let us fix number of actions 2 and positive integer 1. We consider dueling bandit game with context space = Rrr and an action space = {1, . . . , }. Preference probabilities are defined as follows P(y yx) σ(Ay,y Ay,y) , ΘxV , 8 Figure 1: Comparison of NashMP (in red) with baseline methods across different optimization horizons {103, 104, 5 104}. Our method consistently achieves lower suboptimality as the optimization horizon increases. Suboptimality is averaged over 10 random seeds; shaded regions indicate one standard deviation. where RY and RY are fixed matrices, Θx Rrr is corresponding context matrix, and σ() is sigmoid function. This type of dueling bandit instance is generalization of low-rank linear bandit problem. Notice that for any 2 this problem does not admit Bradley-Terry model. The distribution over contexts ρ is assumed to be standard Gaussian random matrix (i.e., elements of Θx are i.i.d. with distribution (0, 1)). We aim to find policy π : that approximates β-regularized VNW. We refer to Appendix for more details on the setup. Results. We compare our method with the following baselines: Online DPO [Rafailov et al., 2024], Online IPO [Calandriello et al., 2024], Nash MD [Munos et al., 2023], NashMP with an adaptive κ, Mirror Descent (MD) corresponding to Nash MD without the mixture coefficient equal to 0, and EGPO [Zhou et al., 2025]. The results are presented in Figure 1. We can observe that for 500 steps, NashMP does not provide improvements upon Online IPO; however, starting from approximately 1000 optimization steps, NashMP with an adaptive κ = 10/(k + 10) starts outperforming all the baselines, and the relative improvement increases as optimization continues. Additionally, we observe that the confidence interval for our method is much smaller, showing the influence of additional stabilization. We also notice that the 2-step stabilization procedure of EGPO is insufficient to stabilize in the functional approximation setting, and Online DPO diverges since it does not solve preference game. Also, in Appendix we provide an additional study on the influence of κ on the optimization procedure. 6.2 LLM Alignment In this section, we apply NashMP to perform alignment of large language model (LLM). Experiment setup For our LLM-based experiments, we use the Gemma-2B [Gemma Team, 2024] pretrained model checkpoints and train on the RLHFlow Dong et al. [2024] datasets for all the analysis. In particular, we first perform SFT on the RLHFlow SFT dataset [RLHFlow Team, 2024c] and then all our NLHF experiments on the resulting checkpoint, using subset of RLHFlow Prompt collection [RLHFlow Team, 2024b]. The pairwise judge model is Gemma-2B trained via the Robust Reward Models [Liu et al., 2025] method. All the experiments were performed using the TRL library [von Werra et al., 2020]. Baselines We compare the practical version of NashMP described in Section 5.3 for κ = 0.1 with the following baselines: Online DPO [Guo et al., 2024], Online IPO [Calandriello et al., 2024], NashMD [Munos et al., 2023], and NashMP with η = + that we refer to as Regularized Self-Play. We refer to Appendix for more details on baselines and used hyperparameters. Results We report in Table 2 the pairwise win-rates between the different methods for the judge. We observe that NashMP outperforms all the baselines, including Regularized Self-Play. The only difference between NashMP and Regularized Self-Play is the use of additional regularization with respect to target model, and our results show the value of this regularization. 9 Table 2: Pairwise Win Rates (mean 3σ-confidence intervals). Statistically significant wins are in bold. Confidence intervals are in smaller font size. Row/column for NashMP, κ = 0.1 is highlighted. Win rate SFT Online DPO Online IPO NashMD Reg. Self-Play NashMP, κ = 0.1 SFT 0.83770.0087 0.84460.0091 0.80260.0098 0.84640.0087 0.87170. Online DPO 0.16230.0087 0.52570.0115 0.42120.0116 0.52700.0113 0.56080.0116 Online IPO 0.15540.0091 0.47430.0115 0.38850.0121 0.49640.0118 0.52940.0117 NashMD 0.19740.0098 0.57880.0116 0.61150.0121 0.59690.0119 0.63950.0115 Reg. Self-Play 0.15360.0087 0.47300.0113 0.50360.0118 0.40310.0119 0.53800.0118 NashMP, κ = 0.1 0.12830.0081 0.43920.0116 0.47060.0117 0.36050.0115 0.46200."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we addressed the challenge of efficiently finding NE in regularized preference games arising in NLHF, crucial task for aligning LLMs with human preferences while maintaining proximity to reference policy. We introduced NashMP, novel algorithm inspired by the Mirror Prox method, designed to leverage its strong convergence properties. Our theoretical analysis demonstrates that it achieves linear convergence rate to the NE of the regularized game for its last iterate, significant improvement over the polynomial rates of prior methods. This last-iterate guarantee, coupled with rate independent of the action space size, makes it particularly well-suited for practical applications with large models. However, determining the optimal rates remains an open question, as we are not aware of any established lower bound for this specific setting, to the best of our knowledge. Furthermore, we showed that we can efficiently approximate VNW in the original unregularized game with sample complexity matching state-of-the-art results while offering stronger guarantees for the regularized setting. Our analysis extended to parametrized policies, providing insights into the use of policy gradient methods, and we presented practical deep learning variant that showed competitive performance. Broader impact. Our work advances the efficient alignment of LLMs with human preferences, potentially enhancing AI systems ability to make decisions that are more aligned with human values."
        },
        {
            "title": "References",
            "content": "Alekh Agarwal, Sham Kakade, Jason Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in Markov decision processes. In Conference on Learning Theory, pages 6466. PMLR, 2020. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, and Jared Kaplan. Training helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862, 2022. URL https://api.semanticscholar.org/CorpusID:248118878. Viktor Bengs, Róbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hüllermeier. Preference-based online learning with dueling bandits: survey. J. Mach. Learn. Res., 22(1), jan 2021. ISSN 1532-4435. Aleksandr Beznosikov, Darina Dvinskikh, Andrei Semenov, and Alexander Gasnikov. Bregman proximal method for efficient communications under similarity, 2023. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Daniele Calandriello, Zhaohan Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires, Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, Rishabh Joshi, Zeyu Zheng, and Bilal Piot. Human alignment of large language models through online preference optimisation. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 54095435. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/ calandriello24a.html. 10 Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/ paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. RLHF workflow: From reward modeling to online RLHF. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/forum?id=a13aYUU9eU. Miroslav Dudík, Katja Hofmann, Robert Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Contextual dueling bandits. In Conference on Learning Theory, pages 563587. PMLR, 2015. Martin Gardner. Mathematical games: The paradox of the nontransitive dice and the elusive principle of indifference. Scientific American, 223(12):110114, 1970. Gemma Team. Gemma2-2b. 2024. doi: 10.34740/KAGGLE/M/3301. URL https://huggingface. co/google/gemma-2-2b. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li, editors, Proceedings of The 27th International Conference on Artificial Intelligence and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 44474455. PMLR, 0204 May 2024. URL https://proceedings.mlr.press/v238/gheshlaghi-azar24a.html. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online AI feedback. arXiv preprint arXiv:2402.04792, 2024. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, ICLR (Poster), 2015. URL http://dblp.uni-trier.de/db/conf/ iclr/iclr2015.html#KingmaB14. Alexander Y. Klimenko. Intransitivity in theory and in the real world. Entropy, 17(6):43644412, 2015. ISSN 1099-4300. doi: 10.3390/e17064364. URL https://www.mdpi.com/1099-4300/ 17/6/4364. Germain Kreweras. Aggregation of preference orderings. Mathematics and Social Sciences I: Proceedings of the seminars of Menthon-Saint-Bernard, France (127 July 1960) and of Gösing, Austria (327 July 1962), pages 7379, 1965. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1509.02971. Tianqi Liu, Wei Xiong, Jie Ren, Lichang Chen, Junru Wu, Rishabh Joshi, Yang Gao, Jiaming Shen, Zhen Qin, Tianhe Yu, Daniel Sohn, Anastasia Makarova, Jeremiah Zhe Liu, Yuan Liu, Bilal Piot, Abe Ittycheriah, Aviral Kumar, and Mohammad Saleh. RRM: Robust reward model training mitigates reward hacking. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=88AS5MQnmC. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. 11 B. Martinet. Brève communication. Régularisation dinéquations variationnelles par approximations successives. Revue française dinformatique et de recherche opérationnelle. Série rouge, 4(R3): 154158, 1970. URL https://www.numdam.org/item/M2AN_1970__4_3_154_0/. Kenneth O. May. Intransitivity, utility, and the aggregation of preference patterns. Econometrica, 22: 1, 1954. URL https://api.semanticscholar.org/CorpusID:156169619. Stephen McAleer, Gabriele Farina, Marc Lanctot, and Tuomas Sandholm. Escher: Eschewing importance sampling in games by computing history value function to estimate regret. In International Conference on Learning Representations (ICLR), 2023. Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. In International conference on machine learning, pages 68206829. PMLR, 2020. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, February 2015. ISSN 00280836. URL http://dx.doi.org/10.1038/nature14236. Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mésnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash learning from human feedback, 2023. John Nash Jr. Equilibrium points in N-person games. Proceedings of the National Academy of Sciences of the United States of America, 36(1):4849, 1950. Arkadi Nemirovski. Prox-method with rate of convergence (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on Optimization, 15(1):229251, 2004. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 2773027744, 2022. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/ file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Alexander Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS13, page 30663074, Red Hook, NY, USA, 2013. Curran Associates Inc. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 35053506, 2020. 12 Morgane Rivière, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, et al. Gemma 2: Improving open language models at practical size. CoRR, 2024. RLHFlow Team. Rlhflow pairwise preference dataset, 2024a. URL https://huggingface.co/ datasets/RLHFlow/pair_preference_model_dataset. RLHFlow Team. Rlhflow prompt collection, 2024b. URL https://huggingface.co/datasets/ RLHFlow/prompt-collection-v0.1. RLHFlow Team. Rlhflow-sft-dataset, 2024c. URL https://huggingface.co/datasets/ RLHFlow/RLHFlow-SFT-Dataset-ver2. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM journal on control and optimization, 14(5):877898, 1976. Lior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila Noga, Orgad Keller, Bilal Piot, Idan Szpektor, et al. Multi-turn reinforcement learning with preference human feedback. Advances in Neural Information Processing Systems, 37:118953118993, 2025. Samuel Sokota, Ryan DOrazio, Zico Kolter, Nicolas Loizou, Marc Lanctot, Ioannis Mitliagkas, Noam Brown, and Christian Kroer. unified approach to reinforcement learning, quantal response equilibria, and two-player zero-sum games. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=DpE5UYUQzZH. Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize with human feedback. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 30083021. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf. Gokul Swamy, Christoph Dann, Rahul Kidambi, Steven Wu, and Alekh Agarwal. minimaximalist approach to reinforcement learning from human feedback. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 4734547377. PMLR, 2127 Jul 2024. URL https: //proceedings.mlr.press/v235/swamy24a.html. Amos Tversky. Intransitivity of preferences. Psychological Review, 76:3148, 1969. URL https: //api.semanticscholar.org/CorpusID:144609998. J. von Neumann. Zur Theorie der Gesellschaftsspiele. Mathematische Annalen, 100:295320, 1928. ISSN 0025-5831; 1432-1807/e. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020. Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie Su, and Yaodong Yang. Magnetic preference optimization: Achieving last-iterate convergence for language model alignment. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=PDnEDS244P. Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preference optimization for language model alignment. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=a3PmRgAB5T. Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem. J. Comput. Syst. Sci., 78(5):15381556, sep 2012. ISSN 0022-0000. doi: 10.1016/j.jcss. 2011.12.028. URL https://doi.org/10.1016/j.jcss.2011.12.028. 13 E. Zermelo. Die berechnung der turnier-ergebnisse als ein maximumproblem der wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 29:436460, 1929. URL http://eudml.org/ doc/168081. Yuheng Zhang, Dian Yu, Tao Ge, Linfeng Song, Zhichen Zeng, Haitao Mi, Nan Jiang, and Dong Yu. Improving llm general preference alignment via optimistic online mirror descent. arXiv preprint arXiv:2502.16852, 2025a. Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, and Dong Yu. Iterative nash policy optimization: Aligning LLMs with general preferences via no-regret learning. In The Thirteenth International Conference on Learning Representations, 2025b. URL https://openreview.net/forum?id=Pujt3ADZgI. Runlong Zhou, Maryam Fazel, and Simon Du. Extragradient preference optimization (egpo): arXiv preprint Beyond last-iterate convergence for nash learning from human feedback. arXiv:2503.08942, 2025. Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. ArXiv, abs/1909.08593, 2019. URL https://api.semanticscholar.org/CorpusID:202660943. Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten Rijke. Relative upper confidence bound for the k-armed dueling bandit problem. In Eric P. Xing and Tony Jebara, editors, Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pages 1018, Bejing, China, 2224 Jun 2014. PMLR. URL https:// proceedings.mlr.press/v32/zoghi14.html. 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 16 19 22 22 27 31 33 34 34 36 40 41"
        },
        {
            "title": "Table of Contents",
            "content": "A Analysis of Nash Mirror Prox . . A.1 Exact case . . A.2 Technical lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Analysis of Approximate Nash Mirror Prox . B.1 General Approximate Case . . B.2 Sample-Based Approximate Nash Mirror Prox . . . B.3 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Relationship to Existing Algorithms. Softmax Policy Gradients for Entropy-Regularized Multi-Armed Bandits . . . . . . D.1 Deterministic case . D.2 Stochastic case . . D.3 Technical Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed Experiment Description . . E.1 Matrix Games . . E.2 LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Analysis of Nash Mirror Prox",
            "content": "Let us recall an optimal solution to the β-regularized preference game as π β: (π β, π β) = arg max πΠ arg min πΠ Pβ(π π) , where A.1 Exact case Pβ(π π) P(π π) β KL(ππref ) + β KL(ππref ) . We start from theorem that states the convergence of the exact version of the Nash Mirror Prox NashMP algorithm, where all the steps can be computed exactly. We also recall that for theoretical analysis, we use context-free setting. Theorem (Restatement of Theorem 1). Assume that β 1/2 then after iterations of Nash Mirror Prox with learning rate η 2β and an initial policy π0 = πref , the suboptimality satisfies the inequality SubOptβ(πK+ 2 ) 1 2η (1 + η)K + (cid:114) 1 ηβ (1 + η)K . At the same time, the algorithm enjoys the following linear convergence to the optimal solution in KL-divergence: KL(π βπK) 1 2β (1 + η)K . Moreover, one has the uniform convergence in the span semi-norm: (cid:114) 1 ηβ (1 + η)K 2β log πK log π 1 + η β βsp + (1 + η)K , log πK+ 1 2 log π βsp (1 + η)K 2(1 + η)β + 3 2β (cid:114) 1 ηβ (1 + η)K . Proof. Let us consider an arbitrary step 1. Then we can write down optimality conditions that hold for steps πk+ 1 and πk+1, using Lemma 6 for µ = πk+1 and vk = 1 β P(πk ) 2 (η/β)[P(πk πk+1) P(πk πk+ 1 2 )] + η KL(πk+1πref ) + KL(πk+1πk) η KL(πk+ 1 πref ) KL(πk+ 1 2 πk) (1 + η) KL(πk+1πk+ 1 2 ) . and for µ = π β and vk+ 1 2 = 1 β P(πk+ 1 2 ) (η/β)[P(πk+ 1 2 π β) P(πk+ 1 (cid:16) 2 πk+1)] + η KL(π βπref ) + KL(π (cid:17) βπk) η KL(πk+1πref ) + KL(πk+1πk) (1 + η) KL(π βπk+1) . Summing up these inequalities, the underlined terms cancel out, and we have (η/β)[P(πk πk+1) P(πk πk+ 1 2 ) + P(πk+ 1 (1 + η) KL(πk+1πk+ 1 + (1 + η) KL(π ) + η KL(πk+ 1 2 βπk+1) η KL(π βπref ) KL(π βπk) π β) P(πk+ 1 πref ) + KL(πk+ 1 2 πk) 2 πk+1)] Let us recall the definition Pβ(π π) P(π π) β KL(ππref ) + β KL(ππref ), when we have (η/β)P(πk+ 1 2 π β) η KL(πk+ 1 2 πref ) + η KL(π βπref ) = (η/β)Pβ(πk+ 1 π β). 2 16 Also, using the fact that P(πk+ 1 π (cid:123)(cid:122) (η/β) [Pβ(πk+ 1 (cid:124) , πk+ 1 β) 1/2] (cid:125) 2 2 SubOptβ (πk+ 1 2 ,π β ) ) = 1/2, we can rearrange as follows + (η/β) P(πk πk+ 2 πk+ 1 2 πk+1) (1 + η) KL(πk+1πk+ 1 + (1 + η) KL(π βπk+1) KL(π ) + KL(πk+ 1 βπk) , 2 (12) πk) where P(πk πk+ 1 of preferences, see (1). To analyze P(πk πk+ 1 2ab a2 + b2 and the Pinksers inequality: πk+ 1 2 2 2 πk+1) is an expression that follows from the bilinear representation πk+1), we apply Lemma 4, an inequality πk+ 2 P(πk πk+ 1 2 πk+ 1 2 πk+1) 1 2 1 4 1 2 πk πk+ 1 (cid:16) 2 πk πk+ 1 πk+ 1 2 πk+11 2 1 + πk+ 1 2 πk+12 (cid:17) 2 (cid:16) KL(πk+ 1 2 πk) + KL(πk+1πk+ 2 (cid:17) ) . Finally, combining this with the assumption η 2β, (12) implies (1 + η) KL(π βπk+1) KL(π βπk) (η/β) SubOptβ(πk+ 1 2 , π β) . (13) Convergence in argument Now we shall use (13) to show the linear convergence of the iterates of Nash Mirror Prox in the argument. First, let us show that using the optimality of π β for any µ Π SubOptβ(µ, π β) 0, SubOptβ(µ, π β) = 1 Pβ(µ π β µ) β) = Pβ(π (cid:124) SubOptβ (π (cid:123)(cid:122) 1 2 (cid:125) β ,µ) 0 , Therefore, taking µ = πk+ 1 2 and using non-negativity of KL-divergence, we have for any 1, (1 + η) KL(π βπk+1) KL(π βπk) KL(π βπk) (1 + η)k KL(π βπ0) . Finally, using π0 = πref and Lemma 3 allows us to simplify the statement. Convergence in suboptimality Notably, since the underlying function is not smooth, convergence in argument does not directly imply the convergence for the exploitability gap. To prove the convergence in suboptimality, let us start from (13) and rearrange it 1 2β (η/β) SubOptβ(πk+ 1 β) KL(π (1 + η)k . βπk) , π (14) 2 However, convergence in suboptimality to an equilibrium is not enough. Before we turn to the suboptimality with respect to any competitor. Let us apply the first part of Lemma 6 with another competitor µ = π β and vk = 1 β P(πk ), β) P(πk πk+ 1 η KL(πk+ 1 2 2 )] + η KL(π πref ) KL(πk+ βπref ) + KL(π βπk) πk) (1 + η) KL(π βπk+ 1 ) . 2 (η/β)[P(πk π By rearranging the terms similar to (12), we get (1 + η) KL(π βπk+ 1 ) KL(π βπk) + (cid:16) η β Pβ(πk+ 1 π β) 1/2 (cid:17) + η β P(πk πk+ 2 πk+ 1 2 π β) KL(πk+ 1 πk) . 2 Next, Lemma 4, an inequality ab a2/2 + b2/2, and the Pinskers inequality imply P(πk πk+ 1 2 πk+ 1 π β) 1 2 1 2 πk πk+ 1 (cid:16) KL(πk+ 1 2 1 πk+ 1 2 π β πk) + KL(π βπk+ 1 2 (cid:17) ) . 17 Thus, by the inequality η 2β, we have η KL(π βπk+ 1 2 ) KL(π βπk) . (15) Given this preliminary result, Lemma 5 implies for any policy µ Π, SubOptβ(πk+ 1 2 , µ) SubOptβ(πk+ 1 By the Pinskers inequality and (15), we derive , π β) + πk+ 1 2 π β1 . πk+ 1 2 π β1 (cid:113) Overall, we have"
        },
        {
            "title": "2 KL(π⋆",
            "content": "βπk+ 1 2 ) (cid:114) 2 η KL(π βπk) (cid:114) 1 ηβ (1 + η)k , (16) SubOptβ(πk+ 1 2 , µ) 1 2η (1 + η)k + (cid:114) 1 ηβ (1 + η)k . Uniform convergence in span semi-norm. Next, we establish convergence in terms of the span semi-norm of log-probabilities. We notice that the policy at the step πk+1 can be written as follows β(1 + 1/η) log πk+1(y) = β log πref (y) + β/η log πk(y) P(πk+ 2 y) + ck+1 , where ck+1 is normalization constant. Next, Lemma 2 implies β(y) = β log πref (y) + β/η log π β(1 + 1/η) log π β(y) P(π β y) + . Combining these two representations, we get β(1 + 1/η)(cid:0)log πk+1(y) log π β(y)(cid:1) = β/η (cid:0)log πk(y) log π β πk+ 1 + P(π β(y)(cid:1) y) + (ck+1 c) . Taking span-norm and applying Lemma 4 yields (1 + η)log πk+1 log π βsp log πk log π βsp + η 2β πk+ 2 π β1 . Applying inequality (16), we have (1 + η)log πk+1 log π βsp log πk log π βsp + η 2β (cid:114) 1 ηβ (1 + η)k . Rolling out this expression, we derive log πk+1 log π βsp (1 + η)(k+1)log π0 log π (cid:88) βsp η 2β (cid:114) 1 ηβ + (1 + η)(kj)(1 + η)j/2 , j=0 where the second term could be rewritten as (cid:88) (1 + η)(kj)(1 + η)j/2 = (1 + η)k/2 j= (cid:88) j=0 (1 + η)k/2 1 + η 1 = (1 + η)k/2 Thus, we have ((cid:112)1 + η)j (1 + η)k/2 (cid:88) ((cid:112)1 + η)j k=0 1 + η + 1 η 2(1 + η)(k1)/2 η . log πk+1 log π βsp (1 + η)(k+1)log π0 log π βsp + 1 + η β (cid:114) 1 ηβ (1 + η)(k+1) . Using the fact that π0 = πref , we can simplify the latest expression since by Lemma 2 β log π β(y) = β log πref (y) 1 2 P(π β y) + log π0 log π βsp 1 2β . 18 Next, we establish the uniform convergence to an intermediate point. By the optimality conditions on πk+ 1 , we get 2 β(1 + η) log πk+ 1 (y) = β log πref (y) + β/η log πk(y) P(πk y) + ck+ 1 2 for some constant ck+ 1 2 . Using this expression, we have β(1 + 1/η) (cid:16) log πk+ 1 2 (y) log π β(y) (cid:17) = β/η (cid:0)log πk(y) log π + P(π β πk y) + (ck+ 1 β(y)(cid:1) c) , 2 and thus, using Lemma 4 (1 + η)log πk+ 1 log π βsp log πk log π βsp + η 2β πk π β1 . By the Pinskers inequality and already established results on the convergence of πk, we have log πk+ 1 2 log π βsp 1 β (1 + η)(k+1) + 1 β Finally, we prove the required statement by noting that (cid:114) 1 ηβ (cid:16)(cid:113) 1 (1 + η)k + (cid:114) 1 β (1 + η)k . η 2β(1 + η) (cid:17) η + η 2(1+η) 3 2 η for η 2β 1. Corollary (Restatement of Corollary 1). Assume that β 1/2, then for ε > 0, the final policy (cid:17)(cid:109) of NashMP with η = 2β is ε-VNW in β-regularized preference game after = iterates and 2K preference oracle calls. Additionally, for uniform reference policy πref (y) = 1/Y for all Y, specifically chosen β = β(ε) and an initial policy π0 = πref , the final policy πK+1/2 is ε-VNW in the original preference game after = β log iterates. (cid:16) 2 εβ (cid:108) 1+β log (cid:17)(cid:109) (cid:16) 8 log(Y ) ε (cid:108) 8 log(Y ) ε Proof. The first statement is corollary of Theorem 1 and the inequality log(1 + x) x/(1 + x/2) for any > 0. The second statement follows from the following representation of β-regularized suboptimality: SubOptβ(πK+ 1 ) = max πΠ SubOpt(πK+ 1 (cid:110) , π) + β KL(πK+ 1 2 πref ) β KL(ππref ) (cid:111) 2 max πΠ SubOpt(πK+ 1 2 , π) β min πΠ KL(ππref ) = SubOpt(πK+ 1 2 ) β log(Y ) . Thus, after steps, SubOpt(πK+ 1 ) SubOptβ(πK+ 1 2 ) + β log(Y ) . Taking β = β(ε) = ε/2 1/ log(Y ), we have to guarantee the inequality SubOptβ(πK) ε/2, that holds after = iterates. Substituting β = β(ε), we conclude the proof. (cid:108) 1+β (cid:17)(cid:109) β log (cid:16) 4 εβ A.2 Technical lemmas Lemma 2. Let π β be von Neumann winner in β-regularized preference game: (π β, π β) = arg max πΠ min πΠ P(π π) β KLρ(ππref ) + β KLρ(ππref ) . Then π β satisfies for any supp(ρ), supp(πref (x)): β log π β(yx) = β log πref (yx) P(π β yx) + c(x), where c(x) is function that depends only on x. Proof. By the definition of VNW, we have µ Π : Pβ(π β µ) 1 2 min µΠ Pβ(π β µ) 1 2 . 19 By the symmetry of the game, we get Pβ(π β π In particular, it implies for any supp(ρ), π β arg min µΠ β) = 1/2. Thus, Pβ(π β µ) . π β(x) arg min µ(Y) P(π β µx) + β KL(µπref x) , and, by strong convexity of the problem, the solution is unique and has the following form for all supp(πref (x)), β(yx) πref (yx) exp π P(π β yx) . (cid:19) (cid:18) 1 β Taking the logarithm, we conclude the proof. Lemma 3. Let π β be VNW in β-regularized preference game. Then KLρ(ππref ) 1 2β . Proof. First, we note that since π β is VNW, then 1 Pβ(π β πref ) = P(π β πref ) β KLρ(π βπref ) . After rearranging the terms, we have KL(π βπref ) 1 β (cid:18) P(π β πref ) (cid:19) 1 2 1 2β . Lemma 4. Consider context-free setting, i.e., = . Then, for any policies π, πµ, µ Π, it holds P(π µ) P(π µ) π π1 , 1 2 and P(π π µ µ) 1 2 π π1 µ µ1 . Proof. Let us define vector with components v(y) = P(y µ) 1/2. Notice that v(y) [1/2, 1/2]. Hence, we derive P(π µ) P(π µ) = π π, v π π1 1 2 π π1 . For the second part, we apply the representation (1) to get P(π π µ µ) = π π, P(µ µ) π π1 P(µ µ) . Next, using the fact that µ µ, 1 = 0, we have for any Y, P(µ µ)y = (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) yY (cid:88) yY and thus, we have P(y y)(µ(y) µ(y)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) yY (P(y y) 1/2)(µ(y) µ(y)) max (P(y y) 1/2)(µ(y) µ(y)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) P(y y) 1/2µ µ1 , P(µ µ)y max y,y P(y y) 1/2 µ µ1 . Finally, since P(y, y) [0, 1] for any y, y, we have P(y, y) 1/2 1/2. 20 Lemma 5. Let us consider context-free preference game and let π preference game. Then, for any policies π, µ, it holds β Π be VNW in β-regularized SubOptβ(π, µ) SubOptβ(π, π β) + π π β1 . Proof. By definition, we have SubOptβ(π, µ) = = 1 2 1 2 (cid:124) P(π µ) + β KL(ππref ) β KL(µπref ) P(π π β) + β KL(ππref ) β KL(π βπref ) (cid:125) (cid:123)(cid:122) SubOptβ (π,π β ) 1 + (cid:124) P(π β µ) + β KL(π βπref ) β KL(µπref ) (cid:125) (cid:123)(cid:122) SubOptβ (π β ,µ) + P(π π β π β µ) , where by the bilinearity of (see (1)), we have P(π π β π µ) P(π Lemma 4 implies β µ), and by the symmetry P(π β) + P(π β π β µ) = P(π π β) P(π β) = 1/2. For the last term, β π P(π π β π β µ) π π β1 . Thus we have SubOptβ(π, µ) SubOptβ(π, π β) + SubOptβ(π β, µ) + π π β1 . Next, we notice that by the definition of VNW, µ Π : SubOptβ(π β, µ) = 1 2 Pβ(π β µ) 0 . A.2.1 Mirror Prox Lemmas Let us consider the generalized Nash Mirror Prox iterates defined as follows πk+ 2 = arg min π πk+1 = arg min π (cid:8)ηvk, π + η KL(ππref ) + KL(ππk)(cid:9) , (cid:110) , π + η KL(ππref ) + KL(ππk) ηvk+ 1 2 (cid:111) . (17) In particular, if we consider vk = 1 deterministic version of Nash Mirror Prox. Lemma 6. Each iterate πk+ 1 2 and πk+1 of (17) satisfies for any µ, µ Π, β P(πk ) and vk+ 1 2 = β P(πk+ 1 2 ), we recover the ηvk, µ πk+ 1 2 + η KL(µπref ) + KL(µπk) η KL(πk+ 1 2 πref ) KL(πk+ 1 2 πk) (1 + η) KL(µπk+ 1 ) and ηvk+ 1 2 , µ πk+1 + η KL(µπref ) + KL(µπk) η KL(πk+1πref ) KL(πk+1πk) (1 + η) KL(µπk+1) Proof. First, let us notice that the proof of the first relation automatically results in the proof of the second one due to the same structure; thus, without loss of generality, we prove only the first relation. Let us consider the first-order optimality conditions of the first equation in (17) for the constrained optimization problem: µ Π : ηvk + η KL(πk+ 1 2 πref ) + KL(πk+ 1 2 πk), µ πk+ 2 0 , (18) 21 where the gradient of the KL divergence is taken with respect to the first argument. In particular, we have KL(ππ) = log π log π + 1 , where the logarithm is taken element-wise. Thus, we have KL(πk+ 1 2 πref ), µ πk+ 1 2 = log πk+ 2 log µ + log µ log πref , µ πk+ 1 ) + KL(µπref ) KL(πk+ 1 2 πref ) . = KL(µπk+ 1 2 Using the same argument, we derive KL(πk+ 1 2 πk), µ πk+ 2 = KL(µπk+ 1 2 ) + KL(µπk) KL(πk+ 1 2 πk) . Plugging-in the derived expression to (18) implies ηvk, µ πk+ 1 2 KL(µπref ) KL(πk+ 1 2 (cid:16) + η (cid:16) + KL(µπk) KL(πk+ 1 2 πref ) KL(µπk+ 1 (cid:17) ) πk) KL(µπk+ 2 2 (cid:17) ) 0 and after rearranging the terms, we conclude the proof."
        },
        {
            "title": "B Analysis of Approximate Nash Mirror Prox",
            "content": "B.1 General Approximate Case In this section, we assume that we do not have access to the true preference model but only to the duels results. In this case, the inner optimization step of Nash Mirror Prox becomes infeasible, and we need to approximate it. The iterates of the approximate Nash Mirror Prox are defined as follows (cid:98)πk+1/2 arg min πΠ (cid:98)πk+1 arg min πΠ (cid:8)(η/β)P((cid:98)πk π) + η KL(ππref ) + KL(π(cid:98)πk)(cid:9) , (cid:8)(η/β)P((cid:98)πk+1/2 π) + η KL(ππref ) + KL(π(cid:98)πk)(cid:9) , (19) where the approximate solution is defined as follows: log (cid:98)πk+1/2 log πk+1/2sp εk+1/2, log (cid:98)πk+1 log πk+1sp εk+1, where πk+1/2 and πk+1 are the exact minimizes of the corresponding problems in (19). Theorem 3 (Convergence of Approximate Nash Mirror Prox). Assume β < 1/2. Let (cid:98)πk be the iterates generated by the approximate Nash Mirror Prox algorithm (19) with maxk{εk+1/2, εk} ε. After iterations with constant learning rate η 2β 1 and π0 = πref , the suboptimality SubOptβ((cid:98)πK+1/2) maxµΠ{ 1 2 Pβ((cid:98)πK+1/2 µ)} satisfies SubOptβ((cid:98)πK+1/2) (cid:18) (1 + η)K 2β β η + (cid:19) 6ε η + (cid:115) (cid:18) (1 + η)K 2β 2 η + (cid:19) . 6ε η At the same time, the algorithm enjoys convergence to neighborhood of the optimal solution in KL divergence: KL(π β(cid:98)πK) 1 2β (1 + η)K + 4ε η , as well as the uniform convergence in the span semi-norm: log (cid:98)πK log π βsp (1 + η)K 2β + ε η + 1 + η β (cid:114) 1 ηβ (1 + η)K + (cid:112)2(1 + η) ε βη , log (cid:98)πK+ 1 2 log π βsp (1 + η)K 2β(1 + η) + (1 + η)ε η + 3 2β (cid:114) 1 ηβ (1 + η)K + 2(cid:112)2(1 + η)ε β η (1 + η) . 22 Proof. Let us consider an arbitrary step 0. First, apply the first inequality of Lemma 8 with µ = (cid:98)πk+1, vk = 1 β P((cid:98)πk ) and the proximal center (cid:98)πk: ηvk, (cid:98)πk+1 (cid:98)πk+1/2 + η KL((cid:98)πk+1πref ) + KL((cid:98)πk+1(cid:98)πk) η KL((cid:98)πk+1/2πref ) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) 2(1 + η)εk+1/2 . Substituting vk = 1 β P((cid:98)πk ) yields (η/β)[P((cid:98)πk (cid:98)πk+1) P((cid:98)πk (cid:98)πk+1/2)] + η KL((cid:98)πk+1πref ) + KL((cid:98)πk+1(cid:98)πk) η KL((cid:98)πk+1/2πref ) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) 2(1 + η)εk+1/2 . β, vk+1/2 = Secondly, we apply the second inequality of Lemma 8 with µ = π the proximal center (cid:98)πk: β P((cid:98)πk+1/2 ) and ηvk+1/2, π β (cid:98)πk+1 + η KL(π βπref ) + KL(π η KL((cid:98)πk+1πref ) KL((cid:98)πk+1(cid:98)πk) (1 + η) KL(π β(cid:98)πk) β(cid:98)πk+1) 2(1 + η)εk+1 . Substituting vk+1/2 = 1 β P((cid:98)πk+1/2 ) gives (η/β)[P((cid:98)πk+1/2 π β) P((cid:98)πk+1/2 (cid:98)πk+1)] + η KL(π βπref ) + KL(π β(cid:98)πk) (cid:16) (cid:17) η KL((cid:98)πk+1πref ) + KL((cid:98)πk+1(cid:98)πk) β(cid:98)πk+1) 2(1 + η)εk+1 . Combining these two inequalities, the underlined terms cancel out and we get (1 + η) KL(π (η/β)[P((cid:98)πk (cid:98)πk+1) P((cid:98)πk (cid:98)πk+1/2) + P((cid:98)πk+1/2 π β) P((cid:98)πk+1/2 (cid:98)πk+1)] β(cid:98)πk) η KL((cid:98)πk+1/2πref ) KL((cid:98)πk+1/2(cid:98)πk) + η KL(π βπref ) + KL(π (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) + (1 + η) KL(π β(cid:98)πk+1) 2(1 + η)(εk+1/2 + εk+1) . Using the identity Pβ(π π) = P(π π) β KL(ππref ) + β KL(ππref ), we can group terms like (η/β)P((cid:98)πk+1/2 π β) η KL((cid:98)πk+1/2πref ) + η KL(π βπref ) = (η/β)Pβ((cid:98)πk+1/2 π β). Substituting this into the previous inequality and rearranging yields (η/β)[P((cid:98)πk (cid:98)πk+1) P((cid:98)πk (cid:98)πk+1/2) P((cid:98)πk+1/2 (cid:98)πk+1)] + (η/β)Pβ((cid:98)πk+1/2 π β) + KL(π β(cid:98)πk) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) + (1 + η) KL(π β(cid:98)πk+1) 2(1 + η)(εk+1/2 + εk+1) . Using P(π, π) = 1/2 and the definition of suboptimality SubOptβ(π, π) = 1/2 Pβ(π π), we derive (η/β)[P((cid:98)πk (cid:98)πk+1/2 (cid:98)πk+1/2 (cid:98)πk+1) 1/2] + (η/β)[1/2 SubOptβ((cid:98)πk+1/2, π β)] + KL(π β(cid:98)πk) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) + (1 + η) KL(π β(cid:98)πk+1) 2(1 + η)(εk+1/2 + εk+1) . and after rearranging (η/β) SubOptβ((cid:98)πk+1/2, π + KL(π β(cid:98)πk) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) + (1 + η) KL(π β(cid:98)πk+1) β) (η/β)P((cid:98)πk (cid:98)πk+1/2 (cid:98)πk+1/2 (cid:98)πk+1) (20) 2(1 + η)(εk+1/2 + εk+1) . 23 Bounding the bilinear term as in the proof of Theorem 1, we get P((cid:98)πk (cid:98)πk+1/2 (cid:98)πk+1/2 (cid:98)πk+1) 1 2 KL((cid:98)πk+1/2(cid:98)πk) + 1 2 KL((cid:98)πk+1(cid:98)πk+1/2) . Substituting this bound into (20) gives (η/β) SubOptβ((cid:98)πk+1/2, π β) + η 2β + KL(π [KL((cid:98)πk+1/2(cid:98)πk) + KL((cid:98)πk+1(cid:98)πk+1/2)] β(cid:98)πk) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL((cid:98)πk+1(cid:98)πk+1/2) + (1 + η) KL(π β(cid:98)πk+1) 2(1 + η)(εk+1/2 + εk+1) . Since η 2β, we have η/(2β) 1 0. Dropping the non-positive term involving KL((cid:98)πk+1/2(cid:98)πk) and bounding the term with KL((cid:98)πk+1(cid:98)πk+1/2) yields (η/β) SubOptβ((cid:98)πk+1/2, π β) + KL(π β(cid:98)πk) η KL((cid:98)πk+1(cid:98)πk+1/2) (1 + η) KL(π β(cid:98)πk+1) 2(1 + η)(εk+1/2 + εk+1) . Finally, rearranging gives the following inequality for the approximate case: β(cid:98)πk+1) KL(π β(cid:98)πk) (η/β) SubOptβ((cid:98)πk+1/2, π β) (1 + η) KL(π At the same time, if we plug in πk+1 instead of (cid:98)πk+1 in all the previous bounds, we get + 2(1 + η)(εk+1/2 + εk+1) . (1 + η) KL(π βπk+1) KL(π β(cid:98)πk) (η/β) SubOptβ((cid:98)πk+1/2, π β) + 2(1 + η)εk+1/2 . (21) (22) Convergence in argument Let KL(π SubOptβ((cid:98)πk+1/2, π rence from = 0 to 1 yields β(cid:98)πk) and Ek (εk+1/2 + εk+1). Since β) 0, we have (1 + η)k+1 + 2(1 + η)Ek. Unrolling this recurK (1 + η)K0 + K1 (cid:88) (1 + η)(K1j)(εj+1/2 + εj+1) , (23) j=0 where the accumulated error can be bounded as follows K1 (cid:88) (1 + η)(K1j)(εj+1/2 + εj+1) 4 2 j=0 K1 (cid:88) j=0 (1 + η)(K1j) ε 4ε η . The choice π0 = πref and the bound on 0 from Lemma 3 finish the proof. Convergence in suboptimality From (22) we have (η/β) SubOptβ((cid:98)πk+1/2, π β) (1 + η) KL(π βπk+1) + 2(1 + η)εk+1/2 + 2(1 + η)εk+1/2 . (24) First, apply the first inequality of Lemma 8 with µ = π β and vk = 1 β P((cid:98)πk ) to get (η/β)[P((cid:98)πk π β) P((cid:98)πk πk+1/2)] + η KL(π βπref ) + KL(π η KL((cid:98)πk+1/2πref ) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL(π β(cid:98)πk) β(cid:98)πk+1/2) 2(1 + η)εk+1/2 . By rearranging the terms similarly to (12), we derive (1 + η) KL(π β(cid:98)πk+1/2) KL(π β(cid:98)πk) + η (cid:0)Pβ((cid:98)πk+1/2 π β P((cid:98)πk (cid:98)πk+1/2 (cid:98)πk+1/2 π + η β β) 1/2(cid:1) + 2(1 + η)εk+1/2 β) KL((cid:98)πk+1/2(cid:98)πk) . 24 Next, Lemma 4, an inequality ab a2/2 + b2/2, and the Pinskers inequality imply P((cid:98)πk (cid:98)πk+1/2 πk+1/2 π β) 1 2 KL((cid:98)πk+1/2(cid:98)πk) + 1 2 KL(π β(cid:98)πk+1/2) . Thus, by the inequality η 2β, we have η KL(π β(cid:98)πk) + 2(1 + η)εk+1/2 . Applying Lemma 5 for any policy µ Π and the Pinskers inequality gives SubOptβ((cid:98)πk+1/2, µ) SubOptβ((cid:98)πk+1/2, π β(cid:98)πk+1/2) KL(π β) + (cid:98)πk+1/2 π β1 . (25) Using the Pinskers inequality again and the bound, we derive (25): (cid:98)πk+1/2 π β1 (cid:113)"
        },
        {
            "title": "2 KL(π⋆",
            "content": "β(cid:98)πk+1/2) (cid:114) 2 η (k + 2(1 + η)εk+1/2) . (26) Now substitute this and the bound (24) for SubOptβ((cid:98)πk+1/2, π expression: β) into the overall suboptimality SubOptβ((cid:98)πk+1/2, µ) SubOptβ((cid:98)πk+1/2, π β η (k + 2(1 + η)εk+1/2) + β) + P((cid:98)πk+1/2 π (cid:115) β π β µ) 2(k + 2(1 + η)εk+1/2) η . Now, we use the bound (23) and the fact that 0 1/2β to get + 2(1 + η)εk+ 1 (1 + η)k 2β + 2 k1 (cid:88) j=0 (1 + η)(k1j)(εj+ + εj+1) + 2(1 + η)εk+ 1 2 2 (1 + η)k 2β + 4ε (cid:88) j=0 (1 + η)(k1j) 1 + η)k 2β + 4(1 + η)ε η .. (27) Since 1 + η 3/2, SubOptβ((cid:98)πk+1/2, µ) (cid:18) (1 + η)k 2β β η + (cid:19) 6ε η + (cid:115) (cid:18) (1 + η)k 2β 2 η + (cid:19) . 6ε η Uniform convergence in span semi-norm. Next, we use the same arguments as in the exact case. First, the optimal policy for step + 1 can be represented as follows β(1 + 1/η) log πk+1(y) = β log πref (y) + β/η log (cid:98)πk(y) P((cid:98)πk+1/2 y) + ck+1 , where ck+1 is some normalization constant. Next, Lemma 2 implies β(1 + 1/η) log π β(y) = β log πref (y) + β/η log π β(y) P(π β y) + . Combining these two representations, we have β(1 + 1/η)(cid:0)log πk+1(y) log π β(y)(cid:1) = β/η (cid:0)log (cid:98)πk(y) log π β(y)(cid:1) + P(π β (cid:98)πk+1/2 y) + (ck+1 c) . Taking span-norm and applying Lemma 4 yields (1 + η)log πk+1 log π βsp log (cid:98)πk log π βsp + η 2β (cid:98)πk+1/2 π β1 . Applying inequality (26), we derive (1 + η)log πk+1 log π βsp log (cid:98)πk log π βsp + (cid:115) η 2β 2(k + 2(1 + η)εk+1/2) η . (28) Using the definition of approximation by (cid:98)πk and (27), we have βsp log (cid:98)πk log π (1 + η)log (cid:98)πk+1 log π (cid:115) 2 η (1 + η)k0 + η 2β + (cid:18) βsp + (1 + η)εk+1 (cid:19) 4(1 + η)ε η (cid:115) log (cid:98)πk log π βsp + (1 + η)ε + Rolling out this expression, we get η 2β (1 + η)k ηβ (cid:112)2(1 + η)ε β . + log (cid:98)πk+1 log π βsp (1 + η)(k+1)log π0 log π βsp + (cid:88) j=0 (1 + η)(kj) ε + η 2β (cid:88) j=0 (1 + η)(kj) (cid:115) (1 + η)j ηβ + (cid:88) (1 + η)(kj) j=0 (cid:112)2(1 + η)ε β . The third term could be controlled in the same manner as in the proof of the exact case (see Theorem 1): η 2β (cid:88) (1 + η)(kj) (cid:115) j=0 (1 + η)j ηβ 1 + η β (cid:114) 1 ηβ (1 + η)(k+1) , whereas all other error terms are controlled by geometric sum: log (cid:98)πk+1 log π βsp (1 + η)(k+1)log π0 log π (cid:114) 1 ηβ 2(1 + η) β (1 + η)(k+1) + + βsp + ε η (cid:112)2(1 + η)ε β η . bound log πref log π βsp 1/(2β) allows us to conclude the proof. Finally, we establish the uniform convergence to an intermediate point. By the optimality conditions on πk+1/2 we have β(1 + 1/η) log πk+1/2(y) = β log πref (y) + β/η log (cid:98)πk(y) P((cid:98)πk y) + ck+1/2 for some constant ck+1/2. Using this expression, we get β(1 + 1/η)(cid:0)log πk+1/2(y) log π β(y)(cid:1) = β/η (cid:0)log (cid:98)πk(y) log π β(y)(cid:1) + P(π β (cid:98)πk y) + (ck+1/2 c) , and using Lemma 4 (1 + η)log πk+1/2 log π βsp log (cid:98)πk log π βsp + η 2β (cid:98)πk π β1 . (29) By the Pinskers inequality and already established results on the convergence of (cid:98)πk, we get 2ε log πk+1/2 log π (1 + η)k + βsp + + β η 1 + η (1 + η)k 2β(1 + η) + η 2β(1 + η) ε η(1 + η) (cid:115) 1 β (cid:18) (1 + η)k 2 2β (cid:114) 1 ηβ (cid:19) . + 4ε η Next, we simplify this expression by using the bounds 1/ η for η 1, and (cid:112)η/(1 + η) 1/η : + for a, 0, η/(1 + η) + log πk+1/2 log π βsp (1 + η)k 2β(1 + η) + ε η(1 + η) + 3 2β (cid:114) 1 ηβ (1 + η)k + 2 2ε 1 + η β η . (30) 26 Finally, we apply the approximation of πk+1/2 by (cid:98)πk+1/2 and notice that Now the choice = allows us to conclude the proof. 1 η(1 + η) + 1 1 + η η . Next, we provide an iteration complexity result. Theorem (Restatement of Theorem 2). Assume that β 1/2 and (cid:98)πk+p/2 πk+p/2sp ε for ε (0, 1) and all {0, . . . , 1}, {1, 2}, where πk+p/2 are exact solutions to the iterations, the policy (cid:98)πK+1/2 is corresponding problems in (8). Then after = 2β log(cid:0) 1 (cid:108) 1+β (cid:1)(cid:109) ε 4 ε β -VNW in β-regularized game. Proof. From Theorem 3 it holds for η = 2β, SubOptβ((cid:98)πK+1/2) (cid:18) (1 + 2β)K 2β 1 2 + (cid:19) 3ε β + (cid:115) (cid:18) (1 + 2β)K 2β 1 β + (cid:19) 3ε β 1 4β (cid:0)(1 + 2β)K + 6ε(cid:1) + (cid:113) (1 + 2β)K + 6ε . 1 2 β Thus, we see that the inequality (1 + 2β)K ε is achieved after 1+β with ε 1, we get (cid:1) iterations. Then 2β log(cid:0) 1 ε SubOptβ((cid:98)πK+1/2) 1 β (cid:32) (cid:33) 7ε + (cid:114) 7ε 2 4 ε . β B.2 Sample-Based Approximate Nash Mirror Prox In this section, we propose direct way to approximate Nash Mirror Proxs steps using only the sample-available information on the preference model. In particular, to approximate each step of Nash Mirror Prox in (19), we use the stochastic policy gradient method with softmax parametrization, that is, we parameterize our policies in the following way Given this parameterization, we define θ β as the optimal parameters that correspond to π β. πθ(y) exp{θ(y)} yY exp{θ(y)} (cid:80) . Approximation of the intermediate update. We rewrite the first part of (19) using our parameterization: 2 Jk+ (θ) P((cid:98)πk πθ) + β KL(πθπref ) + KL(πθ(cid:98)πk) , k+1/2 as minimizer to this problem. As result, πk+1/2 = πθ and we define θ . Our goal is to compute an approximation of θ k+1/2 that we denote by θk+1/2. This approximation shall satisfy log πk+1/2 log πθk+1/2 εk+1/2. To construct such an approximation we apply stochastic gradient descent of the form (31) k+1/2 β η where the stochastic gradient can be computed using standard REINFORCE estimate: θk+1/2,t+1 = θk+1/2,t γ (cid:98)Jk+ 1 2 (θk+1/2,t) , (cid:98)Jk+ 1 (θk+1/2,t) = 1 (cid:88) (cid:18) j=1 oj k+1/2,t 1 2 + β log k+1/2,t) (cid:33) (cid:32) πθk+1/2,t(yj πref (yj (cid:33)(cid:19) k+1/2,t) k+1/2,t) log πθk+1/2,t(yj k+1/2,t) , + β η log (cid:32) πθk+1/2,t (yj (cid:98)πk(yj k+1/2,t) 27 k+1/2,t}B sample from (cid:98)πk, {yj j=1 is an i.i.d. k+1/2,t}B where {yj sample from πθk+1/2,t, and {oj k+1/2,t Ber(P(yj k+1/2,t)). We perform Tk+1/2 updates of this form with value of Tk+1/2 to specified later and θk as an initial value. Finally we define θk+1/2 = θk+1/2,Tk+1/2 and (cid:98)πk+1/2 = πθk+1/2 . k+1/2,t}B k+1/2,t against yj j=1 are comparison results of yj k+1/2,t, that is, oj k+1/2,t yj j=1 is an i.i.d. Approximation of the final update. Next, we rewrite the second part of (19) Jk+1(θ) P((cid:98)πk+1/2 πθ) + β KL(πθπref ) + β η KL(πθ(cid:98)πk) , (32) and we define θ we also apply stochastic gradient descent k+1 as minimizer to this problem and set πk+1 = πθ k+1 . To approximate this step, where the stochastic gradient is again computed using standard REINFORCE estimate: θk+1,t+1 = θk+1,t γ (cid:98)Jk+1(θk+1,t) , (cid:98)Jk+1(θk+1,t) = (cid:88) (cid:18) j=1 1 + β log oj k+1,t 1 2 (cid:32) πθk+1,t(yj (cid:98)πk(yj j=1 is an iid sample from (cid:98)πk+1/2, {yj β η log + k+1,t) (cid:33) k+1,t) k+1,t) (cid:32) πθk+1,t(yj πref (yj (cid:33)(cid:19) k+1,t) log πθk+1,t(yj k+1,t) , k+1,t}B j=1 are comparison results of yj where {yj j=1 is an iid sample from πθk+1,t, and {oj k+1,t against yj k+1,t}B k+1,t yj k+1,t)). We perform Tk+1 updates of this form with value of Tk+1 to be specified later. After these steps, we define θk+1 = θk+1,Tk+1 and (cid:98)πk+1 = πθk+1. For complete algorithm description, we refer to Algorithm 1. k+1,t Ber(P(yj k+1,t, that is, oj k+1,t}B Analysis. To perform analysis of this approximate version of the algorithm, we first notice that (θ) and Jk+1(θ) is an entropy regularized multi-armed bandit the problem of minimization of Jk+ 1 problem with the reward function equal to r(y) = P(πk+p y) + β log πref (y) + β/η log (cid:98)πk(y) for {0, 1/2}, and an entropy regularization coefficient equal to β(1 + 1/η). Thus, we can apply the results on stochastic policy gradients for this type of problem, in particular, Proposition 2. Let us define problem-dependent quantity: β min yY (cid:18) π β(y) exp (cid:19) . 34 β η (33) Lemma 7. For ε < 1/3 and under the choice (ε) = log (cid:19) (cid:18) 12 β ε 4 β , B(ε) = (cid:32) 16 log(12YKT 2/δ) β ε (cid:33)2 , γ = η β (1 + η) , the following event E(δ) = (cid:8)k [K] : log (cid:98)πk+1/2 log πk+1/2sp ε, log (cid:98)πk+1 log πk+1sp ε(cid:9) , holds with probability at least 1 δ. Proof. First, note that under the choice θ0 = log πref , Lemma 2 implies θ0 θ βsp = log πref log π βsp 1 β P(π β ) 1/2sp 1 2β . (34) Next, we prove the initial statement by induction over k. Let us define sequence of events Ek(δ) = (cid:8)log (cid:98)πk+1/2 log πk+1/2sp ε, log (cid:98)πk+1 log πk+1sp ε(cid:9) , 28 j<k Ej(δ) holds. In particular, it implies that for any < k, it holds εj+1/2, εj+1 ε. j<k Ej(δ)] δ/K for every [K]. and we show that P[Ek(δ) (cid:84) Assume that (cid:84) For = 0, the condition trivially holds. Let us define ck+1/2(θk) miny πk+1/2(y) exp(2θk θ miny πk+1(y) exp(2θk+1/2 θ policy, ck+1/2 and ck+1. k+1/2sp) and ck+1(θk+1/2) k+1sp) Our next goal is to bound the distance to the optimal Bound on θk θ k+1/2sp. We want to show that θk θ k+1/2sp 10/β."
        },
        {
            "title": "We can select the following optimal solution to this problem",
            "content": "θ k+1/2 = β log πref () + β/η log (cid:98)πk() P((cid:98)πk ) β(1 + 1/η) = η 1 + η log πref () η β (1 + η) P((cid:98)πk ) + 1 1 + η log (cid:98)πk . Since log (cid:98)πk = θk + ck1 for some constant ck R, we have θ k+1/2 θksp = η β(1 + η) (cid:13) (cid:13) β log πref () P((cid:98)πk ) β log (cid:98)πk() (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)sp . Lemma 2 implies β log π β(y) = β log πref (y) P(π β y) + c1 for constant R. Thus, θ k+1/2 θksp η β(1 + η) (cid:0)P((cid:98)πk π β )sp + β log (cid:98)πk β log π βsp (cid:1) . Next, we use Lemma 4 and the Pinskers inequality to derive θ k+1/2 θksp η β(1 + η) (cid:16)(cid:113) 2 KL(π β(cid:98)πk) + β log (cid:98)πk β log π βsp (cid:17) . Now we use the convergence guarantees from Theorem 3: (cid:113) 2 KL(π β(cid:98)πk) (cid:113) 2(1 + η)k KL(π βπ0) + 2 (cid:114) 2ε η , the inequality β log (cid:98)πk β log π βsp (1 + η)kβ log π0 β log π (cid:114) 2 η (1 + η)k KL(π + 2(1 + η) βsp + βπ0) + β ε η (cid:112)2(1 + η) ε η , the inequality KL(π βπ0) log π0 log π θ k+1/2 θksp η β(1 + η) (1 + η)k + βsp and (34) to derive (cid:18) 1 2 (cid:114) 1 η β (1 + η)k + 3 β ε η + 3(1 + η) (cid:112)2(1 + η)ε η (cid:19) . Furthermore, we can simplify this bound using assumptions ε 1 and η β 1 : θ k+1/2 θksp 1 2(1 + η) + ε 1 + η + 3 η β3/2 + 3 (cid:112)2ε(1 + η) β(1 + η) 4(1 + β 2) 10 β . (35) Bound on θk+1/2 θ the bound k+1sp. We want to show that θk+1/2 θ k+1sp 12/β. We start from θ k+1 = β log πref () + β/η log (cid:98)πk() P((cid:98)πk+1/2 ) β(1 + 1/η) = η 1 + η log πref () η β (1 + η) P((cid:98)πk ) + 1 1 + η log (cid:98)πk , which together with Lemma 4 implies θ k+1 θ k+1/2sp η β(1 + η) P((cid:98)πk+1/2 (cid:98)πk )sp η 2β(1 + η) (cid:98)πk+1/2 (cid:98)πk1 . Then, we apply the triangle inequality and Lemma 14 to connect ℓ1-norm with the span semi-norm of parameters: 1 2 (cid:98)πk+1/2 (cid:98)πk1 θk+1/2 θ k+1/2sp + θk θ k+1/2sp ε + 10 β 11 β . Thus, by the triangle inequality θ k+1 θk+1/2 ε + η β(1 + η) (ε + 10 β ) 12 β . Bound on ck+1/2(θk). We want to show that ck+1/2(θk) parameters equal to log-probabilities: log π β(y) log π β log πk+1/2 2θk θ k+1/2sp β. Let us apply Lemma 9 with softmax log π β(y) 2log π β log πk+1/2sp 2θk θ k+1/2sp . log ck+1/2(θk) min yY min yY To control the second term, we apply (30) from the proof of Theorem 3 and do similar simplifications as in (35): log πk+1/2 log π βsp (1 + η)(k+1)θ0 θ βsp + ε η(1 + η) 3 2β (cid:114) 2 η + 1 2β(1 + η) (1 + η)k KL(π βπ0) + 3 2 β η 3 2β η + + 2(cid:112)2(1 + η)ε β η (1 + η) 2 + 3 β η 2 7 βη . Thus, combining these two bounds, we derive log ck+1/2(θk) min yY log π β(y) 34 β η = log β . Bound on ck+1(θk+1/2). We want to show that ck+1(θk+1/2) β. We start with Lemma 9 and softmax parameters equal to log-probabilities: log ck+1(θk+1/2) min yY min yY log π β(y) log π β log πk+1 2θk+1/2 θ k+1sp log π β(y) 2log π β log πk+1sp 2θk+1/2 θ k+1sp . Next, we use the convergence in the span semi-norm from Theorem 3 considering εk+1 = 0 (and, correspondingly, (cid:98)πk+1 = πk+1): log πk+1 log π βsp (1 + η)(k+1)log π0 log π (cid:114) 2 η (1 + η)(k+1) KL(π + βsp + βπ0) + (cid:112)2(1 + η)ε β η ε η θ0 θ βsp + ε η + 2 β(cid:112)η(1 + η) (cid:113) 2θ0 θ βsp , 2(1 + η) β 1 1 + η 30 and, applying (34), we have thus log πk+1 log π βsp 1 2β + 1 η + 2 βη 4 βη , log ck+1(θk+1/2) min yY log π β(y) 32 βη log β. Accuracy bound. The next step is to apply Proposition 2. First, it is easy to verify that Assumption 1 holds with = 1, and our setting corresponds to λ = β(1 + 1/η). Thus, taking γ = 1/(1 + η) η/β, we can see that for (ε) = log 4 β (cid:19) (cid:18) 12 β ε , and B(ε) = (cid:32) 16 log(12YKT 2/δ) (1 + η) β (cid:33)2 1 ε2 , Proposition 2 implies that the following event holds with probability at least 1 δ/K on the event (cid:84) j<k Ej(δ): (cid:26) Ek(δ) = θk+1/2 θ k+1/2sp ε, θk+1 θ k+1sp ε (cid:27) . Applying union bound, we conclude the proof. B.3 Technical Lemmas Let us consider generalized iterates of the approximate Nash Mirror Prox iterates defined as follows πk+1/2 = arg min π πk+1 = arg min π (cid:8)ηvk, π + η KL(ππref ) + KL(π(cid:98)πk)(cid:9) , (cid:8)ηvk+1/2, π + η KL(ππref ) + KL(π(cid:98)πk)(cid:9) , (36) where (cid:98)πk+1/2 and (cid:98)πk+1 are approximation of πk+1/2 and πk+1 in the following sense log (cid:98)πk+1/2 log πk+1/2sp εk+1/2 , log (cid:98)πk+1 log πk+1sp εk+1 . (37) Lemma 8. Let (cid:98)πk+1/2 and (cid:98)πk+1 be approximate Nash Mirror Prox iterates (36) in the sense of (37). Then for any µ, µ Π ηvk, µ (cid:98)πk+1/2 + η KL(µπref ) + KL(µ(cid:98)πk) η KL((cid:98)πk+1/2πref ) KL((cid:98)πk+1/2(cid:98)πk) (1 + η) KL(µ(cid:98)πk+1/2) 2(1 + η)εk+1/2 , and ηvk+1/2, µ (cid:98)πk+1 + η KL(µπref ) + KL(µ(cid:98)πk) η KL((cid:98)πk+1πref ) KL((cid:98)πk+1(cid:98)πk) (1 + η) KL(µ(cid:98)πk+1) 2(1 + η)εk+1 . Proof. First, let us notice that the proof of the first relation automatically results in the proof for the second one due to the same structure; thus, without loss of generality, we prove only the first one. Let us consider the first-order optimality conditions of the first equation in (17) for the constrained optimization problem: µ Π : ηvk + η KL(πk+1/2πref ) + KL(πk+1/2(cid:98)πk), µ πk+1/2 0 , (38) where the gradient of KL divergence is assumed to be taken with respect to the first argument: KL(ππ) = log π log π + 1 , 31 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: Algorithm 1 Sample-Based Approximate Nash Mirror Prox Require: Initial policy parameters θ0, reference policy πref , Nash MP learning rate η > 0, policy gradient learning rate γ > 0, reference regularization parameter β, batch size B, number of gradient updates per phase , total iterations K. Main loop over iterations Ensure: Final policy parameters θK. 1: for = 0 to 1 do 2: 3: 4: 5: 6: Set opponent policy (cid:98)πk (corresponding to θk) Phase 1: Intermediate Update (k (cid:55) + 1/2) Initialize θk+1/2,0 θk for = 0 to 1 do Sample batch {yj Sample batch {yj Obtain duel results {oj Compute stochastic gradient: j=1 i.i.d. from (cid:98)πk j=1 i.i.d. from πθk+1/2,t j=1 comparing yj k+1/2,t}B k+1/2,t}B k+1/2,t}B k+1/2,t against yj k+1/2,t (cid:98)Jk+ 1 2 (θk+1/2,t) = 1 (cid:88) (cid:18)(cid:18) j=1 oj k+1/2,t + β η log Update parameters: (cid:32) πθk+1/2,t(yj πref (yj (cid:33)(cid:19) + β log (cid:19) 1 2 (cid:32) πθk+1/2,t(yj (cid:98)πk(yj k+1/2,t) k+1/2,t) k+1/2,t) (cid:33) (SG1) k+1/2,t) log πθk+1/2,t(yj k+1/2,t) θk+1/2,t+1 θk+1/2,t γ (cid:98)Jk+ 1 2 (θk+1/2,t) end for Set intermediate policy parameters θk+1/2 θk+1/2,Tk+1/2 Set intermediate opponent policy (cid:98)πk+1/2 (corresponding to θk+1/2) Phase 2: Final Update (k + 1/2 (cid:55) + 1) Initialize θk+1,0 θk+1/2 for = 0 to 1 do Sample batch {yj j=1 i.i.d. from (cid:98)πk+1/2 Sample batch {yj j=1 i.i.d. from πθk+1,t Obtain duel results {oj j=1 comparing yj k+1,t}B Compute stochastic gradient (analogous to SG1): k+1,t}B k+1,t}B k+1,t against yj k+1,t (cid:98)Jk+1(θk+1,t) = 1 (cid:88) (cid:18)(cid:18) j= oj k+1,t (cid:19) 1 2 + β log k+1,t) (cid:33) (SG2) (cid:32) πθk+1,t (yj πref (yj (cid:33)(cid:19) k+1,t) log πθk+1,t (yj k+1,t) + β η log (cid:32) πθk+1,t (yj (cid:98)πk(yj k+1,t) k+1,t) 21: Update parameters: θk+1,t+1 θk+1,t γ (cid:98)Jk+1(θk+1,t) end for Set final policy parameters for this iteration θk+1 θk+1,Tk+1 22: 23: 24: end for 25: return θK End main loop where the logarithm is taken element-wise and 1 is vector of all ones. We want to show that (cid:98)πk+1/2 approximately satisfies (38) for any µ Π: ηvk + η KL((cid:98)πk+1/2πref ) + KL((cid:98)πk+1/2(cid:98)πk), µ (cid:98)πk+1/2 2(1 + η)εk+1/2 , Let us define the left-hand side of this expression as ψ((cid:98)πk, µ). Next, let us denote xk = ηvk η log πref log (cid:98)πk. Notice that log πk+1/2 = xk/(1 + η) + Zk+1/21, where Zk+1/2 is lognormalization constant and 1 is vector of all ones. Then (39) ψ((cid:98)πk+1/2, µ) = xk + (1 + η) log (cid:98)πk+1/2, µ (cid:98)πk+1/2 = xk + (1 + η)(log (cid:98)πk+1/2 log πk+1/2) + (1 + η) log πk+1/2, µ (cid:98)πk+1/2 , µ (cid:98)πk+1/2 = xk + (1 + η) log πk+1/2 (cid:125) (cid:124) (cid:123)(cid:122) =(1+η)Zk+1/21 + (1 + η)log (cid:98)πk+1/2 log πk+1/2, µ (cid:98)πk+1/2 . We notice that in the final decomposition, the first term is equal to zero since 1, µ (cid:98)πk+1/2 = 0, and for the second term, by the same reason, we have for any constant log (cid:98)πk+1/2 log πk+1/2, µ (cid:98)πk+1/2 = log (cid:98)πk+1/2 log πk+1/2 + c1, µ (cid:98)πk+1/2 µ (cid:98)πk+1/21log (cid:98)πk+1/2 log πk+1/2 + c1 , Since µ (cid:98)πk+1/21 2, minimization of the expression above over implies (39). Next, we continue from the following expression KL((cid:98)πk+1/2πref ), µ (cid:98)πk+1/2 = log (cid:98)πk+1/2 log µ + log µ log πref , µ (cid:98)πk+1/2 = KL(µ(cid:98)πk+1/2) + KL(µπref ) KL((cid:98)πk+1/2πref ) . Using the same argument, we have KL((cid:98)πk+1/2(cid:98)πk), µ (cid:98)πk+1/2 = KL(µ(cid:98)πk+1/2) + KL(µ(cid:98)πk) KL((cid:98)πk+1/2(cid:98)πk) . Plugging-in the derived expression to (39) implies ηvk, µ (cid:98)πk+1/2 + η(cid:0)KL(µπref ) KL((cid:98)πk+1/2πref ) KL(µ(cid:98)πk+1/2)(cid:1) + (cid:0)KL(µ(cid:98)πk) KL((cid:98)πk+1/2(cid:98)πk) KL(µ(cid:98)πk+1/2)(cid:1) 2(1 + η)εk+1/2 , and after rearranging the terms, we conclude the statement. Relationship to Existing Algorithms. In this appendix we situate NashMP relative to several existing methods. Nash Mirror Descent (NashMD): The NashMD algorithm of Munos et al. [2023] has iterates defined as: (cid:8)ηP(πref π) + ηβ KLρ(ππref ) + (1 ηβ) KLρ(ππk)(cid:9) , πk+1 = arg min πΠ is geometric mixture between πk and πref : πref where πref (yx) (πk(yx))1ηβ(πref (yx))ηβ and η > 0 is learning rate. By defining rescaled learning rate η = ηβ/(1 ηβ), the NashMD updates can be expressed in two-step form: πk+ 1 2 = arg min πΠ πk+1 = arg min πΠ (cid:8)η KLρ(ππref ) + KLρ(ππk)(cid:9) (cid:110) (η/β)P(πk+ 1 π) + η KLρ(ππref ) + KLρ(ππk) (cid:111) (40) . Comparing this to NashMP (6), NashMDs first step omits the preference term P(πk π) present in NashMPs first step. This can be viewed as an approximation where this preference term is treated as constant (e.g., if one assumes P(πk π) 1/2 for π close to πk). NashMP, by contrast, explicitly includes this game interaction term in its extrapolation step. Magnetic Mirror Descent (MMD): MMD [Sokota et al., 2023, Wang et al., 2025] performs single-step update: πk+1 = arg min πΠ (cid:8)(η/β)P(πk π) + η KLρ(ππref ) + KLρ(ππk)(cid:9) . (41) While MMD also incorporates regularization towards πref and πk, it lacks the two-step extrapolationand-update structure of NashMP. Optimistic Nash Policy Optimization (ONPO): Compared to ONPO by Zhang et al. [2025a], NashMP additionally includes explicit regularization towards the reference policy πref via the term η KLρ(ππref ). This regularization can enhance stability and incorporate prior knowledge. Extra-Gradient Policy Optimization (EGPO): Concurrent work by Zhou et al. [2025] on EGPO also utilizes an extra-gradient type procedure. The primary distinction lies in the operational geometry: EGPO performs updates in softmax-parameter space, whereas NashMP applies the Mirror Prox procedure directly in the policy space Π. Softmax Policy Gradients for Entropy-Regularized Multi-Armed Bandits In this section, we consider the following optimization problem (θ) πθ, + λH(πθ) , max θRA where πθ is defined by softmax parametrization as follows The maximum value of this problem is known to be equal to = λ log((cid:80) set of optimal parameters Θ is defined as exp(r(a)/λ)), and the πθ(a) eθa aA eθa . (cid:80) Θ = {r/λ + c1 R} , where 1 RA is vector that contains all the ones. The direct computations shows that the corresponding policy, denoted as π λ(a) exp(r(a)/λ) up to proportionality constant, equal to /λ. λ, has the following form π We are interested in this problem because it models the inexact computation of the proximal steps in the Nash Mirror Prox algorithm, which we need to perform in the functional approximation setting. Notation. Let us define the following matrix H(π) diag(π) ππT , (42) in particular, H(πθ) is Jacobian of the map θ (cid:55) πθ. As result, we have (θ) = H(πθ)(r λ log πθ). Additionally, we define span semi-norm as follows xsp = inf cR c1 = 1 2 (max xi min xj) = 1 2 max i,j (xi xj) . (43) We define the log-sum-exp map as follows LogSumExp(x) log (cid:32) (cid:88) exa (cid:33) , aA that have the following variational characterization: LogSumExp(x) = maxpA{x, + H(p)}. Additionally, it holds log πθ(a) = θa LogSumExp(θ) . D.1 Deterministic case In the deterministic case, since (θ), the updates of the policy gradient method are defined as follows θt+1 = θt + γH(πθt)(r λ log πθt) , (44) where γ > 0 is learning rate parameter. 34 Comparison with Mei et al. [2020] Compared to the results of Mei et al. [2020], our bound does not depend directly on the number of actions, improving the bound by factor of exp( A). Our bound depends only on the distance between the initial parameters and the optimal ones, properties of the optimal policy, learning rate, and strength of regularization. Proposition 1. Let {θt}t0 be iterates of the update rule (44) with learning rate γ λ 1. Then it holds for any 0 θt θsp exp(λγt c(θ0)) θ0 θsp , where c(θ0) exp(2θ0 θsp) min π λ(a) . Proof. Let us consider solution θ = r/λ Θ. Let us consider the following vector, them using the update rule (44) we have ζt = θt θ = θt r/λ , ζt+1 = θt r/λ γH(πθt)(λ(θt r/λ) λLogSumExp(θt)1) = (I λγH(πθt))ζt , where we used the fact that H(π)1 = 0. Then, using Lemma 12 and Lemma 13, under condition λγ 1, we have ζt+1sp (1 λγ min πθt(a))ζtsp, thus, for any 1, we have ζtsp exp λγ t1 (cid:88) j=0 min πθj (a) ζ1sp . (45) log πθt(a) min Next, to lower-bound the minimal probability. To do it, we apply Lemma 9 and got min {log πθt(a)log πθ (a)}+min Notice that from (45) follows that ζtsp = θt θsp ζ1sp, thus πθj (a) min λ(a) exp(2θ1 θsp) , π log πθ (a) min min log πθ (a)2θt θsp . thus, from (45) it follows θt θsp exp (cid:16) λγt exp(2θ1 θsp) min (cid:17) π λ(a) θ1 θsp . Lemma 9. Let θ, θ RA be softmax parameters, then it holds log πθ log πθ 2θ θsp . Proof. For an arbitrary we have log πθ log πθ = (θ LogSumExp(θ) 1) (θ LogSumExp(θ) 1) = θ θ c1 + (LogSumExp(θ) LogSumExp(θ) + c)1 . Let us analyze the second term. Here we have for any θ LogSumExp(θ) = max pA {θ, + H(p)} = θ, πθ + H(πθ) . thus πθ, θ θ + LogSumExp(θ) LogSumExp(θ) + π λ, θ θ + c1 , and, as result, LogSumExp(θ) LogSumExp(θ) + θ θ c1 . Overall, we have for any and, minimizing over free variable in the right-hand side, we have log πθ log πθ 2θ θ c1, log πθ log πθ 2θ θsp . 35 D.2 Stochastic case Now we consider more realistic setting where we do not have access to reward function but can only sample from distribution R(a) such that ERR(a)[R] = r(a). In this case, the exact gradient computation is no longer available. However, given the following form of the gradients: (θ) = Eaπθ [(r(a) λ log πθ(a)) log πθ(a)] , we can derive REINFORCE-style stochastic gradient. For each let Bt be batch size, then we can sample actions at R(at πθt and the corresponding rewards Rt ) to have the following gradient estimate 1), . . . , Rt Bt 1, . . . , at Bt 1 R(at Bt ˆf (θt) 1 Bt Bt(cid:88) (Rt λ log πθt(at j)) log πθt(at j) . j= (46) Next, let us define ξt and ξt(a) = 1 = Rt (cid:80)Bt j=1 ξt r(at 1{a = at nt(a) j) as stochastic noise, nt(a) = (cid:80)Bt j=1 1{a = at j}, ˆπt(a) = nt(a) Bt j}. Then we can rewrite (46) as follows ˆf (θt) = ˆH(ˆπt, πθt)(r + ξt λ log πθt) , where ˆH(ˆπ, π) is defined as follows and the stochastic updates look as follows ˆH((cid:98)π, π) diag((cid:98)π) (cid:98)ππT θt+1 = θt + γt ˆH(ˆπt, πθt)(r + ξt λ log πθt) . (47) (48) We will make the following assumption on the noise. Assumption 1. Let Ra be random variable distributed according to R(a). Then Ra r(a) holds almost surely for any A. Let us define the following events (cid:26) (cid:26) (1)(δ) (2)(δ) (t, a) A, α (0, 1) : (cid:98)πt(a) (1 α)πθt(a) (cid:27) , 3βt(δ) αBt (t, a) : (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 Bt Bt(cid:88) j=1 jwt ξt (cid:12) (cid:12) (cid:12) j(a, a) (cid:12) (cid:12) (cid:12) (cid:118) (cid:117) (cid:117) (cid:116) 2M 2βt(δ) Bt 1 Bt Bt(cid:88) (wt j(a, a))2 , j=1 j(a, a) (cid:0)1{at wt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) : 1 Bt Bt(cid:88) j= (3)(δ) = a} 1{at = a} ((cid:98)πt(a) (cid:98)πt(a))(cid:1) (cid:27) , (cid:32) 1 πθt(at j) (cid:98)πt(at j) (cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:118) (cid:117) (cid:117) (cid:116) 2M 2βt(δ) Bt 1 Bt (cid:32) Bt(cid:88) j=1 1 πθt(at j) (cid:98)πt(at j) (cid:33)2 . ξt Also, we define E(δ) (1)(δ) (2)(δ) (3)(δ) (4)(δ). Lemma 10. Assume Assumption 1. Then, under the choice βt(δ) = log(6At(t + 1)/δ) , it holds P[E (i)(δ)] 1 δ/3 for all {1, 2, 3}. In particular, P[E(δ)] 1 δ. Proof. Let us define (cid:26) (1) t,a = (cid:98)πt(a) 1 2 πθt(a) (cid:27) . 3βt(δ) Bt 36 By the one-sided Bernstein inequality we have for any δ (0, 1) Bt(cid:88) j= (1{at = a} πθt(a)) (cid:112)2Btπθt(a)(1 πθt(a)) log(1/δ) (cid:12) (cid:12) log(1/δ) (cid:12) (cid:12) 1 3 θt δ . Next, for any α (0, 1) an inequality (cid:112)2Btπθt(a)(1 πθt(a)) log(1/δ) (cid:112)2αBtπθt(a) 1/α log(1/δ) αBtπθt(a)+ 2ab + implies 2 α log(1/δ) , thus, taking δ = δ/(4At(t + 1)) we have for any α (0, 1) (cid:98)πt(a) πθt(a) απθt(a) 3 αBt log(3At(t + 1)/δ) . with probability at least 1 δ/(3At(t + 1)). Applying union bound over and we conclude P[E (1)(δ)] δ/3. To analyze events (2)(δ) and (3)(δ), we notice that values of {at of {ξt and union bound to achieve P[E (2)(δ)] 1 δ/3 and P[E (3)(δ)] 1 δ/3. j}j[Bt], thus, we can apply Hoeffding inequality conditionally on random variables {at j}j[Bt] determines distribution j}j[Bt] Lemma 11. Assume conditions of Lemma 10. Then, on the event E(δ), the following inequality holds for any α (0, 1) and any (cid:18) ζt+1sp 1 (1 α)λγt min πθt(a) + (cid:19) ζtsp + γt 3βt(δ) αBt (cid:115) 16M 2 β (δ) Bt . Proof. Let us rewrite the update rule (48) in the following manner θt+1 = θt + γt ˆH((cid:98)πt, πθt)(r λ log πθt) + γt ˆH((cid:98)πt, πθt)ξt . Notice that the softmax parametrization implies that log πθt = θt LogSumExp(θt) 1. Thus, by an inequality ˆH((cid:98)πt, πθt )1 = 0, and defining ζt = θt r/λ, we have ζt+1 = (I λγt ˆH((cid:98)πt, πθt))ζt + γt ˆH((cid:98)πt, πθt)ξt . Next, we compute the span semi-norm of ζt+1. By Lemma 12 and triangle inequality ζt+1sp σsp(1 λγt ˆH((cid:98)πt, πθt)) ζtsp (cid:125) (cid:124) (cid:123)(cid:122) (A) + 1 2 (cid:124) max a,a (cid:12) (cid:12)(γt ˆH((cid:98)πt, πθt )ξt)a (γt ˆH((cid:98)πt, πθt)ξt)a (cid:125) (cid:12) (cid:12) (cid:123)(cid:122) (B) . Next, we analyze these terms separately. Term (A). By Lemma 13 and on the event (1)(δ) E(δ), the first term can be expressed as follows σsp(1 λγ ˆH((cid:98)πt, πθt)) 1 λγt min (cid:98)πt(a) 1 (1 α)λγt min πθt(a) + 3λγtβt(δ) αBt . Thus, (cid:18) (A) 1 (1 α)λγt min πθt(a) + (cid:19) 3λγtβt(δ) αBt ζtsp . Term (B). Next, we study noise term (B). Let us define for any two fixed a, a,a γt (cid:12) (cid:12)( ˆH((cid:98)πt, πθt)ξt)a ( ˆH((cid:98)πt, πθt)ξt)a (cid:12) (cid:12) . Then, we have (B) = 1 2 max a,aA a,a. 37 To analyze a,a, we notice that ( ˆH((cid:98)πt, πθt)ξt)a = (cid:98)πt(a)(ξt(a) πθt, ξt) = (cid:98)πt(a)(ξt(a) (cid:98)πt, ξt + (cid:98)πt πθt, ξt) . By definition of ξt and (cid:98)πt, we have thus (cid:98)πt(a)ξt(a) = ( ˆH((cid:98)πt, πθt)ξt)a ="
        },
        {
            "title": "1\nBt",
            "content": "Bt(cid:88) j="
        },
        {
            "title": "1\nBt",
            "content": "Bt(cid:88) j=1 ξt 1{at = a} , (cid:98)πt, ξt ="
        },
        {
            "title": "1\nBt",
            "content": "Bt(cid:88) j=1 ξt , ξt (cid:0)1{at = a} (cid:98)πt(a)(cid:1) + (cid:98)πt(a)(cid:98)πt πθt, ξt ."
        },
        {
            "title": "The difference between such terms is equal to",
            "content": "a,a γt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)"
        },
        {
            "title": "1\nBt",
            "content": "Bt(cid:88) j=1 j(1{at ξt = a} 1{at (cid:12) (cid:12) (cid:12) = a} ((cid:98)πt(a) (cid:98)πt(a))) (cid:12) (cid:12) (cid:12) = a} ((cid:98)πt(a) (cid:98)πt(a))(cid:1). On the event (2)(δ) (cid:12)(cid:98)πt πθt, ξt(cid:12) (cid:12) (cid:12) . (49) + γt j(a, a) = (cid:0)1{at = a} 1{at Let us define wt E(δ), we have the following bound for the first term in the decomposition above (cid:12) (cid:12) (cid:12) j(a, a) (cid:12) (cid:12) (cid:12) 2M 2βt(δ) Bt j(a, a))2 . (cid:118) (cid:117) (cid:117) (cid:116) 1 Bt 1 Bt jwt ξt Bt(cid:88) Bt(cid:88) (wt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) j=1 j=1 Then we define random variable = 1{a = a} 1{a = a} with (cid:98)πt. We notice that E[Y ] = (cid:98)πt(a) (cid:98)πt(a) and thus 1 Bt Bt(cid:88) (wt j(a, a))2 = Var[Y ] = (cid:98)πt(a) + (cid:98)πt(a) ((cid:98)πt(a) (cid:98)πt(a))2 (cid:98)πt(a) + (cid:98)πt(a) 1 . j= Thus, we have (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 Bt Bt(cid:88) j=1 jwt ξt (cid:12) (cid:12) (cid:12) j(a, a) (cid:12) (cid:12) (cid:12) (cid:115) 2M 2βt(δ) Bt . (50) For the second term, on the event E(δ), we have the following decomposition (cid:12)(cid:98)πt πθt , ξt(cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 Bt Bt(cid:88) j=1 (cid:32) ξt 1 πθt(at j) (cid:98)πt(at j) (cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:118) (cid:117) (cid:117) (cid:116) 2M 2βt(δ) Bt 1 Bt (cid:32) 1 Bt(cid:88) j=1 πθt(at j) (cid:98)πt(at j) (cid:33)2 . Next, we note that the term under the square root resembles χ2-divergence between πθt and (cid:98)πt 1 Bt (cid:32) Bt(cid:88) j=1 1 πθt(at j) (cid:98)πt(at j) (cid:33)2 (cid:88) = a:(cid:98)πt(a)>0 (cid:18) (cid:98)πt(a) 1 (cid:19)2 πθt(a) (cid:98)πt(a) (cid:88) a:(cid:98)πt(a)>0 (πθt(a))2 (cid:98)πt(a) + 1 . To bound this term, we notice that on the event (1)(δ) we have taking arbitrary α (0, 1/2) (cid:98)πt(a) (1 α)πθt(a) 3βt(δ) αBt . Next, we separate actions into two groups: (cid:26) A1 = απθt(a) (cid:27) 3βt(δ) αBt (cid:26) and A2 = (cid:98)πt(a) > 0, απθt(a) < (cid:27) . 3βt(δ) αBt For all actions from the first group A1 it holds πθt (a) A2 we also have (cid:98)πt(a) 1/Bt and thus (cid:98)πt(a) 1 12α , whereas for the second group πθt(a) (cid:98)πt(a) Btπθt(a) = Bt α απθt(a) 3βt(δ) (α)2 . 38 Overall, we have (cid:88) a:(cid:98)πt(a)> (πθt(a))2 (cid:98)πt(a) 3/4, we have thus, taking α = + 1 max (cid:26) 1 2α , 3βt(δ) (α)2 (cid:27) + 1 , (cid:12)(cid:98)πt πθt, ξt(cid:12) (cid:12) (cid:12) (cid:115) 2M 2βt(δ) Bt (16βt(δ) + 1) (cid:115) 34M 2β2 (δ) Bt . (51) Overall, combining bounds (50) and (51) in (49), we get (cid:115) a,a γt 2M 2 βt(δ) Bt + (cid:115) 34M 2 β (δ) Bt (cid:115) γt 64M 2 β (δ) Bt , thus we have (B) γt (cid:115) 16M 2 β2 (δ) Bt . Combining bounds Combining all the results, we have on the event E(δ) (cid:18) ζt+1sp 1 (1 α)λγt min πθt(a) + (cid:19) 3λγtβt(δ) αBt ζtsp + γt (cid:115) 16M 2 β2 (δ) Bt . Proposition 2. Assume conditions of Lemma 10. Let ε (0, θ0 θsp) and suppose Bt is chosen such that (cid:26) 768βt(δ) c(θ0) where c(θ0) = exp(2θ0 θsp) mina π Then, on the event E(δ), for any N, it holds mina πθt(a) c(θ0). Additionally, it holds 256M 2β2 λ2c(θ0)2 λ(a), and γt γ, and βt(δ) is defined in Lemma 10. Bt max (δ) 1 ε2 (cid:27) , , θt θsp max (cid:26) (cid:18) ε, exp λγc(θ0) 4 (cid:19) θ0 θsp (cid:27) . Proof. Let us prove the first statement by induction over t. For = 0 we have by Lemma 9 min log πθ0(a) min log π λ(a) 2θ0 θsp = log c(θ0) . Now, we assume that the statement holds for fixed 1. We want to show that given mina πθt(a) c(θ0), we have ζt+1sp max{ε, ζtsp}. By the inequality for an update rule for α = 1/4 (Lemma 11) (cid:18) ζt+1sp 1 3 4 λγt min πθt(a) + (cid:19) 12βt(δ) Bt ζtsp + γt (cid:115) 16M 2 β (δ) Bt . We apply the induction statement mina πθt(a) c(θ0) to achieve (cid:18) ζt+1sp 1 3 4 λγtc(θ0) + (cid:19) 12βt(δ) Bt ζtsp + γt (cid:115) 16M 2 β2 (δ) Bt . Next, we apply our conditions on Bt (cid:18) ζt+1sp 1 (cid:19) λγtc(θ0) ζtsp + 1 1 4 λγtc(θ0)ε . (52) 39 Now, let us assume that ζtsp > ε. Then we have (cid:18) ζt+1sp 1 1 4 (cid:19) λγtc(θ0) ζtsp ζtsp. (53) Next, we assume that ζtsp ε, then (52) implies (cid:18) ζt+1sp 1 (cid:19) λγtc(θ0) ε + 1 2 1 4 λγtc(θ0)ε ε . Thus, we have ζt+1sp max{ε, ζtsp}. Additionally, given an assumption ε < ζtsp, we have ζt+1sp ζ0sp, thus min log πθt+1(a) min log π λ(a) 2ζt+1sp min log π λ(a) 2ζ0sp = log c(θ0) . Thus, we have for any mina πθt(a) c(θ0). Thus, (52) holds for any N. Given the previous structure of the proof, we notice that if ζtsp ε, then ζt+1sp ε and if ζtsp > ε, then the first inequality of (53) holds for any t. Thus, we have for any (cid:18) (cid:19)t (cid:40) (cid:41) ζtsp max ε, 1 λγc(θ0) ζ0sp . 1 4 D.3 Technical Lemmas Lemma 12. Let Rdd be an matrix, then for any Rd it holds where Axsp σsp(A)x , σsp(A) max 1i,kd 1 2 (cid:88) j=1 Aij Akj Moreover, if 1 is an eigenvector of A, i.e., A1 = λ1 for some λ R, then Axsp σsp(A)xsp . Proof. Let a1, . . . , ad be rows of the matrix A. Then, by the span semi-norm characterization Axsp = 1 2 max 1i,kd ai ak, 1 ai ak1x . j=1 Aij Akj we conclude the first statement. By noticing ai ak1 = (cid:80)d For the second statement, let us consider x+cx1, where cx is chosen to guarantee = xsp. Then, by using fact that A1 = λ1, we have Axsp = A(x cx1)sp = Ax λcx1sp = Axsp. Thus, the first statement applied to concludes the second one. Lemma 13. For any two policies π, (cid:101)π Π and any λ [0, 1], we have σsp(I λH(π)) 1 λ min π(a), σsp(I λ ˆH(π, (cid:101)π)) 1 λ min π(a) , where matrix is defined in (42) and matrix ˆH is defined in (47). Proof. Notice that it is enough to prove only the second statement as the first one follows from it if we take π = (cid:101)π. By definition of σsp, we have σsp(1 λ ˆH(ππ)) = 1 2 max a,aA (cid:88) aA (cid:124) (I λ ˆH(π, (cid:101)π))a,a (I λ ˆH(π, (cid:101)π))a,a (cid:125) (cid:123)(cid:122) a,a . 40 Let us study separate element of maximization a,a. Notice that a,a = 0 for = a, thus we consider only = a. Also, without loss of generality, we can assume that π(a) π(a) since a,a = a,a. By definition we have ˆH(π, (cid:101)π)a,a = 1{a = a}π(a) π(a)(cid:101)π(a), thus we need to estimate a,a = (cid:88) aA 1{a = a}(1 λπ(a)) + 1{a = a}(1 λπ(a)) + λ(π(a) π(a))(cid:101)π(a) = λ(π(a) π(a))(1 (cid:101)π(a) (cid:101)π(a)) + 1 λπ(a) + λ(π(a) π(a))(cid:101)π(a) + 1 λπ(a) + λ(π(a) π(a))(cid:101)π(a) . By choice of λ 1, we can remove the absolute values from the second and third terms in the decomposition above. Thus, since π(a) π(a), we have a,a = 2 λπ(a) λπ(a) + λ(π(a) π(a))(1 2(cid:101)π(a)) = 2(1 λπ(a)) λ(π(a) π(a)) + λ(π(a) π(a))(1 2(cid:101)π(a)) 2(1 λπ(a)) . After maximizing over a, A, we conclude the statement. Lemma 14. Let π, π Π be any two policies. Then we have π π1 2log π log πsp . Proof. Without loss of generality, we can assume log π, log π < +, since otherwise the inequality is trivial. Next, let us define θ = log π, θ = log π two softmax parameters. Next, by the fundamental theorem of calculus applied to map θ (cid:55) πθ exp(θ), we have πθ πθ = H(πξ)(θ θ) , where H(πθ) is Jacobian of the map θ (cid:55) πθ that have the following form H(π) = diag(π) ππT , and ξ is an intermediate point between θ and θ. Also, since H(π)1 = 0 for vector of all ones 1, we have for any πθ πθ = H(πξ)(θ θ + c1) , and, then, we have πθ πθ1 H(πξ)1 θ θ + c1 . In general, computation of ℓ ℓ1 norm is NP-hard, but in our case we can estimate this norm as follows: H(π)x1 = (cid:88) aA π(a) xa π, (cid:125) (cid:123)(cid:122) 2x (cid:124) 2x . Taking minimum over free parameter R, we conclude the statement."
        },
        {
            "title": "E Detailed Experiment Description",
            "content": "E.1 Matrix Games Experiment setup In our experiments, we fixed = 2, = 100, β = 0.01 and reference policy to be uniform distribution πref (yx) = 1/Y . The matrices and are generated as random Gaussian matrices. To parameterize the space of policies, we utilize 3-layer MLP with ReLU action function with 128 hidden units that takes flattened matrix Θx R22 as an input and outputs logits over possible actions. We use the Adam optimizer [Kingma and Ba, 2015] with learning rate 103 and sample random games using batch size of 128. 41 Figure 2: Effect of the soft update parameter κ on the performance of NashMP across optimization horizons {103, 104, 5 104}. Smaller κ values are needed for effective optimization as increases. Suboptimality is averaged over 10 random seeds; shaded regions indicate one standard deviation. The influence of κ We study the influence of the soft update parameter κ on the performance of the Nash Mirror Prox algorithm. We utilize theoretically optimal value of mirror prox learning rate η = 2β and vary value of κ {101, 102, 103} and an adaptive κ = 10/(10 + k) across different optimization horizons of {103, 104, 5 104} The results are presented in Figure 2. In particular, it turns out that for longer optimization horizons, the choice of smaller value of κ is more optimal. Since the average number of steps to fully update the model is equal to 1/κ, we can interpret that the algorithm performs κ full steps of PP method with an optimization quality of each step depending on the value of 1/κ. This interpretation also allows us to provide an adaptive schedule κ = 10/(k + 10) that gives dynamic trade-off between number of proximal steps and the accuracy of each proximal step. E.2 LLM Alignment In this section, we provide implementation and details as well as details on hyperparameter selection. E.2.1 Loss implementation We use library TRL von Werra et al. [2020] as base for our implementation. Following the discussion in Section 5.3, we use the following surrogate loss function to achieve the correct gradients using automatic differentiation in PyTorch Paszke et al. [2019] (cid:101)LNashMP(θt; θt, θtarget ) 1 (cid:88) i=1 (log πθt(yixi) log πθt(y ixi)) SG (cid:18) 1 P(yi ixi) (cid:19) (cid:32) + log πθt(yixi) SG β log + log πθt (y ixi) SG β log (cid:32) (cid:18) πθt(yixi) πref (yixi) (cid:19) (cid:19) (cid:18) πθt(y πref (y ixi) ixi) + + β η β η (cid:32) (cid:32) log log πθt(yixi) πθtarget (yixi) πθt(y πθtarget ixi) (y ixi) (cid:33)(cid:33) (cid:33)(cid:33) , where {xi}i[B] are samples from the prompt dataset, {(yi, i)}i[B]} are generated from policy πθt using temperature sampling with temperature 1, and SG is stop-gradient operations. log π(yx) is computed in an auto-regressive manner as log πθ(yx) = (cid:80)y i=1 log πθ(yix, y<i) = (cid:80)y i=1 logitθ(yix, y<i) LogSumExp(logitθ(x, y<i)), where logitθ are raw logits outputted by the model with parameters θ. For full algorithm desciption, we refer to Algorithm 2. E.2.2 Experiment description We start our experiments from the Google Gemma-2-2B2 [Rivière et al., 2024] pretrained checkpoint. 2Published under Gemma license. 42 Algorithm 2 Deep Learning Implementation of NashMP Require: Reference policy πref with parameters θ0, preference model P, prompt dataset = {xi}N i=1, number of steps , batch size B, hyperparameters β, η, κ. 1: Initialize θ = θ0, θ = θ0 and πtarget = πθ; 2: for = 1 to do 3: 4: 5: 6: Sample batch of prompts {xi}B Sample completions {(yi, i)}B Compute preferences pi = P(yi Compute preference REINFORCE loss i=1 from prompt dataset ; i=1 from πθ(x) using temperature sampling (τ = 1); i) using preference model for {1, . . . , B}; pref_loss(θ)"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 (log πθ(yixi) log πθ(y ixi)) SG(cid:0) 1 2 pi (cid:1) 7:"
        },
        {
            "title": "Compute KL regularization terms",
            "content": "kl_refi SG (cid:16) log πθ(yixi) πref (yixi) (cid:17) , kl_ref SG (cid:16) kl_targeti SG (cid:16) log πθ(yixi) πtarget(yixi) (cid:17) , 8: Compute KL regularization loss kl_tar SG (cid:17) . log πθ(y πref (y ixi) ixi) log πθ(y πtar(y (cid:16) (cid:17) . ixi) ixi) kl_ref_loss(θ) = kl_target_loss(θ) = 1 1 B (cid:88) i=1 (cid:88) i=1 (kl_refi log πθ(yixi) + kl_ref log πθ(y ixi)) (kl_tari log πθ(yixi) + kl_tar log πθ(y ixi)) , 9: Update θ by backpropagation through loss(θ) = pref_loss(θ) + β kl_ref_loss(θ) + β η kl_target_loss(θ); Update target network paramters by soft update θ (1 κ) θ + κ θ 10: 11: end for using processed RLHFlow Team [2024c]. is Supervised fine-tuning (SFT). For SFT, we use the RLHFlow/RLHFlow-SFT-Dataset-ver2 conversadataset tions, format (<bos><start_of_turn>rolencontent<end_of_turn>n...), where the template maps the assistant role to model. System messages are dropped from the input, and training is performed on the train split. The dataset samples are tokenized, with the maximum sequence length set to 8,192 tokens. We utilize sample packing for efficient training on long sequences and pad sequences to the maximum length. Following standard practice for SFT, the loss is computed only on the models output tokens (the assistants turns), and not on the input prompts. This template the Gemma structured following dataset, chat as The model was fully fine-tuned (no LoRA PEFT adapter was used). Training was conducted for 2 epochs. Optimization was performed using the Paged AdamW Loshchilov and Hutter [2019] 32-bit optimizer with learning rate of 2 105. cosine learning rate schedule was applied with warmup ratio of 0.05 of the total training steps. We used micro batch size of 1 sequence per device and accumulated gradients over 16 steps, resulting in an effective batch size of 16 sequences per device. On the 8 GPUs, we thus had an effective batch size of 128. Gradient clipping was applied with maximum norm of 1.0, and no weight decay was used. For improved memory efficiency and speed, we enabled gradient checkpointing and leveraged Flash Attention Dao [2024]. Training utilized BFloat16 (BF16) and TF32 precision where supported. Nash Learning from Human Feedback All subsequent NLHF experiments started from the SFT checkpoint described above. This SFT model also served as the initial policy and the reference policy 43 Table 3: Hyperparameter settings for the evaluated algorithms. Algorithm Online DPO Online IPO Nash MD NashMP Reg. Self-Play Learning Rate 1 106 1 106 1 106 1 106 1 10 β Algorithm-Specific Parameters 1 103 N/A 1 104 N/A 1 104 Mixture parameter α = 0.25 1 104 1 103 N/A Soft update κ = 0.1, Mirror Prox learning rate η = 0.1 (πref ). Consistent with the SFT stage, all models underwent full fine-tuning; no LoRA adapters were employed. Datasets. For generating responses during NLHF training and for final evaluation, we used prompts from the RLHFlow/prompt-collection-v0.1 dataset RLHFlow Team [2024b]. We created fixed training and test splits by randomly selecting Ntrain = 60, 000 prompts for the train set and Ntest = 5, 000 prompts for the test set, using random seed of 42 for reproducibility. Both sets were filtered to include only prompts with length of less than 256 tokens. For further details on the original data mixtures within RLHFlow/prompt-collection-v0.1 and their licenses, we refer to [Dong et al., 2024]. Preference model. The pairwise preference model, used to provide comparison signals, was Gemma2-2B model. This model was trained on the RLHFlow/pair_preference_model_dataset dataset RLHFlow Team [2024a], with its training methodology detailed in Liu et al. [2025]. We employed separate, more capable judge for the final evaluation of model performance: Gemma-2-9B model. This judge was trained on the same RLHFlow/pair_preference_model_dataset dataset using the identical methodology as the 2B preference model. The primary evaluation metric was side-by-side pairwise win rate, and for the hyperparameter selection, we also actively used win rate against the SFT reference policy. Confidence intervals were computed as 3σ-confidence intervals (3 std/ Ntest), justified by normal approximation. Training Configuration and Hyperparameter Tuning. For all NLHF experiments, we used the AdamW optimizer Loshchilov and Hutter [2019]. The learning rate schedule featured 0.1 warmup period (as fraction of total training steps) followed by linear decay. The effective global batch size was 128 prompts, achieved using per-device micro-batch size of 4 prompts, 8 GPUs, and 4 gradient accumulation steps (4 prompts/GPU 8 GPUs 4 grad_accum_steps). Training was conducted for 1 epoch over the Ntrain = 60, 000 prompts, corresponding to approximately 467 update steps (60000/128 468.75). We employed gradient clipping with maximum norm of 1.0. For each algorithm, we perform grid search over its key hyperparameters: learning rate over lr {106, 3 106}, regularization parameter β {104, 103, 102}, Nash MD mixture parameter α {0.125, 0.250}, NashMP soft update parameter κ {0.5, 0.1, 0.01}. For NashMP, we use constant parameter η = 0.1. For each algorithm, we selected the top 3 performing hyperparameter configurations based on the win rate against the SFT reference policy, evaluated using the Gemma-2-9B. These selected checkpoints were then compared side-by-side in the final evaluation using the Gemma-2-9B judge model. The final reported results (see Table 3) represent the performance of the best configuration found through this process. To manage memory and improve throughput during all NLHF training phases, we utilized DeepSpeed ZeRo-2 [Rasley et al., 2020] and BFloat16 (BF16) mixed-precision. Computational Resources and Runtimes. All experiments were conducted on 8 NVIDIA A100 (80GB) GPUs. full training and evaluation cycle for NLHF and most baselines averaged 9.5 hours per algorithm. The Nash-MD algorithm required approximately 24 hours for complete run. Generation Parameters. During response generation, both for collecting experiences within NLHF algorithms (e.g., generating samples per prompt) and for final evaluation on the test set, we used temperature sampling with temperature of τ = 1.0. The maximum generation length was capped at 256 tokens. 44 Table 4: Pairwise Win Rates (mean 3σ-confidence intervals). Statistically significant wins are in bold. Confidence intervals are in smaller font size. Win rate NashMP, κ = 0.5 NashMP, κ = 0.1 NashMP, κ = 0. NashMP, κ = 0.5 0.5341 0.0121 0.5212 0.0115 NashMP, κ = 0.1 0.4659 0.0121 0.4851 0.0118 NashMP, κ = 0.01 0.4788 0.0115 0.5149 0.0118 On hyperparameters of NashMP. During our hyperparameter selection procedure of NashMP, its versions with different values of κ were the top-3 in win rate against the reference model among all other produced checkpoints; thus, according to our protocol, we performed side-by-side comparison. We refer to Table 4 for results. We see that both values κ = 0.1 and κ = 0.01 outperform value κ = 0.5. We explain this effect by the low accuracy of the approximation of the proximal step by one gradient update, and we need to perform 1/κ gradient updates, which is equal to 10 or 100 in our case. At the same time, this inexactness explains the chosen value η = 0.1 that is much larger than the theoretical value η = 2β = 2 104. In particular, in our preliminary experiments, we observed that value of η = β results in an overly conservative policy due to extremely strong regularization. Indeed, if we compute the total power of regularization as sum of two regularization coefficients, the choice η = β implies regularization of strength 1 + β, which is overly pessimistic. Current choice implies the total power of regularization is equal to β(1 + 1/η) = 11 β, which is comparable with all the baselines. Limitations. We notice that using the target network increases the memory footprint of the model, which is limitation of our method. However, it does not significantly increase the running time of the algorithm and is straightforward to implement."
        }
    ],
    "affiliations": [
        "CMAP, CNRS, École Polytechnique",
        "Duisburg-Essen University",
        "ENS Lyon",
        "Google DeepMind",
        "HSE University",
        "Hugging Face",
        "LMO, Université Paris-Saclay",
        "Mohamed Bin Zayed University of AI",
        "Stealth Startup / Inria / ENS"
    ]
}