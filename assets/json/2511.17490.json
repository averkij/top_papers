{
    "paper_title": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination",
    "authors": [
        "Yolo Y. Tang",
        "Daiki Shimada",
        "Hang Hua",
        "Chao Huang",
        "Jing Bi",
        "Rogerio Feris",
        "Chenliang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), a video reasoning LMM that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose a multi-stage rumination learning framework that progressively finetunes a 7B LMM to learn atomic and mixing visual operations via SFT and GRPO-based RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixel-grounded multimodal reasoning. Project Page: https://yunlong10.github.io/Video-R4/"
        },
        {
            "title": "Start",
            "content": "Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination Yolo Yunlong Tang1, Daiki Shimada2, Hang Hua3, Chao Huang1, Jing Bi1, Rogerio Feris3, Chenliang Xu1 1University of Rochester, 2Sony Group Corporation, 3MIT-IBM Watson AI Lab https://yunlong10.github.io/Video-R4/ 5 2 0 2 6 2 ] . [ 3 0 9 4 7 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Understanding text-rich videos requires reading small, transient textual cues that often demand repeated inspection. Yet most video QA models rely on single-pass perception over fixed frames, leading to hallucinations and failures on fine-grained evidence. Inspired by how humans pause, zoom, and re-read critical regions, we introduce Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), video reasoning agent that performs visual rumination: iteratively selecting frames, zooming into informative regions, re-encoding retrieved pixels, and updating its reasoning state. We construct two datasets with executable rumination trajectories: Video-R4-CoT-17k for supervised practice and Video-R4-RL-30k for reinforcement learning. We propose multi-stage rumination learning framework that progressively finetunes 7B LMM to learn atomic and mixing visual operations via SFT and GRPObased RL. Video-R4-7B achieves state-of-the-art results on M4-ViteVQA and further generalizes to multi-page document QA, slides QA, and generic video QA, demonstrating that iterative rumination is an effective paradigm for pixelgrounded multimodal reasoning. 1. Introduction Understanding text-rich videos requires precise reading of small, transient textual cues that often appear only in specific frames or localized regions. Recent video question answering and text-centric video benchmarks have highlighted these challenges in news videos, driving scenes, egocentric recordings, and UI or slide walkthroughs [22, 31, 42, 43, 67, 86, 90, 96], while broader surveys on video understanding with large multimodal models (LMMs) underline the difficulty of scaling such capabilities to long, complex videos [62]. Despite advances in video-focused LMMs and long-video benchmarks [16, 20, 27, 36, 44, 46, 55, 69, 77], most systems operate under single-pass perception paradigm, processing fixed set of frames and reFigure 1. Video-R4 performs iterative visual rumination by selecting frames, zooming into regions, and re-encoding pixels, forming closed-loop readretrieverefocusreinforce cycle for grounded video reasoning. lying heavily on text-only chain-of-thought to fill in missing details. This design leads to brittle behavior in textrich scenarios. Once set of frames has been selected and encoded, the model typically cannot revisit frames, re-examine regions, or refine beliefs when initial perceptions are incomplete. Text-only chain-of-thought prompting can improve reasoning [9, 23, 39, 63, 64, 73, 87, 91], but when predictions are not grounded in pixels, it can also amplify hallucinations about content that was never observed. Meanwhile, coordinate-grounded approaches in TextVQA, TextVideoQA, and document VQA predict frame indices, Figure 2. Our Video-R4-7B model achieves state-of-the-art performance on the text-rich video understanding dataset M4-ViteVQA, and is also compatible with the LMMs with the same size on the general video QA benchmarks. bounding boxes, or layout regions as intermediate evidence [21, 26, 31, 60, 61, 66, 94, 95], yet these coordinates are usually treated as static endpoints rather than actionable instructions: the referenced pixels are rarely brought back into the models context to be re-read and compared. In contrast, human viewers naturally adopt an iterative pause-zoom-check strategy when watching text-heavy videos such as screen recordings, lecture slides, or UI demos. We pause at relevant moment, zoom into region, reread the text, compare across frames, and revise our understanding as new evidence emerges. This observation forms the core inspiration for our work: if an LMM were equipped with the ability to act on the video, select frames, zoom into regions, fetch higher-resolution pixels, and incorporate them back into its context, it could escape the limitations of one-shot perception and move toward pixelgrounded, multi-step reasoning. Motivated by this, we propose Video-R4 (Reinforcing Text-Rich Video Reasoning with Visual Rumination), video reasoning LMM that performs visual rumination. As shown in Figure 1, the model executes cycles of selecting informative frames, zooming into fine-grained regions, reencoding the retrieved pixels, and updating its internal reasoning state. This closed-loopread, retrieve, refocus, reinforceturns temporal selection and spatial zoom into explicit decision steps, allowing the model to accumulate and verify evidence across multiple iterations rather than relying on single perception of the video. Our design is complementary to recent RL-based reasoning efforts in language/- multimodal models [9, 13, 14, 18, 25, 30, 37, 39, 45, 48, 56, 57, 81], but specifically targets text-rich video reasoning with an explicit control interface for visual operations. Training such behavior is nontrivial: multi-step rumination requires not only learning how to use visual operations, but also when and why to apply them. To this end, we curate executable trajectories from the M4-ViteVQA dataset [90] and design multi-stage rumination learning framework that progressively teaches atomic and compositional operations via GRPO-style reinforcement learning built on PPO [53]. Our reward design draws on ideas from diversityand representativeness-based video summarization [93] and recent curiosity-driven and vision-centric reinforcement learning for pixel-space reasoning [48, 58]. This staged curriculum emerges as strong inductive bias, yielding faster convergence and significantly higher final performance than collapsed or single-stage methods. Empirically, Video-R4 sets new state of the art on M4-ViteVQA [90] and generalizes well beyond its training domain. Despite being trained exclusively on text-rich videos, the model transfers effectively to multi-page document QA and slides QA benchmarks [26, 60, 61, 66], as well as to general video QA benchmarks such as MVBench, Video-MME, and Video-MMMU [16, 20, 36]. These results suggest that iterative visual rumination forms broadly useful paradigm for multimodal reasoning over both videos and long documents. In summary, our contributions are: We construct two curated datasets for executable textrich video reasoning: Video-R4-CoT-17k for supervised rumination pratice and Video-R4-RL-30k for reinforcement learning, enabling study of temporal selection, spatial zooming, and multi-step evidence acquisition. We introduce Video-R4, video reasoning LMM that performs iterative visual rumination by selecting frames, zooming into regions, re-encoding pixels, and updating its reasoning state, and continually updating its internal state to support pixel-grounded reasoning. 2 Figure 3. Data curation pipeline for creating the Video-R4-CoT-17k for supervised deliberate rumination practice fine-tuning (DRP-SFT) and compositional rumination practice fine-tuning (CRP-SFT), as well as the Video-R4-RL-30k dataset for reinforcement learning. The light blue parts are intended to be used as the models inputs, while the pink parts are expected to be produced by the model as outputs. We develop multi-stage rumination learning framework that incrementally teaches atomic operations, compositional sequences, and operation control via GRPObased reinforcement learning. This staged curriculum yields faster convergence and substantially stronger final performance than single-stage or collapsed alternatives. We achieve state-of-the-art performance on M4ViteVQA and demonstrate robust transfer to multi-page document QA, slides QA, and general video QA, highlighting the broad applicability of iterative visual rumination beyond the training domain. 2. Method: Video-R4 2.1. Data Curation Data Source. As shown in Figure 3, we start from the training split of M4-ViteVQA, text-rich VideoQA benchmark with more than fifty thousand question-answering pairs and diverse real-world scenes [90]. Each sample provides video, one question, and its answer, together with pre-extracted OCR tokens and object detection results. These annotations retain text content, frame indices, object labels, and bounding boxes, which form the evidence pool for rumination trajectory synthesis. Evidence Matching. We aim to recover the evidence needed to answer each question and thereby prepare explicit chains of thought. Starting from the gold answer, we apply rule-based string matching between answers and OCR tokens, as well as between linguistic mentions of entities and object labels. We adopt fuzzy matching with edit distance to handle recognition noise and minor wording variation [33]. For matched samples, we record supporting frame indices and bounding boxes and mark whether the matched text or object is truly helpful for solving the question. For unmatched samples, we estimate question difficulty and keep moderately difficult cases as candidates for Video-R4-RL30k and drop the rest of the samples, which can make the GRPO-based RL more stable. The full matching rules details are given in the appendix. Rumination Trajectory Synthesis. Given matched evidence, we synthesize rumination trajectories using lightweight chain-of-thought template. The template interleaves internal thinking steps and visual operations applied to the video. We define two atomic visual operations inspired by recent work on video reasoning [9, 14]. Clipping selects several key frames by their indices and prompts the model to describe each frame. Cropping focuses on one frame and extracts salient region with bounding box, followed by region-level caption. All frames and regions are restricted to those produced by the matching stage, ensuring that every step remains grounded in observed evidence. We then fill the template using strong video-capable multimodal model and further check temporal consistency and answer correctness [1, 10]. Valid trajectories become supervision for the chain of thought dataset Video-R4-CoT-17k. Quality Control. We develop an annotation interface that displays each trajectory alongside visualized key frames, cropped regions, and the corresponding question-answer pair. Annotators quickly scan the rumination steps, verify that every visual operation points to the right evidence, and confirm that the final answer follows from the collected clues. Samples with hallucinated or missing evidence are edited or removed. Beyond M4-ViteVQA, we gather additional text-centric VideoQA instances from public datasets and convert them into the same format [31, 58, 67, 96]. After automatic and manual filtering, we obtain about 17k trajectories for Video-R4-CoT-17k and about 30k reinforcement learning samples for Video-R4-RL-30k. 2.2. Preliminary of GRPO where d(, ) denotes cosine similarity: We adopt Group Relative Policy Optimization (GRPO) [18] as the core policy optimization algorithm. Firstly, the policy πθ samples group of distinct candidate responses (or trajectories) {o1, . . . , oG} for given input query q. After calculating with predefined reward functions, we obtain their corresponding rewards {R1, . . . , RG}. We compute the group-wise mean and standard deviation, and define the relative quality of the i-th response as: Ai = Ri mean({Rj}G std({Rj}G j=1) j=1) (1) The policy is optimized to increase the probability of actions with higher group-relative advantage and decrease those with lower advantage. Following PPO [53], we apply clipped objective to stabilize updates: JGRPO(θ) = Eq,{oi} (cid:32) clip πθ(oi q) πθold(oi q) (cid:34) 1 (cid:32) (cid:88) i=1 (cid:32) min (cid:33) (cid:33) πθ(oi q) πθold (oi q) Ai, , 1 ϵ, 1 + ϵ Ai γ DKL(πθ πref ) (cid:35) , (2) where DKL is KL-divergence term to prevent the optimized policy πθ from far from the original LMM πref, and γ is regularization coefficient. This group-relative formulation reduces variance compared to individual-sample policy gradient updates, improves optimization robustness, and encourages relative action ranking rather than relying solely on absolute reward magnitudes. 2.3. Reward Design Our reward function combines four components: the original reward (e.g., answer correctness and format), Diversity Reward Rdiv, Representativeness Reward Rrep, and Curiosity Reward Rcur. The overall reward is: = + λdivRdiv + λrepRrep + λcurRcur, where the choice of the coefficients, λdiv, λrep, and λcur, can be found in the appendix (Section 10). (3) Diversity Reward. Following prior unsupervised summarization objectives [93], we encourage selected regions to be mutually dissimilar in feature space. Let denote the set of features of the input frames and ˆV = ˆV ˆV denote the set of features of the selected frames ( ˆV ) and regions ( ˆV r), where ˆV . We define representativeness reward to encourage the policy to avoid redundant region selections: Rdiv( ˆV r) = 1 ˆV r( ˆV 1) ˆV (cid:88) ˆV r1 (cid:88) i=1 j=i d(vi, vj) = 1 vj vi2vj2 . (5) This objective computes the average pairwise distance between all selected region features in ˆV r. The normalization by ˆV r( ˆV 1) makes the scale of Rdiv comparable across different numbers of selected regions, so the policy is not trivially rewarded for increasing ˆV r. Using cosine-based distance further makes the reward depend on the orientation of features rather than their magnitude, which aligns it with semantic differences captured by the encoder. As result, Rdiv biases the training dynamics toward solutions where the selected crops are spread out in the feature space instead of collapsing to narrow cluster. Representativeness Reward. To ensure the selected frames ˆV remain informative, we encourage them to represent the global video frame set with the representativeness reward: Rrep(V, ˆV ) = exp 1 V (cid:88) i=1 min vj ˆV [1] vi vj , (6) where 2 denotes the Euclidean distance in the feature space and ˆV [1] is the set of frame features selected in the last clipping operation. To reduce computational redundancy, we only use ˆV [1] to compute Rrep. This reward measures how well the selected frames cover the entire video in feature space: for each frame vi , we keep only the distance to its closest selected frame in ˆV [1] and average these distances over all frames. When the selected frames are placed near the implicit cluster centers of , most frames are close to at least one selection, the average distance is small, and Rrep stays close to 1 [93]. Conversely, if the policy selects outliers or redundant frames, many frames remain far from any selection, the average distance grows and the exponential term sharply reduces the reward, pushing the policy toward compact set of prototypical frames that best represent the video. Curiosity Reward. To balance exploration and the tendency to overuse the visual operations, we incorporate curiosity reward [58]: Rcur( ˆVi) = α 1 (cid:88) j=1 I(cid:2) ˆVj > 0(cid:3) I(cid:2) ˆVi > 0(cid:3) (cid:16) β ˆVi (cid:17) , + + (7) d(vi, vj), (4) where is the number of rollouts, ()+ is the ReLU function, I[] is the indicator function, α and β are coeficients, 4 Figure 4. Overview of multi-stage rumination training framework. and is the threshold. The first term becomes positive only when the overall fraction of rollouts that invoke the visual operation falls below H, and the factor I[ ˆVi > 0] ensures that only rollouts that actually call the tool receive this bonus. This encourages the policy to explore visual tool calls when they are globally under-utilized, rather than collapsing to purely text-only strategy. In contrast, the second term is activated only when ˆVi exceeds , imposing linear penalty on excessive calls and preventing the policy from over-relying on the visual operation. Together, these two terms guide the policy toward regime where the tool is used when beneficial, but neither ignored nor abused. 2.4. Training Framework As illustrated in Figure 4, we train Video-R4 with fourstage rumination training framework: Deliberate Rumination Practice (DRP) SFT, first GRPO-based RL stage, Compositional Rumination Practice (CRP) SFT, and second RL stage. Deliberate Rumination Practice (DRP). The first stage focuses on learning each atomic visual operation in isolation. In Deliberate Rumination Practice SFT (DRP-SFT), every training trajectory exposes only one type of rumination: either cropping over single frame or clipping over video. For image-centric trajectories, the input consists of one frame, and the model can repeatedly crop regions, zoom in, and reason over the re-encoded crops. For videocentric trajectories, the input is an ordered list of frames, and the model can select short consecutive clips from the sequence and reason over the selected frames. DRP-SFT uses about 7k trajectories, including 5k image-based cropping and 2k video-based clipping examples. We minimize token-level cross-entropy on both natural-language tokens and operation arguments, so the model learns when to call cropping or clipping (under single available tool per trajectory) and how to propose spatial or temporal regions conditioned on its current rumination state. This stage produces DRP-initialized checkpoint with strong but still non-compositional cropping and clipping skills. Compositional Rumination Practice (CRP). The second stage teaches the model to interleave cropping and clipping within single reasoning trajectory. In Compositional Rumination Practice SFT (CRP-SFT), we fine-tune from the DRP-SFT checkpoint on 10k trajectories from Video-R4-CoT-17k, where both atomic operations are available. These trajectories exhibit typical patterns such as first clipping to locate relevant segment, then cropping on key frames to read fine-grained text, and finally clipping again to verify an earlier hypothesis. Compared with DRPSFT, CRP-SFT shifts the objective from mastering individual tools to learning longer readgroundverify procedures, where the model must choose which operation to invoke, how many times to use it, and how to schedule and chain multiple crops and clips over time. Multi-stage Rumination Learning. Beyond supervised chain-of-thought trajectories, we further refine rumination behavior using GRPO-based RL with the reward defined in Section 2.3. We take the Video-R4-RL-30k collection from our data curation pipeline and split it into two subsets of 15k trajectories. Our final schedule is DRP-SFT RLd CRP-SFT RLc. In the first reinforcement stage RLd, we start from the DRP-SFT checkpoint and apply GRPO on the first 15k trajectories. This stage encourages the model to explore cropping and clipping under the outcome-based reward while staying close to the deliberate single-tool rumination patterns learned in DRP. We then run CRP-SFT on the 10k compositional trajectories so that longer mixed-operation strategies are distilled back into the policy. Finally, the second reinforcement stage RLc initializes from the CRP-SFT checkpoint and optimizes on the re5 Table 1. Performance comparison on the testset of the M4-ViteVQA dataset. Best non-human scores in bold. The parameters of the LMM-based models are 7B or 8B. All the RL-based models compared are based on Qwen2.5-VL. The human performances are from [90]. Models LMMBased VisualGrounded RLBased Task 1 - Split 1 Task 1 - Split 2 Task Acc. (%) ANLS (%) Acc. (%) ANLS (%) Acc. (%) ANLS (%) JustAsk [74] All-in-one-B [69] Video-LLaVA-7B [40] T5-ViteVQA [90] VideoLLaMA2-7B [11] Qwen2-VL-7B [70] TEA-L [85] NVILA-8B [49] GAT-L [86] Video-R1-7B [15] Qwen2.5-VL [1] Pixel-Reasoner [58] Video-R4-7B (ours) Human 10.05 10.87 15.43 22.17 20.76 35.22 34.78 37.73 38.30 37.10 26.53 52.91 56.17 85.27 14.10 14.80 17.15 29.10 23.55 45.84 43.71 47.23 48.23 48.25 44.91 61.44 65. 89.30 5.47 5.66 11.19 16.68 18.33 27.25 28.43 30.10 30.90 33.67 24.34 48.88 52.69 78.41 8.60 7.80 12.02 23.80 20.45 38.45 38.13 41.52 41.81 44.94 39.60 58.23 61.89 82.80 3.60 3.28 9.38 9.29 16.54 21.23 18.83 22.89 22.13 43.16 32.81 58.97 64. 82.26 6.70 4.60 11.80 13.60 21.08 28.79 28.90 30.34 30.75 53.37 50.82 65.32 69.99 85.10 maining 15k trajectories, sharpening decisions about when to stop, when to re-zoom, and how aggressively to explore alternative clips and crops. 3. Experiments 3.1. Experiment Setups Benchmarks. We experiment with the text-rich video reasoning on the testset of the M4-ViteVQA [90] dataset. Models are also tested on three commonly-used general video QA benchmarks: MVBench [36], Video-MME [16], and Video-MMMU [20]. The generalization ability is evaluated on the multi-page document QA benchmark MPDocVQA [66] and slides QA benchmark SlidesVQA [61]. Evaluation Metrics. For text-rich video QA and multipage document QA, we use accuracy (exact match, EM) [52, 75] and Average Normalized Levenshtein Similarity (ANLS) [8]. For general video QA, we use accuracy. For slides QA, we use EM [52, 75] and Macro-averaged F1 score. The details about EM, ANLS, and (Macro-averaged) F1 can be found in the appendix (Section 8). compare Video-R4-7B against Baselines. We three groups of representative methods: (1) Conventional video QA and long-context language models that do not rely on multimodal LMMs, including JustAsk [74], All-in-oneB [69], T5-ViteVQA [90], and long-sequence Transformers such as Longformer [2] and Big-Bird [80]. (2) Instructiontuned video LMMs without explicit reasoning-by-grounding or RL, e.g., Video-LLaVA [40], VideoLLaMA2 [11], Qwen2-VL [70], Qwen2.5-VL [1], and NVILA [49]; and visual-grounded designs tailored for text-rich videos, such (3) RL-tuned reasoning as TEA-L [85] and GAT-L [86]. LMMs built on strong LMM backbones, including VideoR1 [15] and Pixel-Reasoner [58]. We also report human performance from the M4-ViteVQA benchmark [90]. Unless otherwise noted, LMM-based competitors use 7B/8B backbones, matching our model size for fair comparison. 3.2. Main Results Text-Rich Video QA. Table 1 summarizes results on the M4-ViteVQA testset [90]. Video-R4-7B establishes new state of the art among non-human systems across all three evaluation splits. The largest margin appears on Task 2, where Video-R4-7B reaches 64.21 Acc against 43.16 for Video-R1 [15]. The Improvements are consistent in both accuracy and ANLS metrics. Beyond the aggregate scores, we also observe that allowing the model to execute longer sequence of visual operations at inference monotonically improves accuracy. When the inference allows deeper visual rumination, performance scales up. This aligns with the broader test-time scaling phenomenon and indicates that longer visual rumination increases the chance of finding and verifying small text cues rather than relying on single pass. Finding 1: Allowing longer rumination and more pixel-grounded steps consistently boosts accuracy, demonstrating test-time scaling effect. Ablation Study. Table 2 compares training recipes. The full schedule DRP-SFT RLd CRP-SFT RLc achieves the best end performance and also converges faster early on than direct CRP-SFT or DRP-SFT followed by CRP-SFT. The benefit remains even when later losses become similar. On Task 2, Acc moves from 51.92 for DRP6 Table 2. Ablation results of Video-R4 training framework, tested on the testset of M4-ViteVQA dataset. Best scores in bold and secondbest in underline. Training Framework Task 1 - Split 1 Task 1 - Split 2 Task 2 Acc. (%) ANLS (%) Acc. (%) ANLS (%) Acc. (%) ANLS (%) DRP-SFT RLd CRP-SFT RLc (full) DRP-SFT RLd CRP-SFT RLc (w/o Rrep) DRP-SFT RLd CRP-SFT RLc (w/o Rdiv) DRP-SFT RLd CRP-SFT RLc (w/o Rcur) DRP-SFT RLd CRP-SFT RLc (w/o Rdiv, Rrep) DRP-SFT CRP-SFT RLc CRP-SFT RLc DRP-SFT RLd CRP-SFT CRP-SFT DRP-SFT CRP-SFT DRP-SFT Base Model (Qwen2.5-VL-7B-Instruct) 56.17 55.70 55.56 54.35 55.73 54.98 54.23 50.08 46.76 44.58 32.80 26. 65.22 65.04 65.24 63.92 64.99 63.74 63.75 64.15 62.67 63.09 57.92 44.91 52.69 52.54 52.26 50.90 52.02 51.50 51.26 46.17 40.47 42.23 32.07 24.34 61.89 61.86 62.13 61.10 61.35 60.80 60.39 60.67 59.68 60.58 54.74 39.60 64.21 62.65 62.41 61.92 62.24 60.44 61.43 56.27 49.47 51.92 33.74 32.81 69.99 71.08 70.11 69.20 68.38 68.59 68.28 68.81 66.33 69.07 63.70 50.82 SFT CRP-SFT and 61.43 for CRP-SFT RLc to 64.21 with the full recipe. During RL the policy develops marked preference for cropping over clipping. Cropping isolates and enlarges single informative frame, which reduces redundancy and helps read fine text, consistent with how humans pause and zoom when analyzing videos. Reward ablations indicate trade-off between repetition and diversity controls, which shape the balance between careful reading and broad exploration. Finding 2: The DRP RL CRP RL schedule yields the best performance, indicating that atomic, first then compositional learning, interleaved with RL, is most effective. 3.3. Generalization Experiments General Video QA. Without dataset-specific tuning, Video-R4-7B transfers competitively to general video QA as summarized in Table 3. the top on MVBench [36] and Video-MME [16] and sets new best 52.2 on Video-MMMU [20]. Video-MMMU contains many educational/lecture videos that are intrinsically textrich, and the readgroundverify routine learned on M4ViteVQA appears to be well-aligned with these data. is near It Finding 3: RL encourages preference for cropping over clipping, as zooming provides more informative and less redundant evidence, mirroring how humans pause and inspect frames. Multi-page Document & Slides QA. After training on text-rich video QA, Video-R4-7B transfers to long document understanding with no additional tuning. On MPDocVQA, the zero-shot result of Video-R4-7B is 53.21 Acc and 62.22 ANLS, surpassing both the zero-shot and trained 7 Hi-VT5 variants [66]. On the testset of SlidesVQA, VideoR4-7B reaches 43.0 EM and 52.2 F1 versus 33.5 and 41.7 for M3D [61] as shown in Table 3. These demonstrate that once trained to locate, read, and verify dispersed textual evidence over time, the model can reuse the same procedure across pages and slides with minimal friction. Finding 4: Training on text-rich videos transfers well to multi-page documents, slides, and general video QA, with strong gains on the text-heavy Video-MMMU benchmark. 4. Related Work Text-Rich Video Understanding. Early work on textrich visual understanding primarily studied single images, where TextVQA systems integrate OCR, layout cues, and semantic reasoning to read scene text [3, 17, 21, 94, 95]. TextVideoQA extends this problem to dynamic scenes, requiring models to track temporal changes and transient text signals [31, 67, 90, 96]. M4-ViteVQA [90] provides the first large-scale benchmark, while later datasets such as RoadTextVQA and NewsVideoQA [31, 67] study domainspecific settings. Beyond general video LMMs, several architectures explicitly optimize grounding over text regions: TEA-L and GAT-L introduce tracking and graph reasoning over OCR boxes [85, 86], and Pixel-Reasoner employs pixel-level cropping actions for fine-grained evidence acquisition [58]. Related tasks such as multi-page document QA and slides QA tackle long-range textual grounding using hierarchical or layout-aware encoders [2, 26, 61, 66, 80]. In contrast, Video-R4 builds on generic video LMM and learns explicit spatio-temporal operations through RL. Video Understanding with LMMs. Visual instruction tuning has established strong foundation for aligning Table 3. Fine-tuning on Video-R4-CoT-17k and Video-R4-RL-30k, Video-R4 demonstrates strong generalization capabilities, effectively handling not only general video QA but also multi-page document QA and slides QA, without the need for further dataset-specific training. (a) Results on general video QA benchmarks. (c) Results on the test set of SlidesVQA. Models MVBench Video-MME Video-MMMU Video-LLaVA-7B [40] VideoLLaMA2-7B [11] Qwen2.5-VL-7B [1] Video-R1-7B [15] Pixel-Reasoner [58] Video-R4-7B (ours) 42.9 54.6 57.4 62.7 65.4 64.5 39.9 46.6 53.1 57.4 54.6 54.5 47.8 49.8 47.7 52.2 (b) Results on the validation set of the MP-DocVQA dataset. Models Zero-Shot Acc. (%) ANLS (%) LayoutLMv3 [26] Big-Bird [80] Hi-VT5 (w/o train) [66] Longformer [2] Hi-VT5 [66] Video-R4-7B (ours) 38.47 41.06 42.10 43.91 48.28 53.21 45.38 49.29 58.64 52.87 62.01 62.22 Models Q-only [61] UniVL [50] PreasM [76] T5 [51] T5 + zlay [51] LayoutT5 [60] LayoutLMv2 [68] FiD [29] FiD + zlay [29] M3D [61] Video-R4-7B (ours) Human ZeroShot Dev Test EM EM F1 9.4 8.8 36.3 35.2 36.9 38.9 26.5 37.6 38.1 41.3 49.5 11.4 12.1 41.9 41.3 43.2 44.8 33.4 42.9 43.3 47.1 56.0 10.7 10.6 30.7 29.3 31.0 31.7 21.4 30.4 30.6 33.5 43. 13.5 14.1 38.2 37.9 39.7 39.9 29.3 38.9 38.9 41.7 52.2 89.8 93.0 imagelanguage models with general-purpose language backbones [10, 34, 41, 41, 65, 88, 97]. Building upon this paradigm, video LMMs extend 2D visual alignment to temporal sequences. Early methods rely on sparse frame sampling and lightweight temporal fusion [34, 46], while recent systems strengthen spatialtemporal modeling, token compression, and long-context handling. VideoLLaVA [40] and VideoLLaMA2 [11] incorporate temporal attention and audio streams; Qwen2-VL and Qwen2.5-VL adopt dynamic-resolution inputs for higher-fidelity video perception [1, 70]; NVILA further scales visual backbones for video LMMs [49]. For long or streaming videos, works such as LongVA [82], Video-XL and successors [44, 55], VideoChat/VideoChatFlash [35, 38], InternVideo2 [71], and MVBench/Video-MME [16, 36] explore compression, memory, and benchmarking. Most existing approaches [4], however, still treat video understanding as single-pass perception task over fixed frames, without modeling iterative zoom-and-check behaviors on text-rich regions. Video-R4 instead performs closed-loop visual rumination, enabling deliberate evidence gathering and multi-step grounding. Large Multimodal Model Reasoning. LLM reasoning has progressed from chain-of-thought supervision [6, 7, 73, 87, 91] to outcome-driven RL, where rule-based rewards and GRPO enable strong long-form reasoning without dense annotations [9, 18, 30, 39]. This paradigm has inspired advances in text reasoning [78, 79, 84] and domain-specific R1-style models in math, finance, and medicine [32, 47, 54, 83]. For visionlanguage models, recent work applies verifiable visual rewards to teach multistep perceptual reasoning [5, 12, 13, 24, 25, 45, 48, 59, 81, 92]. Closer to our task, [14, 72, 89] explore temporal grounding in videos using GRPO objectives. Video-R4 differs by reasoning with structured tool interface and tailored reward scheme. Our diversity/representativeness rewards shape spatialtemporal coverage, while curiosity reward regulates tool frequency. Combined with the DRP/CRP rumination curriculum, this encourages human-like zoom-and-read behaviors crucial for recovering small, dispersed text in long videos and multi-page documents. 5. Conclusion We presented Video-R4, video reasoning agent that acquires evidence through iterative visual rumination. By decomposing video understanding into frame selection, spatial zooming, and re-encoding cycles, Video-R4 overcomes the limitations of single-pass perception and enables reliable grounding on text-rich, fine-grained visual cues. To support this paradigm, we curated Video-R4-CoT-17k and Video-R4-RL-30k, providing the first executable supervision for multi-step video rumination. Our multi-stage training strategy, combining supervised trajectories with GRPObased reinforcement learning, proves essential for stabilizing and improving rumination behavior. Empirical results show that Video-R4 achieves state-of-the-art performance on M4-ViteVQA and generalizes to broader multimodal reasoning tasks. We believe rumination-based LMMs can extend to longer videos, multimodal evidence fusion, and more open-ended reasoning, pushing LMMs toward robust, human-like video understanding. Acknowledgements. This work was supported by Sony Group Corporation. We would like to thank Sayaka Nakamura and Jerry Jun Yokono for their insightful discussion."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3, 6, 8, 2, 5 [2] Iz Beltagy, Matthew Peters, and Arman Cohan. LongarXiv preprint former: The long-document transformer. arXiv:2004.05150, 2020. 6, 7, 8 [3] Jing Bi, Jiebo Luo, and Chenliang Xu. Procedure planning in instructional videos via contextual modeling and modelbased policy learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1561115620, 2021. 7 [4] Jing Bi, Yunlong Tang, Luchuan Song, Ali Vosoughi, Nguyen Nguyen, and Chenliang Xu. Eagle: Egocentric In Proceedings of the aggregated language-video engine. 32nd ACM International Conference on Multimedia, page 16821691. ACM, 2024. 8 [5] Jing Bi, Junjia Guo, Susan Liang, Guangyu Sun, Luchuan Song, Yunlong Tang, Jinxi He, Jiarui Wu, Ali Vosoughi, Chen Chen, and Chenliang Xu. Verify: benchmark of visual explanation and reasoning for investigating multimodal reasoning fidelity, 2025. [6] Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, and Chenliang Xu. Why reasoning matters? survey of advancements in multimodal reasoning (v1), 2025. 8 [7] Jing Bi, Guangyu Sun, Ali Vosoughi, Chen Chen, and Chenliang Xu. Diagnosing visual reasoning: Challenges, insights, and path forward, 2025. 8 [8] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVF international conference on computer vision, pages 42914301, 2019. 6, 2 [9] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. 1, 2, 3, 8 [10] Zhe Chen, Weiyun Wang, Hao Tian, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. 3, 8 [11] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 6, 8, [12] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. 8 [13] Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue. Sophiavl-r1: Reinforcing mllms reasoning with thinking reward. arXiv preprint arXiv:2505.17018, 2025. 2, 8 [14] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. arXiv Video-r1: Reinforcing video reasoning in mllms. preprint arXiv:2503.21776, 2025. 2, 3, 8 [15] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Junfei Wu, Xiaoying Zhang, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. 6, 8, 5 [16] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2410824118, 2025. 1, 2, 6, 7, 8 [17] Dan Guo, Kun Li, Bin Hu, Yan Zhang, and Meng Wang. Benchmarking micro-action recognition: Dataset, method, and application. IEEE Trans. Circuits Syst. Video Technol., 34(7):62386252, 2024. [18] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 4, 8 [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 4 [20] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. 1, 2, 6, 7 [21] Ronghang Hu, Amanpreet Singh, Trevor Darrell, and MarIterative answer prediction with pointercus Rohrbach. In Proc. augmented multimodal transformers for textvqa. IEEE/CVF Conf. Comput. Vis. Pattern Recognit, pages 999210002, 2020. 2, 7 [22] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality arXiv preprint of pre-trained vision-language models. arXiv:2410.09733, 2024. 1 [23] Hang Hua, Qing Liu, Lingzhi Zhang, Jing Shi, Soo Ye Kim, Zhifei Zhang, Yilin Wang, Jianming Zhang, Zhe Lin, and Jiebo Luo. Finecaption: Compositional image captioning focusing on wherever you want at any granularity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2476324773, 2025. [24] Chao Huang, Zeliang Zhang, Jiang Liu, Ximeng Sun, Jialian Wu, Xiaodong Yu, Ze Wang, Chenliang Xu, Emad Barsoum, and Zicheng Liu. Directional reasoning injection for finetuning mllms. arXiv preprint arXiv:2510.15050, 2025. 8 9 [25] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2, 8 [26] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified In Proceedings of the 30th ACM text and image masking. international conference on multimedia, pages 40834091, 2022. 2, 7, 8 [27] Zhenpeng Huang, Xinhao Li, Jiaqi Li, Jing Wang, Xiangyu Zeng, Cheng Liang, Tao Wu, Xi Chen, Liang Li, and Limin Wang. Online video understanding: comprehensive benchmark and memory-augmented method. arXiv preprint arXiv:2501.00584, 2024. 1 [28] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [29] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question anIn Proceedings of the 16th conference of the euswering. ropean chapter of the association for computational linguistics: main volume, pages 874880, 2021. 8 [30] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2, 8 [31] Soumya Jahagirdar, Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Watching the news: Towards videoqa models that can read. In IEEE/CVF Winter Conf. Appl. Comput. Vision, pages 44414450, 2023. 1, 2, 3, 7 [32] Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. 8 [33] Vladimir Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10:707710, 1966. 3, 2 [34] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the 40th International Conference on Machine Learning, pages 1973019742. PMLR, 2023. [35] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 8 [36] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1, 2, 6, 7, 8 [37] Wendi Li and Yixuan Li. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287, 2024. 2 [38] Xinhao Li, Yi Wang, Jiashuo Yu, Xiangyu Zeng, Yuhan Zhu, Haian Huang, Jianfei Gao, Kunchang Li, Yinan He, Chenting Wang, et al. Videochat-flash: Hierarchical compression for long-context video modeling. arXiv preprint arXiv:2501.00574, 2024. 8 [39] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. 1, 2, 8 [40] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, 2024. 6, 8, [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 8 [42] Pinxin Liu, Luchuan Song, Junhua Huang, Haiyang Liu, and Chenliang Xu. Gesturelsm: Latent shortcut based co-speech gesture generation with spatial-temporal modeling, 2025. 1 [43] Pinxin Liu, Pengfei Zhang, Hyeongwoo Kim, Pablo Garrido, Ari Shapiro, and Kyle Olszewski. Contextual gesture: Cospeech gesture video generation through context-aware gesture representation. In Proceedings of the 33rd ACM International Conference on Multimedia, page 98039812, New York, NY, USA, 2025. Association for Computing Machinery. 1 [44] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025. 1, 8 [45] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 2, 8 [46] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution. arXiv preprint arXiv:2409.12961, 2024. 1, 8 [47] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, et al. Fin-r1: large language model for financial reasoning through reinforcement learning. arXiv preprint arXiv:2503.16252, 2025. [48] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. 2, 8 [49] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 41224134, 2025. 6, 8, 5 [50] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020. 8 10 [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 8 [52] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016. 6, [53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 4 [54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 8 [55] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale arXiv preprint arXiv:2409.14485, video understanding. 2024. 1, 8 [56] Luchuan Song, Lele Chen, Celong Liu, Pinxin Liu, and Chenliang Xu. Texttoon: Real-time text toonify head avatar from single video. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 2 [57] Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, and Chenliang Xu. Streamme: Simplify 3d gaussian avatar within live stream. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 110, 2025. 2 [58] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 2, 3, 4, 6, 7, 8, 5 [59] Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. Latent chain-of-thought for visual reasoning. arXiv preprint arXiv:2510.23925, 2025. 8 [60] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. Visualmrc: Machine reading comprehension on document images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1387813888, 2021. 2, [61] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1363613645, 2023. 2, 6, 7, 8 [62] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology, 2025. 1 [63] Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, et al. Vidcomposition: Can mllms analyze compositions in compiled videos? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84908500, 2025. 1 [64] Yunlong Tang, Pinxin Liu, Mingqian Feng, Zhangyun Tan, Rui Mao, Chao Huang, Jing Bi, Yunzhong Xiao, Susan Liang, Hang Hua, et al. Mmperspective: Do mllms understand perspective? comprehensive benchmark for perspecIn The Thirtytive perception, reasoning, and robustness. ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. [65] Yolo Yunlong Tang, Jing Bi, Pinxin Liu, Zhenyu Pan, Zhangyun Tan, Qianxiang Shen, Jiani Liu, Hang Hua, Junjia Guo, Yunzhong Xiao, Chao Huang, Zhiyuan Wang, Susan Liang, Xinyi Liu, Yizhi Song, Junhua Huang, Jia-Xing Zhong, Bozheng Li, Daiqing Qi, Ziyun Zeng, Ali Vosoughi, Luchuan Song, Zeliang Zhang, Daiki Shimada, Han Liu, Jiebo Luo, and Chenliang Xu. Video-lmm post-training: deep dive into video reasoning with large multimodal models, 2025. 8 [66] Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023. 2, 6, 7, 8 [67] George Tom, Minesh Mathew, Sergi Garcia-Bordils, Dimosthenis Karatzas, and CV Jawahar. Reading between the In Proc. Int. Conf. Doc. lanes: Text videoqa on the road. Anal. Recognit., pages 137154, 2023. 1, 3, 7 [68] Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xiaodong He, and Bowen Zhou. Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents. In Proceedings of the AAAI conference on artificial intelligence, pages 90739080, 2020. 8 [69] Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Kevin Qinghong Lin, Satoshi Tsutsui, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified videoIn Proceedings of the IEEE/CVF language pre-training. Conference on Computer Vision and Pattern Recognition (CVPR), pages 65986608, 2023. 1, 6, 5 [70] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 6, 8, [71] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Zun Wang, Yansong Shi, et al. Internvideo2: Scaling foundation models for mulIn European Conference on timodal video understanding. Computer Vision, pages 396416. Springer, 2024. 8 [72] Ye Wang, Boshen Xu, Zihao Yue, Zihan Xiao, Ziheng Wang, Liang Zhang, Dingyi Yang, Wenxuan Wang, and Qin Jin. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377, 2025. 8 [73] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1, 8 [74] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer ques11 In Proceedings of tions from millions of narrated videos. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 16861697, 2021. 6, [75] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pages 23692380, 2018. 6 [76] Ori Yoran, Alon Talmor, and Jonathan Berant. Turning tables: Generating examples from semi-structured tables for In Proendowing language models with reasoning skills. ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 60166031, 2022. 8 [77] Huaying Yuan, Zheng Liu, Minhao Qin, Hongjin Qian, Shu, Zhicheng Dou, and Ji-Rong Wen. Memory-enhanced retrieval augmentation for long video understanding. arXiv preprint arXiv:2503.09149, 2025. 1 [78] Jiakang Yuan, Tianshuo Peng, Yilei Jiang, Yiting Lu, Renrui Zhang, Kaituo Feng, Chaoyou Fu, Tao Chen, Lei Bai, Bo Zhang, et al. Mme-reasoning: comprehensive benchmark for logical reasoning in mllms. arXiv preprint arXiv:2505.21327, 2025. 8 [79] Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024. 8 [80] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. 6, 7, 8 [81] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. 2, [82] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 8 [83] Pengfei Zhang, Pinxin Liu, Pablo Garrido, Hyeongwoo Kim, and Bindita Chaudhuri. Kinmo: Kinematic-aware human In Proceedings of motion understanding and generation. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1118711197, 2025. 8 [84] Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv preprint arXiv:2506.03106, 2025. 8 [85] Yan Zhang, Gangyan Zeng, Huawen Shen, Daiqing Wu, Yu Zhou, and Can Ma. Track the answer: Extending textvqa from image to video with spatio-temporal clues. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1027510283, 2025. 6, 7, 5 12 [86] Yan Zhang, Gangyan Zeng, Daiqing Wu, Huawen Shen, Binbin Li, Yu Zhou, Can Ma, and Xiaojun Bi. Gather and trace: Rethinking video textvqa from an instance-oriented perspective. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 876885, 2025. 1, 6, 7, 5 [87] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. 1, 8 [88] Zeliang Zhang, Xiaodong Liu, Hao Cheng, Chenliang Xu, and Jianfeng Gao. Diversifying the expert knowledge for task-agnostic pruning in sparse mixture-of-experts. In Findings of the Association for Computational Linguistics: ACL 2025, pages 86102, 2025. [89] Jiaxing Zhao, Xihan Wei, and Liefeng Bo. R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning. arXiv e-prints, pages arXiv2503, 2025. 8 [90] Minyi Zhao, Bingjia Li, Jie Wang, Wanqing Li, Wenjing Zhou, Lan Zhang, Shijie Xuyang, Zhihang Yu, Xinkun Yu, Guangze Li, et al. Towards video text visual question answering: Benchmark and baseline. Advances in Neural Information Processing Systems, 35:3554935562, 2022. 1, 2, 3, 6, 7, 5 [91] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. 1, 8 [92] Hengguang Zhou, Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. R1-zeros aha moment in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. 8 [93] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for unsupervised video summarization with In Proceedings of the diversity-representativeness reward. AAAI conference on artificial intelligence, 2018. 2, 4 [94] Sheng Zhou, Dan Guo, Jia Li, Xun Yang, and Meng Wang. Exploring sparse spatial relation in graph inference for textIEEE Trans. Image Process., 32:50605074, based vqa. 2023. 2, 7 [95] Sheng Zhou, Dan Guo, Xun Yang, Jianfeng Dong, and Meng Wang. Graph pooling inference network for text-based vqa. ACM Trans. Multimedia Comput. Commun. Appl., 20(4):1 21, 2024. 2, [96] Sheng Zhou, Junbin Xiao, Qingyun Li, Yicong Li, Xun Yang, Dan Guo, Meng Wang, Tat-Seng Chua, and Angela Yao. Egotextvqa: Towards egocentric scene-text aware video question answering. arXiv preprint arXiv:2502.07411, 2025. 1, 3, 7 [97] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 8 Video-R4: Reinforcing Text-Rich Video Reasoning with Visual Rumination"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Limitations Despite these results, Video-R4 still has several limitations. First, the data curation pipeline relies on pre-extracted OCR results and object detections, so recognition errors or missing text can directly hurt both rumination trajectories and final answers. Second, the current tool interface supports only frame selection and spatial cropping with bounded trajectory length, which may be insufficient for very long or fast-changing videos that require richer operations (e.g., tracking, retiming, or audio-aware cues). Third, our training data are primarily derived from M4-ViteVQA and few related text-centric datasets, and experiments are conducted on 7B backbone, leaving open questions about robustness under more diverse domains and larger model scales. Finally, the GRPO reward combines hand-designed proxies such as diversity, representativeness, and curiosity, which only approximate human notions of faithfulness and interpretability. Future work could relax these assumptions by broader operation types, more diverse optimization methods, and rewards. 7. Dataset Details Dataset Statistics. Figure 5 presents the overall statistics of Video-R4-CoT-17k. The dataset is predominantly videobased, with images forming smaller subset. The word cloud highlights frequent reasoning-related expressions such as visual, information, and various operationoriented verbs. The question length distribution centers on medium-length prompts, while the plots of visual operation counts and conversation turns show that CoT trajectories typically require several visual operations and involve multi-round interactions. Figure 6 summarizes the statistics of Video-R4-RL-30k. The corresponding word cloud shows more object-focused vocabulary (e.g., object, person, left, color), consistent with the concise, direct style characteristic of RL-refined queries. Rule-Based Evidence Matching. The Rule-Based Evidence Matching algorithm is shown in Algorithm 1. For each training instance q, we denote by qtext the question text and by qans the answer expression, which may the asbe single string or small set of candidates; sociated video is v(q). Each instance carries two supervisory attributes: the temporal specification src1(q) {Single frame, Multi frame}, indicating whether evidence is restricted to one frame or may span multiple frames, and the modality specification src2(q) {Text, Visual}, indicating whether evidence is primarily textual (OCR) or Figure 5. Overall statistics of the Video-R4-CoT-17k dataset, including the ratio of video versus image samples, word cloud of frequently appearing terms, question length distribution, distribution of visual operation counts per sample, and conversation turn count distribution. visual (objects). We write Aq = tok(qans) and Wq = tok(qtext) for the normalized token sets of the answer and question, respectively. For video v, let Fv be the 1 tion call. For clipping, the parameters are the indices of the selected frames. For cropping, the parameters include frame index and the bounding box coordinates. The final turn predicts the answer to the question with { } format. The template produces dialogue that contains several placeholders. These placeholders include the input video caption and descriptive analysis of the visual observations obtained in each turn. They will be filled in during the following stages. LMM-Based CoT Synthesis and Refinement. We use Qwen2.5-VL [1] to generate video captions, clip captions, and region captions. The input includes the original video frame sequence for the video captions and the text prompts. For clip and region captions, the original video frame sequence, the clips/regions in the current turn, and the text in the context serve as input. The think processes are then generated, focusing on whether the current visual cues obtained can answer the question sufficiently. Then we replace all the placeholders in the templates to get the multi-turn CoT trajectories. We use GPT-4o [28] to further refine the trajectories to make them more coherent, natural, and reasonable. Quality Control Tool. We develop quality control tool to quickly review all the QA queries and the corresponding synthesized trajectories. As shown in Figure 8, the tool supports quick browsing, sample saving, dropping functions, and fixing mode, where human annotators can directly revise the content of the chain-of-thought trajectories, including both text and visual cues. 8. Evaluation Metrics Average Normalized Levenshtein Similarity (ANLS). Exact-match metrics are brittle for text-centric VQA because minor OCR errors can flip correct rationale into an incorrect string. Therefore, [8] proposed ANLS, which turns the normalized Levenshtein distance [33] between prediction and reference into similarity score with cutoff. Let oqi be the models answer for question qi, and {aij}M j=1 the set of ground-truth strings. Denote by L(, ) [0, 1] the normalized Levenshtein distance. With threshold τ = 0.5, the per-pair similarity is s(aij, oqi) = (cid:40) 1 L(aij, oqi) 0 if L(aij, oqi) < τ, otherwise. (8) Then take the best match across references for each question and average over questions: ANLS = 1 (cid:88) i=1 (cid:18) (cid:19) max s(aij, oqi) . (9) Figure 6. Overall statistics of the Video-R4-RL-30k dataset. set of candidate frames considered during evidence mining. For every frame Fv, we assume paragraphlevel OCR regions Pv,f , fine-grained OCR text detections Tv,f = {(sv,f,i, btext v,f,i)}i with strings and corresponding boxes, and object detections Ov,f = {(ℓv,f,k, bobj v,f,k)}k with discrete labels and boxes. Matching and geometry are treated abstractly via the primitives scoretext(s, A) [0, 1] for textanswer relevance, scorename(n, ) [0, 1] for nametoken compatibility, iou(b1, b2) for box overlap, extend(b) for deterministic enlargement, and merge(B) for minimal axis-aligned merging of box set B. The goal is, for each question q, to return subset Rq Fv(q) of relevant frames and per-frame evidence region Bev q,f obtained by combining textual and, when applicable, object cues. We denote by btext q,f the best OCR-derived box selected in frame for question before paragraph refinement. Template-Based Context Synthesis. We construct set of multi-turn dialogues for each annotation through predefined templates. Each dialogue contains system message, question, the path to the input video, and sequence of turns. The turns follow chain-of-thought format. Each turn provides an analysis of the visual information obtained through the visual operation applied in the previous step. The first turn, instead, provides an overall description of the input video. The turn then continues with brief reasoning segment that connects to the next action and ends with statement describing the next visual operation, where the format of the visual operation follows [58], with <tool call> labels to prompt the visual operations, and tool names and parameters are needed for single func2 Algorithm 1 Rule-Based Evidence Matching 1: for each question do initialize Rq 2: for each frame Fv(q) do 3: find best OCR match btext 4: if match exists then Rq Rq {f } end if q,f using scoretext(, Aq) 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for for each Rq do refine btext end for if src2(q) = Text then q,f by selecting Pv,f with maximal iou, then set btext q,f extend(p) choose single or multiple frames according to src1(q) output refined text boxes {btext q,f } continue end if for each Rq do 16: 17: 18: 19: 20: 21: 22: end for collect object boxes whose names match Aq Wq via scorename merge all matched boxes with btext q,f merge() q,f : Bev end for select single or multiple frames according to src1(q) output Rq and {Bev q,f } Figure 7. Comparison of training behaviors across fine-tuning strategies. Subfigures (a) and (b) show that models pre-finetuned on DRPSFT data converge more quickly and achieve lower final loss when training on CRP-SFT, indicating that decomposing visual operations before interleaved training is beneficial. (c) Video-R4-7B progressively increases its response length during RL, suggesting emergent allocation of more thinking time. (d) Correspondingly, the average reward improves and remains stable across iterations. 3 Predictions with edit distance τ (over half the characters wrong) receive zero credit, while smaller deviations are rewarded proportionally. This softly penalizes OCR noise while still emphasizing exactness. Exact Match (EM). This metric quantifies the proportion of predictions that exactly coincide with any of the ground truth answers, thereby providing strict measure of answer correctness [52]. (Macro-averaged) F1 score. This metric assesses the token-level overlap between prediction and the ground truth answer by treating both as bags of tokens and computing their F1 score. For each question, the highest F1 score across all ground truth answers is selected, and the final metric is obtained by averaging these maxima over the full set of questions [52]. 9. More Evidence to Support our Findings Figure 7 (c) demonstrates Video-R4-7B naturally learns to solve reasoning tasks with more thinking time, which is evidence to support our Finding 1. Figure 7 (a) and (b) show the training curve of CRP-SFT under different settings, demonstrating that the models pre-finetuned on DRP-SFT data have faster convergence. Even though losses converge during fine-tuning across different settings, the model fine-tuned on DRP-SFT data achieves better final performance on the benchmarks. This shows that it is helpful to learn different types of visual operations separately before interleaving them during training. As shown in Table 4, results on the validation set of M4-ViteVQA are also reported, demonstrating that Video-R4-7B establishes new state-ofthe-art in text-rich video understanding and reasoning. 10. Training Details For DRP-SFT, we use 7k data from Video-R4-CoT-17k for fine-tuning. The learning rate of 1 106 is adopted. We fully fine-tune the model instead of using LoRA [19]. For the RL after DRP-SFT, we use accuracy and the curiosity reward. There are 15k samples from Video-R4-RL30k used during the stage. Following [58], the curiosity rewards hyperparameters are set as follows: α = 0.5, β = 0.05, and = 0.3. GRPO [18] is adopted as the policy optimization method. Eight responses are sampled for each sample. For the CRP-SFT, 10k samples from VideoR4-CoT-10k are used, and other hyperparameters are the same as those in DRP-SFT. For the RL after CRP-SFT, we adopt accuracy, diversity, representativeness, and curiosity reward, with the coefficients λdiv = λrep = λcur = 1 [93]. All the models are trained on one H100 80G GPU. 4 Figure 8. Interface of the quality control tool used to review QA queries and synthesized chain-of-thought trajectories. The tool enables rapid browsing, frame inspection, saving or dropping samples, and in-place editing of both textual and visual reasoning steps to streamline annotation and correction workflows. 11. More Visualization Results As shown in Figures 9 and 10, we present additional visualizations of the trajectory samples. Table 4. Performance comparison on the M4-ViteVQA validation set and testset. Models Task 1 - Split 1 Task 1 - Split Task 2 Val Test Val Test Val Test Acc.(%) ANLS(%) Acc.(%) ANLS(%) Acc.(%) ANLS(%) Acc.(%) ANLS(%) Acc.(%) ANLS(%) Acc.(%) ANLS(%) JustAsk [74] All-in-one-B [69] Video-LLaVA-7B [40] T5-ViteVQA [90] VideoLLaMA2-7B [11] Qwen2-VL-7B [70] TEA-L [85] NVILA-8B [49] GAT-L [86] Qwen2.5-VL [1] Video-R1-7B [15] Pixel-Reasoner [58] Video-R4-7B (ours) Human 10.81 11.47 15.82 23.17 20.04 36.77 37.49 37.89 38.01 22.22 38.10 54.44 57.33 15.40 15.30 17.77 30.10 21.73 46.56 46.38 47.67 47.53 48.67 50.80 63.57 66.92 10.05 10.87 15.43 22.17 20.76 35.22 34.78 37.73 38.30 26.53 37.10 52.91 56.17 85.27 14.10 14.80 17.15 29.10 23.55 45.84 43.71 47.23 48.23 44.91 48.25 61.44 65.22 89. 7.16 6.85 13.14 17.59 18.30 28.55 28.27 30.25 31.35 17.84 38.40 54.69 57.65 10.00 9.20 14.29 23.10 19.63 39.34 36.32 40.58 41.33 46.72 49.62 62.58 65.15 5.47 5.66 11.19 16.68 18.33 27.25 28.43 30.10 30.90 24.34 33.67 48.88 52.69 78. 8.60 7.80 12.02 23.80 20.45 38.45 38.13 41.52 41.81 39.60 44.94 58.23 61.89 82.80 4.86 4.20 10.89 12.30 19.68 22.95 22.83 23.79 24.54 22.31 47.77 63.78 69.03 6.70 5.00 13.23 16.10 23.62 32.65 30.21 32.89 33.30 42.21 58.52 69.93 75.45 3.60 3.28 9.38 9.29 16.54 21.23 18.83 22.89 22.13 32.81 43.16 58.97 64.21 82.26 6.70 4.60 11.80 13.60 21.80 28.79 28.90 30.34 30.75 50.82 53.37 65.32 69.99 85.10 Figure 9. Trajectories visualization. Figure 10. More visualization with longer trajectories."
        }
    ],
    "affiliations": [
        "MIT-IBM Watson AI Lab",
        "Sony Group Corporation",
        "University of Rochester"
    ]
}