{
    "paper_title": "Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception",
    "authors": [
        "Ziqi Pang",
        "Xin Xu",
        "Yu-Xiong Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at https://github.com/ziqipang/ADDP."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 5 4 1 1 . 4 0 5 2 : r Published as conference paper at ICLR ALIGNING GENERATIVE DENOISING WITH DISCRIMINATIVE OBJECTIVES UNLEASHES DIFFUSION FOR VISUAL PERCEPTION Ziqi Pang Xin Xu Yu-Xiong Wang University of Illinois Urbana-Champaign {ziqip2, xinx8, yxw}@illinois.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "With success in image generation, generative diffusion models are increasingly adopted for discriminative scenarios because generating pixels is unified and natural perception interface. Although directly re-purposing their generative denoising process has established promising progress in specialist (e.g., depth estimation) and generalist models, the inherent gaps between generative process and discriminative objectives are rarely investigated. For instance, generative models can tolerate deviations at intermediate sampling steps as long as the final distribution is reasonable, while discriminative tasks with rigorous ground truth for evaluation are sensitive to such errors. Without mitigating such gaps, diffusion for perception still struggles on tasks represented by multi-modal understanding (e.g., referring image segmentation). Motivated by these challenges, we analyze and improve the alignment between the generative diffusion process and perception objectives centering around the key observation: how perception quality evolves with the denoising process. (1) Notably, earlier denoising steps contribute more than later steps, necessitating tailored learning objective for training: loss functions should reflect varied contributions of timesteps for each perception task. (2) Perception quality drops unexpectedly at later denoising steps, revealing the sensitiveness of perception to training-denoising distribution shift. We introduce diffusion-tailored data augmentation to simulate such drift in the training data. (3) We suggest novel perspective to the long-standing question: why should generative process be useful for discriminative tasks interactivity. The denoising process can be leveraged as controllable user interface adapting to users correctional prompts and conducting multi-round interaction in an agentic workflow. Collectively, our insights enhance multiple generative diffusion-based perception models without architectural changes: state-of-the-art diffusion-based depth estimator, previously underplayed referring image segmentation models, and perception generalists. Our code is available at https://github.com/ziqipang/ADDP."
        },
        {
            "title": "INTRODUCTION",
            "content": "The success of diffusion models (Ho et al., 2020; Ramesh et al., 2022; Rombach et al., 2023) has gone beyond pure image generation recently. They emerge as attractive candidates for perception models (Gan et al., 2024; Ke et al., 2024) with the prior knowledge stored in their pre-trained weights and the vision of generalist models via unifying various perception tasks under the same pixel generation interface. Recent approaches finetune pre-trained diffusion models and have made promising progress in both state-of-the-art depth estimation (Ke et al., 2024) specialist and generalist models (Gan et al., 2024) supporting tasks from geometric depth estimation to semantic segmentation and detection. However, they commonly focus on designing the generative format without investigating the fundamental gaps between the generative diffusion process and discriminative tasks: the generative process aims at sampling reasonable distributions, while discriminative tasks require precise matches with rigorous ground truth. Without addressing such discrepancies, generative diffusion models noticeably under-perform when perception tasks involve intricate multi-modal reasoning, e.g., referring image segmentation (RIS), which consequently constrains the exploration of generative perception (Geng et al., 2023). Therefore, the investigation of this paper aspires to bridge the gaps between the generative denoising process and discriminative perception. Equal contribution. 1 Published as conference paper at ICLR 2025 Figure 1: We demonstrate the gaps between generative denoising process and perception tasks using referring image segmentation (RIS), where the diffusion model learns to color the referred object with red masks. (a)(b) The perception quality (Intersection-over-Union, IoU) at intermediate denoising steps, which come from the same denoising trajectory, reveals the uneven contribution of timesteps and training-denoising distribution shift, addressed by our enhanced learning objective (c) We discover that the generative denoising process is also unique user and training data. interface for discriminative perception, because of its capabilities to interact with the correctional guidance from users or foundation models. In principle, the most profound difference between generative denoising process and conventional discriminative model is the iterative sampling procedure, where diffusion model gradually approximates the final prediction by sampling from score function (Song et al., 2021) step by step. However, such an intuition does not align with the reality of perception. As an intuitive illustration, we choose the challenging RIS task and inspect how the perception quality evolves during the denoising process  (Fig. 1)  . Following previous diffusion-based perception (Gan et al., 2024; Geng et al., 2023), our model adopts an image editing format and specifies RIS as editing the target region, i.e., the objects referred by the language prompt, to red masks. Ideally, the denoising timesteps should gradually refine these red masks to distinguish the target object, but their IoU (Fig. 1a) and appearances (Fig. 1b) unveil the opposite: (1) the contribution of timesteps is significantly uneven; and (2) perception quality drops surprisingly at later denoising steps. Centering around these observations, we align the training of the denoising process with the reality of the sampling process in diffusion-based perception models, including the learning objective and training data. The uneven contribution of denoising steps (Fig. 1a) motivates us to enhance the learning objective of diffusion models by reflecting the perception contribution of every timestep in the loss functions. In conventional diffusion training, e.g., DDPM (Ho et al., 2020), the timesteps are treated uniformly to learn single-step denoising. However, perception tasks need to minimize the distance between the ground truth and accumulation of multi-step denoising, which necessitates enhancing the training of more critical steps. Moreover, the surprising decrease of perception quality (Fig. 1b) arises from the training-denoising distribution shift, which is unique under the diffusion-based perception context: deviated distribution from sampling steps can still produce reasonable images, but they are wrong for discriminative tasks with rigorous ground truth. To train diffusion models that are robust to such distribution shift, we leverage data augmentations to simulate the erroneous intermediate denoising steps by purposefully corrupting the ground truth. Such improvement to training data addresses distribution shifts in the denoising process and maintains the perception quality until later steps. Finally, we suggest novel perspective to the long-standing question: how can the stochastic generative process be useful for discriminative tasks? This becomes an increasingly important question when diffusion models are used as feature extractors (Zhao et al., 2023) without the denoising process. Instead, we propose that the generative process enables an interactive and interpretable user interface. Specifically, diffusion model can be guided by the correctional prompts from users to adjust their predictions progressively (as in Fig. 1c) with classifier-free guidance (Ho & Salimans, 2022), which is rare ability for conventional single-step discriminative models. For example, our diffusion model enables using language as the multi-round reasoning interface for RIS in an agentic workflow (Ng, 2024) built from GPT4 (Achiam et al., 2023). To conclude, we have made the following contributions to align the generative denoising process in diffusion models for perception: 2 Published as conference paper at ICLR 2025 1. Learning objective. We illustrate the uneven contribution across denoising timesteps and reveal such importance by specifying the sampling weights of timesteps accordingly. 2. Training data. We demonstrate the training-denoising distribution shift and introduce diffusiontailored data augmentation to effectuate especially the later denoising steps. 3. User interface. We suggest the unique interactivity advantage of diffusion models for perception: they can progressively modify the predictions via the correctional prompts from humans or foundation models, which is essential for an agentic workflow and human-involved applications. Our insights are collectively named ADDP (Aligning Diffusion Denoising with Perception). Its enhancements generalize across diverse generative diffusion-based perception models, including state-of-the-art diffusion-based depth estimator Marigold (Ke et al., 2024) and generalist InstructCV (Gan et al., 2024). Our ADDP also extends the usability of diffusion-based perception to multi-modal referring image segmentation, where we enable diffusion model to catch up with some discriminative baselines for the first time. We hope ADDP overcomes the limitations of generative diffusion-based perception and unlocks new opportunities in this domain."
        },
        {
            "title": "2 PRELIMINARIES\nDiffusion Models. Diffusion models have been analyzed in multiple formulations (Ho et al., 2020;\nKarras et al., 2022; Song et al., 2021), and here we adopt the DDPM (Ho et al., 2020) style since most\ndiffusion-based perception models are implemented in DDPM’s way. In DDPM, diffusion models\nlearn the image distribution P(x) via a reverse Markov chain with length T . It gradually denoises\na random variable xT , which commonly follows Gaussian distribution, into the target variable x0.\nDuring training, the model learns a denoising objective ε with a neural network εθ (·),\nt ∼ Uniform({1, ..., T }), ε ∼ N (0, I), xt =\n¯αt x0 +\n2. (1)\nTo synthesize high-resolution images, recent latent diffusion models, e.g., Stable Diffusion (Rom-\nbach et al., 2023), encode images into a latent space for denoising. By training at scale (Schuhmann\net al., 2021), these models can integrate text conditions as εθ (xt ,t, D), where D denotes a language\ndescription. Such prior knowledge is the basis of using latent diffusion models for perception.",
            "content": "1 αt ε, = E(xt ,t,ε)ε εθ (xt ,t)2 (cid:112) Visual Perception Tasks. We investigate diverse perception tasks and diffusion models to understand the gaps between generative models and discriminative objectives. (1) Depth Estimation. We focus on the state-of-the-art diffusion-based Marigold (Ke et al., 2024). Its diffusion model operates as εθ (xt , I,t), where xt is the image latent for depth maps and is latent of the input image. (2) Referring Image Segmentation (RIS). RIS involves an input image and referring description for the target object. We treat RIS as an image editing task and adopt the common framework of InstructPix2Pix (Brooks et al., 2023). Concretely, the objective is to edit the pixels of the target object to red via the diffusion model of εθ (xt , I, D,t), where xt is the latent of the image with red segments on the target object. Experimental analysis of our editing formats is in Sec. B.1. (3) Generalist Perception. We follow InstructCV (Gan et al., 2024) to build generalist multi-task perception model for depth estimation, semantic segmentation, and object detection. This generalist model similarly uses InstructPix2Pix to unify diverse tasks into image editing. Concretely, the model operates as εθ (xt , I, D,t), where is the description prompt of the task e.g., Detect %Category%, and the output xt is image latent for depth maps, segmentation masks, or bounding boxes."
        },
        {
            "title": "3 METHOD\nMotivated by the gaps between the diffusion process and perception objectives (Fig. 1), we propose\nthe corresponding alignments in Fig. 2, which are simple and plug-and-play for diffusion models.\n(1) Sec. 3.1: improving the learning objective by resembling the uneven contribution of timesteps.",
            "content": "Figure 2: Method overview. We align the generative diffusion models with perception tasks from learning objective, training data, and user interface. Notations follow DDPM (Ho et al., 2020). 3 Published as conference paper at ICLR 2025 (2) Sec. 3.2: simulating the distribution shift by improving the training data with data augmentation. (3) Sec. 3.3: re-purposing classifier-free guidance to enable interactive user interfaces via the generative denoising process. 3.1 LEARNING OBJECTIVE: CONTRIBUTION-AWARE TIMESTEP SAMPLING We observe the uneven contribution of denoising timesteps in the evolution of perception quality  (Fig. 1)  , e.g., earlier steps closer to =T have more significant influence than later steps closer to =0. This aligns with image generation observations (Zhang et al., 2024b) where earlier steps conduct more influential semantic planning. However, such properties are not reflected in the learning objective of conventional diffusion training (Eqn. 1): the timesteps are uniformly sampled, and the optimization targets are all ε (0, I) with similar scales. So why and how should diffusion models tailor their training for distinct timesteps under perception scenarios? Necessity of Distinguishing Timesteps in Diffusion Training. This design is rooted in the different objectives of generative and discriminative tasks. Diffusion models can learn single-step score functions for generative tasks to sample reasonable images, but discriminative tasks require the final predictions, which are multi-step integral of score functions, to precisely match rigorous ground truths. To better explain its impact on the learning objective, we define contribution factors ct , denoting the relative contribution of timestep for the final result x0, i.e., x0 t=1 ct εθ (xt ,t)1. Then the distance between prediction x0 and ground truth x0 is decomposed as below, where εt is the ground truth noise at timestep t: ( x0,x0) x0 x02 2 ( x0,x0,ε1,..,εT ) t=1 c2 εt εθ (xt ,t)2 2. (2) Before discussing the implication of Eqn. 2, we clarify the truncation terms for the right-hand side. (1) E( x0,x0,ε1,..,εT ) t1=t2 (εt1 εθ (xt1,t1))(εt2 εθ (xt2,t2))2 2. As εt is randomly sampled from (0, I), the terms (εt εθ (xt ,t)) are independent, making the whole truncation term zero; thus, this term can be ignored. (2) The intermediate xt in the denoising process may drift from the precise trajectory xt for precisely sampling the ground truth x0. Our approximation ignores the truncation errors caused by this drift since it is intractable over iterative sampling. With these analyses, we proceed with the right-hand side of Eqn. 2 to improve the diffusion loss functions. natural implication from Eqn. 2 is that the contribution c2 needs to be reflected in the training loss of each timestep Lt =εt εθ (xt ,t)2 2. This does not contradict the original diffusion objective since the model still learns to fit the score function on single timesteps. However, the errors from more influential timesteps are penalized more in our way, aligning better with the perception objective. As side note, this principle is consistent with how rectified flow models re-weight the loss functions of timesteps to guide the optimization (Kingma & Gao, 2024). However, we additionally offer guidelines to utilize and estimate the values of c2 for specific perception objectives. Concretely, one can: (1) scale the loss values by c2 , or (2) scale the sampling probability of timesteps with multinomial distribution using c2 as sampling weights for  (Fig. 2)  . Both variants improve diffusion for perception, but the probability scaling method performs better (Sec. 4.3.1). The following parts discuss how to estimate the values of c2 . Deriving c2 models since each step εθ (xt ,t) can be converted to x0 following DDPM (Ho et al., 2020): from Diffusion Formulation. To start with, c2 is an inherent property of diffusion x0 = 1 αt xt 1 αt αt εθ (xt ,t). (3) 1 αt αt Therefore, we can interpret x0 can be explained by εθ (xt ,t). Then we normalize them to acquire c2 as the relative importance of timestep t, indicating how much of = ( 1 αt αt )/ i=1( 1 αi αi ). Deriving c2 from Perception Statistics. The above derivation is discussed in terms of the latents xt (Eqn. 2). However, we find the contribution of timesteps sensitive to the perception tasks and diffusion models. For example, Marigold (Ke et al., 2024) for depth estimation exhibits smoother precision (δ1) curve during denoising  (Fig. 3)  compared with the IoU in RIS (Fig. 1a). Therefore, c2 becomes unique property for each diffusion-based perception scenario and motivates us to estimate each task statistically with the same principle of c2 : how much of the final prediction can 1We use here to accommodate varying noise schedulers and the normalization of estimating ct . 4 Published as conference paper at ICLR 2025 Figure 4: Data augmentation of (a) Gaussian blurring for depth estimation, and (b) color / shape / location for RIS. We use large / small intensities of augmentations to simulate different scales of distribution shifts at the earlier / later steps of denoising. be explained by an intermediate denoising step? Concretely, we can derive this via the coefficient of determination in regression analysis, denoted as R2, which measures the goodness of fit. This procedure involves three steps. (1) Data collection. We apply diffusion-based perception baseline to validation samples and get metric values of intermediate denoising steps. Without loss of generality, we take IoU from RIS as an example and acquire {IoUt,i}tT,iN. (2) Initial regression. We initialize the estimation with the first step c2 by running linear regression of IoU0,: =β + βT IoUT,:. The R2 value of this regression, denoted as (R2)T , is the proportion of the final IoU explained by the first denoising step, so we adopt it as c2 (3) Iterative estimation. We iteraT . tively add new timesteps into the regression model and set c2 (R2)t (R2)t+1, indicating the increase in explained IoU with the new timestamp. Please note that (R2)t (R2)t+1 is non-negative since adding new variables can only improve the regression fit. More discussions and implementation details are in Sec. A.2. Figure 3: Evolution of δ1 (intuitively the accuracy for depth estimation) from Marigold (Ke et al., 2024) shows smoother patterns than RIS. We copy the RIS curve from Fig. 1a here for easier comparison. 3.2 TRAINING DATA: DIFFUSION-TAILORED DATA AUGMENTATION In RIS scenarios, we observe the unexpected IoU drop at later denoising steps (Fig. 1a), where the masks gradually deviate from correct regions and show hallucinated shapes (Fig. 1b). This reveals the training-denoising distribution shift: xt during training (Eqn. 1) comes from the ground truth, while it might deviate from the ideal sampling path during inference. Generative model studies (Ning et al., 2023; 2024) call this exposure bias. However, it is even more critical for discriminative scenarios: shifted xt might still produce reasonable images belonging to the distribution of ground truth but no longer fit the desired ground truth of that sample precisely. Necessity of Simulating Distribution Shift. The ideal solution for distribution shift is to train the diffusion models with xt sampled from the actual denoising process. However, this is computationally infeasible due to the iterative nature of denoising. Therefore, we take step back and introduce the solution of simulating the distribution shift for training with augmentations to the ground truth. Diffusion-tailored Data Augmentation. We purposefully corrupt the ground truth x0 into 0 so that the xt for training (Eqn. 1) reflects distribution shift. Such corruption depends on the timesteps by using more intense augmentation for earlier timesteps: Intuitively, the perception results are coarser at the initial denoising and should be simulated with larger deviations from the real ground truth. When incorporated into the training pipeline of DDPM, the procedure becomes: 0 = Augment(x0,t), ε (0, I), xt = αt 0 + 1 αt ε. (4) (cid:112) We design different augmentations to capture the typical distribution shift for each task as in Fig. 4. For instance, the RIS format is red mask, so our designed augmentation involves color (color changes), location ( transformations to masks), and shape (random erasing of mask parts); while depth estimation mimics coarse boundaries and adopts Gaussian blur. As critical implementation details, we discover that x0-prediction of diffusion models are more suitable for such data augmentation than its mathematically equivalent ε-prediction, and the benefits of varying augmentation intensities w.r.t timesteps. More details are discussed in Sec. A.3. Discussion: Distinctions with Conventional Data Augmentation. Our data augmentation significantly improves the performance and effectuated later denoising steps (Sec. 4.3). Moreover, our 5 Published as conference paper at ICLR 2025 insights are different from conventional data augmentation. Compared with perception studies, e.g., RIS, cannot adopt augmentation since masks close to image borders disable random cropping, and referring with up/down/left/right disable random flipping. Compared with diffusion studies, we extend the boundary of the common practice of merely training on ground truth denoising trajectories the diffusers εθ () can explicitly learn to correct problematic input into precise prediction. 3.3 USER INTERFACE: INTERACTIVITY VIA CORRECTIONAL GUIDANCE We suggest novel perspective on the value of diffusion denoising process for discriminative tasks: with perception tasks intrinsically deterministic, why and how would the generative sampling in diffusion models be helpful? This long-standing problem is increasingly important with emerging studies using pre-trained diffusion models as single-step generative models or feature extractors (Parmar et al., 2024; Xu et al., 2024), without leveraging the multi-step generative process. Besides the vision of unifying perception into pixel synthesis, we demonstrate that generative model can serve as an interactive user interface for perception, which is especially critical for human-involved applications and beyond the capabilities of conventional discriminative models. Interactivity via Denoising. Diffusion models can be expressed with score-matching formulation (Song et al., 2020), where εθ (xt ,t) matches score function xt log p(xt ). This provides natural way to control the generation with explicit user guidance from the interaction with humans or foundation models. Multi-modal understanding, i.e., the previously overlooked RIS in diffusion-based perception, is an ideal application to demonstrate the unique interactivity value. We show an intuitive example in Fig. 1c. Figure 5: correctional guidance D. Interacting with Correctional Guidance via Classifier-free Guidance. Our approach is based on classifier-free guidance (Ho & Salimans, 2022): given condition C, adding (εθ (xt ,t,C) εθ (xt ,t, φ )) (φ denotes empty condition) to the original prediction can nudge the diffusion result to align better with C. In the case of RIS, we consider both conditions of referring description and image I. If the model makes an error and the user can specify the error with language descriptions D, the original prediction can be corrected by adding (εθ (xt ,t, D, I)εθ (xt ,t, D, I))  (Fig. 5)  . Here, we refer to as correctional guidance, similar to negative prompts (Podell et al., 2024) but grounded in perception and visionlanguage reasoning. Based on the compositionally of score functions (Liu et al., 2022), we utilize the correctional guidance in addition to the image guidance term of wI: εθ (xt ,t, D, D, I) = εθ (xt ,t, φD, φI) + wI εθ (xt ,t, φD, I) εθ (xt ,t, φD, φI) (cid:16) (cid:17) + + wD (cid:17) (cid:16) εθ (xt ,t, D, I) εθ (xt ,t, φD, I) (cid:17) (cid:16) εθ (xt ,t, D, I) εθ (xt ,t, D, I) . (5) and wD are scalers for the correctional guidance strength. By setting wD > the margins between denoising from and D. More discussion and details are in Sec. A.4. D, Eqn. 5 increases Integration with Agentic Workflows. To validate the value of such user interface at large scale, we construct an agentic workflow (Ng, 2024) with GPT4o (Achiam et al., 2023) to automatically generate the correctional prompts D. Concretely, we propose two-round proof-of-concept workflow. (1) Use LLaVA (Liu et al., 2023b) to provide detailed caption of the original image I. (2) language-only GPT4o guesses top-k confusing objects in the image based on the referring and image caption. (3) Our diffusion model generates new predictions from each correctional prompt using Eqn. 5, and applies majority vote to produce final mask. More details are in Sec. A.4."
        },
        {
            "title": "4 EXPERIMENTS\n4.1 DATASETS AND IMPLEMENTATION DETAILS\nOur insights are generalizable for diffusion-based perception and cover diverse scenarios. (1) We im-\nprove the state-of-the-art diffusion-based Marigold (Ke et al., 2024) for depth estimation, following\nthe same zero-shot evaluation setups. (2) We investigate RIS, where previous diffusion-based mod-\nels show large gaps to discriminative counterparts. We follow the standard practice by fine-tuning an",
            "content": "6 Published as conference paper at ICLR 2025 Method DiverseDepth MiDaS LeReS Omnidata HDN DPT Diode ETH3D ScanNet NYUv2 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 AbsRel δ1 70.4 63.0 78.4 83.5 86.7 90.1 63.1 71.5 76.6 74.2 78.0 75.8 88.2 84.6 91.7 93.6 93.9 93.4 69.4 75.2 77.7 77.8 83.3 94.6 87.5 88.5 91.6 94.5 94.8 90. 22.8 18.4 17.1 16.6 12.1 7.8 10.9 12.1 9.1 7.5 8.0 8.2 11.7 11.1 9.0 7.4 6.9 9.8 19.0 23.6 14.9 14.9 11.5 10.0 37.6 33.2 27.1 33.9 24.6 18.2 KITTI Marigold +ADDP (Ours) 7.1 6.3 95.1 96.1 6.9 6.3 94.5 95.6 6.0 5. 95.9 96.3 31.0 29.6 77.2 77.5 10.5 10.0 90.4 90.6 Average Rank 7.4 7.1 5.1 4.7 3.1 3.8 2.4 1.4 Table 1: Comparison with diffusion-based depth estimator Marigold (Ke et al., 2024) with identical pre-training and zero-shot generalization to real-world benchmarks. Bold numbers are the best, underscored are the second best. Our method ADDP uses contribution-aware timestep sampling (Sampling) and diffusion-tailored data augmentation (Aug) and consistently improves Marigold across these scenarios. Method MCN EFN VLT ReSTR CRIS LAVT VPD ReLA PVD UNINEXT RefCOCO RefCOCO+ G-Ref val test-A test-B val test-A test-B val test Discriminative Encoder-Decoder Based 62.44 62.76 65.65 67.22 70.47 72.73 73.25 73.82 74.82 77.90 64.20 65.69 68.29 69.30 73.18 75.82 - 76.48 77.11 79.68 59.71 59.67 62.73 64.45 66.10 68.79 - 70.18 69.52 75. 50.62 51.50 55.50 55.78 62.27 62.14 62.69 66.04 63.38 66.20 54.99 55.24 59.20 60.44 68.08 68.38 - 71.02 68.60 71.22 Generative Image Synthesis Based 44.69 43.01 49.36 48.27 53.68 55.10 - 57.65 56.92 59.01 40.15 39.04 35.31 48. 49.22 51.93 52.99 - 59.87 61.24 61.96 65.00 63.13 70.04 48.74 51.17 43.99 55.85 49.40 - 56.65 - 60.36 62.09 - 65.97 63.62 70.52 49.13 52.02 45.43 57. Unified-IO InstructDiffusion InstructPix2Pix-SD1.5 + ADDP (Ours) 46.42 61.74 60.87 66.86 46.06 65.20 63.70 67. 48.05 60.17 58.39 63.72 40.50 46.57 44.98 55.35 42.17 52.32 51.93 58. 64.96 69.14 66.72 70.27 InstructPix2Pix-SD2.0 + ADDP (Ours) 62.63 67.46 Table 2: RIS Comparison. Our insights collectively mitigate the gaps between generative and discriminative ones by large progress. Although not achieving the state of the art, our improvements empower the common diffusion baseline, i.e., RIS finetuned InstructPix2Pix, to catch up with some representative discriminative baselines for the first time. We hope this improved baseline removes the constraints and encourages new opportunities for perception with generative diffusion models. 38.99 51.67 50.28 59. 47.13 57.58 53.32 61.65 50.58 59.60 InstructPix2Pix (Brooks et al., 2023) model on RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), and G-Ref (Nagaraja et al., 2016) separately for 60 epochs. (3) We follow InstructCV (Gan et al., 2024) and prove the effectiveness of our insights under multi-task generalist setting, where single model addresses depth estimation, semantic segmentation, and object detection. Due to space limits, the detailed training and evaluation setups for these experiments are in Sec. A. 4.2 MAIN RESULTS 4.2.1 DEPTH ESTIMATION Diffusion-based perception methods are already effective for depth estimation, represented by the recent Marigold (Ke et al., 2024). Although only trained on synthetic depth, Marigold performs competitively in zero-shot way. We apply both of our improvements on learning objectives (Sec. 3.1) and training data (Sec. 3.2) to Marigold and show the quantitative comparison following Marigolds style in Table 1. Notably, our proposed techniques consistently improve Marigold across all the benchmarks. We conduct detailed ablations of the two techniques for depth estimation in Sec. B.4. 4.2.2 REFERRING IMAGE SEGMENTATION (RIS) Improvement of Generative RIS. We format RIS as an image editing problem and separately train on RefCOCO, RefCOCO+, and G-Ref, for fair comparison with previous studies. As in Table 2, we focus on the comparisons with generative methods and include discriminative approaches for context. We first emphasize the significant challenge of RIS for generative perception methods, despite their strong performance in other tasks like depth estimation. This limitation constrains the 7 Published as conference paper at ICLR 2025 Figure 6: Interactive interface enables diffusion models to adaptively correct their predictions via language models guidance D. Such capabilities of progressiveness are beyond conventional discriminative models and are an emerging advantage of the generative denoising process in perception. Method InstructCV +ADDP (Ours) NYUv2 (Depth Estimation) ADE20K (Semantic Segmentation) COCO (Object Detection) mIoU 46.67 48. mAP@0.5 46.6 48.1 RMSE 0.302 0.288 Table 4: Generalist Perception. We follow InstructCV (Gan et al., 2024) and build multi-task generalist perception model using InstructPix2Pix without task-specific components. Our techniques show consistent improvement across these three fundamental perception tasks. development of generative perception research. Note, our claim is not on achieving state-of-the-art RIS performance. Rather, we demonstrate that our ADDP, with its plug-and-play insights, substantially narrows the gap between generative and discriminative methods, enabling common diffusion framework InstructPix2Pix to catch up with some RIS baselines for the first time, without modifying the model or introducing extra data. We hope our enhanced diffusion-based method inspires further exploration of generative perception in tackling multi-modal understanding challenges. From Table 2, we have the following conclusions. (1) Compared with other generative methods, especially the baseline of InstructPix2Pix, we significantly and consistently improve all the RIS subsets through the integration of better learning objective (contribution-aware timestamp sampling) and training data (diffusion-tailored data augmentation). (2) Compared with the few existing methods adapting pre-trained image generative models for perception, we outperform them by large margin without tailoring the model architecture or using more data for RIS, especially another InstructPix2Pix-based method InstructDiffusion (Geng et al., 2023). These indicate that our discovered insights are critical for designing generative perception models and open new opportunities for diffusion-based perception. (3) Our insights generalize across different stable diffusion models (Rombach et al., 2023), enhancing them by large margin. More ablations are in Sec. 4.3. Effectiveness of Generative Denoising as Interactive User Interfaces. To validate the benefits of interactivity (Sec. 3.3) and support the value of the generative process for perception, we evaluate our proof-of-concept agentic workflow with correctional guidance on the validation sets of RIS, Please note that our Table 2 does not use such interactive reasoning to avoid unfair comparison with other RIS methods. As in Table 3, our workflow can improve the grounding and gain larger advantages on harder scenarios (G-Ref), especially the most challenging G-Ref. In Fig. 6, we further illustrate qualitative examples of how our generated correctional prompts modify the grounding results via the reasoning conducted by language models. These results indicate that the interactive interface of diffusion models is beneficial for perception tasks involving reasoning or user interaction. Table 3: Effectiveness of correctional guidance, especially on hard scenarios (G-Ref). InstrucPix2Pix +Sampling+Aug (Ours) +Correctional Guidance (Ours) RefCOCO RefCOCO+ G-Ref 47.14 55.35 56.13 60.87 66.86 66.93 50.28 55.85 56. Method 4.2.3 GENERALIST PERCEPTION key motivation for using diffusion models for perception is the vision of building generalist perception models by unifying diverse tasks into image generation. We follow InstructCV (Gan et al., 2024) in this endeavor and solve three fundamental perception tasks simultaneously: depth estimation, semantic segmentation, and object detection. They are formatted as image editing, addressed with the InstructPix2Pix (Brooks et al., 2023) framework. As shown in Table 4, our ADDP consistently improves the InstructCV using vanilla InstructPixPix across all three tasks. This validates the effectiveness of our approach for broader diffusion-based perception models. 4.3 ABLATION STUDIES We analyze the effectiveness of our insights through series of ablation studies on the most challenging RIS task. Without special mention, the experiments are conducted on the RefCOCO benchmark with 20 epochs of training as Sec. 4.1 and evaluation on the RefCOCOs validation set. More ablation studies are provided in Sec. B. 8 Published as conference paper at ICLR 2025 oIoU Method c2 N/A 1, ..., c2 Perception Stats 56.42 58.21 63.05 64.00 ), where t in diffusion training. Uniform Loss Scaling Diffusion Prob Scaling Diffusion Prob Scaling Table 5: Strategies of using the contribution c2 4.3.1 CONTRIBUTION-AWARE TIMESTAMP SAMPLING In Table 5, we analyze the strategies proposed in Sec. 3.1: enlarging the contribution of earlier denoising steps in learning objectives. Specifically, we compare four strategies: (1) Uniform: the original DDPM strategy, where the timesteps are uniformly sampled, and the losses are not scaled; (2) Loss Scaling (Diffusion): scaling the loss of timestep by c2 (3) estimated from the diffusion formulation. Prob Scaling (Diffusion): Sampling the timesteps by Multinomial(c2 is derived from the diffusion formulation. (4) Prob Scaling (Perception Stats): Sampling the timesteps with the c2 estimated from the perception (IoU) statistics. As shown in Table 5, reflecting the contribution of timesteps in either sampling or loss weights enhances the uniform baseline. With c2 from diffusion weights, scaling the sampling probability is better than scaling the loss, which is likely due to that εθ (xt ,t) is trained with more iterations at earlier denoising steps under the probability scaling. Moreover, c2 estimated from the perception tasks performs the best, since this is most closely aligned with the objective. These comparisons support our design in Sec. 3.1 to improve the learning objective of diffusion for perception. 4.3.2 DIFFUSION-TAILORED DATA AUGMENTATION Effectiveness of Data Augmentation. In Fig. 7, we compare the IoU-Timestep curves before and after applying data augmentation. Specifically, we calculate the IoU at the 2nd, 20th, 40th, 60th, 80th, and 100th sampling out of 100 denoising steps in total. Fig. 7 validates our diffusion-tailored data augmentation from (1) The quality of masks significantly improves. two aspects. Such an increase mostly comes from the earlier denoising steps, indicating the benefits of providing more challenging inputs to the diffusion model and enforcing the model to correct the errors. (2) The trend of IoU-Timestep curve shows that IoU keeps increasing slowly after =800, contrasting the decrease of InstructPix2Pix and InstructPix2Pix+Sampling between =800 and =200. Despite subsequent slight drop in metrics at the final stage, our data augmentation largely decreases the overall drops after the early steps. Therefore, our enhanced training data indeed aligns the denoising process with perception tasks by mitigating the training-denoising distribution shifts. Figure 7: IoU-Timestep curves. augmentation deOur data creases the training-denoising distribution shifts. Data Augmentation Intensity. In Fig. 8, we investigate the relationship between the intensity of data augmentation and final performance. As in Sec. 3.2 and Sec. A.3, the intensity of data augmentation specifies the corruption to the ground truth. For RIS, larger intensities indicate larger changes in ground truth masks color, location, and shape. We regard the intensity used in Table 2 and Table 5 as the base level (1), which introduces visually reasonable corruptions. We also evaluate performance under conditions of no augmentation (0), reduced intensity (0.5), and increased intensity (2). As demonstrated in Fig. 8, higher augmentation intensity leads to improved performance, indicating that more intense data augmentation enhances the discriminative capabilities of diffusion models. These findings validate the effectiveness of our data augmentation strategy. We utilize the median intensity data augmentation in the main experiments, because it visually aligns with our observed data drift during the denoising phase. Further investigation of more intense augmentations will be our future work. Figure 8: Augmentation Intensity."
        },
        {
            "title": "5 RELATED WORK\nDiffusion Models. Diffusion models (Ho et al., 2020; Karras et al., 2022; 2024; Sohl-Dickstein\net al., 2015) are probabilistic models denoising from Gaussian noises, guided by a reverse Marko-\nvian process. They exhibit better training stability than generative adversarial networks (Esser\net al., 2021; Goodfellow et al., 2014) or variational auto-encoders (Kingma & Welling, 2014; Van\nDen Oord et al., 2017). The recent advances in diffusion models have achieved outstanding text-to-\nimage synthesis ability (Ramesh et al., 2022), especially with the latent diffusion models represented",
            "content": "9 Published as conference paper at ICLR 2025 by the Stable Diffusion series (Esser et al., 2024; Podell et al., 2024; Rombach et al., 2023). The capabilities of generating realistic images with conditioning have motivated numerous applications represented by image editing (Brooks et al., 2023; Hertz et al., 2022; Sheynin et al., 2024) and controllable image generation (Zhang et al., 2023). Besides training stability, diffusion models have the intuition of score-matching functions (Song et al., 2021) and support guidance at the denoising time (Dhariwal & Nichol, 2021; Ho & Salimans, 2022), which is crucial for improving the consistency with conditioning. The success of diffusion models is also progressing quickly in other modalities, such as 3D (Poole et al., 2022) and video generation (Ho et al., 2022). Our study mainly improves such diffusion models under perception perspective (Gan et al., 2024; Geng et al., 2023; Ke et al., 2024; Xing et al., 2023). Concretely, our insights are plug-and-play alignment to train better diffusion-based perception models, effectuating the generative denoising process for perception objectives. Furthermore, we illustrate how classifier-free guidance (Ho & Salimans, 2022) can be uniquely re-purposed for vision-language reasoning and imply the unique value of generative models for discriminative tasks. Diffusion Models for Perception. Recent studies adopting pre-trained diffusion models for perception, e.g., Stable Diffusion (Rombach et al., 2023), can be categorized into three groups. (1) Diffusion models can synthesize virtual training examples (Nguyen et al., 2024; Tian et al., 2023; 2024; Wu et al., 2023) for perception models. (2) The most profound trend is to leverage pre-trained backbones in diffusion models as feature extractors in perception tasks, supporting tasks like segmentation (Xu et al., 2023; Zhao et al., 2023), depth estimation (Xu et al., 2024; Zhao et al., 2023), 3D understanding (Man et al., 2024), and finding correspondence (Hedlin et al., 2024; Luo et al., 2024; Namekata et al., 2024; Tang et al., 2023; Zhang et al., 2024a). However, these methods do not fully leverage the generative capabilities of diffusion models. (3) Our focus is the last category, which unleashes the generation ability of diffusion models and envisions pixel synthesis as the pivot to developing generalist models (Gan et al., 2024; Geng et al., 2023; Xing et al., 2023). Learning perception tasks also improve the precision of generation, e.g., EMU-Edit (Sheynin et al., 2024). Despite the success in depth estimation (Ke et al., 2024), we notice that generative perception remains challenging and inferior to discriminative methods on domains like multi-modal reasoning. Our studies enhance the training and inference of diffusion models by aligning the denoising process with discriminative tasks from the perspectives of learning objectives and training data. Such improvements bring consistent improvement under several scenarios and critically empower competitive diffusion-based baselines for multi-model understanding. In addition, we suggest the unique value of the generative process for visual perception as interactive user interfaces. We hope our discoveries open new opportunities and enable more studies in perception using diffusion models."
        },
        {
            "title": "6 CONCLUSION\nThis study investigates the missing parts of diffusion models for perception tasks from the fundamen-\ntal distinction between generative and discriminative tasks: generation requires sampling diverse and\nreasonable contents, while discriminative perception needs a precise match with the rigorous ground\ntruth. We unveil the gap between the conventional diffusion denoising process and perception tasks\nand propose plug-and-play enhancements in learning objective (contribution-aware timestamp sam-\npling) and training data (diffusion-tailored data augmentation). In addition, we highlight the unique\nadvantage of diffusion models as interactive and interpretable user interface for perception tasks,\nempowering multi-round reasoning via agentic workflows. We hope our insights will foster further\nexploration and improvement of generative models for perception.",
            "content": "Discussion and Limitations. We investigate wide range of diffusion-based perception models and unlock significantly improved baselines. However, we acknowledge that generative perception is inherently challenging: the methods using the denoising process, instead of treating diffusion models as feature extractors, might still underperform on challenging tasks, such as the RIS in our paper. Moreover, the diffusion models are pre-trained for image generation purposes without alignment with perception use cases. For example, Stable Diffusion (Podell et al., 2024; Esser et al., 2024) series employ data filtering to only train on highly aesthetic images, which potentially hurts the generalization to perception data. Therefore, how to guide diffusion models for perception tasks during the pre-training stage will be meaningful for future work. Finally, our ADDP methodology could be relevant for generative tasks with relatively well-defined ground truth, such as super-resolution and 3D reconstruction. We hope ADDP can inspire further exploration in such directions. 10 Published as conference paper at ICLR 2025 ACKNOWLEDGMENTS This work was supported in part by NSF Grant 2106825, NIFA Award 2020-67021-32799, the Toyota Research Institute, the IBM-Illinois Discovery Accelerator Institute, the Amazon-Illinois Center on AI for Interactive Conversational Experiences, Snap Inc., and the Jump ARCHES endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation. This work used computational resources, including the NCSA Delta and DeltaAI supercomputers through allocations CIS230012 and CIS240387 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, as well as the TACC Frontera supercomputer, Amazon Web Services (AWS), and OpenAI API through the National Artificial Intelligence Research Resource (NAIRR) Pilot. ETHICS STATEMENT The studies conducted in this paper do not have explicit ethics concerns. However, our method potentially shares the social biases of pre-trained diffusion models during data filtering, annotation, and training stages. Therefore, we aim to understand such generative models for perception scenarios and encourage cautious applications with human involvement. REPRODUCIBILITY STATEMENT We ensure the reproducibility of all the results in the paper. The implementation details are enumerated in Sec. 4.1 and Sec. A. We have released the code at https://github.com/ziqipang/ADDP."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to follow image editing instructions. In CVPR, 2023. Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prioritized training of diffusion models. In CVPR, 2022. Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, and Ahmed Alaa. InstructCV: Instruction-tuned text-to-image diffusion models as vision generalists. In ICLR, 2024. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? The KITTI vision benchmark suite. In CVPR, 2012. Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. InstructDiffusion: generalist modeling interface for vision tasks. arXiv preprint arXiv:2309.03895, 2023. 11 Published as conference paper at ICLR 2025 Hyojun Go, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, and Seungtaek Choi. Addressing negative transfer in diffusion models. In NeurIPS, 2024. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In ICCV, 2023. Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. In NeurIPS, 2024. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv:2204.03458, 2022. Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. JMLR, 2005. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. In NeurIPS, 2022. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In CVPR, 2024. Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: universal training technique of score-based diffusion model for high precision score estimation. In ICML, 2022. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. In NeurIPS, 2024. Diederik Kingma. Adam: method for stochastic optimization. In ICLR, 2014. Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv:2310.03744, 2023a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023b. Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua Tenenbaum. Compositional visual generation with composable diffusion models. In ECCV, 2022. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Published as conference paper at ICLR 2025 Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. In NeurIPS, 2024. Yunze Man, Shuhong Zheng, Zhipeng Bao, Martial Hebert, Liang-Yan Gui, and Yu-Xiong Wang. Lexicon3D: Probing visual foundation models for complex 3D scene understanding. arXiv preprint arXiv:2409.03757, 2024. Varun Nagaraja, Vlad Morariu, and Larry Davis. Modeling context between objects for referring expression understanding. In ECCV, 2016. Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. EmerDiff: Emerging pixel-level semantic knowledge in diffusion models. In ICLR, 2024. Andrew Ng. The batch issue 242. https://www.deeplearning.ai/the-batch/ issue-242/, 2024. Quang Nguyen, Truong Vu, Anh Tran, and Khoi Nguyen. Dataset diffusion: Diffusion-based synthetic data generation for pixel-level semantic segmentation. In NeurIPS, 2024. Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input perturbation reduces exposure bias in diffusion models. In ICML, 2023. Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and Itir Onal Ertugrul. Elucidating the exposure bias in diffusion models. In ICLR, 2024. NaHyeon Park, Kunhee Kim, Song Park, Jung-Woo Ha, and Hyunjung Shim. TADA: Timestepaware data augmentation for diffusion models. NeurIPS Workshop, 2023. Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, and Jun-Yan Zhu. One-step image translation with text-to-image models. arXiv preprint arXiv:2403.12036, 2024. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D diffusion. In ICLR, 2022. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 44(3): 16231637, 2020. Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. HyperSim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In CVPR, 2023. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022. Thomas Schops, Johannes Schonberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. multi-view stereo benchmark with high-resolution images and multi-camera videos. In CVPR, 2017. 13 Published as conference paper at ICLR 2025 Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of CLIP-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu Edit: Precise image editing via recognition and generation tasks. In CVPR, 2024. Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about red circle? visual prompt engineering for vlms. In ICCV, 2023. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from RGBD images. In ECCV, 2012. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. In NeurIPS, 2023. Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola. Learning vision from models rivals learning vision from data. arXiv preprint arXiv:2312.17742, 2023. Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. StableRep: Synthetic images from text-to-image models make strong visual representation learners. In NeurIPS, 2024. Aaron Van Den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NeurIPS, 2017. Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. Kai Wang, Mingjia Shi, Yukun Zhou, Zekai Li, Zhihang Yuan, Yuzhang Shang, Xiaojiang Peng, Hanwang Zhang, and Yang You. closer look at time steps is worthy of triple speed-up for diffusion model training. arXiv preprint arXiv:2405.17403, 2024. Zhaoqing Wang, Yu Lu, Qiang Li, Xunqiang Tao, Yandong Guo, Mingming Gong, and Tongliang Liu. CRIS: Clip-driven referring image segmentation. In CVPR, 2022. Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models. In ICCV, 2023. Zhen Xing, Qi Dai, Zihao Zhang, Hui Zhang, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. VIDarXiv preprint iff: Translating videos via multi-modal instructions with diffusion models. arXiv:2311.18837, 2023. Guangkai Xu, Yongtao Ge, Mingyu Liu, Chengxiang Fan, Kangyang Xie, Zhiyue Zhao, Hao Chen, and Chunhua Shen. Diffusion models trained with large data are transferable visual models. arXiv preprint arXiv:2403.06090, 2024. Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Openvocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, 2023. Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In ECCV, 2016. 14 Published as conference paper at ICLR 2025 Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. In NeurIPS, 2024a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. Wentianding Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jurgen Schmidhuber. Cross-attention makes inference cumbersome in text-to-image diffusion models. arXiv preprint arXiv:2404.02747, 2024b. Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing textto-image diffusion models for visual perception. In ICCV, 2023. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 2019. 15 Published as conference paper at ICLR 2025 IMPLEMENTATION DETAILS A.1 DATASETS AND IMPLEMENTATION DETAILS A.1.1 DEPTH ESTIMATION We strictly follow the setting in Marigold (Ke et al., 2024) for both training and evaluation. Concretely, the model is trained on the virtual depth maps in Hypersim (Roberts et al., 2021) and VirtualKITTI (Cabon et al., 2020) with initialization from stable diffusion 2 (Rombach et al., 2023). The evaluation is conducted in zero-shot style on multiple real-world datasets, including NYUv2 (Silberman et al., 2012), ScanNet (Dai et al., 2017), DIODE (Vasiljevic et al., 2019), KITTI (Geiger et al., 2012), and ETH3D (Schops et al., 2017). The evaluation follows affine-invariant depth evaluation (Ranftl et al., 2020). We adopt the two metrics used in Marigold: Absolute Mean Relative Error (AbsRel) and δ1 accuracy, which measure the overall errors and precision of depth estimation. For both training and evaluation, we keep Marigolds codebase and hyper-parameters identical. Most importantly, our diffusion model is trained with 1000 steps of DDPM (Ho et al., 2020) and denoised with 50 steps of DDIM (Song et al., 2020). Our improvement in learning objectives and training data are described later in Sec. A.2 (contribution-aware timestep sampling) and Sec. A.3 (diffusion-tailored data augmentation). A.1.2 REFERRING IMAGE SEGMENTATION Datasets and Evaluation. We follow the standard practice of separately training models on RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), and G-Ref (Nagaraja et al., 2016) (UMD split), which are created from the MSCOCO dataset (Lin et al., 2014), then evaluate on their validation and test sets. We adopt the standard metric of overall intersection over union (oIoU). This metric accumulates the intersection and unions across the whole dataset and emphasizes larger objects. Before evaluating, we resize our prediction and ground truth to the resolution of 512 512 following previous works (Wang et al., 2022). Training and Inference. Our baseline is InstructPix2Pix (Brooks et al., 2023), taking the referring prompts as the text conditioning and input image as the image conditioning. Our diffusion model is trained with 1000 steps of DDPM (Ho et al., 2020) and denoised with 100 steps of DDIM (Song et al., 2020). During training, we will resize all the images to the resolution of 256 256, optimizing with the AdamW optimizer (Kingma, 2014; Loshchilov, 2017), batch size of 128, learning rate of 104, and cosine annealing scheduler (Loshchilov & Hutter, 2016). The classifier-free guidance (Ho & Salimans, 2022) weights are manually tuned on the RefCOCO validation set to guarantee optimal performance, which is 1.5 for image conditioning and 7.5 for text conditioning in the InstructPix2Pix model and 1.5 for image conditioning and 3.0 for text conditioning in our enhanced model. The models presented in Table 2 are trained with 60 epochs, where each epoch indicates enumerating each image once. The detailed configurations for our proposed insights (Sec. 3) are described in Sec. A.2 (contribution-aware timestep sampling), Sec. A.3 (diffusion-tailored data augmentation), and Sec. A.4 (correctional guidance). Evaluation Post-processing. During the evaluation of diffusion for perception, we convert the edited images with red masks into binary masks for IoU computation. Since the generated images might not have perfect red color with RGB values of (255, 0, 0), we apply color threshold δc to convert the image to mask. Mi, = (cid:26)0, 1, Ii, (255, 0, 0)2 > δc Ii, (255, 0, 0)2 δc (A) Ii, and Mi, denote the RGB value of the image and mask value at pixel (I, j). This equation intuitively means that we recognize pixel as masked region if its color is close enough (within δc) to the perfect red color. In our experiments on the validation set of RefCOCO (Yu et al., 2016), we find that δc =50 is consistently reasonable threshold. Finally, we clarify that this protocol is only for the convenience of evaluation and is not our major concern. A.1.3 GENERALIST INSTRUCTCV For the generalist model, we primarily follow the experimental setup of InstructCV (Gan et al., 2024), which adopts InstructPix2Pix (Brooks et al., 2023) to conduct perception in the form of image editing. Although the original InstructCV also conducts image classification, its format of turning the whole image red if the image matches certain class is less intuitive, and we find that including image classification causes large variances in the remaining three tasks. Therefore, we focus on depth estimation, semantic segmentation, and object detection. Specifically, we use NYUv2 (Silberman et al., 2012) for depth estimation, ADE20K (Zhou et al., 2017; 2019) for se16 Published as conference paper at ICLR 2025 mantic segmentation, and COCO (Lin et al., 2014) for object detection. We mainly focus on the contribution-aware timestep sampling for Table 4, with details in Sec. A.2. We empirically reweight the ratio of training samples from each dataset to 0.3, 0.3, and 0.4, respectively. The model is trained for 20 epochs, with 100k iterations per epoch. A.2 LEARNING OBJECTIVES: CONTRIBUTION-AWARE TIMESTEP SAMPLING We mainly describe how we estimate and use the c2 diffusion models. The list of tasks are in Sec. 2. defined in Sec. 3.1 to improve the training of VALUES A.2.1 ESTIMATING c2 Protocols. To avoid variance of timesteps, we merge the 1000 steps from DDPM (Ho et al., 2020) into 10 groups, where the 100 timesteps in group share the same c2 . (1) For the depth estimation using Marigold (Ke et al., 2024), we infer pre-trained Marigold model on NYUv2 (Silberman et al., 2012) (654 samples) to get the results at intermediate denoising steps. The metric of RMSE is used. (2) For RIS task, we use = 1, 000 validation samples from RefCOCO (Yu et al., 2016) to estimate the results with IoU. The weights from SD1.5 are directly adopted in SD2.0 and SDXL. (3) For generalist perception with InstructCV (Gan et al., 2024), we use = 500 validation samples each from NYUv2 (Silberman et al., 2012), ADE20K (Zhou et al., 2017; 2019), and COCO (Lin et al., 2014). We estimate c2 value as the shared sampling weight. Estimated c2 In the RIS experiments, we discover that = 0 has an extremely low probability, while the earlier ones closer to =T have large weights. However, we empirically round up their probabilities to 1% to avoid any timesteps from insufficient training. comparison of the probabilities between diffusion-based formulation and empirical estimation from perception statistics (RIS and depth estimation) is in Fig. A. As clearly illustrated, (1) the weights estimated from IoU statistics are more unevenly distributed than diffusion weights, indicating the importance of the first few steps for multi-modal understanding. (2) The weights estimated for Marigold and generalist perception are smoother than RIS, which validates that the evolution of perception quality is unique property for each diffusion-based perception model. for each task separately and adopt the average c2 Values. Figure A: Comparison of the sampling weights derived from normalized c2 , estimated by diffusion formulation or RIS/Marigold/Generalist perception statistics (Sec. 3.1). The dashed blue line denotes the probability of uniform sampling from one of the ten timestamp groups. A.2.2 ALGORITHMS Algorithm demonstrates the process for estimating the contribution factors c2 , and Algorithm illustrates how the contribution factors are utilized during diffusion training via timestamp sampling. A.3 DIFFUSION-TAILORED DATA AUGMENTATION We introduce the detailed implementation of diffusion-tailored data augmentation (Sec. 3.2), which aims at simulating the training-denoising distribution shift during the training time. 17 Published as conference paper at ICLR 2025 Algorithm t Estimation Require: DDPM-trained Diffusion-based Perception Model, Perception Quality Metric Q(). 1: Evaluate on samples with intermediate steps, and obtain = {Qt,i}tT,iN 2: {QT +1,i = 0}iN 3: Perform linear regression on Q0,: = β + βT +1QT +1,:, and compute (R2)T +1 4: for = T, 1, . . . , 1 do 5: 6: 7: 8: end for 9: return (c2 Perform linear regression on Q0,: = β + Compute (R2)t (R2)t+1 (R2)t c2 Q0,:: the final result how well Qi,0 is explained by {Qi,s}tsT Dummy initializing s=t βsQs,: )T t=1 Algorithm Contribution-aware Timestep Sampling Require: Contribution factors (c2 1: repeat x0 q(x0), ε (0, I) 2: Multinomial(c2 1, ..., c2 3: Take gradient descent step on θ ε εθ ( 4: 5: until converge or reach the maximum iteration , ..., c2 ) )T t= αt x0 + 1 αt ε,t)2 A.3.1 DESIGNS AND IMPLEMENTATIONS Depth Estimation. In the experiments of using Marigold (Ke et al., 2024) for depth estimation, we use Gaussian blur as data augmentation to simulate the rough prediction results at the early denoising steps. Concretely, our Gaussian blur has kernel size of 31 pixels with the maximum intensity of 10 t/T , where is the current timestep, and =1000 is the maximum timestep in DDPM. An intuitive illustration is Fig. 4a. RIS. Considering the potential drift of predictions that can occur during the denoising steps, we apply augmentation on three different aspects during training: color, location, and shape. For color changes, we randomly adjust the color masks brightness, contrast, and saturation with the maximum intensity of 0.2 t/T . For location changes, we randomly rotate, translate, or scale the mask region. The maximum rotation is 10 degrees, the maximum translation is 0.05 of the image scale, and the scale changes is confined between 0.95 and 1.05 times. For erasing, we randomly crop out parts of the mask with scale between 0.01 and 0.05. The intuitive demonstration is in Fig. 4b. We acknowledge that our design of augmentation might not be the optimal one. However, we mainly aim to simulate the imperfect intermediate denoising results and illustrate the broad space of data augmentation for diffusion-based perception. A.3.2 PREDICTION TARGET αt x0 ε = xt During the training of diffusion models, it is common practice to adopt Gaussian noise ε as the objective. However, the integration of data augmentation introduces additional complexity, thereby altering the gradient direction of p(x). To account for the impact of data augmentation, we redefine the training objective as αt 1 αt Despite this adjustment, empirical results indicate that predicting ε is still less effective than predicting x0. potential explanation for this observation is that correcting the learning target introduces additional discrepancies between the training and denoising phases. In contrast, predicting x0 helps mitigate this issue by directly predicting the final outputs, thereby enhancing model consistency across different phases. The ablation study is presented in Sec. B.3. In addition, we incorporate classifier-free guidance (Ho & Salimans, 2022) when generating x0. In accordance with the formulation in InstructPix2Pix (Brooks et al., 2023), we set wD = 3.0 and wI = 1.5. (Augment(x0,t) x0) . 1 αt = ε + (B) 18 Published as conference paper at ICLR 2025 A.4 INTERACTIVITY WITH CORRECTIONAL PROMPTS This section provides the detailed steps of how we convert the generative denoising process into interactive user interfaces. The details of our evaluation on RIS with an agentic workflow is also discussed. A.4.1 FORMULATION OF CORRECTIONAL PROMPTS We justify the formulation of our correctional prompts (Sec. 3.3) following the derivation in InstructPix2Pix (Brooks et al., 2023). We treat the correctional prompt as an auxiliary condition alongside the image and referring D, so the objective conditional probability for denoising is: P(xD, D, I) = P(x, D, D, I) P(D, D, I) = P(D, D, Ix)P(D, Ix)P(Ix)P(x) P(D, D, I) By taking the logarithm and derivative of the conditional probability above, we have the score function (Hyvarinen & Dayan, 2005) as below, corresponding to our correctional guidance in Eqn. 5. log P(xD, D, I) =x log P(x) + log P(Ix) log P(D, Ix) + log P(D, D, Ix). (C) (D) A.4.2 PROMPTS AND WORKFLOW When building the agentic workflow in Sec. 3.3, we leverage the following three steps as depicted in Fig. B, where our correctional guidance votes the masks for right predictions. Please note that the motivation of our workflow is to simulate the rough thinking process of how human interacts and correct the predictions of referring segmentation. (1) LLaVA Captioning. We first prompt LLaVA (Liu et al., 2023a;b) to create detailed caption of each image, specifically the llava v1.6 vicuna 13b, with the prompt showing in Fig. B. The special consideration of our prompt is to pay attention to both foreground and background objects, which both frequently appear in the referring segmentation dataset. After this step, we can input the image descriptions for foundation models to reason. (2) Correctional Guidance Generation with GPT. Then we prompt GPT4 (Achiam et al., 2023) to analyze the captions and referring expressions to name the confusing objects for referring segmentation, which act as the correctional prompts. We acknowledge that this is not an ideal agentic workflow but is computationally feasible within our budget since each validation set of RefCOCO (Yu et al., 2016) has around 10,000 referring phrases. Specifically, we use gpt 4o 2024 05 13 to conduct the prompts in the middle of Fig. 5. We explicitly use examples to guide GPT into reasoning the confounding objects for referring segmentation while providing the arguments. The output of the GPT will contain three confounding objects, namely the correctional prompts. (3) Referring Mask Prediction. We independently predict the referring masks for each of the k=3 correctional prompts, applying guidance weights of wI = 1.5, = 2.0, and wD = 3.0 in accordance with Eqn. 5. The final mask is derived from pixel-wise majority voting across the = 3 masks, as depicted in Fig. B. Figure B: Our workflow of generating correctional prompts shows the advantage of the interactivity of diffusion-based perception. 19 Published as conference paper at ICLR"
        },
        {
            "title": "B ADDITIONAL ABLATION STUDIES",
            "content": "B.1 REFERRING IMAGE SEGMENTATION MASK FORMAT As described in Sec. 2, we format referring image segmentation (RIS) as an image editing problem: painting the regions of target objects with solid red masks. This section analyzes why we chose red masks in our experiments. Figure C: Comparison of mask encoding methods for RIS. Note that ground-truth masks are used here for demonstration purposes. Figure D: Comparison of post-processing. The solid red mask simplifies the post-processing for evaluation, which also encourages us to adopt it as the RIS format. Solid or Transparent Masks. Previous studies that have adopted different formats: InstructDiffusion (Geng et al., 2023) utilizes transparent masks, InstructCV (Gan et al., 2024) employs binary masks with RGB channels, while our strategy is solid red mask, as visually compared in Fig. C. When trained with InstructPix2Pix (Brooks et al., 2023) for 20 epochs, the use of binary masks results in sub-optimal result, achieving an oIoU of 28.18%, significantly lower than the 56.42% obtained by red masks. This performance gap is likely attributable to the lack of contextual information in the generated binary masks, which makes the grounding harder. As presented in Table 2, our red masks (InstructPix2Pix) achieve performance comparable to transparent masks (InstructDiffusion) with fewer training iterations, thereby underscoring the effectiveness of red masks. In addition, red masks facilitate simpler post-processing via straightforward pixel-wise thresholding, whereas applying thresholding to transparent masks produces excessively noisy results, as shown in Fig. D. Although incorporating an additional U-Net (Ronneberger et al., 2015) for segmentation post-processing might enhance the results (Geng et al., 2023), this introduces additional complexity to the evaluation process. Therefore, given the effectiveness and efficiency, we choose to use red masks in our experiments. Color of Masks. We additionally analyze the influence of colors on the RIS performance. These experiments train the baseline InstructPix2Pix and our enhancements with 40 epochs on RefCOCO. As shown in Table A, our insights consistently improve RIS for all the scenarios. The red masks perform the best, so we chose it as our default setting. We hypothesize that the influence of color is similar to the discovery in visual prompt tuning (Shtedritski et al., 2023): distribution of images affect models performance under different colors of visual prompts. Red Blue Green InstructPix2Pix +Sampling+Aug 60.15 66.51 Table A: Comparison of mask colors for RIS. 53.54 65.72 53.88 65.87 Published as conference paper at ICLR 2025 INTERMEDIATE DENOISING RESULTS B.2 As discussed in Sec. 4.3, with contribution-aware timestep sampling and diffusion-tailored data augmentation, we observe less pronounced decrease in IoU. Since the final outputs of diffusion models are typically obtained at timestep =0, we base our main results on timestep =0 for fair comparisons in Table 2. However, as indicated by Fig. 7, the IoU at timestep = 200 is the highest among all timesteps. Hence, we also report results from this timestep in Table B, which show slightly better predictions than those at timestep = 0. Further bridging this gap indicates better alignment between the denoising process and perception objective, and will be the future work. Timestamp RefCOCO RefCOCO+ G-Ref val test-A test-B val test-A test-B val test = 0 = 200 66.86 67.34 67.39 68.36 63.72 64.44 55.35 56. 58.72 59.87 48.45 49.06 55.85 57.41 57.05 57.90 Table B: Comparison on the oIoU of RIS at intermediate denoising steps. The SD1.5-based RIS models from Table 2 are used for this comparison. B.3 TRAINING DATA: DATA AUGMENTATION FOR DIFFUSION MODELS x0-prediction v.s. ε-prediction. As mentioned in Sec. A.3.2, using corrupted input as data augmentation (Sec. 3.2) is sensitive to the prediction target of the scheduler in diffusion models. For example, the default ε-prediction in DDPM (Ho et al., 2020) might lead to problematic score functions when the input is not sampled from the ground truth trajectories, while x0-prediction (Ramesh et al., 2022) or velocity prediction (Salimans & Ho, 2022). Here, the experiments are all conducted with the data augmentation proposed in Sec. 3.2. ε-prediction refers to predicting the corrected noise ε , as defined in Eqn. B. Directly fitting Gaussian noise ε does not produce reasonable results and is not included in the table. The results indicate that x0-prediction yields better performance. Furthermore, the results from InstructPix2Pix demonstrate that switching to x0-prediction alone does not enhance performance, indicating that the improvements in InstructPix2Mask are from data augmentation rather than merely switching to x0-prediction. Timestamp-dependent Data Augmentation. In Table D, we investigate the impact of varying the intensity of data augmentation across timesteps. We compare the linearly increasing intensity with the constant intensity. The results indicate that the dynamic strategy outperforms the static one. This improvement is likely due to the fact that timestamp-dependent augmentation aligns more closely with the original diffusion formulation by introducing less noise as approaches 0, since our goal is to simulate the distribution shift. ε-prediction Intensity oIoU InstructPix2Pix + ADDP 56.42 59.64 Table C: Analysis on Training objectives. x0-prediction 53.39 66. constant linear 66.07 66.19 Table D: Augmentation Scales. Analysis of Individual Augmentations We compare the three types of augmentation, color, shape, and location corruptions, to the ground truth masks proposed in Sec. 3.2. As shown in Table E, we discovered that the augmentations have different effectiveness in enhancing diffusion-based perception models. Color Shape Location oIoU ADDP (w/o Aug) 64.0 Color Only Shape Only Location Only ADDP (All Augs) Table E: Comparison of augmentation types for RIS. 64.2 64.5 65.9 66. B.4 ADDITIONAL ANALYSIS ON MARIGOLD-BASED DEPTH ESTIMATION In addition to the analysis in Table 1, we analyze the separate effects of contribution-aware timestep sampling (Sec. 3.1) and diffusion-tailored data augmentation (Sec. 3.2) for Marigold-based depth 21 Published as conference paper at ICLR 2025 estimation. As shown in Table F, our proposed techniques from ADDP both improve Marigold step by step. Notably, our data augmentation enhances Marigold even though its perception quality  (Fig. 3)  does not show significant drops as RIS, indicating the vast existence of distribution shifts. This supports the effectiveness of our proposed techniques in addition to Sec. 4.3: both our contribution-aware timestep sampling (Sec. 3.1) and diffusion-tailored data augmentation (Sec. 3.2) are beneficial for diffusion-based perception. Method Marigold (Ke et al., 2024) +Sampling (Ours) +Sampling+Aug (Ours) ETH3D ScanNet NYUv2 Diode AbsRel 7.1 6.3 6.3 δ1 AbsRel 95.1 96.1 96.1 6.9 6.4 6. δ1 AbsRel 94.5 95.3 95.6 6.0 5.7 5.6 δ1 AbsRel 95.9 96.1 96.3 31.0 30.6 29.6 KITTI δ1 AbsRel 77.2 77.2 77. 10.5 10.3 10.0 δ1 AbsRel 90.4 90.4 90.6 12.3 11.9 11.6 Average δ1 90.6 91.0 91.2 Rank 2.9 2.0 1. Table F: Ablation analysis on the effects of contribution-aware timestep sampling (Sampling) and diffusion-tailored data augmentation (Aug) for Marigold. Both of them contribute positively to diffusion-based perception. B.5 COMPARISON WITH SAMPLING WEIGHTS FROM GENERATIVE STUDIES Our ADDP leverages the ct estimated from the perception statistics to control the sampling of timesteps. Studies on diffusion models designed for generative tasks also notice the benefits of better loss scaling or probability scaling of timesteps. The comparison between our ADDP and their weights is in Table G. For fair comparison, we scale the probability of timestep sampling for all the methods and disable our data augmentations. Uniform (DDPM (Ho et al., 2020)) Estimated ct (Ours) P2 (Choi et al., 2022) Min-SNR (Hang et al., 2023) MLT (Go et al., 2024) SpeedD (Wang et al., 2024) Soft Truncation (Kim et al., 2022) Method oIoU 56.42 64. 57.90 26.13 55.60 57.89 57.49 Such an improvement primarily arises from better alignment between our weights estimated from the perception statistics with the perception tasks, whereas the others are mainly considered from generative task perspective. This demonstrates the effectiveness and necessity of our investigation on diffusion-based perception instead of fully relying on the results from generative tasks as guidance. Table G: Comparison with Sampling Weights from Generative Models Studies. B.6 COMPARISON WITH AUGMENTATION STRATEGIES FROM GENERATIVE STUDIES Method The studies of generative models also notice the distribution shift between training and denoising, termed as exposure bias (Ning et al., 2024). Therefore, we compare the augmentation strategies with our method under diffusion-based perception setting. Since TADA (Park et al., 2023) primarily focuses on the intensity of data augmentation, we replace our linear intensity schedule with that from TADA while keeping the identical set of augmentations: color, shape, and location. As shown in Table H, our method outperforms these related approaches from generative task perspective, indicating the uniqueness of our diffusion-based perception investigation. Table H: Comparison with Augmentation Strategies from Generative Models Studies. TADA (Park et al., 2023) Epsilon Scaling (Ning et al., 2024) w/o Augmentation w/ Augmentation (Ours) 65.76 64.13 64.00 66.19 oIoU B.7 INFLUENCE OF SAMPLING STEPS ON OUR OBSERVATIONS When observing the behaviors of diffusion models for perception tasks  (Fig. 1)  , we primarily follow the setting of InstructPix2Pix (Brooks et al., 2023) and adopt 100 sampling steps. However, the number of sampling steps might influence the properties of diffusion model. We analyze the number of sampling steps to check our observation. Specifically, we let an InstructPix2Pix baseline model trained on RefCOCO generate intermediate referring segmentation results at different sampling steps: 50, 100, and 200. The evaluation of IoU regarding timesteps is in Fig. E. As shown in Fig. E, our observations are consistent across different sampling steps: (1) the contributions across timesteps are significantly uneven, with earlier steps contributing more than the later ones; (2) the distribution shift between training and denoising causes the performance drop at later 22 Published as conference paper at ICLR Figure E: Observations with different sampling steps. denoising steps. When using fewer sampling steps, only the earlier steps suffer from insufficient diffusion denoising, but the performance quickly catches up after = 900. These combined verified that our observed issues of diffusion-based perception are consistent."
        }
    ],
    "affiliations": [
        "University of Illinois Urbana-Champaign"
    ]
}