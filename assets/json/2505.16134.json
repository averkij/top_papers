{
    "paper_title": "Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models",
    "authors": [
        "Menschikov Mikhail",
        "Alexander Kharitonov",
        "Maiia Kotyga",
        "Vadim Porvatov",
        "Anna Zhukovskaya",
        "David Kagramanyan",
        "Egor Shvetsov",
        "Evgeny Burnaev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models exhibit positional bias -- systematic neglect of information at specific context positions -- yet its interplay with linguistic diversity remains poorly understood. We present a cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi, Vietnamese), examining how positional bias interacts with model uncertainty, syntax, and prompting. Key findings: (1) Positional bias is model-driven, with language-specific variations -- Qwen2.5-7B favors late positions, challenging assumptions of early-token bias; (2) Explicit positional guidance (e.g., correct context is at position X) reduces accuracy across languages, undermining prompt-engineering practices; (3) Aligning context with positional bias increases entropy, yet minimal entropy does not predict accuracy. (4) We further uncover that LLMs differently impose dominant word order in free-word-order languages like Hindi."
        },
        {
            "title": "Start",
            "content": "Position of Uncertainty: Cross-Linguistic Study of position bias in Large Language Models Menschikov Mikhail,1,2, Alexander Kharitonov,3, Maiia Kotyga4, Vadim Porvatov2,3, Anna Zhukovskaya5, David Kagramanyan6, Egor Shvetsov2, and Evgeny Burnaev2,7 these authors contributed equally to this work 1ITMO, Saint-Petersburg, Russia, 2Skoltech, Moscow, Russia, 3Sber, Moscow, Russia, 4MIPT, Moscow, Russia, 5Lomonosov MSU, Moscow, Russia, 6HSE University, Moscow, Russia, 7AIRI, Moscow, Russia Corresponding author: m.menschikov@skoltech.ru Identifies project curator - 5 2 0 2 2 2 ] . [ 1 4 3 1 6 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models exhibit position bias systematic neglect of information at specific context positionsyet its interplay with linguistic diversity remains poorly understood. We present cross-linguistic study across five typologically distinct languages (English, Russian, German, Hindi, Vietnamese), examining how position bias interacts with model uncertainty, syntax, and prompting. Key findings of this paper are the following: (1) position bias is model-driven, with languagespecific variations Qwen2.5-7B-Instruct favors late positions, challenging assumptions of early-token bias; (2) Explicit relevance information (e.g., context is relevant to given query) reduces accuracy across languages, undermining prompt-engineering practices; (3) Aligning context with position bias increases entropy, yet minimal entropy does not fully correspond to high accuracy. (4) Position bias does not provoke violation of language-specific word orders when it comes to languages that are typologically different from English. The code for our experiments is available here: link."
        },
        {
            "title": "Introduction",
            "content": "Many recent large language models (LLM)-based applications require support for long context1. However, developing new training strategies for handling longer contexts is not enough, as new problems emerge. One such problem is position bias systematic neglect of information at specific positions, usually in the middle [Baker et al., 2024]. While position bias is well-documented in Englishcentric studies [Zhang et al., 2024a, Baker et al., 2024], its behavior in multilingual settings remains underexplored and most developed bias mitigation strategies [Peysakhovich and Lerer, 2023, Zhao et al., 2021, Zhang et al., 2024b, Guo and Vosoughi, 2024] were 1Such applications may include Retrieval-Augmented Generation, Autonomous Agents, Scientific Research, Customer Support, etc. evaluated solely on English datasets. However, as we discuss in Section 2, lexical and morphological variations across languages must be considered for multilingual models. Therefore, as preliminary step for further adaptation of multilingual LLMs for applications with long context, we aim to answer the following questions in this work: Q1: Is position bias primarily model-driven phenomenon, or do language-specific patterns emerge due to lexical, morphological, and syntactic differences [Ghosh et al., 2024a]? Q2: Does position bias cause models to favor Englishlike SVO (Subject, Verb, Object) word order when generating text in languages with other dominant word orders? Q3: Do prompt-based strategies (e.g., explicit position guidance [Zhang et al., 2024b]) effectively mitigate bias across languages? Can we focus model attention via prompting, and would it improve model performance? Q4: Finally, we are interested in deeper understanding of how position bias affects model generation, and thus we perform formal and empirical analysis of how position bias affects entropy of the output distribution. Contribution. We conduct multilingual analysis of position bias across five typologically diverse languages (English, German, Russian, Hindi, Vietnamese), revealing: Position bias is predominantly model-driven but exhibits language-specific variations. We observe stronger late-position bias in Qwen2.57B-Instruct, contradicting prior claims of inherent early-token preference [Wu et al., 2025, Barbero et al., 2025]. Explicitly instructing models of correct context placement (e.g. correct context has label 1 ) consistently degrades accuracy across all languages, challenging prompt-engineering practices [Zhang et al., 2024b]. Formal and empirical analysis of entropy reveals counterintuitive dynamics: when placement of correct context is aligned with position bias, entropy often increases, demonstrating higher model uncertainty. While we do not find any connection between position bias and word order, we observe some modeldriven patterns. Figure 1: Experiment Structure. For each question, the correct answer is placed in one of three positions: top , middle , or bottom of the context. Each context is assigned binary score (0 or 1), indicating its relevance to the question. Three scoring scenarios are evaluated: Aligned : The relevant context is assigned score of 1 and all other assigned as 0. All Zero : All contexts, including the relevant one, are assigned score of 0. No Scores : Relevance scores are omitted entirely. This design evaluates how scoring mechanisms and answer positioning influence model performance across varying levels of contextual guidance. These findings yield following practical implications: (1) Since bias is predominantly model-driven, existing English-optimized mitigation methods would probably work with other languages but require language-specific adjustments; (2) Some Chain-of-Thought strategies, which rely on positional guidance, demand careful consideration, as we observed constant model degradation when positional instructions are provided; (3) Retrievalaugmented generation (RAG) systems, which adapt reordering or relevancy scores strategies (discussed in Section 2), need to account for language and model characteristics; these strategies usually assume that the model attends more to first (recent) tokens, which is not always true. (4) Entropy dynamics complicates uncertaintybased bias mitigation approaches [Duan et al., 2024] and may be important for uncertainty quantification strategies."
        },
        {
            "title": "The code for our experiments is available in the",
            "content": "GitHub repository."
        },
        {
            "title": "2 Related Work",
            "content": "What causes position bias? Prior work identifies multiple contributing factors. [Zhang et al., 2024a] attribute position bias to U-shaped attention patterns in transformers, which prioritize extremal positions and degrade performance for mid-context evidence. Theoretical and empirical studies further demonstrate that transformers attention is inherently biased toward earlier tokens [Wu et al., 2025, Barbero et al., 2025]. [Wu et al., 2025] explains position bias due to \"causal masking inherently biases attention toward earlier positions, as tokens in deeper layers attend to increasingly contextualized representations of earlier tokens\". Our results reveal exceptions where later positions are desired, highlighting the complexity of the problem. Training data biases, such as serial-position effects in corpora, shape how models prioritize sequence positions [Wu et al., 2025, Guo and Vosoughi, 2024]. Interplay of Culture, Language, and Model Design. The way we perceive the world is influenced not only by our culture [Oyserman and Lee, 2008] but also by the language we speak [Boroditsky et al., 2003]. The latter point is particularly relevant for LLMs, since they are trained on specific languages. Recent studies have shown that multilingual LLMs often initiate their \"thinking process\" in English, pivoting to the prompts original language in the middle layers [Zhang et al., 2024a, Peysakhovich and Lerer, 2023]. These models exhibit lower lexical naturalness in non-English languages, with the naturalness gap being more pronounced for languages structurally distant from English [Guo et al., 2024]. While the volume of training data plays crucial role [Arnett and Bergen, 2025], linguistic complexity including lexical and morphological variations across languages must also be considered [Ghosh et al., 2024a, Dang et al., 2024, Ismayilzada et al., 2025]. Additionally, architectural design choices affect languages in different ways; for instance, removing positional encoding from language models would most degrade performance in languages with limited morphological systems [Ghosh et al., 2024a]2. 2This raises an important question: Has the development of LLM architectures over the last decade been primarily geared toward English? As noted, \"English-centric practices in NLP may have impeded progress for language models in other At the same time, most of bias mitigation approaches evaluate their performance in English [Zhang et al., 2024a, Peysakhovich and Lerer, 2023, Zhang et al., 2024b, Yu et al., 2024a, Wang et al., 2024a]. These approaches fall into two categories: prompt-based techniques and architectural interventions. Architectural methodssuch as positional encodings, alternative masking schemes, and calibration mechanisms address root causes but often require retraining and introduce computational overhead [Zhang et al., 2024a, Wu et al., 2025, Zhao et al., 2021]. Prompt-based strategies, including query-aware contextualization and recency prompting, aim to redirect attention dynamically [Peysakhovich and Lerer, 2023, Wang et al., 2024a, Yu et al., 2024a]. We focus on prompt-based strategies, using as starting point the work done by [Zhang et al., 2024b]. The authors studied whether model can improve its performance when given explicit placement of the correct answer. They used two types of instructions relative and absolute and found that models lack relative awareness and that implicit information about absolute placement of the correct prompt marginally improves model performance. However, the authors did not study scenario when the placement prompt is absent at all. Practical considerations: position bias affects Chain of Thought Strategies (CoT), CoT struggles with position bias even when reasoning steps are correct, models often fail to retrieve evidence from middle positions [Zhang et al., 2024a]. In [Zhang et al., 2024a, Yu et al., 2024a], authors analyzed error propagation in multi-hop reasoning. In RAG systems, one of the mitigating strategies is context ordering [Wang et al., 2024b, Alessio et al., 2024, Jin et al., 2024] (we discuss these approaches in Appendix A). While conventional approaches often assume monotonic relationship between document position and attention (e.g., prioritizing the first/last positions), our analysis reveals that position bias may exhibit language-specific patterns and is not always maximized at early tokens. This observation challenges assumptions in methods like Long-Context RAG, which rely on fixed position prioritization, and highlights the need for language-adaptive reordering. While predictive entropy is widely used to quantify model uncertainty [Huang et al., 2024, Sychev et al., 2025], its relationship with position bias remains un- [Duan et al., 2024] note that uncertainty explored. estimates can be token-biased, but how position bias interacts with uncertainty dynamics is unclear."
        },
        {
            "title": "3.1 Position bias formalization",
            "content": "To evaluate the effect of position bias, we consider the question answering task. For each question we have the ground truth answer and number of contexts ctx1, . . . , ctxN . In our experiments, we always have one relevant context for each question, and all other 1 contexts are chosen randomly from the dataset. The response of the odel with rompt function to the when the relevant context is placed on the position is denoted as Ri = odel(P rompt(Q, ctx1, . . . , ctxN , i)). We say, that model exhibits position bias on dataset towards position in contrast to position if the expected Accuracy for is higher than for j: (Q,A)D Acc(A, Ri) > (Q,A)D Acc(A, Rj)"
        },
        {
            "title": "3.2 Context Placement Strategies",
            "content": "This experimental series investigates whether explicit information about the relevance of contexts to given query can enhance performance. We examine practical scenario where context relevance can be quantified (e.g., via cosine similarity between context and query embeddings) and evaluate how integrating relevance scores into prompts affects context selection. The experimental framework is illustrated in Figure 1. Scoring Configurations: Aligned: Relevant context assigned score rs = 1 and all other contexts assigned as rs = 0. All Zero: All contexts (including relevant) assigned score rs = 0. We hypothesize that this intentional mislabeling will degrade model performance if scores influence context selection. No Scores: Relevance scores omitted entirely from prompts - we want to evaluate whether explicit relevance scores help model at all. According to the experiment, relevance scores rs1, . . . , rsN were included in the prompt. As depicted in Figure 1 we consider three positions of relevant contexts TOP(first), MIDDLE(N /2) or BOTTOM(last). Context Volume Analysis: Prior research demonstrates increased position bias with longer contexts [Baker et al., 2024, Peysakhovich and Lerer, 2023]. To assess whether relevance scores mitigate this effect, we vary context quantity {5, 10, 15}. This design enables systematic evaluation of score efficacy across different information loads."
        },
        {
            "title": "3.3 Average Predictive Entropy",
            "content": "We adopt the token-wise entropy framework from [Lu et al., 2022], normalized by the total number of tokens to quantify uncertainty in model responses [Lu et al., 2022, Wang et al., 2025]. Let represent the input prompt and = {z1, z2, ..., zn} denote generated completion sequence of tokens. For given LLM, the conditional probability of generating the i-th token zi, given the preceding tokens s<i = {z1, ..., zi1} and the prompt x, is denoted as p(zi s<i, x) for 1 n. The average predictive entropy3 (denoted PEavg) is defined as: languages\" [Arnett and Bergen, 2025]. While this issue is important, it falls outside the scope of this paper 3For brevity, we refer to this value as entropy in our work log p(s x) PEavg(s, x) = 1 1 = i=1 log p(zi s<i, x). (1) This formulation computes the average uncertainty per token by decomposing the joint probability p(s x) into product of conditional probabilities via the chain rule. The normalization by ensures comparability across sequences of varying lengths, aligning with interpretations of entropy as an \"average uncertainty\" measure."
        },
        {
            "title": "4 Effect of position bias on Entropy",
            "content": "This section formalizes how position bias toward the first token in transformer-based LLMs propagates across layers and analyzes its impact on attention entropy. We assume standard multi-head self-attention architecture and derive conditions under which position bias homogenizes token representations, ultimately increasing entropy in the final layer. Notation: Let (0) = [x1, x2, . . . , xn] Rdn denote the input token embeddings at layer 0, where x1 is the first token and is the embedding dimension. At each layer 1, the self-attention operation computes: A(t) = softmax (X (l)W (l) (X (l)W (t) ) ) (2) A(l)X (l)W (l) (t+1) = (l) , (l) (3) Rdd are learnable where (l) , (l) projection matrices, A(l) Rnn contains the attention dQK = 1 for simplicity. weights and , (l) Assumptions: To isolate the effect of position bias, we make simplifying assumptions: (1) dominant first token attention, (2) position bias does not change over layers and (3) attention can be represented as linear combination of contextual attention Acont and positional attention Apos. For all layers 1, the positional attention weights are sharply concentrated on the first token: pos(l) i,j pos(l) i,j { 1 0 if = 1, otherwise. Or in vector form Apos(l) 1 e1. Linear combination of attention: = λ1 Acont + λ2 Apos, λ1 + λ2 = 1; λ1, λ2 [0; 1] where λ are normalizing weights for each attention type. Token Homogenization: Under these assumptions, the hidden state of token at layer becomes: Where (l) = (l) (l) . If λ2 > λ1 recursively applying this across layers yields: x(0) x(L) 1 i, (5) implying all tokens collapse to copy of the initial first token embedding x(0) (up to projection transfor1 mations). Although token collapse would not happen in real scenario, for example, due to residual connections, tokens may become more similar to tokens under position bias. 1 con(l) i,j x(0) Entropy Dynamics: Let (l) denote the general attention entropy at layer l. As tokens homogenize (x(l) 1 ), queries and keys become indistinguishable forcing contextual attention weights to unifori, j, which maximizes entropy: mity: (l) log n. Aligned positional and contextual attention: In this section, under many modelling assumptions, we demonstrate connection between entropy, attention and position bias. Finally, if Acon and Apos are aligned, homogenization would occur with higher probability. This reveals counterintuitive outcome - when the correct context aligns with the models position bias, the model would give higher attention to those tokens, thus eventually increasing entropy. We further discuss our empirical results in Section 4, where we observe that minimal entropy does not always occur when correct context placement is aligned with position bias. Predictive and Attention Entropy: Attention mechanisms are designed to prioritize relevant tokens. If attention is uniform (high entropy), the model cannot leverage contextual cues effectively. Without focused attention, the model lacks strong signals to predict the next token confidently, leading to less certain outputs (higher predictive entropy).4 Multilingual Caveat: In multilingual LLMs, position bias may shift across layers e.g., early layers prioritize English tokens, later layers pivot to the prompts language [Zhong et al., 2024, Schut et al., 2025], which may have its own positional preferences. While this violates our assumption of static position bias, entropy dynamics remains the same if homogenization occurs."
        },
        {
            "title": "4.1 Word Order Analysis",
            "content": "In this section, we want to investigate connections between relevant context position, models and dominant word order of language. Particularly, we are interested if position bias would amplify the models dominant language word order or de-amplify it. We focus on the No Scores configuration with five contexts and two languages: Hindi and German, due to their nonSVO dominant word order according to The World Atlas of Language Structures. We parse sentences using x(l) (l) [ j=1 (λ1Apos(l) i,j + λ2Acont(l) i,j ) (l) x(l1) = λ2W (l) (l) = λ2P (l)x(l1) e1x(l1) 1 + λ1xcon(l) , i. + λ1xcon(l) ] (4) 4Example:In the sentence \"The capital of France is ***,\" the token \"France\" is critical. If attention is uniform, the model might equally weigh irrelevant tokens (e.g., \"The\" or \"of\"), failing to emphasize \"France.\" This ambiguity increases uncertainty in predicting \"Paris.\" Stanza [Qi et al., 2020] and follow similar to [Choi et al., 2021]. We extract every verb and identify its dependents. If verb has both dependent with subj relation (or if such relation is found for the nearest preceding verb connected via conj dependency) and one with an obj relation, we record the word order of these triplets using the abbreviations S, V, and O. We then analyze the distribution of SVO and SOV patterns relative to all extracted triplets."
        },
        {
            "title": "5.1 Datasets",
            "content": "In this paper, we utilized three datasets covering five languages with divergent syntactic structures and semantics distribution; dataset statistics are outlined in Table 1, and full description can be found in Appendix B. Language English Russian German Hindi Vietnamese Source SQuAD 2.0 [Rajpurkar et al., 2018] MTSBerQuAD (link) MLQA [Lewis et al., 2020] MLQA [Lewis et al., 2020] MLQA [Lewis et al., 2020] Size 150k 60k 5k 5k 5k Table 1: Summary of datasets used in this study by languages with associated sources and question-answer pairs. Preprocessing: To manage computational constraints, we limited our analysis to 2,000 QuestionAnswer (QA) pairs per language. The following twostage preprocessing pipeline was applied before sampling to ensure data quality and consistency: (1) Duplicate removal: All duplicate QA pairs were excluded. (2) Answer validation: QA pairs lacking valid answers (e.g., missing or ambiguous responses) were discarded."
        },
        {
            "title": "5.2 Models",
            "content": "To disentangle whether position bias in LLMs arises from model-specific design/training or languagespecific characteristics, we selected two models Qwen2.5 7B Instruct (next Qwen) and Llama3 8B Instruct (next Llama) that share multilingual support (covering English, German, Russian, Hindi, Vietnamese) but differ in architectural and training paradigms. This enables isolating model-driven effects (e.g., positional encoding, training data scope) from language-driven ones, aligning with our finding that position bias is predominantly model-driven [Lu et al., 2022]. We provide side-by-side comparison of the models in Appendix to justify the rationale for their inclusion."
        },
        {
            "title": "6.1 LLM as a Judge",
            "content": "Traditional statistical evaluation metrics such as BLEU [Papineni et al., 2002], ROUGE [Lin, 2004], and Meteor Universal [Denkowski and Lavie, 2014] struggle to distinguish syntactically similar but semantically distinct texts. While semantic methods like BERTScore [Zhang et al., 2019] were introduced to address these limitations, our experiments reveal that BERTScore lacks sufficient differentiability, often failing to capture nuanced distinctions between correct and incorrect answers. Therefore, we adopt the LLM as judge [Zheng et al., 2023] framework and choose Mistral Large5 as our judge for the following reasons: 1) Prior studies demonstrate its robust alignment with human annotators and generalizability across tasks [Bavaresco et al., 2024], [Kim et al., 2024]; 2) It offers free API for research purposes, making it accessible for large-scale evaluation; 3) Its architecture differs from the families of models used for response generation, mitigating bias toward self-generated outputs. The judge evaluates QA pairs using structured prompt containing the question, ground truth and model answer. It labels 1 for correct answers and 0 for incorrect ones, and we use accuracy as our main metric, prompts and details are provided in Appendix E."
        },
        {
            "title": "6.2 Human Evaluation",
            "content": "To validate the reliability of our LLM as judge, we run human annotation on 150 questions for the English and Russian languages. We consider the generations of the Llama model and annotate each response with overlap3. Experts follow the same criteria as the judge. To quantify inter-annotator agreement, we calculate Krippendorffs α [Krippendorff, 2011], achieving mean α = 0.755, indicating high reliability of assessment. We further assess the judges alignment with human annotators by computing the Pearson correlation between the judges scores and the majority vote of human labels. strong mean correlation = 0.716 was observed. Detailed information about the annotation process could be found in Appendix E."
        },
        {
            "title": "7.1 Position bias is mostly driven by models",
            "content": "From Table 2, we observe that Qwen generally achieves the highest performance when the correct context is placed at the BOTTOM, while Llama excels when the correct context is positioned at the TOP across all languages. Notably, Qwen demonstrates superior performance for Vietnamese and Russian in specific scenarios. However, despite its overall stronger average performance, Llama outperforms Qwen in Vietnamese. This discrepancy may stem from an alignment between Vietnamese language-specific position bias and the inherent position bias of Llama. Overall, our findings indicate that while language-specific position biases may exist, they are likely overshadowed by the dominant positional preferences intrinsic to the models themselves. Finally, these findings challenge conventional assumptions about primacy effects in LLMs. 5https://mistral.ai/news/mistral-large-2407 Language Position English Russian German Hindi Vietnamese TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM Mean Aligned 0.943 0.941 0.949 0.938 0.927 0.937 0.648 0.629 0.655 0.591 0.579 0.642 0.722 0.676 0.718 0. Qwen All-Zero No-Scores Mean 0.941 0.952 0.937 0.945 0.951 0.956 0.927 0.934 0.929 0.944 0.936 0.939 0.615 0.644 0.608 0.628 0.636 0.649 0.583 0.619 0.577 0.609 0.626 0.643 0.693 0.713 0.668 0.707 0.689 0.700 0.656 0.927 0.931 0.948 0.909 0.917 0.935 0.553 0.566 0.602 0.540 0.543 0.592 0.644 0.619 0.647 0.589 Aligned 0.955 0.926 0.932 0.928 0.908 0.911 0.680 0.602 0.613 0.532 0.448 0.494 0.764 0.714 0.726 0.619 Llama All-Zero No-Scores Mean 0.945 0.951 0.909 0.946 0.896 0.930 0.890 0.906 0.855 0.888 0.846 0.866 0.628 0.719 0.527 0.668 0.522 0.646 0.523 0.729 0.412 0.676 0.392 0.644 0.727 0.737 0.652 0.707 0.618 0.656 0.687 0.929 0.858 0.826 0.837 0.768 0.761 0.485 0.312 0.307 0.309 0.112 0.038 0.679 0.536 0.473 0.361 Table 2: Analysis of position bias, accuracy is reported for question answering tasks, Position specifies relevant context placement. Aligned, All-Zero and No-Scores specify instruction strategy for contexts as depicted in Figure 1. Highest accuracy is in bold. Language Position Qwen Llama English Russian German Hindi Vietnamese TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM 0.095 0.098 0.096 0.114 0.118 0.115 0.112 0.124 0.123 0.076 0.080 0.081 0.107 0.113 0.107 0.224 0.225 0.228 0.198 0.205 0.209 0.218 0.200 0.199 0.286 0.280 0.280 0.278 0.293 0.303 Table 3: Entropy from Equation 1 for two models averaged across three scenarios (Aligned, All-Zero, NoScores). The smallest value per row is in bold; full un-averaged results are in Appendix G."
        },
        {
            "title": "7.2 Positional guidance",
            "content": "Sensitivity to prompt guidance: Both models demonstrate high sensitivity to positional cues when contextual scoring is manipulated. significant drop in accuracy occurs when misleading scores are introduced: For Qwen , performance decreases from 0.651 (Aligned ) to 0.589 (All-Zero ), while LLaMA exhibits sharper decline, dropping from 0.619 (Aligned ) to 0.361 (AllZero ). Furthermore, this sensitivity varies across languages. For instance, LLaMAs performance in Hindi collapses under the All-Zero condition (0.038, BOTTOM ), whereas in English, the decline is less pronounced. Score Omission Enhances Robustness: Surprisingly, the No Scores scenario consistently outperformed other configurations, with Qwen achieving 0.656 and LLaMA 0.687 mean accuracy compared to the Aligned scenario (Qwen: 0.651, LLaMA: 0.619), particularly in lowresource languages such as Hindi. This suggests that integrating relevance scores may hurt performance rather than mitigating position bias in multilingual settings. Performance gains from positional guidance are only observed in English for LLaMA and in Vietnamese for both models. Language-Specific Sensitivity: High-resource languages (English, Russian) showed minimal variation across scenarios ( < 2.5% in SQuAD 2.0), while MLQA languages exhibited stark differences. Vietnamese stood out as an outlier, showing exceptional stability ( = 5.3% vs. German = 15.2% in Qwen), potentially due to orthographic or syntactic properties mediating position bias."
        },
        {
            "title": "7.3 Entropy",
            "content": "From Tables 2 and 3, it can be seen that there is no strict correlation between the evaluation of position bias and entropy. The Qwen model has an inverse relationship: in evaluation position bias, the best average accuracy falls on the BOTTOM row, while the lowest entropy occurs at the TOP position. In the Llama model, in contrast, the TOP row gives the best result in terms of accuracy, while the lowest entropy varies. It is noteworthy that in the All-Zero scenario, the entropy and model accuracy are minimal among all the conditions studied  (Table 4)  . likely explanation is that the model generates \"no answer\" responses with excessive confidence, which contradicts the expected behavior. However, further analysis of context-only / no-context modes aligns with common intuition that the accuracy is high at low entropy with context provided, whereas in the no-context mode, the accuracy is low at high entropy, (Figure 2. We perform statistical significance tests for these experiments and discuss them in Section 9. Model Qwen Llama Aligned 0.106 0.268 All Zero 0.097 0. No Scores 0.105 0.256 Table 4: Entropy from Equation 1 for two models averaged along three context placements: MIDDLE, BOTTOM, TOP. The smallest value is in bold. Figure 2: Results when context is provided and not provided. We consider only one relevant context in this experiment to analyze if adding contexts helps the model or if it can generate an answer based on its internal knowledge."
        },
        {
            "title": "7.4 Word Order",
            "content": "We do not observe any evidence that position bias drives models to favor some specific word order. For example, in Hindi (SOV-dominant) we would expect SVO rates to be lowest when the relevant context appears last for Qwen and first for Llama, but we do not observe that  (Table 5)  . For German, since the dominant order depends on clause type (SVO in main clauses; SOV in subordinate clauses; [Dryer, 2013]), we look at SVO prevalence (SVO-SOV difference) in Table 6 and do not find connection to position bias as well, since the prevalence comes naturally from the proportion of complex sentences. For model-level we observe the following differences: 1) Llama generates more SVO sentences than Qwen across Hindi, German, and Russian  (Table 7)  6; 2) Llama 6Russian is considered despite its SVO dominance, as its Model Word order Llama Qwen SVO SOV SVO SOV Top 5.39 88.52 2.34 93.98 Middle 5.12 88.49 4.15 91 Bottom 4.75 88.43 2.96 91.78 Table 5: Percentage of word orders for Hindi, cells where context placements align with position bias are highlighted in green. Bolded results indicate stronger alignment with expected word order. Model Position Difference SVO - SOV % of Complex Sentences Llama Qwen Top Middle Bottom Top Middle Bottom 26.88 32.13 40. 34.17 29.56 29.78 13.56 12.13 10. 8.59 9.54 8.96 Table 6: Prevalence of SVO over SOV word order and the proportion of complex sentences in German. produces more complex sentences than Qwen across all three context positions  (Table 8)  ; 3) Both models tend to generate more complex sentences when the relevant context is in the top position  (Table 9)  . Model Hindi German Russian Llama 4.75 - 5.39 % 60.31 - 67.46 % 95.9 - 96.15 % Qwen 2.34 - 4.15 % 57.97 - 61.39 % 93.61 - 94.52 % Table 7: Percentage of SVO structures in generated responses across all three positions of relevant context."
        },
        {
            "title": "Llama",
            "content": "9.94 % 12.19 % 7.19%"
        },
        {
            "title": "Qwen",
            "content": "6.88% 9.03 % 7% Table 8: Mean percentage of complex sentences among all sentences containing at least one triplet of subject, verb, and object. Language Hindi German Russian Llama Qwen Top Middle Bottom Top Middle Bottom Table 9: The context position in which the proportion of complex sentences is highest, given specific model and language rich morphology allows flexible word order [Ghosh et al., 2024b]."
        },
        {
            "title": "8 Conclusion",
            "content": "influences predictive entropy. In this work, we provide comprehensive analysis of the position bias effect, going beyond the borders of the English language and studying this phenomenon across 5 languages with diverse morphological and syntactical structures. Our findings reveal that position bias is rather model-driven, however minor languagespecific effects still exist. Specifically, Qwen exhibits late-position bias against prior claims about the primacy effect in LLM. Explicit positional guidance does not mitigate position bias effectively and could even lead to performance degradation. Our theoretical and empirical results show that, despite the performance of models typically increasing when the relevant information is placed in the position to which models are biased, the uncertainty is increased as well. Word order is independent of position bias, rather demonstrating model-specific nature. Overall, our findings could be applied to RAG, CoT, long-context and multilingual tasks."
        },
        {
            "title": "9 Limitations",
            "content": "Entropy Analysis: To assess statistical significance, we performed pairwise t-tests with Holm-Bonferroni correction across three positional pairings: (1) top vs. middle, (2) top vs. bottom, and (3) middle vs. bottom. language configuration was classified as having significant effect if statistical significance (p < 0.05) emerged in at least two of the three comparisons. Results indicated that two languagesGerman and Vietnameseshowed consistent significance for both Llama and Qwen models. Notably, this alignment across distinct model architectures highlights these languages unique susceptibility to position bias. Word Order Analysis: We recognize that rigorous analysis of subject-verb-object relationships would necessitate examining broader spectrum of syntactic dependencies. However, such detailed linguistic theoretical inquiry extends beyond the methodological boundaries of this work. Computational Limitations: Our study employed 2,000 question-answer pairs per language. When extrapolated across nine experimental scenarios, this yielded 18,000 model evaluations per language computationally intensive process. Additionally, our findings are constrained by evaluation across only two model families (Llama and Qwen), which restricts broader conclusions about architectural generality. Entropy Analysis: Our attention entropy analysis is subject to two key limitations. First, while token homogenization (the assumption that all tokens are treated equally) requires deeper mechanistic investigation, such exploration falls outside the scope of this work. Second, we have not formally established the formal link between attention entropy and predictive entropy. This gap prevents us from fully validating our hypothesis that aligning positional and contextual attention patterns"
        },
        {
            "title": "References",
            "content": "George Arthur Baker, Ankush Raut, Sagi Shaier, Lawrence Hunter, and Katharina von der Wense. Lost in the middle, and in-between: Enhancing language models ability to reason over long conArXiv, abs/2412.10079, texts in multi-hop qa. 2024. URL https://api.semanticscholar.org/ CorpusID:274763060. Zhenyu (Allen) Zhang, Runjin Chen, Shiwei Liu, Zhewei Yao, Olatunji Ruwase, Beidi Chen, XiFound in aoxia Wu, and Zhangyang Wang. the middle: How language models use long contexts better via plug-and-play positional encoding. ArXiv, abs/2403.04797, 2024a. URL https://api. semanticscholar.org/CorpusID:268296885. Alexander Peysakhovich and Adam Lerer. Attention sorting combats recency bias in long conArXiv, abs/2310.01427, text 2023. URL https://api.semanticscholar.org/ CorpusID:263609111. language models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving fewshot performance of language models. In International conference on machine learning, pages 12697 12706. PMLR, 2021. Meiru Zhang, Zaiqiao Meng, and Nigel Collier. Can we instruct llms to compensate for position bias? In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1254512556, 2024b. Xiaobo Guo and Soroush Vosoughi. tion effects of large language models. abs/2406.15981, 2024. semanticscholar.org/CorpusID:270702994. Serial posiArXiv, URL https://api. Poulami Ghosh, Shikhar Vashishth, Raj Dabre, morphologyand Pushpak Bhattacharyya. based investigation of positional encodings. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2103521045, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi:10.18653/v1/2024.emnlpmain.1170. URL https://aclanthology.org/ 2024.emnlp-main.1170/. Xinyi Wu, Yifei Wang, Stefanie Jegelka, and Ali Jadbabaie. On the emergence of position ArXiv, abs/2502.01951, bias in transformers. 2025. URL https://api.semanticscholar.org/ CorpusID:276107602. Federico Barbero, Alvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael M. Bronstein, Petar Velivckovi c, and Razvan Pascanu. Why do llms attend to the first token? ArXiv, abs/2504.02732, 2025. URL https://api.semanticscholar.org/ CorpusID:277510528. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. Shifting attention to relevance: Towards the predictive uncertainty quantification of free-form large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.acl-long.276. URL https: //aclanthology.org/2024.acl-long.276/. Daphna Oyserman and Spike WS Lee. Does culture influence what and how we think? effects of priming individualism and collectivism. Psychological bulletin, 134(2):311, 2008. Lera Boroditsky, Lauren Schmidt, and Webb Phillips. Sex, syntax, and semantics. Language in mind: Advances in the study of language and thought, 22(6179):3, 2003. Yanzhu Guo, Simone Conia, Zelin Zhou, Min Li, Saloni Potdar, and Henry Xiao. Do large language models have an english accent? evaluating and improving the naturalness of multilingual llms. ArXiv, abs/2410.15956, 2024. URL https://api. semanticscholar.org/CorpusID:273502421. Catherine Arnett and Benjamin Bergen. Why do language models perform worse for morphologically complex languages? In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert, editors, Proceedings of the 31st International Conference on Computational Linguistics, pages 66076623, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https://aclanthology. org/2025.coling-main.441/. Anh Dang, Limor Raviv, and Lukas Galke. Morphology matters: Probing the cross-linguistic morphological generalization abilities of large language models through wug test. In 13th edition of the Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2024), pages 177188. Association for Computational Linguistics (ACL), 2024. Mete Ismayilzada, Defne Circi, Jonne Sälevä, Hale Sirin, Abdullatif Köksal, Bhuwan Dhingra, Antoine Bosselut, Duygu Ataman, and Lonneke Van Der Plas. Evaluating morphological compositional generalization in large language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 12701305, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https: //aclanthology.org/2025.naacl-long.59/. Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, and Lili Qiu. Mitigate position bias in large language models via scaling single dimension. ArXiv, abs/2406.02536, URL https://api.semanticscholar. 2024a. org/CorpusID:270226813. Ziqi Wang, Hanlin Zhang, Xiner Li, Kuan-Hao Huang, Chi Han, Shuiwang Ji, Sham M. Kakade, Hao Peng, and Heng Ji. Eliminating position bias of language models: mechanistic approach. ArXiv, abs/2407.01100, 2024a. URL https://api. semanticscholar.org/CorpusID:270870323. Yuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. REAR: relevance-aware retrieval-augmented framework for open-domain question answering. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 56135626, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi:10.18653/v1/2024.emnlp-main.321. URL https: //aclanthology.org/2024.emnlp-main.321/. Marco Alessio, Guglielmo Faggioli, Nicola Ferro, Franco Maria Nardini, Raffaele Perego, et al. Improving rag systems via sentence clustering and reordering. In CEUR WORKSHOP PROCEEDINGS, volume 3784, pages 3443, 2024. Bowen Jin, Jinsung Yoon, Jiawei Han, and Sercan Ö. Arik. Long-context llms meet rag: Overcoming challenges for long inputs in rag. ArXiv, abs/2410.05983, 2024. URL https://api.semanticscholar.org/ CorpusID:273229050. Hsiu-Yuan Huang, Yutong Yang, Zhaoxi Zhang, Sanwoo Lee, and Yunfang Wu. survey of uncertainty estimation in llms: Theory meets practice. ArXiv, abs/2410.15326, 2024. URL https://api. semanticscholar.org/CorpusID:273502635. Petr Sychev, Andrey Goncharov, Daniil Vyazhev, Edvard Khalafyan, and Alexey Zaytsev. When an llm is apprehensive about its answers - and when its uncertainty is justified. ArXiv, abs/2503.01688, 2025. URL https://api.semanticscholar.org/ CorpusID:276776200. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80868098, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.acl-long.556. URL https: //aclanthology.org/2022.acl-long.556/. Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Yue Zhang, Ren Wang, Xiaoshuang Shi, and Kaidi Xu. Word-sequence entropy: Towards uncertainty estimation in free-form medical question answering applications and beyond. Engineering Applications of Artificial Intelligence, 139: 109553, 2025. Chengzhi Zhong, Fei Cheng, Qianying Liu, Junfeng Jiang, Zhen Wan, Chenhui Chu, Yugo Murawaki, and Sadao Kurohashi. Beyond english-centric llms: What language do multilingual language models think in? ArXiv, abs/2408.10811, 2024. URL https://api. semanticscholar.org/CorpusID:271909321. Lisa Schut, Yarin Gal, and Sebastian Farquhar. Do ArXiv, URL https://api. multilingual abs/2502.15603, 2025. semanticscholar.org/CorpusID:276557950. llms think in english? Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher Manning. Stanza: python natural language processing toolkit for many human languages. arXiv preprint arXiv:2003.07082, 2020. Hee-Soo Choi, Bruno Guillaume, Karën Fort, and Guy Perrier. Investigating dominant word order on Universal Dependencies with graph rewriting. In Ruslan Mitkov and Galia Angelova, editors, Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021), pages 281290, Held Online, September 2021. INCOMA Ltd. URL https://aclanthology.org/ 2021.ranlp-1.33/. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you dont know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, 2018. Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. Mlqa: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. Michael Denkowski and Alon Lavie. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the ninth workshop on statistical machine translation, pages 376380, 2014. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernandez, Albert Gatt, E. Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, Andre F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya Surikuchi, Ece Takmaz, and Alberto Testoni. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks. ArXiv, abs/2406.18403, 2024. URL https://api. semanticscholar.org/CorpusID:270738074. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 43344353, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi:10.18653/v1/2024.emnlp-main.248. URL https: //aclanthology.org/2024.emnlp-main.248/. Klaus Krippendorff. Computing krippendorffs alphareliability, 2011. Matthew S. Dryer. Order of subject, object and verb (v2020.4). In Matthew S. Dryer and Martin Haspelmath, editors, The World Atlas of Language Structures Online. Zenodo, 2013. doi:10.5281/zenodo.13950591. URL https://doi. org/10.5281/zenodo.13950591. Poulami Ghosh, Shikhar Vashishth, Raj Dabre, morphologyand Pushpak Bhattacharyya. based investigation of positional encodings. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2103521045, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi:10.18653/v1/2024.emnlpmain.1170. URL https://aclanthology.org/ 2024.emnlp-main.1170/. Tan Yu, Anbang Xu, and Rama Akkiraju. In defense of rag in the era of long-context language models. ArXiv, abs/2409.01666, 2024b. URL https://api. semanticscholar.org/CorpusID:272368207. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675, 2019. URL https://api.semanticscholar.org/ CorpusID:127986044. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, 2016. efficient foundation language models, 2023. URL https://arxiv.org/abs/2302.13971. Pavel Efimov, Andrey Chertok, Leonid Boytsov, and Pavel Braslavski. Sberquadrussian reading compreIn Exhension dataset: Description and analysis. perimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 2225, 2020, Proceedings 11, pages 315. Springer, 2020. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings. neurips.cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 48954901, Singapore, December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.298. URL https: //aclanthology.org/2023.emnlp-main.298/. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. ISSN 0925-2312. doi:https://doi.org/10.1016/j.neucom.2023.127063. https://www.sciencedirect.com/ URL science/article/pii/S0925231223011864. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407. 10671. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and"
        },
        {
            "title": "A RAG systems with context reordering",
            "content": "REAR [Wang et al., 2024b] integrates document relevance scores into LLMs via embeddings, guiding generation to use internal knowledge (low relevance) or external evidence (high relevance). Long-Context LLMs Meet RAG [Jin et al., 2024] addresses the \"lost-in-the-middle\" problem by reordering retrieved documents, placing highest-scoring ones at sequence boundaries to optimize LLM attention. OP-RAG [Yu et al., 2024b] order preserving RAG preserves original document order (vs. sorting chunks), demonstrating improved answer quality through position-aware context organization. However, authors do not mention multidocument scenario. Clustering & Reordering RAG [Alessio et al., 2024] - cluster sentences by query similarity and sort clusters in descending similarity order for improved retrieval quality."
        },
        {
            "title": "B Datasets",
            "content": "7 [Rajpurkar et al., 2018] is an English reading-comprehension benchmark built on Wikipedia passages. SQuAD2.0 It combines 100 000 span-answerable questions from SQuAD 1.1 [Rajpurkar et al., 2016] with more than 50 000 adversarial questions whose answers are deliberately absent. MTSBerQuAD 8 is an extension of the SberQuAD dataset [Efimov et al., 2020] which is Russian counterpart of SQuAD 2.0. It includes more than 60 000 quesion-answer pairs with improved readability and consistency. MLQA 9 [Lewis et al., 2020] is multilingual benchmark built from aligned Wikipedia passages including 12 000 question-answer pairs in English and about 5 000 in each of the other six languages: Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese. Among these languages, we utilized German, Hindi and Vietnamese. Models: Similarities and differences Shared Foundations. Both models are Transformer-based decoders [Vaswani et al., 2017] with the following commonalities that may influence position bias: 1. Grouped Query Attention (GQA): Enhances inference efficiency by reducing key-value cache size [Ainslie et al., 2023]. 2. Rotary Positional Embeddings (RoPE): Dynamically encodes positional information for variable-length sequences [Su et al., 2024]. 3. Multilingual Support: Covers all five languages in our study [Yang et al., 2024, Touvron et al., 2023]. Key Differences. We identify three critical distinctions between the models: 1. Training Dataset Composition: Qwen: Prioritizes knowledge, coding, and mathematics, trained on 18 trillion tokens [Yang et al., 2024]. Llama: Focuses on dialogue applications, pretrained on 15 trillion multilingual tokens [Touvron et al., 2023]. 2. Architectural Innovations: Llama introduces document-aware attention mask [Touvron et al., 2023] to prevent cross-document interference in long sequences. 3. Vocabulary Design: Llama expands its token vocabulary to 128K tokens (vs. Qwens 151K) for improved non-English language handling [Touvron et al., 2023]."
        },
        {
            "title": "D Technical Details",
            "content": "Models inference. To achieve reproducibility of the obtained results, the LLM-inference was performed using deterministic generation strategy. The following hyperparameter were used/set: \"max_new_tokens\" 1024, \"do_sample\" False, \"num_beams\" 1. Computational Resources. The experiments were run in Docker container on dedicated server with the following hardware: CPU: AMD Ryzen 9 7900X 12-Core Processor, GPU: NVIDIA GeForce RTX 3090 24GB, RAM: Kingston FURY Beast Black 32GB, SSD: M.2 NVMe Samsung 990 PRO 1T."
        },
        {
            "title": "Aligned All Zero",
            "content": "15.8 37.7 27.2 98.1 33.6 15.8 37.3 25.2 87.3 30.5 No Scores 14.1 32.7 26.1 98.6 32."
        },
        {
            "title": "Aligned All Zero",
            "content": "17.3 32.7 30.8 63.1 40.1 17.4 30.7 24.6 46.1 35.8 No Scores 14.8 24.8 29.2 66.9 30.0 Table 10: Averaged GPU-time (in minutes), spent to obtain results for specific experimental setup: language, model, and context placement strategie Required GPU-time for experiments. our experiments on GPU. In total it is required approximately 20 GPU-hours to reproduce the experiments. In Table 10 you can observe average time that was required to conduct"
        },
        {
            "title": "E Evaluation Details",
            "content": "E.1 LLM-as-a-Judge Verification Krippendorffs alpha and Pearson correlation coefficients, calculated for each experimental setup can be seen in Tables 11 and 12 correspondingly. Comparison of human and Llama evaluation can be seen in Figure 3."
        },
        {
            "title": "Position\nTOP\nMIDDLE\nBOTTOM\nTOP\nMIDDLE\nBOTTOM",
            "content": "Aligned 0.783 0.611 0.704 0.814 0.742 0.801 All Zero 0.663 0.861 0.916 0.825 0.865 0.855 No Scores 0.595 0.685 0.718 0.674 0.695 0.776 Table 11: Krippendorffs alpha coefficient, calculated for each experimental setup"
        },
        {
            "title": "Position\nTOP\nMIDDLE\nBOTTOM\nTOP\nMIDDLE\nBOTTOM",
            "content": "Aligned 0.612 0.739 0.738 0.488 0.709 0.669 All Zero 0.632 0.83 0.908 0.704 0.87 0.888 No Scores 0.604 0.791 0.685 0.518 0.769 0.732 Table 12: Pearson correlation coefficient, calculated for each experimental setup"
        },
        {
            "title": "Mean",
            "content": "0.726 0."
        },
        {
            "title": "Mean",
            "content": "0.727 0.705 E.2 Human and LLM instructions We prompt Mistral-Large to judge whether the LLM responses are correctly answering questions. For each dataset we create an evaluation prompt on the language of this dataset and add 4 shots as examples of judgments. The resulting prompt consists of system prompt: \"You are an AI assistant who speaks English.\", which we translate to other languages and user prompt. User prompts for each language could be found in Table 14. For human annotation we consider only English and Russian languages, since our annotators speaks these languages. We use the same instructions as for LLM-as-a-Judge settings, omitting shots. E.3 Human Annotators Information Annotation was conducted by the authors of the work, so no additional recruitment or payment are required on this stage. All assessors held bachelors degree and had prior experience in the evaluation of LLM responses. 7https://huggingface.co/datasets/rajpurkar/squad_v2 8https://huggingface.co/datasets/MTS-AI-SearchSkill/MTSBerquad 9https://github.com/facebookresearch/MLQA Figure 3: Human evaluation and LLM as Judge for Aligned, All Zero, and No Scores settings at three postions (TOP, MIDDLE, BOTTOM) for two languages: (a) English and (b) Russian. Bars in green represent human evaluations, while the blue bars represent the Llama model."
        },
        {
            "title": "F Context Volume Analysis",
            "content": "From Figure 4 we can observe that our Aligned strategie does not effect on position bias with increasing of information load. With context quantity = 15 for Llama we can see significant accuracy decrease, compared to other quantities. From the other hand, for Qwen position bias does not correlate with passed number of contexts. This result can be explained by the fact that for Qwen training larger dataset with long contexts was used, compared to Llama. This feature increase for Qwen the size of its attention window and allowed to conditioning on larger amount of input knowledge during response generation. Figure 4: Accuracy dependence on the number of contexts, added to the user-prompt, and position of the relevant context in list with Aligned placement strategie: (a) Llama; (b) Qwen"
        },
        {
            "title": "G Extended Results",
            "content": "Evaluation of Entropy in Qwen and Llama responses across three scenario can be seen in Table 13."
        },
        {
            "title": "H Prompts",
            "content": "Our userprompts for LLM-inference in terms of context placement strategies can be seen in Tables 15, 16. For Aligned and All Zero strategies items in contextslist has following format: \"- [{score}] {document}\". For No Scores strategy items has the following format: \"- {document}\". As systemprompt the same instruction was used for all languages (translated correspondingly): \"You are an AI assistant who helps solve user issues.\". Language Position Aligned English Russian German Hind TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP MIDDLE BOTTOM TOP Vietnamese MIDDLE BOTTOM Mean 0.092 0.093 0.092 0.104 0. 0.112 0.117 0.125 0.128 0.078 0. 0.082 0.113 0.117 0.111 0.106 Qwen Llama All Zero 0.092 0.101 0.098 0. 0.117 0.109 0.106 0.118 0.117 0. 0.076 0.081 0.101 0.102 0.099 0. No Scores 0.100 0.100 0.097 0.128 0. 0.125 0.114 0.129 0.124 0.076 0. 0.079 0.107 0.120 0.111 0.105 Mean Aligned 0.095 0.098 0.096 0.114 0. 0.115 0.112 0.124 0.123 0.076 0. 0.081 0.107 0.113 0.107 0. 0.237 0.240 0.202 0.216 0.214 0. 0.220 0.225 0.302 0.294 0.288 0. 0.287 0.298 0.268 All Zero 0.247 0. 0.240 0.223 0.219 0.222 0.203 0. 0.152 0.320 0.290 0.276 0.296 0. 0.298 0.254 No Scores 0.193 0.201 0. 0.169 0.181 0.191 0.219 0.224 0. 0.236 0.257 0.275 0.266 0.294 0. 0.256 Mean 0.224 0.225 0.228 0. 0.205 0.209 0.218 0.200 0.199 0. 0.280 0.280 0.278 0.293 0.303 Table 13: Entropy evaluation: Qwen (left block) and Llama (right block). Language User Prompt English Russian German Hindi Vietnamese Table 14: The 4-shot User prompts for LLM-as-a-Judge, designed for each language. Language User Prompt English Russian German Hindi Vietnamese Table 15: Userprompts in five languages for LLMinference in the No Scores context placement strategy Language User Prompt English Russian German Hindi Vietnamese Table 16: Userprompts in five languages for LLMinference in the Aligned and All Zero context placement strategies"
        }
    ],
    "affiliations": [
        "AIRI, Moscow, Russia",
        "HSE University, Moscow, Russia",
        "ITMO, Saint-Petersburg, Russia",
        "Lomonosov MSU, Moscow, Russia",
        "MIPT, Moscow, Russia",
        "Sber, Moscow, Russia",
        "Skoltech, Moscow, Russia"
    ]
}