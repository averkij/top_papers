{
    "paper_title": "MAEB: Massive Audio Embedding Benchmark",
    "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Chenghao Xiao",
        "Roman Solomatin",
        "Animesh Jha",
        "Rahul Chand",
        "Silky Singh",
        "Kaitlyn Wang",
        "Ali Sartaz Khan",
        "Marc Moussa Nasser",
        "Sufen Fong",
        "Pengfei He",
        "Alan Xiao",
        "Ayush Sunil Munot",
        "Aditya Shrivastava",
        "Artem Gazizov",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce the Massive Audio Embedding Benchmark (MAEB), a large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, a collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and a leaderboard at https://github.com/embeddings-benchmark/mteb."
        },
        {
            "title": "Start",
            "content": "MAEB: Massive Audio Embedding Benchmark Adnan El Assadi 1 Isaac Chung 2 Chenghao Xiao 3 Roman Solomatin 4 5 Animesh Jha 6 Rahul Chand 6 Silky Singh 6 Kaitlyn Wang 6 Ali Sartaz Khan 6 Marc Moussa Nasser 6 Sufen Fong 6 Pengfei He 6 Alan Xiao 6 Ayush Sunil Munot 7 Aditya Shrivastava 8 Artem Gazizov 9 Niklas Muennighoff 6 Kenneth Enevoldsen"
        },
        {
            "title": "Abstract",
            "content": "We introduce the Massive Audio Embedding Benchmark (MAEB), large-scale benchmark covering 30 tasks across speech, music, environmental sounds, and cross-modal audio-text reasoning in 100+ languages. We evaluate 50+ models and find that no single model dominates across all tasks: contrastive audio-text models excel at environmental sound classification (e.g., ESC50) but score near random on multilingual speech tasks (e.g., SIB-FLEURS), while speech-pretrained models show the opposite pattern. Clustering remains challenging for all models, with even the best-performing model achieving only modest results. We observe that models excelling on acoustic understanding often perform poorly on linguistic tasks, and vice versa. We also show that the performance of audio encoders on MAEB correlates highly with their performance when used in audio large language models. MAEB is derived from MAEB+, collection of 98 tasks. MAEB is designed to maintain task diversity while reducing evaluation cost, and it integrates into the MTEB ecosystem for unified evaluation across text, image, and audio modalities. We release MAEB and all 98 tasks along with code and leaderboard at https://github.com/ embeddings-benchmark/mteb. 6 2 0 2 7 1 ] . [ 1 8 0 0 6 1 . 2 0 6 2 : r 1. Introduction Audio and speech representations support diverse applications such as voice assistants and music recommendation systems. However, evaluation protocols for audio embedding models vary significantly, spanning speech recognition, 1Carleton University 2Zendesk 3Durham University 4MIRAI 5SaluteDevices 6Stanford University 7Indian Institute of Technology, Kharagpur 8Capital One 9Harvard University 10Aarhus University. Correspondence to: Adnan El Assadi <adnanelassadi@cmail.carleton.ca>. Preprint. February 19, 2026. 1 zero-shot classification, and audio-text retrieval. Existing audio benchmarks often focus on specific tasks (e.g., vocal sound classification (Gong et al., 2022)) or narrow domains (e.g., environmental sounds (Piczak, 2015)) while often ignoring others, limiting insight into how well embeddings transfer across different applications. Without unified evaluation framework, the field remains fragmented, making it difficult to compare models or track meaningful progress across the full landscape of audio tasks. Additionally, the absence of integrated development and maintenance infrastructure has led to stagnation in existing benchmarks, with many becoming outdated as the field rapidly evolves. We introduce the Massive Audio Embedding Benchmark (MAEB) to provide unified, comprehensive evaluation protocol to spur the fields advancement toward universal audio embedding models. Building on the success of MTEB (Muennighoff et al., 2023), MMTEB (Enevoldsen et al., 2025), and MIEB (Xiao et al., 2025b), which have unified and expanded evaluation of embedding models for text and image through continual development and community maintenance, we extend this proven framework to the audio domain. MAEB spans 30 audio tasks grouped into 7 categories. Aligning with MTEBs approach, we include Classification, Zero-shot Classification, Clustering, Pair Classification, Retrieval, and Reranking tasks adapted for audio data. Notably, we consider audio-specific aspects such as multilingual audio understanding, long-form audio processing, and cross-modal audio-text tasks that have been largely absent from prior audio benchmarks. Beyond traditional speech recognition tasks, we emphasize comprehensive audio understanding capabilities through: 1) Diverse acoustic domains, including speech, music, environmental sounds, and bioacoustics; 2) Cross-modal abilities, particularly in zeroshot settings leveraging text descriptions; 3) Complex recognition tasks requiring fine-grained audio understanding; 4) Multilingual audio processing across various languages and dialects. To ensure efficient evaluation and broader adoption, MAEB allows for evaluation of small audio-only model in 2 GPU hours while not compromising on coverage. We also MAEB: Massive Audio Embedding Benchmark Figure 1. overview of task types and example subtypes in MAEB+. Values in parentheses denote numbers for MAEB. provide MAEB(audio), 19-task audio-only subset for evaluating audio-only models, and MAEB+, our full unfiltered collection of 98 tasks. Additionally, we provide modular architecture that simplifies the addition of new audio models and datasets, ensuring that MAEB can evolve with the rapidly advancing field of audio representation learning. Our evaluation of 53 models reveals that no single model dominates across all audio domains; each excels in specific areas while underperforming in others. Preliminary evidence from four Audio LLMs suggests that MAEB encoder quality may correlate with downstream Audio LLM performance (R2 = 0.86, = 4; see Figure 3), validating the benchmarks relevance for multimodal audio understanding. To summarize, MAEB makes the following key contributions: 1. We provide the first comprehensive benchmark for audio embeddings that spans multiple domains, languages, and task types, 2. We establish baseline evaluations using representative set of 53 models, revealing strengths and weaknesses across different audio understanding capabilities, 3. We identify critical areas where current models struggle, particularly in multilingual contexts and crossmodal understanding, providing clear directions for future research, 4. We create flexible, extensible framework that enables the audio research community to standardize evaluation practices and track progress more effectively. 2. MAEB MAEB is fully integrated into the MTEB ecosystem (Muennighoff et al., 2023), extending its unified evaluation framework to the audio modality alongside text (Enevoldsen et al., 2025) and image (Xiao et al., 2025b) embeddings. This integration provides several advantages: (1) tried-and-tested implementations with standardized metrics and evaluation protocols validated across thousands of submissions; (2) extensibility through minimal interface that allows adding new models or tasks with minimal code changes; (3) reproducibility via versioned code and artifacts, with results stored in public repository; and (4) long-term maintenance and community-driven development (Chung et al., 2025). MAEB seeks to broadly evaluate embedding quality for downstream tasksit does not assess transcription, generation, or other capabilities outside the scope of representation learning. 2.1. Benchmark Construction Dataset Selection We curate datasets according to four guiding principles: (1) domain diversity across speech, music, environmental sounds, and bioacoustics; (2) task diversity spanning classification, clustering, pair classification, retrieval, and reranking; (3) linguistic diversity across languages and dialects; and (4) quality and accessibility, prioritizing datasets with established usage, clear licensing, and public availability. Task Selection Evaluating models across our full dataset collection, MAEB+, would be prohibitively expensive for MAEB: Massive Audio Embedding Benchmark most groups. Following MMTEB and MIEB, which demonstrated that principled filtering maintains high rank correlation with exhaustive evaluation, we construct MAEB using five selection criteria: (1) Validity: For directional tasks (e.g., retrieval), we prioritize the more semantically valid direction (e.g., text-to-audio over audio-to-text when text queries better reflect realistic use cases); (2) Unique coverage: Tasks providing exclusive coverage of domain or capability are retained regardless of other factors (e.g., the only bioacoustics clustering task); (3) Linguistic breadth: Among comparable tasks, we retain those covering more languages; (4) Redundancy removal: We compute pairwise correlation matrices across model rankings and remove tasks with Spearman ρ > 0.8 to retained task, keeping the task with broader coverage or lower runtime; (5) Runtime efficiency: Among otherwise equivalent tasks, we select those with lower computational cost. As an intermediate step in task selection, we create MAEB(extended) with 89 tasks by applying initial validity and unique coverage filters to MAEB+. From this intermediate collection, we apply redundancy removal and runtime efficiency criteria to produce the final MAEB (30 tasks). Table 1 compares GPU runtime between MAEB and MAEB(extended) across representative models, showing 2.23.3 speedup depending on model type. MAEB maintains strong correlation with MAEB(extended) in terms of model scores (Pearson r=0.981) and model ranking (Spearman ρ=0.912), indicating that it preserves relative model performance while substantially reducing evaluation time. Table 1. Benchmark runtime comparison (GPU hours) between MAEB and MAEB(extended). Runtime measured on single NVIDIA A100 GPU. 2.2. Tasks and Evaluation We follow similar approach to MMTEB and MIEB to extend tasks to the audio domain. Classification logistic regression is trained on audio embeddings to predict labels (Alain & Bengio, 2018; Radford et al., 2021). We use few-shot linear probing (Muennighoff et al., 2023; Cherti et al., 2023) with 8 examples per class, balancing evaluation quality with computational efficiency. Zero-shot Classification Audio embeddings are directly matched to class labels converted to text prompts (e.g., This is sound of dog bark) without training classifier. We measure accuracy following Radford et al. (2021). Clustering We use MiniBatchKMeans (with set to the number of true labels) and V-measure (Rosenberg & Hirschberg, 2007) as the main metric to evaluate whether embeddings group meaningfully according to semantic categories. Retrieval Retrieval evaluates finding relevant documents from corpus given query, including uni-modal (audioto-audio) and cross-modal (text-to-audio, audio-to-text) scenarios. Documents are ranked by cosine similarity, with CV Recall@5 (cross-validation recall at 5) as the main metric. Pair Classification Given two audio inputs, the task is to predict whether they are similar according to criterion (e.g., same speaker, same sound class). Similarity is computed between embeddings, and average precision based on cosine similarity serves as the main metric."
        },
        {
            "title": "Speedup",
            "content": "YAMNet wav2vec2-xls-r-2b larger_clap_general CLAP-htsat-fused 3.7M 2B 630M 194M 2.01 26.93 11.52 13.03 6.02 45.62 32.23 35.35 3.0 1.7 2.8 2.7 Reranking Unlike retrieval over full corpora, reranking evaluates ranking quality on pre-selected candidate sets containing relevant documents and hard negatives. This tests fine-grained discrimination, with MAP@1000 (mean average precision at 1000) as the main metric. For comprehensive evaluation, we release the full unfiltered collection as MAEB+. See the full dataset list in Appendix A. Benchmark Ranking Following the same protocol in MMTEB (Enevoldsen et al., 2025), we compute model ranks using Borda count (Colombo et al., 2022) by treating each task as preference voter over models. While the Borda count has several advantages over the mean (including scale invariance and robustness to outliers), it is not continuous measure; thus, we provide both the Borda rank and the mean in the leaderboard. 3. Experimental Settings 3.1. Models We seek to evaluate the broad category of audio embedding models, and select 50+ audio encoders representing four broad development categories. Audio Encoders includes models trained specifically on audio through various methods. Self-supervised speech models learn contextualized representations through masked prediction and clustering objectives, including Wav2Vec2/XLSR (Baevski et al., 2020; Babu et al., 2021), WavLM (Chen et al., 2022a), HuBERT (Hsu et al., 2021), Data2Vec (Baevski et al., 2022), UniSpeech (Wang et al., 2021b), 3 MAEB: Massive Audio Embedding Benchmark SEW-D (Wu et al., 2021), and MCTCT (Lugosch et al., 2022). Transformer-based models apply vision transformer architectures to audio spectrograms, including AST (Gong et al., 2021). CNN-based models employ convolutional architectures trained on large-scale audio datasets, including CNN14 (Kong et al., 2020), YAMNet (Gemmeke et al., 2017), and VGGish (Hershey et al., 2017). Neural codec models provide audio compression through learned representations, including Encodec (Défossez et al., 2022). Sequence-to-Sequence Models includes models trained for sequence-to-sequence objective, e.g., for speech recognition and translation. This category includes Whisper (Radford et al., 2022), MMS (Pratap et al., 2023), SeamlessM4T (Communication et al., 2023), and SpeechT5 ASR (Ao et al., 2022). Contrastive Alignment Models includes models that learn joint audio-text embedding spaces through contrastive alignment objective, including CLAP (Wu et al., 2024), MSCLAP (Elizalde et al., 2023), Wav2CLIP (Wu et al., 2022), MuQ-MuLan (Zhu et al., 2025), and SpeechT5 Multimodal (Ao et al., 2022). Large Audio-Language Models are models derived from generative multimodal LLMs, which are then adapted for embeddings, e.g., by utilizing their hidden states or through contrastive refinement. These include Qwen2-Audio (Chu et al., 2024) and LCO-Embedding (Xiao et al., 2025a). Note that the categories are not perfect; for instance, LCOEmbedding (Xiao et al., 2025a) and Wav2Vec2/XLS-R (Baevski et al., 2020; Babu et al., 2021) both utilize contrastive loss during training. Please refer to Appendix for all model details. 3.2. Implementation Details All models implement consistent preprocessing with audio truncated to maximum of 30 seconds, or shorter where required by model architecture or memory constraints. Audio is resampled to model-specific sampling rates (16kHz for speech models, 48kHz for CLAP and MS-CLAP variants, 24kHz for MuQ-MuLan and Encodec) and converted to mono when required. For embedding extraction, we use model-native approaches: transformer models employ mean pooling over temporal dimensions, CNN models use global average pooling, and specialized architectures follow their intended pooling strategies. Contrastive models (CLAP, MS-CLAP, Wav2CLIP, MuQ-MuLan) use their audio encoder branches with L2 normalization for retrieval compatibility. Large audio-language models extract embeddings from the final hidden layer using last-token pooling. 4. Results Table 2 presents the top 30 models on the MAEB benchmark. The table includes both MAEB rank (over all 30 tasks) and Audio-only rank (over the 19 audio-only subset tasks) to highlight how models perform differently across task types. LCO-Embedding-Omni-7B ranks first overall by Borda count, achieving the highest average scores (52.2% overall, 50.3% cross-modal retrieval, 64.5% zero-shot) across all categories. Qwen2-Audio-7B ranks second overall by Borda count (overall average 33.7%) but ranks first on audio-only tasks by Borda count (50.8% average) and excels in reranking (80.8%) and clustering (12.7%). Whisper-medium achieves third place overall by Borda count (overall average 46.7%) with strong audio-only performance (48.2%) but cannot perform cross-modal tasks. CLAP variants (larger_clap_general at 4th, larger_clap_music_and_speech at 6th) demonstrate balanced cross-modal capabilities. We provide detailed per-task results for each category in Appendix E. Figure 2 visualizes the performance of leading models on 94 tasks in MAEB+ across 5 acoustic domains (see Appendix for task details). For each domain, we select the model achieving the highest average score across all task types. We observe distinct specialization patterns: LCO-EmbeddingOmni-7B leads in the Speech domain with an aggregate score of 68.2, driven by strong speech-text alignment, while the Audio Spectrogram Transformer (AST) dominates the Music (71.6), Environmental (63.8), and Bioacoustics (45.2) domains, likely benefiting from its AudioSet pre-training on diverse non-speech events. Qwen2-Audio establishes itself as the leader in Emotion recognition (44.7), demonstrating the advantages of multimodal instruction-tuning for paralinguistic understanding. The disjointed, non-overlapping shapes confirm that no single encoder achieves universal performance across all acoustic domains, the dashed target of 80 remains unmet in every category. This validates our finding that specialized models excel in their respective domains but fail to generalize broadly across the full acoustic spectrum. 4.1. Key Findings on Model Performance Our comprehensive evaluation over MAEB reveals four critical weaknesses in current audio representations, each suggesting specific directions for future model development. (a) No universal audio model exists. Speech-trained models (Wav2Vec2, Whisper) underperform on music tasks, while music-focused models (CLAP variants) struggle with speech understanding, confirming that no single architecture achieves universal audio representation. As shown in Table 2, Whisper-medium achieves strong classification performance (51.7%) but struggles with clustering (5.0%), 4 MAEB: Massive Audio Embedding Benchmark Table 2. Top 30 models on the MAEB benchmark (30 tasks spanning audio-only and audio-text evaluation). Results are ranked using Borda count. The Audio column shows the models rank on MAEB(audio-only) for reference. We provide averages across all tasks, and per task category. Eng. shows the average for English-only tasks, Multi. shows the average excluding tasks with no linguistic content (zxx), and Aud. shows the average for audio-only tasks. Task categories are abbreviated as: Classification (Clf), Multilabel Classification (M.Clf), Pair Classification (PC), Reranking (Rrnk), Clustering (Clust), Audio Retrieval (A. Rtrvl), Cross-modal Retrieval (X. Rtrvl), Zero-shot Classification (Zero Clf.). We highlight the best score in bold and the best score with each model category using grey cell."
        },
        {
            "title": "Model",
            "content": "MAEB Audio All Cat. Eng. Multi. Aud. Clf M.Clf PC Rrnk Clust A. Rtrvl X. Rtrvl Zero Clf. Rank ()"
        },
        {
            "title": "Number of datasets",
            "content": "Large audio-language models LCO-Embedding-Omni-7B Qwen2-Audio-7B LCO-Embedding-Omni-3B"
        },
        {
            "title": "Contrastive Alignment Models",
            "content": "larger_clap_general larger_clap_music_and_speech clap-htsat-unfused clap-htsat-fused msclap-2023 wav2clip MuQ-MuLan-large msclap-2022 Sequence-to-sequence Models whisper-medium whisper-base whisper-small whisper-large-v3 whisper-tiny speecht5_multimodal mms-1b-l1107 mms-1b-all"
        },
        {
            "title": "Audio Encoders",
            "content": "ast-finetuned-audioset-10-10-0.4593 vggish wavlm-large hubert-base-ls960 yamnet wav2vec2-lv-60-espeak-cv-ft wav2vec2-xls-r-2b cnn14-esc50 1 2 5 4 6 7 10 12 14 16 19 3 8 9 11 13 22 25 29 15 17 18 20 21 23"
        },
        {
            "title": "MAEB",
            "content": "(30) (30) (15) (23) (19) (10) (2) (3) (1) (3) (1) (8) (2) 5 1 11 3 4 9 14 12 13 16 28 2 6 7 8 10 37 27 29 15 17 18 19 20 22 23 21 52.2 55.6 50.9 33.7 34.0 30.1 50.7 52.7 49. 32.2 37.1 29.8 31.9 37.0 29.7 30.0 35.9 29.1 30.7 36.2 29.0 31.1 38.0 28.7 25.5 32.7 23.2 27.0 37.7 22.2 29.8 36.1 29.7 46.7 46.0 41.7 42.7 41.9 38.7 43.2 42.6 38.8 42.1 42.8 37.3 42.1 41.8 37.0 25.8 29.6 23.2 38.6 37.0 32.5 38.8 37.5 33.3 44.2 50.1 40.4 39.1 45.8 38.0 37.9 41.1 35.4 37.5 40.5 36.7 38.0 44.9 37.1 38.5 35.8 34.9 38.7 37.5 35.8 33.2 38.4 34.0 53.6 27.6 52.0 28.3 28.1 25.9 27.3 26.7 21.5 22.3 27.3 44.2 39.6 40.5 40.0 39.0 23.5 37.4 38. 36.8 34.9 36.6 35.6 32.6 36.6 34.1 31.8 52.2 50.8 50.0 58.0 62.7 56.4 45.7 10.7 41.6 67.3 56.9 66.7 78.7 80.8 75. 1.7 12.7 1.3 45.1 45.1 42.4 43.2 43.7 38.8 40.9 39.9 48.2 44.4 44.8 43.8 44.0 38.4 40.5 40.6 44.5 40.9 39.7 39.3 39.0 40.4 40.5 35.0 51.7 51.3 45.2 44.5 45.0 39.4 40.7 38.3 57.5 53.0 53.4 50.7 51.0 42.9 48.1 47. 48.9 41.8 43.9 43.2 40.1 48.6 48.4 33.5 2.3 2.7 1.8 4.0 5.8 13.0 10.3 7.6 22.3 11.7 15.5 17.1 14.9 5.9 12.4 14.9 26.1 9.7 7.1 8.3 16.6 8.2 7.8 9.4 51.9 52.1 52.6 52.0 53.6 53.6 51.9 51.7 53.9 52.1 52.6 52.5 51.5 57.9 51.5 52. 51.2 52.8 52.3 51.9 54.6 53.7 50.8 54.2 66.8 65.6 66.5 61.3 75.4 68.9 85.4 62.9 67.6 65.0 64.2 63.9 63.4 56.5 58.8 59.5 77.6 78.7 68.8 66.3 81.7 55.6 62.9 53.8 6.6 7.7 12.5 22.7 15.2 6.0 4.3 19.9 5.0 5.0 3.9 3.4 7.4 1.1 1.0 1. 6.9 7.8 2.4 2.7 1.6 1.6 1.4 7.4 78.2 33.9 67.7 93.2 94.3 88.8 82.8 87.3 68.9 95.2 82.4 69.5 64.5 66.2 69.1 62.7 55.6 50.3 48.8 90.2 83.8 71.8 70.7 74.5 46.9 53.7 72.3 50.3 1.6 50. 9.8 9.3 8.8 9.2 9.4 1.0 1.1 13.7 - - - - - 1.3 - - - - - - - - - - 64.5 12.4 62.2 14.9 13.2 11.3 13.2 12.6 10.8 12.6 12.1 - - - - - 15.9 - - - - - - - - - - whereas CLAP variants show more balanced performance across categories but lower peak scores on speech-specific tasks. Models pretrained on massively multilingual automatic speech recognition data (SeamlessM4T, MMS) substantially outperform other approaches on multilingual classification SeamlessM4T-v2-large achieves the best performance on 10 of 12 languages in MInDS-14  (Table 9)  . Yet this strength does not transfer to music or environmental sound tasks. Conversely, audio-text models like CLAP variants, despite their strength on environmental audio, score below 15% across all languages on MInDS-14, near random chance for intent classification. While LCO-Embedding-Omni-7B and Qwen2-Audio-7B both rank at the top and leverage similar training approaches, they obtain drastically different scores on cross-modal retrieval tasks (50.3% and 1.6%, respectively). This highlights that scale and multimodal pretraining do not guarantee balanced performance. This indicates that training paradigm, data curation, and architectural choices matter more than parameter count for general audio embedding quality, echoing findings from text embedding research. Direction: The specialization gap calls for domain-agnostic architectures that generalize across speech, music, and environmental sound without sacrificing domain-specific capabilities. Future work should explore unified training objectives and architectural innovations that maintain strong performance across the full acoustic spectrum. (b) Multilingual audio understanding remains unsolved. Despite evaluation across 200+ languages via SIBFLEURS (Adelani et al., 2024) (94 languages), CommonVoice (Ardila et al., 2020) (43 languages), MInDS-14 (Gerz 5 MAEB: Massive Audio Embedding Benchmark (c) Acoustic versus linguistic representations trade off. Multilingual evaluation reveals fundamental trade-offs between acoustic and linguistic representations that current architectures cannot reconcile. On VoxPopuli tasks  (Table 17)  , CLAP-htsat-unfused achieves 94.4% on gender identification but only 30.0% on language identification, while Whisper-medium shows the inverse pattern (59.2% vs 99.4%). This suggests that models optimized for acoustic properties (timbre, speaker characteristics) develop fundamentally different representations than those optimized for linguistic content. This trade-off extends to audio-text alignment more broadly. The performance gap between audio-only and audio-text tasks is substantial: as shown in Table 2, AST achieves 44.2% overall but cannot perform cross-modal tasks (showing - for Retrieval and Zero-shot Classification), while CLAP variants achieve around 30-32% overall despite enabling cross-modal tasks. Within audio-text tasks, most models show weak retrieval performance (CLAP variants around 8-14%), though LCO-Embedding-Omni-7B achieves 50.3% cross-modal retrieval and 64.5% zeroshot classification, demonstrating that stronger cross-modal alignment is possible with appropriate training. Models struggle especially with complex audio scenes and abstract musical concepts, suggesting current training objectives fail to capture deeper semantic relationships beyond surfacelevel correspondences. Direction: Future architectures should explore disentangled representations or multi-task learning approaches that capture both acoustic properties (speaker, timbre) and linguistic content simultaneously, enabling models to perform well on both gender identification and language identification without sacrificing one for the other. (d) Clustering exposes fundamental representation gaps. Clustering tasks prove universally challenging across all evaluated models, revealing consistent weakness in semantic structure. Even the best-performing model on clustering (clap-htsat-fused) achieves only 22.7%, while top-ranked models show inconsistent clustering performance: Qwen2Audio-7B (2nd overall) scores 12.7%, LCO-EmbeddingOmni-7B (1st overall, highest average scores) achieves only 1.7%, and whisper-medium (3rd overall) reaches just 5.0%. This disconnect between supervised and unsupervised task performance suggests that current audio embeddings lack the semantic organization necessary for grouping related audio without explicit labelsa fundamental limitation for applications requiring audio organization, discovery, or similarity-based retrieval at scale. Direction: Incorporating clustering-aware losses or contrastive objectives that explicitly encourage semantically coherent embedding neighborhoods could address this gap, Figure 2. Domain-level performance on 94 tasks in MAEB+. Radial plot shows the top-performing model for each of the five acoustic domains: Speech (44 tasks), Music (13), Environmental (29), Bioacoustics (2), and Emotion (6). The dashed line represents an 80 target for universal performance, which remains unmet. Scores are averaged across all available task types (classification, clustering, retrieval, reranking). See Appendix for methodology. et al., 2021) (14 languages), VoxPopuli (Wang et al., 2021a) (5 languages), and FLEURS (Schmidt et al., 2025) (102 languages), models demonstrate strong bias toward highresource languages with severely degraded performance on African, Indigenous, and minority languages. On SIBFLEURS classification  (Table 10)  , high-resource European languages achieve 4060% accuracy while low-resource languages like Umbundu, Yoruba, and Xhosa remain below 20% even for the best models. This disparity becomes catastrophic for cross-modal tasks. While audio-to-audio retrieval maintains reasonable performance across languages (5099% on JamAlt, Table 37), cross-modal audio-text retrieval collapses in multilingual settings. On FLEURS retrieval across 102 languages (Tables 2733), even the best CLAP models achieve below 3% for most language pairs, with audio-to-text and text-to-audio retrieval scores often below 1%. Current audio-text alignment approaches, trained predominantly on English data, fail completely to generalize to multilingual scenariosa critical gap for global audio retrieval applications. Direction: We recommend extending contrastive audio-text pretraining to multilingual corpora and implementing crosslingual transfer learning to leverage high-resource language knowledge for the 100+ languages where current models achieve near-random performance. 6 MAEB: Massive Audio Embedding Benchmark subset of available models. Audio length management poses challenges: models with native limits below 30 seconds retain those settings, while others are limited to 30 seconds for memory management, restricting applicability to long-form content like podcasts or lectures. While future standardization around pre-processing pipelines could streamline evaluation, our approach currently reflects the diverse sampling rate requirements inherent to different audio domains rather than benchmark limitation. Large-scale models (Whisper-large-v3: 1.55B parameters, Wav2Vec2XLS-R-2B: 2B parameters) require substantial computational resources, limiting accessibility. Dataset Coverage Limitations The benchmark exhibits several coverage gaps. Domain representation skews toward Western musical traditions and standard speech patterns. Language coverage, while spanning 100+ languages, remains limited for many underrepresented language families, with some languages appearing in only single datasets, preventing comprehensive cross-task evaluation. The language distribution of MAEB is shown in Figure 4. Task coverage across 30 tasks in MAEB (98 in MAEB+) and 7 categories still lacks certain capabilities including audio generation quality assessment and real-time processing evaluation. Ecological validity is limited as many tasks use clean, studio-recorded audio that does not reflect realworld conditions with noise, reverberation, and compression artifacts. 6. Related Work Text Embedding Benchmarks Large, standardized benchmarks have been critical for driving progress in representation learning. For text, MTEB provides comprehensive evaluation suite spanning 8 task families across 58 datasets and 112 languages, enabling systematic assessment of generalization beyond task-specific setups (Muennighoff et al., 2023). Recent expansions toward massive multilingual and multimodal evaluations such as MMTEB for multilingual text embeddings and MIEB for image embeddings reinforce the value of broad, regularly maintained leaderboards with consistent protocols (Enevoldsen et al., 2025; Xiao et al., 2025b). These efforts motivate analogous, up-to-date benchmarking for audio embeddings. Audio Representation Benchmarks HEAR (Turian et al., 2022) represents one of the first attempts to evaluate generalpurpose audio embeddings across diverse domains such as speech recognition, music tagging, and environmental sound classification. Evaluating 29 models on 19 downstream tasks, HEAR primarily tests pretrained features with simple classifiers like multilayer perceptrons (MLPs), leaving room for exploration with more complex architectures. Figure 3. MAEB+ embedding quality correlates with Audio LLM performance. MMAU evaluates Audio LLMs across Speech, Music, and Sound, the same domains covered by MAEB+. Each point plots an Audio LLMs overall MMAU score (y-axis, averaged across domains) against its encoders MAEB+ score (x-axis, computed from 26 classification tasks aligned with MMAU domains). Preliminary correlation (R²=0.86, p=0.072, n=4) suggests positive relationship between embedding quality and downstream reasoning, though the small sample size and statistical marginality warrant caution in interpreting this relationship. enabling applications that require audio organization without explicit labels. 4.2. Correlation with Audio LLM Performance To assess whether MAEB scores translate to real-world multimodal capabilities, we examine the relationship between encoder quality and Audio LLM performance on the MMAU benchmark (Sakshi et al., 2024). MMAU evaluates multimodal audio understanding through expert-annotated questions organized into three domains: Speech, Music, and Sound. To ensure direct comparison, we compute the encoders embedding quality using subset of 26 classification tasks from MAEB+ selected to align with these three domains (see Appendix for the full task list). We compare four Audio LLMs that use different encoder architectures: Qwen2-Audio (Qwen2-Audio encoder), SALMONN (Whisper), LTU (AST), and Pengi (CLAP). Figure 3 shows preliminary positive correlation across four models. Given the strong correlation between MAEB and MAEB(extended) established in subsection 2.1, this result suggests that the efficient MAEB benchmark serves as reliable predictive signal for downstream Audio LLM performance. 5. Limitations Technical Constraints While our evaluation includes 50+ models spanning multiple architectures, this represents only 7 MAEB: Massive Audio Embedding Benchmark Figure 4. Language distribution in the MAEB+ collection. English dominates with 70 tasks. We use zxx (No Linguistic Content) to tag datasets with no languages present. Despite this progress, comprehensive evaluation of audio embeddings remains limited. Task coverage is narrow, focusing primarily on classification while neglecting systematic evaluation across fundamental applications such as retrieval, and clustering. Similarly, zero-shot performance testing remains fragmented with prior work exploring approaches such as using textual label embeddings, sentence descriptions, or even image embeddings of sound classes (Xie et al., 2021; Mercea et al., 2022), but these efforts are isolated and not integrated into comprehensive evaluation frameworks. Large-scale multilingual support also remains an outstanding issue despite the importance of supporting diverse languages and accents (Xu et al., 2024). Maintenance and reproducibility pose ongoing challenges, with outdated datasets and inconsistent evaluation protocols hindering fair model comparison of current models. MAEB addresses these limitations by building into an existing and maintained framework for evaluating embeddings, drawing on lessons from MTEB while adapting to the unique challenges of audio representation learning. Separately, AudioBench (Wang et al., 2024) and MMAU (Sakshi et al., 2024) focus on evaluating AudioLLMs rather than embedding models. AudioBench evaluates instructionfollowing capabilities across eight tasks using 26 datasets, while MMAU introduces multimodal benchmarks requiring reasoning across speech, sound, and music domains. 7. Conclusion We introduce the Massive Audio Embedding Benchmark (MAEB), comprising 30 tasks across 100+ languages with baselines from 50+ models. Our evaluation reveals critical gaps in current audio representations. No single model achieves universal performance: LCO-Embedding-Omni-7B ranks first overall, achieving the strongest cross-modal retrieval (50.3%) and zero-shot classification (64.5%) averages in our MAEB evaluation. Qwen2Audio-7B ranks second overall and ranks first on audio-only tasks, excelling particularly in reranking (80.8%) and clustering (12.7%). Speech-pretrained models (e.g., Whisper) perform strongly on audio-only tasks but cannot support cross-modal evaluation, while contrastive audio-text models (e.g., CLAP variants) provide cross-modal capabilities but remain weak on multilingual speech tasks. Clustering proves universally challenging (best model: 22.7%), exposing fundamental limitations in semantic structure. We observe stark trade-offs between acoustic and linguistic features, with models excelling at gender identification struggling on language identification and vice versa. Cross-modal multilingual retrieval reveals stark capability gap: LCO models achieve 50%+ accuracy across 100+ languages, while most other models (CLAP, Whisper, ASR encoders) remain below 2%, highlighting the critical role of speech-text alignment for this task. Preliminary analysis across four Audio LLMs suggests positive relationship between MAEB encoder quality and downstream performance, validating the benchmarks relevance for multimodal audio understanding. MAEB integrates into the MTEB ecosystem, enabling unified evaluation across text, image, and audio modalities. We release code, tasks, and leaderboards to support communitydriven progress toward robust, multilingual audio representations."
        },
        {
            "title": "Impact Statement",
            "content": "Large benchmarks create barriers for low-resource communities and incur high environmental costs. We have reduced large datasets to reasonable sizes and include kilogram CO2 measures per task, allowing users to assess environmental benchmarking costs."
        },
        {
            "title": "References",
            "content": "Adelani, D. I., Liu, H., Shen, X., Vassilyev, N., Alabi, J. O., Mao, Y., Gao, H., and Lee, A. E.-S. Sib-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects, 2024. URL 8 MAEB: Massive Audio Embedding Benchmark https://arxiv.org/abs/2309.07445. Adigwe, A., Tits, N., Haddad, K. E., Ostadabbas, S., and Dutoit, T. The emotional voices database: Towards controlling the emotion dimension in voice generation systems, 2018. URL https://arxiv.org/abs/ 1806.09514. Agostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti, M., Caillon, A., Huang, Q., Jansen, A., Roberts, A., Tagliasacchi, M., Sharifi, M., Zeghidour, N., and Frank, C. Musiclm: Generating music from text, 2023. URL https://arxiv.org/abs/2301.11325. Alain, G. and Bengio, Y. Understanding intermediate layers using linear classifier probes, 2018. URL https:// arxiv.org/abs/1610.01644. Allauzen, C., Heigold, G., Ma, J., Variani, E., Riley, M., and Bagby, T. Massive sound embedding benchmark (MSEB). In The Thirty-ninth Annual Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. URL https://neurips.cc/ virtual/2025/poster/121597. Anantapadmanabhan, A., Bellur, A., and Murthy, H. A. Modal analysis and transcription of strokes of the mridangam using non-negative matrix factorization. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 181185, 2013. doi: 10.1109/ ICASSP.2013.6637633. Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., Liu, S., Ko, T., Li, Q., Zhang, Y., Wei, Z., Qian, Y., Li, J., and Wei, F. SpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 57235738, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.393. URL https: //aclanthology.org/2022.acl-long.393/. Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common voice: massively-multilingual speech corpus, 2020. URL https://arxiv.org/ abs/1912.06670. Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A., and Auli, M. Xls-r: Selfsupervised cross-lingual speech representation learning at scale, 2021. URL https://arxiv.org/abs/ 2111.09296. Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:1244912460, 2020. Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M. data2vec: general framework for self-supervised learning in speech, vision and language, 2022. URL https://arxiv.org/abs/2202.03555. Bakhturina, E., Lavrukhin, V., Ginsburg, B., and Zhang, Y. Hi-Fi Multi-Speaker English TTS Dataset. arXiv preprint arXiv:2104.01497, 2021. Bazilinskyy, P., van der Aa, A., Schoustra, M., Spruit, J., Staats, L., van der Vlist, K. J., and de Winter, J. An auditory dataset of passing vehicles recorded with smartphone. In 12th International Symposium on Tools and Methods of Competitive Engineering (TMCE 2018), pp. 417422, 2018. Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J. N., Lee, S., and Narayanan, S. S. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42(4): 335359, 2008. Cao, H., Cooper, D. G., Keutmann, M. K., Gur, R. C., Nenkova, A., and Verma, R. CREMA-D: Crowd-sourced emotional multimodal actors dataset. IEEE Trans. Affect. Comput., 5(4):377390, oct 2014. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., Jin, M., Khudanpur, S., Watanabe, S., Zhao, S., Zou, W., Li, X., Yao, X., Wang, Y., Wang, Y., You, Z., and Yan, Z. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. In Proc. Interspeech 2021, 2021. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L., Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., Yu, X., and Wei, F. Wavlm: Large-scale selfsupervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16 (6):15051518, October 2022a. ISSN 1941-0484. doi: 10.1109/jstsp.2022.3188113. URL http://dx.doi. org/10.1109/JSTSP.2022.3188113. Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S., Wu, J., Qian, Y., Wei, F., Li, J., and Yu, X. Unispeech-sat: Universal speech representation learning with speaker In ICASSP 2022 - 2022 IEEE Inaware pre-training. ternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 61526156, 2022b. doi: 10.1109/ICASSP43922.2022.9747077. 9 MAEB: Massive Audio Embedding Benchmark Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 28182829, 2023. Lee, A., Ma, X., Mourachko, A., Peloquin, B., Pino, J., Popuri, S., Ropers, C., Saleem, S., Schwenk, H., Sun, A., Tomasello, P., Wang, C., Wang, J., Wang, S., and Williamson, M. Seamless: Multilingual expressive and streaming speech translation, 2023. URL https://arxiv.org/abs/2312.05187. Chu, Y., Xu, J., Yang, Q., Wei, H., Wei, X., Guo, Z., Leng, Y., Lv, Y., He, J., Lin, J., Zhou, C., and Zhou, J. Qwen2audio technical report, 2024. URL https://arxiv. org/abs/2407.10759. Conneau, A., Baevski, A., Collobert, R., Mohamed, A.-r., and Auli, M. Unsupervised cross-lingual representation In Proc. Interspeech learning for speech recognition. 2020, pp. 24262430, 2020. Chung, I., Kerboua, I., Kardos, M., Solomatin, R., and Enevoldsen, K. Maintaining mteb: Towards long term usability and reproducibility of embedding benchmarks, 2025. URL https://arxiv.org/abs/ 2506.21182. Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., Riesa, J., Rivera, C., and Bapna, A. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pp. 798805. IEEE, 2023. Chung, J. S., Nagrani, A., and Zisserman, A. Voxceleb2: Deep speaker recognition. In Proceedings of Interspeech, 2018. Drossos, K., Lipping, S., and Virtanen, T. Clotho: An audio captioning dataset, 2019. URL https://arxiv. org/abs/1910.09387. Cífka, O., Schreiber, H., Miner, L., and Stöter, F. Lyrics transcription for humans: readability-aware benchmark. In Proceedings of the 25th International Society for Music Information Retrieval Conference, pp. 737744. ISMIR, 2024. doi: 10.5281/ZENODO.14877443. URL https: //doi.org/10.5281/zenodo.14877443. Clark, R. and Richmond, K. detailed report on the cmu arctic speech database. Technical Report CMU-LTI-03177, Carnegie Mellon University, Language Technologies Institute, 2003. Colombo, P., Noiry, N., Irurozki, E., and Clémençon, new perspectives S. What are the best systems? In Koyejo, S., Mohamed, S., on nlp benchmarking. Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2691526932. Curran Associates, Inc., 2022. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/ ac4920f4085b5662133dd751493946a6-Paper-Conference. pdf. Communication, S., Barrault, L., Chung, Y.-A., Meglioli, M. C., Dale, D., Dong, N., Duppenthaler, M., Duquenne, P.-A., Ellis, B., Elsahar, H., Haaheim, J., Hoffman, J., Hwang, M.-J., Inaguma, H., Klaiber, C., Kulikov, I., Li, P., Licht, D., Maillard, J., Mavlyutov, R., Rakotoarison, A., Sadagopan, K. R., Ramakrishnan, A., Tran, T., Wenzek, G., Yang, Y., Ye, E., Evtimov, I., Fernandez, P., Gao, C., Hansanti, P., Kalbassi, E., Kallet, A., Kozhevnikov, A., Gonzalez, G. M., Roman, R. S., Touret, C., Wong, C., Wood, C., Yu, B., Andrews, P., Balioglu, C., Chen, P.-J., Costa-jussà, M. R., Elbayad, M., Gong, H., Guzmán, F., Heffernan, K., Jain, S., Kao, J., Défossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression, 2022. URL https: //arxiv.org/abs/2210.13438. Elizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H. Clap: Learning audio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Enevoldsen, K., Chung, I., Kerboua, I., Kardos, M., Mathur, A., Stap, D., Gala, J., Siblini, W., Krzeminski, D., Winata, G. I., Sturua, S., Utpala, S., Ciancone, M., Schaeffer, M., Sequeira, G., Misra, D., Dhakal, S., Rystrøm, J., Solomatin, R., Ömer Çagatan, Kundu, A., Bernstorff, M., Xiao, S., Sukhlecha, A., Pahwa, B., Poswiata, R., GV, K. K., Ashraf, S., Auras, D., Plüster, B., Harries, J. P., Magne, L., Mohr, I., Hendriksen, M., Zhu, D., Gisserot-Boukhlef, H., Aarsen, T., Kostkan, J., Wojtasik, K., Lee, T., Šuppa, M., Zhang, C., Rocca, R., Hamdy, M., Michail, A., Yang, J., Faysse, M., Vatolin, A., Thakur, N., Dey, M., Vasani, D., Chitale, P., Tedeschi, S., Tai, N., Snegirev, A., Günther, M., Xia, M., Shi, W., Lù, X. H., Clive, J., Krishnakumar, G., Maksimova, A., Wehrli, S., Tikhonova, M., Panchal, H., Abramov, A., Ostendorff, M., Liu, Z., Clematide, S., Miranda, L. J., Fenogenova, A., Song, G., Safi, R. B., Li, W.- D., Borghini, A., Cassano, F., Su, H., Lin, J., Yen, H., Hansen, L., Hooker, S., Xiao, C., Adlakha, V., Weller, O., Reddy, S., and Muennighoff, N. Mmteb: Massive multilingual text embedding benchmark, 2025. URL https://arxiv.org/abs/2502.13595. Engel, J., Resnick, C., Roberts, A., Dieleman, S., Eck, D., Simonyan, K., and Norouzi, M. Neural audio synthesis 10 MAEB: Massive Audio Embedding Benchmark of musical notes with wavenet autoencoders, 2017. URL https://arxiv.org/abs/1704.01279. Fonseca, E., Plakal, M., Ellis, D. P. W., Font, F., Favory, X., and Serra, X. Learning sound event classifiers from web audio with noisy labels. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 2125. IEEE, 2019. Fonseca, E., Favory, X., Pons, J., Font, F., and Serra, X. Fsd50k: an open dataset of human-labeled sound events. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:829852, 2021. Gemmeke, J. F., Ellis, D. P. W., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In Proc. IEEE ICASSP 2017, New Orleans, LA, 2017. Gerz, D., Su, P., Kusztos, R., Mondal, A., Lis, M., Singhal, E., Mrkšic, N., Wen, T., and Vulic, I. Multilingual and cross-lingual intent detection from spoken data. CoRR, abs/2104.08524, 2021. URL https://arxiv.org/ abs/2104.08524. Gong, Y., Chung, Y.-A., and Glass, J. Ast: Audio spectrogram transformer, 2021. URL https://arxiv.org/ abs/2104.01778. Gong, Y., Yu, J., and Glass, J. Vocalsound: dataset for improving human vocal sounds recognition. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, May 2022. doi: 10.1109/icassp43922.2022. 9746828. URL http://dx.doi.org/10.1109/ ICASSP43922.2022.9746828. Groh, R., Goes, N., and Kist, A. M. Spoken-100: cross-lingual benchmarking dataset for the classification of spoken numbers in different languages, 2024. URL https://arxiv.org/abs/2403.09753. Hershey, S., Chaudhuri, S., Ellis, D. P. W., Gemmeke, J. F., Jansen, A., Moore, R. C., Plakal, M., Platt, D., Saurous, R. A., Seybold, B., Slaney, M., Weiss, R. J., and Wilson, K. Cnn architectures for large-scale auIn 2017 IEEE International Condio classification. ference on Acoustics, Speech and Signal Processing (ICASSP), pp. 131135. IEEE Press, 2017. doi: 10.1109/ ICASSP.2017.7952132. URL https://doi.org/ 10.1109/ICASSP.2017.7952132. Homburg, H., Mierswa, I., Möller, B., Morik, K., and Wurst, M. benchmark dataset for audio classification and clustering. In ISMIR, volume 2005, pp. 52831, 2005. Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A.-r. Hubert: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:34513460, 2021. James, J., Li, T., and Watson, C. An open source emotional speech corpus for human robot interaction applications. In Proc. Interspeech 2018, 2018. Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119132, 2019. Klinck, H., Cañas, J. S., Demkin, M., Dane, S., Kahl, S., and Denton, T. Birdclef+ 2025. https://kaggle.com/ competitions/birdclef-2025, 2025. Kaggle. Koepke, A., Oncescu, A.-M., Henriques, J., Akata, Z., and Albanie, S. Audio retrieval with natural language queries: benchmark study. In IEEE Transactions on Multimedia, 2022. Kong, Q., Cao, Y., Iqbal, T., Wang, Y., Wang, W., and Plumbley, M. D. Panns: Large-scale pretrained audio neural networks for audio pattern recognition, 2020. URL https://arxiv.org/abs/1912.10211. Li, C.-H., Ma, S.-L., Zhang, H.-W., Lee, H.-y., and Lee, L.-s. Spoken squad: study of mitigating the impact of speech recognition errors on listening comprehension. In Interspeech, pp. 34593463, 2018. Lin, G.-T., Chuang, Y.-S., Chung, H.-L., wen Yang, S., Chen, H.-J., Dong, S., Li, S.-W., Mohamed, A., yi Lee, H., and shan Lee, L. Dual: Discrete spoken unit adaptive learning for textless spoken question answering, 2022. URL https://arxiv.org/abs/2203.04911. Livingstone, S. R. and Russo, F. A. The ryerson audio-visual database ofal speech and song (ravdess): dynamic, multimodal set of facial and vocal expressions in north american english. PLOS ONE, 13(5):135, 05 2018. doi: 10.1371/journal.pone.0196391. URL https://doi. org/10.1371/journal.pone.0196391. Hershey, S., Ellis, D. P. W., Fonseca, E., Jansen, A., Liu, C., Moore, R. C., and Plakal, M. The benefit of temporallystrong labels in audio event classification, 2021. URL https://arxiv.org/abs/2105.07031. Lugosch, L., Likhomanenko, T., Synnaeve, G., and Collobert, R. Pseudo-labeling for massively multilingual speech recognition, 2022. URL https://arxiv. org/abs/2111.00161. 11 MAEB: Massive Audio Embedding Benchmark Martin-Morato, I. and Mesaros, A. What is the ground truth? reliability of multi-annotator data for audio tagging, 2021. URL https://arxiv.org/abs/2104.04214. Mercea, O.-B., Riesch, L., Koepke, A. S., and Akata, Z. Audio-visual generalised zero-shot learning with crossIn Proceedings of the modal attention and language. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1055310563, June 2022. Mesaros, A., Heittola, T., and Virtanen, T. multi-device dataset for urban acoustic scene classification. In Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018), Tampere, Finland, 2018. Tampere University of Technology. URL https://arxiv.org/abs/1807.09840. Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. Mteb: Massive text embedding benchmark, 2023. URL https://arxiv.org/abs/2210.07316. Park, C., Min, C., Bhattacharya, S., and Kawsar, F. Augmenting conversational agents with ambient acoustic contexts. In 22nd International Conference on HumanComputer Interaction with Mobile Devices and Services, MobileHCI 20, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450375160. doi: 10.1145/3379503.3403535. URL https://doi. org/10.1145/3379503.3403535. Piczak, K. J. Esc: Dataset for environmental sound In Proceedings of the 23rd ACM Inclassification. ternational Conference on Multimedia, MM 15, pp. 10151018, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450334594. doi: 10. 1145/2733373.2806390. URL https://doi.org/ 10.1145/2733373.2806390. Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S., Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Zhang, X., Hsu, W.-N., Conneau, A., and Auli, M. Scaling speech technology to 1,000+ languages, 2023. URL https://arxiv. org/abs/2305.13516. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision, 2022. URL https://arxiv. org/abs/2212.04356. Raponi, S., Ali, I., and Oligeri, G. Sound of guns: Digital forensics of gun audio samples meets artificial intelligence, 2021. URL https://arxiv.org/abs/ 2004.07948. Rauch, L., Schwinger, R., Wirth, M., Heinrich, R., Huseljic, D., Herde, M., Lange, J., Kahl, S., Sick, B., Tomforde, S., and Scholz, C. Birdset: large-scale dataset for audio classification in avian bioacoustics, 2024. URL https://arxiv.org/abs/2403.10380. Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawid, N., Heba, A., Zhong, J., et al. Speechbrain: general-purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021. Rosenberg, A. and Hirschberg, J. V-measure: conditional entropy-based external cluster evaluation measure. In Eisner, J. (ed.), Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pp. 410420, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/D07-1043/. Sakshi, S., Tyagi, U., Kumar, S., Seth, A., Selvakumar, R., Nieto, O., Duraiswami, R., Ghosh, S., and Manocha, D. Mmau: massive multi-task audio understanding and reasoning benchmark, 2024. URL https://arxiv. org/abs/2410.19168. Salamon, J., Jacoby, C., and Bello, J. P. dataset and taxonomy for urban sound research. In Proceedings of the 22nd ACM international conference on Multimedia, pp. 10411044. ACM, 2014. Schmidt, F. D., Vulic, I., Glavaš, G., and Adelani, D. I. Fleurs-slu: massively multilingual benchmark for spoken language understanding, 2025. URL https: //arxiv.org/abs/2501.06117. Shon, S., Pasad, A., Wu, F., Brusco, P., Artzi, Y., Livescu, K., and Han, K. J. Slue: New benchmark tasks for spoken language understanding evaluation on natural speech, 2022. URL https://arxiv.org/abs/2111.10367. Shon, S., Arora, S., Lin, C.-J., Pasad, A., Wu, F., Sharma, R., Wu, W.-L., Lee, H.-Y., Livescu, K., and Watanabe, S. Slue phase-2: benchmark suite of diverse spoken language understanding tasks, 2023. URL https:// arxiv.org/abs/2212.10525. Sinisetty, G., Ruban, P., Dymov, O., and Ravanelli, M. Commonlanguage, June 2021. URL https://doi.org/ 10.5281/zenodo.5036977. MAEB: Massive Audio Embedding Benchmark Stoter, F.-R., Chakrabarty, S., Edler, B., and Habets, E. A. P. Classification vs. regression in supervised learning for single channel speaker count estimation. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 436440. IEEE, April 2018. doi: 10.1109/icassp.2018.8462159. URL http://dx. doi.org/10.1109/ICASSP.2018.8462159. Tian, M., Srinivasamurthy, A., Sandler, M., and Serra, X. study of instrument-wise onset detection in beijing opera percussion ensembles. In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 21592163, 2014. doi: 10.1109/ICASSP. 2014.6853981. Turian, J., Shier, J., Khan, H. R., Raj, B., Schuller, B. W., Steinmetz, C. J., Malloy, C., Tzanetakis, G., Velarde, G., McNally, K., Henry, M., Pinto, N., Noufi, C., Clough, C., Herremans, D., Fonseca, E., Engel, J., Salamon, J., Esling, P., Manocha, P., Watanabe, S., Jin, Z., and Bisk, Y. Hear: Holistic evaluation of audio representations, 2022. URL https://arxiv.org/abs/2203.03022. Tzanetakis, G. and Cook, P. Musical genre classification of audio signals. IEEE Transactions on Speech and Audio Processing, 10(5):293302, 2002. doi: 10.1109/TSA. 2002.800560. Valk, J. and Alumäe, T. Voxlingua107: dataset for spoken language recognition, 2020. URL https://arxiv. org/abs/2011.12998. Wang, B., Zou, X., Lin, G., Sun, S., Liu, Z., Zhang, W., Liu, Z., Aw, A., and Chen, N. F. Audiobench: universal benchmark for audio large language models. arXiv preprint arXiv:2406.16020, 2024. Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., Williamson, M., Pino, J., and Dupoux, E. VoxPopuli: large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 9931003, Online, August 2021a. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.80. URL https: //aclanthology.org/2021.acl-long.80. Wang, C., Wu, Y., Qian, Y., Kumatani, K., Liu, S., Wei, F., Zeng, M., and Huang, X. Unispeech: Unified speech representation learning with labeled and unlabeled data, 2021b. URL https://arxiv.org/ abs/2101.07597. Wang, Z., Subakan, C., Jiang, X., Wu, J., Tzinis, E., Learning repreRavanelli, M., and Smaragdis, P. sentations for new sound classes with continual selfIEEE Signal Processing Letters, supervised learning. 29:26072611, 2022. ISSN 1558-2361. doi: 10.1109/ lsp.2022.3229643. URL http://dx.doi.org/10. 1109/LSP.2022.3229643. Warden, P. Speech commands: dataset for limitedvocabulary speech recognition. CoRR, abs/1804.03209, URL http://arxiv.org/abs/1804. 2018. 03209. Wu, F., Kim, K., Pan, J., Han, K., Weinberger, K. Q., and Artzi, Y. Performance-efficiency trade-offs in unsupervised pre-training for speech recognition, 2021. URL https://arxiv.org/abs/2109.06870. Wu, H.-H., Seetharaman, P., Kumar, K., and Bello, J. P. Wav2clip: Learning robust audio representations from clip. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022. Wu, Y., Chen, K., Zhang, T., Hui, Y., Nezhurina, M., Berg-Kirkpatrick, T., and Dubnov, S. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation, 2024. URL https://arxiv.org/abs/2211.06687. Xiao, C., Chan, H. P., Zhang, H., Xu, W., Aljunied, M., and Rong, Y. Scaling language-centric omnimodal representation learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025a. Xiao, C., Chung, I., Kerboua, I., Stirling, J., Zhang, X., Kardos, M., Solomatin, R., Al Moubayed, N., Enevoldsen, K., and Muennighoff, N. Mieb: Massive image embedding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2218722198, 2025b. Xie, H., Räsänen, O., and Virtanen, T. Zero-shot audio classification with factored linear and nonlinear acousticsemantic projections. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 326330. IEEE, 2021. Xu, S., Dong, W., Guo, Z., Wu, X., and Xiong, D. Exploring multilingual concepts of human values in large language models: Is value alignment consistent, transferable and controllable across languages? In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1771 1793, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. findings-emnlp.96. URL https://aclanthology. org/2024.findings-emnlp.96/. 13 MAEB: Massive Audio Embedding Benchmark Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. Libritts: corpus derived from librispeech for text-to-speech, 2019. URL https: //arxiv.org/abs/1904.02882. Zhu, H., Zhou, Y., Chen, H., Yu, J., Ma, Z., Gu, R., Luo, Y., Tan, W., and Chen, X. Muq: Self-supervised music representation learning with mel residual vector quantization, 2025. URL https://arxiv.org/abs/2501. 01108. Zohar, J., Cãar, S., Jason, F., Yuxin, P., Hereman, N., and Adhish, T. Jakobovski/free-spoken-digit-dataset: V1.0.8, aug 2018. URL https://doi.org/10. 5281/zenodo.1342401. 14 MAEB: Massive Audio Embedding Benchmark Table 3. MAEB+ Audio-Only Tasks Overview. Tasks are grouped by type and show MAEB benchmark membership, dataset size, total audio duration, language coverage, domains, and main evaluation metric. * denotes values from huge datasets. Dataset Citation MAEB N. Samples Total Duration(s) N. Langs Domains Main Metric Any2AnyRetrieval JamAltArtistA2ARetrieval Classification AmbientAcousticContext BeijingOpera BirdCLEF CREMA_D CommonLanguageAgeDetection CommonLanguageGenderDetection CommonLanguageLanguageDetection ESC50 FSDD GTZANGenre GunshotTriangulation IEMOCAPEmotion IEMOCAPGender LibriCount MInDS14 MridinghamStroke MridinghamTonic NSynth SpeechCommands SpokeNEnglish SpokenQAForIC TUTAcousticScenes UrbanSound8k VocalSound VoxCelebSA VoxLingua107_Top10 VoxPopuliAccentID VoxPopuliGenderID VoxPopuliLanguageID Clustering AmbientAcousticContextClustering CREMA_DClustering ESC50Clustering GTZANGenreClustering MusicGenreClustering VehicleSoundClustering VoiceGenderClustering VoxCelebClustering VoxPopuliAccentClustering VoxPopuliGenderClustering MultilabelClassification AudioSet AudioSetMini BirdSet FSD2019Kaggle FSD50K SIBFLEURS PairClassification CREMADPairClassification ESC50PairClassification NMSQAPairClassification VocalSoundPairClassification VoxPopuliAccentPairClassification Reranking ESC50AudioReranking FSDnoisy18kAudioReranking GTZANAudioReranking UrbanSound8KAudioReranking VocalSoundAudioReranking (Cífka et al., 2024) (Park et al., 2020) (Tian et al., 2014) (Klinck et al., 2025) (Cao et al., 2014) (Sinisetty et al., 2021) (Sinisetty et al., 2021) (Sinisetty et al., 2021) (Piczak, 2015) (Zohar et al., 2018) (Tzanetakis & Cook, 2002) (Raponi et al., 2021) (Busso et al., 2008) (Busso et al., 2008) (Stoter et al., 2018) (Gerz et al., 2021) (Anantapadmanabhan et al., 2013) (Anantapadmanabhan et al., 2013) (Engel et al., 2017) (Warden, 2018) (Groh et al., 2024) (Shon et al., 2023) (Mesaros et al., 2018) (Salamon et al., 2014) (Gong et al., 2022) (Shon et al., 2022) (Valk & Alumäe, 2020) (Wang et al., 2021a) (Wang et al., 2021a) (Wang et al., 2021a) (Park et al., 2020) (Cao et al., 2014) (Piczak, 2015) (Tzanetakis & Cook, 2002) (Homburg et al., 2005) (Bazilinskyy et al., 2018) (Chung et al., 2018) (Shon et al., 2022) (Wang et al., 2021a) (Wang et al., 2021a) (Gemmeke et al., 2017) (Gemmeke et al., 2017) (Rauch et al., 2024) (Fonseca et al., 2021) (Fonseca et al., 2021) (Schmidt et al., 2025) (Cao et al., 2014) (Piczak, 2015) (Lin et al., 2022) (Gong et al., 2022) (Wang et al., 2021a) (Piczak, 2015) (Fonseca et al., 2019) (Tzanetakis & Cook, 2002) (Salamon et al., 2014) (Gong et al., 2022) 6.7k 1k 236 1k 7.4k 2k 2k 2k 2k 300 1k 88 10k 10k 5.7k 7k 7k 7k 3k 4.9k 3.2k 6.1k 2k 8.7k 3.6k 3.4k 972 2k 500 500 1k 2k 2k 1k 1.9k 1.7k 2k 2k 2k 500 * 2.2k * 9k 2k 11.4k 7.4k 2k 171 720 7.4k 4.4k 4.2k 1.4k 5.2k 4.2k 15 22992 1046 393 33602 18924 8685 8777 8637 10000 129 30024 132 44775 44775 28600 78225 2462 2462 12008 4890 2829 12967 20000 31501 14934 27337 9634 22381 5122 5122 1046 5246 10000 30024 18965 6819 14559 16124 23097 * 21316 * 92834 21157 152396 37858 20000 3245 6010 169638 22000 21924 42033 17904 17371 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 12 1 1 1 1 1 1 1 1 1 1 1 1 5 5 1 1 1 1 1 1 1 1 1 1 1 1 1 1 101 1 1 1 1 1 1 1 1 1 1 Music ndcg_at_10 Spoken, Speech Music Spoken, Speech, Bioacoustics Emotion Spoken, Scene, Speech Spoken, Scene, Speech Spoken, Scene, Speech Spoken Music Music Spoken, Emotion Spoken, Speech Speech Speech, Spoken Music Music Music Speech Spoken Spoken AudioScene AudioScene Spoken Spoken Speech Spoken, Speech Spoken, Speech Spoken, Speech Spoken, Speech Speech Spoken, Speech Music Music Scene Spoken Spoken, Speech Spoken, Speech Spoken, Speech Web, Music, Speech... Web, Music, Speech... Spoken, Speech, Bioacoustics Web Web Encyclopaedic Spoken Encyclopaedic Spoken Spoken Spoken AudioScene AudioScene Music Spoken Spoken accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy accuracy v_measure v_measure v_measure v_measure v_measure v_measure v_measure v_measure v_measure v_measure lrap lrap accuracy accuracy accuracy accuracy max_ap max_ap max_ap max_ap max_ap map_at_1000 map_at_1000 map_at_1000 map_at_1000 map_at_1000 MAEB: Massive Audio Embedding Benchmark Table 4. MAEB+ Audio-Text Cross-Modal Tasks Overview. Tasks include zero-shot classification and bidirectional retrieval between audio and text modalities, with dataset size, total audio duration, and main evaluation metric. * denotes values from huge datasets. Dataset Citation MAEB N. Samples Total Secs N. Langs Modality Domains Audio-to-Text Retrieval AudioCapsA2TRetrieval AudioSetStrongA2TRetrieval CMUArcticA2TRetrieval ClothoA2TRetrieval CommonVoiceMini17A2TRetrieval CommonVoiceMini21A2TRetrieval EmoVDBA2TRetrieval FleursA2TRetrieval GigaSpeechA2TRetrieval GoogleSVQA2TRetrieval HiFiTTSA2TRetrieval JLCorpusA2TRetrieval JamAltLyricA2TRetrieval LibriTTSA2TRetrieval MACSA2TRetrieval MusicCapsA2TRetrieval SoundDescsA2TRetrieval UrbanSound8KA2TRetrieval Text-to-Audio Retrieval AudioCapsT2ARetrieval AudioSetStrongT2ARetrieval CMUArcticT2ARetrieval ClothoT2ARetrieval CommonVoiceMini17T2ARetrieval CommonVoiceMini21T2ARetrieval EmoVDBT2ARetrieval FleursT2ARetrieval GigaSpeechT2ARetrieval GoogleSVQT2ARetrieval HiFiTTST2ARetrieval JLCorpusT2ARetrieval JamAltLyricT2ARetrieval LibriTTST2ARetrieval MACST2ARetrieval MusicCapsT2ARetrieval SoundDescsT2ARetrieval SpokenSQuADT2ARetrieval UrbanSound8KT2ARetrieval Zero-shot Classification ESC50_Zeroshot RavdessZeroshot SpeechCommandsZeroshotv0.01 SpeechCommandsZeroshotv0.02 UrbanSound8kZeroshot (Kim et al., 2019) (Hershey et al., 2021) (Clark & Richmond, 2003) (Drossos et al., 2019) (Ardila et al., 2020) (Ardila et al., 2020) (Adigwe et al., 2018) (Conneau et al., 2023) (Chen et al., 2021) (Allauzen et al., 2025) (Bakhturina et al., 2021) (James et al., 2018) (Cífka et al., 2024) (Zen et al., 2019) (Martin-Morato & Mesaros, 2021) (Agostinelli et al., 2023) (Koepke et al., 2022) (Salamon et al., 2014) (Kim et al., 2019) (Hershey et al., 2021) (Clark & Richmond, 2003) (Drossos et al., 2019) (Ardila et al., 2020) (Ardila et al., 2020) (Adigwe et al., 2018) (Conneau et al., 2023) (Chen et al., 2021) (Allauzen et al., 2025) (Bakhturina et al., 2021) (James et al., 2018) (Cífka et al., 2024) (Zen et al., 2019) (Martin-Morato & Mesaros, 2021) (Agostinelli et al., 2023) (Koepke et al., 2022) (Li et al., 2018) (Salamon et al., 2014) (Piczak, 2015) (Livingstone & Russo, 2018) (Warden, 2018) (Warden, 2018) (Salamon et al., 2014) 5.3k 1k 2.6k 6.6k 46.8k 58.5k 2.9k 155620 13.5k 342.9k 600 2.5k 6.7k 9.4k 786 8.6k * 10.2k 5.3k 1k 2.6k 6.6k 46.8k 58.5k 2.9k 155620 13.5k 342.9k 600 2.5k 6.7k 9.4k 786 8.6k * 600 10.2k 2k 1.4k 2.6k 4.1k 2k 8708 5065 4134 23636 120220 149040 7231 1018098 44982 879901 1280 5083 11496 30433 3930 42844 * 18334 8708 5065 4134 23636 120220 149040 7231 1018098 44982 879901 1280 5083 11496 30433 3930 42844 * 3557 18334 10000 5329 2567 4074 7378 2 1 1 1 50 114 1 102 1 20 1 1 4 1 1 1 1 1 2 1 1 1 50 114 1 102 1 20 1 1 4 1 1 1 1 1 1 1 1 1 1 a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t a2t t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a t2a a2t a2t a2t a2t a2t Encyclopaedic, Written AudioScene Spoken Encyclopaedic, Written Spoken Spoken Spoken Spoken Spoken Spoken Spoken Spoken Music Spoken AudioScene Music Encyclopaedic, Written AudioScene Encyclopaedic, Written AudioScene Spoken Encyclopaedic, Written Spoken Spoken Spoken Spoken Spoken Spoken Spoken Spoken Music Spoken AudioScene Music Encyclopaedic, Written Academic, Encyclopaedic, Non-fiction AudioScene Spoken Spoken Spoken Spoken AudioScene Main Metric cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 ndcg_at_10 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 ndcg_at_10 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 cv_recall_at_5 accuracy accuracy accuracy accuracy accuracy 16 MAEB: Massive Audio Embedding Benchmark Figure 5. Domain distributions in the MAEB+ collection, MAEB, and MAEB(audio-only). A. Tasks overview This appendix provides detailed information on all tasks within MAEB, including size, language, metrics, and other relevant details in Table 3 and Table 4. The domain distribution of MAEB is shown in Figure 5. B. Overview of Models All models used in the evaluations are listed in Table 5. B.1. Audio Encoders Transformer-based Models: AST (Audio Spectrogram Transformer) (Gong et al., 2021) applies vision transformer architecture to mel-spectrograms. For retrieval evaluation, we extract the pooler output embedding (768-dim), which corresponds to the [CLS] token representation that captures global audio characteristics. Self-supervised Speech Models: Wav2Vec2 (Baevski et al., 2020) learns contextualized speech representations through masked prediction on quantized latent speech units. We evaluate ten variants ranging from base (95M) to XLS-R 2B (2B parameters), extracting embeddings from the final transformer layer with mean pooling across the temporal dimension. The XLS-R variants (Babu et al., 2021) extend this to 128 languages through multilingual pre-training on 436k hours of speech. WavLM (Chen et al., 2022a) enhances Wav2Vec2 with masked speech prediction and denoising objectives, showing particular strength on noisy audio. We evaluate seven specialized variants: base models, speaker verification (SV), speaker diarization (SD), and combinations thereof. The denoising pre-training makes WavLM particularly robust for retrieval tasks involving real-world audio conditions. HuBERT (Hsu et al., 2021) learns discrete speech units through iterative k-means clustering and masked prediction. We evaluate base (95M) and large fine-tuned (317M) variants, using the final layer representations which capture both acoustic and linguistic information through the learned discrete units. Data2Vec (Baevski et al., 2022) provides unified self-supervised framework using the same learning objective across modalities. For audio, we extract contextualized embeddings from the transformer encoder with mean pooling, leveraging representations that benefit from cross-modal learning insights. SEW-D (Wu et al., 2021) offers performance-efficiency trade-offs through squeezed and efficient transformer architectures. We evaluate three variants (tiny: 20M, mid: 139M, base: 95M parameters), extracting embeddings from the final hidden layer with mean pooling. UniSpeech (Wang et al., 2021b) combines self-supervised pre-training with multi-task fine-tuning for universal speech representations. MCTCT (Lugosch et al., 2022) supports 60 languages through multilingual connectionist temporal classification, using pseudo-labeling for low-resource language adaptation. We extract embeddings from the final hidden states with mean 17 MAEB: Massive Audio Embedding Benchmark pooling. CNN-based Models: CNN14 (Kong et al., 2020) employs 14-layer CNN with global average pooling, trained on AudioSets 2M audio clips. We extract 2048-dimensional embeddings from the penultimate layer before classification. YAMNet (Gemmeke et al., 2017) uses MobileNet architecture optimized for mobile deployment, providing 1024-dimensional features from efficient depthwise separable convolutions. VGGish (Hershey et al., 2017) adapts VGG for audio through mel-spectrogram processing, yielding compact 128-dimensional embeddings. Neural Codec Models: Encodec (Défossez et al., 2022) provides neural audio compression through residual vector quantization. For retrieval evaluation, we extract continuous embeddings from the encoder before quantization (128-dim), applying mean pooling over the temporal dimension. B.2. Sequence-to-Sequence Models Whisper (Radford et al., 2022) provides robust multilingual speech recognition across 99 languages. For retrieval, we extract embeddings from the encoder at the final layer, using mean pooling across the sequence dimension. We evaluate five model sizes (tiny: 39M to large-v3: 1.55B parameters). MMS (Pratap et al., 2023) supports over 1,000 languages through massive multilingual pre-training. We evaluate three variants (1B-all, 1B-fl102, 1B-l1107) differing in language coverage, using the Wav2Vec2-style encoder with languagespecific adapter loading when available. SeamlessM4T (Communication et al., 2023) provides unified speech-text translation across 100+ languages. For retrieval, we extract embeddings from the speech encoder component before translation processing, capturing multilingual audio semantics. SpeechT5 ASR (Ao et al., 2022) provides speech recognition through unified encoder-decoder architecture (152M parameters). We extract embeddings from the encoder representations. B.3. Contrastive Alignment Models CLAP (Wu et al., 2024) learns joint audio-text representations through contrastive learning on 633k audio-text pairs. We evaluate five LAION variants: htsat-fused/unfused (153M parameters) and larger variants (193M) specialized for general audio, music, and combined music-speech. The key implementation detail is using the audio encoder branch with L2 normalization. MS-CLAP (Elizalde et al., 2023) (2022: 196M, 2023: 160M parameters) uses different architectures and training data, providing complementary audio-text alignment capabilities. Wav2CLIP (Wu et al., 2022) bridges audio and vision by learning audio representations that align with CLIPs visual embedding space. For retrieval, we extract features from the audio encoder (11.7M parameters) while text encoding uses the standard CLIP text encoder (151M parameters). MuQ-MuLan (Zhu et al., 2025) specializes in joint music-text understanding through contrastive learning on music data. We extract 512-dimensional embeddings from the audio encoder branch. SpeechT5 Multimodal (Ao et al., 2022) provides unified speech-text modeling through shared encoder-decoder architecture (298M parameters). We extract embeddings from the shared encoder representations. B.4. Large Audio-Language Models Qwen2-Audio (Chu et al., 2024) integrates audio understanding into large language models (7B parameters). We extract embeddings from the final hidden layer using last-token pooling, selecting the embedding at the last non-padding position for each sample. LCO-Embedding (Xiao et al., 2025a) provides language-centric omnimodal representations through contrastive learning on multimodal data. We evaluate two variants (3B: 4.7B parameters, 7B: 8.9B parameters), extracting embeddings from the final hidden layer using last-token pooling. 18 MAEB: Massive Audio Embedding Benchmark Table 5. List of all models evaluated in MAEB. Model sizes are in millions of parameters."
        },
        {
            "title": "Model Size Modalities",
            "content": "laion/clap-htsat-fused(Wu et al., 2024) laion/clap-htsat-unfused(Wu et al., 2024) laion/larger_clap_general(Wu et al., 2024) laion/larger_clap_music(Wu et al., 2024) laion/larger_clap_music_and_speech(Wu et al., 2024) MIT/ast-finetuned-audioset-10-10-0.4593(Gong et al., 2021) speechbrain/cnn14-esc50(Wang et al., 2022) facebook/data2vec-audio-base-960h(Baevski et al., 2022) facebook/data2vec-audio-large-960h(Baevski et al., 2022) facebook/encodec_24khz(Défossez et al., 2022) facebook/hubert-base-ls960(Hsu et al., 2021) facebook/hubert-large-ls960-ft(Hsu et al., 2021) speechbrain/m-ctc-t-large(Ravanelli et al., 2021) facebook/mms-1b-all(Pratap et al., 2023) facebook/mms-1b-fl102(Pratap et al., 2023) facebook/mms-1b-l1107(Pratap et al., 2023) microsoft/msclap-2022(Elizalde et al., 2023) microsoft/msclap-2023(Elizalde et al., 2023) OpenMuQ/MuQ-MuLan-large(Zhu et al., 2025) Qwen/Qwen2-Audio-7B(Chu et al., 2024) LCO-Embedding/LCO-Embedding-Omni-3B(Xiao et al., 2025a) LCO-Embedding/LCO-Embedding-Omni-7B(Xiao et al., 2025a) facebook/seamless-m4t-v2-large(Communication et al., 2023) asapp/sew-d-base-plus-400k-ft-ls100h(Wu et al., 2021) asapp/sew-d-tiny-100k-ft-ls100h(Wu et al., 2021) asapp/sew-d-mid-400k-ft-ls100h(Wu et al., 2021) microsoft/speecht5_asr(Ao et al., 2022) microsoft/speecht5_tts(Ao et al., 2022) microsoft/speecht5_multimodal(Ao et al., 2022) microsoft/unispeech-sat-base-100h-libri-ft(Chen et al., 2022b) google/vggish(Hershey et al., 2017) lyrebird/wav2clip(Wu et al., 2022) facebook/wav2vec2-xls-r-300m(Babu et al., 2021) vitouphy/wav2vec2-xls-r-300m-phoneme(Babu et al., 2021) facebook/wav2vec2-xls-r-1b(Babu et al., 2021) facebook/wav2vec2-xls-r-2b(Babu et al., 2021) facebook/wav2vec2-xls-r-2b-21-to-en(Babu et al., 2021) facebook/wav2vec2-base(Baevski et al., 2020) facebook/wav2vec2-base-960h(Baevski et al., 2020) facebook/wav2vec2-large(Baevski et al., 2020) facebook/wav2vec2-large-xlsr-53(Conneau et al., 2020) facebook/wav2vec2-lv-60-espeak-cv-ft(Baevski et al., 2020) microsoft/wavlm-base(Chen et al., 2022a) microsoft/wavlm-base-sd(Chen et al., 2022a) microsoft/wavlm-base-plus(Chen et al., 2022a) microsoft/wavlm-base-plus-sv(Chen et al., 2022a) microsoft/wavlm-base-plus-sd(Chen et al., 2022a) microsoft/wavlm-base-sv(Chen et al., 2022a) microsoft/wavlm-large(Chen et al., 2022a) openai/whisper-tiny(Radford et al., 2022) openai/whisper-base(Radford et al., 2022) openai/whisper-small(Radford et al., 2022) openai/whisper-medium(Radford et al., 2022) openai/whisper-large-v3(Radford et al., 2022) google/yamnet(Gemmeke et al., 2017) 19 153 153 193 193 193 86 80 93 313 23 95 317 1058 1000 1000 1000 196 160 630 7000 4703 8932 2300 95 19 139 151 146 297 94 72 163 300 300 1000 2000 2000 95 95 317 317 317 94 94 94 94 94 94 316 39 74 244 769 1550 3 audio, text audio, text audio, text audio, text audio, text audio audio audio audio audio audio audio audio audio audio audio audio, text audio, text audio, text audio, text audio, text audio, text audio audio audio audio audio text audio, text audio audio audio, text audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio audio C. Correlation Analysis Tasks MAEB: Massive Audio Embedding Benchmark For the correlation analysis presented in Figure 3, we utilized the following subset of 26 classification tasks from MAEB+, grouped by domain to align with the MMAU benchmark: Speech (13 tasks): SpeechCommands, FSDD, CommonLanguage (Age, Gender, Language), VoxPopuli (Accent, Gender, Language), VoxLingua107, LibriCount, VocalSound, VoxCelebSA, SpokeNEnglish. Music (5 tasks): GTZAN Genre, Beijing Opera, Mridingham (Stroke, Tonic), NSynth. Sound (8 tasks): ESC50, UrbanSound8k, TUT Acoustic Scenes, Ambient Acoustic Context, Gunshot Triangulation, AudioSet Mini, FSD50K, FSD2019 Kaggle. D. Domain Radar Chart Methodology The domain radar chart (Figure 2) visualizes model performance across five core acoustic domains. 94 tasks from MAEB+ are assigned to domains based on their primary audio content and intended application. Score Computation For each model and domain, we compute the arithmetic mean of the main scores across all tasks assigned to that domain. All metrics (e.g., Accuracy, v_measure, nDCG, AP), which are natively in the [0, 1] range, are aggregated on shared 0100 scale for consistent visualization. This aggregation ensures that different task types contribute equally to the domain average. Full Task Breakdown per Domain Below we list all 94 tasks contributing to the domain scores, categorized by their acoustic content: Speech (44 tasks): SpeechCommands, FSDD, CommonLanguageAgeDetection, CommonLanguageGenderDetection, CommonLanguageLanguageDetection, VoxPopuliAccentID, VoxPopuliGenderID, VoxPopuliLanguageID, VoxLingua107_Top10, LibriCount, VocalSound, VoxCelebSA, SpokeNEnglish, SpokenQAForIC, MInDS14, IEMOCAPGender, VoiceGenderClustering, VoxCelebClustering, VoxPopuliAccentClustering, VoxPopuliGenderClustering, VocalSoundPairClassification, VoxPopuliAccentPairClassification, VocalSoundAudioReranking, CMUArcticA2TRetrieval, CMUArcticT2ARetrieval, EmoVDBA2TRetrieval, EmoVDBT2ARetrieval, GigaSpeechA2TRetrieval, GigaSpeechT2ARetrieval, HiFiTTSA2TRetrieval, HiFiTTST2ARetrieval, JLCorpusA2TRetrieval, JLCorpusT2ARetrieval, LibriTTSA2TRetrieval, LibriTTST2ARetrieval, CommonVoiceMini17A2TRetrieval, CommonVoiceMini17T2ARetrieval, CommonVoiceMini21A2TRetrieval, CommonVoiceMini21T2ARetrieval, FleursA2TRetrieval, FleursT2ARetrieval, SpokenSQuADT2ARetrieval, SpeechCommandsZeroshotv0.01, and SpeechCommandsZeroshotv0.02. Music (13 tasks): GTZANGenre, BeijingOpera, MridinghamStroke, MridinghamTonic, NSynth, GTZANGenreClustering, MusicGenreClustering, GTZANAudioReranking, JamAltArtistA2ARetrieval, JamAltLyricA2T, JamAltLyricT2A, MusicCapsA2TRetrieval, and MusicCapsT2ARetrieval. Environmental (29 tasks): ESC50, UrbanSound8k, TUTAcousticScenes, AmbientAcousticContext, GunshotTriangulation, AudioSetMini, FSD50K, FSD2019Kaggle, ESC50Clustering, AmbientAcousticContextClustering, VehicleSoundClustering, ESC50PairClassification, ESC50AudioReranking, UrbanSound8KAudioReranking, FSDnoisy18kAudioReranking, AudioCapsA2T, AudioCapsT2A, AudioSetStrongA2T, AudioSetStrongT2A, ClothoA2T, ClothoT2A, MACSA2T, MACST2A, SoundDescsA2T, SoundDescsT2A, UrbanSound8KA2T, UrbanSound8KT2A, ESC50_Zeroshot, and UrbanSound8kZeroshot. Bioacoustics (2 tasks): BirdCLEF and BirdSet. Emotion (6 tasks): CREMA_D (Classification, Clustering, PairClassification), IEMOCAP Emotion, NMSQA PairClassification, and Ravdess Zeroshot. Model Selection & Visualization To maintain clarity, the chart displays only representative models that achieve the highest average score in at least one domain. This highlights both domain specialists and generalists. 20 MAEB: Massive Audio Embedding Benchmark Missing Results and Task Averaging Domain-averaged scores are computed using the arithmetic mean of all tasks within domain for which results are available. If model cannot perform specific task type (e.g., an audio-only encoder evaluated on text-to-audio retrieval), those tasks are omitted from the average rather than being treated as zero-score. This approach ensures the radar chart reflects the performance quality of models existing capabilities within domain. E. Per Task Category Results E.1. Zero-Shot Classification Table 22 presents results of zero-shot classification tasks. LCO models (LCO-Embedding-Omni-7B) achieve the highest overall zero-shot performance (76.2%), significantly outperforming other models. Specifically, LCO models excel on speech commands (SpeechCmd v0.01, v0.02) with near-perfect scores (> 96%) and show strong performance on emotional speech (Ravdess). CLAP models (larger_clap_general, msclap-2023) excel on environmental sound tasks (ESC50), with larger_clap_general achieving the top score (90.5%), demonstrating the effectiveness of contrastive audio-text pretraining for open-vocabulary environmental sound classification. However, CLAP models generally underperform LCO models on speech-specific tasks. Msclap-2023 achieves the strongest performance on UrbanSound8k (83.0%). Overall, while CLAP models are robust for environmental sounds, LCO-Embedding models demonstrate superior generalization across the broader diverse set of zero-shot tasks, particularly in the speech domain. E.2. Linear Probe Classification Table 6, Table 7, Table 8, Table 9, Table 10, Table 11, Table 12, Table 13, Table 14, Table 15, Table 16, and Table 17 present results of classification tasks. As shown in Tables, Qwen2-Audio-7B achieves the highest classification average (61.7%), surpassing the previously reported baselines. The audio-LLM model (Qwen2-Audio-7B) achieves top performance on wide range of tasks including emotion recognition (CREMA-D, IEMOCAPEmotion), music tasks (BeijingOpera, GTZANGenre, MridinghamStroke, MridinghamTonic, NSynth), and vocal sound classification (VocalSound). LCO-Embedding models also demonstrate exceptional performance, particularly dominating language and speaker tasks such as MInDS14 (> 98%) and VoxCelebSA, where they outperform ASR-based models. Only on specific environmental tasks does the AudioSet-finetuned model (ast-finetuned-audioset-10-10-0.4593) retain dominance (AmbientAcousticContext, BirdCLEF). Whisper models (whispermedium) perform well on accent classification (VoxPopuliAccent) but are generally outperformed by Audio-LLMs and LCO models on broader semantic classification benchmarks. E.3. Multilabel Classification Table 21 presents results of multilabel classification tasks. The LCO-Embedding model (LCO-Embedding-Omni-7B) achieves top performance on FSD2019Kaggle, while Qwen2-Audio-7B leads on FSD50K, leveraging its broad semantic understanding for complex multi-tag scenarios. This contrasts with earlier findings where AudioSet-finetuned models were dominant; here, large-scale trained multimodal models show superior capability in handling diverse acoustic tagging tasks. E.4. Clustering Table 18 and Table 19 present results of clustering tasks. The CLAP variant larger_clap_music_and_speech achieves the highest clustering average (35.3%), closely followed by clap-htsat-unfused (35.0%). These models excel because their contrastive objectives naturally structure the embedding space to group semantically similar audio clips, which is ideal for clustering. ASR encoders and Audio-LLMs generally trail behind contrastive models in this category, as their representations are either too phonetically granular (ASR) or generation-oriented (LLM) rather than density-optimized for unsupervised grouping. E.5. Pair Classification Table 20 presents results of pair classification tasks. LCO-Embedding-Omni-7B achieves the highest pair classification score (79.2%), significantly outperforming whisper-medium (59.9%). This dominance suggests that LCO models capture highly discriminative features suitable for determining verification and similarity across diverse audio pairs. While CLAP models show competence in environmental sound pairs, the LCO models robust performance across speech and mixed 21 MAEB: Massive Audio Embedding Benchmark domains drives its superior average. E.6. Retrieval Table 24, Table 25, Table 27, Table 28, Table 29, Table 31, Table 32, Table 33, Table 35, Table 35, and Table 36 present results of retrieval tasks. Results indicate strong split by domain. LCO-Embedding models achieve near-perfect performance on speech-text retrieval tasks (CMU Arctic, EmoVDB, HiFiTTS, LibriTTS), likely due to extensive speech-text alignment during training. In contrast, CLAP models (larger_clap_general) remain superior for environmental sound retrieval (AudioCaps, AudioSetStrong, Clotho), where their specific training on general audio-text pairs provides an advantage. UrbanSound8K retrieval is an exception where LCO models outperform CLAP substantially. Overall, LCO models dominate the speech retrieval landscape, while CLAP retains the edge in general acoustic event retrieval. E.7. Reranking Table 23 presents results of reranking tasks. LCO-Embedding-Omni-7B achieves the highest average performance (86.0%), demonstrating exceptional capability in distinguishing relevant from non-relevant audio candidates. It tops not only vocal tasks (VocalSound, UrbanSound8K) but also proves highly effective generally. Microsofts msclap-2023 is the top performer on specific environmental reranking tasks like ESC50AudioReranking and FSDnoisy18kAudioReranking. The results highlight that while specialized models like MSCLAP are powerful for specific acoustic domains, recent multimodal embeddings like LCO provide more versatile and high-performance solution across diverse reranking challenges. 22 2 Model AmbientAcoustic BeijingOpera BirdCLEF CommonLangAge CommonLangGender CREMA-D ESC50 FSDD Table 6. English classification results (datasets 18 of 23). Qwen/Qwen2-Audio-7B LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B openai/whisper-medium openai/whisper-small MIT/ast-finetuned-audioset-10-10-0.4593 facebook/wav2vec2-xls-r-2b openai/whisper-base microsoft/msclap-2023 laion/clap-htsat-unfused laion/clap-htsat-fused openai/whisper-tiny laion/larger_clap_general openai/whisper-large-v3 microsoft/wavlm-large laion/larger_clap_music_and_speech facebook/mms-1b-l1107 facebook/wav2vec2-lv-60-espeak-cv-ft facebook/hubert-base-ls960 facebook/mms-1b-fl102 facebook/wav2vec2-xls-r-1b facebook/seamless-m4t-v2-large microsoft/wavlm-base-sv microsoft/wavlm-base-sd microsoft/wavlm-base facebook/mms-1b-all vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/speecht5_multimodal facebook/hubert-large-ls960-ft facebook/wav2vec2-base microsoft/msclap-2022 google/vggish google/yamnet microsoft/unispeech-sat-base-100h-libri-ft asapp/sew-d-tiny-100k-ft-ls100h lyrebird/wav2clip facebook/data2vec-audio-base-960h facebook/data2vec-audio-large-960h facebook/wav2vec2-base-960h facebook/wav2vec2-xls-r-300m OpenMuQ/MuQ-MuLan-large asapp/sew-d-mid-400k-ft-ls100h speechbrain/m-ctc-t-large facebook/wav2vec2-large speechbrain/cnn14-esc50 asapp/sew-d-base-plus-400k-ft-ls100h laion/larger_clap_music facebook/encodec_24khz facebook/wav2vec2-large-xlsr-53 45.33 39.67 38.42 39.85 36.89 48.86 32.88 32.05 46.07 46.72 42.78 30.17 47.80 35.31 29.61 47.18 24.71 25.25 25.87 26.85 31.29 24.36 23.17 23.17 23.17 23.01 25.25 27.20 27.20 27.20 18.77 22.76 26.91 43.86 38.49 40.46 21.72 15.14 34.61 17.86 15.12 16.68 27.55 22.34 14.03 12.15 18.47 18.13 11.64 4.83 13.05 9. 97.45 92.79 93.22 91.54 88.13 97.03 83.06 89.39 91.11 91.96 92.77 83.88 93.62 86.01 57.68 91.52 78.88 87.32 65.76 77.98 70.74 66.15 65.71 65.71 65.71 78.81 86.01 62.76 62.76 62.76 77.13 58.55 75.41 93.20 87.70 89.39 58.49 56.79 88.10 53.01 70.81 49.14 81.78 76.32 43.72 43.24 48.75 83.86 40.29 62.23 42.35 54.65 37.10 34.10 31.60 29.00 26.40 45.20 31.20 27.50 17.30 16.40 18.00 25.10 17.00 21.60 19.80 16.40 21.00 20.50 18.30 19.90 16.80 11.10 10.20 10.20 10.20 21.10 17.50 12.00 12.00 12.00 15.80 14.90 11.10 13.20 10.50 16.80 9.20 9.00 9.70 7.40 9.50 7.10 7.80 7.40 5.50 12.30 7.80 11.20 4.50 3.70 1.80 2.30 17.59 16.16 16.87 17.63 15.85 12.82 17.36 18.18 15.35 15.98 15.27 16.45 20.51 19.28 16.86 16.59 18.48 15.43 15.00 17.03 17.98 16.56 15.37 15.37 15.37 15.84 19.96 18.53 18.53 18.53 19.50 19.37 15.02 14.27 14.70 17.18 15.89 19.25 13.11 18.53 17.18 16.37 15.39 19.34 16.27 17.35 17.89 14.52 19.97 17.62 19.04 15.69 48.83 30.70 33.22 36.63 45.87 52.33 39.80 46.23 62.20 73.26 70.90 45.89 43.71 33.13 40.58 43.87 34.05 40.34 46.67 33.41 33.12 33.26 41.45 41.45 41.45 34.94 37.80 35.82 35.82 35.82 29.98 32.63 39.29 58.32 60.45 40.12 38.43 36.75 39.84 34.65 32.86 35.83 29.25 33.21 39.79 35.91 30.15 40.27 42.13 47.31 31.87 36.55 73.99 36.05 31.03 53.98 49.19 37.84 45.94 48.05 37.06 37.56 38.75 45.93 39.83 48.94 38.93 40.18 29.21 34.95 40.49 30.68 41.95 28.11 40.11 40.11 40.11 31.63 33.71 33.71 33.71 33.71 32.52 31.67 37.97 27.94 34.79 25.87 33.43 30.03 44.45 27.98 26.19 29.47 35.34 34.35 31.46 26.77 35.46 34.53 34.44 30.87 29.83 28.33 96.30 94.15 94.40 84.00 77.35 96.20 73.45 72.35 97.40 97.05 96.50 64.90 97.45 71.65 64.20 97.20 55.25 52.00 59.40 57.10 64.45 44.60 50.40 50.40 50.40 52.30 54.55 57.50 57.50 57.50 46.55 46.20 46.95 90.95 61.15 79.70 41.95 39.55 72.10 31.45 29.60 27.00 43.05 38.50 25.45 17.05 32.30 63.50 21.30 9.75 11.50 6. 90.33 99.00 98.27 87.73 91.47 64.27 95.53 82.67 56.37 49.33 45.67 83.60 40.20 77.00 91.47 43.53 95.13 88.40 93.73 92.93 82.80 92.40 96.40 96.40 96.40 93.53 94.07 91.40 91.40 91.40 89.37 98.00 54.80 29.07 27.23 34.50 91.87 85.87 21.37 68.87 63.33 82.87 64.47 27.33 69.53 59.50 49.33 17.20 24.67 10.00 24.27 18."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "2 4 Model GTZANGenre GunshotTri IEMOCAPEmo IEMOCAPGender LibriCount MridinghamStroke MridinghamTonic NSynth Table 7. English classification results (datasets 916 of 23). Qwen/Qwen2-Audio-7B LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B openai/whisper-medium openai/whisper-small MIT/ast-finetuned-audioset-10-10-0.4593 facebook/wav2vec2-xls-r-2b openai/whisper-base microsoft/msclap-2023 laion/clap-htsat-unfused laion/clap-htsat-fused openai/whisper-tiny laion/larger_clap_general openai/whisper-large-v3 microsoft/wavlm-large laion/larger_clap_music_and_speech facebook/mms-1b-l1107 facebook/wav2vec2-lv-60-espeak-cv-ft facebook/hubert-base-ls960 facebook/mms-1b-fl102 facebook/wav2vec2-xls-r-1b facebook/seamless-m4t-v2-large microsoft/wavlm-base-sv microsoft/wavlm-base-sd microsoft/wavlm-base facebook/mms-1b-all vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/speecht5_multimodal facebook/hubert-large-ls960-ft facebook/wav2vec2-base microsoft/msclap-2022 google/vggish google/yamnet microsoft/unispeech-sat-base-100h-libri-ft asapp/sew-d-tiny-100k-ft-ls100h lyrebird/wav2clip facebook/data2vec-audio-base-960h facebook/data2vec-audio-large-960h facebook/wav2vec2-base-960h facebook/wav2vec2-xls-r-300m OpenMuQ/MuQ-MuLan-large asapp/sew-d-mid-400k-ft-ls100h speechbrain/m-ctc-t-large facebook/wav2vec2-large speechbrain/cnn14-esc50 asapp/sew-d-base-plus-400k-ft-ls100h laion/larger_clap_music facebook/encodec_24khz facebook/wav2vec2-large-xlsr-53 93.10 82.30 81.00 76.00 71.50 80.70 73.20 70.90 78.10 74.90 67.90 68.40 84.50 71.90 67.70 83.50 58.30 55.20 69.00 56.80 66.10 52.50 63.00 63.00 63.00 57.10 60.90 62.90 62.90 62.90 52.50 50.60 62.40 58.70 79.30 79.30 51.60 51.10 59.10 40.30 43.60 42.70 42.60 88.30 43.40 39.20 54.30 41.40 40.10 25.80 29.90 22.10 100.00 96.67 96.60 94.25 94.44 98.82 94.31 89.80 87.45 69.41 70.59 90.92 86.27 81.90 100.00 77.25 88.56 87.45 98.89 90.98 91.90 72.61 95.49 95.49 95.49 91.90 85.36 97.71 97.71 97.71 93.20 93.14 97.71 57.97 86.41 80.46 91.05 89.80 76.27 81.76 70.52 79.41 70.46 51.05 76.14 57.78 84.18 83.20 52.48 60.13 46.60 33.07 29.96 24.35 22.53 25.60 24.21 20.49 23.86 23.82 22.10 22.61 21.17 23.38 23.08 22.18 20.25 24.07 16.85 19.02 20.59 16.35 23.30 22.79 21.20 21.20 21.20 17.97 20.79 18.42 18.42 18.42 19.79 17.84 20.23 15.54 19.32 14.84 18.17 17.01 16.33 15.49 15.58 15.11 14.68 16.07 16.00 16.17 16.78 18.09 18.68 11.58 10.57 16.06 92.96 70.75 59.42 76.05 69.93 87.00 70.27 72.16 85.86 92.58 93.62 68.88 89.28 60.25 63.05 93.12 54.69 68.31 76.19 52.44 61.82 53.72 67.32 67.32 67.32 56.23 51.16 55.00 55.00 55.00 52.14 54.25 68.88 89.35 91.54 76.91 60.45 52.57 65.83 55.84 54.59 51.82 53.12 57.45 56.26 50.96 54.28 59.37 55.73 65.75 53.46 50.10 49.60 33.22 30.35 57.87 53.37 42.15 50.24 50.21 42.06 48.69 47.81 50.17 48.50 57.22 52.26 47.99 41.21 45.30 48.92 39.60 49.79 44.14 50.73 50.73 50.73 41.78 42.54 52.06 52.06 52.06 43.25 44.27 54.95 39.74 45.61 41.33 45.35 43.43 34.65 48.44 42.48 46.15 37.54 36.70 43.13 34.86 45.17 28.37 42.74 20.49 21.24 25. 84.33 61.89 61.52 69.21 69.06 79.20 70.59 60.68 79.46 71.66 74.09 61.70 71.05 54.84 60.87 70.99 61.02 66.60 55.41 54.84 65.17 39.39 46.68 46.68 46.68 51.63 57.39 42.25 42.25 42.25 42.43 47.93 57.98 47.08 50.48 56.01 39.79 30.49 46.52 33.27 28.48 36.43 55.84 42.07 29.28 21.43 44.46 30.59 22.69 38.86 18.35 24.65 61.17 42.07 39.54 49.51 45.49 54.16 44.62 44.67 52.09 49.52 47.00 44.03 49.45 37.54 31.42 48.99 30.70 37.29 31.58 30.89 39.85 35.37 29.68 29.68 29.68 27.42 33.97 26.47 26.47 26.47 30.74 24.22 35.14 29.14 32.77 35.16 25.34 23.65 40.28 20.77 21.21 24.32 35.83 38.25 19.38 24.27 26.64 26.06 19.49 18.39 24.04 19.13 63.04 58.09 59.02 51.33 49.88 56.26 45.02 47.07 62.64 59.80 60.23 46.47 59.31 46.04 47.14 58.64 44.29 42.62 46.36 43.84 46.65 43.30 43.10 43.10 43.10 43.25 42.02 46.38 46.38 46.38 40.13 40.26 42.65 50.21 43.80 45.81 42.50 39.91 46.04 40.42 38.22 38.89 43.06 52.97 36.68 40.72 40.34 39.83 36.46 38.27 37.30 32."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "2 5 Table 8. English classification results (datasets 1723 of 23). Model SpeechCommands SpokenQA TUTAcoustic VocalSound VoxCelebSA VoxPopuliAccent MInDS14 Qwen/Qwen2-Audio-7B LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B openai/whisper-medium openai/whisper-small MIT/ast-finetuned-audioset-10-10-0.4593 facebook/wav2vec2-xls-r-2b openai/whisper-base microsoft/msclap-2023 laion/clap-htsat-unfused laion/clap-htsat-fused openai/whisper-tiny laion/larger_clap_general openai/whisper-large-v3 microsoft/wavlm-large laion/larger_clap_music_and_speech facebook/mms-1b-l1107 facebook/wav2vec2-lv-60-espeak-cv-ft facebook/hubert-base-ls960 facebook/mms-1b-fl102 facebook/wav2vec2-xls-r-1b facebook/seamless-m4t-v2-large microsoft/wavlm-base-sv microsoft/wavlm-base-sd microsoft/wavlm-base facebook/mms-1b-all vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/speecht5_multimodal facebook/hubert-large-ls960-ft facebook/wav2vec2-base microsoft/msclap-2022 google/vggish google/yamnet microsoft/unispeech-sat-base-100h-libri-ft asapp/sew-d-tiny-100k-ft-ls100h lyrebird/wav2clip facebook/data2vec-audio-base-960h facebook/data2vec-audio-large-960h facebook/wav2vec2-base-960h facebook/wav2vec2-xls-r-300m OpenMuQ/MuQ-MuLan-large asapp/sew-d-mid-400k-ft-ls100h speechbrain/m-ctc-t-large facebook/wav2vec2-large speechbrain/cnn14-esc50 asapp/sew-d-base-plus-400k-ft-ls100h laion/larger_clap_music facebook/encodec_24khz facebook/wav2vec2-large-xlsr75.60 94.21 94.40 72.43 73.80 24.27 65.13 62.18 29.77 23.33 24.86 60.03 10.50 65.17 83.46 10.51 83.85 83.89 79.65 80.88 62.09 84.55 83.23 83.23 83.23 75.37 83.37 83.06 83.06 83.06 78.59 85.28 40.35 19.04 12.64 14.47 81.86 80.39 10.51 74.43 69.45 77.05 20.18 11.10 65.20 76.00 15.93 8.96 51.67 4.71 9.66 4.32 21.35 36.58 37.00 21.74 19.96 15.86 17.76 17.92 14.52 14.46 14.30 18.36 15.06 19.21 20.68 15.68 27.17 23.64 20.47 25.73 18.02 32.22 22.59 22.59 22.59 24.64 20.08 18.22 18.22 18.22 23.52 23.69 15.03 13.61 14.13 13.10 18.54 22.19 13.31 20.45 21.65 16.55 13.59 14.25 18.13 20.83 14.05 13.17 17.58 13.76 11.81 11.65 34.30 25.35 25.00 26.55 23.55 30.45 24.95 25.10 28.20 30.95 30.15 25.45 30.75 22.65 24.15 29.85 20.55 22.80 24.35 19.15 23.00 15.60 24.00 24.00 24.00 19.55 20.70 27.15 27.15 27.15 20.65 19.65 21.25 26.30 26.15 25.10 20.50 22.80 22.40 19.45 15.85 18.95 19.25 18.45 15.85 11.95 16.20 21.25 14.05 19.35 18.20 16.60 91.82 91.77 91.31 80.13 77.59 82.11 68.98 72.11 78.41 81.53 80.88 68.14 74.46 76.76 66.68 73.64 66.83 64.70 58.84 64.46 63.14 64.62 53.00 53.00 53.00 59.97 59.65 55.63 55.63 55.63 53.22 57.06 54.45 71.94 39.46 47.82 47.62 48.96 38.29 55.42 53.82 51.99 39.75 34.99 40.23 49.20 40.77 42.58 37.18 30.54 26.90 24.62 29.54 43.40 48.97 33.92 32.45 28.33 28.73 31.26 26.18 33.72 29.29 29.66 30.47 31.46 33.44 33.78 30.62 28.12 32.91 31.95 28.62 43.55 33.61 33.61 33.61 28.36 27.14 30.30 30.30 30.30 28.39 33.64 28.88 25.54 27.08 27.46 30.13 29.75 25.78 32.07 30.56 30.42 17.69 25.11 30.65 30.36 31.17 23.07 32.13 27.05 23.55 12.09 39.35 10.33 8.97 54.04 38.85 23.61 34.29 28.47 17.69 18.00 17.59 27.52 23.86 28.87 49.82 24.06 44.36 27.07 28.07 21.00 26.27 13.28 23.51 23.51 23.51 15.59 15.64 17.69 17.69 17.69 20.90 14.34 26.07 14.74 18.45 18.80 17.59 15.79 14.19 13.33 12.38 10.08 9.07 11.53 13.68 13.13 12.03 10.58 14.59 10.53 9.42 8. 25.51 98.14 98.48 48.30 35.64 7.94 15.21 29.56 7.43 9.29 10.30 31.25 8.11 31.92 20.77 7.77 64.02 42.07 15.53 77.03 13.34 89.18 21.78 21.78 21.78 58.46 27.37 12.84 12.84 12.84 41.89 35.63 10.13 7.77 7.26 5.91 16.05 26.01 16.04 37.50 47.81 23.13 10.47 18.92 15.36 52.88 11.65 10.80 15.85 9.63 8.61 7."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": ". MAEB: Massive Audio Embedding Benchmark Table 9. MInDS-14 classification results across languages. Best result per language in bold. Model cs de en es fr it ko nl pl pt ru zh 8.8 9. LCO-Embedding/LCO-Embedding-Omni-7B 72.3 91.5 98.1 97.9 97.4 84.2 92.9 94.2 68.7 82.5 95.6 97.8 LCO-Embedding/LCO-Embedding-Omni-3B 72.5 89.8 98.5 96.9 97.4 84.8 93.2 92.1 67.6 81.0 95.2 98.2 92.3 89.4 89.2 92.0 90.4 78.9 90.5 90.2 67.8 63.4 90.4 93.0 facebook/seamless-m4t-v2-large 77.0 78.4 77.0 76.8 75.3 71.8 70.3 78.7 69.8 66.2 66.6 71.3 facebook/mms-1b-fl102 64.3 54.7 58.5 46.1 62.0 51.1 69.9 64.2 47.3 53.5 54.0 76.9 facebook/mms-1b-all 55.6 57.4 64.0 58.2 63.1 51.9 44.8 56.7 43.8 46.5 52.9 37.6 facebook/mms-1b-l1107 50.5 53.0 48.3 57.6 44.9 48.3 53.2 54.1 43.8 47.8 47.3 47.4 openai/whisper-medium 49.8 53.7 52.9 43.0 60.3 46.8 30.1 41.7 30.6 32.9 47.5 45.6 speechbrain/m-ctc-t-large 35.9 38.0 35.6 40.9 36.7 35.5 38.4 40.4 29.2 35.9 37.5 39.4 openai/whisper-small 34.3 31.2 31.9 30.9 34.5 32.8 40.2 38.1 24.9 33.1 31.2 31.4 openai/whisper-large-v3 30.5 35.2 29.6 35.4 26.0 31.2 29.4 29.5 26.3 31.0 23.9 28.3 openai/whisper-base 27.5 21.9 42.1 22.6 31.2 20.0 21.3 28.3 19.9 25.0 25.2 28.5 facebook/wav2vec2-lv-60-espeak-cv-ft 24.6 28.3 31.3 25.3 23.4 27.2 25.3 28.1 21.9 26.5 18.7 21.5 openai/whisper-tiny 20.9 28.5 25.5 22.0 28.0 28.0 27.5 27.4 20.7 27.2 20.4 16.9 Qwen/Qwen2-Audio-7B 26.1 17.2 47.8 17.1 18.4 22.4 14.4 22.9 22.6 22.2 20.2 13.5 facebook/data2vec-audio-large-960h 23.9 20.5 27.4 20.4 25.1 16.7 19.4 26.8 15.8 21.4 20.4 22.9 vitouphy/wav2vec2-xls-r-300m-phoneme 19.2 20.1 41.9 14.2 17.8 14.1 16.9 24.9 12.8 18.0 17.6 15.5 microsoft/speecht5_multimodal 19.0 18.5 26.0 10.1 14.3 16.2 18.9 20.6 15.8 18.4 13.5 14.7 asapp/sew-d-tiny-100k-ft-ls100h 15.8 14.1 35.6 11.5 15.6 13.9 14.5 15.8 15.1 17.7 13.2 14.5 facebook/hubert-large-ls960-ft 15.5 10.6 37.5 13.2 16.0 13.9 13.8 17.4 17.3 12.9 11.5 11.3 facebook/data2vec-audio-base-960h 14.9 12.1 18.7 17.2 16.5 18.9 13.8 22.1 11.2 19.9 13.3 OpenMuQ/MuQ-MuLan-large 16.9 11.9 23.1 11.7 16.0 13.8 12.2 12.1 11.2 17.0 11.7 12.8 facebook/wav2vec2-base-960h 13.9 14.1 15.2 facebook/wav2vec2-xls-r-2b 12.3 14.1 12.1 20.8 10.1 11.5 12.1 13.5 14.7 14.9 14.6 10.8 12.4 microsoft/wavlm-large 11.1 13.6 12.0 14.1 14.4 13.4 12.1 12.2 11.5 14.2 21.8 microsoft/wavlm-base-sv 11.1 13.6 12.0 14.1 14.4 13.4 12.1 12.2 11.5 14.2 21.8 microsoft/wavlm-base-sd 11.1 13.6 12.0 14.1 14.4 13.4 12.1 12.2 11.5 14.2 21.8 microsoft/wavlm-base 13.2 12.5 11.2 12.8 11.0 14.4 11.7 10.4 13.9 14.4 15.5 facebook/hubert-base-ls960 12.4 14.1 12.7 12.5 10.0 18.0 10.9 10.2 12.0 14.2 13.3 facebook/wav2vec2-xls-r-1b 9.8 lyrebird/wav2clip 9.2 12.2 11.9 11.8 11.0 14.4 14.6 10.0 11.8 11.9 10.3 12.8 microsoft/wavlm-base-plus-sv 12.2 11.9 11.8 11.0 14.4 14.6 10.0 11.8 11.9 10.3 12.8 microsoft/wavlm-base-plus 12.2 11.9 11.8 11.0 14.4 14.6 10.0 11.8 11.9 10.3 12.8 microsoft/wavlm-base-plus-sd 9.4 9.8 13.1 11.9 15.9 asapp/sew-d-base-plus-400k-ft-ls100h 11.8 10.8 10.6 11.1 12.7 11.9 15.4 asapp/sew-d-mid-400k-ft-ls100h 8.4 9.6 12.2 8.9 microsoft/unispeech-sat-base-100h-libri-ft 16.0 7.4 9.0 9.3 8.4 11.2 11.3 11.6 facebook/wav2vec2-large 7.2 10.2 11.0 9.6 laion/clap-htsat-unfused 9.8 9.3 6.2 10.5 9.5 10.5 10.1 9.8 facebook/wav2vec2-base 9.0 10.2 8.3 10.5 8.3 9.6 facebook/wav2vec2-xls-r-300m 7.2 9.3 6.7 10.5 10.3 8.9 laion/clap-htsat-fused 8.8 8.5 7.8 10.8 8.3 8.4 speechbrain/cnn14-esc50 10.0 8.2 9.7 8.6 8.7 9.2 facebook/encodec_24khz 6.8 8.0 9.6 9.2 8.2 laion/larger_clap_music 9.2 7.6 10.1 8.5 7.8 8.5 8.0 facebook/wav2vec2-xls-r-2b-21-to-en 10.3 6.9 8.7 7.4 7.9 7.5 facebook/wav2vec2-large-xlsr-53 5.8 10.6 7.6 8.1 12.1 6.4 laion/larger_clap_general 5.4 10.3 6.1 7.9 9.7 6.4 MIT/ast-finetuned-audioset-10-10-0.4593 6.6 4.5 7.4 9.3 7.1 microsoft/msclap-2023 9.6 3.6 11.5 5.9 7.8 9.0 5.0 laion/larger_clap_music_and_speech 5.4 8.0 6.5 7.3 8.8 6.8 google/vggish 7.8 5.7 7.8 7.8 6.5 7.0 microsoft/msclap-2022 5.8 7.5 5.2 5.9 7.0 6.6 google/yamnet 10.3 12.9 14.3 9.7 13.9 9.6 7.6 13.7 9.3 7.6 9.1 14.1 9.8 11.6 12.6 9.1 9.4 13.7 8.5 15.2 8.5 5.4 12.3 13.1 7.8 13.7 8.7 6.3 9.6 10.9 7.8 9.9 8.9 6.7 15.2 6.8 5.0 8.1 9.1 5.0 11.8 8.2 4.6 10.4 7.5 7.6 9.3 8.7 5.8 11.3 9.8 5.2 10.8 7.3 6.3 9.6 5.7 6.1 8.8 7.8 9.9 10.6 8.1 9.3 9.5 9.2 7.9 9.2 10.2 8.1 8.9 7.5 8.7 8.0 7.3 6.6 5.4 6.1 5.0 7.6 8.6 8.6 8.6 8.8 9.3 8.6 9.1 5.1 6.8 7.8 8.8 8.4 8.2 8.2 5.8 9.3 6.0 5.8 5.4 4.3 5.8 4.9 4.1 9.5 8.6 9.6 8.9 9.3 9.0 7.4 8.9 6.8 8.1 9.6 7.4 7.3 6.3 9.3 8.6 5.2 4.9 8.3 8.1 14.2 16.0 13.2 15.2 12.6 12.7 10. 13.2 15.7 12.5 16.8 11.7 19.7 9.9 9.9 9.9 9.9 9.1 16.4 8.9 7.8 9. 26 2 7 Table 10. SIB-FLEURS classification results (languages 115 of 102). Best per language in bold. Model afr_Latn amh_Ethi arb_Arab asm_Beng ast_Latn azj_Latn bel_Cyrl ben_Beng bos_Latn bul_Cyrl cat_Latn ceb_Latn ces_Latn ckb_Arab cym_Latn LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr47.3 40.2 42.0 26.8 17.9 25.0 20.6 25.0 26.0 21.5 22.4 17.0 12.5 18.9 13.4 15.2 10.7 16.8 22.4 14.3 12.6 13.4 15.2 17.0 14.2 15.2 11.5 15.1 14.3 14.3 18.8 11.6 13.4 9.8 9.8 9.8 12.5 14.3 17.0 16.0 11.7 14.3 14.2 15.2 15.2 15.2 8.9 4.5 8.0 5.4 3.6 4.4 7.1 39.3 35.7 41.9 18.8 23.1 27.7 15.3 22.4 21.3 18.8 17.9 22.3 14.3 22.2 17.9 18.7 22.2 17.9 13.4 16.9 20.5 15.3 8.9 10.8 14.2 15.1 11.7 16.1 12.5 14.3 10.6 16.9 14.2 13.4 13.4 13.4 15.1 14.3 17.8 10.6 11.5 9.8 19.6 11.5 11.5 11.5 10.6 9.0 10.8 10.0 8.0 6.2 4.4 71.5 65.2 54.6 28.6 30.4 21.5 28.7 19.6 23.2 18.7 20.6 14.3 18.7 8.9 10.8 10.7 18.9 16.2 14.3 16.1 8.1 14.3 8.9 20.4 14.2 13.3 14.4 12.5 9.8 12.6 15.2 12.5 9.9 14.3 14.3 14.3 15.1 14.3 11.5 14.3 10.8 11.6 8.9 15.1 15.1 15.1 6.2 15.1 9.8 12.5 8.8 6.2 7.9 32.1 36.6 51.7 15.9 34.9 26.0 26.7 9.7 26.7 24.9 20.6 16.8 26.8 18.7 17.7 20.5 14.2 20.5 14.1 15.1 17.7 15.2 14.3 15.2 14.2 17.8 16.0 16.1 8.9 15.2 8.9 16.0 12.4 15.1 15.1 15.1 10.6 6.1 15.2 7.1 9.9 8.8 11.6 10.8 10.8 10.8 5.3 7.1 12.5 8.9 7.0 5.3 3.6 70.6 65.4 51.8 44.5 27.7 27.7 41.1 37.5 31.4 31.3 29.5 28.5 9.8 25.0 28.5 14.3 15.1 21.4 19.6 7.2 19.6 17.9 16.2 8.0 14.2 17.8 17.0 12.5 14.3 17.0 17.8 13.4 8.9 17.8 17.8 17.8 20.4 13.3 16.8 9.8 14.3 10.7 12.5 10.7 10.7 10.7 14.4 11.5 13.4 8.9 11.5 7.9 5.3 50.9 53.6 41.0 36.5 20.5 17.8 18.6 30.2 30.3 16.9 15.2 18.5 17.7 26.8 16.0 11.6 18.7 10.7 18.7 15.1 16.0 18.7 14.4 18.0 14.2 14.3 10.7 11.7 10.6 11.7 7.2 16.1 8.9 14.2 14.2 14.2 13.4 10.8 9.8 11.6 10.7 7.1 11.5 8.9 8.9 8.9 6.2 8.8 9.8 5.3 7.1 5.3 7. 70.5 63.3 43.8 39.1 34.0 29.4 24.9 25.9 28.7 19.6 22.4 27.6 18.7 14.4 21.3 11.5 10.8 15.1 13.4 16.0 16.0 10.7 16.1 19.6 14.2 10.7 13.4 9.8 8.1 12.5 10.7 15.2 8.9 9.8 9.8 9.8 13.5 14.3 17.9 8.9 11.7 11.6 9.8 8.9 8.9 8.9 6.2 8.9 4.5 8.9 7.1 7.9 7.9 41.1 32.9 46.5 24.0 27.7 25.9 23.2 25.0 25.9 24.0 27.7 18.7 27.8 21.4 20.4 14.3 24.1 15.3 24.9 16.0 20.6 21.5 16.1 12.5 14.2 15.1 15.2 19.6 20.5 11.6 17.9 20.5 22.2 16.1 16.1 16.1 17.0 17.8 15.1 16.8 14.2 15.2 13.3 17.8 17.8 17.8 17.8 9.7 11.6 13.3 7.1 5.3 5.3 51.7 51.8 48.1 29.5 22.3 26.9 35.9 22.3 35.7 19.7 24.2 14.3 17.9 23.2 14.3 16.1 23.3 18.8 16.1 20.7 8.9 7.9 17.1 12.5 14.2 15.2 8.9 14.3 13.3 15.1 16.9 9.8 18.6 17.7 17.7 17.7 13.3 13.4 9.8 13.2 14.9 15.1 14.2 7.9 7.9 7.9 10.6 6.1 6.1 7.1 12.3 5.3 4.4 55.4 49.2 53.6 29.6 16.1 21.5 21.6 19.7 18.7 20.6 14.3 16.0 19.8 21.5 16.0 10.7 13.4 16.1 12.4 13.4 13.4 12.6 16.9 16.0 14.2 12.6 19.6 19.7 6.2 12.6 9.8 7.2 8.1 5.3 5.3 5.3 6.2 8.9 9.0 9.0 10.7 6.3 7.2 2.7 2.7 2.7 8.0 5.3 7.9 8.9 8.9 7.0 4.4 74.3 68.8 45.7 32.1 26.8 18.7 17.8 22.1 15.1 32.9 17.0 15.1 20.4 19.7 15.9 16.9 18.7 8.0 15.0 18.9 5.3 16.0 14.2 15.2 14.2 22.1 9.7 14.2 14.2 18.7 17.7 13.3 16.9 15.9 15.9 15.9 18.6 14.1 11.5 18.6 14.2 14.2 8.0 13.4 13.4 13.4 10.6 10.6 7.1 8.9 10.6 7.0 7.0 53.4 48.1 24.2 31.1 19.6 21.4 32.8 32.9 16.2 18.7 11.5 18.7 20.6 17.0 20.4 12.5 11.5 11.6 22.2 14.3 8.9 13.4 9.9 9.8 14.2 15.1 16.1 14.3 18.7 17.9 15.1 14.3 13.4 15.1 15.1 15.1 9.7 11.7 12.5 15.2 11.6 10.6 12.5 11.6 11.6 11.6 10.6 9.8 5.4 9.8 8.9 3.6 6. 45.5 53.6 46.6 30.4 27.6 18.7 28.5 17.7 22.5 17.0 23.2 11.6 11.6 12.6 10.7 14.3 14.5 11.7 14.3 15.1 17.9 12.6 10.8 11.8 14.2 18.0 11.7 17.1 10.8 16.2 8.9 9.0 9.0 10.7 10.7 10.7 14.4 12.6 9.0 10.8 8.9 8.1 13.4 10.8 10.8 10.8 4.4 5.4 4.5 2.7 4.5 4.4 5.3 28.5 21.4 32.9 11.6 23.2 20.4 15.9 11.5 15.9 16.2 9.9 9.8 18.7 22.2 9.9 9.9 8.9 9.9 11.5 14.3 15.2 17.0 21.3 14.2 14.2 9.8 16.8 16.0 10.7 9.6 10.8 8.9 8.1 12.4 12.4 12.4 10.7 8.9 7.1 12.5 9.8 8.8 8.9 8.9 8.9 8.9 10.8 6.2 9.8 8.9 7.1 3.6 5.3 32.0 21.4 34.8 21.4 25.8 24.9 14.2 18.7 22.4 20.4 17.9 14.3 16.0 22.2 8.9 19.6 19.5 18.7 17.0 11.7 7.1 15.1 12.4 17.9 14.2 13.3 9.8 13.3 10.7 9.7 10.6 13.4 6.3 10.8 10.8 10.8 8.0 7.2 15.1 7.2 11.7 11.6 16.0 5.5 5.5 5.5 10.7 7.1 9.9 4.4 4.4 7.9 3."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "2 8 Table 11. SIB-FLEURS classification results (languages 1630 of 102). Best per language in bold. Model dan_Latn deu_Latn ell_Grek eng_Latn est_Latn fin_Latn fra_Latn fuv_Latn gaz_Latn gle_Latn glg_Latn guj_Gujr hau_Latn heb_Hebr hin_Deva LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 39.3 43.8 49.1 31.3 25.8 17.0 26.9 23.2 24.9 19.8 18.7 22.3 19.6 15.1 17.0 22.3 16.9 13.5 16.1 13.3 17.0 15.3 14.2 15.2 14.2 12.4 10.7 6.2 16.0 14.2 7.1 14.2 14.2 10.8 10.8 10.8 10.7 7.1 8.1 10.6 8.9 6.2 14.3 11.5 11.5 11.5 5.3 13.4 10.7 7.1 6.2 5.3 5. 71.3 68.7 52.7 41.0 29.6 33.2 36.6 26.9 29.4 33.0 24.9 26.7 14.2 15.1 31.2 15.2 17.0 17.8 22.3 16.2 19.6 16.0 14.3 13.4 14.2 13.3 16.9 16.9 15.3 12.5 14.3 16.0 17.9 13.4 13.4 13.4 12.6 18.8 17.8 9.8 15.3 18.9 17.8 16.2 16.2 16.2 11.6 14.2 16.8 8.9 15.1 5.3 5.3 29.4 42.0 38.4 27.7 17.7 18.7 23.2 18.0 14.3 19.5 13.4 15.2 16.9 17.9 9.8 22.3 9.8 16.0 8.1 8.9 10.7 9.8 7.1 14.2 14.2 20.6 11.6 5.4 8.8 11.7 7.0 7.0 5.3 7.2 7.2 7.2 9.0 9.0 4.5 4.4 12.5 9.8 7.9 7.2 7.2 7.2 9.0 3.6 8.0 1.7 4.5 7.0 4.4 70.6 68.9 57.2 28.6 20.6 21.5 16.1 25.8 22.3 31.2 26.0 16.1 16.9 20.5 19.6 20.5 21.3 16.0 12.5 18.7 18.7 19.5 11.5 20.6 14.2 10.6 14.3 15.1 25.1 14.2 21.4 15.1 18.8 15.1 15.1 15.1 16.8 19.6 10.7 14.3 13.3 12.4 18.7 13.4 13.4 13.4 8.9 8.8 12.4 9.8 13.4 5.3 4.4 34.8 36.6 49.1 28.5 27.7 19.7 21.4 19.5 17.8 22.4 25.1 13.3 12.5 14.3 16.9 14.2 14.3 13.4 17.8 16.0 12.4 15.1 17.9 15.1 14.2 16.1 20.5 17.8 13.3 16.2 15.1 10.7 8.1 14.2 14.2 14.2 16.9 14.2 18.7 12.5 6.2 10.7 8.9 8.8 8.8 8.8 15.1 12.4 13.4 11.6 8.9 5.3 4.4 40.2 32.1 47.2 28.6 21.3 26.8 18.7 33.0 24.1 22.3 24.0 17.8 19.6 20.7 16.9 14.2 16.0 15.2 16.0 16.0 13.4 15.2 17.8 12.6 14.2 9.8 11.6 14.2 15.0 9.9 11.6 9.8 8.0 10.6 10.6 10.6 10.7 8.9 8.9 15.1 10.8 14.2 5.4 17.9 17.9 17.9 8.0 7.2 6.2 6.2 5.4 5.3 4.4 81.3 74.1 57.3 34.8 20.6 27.6 22.3 33.0 24.0 29.4 26.8 20.4 21.5 17.7 17.0 22.2 21.5 12.5 13.3 13.4 10.7 9.8 17.8 11.6 14.2 17.7 15.2 20.6 13.2 20.6 21.1 15.9 16.0 16.0 16.0 16.0 17.7 13.2 12.5 13.3 14.1 15.1 15.1 12.4 12.4 12.4 18.6 8.0 9.8 8.9 8.9 5.3 3. 25.8 18.7 15.2 19.6 24.2 16.1 17.9 19.7 17.8 16.9 20.6 26.9 12.6 11.5 22.3 17.1 23.2 17.0 23.1 25.1 17.0 15.1 14.3 16.1 14.2 14.3 16.1 14.3 23.3 9.8 17.0 17.8 15.3 20.5 20.5 20.5 12.4 20.6 16.1 13.4 20.6 18.8 16.1 17.9 17.9 17.9 12.5 13.4 7.9 10.6 7.1 6.2 6.2 39.4 31.2 11.5 16.1 30.3 19.7 13.5 16.9 14.3 12.5 17.0 17.8 12.5 13.4 13.4 15.2 10.8 19.6 13.4 13.4 8.9 8.9 15.2 18.7 14.2 12.5 10.7 16.2 19.6 12.5 23.2 13.2 19.6 15.3 15.3 15.3 9.8 13.3 16.0 12.4 15.1 14.2 16.9 15.1 15.1 15.1 9.8 9.8 10.7 9.8 12.4 7.0 7.0 28.6 27.7 33.0 20.4 32.2 23.2 32.9 23.1 31.4 23.4 25.0 14.2 17.8 25.8 15.1 17.1 18.0 17.0 12.5 12.5 17.0 13.4 13.4 15.2 14.2 20.6 17.9 15.3 13.4 16.1 17.9 13.4 10.6 17.0 17.0 17.0 14.3 15.2 14.3 12.5 13.4 8.9 14.3 16.1 16.1 16.1 13.4 12.5 10.7 10.8 8.1 3.6 6.2 72.2 67.6 53.6 22.3 26.8 22.3 26.7 24.1 26.6 24.1 15.9 17.9 21.3 21.4 13.3 16.9 14.3 16.9 15.2 15.1 8.0 12.5 11.7 12.5 14.2 12.6 16.0 17.0 9.8 10.6 20.5 13.3 10.7 15.8 15.8 15.8 12.4 8.0 10.7 11.5 6.2 8.9 17.8 9.8 9.8 9.8 9.9 10.7 15.1 7.1 6.2 7.0 7.9 64.3 49.1 49.8 26.0 25.8 26.8 26.7 17.9 20.3 24.2 21.3 17.8 17.0 20.4 12.4 21.5 17.1 13.4 11.6 11.7 20.4 14.3 7.9 20.5 14.2 15.1 10.7 7.9 16.9 8.9 10.8 12.5 10.7 8.0 8.0 8.0 8.1 15.1 12.5 8.1 9.8 10.7 15.2 8.1 8.1 8.1 10.8 4.5 8.1 5.4 3.5 7.0 5.3 22.4 19.6 16.0 17.8 25.9 21.5 33.0 20.4 34.0 15.1 18.7 16.9 20.5 17.0 9.7 16.0 17.8 21.3 7.1 17.7 17.9 16.0 15.2 17.9 14.2 13.3 11.6 14.3 14.2 9.0 15.1 14.2 15.2 9.7 9.7 9.7 12.5 13.3 9.7 10.6 7.1 14.2 14.2 12.4 12.4 12.4 8.0 11.6 8.1 12.5 7.1 6.2 7. 28.5 31.2 43.6 15.9 25.8 19.6 20.6 20.4 24.2 17.9 17.7 16.9 12.5 16.0 11.6 15.1 14.3 22.4 10.7 19.6 14.3 8.9 8.9 12.5 14.2 13.3 8.0 7.1 8.1 15.2 6.3 9.7 11.6 12.5 12.5 12.5 11.6 8.9 6.3 5.3 7.9 10.6 14.3 4.4 4.4 4.4 10.6 10.7 5.4 3.5 4.4 5.3 7.0 73.2 65.2 46.5 31.3 25.1 28.7 35.8 31.2 26.8 23.1 27.0 26.0 25.0 28.5 21.5 17.0 13.4 15.1 17.9 22.3 11.6 18.6 17.9 23.3 14.2 18.7 16.1 13.3 13.5 10.7 14.3 14.3 16.1 12.5 12.5 12.5 10.8 15.1 17.0 9.8 12.5 15.2 15.1 11.5 11.5 11.5 10.7 11.6 5.3 8.9 10.7 5.3 4."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "2 9 Table 12. SIB-FLEURS classification results (languages 3145 of 102). Best per language in bold. Model hrv_Latn hun_Latn hye_Armn ibo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kam_Latn kan_Knda kat_Geor kaz_Cyrl kea_Latn khk_Cyrl LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 51.8 55.4 56.2 31.2 28.5 27.7 24.2 30.3 37.5 23.2 28.5 22.3 14.2 22.3 18.7 14.3 25.9 21.4 18.7 12.5 16.0 18.8 14.4 13.3 14.2 13.4 23.2 14.3 17.8 13.4 18.7 11.6 19.5 9.0 9.0 9.0 16.1 15.2 15.1 12.5 12.5 9.8 18.7 8.9 8.9 8.9 9.8 7.9 12.5 11.5 12.5 3.6 5. 44.6 29.4 40.1 37.4 27.7 26.8 23.2 24.8 14.2 24.1 25.8 19.6 15.3 18.7 13.2 15.1 16.8 12.5 16.8 20.5 15.0 17.8 17.0 12.4 14.2 14.3 16.0 14.2 20.3 12.4 18.6 13.3 13.4 12.3 12.3 12.3 10.6 7.1 13.2 18.6 13.2 11.4 10.7 10.6 10.6 10.6 10.6 7.9 6.2 8.9 8.9 8.8 5.3 25.9 29.4 48.4 20.5 22.3 23.1 22.4 17.8 25.9 18.7 21.3 15.2 15.2 21.4 19.6 22.3 19.7 21.3 10.6 16.9 17.0 17.8 13.3 20.6 14.2 5.3 8.0 8.1 17.9 10.6 17.0 12.5 15.2 19.7 19.7 19.7 15.3 9.0 9.8 20.6 10.8 12.5 13.4 11.6 11.6 11.6 13.4 7.0 9.7 8.0 6.2 5.3 4.4 25.0 22.3 20.4 19.6 27.6 22.3 13.5 13.5 16.0 22.3 17.8 16.0 18.8 12.6 15.1 13.4 15.3 21.3 19.6 12.5 13.4 11.7 8.9 8.1 14.2 15.3 9.9 17.1 12.5 17.0 14.3 13.4 13.3 13.5 13.5 13.5 14.3 12.5 11.7 16.8 12.5 9.1 16.1 9.8 9.8 9.8 8.0 9.8 8.1 8.0 11.5 4.4 6.2 82.9 78.4 54.4 33.8 30.3 27.6 22.4 32.1 14.2 23.2 22.3 24.0 15.2 15.1 11.5 12.5 14.2 14.2 13.2 16.9 10.8 16.9 14.2 13.3 14.2 17.0 15.2 16.0 12.5 14.3 14.2 10.6 15.2 10.6 10.6 10.6 13.4 10.7 12.5 18.7 15.1 12.5 12.5 13.3 13.3 13.3 17.0 7.1 4.4 8.9 7.9 8.8 5.3 33.0 24.0 61.6 15.1 29.5 25.9 21.3 21.4 19.7 15.8 17.8 25.0 15.2 16.1 18.7 22.3 19.6 12.5 16.0 13.3 12.5 13.4 14.2 20.7 14.2 15.3 11.6 16.0 16.8 17.9 15.9 13.3 14.2 15.1 15.1 15.1 17.8 16.9 16.9 16.9 17.0 13.4 15.1 16.8 16.8 16.8 12.5 14.2 10.6 11.5 13.2 4.4 5.3 72.3 67.9 49.1 40.9 25.9 31.1 24.9 31.9 35.7 33.0 29.5 26.7 19.7 24.9 27.6 18.7 14.2 19.6 25.8 18.8 21.3 21.4 13.3 19.7 14.2 16.0 18.7 18.8 19.6 17.0 20.4 14.2 19.6 18.6 18.6 18.6 15.1 17.7 18.8 18.6 21.3 14.2 11.5 17.0 17.0 17.0 13.4 13.3 8.1 12.4 14.2 6.2 5. 50.2 43.8 44.7 19.6 19.6 22.4 17.0 23.2 22.3 17.2 22.5 20.5 14.3 16.0 17.9 18.8 15.1 17.1 14.3 14.3 20.6 14.3 16.9 16.8 14.2 20.6 18.6 21.4 16.9 17.9 10.7 16.0 10.7 17.0 17.0 17.0 19.7 16.1 17.0 11.6 18.7 15.1 13.4 14.2 14.2 14.2 15.9 7.9 10.6 7.0 9.7 7.9 4.4 74.2 72.3 49.4 26.8 19.8 19.8 17.9 19.6 22.3 14.3 18.9 24.2 15.3 18.0 16.2 13.5 22.5 10.7 15.2 15.2 15.2 19.8 11.7 13.5 14.2 16.0 15.3 14.3 11.7 14.2 9.0 11.7 9.8 6.4 6.4 6.4 8.0 10.8 8.9 8.1 9.1 8.1 4.5 7.2 7.2 7.2 16.0 4.5 7.2 8.1 7.1 7.0 6.2 26.0 29.5 14.3 24.1 24.2 18.8 25.0 14.2 13.3 18.9 18.9 24.0 25.2 12.5 18.7 18.7 13.4 11.6 15.1 17.0 20.5 15.2 12.4 14.3 14.2 12.4 11.5 12.4 13.3 8.8 15.1 13.4 13.4 17.0 17.0 17.0 15.1 11.5 11.5 13.4 11.6 12.4 11.5 11.5 11.5 11.5 11.6 7.2 9.8 4.4 7.1 7.0 6.2 51.0 44.7 49.1 17.8 20.5 23.3 26.8 17.7 21.4 11.6 15.2 12.5 11.7 8.9 13.4 12.5 7.2 9.8 6.2 15.2 10.7 13.3 16.2 9.7 14.2 9.9 16.1 15.2 7.1 12.5 7.1 6.3 8.0 11.5 11.5 11.5 8.1 7.9 13.2 7.9 6.3 9.8 9.0 6.3 6.3 6.3 10.0 9.8 12.5 7.1 9.8 4.4 5.3 34.9 21.4 57.2 16.1 23.1 24.9 30.5 13.4 20.5 21.4 17.0 11.6 14.3 17.9 13.5 18.7 16.1 18.8 9.0 14.3 11.7 9.8 12.5 18.8 14.2 11.6 10.7 11.5 8.9 15.1 10.7 15.2 12.5 9.8 9.8 9.8 15.1 8.0 12.6 7.2 5.3 13.4 10.8 8.0 8.0 8.0 8.8 8.1 9.8 7.2 8.1 4.4 6.2 44.7 41.0 48.1 24.0 26.8 24.1 20.5 24.9 18.7 17.0 23.2 20.5 15.2 21.5 18.7 22.3 15.3 15.0 19.5 12.6 17.0 13.4 20.6 16.0 14.2 18.8 16.1 16.2 12.5 18.9 5.4 18.7 16.0 15.9 15.9 15.9 20.4 15.1 9.8 12.5 14.1 17.7 14.2 14.1 14.1 14.1 13.2 13.3 11.5 13.3 8.8 5.3 6. 72.4 65.2 36.6 28.5 32.1 25.9 33.9 26.6 25.1 20.5 18.7 16.0 19.5 19.6 16.1 13.3 15.1 13.3 16.1 25.0 14.3 12.6 13.3 12.4 14.2 12.6 13.4 9.8 9.8 13.4 16.0 13.5 15.1 9.0 9.0 9.0 16.1 11.6 13.4 14.3 14.3 10.7 10.6 17.0 17.0 17.0 16.8 8.9 11.5 8.9 7.2 7.0 2.6 20.5 24.2 34.7 23.1 31.3 18.8 18.8 21.4 25.8 22.3 25.9 18.8 18.8 15.2 17.0 13.4 10.7 13.3 14.2 12.5 15.2 7.0 16.0 17.0 14.2 12.6 18.8 16.9 15.1 16.0 13.4 19.7 13.4 16.1 16.1 16.1 11.7 18.8 14.3 11.6 11.6 12.5 7.1 12.5 12.5 12.5 12.5 9.8 9.8 12.4 7.0 6.2 4."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "3 0 Model khm_Khmr kir_Cyrl kor_Hang lao_Laoo lin_Latn lit_Latn ltz_Latn lug_Latn luo_Latn lvs_Latn mal_Mlym mar_Deva mkd_Cyrl mlt_Latn mri_Latn Table 13. SIB-FLEURS classification results (languages 4660 of 102). Best per language in bold. LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 23.3 22.3 30.4 26.8 27.7 18.7 15.2 21.4 26.9 24.3 16.9 24.9 10.6 19.7 17.8 13.5 19.7 15.2 18.7 15.3 17.0 15.2 16.9 28.7 14.2 14.2 17.0 15.3 16.1 9.9 21.5 17.0 19.6 18.8 18.8 18.8 16.1 17.9 12.4 21.5 25.0 17.0 15.1 20.5 20.5 20.5 19.6 12.5 13.3 13.3 12.4 7.0 7. 43.8 48.1 55.1 20.4 24.9 16.1 14.4 20.4 13.4 17.7 17.7 9.8 15.1 20.5 9.8 13.3 11.6 17.0 11.5 10.7 13.4 12.6 9.8 12.5 14.2 10.7 14.3 10.8 10.7 7.2 11.6 11.6 14.4 14.2 14.2 14.2 10.7 9.8 12.5 13.3 11.6 10.7 14.2 6.2 6.2 6.2 9.0 7.0 5.3 3.6 6.2 3.6 5.3 68.7 70.5 40.4 27.7 19.7 29.6 18.7 22.3 17.0 21.3 17.9 16.9 16.2 25.1 19.6 15.3 20.5 15.2 16.1 23.2 18.7 14.3 10.8 11.6 14.2 20.4 17.0 15.3 10.8 15.2 9.8 16.0 13.4 19.6 19.6 19.6 15.3 14.3 14.3 13.3 17.0 16.2 9.8 15.2 15.2 15.2 13.4 12.4 13.4 8.0 12.4 2.7 4.4 49.1 42.7 33.2 24.0 30.4 23.2 25.0 18.7 23.3 17.9 17.0 17.8 16.2 14.3 11.5 16.2 14.3 14.3 13.3 18.9 10.7 9.8 9.8 13.4 14.2 14.3 15.1 13.4 14.2 14.3 13.3 12.5 12.4 17.8 17.8 17.8 12.4 9.8 10.8 13.3 8.9 8.9 7.9 8.0 8.0 8.0 12.5 9.8 5.3 7.9 9.7 5.3 5.3 32.9 33.0 23.2 30.4 26.7 23.1 25.1 22.4 17.9 21.3 19.6 21.4 27.0 11.5 26.8 19.6 21.4 14.2 24.9 16.0 17.8 16.0 18.5 8.9 14.2 12.5 7.2 11.5 14.3 13.3 14.3 17.7 20.6 21.3 21.3 21.3 15.1 24.0 12.5 16.1 18.6 20.4 19.6 16.9 16.9 16.9 10.7 9.8 8.0 8.0 10.6 7.0 5.3 42.7 39.1 55.3 32.8 32.0 23.3 27.7 31.0 41.1 24.0 30.2 18.7 17.0 16.2 16.0 10.7 13.3 20.5 13.3 12.4 17.7 17.8 17.0 12.5 14.2 12.5 7.1 8.1 17.8 12.5 15.1 14.2 13.3 8.9 8.9 8.9 10.6 9.8 7.1 16.0 12.4 13.3 15.1 11.5 11.5 11.5 11.6 12.4 6.2 8.0 7.1 4.4 4.4 65.2 59.9 37.5 32.0 26.0 22.3 23.2 29.3 28.7 20.6 24.9 26.7 18.7 18.8 22.3 19.6 17.0 17.8 16.1 25.8 9.8 14.2 9.8 10.7 14.2 14.3 14.3 16.1 20.3 15.2 15.9 16.1 15.8 21.5 21.5 21.5 23.2 20.5 17.7 19.5 15.1 16.0 15.1 13.2 13.2 13.2 15.2 10.7 8.0 7.1 11.6 7.9 5. 21.4 24.2 35.7 20.6 27.7 25.0 11.6 10.8 23.2 26.8 19.7 19.7 24.1 19.6 16.1 15.2 17.0 17.2 13.4 11.6 13.5 14.2 21.4 12.4 14.2 15.3 12.5 18.8 16.9 18.8 18.7 18.7 18.7 13.3 13.3 13.3 12.6 9.0 8.9 10.7 11.6 15.2 14.2 15.3 15.3 15.3 17.0 11.6 10.7 14.3 7.1 6.2 5.3 36.6 29.4 21.4 19.5 25.7 22.3 25.8 20.5 30.3 25.0 22.3 15.2 20.6 16.1 15.1 17.9 17.0 12.5 21.4 15.2 17.9 21.3 16.1 18.8 14.2 9.0 10.6 11.7 18.7 9.9 14.3 15.1 16.0 12.5 12.5 12.5 7.1 15.9 13.3 16.0 16.0 14.3 14.3 16.8 16.8 16.8 9.8 9.8 6.2 8.9 5.3 2.7 7.9 39.2 41.9 52.8 34.9 35.6 26.6 27.7 29.5 25.0 32.1 34.7 22.4 25.1 10.8 18.8 14.2 18.6 13.3 23.2 18.7 13.5 19.6 11.5 12.5 14.2 10.8 12.5 12.5 14.3 18.7 13.4 12.5 11.7 17.1 17.1 17.1 16.2 15.2 16.0 13.4 12.5 13.4 13.4 15.2 15.2 15.2 17.9 12.5 7.2 13.4 10.7 4.4 6.2 38.3 33.0 44.5 25.0 21.3 25.0 16.0 19.6 18.7 15.3 23.2 14.3 20.5 13.4 13.3 19.6 19.6 20.6 11.6 12.5 16.9 9.9 13.4 15.1 14.2 16.2 8.1 13.4 16.1 17.0 19.8 16.0 11.7 16.2 16.2 16.2 13.4 15.2 11.6 14.3 9.8 12.5 11.6 9.8 9.8 9.8 14.3 10.7 11.7 9.8 8.1 5.3 6.2 56.2 52.8 41.9 34.7 27.6 32.9 15.1 33.9 19.6 24.9 29.4 24.1 19.6 23.2 17.0 16.0 18.7 20.5 17.9 9.8 12.3 15.2 17.9 14.3 14.2 15.9 19.7 16.9 13.2 13.4 15.1 9.8 8.9 17.9 17.9 17.9 16.0 16.9 15.1 13.4 13.3 12.5 17.7 13.3 13.3 13.3 8.9 10.6 8.8 12.4 10.7 7.0 4.4 49.8 58.9 64.3 32.2 25.1 26.1 28.7 28.6 19.7 25.1 24.1 25.1 28.5 12.6 20.6 13.4 13.3 11.5 17.7 15.1 14.3 21.4 11.5 14.3 14.2 15.1 16.9 12.5 15.1 16.0 8.0 14.3 19.6 8.9 8.9 8.9 6.2 12.5 14.2 10.8 11.6 8.9 12.5 14.3 14.3 14.3 11.6 11.6 8.0 9.9 8.9 5.3 4. 59.7 56.2 43.8 21.3 24.9 28.7 17.9 17.9 17.9 26.8 19.7 9.7 11.6 14.4 10.7 9.8 13.4 9.9 8.1 13.4 10.7 13.5 14.3 9.8 14.2 10.7 10.7 12.6 8.9 12.5 12.4 12.5 13.4 8.9 8.9 8.9 10.7 10.7 8.9 8.9 9.8 9.8 11.5 11.5 11.5 11.5 10.8 10.6 11.6 9.8 14.2 4.4 5.3 21.4 23.3 25.9 28.6 24.2 22.4 19.8 25.0 9.8 20.6 19.8 18.0 17.9 13.3 16.9 17.0 20.6 16.2 18.9 9.0 17.2 11.6 14.3 15.1 14.2 13.4 9.8 13.5 21.3 14.3 13.3 16.1 15.2 12.5 12.5 12.5 9.9 11.6 17.9 12.5 13.4 11.5 9.8 9.8 9.8 9.8 8.8 7.1 7.2 8.9 7.9 6.2 4."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "3 1 Table 14. SIB-FLEURS classification results (languages 6175 of 102). Best per language in bold. Model mya_Mymr nld_Latn nob_Latn npi_Deva nso_Latn nya_Latn oci_Latn ory_Orya pan_Guru pbt_Arab pes_Arab pol_Latn por_Latn ron_Latn rus_Cyrl LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 15.3 20.5 34.0 8.1 27.7 25.0 22.4 12.5 9.8 15.2 18.9 15.2 14.3 12.6 8.9 11.6 11.7 14.4 8.1 9.9 13.5 9.7 12.5 10.7 14.2 9.8 13.4 6.2 13.3 10.8 7.2 15.2 8.9 9.8 9.8 9.8 4.5 9.8 6.3 6.2 6.3 8.9 10.7 6.2 6.2 6.2 9.8 6.2 5.4 2.7 5.3 4.4 7. 71.5 66.0 52.9 31.3 32.2 30.4 28.6 25.9 26.0 19.6 24.2 21.3 14.2 18.7 17.8 21.6 19.8 17.0 20.5 11.7 16.1 17.7 20.5 12.6 14.2 18.7 16.9 17.0 22.3 19.6 13.4 17.0 21.3 22.3 22.3 22.3 18.9 17.9 17.0 17.0 20.6 19.7 17.0 10.7 10.7 10.7 8.9 8.9 5.3 9.8 8.9 6.2 7.1 42.0 50.9 46.5 38.3 30.4 27.6 20.6 27.6 14.4 27.6 21.4 19.7 11.7 20.6 22.4 11.7 13.5 15.2 12.6 19.7 15.2 17.1 10.8 16.0 14.2 10.7 10.7 9.8 12.5 13.5 10.7 15.2 17.0 16.0 16.0 16.0 13.5 19.7 12.5 12.5 15.3 16.0 5.5 12.5 12.5 12.5 8.1 5.3 6.3 5.3 5.3 3.6 5.3 55.3 46.5 42.0 26.7 24.2 20.6 18.9 28.5 14.3 12.5 19.8 15.1 17.0 10.8 10.8 13.4 10.8 16.1 14.3 12.6 17.0 8.9 11.5 10.7 14.2 12.5 12.5 14.2 10.6 9.9 10.5 11.5 12.6 13.3 13.3 13.3 9.8 13.4 14.2 3.6 10.8 8.9 6.2 9.8 9.8 9.8 5.3 7.9 12.4 8.8 8.0 4.4 4.4 23.3 23.3 20.5 17.7 30.4 26.8 14.3 24.0 35.7 21.3 22.3 11.5 14.2 14.2 14.2 15.1 20.5 24.1 16.8 10.6 16.8 14.2 17.0 10.8 14.2 12.5 15.2 17.0 11.5 17.8 9.7 20.5 11.4 11.6 11.6 11.6 15.0 10.6 9.8 16.8 9.7 15.1 9.8 13.4 13.4 13.4 11.5 8.9 9.8 12.5 10.6 5.3 6.2 23.1 26.8 45.6 17.8 33.0 25.0 33.1 21.4 18.7 25.1 15.1 22.2 17.0 17.7 11.5 17.0 17.0 19.6 19.6 12.5 8.9 18.8 17.0 10.6 14.2 15.1 15.0 15.1 12.3 9.9 12.5 11.5 15.2 11.6 11.6 11.6 13.3 10.8 12.4 7.0 15.1 11.6 12.4 15.1 15.1 15.1 16.0 10.7 8.8 8.8 7.9 5.3 6.2 56.3 55.2 30.4 26.7 26.8 29.4 20.6 25.8 29.6 26.8 20.4 18.6 13.4 17.9 18.7 20.5 19.5 17.8 18.8 22.4 14.3 16.0 15.1 14.3 14.2 12.5 11.6 12.6 17.7 12.5 12.6 22.1 18.7 15.0 15.0 15.0 17.0 18.7 18.7 16.8 21.3 17.8 10.7 14.2 14.2 14.2 15.1 13.3 9.7 10.7 9.8 4.4 6. 49.0 37.7 40.2 24.9 26.8 15.2 29.4 14.2 13.4 12.5 20.5 18.8 14.3 17.0 17.0 11.7 11.6 14.3 18.7 17.8 17.9 15.1 9.8 13.4 14.2 21.5 9.9 13.4 18.7 11.5 15.1 9.8 6.2 12.3 12.3 12.3 12.5 9.7 8.0 5.4 12.3 12.5 8.9 9.8 9.8 9.8 10.7 8.9 9.8 9.7 5.4 6.2 4.4 53.7 41.1 50.0 33.0 25.0 27.6 23.1 25.8 18.7 22.5 21.3 23.2 14.3 14.3 19.5 18.8 16.0 14.2 16.9 20.5 24.8 14.3 16.1 11.7 14.2 13.4 16.9 15.2 13.3 18.7 8.8 8.1 12.5 8.8 8.8 8.8 7.9 8.9 15.1 7.2 10.6 15.9 8.0 10.6 10.6 10.6 12.4 8.9 12.3 10.6 6.2 6.2 8.8 29.3 29.4 25.9 21.4 25.0 20.6 17.9 21.3 16.2 22.2 20.6 16.1 22.3 11.7 14.3 18.9 11.7 16.1 17.0 17.8 16.0 11.7 15.2 18.0 14.2 17.0 15.3 16.1 14.3 13.4 15.1 18.8 11.6 15.1 15.1 15.1 16.9 14.3 10.8 16.8 13.4 12.5 16.0 13.4 13.4 13.4 9.8 10.7 5.3 11.6 9.7 4.4 7.0 38.5 27.5 55.3 24.8 21.3 24.8 31.1 24.0 26.7 21.3 25.0 20.5 22.3 20.4 19.7 14.2 15.2 17.8 15.1 15.2 10.8 14.2 7.2 14.3 14.2 17.8 12.5 14.2 9.0 14.3 11.7 15.2 10.8 10.7 10.7 10.7 16.9 15.2 11.5 11.6 10.7 12.4 9.0 10.7 10.7 10.7 8.9 5.3 7.9 4.5 9.7 7.0 5.3 66.2 71.3 43.8 36.6 22.4 30.3 9.8 33.1 10.7 24.1 25.9 24.9 16.0 9.8 20.5 10.8 8.9 17.8 16.0 10.8 10.7 9.7 11.6 9.8 14.2 7.2 11.7 9.8 5.4 7.2 8.9 6.2 9.8 6.2 6.2 6.2 4.4 5.3 4.4 7.9 4.4 8.9 2.7 4.4 4.4 4.4 7.1 4.4 3.6 4.4 5.3 6.2 6.2 71.4 75.9 50.9 42.8 33.8 30.3 18.7 29.4 17.0 33.8 32.0 25.9 21.3 16.0 16.1 16.0 23.1 15.2 16.9 20.6 20.5 9.8 9.9 12.5 14.2 12.4 9.8 12.5 14.3 9.0 17.7 24.1 17.0 16.0 16.0 16.0 10.8 18.7 19.6 16.0 15.1 11.7 14.2 12.6 12.6 12.6 11.6 7.0 16.8 15.0 11.5 6.2 5. 58.0 57.2 44.9 33.2 18.0 18.8 20.8 25.2 13.5 26.0 25.1 20.7 13.5 16.1 12.5 17.0 11.7 13.6 15.2 11.7 18.9 15.3 16.1 6.3 14.2 13.4 15.1 9.8 18.8 10.8 11.7 10.8 13.4 14.3 14.3 14.3 13.4 12.5 10.8 11.7 13.4 10.7 13.4 11.6 11.6 11.6 11.6 9.8 8.0 11.6 8.1 4.4 6.2 69.7 75.0 52.7 33.0 29.5 21.5 23.2 28.6 19.6 22.4 20.6 15.1 9.7 9.8 18.8 17.0 13.4 7.9 14.2 15.2 17.9 9.8 11.6 18.0 14.2 12.5 17.1 16.0 14.3 17.0 14.2 13.3 10.6 11.6 11.6 11.6 12.5 13.3 15.9 8.9 13.3 17.8 12.4 10.6 10.6 10.6 8.9 8.9 7.2 8.9 14.3 6.2 6."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "3 2 Table 15. SIB-FLEURS classification results (languages 7690 of 102). Best per language in bold. Model slk_Latn slv_Latn sna_Latn snd_Arab som_Latn spa_Latn srp_Cyrl swe_Latn swh_Latn tam_Taml tel_Telu tgk_Cyrl tgl_Latn tha_Thai tur_Latn LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 51.8 49.0 50.1 33.1 37.5 22.3 32.3 30.4 23.3 19.6 23.2 18.9 9.8 17.0 16.9 19.7 11.7 16.9 11.6 13.3 12.5 17.1 12.5 9.8 14.2 14.3 13.5 18.9 12.3 22.5 14.2 11.6 17.8 10.7 10.7 10.7 11.7 14.3 12.5 9.7 11.6 10.7 11.6 11.6 11.6 11.6 11.7 7.2 5.3 7.1 3.5 6.2 5.3 55.3 44.5 53.6 29.3 30.3 24.1 31.2 21.3 26.7 24.0 25.0 13.2 13.4 19.6 12.4 18.6 19.6 13.4 13.4 18.7 11.6 18.8 25.1 14.3 14.2 18.8 26.0 18.8 15.9 14.2 12.5 13.4 11.6 11.6 11.6 11.6 13.4 13.4 13.3 10.6 16.0 18.8 14.3 16.0 16.0 16.0 22.3 8.0 11.5 7.9 15.9 5.3 6.2 32.3 40.2 24.2 28.6 24.2 27.8 33.1 21.6 22.3 28.7 31.3 30.6 16.1 23.3 16.1 17.8 20.7 21.4 17.9 17.9 18.7 16.1 18.9 15.2 14.2 17.1 22.2 16.0 17.9 17.0 19.5 16.8 17.9 11.6 11.6 11.6 19.6 14.2 17.0 24.2 17.9 19.6 19.7 14.3 14.3 14.3 13.5 11.6 9.8 12.5 12.5 4.4 5. 38.4 37.5 20.6 23.2 26.6 25.8 17.1 19.6 13.5 13.4 20.5 24.2 19.7 17.0 13.4 16.8 15.1 9.9 10.8 20.6 13.3 9.9 13.4 9.8 14.2 15.2 14.2 9.8 12.5 18.8 11.6 12.5 12.5 11.6 11.6 11.6 7.2 9.8 15.2 7.2 11.6 8.9 14.4 8.0 8.0 8.0 9.9 9.9 10.7 6.2 9.8 4.4 7.0 22.2 25.8 30.3 15.2 21.4 28.6 15.2 13.3 17.0 23.3 19.6 17.0 18.9 18.7 13.3 21.4 16.1 15.2 14.2 15.2 16.0 11.6 12.5 16.0 14.2 8.1 15.2 11.6 14.3 11.7 20.6 7.1 15.2 14.2 14.2 14.2 9.8 14.3 9.9 7.1 8.9 10.8 17.8 7.1 7.1 7.1 6.2 13.4 9.8 11.6 11.5 6.2 4.4 75.1 69.6 53.4 30.3 19.6 21.4 22.3 22.3 17.8 28.7 23.2 19.6 13.4 18.0 15.2 13.4 15.2 10.6 12.5 16.1 10.6 14.4 12.4 10.0 14.2 15.2 14.2 14.3 13.3 15.2 13.4 11.5 9.0 9.8 9.8 9.8 12.3 10.8 10.6 9.8 7.9 7.1 17.7 8.0 8.0 8.0 11.6 6.2 5.3 4.4 8.0 3.6 4.4 59.8 51.8 56.2 33.0 23.2 25.0 14.3 30.4 17.0 23.1 24.2 22.3 25.1 22.3 17.9 18.0 17.9 16.2 16.1 10.7 7.1 16.1 13.4 18.7 14.2 20.6 15.2 16.1 19.7 14.2 15.9 18.9 20.4 9.7 9.7 9.7 19.6 12.5 12.6 15.2 16.0 17.0 11.5 14.2 14.2 14.2 16.1 10.8 9.8 12.6 8.1 6.2 5.3 50.9 52.7 45.6 27.7 22.2 18.7 27.8 27.6 17.9 18.7 20.6 15.2 17.7 16.9 12.5 14.4 14.3 16.2 13.4 18.8 13.4 22.3 12.6 8.1 14.2 14.4 9.0 15.3 18.7 16.2 16.1 9.9 17.0 14.3 14.3 14.3 10.0 14.3 5.3 17.0 11.6 15.2 11.5 14.3 14.3 14.3 17.8 10.7 9.0 6.3 6.2 3.6 4.4 29.5 29.5 41.8 26.8 37.5 33.1 24.1 22.3 26.8 20.6 31.3 19.6 19.7 20.6 26.8 10.8 19.7 24.2 21.4 14.2 18.9 18.9 19.7 8.9 14.2 17.9 17.9 22.3 14.2 23.3 14.3 20.7 14.3 14.3 14.3 14.3 22.3 15.1 15.9 18.7 12.5 9.7 8.9 8.9 8.9 8.9 8.9 19.6 9.7 13.3 8.9 3.6 3. 24.0 31.3 41.1 27.7 26.7 24.0 34.0 25.0 26.8 22.2 18.8 24.1 24.1 19.8 18.6 17.0 18.8 18.7 22.3 17.7 15.1 11.6 16.2 11.7 14.2 10.7 13.3 8.1 11.5 10.8 16.0 16.8 12.4 13.4 13.4 13.4 17.8 10.6 12.5 11.5 11.6 9.7 10.7 11.6 11.6 11.6 11.5 10.6 7.1 7.9 6.2 7.0 6.2 50.8 50.0 48.2 29.4 23.2 24.9 26.8 25.8 26.0 23.1 14.2 22.3 16.0 16.1 11.5 20.5 15.1 17.0 14.2 14.3 17.7 16.0 17.0 13.4 14.2 15.1 10.8 11.6 15.1 17.9 12.5 16.0 12.4 9.9 9.9 9.9 17.8 17.8 13.3 15.0 11.7 15.1 17.7 14.2 14.2 14.2 14.3 9.8 8.9 14.2 8.9 5.3 6.2 34.7 28.5 42.6 28.5 26.0 24.1 21.6 15.1 18.0 19.6 25.0 18.8 22.3 25.9 14.3 9.7 17.0 14.3 14.2 9.8 15.2 13.5 23.2 14.3 14.2 10.8 17.8 10.6 9.8 13.3 12.5 7.2 15.1 11.7 11.7 11.7 11.7 9.8 8.9 9.8 12.5 8.1 8.9 11.6 11.6 11.6 11.6 12.5 10.7 8.0 9.8 6.2 7.0 50.2 43.0 37.7 26.6 20.6 15.1 17.8 16.8 22.3 18.7 14.2 15.9 16.2 17.0 16.8 18.9 16.9 16.0 17.6 17.8 15.1 18.0 15.2 14.3 14.2 10.6 18.8 19.7 9.7 18.7 14.2 15.0 5.3 15.9 15.9 15.9 13.3 15.1 13.3 11.5 9.6 16.8 9.8 11.5 11.5 11.5 13.3 9.7 12.3 12.3 10.6 6.2 5.3 72.2 65.8 44.5 34.7 32.8 24.0 22.5 30.3 26.0 20.6 16.9 22.3 12.5 17.9 14.3 19.8 12.5 17.9 10.8 17.0 13.4 13.4 16.2 15.2 14.2 20.6 19.8 12.6 12.5 16.2 13.5 11.6 10.7 11.6 11.6 11.6 8.0 16.0 11.5 8.9 8.9 10.7 11.6 5.4 5.4 5.4 7.1 10.6 11.4 9.6 12.5 6.2 3.6 43.9 49.1 49.1 33.0 28.7 24.3 28.7 22.3 24.1 26.0 25.8 24.2 16.0 21.5 20.6 22.4 17.9 19.8 22.4 12.5 12.5 16.9 23.1 15.2 14.2 11.6 17.0 14.4 15.2 12.5 13.5 17.0 14.4 10.8 10.8 10.8 11.7 12.6 11.7 10.8 15.4 11.6 10.6 12.6 12.6 12.6 5.3 7.2 11.6 7.1 7.2 6.2 2."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "3 3 Table 16. SIB-FLEURS classification results (languages 91102 of 102). Best per language in bold. Model ukr_Cyrl umb_Latn urd_Arab uzn_Latn vie_Latn wol_Latn xho_Latn yor_Latn zho_Hans zho_Hant zsm_Latn zul_Latn LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B facebook/seamless-m4t-v2-large openai/whisper-medium facebook/mms-1b-fl102 facebook/mms-1b-all OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 lyrebird/wav2clip speechbrain/m-ctc-t-large facebook/mms-1b-l1107 openai/whisper-small facebook/data2vec-audio-large-960h speechbrain/cnn14-esc50 openai/whisper-base vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/speecht5_multimodal facebook/data2vec-audio-base-960h openai/whisper-tiny Qwen/Qwen2-Audio-7B facebook/wav2vec2-lv-60-espeak-cv-ft facebook/encodec_24khz microsoft/msclap-2022 facebook/wav2vec2-xls-r-300m facebook/wav2vec2-xls-r-2b-21-to-en MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2023 google/vggish asapp/sew-d-tiny-100k-ft-ls100h google/yamnet asapp/sew-d-base-plus-400k-ft-ls100h facebook/wav2vec2-xls-r-2b facebook/hubert-large-ls960-ft microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/wav2vec2-base facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b asapp/sew-d-mid-400k-ft-ls100h microsoft/wavlm-large microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-base-960h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd facebook/wav2vec2-large laion/larger_clap_general laion/clap-htsat-fused laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 68.6 71.3 54.4 32.1 32.0 27.8 23.3 29.4 23.4 17.8 31.2 24.0 23.2 15.2 18.7 20.4 16.0 17.8 18.5 19.6 16.9 22.4 11.5 12.5 14.2 13.4 9.8 12.5 18.7 4.5 13.3 15.1 12.5 15.1 15.1 15.1 11.5 14.2 17.8 21.4 14.1 14.2 12.4 15.1 15.1 15.1 14.1 8.0 7.0 6.2 8.8 7.9 4. 17.8 20.6 15.3 17.8 27.7 25.8 8.9 18.6 22.5 29.5 20.5 20.6 19.6 12.5 13.4 18.8 10.6 16.0 15.0 16.0 13.3 12.5 14.3 21.4 14.2 12.5 10.8 10.7 9.8 17.0 14.1 10.8 10.6 9.0 9.0 9.0 9.0 6.3 10.7 15.1 8.1 15.2 13.4 8.9 8.9 8.9 6.2 7.1 7.2 5.3 4.5 5.3 7.0 55.5 57.2 43.8 21.5 24.9 23.1 21.5 23.3 13.4 19.6 18.7 17.9 16.1 10.7 13.5 19.6 19.6 11.7 14.4 9.8 17.0 13.4 8.9 12.6 14.2 8.9 12.5 12.5 16.0 8.9 13.4 16.1 15.1 19.6 19.6 19.6 12.5 16.9 14.3 20.5 13.4 17.9 13.4 17.8 17.8 17.8 11.6 9.8 9.7 13.3 9.8 6.2 4.4 51.1 40.2 58.9 23.0 31.4 31.2 25.1 19.5 12.6 18.7 23.1 16.8 17.0 18.8 15.1 13.2 11.6 17.9 15.9 16.2 11.6 7.1 13.4 18.8 14.2 12.5 16.9 12.5 15.1 8.1 14.2 10.8 8.0 8.1 8.1 8.1 16.8 10.7 12.5 13.3 11.6 11.5 12.4 10.7 10.7 10.7 12.5 5.3 9.7 7.1 7.1 2.7 3.6 69.6 66.0 50.8 30.4 22.4 22.4 25.1 25.1 23.3 15.1 23.2 17.7 21.4 16.0 16.9 14.3 24.0 22.2 17.0 20.5 15.2 13.3 21.4 13.4 14.2 23.2 15.2 16.0 13.3 10.7 14.3 16.0 18.6 16.1 16.1 16.1 10.7 18.8 14.3 14.3 22.4 16.1 16.9 19.7 19.7 19.7 10.7 12.4 12.4 15.1 11.5 4.4 5.3 34.0 33.0 25.1 14.2 28.3 18.7 23.2 12.4 23.3 20.3 20.6 14.3 21.4 18.8 11.6 17.9 15.2 19.7 14.2 12.4 16.1 20.6 10.8 9.9 14.2 19.7 13.4 11.7 10.7 19.6 8.0 10.7 17.9 10.6 10.6 10.6 6.2 8.0 13.4 6.2 9.8 7.2 11.5 9.8 9.8 9.8 7.0 7.1 7.2 7.1 7.1 7.0 4.4 31.1 25.0 19.7 15.2 22.4 24.0 33.0 23.1 30.4 26.0 19.7 17.8 18.5 14.3 17.8 19.8 14.2 9.8 18.6 16.8 19.7 23.4 19.8 18.9 14.2 8.0 16.1 19.7 9.8 15.3 20.4 16.1 15.9 18.6 18.6 18.6 18.7 13.4 14.3 17.7 16.8 10.6 15.9 16.0 16.0 16.0 14.2 14.2 9.8 16.0 13.3 3.6 4. 27.8 32.1 33.8 16.9 26.7 29.4 24.1 14.2 24.9 27.5 25.8 13.3 14.2 21.4 15.0 14.2 16.8 17.8 15.9 11.6 14.3 19.6 19.6 10.6 14.2 15.2 11.6 16.0 9.7 14.2 10.7 7.1 11.5 8.9 8.9 8.9 14.2 15.1 9.7 12.4 12.5 8.9 15.0 12.5 12.5 12.5 8.1 4.4 8.9 6.2 6.2 4.4 7.0 67.7 65.9 49.0 22.3 26.0 22.3 10.8 16.1 11.6 16.0 14.3 16.2 16.0 10.8 13.5 17.8 15.3 19.7 10.8 12.4 10.7 9.8 9.8 13.5 14.2 9.0 17.9 18.8 10.8 16.1 8.1 9.0 10.8 11.7 11.7 11.7 8.1 11.7 9.0 14.3 8.1 9.9 7.2 10.7 10.7 10.7 9.8 6.3 6.3 6.3 6.3 3.6 4.4 64.2 62.5 33.1 29.5 24.2 19.8 25.1 22.3 17.9 19.7 20.6 23.2 17.0 16.0 15.2 13.5 16.1 17.1 16.0 17.7 13.4 10.7 14.3 16.8 14.2 18.8 11.6 11.7 9.8 12.6 5.4 9.8 13.4 11.6 11.6 11.6 10.8 11.6 11.6 11.6 12.5 11.7 9.1 7.2 7.2 7.2 12.6 8.9 7.1 7.1 8.9 3.6 4.4 63.4 55.5 51.8 30.5 26.7 27.7 28.6 27.7 30.5 23.3 22.4 19.7 15.2 21.4 21.5 18.0 17.9 17.9 18.8 14.3 18.8 10.6 15.3 19.8 14.2 6.3 11.7 15.1 19.6 10.0 14.3 12.6 10.7 12.5 12.5 12.5 11.5 14.4 10.8 10.7 9.9 9.9 7.2 11.6 11.6 11.6 9.0 9.8 8.9 4.5 4.5 6.2 7.9 26.0 26.7 16.1 16.0 26.8 28.7 25.0 19.8 19.7 13.4 19.7 15.1 21.5 8.9 16.1 17.0 17.1 14.3 17.0 13.4 16.2 13.3 16.9 8.0 14.2 10.7 13.4 9.9 13.3 15.2 18.7 9.8 19.6 12.6 12.6 12.6 18.6 12.5 9.8 11.6 14.3 15.2 10.6 16.1 16.1 16.1 16.1 8.1 7.1 10.6 10.7 7.0 5."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "MAEB: Massive Audio Embedding Benchmark Table 17. VoxPopuli classification results. GenderID = gender classification, LanguageID = language identification. Both tasks are evaluated on multilingual audio samples containing English, French, Spanish, Polish, and German. Best result per task in bold."
        },
        {
            "title": "GenderID LanguageID",
            "content": "laion/larger_clap_general Qwen/Qwen2-Audio-7B laion/larger_clap_music_and_speech openai/whisper-tiny openai/whisper-small openai/whisper-base openai/whisper-medium speechbrain/m-ctc-t-large microsoft/wavlm-large facebook/wav2vec2-lv-60-espeak-cv-ft facebook/mms-1b-all openai/whisper-large-v3 facebook/wav2vec2-xls-r-2b facebook/mms-1b-l1107 facebook/mms-1b-fl102 facebook/hubert-base-ls960 facebook/hubert-large-ls960-ft facebook/seamless-m4t-v2-large facebook/data2vec-audio-large-960h microsoft/wavlm-base-sv microsoft/wavlm-base microsoft/wavlm-base-sd microsoft/speecht5_multimodal facebook/wav2vec2-xls-r-1b facebook/data2vec-audio-base-960h vitouphy/wav2vec2-xls-r-300m-phoneme asapp/sew-d-base-plus-400k-ft-ls100h microsoft/msclap-2023 laion/clap-htsat-fused facebook/wav2vec2-base MIT/ast-finetuned-audioset-10-10-0.4593 laion/clap-htsat-unfused asapp/sew-d-tiny-100k-ft-ls100h microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus microsoft/unispeech-sat-base-100h-libri-ft asapp/sew-d-mid-400k-ft-ls100h LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B google/vggish microsoft/msclap-2022 facebook/wav2vec2-base-960h google/yamnet lyrebird/wav2clip OpenMuQ/MuQ-MuLan-large speechbrain/cnn14-esc50 laion/larger_clap_music facebook/wav2vec2-large facebook/wav2vec2-xls-r-300m facebook/encodec_24khz facebook/wav2vec2-large-xlsr-53 84.6 68.2 84.4 66.6 62.8 63.6 59.2 56.0 64.4 56.2 53.2 53.0 76.0 54.0 49.8 76.4 56.2 52.4 52.2 71.8 71.8 71.8 56.8 71.2 56.8 53.0 57.6 86.2 93.2 76.2 86.8 94.4 57.6 57.2 57.2 57.2 60.0 53.8 52.2 51.8 82.0 89.6 51.0 79.4 74.2 58.6 70.0 69.2 53.8 52.0 50.2 51.6 34 84.6 99.0 81.4 96.6 99.2 98.0 99.4 99.4 89.0 97.0 99.4 98.2 74.8 95.2 97.2 67.6 86.8 89.0 87.6 65.4 65.4 65.4 78.8 62.8 73.6 77.2 71.6 39.6 32.0 48.8 37.8 30.0 66.2 64.4 64.4 64.4 58.6 63.2 64.2 63.4 32.0 24.2 58.8 27.2 31.4 41.4 29.8 28.2 30.0 29.4 25.6 19.4 3 5 Model AmbientAcoustic CREMA-D ESC50 GTZANGenre MusicGenre VehicleSound VoiceGender VoxCeleb VoxPopuliAccent Table 18. English clustering results. laion/larger_clap_music_and_speech laion/clap-htsat-unfused laion/larger_clap_general Qwen/Qwen2-Audio-7B microsoft/msclap-2023 MIT/ast-finetuned-audioset-10-10-0.4593 laion/clap-htsat-fused microsoft/msclap-2022 LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B google/vggish google/yamnet OpenMuQ/MuQ-MuLan-large lyrebird/wav2clip openai/whisper-medium openai/whisper-large-v3 openai/whisper-base openai/whisper-small openai/whisper-tiny microsoft/wavlm-large speechbrain/cnn14-esc50 facebook/hubert-base-ls960 vitouphy/wav2vec2-xls-r-300m-phoneme microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus-sv facebook/mms-1b-all microsoft/wavlm-base-sv microsoft/wavlm-base-sd microsoft/wavlm-base facebook/wav2vec2-base facebook/seamless-m4t-v2-large facebook/mms-1b-fl102 facebook/wav2vec2-xls-r-2b facebook/encodec_24khz facebook/wav2vec2-lv-60-espeak-cv-ft facebook/wav2vec2-xls-r-300m microsoft/speecht5_multimodal facebook/mms-1b-l1107 microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-large facebook/wav2vec2-xls-r-1b asapp/sew-d-tiny-100k-ft-ls100h asapp/sew-d-mid-400k-ft-ls100h facebook/wav2vec2-large-xlsr-53 speechbrain/m-ctc-t-large facebook/hubert-large-ls960-ft laion/larger_clap_music facebook/wav2vec2-base-960h facebook/data2vec-audio-base-960h asapp/sew-d-base-plus-400k-ft-ls100h facebook/data2vec-audio-large-960h 43.51 43.68 43.78 39.92 44.12 46.76 40.43 44.02 42.12 41.29 37.38 38.09 23.13 36.35 29.90 32.26 25.44 26.93 24.05 26.88 22.29 25.94 23.29 27.89 27.89 27.89 22.00 25.12 25.12 25.12 25.49 26.40 23.77 21.60 23.76 23.48 25.02 19.88 21.40 21.82 18.93 21.85 18.12 16.39 17.01 17.51 21.83 18.36 19.54 19.29 15.06 17.39 14.43 12.78 13.17 32.37 10.73 7.20 10.82 5.36 0.62 0.62 10.79 4.02 11.18 13.24 14.02 8.70 12.54 10.33 9.82 7.10 11.92 7.93 4.87 5.65 5.65 5.65 4.54 6.50 6.50 6.50 12.42 0.83 3.21 3.31 11.61 2.32 7.10 2.69 2.25 4.14 7.79 2.71 5.59 5.56 7.81 2.32 3.02 10.85 2.85 2.12 4.51 2.31 94.46 94.76 94.10 88.69 95.33 91.36 93.30 88.66 92.35 92.37 54.20 63.94 43.55 69.29 55.02 54.55 47.33 46.48 40.45 44.10 59.82 42.08 42.29 44.69 44.69 44.69 37.42 39.71 39.71 39.71 36.72 42.19 38.85 33.75 37.56 39.40 35.17 39.82 35.76 38.38 31.91 34.38 32.84 29.74 27.42 27.66 37.25 33.01 31.10 32.09 28.05 31. 63.82 55.70 65.09 73.94 61.91 61.60 47.10 45.46 62.86 59.33 61.34 61.75 76.01 44.01 38.81 32.25 33.86 34.24 32.64 36.94 22.02 35.69 28.92 35.27 35.27 35.27 27.01 26.40 26.40 26.40 26.65 25.65 24.93 29.62 25.57 21.21 12.34 23.25 28.04 17.35 16.64 16.27 19.61 15.11 14.49 19.71 19.68 18.28 19.58 18.52 16.05 21.62 45.30 40.07 43.38 41.84 36.57 47.20 37.00 25.84 45.06 45.25 39.65 40.04 53.89 29.43 26.85 24.50 24.28 24.39 23.97 21.70 14.40 18.47 19.31 20.48 20.48 20.48 18.59 16.01 16.01 16.01 12.79 18.25 15.96 5.74 11.16 11.66 7.36 18.94 16.96 14.17 8.12 7.86 14.17 11.33 10.44 12.79 13.91 10.42 15.39 16.90 13.07 16.11 4.72 2.66 3.37 5.52 2.86 13.37 4.71 7.98 3.63 2.18 4.46 0.35 0.92 3.97 1.08 1.38 2.50 1.29 12.39 0.18 2.47 0.13 1.24 1.79 1.79 1.79 0.32 0.41 0.41 0.41 3.09 0.42 0.15 0.94 3.80 2.37 2.30 0.47 0.44 0.33 1.41 0.13 0.34 0.63 1.72 0.97 0.59 3.73 0.61 1.33 0.63 0.15 71.33 68.11 26.63 13.43 31.43 14.07 40.80 53.15 10.47 3.90 26.80 21.93 0.02 0.69 13.58 11.85 13.85 12.19 11.11 15.26 14.45 14.45 21.93 5.32 5.32 5.32 17.81 12.94 12.94 12.94 6.59 9.13 11.02 21.68 0.10 7.18 21.93 0.99 2.87 11.41 21.93 21.93 9.92 22.00 21.93 19.26 3.24 1.88 9.09 6.21 17.90 4.05 0.70 1.06 0.68 0.18 0.78 1.01 0.81 0.64 6.69 7.64 0.51 0.14 0.39 0.38 1.07 1.13 1.01 1.07 0.93 0.51 0.59 0.45 0.09 0.40 0.40 0.40 0.64 0.43 0.43 0.43 0.39 0.38 0.58 0.44 0.47 0.08 0.71 0.43 0.64 0.43 0.52 1.08 0.61 0.67 0.33 0.76 0.53 0.27 0.39 0.34 0.45 0.44 10.61 9.35 11.20 4.25 9.58 7.55 9.34 8.90 3.22 3.04 10.72 6.47 6.65 6.63 3.28 3.06 2.97 2.99 2.75 3.35 4.47 2.85 4.43 2.84 2.84 2.84 2.70 3.13 3.13 3.13 4.21 3.01 2.99 2.81 3.46 7.33 2.57 6.48 3.74 3.48 3.18 3.29 3.91 2.90 3.06 3.23 3.18 6.18 3.57 2.73 2.79 2."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "MAEB: Massive Audio Embedding Benchmark Table 19. VoxPopuli gender clustering results. Task clusters audio samples by speaker gender across multilingual audio samples containing German, English, French, Spanish, and Polish. Best result in bold. Model VoxPopuliGender laion/clap-htsat-fused microsoft/msclap-2022 microsoft/msclap-2023 laion/clap-htsat-unfused google/vggish speechbrain/cnn14-esc50 laion/larger_clap_music_and_speech laion/larger_clap_general laion/larger_clap_music LCO-Embedding/LCO-Embedding-Omni-3B facebook/wav2vec2-large-xlsr-53 OpenMuQ/MuQ-MuLan-large LCO-Embedding/LCO-Embedding-Omni-7B lyrebird/wav2clip microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-xls-r-1b google/yamnet vitouphy/wav2vec2-xls-r-300m-phoneme facebook/wav2vec2-xls-r-300m facebook/mms-1b-l1107 asapp/sew-d-tiny-100k-ft-ls100h facebook/seamless-m4t-v2-large asapp/sew-d-base-plus-400k-ft-ls100h speechbrain/m-ctc-t-large Qwen/Qwen2-Audio-7B microsoft/wavlm-large facebook/hubert-base-ls960 facebook/wav2vec2-base-960h facebook/mms-1b-all facebook/data2vec-audio-base-960h microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/hubert-large-ls960-ft openai/whisper-base asapp/sew-d-mid-400k-ft-ls100h openai/whisper-tiny MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sv facebook/wav2vec2-xls-r-2b facebook/wav2vec2-large facebook/wav2vec2-lv-60-espeak-cv-ft facebook/data2vec-audio-large-960h facebook/mms-1b-fl102 openai/whisper-small microsoft/speecht5_multimodal facebook/wav2vec2-base facebook/encodec_24khz openai/whisper-large-v3 openai/whisper-medium 52.68 46.37 32.09 22.20 8.20 7.72 3.80 3.33 1.40 1.02 0.94 0.76 0.73 0.71 0.57 0.33 0.29 0.25 0.21 0.16 0.12 0.11 0.10 0.08 0.07 0.07 0.07 0.06 0.06 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.04 0.03 0.03 0.03 0.03 0.02 0.02 0.02 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0. 36 MAEB: Massive Audio Embedding Benchmark Table 20. Pair classification results. Models are evaluated on audio pair similarity tasks. Best result per task in bold."
        },
        {
            "title": "Model",
            "content": "CREMA-D ESC"
        },
        {
            "title": "VoxPopuliAccent",
            "content": "LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B microsoft/msclap-2023 laion/clap-htsat-unfused laion/larger_clap_music_and_speech laion/larger_clap_general Qwen/Qwen2-Audio-7B laion/clap-htsat-fused microsoft/msclap-2022 MIT/ast-finetuned-audioset-10-10-0.4593 speechbrain/cnn14-esc50 lyrebird/wav2clip google/yamnet microsoft/speecht5_multimodal openai/whisper-medium google/vggish openai/whisper-small OpenMuQ/MuQ-MuLan-large openai/whisper-large-v3 microsoft/wavlm-large facebook/mms-1b-all openai/whisper-base facebook/wav2vec2-lv-60-espeak-cv-ft facebook/mms-1b-fl102 microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus microsoft/wavlm-base-sd microsoft/wavlm-base microsoft/wavlm-base-sv facebook/hubert-base-ls960 microsoft/unispeech-sat-base-100h-libri-ft facebook/data2vec-audio-large-960h vitouphy/wav2vec2-xls-r-300m-phoneme speechbrain/m-ctc-t-large facebook/wav2vec2-xls-r-2b openai/whisper-tiny facebook/wav2vec2-large facebook/hubert-large-ls960-ft asapp/sew-d-tiny-100k-ft-ls100h facebook/wav2vec2-base facebook/mms-1b-l1107 facebook/wav2vec2-xls-r-300m facebook/seamless-m4t-v2-large facebook/encodec_24khz facebook/data2vec-audio-base-960h facebook/wav2vec2-base-960h facebook/wav2vec2-large-xlsr-53 asapp/sew-d-mid-400k-ft-ls100h facebook/wav2vec2-xls-r-1b asapp/sew-d-base-plus-400k-ft-ls100h laion/larger_clap_music 97.60 96.92 51.09 46.98 46.14 47.42 48.85 47.85 50.09 47.56 53.02 49.47 58.28 66.28 46.45 47.78 46.81 46.19 48.41 48.98 53.90 46.99 52.73 52.06 48.42 48.42 48.42 53.00 53.00 53.00 48.43 51.83 52.38 47.08 58.98 48.61 45.06 54.16 50.00 48.21 47.00 50.07 49.32 49.13 47.54 50.35 51.92 53.78 48.42 49.32 49.41 44.06 99.44 99.54 99.22 99.31 99.35 99.14 92.54 98.79 98.36 95.49 90.39 93.09 84.82 72.01 82.02 83.52 76.59 75.89 78.37 79.55 74.04 76.38 75.31 75.16 79.07 79.07 79.07 74.21 74.21 74.21 76.66 73.88 70.38 78.21 55.57 71.49 73.03 64.11 72.70 69.94 72.45 65.65 72.67 68.20 70.44 64.46 65.34 62.75 62.06 64.15 61.22 61.25 53.56 52.40 57.65 56.91 56.66 54.71 68.87 54.95 53.06 54.26 57.07 58.76 53.15 54.38 59.95 55.79 58.49 57.44 57.51 55.80 53.25 57.80 53.21 52.56 54.86 54.86 54.86 54.85 54.85 54.85 56.05 54.35 53.96 53.10 52.74 52.83 57.52 55.63 53.88 54.01 56.03 53.02 54.09 53.27 55.41 52.35 53.26 54.12 54.20 52.56 54.40 55.18 37 94.47 94.65 84.22 81.58 80.36 81.09 71.99 79.95 74.51 71.45 65.55 58.84 59.67 60.84 55.88 57.06 57.47 59.32 54.69 53.13 56.03 55.44 51.59 56.25 53.51 53.51 53.51 52.83 52.83 52.83 53.76 54.40 57.79 53.74 62.41 57.08 53.49 55.48 52.69 56.34 51.60 58.33 51.30 53.34 50.32 55.43 51.04 49.96 52.18 49.94 52.19 52. 50.73 50.67 52.05 53.96 53.49 53.55 52.93 53.18 52.08 51.83 52.58 52.56 52.42 53.14 55.40 54.83 52.36 52.04 51.55 52.16 51.24 51.54 55.08 51.40 50.91 50.91 50.91 51.31 51.31 51.31 51.29 51.27 50.74 51.03 51.98 50.97 51.87 51.44 50.66 51.21 52.15 51.40 50.24 51.49 50.93 50.97 51.09 51.65 50.70 51.51 50.11 51.60 MAEB: Massive Audio Embedding Benchmark Table 21. Multilabel classification results. Models are evaluated on audio tagging tasks where each sample can have multiple labels. Best result per task in bold."
        },
        {
            "title": "BirdSet",
            "content": "FSD2019Kaggle FSD50K MIT/ast-finetuned-audioset-10-10-0.4593 LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B google/yamnet Qwen/Qwen2-Audio-7B openai/whisper-medium openai/whisper-small openai/whisper-large-v3 openai/whisper-base openai/whisper-tiny laion/larger_clap_general google/vggish laion/clap-htsat-unfused laion/larger_clap_music_and_speech laion/clap-htsat-fused microsoft/msclap-2023 facebook/wav2vec2-xls-r-2b microsoft/msclap-2022 facebook/mms-1b-fl102 facebook/hubert-base-ls960 facebook/wav2vec2-xls-r-1b microsoft/wavlm-large lyrebird/wav2clip facebook/mms-1b-all facebook/data2vec-audio-large-960h microsoft/wavlm-base microsoft/wavlm-base-sd microsoft/wavlm-base-sv facebook/mms-1b-l1107 facebook/wav2vec2-lv-60-espeak-cv-ft speechbrain/m-ctc-t-large facebook/wav2vec2-base OpenMuQ/MuQ-MuLan-large microsoft/wavlm-base-plus-sv microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus facebook/seamless-m4t-v2-large microsoft/speecht5_multimodal vitouphy/wav2vec2-xls-r-300m-phoneme facebook/hubert-large-ls960-ft asapp/sew-d-tiny-100k-ft-ls100h speechbrain/cnn14-esc50 microsoft/unispeech-sat-base-100h-libri-ft facebook/data2vec-audio-base-960h facebook/wav2vec2-xls-r-300m facebook/wav2vec2-large asapp/sew-d-mid-400k-ft-ls100h facebook/wav2vec2-base-960h asapp/sew-d-base-plus-400k-ft-ls100h laion/larger_clap_music facebook/wav2vec2-large-xlsr-53 facebook/encodec_24khz facebook/wav2vec2-xls-r-2b-21-to-en 6.55 3.03 3.05 2.51 0.81 2.77 5.35 5.50 3.83 4.24 0.00 0.00 0.00 0.00 0.00 0.00 0.43 0.00 3.43 5.03 0.62 0.15 0.00 1.59 6.03 2.34 2.34 2.34 0.35 0.21 - 0.59 0.00 0.16 0.16 0.16 0.03 0.31 0.27 0.22 0.95 0.00 0.04 0.25 0.00 0.01 0.20 0.06 0.16 0.00 0.00 0.00 0.00 55.10 43.60 42.75 46.46 46.28 38.15 36.19 35.36 33.59 31.94 43.37 38.69 41.57 40.23 37.73 38.28 32.39 36.02 24.95 27.52 30.33 29.71 32.35 24.73 22.84 25.92 25.92 25.92 26.28 24.99 19.94 26.03 28.38 27.17 27.17 27.17 25.56 26.54 26.49 24.70 24.09 24.63 24.37 22.97 24.01 23.34 22.35 22.57 21.68 20.49 19.67 19.56 17.81 36.90 44.19 42.97 18.95 7.05 17.84 14.03 9.17 9.95 7.53 0.00 4.25 0.00 0.00 0.00 0.00 4.06 0.01 6.14 2.34 2.77 2.55 0.00 4.82 0.86 1.07 1.07 1.07 2.45 3.91 1.69 1.89 0.00 0.46 0.46 0.46 1.50 1.10 0.67 0.84 0.69 0.04 0.10 0.55 0.02 0.25 0.17 0.07 0.08 0.00 0.00 0.00 0.00 2.79 4.90 4.22 1.34 13.78 3.25 1.94 1.22 1.55 1.25 0.73 0.47 1.03 1.06 0.76 0.00 0.12 0.02 1.23 0.56 0.09 0.59 0.00 1.16 0.40 0.52 0.52 0.52 0.50 0.14 0.04 0.07 0.00 0.24 0.24 0.24 0.91 0.01 0.02 0.37 0.28 0.00 0.02 0.44 0.00 0.00 0.06 0.00 0.07 0.00 0.00 0.00 0.00 MAEB: Massive Audio Embedding Benchmark Table 22. Zero-shot classification results. Models classify audio using text descriptions without task-specific training. Best result per task in bold."
        },
        {
            "title": "Model",
            "content": "ESC"
        },
        {
            "title": "Ravdess",
            "content": "SpeechCmd v0.01 SpeechCmd v0.02 UrbanSound8k LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B laion/larger_clap_general microsoft/msclap-2023 laion/larger_clap_music_and_speech microsoft/msclap-2022 laion/clap-htsat-unfused laion/clap-htsat-fused lyrebird/wav2clip microsoft/speecht5_multimodal OpenMuQ/MuQ-MuLan-large Qwen/Qwen2-Audio-7B laion/larger_clap_music 87.80 87.65 90.50 89.85 82.85 80.25 81.70 74.00 40.75 1.35 2.70 1.00 2.00 31.67 26.94 17.29 15.21 17.29 13.61 13.26 14.65 11.81 12.99 14.72 14.37 13. 96.96 96.61 12.39 9.78 9.23 10.09 9.58 11.06 9.74 19.21 10.95 9.89 10.09 97.42 97.40 12.44 10.01 9.03 10.51 9.35 11.76 9.72 18.80 10.48 10.38 9.72 67.38 68.80 79.64 83.01 76.86 76.17 74.41 60.16 34.18 8.30 15.33 11.82 10.35 39 MAEB: Massive Audio Embedding Benchmark Table 23. Reranking results. Models are evaluated on audio-to-audio reranking tasks. Best result per task in bold."
        },
        {
            "title": "Model",
            "content": "ESC50 FSDnoisy18k"
        },
        {
            "title": "GTZAN",
            "content": "UrbanSound8K"
        },
        {
            "title": "VocalSound",
            "content": "LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B microsoft/msclap-2023 Qwen/Qwen2-Audio-7B MIT/ast-finetuned-audioset-10-10-0.4593 microsoft/msclap-2022 laion/clap-htsat-unfused laion/larger_clap_general laion/larger_clap_music_and_speech google/yamnet laion/clap-htsat-fused google/vggish lyrebird/wav2clip OpenMuQ/MuQ-MuLan-large speechbrain/cnn14-esc50 openai/whisper-medium openai/whisper-large-v3 openai/whisper-base microsoft/wavlm-large microsoft/wavlm-base-plus-sd microsoft/wavlm-base-plus microsoft/wavlm-base-plus-sv openai/whisper-small vitouphy/wav2vec2-xls-r-300m-phoneme facebook/hubert-base-ls960 facebook/seamless-m4t-v2-large microsoft/speecht5_multimodal openai/whisper-tiny facebook/wav2vec2-lv-60-espeak-cv-ft facebook/mms-1b-fl102 facebook/mms-1b-all microsoft/wavlm-base microsoft/wavlm-base-sd microsoft/wavlm-base-sv facebook/mms-1b-l1107 facebook/hubert-large-ls960-ft facebook/wav2vec2-base microsoft/unispeech-sat-base-100h-libri-ft facebook/wav2vec2-xls-r-2b facebook/data2vec-audio-large-960h facebook/wav2vec2-xls-r-300m facebook/encodec_24khz speechbrain/m-ctc-t-large asapp/sew-d-tiny-100k-ft-ls100h facebook/wav2vec2-xls-r-1b facebook/wav2vec2-base-960h facebook/wav2vec2-large facebook/data2vec-audio-base-960h asapp/sew-d-mid-400k-ft-ls100h facebook/wav2vec2-large-xlsr-53 asapp/sew-d-base-plus-400k-ft-ls100h laion/larger_clap_music 97.58 97.16 97.98 94.34 96.89 96.28 89.47 88.27 88.26 83.59 86.88 75.96 85.52 62.63 83.09 72.91 69.53 67.12 63.89 66.80 66.80 66.80 63.91 62.59 62.52 60.11 60.03 57.93 58.87 58.63 55.01 61.71 61.71 61.71 55.04 56.18 54.83 58.76 48.89 49.83 55.56 55.96 42.43 51.82 50.28 49.20 49.90 48.39 46.83 46.35 46.61 40.74 84.61 83.19 85.34 68.44 77.35 79.11 71.21 67.87 68.97 63.59 67.16 55.66 49.26 55.42 51.42 44.26 42.96 44.32 44.73 43.38 43.38 43.38 41.89 48.80 41.28 45.09 45.61 42.30 47.86 41.97 40.79 39.18 39.18 39.18 39.63 41.73 41.66 41.02 40.17 40.53 43.39 41.52 39.69 37.48 37.24 36.21 35.66 37.03 37.49 38.17 34.66 37.10 40 78.71 75.37 75.43 80.85 77.65 62.91 66.45 66.78 65.65 81.73 61.30 78.67 68.94 85.41 53.78 67.64 63.93 64.98 68.79 66.78 66.78 66.78 64.16 58.88 66.29 50.21 56.50 63.40 55.64 57.33 59.50 59.20 59.20 59.20 58.79 53.48 58.51 50.25 62.87 56.91 46.78 51.64 56.18 51.36 53.49 51.67 52.35 48.99 47.81 46.15 46.67 48.55 79.27 77.93 76.50 70.66 75.91 77.96 69.14 70.85 69.84 70.47 64.42 61.62 60.26 50.58 55.89 52.20 50.58 48.57 49.38 49.34 49.34 49.34 49.89 48.51 47.72 49.41 43.49 46.04 43.13 44.84 43.99 46.64 46.64 46.64 46.02 45.86 40.30 47.71 40.73 40.31 47.27 40.62 36.52 43.02 39.78 37.62 40.56 41.58 39.77 39.33 36.50 38. 89.94 89.28 72.12 80.29 65.56 59.69 52.02 53.02 53.54 41.89 49.91 38.20 38.08 38.62 46.14 46.70 45.98 44.26 40.43 38.00 38.00 38.00 44.08 44.06 39.06 48.44 45.06 40.35 41.12 43.32 43.04 35.57 35.57 35.57 38.13 39.78 40.69 37.80 35.54 40.21 34.36 34.47 47.16 37.61 35.95 41.36 35.42 37.33 35.12 33.82 35.76 34.36 Table 24. English retrieval results (datasets 19 of 27). Model AudioCaps A2T AudioCaps T2A AudioSetStrong A2T AudioSetStrong T2A CMU Arctic A2T CMU Arctic T2A Clotho A2T Clotho T2A EmoVDB A2T LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B laion/larger_clap_general laion/larger_clap_music_and_speech microsoft/msclap-2023 laion/clap-htsat-unfused microsoft/msclap-2022 laion/clap-htsat-fused OpenMuQ/MuQ-MuLan-large lyrebird/wav2clip microsoft/speecht5_multimodal Qwen/Qwen2-Audio-7B laion/larger_clap_music 56.74 54.25 76.22 74.18 53.11 75.20 64.10 66.03 1.70 13.14 0.91 0.45 0. 53.82 46.95 57.06 63.14 49.94 62.30 52.73 52.73 2.67 2.52 0.66 0.64 0.57 44.14 41.21 87.30 46.29 53.71 46.29 33.01 39.65 6.25 14.65 0.98 1.56 1.37 42.19 34.77 85.55 47.07 53.71 46.29 25.39 33.59 5.66 1.76 1.17 0.78 0.98 99.92 99.70 21.23 22.21 1.82 1.67 0.91 0.61 0.38 0.46 1.36 0.83 0.38 99.54 99.70 34.12 44.20 1.44 0.61 0.99 0.91 0.61 0.61 1.44 1.21 0.38 37.32 34.74 44.69 38.95 54.64 44.69 71.77 40.57 2.30 5.55 0.48 0.77 0. 33.89 32.70 33.72 32.39 41.93 34.18 58.73 33.57 2.04 1.15 0.48 0.45 0.47 97.37 98.34 1.80 3.94 2.63 1.93 2.00 1.93 0.55 0.41 1.11 0.48 0.28 Table 25. English retrieval results (datasets 1018 of 27). 4 1 Model EmoVDB T2A GigaSpeech A2T GigaSpeech T2A HiFiTTS A2T HiFiTTS T2A JLCorpus A2T JLCorpus T2A LibriTTS A2T LibriTTS T2A LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B laion/larger_clap_general laion/larger_clap_music_and_speech microsoft/msclap-2023 laion/clap-htsat-unfused microsoft/msclap-2022 laion/clap-htsat-fused OpenMuQ/MuQ-MuLan-large lyrebird/wav2clip microsoft/speecht5_multimodal Qwen/Qwen2-Audio-7B laion/larger_clap_music 97.24 98.69 2.42 4.98 2.42 2.35 1.18 1.93 0.62 0.35 0.62 0.41 0.35 83.11 82.96 0.13 0.15 0.19 0.09 0.19 0.15 0.12 0.07 0.21 0.06 0.07 83.29 83.13 0.16 0.21 0.10 0.36 0.16 0.18 0.01 0.09 0.39 0.09 0.07 99.67 99.67 4.33 3.00 3.00 4.00 2.67 1.33 2.00 0.67 2.67 3.00 1.67 100.00 100.00 5.33 3.67 3.33 5.33 3.67 5.00 2.33 2.33 11.67 2.33 1. 69.83 70.50 4.46 3.96 6.42 4.00 3.71 3.21 3.54 2.04 6.96 7.21 3.33 65.33 70.67 6.67 9.33 6.67 6.00 3.33 3.33 3.33 2.67 4.67 10.00 2.67 99.96 99.96 0.21 0.13 0.36 0.45 0.15 0.43 0.08 0.13 0.17 0.21 0.11 99.91 99.94 0.64 0.70 0.36 0.43 0.21 0.45 0.06 0.17 1.00 0.47 0.11 Table 26. English retrieval results (datasets 1927 of 27)."
        },
        {
            "title": "Model",
            "content": "MACS A2T MACS T2A MusicCaps A2T MusicCaps T2A SoundDescs A2T SoundDescs T2A SpokenSQuAD T2A UrbanSound8K A2T UrbanSound8K T2A LCO-Embedding/LCO-Embedding-Omni-7B LCO-Embedding/LCO-Embedding-Omni-3B laion/larger_clap_general laion/larger_clap_music_and_speech microsoft/msclap-2023 laion/clap-htsat-unfused microsoft/msclap-2022 laion/clap-htsat-fused OpenMuQ/MuQ-MuLan-large lyrebird/wav2clip microsoft/speecht5_multimodal Qwen/Qwen2-Audio-7B laion/larger_clap_music 16.03 18.32 30.03 28.24 15.52 25.19 40.46 29.52 1.02 1.78 1.53 1.27 1.27 29.77 22.65 33.08 30.53 27.23 27.99 41.73 29.26 1.78 1.78 1.53 1.53 1.27 20.93 19.69 17.36 13.36 18.48 11.70 4.51 8.48 12.38 3.88 0.12 0.14 0.12 24.83 18.83 16.49 14.02 19.22 11.89 3.27 7.54 11.19 0.61 0.16 0.21 0. 21.63 18.82 23.45 24.07 38.14 20.52 6.71 15.99 0.34 2.85 0.08 0.18 0.08 32.79 29.09 22.96 24.44 37.28 21.33 6.89 17.81 0.48 1.44 0.00 0.18 0.12 74.00 72.00 2.00 2.00 0.00 1.33 2.33 2.00 1.00 0.00 3.33 1.00 4.33 0.79 0.67 0.94 1.02 0.90 0.98 0.90 0.59 0.26 0.37 0.14 0.14 0.10 0.92 0.88 0.96 0.94 0.94 0.98 0.88 0.94 0.27 0.18 0.18 0.22 0."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "Table 27. FLEURS A2T retrieval results (languages 126 of 102). Best per language in bold. Model af am ar as ast az be bg bn bs ca ceb ckb cmn cs cy da de el en es et fa ff fi fil LCO-Embedding/LCO-Embedding-Omni-3B 88.3 41.9 98.8 43.1 100.0 82.2 98.3 91.2 59.8 87.4 99.9 80.8 47.6 100.0 89.3 35.5 65.3 100.0 61.8 100.0 100.0 57.9 37.3 62.0 50.4 80.5 LCO-Embedding/LCO-Embedding-Omni-7B 90.5 37.0 98.8 40.5 100.0 75.9 97.9 92.4 60.8 84.2 99.6 79.1 39.3 100.0 78.1 33.3 68.4 99.9 39.5 100.0 100.0 54.1 40.5 65.2 36.5 77.6 0.8 Qwen/Qwen2-Audio-7B 0.6 microsoft/speecht5_multimodal 0.3 laion/larger_clap_music_and_speech 0.3 laion/clap-htsat-fused 0.6 laion/larger_clap_general 0.5 laion/clap-htsat-unfused 0.6 lyrebird/wav2clip 0.3 microsoft/msclap-2022 0.4 OpenMuQ/MuQ-MuLan-large 0.6 laion/larger_clap_music 0.6 microsoft/msclap-2023 1.4 1.7 1.0 1.1 1.4 0.8 1.0 0.7 0.6 0.8 0.4 1.7 1.0 0.5 1.1 0.3 0.4 0.5 0.6 0.5 0.5 0.6 0.5 1.1 0.8 0.6 0.5 0.3 0.5 0.6 0.5 0.5 0.4 1.3 1.3 1.7 1.1 0.6 0.7 1.1 1.1 1.5 0.9 0. 1.1 1.1 0.5 0.8 0.5 0.5 0.3 1.0 0.7 0.5 0.4 1.2 1.4 1.2 0.8 1.1 1.2 1.1 0.5 1.0 0.5 0.3 0.9 0.9 0.3 0.6 0.7 0.5 0.1 0.7 0.6 0.6 0.7 2.0 1.1 0.6 0.2 0.9 0.6 1.2 1.2 0.6 0.8 0.9 3.4 1.4 0.9 0.9 1.5 0.5 0.5 0.5 1.2 0.8 0.8 3.6 0.7 1.2 0.4 0.9 1.4 1.0 0.6 0.8 0.6 0. 1.6 0.7 0.6 0.8 0.3 0.9 0.6 0.7 0.8 0.6 0.7 0.9 0.5 0.6 0.6 0.9 0.5 0.5 0.3 0.3 0.7 0.3 1.4 1.1 1.1 0.6 1.1 1.4 0.6 0.8 0.6 0.8 0.8 1.4 1.5 0.5 0.8 0.8 0.4 0.3 0.1 0.4 0.3 0.8 1.9 2.3 2.7 2.7 1.9 2.3 1.1 3.0 2.3 1.9 1.5 1.7 1.0 1.4 0.4 0.8 1.2 1.2 1.6 0.8 1.0 1. 2.8 2.1 1.4 0.9 0.9 1.2 1.2 0.5 0.9 0.7 1.4 1.2 1.1 0.2 0.6 0.7 0.6 0.7 0.5 0.4 0.5 0.6 1.9 0.4 1.9 0.8 1.2 1.1 0.7 0.2 0.1 0.6 0.6 1.1 0.7 0.8 0.3 0.8 0.3 0.5 0.5 0.4 0.5 0.7 0.8 0.6 0.4 0.4 0.6 1.0 0.7 0.8 0.5 0.5 0.5 0.9 1.7 1.5 0.5 1.2 0.8 0.8 0.6 0.9 0.8 0. 1.2 0.5 0.5 0.4 0.5 0.5 0.3 0.4 0.7 0.4 0.5 1.3 0.4 0.9 0.4 0.5 0.5 0.4 0.4 0.4 0.5 0.6 1.7 1.1 0.7 0.4 0.7 0.6 0.6 0.5 0.7 0.4 0.6 4 2 Table 28. FLEURS A2T retrieval results (languages 2752 of 102). Best per language in bold. Model fr ga gl gu ha he hi hr hu hy id ig is it ja jv ka kam kea kk km kn ko ky lb lg LCO-Embedding/LCO-Embedding-Omni-3B 100.0 29.2 100.0 91.8 40.4 38.3 99.0 95.1 35.5 36.6 99.9 35.5 73.9 100.0 100.0 89.3 52.6 33.9 98.0 61.4 16.0 64.0 100.0 75.3 92.7 44.5 LCO-Embedding/LCO-Embedding-Omni-7B 100.0 27.6 100.0 92.2 31.1 37.2 98.8 87.7 31.7 27.5 99.9 32.7 54.3 100.0 100.0 89.3 47.4 38.0 98.6 59.3 16.5 64.9 100.0 69.7 93.3 42.3 1.0 Qwen/Qwen2-Audio-7B 0.7 microsoft/speecht5_multimodal 1.0 laion/larger_clap_music_and_speech 0.6 laion/clap-htsat-fused 1.0 laion/larger_clap_general 1.4 laion/clap-htsat-unfused 0.7 lyrebird/wav2clip 0.8 microsoft/msclap-2022 0.6 OpenMuQ/MuQ-MuLan-large 0.8 laion/larger_clap_music 1.0 microsoft/msclap0.8 21.7 0.7 15.2 0.5 10.9 0.7 13.0 0.6 15.2 0.9 13.0 0.5 15.2 6.5 0.7 8.7 0.6 8.7 0.4 6.5 0.2 1.4 0.7 0.4 0.5 0.5 0.2 0.6 0.4 0.4 0.6 0.3 1.2 1.2 0.6 0.9 0.5 0.1 0.3 0.3 0.6 0.5 0.4 1.0 1.0 1.1 0.9 0.6 0.1 0.6 0.6 0.6 0.8 0.9 1.9 2.2 0.7 0.7 1.2 1.0 0.5 1.2 1.9 1.2 1.4 0.8 1.1 0.5 0.5 1.0 0.4 0.8 0.4 0.8 0.7 0. 1.3 0.9 0.4 0.3 0.6 0.5 0.6 0.5 0.4 0.5 0.3 1.6 0.6 0.8 0.5 0.8 0.8 0.9 0.5 0.7 0.7 0.6 1.0 1.0 1.0 0.7 0.8 1.0 0.6 0.7 0.8 1.0 0.4 1.5 0.6 0.8 0.5 0.7 0.6 1.2 0.6 0.6 0.6 0.6 0.4 0.6 0.4 1.0 0.6 0.6 0.3 0.6 0.9 0.6 0.5 0.6 0.7 0.7 0.2 1.1 0.7 0.6 0.6 1.0 0.5 0. 2.6 1.3 2.1 1.3 0.8 1.8 1.3 1.6 1.6 1.3 1.0 3.3 0.9 0.9 0.4 1.0 0.9 0.7 0.6 0.6 0.6 1.0 1.2 1.1 0.8 0.6 0.9 0.6 0.1 0.4 0.6 0.4 0.5 2.9 1.1 0.9 0.5 0.5 0.9 0.4 0.3 0.8 0.5 0.2 1.2 1.1 0.3 0.6 0.5 0.8 0.4 0.7 0.5 0.6 0.4 0.8 1.1 1.0 1.3 1.0 1.0 0.6 1.1 0.6 0.8 0. 1.5 0.3 0.7 0.5 0.9 0.7 0.7 0.5 0.9 0.5 0.4 1.2 0.9 0.7 1.1 1.4 0.8 0.6 0.8 0.8 0.6 0.8 1.3 0.9 0.4 0.8 0.6 0.5 0.4 0.5 0.3 0.5 0.4 2.9 0.9 1.1 0.9 0.9 0.8 0.8 0.6 0.3 0.8 0.9 2.5 1.6 1.2 1.2 0.9 0.7 0.7 0.7 0.4 0.7 0.4 3.1 1.2 1.0 0.8 0.7 0.5 0.6 0.5 0.7 0.6 0. Table 29. FLEURS A2T retrieval results (languages 5378 of 102). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "ln lo lt luo lv mi mk ml mn mr ms mt my nb ne nl nso ny oc om or pa pl ps pt ro"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "LCO-Embedding/LCO-Embedding-Omni-3B 43.1 54.3 51.8 66.0 57.9 23.8 95.3 55.8 31.6 80.6 99.6 89.5 6.2 92.2 80.0 100.0 43.4 43.5 91.7 97.6 64.9 85.0 98.3 59.8 100.0 96.5 LCO-Embedding/LCO-Embedding-Omni-7B 47.5 74.6 55.2 68.0 56.5 22.4 92.9 56.5 22.1 82.6 99.9 89.0 6.7 83.8 80.9 99.7 43.3 41.5 93.1 90.2 65.3 89.9 86.3 60.0 100.0 93.2 1.2 Qwen/Qwen2-Audio-7B 1.0 microsoft/speecht5_multimodal 0.8 laion/larger_clap_music_and_speech 0.9 laion/clap-htsat-fused 0.7 laion/larger_clap_general 0.1 laion/clap-htsat-unfused 0.8 lyrebird/wav2clip 1.0 microsoft/msclap-2022 0.5 OpenMuQ/MuQ-MuLan-large 0.5 laion/larger_clap_music 0.6 microsoft/msclap-2023 1.0 14.6 1.5 0.7 12.2 0.9 0.7 19.5 0.5 0.8 19.5 0.6 0.6 7.3 0.6 0.8 9.8 1.1 0.3 0.8 9.8 0.6 12.2 0.6 0.2 9.8 0.4 0.5 12.2 0.6 0.2 12.2 0.5 1.9 0.7 4.5 0.8 0.6 3.1 0.5 0.6 2.5 0.5 0.1 1.4 0.5 0.3 1.7 0.4 0.5 2.5 0.6 0.6 0.8 0.3 0.6 1.7 0.6 0.6 1.1 0.4 0.5 1.4 0.6 0.6 1.7 0.6 1.0 1.1 1.3 1.1 0.5 0.8 0.5 0.6 0.6 0.9 3.7 1.0 0.8 0.4 0.7 0.4 0.3 0.5 0.5 0.5 0.3 1.2 1.2 0.3 0.7 1.4 0.7 0.5 0.9 1.4 0.7 0. 0.8 1.2 0.5 0.8 0.8 0.5 0.7 0.9 1.2 0.7 0.7 1.1 0.5 1.2 0.5 1.1 0.5 0.5 0.8 0.5 0.7 0.5 1.0 2.0 1.6 1.0 0.2 1.4 0.8 1.6 1.0 0.8 0.6 0.4 1.7 1.0 1.5 1.0 1.3 1.3 0.8 1.5 1.0 1.5 4.0 1.7 1.7 1.0 1.5 1.2 1.7 1.2 0.7 1.0 1.7 0.6 0.5 0.9 0.4 0.7 0.4 0.4 0.5 0.5 0.5 0. 1.6 3.1 3.1 2.0 2.0 2.7 1.6 2.0 2.3 2.0 2.0 2.1 1.4 0.7 0.7 0.5 0.7 0.7 0.5 0.8 0.5 0.6 0.8 0.7 1.2 0.4 0.7 0.5 0.5 0.5 0.5 0.5 0.3 1.5 0.8 0.7 0.2 0.4 0.5 0.6 0.6 0.3 0.4 0.6 0.8 0.7 0.5 0.7 0.6 0.5 0.4 0.5 0.7 0.6 0.4 0.9 0.6 0.6 0.4 0.1 0.6 0.5 0.4 0.4 0.6 0. 0.9 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.8 0.4 0.8 1.1 0.6 0.7 0.4 0.4 0.8 1.0 0.3 0.4 0.7 0.7 1.6 1.3 0.8 0.7 0.3 0.9 0.5 0.7 0.8 0.5 0.7 2.2 1.9 2.7 1.4 1.9 1.9 1.1 1.4 1.4 1.1 1.4 Table 30. FLEURS A2T retrieval results (languages 79102 of 102). Best per language in bold. Model ru sd sk sl sn so sr sv sw ta te tg th tr uk umb ur uz vi wo xh yo yue zu LCO-Embedding/LCO-Embedding-Omni-3B 100.0 77.3 96.6 78.1 45.1 37.8 89.7 86.6 44.1 32.8 85.8 66.5 99.7 99.1 98.3 35.1 99.0 52.9 99.2 66.8 43.1 29.5 100.0 33.0 LCO-Embedding/LCO-Embedding-Omni-7B 100.0 76.6 92.2 83.8 43.7 33.3 89.6 78.1 48.9 26.2 72.0 69.3 99.7 97.7 98.9 30.1 99.0 54.5 99.1 66.8 39.4 31.5 100.0 27.9 1.3 Qwen/Qwen2-Audio-7B 0.7 microsoft/speecht5_multimodal 0.8 laion/larger_clap_music_and_speech 0.7 laion/clap-htsat-fused 1.1 laion/larger_clap_general 0.5 laion/clap-htsat-unfused 0.9 lyrebird/wav2clip 0.5 microsoft/msclap-2022 0.8 OpenMuQ/MuQ-MuLan-large 0.6 laion/larger_clap_music 0.6 microsoft/msclap-2023 0.8 1.1 0.5 0.5 1.6 1.6 0.3 2.1 1.6 1.6 1.6 3.0 2.0 2.0 1.3 0.7 2.0 1.0 2.0 1.0 1.7 1. 0.5 1.1 0.5 0.2 0.2 0.8 0.8 0.5 0.4 0.5 0.5 1.3 1.5 0.6 0.9 0.5 0.6 0.4 0.6 0.9 0.6 0.2 1.5 1.2 0.7 1.2 0.3 0.8 0.4 0.5 0.9 0.7 0.9 1.5 0.7 0.5 0.9 0.9 0.8 0.7 0.8 0.4 0.5 0.7 1.3 1.2 1.5 0.7 0.8 1.0 0.8 0.8 0.7 0.8 0.7 0.7 0.6 0.3 0.2 0.6 0.3 0.6 0.5 0.5 0.5 0. 1.0 0.5 1.6 0.5 0.8 0.6 0.5 0.7 0.6 0.6 0.8 1.0 0.8 0.8 0.9 0.4 0.6 0.5 0.5 0.4 0.5 0.6 1.6 1.3 1.1 0.9 0.7 0.8 0.7 0.7 0.9 0.5 1.1 1.8 1.8 0.8 1.0 1.0 1.2 0.8 1.6 1.2 0.8 1.4 0.5 1.4 1.0 1.4 1.0 0.5 1.0 1.0 0.3 1.2 1.0 1.9 1.5 1.3 1.1 1.7 0.4 2.1 1.3 1.1 0.8 0. 0.5 1.3 0.5 0.5 0.3 0.5 0.7 0.5 0.8 0.7 0.5 1.9 0.6 1.2 0.4 1.3 0.5 0.5 0.7 0.7 0.4 0.7 2.2 1.6 1.6 0.8 1.1 1.6 1.3 1.1 1.6 1.3 1.3 0.9 0.7 0.8 0.3 0.6 0.5 0.7 0.8 0.5 0.5 0.8 2.2 1.3 0.8 0.8 0.5 0.8 0.9 0.4 0.6 0.5 0.9 0.7 0.9 0.9 0.5 0.7 0.1 0.6 0.7 0.6 0.6 1. 1.8 0.6 1.3 0.8 1.0 0.6 0.6 0.9 0.9 0.6 0.6 1.0 1.2 0.5 0.4 0.6 0.6 0.2 0.3 0.5 0.5 0.3 1.7 1.0 0.9 0.9 0.6 0.9 0.4 0.6 0.9 0.9 0.4 Table 31. FLEURS T2A retrieval results (languages 126 of 102). Best per language in bold. Model af am ar as ast az be bg bn bs ca ceb ckb cmn cs cy da de el en es et fa ff fi fil LCO-Embedding/LCO-Embedding-Omni-3B 92.4 26.9 97.9 47.9 100.0 82.8 98.1 91.9 64.5 87.1 99.4 88.4 43.2 99.9 90.2 26.1 73.7 99.9 69.2 100.0 100.0 66.9 45.0 65.3 58.9 86.5 LCO-Embedding/LCO-Embedding-Omni-7B 90.2 25.6 98.6 42.5 99.8 76.4 97.5 93.0 62.9 82.2 98.7 81.9 38.0 100.0 79.5 26.3 68.5 99.9 44.5 100.0 100.0 55.2 43.7 54.7 39.8 80.8 0.4 Qwen/Qwen2-Audio-7B 0.5 microsoft/speecht5_multimodal 1.0 laion/larger_clap_music_and_speech 0.8 laion/larger_clap_general 0.5 lyrebird/wav2clip 0.5 laion/clap-htsat-unfused 0.5 microsoft/msclap-2023 0.7 laion/clap-htsat-fused 0.2 OpenMuQ/MuQ-MuLan-large 0.4 microsoft/msclap-2022 0.5 laion/larger_clap_music 0.4 0.7 0.8 0.5 0.3 0.9 0.7 0.2 0.3 0.5 0. 1.0 1.4 1.9 1.2 0.6 0.7 0.6 0.8 0.7 0.6 0.7 0.5 1.2 0.6 1.0 0.8 0.4 0.4 0.6 0.5 0.6 0.5 1.7 1.1 0.9 0.6 0.8 1.2 0.5 0.8 0.5 1.1 0.8 1.2 1.2 1.0 1.3 1.2 0.5 0.5 1.0 0.5 0.3 0.6 4.3 3.4 1.4 1.9 1.1 2.0 0.5 0.5 1.2 0.8 0.8 2.4 0.6 1.0 0.8 0.2 0.6 0.7 0.2 0.7 0.7 0. 0.9 0.9 1.1 0.8 0.4 0.6 0.7 0.7 0.9 0.9 0.6 0.9 0.9 0.8 0.6 0.6 0.8 0.3 0.6 0.3 0.3 0.6 1.2 1.1 0.8 1.8 0.5 0.8 0.3 0.5 0.8 0.8 0.8 0.5 1.0 0.9 0.9 0.8 0.2 0.5 0.8 0.4 0.7 0.5 3.4 1.5 3.0 4.2 1.5 2.3 2.7 3.8 1.1 2.3 1.9 2.3 1.4 1.0 1.0 1.0 1.0 1.2 0.8 1.0 1.0 1. 1.4 1.2 1.4 0.7 0.5 1.2 0.9 1.2 1.6 1.2 1.2 0.6 1.4 0.5 0.5 0.6 0.6 0.7 0.4 0.3 0.4 0.5 2.5 0.4 1.9 1.5 0.7 0.5 0.6 0.5 0.6 0.5 0.5 0.5 1.1 0.8 0.7 0.4 0.8 0.4 0.3 0.4 0.4 0.5 1.2 0.7 0.6 0.4 0.6 0.4 0.6 0.3 0.6 0.6 0.5 1.5 1.5 0.6 0.8 0.6 0.6 0.8 0.5 0.5 0.9 0. 0.8 0.9 0.5 0.5 0.4 0.5 0.5 0.7 0.4 0.5 0.4 1.0 0.6 1.7 1.1 0.3 0.9 0.6 0.9 0.4 0.4 0.5 1.0 0.7 0.7 0.4 0.9 0.8 0.5 0.3 0.4 0.4 0.5 1.3 0.5 1.2 0.5 0.4 0.2 0.4 1.1 0.7 0.6 0.4 1.7 1.0 0.6 0.4 0.4 0.5 0.3 0.4 0.6 0.5 0.5 1.3 1.1 0.9 0.9 1.3 0.7 1.1 1.3 1.5 0.6 0. Table 32. FLEURS T2A retrieval results (languages 2752 of 102). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "fr ga gl gu ha he hi hr hu hy id ig is it ja jv ka kam kea kk km kn ko ky lb lg"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "4 3 LCO-Embedding/LCO-Embedding-Omni-3B 99.9 29.8 99.9 91.8 32.5 43.8 98.3 94.6 43.1 39.4 99.9 31.9 71.7 100.0 99.7 91.5 59.4 30.8 97.5 67.8 15.4 72.1 100.0 75.1 92.9 38.3 LCO-Embedding/LCO-Embedding-Omni-7B 100.0 27.2 100.0 92.1 31.4 42.8 98.6 87.3 36.2 32.2 99.9 23.3 76.1 100.0 99.8 93.4 52.8 33.6 97.9 59.8 15.7 73.5 100.0 70.4 92.2 36.9 1.1 Qwen/Qwen2-Audio-7B 1.1 microsoft/speecht5_multimodal 1.0 laion/larger_clap_music_and_speech 1.5 laion/larger_clap_general 0.4 lyrebird/wav2clip 0.3 laion/clap-htsat-unfused 1.0 microsoft/msclap-2023 0.8 laion/clap-htsat-fused 1.4 OpenMuQ/MuQ-MuLan-large 0.8 microsoft/msclap-2022 0.7 laion/larger_clap_music 0.5 19.6 1.3 13.0 0.4 10.9 0.8 17.4 0.4 15.2 0.4 13.0 0.6 13.0 0.5 6.5 0.4 13.0 0.6 8.7 0.4 10.9 0.5 0.5 0.6 0.1 0.5 0.7 0.5 0.5 0.7 0.5 0.5 0.6 0.7 0.9 1.0 0.5 0.2 0.3 0.5 0.3 0.5 0.5 0.8 1.2 0.8 1.4 0.7 0.7 1.1 0.5 1.0 0.7 0. 1.3 1.0 0.5 0.6 0.6 0.5 0.8 0.5 0.8 0.3 0.6 1.2 1.9 1.2 1.4 1.7 1.2 1.2 1.7 1.0 0.7 1.2 1.1 0.9 1.3 1.1 0.3 0.4 0.7 0.7 0.5 0.2 0.5 0.7 0.9 1.4 0.8 0.7 0.7 0.9 0.3 1.2 1.0 0.6 0.8 0.6 0.6 0.4 0.4 0.5 0.5 0.7 0.9 0.6 0.5 0.7 1.1 0.6 0.6 0.5 0.7 0.8 0.5 0.4 0.4 0. 0.6 1.1 1.0 1.0 0.7 0.8 0.2 0.2 0.2 0.8 0.5 0.9 0.9 0.6 0.6 0.5 0.4 0.6 0.5 0.6 0.8 0.6 1.0 0.7 0.6 0.1 0.7 0.7 0.5 0.5 1.0 0.6 0.6 2.4 1.6 1.8 1.3 1.6 1.3 1.6 1.6 1.0 1.0 1.3 2.5 1.0 2.5 1.9 0.6 0.9 0.4 1.0 0.4 0.6 0.7 0.5 0.7 1.2 1.4 0.6 0.8 0.7 0.2 0.1 0.6 0. 2.3 0.9 1.3 0.4 0.9 0.4 0.8 0.8 0.6 0.5 0.5 1.3 1.2 0.5 0.4 0.5 0.5 0.5 0.5 0.4 0.5 0.5 0.5 1.0 0.5 0.6 0.8 1.0 0.5 0.8 0.8 0.6 1.1 0.4 1.0 0.8 1.2 0.6 0.6 0.6 0.7 0.6 0.7 0.6 0.8 1.0 0.4 0.5 0.4 0.8 0.4 0.4 0.4 0.5 0.5 0.9 0.8 0.5 1.2 0.6 0.9 0.8 0.5 0.5 0.3 0. 1.0 1.6 1.0 0.9 0.6 0.3 0.7 0.9 0.7 0.7 0.7 2.7 0.7 2.0 1.7 0.3 0.8 0.9 0.5 0.5 0.7 0.7 Table 33. FLEURS T2A retrieval results (languages 5378 of 102). Best per language in bold. Model ln lo lt luo lv mi mk ml mn mr ms mt my nb ne nl nso ny oc om or pa pl ps pt ro LCO-Embedding/LCO-Embedding-Omni-3B 45.6 40.7 63.2 73.0 66.4 23.2 95.0 65.2 24.1 82.5 99.6 89.1 5.2 93.8 82.0 99.7 40.0 38.2 91.0 73.2 62.2 83.4 97.0 63.5 100.0 95.7 LCO-Embedding/LCO-Embedding-Omni-7B 43.7 64.0 58.2 65.2 58.6 21.0 92.2 64.2 21.3 80.5 99.6 86.5 4.3 84.9 80.2 99.7 31.9 35.5 90.4 70.7 62.5 87.3 84.2 59.0 100.0 91.4 1.2 Qwen/Qwen2-Audio-7B 0.9 microsoft/speecht5_multimodal 1.5 laion/larger_clap_music_and_speech 1.4 laion/larger_clap_general 0.7 lyrebird/wav2clip 0.5 laion/clap-htsat-unfused 0.5 microsoft/msclap-2023 0.8 laion/clap-htsat-fused 1.0 OpenMuQ/MuQ-MuLan-large 0.9 microsoft/msclap-2022 0.6 laion/larger_clap_music 0.5 17.1 1.5 1.0 19.5 0.7 0.6 12.2 0.7 0.7 0.3 7.3 0.6 17.1 0.6 0.4 12.2 0.7 0.7 12.2 0.6 0.8 17.1 0.8 0.7 12.2 0.5 1.0 12.2 0.6 0.5 12.2 0.6 0.5 0.9 3.1 0.6 0.8 2.5 1.1 0.5 1.7 1.3 0.7 2.0 0.4 0.5 1.7 0.5 0.5 2.0 0.4 0.8 1.1 0.3 0.5 1.4 0.6 0.7 1.7 0.8 0.8 2.5 0.5 0.6 1.1 1.2 1.2 1.2 0.8 1.0 1.2 1.6 0.6 1.6 1.0 1. 2.5 1.2 2.6 1.3 0.3 0.5 0.7 0.7 0.4 1.0 0.4 0.3 0.6 0.6 0.3 0.4 0.4 0.6 0.4 0.6 0.7 0.4 0.5 1.0 0.9 1.0 0.8 1.3 0.9 0.6 0.8 0.6 0.5 2.3 3.9 2.3 3.1 2.3 2.7 2.7 2.3 3.1 2.3 1.6 1.2 0.7 0.8 0.8 0.4 1.1 1.6 1.5 0.5 0.7 0.7 0.7 0.4 0.4 0.3 0.6 0.6 0.6 0.5 0.5 0.5 0. 0.9 0.5 1.3 0.5 0.5 0.6 0.2 0.4 0.4 0.9 0.6 1.0 0.9 0.5 0.5 1.0 0.5 0.9 0.5 0.9 0.9 0.9 0.8 0.5 1.0 1.0 0.3 0.3 0.8 0.5 0.7 0.8 0.5 1.1 0.5 0.6 0.3 0.6 0.3 0.5 0.8 0.4 0.5 0.5 0.6 0.8 1.1 1.3 0.3 0.0 0.6 1.0 0.7 0.6 0.5 0.1 1.2 0.7 1.1 0.7 0.9 1.3 1.2 0.5 0.5 0. 0.3 0.8 0.6 0.5 0.5 0.3 0.7 0.7 0.4 0.4 0.5 1.0 1.5 1.0 0.4 0.7 0.8 0.4 0.6 0.8 0.4 0.7 2.0 1.5 1.5 1.5 1.2 1.2 1.0 1.2 1.0 1.7 1.2 0.9 1.3 2.4 1.2 0.7 0.9 0.9 0.7 0.7 0.8 0.7 1.4 2.2 2.5 2.2 2.2 1.1 1.4 1.6 1.1 1.9 1.4 1.0 1.5 2.1 1.9 0.8 1.5 1.5 2.1 0.6 1.5 1. Table 34. FLEURS T2A retrieval results (languages 79102 of 102). Best per language in bold. Model ru sd sk sl sn so sr sv sw ta te tg th tr uk umb ur uz vi wo xh yo yue zu LCO-Embedding/LCO-Embedding-Omni-3B 100.0 75.8 95.6 83.2 35.5 32.1 91.1 83.7 45.0 36.2 85.2 66.3 99.3 99.2 98.3 36.7 100.0 56.8 99.0 58.0 33.6 33.8 99.9 20.8 LCO-Embedding/LCO-Embedding-Omni-7B 100.0 74.8 91.5 83.2 32.3 21.6 91.0 75.4 47.4 33.3 74.8 68.0 99.7 98.0 97.5 29.8 99.0 53.8 99.2 56.1 28.8 31.9 100.0 19.3 0.8 Qwen/Qwen2-Audio-7B 1.2 microsoft/speecht5_multimodal 0.9 laion/larger_clap_music_and_speech 1.1 laion/larger_clap_general 0.7 lyrebird/wav2clip 0.6 laion/clap-htsat-unfused 0.7 microsoft/msclap-2023 0.8 laion/clap-htsat-fused 0.6 OpenMuQ/MuQ-MuLan-large 0.5 microsoft/msclap-2022 0.6 laion/larger_clap_music 1.1 1.0 0.7 0.5 0.6 0.7 0.7 0.7 0.4 0.5 0.6 2.0 2.3 1.7 1.3 2.0 1.7 2.0 2.0 2.3 2.3 1.7 0.7 1.1 0.6 0.7 0.6 0.7 0.4 0.7 0.5 0.4 0.5 1.0 1.2 1.0 0.8 0.6 1.0 0.6 1.0 0.7 0.8 0.6 1.6 0.8 1.2 0.8 0.9 0.7 0.7 0.5 0.7 0.5 0.7 2.1 1.6 1.2 0.8 1.6 0.8 1.2 1.2 1.0 0.6 1. 0.7 0.5 0.5 0.4 0.7 0.4 0.4 0.2 0.8 0.4 0.6 1.2 1.2 0.8 0.8 0.8 0.8 0.8 1.2 0.8 0.5 1.0 0.5 0.6 0.4 0.8 0.5 0.6 0.4 0.5 0.3 0.2 0.5 1.1 0.5 0.9 1.8 0.5 1.1 0.8 0.4 1.3 0.9 0.7 0.8 1.4 0.7 1.0 0.8 0.7 1.0 0.3 1.2 1.0 0.8 1.7 1.3 1.1 0.8 1.1 1.3 1.3 0.8 1.5 1.1 1. 0.4 0.6 0.5 0.5 0.5 0.6 0.3 0.4 0.5 0.7 0.5 0.5 0.8 0.5 0.8 0.6 0.7 0.5 0.5 0.6 0.6 0.6 1.4 1.2 0.8 0.8 0.6 0.5 0.4 0.8 1.2 0.5 0.6 1.3 0.5 0.8 0.9 0.6 0.7 0.4 0.6 0.8 0.2 0.6 0.9 1.0 1.3 1.6 0.6 0.6 0.9 0.8 0.5 0.5 0.6 1.9 1.3 1.3 1.6 1.6 1.3 1.3 0.8 1.3 1.1 1. 0.8 1.0 0.8 0.8 0.5 0.9 0.4 0.7 0.6 0.3 0.5 0.5 1.3 1.7 1.3 0.4 0.3 0.9 1.3 0.4 0.5 0.7 1.6 1.8 1.3 2.1 1.1 0.8 1.1 2.4 1.6 1.3 1.1 1.3 1.7 2.4 2.0 0.7 1.0 0.6 0.3 0.9 0.4 0.7 0.7 0.4 0.8 0.5 0.4 0.8 0.8 0.4 0.7 0.3 0.4"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "MAEB: Massive Audio Embedding Benchmark Table 35. JamAlt A2T retrieval results (languages 14 of 4). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "de en es fr LCO-Embedding/LCO-Embedding-Omni-3B 73.5 76.4 81.4 72.5 LCO-Embedding/LCO-Embedding-Omni-7B 66.1 71.1 77.2 71.8 0.9 0.7 1.8 1.0 microsoft/msclap-2023 0.7 0.8 1.1 1.5 Qwen/Qwen2-Audio-7B 0.7 1.7 0.8 0.8 microsoft/speecht5_multimodal 1.0 0.5 0.8 1.6 laion/larger_clap_music_and_speech 1.7 0.7 0.6 0.8 laion/larger_clap_music 0.9 0.7 0.9 1.1 laion/larger_clap_general 0.9 1.0 0.4 1.1 laion/clap-htsat-fused 0.6 0.6 1.1 0.9 laion/clap-htsat-unfused 0.5 0.9 0.5 0.8 microsoft/msclap-2022 0.4 0.5 0.4 1.0 lyrebird/wav2clip 0.7 0.8 0.4 0.2 OpenMuQ/MuQ-MuLan-large Table 36. JamAlt T2A retrieval results (languages 14 of 4). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "de en es fr LCO-Embedding/LCO-Embedding-Omni-3B 75.6 79.6 81.7 75.4 LCO-Embedding/LCO-Embedding-Omni-7B 72.4 78.6 80.8 76.0 1.2 2.0 1.7 1.8 Qwen/Qwen2-Audio-7B 1.1 3.0 1.0 0.9 microsoft/speecht5_multimodal 1.5 1.3 0.6 1.0 laion/larger_clap_music_and_speech 0.8 1.5 0.9 0.7 laion/clap-htsat-unfused 0.8 1.2 0.8 1.0 laion/larger_clap_general 0.8 1.1 0.6 0.7 microsoft/msclap-2023 0.5 0.8 0.7 1.0 laion/clap-htsat-fused 0.8 0.5 0.7 0.8 lyrebird/wav2clip 0.5 0.5 0.8 0.9 microsoft/msclap-2022 0.7 0.6 0.7 0.7 laion/larger_clap_music 0.6 0.7 0.3 0.3 OpenMuQ/MuQ-MuLan-large MAEB: Massive Audio Embedding Benchmark Table 37. JamAlt A2A retrieval results (languages 14 of 4). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "de en es fr 99.1 94.3 97.9 96.3 laion/larger_clap_music_and_speech 97.6 95.2 97.6 94.7 OpenMuQ/MuQ-MuLan-large 97.9 93.2 96.8 94.6 laion/larger_clap_general 97.7 90.2 96.2 94.4 MIT/ast-finetuned-audioset-10-10-0.4593 97.0 88.8 94.2 91.8 laion/clap-htsat-unfused 96.5 87.3 94.9 92.5 microsoft/msclap-2023 92.4 83.8 90.5 89.2 google/vggish 92.8 82.8 89.0 86.1 laion/clap-htsat-fused 92.3 82.4 89.4 84.6 microsoft/msclap-2022 88.8 75.8 83.0 88.1 facebook/encodec_24khz 84.0 74.5 82.3 78.3 google/yamnet 82.8 71.8 80.8 78.7 microsoft/wavlm-large 86.0 68.9 81.0 78.1 lyrebird/wav2clip 85.8 72.3 74.2 75.7 speechbrain/cnn14-esc50 facebook/hubert-base-ls960 81.6 70.7 79.5 76.2 LCO-Embedding/LCO-Embedding-Omni-7B 76.2 78.2 76.2 69.5 76.3 69.1 71.1 74.9 openai/whisper-large-v3 77.2 66.0 73.7 71.9 microsoft/wavlm-base-plus 77.2 66.0 73.7 71.9 microsoft/wavlm-base-plus-sv 77.2 66.0 73.7 71.9 microsoft/wavlm-base-plus-sd 75.7 69.5 70.6 72.6 openai/whisper-medium 82.3 64.5 69.3 69.4 laion/larger_clap_music 73.7 66.2 68.5 71.4 openai/whisper-small 75.5 64.5 67.2 71.8 openai/whisper-base 75.3 62.7 67.5 71.1 openai/whisper-tiny facebook/wav2vec2-base 74.5 56.9 69.9 70.5 LCO-Embedding/LCO-Embedding-Omni-3B 68.7 67.7 69.9 62.3 71.3 52.4 64.1 66.4 facebook/wav2vec2-large 72.0 56.0 62.1 62.2 facebook/wav2vec2-xls-r-300m 64.9 58.6 65.6 61.3 microsoft/wavlm-base 64.6 58.7 65.0 61.4 microsoft/wavlm-base-sv 64.6 58.7 65.0 61.4 microsoft/wavlm-base-sd 71.3 53.7 62.8 61.1 facebook/wav2vec2-xls-r-2b 65.0 56.2 61.9 63.9 vitouphy/wav2vec2-xls-r-300m-phoneme 62.8 55.6 60.3 59.5 microsoft/speecht5_multimodal 67.2 53.1 60.6 57.4 facebook/wav2vec2-xls-r-1b 60.0 52.4 56.3 54.6 facebook/seamless-m4t-v2-large 59.7 53.0 54.1 52.9 facebook/mms-1b-fl102 57.7 50.5 50.6 50.2 facebook/hubert-large-ls960-ft 53.5 50.3 52.7 52.2 facebook/mms-1b-l1107 57.7 48.8 50.8 50.6 facebook/mms-1b-all 54.4 49.2 52.0 50.4 microsoft/unispeech-sat-base-100h-libri-ft 55.4 46.9 52.4 46.7 facebook/wav2vec2-lv-60-espeak-cv-ft 53.7 44.1 48.4 46.5 facebook/wav2vec2-large-xlsr-53 49.0 43.8 49.2 45.6 asapp/sew-d-tiny-100k-ft-ls100h 51.0 41.8 47.2 47.4 speechbrain/m-ctc-t-large 48.4 44.6 49.0 44.1 facebook/data2vec-audio-base-960h 49.1 43.6 46.9 43.1 facebook/wav2vec2-base-960h 43.4 41.5 43.9 41.1 facebook/data2vec-audio-large-960h 39.9 38.6 40.9 37.2 asapp/sew-d-mid-400k-ft-ls100h 38.5 37.0 38.7 36.2 asapp/sew-d-base-plus-400k-ft-ls100h 42.7 33.9 35.8 37.9 Qwen/Qwen2-Audio-7B 7.6 9.7 4.7 5.8 facebook/wav2vec2-xls-r-2b-21-to-en Table 38. CommonVoiceMini17 A2T retrieval results (languages 113 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "ar ast be bg bn br cs cy da de el en es LCO-Embedding/LCO-Embedding-Omni-3B 94.4 98.3 99.4 69.4 58.0 67.4 86.6 38.4 56.6 100.0 46.4 99.8 100.0 LCO-Embedding/LCO-Embedding-Omni-7B 94.8 99.2 99.4 66.2 56.6 56.6 70.2 27.2 49.0 99.8 21.6 99.6 100.0 6.2 Qwen/Qwen2-Audio-7B 2.0 microsoft/speecht5_multimodal 0.4 laion/clap-htsat-unfused 1.6 microsoft/msclap-2023 1.4 microsoft/msclap-2022 1.0 laion/clap-htsat-fused 1.0 OpenMuQ/MuQ-MuLan-large 0.2 laion/larger_clap_general 1.0 lyrebird/wav2clip 0.4 laion/larger_clap_music_and_speech 1.2 laion/larger_clap_music 2.6 8.4 2.8 2.8 2.0 1.4 2.6 1.4 2.2 1.2 8.4 0.2 1.0 0.8 1.6 1.8 1.0 1.6 1.0 3.4 1.4 1.4 1.6 2.0 2.2 1.2 1.2 0.8 4.2 1.4 1.0 0.8 0.8 1.8 1.2 1.6 1.2 7.6 1.4 1.2 1.2 1.2 0.8 1.4 1.2 1.8 5.9 0.6 0.4 0.8 1.4 1.0 1.4 0.8 1.0 6.7 1.0 1.0 1.2 0.6 1.6 1.2 0.8 0.4 6.7 1.6 1.4 1.4 1.0 0.4 1.6 2.0 1.2 3.4 1.0 1.4 1.2 1.0 0.6 1.2 1.4 0.8 5.9 0.4 1.0 1.4 1.4 1.8 1.4 1.2 1.0 4.2 1.0 1.0 0.8 1.0 1.2 1.0 0.8 2.6 4.2 0.6 1.0 1.4 1.4 1.6 1.2 1.2 1.2 1.2 1.4 1.0 0.4 1.0 1.2 1.4 1.4 0.8 1.6 1.0 1.0 4.4 1.2 1.2 0.8 1.0 1.0 1.4 0.8 0.8 1.8 1.0 Table 39. CommonVoiceMini17 A2T retrieval results (languages 1426 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "et fa fi fr frold gl ha hi hu it ja ka ko LCO-Embedding/LCO-Embedding-Omni-3B 46.8 25.2 39.0 99.8 99.8 100.0 61.6 94.0 50.0 99.6 95.2 61.2 99.1 LCO-Embedding/LCO-Embedding-Omni-7B 39.4 21.4 24.2 99.4 99.4 100.0 51.2 94.4 42.4 99.6 95.8 54.4 98.8 2.4 1.4 2.0 6.0 3.4 2.8 4.4 Qwen/Qwen2-Audio-7B 1.8 0.8 1.8 2.0 1.2 1.2 1.2 microsoft/speecht5_multimodal 2.1 1.4 1.2 0.8 1.4 1.2 2.4 laion/clap-htsat-unfused 1.5 1.0 1.0 1.4 1.0 1.6 2.1 microsoft/msclap-2023 1.2 0.2 0.8 1.0 1.0 1.2 1.8 microsoft/msclap-2022 2.4 1.6 1.8 0.6 2.2 0.8 1.2 laion/clap-htsat-fused 1.5 0.8 1.2 0.6 1.2 0.6 1.8 OpenMuQ/MuQ-MuLan-large 1.8 0.6 0.8 1.2 1.0 1.0 1.5 laion/larger_clap_general 1.8 0.8 1.2 1.2 1.0 0.8 2.1 lyrebird/wav2clip 2.4 1.0 1.6 1.8 1.0 1.0 0.6 laion/larger_clap_music_and_speech 1.5 0.8 1.0 1.0 0.8 1.0 1.5 laion/larger_clap_music 2.6 2.8 2.0 4.6 1.4 1.4 3.2 2.4 1.0 1.0 1.2 0.6 1.0 1.2 2.0 1.6 1.2 1.6 1.2 0.4 1.0 1.4 0.6 0.8 1.4 0.2 1.0 0.6 0.6 0.4 0.6 1.2 1.0 0.8 1.0 0.6 1.0 0.4 1.2 0.8 1.0 1.0 0.8 1.0 4.6 2.4 0.6 1.6 0.4 0.8 0.6 1.2 0.6 0.8 1.0 3.6 1.8 1.4 1.0 0.8 1.8 0.8 1.2 1.2 0.8 0."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "4 7 4 8 Table 40. CommonVoiceMini17 A2T retrieval results (languages 2739 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "lt lv mk ml mn mr nl oc pl pt ro ru sk LCO-Embedding/LCO-Embedding-Omni-3B 62.8 43.4 76.2 57.8 20.2 78.6 99.0 90.9 88.4 99.0 86.0 99.2 82.2 LCO-Embedding/LCO-Embedding-Omni-7B 59.6 38.8 72.9 40.8 11.8 81.2 99.0 90.2 66.2 98.8 78.8 99.2 80.2 1.8 2.2 3.3 2.2 2.0 2.0 2.8 5.1 2.6 2.4 2.6 3.0 2.4 Qwen/Qwen2-Audio-7B 2.4 2.0 1.5 0.8 1.0 2.4 2.2 2.0 1.8 1.6 1.0 0.6 1.8 microsoft/speecht5_multimodal 0.8 1.4 1.5 1.6 1.2 1.4 0.8 2.4 1.8 0.6 1.0 1.6 1.0 laion/clap-htsat-unfused 1.4 0.4 1.5 1.4 0.8 1.2 1.0 2.4 1.8 1.2 1.4 1.4 0.8 microsoft/msclap-2023 1.2 1.2 0.9 1.0 1.2 1.4 0.6 2.0 1.0 1.6 1.2 0.6 0.8 microsoft/msclap-2022 0.8 1.2 2.1 0.8 0.6 1.6 1.4 3.5 1.4 1.4 0.8 1.6 2.0 laion/clap-htsat-fused 1.0 1.0 1.5 1.0 1.2 1.2 1.2 2.4 1.2 0.8 1.2 0.8 1.0 OpenMuQ/MuQ-MuLan-large 0.4 1.8 2.4 1.2 1.0 0.8 1.0 3.1 2.0 1.8 1.2 0.6 1.8 laion/larger_clap_general 1.6 1.2 1.5 1.6 1.0 1.4 1.2 1.6 1.4 1.4 1.0 1.2 0.8 lyrebird/wav2clip 1.4 0.8 3.0 0.4 0.8 1.8 0.6 3.1 1.0 0.6 0.8 1.2 0.6 laion/larger_clap_music_and_speech 1.0 0.8 1.2 0.8 1.0 1.0 1.0 2.0 1.0 1.0 1.0 1.0 1.0 laion/larger_clap_music Table 41. CommonVoiceMini17 A2T retrieval results (languages 4050 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "sl sr sv sw ta te th tr uk ur vi LCO-Embedding/LCO-Embedding-Omni-3B 59.8 58.6 68.2 53.0 42.4 91.8 92.0 72.2 86.6 82.2 84.4 LCO-Embedding/LCO-Embedding-Omni-7B 63.0 54.0 46.8 49.4 21.6 59.2 92.0 63.4 86.6 84.6 86.6 1.2 1.8 2.2 2.0 2.0 16.3 1.4 1.2 2.2 1.2 1.6 Qwen/Qwen2-Audio-7B 1.0 0.4 1.8 2.4 0.6 6.1 0.8 0.8 1.6 1.0 1.0 microsoft/speecht5_multimodal 0.8 1.2 1.0 1.4 1.0 12.2 1.2 2.0 1.2 1.2 1.4 laion/clap-htsat-unfused 2.0 2.0 0.4 1.4 1.4 10.2 1.4 1.2 1.2 1.2 0.8 microsoft/msclap-2023 0.6 1.8 1.0 1.4 1.2 14.3 1.2 1.6 1.8 1.2 0.6 microsoft/msclap-2022 0.8 1.0 1.2 1.6 0.8 10.2 0.4 1.2 1.4 1.0 1.4 laion/clap-htsat-fused 1.2 0.8 0.2 1.0 2.0 16.3 1.8 1.4 1.2 0.6 1.6 OpenMuQ/MuQ-MuLan-large 1.8 0.6 1.6 0.6 1.0 10.2 1.8 1.6 1.6 0.6 0.6 laion/larger_clap_general 1.0 1.4 1.0 1.0 1.0 10.2 0.8 0.8 1.0 1.2 0.6 lyrebird/wav2clip 0.6 0.6 1.8 1.6 0.2 4.1 1.2 1.2 0.8 1.2 1.2 laion/larger_clap_music_and_speech 1.2 0.8 1.0 1.0 1.0 10.2 1.0 1.0 1.0 1.0 1.2 laion/larger_clap_music"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "Table 42. CommonVoiceMini17 T2A retrieval results (languages 113 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "ar ast be bg bn br cs cy da de el en es LCO-Embedding/LCO-Embedding-Omni-3B 94.2 99.2 98.8 73.2 58.2 51.0 87.4 20.0 63.2 99.8 41.2 99.6 100.0 LCO-Embedding/LCO-Embedding-Omni-7B 95.0 97.5 99.0 65.6 49.4 47.0 72.6 16.2 56.0 99.6 19.4 99.8 100.0 5.6 Qwen/Qwen2-Audio-7B 1.4 microsoft/speecht5_multimodal 1.6 microsoft/msclap-2023 0.8 laion/clap-htsat-fused 0.2 laion/clap-htsat-unfused 1.6 microsoft/msclap-2022 0.6 laion/larger_clap_general 0.8 laion/larger_clap_music_and_speech 0.6 OpenMuQ/MuQ-MuLan-large 0.6 lyrebird/wav2clip 1.2 laion/larger_clap_music 1.6 8.4 3.0 1.0 2.2 1.8 2.0 2.8 2.2 3.0 1.0 5.2 1.0 9.2 1.0 1.4 1.4 2.0 1.8 1.6 1.4 1.6 0.8 2.0 0.8 10.1 1.0 1.6 1.0 1.6 1.2 1.8 1.6 1.6 1.0 0.8 0.6 4.2 1.6 1.8 0.6 1.2 1.6 1.6 1.6 1.8 1.2 1.2 1.0 6.7 0.8 1.2 1.2 1.4 1.4 1.4 1.4 1.2 1.4 1.0 1.8 6.7 1.2 1.0 1.0 1.0 0.8 0.8 0.6 0.8 0.8 1.6 0.8 7.6 1.0 1.0 1.2 1.6 1.2 0.8 1.4 0.6 1.6 1.4 1.0 4.2 1.2 1.0 0.8 0.8 1.6 0.8 1.6 2.6 0.8 0.8 1.6 3.4 1.4 0.8 0.4 1.0 1.2 1.4 1.0 0.8 1.8 0.4 1.4 5.0 0.8 0.8 0.8 1.2 1.0 0.8 1.2 0.8 1.2 0.8 1.0 4.2 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.2 0.8 1.0 4 9 Table 43. CommonVoiceMini17 T2A retrieval results (languages 1426 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "et fa fi fr frold gl ha hi hu it ja ka ko LCO-Embedding/LCO-Embedding-Omni-3B 46.6 25.6 41.6 99.6 99.6 99.8 49.7 92.8 48.8 99.6 95.0 58.8 98.8 LCO-Embedding/LCO-Embedding-Omni-7B 41.8 26.0 22.0 99.4 99.4 99.8 40.9 91.8 42.2 99.8 96.2 50.6 98.5 4.4 2.4 1.6 1.2 6.4 1.4 2.4 4.4 Qwen/Qwen2-Audio-7B 1.4 1.8 1.2 1.4 1.2 1.0 0.8 0.9 microsoft/speecht5_multimodal 0.8 0.6 1.2 1.4 1.2 0.8 0.8 2.1 microsoft/msclap-2023 1.2 1.5 1.0 1.2 0.4 1.2 1.0 1.8 laion/clap-htsat-fused 1.6 1.5 1.4 0.8 1.2 2.6 1.0 2.1 laion/clap-htsat-unfused 0.6 3.7 0.8 0.6 1.2 1.2 0.6 1.2 microsoft/msclap-2022 0.4 1.2 1.0 1.2 1.2 0.8 1.2 1.2 laion/larger_clap_general 1.0 1.2 1.0 0.6 0.8 1.0 1.0 1.2 laion/larger_clap_music_and_speech 0.8 0.9 1.0 0.6 1.0 1.0 1.0 2.1 OpenMuQ/MuQ-MuLan-large 0.8 1.8 1.0 1.4 1.2 1.2 1.0 0.9 lyrebird/wav2clip 1.0 1.5 1.0 1.2 1.0 1.0 0.8 1.8 laion/larger_clap_music 2.8 1.4 0.8 4.2 2.0 1.0 1.8 1.0 1.8 1.2 1.4 1.2 1.2 1.0 1.8 1.4 1.4 0.6 1.4 1.2 1.4 1.0 0.6 1.4 0.6 1.0 0.6 1.0 0.4 1.4 1.8 2.4 1.2 1.2 1.6 1.0 1.0 1.4 1.0 1.0 1.2 1.0 1.0 1.0 4.2 1.0 1.2 1.4 1.2 1.4 1.0 2.4 1.0 1.0 1."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "5 0 Table 44. CommonVoiceMini17 T2A retrieval results (languages 2739 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "lt lv mk ml mn mr nl oc pl pt ro ru sk LCO-Embedding/LCO-Embedding-Omni-3B 66.6 47.8 78.3 52.2 11.6 80.8 98.6 91.7 88.6 98.4 84.0 99.2 80.8 LCO-Embedding/LCO-Embedding-Omni-7B 55.6 40.4 75.0 32.4 6.6 77.0 98.4 90.2 66.4 98.0 73.8 99.0 79.0 1.8 1.4 3.6 2.8 1.6 1.4 2.8 6.3 1.6 3.4 2.8 2.2 2.8 Qwen/Qwen2-Audio-7B 2.0 2.0 1.8 1.0 0.8 0.8 1.6 1.6 1.6 1.4 1.8 1.4 1.0 microsoft/speecht5_multimodal 1.6 1.2 2.1 1.4 1.4 1.0 1.2 2.4 2.0 1.0 1.4 1.0 2.0 microsoft/msclap-2023 1.4 2.0 1.8 0.8 1.4 0.8 1.2 2.8 1.4 2.0 0.8 1.8 2.4 laion/clap-htsat-fused 0.6 1.6 1.5 1.2 0.4 1.0 1.2 3.5 1.8 2.2 1.4 1.0 1.8 laion/clap-htsat-unfused 1.6 0.6 1.8 1.8 0.8 1.2 1.0 2.0 0.8 1.0 0.8 1.6 0.6 microsoft/msclap-2022 1.4 1.0 1.5 0.8 1.4 1.0 0.2 3.1 1.0 1.2 0.6 1.6 2.0 laion/larger_clap_general 1.4 0.4 2.1 0.6 1.0 1.0 1.2 2.4 0.6 1.2 2.0 1.4 1.8 laion/larger_clap_music_and_speech 0.8 1.2 2.1 1.2 1.0 0.8 1.0 2.0 0.8 0.8 0.8 1.4 1.0 OpenMuQ/MuQ-MuLan-large 1.0 0.8 1.5 1.2 0.8 1.2 1.2 2.8 1.2 1.0 1.0 0.8 1.2 lyrebird/wav2clip 1.0 0.8 1.5 1.0 1.0 1.0 1.0 2.0 1.0 0.8 0.8 1.0 1.0 laion/larger_clap_music Table 45. CommonVoiceMini17 T2A retrieval results (languages 4050 of 50). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "sl sr sv sw ta te th tr uk ur vi LCO-Embedding/LCO-Embedding-Omni-3B 65.2 49.8 67.0 47.0 29.8 73.5 90.4 72.2 91.4 80.8 81.8 LCO-Embedding/LCO-Embedding-Omni-7B 66.4 44.6 50.8 41.2 23.2 61.2 90.8 63.6 86.6 83.4 84.6 1.4 1.0 2.4 1.2 0.8 14.3 1.6 1.2 2.2 2.4 1.6 Qwen/Qwen2-Audio-7B 1.0 1.2 1.4 1.8 1.0 14.3 0.8 1.4 0.8 0.8 1.2 microsoft/speecht5_multimodal 2.0 1.8 1.2 0.6 0.8 8.2 1.4 0.8 1.2 0.8 0.8 microsoft/msclap-2023 0.6 0.2 1.2 1.4 1.6 10.2 1.4 1.8 0.8 1.0 1.8 laion/clap-htsat-fused 0.4 1.4 1.0 1.4 0.8 8.2 1.2 1.6 1.2 1.0 1.4 laion/clap-htsat-unfused 0.4 1.6 1.4 0.6 2.0 12.2 1.0 1.0 1.6 0.6 1.8 microsoft/msclap-2022 1.6 0.8 1.0 1.0 1.0 12.2 1.2 0.8 1.0 1.0 1.0 laion/larger_clap_general 1.0 1.0 1.8 0.8 1.2 8.2 0.8 1.0 0.8 1.2 0.8 laion/larger_clap_music_and_speech 1.2 0.8 1.2 1.6 1.2 12.2 1.2 1.8 1.6 0.8 0.8 OpenMuQ/MuQ-MuLan-large 1.0 1.6 0.8 1.0 1.0 10.2 1.0 1.0 1.4 1.4 1.0 lyrebird/wav2clip 1.0 0.8 1.0 1.0 1.0 10.2 1.0 1.0 1.0 1.0 1.0 laion/larger_clap_music"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "Table 46. CommonVoiceMini21 A2T retrieval results (languages 129 of 114). Best per language in bold. Model ab af am ar as ast az ba bas be bg bn br ca ckb cnh cs cv cy da dav de dv dyu el en eo es et LCO-Embedding/LCO-Embedding-Omni-3B 53.5 85.5 55.0 92.5 41.5 99.4 83.7 52.0 74.5 99.0 83.0 71.5 79.0 98.0 65.5 66.0 83.5 76.5 45.5 63.0 63.5 99.5 14.0 93.7 51.5 98.5 91.0 100.0 52.0 LCO-Embedding/LCO-Embedding-Omni-7B 44.0 85.5 35.5 93.0 26.5 97.5 81.5 46.5 69.5 98.0 79.0 64.5 66.5 96.5 47.5 62.5 69.0 65.5 33.5 56.0 53.5 99.5 5.5 88.9 30.5 98.5 88.5 100.0 49.0 5.0 4.0 Qwen/Qwen2-Audio-7B 4.0 2.0 microsoft/speecht5_multimodal 2.5 3.5 laion/clap-htsat-unfused 2.5 3.5 laion/clap-htsat-fused 2.5 2.5 OpenMuQ/MuQ-MuLan-large 2.0 2.5 microsoft/msclap-2023 3.5 1.0 laion/larger_clap_general 2.0 2.5 microsoft/msclap-2022 2.0 2.5 laion/larger_clap_music 3.0 3.0 lyrebird/wav2clip 2.0 2.5 laion/larger_clap_music_and_speech 6.0 6.3 4.5 1.5 1.5 9.5 2.5 12.7 3.0 1.5 4.0 9.5 1.5 11.1 3.0 1.5 2.5 7.9 2.5 6.3 1.5 3.0 11.1 2.5 2.0 7.9 2.5 3.0 3.0 7.9 2.5 12.7 2.0 9.6 12.0 3.0 3.0 7.6 5.7 4.5 4.3 3.2 3.0 2.2 5.1 2.0 5.4 3.8 2.0 7.6 3.8 2.0 6.5 3.8 3.0 7.6 3.8 3.0 5.4 3.2 4.5 5.4 3.2 1.5 4.3 3.8 9.5 2.5 3.0 1.5 3.0 1.0 5.5 1.5 2.5 3.0 1.5 8.5 6.0 4.0 3.5 1.5 3.5 1.0 2.5 3.0 2.0 4.0 6.5 4.5 3.5 4.0 2.0 3.0 3.0 3.5 2.5 2.5 2.0 3.5 4.5 3.5 4.5 2.5 3.0 3.5 3.5 2.5 3.5 3. 5.0 3.0 3.5 3.0 3.0 2.0 4.0 2.0 2.5 1.5 1.0 5.5 5.0 3.5 3.5 5.0 4.5 2.0 2.5 2.5 2.0 3.0 6.5 4.0 4.5 4.0 2.5 3.0 3.0 2.5 2.0 2.5 3.0 5.0 7.0 3.0 2.0 3.5 3.0 3.5 2.5 3.0 2.0 0.5 8.0 4.0 3.5 1.5 2.5 3.0 2.0 3.5 2.5 2.0 5.5 6.0 2.0 3.0 1.5 1.5 2.5 3.0 3.0 2.0 2.5 2. 5.0 2.5 2.5 3.0 2.5 2.0 3.5 3.0 2.5 1.0 2.0 7.7 6.0 5.1 5.1 4.3 5.1 1.7 4.3 4.3 4.3 6.0 5.0 3.0 1.5 2.0 4.0 1.5 2.0 2.5 2.5 2.0 1.5 4.5 3.0 3.5 2.5 3.0 1.5 2.5 3.0 2.5 1.5 1.5 7.0 3.5 2.0 1.5 2.5 3.0 2.0 3.0 2.5 0.5 1.0 5.5 3.0 4.0 3.5 4.5 3.0 2.5 3.5 2.5 3.0 3. 4.0 3.0 1.5 1.5 4.0 2.5 3.0 2.0 2.5 0.5 2.0 4.0 4.5 2.5 1.5 3.0 3.0 3.0 2.5 2.5 2.5 1.5 3.5 2.0 6.0 3.5 3.5 3.0 3.5 3.0 2.5 4.0 1.5 4.0 2.0 4.0 1.5 1.5 2.5 5.5 2.0 2.5 1.5 1.5 4.5 2.5 1.5 3.5 3.0 2.5 4.5 1.5 2.0 2.5 2.5 Table 47. CommonVoiceMini21 A2T retrieval results (languages 3058 of 114). Best per language in bold. Model eu fa fi fr fy ga gl gn ha he hi hsb hu hy ia id it ja ka kab kk kln kmr ko ky lg lij lt ltg LCO-Embedding/LCO-Embedding-Omni-3B 75.0 38.5 53.5 100.0 93.0 41.0 100.0 76.0 69.5 40.0 97.0 72.5 56.0 49.5 99.5 96.5 100.0 93.0 72.0 52.5 51.0 68.0 75.0 99.5 61.0 57.5 95.5 61.2 67.0 LCO-Embedding/LCO-Embedding-Omni-7B 69.0 33.5 33.0 100.0 92.0 35.5 100.0 58.0 55.5 31.5 96.0 73.5 45.5 36.5 99.5 95.5 100.0 94.5 63.0 42.5 31.5 57.0 59.0 98.0 57.0 46.0 92.5 63.6 56.4 2.2 5.5 Qwen/Qwen2-Audio-7B 1.8 2.5 microsoft/speecht5_multimodal 2.4 1.0 laion/clap-htsat-unfused 2.0 2.0 laion/clap-htsat-fused 0.8 3.0 OpenMuQ/MuQ-MuLan-large 1.0 2.0 microsoft/msclap-2023 0.8 3.0 laion/larger_clap_general 1.6 3.0 microsoft/msclap-2022 1.0 3.0 laion/larger_clap_music 1.0 3.0 lyrebird/wav2clip 1.0 2.5 laion/larger_clap_music_and_speech 5.0 3.5 3.5 2.5 2.5 3.0 3.0 3.0 2.0 2.0 3.5 3.5 1.5 2.5 2.5 1.5 3.5 2.0 3.0 2.5 2.0 2.5 6.0 5.0 2.5 2.5 3.5 3.0 3.0 2.0 2.5 3.5 2.5 4.0 2.5 2.0 2.0 1.5 1.5 2.5 3.0 3.0 3.0 2.0 1.8 2.4 1.2 1.4 1.8 1.2 1.2 0.8 1.0 1.2 1.4 5.5 4.0 4.0 3.0 4.0 3.0 3.0 2.0 2.5 3.0 2. 4.0 3.5 2.5 3.0 3.5 3.0 1.0 3.0 2.5 1.5 1.0 5.5 4.5 2.5 4.0 3.0 3.5 3.0 4.0 2.5 1.5 2.5 4.0 2.0 4.5 3.0 3.0 3.5 3.0 3.0 2.5 2.5 2.0 8.5 4.5 2.5 3.0 3.5 1.5 1.5 2.0 2.5 3.5 3.0 5.0 5.5 3.0 5.0 2.5 4.0 3.0 4.0 2.0 2.0 2.0 5.0 2.0 1.5 2.5 3.5 2.5 1.5 2.0 2.5 2.0 1. 7.5 4.5 4.5 1.5 3.0 2.5 2.5 4.0 2.5 3.0 1.0 3.5 2.5 4.0 5.5 4.5 3.0 1.5 1.0 2.0 1.5 3.0 6.0 4.5 4.0 5.0 3.5 3.5 1.5 2.5 2.5 3.5 4.5 3.5 4.0 3.5 3.5 4.5 3.5 3.5 3.0 3.0 2.0 1.5 3.0 3.0 2.0 3.0 2.0 2.5 3.0 2.5 3.0 3.0 2.0 5.5 3.0 3.5 4.0 2.5 3.0 4.5 3.5 2.5 1.0 2. 5.5 5.0 2.5 3.5 5.5 4.0 3.0 3.5 3.5 3.5 1.5 4.5 3.0 1.5 2.5 3.0 3.5 3.5 3.0 2.5 3.0 4.5 5.5 7.0 3.0 3.0 2.5 2.0 5.0 3.0 2.0 3.0 3.5 4.0 2.5 2.5 2.5 3.5 0.5 2.0 2.5 2.5 1.5 2.0 3.0 3.5 2.5 1.5 2.0 4.0 4.0 3.0 3.0 3.5 1.0 5.0 3.5 2.0 3.5 2.5 2.5 3.5 2.5 2.5 2.5 3. 3.5 5.5 4.0 2.5 2.5 3.0 1.5 2.0 2.5 3.5 2.5 4.5 6.0 3.0 3.0 3.0 2.5 1.5 2.5 2.5 2.0 2.5 9.5 4.0 2.0 3.0 2.0 3.5 2.5 3.0 3.0 2.5 2.0 5 1 Table 48. CommonVoiceMini21 A2T retrieval results (languages 5987 of 114). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "luo lv mdf mhr mk ml mn mr mrj mt myv nan ne nl nn oc or os pa pl ps pt rm ro ru rw sah sat sc"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "LCO-Embedding/LCO-Embedding-Omni-3B 86.0 40.6 92.5 79.0 78.2 55.8 16.2 78.0 74.4 80.0 88.0 59.2 61.4 98.8 66.5 90.9 63.1 66.9 76.6 87.0 50.8 98.6 98.4 85.0 99.0 46.5 57.5 4.4 94.5 LCO-Embedding/LCO-Embedding-Omni-7B 75.0 35.8 86.0 65.4 74.0 39.8 10.4 79.0 57.0 77.2 81.3 61.8 48.5 99.4 53.6 89.4 58.8 59.2 78.6 66.6 47.6 98.8 97.9 76.6 99.0 36.5 43.0 6.2 93.5 5.0 8.8 5.0 3.2 Qwen/Qwen2-Audio-7B 2.0 5.3 7.0 1.9 microsoft/speecht5_multimodal 3.0 5.3 2.5 0.8 laion/clap-htsat-unfused 4.0 3.5 4.0 2.4 laion/clap-htsat-fused 3.5 3.5 3.5 1.3 OpenMuQ/MuQ-MuLan-large 1.5 3.5 2.5 0.8 microsoft/msclap-2023 4.5 5.3 3.5 1.9 laion/larger_clap_general 2.5 4.4 2.5 1.9 microsoft/msclap-2022 2.0 6.2 2.5 1.6 laion/larger_clap_music 3.0 1.8 3.0 1.9 lyrebird/wav2clip 3.0 3.5 1.0 1.3 laion/larger_clap_music_and_speech 3.3 0.7 1.4 2.1 0.7 1.2 1.7 1.4 1.0 1.4 1.4 4.0 3.5 4.0 3.0 2.5 2.0 2.5 3.0 2.5 2.0 1.5 4.2 1.0 0.4 1.0 1.2 1.2 0.8 1.2 1.0 1.0 1.0 7.7 1.5 3.8 5.4 6.9 5.4 6.2 4.6 3.8 3.1 3.8 4.7 1.8 2.2 2.2 1.8 1.8 2.9 1.5 1.8 2.2 2. 1.7 1.9 1.5 1.9 1.7 1.2 1.0 1.5 1.2 0.7 1.7 1.6 0.8 1.0 1.6 1.2 1.0 0.6 1.4 0.8 1.4 0.6 2.2 1.0 1.0 1.0 0.6 1.8 1.0 1.4 1.0 1.0 1.2 3.5 3.0 2.5 0.5 0.9 0.9 1.2 1.6 1.2 1.4 2.1 3.8 2.0 1.2 1.6 1.0 1.6 1.0 1.0 1.0 1.2 1.8 2.4 1.2 0.8 1.4 1.0 1.6 1.4 1.4 0.8 1.2 1. 2.0 1.0 1.4 1.2 1.2 1.4 0.6 0.8 1.2 1.0 1.6 4.5 4.0 4.5 3.5 2.5 3.5 3.0 2.0 3.0 3.0 3.5 7.5 4.7 8.4 5.6 3.7 8.4 3.7 5.6 4.7 6.5 5.6 2.0 1.0 1.0 1.6 0.8 1.2 1.8 1.2 0.8 0.8 1.2 1.8 1.2 1.4 2.2 0.4 1.4 1.0 0.8 1.0 0.8 2.2 1.0 0.8 0.4 2.0 1.0 1.2 1.4 1.4 1.0 0.8 1. 2.6 1.0 2.0 1.0 1.2 1.0 0.8 1.2 1.0 1.4 0.2 2.0 1.0 1.6 1.8 1.0 1.0 1.8 0.8 0.8 0.8 0.6 1.4 0.4 1.2 0.8 0.8 1.0 0.0 0.8 1.0 1.4 1.0 3.0 1.4 0.6 1.8 0.8 1.6 1.4 1.2 1.0 0.6 1.2 2.2 1.4 1.4 0.8 1.4 1.4 1.4 1.6 0.8 1.4 0.8 3.7 2.2 2.2 2.9 1.5 1.8 2.2 2.6 1.5 1.5 1. 1.6 2.6 0.8 1.0 0.6 1.4 1.8 1.0 1.0 1.0 0.2 1.2 0.6 0.8 1.6 0.8 0.6 0.6 1.0 1.0 1.2 0.6 Table 49. CommonVoiceMini21 A2T retrieval results (languages 88114 of 114). Best per language in bold. Model sk skr sl sq sr sv sw ta te th tig tk tn tok tr tt ug uk ur uz vi yi yo yue zgh zh zza LCO-Embedding/LCO-Embedding-Omni-3B 87.5 61.0 70.0 79.0 62.0 78.0 52.0 56.5 90.2 93.5 59.0 55.5 49.5 85.5 88.0 54.0 66.5 93.0 91.5 66.5 86.5 61.5 49.0 98.5 62.0 99.5 69.0 LCO-Embedding/LCO-Embedding-Omni-7B 82.5 62.0 70.5 74.0 63.5 65.5 56.0 28.5 63.9 95.0 40.5 41.5 35.5 83.0 75.5 42.0 58.5 94.0 90.5 55.0 89.0 62.2 33.0 99.0 66.0 99.5 58.5 4.5 Qwen/Qwen2-Audio-7B 3.0 microsoft/speecht5_multimodal 2.5 laion/clap-htsat-unfused 2.5 laion/clap-htsat-fused 2.5 OpenMuQ/MuQ-MuLan-large 4.0 microsoft/msclap-2023 1.5 laion/larger_clap_general 2.5 microsoft/msclap-2022 2.5 laion/larger_clap_music 2.5 lyrebird/wav2clip 3.5 laion/larger_clap_music_and_speech 5.0 18.0 5.0 3.0 8.2 4.0 2.0 8.2 2.5 1.0 9.8 2.0 2.5 9.8 2.5 2.5 6.6 2.0 3.5 6.6 1.5 2.0 3.3 2.0 2.5 8.2 3.0 4.0 8.2 2.5 2.5 4.9 2.0 4.0 1.5 2.0 3.0 3.0 3.0 3.0 2.5 3.0 2.5 1.5 4.5 3.0 2.5 3.5 4.0 4.5 2.0 3.5 2.5 4.0 2.5 6.5 2.5 4.5 5.0 2.0 4.0 1.5 2.5 2.0 4.0 3.5 3.5 5.0 3.5 2.0 4.0 2.5 3.0 4.0 2.5 3.0 4.0 4.5 3.0 3.0 3.5 3.0 3.0 4.0 3.0 2.5 2.0 3. 6.5 4.5 1.5 3.0 3.5 1.5 2.5 2.0 2.5 2.5 3.0 4.0 1.0 2.0 3.0 2.5 3.0 3.5 3.0 2.5 2.0 2.0 4.2 2.1 4.9 2.8 4.2 3.5 0.7 2.8 4.9 2.8 2.8 2.0 1.0 4.0 2.0 3.0 3.5 3.0 1.5 3.0 1.5 2.5 3.5 2.0 3.5 3.5 2.5 2.0 0.5 1.0 2.0 3.0 2.0 6.5 2.5 2.5 2.5 3.0 2.0 5.0 2.5 2.0 3.0 1. 3.5 2.5 2.0 2.5 4.0 2.5 2.5 2.0 2.5 2.0 2.0 4.5 2.5 2.5 3.0 2.0 3.0 4.5 2.5 2.5 1.5 2.0 6.5 4.0 3.0 3.0 1.5 2.5 4.5 2.5 2.5 1.5 2.0 2.5 2.5 4.0 2.5 2.5 3.0 1.5 3.0 2.0 2.0 2.5 3.5 4.5 2.5 3.0 2.5 4.0 3.5 1.5 2.5 1.5 1.0 5.0 3.0 4.5 3.0 1.0 3.0 2.0 3.0 2.5 3.5 3. 5.5 3.0 4.5 3.0 2.0 3.5 2.5 2.5 3.0 3.0 3.5 5.0 4.0 3.5 1.5 2.0 3.5 2.5 5.0 2.5 2.5 3.0 5.0 4.5 2.5 2.5 1.5 2.5 2.5 2.5 2.5 3.5 3.5 3.0 3.5 3.0 2.5 4.5 2.5 1.5 3.0 3.0 2.5 2.5 5.0 3.0 2.5 2.5 2.5 2.0 3.0 2.0 2.5 1.5 2.5 4.0 4.5 1.0 3.0 3.5 2.0 2.5 2.0 2.5 2.5 4. Table 50. CommonVoiceMini21 T2A retrieval results (languages 129 of 114). Best per language in bold. Model ab af am ar as ast az ba bas be bg bn br ca ckb cnh cs cv cy da dav de dv dyu el en eo es et LCO-Embedding/LCO-Embedding-Omni-3B 22.0 83.8 22.0 92.0 42.0 99.4 83.7 29.0 58.0 99.5 84.0 69.0 60.0 99.0 45.5 48.5 86.0 55.0 23.0 74.0 51.0 99.5 6.5 87.3 50.5 99.0 96.0 100.0 53.5 LCO-Embedding/LCO-Embedding-Omni-7B 19.5 83.8 22.5 91.0 28.0 96.2 79.3 29.5 48.0 98.0 75.5 62.0 54.0 98.0 32.5 42.5 72.5 53.5 28.5 60.5 33.0 99.5 3.5 81.0 23.5 98.5 91.5 100.0 52.0 2.5 5.5 Qwen/Qwen2-Audio-7B 3.5 2.0 microsoft/speecht5_multimodal 3.5 3.0 laion/clap-htsat-unfused 3.5 2.0 laion/clap-htsat-fused 3.0 2.5 microsoft/msclap-2023 2.0 4.0 microsoft/msclap-2022 5.0 1.5 OpenMuQ/MuQ-MuLan-large 4.0 2.5 laion/larger_clap_general 1.5 3.0 laion/larger_clap_music_and_speech 1.5 3.0 lyrebird/wav2clip 2.5 2.5 laion/larger_clap_music 6.5 2.0 7.9 4.0 2.5 14.3 2.5 5.0 2.5 11.1 2.5 1.0 2.0 15.9 3.5 3.5 2.0 12.7 1.5 2.5 2.5 4.0 9.5 4.0 3.0 2.5 9.5 4.5 2.0 1.5 6.3 3.5 3.5 2.0 4.8 2.5 3.0 3.0 7.9 2.5 2.5 2.5 9. 4.0 11.0 4.5 3.0 5.5 3.5 2.0 5.0 2.0 3.0 3.0 3.0 2.0 2.0 3.0 2.0 2.0 1.5 3.5 3.0 2.5 3.0 2.5 5.5 10.3 6.5 2.5 7.7 1.5 2.0 5.1 3.0 1.5 6.0 2.5 2.5 6.0 2.0 2.5 6.0 3.0 4.0 6.0 2.0 3.0 2.6 3.5 2.5 3.4 4.5 2.5 4.3 2.5 3.0 3.4 2.5 8.0 2.5 3.5 1.5 2.5 4.0 3.0 2.5 1.5 2.0 2.5 2.0 2.0 2.0 1.0 2.5 2.5 4.5 3.5 2.5 2.5 2.5 2.5 4.5 3.5 1.5 2.5 3.0 1.5 3.0 3.0 1.5 2.5 5.0 4.5 4.5 3.5 3.5 1.0 2.5 4.0 2.0 1.5 2. 2.5 3.0 2.5 4.0 2.0 2.0 2.0 2.0 2.5 2.0 2.5 4.0 2.5 1.5 1.5 2.0 2.5 2.0 3.0 1.5 1.5 2.0 4.0 3.0 3.5 5.0 3.0 3.0 3.0 3.0 2.5 3.0 2.5 5.5 3.5 3.0 1.5 2.5 2.0 2.5 1.5 1.0 3.0 2.5 2.5 2.5 3.0 2.5 3.0 1.5 3.0 2.0 2.0 3.0 2.5 3.0 2.5 1.5 3.5 3.0 4.5 2.5 3.0 4.0 2.5 3. 4.5 7.5 3.0 3.0 4.0 3.5 2.5 2.0 3.0 2.5 2.5 6.0 1.5 3.0 2.5 2.0 3.0 3.5 2.5 2.5 3.0 2.5 7.5 3.5 5.5 3.5 3.5 3.0 2.0 2.5 1.5 4.0 2.5 2.5 5.5 0.5 2.5 3.0 2.5 2.5 2.5 2.0 3.0 3.0 4.0 3.5 4.5 4.5 1.5 3.5 3.5 3.5 1.5 1.5 2.5 3.5 2.5 3.5 2.0 3.0 3.0 1.0 1.0 2.5 3.5 2. 7.6 5.1 3.2 2.5 3.8 3.8 3.8 4.5 5.1 5.1 3.2 5.4 6.5 3.3 2.2 3.3 7.6 5.4 6.5 9.8 5.4 5.4 5 2 Table 51. CommonVoiceMini21 T2A retrieval results (languages 3058 of 114). Best per language in bold."
        },
        {
            "title": "Model",
            "content": "eu fa fi fr fy ga gl gn ha he hi hsb hu hy ia id it ja ka kab kk kln kmr ko ky lg lij lt ltg"
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        },
        {
            "title": "M\na\ns\ns\ni\nv\ne\nA\nu\nd\ni\no\nE\nm\nb\ne\nd\nd\ni\nn\ng\nB\ne\nn\nc\nh\nm\na\nr\nk",
            "content": "LCO-Embedding/LCO-Embedding-Omni-3B 58.5 33.5 54.5 100.0 90.5 34.0 99.5 60.0 55.0 45.0 95.5 77.5 53.5 47.0 99.0 94.5 100.0 93.5 69.5 38.0 25.5 48.5 53.0 98.5 44.5 31.5 94.5 68.4 57.2 LCO-Embedding/LCO-Embedding-Omni-7B 58.5 30.5 33.5 100.0 89.5 20.5 100.0 47.5 44.5 29.5 94.5 72.0 48.5 27.5 98.5 96.0 100.0 92.0 62.0 35.5 20.0 36.5 43.5 97.5 43.0 24.5 90.5 58.4 47.0 1.8 5.0 Qwen/Qwen2-Audio-7B 1.2 3.0 microsoft/speecht5_multimodal 1.4 3.0 laion/clap-htsat-unfused 1.0 1.5 laion/clap-htsat-fused 2.2 2.5 microsoft/msclap-2023 1.6 3.0 microsoft/msclap-2022 1.4 3.0 OpenMuQ/MuQ-MuLan-large 0.6 2.5 laion/larger_clap_general 1.4 1.5 laion/larger_clap_music_and_speech 1.0 2.5 lyrebird/wav2clip 0.8 2.5 laion/larger_clap_music 5.0 2.5 2.5 4.0 2.5 3.0 4.5 2.0 2.0 2.5 2.5 7.0 4.5 2.5 1.5 3.0 2.5 2.0 3.0 2.5 3.0 2.5 1.6 1.6 1.0 1.6 1.2 0.8 1.2 0.8 1.4 1.2 1.0 4.0 3.0 2.5 3.0 1.0 4.0 3.0 2.5 2.0 1.5 2.0 2.5 3.0 5.5 5.0 4.5 4.5 3.0 2.5 2.5 3.0 2. 2.0 2.5 2.5 2.5 3.5 3.5 3.5 2.0 2.0 2.0 2.5 5.0 3.0 2.5 2.0 3.5 3.0 1.5 2.5 3.5 3.0 2.0 3.5 3.5 3.5 4.5 2.0 2.0 3.5 4.0 3.0 2.5 3.0 3.5 4.5 2.0 2.5 2.0 1.5 1.5 2.0 4.0 2.0 2.5 4.0 3.0 2.0 4.0 3.5 2.5 2.0 3.5 1.5 2.0 2.5 4.5 6.0 4.0 3.5 3.0 3.5 3.0 3.5 3.0 3.5 2. 4.0 3.5 5.0 4.0 3.5 4.5 4.0 3.5 2.5 4.0 2.5 8.5 3.5 2.5 3.0 1.5 3.5 2.0 5.0 2.5 4.0 2.5 5.0 4.0 3.0 3.5 3.0 4.0 1.5 3.5 1.5 1.5 2.5 3.5 1.5 1.5 4.0 4.5 1.5 2.5 2.5 1.0 2.5 2.5 4.5 2.5 3.5 2.0 2.5 3.0 2.5 3.0 3.0 2.5 2.5 7.0 5.0 3.0 2.5 4.5 4.5 2.0 3.0 4.0 2.5 2. 8.0 5.0 3.5 3.0 2.0 1.5 2.5 2.0 1.5 2.5 3.0 4.0 4.0 4.5 5.5 4.0 3.0 2.0 3.5 4.0 2.0 2.0 4.0 3.0 2.5 2.5 3.0 3.5 1.5 2.0 2.0 2.5 2.5 2.0 3.0 3.0 2.0 2.0 2.5 2.5 2.5 3.5 2.5 2.5 3.5 2.0 2.0 2.5 4.5 3.0 1.0 3.0 4.0 2.5 2.5 4.0 4.5 3.0 5.5 3.5 1.5 3.0 3.0 3.0 2.5 2. 4.5 3.0 3.5 2.5 2.5 3.5 2.0 3.0 3.0 2.0 2.5 3.5 3.0 2.5 2.5 2.0 2.5 4.0 3.0 2.0 2.0 2.5 4.5 5.0 2.5 4.0 3.5 3.0 2.0 1.5 3.5 2.0 3.0 2.0 2.5 4.0 4.0 2.5 2.5 3.0 2.0 3.5 3.0 2.5 Table 52. CommonVoiceMini21 T2A retrieval results (languages 5987 of 114). Best per language in bold. Model luo lv mdf mhr mk ml mn mr mrj mt myv nan ne nl nn oc or os pa pl ps pt rm ro ru rw sah sat sc LCO-Embedding/LCO-Embedding-Omni-3B 61.5 47.0 76.6 50.4 78.2 49.0 8.0 77.6 43.4 74.6 68.3 60.8 50.7 98.4 67.5 91.2 56.0 45.4 67.6 89.4 38.2 97.8 96.8 79.2 98.6 26.5 25.5 3.5 94.5 LCO-Embedding/LCO-Embedding-Omni-7B 62.0 38.8 63.6 41.0 71.8 32.6 5.4 76.0 33.4 70.4 61.1 63.8 39.7 98.2 51.9 87.2 61.9 33.1 70.0 63.8 35.6 98.2 95.4 69.8 98.4 23.0 27.0 3.5 94.0 3.0 5.3 3.5 2.1 Qwen/Qwen2-Audio-7B 1.5 4.4 4.0 1.1 microsoft/speecht5_multimodal 2.0 7.1 5.0 1.9 laion/clap-htsat-unfused 2.0 2.7 4.5 1.3 laion/clap-htsat-fused 2.5 3.5 2.5 1.6 microsoft/msclap-2023 0.5 4.4 2.5 1.6 microsoft/msclap-2022 3.5 4.4 1.5 1.1 OpenMuQ/MuQ-MuLan-large 2.0 4.4 3.0 1.3 laion/larger_clap_general 2.5 5.3 2.0 2.1 laion/larger_clap_music_and_speech 2.5 4.4 3.0 1.3 lyrebird/wav2clip 2.5 4.4 2.5 1.3 laion/larger_clap_music 1.8 5.4 1.4 1.0 2.3 1.0 1.2 0.8 4.6 0.7 10.0 1.4 1.0 2.3 1.7 0.8 4.6 1.7 1.2 3.8 1.0 1.0 3.1 1.4 1.0 3.8 1.0 1.2 4.6 1.2 1.2 3.1 1. 3.4 1.8 0.8 1.0 0.4 1.0 1.2 0.6 1.4 0.6 1.4 1.0 1.0 0.8 1.0 1.6 1.0 1.4 0.8 0.8 0.8 1.0 0.8 1.0 0.8 0.8 0.8 1.0 1.0 0.8 1.0 1.0 1.0 4.0 1.1 2.6 1.8 2.2 1.5 2.6 1.5 1.5 2.9 1.8 1.6 1.6 1.6 1.8 1.4 0.8 1.2 1.2 1.0 1.0 1.0 4.7 4.7 8.4 5.6 6.5 6.5 7.5 2.8 5.6 4.7 4.7 4.0 6.5 3.5 3.5 3.5 3.5 1.5 2.5 1.5 2.5 2.0 3.5 4.0 3.5 2.0 6.5 1.5 3.5 4.0 3.5 3.0 2. 1.6 0.8 0.6 1.2 1.4 0.4 0.4 1.0 1.4 1.2 1.0 1.0 1.2 2.4 1.0 1.2 0.8 1.4 0.4 1.4 1.2 1.0 1.4 1.2 1.4 0.6 0.8 0.8 1.2 1.4 1.2 1.4 1.2 2.6 1.8 1.6 1.4 2.0 1.6 1.4 1.0 1.0 1.0 1.0 4.4 1.2 2.2 1.6 1.4 2.2 1.2 1.0 1.2 1.0 0.8 2.6 2.4 1.2 0.8 1.2 1.6 0.8 0.8 1.2 1.4 1. 2.4 1.9 2.2 2.7 1.5 1.2 2.4 1.2 1.7 1.2 1.2 1.4 1.8 1.2 2.0 1.2 1.2 1.0 1.2 1.6 1.4 1.0 1.6 1.2 1.4 1.0 0.8 1.0 0.2 1.2 1.2 1.0 1.0 3.7 1.8 1.8 2.6 1.8 2.2 2.6 1.5 1.8 1.8 1.8 3.0 1.8 1.2 1.4 1.6 1.8 1.4 1.4 0.9 1.4 1.2 4.0 1.0 1.0 1.6 1.0 1.4 2.0 1.2 1.0 1.2 1. 1.6 1.6 1.2 2.0 2.0 1.6 1.0 1.4 1.2 0.8 1.0 1.8 1.0 1.4 2.0 0.6 1.2 0.2 1.2 0.8 0.6 1.0 Table 53. CommonVoiceMini21 T2A retrieval results (languages 88114 of 114). Best per language in bold. Model sk skr sl sq sr sv sw ta te th tig tk tn tok tr tt ug uk ur uz vi yi yo yue zgh zh zza LCO-Embedding/LCO-Embedding-Omni-3B 89.0 48.0 74.5 72.0 55.5 74.0 52.5 40.5 77.0 92.5 30.0 38.0 29.5 67.5 83.0 32.5 45.0 93.5 88.5 50.0 86.0 44.8 25.5 99.5 26.5 99.0 63.0 LCO-Embedding/LCO-Embedding-Omni-7B 84.5 45.0 75.0 71.0 52.5 63.0 51.5 24.0 57.4 94.5 24.0 32.5 20.5 62.5 76.5 33.0 41.5 92.5 90.5 43.5 87.5 52.4 10.5 99.5 30.0 99.0 50.5 2.5 Qwen/Qwen2-Audio-7B 5.5 microsoft/speecht5_multimodal 2.0 laion/clap-htsat-unfused 5.5 laion/clap-htsat-fused 2.5 microsoft/msclap-2023 2.5 microsoft/msclap-2022 3.5 OpenMuQ/MuQ-MuLan-large 2.5 laion/larger_clap_general 3.0 laion/larger_clap_music_and_speech 2.5 lyrebird/wav2clip 2.0 laion/larger_clap_music 2.5 14.8 3.0 2.5 8.2 1.0 2.5 6.6 2.0 2.5 8.2 1.0 2.0 6.6 2.5 2.0 6.6 2.5 3.5 4.0 8.2 2.5 11.5 3.0 3.0 8.2 2.5 3.0 8.2 2.0 3.0 8.2 2.5 2.0 1.5 4.5 1.5 4.0 1.5 2.5 3.0 3.0 2.5 2.5 5.0 5.5 2.5 3.5 3.0 2.0 2.0 2.5 2.0 2.5 2.5 3.5 4.5 3.0 2.5 6.5 1.5 4.5 2.0 2.5 2.0 2.5 5.0 3.0 2.5 3.0 3.5 4.5 5.0 3.0 3.0 2.5 2.5 4.5 7.5 4.5 5.0 4.5 2.0 3.0 3.0 4.0 2.0 2. 3.5 4.5 3.5 4.5 2.5 5.5 3.0 1.5 2.0 3.0 2.5 5.0 3.5 2.0 2.5 2.5 1.5 1.0 2.0 3.0 2.0 2.5 4.9 4.2 4.2 5.6 1.4 4.2 4.2 1.4 2.1 4.2 3.5 6.0 1.0 4.5 2.5 2.5 2.5 2.5 3.0 2.0 2.0 2.5 4.0 2.5 1.0 4.0 3.0 2.0 3.0 2.5 0.5 2.0 2.5 4.5 4.0 3.0 1.5 2.5 1.5 3.5 2.0 2.5 2.5 2. 3.5 1.5 2.5 1.5 2.0 3.0 2.5 1.0 2.5 2.5 2.5 2.5 2.5 3.0 3.0 3.5 2.0 2.0 4.5 2.5 2.5 2.5 3.5 2.5 2.5 3.0 2.0 2.5 1.5 2.0 2.0 2.5 2.5 4.5 2.0 5.0 4.0 5.5 2.5 2.0 2.0 2.5 4.0 2.5 5.0 3.5 5.0 3.0 3.0 2.0 4.0 4.0 3.0 2.0 2.5 6.5 2.0 1.5 4.5 3.0 3.5 3.5 2.5 1.5 3.5 2. 4.5 3.0 4.5 2.0 3.5 4.0 3.0 4.5 3.5 2.5 2.5 5.0 3.5 2.5 2.5 3.0 0.5 2.5 2.0 2.5 2.0 2.5 3.5 3.0 2.5 1.5 2.5 2.0 1.0 2.5 2.5 1.5 2.5 6.0 2.5 7.5 2.5 2.5 3.0 1.5 2.5 2.5 1.5 2.5 1.5 2.5 2.0 2.0 2.0 1.0 2.0 2.0 3.0 3.0 2.5 6.5 2.5 3.0 2.5 3.0 2.0 3.5 1.5 1.5 2.0 2."
        },
        {
            "title": "M\nA\nE\nB",
            "content": ":"
        }
    ],
    "affiliations": [
        "Aarhus University",
        "Capital One",
        "Carleton University",
        "Durham University",
        "Harvard University",
        "Indian Institute of Technology, Kharagpur",
        "MIRAI",
        "SaluteDevices",
        "Stanford University",
        "Zendesk"
    ]
}