{
    "paper_title": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill Assessment",
    "authors": [
        "Edoardo Bianchi",
        "Antonio Liotta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), a novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristics-from high-frequency sampling for dynamic sports to fine-grained segmentation for sequential skills-demonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications."
        },
        {
            "title": "Start",
            "content": "PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill Assessment 1st Edoardo Bianchi Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy edbianchi@unibz.it 2nd Antonio Liotta Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy antonio.liotta@unibz.it 5 2 0 2 ] . [ 1 6 9 9 4 0 . 6 0 5 2 : r AbstractAutomated sports skill assessment requires capturing fundamental movement patterns that distinguish expert from novice performance, yet current video sampling methods disrupt the temporal continuity essential for proficiency evaluation. To this end, we introduce Proficiency-Aware Temporal Sampling (PATS), novel sampling strategy that preserves complete fundamental movements within continuous temporal segments for multi-view skill assessment. PATS adaptively segments videos to ensure each analyzed portion contains full execution of critical performance components, repeating this process across multiple segments to maximize information coverage while maintaining temporal coherence. Evaluated on the EgoExo4D benchmark with SkillFormer, PATS surpasses the state-of-the-art accuracy across all viewing configurations (+0.65% to +3.05%) and delivers substantial gains in challenging domains (+26.22% bouldering, +2.39% music, +1.13% basketball). Systematic analysis reveals that PATS successfully adapts to diverse activity characteristicsfrom high-frequency sampling for dynamic sports to finegrained segmentation for sequential skillsdemonstrating its effectiveness as an adaptive approach to temporal sampling that advances automated skill assessment for real-world applications. Index TermsProficiency Estimation, Action Quality Assessment, Sports Analytics, Multi-view Video Understanding I. INTRODUCTION Automated sports skill assessment represents critical challenge with applications in training, coaching, and talent development. Unlike action recognition, skill assessment requires capturing how well an action is executed through subtle temporal dynamics that distinguish expert from novice performance. In sports contexts, expertise manifests through temporal patterns: the rhythm of basketball dribbling, timing coordination in soccer control, or fluid progression in athletic techniques. These patterns emerge from the execution of fundamental movement componentsthe basic building blocks of skilled performance that must be observed in their natural temporal context to accurately assess proficiency. However, current video sampling approaches fundamentally disrupt this continuity. Uniform sampling at regular intervals potentially misses critical transitions, random sampling lacks systematic coverage, and motion-density sampling ignores the structured temporal nature of skilled performance. Even segment-based approaches typically employ sparse sampling within segments, breaking the natural flow essential for skill assessment. These limitations stem from failure to recognize that athletic proficiency manifests through structured temporal patterns requiring continuous observation of complete fundamental movements. Our key insight is that effective skill assessment requires analyzing continuous video portions containing at least one complete fundamental movement, repeated across multiple segments to maximize information capture while preserving temporal coherence. In this work, we introduce Proficiency-Aware Temporal Sampling (PATS), novel sampling strategy specifically designed for multi-view sports skill assessment. PATS preserves complete fundamental movements within continuous temporal segments, ensuring that each analyzed portion contains the full execution of critical performance components. By repeating this process across multiple non-overlapping segments, PATS maximizes information coverage while maintaining the temporal continuity essential for accurate proficiency assessment. This work presents the following key contributions: proficiency-aware sampling strategy that preserves fundamental movements within continuous temporal segments. Synchronized multi-view integration that preserves temporal coherence across camera perspectives. Architecture-agnostic design enabling seamless integration with existing temporal modeling frameworks without computational overhead. When applied to SkillFormer [1], recent and competitive multi-view proficiency estimation framework, PATS achieves consistent improvements across all viewing configurations (+0.65% to +3.05%) and delivers substantial scenario-specific gains (+26.22% bouldering, +2.39% music, +1.13% basketball), establishing new accuracy standards and demonstrating its ability to capture coherent and meaningful temporal patterns essential for skill assessment. PATS operates as preprocessing step that enhances model accuracy without adding computational overhead, maintaining efficiency while providing an adaptive approach to temporal sampling for sports skill assessment. II. BACKGROUND AND RELATED WORK A. Temporal Sampling Methods Temporal sampling strategies have evolved from uniform approaches to sophisticated content-aware methods. While Temporal Segment Networks (TSN) [2] achieves computational efficiency through sparse sampling, it sacrifices temporal continuity essential for skill assessment. Recent advances including motion-density methods, temporal correspondence techniques like sandwich sampling [3], channel sampling strategies [4], and temporal contextualization [5] have primarily focused on general action recognition, with limited consideration for the specialized requirements of proficiency assessment where temporal dynamics distinguish expert from novice performance. B. Action Quality Assessment Action Quality Assessment (AQA) evaluates how well actions are performed, requiring sensitivity to subtle performance differences [6]. Modern methods have evolved from handcrafted features to deep learning architectures using pretrained backbones like I3D [7] and transformers [8]. Recent advances include spatial-aware modeling [9], temporal-aware approaches leveraging procedural action structure [10], and quality prediction strategies [11], [12]. Multi-modal approaches integrate skeleton features [13], [14] and audio information [15], [16] for enhanced understanding. Multi-view methods like SkillFormer [1] and EgoPulseFormer [17] fuse egocentric and exocentric features using cross-attention mechanisms and physiological signals, respectively. Datasets such as EgoExoLearn [18] and EgoExo4D [19] provide benchmarks for multi-perspective skill evaluation. However, existing methods inherit temporal sampling limitations, failing to preserve the temporal dynamics that distinguish expert from novice performance. Our work addresses these limitations through specialized sampling strategies designed for skill assessment scenarios. III. PROPOSED METHODOLOGY A. Proficiency-Aware Temporal Sampling (PATS) We introduce Proficiency-Aware Temporal Sampling (PATS), novel temporal sampling strategy designed to preserve the sequential nature of skilled performances for accurate proficiency assessment. Unlike traditional uniform sampling methods that randomly select frames across video, PATS extracts continuous temporal segments that maintain the natural flow of athletic movements and performance dynamics. 1) Method Overview and Parameters: PATS is controlled by three key parameters that balance temporal coverage with continuity preservation: Ntarget (total number of frames to extract from the input video), Ns (number of temporal segments to divide the video into), and ds (desired duration in seconds for each temporal segment). The core principle of PATS is to extract Ns continuous temporal segments of duration ds, distributed across the entire video timeline, yielding exactly Ntarget frames total. This approach ensures comprehensive temporal coverage while preserving the sequential relationships critical for skill assessment. 2) Adaptive Segment Positioning: Given an input video of total duration seconds with Ntotal frames captured at frame rate ps, PATS first determines the effective segment duration to prevent temporal overlap and ensure adequate spacing: (cid:18) ds,ef = min ds, (cid:19) 0.8T Ns (1) The 0.8 scaling factor ensures sufficient buffer space between segments while maximizing temporal coverage. Segment start times are then distributed uniformly across the feasible temporal range. To ensure segments remain within video boundaries, the maximum allowable start time is: tmax = max(0, ds,ef ) (2) For multiple segments (Ns > 1), start times are positioned using: tstart,i = tmax Ns 1 , = 0, 1, ..., Ns 1 (3) For single-segment scenarios (Ns = 1), the segment begins at tstart,0 = 0. 3) Frame Allocation and Extraction: The algorithm ensures exactly Ntarget frames are extracted by distributing them across segments. The number of frames allocated to each segment is: Nf rs,i = (cid:23) (cid:22) Ntarget Ns + 1[i < Ntarget mod Ns] (4) where 1[] is the indicator function. This allocation guarantees that remainder frames are distributed to the first segments, ensuring exactly Ntarget total frames."
        },
        {
            "title": "For",
            "content": "each segment with boundaries [tstart,i, tstart,i + ds,ef ], the corresponding frame indices are computed as: temporal fs,i = tstart,i ps fe,i = min((tstart,i + ds,ef ) ps, Ntotal) (5) (6) Within each segment, frames are extracted using continuous sampling: framesi = (cid:40)(cid:106) fs,i+fe,i (cid:107) 2 if Nf rs,i = 1 linspace(fs,i, fe,i 1, Nf rs,i) otherwise (7) 4) Robust Edge Case Handling: The algorithm includes comprehensive handling for challenging scenarios to ensure reliable operation across diverse video conditions: Insufficient Video Duration: If 0 or Ntotal < Ntarget, the method falls back to uniform sampling. Minimal Segment Duration: When ds,ef < 0.5 seconds, the algorithm reverts to uniform sampling to preserve temporal coherence. Boundary Violations: Segment boundaries exceeding video limits are automatically adjusted to ensure fe,i Ntotal. Frame Count Adjustment: The algorithm guarantees exactly Ntarget output frames through uniform subsampling (excess frames) or cyclic repetition (insufficient frames). The final output is sorted list of exactly Ntarget frame indices clipped to [0, Ntotal 1], ensuring robust behavior across all input conditions. B. Integration with SkillFormer PATS is designed as general video sampling strategy that can enhance any proficiency estimation architecture relying on temporal dynamics. We integrate PATS with SkillFormer [1] for three key reasons: (1) SkillFormer represents the current state-of-the-art for proficiency estimation on EgoExo4D to the best of our knowledge, (2) PATS specifically targets proficiency estimation tasks where temporal continuity distinguishes expert from novice performance, and (3) both approaches focus on sports domains, which constitute significant portion of EgoExo4D scenarios. PATS enhances SkillFormers multi-view architecture by preserving temporal continuity essential for effective crossview fusion. SkillFormers CrossViewFusion module employs multi-head cross-attention mechanisms to correlate features across camera perspectives, requiring meaningful temporal relationships to capture performance nuances. By providing continuous temporal segments rather than sparse frames, PATS enables SkillFormers attention computations to effectively analyze movements across multiple views. PATS seamlessly replaces SkillFormers uniform temporal sampling while maintaining full compatibility with the original architecture. The preserved temporal structure enhances proficiency assessment across diverse sporting activities without requiring architectural modifications. IV. EXPERIMENTAL SETUP A. Dataset We evaluate on the Ego-Exo4D dataset [19], which provides synchronized multi-view recordings across diverse real-world scenarios with over 1,200 hours of video from 740 participants. Our experiments focus on the official demonstrator proficiency benchmark, encompassing six activity domains: cooking, music, basketball, bouldering, soccer, and dance. Each video is annotated with four skill levels: Novice, Early Expert, Intermediate Expert, and Late Expert. the Following established protocols [1], [17], we adopt official dataset partitions, reserving 10% of the training set for validation and conducting final evaluation on the official held-out validation set. The datasets synchronized multi-view footage provides an ideal framework for assessing temporal sampling strategies in skill assessment scenarios. B. Implementation Details pre-trained on Kinetics-600 [21], and fine-tuned for 4 epochs using AdamW optimizer with weight decay of 0.01 and LoRA adaptation. We adopt SkillFormers established hyperparameters for each experimental setup, including projector hidden dimensions, LoRA rank and scaling factors, learning rate, and batch size configurations. The sole modification to the baseline framework is the temporal sampling strategy: while SkillFormer employs uniform frame sampling, our approach integrates PATS as described in Section III. We systematically tune the PATSspecific parameters-total number of frames (Ntarget), number of temporal segments (Ns), and segment duration (ds)-through grid search as detailed in Section IV-C, while keeping all other model and training configurations identical to the baseline. Selected frames undergo standard preprocessing: resizing to 224-pixel shortest edge, center-cropping to 224224, rescaling to [0, 1], and normalization with mean [0.45, 0.45, 0.45] and standard deviation [0.225, 0.225, 0.225]. Training was conducted on single NVIDIA A100 GPU. C. PATS Hyperparameter Selection Rationale We conduct systematic grid search across PATS parameters to identify optimal configurations for different view setups and activity characteristics. Our parameter selection balances comprehensive evaluation with computational constraints while ensuring coverage of diverse temporal sampling scenarios. We evaluate 24 and 32 frames per video to explore the tradeoff between computational efficiency and temporal resolution. Segment count selection spans 2 to 12 segments: coarse segmentation (2 segments) captures preparation-execution dichotomy in continuous actions, medium segmentation (6-8 segments) enables multi-phase analysis for dynamic activities, and fine segmentation (12 segments) provides granular resolution for sequential tasks. Duration configuration explores 1-second and 3-second segments based on sports biomechanics principles: short duration (1s) captures rapid execution phases in high-frequency activities, while standard duration (3s) encompasses complete movement sequences optimal for most scenarios. The combination yields effective sampling rates from 0.89 to 5.33 FPS. High rates (4.0-5.33 FPS) prove optimal for dynamic activities requiring fine temporal resolution, while lower rates (0.89 FPS) suffice for structured, sequential activities like music performance. V. RESULTS We evaluate PATS through three key dimensions: (1) overall accuracy against state-of-the-art baselines, (2) systematic analysis of PATS parameters, and (3) scenario-specific analysis. All experiments are conducted on the EgoExo4D benchmark under consistent evaluation protocols. To ensure fair comparison, we maintain identical training configurations to the original SkillFormer implementation [1]. All models are initialized from TimeSformer backbone [20], A. State-of-the-Art Comparison Table demonstrates that SkillFormer+PATS surpasses the state-of-the-art accuracy across all viewing configurations. Our TABLE COMPARISON WITH EGOEXO4D PROFICIENCY ESTIMATION BASELINES (FROM [19]) AND SKILLFORMER (FROM [1]). WE REPORT ACCURACY (%) FOR EGOCENTRIC (EGO), EXOCENTRIC (EXOS), AND COMBINED VIEWS (EGO+EXOS). SKILLFORMER + PATS OUTPERFORMS THE BASELINES IN ALL SETTINGS. BOLD DENOTES THE BEST ACCURACY; UNDERLINED VALUES INDICATE SECOND-BEST. Pretrain Ego Exos Ego+Exos Params Epochs Method Random Majority-class TimeSformer TimeSformer TimeSformer TimeSformer TimeSformer - - - K400 24.9 31.1 42.3 42.9 HowTo100M 46.8 44.4 45.9 EgoVLP EgoVLPv2 SkillFormer-Ego SkillFormer-Exos SkillFormer-EgoExos SkillFormer-Ego+PATS SkillFormer-Exos+PATS SkillFormer-EgoExos+PATS K600 K600 K600 K600 K600 K600 45.9 - - 47.3 - - 24.9 31.1 40.1 39.1 38.2 40.6 38.0 - 46.3 - - 46.6 - 24.9 31.1 40.8 38.6 39.7 39.5 37.8 - - 47.5 - - 48.0 - - 121M 121M 121M 121M 121M 14M 20M 27M 14M 20M 27M - - 15 15 15 15 15 4 4 4 4 4 4 TABLE II ALL EXPERIMENTAL CONFIGURATIONS WITH TRAINING HYPERPARAMETERS AND OVERALL ACCURACY (%). FIXED ACROSS ALL RUNS: EPOCHS=4, BATCH SIZE=16, OUTPUT DIM=768, ATTENTION HEADS=16. ABBREVIATIONS: SEGS=SEGMENTS, R=LORA RANK, A=LORA ALPHA, HID=HIDDEN DIMENSION, PAR=PARAMETERS, ACC=OVERALL ACCURACY. Views Frames Segs Duration (s) FPS Hid LR Par Acc Note Ego Exos Ego+Exos 24 32 32 32 24 32 32 32 24 32 32 32 6 2 8 12 6 2 8 12 6 2 8 3 3 1 3 3 3 1 3 3 3 1 3 1.33 5.33 4.00 0.89 1.33 5.33 4.00 0.89 1.33 5.33 4.00 0. 32 64 1536 5e-5 14M 96 2048 3e-5 20M 64 2560 2e-5 27M 45.6 47.3 Bouldering specialist 42.0 46.1 Music specialist 44.9 43.4 46.6 Cooking specialist 44.4 44.6 45.3 Basketball specialist 47.0 Dancing specialist 48. approach delivers consistent improvements over the original SkillFormer: 47.3% accuracy for egocentric views (+3.05%), 46.6% for exocentric views (+0.65%), and 48.0% for combined views (+1.05%). These gains are achieved while maintaining computational efficiency with 14-27M parameters and 4 training epochs. poral segmentation correlates inversely with action continuitycontinuous actions require fewer segments (2) while finegrained sequential skills benefit from finer segmentation (12). These consistent patterns across diverse activities demonstrate PATS adaptability to different skill assessment domains. B. PATS Parameter Analysis C. Scenario-Specific Analysis Our systematic grid search across 12 major configurations (Table II) reveals clear optimization patterns for proficiency estimation, synthesized in Table III. Three key principles emerge from the analysis. First, 32 frames proves universally optimal across all scenarios, while sampling rates diverge by activity type: dynamic activities require high rates (4.0-5.33 FPS) while structured sequential activities perform best at lower rates (0.89 FPS). Second, view selection aligns with skill characteristics: egocentric views excel for proprioceptive activities (bouldering, music), while fundamental-based sports benefit from fused ego-exocentric perspectives for comprehensive technique analysis. Third, temTable IV shows the optimal configurations identified for each scenario, revealing domain-specific preferences that align with skill characteristics. Basketball achieves the highest absolute accuracy (78.76%) using rapid multi-view sampling (5.33 FPS, 2 segments) that preserves continuous gameplay flow. Music reaches 74.14% through fine-grained egocentric capture (0.89 FPS, 12 segments) suited to sequential note execution patterns. Cooking performs optimally with exocentric-only views (60.53%) using high-frequency sampling (4.00 FPS, 8 segments) for technique visibility, while bouldering achieves 42.31% with rapid egocentric sampling (5.33 FPS, 2 segments) focusing on proprioceptive feedback. TABLE III OPTIMAL CONFIGURATION PATTERNS IDENTIFIED ACROSS DIFFERENT ACTIVITY TYPES. Parameter Frame Count Sampling Rate Optimal Range Activity Type 32 frames All top-performing scenarios High FPS (4.0-5.33) Low FPS (0.89) Dynamic activities (Basketball, Cooking, Dancing, Bouldering) Structured activities (Music) View Configuration Ego Exo only EgoExo Individual skill activities (Music, Bouldering) External observation scenarios (Cooking) Team sports & complex interactions (Basketball, Dancing) Temporal Segmentation Few segments (2) Medium segments (8) Dynamic activities (Cooking, Dancing) Many segments (12) Sequential, fine-grained tasks (Music) Continuous actions (Basketball, Bouldering) Duration Strategy Short clips (1s) Standard clips (3s) High-frequency activities (Cooking, Dancing) Most scenarios (Basketball, Music, Bouldering) TABLE IV OPTIMAL CONFIGURATION DETAILS FOR EACH SCENARIO. Scenario Best Acc. (%) Views Frames Segs Duration (s) Basketball Cooking Dancing Music Bouldering Soccer 78.76 60.53 26.50 74.14 42.31 66. Ego+Exos Exos Ego+Exos Ego Ego All 32 32 32 32 32 24/32 2 8 8 12 2 Various 3 1 1 3 3 3 FPS 5.33 4.00 4.00 0.89 5. Configuration Strategy Rapid sampling, minimal fragmentation High-frequency, external views High-frequency Fine-grained, egocentric capture Rapid sampling, proprioceptive focus Various Consistent across configs TABLE ACCURACY COMPARISON ACROSS DIFFERENT APPROACHES AND VIEWING CONFIGURATIONS. Scenario Maj. Baseline SkillFormer SkillFormer+PATS Ego Exos Ego+Exos Ego Exos Ego+Exos Ego Exos Ego+Exos Basketball Cooking Dancing Music Bouldering Soccer 36.19 50.00 51.61 58.97 0.00 62.50 51.43 45.00 55.65 46.15 25.31 56.25 52.30 35.00 42.74 69.23 17.28 75.00 55.24 35.00 42.74 56.41 17.28 75.00 69.03 31.58 20.51 72.41 30.77 70. 70.80 47.37 15.38 68.97 33.52 66.67 77.88 60.53 13.68 68.10 31.87 66.67 64.60 39.47 22.22 74.14 42.31 66.67 72.57 60.53 20.51 69.83 36.81 66.67 78.76 50.11 26.50 69.01 36.26 66.67 Table provides detailed per-scenario comparisons across all methods and viewing configurations. PATS delivers substantial improvements in several domains: bouldering shows the largest gain (+26.22% over SkillFormer), music improves by +2.39%, and basketball by +1.13%, with the latter reaching the best overall performance across all activities. These results demonstrate PATS effectiveness for proprioceptive skills requiring precise temporal coordination. However, PATS shows mixed results in some scenarios. For dancing, while PATS improves over SkillFormer (26.50% vs 20.51%), it remains below baseline methods (55.65%), suggesting that this domain may require alternative PATS configurations with different parameter combinations beyond those explored in our grid search. For soccer, PATS shows decline specifically in egocentric view accuracy, dropping from 70.83% with SkillFormer to 66.67% with PATS, while maintaining comparable performance in other viewing configurations, indicating that egocentric temporal sampling may be less suitable for soccer fundamental assessment. Overall, these results demonstrate PATS ability to adapt temporal sampling strategies to diverse skill domains while maintaining computational efficiency. VI. LIMITATIONS AND FUTURE WORK Despite significant advances, PATS faces challenges in certain activity domains. In subjective domains like dancing, baseline methods outperform our approach, suggesting inadequate capture of important rhythmic and aesthetic components. Soccer presents another limitation, where PATS degrades egocentric performance while other viewing configurations remain stable, indicating that our sampling strategy may negatively impact specific activity-view combinations. The requirement for scenario-specific configuration also limits practical applicability across new domains. [11] Y. Tang, Z. Ni, J. Zhou, D. Zhang, J. Lu, Y. Wu, and J. Zhou, Uncertainty-aware score distribution learning for action quality assessment, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 98399848. [12] X. Yu, Y. Rao, W. Zhao, J. Lu, and J. Zhou, Group-aware contrastive regression for action quality assessment, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 79197928. [13] E. Bianchi and O. Lanz, Gate-shift-pose: Enhancing action recognition in sports with skeleton information, in Proceedings of the Winter Conference on Applications of Computer Vision (WACV) Workshops, February 2025, pp. 12571264. [14] H. Duan, Y. Zhao, K. Chen, D. Lin, and B. Dai, Revisiting skeletonbased action recognition, in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 29592968. [15] J. Xia, M. Zhuge, T. Geng, S. Fan, Y. Wei, Z. He, and F. Zheng, Skating-mixer: Long-term sport audio-visual modeling with mlps, 2022. [Online]. Available: https://arxiv.org/abs/2203.03990 [16] L.-A. Zeng and W.-S. Zheng, Multimodal action quality assessment, IEEE Transactions on Image Processing, 2024. [17] B. Braun, R. Armani, M. Meier, M. Moebus, and C. Holz, egoppg: Heart rate estimation from eye-tracking cameras in egocentric systems to benefit downstream vision tasks, 2025. [Online]. Available: https://arxiv.org/abs/2502.20879 [18] Y. Huang, G. Chen, J. Xu, M. Zhang, L. Yang, B. Pei, H. Zhang, D. Lu, Y. Wang, L. Wang, and Y. Qiao, Egoexolearn: dataset for bridging asynchronous egoand exo-centric view of procedural activities in real world, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. [19] K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, T. Afouras, K. Ashutosh, V. Baiyya, S. Bansal, B. Boote, E. Byrne, Z. Chavis, J. Chen, F. Cheng, F.-J. Chu, S. Crane, A. Dasgupta, J. Dong, M. Escobar, C. Forigua, A. Gebreselasie, S. Haresh, J. Huang, M. M. Islam, S. Jain, R. Khirodkar, D. Kukreja, K. J. Liang, J.-W. Liu, S. Majumder, Y. Mao, M. Martin, E. Mavroudi, T. Nagarajan, F. Ragusa, S. K. Ramakrishnan, L. Seminara, A. Somayazulu, Y. Song, S. Su, Z. Xue, E. Zhang, J. Zhang, A. Castillo, C. Chen, X. Fu, R. Furuta, C. Gonzalez, P. Gupta, J. Hu, Y. Huang, Y. Huang, W. Khoo, A. Kumar, R. Kuo, S. Lakhavani, M. Liu, M. Luo, Z. Luo, B. Meredith, A. Miller, O. Oguntola, X. Pan, P. Peng, S. Pramanick, M. Ramazanova, F. Ryan, W. Shan, K. Somasundaram, C. Song, A. Southerland, M. Tateno, H. Wang, Y. Wang, T. Yagi, M. Yan, X. Yang, Z. Yu, S. C. Zha, C. Zhao, Z. Zhao, Z. Zhu, J. Zhuo, P. Arbelaez, G. Bertasius, D. Damen, J. Engel, G. M. Farinella, A. Furnari, B. Ghanem, J. Hoffman, C. Jawahar, R. Newcombe, H. S. Park, J. M. Rehg, Y. Sato, M. Savva, J. Shi, M. Z. Shou, and M. Wray, Ego-exo4d: Understanding skilled human activity from firstand third-person perspectives, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024, pp. 19 38319 400. [20] G. Bertasius, H. Wang, and L. Torresani, Is space-time attention all you need for video understanding? 2021. [Online]. Available: https://arxiv.org/abs/2102.05095 [21] J. Carreira and A. Zisserman, Quo vadis, action recognition? new model and the kinetics dataset, in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 47244733. Future research should focus on automated configuration selection mechanisms, enhanced temporal modeling for rhythmic activities, and multi-modal integration incorporating audio and haptic feedback. Meta-learning approaches for rapid domain adaptation could further improve generalization across diverse skill assessment domains. VII. DISCUSSION AND CONCLUSIONS This work introduces PATS (Proficiency-Aware Temporal Sampling), achieving consistent state-of-the-art improvements across egocentric, exocentric, and combined viewing configurations (+0.65% to +3.05%) while maintaining computational efficiency. Our systematic analysis reveals actionable design principles: 32 frames prove universally optimal, sampling rates should match activity dynamics (high for dynamic, low for structured activities), and segmentation strategies must align with action continuity. The substantial scenario-specific improvements, particularly in bouldering (+26.22%), music (+2.39%), and basketball (+1.13%), validate that domain-aware temporal sampling meaningfully enhances proficiency estimation. The alignment between optimal configurations and activity semantics demonstrates PATS semantic grounding and adaptability across diverse skill domains. PATS represents significant advancement in automated skill assessment, providing practitioners with principled temporal sampling guidelines while establishing robust foundation for accurate and interpretable skill assessment systems in sports training and education."
        },
        {
            "title": "REFERENCES",
            "content": "[1] E. Bianchi and A. Liotta, Skillformer: Unified multi-view video understanding for proficiency estimation, 2025. [Online]. Available: https://arxiv.org/abs/2505.08665 [2] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool, Temporal segment networks for action recognition in videos, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 11, pp. 27402755, 2019. [3] Y. Liu et al., When the future becomes the past: Taming temporal correspondence for self-supervised video representation learning, arXiv preprint arXiv:2503.15096, 2025. [4] K. Kim, S. N. Gowda, O. Mac Aodha, and L. Sevilla-Lara, Capturing temporal information in single frame: Channel sampling strategies for action recognition, in 33rd British Machine Vision Conference 2022, BMVC 2022. BMVA Press, 2022. [5] M. Kim, D. Han, T. Kim, and B. Han, Leveraging temporal contextualization for video action recognition, in European Conference on Computer Vision. Springer, 2024, pp. 7491. [6] K. Zhou, R. Cai, L. Wang, H. P. Shum, and X. Liang, comprehensive survey of action quality assessment: Method and benchmark, arXiv preprint arXiv:2412.11149, 2024. [7] J. Carreira and A. Zisserman, Quo vadis, action recognition? new model and the kinetics dataset, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 62996308. [8] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, Video swin transformer, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 32023211. [9] S. Wang, D. Yang, P. Zhai, C. Chen, and L. Zhang, Tsa-net: Tube selfattention network for action quality assessment, in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 4902 4910. [10] J. Xu, Y. Rao, X. Yu, G. Chen, J. Zhou, and J. Lu, Finediving: fine-grained dataset for procedure-aware action quality assessment, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 29492958."
        }
    ],
    "affiliations": [
        "Faculty of Engineering Free University of Bozen-Bolzano Bozen-Bolzano, Italy"
    ]
}