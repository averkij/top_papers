{
    "paper_title": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens",
    "authors": [
        "Dongwon Kim",
        "Ju He",
        "Qihang Yu",
        "Chenglin Yang",
        "Xiaohui Shen",
        "Suha Kwak",
        "Liang-Chieh Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image tokenizers form the foundation of modern text-to-image generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from a simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce a family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models."
        },
        {
            "title": "Start",
            "content": "Democratizing Text-to-Image Masked Generative Models with Compact Text-Aware One-Dimensional Tokens Dongwon Kim*1,2, Ju He*1, Qihang Yu*1, Chenglin Yang1, Xiaohui Shen1, Suha Kwak2, Liang-Chieh Chen1 2 POSTECH * equal contribution 1 ByteDance Seed 5 2 0 J 3 1 ] . [ 1 0 3 7 7 0 . 1 0 5 2 : r https://tacju.github.io/projects/maskgen.html"
        },
        {
            "title": "Abstract",
            "content": "Image tokenizers form the foundation of modern text-toimage generative models but are notoriously difficult to train. Furthermore, most existing text-to-image models rely on large-scale, high-quality private datasets, making them challenging to replicate. In this work, we introduce Text-Aware Transformer-based 1-Dimensional Tokenizer (TA-TiTok), an efficient and powerful image tokenizer that can utilize either discrete or continuous 1-dimensional tokens. TA-TiTok uniquely integrates textual information during the tokenizer decoding stage (i.e., de-tokenization), accelerating convergence and enhancing performance. TA-TiTok also benefits from simplified, yet effective, one-stage training process, eliminating the need for the complex two-stage distillation used in previous 1-dimensional tokenizers. This design allows for seamless scalability to large datasets. Building on this, we introduce family of text-to-image Masked Generative Models (MaskGen), trained exclusively on open data while achieving comparable performance to models trained on private data. We aim to release both the efficient, strong TA-TiTok tokenizers and the open-data, open-weight MaskGen models to promote broader access and democratize the field of text-to-image masked generative models. 1. Introduction In recent years, text-to-image generation has seen remarkable progress across various frameworks, including diffusion models [23, 50, 51], autoregressive visual models [26, 57, 58], and masked generative models [7, 12, 24]. crucial component of these models is robust image tokenizereither discrete [61] or continuous [35]which transforms images into tokenized representations. These models then incorporate text conditions with the tokenized representations using methods such as cross-attention [50], concatenation [8], or conditioning embeddings [44], ultimately leveraging the tokenizer to generate high-quality images aligned with input text prompts. Despite these advancements, replicating these models remains challenging due to the substantial resources required. Model sizes have grown from millions to billions of parameters, leading to prohibitively high training and inference costs. Furthermore, many of these models rely on large-scale, high-quality proprietary image-text datasets, which are critical for achieving high-fidelity generation but prevent the research community from fully reproducing results. Given these obstacles, pivotal question arises: Can we develop text-to-image generative model that is both efficient and effective using only open data, enabling reproducibility? In this work, we address this question by building on the recent concept of compact one-dimensional tokenizers to facilitate both efficient and high-quality image tokenization. Traditional tokenizers [22, 35, 61] rely on 2D gridbased latent representations, which struggle to handle the inherent redundancies in images, as neighboring patches often display similarities. Recently, Yu et al. [69] introduced the Transformer-based 1-Dimensional Tokenizer (TiTok), which efficiently tokenizes images into compact 1D latent sequences by removing the fixed correspondence between latent representation and 2D image patches (i.e., each 1D token can represent any region in an image, rather than being tied to specific patch). This approach results in highly efficient tokenizer, significantly improving sampling speed compared to previous methods [11, 22, 44, 61]. However, extending TiTok to support text-to-image generation presents three main challenges: (1) its reliance on complex two-stage training pipeline, which limits scalability to larger datasets necessary for diverse text-to-image generation beyond ImageNet [17]; (2) its restriction to VectorQuantized (VQ) variant, leaving unexplored the potential benefits of continuous Variational Autoencoder (VAE) representation; and (3) its focus on reconstructing low-level image details, which may lack the high-level semantics needed for effective alignment with textual descriptions. 1 Figure 1. Text-to-Image (T2I) Generation Results by MaskGen. MaskGen, powered by the proposed compact text-aware 1D tokenizer TA-TiTok, is an efficient masked generative model that achieves state-of-the-art performance on multiple T2I benchmarks using only open data. The open-data, open-weight MaskGen models are designed to promote broader access and democratize T2I masked generative models. To address these limitations, we introduce several key innovations. First, we streamline the training process for the 1D tokenizer by developing an efficient one-stage training procedure, removing the need for the complex two-stage pipeline used in the original framework [69]. This improvement enables scalable training of the 1D tokenizer on largescale text-image datasets without multi-stage complexity. Second, we extend the 1D tokens to continuous VAE representations, which allow for more consistent and accurate image reconstructions than the VQ counterpart. This approach combines the sampling efficiency of 1D tokens (due to the reduced number of tokens) with the improved reconstruction quality afforded by the continuous VAE representation, eliminating the quantization loss seen in VQ. Third, we incorporate textual information during the detokenization stage to enhance semantic alignment with text prompts. Specifically, by concatenating CLIP [47] embeddings of captions with the tokenized image representations, we enable higher-quality image reconstructions that better retain fine details. This approach powers TA-TiTok, our novel and efficient Text-Aware Transformer-based 1-Dimensional Tokenizer, trained on the large-scale dataset (e.g., DataComp [25]) to capture broad and diverse range of concepts. Building upon TA-TiTok, we introduce MaskGen, family of text-to-image Masked Generative models. MaskGen is versatile framework that supports both discrete and continuous token representations. For images represented by discrete tokens, MaskGen is trained using cross-entropy loss [11], while for images with continuous tokens, it leverages the recent diffusion loss [39]. To encode captions as text conditioning, we utilize CLIP [47], instead of the more resource-intensive T5-XXL encoder [48], used in other recent text-to-image models [12, 23, 51]. Although effective, T5-XXL incurs significantly higher computational and storage costs. CLIP provides more efficient alternative, making our approach more accessible to research groups with limited compute resources. In terms of architecture, we adopt straightforward design: we concatenate text conditions with image tokens before feeding them to the Diffusion Transformer [44], applying separate adaptive LayerNorm (adaLN [6, 44]) parameters for text and image modalities, as in MM-DiT [23]. Additionally, we find it beneficial to incorporate the aesthetic score as an additional conditioning signal via adaLN. This allows for more nuanced control over the generated images, beyond text input alone. To ensure reproducibility, we exclusively use images 2 from publicly available sources, including DataComp [25], LAION [53], and CC12M [13], as well as synthetic data from JourneyDB [56] and DALL-E 3 [9, 21]. Given the noisiness of web-sourced text-image pairs, we filter images based on aesthetic scores ( 5), resolution (aspect ratio < 2 and longer side 256), and remove any images containing watermarks. Following the approach of DALL-E 3 [9], we further enhance text quality by recaptioning high-aesthetic subsets from DataComp and subsets from LAION (LAIONpop and LAION-art [53]) using the state-of-the-art vision language model Molmo [16]. Notably, despite being trained entirely on publicly available datasets, MaskGen achieves strong performance and efficiency in text-to-image generation. On MJHQ-30K [36], the lighter MaskGen-L (568M) with discrete tokens achieves generation FID of 7.74, outperforming Show-o [63] (14.99) with 30.3 faster inference throughput. It also surpasses SD-2.1 [50] (26.96) and PixArt-α [14] (9.85) while requiring only 2% and 21% of their training times, respectively. Furthermore, the larger MaskGen-XL (1.1B) achieves FID scores of 7.51 and 6.53 on MJHQ-30K [36] and overall scores of 0.57 and 0.55 on the GenEval [27] benchmark using discrete and continuous tokens, respectively. To promote further research on text-to-image masked generative models, we will release the training code and model weights for both TA-TiTok and MaskGen. To our knowledge, MaskGen is the first open-weight, open-data masked generative model for text-to-image synthesis to achieve performance comparable to state-of-the-art models, advancing the democratized access to high-performance masked generative models in this field. 2. Related Work Image Tokenization. Modern generative image models rely on image tokenization for efficient generation [11, 22, 50, 65]. During training, images are encoded into discrete [61] or continuous [35] tokens, allowing the model to focus on learning semantically meaningful information [50] rather than directly working with pixels [29]."
        },
        {
            "title": "Image tokenization approaches fall",
            "content": "into two main paradigms. The first, discrete tokenization [61], maps each token to codebook entry and is well-suited to autoregressive [22] or masked generative models [11], as it enables techniques directly from language models [10]. Introduced in VQ-VAE [61] and later improved in VQGAN [22] with adversarial loss [28], this approach has been further scaled through advanced codebook management techniques [64, 67, 72]. The second paradigm, continuous tokenization, follows the VAE [35] framework, enabling latent representations drawn from normal distribution. While less common with masked generative models due to the simpler loss definitions with discrete tokens, continuous tokenization was recently 3 adapted for such models in MAR [39], using diffusion module to sample tokens from the normal distribution. Image Generation with Sequence Models. Initially developed for language tasks, sequence models like BERT [18] and GPT [10] have been effectively adapted for image generation. Early approaches focused on autoregressive pixel generation [15, 29, 43, 60], but recent methods model the joint distribution of image tokens, leading to two main approaches: autoregressive [22] and masked generative models [11]. Autoregressive models predict tokens sequentially, following GPTs strategy [42, 49, 57, 65, 68], while masked generative models adopt BERT-like objective, predicting masked tokens simultaneously. This approach gives masked models substantial edge in sampling speed, as they do not require token-by-token generation [11, 38, 62, 66, 67]. Building on these efficiency benefits, our work develops an open-source masked generative model that leverages 1D tokenization for efficient text-to-image generation. Text-to-Image Generation. While diffusion models dominate text-to-image generation [8, 14, 41, 44, 45, 50, 51], sequence models have shown strong potential as well [12, 24, 26, 65]. Examples like Muse [12] and Parti [65] demonstrate the success of masked generative and autoregressive approaches for generating high-quality images from text. Recent innovations in diffusion models, such as improved architectures [23, 44], micro-conditioning for finer control [45], and advanced image recaptioning for better text-image alignment [9], offer potential benefits for masked generative models. In our work, we integrate these recent improvements from diffusion models into the masked generative model framework. By incorporating these improvements, we aim to enhance the quality of generation while maintaining their inherent advantages in sampling efficiency. 3. Preliminary TiTok [69] is transformer-based, 1-dimensional VectorQuantized (VQ) tokenizer that diverges from traditional 2D grid-based latent space tokenization, instead opting for compact 1D representation that bypasses 2D spatial structure preservation. Given an input image RHW 3, the tokenization phase of TiTok involves downscaling the image by factor of , resulting in patches D. These patches are concatenated with set of latent tokens RKD. The combined sequence is then passed through Vision Transformer (ViT) [20] encoder, Enc, to generate embeddings. Only the embeddings corresponding to the latent tokens, Z1D RKD, are retained, forming compact 1D latent representation. This representation is then quantized through quantizer Quant by mapping it to the nearest codes in learnable codebook. In the de-tokenization phase, TiTok uses sequence of D, which are concatenated with mask tokens Figure 2. Overview of TA-TiTok (Text-Aware Transformer-based 1-Dimensional Tokenizer). (a) TA-TiTok introduces three key enhancements to TiTok [69]: First, an efficient one-stage training procedure replaces the need for complex two-stage pipeline. Second, TA-TiTok supports 1D tokens in both discrete (VQ) and continuous (KL) formats. Third, it incorporates textual information (using CLIPs text encoder) during de-tokenization to improve semantic alignment with text captions. (b) comparison of reconstruction results shows that TA-TiTok achieves superior reconstruction quality over TiTok. the quantized codes. The resulting sequence is processed by ViT decoder, Dec, to reconstruct the image ˆI. Formally, with denoting concatenation, the tokenization and detokenization processes in TiTok can be represented as: Z1D = Enc(P L), ˆI = Dec(Quant(Z1D) M). Masked Generative Models with Discrete Tokens [11, 66] adapt the masked language modeling framework [18] for image generation. During training, portion of image tokens is masked, and bidirectional transformer predicts these tokens using the surrounding context. The model employs classification head to select tokens from predefined codebook [61] and uses cross-entropy loss for training. During sampling, the model iteratively predicts tokens for masked positions, retaining high-confidence tokens while re-masking uncertain ones until all positions are filled [11]. The completed sequence of tokens is then de-tokenized into pixel space to form the final image. Masked Generative Models with Continuous Tokens maintain conceptual similarity to discrete-token models but operate on continuous tokens, which reduces information loss from quantization. Recently, the diffusion loss [24, 39] was introduced, enabling these models to approximate the distribution of each image token independently. In this framework, Transformers generate conditioning vector for each masked token, which is then input to small multi-layer perceptron (MLP) that learns denoising function [33] conditioned on it. This per-token conditioning and denoising allow the sampling process to directly apply to the probability distribution of each token [39]. 4. Method In this section, we first present TA-TiTok, text-aware transformer-based 1-dimensional tokenizer (Sec. 4.1), followed by MaskGen, family of text-to-image (T2I) masked generative models built upon TA-TiTok (Sec. 4.2). 4.1. TA-TiTok: Text-aware 1D Tokenizer TA-TiTok is novel text-aware image tokenizer that introduces three key enhancements to TiTok [69]. First, we develop an improved one-stage training approach; second, we extend TiTok to support both discrete and continuous tokens; and third, we incorporate textual information during de-tokenization to enhance semantic alignment. Each improvement is detailed below. Improved One-Stage Training Recipe. The recently proposed class-conditional masked generative model, MaskBit [62], introduced several techniques to enhance VQGAN [22] training, two of which we incorporate in the training of our proposed TA-TiTok. First, MaskBit demonstrated that using ResNet50 [30] for perceptual loss [34] yields richer features than the VGG network [54] used in LPIPS [71], thereby improving tokenizer training. Second, we strengthen the PatchGAN [22] discriminator by replacing traditional average pooling with blur kernels [70] and adding LeCAM regularization [59] during training. Our experiments, consistent with MaskBits findings, confirm that these enhancements lead to improved image reconstruction quality through 1D tokens. Extending TiTok to Support Continuous Tokens. To maximize the efficiency of the TiTok framework in diffusion4 based models, we extend its discrete 1D tokens to continuous 1D VAE representations. Rather than using quantizer Quant to map the 1D latents Z1D to the nearest codebook entries, we model Z1D as Gaussian distribution and apply KL divergence regularization, resulting in compact 1D VAE representation. This continuous representation retains the efficiency and structure of the TiTok framework, consistently improving reconstruction quality by avoiding the information loss associated with quantization. Moreover, this KL variant of TiTok integrates seamlessly with diffusion models, serving as drop-in replacement for standard 2D VAEs [39, 50, 55]. In the supplementary materials (Tab. 14 in Appendix Sec. F), we validate this approach using the state-of-the-art image generation model MAR [39] on ImageNet [17], where our modification achieves significant reduction in training costs and an increase in inference speed, all while maintaining comparable performance. This enhancement thus contributes to both the efficiency and flexibility of diffusion-based generation. For clarity, we refer to both variants collectively as TiTok, using the labels VQ and KL to denote the discrete and continuous versions, respectively, in the following text. Text-aware De-tokenization. While TiTok effectively utilizes compact 1D tokens to capture richer semantic information than traditional 2D tokenizers, its primary focus remains on reconstructing low-level image details in the detokenization stage. Additionally, previous image tokenizers have largely overlooked the potential of textual information to enhance high-level semantic alignment, even when such information is available. This gap motivates us to introduce TA-TiTok, text-aware, transformer-based 1D tokenizer designed to improve alignment with textual descriptions. As shown in Fig. 2(a), the tokenization stage in TA-TiTok mirrors that of TiTok, transforming images into compact 1D tokens, either discrete or continuous. In the de-tokenization stage, however, TA-TiTok incorporates text guidance by using text embeddings generated by pre-trained visionlanguage model (e.g., CLIPs text encoder [47]). These text embeddings are projected through linear layer to align with the channel dimensions of our TA-TiToks ViT decoder, resulting in RN D, where is the number of context tokens predefined by the vision-language model (e.g., 77 for CLIPs text encoder). This text embedding is then concatenated with the latent tokens Z1D and the mask tokens before passing through the decoder Dec, yielding the reconstructed image ˆI. Formally, the de-tokenization phase of the VQ variant can be expressed as: ˆI = Dec(Quant(Z1D) M). For the KL variant, the de-tokenization phase follows similar formulation but omits the quantizer Quant, as it operates on continuous representations directly. TA-TiTok Design Discussion. Notably, TA-TiTok incurs minimal additional computational cost compared to TiTok. By extending the de-tokenization sequence length by (i.e., the total number of tokens becomes + K, where = 77 for CLIPs text tokens and represents 32, 64, or 128 latent tokens), TA-TiTok still requires fewer computations than typical 2D tokenizers [39], which utilize 256 tokens. This design enables TA-TiTok to retain high efficiency while achieving reconstructions that closely align with text descriptions, effectively mitigating the information loss associated with compact 1D tokenization, as shown in Fig. 2(b). The design of TA-TiTok incorporates text tokens exclusively into the tokenizer decoder with minimal modifications. To validate this approach, we also experimented with adding text tokens to both the encoder and decoder. The results showed similar performance to the decoder-only approach, suggesting that incorporating textual information during detokenization is sufficient for capturing high-level semantic information. Based on these findings, we adopt the simpler decoder-only design. For additional details, see Tab. 9 in Appendix Sec. E. 4.2. MaskGen: T2I Masked Generative Model To fully leverage TA-TiToks capabilities, we propose MaskGen, family of text-to-image (T2I) masked generative models. As illustrated in Fig. 3, MaskGen utilizes TA-TiTok to tokenize images into tokens and CLIP [47] text encoder to extract both global and pooled text embeddings. Inspired by [23], we concatenate the global text embedding with the image tokens, feeding this combined sequence into multimodal Diffusion Transformer (MM-DiT) blocks [23, 44] for attention operations. To accommodate the distinct properties of text and image embeddings, we apply separate adaptive LayerNorm (adaLN [6, 44]) layers, where the scale and shift parameters are computed based on the sum of the embedding vectors of the pooled text embedding and the time step embedding. We note that while more powerful text encoder, such as T5-XXL [48], could be directly integrated into the pipeline to enhance performance, as demonstrated in studies like [12, 23, 51], we choose CLIP for its computational efficiency and reduced storage demands, making MaskGen more accessible in resource-constrained settings. We also incorporate aesthetic scores [53] as another condition by projecting them similarly into sinusoidal embeddings and appending them to the pooled text embedding. This feature provides an extra layer of control over the image quality and style during sampling, making the generation process more adaptable and flexible. MaskGen is versatile framework that accommodates both discrete and continuous tokens produced by TA-TiTok. For discrete tokens, MaskGen is trained with cross-entropy loss, as in [11, 12], to predict the correct one-hot codebook index for masked tokens. In the case of continuous 5 Figure 3. Overview of MaskGen. MaskGen is family of text-to-image masked generative models that supports both discrete (VQ variant) and continuous (KL variant) token representations. For discrete tokens, MaskGen is trained with cross-entropy loss [11], while for continuous tokens, it employs diffusion loss [39]. The architecture is designed by concatenating text conditions with TA-TiToks latent tokens (both masked and unmasked) and feeding them into Diffusion Transformer blocks [44], with separate adaptive LayerNorm (adaLN) layers for text and image modalities, following MM-DiT [23]. Additionally, aesthetic scores are incorporated as conditioning signals via adaLN. To encode captions, MaskGen uses the CLIP text encoder [47] instead of the more resource-intensive T5-XXL [48], making it more accessible to research groups with limited computational resources. tokens, MaskGen leverages the recently proposed diffusion loss [39], applying small MLP to directly approximate the distribution of each masked token. This adaptability makes MaskGen capable of handling various token types with ease. Additionally, thanks to the compact 1D token sequence produced by TA-TiTok, MaskGen is highly efficient, reducing training costs and enhancing sampling speed by minimizing token count. Together, these features help MaskGen to democratize access to efficient, high-performance masked generative models for text-to-image generation. 5. Experimental Results In this section, we provide the implementation details of TATiTok and MaskGen in Sec. 5.1. We then present ablation studies on key design choices in Sec. 5.2. The main results are further discussed in Sec. 5.3. 5.1. Implementation Details We implement TA-TiTok based on TiTok [69] with minimal modifications. We then introduce MaskGen model variants and the datasets used for training, along with the training and evaluation protocols applied in our study. TA-TiTok Model Variants. We present three variants of our TA-TiTok, each containing = 32, 64, or 128 1D latent tokens, following the architecture of TiTok. Both the tokenizer and de-tokenizer utilize patch size of = 16. For the VQ variant, the codebook is configured with 8192 entries, where each entry is vector with 64 channels. For the KL variant, we use continuous embedding with 16 channels, following MAR [39]. For the encoder (Enc) and decoder (Dec), we find that increasing the size of Dec to ViT-L [20] model MaskGen-L 16 MaskGen-XL 20 depth width mlp heads #params 1024 4096 1280 16 16 568M 1.1B Table 1. Architecture Configuration of MaskGen. Following prior work, we scale up MM-DiT blocks across two configurations. is beneficial when training on large-scale datasets across all variants. For Enc, we adopt ViT-B for all variants except the KL variant with 128 tokens, where ViT-S is sufficient. MaskGen Model Variants. We introduce two variants of MaskGen: MaskGen-L (large, 568M parameters) and MaskGen-XL (extra-large, 1.1B parameters), with configurations detailed in Tab. 1. For continuous token processing in MaskGen, we incorporate an additional DiffLoss MLP [39], comprising 8 MLP layers with channel sizes aligned to the transformers, adding an extra 44M and 69M parameters for MaskGen-L and MaskGen-XL, respectively. To offset the additional training and inference cost introduced by the DiffLoss MLP, we use 128 tokens for the discrete MaskGen variant and 32 tokens for the continuous MaskGen variant. Ablation studies on the impact of token count for MaskGen are provided in the Appendix. Dataset (recaption). For training TA-TiTok and MaskGen, we utilized various open-source datasets: DataComp1B [25], CC12M [13], LAION-aesthetic [1], LAION-art [3], LAION-pop [4], JourneyDB [56], and DALLE3-1M [21]. All training images are filtered to ensure their longer side is greater than 256 pixels and their aspect ratio is less than 2. TA-TiTok is trained exclusively with DataComp-1B. MaskGen undergoes two-stage training process: first, it 6 tokenizer TiTok [69] VQ 1-stage [22] VQ our 1-stage TiTok arch training setting #tokens rFID IS 5.15 120.5 2.43 179.3 64 64 Table 2. Ablation on Improved One-Stage Training Recipe. Both models are trained and evaluated on ImageNet. is pre-trained for image-text alignment using DataComp1B, CC12M, and LAION-aesthetic, where DataComp-1B and CC12M are additionally filtered to include only images with aesthetic scores [2] higher than 5.0. Following pre-training, MaskGen is fine-tuned using aesthetic images from LAION-art, LAION-pop, JourneyDB, DALLE3-1M, and DataComp-1B (filtered for aesthetic scores above 6.0). For LAION-art, LAION-pop, and the filtered DataComp-1B datasets, we enhance the image captions using Molmo [16], employing one of four different prompts along with the original caption. Details regarding the dataset preparation and recaptioning process are provided in the Appendix. Training. We adhere strictly to the hyperparameters used to train TiTok across all TA-TiTok variants. Specifically, TATiTok is trained with batch size of 1024 for 1 epoch (650k steps) on the filtered DataComp dataset, using maximum learning rate of 1e-4 and cosine learning rate schedule. For MaskGen with discrete tokens, we employ batch size of 4096, leveraging weight tying [46] to stabilize training, with cosine learning rate schedule and maximum learning rate of 4 10-4. For MaskGen with continuous tokens, to accommodate the diffusion loss, we use constant learning rate schedule with maximum rate of 1 10-4 and batch size of 2048. Masked tokens are sampled by randomly selecting the masking rate from [0, 1] on cosine schedule, following MaskGIT [11], and text conditioning is randomly dropped with 0.1 probability to enable classifier-free guidance [32]. Full training hyperparameters are in the Appendix. Evaluation. Our evaluation pipeline closely follows prior works [12, 65]. The images are generated from text prompts without rejection sampling, and classifier-free guidance [32] has been used to enhance generation quality. Unless specified otherwise, MaskGen uses 16 and 32 sampling steps for VQ and KL architectures, respectively. To assess different aspects of our models performance, we employ multiple evaluation metrics. For TA-TiTok, we measure reconstruction quality using reconstruction FID (rFID) [31] and inception score (IS) [52] on ImageNet [17] validation set. For MaskGens text-to-image generation capabilities, we utilize comprehensive set of metrics: FID on MJHQ [37] to assess aesthetic quality, and GenEval [27] score to measure the alignment between text prompts and their corresponding generated images. arch VQ KL IS TiTok rFID TA-TiTok tokens rFID IS # 7.72 98.3 4.09 (-3.63) 215.9 (+117.6) - 32 4.25 138.0 2.68 (-1.57) 213.5 (+75.5) - 64 128 - 2.63 168.1 1.78 (-0.85) 216.9 (+48.8) 32 16 2.56 171.7 1.53 (-1.03) 222.0 (+50.3) 64 16 1.64 198.0 1.47 (-0.17) 220.7 (+22.7) 128 16 1.02 209.7 0.90 (-0.12) 227.7 (+18.0) Table 3. Ablation on Text-Aware De-tokenization Design. To ensure fairness, all models are trained using our improved one-stage training recipe on the DataComp dataset and evaluated in zeroshot setting on the ImageNet validation set. Relative improvements for text-aware variants are highlighted in blue. #: Number of tokens. c: Channels of continuous tokens. tokenizer TA-TiTok KL arch #tokens text guidance rFID IS 1.62 213.6 ID 1.53 222.0 Embedding 32 Table 4. Ablation on Text Guidance Type. Models are trained on DataComp and zero-shot evaulated on ImageNet validation set. ID refers to numerical IDs extracted by CLIP text tokenizer, Embedding denotes text features extracted by CLIP text encoder. 5.2. Ablation Studies Improved One-Stage Training Recipe. Tab. 2 summarizes the performance gains achieved by our improved one-stage training recipe compared to the original training schemes in [69]. As shown, the adopted one-stage training significantly outperforms the original one-stage training with an rFID improvement of 2.72. Effect of Text-Aware De-Tokenization. We evaluate the impact of text-aware de-tokenization (TA-TiTok) in Tab. 3. For consistency, all models are trained using our improved one-stage training recipe. Tokenizers are trained on the DataComp dataset [25] and evaluated in zero-shot setting on the ImageNet validation set [17], where the caption is simply represented as photo of class without any prompt engineering. We compare two architecturesdiscrete tokens (VQ) and continuous tokens (KL)and vary the token count between 32, 64, and 128. As shown in the table, continuous tokens consistently outperform discrete tokens, aligning with findings in [22]. Additionally, TA-TiTok consistently outperforms TiTok (the non-text-aware variant) across all configurations. Notably, the performance gain is most pronounced with smaller number of tokens (e.g., 32) and with discrete tokens. We attribute this to the fact that the latent tokens primarily capture low-level image details, while highlevel semantics provided by text embeddings enrich these representations. Consequently, the improvement is more substantial with fewer tokens (where learning semantic detail is more challenging) and with vector-quantized tokens (where quantization introduces information loss). 7 arch type generator VQ AR LlamaGen [57] tokenizer VQGAN [57] MAGVIT-v2 [63] VQ AR Show-o [63] TA-TiTok TA-TiTok VQ Mask. MaskGen-L (ours) VQ Mask. MaskGen-XL (ours) #params 775M 1.3B 568M 1.1B resolution 512 512 256 256 256 256 256 256 open-data - - 20.0 35. VAE [50] VAE [50] VAE [45] TA-TiTok TA-TiTok KL Diff. Stable-Diffusion-2.1 [50] KL Diff. PixArt-α [14] KL Diff. SDXL [45] KL Mask. MaskGen-L (ours) KL Mask. MaskGen-XL (ours) 860M 630M 2.6B 768 768 256 256 1024 1024 568M + 44M 256 256 1.1B + 69M 256 256 1041.6 94.1 - 18.5 30. FID - 25.59 1.0 14.99 30.3 7.74 18.5 7.51 - 7.9 - 26.96 9.85 8.76 11.1 7.24 6.53 9.1 Table 5. Zero-Shot Text-to-Image Generation Results on MJHQ-30K. Comparison of MaskGen with state-of-the-art open-weight models. VQ denotes discrete tokenizers and KL stands for continuous tokenizers. type indicates the generative model type, where AR, Diff. and Mask. refer to autoregressive models, diffusion models and masked transformer models, respectively. T: Generator training cost, measured in 8 A100 days using float16 precision. I: Generator inference throughput, measured in samples per second on single A100 with batch size 64 using float16 precision. We compare inference throughput with methods using the same resolution. arch generator VQ LlamaGen [57] tokenizer VQGAN [57] MAGVIT-v2 [63] VQ Show-o [63] TA-TiTok TA-TiTok VQ MaskGen-L (ours) VQ MaskGen-XL (ours) #params 775M 1.3B 568M 1.1B VAE [50] VAE [50] VAE [50] VAE [45] TA-TiTok TA-TiTok KL Stable-Diffusion-1.5 [50] KL PixArt-α [14] KL Stable-Diffusion-2.1 [50] KL SDXL [45] KL MaskGen-L (ours) KL MaskGen-XL (ours) 860M 630M 860M 2.6B 568M + 44M 1.1B + 69M open-data S. Obj. T. Obj. Count. Colors Position C. Attri. Overall 0.71 0.95 0.98 0.99 0.97 0.96 0.98 0.98 0.99 0.99 0.34 0.52 0.57 0.61 0.38 0.49 0.51 0.74 0.57 0. 0.21 0.49 0.46 0.55 0.35 0.47 0.44 0.39 0.36 0.47 0.58 0.82 0.80 0.81 0.76 0.79 0.85 0.85 0.80 0.77 0.07 0.11 0.11 0.13 0.04 0.06 0.07 0.15 0.11 0. 0.04 0.28 0.25 0.31 0.06 0.11 0.17 0.23 0.29 0.34 0.32 0.53 0.53 0.57 0.43 0.48 0.50 0.55 0.52 0.55 Table 6. Zero-Shot Text-to-Image Generation Results on GenEval. Comparison of MaskGen with state-of-the-art open-weight models. Text Guidance Type. In our text-aware de-tokenization design, we can use either the numerical IDs from the CLIP text tokenizer or the embeddings from the CLIP text encoder. We ablate this design choice in Tab. 4, where the latter option yields marginal improvement. 5.3. Main Results MJHQ-30K. We report the zero-shot text-to-image generation results on MJHQ-30K [37] in Tab. 5. MaskGen-XL (discrete tokens) achieves significantly better FID compared to recent autoregressive models like LlamaGen [57] (7.51 vs. 25.59) and Show-o [63] (7.51 vs. 14.99), both of which also use VQ tokenizers. Additionally, it offers 18.5 improvement in inference throughput over Show-o, with further gains (30.3 faster) achieved by the lighter MaskGen-L model, albeit with slight performance drop. MaskGen also demonstrates remarkable resource efficiency. MaskGen-L completes training in just 20.0 8-A100 days, while MaskGen-XL finishes within 35.0 8-A100 days, showcasing both strong performance and efficiency. For continuous tokens, MaskGen delivers competitive results against recent diffusion models. With only 32 tokens, the MaskGen-L outperforms PixArt-α [14] (630M) (7.24 vs. 9.85), offering 1.4 faster inference throughput while using fewer parameters and requiring less than one fifth of the training resources (18.5 vs. 94.1 8-A100 days). MaskGenXL further improves the FID score to 6.53, achieving much better performance compared to SDXL [45] (6.53 vs. 8.76), 2.6B-parameter model trained on high-quality private data, despite MaskGen-XL being trained exclusively on open data for approximately 30.5 8-A100 days. GenEval. Tab. 6 summaries the zero-shot text-to-image generation results on GenEval [27]. Using discrete tokens, MaskGen-L (568M) achieves an overall score of 0.53, significantly outperforming the recent autoregressive model LlamaGen [57] by 0.21 and performing on par with the larger Show-o [63] (1.3B). Moreover, the larger MaskGenXL achieves the highest overall score on the benchmark, with score of 0.57. This result notably surpasses SDXL [45], 2.6B-parameter model (2.36 larger than MaskGen-XL) trained on proprietary data. Meanwhile, our MaskGen-XL with continuous tokens also achieves an overall score of 0.55, comparable to recent diffusion models [50], but with much lower training costs and exclusively trained on open data. 8 6. Conclusion A. More Implementation Details We introduced TA-TiTok, text-aware 1D tokenizer that improves semantic alignment in text-to-image generation, and MaskGen, versatile masked generative model supporting both discrete and continuous tokens. Through compact 1D tokenization, MaskGen reduces training costs and accelerates sampling, making efficient, high-quality generation more accessible. By leveraging publicly available data and an efficient tokenizer design, MaskGen democratizes access to high-performance masked generative models. Our release of training code and model weights aims to drive further progress in text-to-image generation. Limitations and Future Work. While MaskGen achieves competitive generation quality and benchmark scores comparable to recent text-to-image models, including those leveraging proprietary training data, we acknowledge several aspects for future exploration. First, the current KL variant of MaskGen is designed to use 32 tokens. While increasing the token count improves tokenization quality, leading to better-reconstructed samples, it also significantly raises training costs due to longer convergence times. Additionally, scaling up the generator remains challenge, as the current MaskGen-XL is constrained to 1.1B parameters due to limited computational resources. Second, the current implementation of MaskGen operates at resolution of 256 256. However, the scalability of its core architectural design1-dimensional tokenization and masked generationhas been validated in high-resolution implementations like Muse [12]. This work emphasizes establishing fully open-source, open-data text-to-image masked generative model using compact text-aware 1-dimensional tokenization. Future work will focus on optimizing convergence speed, model scaling up, and enabling high-resolution outputs."
        },
        {
            "title": "Appendix",
            "content": "We provide additional information in the supplementary material, as outlined below: Sec. A: Further implementation details, including dataset filtering, recaptioning, and MaskGen training hyperparameters. Sec. B: Ablation studies on the place of text guidance in TA-TiTok. Sec. C: Comparisons between MaskGen using discrete and continuous tokens. Sec. D: Ablation studies on the number of tokens and aesthetic score condition in MaskGen. Sec. E: Additional zero-shot text-to-image generation results on COCO validation set. Sec. F: Results demonstrating the performance of our KL variant of TiTok on ImageNet. Sec. G: More qualitative examples generated by MaskGen. Dataset Filtering. To prepare training data, we applied three filtering criteria: resolution, aesthetic, and watermark filtering. Details of applied filtering criteria and the total size of the dataset after filtering are presented in Tab. 7. Resolution filtering was applied to all datasets during the training of both TA-TiTok and MaskGen. This filtering ensured that the longer side of each image exceeded 256 pixels and maintained an aspect ratio below 2. For MaskGen training, we implemented aesthetic filtering using the LAION-aestheticv2 predictor [2] to retain only high-quality images. Images with scores above 5.0 were kept during the pre-training stage, while stricter threshold of 6.0 was applied during finetuning to ensure even higher quality. Additionally, we employed watermark filtering for MaskGen using the LAIONWatermarkDetector [5], removing images with watermark probability exceeding 0.5 to prevent unwanted watermark artifacts in generated images. Synthetic datasets such as JourneyDB [56] and DALLE3-1M [21] were exempted from these filtering processes as they inherently met our high resolution and quality standards. Dataset Recaptioning. To improve the text quality of DataComp [25], LAION-art [3], and LAION-pop [4], we utilize state-of-the-art vision-language models, Molmo-7B-D0924 [16], to enhance captions based on both the image and its original caption. Specifically, we randomly sample one of four prompts as shown in Fig. 4 to generate updated captions. Since the augmented captions are often significantly longer than the original ones and frequently start with similar patterns (e.g., The image depicts/displays/showcases/shows/features...), we apply prefix filtering to remove these repetitive prefixes, preventing information leakage. During training, we further address this by employing 95:5 ratio, randomly sampling between augmented and original captions to ensure balanced learning, following [9]. few recaption results are shown in Fig. 5, highlighting how the augmented captions provide richer details and align better with the image content. Training Hyper-parameters. Tab. 8 provides the complete list of hyper-parameters used for training MaskGen with both discrete and continuous tokenizers. B. Ablation on Text Guidance Place in TA-"
        },
        {
            "title": "TiTok",
            "content": "In the design of TA-TiTok, we only incorporate the text guidance (i.e., the text tokens from CLIP) into the tokenizer decoder to better capture high-level semantics and align with textual descriptions during both reconstruction and generation. In this study, we investigate whether injecting text guidance into both the encoder and decoder of TA-TiTok can further enhance the quality of the encoded latent tokens. This evaluation is conducted on the ImageNet validation set [17] 9 model dataset filtering resolution aesthetic watermark recaptioning samples TA-TiTok: tokenizer MaskGen: pre-training MaskGen: fine-tuning DataComp [25] DataComp [25] CC12M [13] LAION-aesthetic [1] DataComp [25] LAION-art [3] LAION-pop [4] DALLE3-1M [21] JourneyDB [56] (5.0) (5.0) (6.0) 685.8M 219.8M 4.8M 28.3M 3.6M 4.2M 0.4M 1.0M 4.1M Table 7. Training Data Details. Filtering criteria applied to each publicly available dataset include resolution (aspect ratio < 2 and longer side 256), aesthetic score (predicted score exceeding the specified value in parentheses), and watermark detection (removal of images predicted to contain watermarks). For datasets with noisy web-crawled captions, Molmo [16] is used for recaptioning. The final column shows the number of text-image pairs remaining after filtering. 1. Describe the image in detail while considering the provided caption: '{original_caption}'. Correct any errors and improve the caption, ensuring the final description is in English and within 77 tokens. Return only the corrected caption. 2. Analyze the image and the caption '{original_caption}'. Write detailed and accurate description of the image in English, correcting any mistakes or low-quality aspects of the original caption. Keep the final caption under 77 tokens, and return only the caption. 3. Use the caption '{original_caption}' as reference to create detailed and improved description of the image in English. Correct any errors and make sure the new caption is concise and within 77 tokens. Return only the revised caption. 4. Given the image and the original caption '{original_caption}', describe the image in detailed and accurate way in English, improving upon the original caption where necessary. Ensure the description is within 77 tokens. Return only the corrected caption. Prompts Used for Recaptioning. Figure 4. One of the four prompts are used when recpationing each image. {original_caption} is replaced with the original caption paired with the image. hyper-parmeters optimizer β1 β2 weight decay lr (pre-training) lr (fine-tuning) lr scheduling lr warmup steps batch size training steps (pre-training) training steps (fine-tuning) discrete continuous AdamW AdamW 0.9 0.96 0.03 0.0004 0.0001 cosine 10K 4096 500K 250K 0.9 0.95 0.02 0.0002 0.0002 constant 50k 2048 1000k 500k Table 8. Training Hyper-parameters for MaskGen. arch KL tokens Encoder + Decoder Decoder Only IS rFID # 218.4 32 16 1.65 221.5 64 16 1.39 227.1 128 16 0. rFID 1.53 1.47 0.90 IS 222.0 220.7 227.7 Table 9. Ablation on Text Guidance Place. In the TA-TiTok design, we ablate on adding the text guidance to both encoder and decoder or just decoder. Adding text guidance to only the decoder results in similar reconstruction performances but enjoys simpler structure. Models are trained on DataComp and zero-shot evaulated on ImageNet validation set. C. Comparisons Between MaskGen Using Discrete and Continuous Tokens using reconstruction metrics in zero-shot setting, where the caption is represented as photo of class without any prompt engineering. As shown in Tab. 9, injecting textual guidance into both the encoder and decoder has negligible impact on reconstruction quality. This finding suggests that incorporating text guidance in the decoder alone is sufficient to provide semantic information to the model. Performance Comparisons Between VQ and KL Variants. The KL variant of MaskGen consistently outperforms the VQ variant on MJHQ-30K FID but performs slightly worse on GenEvals overall score. We hypothesize that the KL variant excels in generating diverse, high-aesthetic images, contributing to improved FID on MJHQ-30K. However, it falls behind on GenEval, which emphasizes object-focused 10 Figure 5. Re-captioning Results. Augmented captions, generated by Molmo [16], offer richer details and improved alignment with image content. tokenizer arch type generator TA-TiTok VQ Mask. MaskGen-L (ours) TA-TiTok VQ Mask. MaskGen-XL (ours) #params 568M 1.1B resolution 256 256 20.0 30.3 256 256 35.0 18. MJHQ-30K GenEval Overall 0.53 0.57 FID 7.74 7.51 568M + 44M 256 256 18.5 11.1 TA-TiTok KL Mask. MaskGen-L (ours) TA-TiTok KL Mask. MaskGen-XL (ours) 1.1B + 69M 256 256 30.5 9.1 7.24 6.53 0.52 0.55 Table 10. Comparison of MaskGen using Discrete and Continuous Tokens on MJHQ-30K and GenEval. VQ denotes discrete tokenizers and KL stands for continuous tokenizers. type indicates the generative model type, where Mask. refer to masked transformer models. T: Generator training cost, measured in 8 A100 days using float16 precision. I: Generator inference throughput, measured in samples per second on single A100 with batch size 64 using float16 precision. FID: MJHQ-30K FID. Overall: GenEval average score. compositional properties such as position, count, and color. In contrast, the VQ variant, constrained by finite codebook, generates less diverse but more compositionally accurate images, leading to higher scores on GenEval. Fig. 7 visually compares generated samples, where the KL variant demonstrates slightly better overall generation quality. Training and Inference Cost Comparisons Between VQ and KL Variants. The VQ variant of MaskGen benefits from faster training and significantly faster inference, primarily due to inherent differences in the diffusion process used in the KL variant. While the KL variant excels in generating more diverse and higher-aesthetic images, this advantage comes with increased computational demands. To address this gap in training and inference efficiency, we employ 128 tokens for the VQ variant and 32 tokens for the KL variant, effectively controlling the training cost to remain at comparable level, as shown in Tab. 10. D. Ablation Studies for MaskGen Experimental Setup. For efficient ablation experiments, we utilize the discrete version of MaskGen. Performance is evaluated using the FID metric on MJHQ-30K [37] and the overall GenEval [27] score. Number of Tokens. Tab. 11 presents an ablation study arch generator VQ MaskGen-L #tokens I 16.0 47.6 17.5 40.2 20.0 30.3 MJHQ-30K GenEval Overall 0.43 0.50 0.53 FID 9.11 7.85 7.74 32 64 128 Table 11. Zero-Shot Text-to-Image Generation Results on MJHQ-30K and GenEval with Varying Number of Tokens. MaskGen achieves better generation quality with more tokens but incurs longer training times and slower inference speeds. T: Generator training cost, measured in 8 A100 days using float16 precision. I: Generator inference throughput, measured in samples per second on single A100 with batch size 64 using float16 precision. on the number of tokens used for text-to-image generation with MaskGen. As observed, increasing the token count improves generation quality but comes at the expense of longer training times and slower inference speeds. Aesthetic Score Conditioning. Tab. 12 illustrates the impact of incorporating aesthetic score conditioning signals. As shown, using aesthetic scores improves the quality of the generation. In Fig. 6, generated images using different aesthetic score conditioning are presented. Images are generated using the same hyper-parameters and prompts, except for the aesthetic score used for conditioning. The 11 tokenizer arch tokens # rFID generator gFID IS I VAE [39] TiTok (ours) KL KL 256 16 0.54 MAR [39] 2.45 275.5 8.0 1.0 2.96 246.9 2.1 8.1 2.70 252.9 3.2 3.2 64 16 1.54 128 16 1.31 MAR [39] Table 14. Class-conditional ImageNet-1K 256 256 Generation Results Evaluated with ADM [19], using continuous tokens (i.e., KL architecture). #: Number of tokens. c: Channels of continuous tokens. T: Generator training cost, measured in 8 A100 days using float32 precision. I: Generator inference throughput, measured in samples per second on single A100 with float32 precision. E. Zero-Shot Text-to-Image Generation Results on COCO In Tab. 13, we evaluate zero-shot text-to-image generation on the COCO dataset [40] by randomly sampling 30K imagecaption pairs from the COCO 2014 validation split and reporting the FID, as is standard in the literature. Since the finetuning stage of MaskGen often generates more aesthetically appealing images that deviate from the COCO dataset distribution, we perform the evaluation using MaskGen at the pretraining stage to ensure consistency with the datasets characteristics. Notably, MaskGen-L (KL variant with continuous tokens) achieves an FID-30K of 9.66, while MaskGen-XL (KL variant) further improves to 8.98. These results demonstrate that MaskGen achieves performance comparable to other state-of-the-art text-to-image models, highlighting its effectiveness even in the zero-shot setting. F. KL variant of TiTok on ImageNet We evaluate the KL variant of TiTok as drop-in replacement for standard 2D VAEs [35, 39] in class-conditional image generation on ImageNet [17]. Results, reported in Tab. 14, are based on the MAR [39] framework with its base model, after 400 epochs using unchanged MAR hyper-parameters. MAR with TiTok (our KL variant) achieves significant training time reductions (3.8 with 64 tokens, 2.7 with 128 tokens) and inference speedups (8.1 with 64 tokens, 3.2 with 128 tokens), thanks to its efficient 1D token design. Despite the substantial reduction in computational overhead, MAR with TiTok maintains performance comparable to MAR with conventional 2D VAEs using 256 tokens, highlighting TiToks potential as an efficient and robust image tokenizer for class-conditional generation. Figure 6. Generated Images with Varying Aesthetic Score Conditioning. Conditioning on higher aesthetic scores produces generated images with enhanced fine-grained details. arch generator #tokens aesthetic VQ MaskGen-L 64 MJHQ-30K GenEval Overall 0.49 0.50 FID 8.66 7.85 Table 12. Zero-Shot Text-to-Image Generation Results on MJHQ-30K and GenEval with Ablation of Additional Conditions. Incorporating aesthetic scores as conditioning signals enhances generation quality. arch generator tokenizer MAGVIT-v2 [63] VQ Show-o [63] TA-TiTok TA-TiTok VQ MaskGen-L (ours) VQ MaskGen-XL (ours) #params 1.3B 568M 1.1B VAE [50] VAE [50] VAE [50] TA-TiTok TA-TiTok KL LDM [50] KL Stable-Diffusion-1.5 [50] KL PixArt-α [14] KL MaskGen-L (ours) KL MaskGen-XL (ours) 1.4B 860M 630M 568M + 44M 1.1B + 69M open-data FID-30K 9.24 13.62 13.01 12.64 9.62 7.32 9.66 8.98 Table 13. Zero-Shot Text-to-Image Generation Results on COCO-30K. Comparison of MaskGen with state-of-the-art openweight models. results show that the aesthetic score correlates highly with the dramatic lighting and fine-grained detail. For example, in the third row, we can observe that higher aesthetic score leads to more detailed depictions of trees and stars in the night sky, while using low aesthetic score leads to simpler representation. This enables more fine-grained control of the generated images based on the users needs. G. Qualitative Examples of MaskGen Fig. 8, Fig. 9, Fig. 10, Fig. 11, Fig. 12, Fig. 13, and Fig. 14 showcase additional qualitative examples of text-to-image generation using MaskGen. By utilizing the efficient and compact text-aware tokenizer TA-TiTok, MaskGen demonstrates its ability to produce high-fidelity and diverse images. 12 Figure 7. Generated Images by MaskGen with Different Tokenizer Types. For each caption, the top row displays images generated using continuous tokens (KL), while the bottom row shows images generated using discrete tokens (VQ). Long prompts are truncated for brevity. 13 Figure 8. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images. Figure 9. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images. 15 Figure 10. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images. 16 Figure 11. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images. Figure 12. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images. 18 Figure 13. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images. 19 Figure 14. Qualitative examples of Text-to-Image (T2I) Generation with MaskGen. MaskGen, equipped with the efficient and compact text-aware 1D tokenizer TA-TiTok, generates high-fidelity and diverse images."
        },
        {
            "title": "References",
            "content": "[1] LAION2B-en-aesthetic. https://huggingface.co/ datasets/laion/laion2B-en-aesthetic, . 6, 10 [2] LAION-aesthetics predictor V2. https://github.com/ christophschuhmann / improved - aesthetic - predictor, . 7, 9 [3] LAION-art. https://huggingface.co/datasets/ laion/laion-art, . 6, 9, 10 [4] LAION-pop. https : / / huggingface . co / datasets/laion/laion-pop, . 6, 9, [5] LAION-5B-WatermarkDetection. https : / / github . com / LAION - AI / LAION - 5B - WatermarkDetection, . 9 [6] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 2, 5 [7] Jinbin Bai, Tian Ye, Wei Chow, Enxin Song, Qing-Guo Chen, Xiangtai Li, Zhen Dong, Lei Zhu, and Shuicheng Yan. Meissonic: Revitalizing masked generative transformers for efficient high-resolution text-to-image synthesis. arXiv preprint arXiv:2410.08261, 2024. 1 [8] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. 1, 3 [9] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Improving image generation with Lee, Yufei Guo, et al. better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 3, [10] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 2020. 3 [11] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. 1, 2, 3, 4, 5, 6, 7 [12] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, José Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Textto-image generation via masked generative transformers. In ICML, 2023. 1, 2, 3, 5, 7, 9 [13] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 3, 6, 10 [14] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. 3, 8, 12 [15] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. [16] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 3, 7, 9, 10, 11 [17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. 1, 5, 7, 9, 12 [18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2018. 3, 4 [19] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. [20] Alexey Dosovitskiy. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 3, 6 [21] Ben Egan, Alex Redden, XWAVE, and SilentAnDalle3 1 Million+ High Quality Captagonist. https : / / huggingface . co / datasets / tions. ProGamerGov/synthetic-dataset-1m-dalle3high-quality-captions, 2024. 3, 6, 9, 10 [22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. 1, 3, 4, 7 [23] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 1, 2, 3, 5, 6 [24] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. 1, 3, [25] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. NeurIPS, 2023. 2, 3, 6, 7, 9, 10 [26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022. 1, 3 [27] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment. NeurIPS, 2023. 3, 7, 8, 11 [28] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. 3 [29] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive networks. In ICML, 2014. 3 [30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [31] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two 21 time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. 7 [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 7 [33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 4 [34] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. [35] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 1, 3, 12 [36] Daiqing Li, Aleks Kamko, Ali Sabet, Ehsan Akhgari, Linmiao Xu, and Suhail Doshi. Playground v2. 3 [37] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 7, 8, 11 [38] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In CVPR, 2023. [39] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. NeurIPS, 2024. 2, 3, 4, 5, 6, 12 [40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 12 [41] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. NeurIPS, 2024. 3 [42] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. 3 [43] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, 2018. 3 [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 1, 2, 3, 5, 6 [45] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 3, 8 [46] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859, 2016. 7 [47] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 5, 6 [48] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 2020. 2, 5, 6 [49] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 3 [50] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 1, 3, 5, 8, [51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 2022. 1, 2, 3, 5 [52] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 2016. 7 [53] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. 3, 5 [54] Simonyan and Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 4 [55] stabilityai, 2023. 5 [56] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. NeurIPS, 2023. 3, 6, 9, 10 [57] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 1, 3, [58] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 [59] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In CVPR, 2021. 4 [60] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. NeurIPS, 2016. 3 [61] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017. 1, 3, 4 [62] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv preprint arXiv:2409.16211, 2024. 3, [63] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 3, 8, 12 [64] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022. 3 [65] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei 22 Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. TMLR, 2022. 3, 7 [66] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, 2023. 3, 4 [67] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. [68] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and LiangChieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024. 3 [69] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. NeurIPS, 2024. 1, 2, 3, 4, 6, 7 [70] Richard Zhang. Making convolutional networks shiftinvariant again. In ICML, 2019. 4 [71] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 4 [72] Chuanxia Zheng and Andrea Vedaldi. Online clustered codebook. In ICCV, 2023."
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "POSTECH"
    ]
}