{
    "paper_title": "Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts",
    "authors": [
        "Chen Yang",
        "Guangyue Peng",
        "Jiaying Zhu",
        "Ran Le",
        "Ruixiang Feng",
        "Tao Zhang",
        "Xiyun Xu",
        "Yang Song",
        "Yiming Jia",
        "Yuntao Wen",
        "Yunzhi Xu",
        "Zekai Wang",
        "Zhenwei An",
        "Zhicong Sun",
        "Zongchao Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models."
        },
        {
            "title": "Start",
            "content": "Nanbeige4.1-3B: Small General Model that Reasons, Aligns, and Acts Nanbeige LLM Lab, Boss Zhipin"
        },
        {
            "title": "Abstract",
            "content": "We present Nanbeige4.1-3B, unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable longhorizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models. The model checkpoint is available at https://huggingface.co/Nanbeige/Nanbeige4.1-3B 6 2 0 2 3 1 ] . [ 1 7 6 3 3 1 . 2 0 6 2 : r Figure 1: Performance of Nanbeige4-3B-Thinking vs. Qwen model series. Preprint."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in small language models (SLMs) have demonstrated that compact models can achieve impressive performance on specialized tasks such as mathematical reasoning or code generation [21, 19]. Despite the small size, these models perform competitively by leveraging innovations in architecture, training data, and training algorithm, making them highly effective in practical applications. However, most existing SLMs exhibit fragmented capabilities: reasoning-focused models often struggle with long-horizon interactions (e.g., deep search) [17], while code or agent specialized models typically lack robust general reasoning abilities, such as creative writing or human preference alignment [24]. Consequently, building truly unified generalist model at the 3B scale remains an open challenge and practical question is raised: How far can 3B model be pushed as generalist without compromising its existing strengths? In this work, we present Nanbeige4.1-3B, generic small language model that is remarkably powerful in reasoning, coding, and agentic behaviors, Built upon Nanbeige4-3B [21]. For general reasoning, we extend our previous pair-wise preference modeling by incorporating point-wise reward signals, ensuring that responses are both strong in isolation and preferred under direct comparison. For code generation, we move beyond correctness as the sole objective and explicitly reward algorithmic efficiency, encouraging solutions that are not only functionally correct but also computationally efficient. For agentic behavior, we emphasize long-horizon planning. We construct high-quality training data through Wiki-graph random walks and define rewards at both the interaction (turn) and full-trajectory levels, allowing the model to receive credit for planning and execution over hundreds of steps. Throughout training, we employ careful SFT data mixing and multi-stage reinforcement learning [15] to maintain balance across these domains. Through the aforementioned approach, we have trained model with an exceptionally broad and stable capability profile. Across reasoning and coding tasks, Nanbeige4.1-3B consistently surpasses existing open-source SLMs (e.g., Qwen3-4B and Qwen3-8B) and shows clear improvements over Nanbeige4-3B-Thinking-2511. More notably, Nanbeige4.1-3B exhibits deep-search and long-horizon agentic behavior rarely observed in general-purpose small language models. While models such as Qwen3-4B and Qwen3-8B fail to sustain meaningful exploration beyond few turns, Nanbeige4.1-3B reliably solves complex search-oriented tasks through extended tool interaction. In this regime, generalist 3B model attains deep-search capability comparable to that of specialized search-oriented models at the tens-of-billions scale, and approaches the search performance of 100B+ general-purpose large models. These results indicate that long-horizon agency can be achieved in compact generalist models when training objectives and credit assignment are properly aligned. We open-source Nanbeige4.1-3B to support research into efficient, agent-capable language models. Beyond the checkpoint itself, we hope this release contributes to the communitys understanding of how to jointly train reasoning, coding, and long-horizon behaviors under strict capacity constraints."
        },
        {
            "title": "2 Methods",
            "content": "In this section, we present the methodology used to equip compact 3B model with broad generalist capabilities. We first describe how we individually optimize the model for general reasoning, longhorizon search (agentic behavior), and code generation, focusing on the design of training signals and reward structures tailored to each capability. We then detail how these heterogeneous objectives are integrated through data mixing and multi-stage training, enabling the model to retain domain-specific strengths while emerging as unified general model under strict capacity constraints."
        },
        {
            "title": "2.1 General Abilities",
            "content": "In this section, we elaborate on the optimization of Nanbeige4.1s general capabilities. Our improvements focus on two key aspects: refining the data construction strategies in the SFT phase, and upgrading the General RL training paradigm through progressive integration of Point-wise and Pair-wise reinforcement learning."
        },
        {
            "title": "2.1.1 SFT",
            "content": "Nanbeige4.1-3B is built upon the Nanbeige4-3B-Base [21] with an enhanced SFT recipe, focusing on data distribution, length scaling, and training data quality. First, we redesign the SFT data mixture. Compared to the previous Nanbeige4-3B-2511 version, we increase the proportion of code-related data and introduce higher ratio of challenging problems in mathematics and general domains. This shift encourages stronger reasoning depth and improves robustness on difficult benchmarks. Second, we extend the context length beyond the previous two-stage curriculum (32k 64k) by introducing third stage at 256k tokens to better support complex reasoning and long-horizon scenarios. In the final 256k stage, we adopt specialized data mixture designed to strengthen agentic and reasoning capabilities, consisting of code (27%), deep-Search (26%), STEM (23%), tool-use (13%), and general domains (10%). Third, we further optimize the Solution Refinement and Chain-of-Thought (CoT) Reconstruction framework originally introduced in Nanbeige4-3B-2511. Specifically, we scale up the number of refinement iterations in the Solution Refinement loop, allowing stronger critiquerevision cycles to produce higher-quality final solutions. In addition, we train more capable CoT Reconstruction model to generate cleaner and more faithful reasoning traces from refined answers. As shown in Table 1, these improvements result in substantial gains across benchmarks. Nanbeige4.13B-SFT demonstrates consistent improvements in coding, mathematics, and alignment metrics, laying stronger foundation for subsequent reinforcement learning stages. Table 1: Performance Uplift from Nanbeige4-3B-SFT to Nanbeige4.1-3B-SFT"
        },
        {
            "title": "Benchmark",
            "content": "Nanbeige4-3B-SFT Nanbeige4.1-3B-SFT"
        },
        {
            "title": "Alignment",
            "content": "LCB V6 LCB Pro Medium Hmmt Nov Imo-Answer-Bench Arena-Hard V2 Multi-Challenge 45.5 1.8 60.7 34.8 45.5 42. 62.0 22.8 74.3 48.9 60.2 44.4 +16.5 +21.0 +13.6 +14. +14.7 +1."
        },
        {
            "title": "2.1.2 Point-wise RL",
            "content": "After SFT, we still observe some degradation issues in Nanbeige4.1-3B-SFT, such as repetition and redundant thinking, which have also been reported in prior work [5]. To address these issues and establish more stable foundation for RL, we introduce point-wise reinforcement learning stage. We train general reward model to evaluate rollout responses, following prior work [15]. The model is trained on curated large-scale human preference data. We found that the trained reward model naturally suppresses overly redundant, repetitive, and low-readability answers. We then perform GRPO [14] to optimize Nanbeige4.1-3B-SFT, sampling 8 rollouts per prompt and using the general reward model to score each response as the training signal. With point-wise RL, we significantly reduce formatting errors and redundant reasoning. On LiveCodeBench-v6, the point-wise RL greatly improves length stability, reducing overlong truncation from 5.27% to 0.38%. As shown in Table 2, it also advances Arena-Hard V2 from 60.2 to 66.6, with the hard-prompt subset improving from 46.1 to 54.1. These gains are reflected in more consistent, well-structured outputs and high-quality code presentation."
        },
        {
            "title": "2.1.3 Pair-wise RL",
            "content": "Although the Point-wise RL training provides effective alignment signals, the amount of high-quality preference data is limited, which constrains further improvement. To address this, we introduce Pair-wise RL to fully leverage preference information from strongweak model comparisons and further enhance model performance. 3 We train pair-wise reward model on paired comparison data spanning code generation and LMArenastyle conversations (single-turn and multi-turn). We generate response pairs with strong model and weak model, then apply the same checklist filtering strategy as Nanbeige4 to derive reliable winloss labels [21]. Following the framework [18], we mitigate position bias by adding swap-consistency regularizer, defined as the mean squared error between the predicted reward difference for response pair and the negated reward difference for the swapped pair. We then run Pair-wise RL by formulating the reward as binary outcome based on whether the generated rollout outperforms the reference answer. For multi-turn scenarios, we concatenate the full dialogue history into the pairwise reward models input. As shown in 2, Nanbeige4.1-3B achieves comprehensive performance breakthroughs after Pair-wise RL. By deeply exploiting contextual information in multi-turn dialogues, Alignment metrics show significant gains, with Multi-Challenge increasing from 47.72 to 55.14. Additionally, Pair-wise RL notably boosts performance on the Arena-Hard V2 benchmark, with an improvement from 66.6 to 73.8, showcasing its effectiveness in refining alignment. These results confirm that Pairwise RL sharpens preference boundaries, providing the informative supervision signals needed for overall improvement across various benchmarks. Table 2: Improvements via General RL Training (SFT Point-wise RL Pair-wise RL)"
        },
        {
            "title": "Code",
            "content": "LCB V"
        },
        {
            "title": "Alignment",
            "content": "Arena-Hard V2 Multi-Challenge"
        },
        {
            "title": "SFT",
            "content": "62.0 60.2 44.4 Point-wise RL Pair-wise RL 66.0 66.6 47.7 +4.0 +6.4 +3.3 65.6 73.8 55. -0.4 +7.2 +7."
        },
        {
            "title": "2.2 Deep Search Ability",
            "content": "In this section, we primarily focus on enhancing the deep search capabilities of our model, specifically on the data pipeline and the staged training process. Deep search is defined as retrieval-centric task that operates under complex multi-hop reasoning and extremely long-context settings. In this paradigm, models iteratively interact with the environment to acquire information, enabling them to solve challenging search problems."
        },
        {
            "title": "2.2.1 Data Construction",
            "content": "To enhance the search capability of our base model, we construct large-scale, complex search dataset, which includes substantial number of multi-hop QA pairs derived from entity-relation graphs built upon Wikipedia, as well as high-quality long-range search trajectories that have undergone multistage rigorous filtering. The entire data construction pipeline is illustrated in Figure 2. To facilitate further research, we have open-sourced the constructed dataset on HuggingFace 1. Figure 2: data construction pipeline for deep search, including complex multi-hop QA sampling and the synthesis of long-horizon reasoning trajectories. Temporal-Aware Head Entity Selection and Question Synthesis with Random Walking. To ensure the timeliness and complexity of the synthesized QA data, we first extract informative head 1https://huggingface.co/datasets/Nanbeige/ToolMind-Web-QA 4 Table 3: Preliminary evaluation results on search benchmarks for the synthetic QA."
        },
        {
            "title": "Benchmark",
            "content": "GAIA (text-only)"
        },
        {
            "title": "Browse\nComp",
            "content": "Browse Comp-ZH HLE (text-only) SEAL-0 xBench DeepSearch-05 xBench DeepSearch-10 Nanbeige4-3B- + Synthetic QA 19.4 58.3 0.8 14.4 3. 30.1 13.9 22.4 12.6 36.0 33. 76.0 11.0 30.0 entities from Wikipedia that have been updated within the past six months. Following the framework of [20], we construct an entity-relation graph and perform conditional random walks to extract relational paths of predefined length. These chains, along with their detailed temporal contexts, are then fed into robust LLM to synthesize intricate questions. Trajectory Synthesis and Turn-level Judgment. To synthesize search trajectories, we employ multiple agent frameworks to address the generated queries, sampling diverse set of reasoning paths. These trajectories are subsequently mapped into multi-turn tool-invocation sequences from unified agent perspective. To further guarantee the quality of the synthesized data, we implement rigorous turn-level judgment mechanism. Specifically, we employ critic model to evaluate each step of the interaction based on three dimensions: logical soundness, tool-call accuracy, and informational gain. Any turn that fails to meet these criteria does not participate in model training or provide negative reward for the model. This fine-grained filtering ensures that the final trajectories provide high-fidelity signal for supervised fine-tuning and preference alignment."
        },
        {
            "title": "2.2.2 Preliminary Performance",
            "content": "To empirically validate the effectiveness of our proposed data construction pipeline, we conduct controlled experiment using Nanbeige4-3B-2511 as the base model. Specifically, we train the model exclusively on the synthetic multi-hop QA and search trajectories generated via the methods described in Section 2.2.1, intentionally excluding other open-source data. We evaluate our model on range of long-horizon benchmarks, including GAIA [9], BrowseComp [16], BrowseCompZH [23], Humanitys Last Exam (HLE) [12], SEAL-0 [11], xBench-DeepSearch-2505 [3], and xBench-DeepSearch-2510 [3]. For HLE and GAIA, we only test and report the results of the text-only subset. In addition, HuggingFace has been explicitly disabled in these tools. The model is evaluated within the Mindflow framework 2, employing suite of tools: Serper 3 for environment search, Jina 4 for webpage content extraction, and E2B Sandbox 5 as the secure sandbox environment. comparative analysis of our results across specific stages is presented below: The quantitative results are summarized in Table 3. The incorporation of our synthetic data yields significant performance improvement across all benchmarks compared to the base model. Notably, the model achieves substantial leap on xBench-DeepSearch-2505 (improving from 33.0 to 76.0), demonstrating that our data synthesis pipeline effectively endows the model with robust multi-hop reasoning and long-context search abilities."
        },
        {
            "title": "2.3 Coding Ability",
            "content": "In this section, we primarily focus on enhancing the coding capabilities of our base model, including the data construction pipeline, staged training strategy, and evaluation settings."
        },
        {
            "title": "2.3.1 Judge System",
            "content": "We build unified judge system that is shared across SFT data construction, RL data construction, and subsequent RL training and evaluation. It combines multi-language sandbox for executionbased correctness checking with dedicated instruct judge model for time-complexity comparison. The instruct model is specifically trained for fast complexity assessment in RL settings, enabling 2https://github.com/MiroMindAI/MiroThinker 3https://serper.dev/ 4https://jina.ai/ 5https://e2b.dev/ 5 Figure 3: Gated time-complexity reward design in code RL. The time reward Rtime is activated only when solution passes all test cases (PassRate = 1), and the judge system provides online feedback by comparing the predicted time complexity against the reference optimal bound. efficient online comparison between the predicted complexity of model-generated solutions and the reference optimal bound."
        },
        {
            "title": "2.3.2 Data Construction",
            "content": "Our data construction consists of two components: SFT data construction with our judge system to filter time-optimal solutions, and RL data construction with on-policy difficulty filtering to improve sample efficiency. SFT Data Construction. Our SFT data construction uses this judge system to assess solution quality from two key aspects: (1) functional correctness by executing the program in sandbox, and (2) time complexity by combining execution signals with model-based complexity analysis. During data generation, we sample multiple candidate solutions per problem. The candidates are then verified by the judge system, and we keep those that are judged to be time-optimal (or among the best complexity class) for the given problem. RL Data Construction. Each RL sample contains problem statement, test cases, timecomplexity-optimal solution, and the corresponding optimal complexity label. The optimal solution and complexity are obtained by prompting multiple strong LLMs and employing strong LLM to synthesize the candidates into single best solution, which are then used as supervision signals for reward shaping and difficulty control. In both stages, we perform on-policy filtering by running multiple rollouts per problem (n = 8) and selecting samples based on how many rollouts meet stage-specific criterion. In Stage 1, we use difficulty-based criterion: problem is retained if the policy can solve it in moderate number of rollouts (we keep problems with in [1, 5] successful solves out of 8). In Stage 2, we use complexity-based criterion: we count how many rollouts produce solutions whose estimated time complexity satisfies the target bound, and retain problems with in [1,5] complexity-satisfying rollouts out of 8."
        },
        {
            "title": "2.3.3 Staged Training Process",
            "content": "Starting from the General-RL checkpoint, we further conduct two stages of code RL. In Stage 1, we optimize for solution correctness using pass-rate reward, defined as the fraction of test cases passed for each problem. In Stage 2, after the policy can reliably solve problems, we additionally encourage higher-quality solutions by introducing time-complexity reward only when all test cases are passed; otherwise the reward reduces to correctness-only signals. Specifically, the judge system provides online feedback by comparing the models output against the reference optimal complexity and checking whether the generated solution matches the reference optimal solution when applicable. As illustrated in Fig. 3, the time-complexity reward is activated only for fully correct solutions. = (cid:26)Rformat + Rcorrectness, PassRate < 1, Rformat + Rcorrectness + Rtime, PassRate = 1. As concrete illustration, Appendix presents LiveCodeBench case studies comparing model outputs before vs. after the time-reward stage, highlighting typical complexity-class improvements. Figure 4: Training dynamics of two-stage code RL. We track the reward (including the gated Rtime in Stage 2) and LiveCodeBench performance across training, showing consistent improvements from Stage 1 to Stage 2."
        },
        {
            "title": "2.3.4 Training Dynamics",
            "content": "Across the two-stage code RL, we observe consistent improvements in both reward signals and downstream coding performance. In Stage 1, the correctness reward (Rcorrectness) increases sharply, reflecting rapid gains in producing valid and correct solutions. In Stage 2, Rcorrectness improves more modestly, while the gated time reward (Rtime) rises substantially, indicating that the policy is indeed optimizing time complexity once correctness is largely achieved. The overall reward and performance trends across the two-stage training are shown in Fig. 4."
        },
        {
            "title": "2.4 Whole Training Recipe of Nanbeige4.1",
            "content": "Nanbeige4.1-3B is initialized from Nanbeige4-3B-Base and further optimized through structured post-training pipeline combining large-scale SFT and cascaded RL. We first conduct extended SFT, increasing the maximum context length from 64K to 256K compared to the previous Nanbeige4-3B-2511 version. This longer context window is essential for supporting long-horizon reasoning and multi-turn deep-search planning. In the RL phase, we adopt staged optimization strategy. General RL is performed sequentially with point-wise RL followed by pair-wise RL to enhance both standalone response quality and comparative preference alignment. Code RL is then conducted in two stages: correctness stage that maximizes execution pass rate, followed by gated time-complexity stage that activates efficiency rewards only when full correctness is achieved. Finally, we apply lightweight agentic RL stage to strengthen tool-use and search behavior. This unified training recipe enables Nanbeige4.1-3B to maintain strong domain-specific performance while emerging as well-balanced generalist model under strict capacity constraints."
        },
        {
            "title": "3 Experiment",
            "content": "Our evaluation comprises three components: general reasoning, deep-search agentic tasks, and real-world coding challenge. General reasoning benchmarks measure core capability boundaries. Deep-search tasks evaluate long-horizon planning and tool-augmented multi-step reasoning in realistic environments. The real-world algorithm challenge provides an out-of-distribution stress test."
        },
        {
            "title": "3.1 General Task Evaluations",
            "content": "For general reasoning capabilities, we evaluate across five major categories: code, math, science, alignment, and tool-use. Code. We report results on LiveCodeBench-V5, LiveCodeBench-V6 [6], and LiveCodeBench-Pro [22] to assess code generation ability and execution-based correctness under increasing difficulty. 7 Table 4: Evaluation results across code, math, science, alignment, and tool-use benchmarks. Qwen3-30B A3B-2507 Nanbeige4-3B 2511 Qwen3-Next -80B-A3B Qwen3-4B 2507 Qwen3-32B Nanbeige4.1-3B"
        },
        {
            "title": "Benchmark",
            "content": "LCB-V6 LCB-Pro-Easy LCB-Pro-Medium 57.4 40.2 5.3 AIME 2026 HMMT Nov IMO-Answer-Bench 81.46 68.33 48.00 GPQA HLE (Text-only) Arena-Hard-V2 Multi-Challenge BFCL-V4 Tau2-Bench 65.8 6.72 34.9 41.14 44.87 45."
        },
        {
            "title": "Code",
            "content": "66.0 60.8 3."
        },
        {
            "title": "Math",
            "content": "87.30 71.25 54."
        },
        {
            "title": "Science",
            "content": "73.4 11."
        },
        {
            "title": "Alignment",
            "content": "60.2 49."
        },
        {
            "title": "Tool Use",
            "content": "48.60 47.70 55.7 42.3 3.5 75.83 57.08 43.94 68.4 9.31 56.0 38.72 47.90 45. 68.7 78.8 14.3 89.24 81.67 58.00 77.2 13.70 62.3 56.52 50.51 57.40 46.0 40.2 5. 84.10 66.67 38.25 82.2 10.98 60.0 41.20 53.80 41.77 76.9 81.4 28.1 87.40 77.92 53. 83.8 12.60 73.2 52.21 56.50 48.57 Mathematics. We include IMO-Answer-Bench [8], HMMT [1], and AIME-2026-I 6 to evaluate symbolic reasoning and competition-level problem solving. Science. We benchmark on GPQA [13] and HLE [12] to measure multi-step scientific reasoning and domain knowledge integration. Alignment. We use Arena-Hard-V2 [7] and Multi-Challenge [4] to assess preference modeling robustness and response quality under adversarial or challenging prompts. Tool-use. We evaluate BFCL [10] and Tau2-Bench [2], which test function-calling reliability and multi-step tool use capability. For compared baseline models, we include open-source models at similar scale (Qwen3-4B-2507) and our previous release (Nanbeige4-3B-2511), as well as substantially larger open models including Qwen3-30B-A3B-2507, Qwen3-32B, and Qwen3-Next-80B-A3B. This setup allows us to assess both same-scale competitiveness and cross-scale efficiency. Overall Results. Table 4 shows that Nanbeige4.1-3B substantially outperforms both Qwen34B-2507 and its predecessor Nanbeige4-3B-2511 across all evaluated domains, demonstrating the effectiveness of our post-training strategy. More notably, despite having only 3B parameters, Nanbeige4.1-3B consistently surpasses 30B32B class models (Qwen3-30B-A3B-2507 and Qwen3-32B) on the majority of benchmarks, including coding, alignment, and tool-use tasks. The gains are particularly pronounced on execution-based coding benchmarks such as LiveCodeBench-V6 and LiveCodeBench-Pro-Medium, where Nanbeige4.1-3B achieves large absolute margins. When compared to Qwen3-Next-80B-A3B, Nanbeige4.1-3B remains competitive and exhibits complementary strengths: it leads on several coding and alignment benchmarks, while the 80B model retains advantages on selected mathematics and tool-use tasks. These results suggest that strong cross-domain reasoning performance can be achieved through targeted post-training and agent-oriented optimization, even at significantly smaller parameter scale. 6https://huggingface.co/datasets/MathArena/aime_2026_I 8 Table 5: Performance comparison across various deep search benchmarks."
        },
        {
            "title": "Benchmark",
            "content": "GAIA (text-only)"
        },
        {
            "title": "Browse\nComp",
            "content": "Browse Comp-ZH HLE (text-only) SEAL-0 xBench DeepSearch-05 xBench DeepSearch-10 Tongyi-DeepResearch-30B MiroThinker-v1.0-8B AgentCPM-Explore-4B GLM-4.6-357B Minimax-M2-230B DeepSeek-V3.2-671B Qwen3-4B-2507 Qwen3-8B Qwen3-14B Qwen3-32B Qwen3-30B-A3B-2507 Qwen3-Next-80B-A3B 70.90 66.40 63.90 71.90 75.70 63.50 28.33 19.53 30.23 30.17 31.63 34."
        },
        {
            "title": "Research Agents",
            "content": "43.40 31.10 25.00 46.70 40.20 29.00 32.90 21.50 19.10 - 40.40 40."
        },
        {
            "title": "Large Foundation Models with Tools",
            "content": "45.10 44.00 67.60 49.50 48.50 65.00 30.40 31.80 40.80 - - 38."
        },
        {
            "title": "Small Foundation Models with Tools",
            "content": "1.57 0.79 2.36 3.15 1.57 5.60 7.92 5.15 7.11 7.34 4.12 8."
        },
        {
            "title": "Ours",
            "content": "11.13 10.24 10.17 9.26 14.81 9.26 15.74 6.34 12.64 8.15 9.24 18.18 Nanbeige4-3B-2511 Nanbeige4.1-3B 19.42 69.90 0.79 19.12 3.09 31. 13.89 22.29 12.61 41.44 75.00 60.60 70.00 70.00 72.00 71.00 34.00 31.00 34.00 39.00 25.00 27.00 33.00 75. - - - - - - 5.00 2.00 9.00 8.00 10.00 6.00 11.00 39."
        },
        {
            "title": "3.2 Deep Search Task Evaluations",
            "content": "In this section, we present comprehensive evaluation of the performance of Nanbeige4.1-3B on deep search tasks. We evaluate its capabilities in handling complex tasks by benchmarking against general foundation models of comparable size, specialized search-agent models, and significantly larger-scale foundation models."
        },
        {
            "title": "3.2.1 Experimental Setup and Comparison Baselines",
            "content": "All experimental settings are the same as those of Section 2.2.2. These benchmarks require iterative retrieval, planning, tool interaction, and cross-document reasoning, reflecting realistic autonomous agent workloads. To thoroughly contextualize the performance of Nanbeige4.1-3B, we conduct diverse comparison against several categories of existing models. This includes general-purpose foundation models equipped with tools (e.g., Qwen-series models), specialized search-agent models such as MiroThinker-8B, AgentCPM-Explore-4B, and Tongyi-DeepResearch-30B. We also incorporate large-scale open foundation models exceeding 100B parameters, comprising Minimax-M2.1, GLM-4.6, and DeepSeek-V3.2 for comparison."
        },
        {
            "title": "3.2.2 Overall Performance Analysis",
            "content": "Table 5 presents comprehensive performance comparison across various search agent benchmarks. While \"Research Agents\" (e.g., Tongyi-DeepResearch-30B) and \"Large Foundation Models with Tools\" (e.g., Minimax-M2, DeepSeek-V3.2) generally exhibit strong capabilities in their respective categories, particularly on GAIA and browsing tasks, the \"Small Foundation Models with Tools\" (Qwen3 series) typically show lower performance levels. Our model, Nanbeige4.1-3B, demonstrates remarkable leap in performance compared to its baseline Nanbeige4-3B-2511 and significantly outperforms other small foundation models with tools across all benchmarks. More critically, Nanbeige4.1-3B achieves state-of-the-art results across nearly all evaluated benchmarks, including GAIA (69.90), xBench-DeepSearch-05 (75.00), and SEAL-0 (41.44). These scores not only surpass its direct competitors in the small model category but also place it on par with, or even exceeding, the performance of many larger \"Research Agents\" and \"Large Foundation Models with Tools.\" This robust performance underscores Nanbeige4.1-3Bs exceptional effectiveness in complex search and agentic tasks, demonstrating that competitive capabilities can be achieved efficiently even with smaller model size through our proposed methodology."
        },
        {
            "title": "3.3 Real-world Algorithmic Challenges: LeetCode Weekly Contests",
            "content": "Beyond curated academic benchmarks, we further evaluate Nanbeige4.1-3B on real-world competitive programming tasks drawn from recent LeetCode weekly contests 7, providing practical out-ofdistribution stress test of algorithmic reasoning ability. We apply Nanbeige4.1-3B, alongside Qwen models of varying scales, to solve contest problems under standard competitive programming settings. The generated solutions are directly submitted to the official LeetCode platform, and performance is measured by the final acceptance rate. Table 6: Comparison of Pass Rates on LeetCode Weekly Contests 484488."
        },
        {
            "title": "Benchmark",
            "content": "Qwen3-4B-2507 Qwen3-32B Qwen3-30B-A3B-2507 Nanbeige4.1-3B LeetCode Pass Rate (%) 55.0 50.0 65.0 85. As shown in Table 6, Nanbeige4.1-3B successfully solves 17 out of 20 problems, achieving an overall pass rate of 85.0%. In virtual participation mode, where ranking is determined by correctness and time efficiency, our model achieves 1st place in Weekly Contest 487 and 3rd place in Weekly Contest 488. Compared to Qwen3 models of similar or substantially larger scales, Nanbeige4.1-3B demonstrates clear performance advantage on these real-world contest tasks. Detailed problem statements and representative solution examples are provided in Appendix B."
        },
        {
            "title": "4 Conclusion",
            "content": "In this work, we present Nanbeige4.1-3B, unified generalist model that demonstrates advanced capabilities in reasoning, coding, and long-horizon search. We achieve this by integrating pointwise and pair-wise reward modeling for precise preference alignment, optimizing code generation for both correctness and computational efficiency, and incorporating both turn and trajectory level signals to enable robust long-horizon agentic planning. Consequently, Nanbeige4.1-3B consistently outperforms comparable small models and remains competitive with larger baselines across diverse benchmarks and real-world challenges. Looking ahead, we aim to push the boundaries of compact models in complex coding and research agent scenarios. Concurrently, we focus on improving inference efficiency by enabling tasks to be solved with shorter outputs and fewer tool invocations, while exploring architectural innovations to further enhance the potential of the compact generalist model. 7https://leetcode.cn/contest/"
        },
        {
            "title": "References",
            "content": "[1] Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions. arXiv preprint arXiv:2505.23281, 2025. [2] Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. τ 2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint, 2025. [3] Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, Chen Sun, Han Hou, Hui Yang, James Pan, Jianan Lou, Jiayi Mao, Jizheng Liu, Jinpeng Li, Kangyi Liu, Kenkun Liu, Rui Wang, Run Li, Tong Niu, Wenlong Zhang, Wenqi Yan, Xuanzheng Wang, Yuchen Zhang, Yi-Hsin Hung, Yuan Jiang, Zexuan Liu, Zihan Yin, Zijian Ma, and Zhiwen Mo. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations, 2025. [4] Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai Hernandez-Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. Multichallenge: realistic multi-turn conversation evaluation benchmark challenging to frontier llms. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1863218702, 2025. [5] Ruixiang Feng, Yuntao Wen, Silin Zhou, Ke Shi, Yifan Wang, Ran Le, Zhenwei An, Zongchao Chen, Chen Yang, Guangyue Peng, Yiming Jia, Dongsheng Wang, Tao Zhang, Lisi Chen, Yang Song, Shen Gao, and Shuo Shang. Pace: Prefix-protected and difficulty-aware compression for efficient reasoning, 2026. [6] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. [7] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint, 2024. [8] Minh-Thang Luong, Dawsen Hwang, Hoang Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, et al. Towards robust mathematical reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3540635430, 2025. [9] Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants, 2023. [10] Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. [11] Thinh Pham, Nguyen Nguyen, Pratibha Zunjare, Weiyuan Chen, Yu-Min Tseng, and Tu Vu. Sealqa: Raising the bar for reasoning in search-augmented language models, 2025. [12] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, and et al. Humanitys last exam, 2025. [13] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint, 2023. [14] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 11 [15] Boxin Wang, Chankyu Lee, Nayeon Lee, Sheng-Chieh Lin, Wenliang Dai, Yang Chen, Yangyi Chen, Zhuolin Yang, Zihan Liu, Mohammad Shoeybi, et al. Nemotron-cascade: Scaling cascaded reinforcement learning for general-purpose reasoning models. arXiv preprint arXiv:2512.13607, 2025. [16] Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. [17] Sen Xu, Yi Zhou, Wei Wang, Jixin Min, Zhibin Yin, Yingwei Dai, Shixi Liu, Lianyu Pang, Yirong Chen, and Junlin Zhang. Tiny model, big logic: Diversity-driven optimization elicits large-model reasoning ability in vibethinker-1.5 b. arXiv preprint arXiv:2511.06221, 2025. [18] Wenyuan Xu, Xiaochen Zuo, Chao Xin, Yu Yue, Lin Yan, and Yonghui Wu. unified pairwise framework for rlhf: Bridging generative reward modeling and policy optimization. arXiv preprint arXiv:2504.04950, 2025. [19] Shaotian Yan, Kaiyuan Liu, Chen Shen, Bing Wang, Sinan Fan, Jun Zhang, Yue Wu, Zheng Wang, and Jieping Ye. Distribution-aligned sequence distillation for superior long-cot reasoning. arXiv preprint arXiv:2601.09088, 2026. [20] Chen Yang, Ran Le, Yun Xing, Zhenwei An, Zongchao Chen, Wayne Xin Zhao, Yang Song, and Tao Zhang. Toolmind technical report: large-scale, reasoning-enhanced tool-use dataset. arXiv preprint arXiv:2511.15718, 2025. [21] Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng, Tao Zhang, Wei Ruan, Xiaoqi Liu, Xiaoxue Cheng, Xiyun Xu, et al. Nanbeige4-3b technical report: Exploring the frontier of small language models. arXiv preprint arXiv:2512.06266, 2025. [22] Zihan Zheng, Zerui Cheng, Zeyu Shen, Shang Zhou, Kaiyuan Liu, Hansen He, Dongruixuan Li, Stanley Wei, Hangyi Hao, Jianzhu Yao, et al. Livecodebench pro: How do olympiad medalists judge llms in competitive programming? arXiv preprint arXiv:2506.11928, 2025. [23] Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. [24] Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu, Zhirong Wu, Qi Dai, et al. Re-trac: Recursive trajectory compression for deep search agents. arXiv preprint arXiv:2602.02486, 2026."
        },
        {
            "title": "A Author List",
            "content": "Authors are listed in alphabetical order by first name. Names marked with an asterisk (*) denote individuals who were previously affiliated with our team. Yang Song is the corresponding author and can be reached at songyang@kanzhun.com. Chen Yang, Guangyue Peng, Jiaying Zhu, Ran Le, Ruixiang Feng, Tao Zhang, Xiyun Xu, Yang Song, Yiming Jia, Yuntao Wen, Yunzhi Xu, Zekai Wang, Zhenwei An*, Zhicong Sun, Zongchao Chen"
        },
        {
            "title": "B Leetcode Contest Examples",
            "content": "The following examples are drawn from LeetCode Weekly Contest 488 under virtual participation. Since the contest was conducted on the Chinese version of LeetCode, the original problem statements presented to the model were in Chinese, and the generated code contained Chinese comments. For clarity, we provide English translations of both the problem descriptions and the models comments below. B.1 Problem Q1: Count Dominant Indices"
        },
        {
            "title": "Problem Description",
            "content": "Given: An integer array nums of length n. Definition: An index is called dominant index if the following condition is met: nums[i] > average(nums[i + 1], nums[i + 2], . . . , nums[n 1]) Your task is to count the number of dominant indices. Note: The average is the sum of the group divided by the number of elements. The rightmost element of the array is not considered dominant index. Model Output (Nanbeige4.1-3B): The model correctly transformed the inequality to avoid floating-point precision issues and utilized suffix sum approach to solve the problem in O(n) time complexity. from typing import List class Solution: def dominantIndices(self, nums: List[int]) -> int: \"\"\" [Translated Comment] Count the number of dominant indices in the array. Dominant Index Definition: Index such that: nums[i] > average(nums[i+1], ..., nums[n-1]) Note: The rightmost element (index n-1) is not considered candidate. Optimization Strategy: 1. Avoid floating-point arithmetic: Transform nums[i] > sum / len into nums[i] * len > sum. 2. Traverse from right to left, dynamically maintaining the suffix sum. 3. Time Complexity: O(n), Space Complexity: O(1). \"\"\" = len(nums) # Boundary condition: No valid dominant index if length <= 1 if <= 1: return 0 count = 0 suffix_sum = 0 # Stores sum of nums[i+1] to nums[n-1] 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 13 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 61 63 64 # Traverse backwards from the second to last element to index 0 for in range(n - 2, -1, -1): suffix_sum += nums[i + 1] # Accumulate current right-side part length = - - 1 # Number of elements on the right # Strict inequality check (avoiding floating point issues) if nums[i] * length > suffix_sum: count += return count end{minted} B.2 Problem Q2: Apply Operations to Maximize Array Sum"
        },
        {
            "title": "Problem Description",
            "content": "Given: An integer array nums. Operation: Repeatedly perform the following until no more operations can be done: If there are two adjacent and equal elements in the array, select the leftmost such pair. Replace them with their sum. The array size decreases by 1 after each merge. Task: Return the final array after all possible merge operations are completed. Model Output (Nanbeige4.1-3B): The model employs stack-based simulation to handle the \"leftmost\" merge constraint efficiently. This mimics the recursive merge process in single pass with O(n) time complexity. from typing import List class Solution: def mergeAdjacent(self, nums: List[int]) -> List[int]: \"\"\" [Translated Comment] Simulate the merge process using stack: - Traverse the array, comparing each element with the stack top. - If equal, continue merging (pop stack top and accumulate) until not equal or stack is empty. - Push the merge result (or original element) onto the stack. - The final stack content is the result after no more merges can be done. Time Complexity: O(n), each element is pushed/popped at most once. Space Complexity: O(n), max stack depth. \"\"\" stack = [] for num in nums: current = num # Continuous merge: when stack top equals current, pop and accumulate while stack and stack[-1] == current: current += stack.pop() stack.append(current) return stack end{minted} subsection{Problem Q3: Count Subarrays with Cost Constraint} begin{tcolorbox}[colback=gray!5, colframe=gray!60, title=textbf{Problem Description}] textbf{Given:} An integer array texttt{nums} and an integer $k$. 14 65 67 68 69 70 71 73 74 75 76 77 79 80 81 82 83 85 86 87 88 89 91 92 93 94 95 97 98 99 100 101 103 104 105 106 107 109 110 111 112 113 115 116 117 textbf{Definition:} The textit{cost} of subarray $text{nums}[l..r]$ is defined as: [ text{cost} = (max(text{nums}[l..r]) - min(text{nums}[l..r])) times (r - + 1) ] textbf{Task:} Return the number of subarrays where the cost is less than or equal to $k$. Model Output (Nanbeige4.1-3B): The model correctly identifies this as sliding window problem. To maintain the maximum and minimum values within the moving window efficiently, it utilizes two monotonic deques, achieving an optimal solution. from collections import deque from typing import List class Solution: def countSubarrays(self, nums: List[int], k: int) -> int: # [Translated] Boundary check: empty array or negative (cost is nonnegative) if not nums or < 0: return 0 = len(nums) dq_max = deque() # Monotonic decreasing deque: stores indices of max values dq_min = deque() # Monotonic increasing deque: stores indices of min values = 0 ans = 0 for in range(n): # Maintain max deque (pop back if <= current value) while dq_max and nums[dq_max[-1]] <= nums[r]: dq_max.pop() dq_max.append(r) # Maintain min deque (pop back if >= current value) while dq_min and nums[dq_min[-1]] >= nums[r]: dq_min.pop() dq_min.append(r) # Calculate cost for current window [l, r] cur_max = nums[dq_max[0]] cur_min = nums[dq_min[0]] length = - + 1 cost = (cur_max - cur_min) * length # Shrink left boundary: if cost > k, move to the right while cost > and <= r: # Remove left boundary influence on extremums if dq_max[0] == l: dq_max.popleft() if dq_min[0] == l: dq_min.popleft() += 1 if > r: break # Recalculate cost after shrinking cur_max = nums[dq_max[0]] cur_min = nums[dq_min[0]] length = - + 1 cost = (cur_max - cur_min) * length 15 118 119 120 121 123 124 125 126 127 129 130 131 132 133 135 136 137 138 139 141 142 143 144 145 147 148 149 150 151 153 154 155 156 157 159 # Accumulate all valid subarrays ending at if <= r: ans += (r - + 1) return ans B.3 Problem Q4: Maximize Score of Pairs"
        },
        {
            "title": "Problem Description",
            "content": "Given: Two integer arrays nums1 (length n) and nums2 (length m), and an integer k. Task: Choose exactly pairs of indices (i1, j1), (i2, j2), . . . , (ik, jk) such that: 0 i1 < i2 < < ik < and 0 j1 < j2 < < jk < The score for each pair (i, j) is nums1[i] nums2[j]. Return the maximum total score (sum of products) achievable. Model Output (Nanbeige4.1-3B): The model proposes Dynamic Programming approach using rolling arrays to optimize space. It constructs 2D state transition curr[i][j] representing the max score selecting specific number of pairs from the prefixes of both arrays. from typing import List class Solution: def maxScore(self, nums1: List[int], nums2: List[int], k: int) -> int: n, = len(nums1), len(nums2) # [Translated] Edge case: k=0 returns 0 if == 0: return 0 # Implicit constraint check >= and >= if > or > m: return 0 # prev[i][j]: Max score choosing t-1 pairs from first and first elements prev = [[0] * (m + 1) for _ in range(n + 1)] NEG_INF = float('-inf') # Iterate to select 1 to pairs for _ in range(1, + 1): # curr initialized to -inf curr = [[NEG_INF] * (m + 1) for _ in range(n + 1)] for in range(1, + 1): num1 = nums1[i - 1] for in range(1, + 1): num2 = nums2[j - 1] # Three choices: # 1. Skip current element of nums1: state from curr[i-1][j] # 2. Skip current element of nums2: state from curr[i][j-1] # 3. Take current pair (i-1, j-1): state from prev[i-1][j-1] + product # (ensures strictly increasing indices) skip_i = curr[i - 1][j] skip_j = curr[i][j - 1] take = prev[i - 1][j - 1] + num1 * num2 curr[i][j] = max(skip_i, skip_j, take) prev = curr # Rolling update: current layer becomes prev for next round # Final result: Max score choosing exactly pairs from and elements 16 160 return int(prev[n][m]) Time-Complexity Case Study Scope and purpose. This section presents small set of time-complexity-focused case studies comparing model outputs before vs. after introducing the gated time reward in code RL. Concretely, we use examples from LiveCodeBench where the two models produce functionally correct solutions but differ in their algorithmic efficiency, and we analyze how the time-reward stage encourages outputs closer to the time-optimal complexity class. C.1 AtCoder ABC388 Excerpt: Simultaneous Kagamimochi Problem Description (Excerpt) Given: mochi with non-decreasing sizes A1 A2 AN . Rule: You can stack mochi of sizes (top) and (bottom) iff 2 . Task: Maximize the number of kagamimochi that can be made simultaneously by choosing 2K mochi and pairing them. 161 163 164 165 166 167 169 170 171 172 173 175 176 177 178 179 181 182 183 184 185 187 188 189 190 191 193 194 195 196 197 Before (Candidate Code). import sys def main(): data = sys.stdin.read().strip().split() if not data: return = int(data[0]) = list(map(int, data[1:1+N])) # The input is already sorted, but we sort to be safe. A.sort() low = 0 high = // 2 while low < high: mid = (low + high + 1) // 2 ok = True for in range(mid): if 2 * A[i] > A[N - mid + i]: ok = False break if ok: low = mid else: high = mid - 1 print(low) if __name__ == \"__main__\": main() After (Candidate Code). import sys def main(): data = sys.stdin.read().strip().split() if not data: return = int(data[0]) = list(map(int, data[1:1+N])) # is already sorted in non-decreasing order as per input specification. = 0 = // 17 199 200 201 202 204 205 206 207 208 210 count = 0 while < // 2 and < N: if A[i] * 2 <= A[j]: count += 1 += 1 += 1 else: += 1 print(count) if __name__ == \"__main__\": main() C.1.1 Complexity Analysis Key observation. Because is sorted, the problem reduces to matching small mochi as the top with large mochi as the bottom, under the constraint 2 A[top] A[bottom], without reusing mochi. Before: binary search over the answer. The before solution binary-searches in [0, N/2]. For each candidate = mid, it checks feasibility by scanning = 0..K 1 and verifying 2A[i] A[N + i]. If we include the extra sort(), the time is O(N log ) + O(N log ) = O(N log ). If we use the input guarantee (already sorted) and drop sort(), the dominating part is the feasibility checks: (cid:32) (cid:88) iterations (cid:33) mid = O(N log ) , because there are O(log ) iterations and each check costs O(mid) O(N ). Space complexity is O(N ) for storing A. After: two-pointers greedy matching. The after solution uses two pointers: over the smaller half ([0, N/2)) and over the larger half ([N/2, )). Pointer only moves forward, and moves forward exactly when valid pair is formed. Each pointer advances at most times, so the total time is O(N ) (no sorting is needed). Space complexity remains O(N ) for the array. Conclusion. Both solutions are correct, but after improves the core algorithmic cost from O(N log ) to O(N ) by eliminating the binary search and performing single-pass greedy matching enabled by the sorted input. C.2 AtCoder ARC195 Excerpt: Twice Subsequence Problem Description (Excerpt) Given: sequence = (A1, . . . , AN ) and target sequence = (B1, . . . , BM ) with 1 2 105. Task: Determine whether there exist at least two subsequences of that match B. Two subsequences are considered different if they use different sets of positions in A, even if the resulting values are identical. Before (Candidate Code). import sys import bisect from collections import defaultdict def main(): 211 212 213 214 215 216 217 218 219 220 222 223 224 225 226 228 229 230 231 232 234 235 236 237 238 240 241 242 243 244 246 247 248 249 250 252 253 254 255 256 258 259 260 261 262 264 265 266 267 268 270 271 272 273 274 276 277 data = sys.stdin.read().strip().split() if not data: return it = iter(data) = int(next(it)) = int(next(it)) = [int(next(it)) for _ in range(N)] = [int(next(it)) for _ in range(M)] # Map each value to the sorted list of indices where it appears in pos_map = defaultdict(list) for i, val in enumerate(A): pos_map[val].append(i) # Find the leftmost (earliest) matching subsequence left = [] pos = -1 for in B: if not in pos_map: print(\"No\") return lst = pos_map[b] # Find first index > pos idx = bisect.bisect_left(lst, pos + 1) if idx == len(lst): print(\"No\") return = lst[idx] left.append(i) pos = # Find the rightmost (latest) matching subsequence right = [] pos = for in reversed(B): if not in pos_map: print(\"No\") return lst = pos_map[b] # Find last index < pos idx = bisect.bisect_left(lst, pos) if idx == 0: print(\"No\") return = lst[idx - 1] right.append(i) pos = right.reverse() # restore order to match # If the two subsequences differ in at least one position, answer Yes for l, in zip(left, right): if != r: print(\"Yes\") return print(\"No\") if __name__ == \"__main__\": main() After (Candidate Code). import sys def main(): data = sys.stdin.read().split() 278 279 280 281 282 284 285 286 287 288 290 291 292 293 294 296 297 298 299 300 302 303 304 305 306 308 309 310 311 312 314 315 316 317 318 if not data: return it = iter(data) = int(next(it)) = int(next(it)) = [int(next(it)) for _ in range(N)] = [int(next(it)) for _ in range(M)] # leftmost matching indices left = [] = 0 for in range(N): if < and A[i] == B[j]: left.append(i) += 1 if == M: break if < M: print(\"No\") return # rightmost matching indices right = [] = - 1 for in range(N-1, -1, -1): if >= 0 and A[i] == B[j]: right.append(i) -= 1 if < 0: break right.reverse() # compare left and right for in range(M): if left[k] != right[k]: print(\"Yes\") return print(\"No\") if __name__ == \"__main__\": main() C.2.1 Complexity Analysis Key idea: earliest vs. latest embedding. Let = (ℓ1, . . . , ℓM ) be the leftmost embedding of in A, obtained greedily by scanning left-to-right and matching B1, B2, . . . as early as possible. Let = (r1, . . . , rM ) be the rightmost embedding of in A, obtained greedily by scanning right-to-left and matching BM , BM 1, . . . as late as possible. If there exists at least one such that ℓk = rk, then we can construct two different matching subsequences: one using the prefix choices from up to some point, and another using the suffix choices from (intuitively, there is slack at position allowing different choice of index while still completing the match). Conversely, if = coordinate-wise, then every match is forced to use the same index for each Bk, so there is exactly one matching subsequence. Before: value-to-positions map + binary search. The before approach preprocesses dictionary pos_map mapping each value to the sorted list of indices where Ai = v. Then: It constructs by, for each Bk, binary searching the first occurrence index > ℓk1. It constructs similarly from the right, binary searching the last occurrence index < rk+1. Finally it checks whether and differ in any coordinate. 20 Time. Building pos_map costs O(N ). Each of the steps performs one binary search on an occurrence list, costing O(log ) in the worst case. Total time is O(N + log ) per test case. Space. The map stores all indices once, so O(N ). After: two-pointer greedy scans. The after approach eliminates the map and binary searches: Construct with single left-to-right scan of using pointer over B. Construct with single right-to-left scan of using pointer over B. Compare and R; output Yes if they differ, else No. Time. Each scan is O(N ), and the final comparison is O(M ), hence total O(N + ) = O(N ) per test case. Space. Arrays left and right store indices, so O(M ) (and thus O(N )). Conclusion. Both solutions rely on the same correctness criterion: there are at least two matching subsequences iff the leftmost and rightmost embeddings differ at some position. The after solution achieves optimal linear time by replacing the index-map + binary-search steps with two greedy linear passes, reducing the per-test-case complexity from O(N + log ) to O(N ) while keeping O(N ) space. C.3 AtCoder ARC194 Excerpt: Minimum Cost Sort Problem Description (Excerpt) Given: permutation of (1, 2, . . . , ). Operation: For 1 1, swap adjacent elements (Pi, Pi+1) with cost i. Task: Compute the minimum total cost to sort in ascending order. 319 320 321 322 324 325 326 327 328 330 331 332 333 334 336 337 338 339 340 342 343 344 345 346 348 349 350 Before (Candidate Code). import sys import heapq def solve(): data = sys.stdin.read().strip().split() if not data: return it = iter(data) = int(next(it)) = [0] + [int(next(it)) for _ in range(N)] # 1-indexed # Initialize min-heap with all adjacent inversions heap = [] for in range(1, N): if P[i] > P[i + 1]: heapq.heappush(heap, i) total_cost = 0 while heap: = heapq.heappop(heap) # Check if the inversion still exists (it might have been fixed already) if not (P[i] > P[i + 1]): continue # Perform the swap P[i], P[i + 1] = P[i + 1], P[i] total_cost += # After swapping, check the three affected positions for new inversions for idx in (i - 1, i, + 1): if 1 <= idx < and P[idx] > P[idx + 1]: heapq.heappush(heap, idx) print(total_cost) 21 352 353 354 355 356 358 359 360 361 362 364 365 366 367 368 370 371 372 373 374 376 377 378 379 380 382 383 384 385 386 388 389 390 391 392 394 if __name__ == \"__main__\": solve() end{minted} paragraph{After (Candidate Code).} begin{minted}[linenos, frame=lines, framesep=2mm, fontsize=small, breaklines]{ python} import sys def solve() -> None: input = sys.stdin.readline = int(input()) = list(map(int, input().split())) # Fenwick tree (1-indexed) bit = [0] * (N + 2) def update(i: int, delta: int) -> None: while <= N: bit[i] += delta += & -i def query(i: int) -> int: = 0 while > 0: += bit[i] -= & -i return total_cost = 0 for val in P: # number of smaller elements to the left of 'val' cnt = query(val - 1) p0 = cnt + 1 # position of 'val' after larger elements to its left have passed if p0 < val: terms = val - p0 # number of right moves needed # sum from p0 to val-1 inclusive total_cost += (p0 + val - 1) * terms // 2 update(val, 1) print(total_cost) if __name__ == \"__main__\": solve() C.3.1 Complexity Analysis Before: heap-driven local bubble swaps. The before solution repeatedly fixes adjacent inversions, always picking the smallest index inversion via min-heap, and then re-checks nearby positions. Each heap operation is O(log ). The number of swaps equals the inversion count of the permutation, which can be Θ(N 2) in the worst case (e.g., reversed permutation). Moreover, indices may be pushed into the heap multiple times and later discarded, but the total number of heap pushes still scales with the number of performed swaps (constant-factor neighborhood updates per swap). Therefore, worst-case time complexity is O(inv(P ) log ) = O(N 2 log ), with O(N ) memory for the array and heap. 22 After: Fenwick tree (BIT) aggregation. The after solution processes values in input order and uses Fenwick tree to count how many smaller elements have appeared so far. For each value val, it infers how far this element must move (in terms of adjacent swaps) in the stable insertion process, and accumulates the corresponding cost by closed-form arithmetic progression. Each query and update is O(log ), performed once per element. Total time complexity is O(N log ) per test case. Space complexity is O(N ) for the BIT and input array. Conclusion. Compared with the swap-simulating heap method that can degrade to O(N 2 log ), the BIT-based solution computes the minimum cost in O(N log ) by aggregating necessary movements and costs without explicitly simulating each adjacent swap."
        }
    ],
    "affiliations": [
        "Boss Zhipin",
        "Nanbeige LLM Lab"
    ]
}