{
    "paper_title": "QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design",
    "authors": [
        "Benjamin Schneider",
        "Dongfu Jiang",
        "Chao Du",
        "Tianyu Pang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-video understanding has emerged as a crucial capability in real-world applications such as video surveillance, meeting summarization, educational lecture analysis, and sports broadcasting. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to a minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, a system-algorithm co-design that substantially accelerates long-video understanding to support real-time downstream applications. It comprises three key innovations: QuickDecoder, a parallelized CPU-based video decoder that achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals processed concurrently; QuickPrefill, a memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components infernece time reduce by a minute on long video inputs, enabling scalable, high-quality video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice."
        },
        {
            "title": "Start",
            "content": "QuickVideo: Real-Time Long Video Understanding with System Algorithm Co-Design Benjamin Schneider , Dongfu Jiang University of Waterloo, Chao Du, ,"
        },
        {
            "title": "SeaAI Lab",
            "content": "Tianyu Pang, Wenhu Chen 5 2 0 2 2 ] . [ 1 5 7 1 6 1 . 5 0 5 2 : r {benjamin.schneider,dongfu.jiang,wenhuchen}@uwaterloo.ca https://github.com/TIGER-AI-Lab/QuickVideo"
        },
        {
            "title": "Abstract",
            "content": "Long video understanding has emerged as crucial capability in real-world applications such as meeting summarization, video surveillance, educational lecture analysis, and content moderation. However, it remains computationally prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential video decoding, the process of converting the raw bit stream to RGB frames can take up to minute for hour-long video inputs, and 2) costly prefilling of up to several million tokens for LLM inference, resulting in high latency and memory use. To address these challenges, we propose QuickVideo, system-algorithm co-design that substantially accelerates long video understanding to support real-time downstream applications. It comprises three key innovations: QuickCodec, parallelized CPU-based video decoder that achieves 23 speedup by splitting videos into keyframe-aligned intervals processed concurrently. QuickPrefill, memory-efficient prefilling method using KV-cache pruning to support more frames with less GPU memory; and an overlapping scheme that overlaps CPU video decoding with GPU inference. Together, these components reduce the time required to process long video input by minute, enabling fast, efficient video understanding even on limited hardware. Experiments show that QuickVideo generalizes across durations and sampling rates, making long video processing feasible in practice."
        },
        {
            "title": "Introduction",
            "content": "Video data has become the dominant modality for conveying information online. As of 2023, video data accounts for two thirds of all data transmitted over the Internet [30]. Much of this data is long video ranging from minutes to hours in duration, from online conferencing, gaming, social networking, and movie streaming. This torrent of online video data demands efficient and automated understanding for problems such as content moderation [2], real-time surveillance [43], and accessibility [22]. Video Large Language Models (VideoLLMs) [4, 48, 7] have emerged as powerful tools to support these downstream tasks. By natively processing entire video inputs, VideoLLMs exhibit phenomenal potential to understand and reason about video content, offering practical solution for managing and extracting information from the exponentially growing flood of video data across the Internet [47]. However, using VideoLLMs for long video understanding suffers from several efficiency challenges. First, the entire video must be decoded from raw bitstreams into RGB frames before the model can begin processing. Current frameworks require up to minute to decode the frames from an hour-long video input, introducing high latency before any context prefill can start. Second, the prefilling step itself is both computationally and memory intensive [37]. Each framerepresenting an instantaneous momentcan consume hundreds of tokens in the model context [49, 8]. As result, The first two authors have equal contribution. Preprint. Under review. Figure 1: An overview of how QUICKVIDEO overlaps video decoding on CPU (QUICKCODEC) and prefill on GPU (QUICKPREFILL). QUICKCODEC concurrently processes intervals of the compressed video bit stream. QUICKPREFILL uses independent groups of frames, therefore it can begin prefill once the first frames are decoded, outputting carefully selected KV vectors. As QUICKCODEC loads frames synchronously, QUICKPREFILL can process the next prefill group immediately. This results in video decoding and prefill being almost enirely overlapped. even modest frame rate (e.g., 2 FPS) for an hour-long video can lead to millions of tokens, far exceeding the memory budget of standard GPUs. Qwen2.5-VL [4] introduced several architecture modification to accelerate video processing. However, using Qwen2.5-VL-7B, prefilling an hour-long HD video sampled at its native 2 fps still requires more than the 80 GB of memory offered by an A100/H100 SXM4 GPU. Even after reducing the frame sampling rate by 4, prefilling still takes over 25 seconds on datacenter-grade hardware. These inefficiencies result in frustrating user experience, characterized by long delays and prohibitively high hardware requirements. Users with limited computational resources are effectively excluded from accessing the long video understanding capabilities of VideoLLMs. To mitigate the computational overhead of long video VideoLLMs use extremely low frame sampling rates when processing long video inputs, instead of their native 1-2 FPS [4, 7]. Frames are sampled as much as minute apart during hour-long video understanding [49, 8]. minute gap between sampled frames can result in missing crucial video segments required for an understanding task. Low frame sampling rates also make fine-grained temporal and motion understanding impossible, as intervening frames are mostly removed [25]. Effective long video understanding thus requires loading and prefilling thousands of frames while preserving temporal continuity. Developing faster, more efficient VideoLLMs is critical for enabling comprehension of videos that span hours. Currently, video decoding and context prefilling are treated as disjoint and sequential stages in the VideoLLM pipeline. Moreover, video decoding is largely overlooked, despite contributing substantially to end-to-end latency. To remedy this, we introduce QUICKVIDEO, framework for faster, memory-efficient long video understanding. QUICKVIDEO reduces the latency and resource requirements of these key bottlenecks in long video understanding. Our framework empowers fast video understanding on video inputs consisting of hundreds of thousands of frames, while maintaining the sampling rates required for fine-grained understanding. QUICKVIDEO introduces three core contributions for accelerating long video understanding in VideoLLMs: (1) System-Level QUICKCODEC: drop-in replacement video decoder designed for VideoLLMs. By redesigning video decoding for VideoLLM frame sampling, we achieve 2-3x speedup compared to existing libraries when loading hour-long video inputs. (2) Algorithm-Level QUICKPREFILL: group-based prefilling strategy combined with key-value (KV) cache pruning, which significantly reduces both computation time and memory usage during the prefilling stage, while incurring less than 3% accuracy degradation in most benchmarks. (3) Co-Design Overlapped Execution Scheme: the strategy tightly couples CPU-based QUICKCODEC and GPU-based QUICKPREFILL, enabling near-complete overlap to maximize efficiency. QUICKVIDEO reduces the time to infer 30 minute video input by more than 3x, from 69.7 seconds to only 20.0 seconds. The results demonstrate the effectiveness of our system-algorithm co-design."
        },
        {
            "title": "2 Background",
            "content": "We provide an overview of VideoLLM inference and key concepts in video processing. Although details vary, this background is broadly applicable to standard VideoLLM architectures and video 2 standards. For clarity, we use video decoding to describe the process of decoding the compressed video into tensor of video frames, and use LLM decoding to denote the process of auto-regressive decoding of large language model. 2.1 VideoLLM Inference VideoLLMs must first decode compressed video into packed frame tensor before tokenization. The resulting raw frames are then passed through visual encoder, which converts them into video tokens suitable for input to the LLM. Unlike text preprocessing, which relies on lightweight tokenizers, video decoding is inherently slow on both CPU and GPU due to its sequential nature [38, 31]. Despite this, prior work in LLM video understanding has largely overlooked the latency incurred by this stage. Following preprocessing, the generation process of VideoLLM consists of two stages: (1) Prefill, where both video and text tokens are processed to compute key-value (KV) caches for each transformer layer; and (2) LLM decoding, where tokens are generated autoregressively using the stored KV representations. The prefill stage is computationally expensive due to the quadratic complexity O(n2 ) of self-attention over long sequences, while the decoding stage is memory-intensive as it requires storing and repeatedly accessing the full KV cache. Let Xv = {xv Xt} represent the video and text tokens, respectively, with video tokens preceding the text. For each transformer layer {1, . . . , L}, the KV cache comprises tensors K(l), V(l) R(Xv )nhdh , where nh is the number of attention heads and dh is the per-head dimensionality. For example, let 8B InternVL-2.5 [7] model process one-hour video at 1 frame per second, the total required memory is around 400GB (see subsection D.2). This memory footprint makes KV cache storage critical bottleneck in VideoLLM inference, significantly limiting the maximum processable video length and constraining the feasible batch size. Xv} and Xt = {xt 1, . . . , xv 1, . . . , xt +Xt 2.2 Long Video Processing Multimedia container formats like MP4 or MKV bundle all the elements required for media playback, including video streams, audio streams, subtitles, and metadata [18]. In these containers, videos are stored as compressed bit streams [18, 38]. In multimedia processing libraries like FFmpeg [32], video decoding is described by queue that enqueues fixed-sized blocks of the bit stream, called packets, as input and dequeues video frames. We denote bit stream = (p0, p1, . . . , pn1) and video = (f0, f1, . . . , fm1) as ordered lists of packets and frames, respectively. Each frame fi is tensor containing 8-bit integers of shape (3 w), where is the pixel height and is the pixel width. In general, packets are not frame aligned, enqueueing single packet to the decoder can cause the decoder to output zero, one or potentially multiple frames [38]. This is because frames require varying amounts of information to encode, and therefore cannot be aligned to fixed-sized packets. Furthermore, video frames are not encoded independently in bit stream, as surrounding frames contain redundant information. Therefore, the video encoder encodes the residual of the frame in the bitstream, instead of the frame itself2 [38, 31]. For this reason, video decoding is largely sequential process, where previous frames must decoded first and then the residual information encoded in the bit stream can be used to decode the next frame [38]. Although the video encoder may also reorder frames in the bit stream for efficiency, the decoder always outputs frames in the order that they should be displayed during playback [32]. Packet and Frame Metadata. Although metadata is not directly encoded in the bit stream or frame itself, for simplicity, we denote metadata corresponding to packets or frames as if they are fields. The packet and frame metadata is stored in the container, not the bit stream [18]. The presentation timestamp (pts) of frame is 64-bit unsigned integer that represents when the frame should be displayed to user [32]. Most formats do not include global frame positioning information in metadata. We instead use Equation (1) to rescale the presentation timestamp for frame to obtain index in V. = (m 1) .pts ptsmax ptsmin (1) ptsmax and ptsmin are the minimum and maximum presentation timestamp for the video stream. Each packet has keyframe flag that marks that video decoding can begin from its position [18, 32]. 2The encoded residual of frame may require information from previous or future frames to decode [38]. 3 2.3 Keyframes and Seeking As video decoding relies on surrounding frames, it is sequential process. However, during playback, users may want to navigate and skip through the video. To support this, the bit stream contains keyframes, which act as reset points from which video decoding can begin. Keyframes are encoded at semi-regular intervals in S, usually few seconds apart. To use keyframes to navigate in S, we use the SEEK subroutine. SEEK(S, pts) finds the keyframe packet pi such that decoding from pi yields all such that .pts pts. However, seeking introduces overhead, as it requires flushing decoder buffers and reinitializing state [32]. Algorithm 1 Seek-based video decoding Require: Bit stream S, Ordered set I, Video Decoder D, h, 1: Allocate memory block of size 3 2: for do 3: 4: 5: 6: 7: return Estimate pts of fi pi SEEK(S, pts) Decode pi, pi+1, . . . until outputs fi Write fi to Seek to the keyframe before fi in Algorithm 1 is standard approach when decoding video for machine learning [12, 27]. For each desired frame fi, given by selected indices in {1, 2, . . . , 1}, the algorithm does the following: It seeks for the keyframe closest to fi in S, and then it decodes packets until outputs fi. fi is saved in the buffer . This algorithm performs well for sparse access patterns, as if there are large gaps between desired frames, seeking before decoding each frame is ideal."
        },
        {
            "title": "3 Method",
            "content": "In this section, we first introduce QUICKVIDEO, mainly consisting of three components: 3.1 QUICKCODEC: Long Video Decoding for VideoLLMs Given bit stream for video = (f1, f2, . . . , fm) with frame height h, frame width and desired degree of concurrency c, our goal is to compute such that {0, 1, . . . , 1} Fj = fI[j] for {1, 2, . . . , 1}. That is, is packed tensor containing all the frames selected in I. We assume that is known from container metadata or an estimate using ptsmax and ptsmin. The efficiency of our algorithm relies on two observations: (1) It is faster to use cores to decode short videos than use cores to sequentially decode 1 long video. Video decoding for human playback focuses on the second case, as humans watch earlier frames while later frames decode. However, due to interframe dependencies, sequential video decoding is difficult to parallelize [38]. Unlike in human playback, VideoLLMs require the entire video input to be loaded upfront. Therefore, we can decompose the loading of long video into loading short videos that span V. However, we cannot begin video decoding from an arbitrary frame, only keyframe. KEYFRAME INTERVALS (Appendix A) is subroutine that parses the metadata of and computes approximately equal length intervals, starting and ending on keyframes, that span V. We parallelize over these intervals in Algorithm 2. (2) Ideally, VideoLLMs sample frames at short, regular interval, usually 1-2 FPS [4]. This is less than the gap between keyframes in standard codecs. Therefore, seek-based decoding must decode from all keyframes regardless, and many seeks are redundant. Our algorithm only requires 1 seek operation per core, instead of seeks proportional to the number of frames sampled. Algorithm 2 describes the core of our video decoding algorithm. The algorithm begins by using metadata to calculate keyframe-aligned intervals span the video (line 1). Lines 2-5 initialize block of shared memory and compute dictionary that maps indices of selected frames to unique memory offsets in . We then decode the long video in parallel intervals (lines 6-19). Video decoding begins by seeking the the start of the interval ptsstart, which is guaranteed to be keyframe (line 7). We enqueue packet to decode (lines 17-18) until the video decoder yields frames 4 intervals that start and end on keyframe Maps frame index to memory offset in Parallelize over intervals Seek to the packet at the start of the keyframe interval Algorithm 2 QUICKCODEC [I[k]] pi SEEK(S, ptsstart) repeat Require: Bit stream S, Ordered set I, Video Decoder D, h, w, c, 1: KEYFRAME INTERVALS(S, c) 2: Allocate shared memory of size 3 3: Initialize memory offset map 4: for {0, 1, . . . , 1} do 5: 6: for all (ptsstart, ptsend) in parallel do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: return Compute with equation 1 if in then [i] Fo D.enqueue(pi) pi pi+1 until .pts ptsend while not empty do D.dequeue() if .pts ptsend then break Get the memory offset for fi in Write frame into shared memory tensor Get next packet in bit stream to process (line 9). If the timestamp of the dequeued frame is greater than or equal to the endpoint of the interval ptsend, parallel processing ends (lines 11-12 and 19). As the intervals in span S, ptsmin and ptsmax are given by the least and greatest values in , respectively. Therefore, we use can Equation (1) to calculate the index of (line 13). Lastly, we save to if is selected frame (lines 14-16). As decoding from keyframe yields all frames with greater pts and outputs frames in pts order, when the parallelized loop exits (line 19), all selected frames with pts in the interval [ptsstart, ptsend) will have been output by and saved to . Therefore, as spans S, when the algorithm returns, will contain all selected frames. 3.2 QUICKPREFILL: Efficient Group-based Pre-filling for VideoLLM After decoding the video bit stream into packed tensors, they are then fed into the VideoLLM for inference. However, LLM generation with long context is well-known challenge due to its intensive memory usage and costly computation. To mitigate this issue, we introduce QUICKPREFILL, grouped prefilling and KV cache pruning method that both accelerates the speed and reduces the memory requirement significantly. Group-based Prefill. Let = v1, v2, . . . , vXv denote the sequence of video tokens, where is the total number of tokens and each token vi Rd is d-dimensional vector. To reduce Xv memory overhead during prefilling, we adopt group-based strategy by partitioning the video token sequence into disjoint groups: = V1, . . . , VG, where each group Vg contains approximately Ng = Xv tokens. Instead of processing the entire sequence at once, we sequentially prefill each group and store its corresponding key-value (KV) cache as K(l) for each transformer layer l. This strategy significantly reduces the peak activation memory usage by factor of and remains effective even when used in conjunction with efficient attention mechanisms such as FlashAttention. Empirically, it enables hour-long video understanding while keeping GPU memory usage within practical limits (e.g., reducing more than 100 GB of memory overhead; see subsection D.2). , V(l) Group-based KV Cache Pruning. While group-based prefill can effectively reduce the peak activation memory, the largest memory bottleneck, KV cache memory is still not resolved. Therefore, when processing each group of video tokens, instead of saving the all KV cache vectors, we will prune unimportant KV cache to maintain an ρ (0, 1] retention ratio and thus reduce the KV cache memory usage by factor of 1 ρ . The pruning decision is based on an importance score function to 5 produce an ordered list where we select the top-k KV cache until the retention ratio is reached: ], where (l) = TopK (s(K(l) ), = ρ Ng) = V(l) = K(l) ], V(l) , V(l) [I (l) [I (l) K(l) (2) where TopK defines function that returns of the indices the top-k largest inputs. There are multiple heuristic importance score function raised in previous works [11, 16, 45], in this paper, we mainly use the following 3 importance function: 1) Key Norms (small): = L2(K(l) ); 2) Value Norms: = L2(V(l) , Q(l)). Here L2 denotes the L2-norm function and Q(l) RXt (nhdh) denotes query vector of the text tokens in layer l. For our QUICKPREFILL, we use Key Norms (small) as the default importance function, due to its good performance. ); 3) Attention Scores: = matmul(K(l) 3.3 Overlapping QUICKCODEC and QUICKPREFILL The preceding sections introduced two complementary components: QUICKCODEC for CPU-based video decoding and QUICKPREFILL for GPU-based group-wise prefilling. However, sequential execution of these components leads to suboptimal resource utilization; the GPU remains idle during video decoding while the CPU is underutilized during prefill. To address this inefficiency, we propose an overlapped execution scheme that enables concurrent processing across CPU and GPU resources. Overlapping QUICKCODEC and QUICKPREFILL requires small adaptation to how frames are loaded. Instead of using cores to load intervals, we divide into intervals, where using KEYFRAME INTERVALS(V, s). We then load the frames from the intervals using cores, where the intervals corresponding to earlier blocks of video are loaded first. This approach allows us to use QUICKCODECs fast video decoding, while ensuring that early frames in are prioritized, allowing QUICKPREFILL to begin prefilling groups on GPU. Once the video frames required for the first group are loaded, QUICKPREFILL immediately begins prefilling. QUICKCODEC continues to load frames using the CPU in the background. After QUICKPREFILL finishes prefill for group, it saves the resulting KV cache vectors and checks if QUICKCODEC has loaded the frames required to process the next block. If the frames are available, QUICKPREFILL can begin processing the next group immediately. This establishes producer-consumer pipeline between CPU video decoding and GPU prefill, where the GPU is only idle if the frames for its next block are not loaded. The performance benefit of this overlapping execution can be formally analyzed. Let tdec and tpref ill denote the total time for decoding and prefilling all video groups, respectively. In the sequential approach, the total execution time is tdec + tpref ill. However, with our overlapped strategy, the execution time reduces to: dec and tg ttotal = max{tdec + tg pref ill, tpref ill + tg Where tg pref ill represent the time to decode frames for the first group and prefill the last group, respectively. is small amount latency added by QUICKCODEC parsing the video metadata. As an individual group corresponds to small number of frames and video tokens, this approach achieves near-optimal overlap between CPU and GPU resources. This results in substantial speedup for hourscale video processing. We note that some VideoLLMs have additional input preprocessing steps, such as calculating position embeddings or normalization, which we exclude from this analysis [4]. dec} + (3)"
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate QuickVideos performance on practical long video understanding tasks. In section 4.1, we benchmark QUICKCODEC against existing frameworks. We also examine the limitations of QUICKCODEC, identifying use-cases where seek-based frameworks (Algorithm 1) have stronger performance. Next, in section 4.2 we evaluate the performance of QUICKPREFILL on four long video understanding benchmarks and analyze the trade-off between accuracy and efficiency. Lastly, in section 4.3, we demonstrate that our prefill and video decoding stages can be almost entirely overlapped, effectively removing minute from the time it takes to infer an long-video input. 4.1 QUICKCODEC Results Video Loading Speed. We benchmark the time to load an hour-long 24 FPS 1920x1080p HD video, sampled at 1 FPS, and then resized to 448x448 pixels. The video is an hour segment of popular movie 6 Figure 2: Speed comparison of Decord, TorchCodec (with Resize), and QUICKCODEC on loading hour-long video. We ablate across different levels of parallelization (core counts). encoded with default settings in FFmpeg using H.264, the most widely used standard [17]. Sampling frames at 1 2 FPS is common for VideoLLMs, adopted as trade-off between computational efficiency and understanding [4]. We resize frames to 448x448 pixels because it is the highest per frame resolution used by most VideoLLMs [48, 7]. We use an AWS m7a.16xlarge instance for our timing. Timings are averaged over five runs, all timings have 95% confidence interval of at most 0.5 seconds. We compare QuickVideo against two video decoding frameworks designed for machine learning and integrated into existing long-video understanding inference pipelines: Decord [12]. Decord is framework for multimedia loading designed for machine learning applications. Although it is no longer actively maintained, it remains standard video decoding framework integrated into popular libraries like Hugging Faces Transformers [39] and, by extension, inference libraries such as vLLM [19]. TorchCodec [27]. TorchCodec is an under-development framework from the PyTorch team. It is designed to offer faster multimedia processing than TorchVision [23]. As it is work-in-progress library, TorchCodec does not support all the features of more mature frameworks. In particular, TorchCodec does not have built-in support for frame resizing. For this reason, we report timings for the video loading and then resizing step using TorchVision. TorchCodec is also not designed to use more than 16 cores for video decoding, we find that increasing core count beyond 16 can sometimes decrease TorchDodecs performance. As shown in Figure 2, QUICKCODEC is faster than other libraries across many levels of parallelization. Whereas other frameworks plateau at 16 cores, QUICKCODEC scales to 64 cores. We highlight the 16 and 32 core cases, as these are the most realistic configurations. Most compute providers configure nodes with between 16 and 32 CPU cores per GPU [15, 3, 24]. Using 16-32 cores, QUICKCODEC is 2-3 times faster than other libraries at loading an hour-long video, reducing the time to load video by more than 20 seconds. Speed across video durations. Our framework requires pre-computing intervals and sufficient keyframes to parallelize over. Therefore, we expect it to perform worse on shorter video durations. We benchmark QUICKCODEC for different video lengths, ranging from 1 minute to an hour-long video. We use the same hour-long movie video, cut to durations ranging from 1 to 60 minutes. We sample at 1 FPS and use 16 cores for video decoding. All timings are averaged over 5 runs on AWS m7a.16xlarge instance and have 95% confidence interval with at most 0.2 second margin of error. We find that QUICKCODEC is faster than other frameworks when loading videos that are more than minute long (Figure 3). Our framework scales better in long-video loading than other libraries, increasing from 1.7x faster than Decord at loading 10 minute video to 2.1x faster at loading hour-long video. We examine additional limitations of QUICKCODEC compared to seek-based decoders in Appendix B. 7 Figure 3: Video decoding performance for different video durations with 1 FPS sampling. 4.2 QUICKPREFILL Results In this section, we evaluate the performance of QUICKPREFILL across four long video understanding benchmarks whose video length ranges from minutes to hours: VideoMME [14], LongVideoBench [40], LVBench [34], and MLVU [46]. All generations are conducted using greedy sampling, and we report results using the lmms-eval framework [44]. All experiments are performed using the Qwen2.5-VL-7B-Instruct model [4] on single A100 (40GB) GPU with 8 replicas. Table 1: Effectiveness of different KV cache pruning methods in the group-based prefilling scenario. We use the Key Norms (small) as the default KV cache pruning method for QUICKPREFILL due to its superior performance and query-agonistic nature. Group Size #Frames KV Pruning ρ - 16 16 16 - 16 16 16 - 16 16 16 - 16 16 16 - Value Norms Attention Scores Key Norms (small) - Value Norms Attention Scores Key Norms (small) - Value Norms Attention Scores Key Norms (small) - Value Norms Attention Scores Key Norms (small) 1 0.5 0.5 0.5 1 0.5 0.5 0.5 1 0.5 0.5 0.5 1 0.5 0.5 0. VideoMME LongVideoBench LVBench MLVU w/o subtitle dev test val Avg Performance 64 Frames 59.69 35.98 52.95 56.17 128 Frames 60.96 37.32 55.20 58.19 256 Frames 61.56 38.89 57.22 60. 1024 Frames 60.43 33.66 58.49 61.59 62.41 47.63 58.63 60.56 66.41 48.56 60.96 63.41 65.78 48.33 62.52 64.04 62.00 47.37 62.22 59. 40.09 30.92 37.83 37.70 42.87 30.73 39.70 39.57 43.90 31.38 41.96 41.90 42.29 29.18 42.03 40.80 63.86 31.38 59.87 62.34 66.86 38.51 64.36 64. 68.65 37.74 67.27 66.73 63.48 32.65 64.45 64.76 56.51 36.48 52.32 54.19 59.27 38.78 55.06 56.54 59.97 39.08 57.24 58.22 57.05 35.71 56.80 56. 100.00% 64.55% 92.58% 95.90% 100.00% 65.42% 92.89% 95.39% 100.00% 65.17% 95.45% 97.08% 100.00% 62.60% 99.56% 99.53% Figure 4: Ablation study on the group size and retention ratio. Data from Table 2. Effectiveness of Different KV Cache Pruning Methods. We evaluate the impact of various KV cache pruning strategies on model accuracy, as summarized in Table 1. Specifically, we compare several pruning techniques against baseline with no pruning applied (ρ = 1). We fix the KV cache retention ratio ρ to 0.5 and set the group size to 16 frames. Among the evaluated strategies, the Key Norms (small) method achieves the best trade-off between efficiency and accuracy, retaining over 95% of the models original performance while reducing KV cache size and computation by half. In the 1024 frames setting, it can even retain more than 98% of 8 the models original performance Notably, this method outperforms alternatives that select tokens based on query attention scores. While prior work [11] demonstrated that negative L2 norms of keys correlate strongly with attention scores in text-only LLMs, our results extend this finding to the VideoLLM prefilling setting. This suggests that key norm-based pruning is not only effective during decoding but also applicable in the context of prefilling, underscoring its generalizability and practical value for long-context video understanding. We further conduct ablation studies to assess the effects of group size and the retention ratio ρ (see Appendix E). As shown in Table 2 and Figure 4, varying the group size has minimal impact on model performance, while increasing the retention ratio ρ consistently improves accuracy, eventually matching the performance of the non-pruned baseline. Smaller group sizes yield lower activation memory, and lower retention ratios result in reduced KV cache memory. These findings offer practical guidance for balancing memory efficiency and model accuracy based on user constraints. 4.3 Latency in end-to-end QUICKVIDEO Inference Figure 5: Latency from video loading, prefill and LLM decoding in an end-to-end inference setting. We compare baseline implementation of Qwen2.5-VL [4], Qwen2.5-VL implemented with QUICKPREFILL and QUICKCODEC, and lastly, our block overlapped design. We implement both QUICKCODEC and QUICKPREFILL into Qwen2.5-VL-7B-Instruct [4] inference pipeline. We implement our two versions: loading the entire video input using QUICKCODEC before prefilling with QUICKPREFILL, as well as our group overlapped variant. We benchmark latency from the video loading, prefill and LLM decoding stages in our end-to-end inference pipeline. For QUICKPREFILL, we use the Key Norms (small) as the KV cache pruning method and set ρ=0.2, and the number of frames in each group to be 32. We use 30 minute video (sampled at 1 FPS), as the baseline implementation runs out of memory when using longer video inputs. We use an A100 80GB SXM4 GPU and an AMD EPYC 7513 32-Core CPU. We allocate 16 cores for video processing. For our overlapped implementation, we use 64 video segments (s) for our parallelized loading. Our timings are averaged over 10 runs. Figure 5 shows the latency from video loading, prefill and LLM decoding of all 3 implementations. After QUICKCODEC reduces the video loading time, we can almost completely overlap video loading and prefill. In the overlapped pipeline, video processing, prefill and LLM decoding completes in only 20.0 seconds, 49.7 second speedup over the baselines 69.7 seconds. Our overlapped implementation has small startup latency, due to metadata parsing and decoding the frames for the first prefill block. This amounts to 2.8 seconds of video loading that cannot be overlapped with prefill."
        },
        {
            "title": "5 Discussion and Related Work",
            "content": "GPU support for video decoding. Video decoding can be accelerated by GPU computing. However, due to interframe dependencies, the speedup is not nearly as large as GPU acceleration for AI computations [27]. Furthermore, especially in the case of long video, GPU-based video decoding can result in device memory problems; the hour-long video we use for benchmarking (Section 4.1) is 3600 3 1920 800 1 byte 16.6 GB before being resized. This results in significant portion of GPU resources being allocated to video tensors, and can cause CUDA out-of-memory errors if not handled delicately. For simplicity, most existing inference libraries default to using CPU for video decoding [19, 39]. More sophisticated pipelines, such as NVIDIAs Cosmos training, use dedicated hardware for handling the video processing [26]. Efficient VideoLLMs Inference. Recent VideoLLMs [21, 20, 7] have demonstrated strong video understanding capabilities. Early models like Video-LLaVA [21] and VideoLLama-2 [9] were limited 9 to around 32 input frames due to constrained training data and unoptimized architectures. More advanced models such as Qwen2.5-VL [4] and InternVideo2.5 [35] can now handle hundreds of frames by adopting architectural innovations including Group Query Attention (GQA) [1], MRoPE [4], and Special Token Merging [7], which reduce KV cache size and enhance temporal reasoning. Nonetheless, the KV cache and activation memory still grow linearly with context length, creating bottlenecks in hour-long video inference. Meanwhile, existing token pruning techniques either address only image-level contexts [36, 6, 28, 42], or optimize for short prefill and long decoding scenarios [11, 45, 41]. In contrast, we target efficient prefill for millions of video tokens, introducing method that achieves substantial memory savings and speedup with minimal accuracy loss, thereby enabling scalable long video understanding on resource-constrained hardware."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced QUICKVIDEO, framework to accelerate long video understanding. Our framework has three core contributions: QUICKCODEC: systems framework for fast video loading, designed for VideoLLM frame sampling. QUICKPREFILL: An efficient algorithm for prefilling video tokens. Co-design: Lastly, we show that our video loading and prefill algorithm can be almost entirely overlapped, drastically reducing the time latency of these stages during inference. Overall, QUICKVIDEO reduces time to infer long video input by more than 3. Our work advances the capabilities for real-time video understanding applications, addressing key efficiency challenges in long video inference."
        },
        {
            "title": "References",
            "content": "[1] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit K. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. ArXiv, abs/2305.13245, 2023. URL https://api.semanticscholar.org/ CorpusID:258833177. [2] Fatih Cagatay Akyon and Alptekin Temizel. Deep architectures for content moderation and movie content rating, 2022. URL https://arxiv.org/abs/2212.04533. [3] Amazon. Amazon ec2 p5 instances. https://aws.amazon.com/ec2/instance-types/ p5/, 2025. Accessed: 2025-05-10. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. [5] Laura Ceci. YouTube: Statista statista.com. hours-of-video-uploaded-to-youtube-every-minute/, 2024. 05-2025]. hours 2022 https://www.statista.com/statistics/259477/ [Accessed 16every minute uploaded video of [6] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, 2024. URL https://api.semanticscholar.org/CorpusID:268358224. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. URL https://arxiv.org/abs/2412.05271. [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. URL https://arxiv.org/abs/2412.05271. [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. URL https://arxiv.org/abs/2406.07476. [10] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. ArXiv, abs/2307.08691, 2023. URL https://api.semanticscholar.org/CorpusID: 259936734. [11] Alessio Devoto, Yu Zhao, Simone Scardapane, and Pasquale Minervini. simple and effective l_2 norm-based strategy for KV cache compression. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1847618499, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1027. URL https://aclanthology.org/2024.emnlp-main.1027/. 11 [12] Distributed (Deep) Machine Learning Community. Decord. https://github.com/dmlc/ decord, 2019. Accessed: 2025-05-10. [13] Steven Feldstein. Global Expansion of AI Surveillance. Carnegie Endowment for International Peace, 2022. [14] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Videomme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. ArXiv, abs/2405.21075, 2024. URL https://api.semanticscholar.org/CorpusID: 270199408. [15] Google. Gpu machine types. https://cloud.google.com/compute/docs/gpus, 2025. Accessed: 2025-05-10. [16] Zhiyu Guo, Hidetaka Kamigaito, and Taro Watanabe. Attention score is not all you need for token importance indicator in kv cache reduction: Value also matters. In Conference on Empirical Methods in Natural Language Processing, 2024. URL https://api.semanticscholar. org/CorpusID:270562019. [17] Michel Kerdranvat, Ya Chen, Rémi Jullian, Franck Galpin, and Edouard François. The video codec landscape in 2020. ITU Journal: ICT Discoveries, 3(1):7383, 2020. [18] Rob Koenen. Mpeg-4 overview. Technical report, International Organization for Standardization, 1999. URL https://sound.media.mit.edu/resources/mpeg4/audio/general/ w3156.pdf. [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. ArXiv, abs/2408.03326, 2024. URL https://api.semanticscholar.org/CorpusID: 271719914. [21] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning In Conference on Empirical united visual representation by alignment before projection. Methods in Natural Language Processing, 2023. URL https://api.semanticscholar. org/CorpusID:265281544. [22] Xingyu Liu, Patrick Carrington, Xiang Anthony Chen, and Amy Pavel. What makes videos accessible to blind and visually impaired people? In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, CHI 21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445233. URL https://doi.org/10.1145/3411764.3445233. [23] TorchVision maintainers and contributors. Torchvision: Pytorchs computer vision library. https://github.com/pytorch/vision, 2016. [24] Microsoft. Nd-h100-v5 sizes series. https://learn.microsoft.com/en-us/azure/ Accessed: virtual-machines/sizes/gpu-accelerated/ndh100v5-series, 2025. 2025-05-10. [25] Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han, Hang Xu, and Li Zhang. Slowfocus: Enhancing fine-grained temporal understanding in video llm. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8180881835. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 94ef721705ea95d6981632be62bb66e2-Paper-Conference.pdf. [26] NVIDIA, :, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klár, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai, 2025. URL https://arxiv.org/abs/2501.03575. [27] PyTorch Team. torchcodec. https://github.com/pytorch/torchcodec, 2025. Accessed: 2025-05-10. [28] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. ArXiv, abs/2403.15388, 2024. URL https://api.semanticscholar.org/CorpusID:268667281. [29] Noam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020. URL https://api.semanticscholar.org/CorpusID:211096588. [30] Hairong Su, Shibo Wang, Shusen Yang, Tianchi Huang, and Xuebin Ren. Reducing traffic wastage in video streaming via bandwidth-efficient bitrate adaptation. IEEE Transactions on Mobile Computing, 23(11):1036110377, November 2024. ISSN 2161-9875. doi: 10.1109/tmc. 2024.3373498. URL http://dx.doi.org/10.1109/TMC.2024.3373498. [31] Gary J. Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand. Overview of the high efficiency video coding (hevc) standard. IEEE Transactions on Circuits and Systems for Video Technology, 22(12):16491668, 2012. doi: 10.1109/TCSVT.2012.2221191. [32] Suramya Tomar. Converting video formats with ffmpeg. Linux Journal, 2006(146):10, 2006. [33] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261272, 2020. doi: 10.1038/s41592-019-0686-2. [34] Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Shiyu Huang, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Lvbench: An extreme long video understanding benchmark. ArXiv, abs/2406.08035, 2024. URL https://api.semanticscholar.org/ CorpusID:270391637. [35] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyun Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kaiming Chen, Wenhai Wang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. ArXiv, abs/2501.12386, 2025. URL https://api.semanticscholar. org/CorpusID:275787960. [36] Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. ArXiv, abs/2502.11494, 2025. URL https://api.semanticscholar. org/CorpusID:276408079. 13 [37] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models, 2024. URL https://arxiv.org/abs/ 2404.03384. [38] T. Wiegand, G.J. Sullivan, G. Bjontegaard, and A. Luthra. Overview of the h.264/avc video coding standard. IEEE Transactions on Circuits and Systems for Video Technology, 13(7): 560576, 2003. doi: 10.1109/TCSVT.2003.815165. [39] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingfaces transformers: State-of-the-art natural language processing, 2020. URL https://arxiv.org/abs/1910. 03771. [40] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. ArXiv, abs/2407.15754, 2024. URL https://api.semanticscholar.org/CorpusID:271329356. [41] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. ArXiv, abs/2309.17453, 2023. URL https://api. semanticscholar.org/CorpusID:263310483. [42] Long Xing, Qidong Huang, Xiao wen Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, and Dahua Lin. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. ArXiv, abs/2410.17247, 2024. URL https://api.semanticscholar.org/CorpusID:273507889. [43] Tongtong Yuan, Xuange Zhang, Kun Liu, Bo Liu, Chen Chen, Jian Jin, and Zhenzhen Jiao. Towards surveillance video-and-language understanding: New dataset, baselines, and challenges, 2023. URL https://arxiv.org/abs/2309.13925. [44] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024. URL https://arxiv.org/abs/2407.12772. [45] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. ArXiv, abs/2306.14048, 2023. URL https://api.semanticscholar.org/CorpusID: 259263947. [46] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [47] Pengyuan Zhou, Lin Wang, Zhi Liu, Yanbin Hao, Pan Hui, Sasu Tarkoma, and Jussi Kangasharju. survey on generative ai and llm for video generation, understanding, and streaming, 2024. URL https://arxiv.org/abs/2404.16038. [48] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479. [49] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Nianchen 14 Deng, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Conghui He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Yingtong Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Han Lv, Lijun Wu, Kaipeng Zhang, Huipeng Deng, Jiaye Ge, Kai Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, 2025. URL https://arxiv.org/abs/2504.10479."
        },
        {
            "title": "A Parallelized Interval Algorithm",
            "content": "Additional video decoding background. The container contains various metadata about packets that we use during our interval parsing algorithm. For locality purposes, modalities such as audio and video are often interleaved in the bit stream S. Therefore, it is important to filter out audio packets when parsing the metadata stream. As packets are not frame-aligned, the pts field does not exactly represent the display time of frame. Also, as packets can be reordered by the decoder, the first or last packets may not correspond to the first and last frames. Algorithm 3 Calculate Parallelized Intervals K, ptsmin, ptsmax SCAN PACKETS(S) {ptsmin, ptsmax} 1 (ptsmax ptsmin) for 1, . . . , 1 do ptsestimate (c p) + ptsmin FINDINSERTIONINDEX(K, ptsestimate) if Kj1 ptsestimate < Kj ptsestimate then 1: procedure KEYFRAME INTERVALS(S, c) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: = {Kj1} = {Kj} else Scan packet metadata. Ordered list of keyframe intervals. Evenly spaced intervals in the video. Scan bit stream to get timestamps. Sorted set of keyframe timestamps. Skip packets are not used to decode video. Skip packets do not have pts metadata. return continue if pi.pts = NULL then if pi.type video then ptsmin 1 ptsmax for pi do 12: procedure SCAN PACKETS(S) 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: if pi.pts < ptsmin then ptsmin pi.pts if pi.pts > ptsmax then ptsmax pi.pts if pi.keyframe = True then {pi.pts} return K, ptsmin, ptsmax continue Algorithm 3 computes intervals that we can parallelize video decoding over. For effective parallelization, it is essential that these intervals are roughly length and keyframe-aligned, such that Algorithm 2 can seek to the start of each interval. SCAN PACKETS parses the metadata of the packet stream to find the location of all keyframes in S, as well as the minimum and maximum pts in S. If the packet does not belong to the video stream or the timestamp is NULL, the packet is skipped. After finding the locations of keyframes on line 2, KEYFRAME INTERVALS computes intervals as follows: We calculate the length of 1 of the stream, in pts units (line 4). On lines 5-10, we search for the keyframes closest to being th through the video, given by ptsestimate. FINDINSERTIONINDEX uses binary search to find where in the list of keyframes ptsestimate would be inserted. After finding the insertion point j, the algorithm checks whether the keyframe before or after is closer to ptsestimate. The closest keyframe location is added to , the list of intervals. [0] = ptsmin and [c 1] = ptsmax, to ensure that the intervals span the video. [1 2] are keyframe-aligned and equally spaced. Therefore, , list containing + 1 values, can be interpreted as intervals: = {(J [i], [i + 1]) 0, 1, . . . , c}. Effect of sampling rates on QUICKCODECs efficiency As QUICKCODEC does not seek between loading frames, all video frames are decoded during video loading. Conversely, seek-based frameworks skip decoding segments of video if there are large gaps between sampled frames. In Figure 6, we find that our framework has faster video loading when there is 4 second or less gap between sampled frames. Our library performs best when using VideoLLM sampling rates (1-2 FPS). Currently, our implementation always loads the whole video, and therefore does not benefit significantly from sparse sampling patterns. Our implementation could be adapted to leverage seeking when it detects that the user has sampled with large gap between frames, closing the performance gap with seek-based libraries [27, 12]. This would make our library more flexible, and eliminate potential performance sharp edge, where users accidentally use our QUICKCODEC for sparse sampling. We leave this as direction for future library improvements. Figure 6: Video decoding performance for different video durations with 1 FPS sampling."
        },
        {
            "title": "C Containers and Video Decoding",
            "content": "A multimedia container file format, like MP4 or MKV, bundles together all the elements required for media playback, including video streams, audio streams, subtitles, images, and metadata [18]. Video streams are compressed into bit streams by codecs . The bit streams are formatted in standards like H.264 [38] and H.265 [31]. codec consists of two algorithms: video encoding algorithm that takes in sequence of frames and outputs compressed bit stream and video decoding algorithm that takes the bit stream as input and outputs video frames. We focus video decoding, as it is the required operation before the video can be used as VideoLLM input."
        },
        {
            "title": "D QUICKPREFILL Efficiency Analysis Details",
            "content": "D.1 Activation Memory Analysis The activation memory of modern LLM architecture mainly comes from two components of each transformer block: 1) Attention Block and 2) MLP Block. We analyze the potential activation memory usage in formulas in the followings and show that group-based prefilling can effectively reduce the activation memory by times, where is the number of groups. Attention Block Modern LLMs commonly adopt FlashAttention [10], memory-efficient attention algorithm that computes exact attention with reduced memory usage by fusing multiple steps and processing attention in blocks. While the naive attention implementation would instantiate the full attention matrix RSS, FlashAttention avoids this by computing attention block by block. Let Q, K, RBSdhead denote the query, key, and value tensors respectively, with nh heads and dhead = dmodel . FlashAttention divides the input sequence into blocks of size Bc (for keys/values) and nh Br (for queries) to process attention efficiently within GPU memory constraints. Following [10], the dominant activation memory in FlashAttention comes from storing Q, K, . The block-based processing means that at any given time, only blocks of the attention matrix of size Br Bc are materialized in memory. Assume using float16 data type, the total activation memory can be expressed as: Mattn (3B nh dhead + nh Br Bc) 2 bytes (4) The first term accounts for storing Q, K, and tensors, while the second term accounts for the block of attention matrix being processed. With appropriate block sizes Br and Bc (typically set 17 based on GPU memory constraints), the second term remains relatively small. Assuming = 1, = (Xv ) 921600, dmodel = 4096, nh = 8, Br = Bc = 1024, we compute: Mattn = (3 1 921600 8 512 + 1 8 1024 1024) 2 bytes + Xt = 60, 584, 722, 432 bytes 21.1 GB (5) (6) (7) While FlashAttention significantly reduces memory requirements compared to naive attention implementation, this analysis shows it still consumes substantial memory for very long sequences. With group-based prefilling using = 225 groups, we can reduce the sequence length by times, reducing Mattn from 21.1 GB to approximately 0.09 GB. This dramatic reduction enables the processing of extremely long sequences that would otherwise be infeasible. MLP Block The SwiGLU (Swish-Gated Linear Unit) [29] enhances transformer models through improved gating mechanisms and has been adopted as the default MLP architecture in many popular LLMs including InternVL2.5 and Qwen2.5 series [4, 7]. For input representation Rdmodel, the SwiGLU operation is defined as: SwiGLU(x) = Wdown(SiLU(Wgatex) Wupx) (8) where Wgate, Wup Rdffdmodel , Wdown Rdmodeldff, and SiLU(x) = σ(x) with σ(x) = 1 For batch of sequences, activation memory analysis reveals requirements at each computational step. With batch size B, sequence length S, hidden dimension dmodel, intermediate dimension dff, and data type float16, the total activation memory for single SwiGLU layer is: 1+ex . Mact = (B (2dmodel + 4dff)) 2 bytes For one hour video sampled with 1 FPS (3600 frames in total), parameters can be set = 1, = (Xv ) 921600, dmodel = 4096, and dff = 14336: + Xt (9) Mact = (1 921600 (2 4096 + 4 14336)) 2 bytes = 241, 591, 910, 400 bytes 112.5 GB (10) (11) (12) This substantial memory requirement highlights the computational challenges in deploying SwiGLUbased models for high-resolution inputs with extended sequence lengths. However, if we prefill the tokens group by group, we can reduce the by times, and thus reduce the activation memory Mact by times. Assuming each group contains tokens of 16 frames, then = 3600 = 225 and we can 16 reduce Mact from 112.5 GB to 0.5 GB, which is substantial improvement. D.2 KV cache Memory Analysis When using InternVL2.5-8B [7], with each frame encoded as 256 tokens (V = 3,600 256 = 921,600), and = 256 text tokens, = 28 layers, nh = 8 heads, and dh = 512, the total memory required to store the KV cache in float16 precision is: Memory = 2 (Xv ) nh dh 2 bytes 393.9 GB . + Xt (13)"
        },
        {
            "title": "E Ablation Study on Group Size and Retention Ratio",
            "content": "Table 2: Ablation study of different group sizes and retention ratio ρ. We use Key Norms (small) as the KV pruning method here. Group Size ρ VideoMME LongVideoBench (val) LVBench MLVU (dev) Avg Performance - 4 8 16 32 64 128 16 16 16 16 16 16 16 1 0.5 0.5 0.5 0.5 0.5 0. 1 0.1 0.2 0.4 0.6 0.8 0.9 65.78 63.78 64.00 64.04 63.59 63.89 63.56 65.78 55.89 59.74 63.22 64.74 65.70 65.85 Varying Group Size 61.56 60.36 60.88 60.21 59.46 60.51 59.24 43.90 42.61 42.35 41.90 41.51 42.29 42. Varying Retention Ratio ρ 61.56 53.40 56.47 58.94 60.81 61.41 61.18 43.90 36.02 39.57 41.19 41.90 43.51 43.71 68.65 66.81 66.94 66.73 66.78 66.83 66.97 68.65 59.02 61.58 65.75 67.48 68.37 68.70 59.97 58.39 58.54 58.22 57.84 58.38 58. 59.97 51.08 54.34 57.27 58.73 59.75 59.86 100.00% 97.36% 97.62% 97.08% 96.44% 97.34% 96.87% 100.00% 85.18% 90.61% 95.51% 97.93% 99.63% 99.82%"
        },
        {
            "title": "F Reproducibility Statement",
            "content": "All machines used for timing experiments are running only essential operating system processes. We report how many runs our results are averaged over, as well 95% confidence intervals constructed using SciPy [33]. We try to use accessible configurations for timings (for example, AWS cloud instances) where possible. All code used for our paper will be open-sourced."
        },
        {
            "title": "G Limitations and Broader Impact",
            "content": "Limitations. As it is slow and resource intensive, most VideoLLMs are not trained to use their 1-2 FPS short video sampling rates when using processing long video [4, 7, 48]. Instead, they use very low sampling rates over large time-spans, as we discussed in Section 1. Therefore, VideoLLMs do not (yet) gain large performance advantage by processing large number of frames. However, it is clear that model that has seconds-long gaps between frames can never capture fine-grained temporal and spatial details. Our hope is that making long video understanding (with realistic sampling rates) practical from systems and algorithm perspective, we will empower the development of such models. Another limitation is that our QUICKCODEC timings only use H.264 coded video for timings. Although H.264 is the dominant standard, it is not universal. Broader Impact. As video has become the dominant modality of data, efficient long video understanding has extremely broad implications, both positive and negative. On the positive side, better long video understanding allows us to better interpret our digital landscape. In 2022, 30,000 hours of video were uploaded to YouTube every hour [5]. That number is absolutely much higher today. Without efficient long video understanding systems, we cannot understand our own digital artifacts, due to the scale at which we create them. Furthermore, long video understanding also has extremely compelling use-cases for information accessibility. video-first internet is difficult to navigate for visually impaired people, with important information potentially only accessible in video format [22]. Efficient, robust long video understanding presents can serve as backbone for tools for assisting video understanding for the visually impaired. However, efficient long video understanding also has potentially negative effects. As peoples lives are increasingly documented as video and uploaded to the internet, long video understanding models could become tool for privacy intrusion [13]."
        }
    ],
    "affiliations": [
        "University of Waterloo"
    ]
}