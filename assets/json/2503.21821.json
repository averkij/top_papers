{
    "paper_title": "PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving",
    "authors": [
        "Kaiyue Feng",
        "Yilun Zhao",
        "Yixin Liu",
        "Tianyu Yang",
        "Chen Zhao",
        "John Sous",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PHYSICS, a comprehensive benchmark for university-level physics problem solving. It contains 1297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop a robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements."
        },
        {
            "title": "Start",
            "content": "PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving Kaiyue Feng * Yilun Zhao Yixin Liu Y"
        },
        {
            "title": "Arman Cohan Y",
            "content": "Y Yale University New York University DNotre Dame University https://github.com/yale-nlp/Physics 5 2 0 2 6 2 ] - . s [ 1 1 2 8 1 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce PHYSICS, comprehensive benchmark for university-level physics problem solving. It contains 1,297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. We develop robust automated evaluation system for precise and reliable validation. Our evaluation of leading foundation models reveals substantial limitations. Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems. Through comprehensive error analysis, exploration of diverse prompting strategies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, we identify key areas for improvement, laying the foundation for future advancements."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in foundation models have shown strong performance on advanced mathematical reasoning tasks (Chen et al., 2023; Fan et al., 2024; Liu et al., 2024; Glazer et al., 2024). While mathematics underpins logical reasoning, we explore applications requiring multi-step reasoning and domainspecific knowledge. Physics, core natural science, provides mathematical framework for modeling (Yung-kuo and Class, 1994; Jackson, 1998; Sakurai and Napolitano, 2020; Pathria and Beale, 2011), spanning both deterministic classical mechanics and probabilistic quantum physics. Its complexity and multi-step reasoning demands make it an ideal domain for evaluating models on advanced problem solving. *Equal contributions. Correspondence: kf2365@nyu.edu, {yilun.zhao, john.sous, arman.cohan}@yale.edu. Full author contributions in Appendix A. Problem: Below (left) is diagram of basic siphon from Wikipedia. Assume that the crosssection of the tube is much smaller than the dimensions of the water reservoir that the siphon is draining. What is the speed of water flow out in terms of hc? What is the maximum speed of smooth water flow out in terms of hb? What is the maximum height hb? Solution: Since the siphon cross-section is much smaller than the reservoir dimensions, we can treat the reservoir as essentially infinite: the speed at which the water level in the reservoir is decreasing is negligible compared to the flow speed through the tube. Set the origin of vertical height at the water level in the reservoir. Then, the constant in Bernoullis equation is 1 Bernoulli states that this expression is the same at all points along the water flow. 2 ρ(0)2 + ρg(0) + P0 = P0. At C, this reads ρghc + P0 = P0, or vc = (cid:112)2ghc . At B, this reads 1 + ρghb + Pb = P0. We can drive Pb down to 0, but no further, or else cavitation occurs (bubble formation) and the flow will no 2 ρv2 2 ρv2 longer be smooth. Setting Pb = 0 gives vmax = (cid:115) 2 (cid:18) P0 ρ (cid:19) . ghb Requiring the above speed to be 0 yields the maximum height hb,max = P0 ρg . Figure 1: An example of classical mechanics problem in PHYSICS. PHYSICS is comprehensive benchmark for university-level physics problem solving which contains 1,297 expert-annotated problems. We present PHYSICS, comprehensive and challenging benchmark designed to assess foundation models physics problem-solving abilities. Unlike existing datasets that primarily consist of multiplechoice questions or focus on primaryto high school-level problems which current frontier models perform relatively well on (Welbl et al., 2017; Lu et al., 2022; Yue et al., 2024b; Hendrycks et al., 2021; Cobbe et al., 2021), PHYSICS is constructed from high-level, open-ended physics problems, specifically drawn from Physics PhD-qualifying exams that current foundation models still struggle Figure 2: For the overall process, we begin by collecting annotated problems from annotators (3.2), followed by validation to create processed dataset. This dataset is then used to prompt models (4.1). The responses from models undergo regular expression pre-processing and SymPy-based processing before final evaluation using an automated system (4.2). on1. In contrast to multiple-choice questions that may potentially allow models to exploit shortcuts or rely on answer recognition (Zheng et al., 2024; Aidar Myrzakhan, 2024), PHYSICS is composed of comprehensive questions which minimize the likelihood of bypassing the reasoning process and demand mastery of understanding theoretical concepts, complex inputs and the ability to integrate ideas from professional knowledge. PHYSICS spans six core physics subjects: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. These fields are selected as they encompass diverse range of problem-solving techniques, requiring deep mathematical modeling, multi-step logical reasoning, and theoretical integration. These settings make PHYSICS ideal for evaluating the reasoning and analytical capabilities of foundation models. We develop reliable automated evaluation system that is able to automate answer extraction, standardize mathematical expressions, and assess accuracy. Correctness is verified using SymPy, an open-source library for symbolic mathematics, for rule-based equivalence checking. When this fails, GPT-4o-based assessment is applied. We conduct an extensive evaluation across 33 frontier proprietary and open-source models. Our evaluation results show that even the bestperforming model, o3-mini, achieves 59.9% accuracy. This reveals critical challenges facing these 1Physics PhD-qualifying exam constitutes problems covering the core areas of physics that every PhD student must master to qualify as physicist. Traditionally, students need to pass this challenging exam before advancing to the research phase of their PhD program. These exams are typically difficult, with passing thresholds ranging from 40% to 70%. models, including their struggles with lengthy reasoning chains, reliance on incorrect assumptions, systematic errors, misunderstanding of images, and misinterpretation of problem statements. These failure modes persist across models, suggesting fundamental limitations in current models capabilities. The performance gap between proprietary and open-source models further illuminates areas for improvements - most open-source models trail significantly, with even the most capable and tested model (Qwen2.5-Math-72B) reaching 32.2% accuracy. Extended chain-of-thought and self-reflective reasoning are also not sufficient to significantly boost performance, with DeepSeek-R1 achieving 44.3% accuracy. Through detailed analysis and examination of specific failure patterns, we provide insights into the current limitations of frontier models and offer guidance for model development in contextual and domain-specific reasoning. We summarize our contributions as follows: We introduce challenging benchmark featuring expert-annotated physics problems spanning six subfields. Our benchmark demands deep multistep reasoning and theoretical knowledge integration, challenging frontier foundation models. We develop robust automated evaluation framework that ensures precise and standardized assessment by leveraging SymPy and GPT-based evaluation, enhancing the reliability of model performance measurement. We conduct comprehensive evaluation of both open-source and proprietary foundation models, systematically analyzing their strengths, weaknesses, and limitations in solving our benchmark. We provide an in-depth analysis of different Level Question Type Evaluation Reasoning Steps Benchmark Multi-modal JEEBENCH (Arora et al., 2023) MATH (Hendrycks et al., 2021) HARDMATH (Fan et al., 2024) GSM8K(Cobbe et al., 2021) GPQA(Rein et al., 2024) SciQ (Welbl et al., 2017) SciEval (Sun et al., 2023) SCIBENCH (Wang et al., 2024) MMMU (Yue et al., 2024a) MMMU-Pro (Yue et al., 2024b) MMVU (Zhao et al., 2025) SCIENCEQA (Lu et al., 2022) OlympiadBench(He et al., 2024) PUTNAMBENCH (Tsoukalas et al., 2024) PHYSICS Size 515 CEE 12,500 K12-Comp Graduate 1,466 K8 8,500 Graduate 227 K4-K8 13,679 - 1657 University 295 University 443 University 3,460 University 3,000 K1-K12 617 Comp 2334 University 1692 University 1297 OE, MC OE OE OE OE MC, OE OE, MC OE OE, MC MC OE, MC MC OE OE OE Rule-Based Rule-Based Rule + Model Rule-Based Rule-Based Rule-Based Rule-Based Rule-Based Rule-Based Rule-Based Rule + Model Rule-Based Rule-Based Rule-Based Rule + Model - - - 5.0 3.6 - - 2.8 - - - 2.4 3.7 - 5. Table 1: Comparison of PHYSICS with other benchmarks. For Level, comp: Competition, CEE: University Entrance Examination, K1-K12: Elementary and High School Level. For Question Type, OE: Open-ended Questions, MC: Multiple-choice Questions. Reasoning Steps are based on the statistics provided in the corresponding reference. prompting techniques, Long CoT, failure case studies, and Retrieval-Augmented Generation (RAG)-based knowledge augmentation, offering insights into guiding future improvements."
        },
        {
            "title": "2 Related Work",
            "content": "As AI continues to advance in reasoning tasks, recent efforts such as Humanitys Last Exam (Phan et al., 2025) highlight the growing need for highdifficulty, domain-specific benchmarks. OpenAIs o1 models (OpenAI et al., 2024b) demonstrate significantly improved performance on existing math benchmarks, reinforcing the necessity for more challenging datasets that push foundation models toward deeper scientific reasoning. In response to this need, the evaluation of foundation models in mathematical reasoning has evolved through increasingly structured benchmarks. Early datasets like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021) assessed elementary to undergraduate-level problem solving, but state-of-the-art models now achieve near-perfect performance on these. More advanced benchmarks, such as HARDMATH (Fan et al., 2024), OlympiadBench (He et al., 2024), PutnamBench (Tsoukalas et al., 2024), and FrontierMath (Glazer et al., 2024), introduced university and olympiad-level problems, further pushing model capabilities. This underscores the need to extend evaluation beyond mathematics into physics, where problemsolving involves real-world principles, physical laws, and multi-step derivations. The added complexity highlights the need for specialized benchmarks for advanced physics reasoning. As shown in Table 1, current scientific reasoning benchmarks, such as ScienceQA (Lu et al., 2022), and SciQ (Welbl et al., 2017), primarily focus on problems up to the high school level. Meanwhile, benchmarks like MMMU (Yue et al., 2024a), MMMUPro (Yue et al., 2024b), SciEval (Sun et al., 2023) and SciKnowEval (Feng et al., 2024) include only limited subset of physics, with stronger focus on multiple choice questions. Thus, existing benchmarks lack the depth needed to evaluate AIs ability to solve advanced physics problems."
        },
        {
            "title": "3 PHYSICS Benchmark",
            "content": "We present PHYSICS, comprehensive benchmark for university-level physics problems. PHYSICS has the following key features: (1) Comprehensive Subject Coverage: The dataset spans six core fields of modern physics, encompassing diverse range of fundamental and advanced topics, ensuring broad and extensive coverage of physics knowledge. (2) Deep Multi-Step Reasoning: Problems emphasize multi-step logical reasoning, requiring expertise in theoretical analysis, mathematical modeling, and complex problem-solving to assess advanced physics proficiency. (3) Automated Verification for Rigorous Evaluation: All solutions are verifiable using regular expressions and SymPy, enabling reliable correctness assessment and structured final answers for automated model benchmarking. (4) Strict Data Quality Control: Strict data quality control is enforced through structured annotation process carried out by expert annotators, with all annotations meticulously cross-verified for accuracy and consistency. 3.1 Problem Selection The benchmark dataset consists of 1,297 PhDqualifying exam physics problems from publicly available sources, covering six key physics fields: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. An example of selected question is shown in Figure 1. These fields were selected because they encompass the fundamental principles essential for modeling and analyzing physical systems across various scales. Together, they offer comprehensive evaluation of models capability to handle deterministic and probabilistic reasoning, maintain mathematical precision, and solve multi-step problems effectively. 3.2 Data Annotation Our annotation process follows structured pipeline to ensure high-quality and consistent data. dedicated team of seven expert annotators, all proficient in physics, carried out the annotation process (detailed biographies are presented in Table 4). Before contributing, each annotator was required to pass qualification test assessing their ability to accurately annotate complex physics problems. The problems were sourced from major textbooks and reputable online resources, with annotators ensuring that the selected questions adhered to copyright and licensing regulations. For the problem annotation, annotators formatted problem statements using LATEX , ensuring precision in mathematical notation, consistency in variable usage, and adherence to standard physics conventions. Key metadata, including problem conditions and underlying assumptions, were documented to enhance clarity and comprehension. For the solution annotation, each problems solution was carefully structured with step-by-step reasoning process, ensuring logical coherence and mathematical accuracy. Annotators incorporated theoretical justifications, derivations, and final answers in structured format suitable for automated verification. Figures and diagrams were annotated to accurately represent physical concepts, insuring the clarity of problem-solving steps. To assess problem difficulty, annotators were required to provide ratings based on the following aspects: Creativity: The time required to identify the key steps necessary to solve the problem. Complexity: The number of attempts needed to arrive at correct solution. 3.3 Data Quality Check To maintain mathematical rigor, annotators were explicitly required to focus on the accurate expression of equations, numerical constants, and symbolic representations. Special attention was given to ensuring that all mathematical formulations adhered to standard conventions and were free of typographical or conceptual errors. To ensure the highest accuracy of annotations, multi-step review mechanism was implemented. Each annotation underwent an independent verification process by the authors to confirm correctness and adherence to the intended guidelines. Ambiguities were solved through effective discussion between authors and annotators. Each problem underwent secondary validation process, where second annotator carefully reviewed the annotations to ensure accuracy and consistency. Any identified errors were corrected, and necessary refinements were made to align with the intended guidelines."
        },
        {
            "title": "3.4 Data Statistics",
            "content": "Table 2 and Figure 3 presents the key statistics of our dataset. It comprises total of 1,297 questions, with 298 being in multimodal format. To provide clear subsets for future researchers to train models and for testing, total of 297 questions are randomly selected as the validation set for model development and validation, while the test set, which contains the remaining 1,000 questions, is reserved for standard evaluation to prevent data contamination. According to the difficulty assessment done by annotators, we select 523 questions as the HARD subset. This subset is designed to provide more challenging benchmark for evaluating the capabilities of current frontier foundation models. Over the entire PHYSICS dataset, the average question length is 83.7 words, while solutions exhibit significantly higher average length of 5.4 reasoning steps, with 24% of them containing more than 10 reasoning steps, reflecting the deep reasoning required for problem solving. These statistics highlight the datasets potential to assess models on multi-step reasoning and precise answer generation in the domain of physics. Category Dataset Overview Total Questions Questions with Figures Validation : Test Split Hard : Regular Questions Value 1,297 298 297 : 1,000 523 : 774 Subject Distribution Number of Subjects Atomic Physics Electromagnetism Classical Mechanics Optics Quantum Mechanics Statistical Physics Question Complexity 6 200 242 221 158 236 240 Avg. Question Length (words) 83.7 Solution Statistics Avg. Solution Length (words) Avg. Reasoning Steps 234.75 5.38 Table 2: Dataset statistics of PHYSICS. Figure 3: Reasoning steps distribution."
        },
        {
            "title": "4 Experiment",
            "content": "This section discusses our experiment setup, main findings, and analysis."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "To systematically evaluate the capabilities of stateof-the-art models and establish clear reference points for future research, we conduct comprehensive assessment across diverse set of models including five proprietary models: GPT-4o (OpenAI et al., 2024a), Gemini-1.5-Pro (Team et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024), o3mini (OpenAI, 2025) and o1-mini (OpenAI et al., 2024b). Also, 28 open-source models are evaluated, including DeepSeek-R1 (DeepSeek-AI et al., 2025), Llama 3.3 and Llama 3.1 series (Grattafiori et al., 2024), Qwen2, Qwen 2.5 and Qwen2.5-Math series (Qwen et al., 2025), QwQ (Team, 2024), Gemma2 series (Gemma Team, 2024), Command- (Cohere, 2024), Phi-4 (Abdin et al., series 2024), Mistral, Mathstral and Mixtral series (Jiang et al., 2024), ChatGLM (GLM et al., 2024), Internlm3 (Cai et al., 2024), Yi-1.5 (AI, 2025), Aria (Li et al., 2025), InternVL2.5 (Chen et al., 2025) and Pixtral (Agrawal et al., 2024). Open-source models are evaluated using the vLLM pipeline (Kwon et al., 2023), while proprietary models are accessed via their official APIs. For text-only models, we tested on the subset of text-only questions. We adopt the Chain-of-Thought prompt used in MATH (Hendrycks et al., 2021) for model evaluation. The models are instructed to output the final answer in boxed format. Chain-of-Thought Prompt for LLM Question: {question} Figure (For multimodal-format problem): {base64 processed image} Prompt: Answer the given question step by step. Begin by explaining your reasoning process clearly. Conclude by stating the final answer using the following format: Provide the final answer at the end in LaTeX boxed format final_answer . Think step by step before answering."
        },
        {
            "title": "4.2 Automated Evaluation System",
            "content": "undergoes the final answer Model Response Processing. The modelsystematic response generated processing to extract in structured manner. First, we leverage Pythons built-in re module to use regular expressions for searching and matching specific patterns within the model-generated response, thereby extracting the final boxed answer (boxed{}). Following this, mathematical expressions are pre-processed to maintain consistency and validity. This includes standardizing notations, such as ensuring that fractions are formatted uniformly (e.g., converting expressions like frac1b to the explicit form frac{1}{b}), reducing variability in formatting. Furthermore, to ensure logical accuracy, only the core mathematical content relevant to the final answer is extracted for comparison. For example, in statements involving logical implications (e.g., = ), only the portion following the implication arrow is considered, as it represents the models asserted conclusion. This extraction approach prevents extraneous elements from affecting correctness judgments. Accuracy Evaluation. To rigorously assess the correctness of model responses, the framework utilizes robust mathematical verification system. Model AMO E&M CM Opt. QM Stats. Val Test Test Set Performance Overall o3-mini o1-mini Gemini-1.5-pro GPT-4o Claude-3.5-Sonnet DeepSeek-R1 Qwen2.5-Math-72B Llama-3.3-70B phi-4 Qwen2.5-72B Qwen2.5-32B Mistral-Small-24B Qwen2.5-7B Qwen2.5-14B Gemma-2-27b Yi-1.5-34B Qwen2.5-Math-1.5B InternVL2-5-38B Aria QwQ-32B-Preview Gemma-2-9b Mistral-7B Llama-3.1-8B Mathstral-7B c4ai-command-r-v01 DeepSeek-R1-Distill-Qwen-32B Gemma-2-2b Qwen2-VL-72B Internlm3-8b DeepSeek-vl2-small THUDM-chatglm3-6b Qwen2.5-Math-7B DeepSeek-math-7b-rl Proprietary Models 64.9 41.8 40.2 44.1 34. 59.8 41.9 31.5 33.4 27.6 Open-Source Models 48.6 34.8 35.8 33.0 30.9 27.5 29.5 28.1 19.7 19.0 15.4 14.8 12.5 14.0 7.5 8.2 10.4 17.4 10.0 7.8 5.4 6.2 3.5 4.6 1.8 2.3 1.7 0.0 38.3 27.3 27.9 19.8 23.0 19.4 19.6 11.2 14.1 16.2 18.0 16.5 12.5 14.2 10.1 9.1 5.1 6.8 12.0 7.5 4.8 3.9 4.6 4.7 1.8 0.0 0.0 0.0 51.5 40.6 32.2 23.4 35.5 43.1 27.4 17.2 27.2 25.4 20.8 17.6 18.7 12.3 13.4 13.2 16.2 7.7 11.7 11.2 16.5 13.7 14.7 9.6 3.8 9.8 10.3 4.0 3.2 4.5 0.7 2.1 1. 66.0 44.3 44.5 33.8 35.1 44.5 36.2 31.4 23.4 27.4 24.7 15.2 17.4 13.5 18.4 19.6 17.2 18.0 9.7 10.6 12.1 11.6 7.4 8.2 7.5 2.3 3.9 2.9 4.0 0.0 0.9 0.0 0.0 52.4 45.4 35.5 35.3 37.2 37.0 27.0 28.2 32.8 28.8 25.5 19.1 21.8 23.8 14.3 11.0 13.3 15.3 13.0 16.7 9.4 10.1 8.4 7.3 2.0 9.1 6.6 11.8 1.8 3.1 0.9 1.4 0.7 60.0 48.0 43.7 45.0 38.4 51.5 37.0 41.3 35.2 33.2 41.1 28.4 22.1 28.2 25.9 25.2 19.5 23.1 14.4 14.8 16.9 17.6 16.1 17.6 11.4 10.2 7.3 4.2 9.2 0.3 2.0 1.5 0. 55.0 44.1 35.3 34.7 31.7 44.2 38.5 34.3 28.7 31.5 23.3 25.1 20.9 25.3 21.7 25.3 15.1 16.7 12.7 12.4 15.2 12.6 9.1 12.0 6.8 7.1 6.1 4.5 4.1 4.8 0.9 1.9 0.9 59.9 43.6 38.4 36.7 34.7 44.3 32.2 31.5 29.1 28.7 27.6 21.8 20.4 19.6 18.3 17.4 16.4 15.3 12.9 12.1 11.9 11.7 11.7 10.8 7.0 6.8 6.1 5.0 4.8 1.7 1.2 1.0 0.4 Table 3: Performance comparison across tasks. : These models are equipped with multi-model abilities. Problems with images are also tested on these models. Abbreviations: AMO (Atomic Physics) E&M (Electromagnetism) CM (Classical Mechanics) Opt. (Optics) QM (Quantum Mechanics) Stats. (Theromodynamics and Statistical Physics). The models are ranked by average test set performance. The processed LATEX answers are parsed into symbolic representations using sympy.parse_latex, which enables direct mathematical equivalence checking through the function sympy.is_equiv. This ensures that two mathematically equivalent expressions are recognized as correct even if they appear in different algebraic forms. In scenarios where questions consist of multiple sub-answers, accuracy is computed as the fraction of correct responses relative to the total number of expected answers. For instance, if question consists of five distinct parts and the model correctly answers three, the computed accuracy score would be 0.6, providing fine-grained measure of performance. Specifically, GPT-4o is used to evaluate answers that rely on natural language explanations, ensuring correctness, relevance, and logical consistency. This approach is particularly important in physics, where conceptual understanding and domain-specific knowledge extend beyond mere mathematical calculations. Additionally, GPT-4o evaluation is employed as fallback when symbolic computation (e.g., SymPy) returns False or fails to verify an answer. By integrating this backup method, we enhance the robustness and reliability of our accuracy evaluation system. Detailed prompts are shown in Appendix C."
        },
        {
            "title": "4.3 Main Findings",
            "content": "Table 3 shows the model performance on the PHYSICS dataset. Our key findings are as follows: PHYSICS challenges current models. Our benchmark presents significant challenges for current foundation models. Even the most advanced proprietary models like o3-mini achieve only 59.9% accuracy, while other mainstream models such as GPT-4o and Gemini-1.5-Pro achieve only approximately 37% accuracy, highlighting substantial gap in their ability to handle complex mathematical and conceptual reasoning. In contrast, human performance on similar tasks is significantly higher. We randomly select 10 tasks for each human expert. Given adequate time, human experts typically achieve scores ranging from 70% to 80%. This disparity highlights the limitations of current models in reasoning through multi-step physics problems. Proprietary Model Performance Analysis. Proprietary models, such as GPT-4o and Gemini-1.5Pro, achieve an accuracy of approximately 37% on our benchmark. While these state-of-the-art models exhibit strong reasoning capabilities and solid grasp of problem comprehension, their performance underscores significant gap in solving PhD-qualifying exam physics problems. Notably, frontier models such as o3-mini, which incorporate system-2 reasoning, demonstrate substantial leap in performance compared to other proprietary models. This improvement underscores the effectiveness of Chain-of-Thought (CoT) reasoning and deep-thinking training, further emphasizing the importance of structured reasoning paradigms in enhancing AI capabilities on challenging tasks. Open-Source Model Performance Analysis. For open-source models, we do observe DeepSeekR1 achieved the highest score of 44.3% in accuracy, due to its advanced training in CoT and reasoning abilities. Other state-of-the-art open-source models, such as Llama-3.3-70B, Phi-4, and Qwen-2.5-72B, achieved an accuracy of approximately 30%. However, beyond these leading models, the majority of open-source models exhibit significantly lower performance, with accuracy lower than 20%. This performance gap highlights substantial disparity between open-source models and their proprietary counterparts. While open-source models continue to make progress, their effectiveness in complex reasoning tasks remains limited compared to stateof-the-art proprietary models. This underscores the need for further advancements in the opensource AI community, particularly in enhancing multimodal and expert-level reasoning capabilities. 4.4 Qualitative Analysis We conduct detailed case study on the topperforming models. Specifically, we focus on GPT-4o, Llama3.3-70B, and DeepSeek-R1 to gain deeper insights into their strengths and weaknesses. Analysis of Multimodal Models. PHYSICS includes problems that requires image understanding. We conduct case studies on specific tasks where interpreting visual information is essential for problem solving. The experiments reveal errors such as failures in accurately interpreting expressed spatial information or misunderstanding the relation between objects in the image. These issues often lead to missing or incorrect information in the models reasoning processes. An example is provided in Appendix E. Inability to integrate Professional Knowledge. One issue with foundation models when reasoning about physics problems is their tendency to misinterpret or overlook fundamental real-world principles that are not explicitly stated in the problem. This limitation indicates their inability to integrate essential background knowledge, often leading to incorrect assumptions or flawed conclusions. Addressing this gap requires models to develop deeper understanding of physical systems and incorporate common knowledge when solving such problems. An example is shown in Appendix D.2. Non-Existent Assumptions. Another frequent issue is the introduction of extraneous conditions that were not explicitly provided in the original question. These additional assumptions alter the problem scope, leading to incorrect conclusions and reducing the reliability of the generated solution. This reflects the need for models with the ability to reason within the given constraints while ensuring that no unwarranted premises are introduced, thereby preserving the accuracy and relevance of the solution. An example is shown in Appendix D.3. Calculation Errors in Complex Equations. Despite demonstrating strong reasoning abilities, models can still struggle with computational accuracy when dealing with intricate mathematical expressions. Errors frequently emerge in multistep calculations, particularly those involving symbolic manipulations and algebraic simplifications. These mistakes undermine the reliability of modelgenerated responses, emphasizing the need for im4.6 RAG-based Knowledge Augmentation We next propose the use of Retrieval-Augmented Generation (RAG) to address performance limitations of foundation models in integrating professional knowledge for physics. To determine if access to external knowledge can mitigate this weakness, we compare the accuracy and reliability of four top-performing models on our benchmark both with and without RAG augmentation. Our RAG implementation utilizes SerpAPIs Google Search, with models prompted (see Appendix C.3) to autonomously formulate search queries based on the presented physics questions. This approach directly assesses whether incorporating retrieved knowledge via RAG mitigates observed deficiencies and improves performance in this challenging context. The context of top-5 search results is concatenated to the original query and fed into the models. As shown in Figure 4, the RAG setting improves performance across all tested models. These findings underscore the potential of retrieval-based augmentation in enhancing model reasoning capabilities. Highlighting the importance of external knowledge integration in addressing gaps in foundational models domain-specific understanding."
        },
        {
            "title": "4.7 Analysis of Models with Long CoT Ability",
            "content": "Models trained for stronger reasoning abilities, such as o3-mini, o1-mini, and DeepSeek-R1, exhibit significant improvements in physics reasoning. Their enhanced capability for long Chain-ofThought (CoT) reasoning enables these foundation models to outperform others, demonstrating the effectiveness of prioritizing advanced AI reasoning abilities in training. common issue observed in DeepSeek-R1 and QwQ-32B which encourage selfreflection (we did not analyze OpenAIs o1-series models as the full reasoning outputs are not available) is the tendency to generate overly extensive reasoning chains. While self-reflection thinking can be beneficial, it often results in an unnecessarily prolonged thinking process, frequently exceeding the 10240-token limit. This leads to incomplete answers or excessive computational overhead, diminishing the overall effectiveness of the response. Examples are shown in Appendix D.1."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce PHYSICS, benchmark designed to evaluate foundation models on PhD-qualifying exam physics problems, comprising 1,297 quesFigure 4: Comparison between different methods. proved numerical precision and robust verification mechanisms. An example is shown in Appendix D.4. Misunderstanding Question. An occasional issue in model evaluations is the failure to correctly interpret the problem statement, leading to irrelevant or inaccurate responses. This issue does not arise often. However, we find that smaller-scale models are more prone to misinterpretation. This manifests in several ways: misidentifying key variables, overlooking critical constraints, or applying incorrect problem-solving frameworks. Such misunderstandings can result in fundamentally flawed reasoning chains that diverge from the intended solution path. Additionally, models may misinterpret ambiguous phrasing, leading to incorrect assumptions that were not implied by the question. An example is shown in Appendix D.5. We believe that addressing the aforementioned shortcomings will be crucial in developing foundation models capable of handling complex reasoning tasks in physics and other scientific domains."
        },
        {
            "title": "4.5 Analysis of Different Prompting Methods",
            "content": "To evaluate the impact of self-reflection on model performance, we compare Chain-of-Thought and Self-Reflection (Renze and Guven, 2024) prompting methods (prompts provided in Appendix C.2) with the four best-performing models on our benchmark. Models prompted with self-reflection generally demonstrate improved reasoning consistency and accuracy compared to CoT prompting (Figure 4). These findings highlight the broad effectiveness of self-reflection across different model scales, as it enhances problem-solving accuracy, making foundation models more reliable and efficient across diverse tasks and architectures. tions across six core subfields. Our results show that even leading proprietary models, o3-mini, achieve only 59.9% accuracy, underscoring fundamental weaknesses in scientific reasoning, conceptual understanding, and mathematical precision. Through detailed failure analysis, we identify five key failure patterns: (1) Inability to integrate professional knowledge, (2) reliance on incorrect assumptions, (3) difficulties in handling multimodal data, (4) calculation errors in multi-step reasoning, and (5) misunderstanding questions. Various prompting methods and RAG-based knowledge augmentation are also explored. These methods demonstrate potential for improvement but do not fully bridge the gap in expert-level physics problem-solving. These findings emphasize the need for enhanced reasoning frameworks, improved mathematical precision, and effective integration of physics knowledge sources to bridge the gap. PHYSICS establishes comprehensive evaluation framework, providing foundation for measuring progress in scientific reasoning and providing insights for future AI model development in specialized domains."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported in part by the NVIDIA Academic Grant Program. We are grateful to the Google TRC program for providing computing resources and Together AI for granting LLM API credits."
        },
        {
            "title": "Limitations",
            "content": "While PHYSICS serves as valuable benchmark for AI-driven physics problem solving, it has some limitations. One limitation is its reliance on automated evaluation using SymPy for rule-based checking and GPT-based assessments. While effective, this approach may fail to recognize equivalent solutions and introduce subjectivity. Incorporating expert human review could enhance evaluation accuracy. Moreover, although PHYSICS is extensive, comprising 1,297 questions, it does not fully encompass the breadth of physics. More advanced and interdisciplinary topics remain underrepresented. Expanding the dataset to include wider range of complex topics would enhance its comprehensiveness. We encourage future research to address these gaps by incorporating more advanced and diverse areas of physics. The SymPy evaluation system has certain limitations. Errors may arise during expression extraction or when assessing mathematical equivalence, despite the use of GPT-4o as fallback mechanism. An illustrative example of such an error is presented in appendix F."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. 2024. Phi-4 technical report. Preprint, arXiv:2412.08905. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, and et al. 2024. Pixtral 12b. Preprint, arXiv:2410.07073. ICML Submission. 01. AI. 2025. Yi: Open foundation models by 01.ai. Preprint, arXiv:2403.04652. Zhiqiang Shen Aidar Myrzakhan, Sondos Mahmoud Bsharat. 2024. Open-llm-leaderboard: From multi-choice to open-style questions for llms evalarXiv preprint uation, benchmark, and arena. arXiv:2406.07545. Anthropic. 2024. Introducing claude 3.5 sonnet. Accessed: 2024-06-21. Daman Arora, Himanshu Gaurav Singh, and Mausam . 2023. Have LLMs advanced enough? challenging problem solving benchmark for large language models. In The 2023 Conference on Empirical Methods in Natural Language Processing. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Internlm2 technical Keyu Chen, and et al. 2024. report. Preprint, arXiv:2403.17297. Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. TheoremQA: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, Singapore. Association for Computational Linguistics. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, and et al. 2025. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. Preprint, arXiv:2412.05271. ICML Submission. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Cohere. 2024. Command and command r+ model card. Accessed: 2025-01-29. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, and et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. ICML Submission. Jingxuan Fan, Sarah Martinson, Erik Y. Wang, Kaylie Hausknecht, Jonah Brenner, Danxian Liu, Nianli Peng, Corey Wang, and Michael Brenner. 2024. HARDMATH: benchmark dataset for challengIn The 4th ing problems in applied mathematics. Workshop on Mathematical Reasoning and AI at NeurIPS24. Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. 2024. Sciknoweval: Evaluating multi-level scientific knowledge of large language models. Preprint, arXiv:2406.09098. Gemma Team. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Elliot Glazer, Ege Erdil, Tamay Besiroglu, Diego Chicharro, Evan Chen, Alex Gunning, Caroline Falkman Olsson, Jean-Stanislas Denain, Anson Ho, Emily de Oliveira Santos, Olli Järviniemi, Matthew Barnett, Robert Sandler, Matej Vrzala, Jaime Sevilla, Qiuyu Ren, Elizabeth Pratt, Lionel Levine, Grant Barkley, Natalie tewart, Bogdan Grechuk, Tetiana Grechuk, Shreepranav Varma Enugandla, and Mark Wildon. 2024. Frontiermath: benchmark for evaluating advanced mathematical reasoning in ai. Preprint, arXiv:2411.04872. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, and et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. ICML Submission. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, and et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. ICML Submission. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). John David Jackson. 1998. Classical Electrodynamics, 3rd edition. Wiley. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, and et al. 2024. Mixtral of experts. Preprint, arXiv:2401.04088. ICML Submission. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Fan Zhou, Chengen Huang, Yanpeng Li, Chongyan Zhu, Xiaoyi Ren, Chao Li, Yifan Ye, Peng Liu, Lihuan Zhang, Hanshu Yan, Guoyin Wang, Bei Chen, and Junnan Li. 2025. Aria: An open multimodal native mixture-of-experts model. Preprint, arXiv:2410.05993. Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, and Kai Chen. 2024. Mathbench: Evaluating the theory and application proficiency of llms with hierarchical mathematics benchmark. Preprint, arXiv:2405.12209. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS). OpenAI. 2025. Openai o3-mini system card. Accessed: February 13, 2025. OpenAI, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, and et al. 2024a. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, and et al. 2024b. Openai o1 system card. Preprint, arXiv:2412.16720. R.K. Pathria and Paul D. Beale. 2011. Statistical Mechanics, 3rd edition. Elsevier. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, and et al. 2025. Humanitys last exam. Preprint, arXiv:2501.14249. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, and et al. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. ICML Submission. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Matthew Renze and Erhan Guven. 2024. Self-reflection in llm agents: Effects on problem-solving performance. Preprint, arXiv:2405.06682. Chuhan Li, Junyang Song, Zhijian Xu, Chengye Wang, Weifeng Pan, Ziyao Shangguan, Xiangru Tang, Zhenwen Liang, Yixin Liu, Chen Zhao, and Arman Cohan. 2025. Mmvu: Measuring expertlevel multi-discipline video understanding. Preprint, arXiv:2501.12380. Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2024. Large language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations. J. J. Sakurai and Jim Napolitano. 2020. Modern Quantum Mechanics, 3rd edition. Cambridge University Press. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2023. Scieval: multi-level large language model evaluation benchmark for scientific research. arXiv preprint arXiv:2308.13149. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, and et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Preprint, arXiv:2403.05530. Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. 2024. Putnambench: multilingual competition-mathematics benchmark for formal theorem-proving. In AI for Math Workshop @ ICML 2024. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. In Forty-first International Conference on Machine Learning. Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy Usergenerated Text, pages 94106, Copenhagen, Denmark. Association for Computational Linguistics. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. 2024a. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9556 9567. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, Yu Su, Wenhu Chen, and Graham Neubig. 2024b. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813. Lim Yung-kuo and The Physics Coaching Class. 1994. Problems and Solutions on Thermodynamics and Statistical Mechanics. World Scientific. Major American Universities Ph.D. Qualifying Questions and Solutions. Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen,"
        },
        {
            "title": "A Author Contributions",
            "content": "Project leads: Kaiyue Feng and Yilun Zhao Initial project idea: Kaiyue Feng, Yilun Zhao, John Sous, and Arman Cohan. Data annotation task management and validation: Kaiyue Feng, Yilun Zhao, and Tianyu Yang. Experiments: Kaiyue Feng, Yilun Zhao, and Yixin Liu. Writing: Kaiyue Feng and Yilun Zhao led the writing, with editing contributions from all authors. Mentorship and advising: Chen Zhao, John Sous, and Arman Cohan."
        },
        {
            "title": "B Annotation Biography",
            "content": "This section presents an overview of the biography of annotators, including their qualifications, majors, and assigned subjects. ID Year Major Assigned Subject(s) 1 2 4 5 6 7 2nd Year Undergraduate 3rd Year Undergraduate 2nd Year Undergraduate Theoretical Physics Quantum Mechanics, Thermodynamics and StaClassical Mechanics, Electromagnetism Quantum Mechanics, Optics Physics Physics 3rd Year Undergraduate Applied Physics tistical Mechanics Thermodynamics and Statistical Mechanics, Atomic Physics 2nd Year Undergraduate Engineering Physics Electromagnetism, Classical Mechanics 3rd Year Undergraduate Physics Thermodynamics and Statistical Mechanics, Atomic Physics Classical Mechanics, Optics 2nd Year Undergraduate Astrophysics Table 4: Biographies of 7 annotators involved in the Physics benchmark construction"
        },
        {
            "title": "C Prompts",
            "content": "This section documents the various prompts designed and utilized in our LLM experiments. Each prompt was crafted to elicit specific types of responses from the LLMs being evaluated. C.1 Math Equivalency Prompt"
        },
        {
            "title": "Prompt for LLM",
            "content": "Question: {question} Image: {base64 processed image} Task: Compare the following LATEX expressions and determine if the numerical content is equivalent. Expression 1: {expr1} Expression 2: {expr2} Return True if they are mathematically equivalent; otherwise, return False. Focus only on numerical and mathematical content. If the expressions involve multiple-choice answers (e.g., A, B, C, D), compare only the letter choices. C.2 Self-reflection Prompt Prompt for LLM Initial Answer Step: \"You are an AI expert specializing in answering advanced physics questions. Think step by step and provide solution and final answer. Provide the final answer at the end in LaTeX boxed format [ boxed{} ]. Example: [ boxed{ final_answer} ]\" Refinement Step: # Put the initial answer into the context of the conversation and ask the LLM to self-check and improve the answer messages.append({\"role\": \"assistant\", \"content\": first_response}) messages.append({ \"role\": \"user\", \"content\": ( \"Please check your previous answer carefully. Identify any mistakes and refine your final answer. Provide the revised answer at the end in LaTeX boxed format [ boxed{} ].\" ) }) C.3 RAG Prompt"
        },
        {
            "title": "Prompt for LLM",
            "content": "Prompt:\"You are physics expert skilled in using search engines to find relevant information. Based on the given physics problem, think about the best way to query Google. Generate up to 3 relevant search queries that focus on key concepts, formulas, or theories. Do not search the entire question; instead, extract essential elements \" Task: \"Generate relevant Google search queries for the following physics problem:{question_text} \""
        },
        {
            "title": "D Error Analysis",
            "content": "D.1 Excessively Long Reasoning Chains Question: It can be shown that the electric field inside dielectric sphere which is placed inside large parallel-plate capacitor is uniform (the magnitude and direction of E0 are constant). If the sphere has radius and relative dielectric constant Ke = ε/ε0, find at point on the outer surface of the sphere (use polar coordinates R, θ). Determine the bound surface charge density at point p. Solutions: The electric field inside the sphere is uniform field E0. The field at point of the outer surface of the sphere is using polar coordinates. Similarly, E0 may be expressed as = Erer + Eteθ, From the boundary conditions for the electric vectors at p, we obtain E0 = E0 cos θer E0 sin θeθ. Hence, εE0 cos θ = ε0Er, E0 sin θ = Et. = KeE0 cos θer E0 sin θeθ. The bound surface charge density at point is σb = er, where is the polarization vector. As we find = (ε ε0)E0, σp = (ε ε0)E0 cos θ = ε0(Ke 1)E0 cos θ. Deepseek-R1 Okay, so have this problem about dielectric sphere inside parallel-plate capacitor. The question says that the electric field inside the sphere is uniform with magnitude E0. need to find the electric field at point on the outer surface of the sphere using polar coordinates and θ. Also, determine the bound surface charge density at point p. Hmm, let me start by recalling some concepts from electromagnetism related to dielectrics. When you place dielectric material into an external electric field, it gets polarized. This polarization creates bound charges: volume bound charges and surface bound charges. The key here might be understanding how these bound charges affect the overall electric field both inside and outside the dielectric. Since the sphere is placed within larger systema parallel-plate capacitorthe initial setup probably involves applying an external electric field, but because the spheres presence modifies the field around itself. wait, actually, the problem states that the electric field inside the sphere is already given as uniform, E0. So maybe we dont need to worry too much about calculating E0; instead, our focus should be on finding the electric field just outside the sphere and then determining the surface charge density there. remember that for linear isotropic dielectrics, the relation between the electric displacement and the electric field is given by = ε0E + P, where is the polarization. And since the relative permittivity Ke is defined as ε/ε0, perhaps well use that somewhere. Also, Gausss Law comes into play when dealing with symmetric systems like spheres. But wait, the original capacitor plates create uniform field, right? However, placing spherical object would perturb that fieldso near the sphere, especially externally, the field wont be exactly uniform anymore. But hold on... This extends to the end Error Category: Excessively Long Reasoning Chains. Error Reason: The model did show reasoning process and do seem to be mimicking humans evaluation process. But the reasoning process is way too long. D.2 Inability to integrate Professional Knowledge422 Question: particle of mass moves one-dimensionally in the oscillator potential In the nonrelativistic limit, where the kinetic energy and momentum are related by (x) = mω2x2. 1 2 = p2 2m , the ground state energy is well known to be 1 2 hω. Allow for relativistic corrections in the relation between and and compute the ground state level shift to order 1 c2 (c = speed of light). Solutions: In relativistic motion, the kinetic energy is given by mc2 = (cid:112) m2c4 + p2c2 mc2. Expanding using binomial approximation: = mc2 (cid:19) 1 2 (cid:18) 1 + p2 m2c2 mc2 mc2 (cid:18) 1 + p2 2m2c2 p4 8m4c4 (cid:19) mc2 = p2 2m p4 8m3c2 . To order 1 state is c2 , the term p4 8m3c2 is treated as perturbation. Then, the energy shift of the ground (cid:28) = p4 8m3c2 (cid:29) = (cid:90) ϕ 0 (cid:19) (cid:18) p4 8m3c2 ϕ0 dx. Substituting the harmonic oscillator ground state wavefunction: ψ0(x) = (cid:17)1/4 (cid:16) mω πh exp (cid:104) mω 2h x2(cid:105) , and computing the expectation value, = 15 (hω)2 mc2 . GPT-4o To compute the relativistic correction to the ground state energy of the harmonic oscillator, we need to account for the relativistic relationship between the kinetic energy and the momentum p. The relativistic kinetic energy is given by: = (cid:112) p2c2 + m2c4 mc2. In the nonrelativistic limit, this can be expanded in powers of = p2 2m p4 8m3c2 + (cid:18) 1 c4 p2 m2c2 using binomial expansion: (cid:19) . 2m , is the usual nonrelativistic kinetic energy, while the second term, p4 The first term, p2 8m3c2 , represents the leading relativistic correction. This correction modifies the Hamiltonian of the system. Step 1: Relativistic Hamiltonian (correct part ommited) Step 2: Ground State of the Harmonic Oscillator (correct part ommited) Step 3: Expectation Value of p4 To compute the first-order correction to the energy, we need the expectation value of p4 in the ground state. Using p4 = (p2)2, we have:"
        },
        {
            "title": "The GPT solution assumes",
            "content": "p4 = (hmω)2. p4 = (p2)2, which is incorrect for quantum harmonic oscillator. Instead, the correct value is: p4 = 3 4 (hmω)2. Step 4: First-Order Energy Correction (rest ommited) Final Answer The relativistic correction to the ground state energy is: Thus, the corrected ground state energy is: = h2ω2 8mc2 . E0 = 1 hω h2ω2 8mc2 . = h2ω2 8mc2 Error Category: Error Assumptions Error Reason: The model demonstrated complete reasoning process but made an error assumption when calculating, leading to incorrect answer D.3 Non-Existent Assumptions Questions: An astronaut places turned-on flashlight in space. The flashlight is spin-stabilized, meaning it rotates about its own axis. The flashlight emits photons, which exert radiation pressure and act as small thrust (photon rocket effect). The batteries last for two hours. We need to determine the additional velocity gained by the flashlight due to photon emission. Solutions: Suppose the flashlight bulb is located at the focus of paraboloid reflector so that almost the entire light output goes out in one direction. If the power of the flashlight is watts and the time interval it is turned on is t, the total energy of the photons emitted is = t. If the orientation of the flashlight does not change, it will gain momentum or an additional speed mv = = , = mc , being the mass of the flashlight, since photon of energy ϵ has momentum ϵ . For example, if = 1 W, = 0.3 kg, = 2 hours, = 1 2 3600 0.3 3 108 = 8 105 m/s. GPT-4o"
        },
        {
            "title": "Incorrect Solution",
            "content": "The force exerted by photon emission is given by: = where: is the power of the flashlight, = 3 108 m/s is the speed of light. Assuming typical flashlight power of = 10 W, we calculate: = 10 3 108 = 3.33 108 Using Newtons second law: Assuming the flashlight has mass of = 1 kg: = = 3.33 108 1 = 3.33 108 m/s2 The time for which the flashlight emits light is: Thus, the velocity change is: = 2 3600 = 7200 = at = (3.33 108) (7200) = 2.4 104 m/s = 0.24 mm/s (1) (2) (3) (4) (5) (6) (7)"
        },
        {
            "title": "Mistake in the Solution",
            "content": "This answer incorrectly assumes that the rotation of the flashlight does not affect its propulsion. However, if the flashlight is spinning, the direction of photon emission changes continuously, meaning the net thrust could average out to zero. The correct approach must account for the rotational dynamics before concluding the velocity gain. D.4 Calculation Errors in Complex Equations Questions: nonrelativistic particle of mass undergoes one-dimensional motion in the potential (z) = g[δ(x a) + δ(x + a)] where > 0 is constant and δ(x) is the Dirac delta function. Find the ground-state energy eigenfunction and obtain an equation which relates the corresponding energy eigenvalue to the constant g. Solutions: Since (x) = (x), the energy eigenfunctions have definite parity. The ground state has even parity, ψ(x) = ψ(x). It is bound state and its energy is negative, < 0. For 0, the Schrödinger equation is (cid:2)(h2/2m) d2/dx2 gδ(x a)(cid:3) ψ(x) = Eψ(x), whose solutions for = are ψ exp(f kx), where = With the condition that the wave function remains finite as and has even parity, we obtain 2mE/h. ψ(x) = (cid:40) Aekx, cosh(kx), 0 a. > a, The continuity of ψ at = requires that = Beka cosh(ka). Thus ψ(x) = (cid:40) Beka cosh(ka)ekx, > a, cosh(kx), 0 a. Normalization (cid:82) 0 ψ2dx + (cid:82) ψ2dx = 1 2 gives = (cid:18) e2ka 2k + 1 + 2ka 2k (cid:19)1/2 At = a, there is discontinuity of the first differential of the wave function (cf Problem 1024): ψ(a+) ψ(a) = (2mg/h2)ψ(a). Substitution of ψ gives which is to be satisfied by k. By symmetry the wave function for the entire space is [1 + tanh(ka)] = 2mg/h2, ψ(x) = (cid:40) Beka cosh(ka)ekx, cosh(kx), > a, a. Llama-3.3-70B"
        },
        {
            "title": "Error in Calculation Example",
            "content": "From the Schrödinger equation, we use the boundary condition at = a: ψ(a+) ψ(a) = 2mg h2 ψ(a). The derivative of the wave function in the two regions is: For > a: Evaluating at = a: For 0 a: Evaluating at = a: ψ(x) = Beka cosh(ka)ekx, ψ(x) = kBeka cosh(ka)ekx. ψ(a+) = kBeka cosh(ka)eka = kB cosh(ka). ψ(x) = cosh(kx), ψ(x) = kB sinh(kx). ψ(a) = kB sinh(ka). Substituting into the boundary condition, kB cosh(ka) kB sinh(ka) = 2mg h2 cosh(ka). Cancelling and rearranging: k(cosh(ka) + sinh(ka)) = 2mg h2 cosh(ka). Error: Incorrect Identity Used (8) (9) (10) (11) (12) (13) (14) (15) (16) cosh(x) + sinh(x) = (Incorrect!) (17) This incorrect step leads to: k(0) = 2mg h2 cosh(ka), (18) Error: Invalid Conclusion This suggests either = 0 (which contradicts the assumption > 0) or cosh(ka) = 0, which is impossible for real ka. D.5 Misunderstanding Question Questions: Consider the static magnetic field given in rectangular coordinates by = B0(xˆx ˆy)/a. (a) Show that this field obeys Maxwells equations in free space. (b) Sketch the field lines and indicate where filamentary currents would be placed to approximate such field. (c) Calculate the magnetic flux per unit length in the ˆz-direction between the origin and the field line whose minimum distance from the origin is R. (d) If an observer is moving with non-relativistic velocity = vˆz at some location (x, y), what electric potential would he measure relative to the origin? (e) If the magnetic field B0(t) is slowly varying in time, what electric field would stationary observer at location (x, y) measure? Solutions: (a) (cid:18) = ˆx + ˆy + ˆz (cid:19) (cid:20) B0 (cid:21) (xˆx ˆy) = = B0 (cid:18) ˆx (ˆx ˆx ˆy ˆy) = 0, + ˆy + ˆz (cid:19) (cid:20) B0 (cid:21) (xˆx ˆy) B0 (b) The magnetic field lines are given by the differential equation (ˆx ˆx ˆy ˆy) = 0. = i.e., or"
        },
        {
            "title": "Hence",
            "content": "dy dx ="
        },
        {
            "title": "By\nBx",
            "content": "= , xdy + ydx = 0, d(xy) = 0. xy = const. The field lines are shown in Fig. 5.17. In order to create such field, four infinitely long straight currents parallel to the direction are symmetrically placed on the four quadrants with flow directions as shown in Fig. 5.17. (rest ommited) InternVL2.5 (a) Verifying Maxwells Equations in Free Space To verify that this field satisfies Maxwells equations, we check Gausss law for magnetism and Ampères law. Step 1: Compute The divergence of magnetic field should always be zero: For our given field, Thus, = Bx + By + Bz . Bx = B0 x, By = B0 y, Bz = 0. = (cid:18) B0 (cid:19) + (cid:18) B0 (cid:19) + (0). Computing the derivatives, = B0 (1) B0 (1) = 0. Since = 0, Gausss law for magnetism is satisfied. Step 2: Compute Ampères law states that in free space (without currents): To compute the curl, Expanding, = 0. = (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) ˆy ˆx x B0 B0 ˆz 0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:18) (0) (B0y/a) (cid:19) ˆx+ (cid:18) (B0x/a) (0) (cid:19) ˆy+ (cid:18) (B0y/a) (B0x/a) (cid:19) ˆz. Since Bz = 0, many terms vanish: = (0 0) ˆx + (0 0) ˆy + (cid:18) B0 (cid:18) B0 (cid:19)(cid:19) ˆz. Thus, = 0. Error: Incorrect conclusion. The correct result is = 2B0 mistakenly claims no currents exist in free space. ˆz, meaning there must be current present. The LLM"
        },
        {
            "title": "E Vision Error",
            "content": "Questions: Switch is thrown to position as shown in Fig. 3.30. (a) Find the magnitude and direction (\"up\" or \"down\" along page) of the currents in R1, R2, and R3, after the switch has been in position for several seconds. Solutions: Let the currents in R1, R2, R3 be i1, i2, i3 respectively. (a) When the switch is thrown to position A, we have instantaneously i1(0) = i2(0) = 2 R1 + R2 = 2 3 + 2 = 0.4 A, i3(0) = 0, i1() = 2 R1 + R2 R3 = 0.59 A. After the switch is in for some time, we have i2() = i3() = R3 R2 + R3 R2 R2 + R3 i1() = 0.12 A, i1() = 0.47 A. As seen from the ends of L1 the resistance in the circuit is and the time constant is = R3 + R1 R2 = 1.7 Ω, τ = L1/R = 5 1.7 = 1 0. s. Using i(t) = i() + [i(0) i()]et/τ , (see Problem 3014), we have i1(t) = 0.59 0.19e0.34t A, the direction is upward, i2(t) = 0.12 + 0.28e0.34t A, the direction is downward, i3(t) = 0.47(1 e0.34t) A, the direction is downward. Figure 5: question image Qwen2-VL-72B Step (a): Steady-State Condition (Switch at Position for Several Seconds) Understanding the Circuit in Steady-State omitted Initial Currents (At = 0) omitted Final Currents (After the Inductor Stabilizes) The equivalent resistance of R2 and R3 is: R2 R3 = 2 0.5 2 + 0.5 = 1 2. = 0.4Ω The total resistance seen by the source: Rtotal = R1 + Req = 3 + 0.4 = 3.4Ω The steady-state current through R1: i1() = 2 3. = 0.59A Using current division: i2() = R3 R2 + R3 i1() = i3() = R2 R2 + R3 i1() = 0.5 2.5 2 2.5 0.59 = 0.12A 0.59 = 0.47A"
        },
        {
            "title": "Time Constant Calculation",
            "content": "The resistance seen from the inductor: Time constant: Transient response: = R3 + (R1 R2) = 0.5 + 1.2 = 1.7Ω τ = L1 = 5 1.7 = 2.94s i1(t) = 0.59 0.19e0.34t (upward) i2(t) = 0.12 + 0.28e0.34t (downward) i3(t) = 0.47(1 e0.34t) (downward) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) Mistake in the solution The image-reading errors stemmed from misinterpreting circuit connections and misreading component values. The LLM incorrectly treated R2 as series with R1 instead of parallel with R3, overestimating total resistance. It also misread R3 as 3Ω instead of 0.5Ω, distorting current calculations. Additionally, the time constant was miscalculated by using R2 + R3 instead of R3 + (R1 R2), leading to incorrect transient responses. These errors affected both steadystate and transient analyses, highlighting the need for careful circuit interpretation and accurate component identification."
        },
        {
            "title": "F Sympy Evaluation Error",
            "content": "Equation 1: Equation2 (Solution) hc2 λ + δ hc λ + δ = = Note: These are algebraically equivalent expressions. However, due to the way the equation is parsed in the code, only the right-hand side is extracted: Extracted expr1: Extracted expr2: This leads to false comparison between the actual expression and single variable E, resulting in an incorrect verification result. Verification Result: SymPy: True (E = E) Cause: The code assumes the right-hand side of = always contains the core expression, which fails when the answer is written on the left."
        }
    ],
    "affiliations": [
        "New York University",
        "Notre Dame University",
        "Yale University"
    ]
}