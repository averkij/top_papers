{
    "paper_title": "Group Relative Attention Guidance for Image Editing",
    "authors": [
        "Xuanpu Zhang",
        "Xuesong Niu",
        "Ruidong Chen",
        "Dan Song",
        "Jianhao Zeng",
        "Penghui Du",
        "Haoxiang Cao",
        "Kai Wu",
        "An-an Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 7 5 6 4 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Group Relative Attention Guidance for Image Editing",
            "content": "Xuanpu Zhang1,2*, Xuesong Niu2, Ruidong Chen1, Dan Song1, Jianhao Zeng1, Penghui Du2, Haoxiang Cao2, Kai Wu2, An-an Liu1 1 Tianjin University 2 Kolors Team, Kuaishou Technology https://little-misfit.github.io/GRAG-Image-Editing/ Figure 1. Variation of editing strength with respect to the relative attention guidance scale. Our approach enables continuous and finegrained control of editing strength, striking user-aligned balance between instruction following and consistency of original image."
        },
        {
            "title": "Abstract",
            "content": "based image editing Recently, on Diffusion-inTransformer (DiT) models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query (Q) and Key (K) tokens share bias vector that is only layer-dependent. We interpret this bias as representing the models inherent editing behavior, while the delta between each token and its corresponding bias encodes *Equal Contribution. This work was conducted during the internship at Kolors Team. Corresponding author. the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance (GRAG), simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing. 1. Introduction Recently, Diffusion Transformer [21] models have once again advanced the field of text-to-image generation [4, 8]. DIT employs multi-modal attention mechanism (MMAttention) [8] as its core to progressively inject semantic information from text into noisy latents, ultimately generating high-quality visual outputs through iterative denoising. Unlike UNet-based models [11, 22, 25, 32, 42] that separate cross-attention and self-attention, the unified attention mechanism of DiTs provides more holistic contextual understanding. This inherent advantage enables it to perform complex image editing even without task-specific fine-tuning [1, 36]. More recently, models such as Kontext [4, 16] and Qwen-Edit [39] further enhance text-driven editing capabilities by continuing training on specialized instruction-editing datasets, demonstrating powerful controllability and generalization. the architecture itself. We hypothesize that the variation of individual tokens from this bias encodes crucial contextual understanding (the theoretical analysis is presented in Section 4). [30] (GRPO) (GRAG), This insight directly motivates our method, Group Relative Attention Guidance guidance mechanism also inspired by the Group Relative strategy. Policy Optimization As illustrated in Figure 3, GRAG first computes the average Key embedding within each token group to determine collective editing direction (the common bias vector). Then, weighting coefficient Î» is used to modulate each tokens vector relative to the bias, enhancing those aligned with the editing intent while suppressing conflicting ones. This process leads to more precise and controllable editing outputs. We validate our method on state-of-theart DIT-based editing models [16, 17, 39]. With fixed guidance scale, our approach achieves better trade-off between the editing responsiveness and image consistency, while continuous coefficient adjustment on fixed samples yields smooth and progressive editing outputs (as shown in Figure 1). Figure 3. Group Relative Attention Guidance. Figure 2. presents the visualization of the embedding features input to the attention layer, where significant bias can be observed across different tokens. However, persistent challenge for these instructionbased models is balancing the trade-off between maintaining fidelity to the source image and responsiveness to the editing instruction. As result, this forces users to rely on external prompt-engineering tools or perform multiple inferences to achieve satisfactory outputs. To address this challenge, we conduct an in-depth investigation into the models internal feature propagation, specifically how textual and visual features are integrated during the editing process. Our analysis reveals that in the MM-Attention, the token distributions of the query and key embeddings tend to cluster around dominant bias vector, as shown in Figure 2. Based on this finding, we demonstrate that by modulating the deviation of each token from this bias, it is able to achieve continuous control over the editing strength, ultimately producing controllable editing outputs. Our investigation begins by analyzing the embedding features within each attention layer [13]. We identify consistent phenomenon: within each layer, feature values concentrate around shared bias vector. Based on the formulation of MM-Attention, this bias phenomenon can be interpreted as an intrinsic inductive pattern introduced by Finally, our contributions can be summarized in three aspects: (a) Through extensive experiments, we identify the presence of bias distribution in the Query and Key embeddings of MM-DIT, and we provide mathematical analysis of its role in image editing tasks. (b) We introduce Group Relative Attention Guidance (GRAG), novel approach that leverages the relative relationships among tokens to modulate the image editing process, enabling precise and controllable editing by modulating their deviations from the group bias. (c) We conduct extensive experiments on multiple baselines, and evaluate performance across diverse editing tasks, demonstrating the effectiveness of our method. 2. Related Work 2.1. Diffusion Transformers Aligning textual and visual representations remains central challenge for transformer-based diffusion models. Early work such as DiT [7, 21, 35] replaced U-Net backbones [22, 25, 26] with transformers and introduced adaptive layer normalization to enable class-conditional generation, but this design limits the ability to achieve denser alignment between textual and visual information. More recent advances, such as MM-DiT [8], address this limitation by introducing unified token space and bidirectional crossmodal attention, allowing text and image tokens to interact within shared sequence. Combined with multiple text encoders like CLIP [23] and T5 [24], this design significantly enhances text understanding and enables more accurate and coherent text-guided generation. Recent studies have begun to leverage the contextual modeling capability of DiT for image editing tasks. Kontext [16] adopts the same model architecture as FLUX [4] and is further trained on instructionbased editing datasets. In contrast, Qwen-Edit[39] replaces the T5 encoder with large vision language model [3, 17] to encode both instructions and reference image information. 2.2. Text-Driven Image Editing Early works such as InstructPix2Pix [5] demonstrated that synthetic instructionresponse pairs can effectively finetune diffusion models for image editing, while training-free methods like Textual Inversion and DreamBooth [9, 28] enabled editing with off-the-shelf generative models [9, 25]. Building on this foundation, subsequent editorsincluding Emu Edit [31], OmniGen [40], HiDream-E1 [6], and ICEdit [43]enhanced instruction-driven editing through refined datasets and architectures, while LoRA-based methods [12] introduced task-specific parameter tuning for diffusion transformers. Proprietary multimodal systems such as GPT-4V [20] and Gemini [34], along with platforms like Midjourney [19] and RunwayML [29], have further integrated these advances into end-to-end creative workflows. Kontext[16] extends the FLUX[4] MM-DiT model for editing tasks, leveraging its strong contextual modeling capability to achieve high consistency with reference images. In contrast, models such as Qwen-Edit[39] enhance instruction comprehension through vision language models, enabling more complex and flexible editing operations. Despite progress, instruction-driven image editing still faces two major challenges: (i) striking balance between editing effectiveness and consistency with the original image, and (ii) achieving precise and continuous control over editing effects. To address these challenges, we investigate the attention-layer representations of DiT and propose Group Relative Attention Guidance (GRAG), which enables precise and controllable editing effects. 3. Preliminaries Multi-Modal Diffusion Transformers. The multi-modal diffusion transformer framework, known as multi-modal diffusion transformers (MM-DiT) [8, 21], merges both textual and visual modalities to generate images that align with the semantics of the textual inputs. FLUX incorporates unified text-image self-attention mechanism, which aligns the multimodal information within each MM-DiT layer. Moreover, FLUX enhances the CLIP [23] text encoder by integrating the T5 [24] encoder, significantly improving its text understanding capabilities. The MM-DiT layer uses combined attention mechanism to fuse textual and visual data. Initially, the text tokens and image tokens are mapped into shared space: Qt = Qi = IW Q, Kt = K, Q, Ki = IW K, Vt = V , Vi = IW , (1) Q, K, V Rdid Rdtd and where represent the projection matrices, and denotes the shared dimension. Subsequently, the joint attention Ajoint is calculated by combining the queries and keys from both the text and image modalities: K, Q, Ajoint = Softmax (cid:18) [Qt Qi][Kt Ki] (cid:19) [Vt Vi] (2) where denotes the token-wise concatenation of the text and image tokens. During the image editing process, the visual information consists of both the editing target and the original image: Qi = [Qe Qs], Ki = [Ke Ks] and Vi = [Ve Vs]. The computation process of the corresponding attention map during editing image token update is as follows: edit = Softmax(Qe[Kt Ki])(i, j) S(i, j) edit e,kj eqi = Ntxt(cid:88) p=1 (cid:124) eqi e,kp + Nimg (cid:88) eqi e,kp + Nimg (cid:88) eqi e,kp (cid:123)(cid:122) Text (cid:125) p=1 (cid:124) (cid:123)(cid:122) Editing (cid:125) p=1 (cid:124) (cid:123)(cid:122) Source (cid:125) , (3) Note: For simplicity, the is omitted. 4. Bias Vector In The Embedding Vectors The attention layer of MM-DiT serves as the key location where editing instructions and conditional image information are fused, with the query and key embeddings directly influencing the proportion of content sampled from each token. Our experiments reveal significant bias in the distribution of embedding features along the sequence dimension, concentrated at fixed positions within each token. We hypothesize that this bias serves as key factor in contextual understanding during the image editing process of DiT. Concentrated distribution of embedding vectors. For each attention layer of the transformer, we extract the query and key embeddings with shape Q, RBSHD. For analysis purposes, we fix the batch size to = 1 and partition the sequence dimension into six semantically meaningful components: Qtext, Qedit, Qsource, and similarly Ktext, Kedit, Ksource. Here, Qtext, Ktext RNtextHD and the remaining components belong to RNimgHD. We apply L2 Figure 4. (Kontext-Layer 2) Aggregating different tokens along the sequence dimension, we visualize the embedding features across the dimension and head axes. The visual features are concentrated at positions corresponding to high RoPE frequencies, while textual features are associated with low frequencies. normalization along the Ntext or Nimg dimension, reducing each component to representation in RHD, where each element Eh,d represents the norm of the corresponding component in head and dimension d. Taking Qedit as an example, Eh,d is computed as: Eh,d = Q:,h,d2 = (cid:118) (cid:117) (cid:117) (cid:116) Nimg (cid:88) s=1 Q2 s,h,d (4) The visualization results of are shown in Figure 4. In the embedding vector space, each dimension index corresponds to component, where the dark red regions in Figure 4 indicate positions with larger magnitudes that contribute more to the inner product between different token embeddings. By examining the relationship between RoPE (Rotary Position Embedding [33]) and dimension indices, we observe that text embeddings concentrate in low-frequency components associated with semantics, while image embeddings concentrate in high-frequency components capturing spatial relations. This finding suggests that the two modalities are not fully aligned in the shared embedding space. Furthermore, we investigate the distribution of token embeddings in the vector space. Figure 5 presents the mean vector magnitudes and standard deviations across different attention heads, further revealing the presence of significant bias vector among tokens in the embedding space. Analysis of the bias vector. The above findings suggest that the query and key embeddings in the attention layer exhibit decomposable structure, where each can be represented as the sum of dominant bias component and an independent variation: qi = qbias + qi, ki = kb + ki (5) We also observe that the feature distributions of the same layer remain highly similar across different time steps and input samples. Based on this phenomenon, we hypothesize that the bias vector qbias, kbias is related to the model Figure 5. (Kontext-Layer 2) Mean vector magnitudes and standard deviations across different attention heads. significant bias vector exists in the embedding space. weights and represents fixed editing action during the image editing process, while the variations of individual tokens relative to this bias vector correspond to the content being edited. Based on Equation 3, we can derive: S(i, j) edit = eqi e,kbias e,kbias eqi Î£t + eqi eqi e,kj e,kbias Î£e + eqi e,kbias Î£s Note: For simplicity, the Î£t = (cid:80)Ntxt p=1 eqi e,kp , (6) , Î£e = Figure 6. An illustration of applying Group Relative Attention Guidance in the MM-DiT image editing model. (a) The MM-Attention map corresponding to the query Qe, where GRAG is applied. (b) The processing of relative modulation to the source images key embeddings. Red denotes enhanced tokens, while blue denotes suppressed tokens. . e,kp e,kp (cid:80)Nimg , Î£s = (cid:80)Nimg p=1 eqi p=1 eqi strong shared bias component in both query and key embedding can dilute the influence of k, thereby reducing the sensitivity of attention scores to specific semantic differences. This insight naturally suggests that by modulating the magnitude of k, one can effectively control the extent to which the conditioning signals (e.g., edit instructions) influence the final output. 5. Group Relative Attention Guidance The variations between individual token embeddings and the bias vector reflect how the editing content relates to the current layers editing action. By modulating their relative relationship, it becomes possible to achieve accurate and continuous control over the editing instructions. Based on this insight, we propose Group Relative Attention Guidance (GRAG). As illustrated in Figure 6, we modify the crossattention component of the MM-Attention corresponding to the query Qe. In Figure 6, Ks is selected as group of tokens, to which group-relative modulation is applied. Algorithm 1 Group-Relative Attention Guidance Input: Embedding Q, K, RBSHD, token index istart, iend, guidance scale Î», Î´. Output: Updated attention ËA. 1: Q, K, = RoP E(Q), RoP E(K), 2: Ks K[:, istart : iend, :, :] 3: Kbias mean(Ks, dim = 1) 4: Ks Kbias 5: K[:, istart : iend, :, :] Î» Kbias + Î´ 6: ËA Attention(Q, K, ) Formally, let ki denote the conditional key embedding corresponding to token i, where = 1, . . . , Nimg. We first compute group-level bias component as the mean of all conditional keys: Kbias = 1 Nimg Nimg (cid:88) j=1 kj (7) The deviation of each token from this bias is then defined as: ki = ki kbias (8) To control the influence of token-level variations, we introduce tunable parameter Î» that scales these deviations: = Î» kbias + Î´ (cid:0)ki Ëki kbias (9) (cid:1) where Ëki relative attention guidance. denotes the updated key embedding under group The scaling factor, Î» and Î´, are introduced to modulate the balance between the shared bias and token-specific variations. Both Î» and Î´ are positive real numbers. Specifically, Î» > 1 enhances the influence of the selected tokens on the final image content, while Î» < 1 reduces their impact. On the other hand, Î´ adjusts the focus intensity towards the selected tokens: Î´ > 1 results in more concentrated and precise editing impact, whereas Î´ < 1 leads to more diffused editing effect. The pseudo-code of Group Relative Attention Guidance is presented in Algorithm 1, which consists of only four lines and can be seamlessly integrated into existing methods. 6. Experiment 6.1. Experiment Setting Implementation Details. We validate our proposed method against six image editing baselines. Kontext [16], Figure 7. Visualization results on training-based image editing method. Figure 8. Visualization results on training-free image editing method. We update the original first-order inversion in StableFlow with second-order ODE inversion method[27, 37], referred to as StableFlow+. Step1X-Edit [17] and Qwen-Edit [39] are training-based image editing method. For reproducibility, the random seed is fixed to 42. All experiments are conducted with batch size of 1 and 24 inference steps. The classifier-free guidance [10] parameter is set following the recommended values for each model, 2.5 for Kontext, 6.0 for Step1X-Edit and 4.0 for Qwen-Edit. Moreover, GRAG is theoretically applicable to regular MM-DiT-based architecture. Therefore, we select three training-free image editing methods based on Flux.1-Dev[16] T2I models (Flowedit [15], Stableflow [1], Stableflow+) to evaluate the generalization ability of our approach. We provide further discussion in Section 7. Evaluation. We evaluate our method on PIE [14]. This benchmark covers diverse range of editing tasks, including object addition/removal, style transfer, and pose modification. For quantitative evaluation, we adopt two complementary perspectives. Following previous works, we adopt LPIPS[41] and SSIM[38] as quantitative metrics to evaluate the content preservation ability in non-edited regions. To assess the alignment between editing results and human preference, we employ the image editing reward model EditScore[18]. EditScore is reward model finetuned on Qwen-2.5VL[2], which measures three aspects: consistency with the original image (Cons), prompt following (PF), and overall edit score (EditScore). 6.2. Qualitative Analysis We apply GRAG to three mainstream MM-DiT-based image editing models, with qualitative results shown in Figure 7. On Step1X-Edit and Qwen-Edit, our method improves consistency between the edited images and the original references while preserving the intended editing effects, yielding more realistic and natural outcomes. Since Step1X-Edit and Qwen-Edit leverage visionlanguage models to encode editing instructions, the additional instruction information often enhances responFigure 9. Visualization results of CFG and GRAG under different scales. Compared to CFG, GRAG more effectively regulates the influence of editing instructions on the original image, demonstrating more accurate and continuous guidance process. siveness but reduces consistency. We select the source image tokens as group and apply GRAG to enhance the response of edit-related tokens to the editing instructions while suppressing the response of irrelevant tokens. For instance, in the first column of Figure 7, GRAG successfully changes the texture of the bird while retaining the details of the tree trunk; in the fifth column, it alters the color of the apple while preserving fine-grained surface details. These examples demonstrate the ability of GRAG to achieve precise and continuous control over edits while maintaining fidelity to the source image. For the original Kontext model, we select the text tokens as the group and apply GRAG to enhance the models response to the editing instructions. As shown on the right side of Figure 7, the baseline fails to respond to the editing instruction, with no change in content, whereas applying GRAG enables successful editing. 6.3. Quantitative Analysis As shown in Tab 1, we perform quantitative evaluations on the PIE dataset. Step1X-Edit and Qwen-Edit exhibit enhanced consistency between the edited outputs and the original images after integrating GRAG, as indicated by improvements in LPIPS, SSIM, and Cons. Although slight decline is observed in PF, the overall EditScore, which reflects overall editing quality, increases. In contrast, Kontext demonstrates noticeable improvement in PF and achieves higher EditScore after applying GRAG. These trends align well with the visual results. Model Kontext-Dev +GRAG Step1X-Edit +GRAG Qwen-Edit +GRAG LPIPS 0.3061 0.3873 0.3228 0.3174 0.3428 0. SSIM Cons 8.9051 0.9213 8.6788 0.8156 8.4714 0.9042 8.6240 0.9137 8.5211 0.8506 8.9440 0.9263 PF 6.9051 7.4177 7.8406 8.0406 8.4806 8.3303 EditScore 6.0887 6.4081 6.8292 7.0045 7.2576 7.3245 Table 1. Quantitative results on Training-Based method. 6.4. Ablation Study Difference with CFG. We compare our approach with the mainstream guidance method, Classifier-Free Guidance (CFG). Unlike CFG, which adjusts the denoising direction during the sampling process, our method directly modulates the editing information within the attention layers. As shown in Table 2 and Figure 10, varying the CFG strength yields few differences. In contrast, GRAG enables precise and continuous control over the editing, producing smooth Figure 10. Comparison of different guidance strategies under varying guidance strengths. The data in the line chart correspond to Table 2. The Î´ parameter yields the most continuous and effective editing guidance. Method CFG = 5.00 CFG = 4.00 CFG = 3.00 CFG = 2.00 CFG = 1.00 Î» = 0.95, Î´ = 1.00 Î» = 1.00, Î´ = 1.00 Î» = 1.05, Î´ = 1.00 Î» = 1.10, Î´ = 1.00 Î» = 1.15, Î´ = 1.00 Î» = 1.00, Î´ = 0.95 Î» = 1.00, Î´ = 1.00 Î» = 1.00, Î´ = 1.05 Î» = 1.00, Î´ = 1.10 Î» = 1.00, Î´ = 1.15 Î» = 0.95, Î´ = 0.95 Î» = 1.00, Î´ = 1.00 Î» = 1.05, Î´ = 1.05 Î» = 1.10, Î´ = 1.10 Î» = 1.15, Î´ = 1.15 LPIPS 0.3381 0.3428 0.3312 0.3297 0.3306 0.3316 0.3428 0.3123 0.3194 0.3307 0.3508 0.3428 0.3188 0.3034 0.2907 0.3454 0.3428 0.3042 0.2971 0.2885 SSIM Cons 8.3989 0.8548 8.5211 0.8506 8.6251 0.8659 8.6669 0.8709 8.7686 0.8734 8.5286 0.8660 8.5211 0.8506 8.5914 0.9156 8.3291 0.9034 8.3720 0.8865 8.2543 0.8344 8.5211 0.8506 8.7549 0.8855 8.9537 0.9206 9.1731 0.9408 8.3560 0.8434 8.5211 0.8506 8.9440 0.9263 9.0234 0.9391 9.1051 0.9448 PF 8.4640 8.4806 8.3954 8.2566 7.6760 8.4886 8.4806 8.3977 7.9251 8.3269 8.4394 8.4806 8.3651 7.9611 6.9291 8.3394 8.4806 8.3303 7.9674 6.6091 EditScore 7.1857 7.2576 7.2761 7.2247 6.8294 7.2725 7.2576 7.2990 7.1992 7.1863 7.1991 7.2576 7.2679 6.9872 6.1730 7.2162 7.2576 7.3245 7.0243 5. Table 2. Ablation analysis of different scales for CFG and GRAG. and consistent adjustments as the editing strength increases, the visual comparsion shown in Figure 9. Such controllability is crucial for customized image editing applications. Effectiveness Analysis of Group Relative. We analyze the influence of the Î» and Î´ parameters in Eq. 9 on the editing results. Three groups of experiments are conducted: only Î», only Î´, and both Î» and Î´ simultaneously. The qualitative results are shown in Figure 9, while the quantitative results on the PIE benchmark are presented in Table 2 and Figure 10. Adjusting Î» alone shows no significant impact on the editing results, corresponding to the fluctuating curves in Figure 10, which indicates that tuning Î» cannot effectively control the editing strength. In contrast, jointly adjusting Î» and Î´ enables certain degree of controllable editing but fails to achieve continuous precision. Moreover, this simultaneous adjustment often degrades visual fidelity, leading to undesirable artifacts such as the distorted flowers in the second column of the bottom-left sample and the visible artifacts in the first column of the bottom-right sample in Figure 9. Adjusting Î´ alone yields the best results, corresponding to the smoothest metric variation in Figure 10 and the most continuous editing transitions in Figure 9. 7. Discussion & Limitation We further examine the applicability of GRAG to editing methods that employ general MM-Attention architectures involving only text and target image tokens. In these methods, GRAG is applied to the attention layers where source image features are injected. As shown in Figure 8, our approach achieves adjustment of the editing results, indicating that GRAG remains effective in general MM-Attention structures. However, its stability in training-free settings is lower than that in training-based models, as evidenced by the quantitative results in Table 3. We attribute this to the fact that GRAG primarily modulates the cross-attention component in MM-Attention (see Figure 6), whereas in untrained T2I models, source image features are introduced through the editedit self-attention branch (Figure 6-b). In such cases, applying GRAG will interfere with existing target image representations. LPIPS Model 0.3758 Flowedit 0.3670 +GRAG 0.3219 StableFlow +GRAG 0.3292 StableFlow+ 0.3691 0.3595 +GRAG SSIM Cons 6.8794 0.8237 7.2223 0.8312 8.9309 0.9185 8.8731 0.9098 7.3599 0.8229 7.7997 0.8316 PF 5.0531 4.8954 2.2177 2.7429 5.3926 4.8395 EditScore 4.6635 4.6697 2.4573 3.0303 5.0970 4.7251 Table 3. Quantitative results on Training-Free method. 8. Conclusion In this work, we revisited the internal attention mechanism of Diffusion-in-Transformer (DiT) models and revealed the presence of shared bias vector that governs editing behavior. Building on this insight, we introduced Group Relative Attention Guidance (GRAG), lightweight yet effective strategy that modulates token deviations from the group bias to achieve fine-grained and continuous control over editing strength. GRAG can be seamlessly integrated into existing DiT-based editors, consistently improving both controllability and fidelity. Our findings provide new insights into the internal dynamics of multi-modal attention and offer practical direction for enhancing controllable image editing in future DiT architectures."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Or Patashnik, Ohad Fried, Egor Nemchinov, Kfir Aberman, Dani Lischinski, and Daniel CohenOr. Stable flow: Vital layers for training-free image editing. arXiv preprint arXiv:2411.14430, 2024. 2, 6 [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. 6 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 3 [4] Black Forest Labs. https : / / huggingface . co / black - forest - labs / FLUX.1-dev, 2024. 2, Flux.1-dev. [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3 [6] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 3 [7] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-Î±: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 2 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 3 [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [10] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6 [11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 3 [13] Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, and Yongfeng Zhang. Massive values in self-attention modules are the key to contextual knowledge understanding. arXiv preprint arXiv:2502.01563, 2025. 2 [14] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Pnp inversion: Boosting diffusion-based editing with 3 lines of code. International Conference on Learning Representations (ICLR), 2024. 6 [15] Vladimir Kulikov, Matan Kleiner, Inbar HubermanSpiegelglas, and Tomer Michaeli. Flowedit: Inversion-free text-based editing using pre-trained flow models. CoRR, abs/2412.08629, 2024. [16] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv preprint arXiv:2506.15742, 2025. 2, 3, 5, 6 [17] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2, 3, 6 [18] Xin Luo, Jiahao Wang, Chenyuan Wu, Shitao Xiao, Xiyan Jiang, Defu Lian, Jiajun Zhang, Dong Li, and Zheng Liu. Editscore: Unlocking online RL for image editing via highfidelity reward modeling. CoRR, abs/2509.23909, 2025. 6 [19] Midjourney. Midjourney. https://www.midjourney. com, 2022. 3 [20] OpenAI. Gpt-4v(ision) system card. https://openai. com/research/gpt-4v-system-card, 2023. 3 [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 2, [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2 [23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 3 [24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 3 [25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Inter- [40] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 1329413304, 2025. 3 [41] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [42] Xuanpu Zhang, Dan Song, Pengxin Zhan, Tianyu Chang, Jianhao Zeng, Qingguo Chen, Weihua Luo, and An-An Liu. Boow-vton: Boosting in-the-wild virtual try-on via In IEEE/CVF Conference mask-free pseudo data training. on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 2639926408. Computer Vision Foundation / IEEE, 2025. 2 [43] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with incontext generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. 3 vention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9, 2015, Proceedings, Part III, pages 234241. Springer, 2015. 2 [27] Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. Semantic image inversion and editing using rectified stochastic differenIn The Thirteenth International Conference tial equations. on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. 6 [28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 3 [29] Runway. Runway. https://runwayml.com, 2023. 3 [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [31] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 3 [32] Jiaming Song, Chenlin Meng, and Stefano Ermon. DenoisIn 9th International Coning diffusion implicit models. ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 2 [33] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [34] Gemini Team et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 3 [35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [36] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. 2 [37] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. CoRR, abs/2411.04746, 2024. [38] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 6 [39] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 2, 3, 6 A. Pytorch Implementation of GRAG The proposed Group Relative Attention Guidance (GRAG) can be seamlessly integrated into existing DiT-based image editing models with only few lines of code modification. Below, we provide an example implementation of GRAG based on typical MM-Attention block from the Diffusers library in PyTorch. Listing 1. Implementation Code of GRAG 1 # Apply RoPE 2 if image_rotary_emb is not None: 3 4 5 6 7 img_freqs, txt_freqs = image_rotary_emb img_query = apply_rotary_emb_qwen(img_query, img_freqs, use_real=False) img_key = apply_rotary_emb_qwen(img_key, img_freqs, use_real=False) txt_query = apply_rotary_emb_qwen(txt_query, txt_freqs, use_real=False) txt_key = apply_rotary_emb_qwen(txt_key, txt_freqs, use_real=False) 8 9 # Apply GRAG scaling 10 11 s_idx, e_idx, bias_scale, delta_scale = 4096, 8192, 1.0, 1.05 12 group_bias = img_key[:, s_idx:e_idx, :, :].mean(dim=1) 13 img_key[:, s_idx:e_idx, :, :] = bias_scale * group_bias + 14 delta_scale * (img_key[:, s_idx:e_idx, :, :] - group_bias) 15 16 # Joint attention computation 17 joint_query = torch.cat([txt_query, img_query], dim=1) 18 joint_key = torch.cat([txt_key, img_key], dim=1) 19 joint_value = torch.cat([txt_value, img_value], dim=1) 20 21 joint_hidden_states = dispatch_attention_fn( joint_query, joint_key, joint_value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False, backend=self._attention_backend, 22 23 24 25 26 28 29 ) B. Additional Feature Visualization B.1. Kontext We provide additional kontext model feature distribution statistics corresponding to Figures 2, 4, and 5 in the main paper. Consistent with the experiments presented in the main text, we analyze different image editing samples (IDs) across various denoising steps and model layers to examine the correlation between feature distributions and these three factors. Figure S1 presents direct visualizations of the feature distributions, where the TokenNumber dimension is downsampled by factor of 4 and the Dim dimension by factor of 2. Figure S2 shows aggregating different tokens along the sequence dimension. Figures S3S6 illustrate the mean and variance of token-wise feature distributions across different attention heads, corresponding to the different embedding features. Figure S1. Additional visualizations of text and image embedding features. Features within the same layer share similar distributions, indicating limited correlation with model inputs or denoising steps. Please zoom in to view finer details. Figure S2. Additional visualizations of aggregating different tokens along the sequence dimension. Please zoom in to view finer details. Figure S3. Additional visualizations of Query-edit embedding mean vector magnitudes and standard deviations across different attention heads. Please zoom in to view finer details. Figure S4. Additional visualizations of Key-text embedding mean vector magnitudes and standard deviations across different attention heads. Please zoom in to view finer details. Figure S5. Additional visualizations of Key-edit embedding mean vector magnitudes and standard deviations across different attention heads. Please zoom in to view finer details. Figure S6. Additional visualizations of Key-src embedding mean vector magnitudes and standard deviations across different attention heads. Please zoom in to view finer details. C. The Use of Large Language Models In this work, we use Gemini-2.5-pro to aid in the writing process. Specifically, the model was used to improve the grammatical structure, refine sentence phrasing, and enhance the overall readability of the text. The core scientific content, methodologies, and conclusions presented in this paper are the original work of the authors. The use of the LLM was restricted to tool for language enhancement."
        }
    ],
    "affiliations": [
        "Kolors Team, Kuaishou Technology",
        "Tianjin University"
    ]
}