{
    "paper_title": "From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images",
    "authors": [
        "Yiming Chen",
        "Junlin Han",
        "Tianyi Bai",
        "Shengbang Tong",
        "Filippos Kokkinos",
        "Philip Torr"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) are adept at answering what is in an image-identifying objects and describing scenes-they often lack the ability to understand how an image feels to a human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, a comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals a significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that a post-training phase can effectively bridge this gap, significantly enhancing the model's alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides a benchmark to measure this human-like perception, a post-training pipeline to enhance it, and a demonstration that this alignment unlocks more human-centric AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 0 8 2 2 . 1 1 5 2 : r From Pixels to Feelings: Aligning MLLMs with Human Cognitive Perception of Images Yiming Chen1, Junlin Han1, Tianyi Bai2, Shengbang Tong4, Filippos Kokkinos3, Philip Torr1 1Oxford University, 2HKUST, 3University College London, 4New York University * Equal contribution. Project page: https://follen-cry.github.io/MLLM-Cognition-project-page/"
        },
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) are adept at answering what is in an imageidentifying objects and describing scenesthey often lack the ability to understand how an image feels to human observer. This gap is most evident when considering subjective cognitive properties, such as what makes an image memorable, funny, aesthetically pleasing, or emotionally evocative. To systematically address this challenge, we introduce CogIP-Bench, comprehensive benchmark for evaluating MLLMs on such image cognitive properties. Our evaluation reveals significant gap: current models are poorly aligned with human perception of these nuanced properties. We then demonstrate that post-training phase can effectively bridge this gap, significantly enhancing the models alignment with human judgments. Furthermore, we show that this learned cognitive alignment is not merely predictive but also transferable to downstream creative tasks. By integrating our cognitively-aligned MLLM into an image generation pipeline, we can guide the synthesis process to produce images that better embody desired traits, such as being more memorable or visually appealing. Our work provides benchmark to measure this human-like perception, post-training pipeline to enhance it, and demonstration that this alignment unlocks more human-centric AI. 1. Introduction Advances in MLLMs, such as GPT [46] and Gemini [53], have led to significant breakthroughs in visual understanding. Their success in tasks like visual question answering and detailed image captioning demonstrates strong ability to perform objective recognition and description [1, 25, 74]. Yet, despite this progress, crucial dimension of visual intelligence remains largely unexplored: the perception of subjective, human-centered cognitive properties [15, 22, 57]. These propertiesencompassing aesthetics, humor, emotion, and memorabilityare fundamental to the human visual experience but pose challenge for current models, which are primarily trained on factual, descriptive data. Previous research has explored individual cognitive image properties, developing models to predict the memorability of an image [18, 22, 29], quantify its aesthetic appeal [30, 35, 49], or even assess its potential for humor [21, 23, 34]. Although valuable, these efforts often focus on specialized, vision-only models or statistical analysis, leaving significant gap: there is no systematic framework to evaluate and enhance these comprehensive cognitive abilities within powerful, general-purpose MLLMs. It remains unclear how well MLLMs align with human judgments on aesthetics, funniness, emotional valence, and memorability; whether they can be taught to acquire this sophisticated understanding; and what the downstream effects would be once they are more aligned with humans. To bridge this gap, we introduce the CogIP-Bench, the first comprehensive benchmark designed to systematically measure the cognitive alignment between MLLMs and humans in four key dimensions. Our extensive evaluation of leading MLLMs in CogIP-Bench reveals significant discrepancy, confirming that these models, despite their advanced capabilities, are poorly aligned with human perception of these subjective traits. For example, all MLLMs, including open-source and API-based models, showed nearly 0 correlation with humans on the memorability level of images, while the average Spearman correlation in other cognitive dimensions remains below 0.5.  (Table 1)  Beyond merely identifying this problem and benchmarking, we demonstrate practical solution. We show that targeted post-training pipeline with training data in 1 Figure 1. We present CogIP-Bench, comprehensive cognition benchmark that evaluates the alignment of cognition score prediction between MLLM and humans. Left: example datapoints for each dimension: aesthetics, funniness, emotion and memorability. Middle: post-training results of three popular MLLMs across different dimensions. Right: results of swapping the MLLM backbone, comparing the effect of cognition-related image generation with the Qwen-Image pipeline. CogIP-Bench can effectively instill this cognitive knowledge into an MLLM, significantly improving its ability to predict human cognitive scores. One more compelling finding is that this newly acquired cognitive alignment is not just passive predictive skill, but transferable capability. By integrating our fine-tuned MLLM as the MLLM backbone in state-of-the-art image generation pipeline (Qwen-Image) [61], we show that it can guide the synthesis process to create images that are intentionally more memorable, aesthetically pleasing, or emotionally resonant. Our main contributions can be summarized as follows: Benchmark: We introduce CogIP-Bench, novel benchmark to evaluate MLLMs on four fundamental cognitive image properties: aesthetics, funniness, emotional valence, and memorability. Alignment: We present post-training pipeline and data that significantly improves an MLLMs alignment with human cognitive judgments, effectively teaching it more human-like sense of these properties. Transferability: We provide the first demonstration that enhancing cognitive alignment in an MLLM can directly translate into more controllable and humancentric outcomes in downstream generative tasks, such as text-to-image synthesis. 2. Related Work 2.1. MLLMs and Their Evaluations Multimodal large language models (MLLM), [8, 9, 32, 36, 37, 46, 47, 53, 54, 59, 67, 68, 71, 72] combine visual encoders with pretrained language models, allowing them to reason jointly over text and images. These systems often use visual encoders such as CLIP [48], along with adapter mechanismssuch as MLPs, querybased transformers, or attention layersto bridge the two modalities. They demonstrate promising application values in broad fields [4, 10, 11, 55, 58, 66, 70, 73] such as CAD drawing, robotics agentic system. MLLM Benchmarks. With the rapid advancement of MLLM, all-round benchmarks have been proposed to assess diverse capabilities of MLLMs [5, 13, 14, 28, 31, 33, 38, 40, 42, 69]. They tested wide range of capabilities that require both visual and textual modalities as input, ranging from text-visual grounding, perception, optical character recognition and compositional reasoning. However, there lacks benchmark that assess the cognitive ability and the degree of alignment, in subjective properties like aesthetics, funniness, emotional valence and memorability, between MLLMs and human. Hence, our CogIP-Bench provides pipeline for assessing how similar MLLMs perceive an image with humans from cognitive perspective. 2.2. Human Cognition in AI Research on the cognitive abilities of AI models has explored both the alignment of decision-making between humans and models [6, 17, 64] and the interpretation of mechanisms of neural network models from neuroscientific and psychological perspectives [26, 51, 52]. Before LLMs, some studies also demonstrated the abilities of AI models in cognitive properties of images; for example, 2 Figure 2. Examples of the CogIP-Bench, for each cognition dimension, we show two images along with their cognition scores and the interpretation of that cognitive dimension. research [18, 22] focus on the memorability of images trying many different AI models, from linear classifiers to ViTs in order to quantify and understand it. GANalyze [15] use Generative Adversarial Networks (GANs) to study the aesthetics, memorability, and emotional valence of images. However, most existing studies primarily focus on text-only large language models [6, 64], vision-only models [52], or simpler recurrent neural networks [26]. Recent work has begun to assess the visual cognition abilities of MLLMs [7], though such analysis are largely behavioralcentering on spatial reasoning and causal inference tasks. In contrast, the cognition abilities examined by our CogIP-Bench emphasize the subjective experience elicited by visual stimuli, assessing the alignment between MLLMs and human in terms of aesthetics, funniness, emotion and memorability properties of images. While prior studies have examined individual cognitive properties, some of which adopt statistical approaches [24], others concentrating on vision-only models [18, 23, 27, 34], and still others evaluating the cognitive abilities of MLLMs [35, 43], these efforts typically address only single dimension of cognition rather than providing comprehensive perspective. Our work is the first systematic attempt to construct benchmark that focuses on comprehensive subjective cognition (covering 4 dimensions) on images, offering new insights into MLLM cognitive evaluation. 3. CogIP-Bench Our benchmark is designed to test the cognition alignment of MLLM with human, in terms of score prediction on image. So each data-point contains(1) an image for perceiving, (2) prompt to provide specific cognition context and instruct the model to predict cognition score and (3) human preference score as ground truth. 3.1. Benchmark Construction Our benchmark contains 4 dimensions: Aesthetics, Funniness, Emotional Valence and Memorability. The collection pipeline is listed as follows: Aesthetics: In the context of images, aesthetics capture the perceptual and affective qualities that evoke visual appeal, harmony, and artistic value in human observers. For this dimension, we draw image data from the LaMem dataset [29], while ground-truth aesthetic scores are provided by the LAION-Aesthetic-PredictorV1 model [30]. This predictor employs CLIP vision encoder with lightweight regression head, trained to estimate aesthetic quality and output continuous scores. Its training is based on annotated ratings collected from photo-community website, ensuring alignment with human judgments of visual appeal [44]. Funniness: Humor is widely accepted by incongruity theory, which suggests that it arises from unexpected subversions of contextual expectations [3]. However, we are doing an unimodal analysis (only assess image without caption) which is inherently stateless process, that is, no presummed expectation of the context, hence we call this cognition dimension funniness. We use the HumorDB dataset [23], which is carefully curated dataset designed to evaluate and advance visual humor understanding by AI systems. It comprises diverse range of images from photos, cartoons, sketches, and AI-generated content all annotated with humorous score, which is collected by crowdsourcing in three annotation tasks: binary classification, range rating and pairwise comparison. Emotional Valence: We use the FindingEmo dataset [43], which is dataset focusing on emotional under3 Model Aesthetics Funniness Emotional Memorability Specific Model 0 0 1 3. 1.470 0.581 4.919 1.782 0.585 7. 1.650 0.504 MSE MAE Spearman MSE MAE Spearman MSE MAE Spearman MSE MAE Spearman 4.491 1.672 Qwen2-VL-7B-Instruct 5.680 1.981 Qwen2.5-VL-7B-Instruct Qwen3-VL-8B-Instruct 6.892 2.141 4.349 1.719 Gemma3-12B-it (Base) 2.202 1.119 llava-v1.6-mistral-7b-hf 4.112 1.681 llava-v1.6-vicuna-7b-hf 2.896 1.347 llava-v1.6-vicuna-13b-hf llava-onevision-qwen2-7b-ov-hf 5.861 1.983 4.885 1.823 Llama-3.2-11B-Vision-Instruct GPT-4o GPT-5 Claude-Haiku-4.5 Gemini-2.5-Pro 4.073 1.597 4.896 1.792 4.884 1.806 6.061 1. Open-source Models 2.442 9.534 2.151 8.073 2.284 9.012 1.646 4.716 2.400 9.872 10.772 2.524 2.331 9.389 10.311 2.555 1.924 6.284 0.440 0.485 0.513 0.651 0.257 -0.147 0.053 0.344 0.404 2.240 9.289 2.255 9.433 2.681 11.756 2.295 9.156 2.127 7.071 2.237 9.375 2.024 6.817 11.619 2.683 12.159 2.796 API-based Models 8.112 3.262 9.982 5. 2.151 1.342 2.403 1.819 0.565 0.731 0.431 0.686 1.922 6.667 6.205 1.902 11.999 2.704 1.926 6.471 0.507 0.533 0.457 0.478 0.595 0.346 0.510 0.436 0.427 0.485 0.459 0.490 0.201 0.505 0.537 0.545 0.588 0.380 0.468 0.467 0.558 0. 0.658 0.657 0.503 0.726 2.250 9.100 -0.019 0.096 2.350 10.200 0.033 2.430 10.200 0.040 8.000 2.140 0.056 28.900 4.700 0.013 2.180 6.500 2.680 11.600 0.017 2.340 0.013 10.200 2.550 9.700 0.135 2.234 7.630 8.500 2.128 8.120 2.177 11.300 2.588 0.057 0.115 0.115 -0.036 Table 1. Test results of various open and closed source MLLMs on our CogIP-Bench. Each column pair shows MSE and MAE for each cognitive subtask (Aesthetics, Funniness, Emotional, Memorability). The dimension-specific models are vision-only models trained to predict the cognition scores of the image for each dimension, serve as reference. Green entry color indicates the best performance under each metric column (separately for open-scource and API-based models), and red indicates the worst. standing of MLLM and containing diverse image-text pairs annotated with human judgements of emotional valence and intensity via online crowdsourcing, where emotional valence is bi-directional metric that emphasizes on the positive or negative quality of an emotion rather than the arousal level of it. It aims to test models ability to perceive, interpret, and align visual cues with emotional semantics. Memorability: People tend to memorize images with central objects, faces or salient actions better than natural landscapes which suggests that there are certain visual intrinsics or pattern that make some images more memorable than others [18, 29] despite personal experiences. To investigate this property and alignment with human perception in MLLM, we used the LaMem dataset along with their memorability scores provided [29] as the source dataset for the memorability dimension to construct our benchmark. The LaMem dataset is largescale memorability dataset that contains 60,000 images from diverse sources and the scores are derived from reliable and large-scale human experiments with carefully designed visual memory game. For each cognition dimension except emotional valence, when sampling images, we define 5 even score ranges, i.e., (0, 2), (2, 4), (4, 6), (6, 8), (8, 10) where the raw scales of aesthetics and funniness are 0-10 but memorability is 0-1 (we scale it to 1-10 accordingly). However, due to the bi-directional nature of emotional valence, its raw score ranges from -3 to 3 hence we split it into 3 buckets: (-3, -1), (-1, 1) and (1, 2) and later scale it to 1-10 when computing the MSE and MAE of test results. We then enforce the same number of datapoints sampled in each bucket to ensure the balance of data in terms of human preferences score. 3.2. Benchmark Details For each cognition property, we collect 800 datapoints for training and 120 for test. Each datapoint is composed of an image, cognition-specific prompt and the ground truth answer which includes the numeric score. The prompt is carefully curated to help the MLLM understand the meaning of each particular cognitive property. More details are provided in the supplementary material. Some examples are shown in Figure 2. 4. Benchmarking Results We evaluate the effectiveness of our CogIP-Bench in aligning subjective cognition on images between humans and MLLMs. In Section 4.1, we outlined our experimental settings. Section 4.2 presents the results from multiple models on our CogIP-Bench and Section 4.3 introduces our post-training pipeline for injecting cognitive image knowledge into MLLMs. In Section 4.4, we show the effect of post-training and the impact on other benchmarks. 4 4.1. Experimental Settings We test open-source as well as API-based MLLMs on our benchmark to evaluate their alignment with human cognition. These open-source models include: Qwen2-VL-7B [59], Qwen2.5-VL-7B [47], Qwen3-VL8B [67], Gemma3-12B-it [53], llava-v1.6-mistral-7b-hf, llava-v1.6-vicuna-7b-hf, llava-v1.6-vicuna-13b-hf [37], llava-onevision-qwen2-7b-ov-hf [32], and Llama-3.211B-Vision-Instruct [16]. The API-based models are: GPT-4o [46], GPT-5 [45], Claude-Haiku-4.5 [2] and Gemini-2.5-Pro [53]."
        },
        {
            "title": "We also report the results of specific models for each",
            "content": "cognition dimension to provide comparisons. Aesthetics: We used LAION aesthetic predictor [30] to get the score, which is the ground truth predictor by definition. Funniness: We trained regressor model with CLIP vision encoder and linear projector as the output layer. We used the images and ground-truth score of our own CogIP-Bench for training. Emotional Valence: same as Funniness. Memorability: We used the off-the-shelf HumanMem predictor model [18]. 4.2. Benchmark Results on MLLMs We tested the cognition alignment capability of MLLMs using our own CogIP-Bench  (Table 1)  . We observed that all models fail to capture the memorability of images with near-zero Spearman correlation with human preferences, which indicates that memorability is the most challenging and abstract cognitive property of images among the four cognitiive dimensions. For open-source models, the alignment with human cognition appears stronger in aesthetics and emotional valence, whereas the funniness dimension exhibits greater variability in alignment scores. In contrast, API-based models demonstrate relatively weaker alignment on aesthetics, but achieve significantly better alignment on funniness and emotional valence, outperforming open-source counterparts in these dimensions. Moreover, we find that MLLMs with better performance on other benchmark, such as the Qwen series and Gemma3, also perform relatively well on our CogIPBench. For the API-based models, GPT-5 yields better performance than others. This suggests that there is positive correlation between these subjective human cognitive properties and the general vision-language capabilities. Since numbers are tokenized into discrete tokens in the same manner as words, the model lacks an explicit understanding of the ordinal or distance relationships between them. To address this limitation, we explore two complementary approaches: First, the prompts are designed to instruct the model to predict categorical label as classification task. For each cognition dimension except emotional valence, the labels are: [very low, low, medium, high, very high], while for emotional valence the labels are: [negative, neutral, positive]. Based on the provided label-to-score mapping rule, the model predicts corresponding numeric score. This two-step formulation enables the model to first provide qualitative judgment, leveraging its strength in natural language reasoning, and subsequently produce quantitative score grounded in that classification. Next, we experimented with the soft-label loss [60]. As mentioned above, the standard cross-entropy loss (LCE) uses one-hot encoding, which means that is Dirac distribution (q(i) = 1 only when = t), where is the index of the target token (1), is the vocabulary size and is the probability distribution of the vocabulary (p RV such that (cid:80)V i=1 pi = 1. ). Hence, the numerical distance information between digit tokens is lost when computing the loss. For example, the numbers 1 and 9 would incur the same loss when the target token is 2, even though 1 is much closer to 2. LCE(p, q(t)) = (cid:88) i=1 qi(t) ( log pi), (1) To address this, we switch to soft-label distribution (qSL) for numerical tokens. Instead of Dirac function, we construct (only for numeric target tokens) as shown in (2), where δ is the old Dirac distribution and η is mixture coefficient balancing the degree of softness of the new loss function. This encourages the model to output digit token that are closer to the target value in continuous manner. We use ψ(t) as triangular function, as suggested by the original paper [60]. qSL(t) = (1 η) δ(t) + η ψ(t) (2) We use LoRA training instead of full fine-tuning to prevent overfitting. More explorations on using reinforcement learning (RL) for post-training are provided in the discussions (Section 6.2). 4.3. Post-training Method 4.4. Post-training Results We employ standard supervised fine-tuning (SFT) to inject cognitive knowledge into the model. key challenge in this numerical score prediction task lies in the inherent insensitivity of language models to numerical values. In this section, we analyze the effect of SFT as reflected in our CogIP-Bench results, comparing the model performance before and after SFT for three models: Qwen2.5VL-7B, Gemma3-12B-it, and Llama-3.2-11B, as shown 5 Model Aesthetics Funniness Emotional Memorability MSE MAE Spearman MSE MAE Spearman MSE MAE Spearman MSE MAE Spearman Specific Model 0 1 3.420 1.470 0.581 4.919 1.782 0.585 7.800 1. 0.504 5.669 1.985 Qwen2.5-VL-7B (base) 3.841 1.526 Qwen2.5-VL-7B (SFT) 4.349 1.719 Gemma3-12B-it (Base) 2.614 1.353 Gemma3-12B-it (SFT) Llama-3.2-11B-Vision-Instruct (Base) 5.980 1.968 5.783 1.882 Llama-3.2-11B-Vision-Instruct (SFT) 0.400 0.450 0.478 0.473 0.449 0.387 9.376 2.387 7.022 2.037 4.716 1.646 3.986 1.566 8.922 2.286 7.121 2.095 0.486 0.509 0.651 0.654 0.298 0.327 6.378 1.875 5.775 1.750 9.156 2.295 8.208 2.208 9.760 2.489 12.167 2. 0.635 0.644 0.588 0.573 0.531 0.495 13.200 2.700 15.500 3.010 8.000 2.140 7.500 2.260 0.086 0.012 0.040 0.165 47.680 3.570 0.036 15.410 3.096 0.146 Table 2. Test results on CogIP-Bench. Each column pair shows MSE and MAE (after normalization) for cognitive subtask (Aesthetics, Funniness, Emotional, Memorability). The blue (better) color indicates the performance improvement of alignment after SFT of each MLLM for each cognitive dimension metric. in Table 2. We then examine how this alignment influences other aspects of each models capability by evaluating them across diverse set of benchmarks. Following the common task categorization used in Cambrian-1, Web-SSL, and LSBS [12, 19, 54], we group the tasks into four categories: Vision-Centric, OCR, General, and Knowledge (Table S1), each comprising three benchmarks [13, 28, 31, 3842, 56, 63, 69, 75]. More details are provided in the supplementary material. From Table 2, we observe that aside from memorabilitywhere all models exhibit difficulty in predictionthere is consistent alignment improvement across nearly all cognitive dimensions and metrics. This suggests that SFT effectively incorporates human subjective preferences in image perception into MLLMs, thereby enhancing their alignment with human judgments. Regarding performance on other benchmarks (Table S1), we find that Qwen2.5-VL-7B and Llama-3.2-11B maintain comparable performance with acceptable variance, while Gemma3-12B-it demonstrates substantial improvements across all categories. These results further indicate positive correlation between subjective cognitive alignment and overall model capability. Model Vision-Centric OCR General Knowledge Qwen2.5-VL-7B-Base Qwen2.5-VL-7B-SFT Gemma3-12B-it-Base Gemma3-12B-it-SFT Llama-3.2-11B-VI-Base Llama-3.2-11B-VI-SFT 0.740 0.738 0.674 0.684 0.686 0.685 0.898 0.896 0.599 0.651 0.790 0. 0.818 0.820 0.735 0.745 0.703 0.700 0.702 0.703 0.633 0.656 0.577 0. Table 3. Benchmark comparison across multiple models with per-section averages (Vision-Centric, OCR, General, Knowledge). 5. Image Generation Results In this section, we demonstrate how cognition-aligned models differ from their base counterparts through an image generation pipeline using consistent random seed for fair comparison. Specifically, we employ QwenImage [61], recently released image generation framework that adopts Qwen2.5-VL-7B-Instruct as its MLLM backbone for semantic encoding. Since this model also serves as the backbone for one of our SFT variants, we can seamlessly integrate it to compare image generations before and after fine-tuning. Uncurated visual examples are shown in Figure 3, highlighting the differences produced by the two LLM backbones. 5.1. Prompt Preparation We split the prompts for image generation into five sets: Aesthetics, Funniness, Emotional, Memorability and General. For each set, we employed ChatGPT-5 to generate 100 text-to-image (T2I) prompts that demonstrate the properties of that type. 5.2. Image Metrics Results For the General-Prompting images, we use some general human preferences and the text-image alignment metrics to analyze the impact of the cognition alignment by SFT. We used ClipScore [20], Human Preference Score (HPS-v2) [62], LAION score [30], and ImageReward score [65]. This is illustrated in Table 4. Among these, ImageReward is human-trained reward model designed to assess prompt-to-image consistency and aesthetic preference. As shown in Table 4, all evaluation metrics improve after replacing the MLLM backbone with its SFT-aligned counterpart. Notably, ImageReward shows substantial gain of +22.8%, indicating that cognition alignment significantly enhances generation quality and strengthens alignment with human visual and aesthetic preferences. For the Cognition-Prompting image generation, we use cognition-specific models to evaluate the cognition scores of each image set  (Table 5)  . Models specifications can be found in 4.1, these specific models outperform most MLLMs across all cognitive dimensions  (Table 1)  . 6 Figure 3. Qualitative comparison of images generated by the Qwen-Image pipeline using different LLM backbones (with the same prompt). The figure shows the effect of pretraining versus supervised fine-tuning (SFT) on image cognition properties. For each image pair, Left: Base model; right: SFT model. Generation prompts are shown under each image pair. We can see that images generated with our SFT MLLM backbone better demonstrate the cognitive cues embedded in the prompts. We observe consistent improvements in cognition scores predicted by the post-trained models across all cognitive dimensions, confirming the effectiveness of post-training in enhancing MLLMs alignment with human cognition on visual inputs. Notably, the emotional cognition dimension exhibits the most significant gain (+19%), suggesting that Qwen-Image has stronger ability to perceive and interpret emotional cues present in the prompts. MLLM Backbone ClipScore HPS PickScore LAION ImageReward Qwen2.5-VL-7B (base) Qwen2.5-VL-7B (ours) 0.330 0.332 0.267 0.277 21.973 22. 6.765 6.796 0.878 1.078 5.3. User Study Here, we present user study evaluating how our cognition alignment pipeline improves human preference towards images generated by the Qwen-Image pipeline. We sampled 30 image pairs for each cognitive dimension, one generated by the baseline MLLM backbone and the other by our aligned version. Five volunteers were then asked to select their preferred image or choose Hard to Tell if undecided. As shown in Figure 4, the images from our aligned model consistently achieved higher preference rates. On average, our model was preferred 1.7 times more frequently than the baseline. Table 4. Image scores (General) for Qwen-Imagegenerated images with different MLLM backbones across various imagequality and human preference metrics. 6. Discussions 6.1. Ablation Study MLLM Backbone Aesthetics Funniness Emotional Memorability Qwen2.5-VL-7B (base) Qwen2.5-VL-7B (ours) 6.364 6.462 3.627 3. 1.420 1.690 7.760 7.770 Image scores (Cognition) predicted by specific Table 5. cognition-predicting models for Qwen-Imagegenerated images with different MLLM backbones. To examine the contribution of each component in our alignment strategy, we conduct an ablation study in four settings: (1) fine-tuning without the soft-label loss, (2) using simplified prompting scheme that omits the describe-then-predict instruction format and directly asks the model to predict the score, (3) fine-tuning only the visual encoder while freezing the language model (LLM) and (4) fine-tuning only the LLM with the visual encoder frozen. We conduct this ablation study using 7 Model Aesthetics Funniness Emotional Memorability Regression SFT Simple-Prompt No Soft-Label 1.526 5.570 3.922 2.037 8.612 6.383 1.750 6.678 5.956 3.010 13.200 14. 0.01 % 0.38 % 0.03 % Table 7. Ablation results (MSE only) demonstrating the effect of our prompting techniques and soft-label loss on our baseline alignment method using Qwen2.5-VL-7B. 6.2. Cognition-Alignment with RL Can RL methods further align MLLMs with human cognition? To investigate this, we apply Group Relative Policy Optimization (GRPO) [50] on our benchmark, fine-tuning the MLLM with reward signal based on the numerical proximity between the models predicted score and the ground-truth (GT). Unlike supervised finetuning, which relies solely on static label supervision, GRPO introduces an adaptive learning signal that encourages models to iteratively refine their predictions toward human-like evaluations. Although, in our benchmark, the GT responses do not include explicit reasoning traces, the reward design implicitly instills numerical alignment awareness into the learning process, encouraging more human-consistent judgment calibration. Empirically, this reinforcement-based method yields improved alignment on aesthetics and emotional cognition dimensions compared to standard SFT (as shown in Table 8), though it exhibits some degradation in broader multimodal reasoning tasks (with the same benchmark setup as in Section 6.1). These results highlight the promise of policy optimization techniques for capturing human cognitive patterns in multimodal alignment. Model Aesthetics Funniness Emotional Memorability Regression SFT GRPO 3.841 1.760 7.022 7.922 5.775 4.514 15.500 16.600 0.01 % 3.38 % Table 8. MSE results of the GRPO fine-tuning using the Qwen2.5-VL-7B model for each dimension (lower is better). 7. Conclusion Our work demonstrates that the subjective, cognitive dimension of visual understanding is no longer beyond the reach of MLLMs. By systematically measuring and then bridging the gap between machine and human perception, we have shown that these models can be taught semblance of human-like taste and intuition. Furthermore, this learned alignment is not only an end in itself but transferable capability that unlocks more intuitive and human-centric creative applicationsa step towards more deeply human-aligned AI. We hope this work serves as step toward creating AI systems that can not only see but also feel the world as human do. Figure 4. Preference percentages of images generated by QwenImage using the baseline MLLM backbone and our fine-tuned version. Qwen2.5-VL-7B. We also include the average regression percentages of these fine-tuned models across four benchmarks: MMVP [56], OCRBench [39], MME [13] and MMMU [69] (each representing one task category [54]). Comparing the freeze-vision and freeze-LLM configurations  (Table 6)  , we find that the vision encoder plays pivotal role in image cognition tasks. The freeze-vision setting yields the weakest alignment across all cognitive dimensions, whereas the freeze-LLM setting achieves slightly better performance, particularly on the aesthetics and funniness dimensions. This observation contrasts with common fine-tuning practices in downstream multimodal tasks, where freezing the visual encoder is often preferred for stability and efficiency. Our findings suggest that the visual subsystem of MLLMs contributes more substantially to human-aligned image cognition. However, the freeze-LLM SFT setting leads to significant decline in general multimodal reasoning tasks, indicating that balanced optimization between the visual and language components remains crucial for maintaining overall model capability. Model Aesthetics Funniness Emotional Memorability Regression SFT Freeze-Vision Freeze-LLM 3.841 5.923 2.511 7.022 7.488 5.031 5.775 7.634 6.427 15.500 32.290 23.440 0.01 % 0.74 % 5.37 % Table 6. Ablation results (MSE only) across four cognitive dimensions, comparing three frozen-encoder configurations of Qwen2.5-VL-7B. The SFT setting is our baseline alignment method. Regression represents the average performance degradation percentage on other multimodal reasoning benchmarks. For the ablation of prompting techniques and softlabel loss  (Table 7)  , we observe that incorporating the soft-label loss generally enhances alignment across all cognitive dimensions, demonstrating its effectiveness in numeric prediction tasks. Finally, the proposed prompting strategy yields the most substantial gains, improving cognitive alignment notably across all dimensions."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning, 2022. 1 [2] Anthropic. Claude sonnet 4.5 system card. Technical report, Anthropic, 2025. Accessed: 2025-11-08. 5 [3] Salvatore Attardo. Theories of humor and their levels. In The Linguistics of Humor: An Introduction, pages 57. Oxford University Press, 2020. 3 [4] Hao Bai, Yifei Zhou, Mert Cemri, Jiayi Pan, Alane Suhr, Sergey Levine, and Aviral Kumar. DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning, 2024. arXiv:2406.11896 [cs]. 2 [5] Tianyi Bai, Yuxuan Fan, Jiantao Qiu, Fupeng Sun, Jiayi Song, Junlin Han, Zichen Liu, Conghui He, Wentao Zhang, and Binhang Yuan. Hallucination at glance: Controlled visual edits and fine-grained multimodal learning. arXiv preprint arXiv:2506.07227, 2025. 2 [6] Marcel Binz, Elif Akata, Matthias Bethge, Franziska Brandle, Fred Callaway, Julian Coda-Forno, Peter Dayan, Can Demircan, Maria K. Eckstein, Noemi Elteto, Thomas L. Griffiths, Susanne Haridi, Akshay K. Jagadish, Li Ji-An, Alexander Kipnis, Sreejan Kumar, Tobias Ludwig, Marvin Mathony, Marcelo Mattar, Alireza Modirshanechi, Surabhi S. Nath, Joshua C. Peterson, Milena Rmus, Evan M. Russek, Tankred Saanum, Johannes A. Schubert, Luca M. Schulze Buschoff, Nishad Singhi, Xin Sui, Mirko Thalmann, Fabian J. Theis, Vuong Truong, Vishaal Udandarao, Konstantinos Voudouris, Robert Wilson, Kristin Witte, Shuchen Wu, Dirk U. Wulff, Huadong Xiong, and Eric Schulz. foundation model to predict and capture human cognition. Nature, pages 18, 2025. Publisher: Nature Publishing Group. 2, 3 [7] Luca M. Schulze Buschoff, Elif Akata, Matthias Bethge, and Eric Schulz. Visual cognition in multimodal large language models, 2024. arXiv:2311.16093 [cs]. 3 [8] Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Lorenzo Baraldi, Marcella Cornia, and Rita Cucchiara. The revolution of multimodal large language models: survey, 2024. [9] Shuo Chen, Zhen Han, Bailan He, Jianzhe Liu, Mark Buckley, Yao Qin, Philip Torr, Volker Tresp, and Jindong Gu. Can multimodal large language models truly perform multimodal in-context learning?, 2024. 2 [10] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. SFT Memorizes, RL Generalizes: Comparative Study of Foundation Model Post-training, 2025. arXiv:2501.17161 [cs]. 2 [11] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023. 2 [12] David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, and Saining Xie. Scaling language-free visual representation learning. arXiv preprint arXiv:2504.01017, 2025. 6 [13] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 2, 6, 8 [14] Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, Zhang Li, Guozhi Tang, Bin Shan, Chunhui Lin, Qi Liu, Binghong Wu, Hao Feng, Hao Liu, Can Huang, Jingqun Tang, Wei Chen, Lianwen Jin, Yuliang Liu, and Xiang Bai. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning, 2024. [15] Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual definitions of cognitive image properties, 2019. 1, 3 [16] Aaron Grattafiori and et al. The llama 3 herd of models, 2024. 5 [17] George Gui and Olivier Toubia. The Challenge of Using LLMs to Simulate Human Behavior: Causal Inference Perspective. SSRN Electronic Journal, 2023. arXiv:2312.15524 [cs]. 2 [18] Junlin Han, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, and Ian Reid. What Images are More Memorable to Machines?, 2023. arXiv:2211.07625 [cs]. 1, 3, 4, 5 [19] Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, and Filippos Kokkinos. Learning to see before seeing: Demystifying llm visual priors from language pre-training. arXiv preprint arXiv:2509.26625, 2025. [20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. 6 [21] Jack Hessel, Ana Marasovic, Jena Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, and Yejin Choi. Do androids laugh at electric sheep? humor understanding benchmarks from the new yorker caption contest. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 688714, 2023. 1 [22] Phillip Isola, Jianxiong Xiao, Devi Parikh, Antonio Torralba, and Aude Oliva. What Makes Photograph MemIEEE Transactions on Pattern Analysis and orable? Machine Intelligence, 36(7):14691482, 2014. Confer9 ence Name: IEEE Transactions on Pattern Analysis and Machine Intelligence. 1, 3 [23] Vedaant Jain, Felipe dos Santos Alves Feitosa, and Gabriel Kreiman. Humordb: Can ai understand graphical humor?, 2025. 1, [24] Hyeongnam Jang, Yeejin Lee, and Jong-Seok Lee. Modeling, quantifying, and predicting subjectivity of image aesthetics, 2022. 3 [25] P. Jayachandran, R. Rajendran, et al. Deep learning-based visual question answering: Survey. AIP Conference Proceedings, 3264(1):030011, 2024. 1 [26] Li Ji-An, Marcus K. Benna, and Marcelo G. Mattar. Discovering cognitive strategies with tiny recurrent neural networks. Nature, pages 19, 2025. Publisher: Nature Publishing Group. 2, 3 [27] Jie Jing, Qing Lin, Shuangpeng Han, Lucia Schiatti, YenLing Kuo, and Mengmi Zhang. Unforgettable Lessons from Forgettable Images: Intra-Class Memorability Matters in Computer Vision Tasks, 2025. arXiv:2412.20761 [cs]. 3 [28] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. 2, 6 [29] Aditya Khosla, Akhil S. Raju, Antonio Torralba, and Aude Oliva. Understanding and predicting image memorability at large scale. In International Conference on Computer Vision (ICCV), 2015. 1, 3, [30] LAION-AI. aesthetic-predictor: linear estimator on top of clip to predict the aesthetic quality of pictures. GitHub repository, https://github.com/ LAIONAI/aestheticpredictor, 2024. MIT License; accessed 23 Oct 2025. 1, 3, 5, 6 [31] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 2, 6 [32] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy Visual Task Transfer, 2024. arXiv:2408.03326 [cs]. 2, 5 [33] Jian Li, Weiheng Lu, Hao Fei, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wankou Yang, Yabiao Wang, and Chengjie Wang. survey on benchmarks of multimodal large language models, 2024. 2 [34] Runjia Li, Shuyang Sun, Mohamed Elhoseiny, and Philip Torr. OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?, 2023. arXiv:2307.11636 [cs]. 1, [35] Boyang Liu, Yifan Hu, Senjie Jin, Shihan Dou, Gonglei Shi, Jie Shao, Tao Gui, and Xuanjing Huang. Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization, 2025. arXiv:2509.21871 [cs] version: 1. 1, 3 [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning, 2023. arXiv:2304.08485 [cs]. 2 [37] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning, 2024. arXiv:2310.03744 [cs]. 2, 5 [38] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multimodal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. 2, 6 [39] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12), 2024. 8 [40] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. [41] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 22632279, Dublin, Ireland, 2022. Association for Computational Linguistics. [42] Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV), pages 21992208, 2021. 2, 6 [43] Laurent Mertens, Elahe Yargholi, Hans Op de Beeck, Jan Van den Stock, and Joost Vennekens. Findingemo: An image dataset for emotion recognition in the wild, 2024. 3 [44] Naila Murray, Luca Marchesotti, and Florent Perronnin. AVA: Large-Scale Database for Aesthetic Visual Analysis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2408 2415. IEEE, 2012. 3 [45] OpenAI. Gpt-5 system card. Technical report, OpenAI, 2025. Accessed: 2025-11-08. [46] OpenAI, Aaron Hurst, Adam Lerer, and et al Goucher. GPT-4o System Card, 2024. arXiv:2410.21276 [cs]. 1, 2, 5 [47] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. 2, 5 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, 10 Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 2 [49] Jian Ren, Xiaohui Shen, Zhe Lin, Radomir Mech, and David Foran. Personalized image aesthetics. In Proceedings of the IEEE international conference on computer vision, pages 638647, 2017. 1 [50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [51] Prajwal Singh, Pankaj Pandey, Krishna Miyapuram, and Shanmuganathan Raman. EEG2IMAGE: Image Reconstruction from EEG Brain Signals, 2023. arXiv:2302.10121 [cs]. 2 [52] Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang, and Xiaorong Gao. Decoding Natural Images from EEG for Object Recognition. 2023. 2, 3 [53] Gemini Team and et al Anil. Gemini: Family of Highly Capable Multimodal Models, 2025. arXiv:2312.11805 [cs]. 1, 2, 5 [54] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Ziteng Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: Fully Open, Vision-Centric Exploration of Multimodal LLMs, 2024. arXiv:2406.16860 [cs]. 2, 6, 8 [55] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. MetaMorph: Multimodal Understanding and Generation via Instruction Tuning, 2024. arXiv:2412.14164 [cs]. 2 [56] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms, 2024. 6, [57] Giuseppe Valenzise, Chen Kang, and Frederic Dufaux. Advances and Challenges in Computational Image Aesthetics, pages 133181. Springer International Publishing, Cham, 2022. 1 [58] Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, and Shenghua Gao. MLLM-Tool: Multimodal Large Language Model For Tool Agent Learning, 2025. arXiv:2401.10727 [cs]. 2 [59] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 2, 5 [60] Pei Wang, Zhaowei Cai, Hao Yang, Davide Modolo, and Ashwin Swaminathan. Enhancing numerical prediction of mllms with soft labeling. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) 2025, 2025. 5 [61] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 2, [62] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 6 [63] xAI. Realworldqa: benchmark for real-world visual question answering. https://huggingface.co/ datasets/xaiorg/RealworldQA, 2024. Accessed: 2025-10-28. 6 [64] Chengxing Xie, Canyu Chen, Feiran Jia, Ziyu Ye, Shiyang Lai, Kai Shu, Jindong Gu, Adel Bibi, Ziniu Hu, David Jurgens, James Evans, Philip Torr, Bernard Ghanem, and Guohao Li. Can Large Language Model Agents Simulate Human Trust Behavior?, 2024. arXiv:2402.04559 [cs]. 2, 3 [65] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: learning and evaluating human preferences for textto-image generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 1590315935, 2023. 6 [66] Jingwei Xu, Chenyu Wang, Zibo Zhao, Wen Liu, Yi Ma, and Shenghua Gao. CAD-MLLM: Unifying Multimodality-Conditioned CAD Generation With MLLM, 2025. arXiv:2411.04954 [cs]. 2 [67] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. 2, [68] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), 2024. 2 [69] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multi11 modal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. 2, 6, 8 [70] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning, 2024. arXiv:2405.10292 [cs]. 2 [71] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recent advances in multimodal large language models, 2024. 2 [72] Zhi Zhang, Srishti Yadav, Fengze Han, and Ekaterina Shutova. Cross-modal information flow in multimodal large language models, 2025. 2 [73] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model, 2024. arXiv:2408.11039 [cs]. 2 [74] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and VQA. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1304113049, 2020. [75] Nannan Zhu, Yonghao Dong, Teng Wang, Xueqian Li, Shengjun Deng, Yijia Wang, Zheng Hong, Tiantian Geng, Guo Niu, Hanyan Huang, Xiongfei Yao, and Shuaiwei Jiao. Cvbench: Evaluating cross-video synergies for complex multimodal understanding and reasoning, 2025."
        },
        {
            "title": "Appendices",
            "content": "A. Detailes of Results on Other Benchmarks This section presents the detailed performance of the three MLLMs on 12 benchmarks spanning four task categories, evaluated both before and after SFT on our CogIP-Bench: Model Vision-Centric OCR General Knowledge MMVP RealWorldQA CVBench Avg OCRBench DocVQA ChartQA Avg MME MMBench SeedBench Avg MMMU MathVista AI2D Avg Qwen2.5-Base Qwen2.5B-SFT Gemma3-Base Gemma3-SFT Llama-3.2-Base Llama-3.2-SFT 0.780 0. 0.723 0.720 0.717 0.707 0.686 0.693 0.607 0.617 0.630 0.648 0.755 0. 0.692 0.716 0.710 0.701 0.740 0.738 0.674 0.684 0.686 0.685 0.884 0. 0.702 0.754 0.755 0.741 0.949 0.951 0.722 0.808 0.825 0.845 86.2 0. 0.372 0.391 - - 0.898 0.849 0.896 0.847 0.599 0.765 0.651 0.776 0.790 0.705 0.793 0.696 0.833 0. 0.749 0.739 0.673 0.672 0.772 0.771 0.692 0.719 0.731 0.732 0.818 0. 0.735 0.745 703 0.700 0.577 0.581 0.557 0.564 0.473 0.494 0.681 0. 0.551 0.580 0.482 0.482 0.847 0.702 0.847 0.703 0.790 0.633 0.823 0.656 0.776 0.577 0.778 0.585 Table S1. Benchmark comparison across multiple models with section averages (Vision-Centric, OCR, General, Knowledge). Green entry color indicates an improvement on the average benchmark result on certain task category of particular MLLM, while the red entry color indicates regression. B. Training Details In this section, we report all hyperparameters used in our experimentsincluding the soft-label parametersfor training the three model variants, as summarized in Table S2. Hyperparameter Qwen2.5 Gemma3 Llama3.2 LoRA Rank LoRA α LoRA Dropout LoRA Target GPU Batch Size Gradient Accumulation Steps Warmup Ratio Learning Rate Learning Rate Scheduler Unfreeze Vision Tower 64 64 0.05 all 4 A40 4 8 0.03 3e-5 Cosine True 64 64 0.05 all 4 A40 4 8 0.03 1e-5 Cosine True 64 64 0.05 all 4 A40 4 8 0.03 9e-5 Cosine True η λ Soft Label 0.15 2.0 0.15 2. 0.08 2.0 Table S2. Hyperparameters specification for training Qwen2.5-VL-7B, Gemma3-12B-it, and Llama-3.2-11B-VI models. C. Example fo User Study This section presents example image pairs used in the user study assessing preferences for images generated by Qwen-Image with either the default or SFT-aligned MLLM backbones. For each cognitive dimension, 30 image pairs were randomly sampled and arranged side-by-side, after which five volunteers were asked to select the preferred imagewithout knowing which model produced itor choose Hard to Tell when undecided. 13 (a) Aesthetics (b) Funniness (c) Emotional Valence (d) Memorability Figure S1. Demonstration of the user study set up, each pair is generated with the same prompt using Qwen-Image using consistent seed. 14 D. Examples of SFT Dataset In this section, we provide example samples from our constructed CogIP-Bench dataset used for supervised fine-tuning, as shown in Figures S2 and S3. The dataset spans four cognitive dimensions. Figure S2. Examples of CogIP-Bench samples for Aesthetics and Funniness Figure S3. Examples of CogIP-bench samples of Emotional Valence and Memorability. 15 E. Prompts In this section, we systematically present all prompts used to construct the CogIP-Bench dataset and to evaluate the SFT effects on Qwen-Image. Specifically: Figure S4 illustrates the prompt used to rewrite the original simple instruction prompts for the datasetshown here using aesthetics as an exampleincluding the reformulation of the from human instruction. Figure S5 presents the prompt employed to rewrite the from gpt ground-truth responses. Figure S6 provides the prompts used to generate text-to-image instructions designed to elicit specific cognitive attributes, along with general-purpose version for comparison. Figure S4. Prompts for rewriting from human instruction in the dataset. 16 Figure S5. Prompts for rewriting from gpt GT in the dataset. Figure S6. Prompt to generate T2I generation prompts for Qwen-Image"
        }
    ],
    "affiliations": [
        "HKUST",
        "New York University",
        "Oxford University",
        "University College London"
    ]
}