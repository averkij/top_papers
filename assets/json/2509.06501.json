{
    "paper_title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
    "authors": [
        "Junteng Liu",
        "Yunji Li",
        "Chi Zhang",
        "Jingyang Li",
        "Aili Chen",
        "Ke Ji",
        "Weiyu Cheng",
        "Zijia Wu",
        "Chengyu Du",
        "Qidi Xu",
        "Jiayuan Song",
        "Zhengmao Zhu",
        "Wenhu Chen",
        "Pengyu Zhao",
        "Junxian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents."
        },
        {
            "title": "Start",
            "content": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Yunji Li2* Junteng Liu1* Chengyu Du2 Qidi Xu2 Chi Zhang2 Jiayuan Song2 1The Hong Kong University of Science and Technology Jingyang Li2 Zhengmao Zhu2 Wenhu Chen3 Aili Chen 2MiniMax Ke Ji2 Weiyu Cheng2 Pengyu Zhao2 3University of Waterloo Zijia Wu2 Junxian He1 jliugi@cse.ust.hk junxianh@cse.ust.hk https://github.com/hkust-nlp/WebExplorer The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as practical path toward long-horizon web agents. 5 2 0 2 8 ] . [ 1 1 0 5 6 0 . 9 0 5 2 : r Figure 1 Performance comparison on BrowseComp-en, BrowseComp-zh and HLE benchmarks across different models. 1. Introduction The rapid advancement of Large Language Models (LLMs) has spurred paradigm shift toward agentic intelligence, where models are equipped with diverse tools to tackle increasingly complex *Equal contribution WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents problems. Web browsing agents (OpenAI, 2025; Perplexity AI, 2025) represent critical component of this shift, enabling autonomous information retrieval from the vast landscape of online sources. Despite significant progress, existing open-source models such as WebThinker (Li et al., 2025b) and WebSailor (Li et al., 2025a) exhibit limited performance on challenging search-based benchmarks like BrowseComp-en/zh (Wei et al., 2025; Zhou et al., 2025), BrowseComp-Plus (Chen et al., 2025) and Xbench (Xbench-Team, 2025), while stronger commercial models remain opaque in their training details (OpenAI, 2025; Zhipu AI, 2025). We hypothesize that the fundamental challenge in developing capable web agents lies in the quality of training data. Current benchmarks have evolved to include queries of extraordinary difficulty For example, over half of BrowseComp-en queries fail to be solved by human annotators. While such challenging queries are rare in typical use cases, constructing high-quality, difficult query-answer pairs is essential for developing agents that can achieve super-human performance on informationseeking tasks. Previous approaches to web navigation data construction have primarily employed two methodologies: Graph-based methods (Li et al., 2025a; Tao et al., 2025) construct explicit graphs through random walks where each node represents website or entity and each edge represents relationship, then utilize subgraphs to formulate QA pairs. Evolution-based approaches (Gao et al., 2025; Wu et al., 2025a) iteratively modify existing queries to increase their complexity. However, these approaches face limitations. Previous graph construction introduces complexity in node expansion and selection heuristics. Meanwhile, evolution-based methods usually increase the complexity by injecting new information to replace original content at each evolution step, potentially resulting in unnatural query formulations and limited flexibility in synthesis. Our framework WebExplorer addresses these limitations through novel approach that leverages model-based exploration to construct the information space and iterative long-to-short query evolution to reduce salient information. This enables more natural and flexible synthesis of challenging web navigation tasks. Rather than constructing explicit graphs through rule-based methods, which is complex process requiring decisions about expansion strategies and node selection, we adopt simpler model-based approach to explore the information space. Starting from seed entity, we leverage powerful LLMs to simulate the graph-building process internally through iterative search and browsing actions. This approach enables flexible, dynamic exploration of information spaces related to the seed entity without the overhead of explicit graph construction. The model then utilizes this explored information space to construct initial query-answer pairs. In our preliminary experiments, however, we observed that these initially constructed QA pairs, while requiring multiple websites to solve, still remained relatively straightforward evidenced by the high success rates achieved by strong proprietary models. To address this limitation, we introduce second-stage evolution process. Specifically, unlike previous work where evolution involves injecting new information (Gao et al., 2025; Wu et al., 2025a), we prompt models to systematically increase query difficulty by removing explicit clues and introducing strategic obfuscation. In contrast to the initial QA pairs with clear search entry points, the evolved QA pairs require longer solution processes with more exploratory search attempts. This systematic evolution generates challenging queries that lead to significantly lower success rates and require more reasoning steps from proprietary models. Through this process, we construct our WebExplorer-QA data. Our training utilizes typical approach combining supervised fine-tuning for cold-start initialization, followed by reinforcement learning using the GRPO algorithm (Shao et al., 2024). Our RL training scales to 128K context length and 100 maximum number of tool calling turns, where we observe consistent increases in both the number of tool calls and benchmark performance. Based on Qwen3-8B (Yang et al., 2025), our WebExplorer-8B achieves state-of-the-art performance at its scale on multiple information-seeking benchmarks, including BrowseComp-en/zh (Wei et al., 2025; 2 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Figure 2 Model-Based Exploration and Iterative Query Evolution Framework. Starting from seed entity (e.g., David Hackett Souter), the framework employs iterative search and browsing actions to construct the information space related to the seed entity. Initial queries (ğ‘„0) and Answers are generated based on this explored information space. Through iterative evolution, salient information is systematically obfuscated (e.g., Remove Birth..., Replace ... or Vague Date...) to produce more challenging queries (ğ‘„1 to ğ‘„ğ‘›). This process ensures the resulting queries require longer reasoning steps and explorations. Zhou et al., 2025), GAIA (Mialon et al., 2024), WebWalkerQA (Wu et al., 2025b), Frames (Krishna et al., 2024), and XBench-DeepSearch (Xbench-Team, 2025). Notably, our model achieves 15.7% on BrowseComp-en and 32.0% on BrowseComp-zh, significantly outperforming the previous leading WebSailor 72B model despite much smaller size. It also achieves 62.7% on WebWalkerQA and 75.7% on FRAMES, establishing the best performance among models up to 100B parameters. Beyond superior performance on information-seeking tasks, WebExplorer-8B demonstrates remarkable generalization to the academic benchmark HLE (Phan et al., 2025), scoring 17.3% and outperforming previous 32B models like WebThinker-32B (Li et al., 2025b). This validates the strong generalization capability of our approach beyond pure information-seeking tasks. 2. WebExplorer: Synthesizing Challenging QA Pairs Contemporary information-seeking benchmarks present great challenges, exemplified by BrowseCompen (Wei et al., 2025) where more than half of the queries remain unsolvable by human annotators despite two hours of dedicated effort. These challenging benchmarks rely predominantly on manual curation (Mialon et al., 2024; Wei et al., 2025; Zhou et al., 2025), resulting in prohibitively high annotation costs and severely limited dataset sizes that preclude their use in large-scale training. Concurrently, existing open-source training data exhibit some limitations: they either lack sufficient scale to enable large-scale model training or present overly simplistic queries that fail to match the difficulty of evaluation benchmarks (Li et al., 2025a; Tao et al., 2025; Wu et al., 2025a). Consequently, the autonomous synthesis of large-scale, challenging search-oriented query-answer pairs emerges as pivotal research challenge in developing advanced web agents. Several works have explored synthesizing challenging information-seeking QA pairs through two predominant methodological paradigms. Graph-based approaches explicitly construct knowledge WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents graphs where each node represents website or entity, then generate QA pairs by selecting subgraphs from the complete graph structure. Typically, larger subgraphs correspond to more challenging queries (Li et al., 2025a; Tao et al., 2025). Evolution-based methods iteratively modify existing queries by injecting new information to systematically increase their complexity (Gao et al., 2025; Wu et al., 2025a), typically evolving queries to become progressively longer. However, the previous rule-based graph construction introduces complexity of node expansion and selection heuristics. Instead, our method WebExplorer utilizes existing LLMs to navigate the web and construct the information space autonomously. Additionally, traditional evolution approaches that inject new information at every step can become unnaturally complex. Rather than following short-to-long evolution, our iterative query evolution operates in long-to-short manner by strategically removing salient information, creating natural obfuscation. As illustrated in Figure 2, through these two stages, WebExplorer proposes simple and flexible framework for challenging QA pair generation. 2.1. Preliminaries Our agent framework builds upon the ReAct paradigm (Yao et al., 2023), where agents execute iterative cycles of reasoning and acting. When presented with query, the agent conducts multiple rounds of Thought-Action-Observation sequences. During each round, the language model formulates Thought ğœğ‘¡ based on the current context, executes structured Action ğ›¼ğ‘¡, and receives corresponding environmental feedback as Observation ğ‘œğ‘¡. The final round contains only Thought ğœğ‘¡ without further actions and observations. Our action space encompasses two primary tools: ğ›¼ğ‘¡ {search, browse}. The search action enables querying search engines to retrieve relevant information, while the browse action facilitates direct webpage access for content extraction. We formalize each action as ğ›¼ğ‘¡ = (ğ›¼ğ‘š ğ‘¡ ), where ğ‘¡ contains the necessary parameters. For search operations, ğ›¼ğ‘ ğ‘¡ specifies the action type and ğ›¼ğ‘ ğ›¼ğ‘š ğ‘¡ encompasses list of query strings, whereas browse operations require target URLs and extraction objectives within ğ›¼ğ‘ ğ‘¡ . More details about these tools can be found in 4.1. ğ‘¡ , ğ›¼ğ‘ The complete execution trajectory spanning ğ‘‡ iterations is represented as: ğ»ğ‘‡ = (ğœ0, ğ›¼0, ğ‘œ0, ğœ1, ğ›¼1, ğ‘œ1, . . . , ğœğ‘‡ ) (1) At each timestep ğ‘¡, the agent produces thought ğœğ‘¡ and selects action ğ›¼ğ‘¡ according to policy ğœ‹(ğœğ‘¡, ğ›¼ğ‘¡ ğ»ğ‘¡1), conditioning on the accumulated trajectory history ğ»ğ‘¡1. We provide detailed trajectory example in Appendix A. 2.2. Model-Based Exploration Traditional graph-based methods (Li et al., 2025a; Tao et al., 2025) require heuristic rules of explicit graph construction, typically initiated from root URL or seed entity, followed by systematic expansion through predefined traversal strategies. The resulting graph contains structured information where usually nodes represent entities and edges form the relationships. Such approaches involve iteratively identifying related entities, extracting their features, and expanding the graph until reaching predetermined size constraints. This process introduces some complexity, requiring careful, heuristic design of expansion strategies and node selection heuristics. In contrast, we propose WebExplorer, which employs different model-based exploration approach by leveraging powerful LLMs to construct the information space autonomously. Our method operates purely through prompting: we provide seed entity as the initial search entry along with three example QA pairs, then instruct the model to conduct iterative search and browsing actions to 4 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Figure 3 Illustration of model-based exploration and initial Query-Answer pair synthesis. Starting from the seed Brazil National Team, the model iteratively explores using (Search) and (Browse) actions to discover interconnected facts, then synthesizes challenging queryanswer pair that requires deep reasoning across multiple discovered connections. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents explore the information space before synthesizing QA pairs. Specifically, given an entity as seed, the models conduct iterative search and browsing actions to construct comprehensive information space encompassing entity-related content. This approach simulates the graph-building process internally, enabling flexible and dynamic exploration without the complexity of graph expansion strategy design and explicit graph maintenance. Subsequently, utilizing the explored information space, models generate QA pairs that necessitate reasoning across multiple websites to reach the correct solution. As shown in Figure 3, starting from the seed Brazil National Team, the model iteratively conducts search and browse actions to discover various interconnected facts. By focusing on one specific eventthe 1976 FA Cupthe model explores diverse information including details about players and referees, ultimately constructing Query-Answer pair that requires deep reasoning across multiple discovered connections. Formally, starting from seed entity ğ‘’0, the exploration process conducts multiple rounds of actions and observations: ğ»ğ‘‡ = (ğ‘’0, ğœ0, ğ›¼0, ğ‘œ0, ğœ1, ğ›¼1, ğ‘œ1, . . . , ğœğ‘‡ ) (2) where ğ» represents the complete information space encompassing all explored content, and ğœğ‘‡ contains the synthesized QA pair. The model autonomously determines when to stop searching to formulate challenging QA pair, eliminating the need for predefined stopping criteria and allowing the model to explore flexibly. 2.3. Iterative Query Evolution Example of BrowseComp-en Query: Please identify the fictional character who occasionally breaks the fourth wall with the audience, has backstory involving help from selfless ascetics, is known for his humor, and had TV show that aired between the 1960s and 1980s with fewer than 50 episodes. Answer: Plastic Man While the initial QA pairs constructed from model-explored information spaces successfully incorporate content from multiple websites, we observe that proprietary models can still solve them with relatively high accuracy. For instance, as shown in Table 1, Claude-4-Sonnet achieves 86.6% accuracy on the initial QA pairs compared to 12.2% on BrowseComp-en and 68.3% on GAIA. This indicates that despite requiring multi-website reasoning, these initial QA pairs remain insufficiently challenging. Through systematic case analysis, we identify that the initial queries contain excessive explicit information and salient clues that substantially reduce their difficulty. For example, the initial query-answer example shown below demonstrates several salient clues such as the official attendance set record and this player died at the age of 44 that serve as clear and strong indicators. Such specific informationincluding dates, locations, and proper namesoften provides direct entry points that enable straightforward solution trajectories without requiring exploratory detours or consideration of alternative reasoning paths. In contrast, examination of challenging information-seeking benchmarks like BrowseComp-en reveals critical distinguishing characteristic: these queries deliberately avoid providing clear, specific clues, instead employing vague descriptions. As the example above shows, the BrowseComp-en query TV show that aired between the 1960s and 1980s with fewer than 50 episodes demonstrates this obfuscation. Therefore, these challenging queries usually do not contain clear search entry points, which necessitate extensive exploration and multiple reasoning attempts before reaching the correct 6 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents answer. Drawing inspiration from BrowseComps design principles, we implement an iterative query evolution process to systematically increase QA pair difficulty. Unlike previous evolution methods that follow short-to-long approach (Gao et al., 2025; Wu et al., 2025a), our evolution primarily reduces excessive information from initial QA pairs to increase difficulty. Given the complete initial QA construction information, we explicitly prompt models to refine queries through three strategic directions: (1) removing salient information, (2) introducing strategic obfuscation to specific details such as dates, locations, and proper names, and (3) identifying alternative descriptions to replace original explicit references. Through multiple iterative evolution cycles, we generate substantially more challenging QA pairs that require extensive exploration and demonstrate significantly reduced success rates. As demonstrated in the example below, the evolved query eliminates some salient descriptions in purple color like this player died at the age of 44 and obscures concrete entities by replacing explicit references such as Manchester United with vague descriptors like First Division giant. Example of Evolved Query Initial Query: football match took place in stadium where the official attendance set record that still stands today for FIFA World Cup matches. The referee of this match was the oldest person to ever officiate World Cup final, and exactly 26 years after this match, he was the chairman of club that defeated Manchester United in an FA Cup final. The player who scored the winning goal in that FA Cup final was born in an area that became part of its current city in 1920, and this player died at the age of 44. In what minute of the FA Cup final was the winning goal scored? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Evolved Query: In the unique FIFA World Cup tournament format that concluded without knockout final, match official later guided Second Division club to victory over First Division giant in the monarchs final attendance at such an occasion. The match-winner had been rejected by the club he supported as child, hailing from district that joined centuriesold Royal Naval stronghold two decades into the 20th century. In which minute did this decisive strike occur? Answer: 83rd minute Answer: 83rd minute Formally, starting with the initial query ğ‘„0 extracted from the previous information space ğ» (0) , we iteratively evolve queries through multiple rounds. For ğ‘˜ = 0, . . . , ğ¾ 1, the (ğ‘˜+1)-th information space ğ» (ğ‘˜+1) is obtained by appending the ğ‘˜-th rounds augmentation to the entire history: ğ» (ğ‘˜+1) = ( ğ» ğ‘˜, ğœ(ğ‘˜) 1 , ğ›¼(ğ‘˜) 1 , ğ‘œ(ğ‘˜) , . . . , ğœ(ğ‘˜) ğ‘šğ‘˜ ), ğ‘˜ = 0, . . . , ğ¾ 1 (3) where each evolved query ğ‘„ğ‘˜+1 is contained in ğœ(ğ‘˜) ğ‘šğ‘˜ (ğ›¼(ğ‘˜) 1 ğ‘˜-th evolution step to further augment the information space. and maintains the same answer ğ´. The sequence ğ‘šğ‘˜ 1) represents the multiple action-observation cycles conducted during the , . . . , ğ›¼(ğ‘˜) , ğ‘œ(ğ‘˜) 1 , ğ‘œ(ğ‘˜) ğ‘šğ‘˜ 1 2.4. The WebExplorer-QA Dataset To ensure diversity in our synthesized QA pairs, we collect seed entities from Wikipedia and incorporate three QA pairs from BrowseComp-en as exemplars in our synthesis prompt. After generating initial QA pairs using our model-based exploration, we apply our iterative query evolution methodology for 5 iterations to synthesize the WebExplorer-QA dataset. These evolved final QA pairs, approximately 40K in total, are subsequently used for supervised fine-tuning and can be used directly 7 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Dataset Initial QA Evolved QA WebDancer SailorFog WebShaper ASearcher Average Turns Accuracy (%) 7.9 86. 9.9 67.1 5.4 62.0 8.2 35.0 8.4 67.4 6.5 62.0 Table 1 Comparison of average tool calling turns and accuracy (%) of Claude-4-Sonnet across web navigation datasets, reporting our Initial QA, Evolved QA, and other QA datasets (WebDancer, SailorFog, WebShaper, ASearcher). (a) Initial QA vs Evolved QA (b) Evolved QA vs BrowseComp-en Figure 4 Tool calling turns distribution comparisons using OpenAI-o3: Initial QA vs Evolved QA (left) and Evolved QA vs BrowseComp-en (right). for reinforcement learning. To validate the quality of WebExplorer-QA and demonstrate the effectiveness of our iterative evolution process, we conduct comprehensive comparative analysis across multiple web navigation datasets. Our evaluation encompasses our initial QA pairs, the final evolved QA pairs, and established datasets from prior work, including WebDancer (Wu et al., 2025a), SailorFog (Li et al., 2025a), WebShaper (Tao et al., 2025), and ASearcher (Gao et al., 2025). Using Claude-4-Sonnet as our evaluation model, we assess both accuracy and the average number of tool calling turns required to solve each QA pair. Additionally, we employ more powerful model, OpenAI-o3, to analyze the tool calling turns distribution between our Initial QA and Evolved QA, as well as between Evolved QA and the challenging benchmark BrowseComp-en. The results presented in Table 1 demonstrate the critical importance of our iterative evolution process for generating challenging QA pairs. The evolution methodology significantly transforms the initial queries, with accuracy dropping substantially from 86.6% to 67.1%, while the average number of solution turns increases notably from 7.9 to 9.9. This substantial shift indicates that our evolution process successfully creates more complex queries requiring extensive multi-step reasoning and exploration. Furthermore, when compared to existing datasets from other works, our evolved WebExplorer-QA achieves the highest average turn count, demonstrating the superior complexity and quality of our synthesized data. The detailed tool calling distribution shown in Figure 4 (left) validates the effectiveness of our evolution process, where easy QA pairs solvable within 4 turns decrease significantly. While Figure 4 (right) shows that gap remains between our Evolved QA and BrowseComp-en in terms of tool calling turns, BrowseComp-en presents excessive difficulty with accuracy below 20% on most open-source models. Therefore, completely mirroring BrowseComp-ens difficulty level is unnecessary for training current open-source models. Overall, WebExplorer-QA, this increased complexity, while maintaining reasonable solvability, 8 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents establishes our dataset as high-quality training data that effectively advances web navigation agent capabilities. 3. Cold Start and Reinforcement learning Our training methodology adopts the established post-training two-phase paradigm: supervised fine-tuning for cold start initialization, followed by reinforcement learning for advanced capability development (Guo et al., 2025; Zhipu AI, 2025). The initial supervised fine-tuning phase enables models to acquire proper invocation of search and browse functions while developing foundational long-horizon search capabilities. Subsequently, reinforcement learning further enhances reasoning abilities, extending model performance to longer contexts and increased maximum turn limits, ultimately achieving more advanced long-horizon problem-solving behaviors. 3.1. Supervised Fine-tuning for Cold Start After synthesizing the challenging QA pairs as described in 2.4, we leverage the commercial model to collect high-quality trajectories for supervised fine-tuning. We employ rejection sampling during data collection, ensuring that our fine-tuning process exclusively utilizes correct trajectories. We adopt the ReAct framework (Yao et al., 2023) as our foundational format, incorporating search and browse as the two primary tools for actions ğ›¼, enclosed by <tool_call> and </tool_call> tags. The framework includes explicit reasoning thoughts ğœ denoted by <think> tags, enabling transparent cognitive processes, and tool responses as observations ğ‘œ marked by <tool_response> tags. Our collected trajectories encompass multiple rounds of action ğ›¼, thought ğœ, and observation ğ‘œ sequences, with an example demonstrated in Appendix A. During this phase, the model learns to decompose complex queries into manageable sub-problems, execute appropriate tool calls, interpret returned information, and synthesize findings into coherent responses. This cold start initialization is crucial for establishing the fundamental capabilities required for subsequent reinforcement learning optimization. 3.2. Reinforcement Learning Following the cold-start phase, which endows the model with fundamental search and browse action capabilities along with long-horizon reasoning ability, we conduct further reinforcement learning training to enhance reasoning performance and optimize decision-making strategies using GRPO algorithm (Shao et al., 2024). Notably, in the RL phase, we can directly utilize the synthesized QA pairs without requiring trajectory demonstrations for the questions, allowing the model to explore diverse reasoning paths autonomously. For reward design, we implement composite reward function that balances structural correctness with answer accuracy: ğ‘… = 0.2 ğ‘…format + ğ‘…correct (4) Format rewards ğ‘…format evaluate the correctness of response formatting, primarily assessing whether tool calls and thought structures adhere to the specified format requirements. This component ensures that the model maintains proper interaction protocols with external tools and preserves the interpretability of its reasoning process. For accuracy rewards ğ‘…correct, we leverage the DeepSeek-V3 model (Liu et al., 2024) as an automated judge to evaluate whether the final responses are correct given the ground truth answers. This automated evaluation approach enables scalable assessment while maintaining high reliability. 9 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Model OpenAI-o3 Claude-4-Sonnet GLM-4.5 DeepSeek-V3.1 Kimi-K2 WebShaper-72B WebShaper-32B (QwQ) WebShaper-32B WebSailor-72B WebSailor-32B WebSailor-7B ASearcher-Web-QwQ WebThinker-32B MiroThinker-32B-DPO-v0.1 13.0 8.7 MiroThinker-8B-DPO-v0. WebExplorer-8B (SFT) WebExplorer-8B (RL) 7.9 15.7 BC-en BC-zh GAIA WebWalkerQA FRAMES Xbench-DS HLE 50.9 12.2 26.4 30.0 14.1 - - - 12.0 10.5 6.7 5.2 2.8 58.1 70.5 29.1 68.3 37.5 66.0 49.2 63.1 57.7 28.8 - - - 30.1 25.5 14.2 60.0 53.3 52.4 55.4 53.2 33.0 15.6 52.8 - 17.0 13.6 21.3 32.0 48.5 57.3 46.6 43.7 50.0 71.7 61. 65.6 61.2 63.0 52.2 49.7 51.4 - - - 34.3 46.5 49.3 45. 59.8 62.7 84.0 80.7 78.9 83.7 72.0 - - - - - - 70. - 71.7 64.4 72.6 75.7 66.7 64.6 70.0 71.2 50.0 - - - 55.0 53.3 34.3 42.1 - - - 47.5 53.7 20.2 20. 21.2 29.8 18.1 - - - - - - 12.5 15.8 11.8 - 16.0 17.3 Table 2 Accuracy (%) of web agents on information-seeking benchmarks. BC-en and BC-zh denote BrowseComp-en and BrowseComp-zh respectively. XBench-DS refers to XBench-DeepSearch. Bold indicates the best performance among open-source models < 100B, while underlined values represent the best performance among models < 10B parameters. All scores of WebExplorer-8B are computed as Avg@4 using LLM-as-Judge. Entries marked with dagger () were reproduced by us under our scaffold: on model name = entire row; on number = that entry only. To accommodate the models expanding reasoning capabilities during training, we implement progressive context length expansion strategy. We begin with maximum length of 64K tokens and tool calling turn limit of 50. As the model generates increasingly complex trajectories, we gradually increase the maximum length to 96K tokens with 75-turn limit, and finally to 128K tokens with 100-turn limit. This progressive expansion allows the model to develop more sophisticated long-horizon reasoning patterns throughout the training process. 4. Experiments 4.1. Experimental Setups Benchmarks We employ several information-seeking benchmarks to comprehensively evaluate different web agent abilities. These include BrowseComp-en (Wei et al., 2025), one of the most challenging information-seeking benchmarks introduced by OpenAI, and its Chinese counterpart BrowseCompzh (Zhou et al., 2025). We also evaluate on GAIA (Mialon et al., 2024), widely-adopted benchmark for General AI Assistants, where we use the 103-sample text-only validation subset following WebShaper. Additional benchmarks include WebWalkerQA (Wu et al., 2025b) for systematic data extraction across website subpages, FRAMES (Krishna et al., 2024) for testing factual responses and reasoning capabilities, and XBench-DeepSearch (Xbench-Team, 2025) focusing on deep information retrieval tasks. Beyond these information-seeking benchmarks, we also evaluate on the frontier academic benchmark HLE (Phan et al., 2025) to verify our models generalization capabilities which contains 10 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Figure 5 Left: Average # tool calls per trajectory during the RL training process. Each tool call (search or browse) is counted separately. Middle: Average trajectory length (# tokens) during the RL training process. Right: The avg@4 scores of BrowseComp-en and BrowseComp-zh during the RL training process. many extremely challenging STEM questions, while our training data is all about Wikipedia-alike knowledge. We report all scores on these benchmarks using Avg@4 and employ LLM-as-Judge evaluation (Liu et al., 2023) with DeepSeek-V3 (Liu et al., 2024) following previous work (Li et al., 2025a; Tao et al., 2025). Models We compare our approach against both proprietary and open-source agents. For proprietary models, we primarily benchmark against OpenAI-o3 and Claude-4-Sonnet (Anthropic, 2025). Among open-source agents, we compare against advanced models including, GLM-4.5 (Zhipu AI, 2025), DeepSeek-V3.1 (Liu et al., 2024), Kimi-K2 (Team et al., 2025), WebShaper (Tao et al., 2025), WebSailor (Li et al., 2025a), ASearcher (Gao et al., 2025), WebThinker (Li et al., 2025b) and MiroThinker (MiroMind Team, 2025). Scaffold Details Following previous work (Li et al., 2025a; Tao et al., 2025), our agent scaffold contains two tools: search and browse. The search tool takes one or multiple queries and returns the top-10 Google search results with title, URL, and snippet for each query. The browse tool takes URL and browse query, then uses Jina (Jina.ai, 2025) to retrieve the full content and employs Gemini 2.5 Flash (Comanici et al., 2025) to answer the browse query based on the URL content. Further details about these two tools can be found in Appendix B. Using this unified scaffold, we evaluate OpenAI-o3, Claude-4-Sonnet, and Kimi-K2 on all benchmarks, and evaluate GLM-4.5 and DeepSeek-V3.1 on subset (marked with where the original reports lack results), ensuring fair cross-model comparison. Scaffolds of most other baselines generally rely on search and browse tools, making comparisons fair; the exception is MiroThinker, which employs additional tools beyond search and browse. Training Details We use the Qwen3-8B model (Yang et al., 2025) to conduct SFT and RL training, resulting in the WebExplorer-8B model. For supervised fine-tuning, we utilize approximately 13K training samples with training batch size of 32 and learning rate of 1e-5 for 4 epochs. For reinforcement learning, we train on approximately 12K samples using the GRPO algorithm, where each group consists of 8 rollouts with batch size of 64 and learning rate of 1e-6. During the RL process, we gradually increase the maximum response length to 128K and the maximum turns limit to 100. 4.2. Experimental Results WebExplorer-8B Establishes State-of-the-Art Performance at Its Scale As shown in Table 2, our WebExplorer-8B demonstrates exceptional performance across diverse information-seeking 11 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents benchmarks. Following supervised fine-tuning, WebExplorer-8B (SFT) already exhibits competitive results, surpassing WebSailor-7B with scores of 7.9% on BrowseComp-en, 72.6% on FRAMES, and 47.5% on XBench-DS. The subsequent reinforcement learning phase yields substantial improvements, with our final model achieving 15.7% on BrowseComp-en and 32.0% on BrowseComp-zh. Notably, WebExplorer-8B (RL) surpasses the previous best sub-10B model, MiroThinker-8B-DPO-v0.1, across all benchmarks, establishing new state-of-the-art performance at its parameter scale. WebExplorer-8B Outperforms Much Larger Models Despite having only 8B parameters, WebExplorer-8B consistently outperforms much larger open-source models, demonstrating remarkable parameter efficiency. On BrowseComp-en, BrowseComp-zh, FRAMES, and WebWalkerQA, our model achieves the best performance among open-source sub-100B models. Specifically, with scores of 15.7% on BrowseComp-en, 32.0% on BrowseComp-zh, and 75.7% on FRAMES, our model surpasses these significantly larger models including WebSailor-72B and even larger model Kimi-K2. Similarly, on WebWalkerQA, we achieve 62.7%, outperforming WebShaper-72B by 10 absolute points. Our model also demonstrates competitive performance on additional benchmarks, achieving 50.0% on GAIA and 53.7% on XBench-DS, rivaling models with more parameters like WebThinker-32B and WebSailor-32B. Strong Generalization from BrowseComp-Inspired Training Data Notably, while our QA pair synthesis methodology, particularly the evolution stage, is inspired by BrowseComp-en, and the exemplars provided in synthesis prompts are both derived from BrowseComp-en, we ensure diversity by collecting seed entities from Wikipedia across different domains. The resulting model also generalizes effectively across diverse benchmarks and domains beyond the BrowseComp-en/zh. As demonstrated previously, WebExplorer-8B achieves strong performance across all informationseeking benchmarks, including GAIA, WebWalkerQA, FRAMES, and XBench-DS. This cross-benchmark success indicates substantial improvement in general information-seeking capabilities, demonstrating the generalization benefits of training on our challenging, complex queries. More remarkably, despite our training data not being STEM-focused, our model extends beyond information-seeking tasks. It achieves 17.3% on the HLE benchmark, which encompasses questions from diverse academic disciplines. This performance surpasses previous 32B models, including ASearcher-Web-QwQ and WebThinker-32B, further validating the robustness and transferability of our approach. RL Training Dynamics and Performance Analysis To analyze the training dynamics, we recorded the average number of tool calls per trajectory and the average response length (number of tokens) during RL training, with results presented in Figure 5. Throughout the RL training process, the average number of tool calls increases steadily from approximately 11 to over 16, significantly exceeding the typical range of fewer than 10 tool calls observed in previous work (Gao et al., 2025). This substantial increase suggests that our model learns to execute more sophisticated multi-step reasoning strategies. Concurrently, the average trajectory length grows during the RL process, scaling to over 40K tokens, as the search tool responses constitute the majority source of trajectory length. Importantly, we observe that performance on both BrowseComp-en and BrowseComp-zh improves consistently throughout this process, with BrowseComp-en increasing from 7.9% to 15.7%, demonstrating strong correlation between increased tool usage complexity and task performance. Similar to the emergence of long chain-of-thought phenomena observed in previous single-turn RL studies on mathematical or logical tasks (Guo et al., 2025; Liu et al., 2025; Yu et al., 2025; Zeng et al., 2025), these findings validate the effectiveness of our RL training approach in developing advanced web agents that can tackle complex information-seeking tasks through extended reasoning chains. 12 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents 5. Conclusion In this work, we present WebExplorer, query-answer data synthesis approach for training advanced web agents. By employing model-based exploration and iterative query evolution, we synthesize challenging query-answer pairs that require long-horizon reasoning spanning over 10 tool calling turns. Leveraging these high-quality QA pairs for supervised fine-tuning followed by GRPObased reinforcement learning, our 8B model WebExplorer-8B achieves state-of-the-art performance across multiple information-seeking benchmarks, including BrowseComp-en, BrowseComp-zh, GAIA, WebWalkerQA, FRAMES, and XBench-DeepSearch. It establishes the best performance at its scale while significantly outperforming larger models such as WebSailor-72B. Beyond information-seeking tasks, WebExplorer-8B demonstrates strong generalization on the academic HLE benchmark, outperforming previous 32B models by nearly 5 points. Our RL training enables the model to scale to over 16 tool calling turns on average, validating the effectiveness of our approach for training long-horizon web agents. The success of WebExplorer demonstrates the potential of autonomously synthesizing challenging information-seeking QA pairs and leveraging supervised fine-tuning and reinforcement learning to build advanced, long-horizon web agents."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing claude 4. Technical report, 2025. URL https://www.anthropic.com/ news/claude-4. Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, and Jimmy Lin. Browsecompplus: more fair and transparent evaluation benchmark of deep-research agent. arXiv preprint arXiv:2508.06600, 2025. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, and Yi Wu. Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl, 2025. URL https://arxiv.org/abs/2508.07976. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Jina.ai. Jina, 2025. URL https://jina.ai. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrieval-augmented generation. In North American Chapter of the Association for Computational Linguistics, 2024. URL https://api.semanticscholar.org/CorpusID:272753636. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025a. 13 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776, 2025b. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Junteng Liu, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, et al. Synlogic: Synthesizing verifiable reasoning data at scale for learning logical reasoning and beyond. arXiv preprint arXiv:2505.19641, 2025. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. Calibrating llm-based evaluator. arXiv preprint arXiv:2309.13308, 2023. GrÃ©goire Mialon, ClÃ©mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=fibxvahvs3. MiroMind Team. Miromind open deep research, 2025. URL https://miromind.ai/blog/ miromind-open-deep-research. Blog post, August 2025. OpenAI. Introducing deep research. Technical report, 2025. URL https://openai.com/index/ introducing-deep-research/. Perplexity AI. Introducing perplexity deep research. https://www.perplexity.ai/hub/blog/ introducing-perplexity-deep-research, February 2025. Accessed: 2025-07-25. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via informationseeking formalization. arXiv preprint arXiv:2507.15061, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal. ArXiv, abs/2501.07572, 2025b. URL https://api.semanticscholar.org/CorpusID:275471576. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Xbench-Team. Xbench-deepsearch, 2025. URL https://xbench.org/agi/aisearch. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Zhipu AI. GLM-4.5: Reasoning, Coding, and Agentic Abililties. https://z.ai/blog/glm-4.5, July 2025. Accessed: 2025-08-07. Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, et al. Browsecomp-zh: Benchmarking web browsing ability of large language models in chinese. arXiv preprint arXiv:2504.19314, 2025. A. Trajectory Example To illustrate how our agent framework operates in practice, we provide detailed trajectory example from the WebExplorer supervised fine-tuning data. This example demonstrates the agents multistep reasoning process, thought, tool call, and tool response when handling complex queries. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Question: In county adjacent to the nations capital, small press gaming company co-founded by someone whose previous pulp adventure game had won silver-level award in category focusing on game mechanics excellence. This person reached specific age milestone during an online funding campaign spanning consecutive years that occurred exactly 6 years after their games award. What was the exact final funding amount raised by this campaign, expressed in US dollars without the dollar sign or comma separators? Thought Step 1 <think> This is complex multi-part query that need to break down systematically. Let me identify the key components: 1. county adjacent to the nations capital (so near Washington DC) . . . Let me start by searching for information about gaming companies near Washington DC that had crowdfunding campaigns, and awards for pulp adventure games. </think> need to work through this step-by-step to identify the specific gaming company, award, and crowdfunding campaign. Let me start by searching for information about gaming companies near Washington DC and awards for pulp adventure games. Action <tool_call> {name: search, arguments: {queries: [small press gaming company Washington DC area, pulp adventure game silver award game mechanics ENnie, gaming company county adjacent Washington DC crowdfunding]}} </tool_call> Observation <tool_response> search result for [small press gaming company Washington DC area] <title>op Washington, DC Gaming Companies (10) - Built In </title> <url>https://builtin.com/companies/location/washington-dc/type/gaming-companies </url> <snippet>Top Washington, DC Gaming Companies (10) Edge Systems Technologies Something Wicked Games Polygon Implicit Solution BetMGM Snaapiq Microcoaching. </snippet> . . . </tool_response> 16 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Thought Step 2 <think> From the search results, can see some relevant information: 1. found mention of Spirit of the Century which won Silver ENnie for Best Rules in 2007, and its described as really well put together game of pulp adventure . . . </think> Let me search more specifically for information about Spirit of the Century, which appears to be pulp adventure game that won silver ENnie, and its publisher. Action <tool_call> {name: search, arguments: {queries: [Spirit of the Century silver ENnie 2007 Evil Hat Productions, Evil Hat Productions Maryland Virginia crowdfunding Kickstarter, Fred Hicks Evil Hat Productions age milestone funding campaign]}} </tool_call> Observation <tool_response> search result for [Spirit of the Century silver ENnie 2007 Evil Hat Productions] <title>Spirit of the Century - Wikipedia </title> <url>https://en.wikipedia.org/wiki/Spirit_of_the_Century </url> <snippet>Spirit of the Century is pulp role-playing game published by Evil Hat Productions, and based on Evil Hats FATE system. . . . 2007 Silver ENnie Award for Best . . . </snippet> . . . </tool_response> Thought STEP 3 <think> Great! Now have some key information: 1. Spirit of the Century is pulp adventure game by Evil Hat Productions . . . </think> Perfect! found that Spirit of the Century by Evil Hat Productions won Silver ENnie for Best Rules in 2007, and Evil Hat is based in Silver Spring, Maryland (Montgomery County, adjacent to DC). Now let me search for their crowdfunding campaigns around 2013 (6 years after 2007) and information about Fred Hicks age milestones. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Action <tool_call> {name: search, arguments: {queries: [Evil Hat Productions Kickstarter 2013 2014 Fred Hicks, Fate Core Kickstarter 2013 funding amount final total, Fred Hicks 40th birthday milestone age Fate Core campaign]}} </tool_call> Observation <tool_response> search result for [Evil Hat Productions Kickstarter 2013 2014 Fred Hicks] <title>Race to Adventure! Spirit of the Century Board Game - Kickstarter </title> <url>https://www.kickstarter.com/projects/evilhat/race-to-adventure-a-spirit-of-the-centurytmboard/posts?ref=project_share </url> <snippet>Evil Hat Productions is raising funds for Race to Adventure! Spirit of the Century Board Game on Kickstarter! Race to Adventure! </snippet> . . . </tool_response> Thought STEP 4 <think> found some crucial information! The Fate Core Kickstarter by Evil Hat Productions raised exactly $433,365 and ended on January 30, 2013. This is 6 years after the 2007 ENnie award for Spirit of the Century. . . . </think> Excellent! found that the Fate Core Kickstarter raised $433,365 and ended on January 30, 2013 - which is exactly 6 years after the 2007 ENnie award. Let me get more details about this campaign to confirm it spans consecutive years and involves an age milestone. Action <tool_call> {name: browse, arguments: { url: https://www.kickstarter.com/projects/evilhat/fate-core, query: When did this campaign start and end? Did it span 2012 and 2013? Any mention of Fred Hicks birthday or age milestone during the campaign?}} </tool_call> WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Observation <tool_response> The campaign started on **December 3, 2012**, and ended on **January 29, 2013**, spanning **56 days** and crossing over from **2012 to 2013**. There is **no mention of Fred Hicks birthday or age milestone** in the provided content. </tool_response> Thought STEP <think> Perfect! This confirms that the Fate Core Kickstarter: . . . </think> Perfect! The Fate Core campaign spanned consecutive years (December 3, 2012 to January 29, 2013) and occurred exactly 6 years after the 2007 ENnie. Now let me search for information about Fred Hicks birthday and age milestones during this period. Action <tool_call> {name: search, arguments: {queries: [Fred Hicks birthday December January 40 years old, Fred Hicks born 1972 1973 Evil Hat, Fred Hicks 40th birthday December 2012 January 2013]}} </tool_call> Observation <tool_response> search result for [Fred Hicks birthday December January 40 years old] <title>Happy birthday to the first general manager of Forsythe Associates ... </title> <url>https://www.facebook.com/groups/649530655242462/posts/1496564073872445 </url> <snippet>Happy birthday to the first general manager of Forsythe Associates [1962-1971], Fred Hicks [1920-1971]. </snippet> . . . </tool_response> Thought STEP 6 <think> Perfect! found the crucial information. . . . </think> Therefore, the exact final funding amount raised by this campaign, expressed in US dollars without the dollar sign or comma separators, is: **433365** 19 WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents B. Tool Schemas Our system utilizes two primary tools for web-based information retrieval: search tool for discovering relevant content and browse tool for detailed content analysis. The following sections detail the schemas for each tool. B.1. Search Tool The search tool enables web search functionality through Google search API, allowing the model to discover relevant URLs and content snippets based on input queries. Search Tool Schema tool_schema: type: function function: name: search description: Web search. parameters: type: object properties: queries: type: array description: The queries will be sent to Google. You will get the brief search results with (title, url, snippet)s for each query. items: type: string required: - queries The search tool accepts an array of query strings and returns search results containing titles, URLs, and content snippets for each query. It is configured to return the top 10 results per query. B.2. Browse Tool The browse tool provides detailed content analysis capabilities by fetching and processing content from specific URLs using combination of content extraction and language model analysis. WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents Browse Tool Schema tool_schema: type: function function: name: browse description: Explore specific information in url. parameters: type: object properties: url: type: string description: The url will be browsed, and the content will be sent to Large Language Model (LLM) as the based information to answer query. query: type: string description: The query to this url content. You will get an answer by another LLM. required: - url - query The browse tool takes URL and specific query as input. It uses Jina (Jina.ai, 2025) for content extraction and Gemini Flash (Comanici et al., 2025) as the generation engine to analyze the retrieved content and provide targeted answers to the input query."
        }
    ],
    "affiliations": [
        "The Hong Kong University of Science and Technology",
        "University of Waterloo"
    ]
}