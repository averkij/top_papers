{
    "paper_title": "ShowUI-Aloha: Human-Taught GUI Agent",
    "authors": [
        "Yichun Zhang",
        "Xiangwu Guo",
        "Yauhong Goh",
        "Jessica Hu",
        "Zhiheng Chen",
        "Xin Wang",
        "Difei Gao",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains a major challenge for autonomous agents, largely due to a lack of scalable, high-quality training data. While recordings of human demonstrations offer a rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, a comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: A recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. A learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. A planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide a scalable solution for collecting and parsing real-world human data, demonstrating a viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans."
        },
        {
            "title": "Start",
            "content": "ShowUI-Aloha: Human-Taught GUI Agent"
        },
        {
            "title": "Yichun Zhang Xiangwu Guo Yauhong Goh",
            "content": "Jessica Hu Mike Zheng Shou"
        },
        {
            "title": "Zhiheng Chen Xin Wang Difei Gao",
            "content": "Show Lab, National University of Singapore https://showlab.github.io/Aloha_Page/ 6 2 0 2 2 1 ] . [ 1 1 8 1 7 0 . 1 0 6 2 : r Figure 1: Overview and evaluation of ShowUI-Aloha. Left: Human-taught demonstrations are converted into grounded action traces, which are lifted into traceand prompt-guided plans and executed on real desktop environments. Middle: Qualitative comparisons across representative multi-step desktop tasks show that Aloha avoids common failure modes of unguided agents, such as context drift, unsupported actions, and stuck states. Right: Quantitative comparison on 361 OSWorld-style tasks executed on Windows and macOS demonstrates that human-guided planning enables higher end-to-end task success than existing autonomous and agentic baselines."
        },
        {
            "title": "Abstract",
            "content": "Graphical User Interfaces (GUIs) are central to human-computer interaction, yet automating complex GUI tasks remains major challenge for autonomous agents, largely due to lack of scalable, high-quality training data. While recordings of human demonstrations offer rich data source, they are typically long, unstructured, and lack annotations, making them difficult for agents to learn from.To address this, we introduce ShowUI-Aloha, comprehensive pipeline that transforms unstructured, in-the-wild human screen recordings from desktop environments into structured, actionable tasks. Our framework includes four key components: recorder that captures screen video along with precise user interactions like mouse clicks, keystrokes, and scrolls. learner that semantically interprets these raw interactions and the surrounding visual context, translating them into descriptive natural language captions. planner that reads the parsed demonstrations, maintains task states, and dynamically formulates the next high-level action plan based on contextual reasoning. An executor that faithfully carries out these action plans at the OS level, performing precise clicks, drags, text inputs, and window operations with safety checks and real-time feedback. Together, these components provide scalable solution for collecting and parsing real-world human data, demonstrating viable path toward building general-purpose GUI agents that can learn effectively from simply observing humans."
        },
        {
            "title": "1 Introduction",
            "content": "Graphical User Interfaces (GUIs) have become the primary medium for human-computer interaction, enabling users to navigate and operate wide range of digital environmentsfrom web browsers and mobile applications to desktop software. Automating GUI tasks through autonomous agents offers significant potential to boost productivity, broaden access to digital tools, and lay the groundwork for advanced AI systems capable of adapting to dynamic environments [9, 40, 29]. Recent progress in vision-language models (VLMs) [24, 34, 44, 28] and agent frameworks [38, 40, 11] has further propelled developments in GUI automation. While these methods exhibit strong capabilities in grounding UI elements, their limited understanding of the underlying software logic continues to hinder task completion, particularly in complex workflows. Early attempts [9, 50, 11, 2, 1, 19, 16, 15, 13, 48] introduce agent frameworks that utilized large language models (LLMs) to decompose user tasks and generate corresponding action plans. These methods typically operated in zero-shot setting, relying entirely on the general knowledge encoded in LLMs, often sourced from web-based materials such as tutorials. Subsequent works [12, 21, 28, 41, 44, 8, 46, 38] progressed toward unified GUI vision-language-action models. These models are trained not only on large collections of static screenshots but, crucially, on human-labeled interaction trajectories, allowing them to directly map visual inputs and task instructions to GUI actions. Compared to the earlier LLM-based agents, these multimodal models exhibit substantially improved task planning abilities, highlighting the importance of human knowledge in advancing GUI agent development. Nevertheless, persistent challenge lies in the scalable collection of GUI automation data. Most publicly available datasets [9, 7, 22, 49, 29, 37, 6, 20] require extensive manual annotation and are largely limited to websites and mobile applications, where metadata is more readily accessible. Consequently, the performance of current methods remains constrained in more complex desktop environments. At the same time, we observe that knowledge workers routinely perform tasks on computers, generating vast amounts of real-world interaction data. This observation raises compelling question: What if computers could learn to use software and perform digital tasks simply by observing how humans naturally and routinely accomplish them? Such human demonstration data are plentiful, rich in contextual information, and faithfully capture authentic user behavior, offering tremendous potential to accelerate the development of capable and generalizable GUI agents. However, collecting and utilizing human demonstration videos introduces several unique challenges. First, the absence of standardized data collection tools hinders the scalability of data acquisition. Moreover, the collected datatypically in the form of interaction trajectories, are inherently unannotated, making it difficult for models to interpret and learn from them. These trajectories usually consist of raw visual inputs, screen recordings, and low-level interaction data such as pixel-level click positions. Yet, for effective learning, models must grasp not only the semantics of these interactions but also the underlying user intentions. Additionally, knowledge workers often perform numerous tasks over extended periods, resulting in untrimmed recordings with no explicit task boundaries. Models must therefore learn to distinguish between interrelated actions that constitute coherent task and unrelated actions that belong to separate workflows. To address these challenges, we present ShowUI-Aloha, human-taught desktop agent that learns directly from in-the-wild demonstrations rather than curated UI labels or synthetic trajectories. Our key insight is that natural user interactions already contain rich intent, temporal structure, and visual groundingif one can reliably capture and interpret them. ShowUI-Aloha adopts recordparselearn paradigm: lightweight instrumentation logs raw keyboardmouse events and screen activity during users normal workflow, and an inference pipeline transforms this noisy signal into compact, semantically meaningful teaching trajectory. This trajectory abstracts away low-level pixels while retaining intent, enabling the agent to understand what was done and why. Built on top of this representation, ShowUI-Aloha employs planning and execution mechanism that generalizes the demonstration to new tasks and unseen UI states. By leveraging natural language abstraction and robust grounding against the live desktop environment, the agent learns transferable procedures that remain effective even when layouts shift, dialogs appear unexpectedly, or the task details differs from the original demonstration. This demonstration-driven paradigm enables ShowUI-Aloha to construct essential task-specific knowledge base while maintaining strong flexibility, emphasizing abstraction over memorization. This demonstration-driven paradigm yields flexible, practical alternative to prior rule-based or template-driven GUI agents, while still being lightweight enough for deployment. This work makes the following contributions: 2 learning pipeline that derives structured teaching trajectories. We develop recordparselearn framework that transforms raw, in-the-wild human desktop interactions into semantically grounded teaching trajectories. lightweight execution system for robust generalization. Building on the learned trajectories, we design planner actor mechanism that executes tasks on live desktop environments with robustness to UI drift, layout changes, and unexpected system states. Comprehensive evaluation on OSWorld-style tasks [40]. We evaluate ShowUI-Aloha on large-scale suite of OSWorld-style tasks spanning diverse desktop applications, demonstrating strong generalization and real-world applicability under user-oriented evaluation protocol. fully open-source desktop agent. We release the entire ShowUI-Aloha frameworkincluding recorder, learner, planner, actor, and evaluation toolsas an open-source project to support transparent, reproducible, and extensible research on GUI agents. Together, these contributions establish ShowUI-Aloha as practical and scalable foundation for demonstration-driven computeruse intelligence. Figure 2: Overview of the Aloha paradigm for GUI agents. Instead of relying on trial-and-error interaction, Aloha leverages single human demonstration to distill reusable task guidance, which is then consistently applied to new task variants and interface layouts, enabling stable and generalizable execution across changing interfaces."
        },
        {
            "title": "2 Related Work",
            "content": "Computer Use Datasets. Recent progress of Large Language Models (LLMs) show promising potential beyond traditional text completion. Notably, agents [45, 32, 30] showcase the capability to autonomously execute complex tasks through seamless tool integration. This agentic capability has naturally extended to the digital GUI automation [47, 35, 25]. To power the development of GUI Agents, recent efforts have concentrated on the novel datasets and benchmarks across three representative platforms: (i) Website [9], are often readily scalable due to the structured nature of HTML and available browser-based tools. However, the ease of automated collection can result in web corpora that are text-rich yet potentially noisy, characterized by lack of rigorous human verification. (ii) Mobile [49], aims to enhance accessibility and interaction within simulated mobile environments like open-source Android and iOS. Datasets in this domain, while valuable, tend to exhibit limitations in diversity, particularly in terms of software difficulties and action spaces. (iii) Desktop [14, 5, 40], are considered highly valuable due to the inherent challenges in their collection. Unlike web and mobile platforms, desktop environments lack automated data collection pipelines. The desktop interaction necessitates the complex integration of dense keyboard and mouse inputs. While benchmarks [5, 40, 18] offer evaluation frameworks for desktop platforms, scaling data collection to large-scale remains significant challenge. This underscores the critical need for the high-quality desktop GUI training corpora, especially those with human expert knowledge. Learning from Human Demonstration. Learning from human demonstrations offers data-efficient approach for training models in both physical [42, 36] and digital [17, 27] environments. Such demonstrations, often in the form of videos, serve as rich records of human experience and action for decision marking. Early works effectively leveraged demonstrations for action understanding from video [10] and for complex robot manipulation tasks [42, 36], underscoring the paradigms ability to learn complex behaviors without extensive hand-engineering. When it comes to the digital settings especially GUI, while many agent-training approaches rely on scaling data through automated crawling or synthetic generation [33], these methods can often lead to datasets lacking in quality and human-like strategic depth. Despite notable successes in adapting agents to new web or mobile environments [23, 19] using demonstration learning and straightforward Supervised Fine-Tuning (SFT), we recognize that raw demonstration trajectories, being primarily records of low-level actions, frequently lack explicit encoding of high-level semantic user intentions or plans. To address this critical gap, our work introduces carefully designed data workflow specifically to refine and augment these raw demonstrations, ultimately enabling more effective agent training by incorporating richer contextual understanding."
        },
        {
            "title": "3 Method",
            "content": "Figure 3: Overview of the Aloha workflow. Human demonstrations are recorded and converted into structured action traces. The actor uses the task prompt and screenshots to generate an execution plan, while the executor performs each action on the computer. Figure 3 shows an overview of the framework. The goal is to unlock scalable data collection and parsing of human demonstrations, suitable for training and evaluating GUI models that can understand and potentially automate complex software workflows. We introduce each core component: Recorder, Learner, Executor and Actor, in the following sections respectively. 3.1 Recorder The recorder is implemented as an all-in-one portable software, which can be easily deployed in new Window or MacOS system with ease. Figure 4: User-facing interface of the ShowUI-Aloha Recorder. The recorder presents minimal floating control panel (top right) for starting and stopping captures, while modal dialog allows users to name or rename each recording with clear constraints on valid characters. These utilities support organized, large-scale data collection and facilitate downstream processing. Video Recording. The recorder aims to record videos with high frame rate, and simutaneously capturing detailed and dense user operations. Considering the compatibility problem across machines, we employ FFmepg(avfoundation on MacOS as replacement) as the underlying video recording implementation, which is widely-used, open-source multimedia framework. When starting recording, the ddagrab filter in FFmpeg is triggered to capture and record the full-screen in full-resolution in 30 FPS (frame per second). 4 Action Recording. During recording video frames, the tool simultaneously records the actions performed by user in real-time. To implement this, we borrow code from KeyCastOW 2, Instead of visualizing, our goal is to record the action history. Thus, in our implementation, the tool is designed to output the user actions into logging file, together with the corresponding timestamps. Specifically, it captures actions such as mouse clicks (left, right, wheel), mouse movements, drags, and keyboard inputs. Raw frame - File Opening Raw frame - Text Selection 2 Raw frame - Format Selection Figure 5: Raw screen frames captured by the Aloha Recorder. These consecutive frames illustrate the natural visual trajectory present in human desktop demonstrations. The Recorder captures full-resolution frames at high frequency, preserving the fine-grained cursor dynamics, UI transitions, and subtle motion patterns that are essential for downstream action cleaning and trace generation. Figure 6: Raw action log captured by the Aloha Recorder of above frames. The recorder logs dense low-level input events such as mouse movements, mouse down/up pairs, drags, and keystrokes with high-frequency timestamps. This raw stream is noisy, redundant, and unstructured, reflecting natural human behavior and motivating the need for the Action Cleaning stage in the Aloha Learner. Integration. The complete recording pipelinecovering both screen capture and interaction loggingis packaged into lightweight 170 MB application with native builds for macOS and Windows. As shown in Fig. 4, the recorder includes compact floating control panel for starting and stopping captures, together with user-friendly renaming dialog that enforces consistent naming conventions for large-scale data collection. Additional utilities such as dynamic path configuration, one-click batch renaming, and API-call integration further streamline high-throughput experimentation and integration into broader systems. Beyond supplying high-quality teaching data for ShowUI-Aloha, this recorder also serves as standalone tool for generating structured screenaction datasets for other GUI-related research tasks. 3.2 Aloha Learner Aloha Learner converts raw human demonstrations into structured, semantic GUI action traces that can be executed and generalized by downstream modules. It consists of three tightly coupled components: raw log parser, screenshot marker, and prompt-driven trace generator. Together, they bridge low-level human input with high-level, machine-actionable representations. 2https://github.com/brookhong/KeyCastOW Action Cleaning. The parser transforms the recorders high-frequency event stream into compact sequence of semantic user actions. It first parses the raw log into primitive eventsmouse down/up, motion, scroll, and keystrokes. Because the recorder samples at high temporal resolution, the initial stream contains redundant, fragmented, and noisy entries. The parser therefore applies multi-stage consolidation pipeline. It merges consecutive keystrokes into coherent typing segments and reconstructs drag operations by linking pressmoverelease triples into continuous trajectories. Mouse down/up pairs are unified into single click events, with spurious single-clicks preceding double-clicks removed to avoid duplication. Scroll actions are normalized across input devices, and special keys such as Backspace and key combinations (e.g., Ctrl+S) are properly handled to faithfully reproduce the final text entered. All actions are then chronologically sorted and written to cleaned log, yielding minimal, semantically aligned sequence that accurately captures user intent. Figure 7: From raw events to grouped interaction primitives. The recorder logs dense, noisy, and highly redundant event streams (left). Aloha Learner consolidates these signals into higher-level interaction primitives (right). Marked Screenshot Generation. For each cleaned action, the screenshot marker produces synchronized visual inputs that make downstream reasoning more robust and unambiguous. Two images are extracted per action: full-screen frame (typically 19201080) that provides global context, and zoomed-in crop tightly centered around the interaction site. As shown by Figure 8, to encode action semantics directly into the visual domain, expressive overlays are added: semitransparent red for click-type events and semitransparent red polyline indicating drag paths. These lightweight but informative markings remove the need for coordinate-level supervision and allow the system to attribute user intent directly from pixels. The size, semitransparency, and placement of these indicators are carefully calibrated to balance precise target localization with sufficient visual clarity, ensuring that downstream components can reliably analyze the underlying UI elements. This step forms structured visual interface for the trace generator, ensuring that even ambiguous GUI regions become machine-interpretable. Figure 8: Example of the zoomed-in and marked crop (left) and full-screen context (right) used by the trace generator. Trace Generation. With the action list cleaned and screenshot pairs enriched with unobtrusive visual markers, the Trace Generator converts these multimodal signals into coherent, stepwise natural-language trace. At its core, this module leverages visionlanguage model (VLM) to jointly interpret the marked crop, the full-screen context, and the recent execution history, yielding semantically grounded descriptions that faithfully reflect user intent. Grouped interaction primitives Semantic, intent-aligned trace Figure 9: From grouped actions to semantic teaching traces. After low-level events are merged into coherent interaction primitives (left), Aloha Learner uses visionlanguage model to reason over the marked screenshots, UI context, and recent action history to produce high-level semantic descriptions (right). Each step includes an Observation of the UI state, Think field with brief reasoning, normalized Action such as click the File menu or drag pikachu.png into dir2, and an Expectation describing how the interface should change. These semantic traces capture user intent and form the core supervision for downstream planning and execution. To initiate each step, the generator constructs structured prompt consisting of: (1) base instruction defining the expected JSON output schema; (2) an action-typespecific delta that injects concise priors about clicks, drags, scrolls, modifier keys, or typing; (3) short summary of up to three previously generated steps; and (4) the high-resolution marked screenshots produced in the preceding stage. This combination allows the model to reason over both the localized interaction sitealready visually annotatedand the broader UI state. Upon receiving this prompt, the VLM produces four-field caption containing an observation, think rationale, action description, and expectation of how the interface will change. lightweight post-processing layer then sanitizes the output by removing spurious coordinate leakage, enforcing crop-first phrasing, and normalizing ambiguous or model-hallucinated operations. The resulting step is appended to the trajectory, and the process continues iteratively until all actions are consumed. This design turns raw demonstrations into clean, executable traces without hand-crafted rules or task-specific templates. By grounding every step in both the marked visual context and preceding reasoning chain, the Trace Generator provides the Aloha Actor with stable, interpretable, and semantically rich representation of human workflows. The final output for each action is formatted as four-field JSON record: Observation (what is visually present), Think (brief reasoning and intent inference), Action (the concrete operation normalized by deltas), and Expectation (the immediate UI change that should occur). This structured representation provides both semantic clarity and operational determinism, enabling the downstream Aloha Actor to reliably execute, monitor, and recover from GUI interactions. Summary. Together, action cleaning, screenshot marking, and prompt-guided trace generation form the core of Alohas learner pipeline. They translate raw humancomputer interaction into coherent, machine-verifiable action planestablishing the foundational visionlanguage interface through which Aloha acquires, understands, and reproduces real-world GUI tasks. 3.3 Aloha Actor Aloha Actor is the central orchestrator of the Aloha automation framework, coupling task-level reasoning with reliable GUI execution. It integrates the Aloha Planner as its cognitive front-end and coordinates multiple execution backends to operate robustly on real desktop environments. Together, they form closed-loop system that plans, acts, verifies, and adaptssubstantially beyond the capabilities of single-call or replay-based LLM agents. Aloha Planner. The planner interprets the users high-level goal, the instantaneous screenshot, and the demonstration-derived Guidance Trajectory. Its backbone is large language modelcommercial (e.g., GPT-4o, Claude), open-source, or locally deployedwhich provides semantic priors but lacks intrinsic grounding of GUI states or demonstration structure. Aloha supplies this missing structure: each planning call is composed using structured templates that include screenshots, annotated action history, and step-aligned demonstration cues. The planner outputs structured next-step plan with fields such as Observation, Reasoning, Current Step, Action, and Expectation, allowing trajectories to serve as soft references rather than rigid scripts. This enables consistent, goal-driven, and context-aware planning even under distribution shifts. Execution Backends as Primitive Actuators. Aloha is deliberately built above existing foundation-model computer-use systems, not as replacement for them. At the execution layer, the actor interfaces with general-purpose computer-use operators exposed by LLM platforms (e.g., OpenAI or Anthropic). These backends provide only low-level motor primitivesclick, double-click, move, scroll, and typeoptionally accompanied by minimal perceptual grounding used solely to localize the target region on the screen. Critically, these operators do not perform task reasoning, trajectory following, goal verification, ambiguity resolution, or recovery. They execute one action at time without maintaining any notion of progress or of the overarching task. In other words, they function strictly as interchangeable perception-assisted actuators. Actor Control Logic. Given the planners step plan, the Actor performs the integration and control logic required for reliable It contextualizes the proposed action with environmental statewindow hierarchy, OS behavior, multi-step automation. application affordancesand chooses the appropriate backend invocation. When the external operator provides multiple possible click locations or ambiguous detections, the Actor resolves these cases using demonstration priors, UI topology, or deterministic heuristics. It selects fallback strategies such as hotkey execution when visual localization is unreliable, verifies post-action states, and triggers replanning when discrepancies arise. All visual and textual traces are logged and fed back into the reasoning loop, enabling stable and iterative correction over long horizons. 3.4 Aloha Executor Aloha Executor serves as the embodied control core of the Aloha framework, responsible for translating high-level agent intentions into precise, verifiable physical interactions on the screen. It forms the final link in the perceptionplanningaction chain, taking structured action commands from the Aloha Actor and turning them into actual mouse, keyboard, and window operations across heterogeneous desktop environments. Parsing and Normalization. The executor first validates and parses the actors output through structured dispatchers. It supports wide set of normalized action typesclick, input, drag, scroll, key, hotkey, wait, and otherseach mapped to specialized parser functions. These parsers handle schema variation and unify coordinates into absolute screen positions through per-monitor offset system. Coordinate Grounding and Safety. For multi-monitor setups, the executor computes per-screen offsets via platform-specific methods (screeninfo on Windows/Linux and Quartz on macOS). Every relative coordinate from the actor is converted into global screen coordinates before execution, ensuring consistent pointer behavior regardless of display configuration. Tool Execution and Feedback. Each parsed action is wrapped and dispatched asynchronously to the corresponding Computer Tool. This tool directly manipulates the desktop: - Mouse Control: movement, click, double-click, drag, hover, and scrolling - Keyboard Control: key sequences, hotkeys, and text typing - Utility Actions: waiting, capturing screenshots, or reporting cursor position Execution results are returned to the pipeline, providing structured runtime feedback to higher layers for logging and retry logic. In general, The Computer Tool encapsulates platform-independent GUI control, automatically handling coordinate scaling, multi-monitor bounding boxes, and interaction animation. It visually annotates clicks and drags through transient overlays for interpretability during demonstrations, and supports flexible scaling."
        },
        {
            "title": "4 Experiments",
            "content": "Because ShowUI-Aloha operates in human-taught setting, the official closed-loop OSWorld [40] evaluation pipeline cannot be applied directly. We therefore evaluate Aloha by reinstantiating the complete OSWorld task suite using the released task specifications, manually executing 361 tasks (excluding eight Google Driverelated tasks, as permitted by the OSWorld authors) with 50-step budget on real desktop environments. 8 Figure 10: Qualitative comparison on Git update workflow. Aloha follows the demonstrated procedure for propagating an edit from scratch folder into Git repository (top row): it modifies emergency_fix.txt, navigates through Documents GitHub GUI_Test, replaces the tracked file in the repository folder, and then issues commit in GitHub Desktop. In contrast, the unguided agent (bottom row) correctly edits and saves the local file, but then directly opens GitHub Desktop without first copying it into the repository path. Because the repo contains no changed files, it repeatedly tries to commit or push in an empty state and becomes stuck, illustrating lack of procedural knowledge about intermediate file-management steps that Aloha inherits from human teaching traces. 4.1 Experimental Setup Experiment Design.Evaluations are conducted on both Windows and macOS platforms. Human testers operate in controlled environment that replicates the original OSWorld setup. For each task, testers perform new demonstration using modified teaching prompt. For example, the original task Copy all files matching *failed.ipynb in the current directory tree to ./fails while preserving the directory hierarchy is adapted into Copy the file pikachu from my desktop photos folder to my desktop folder dir2. This preserves the underlying task logic while altering filenames and paths. Such modifications are crucial to prevent the model from mechanically imitating the teaching sequence, ensuring it instead generates new action plan based on the demonstration trace. After each demonstration, the Aloha pipeline executes the task once, and success is measured using binary score: 1 for successful completion and 0 otherwise. No partial credit is assigned. All 361 tasks are evaluated under this protocol, and the resulting data are collected and analyzed accordingly. Model and Actuator.All experiments use GPT-4o as the visionlanguage model for perception and reasoning. Action execution is carried out through the OpenAI Computer Use API, which serves as lightweight, perception-assisted actuator. As detailed in the Methods section, this backend exposes only primitive OS-level operations (e.g., click, move, scroll, type) and executes them one at time without maintaining task context. No additional reasoning or verification is performed by the actuator during evaluation. Test Devices. We evaluate ShowUI-Aloha on 3 representative desktop platforms to ensure cross-OS robustness. The first platform is macBook Pro equipped with an Apple M4 Pro processor and 24 GB unified memory. The second is Windows desktop with an Intel i7-13700KF CPU, 32 GB RAM, and an NVIDIA RTX 4070 Ti GPU with 12 GB VRAM. The third is Windows laptop (Lenovo Legion 5) equipped with an Intel i7-10750H CPU, 16,GB RAM, and an NVIDIA RTX 2060 GPU with 6,GB VRAM. This combination of macOS and Windows environments mirrors real-world GUI heterogeneity and provides consistent evaluation base for OS-level manipulation tasks. 4.2 Baseline Considerations and Comparison Setting Lack of Direct Baseline. Aloha operates in demonstration-guided regime, where the agent is explicitly taught the task procedure through structured trajectory before execution. Unfortunately, no existing benchmark or prior work provides comparable setting: current OSWorld baselines are all unguided, zero-shot agents, and the official partial-credit scoring system inside the OSWorld evaluator cannot be reproduced outside the closed environment. As result, it is not possible to construct strict apples-to-apples baseline that matches Alohas supervision level, execution protocol, or evaluation metric. We therefore report unguided agents only as contextual anchors rather than direct baselines. Comparison to Unguided Agents. To contextualize Alohas capabilities, we report results from representative set of unguided computer-use agents spanning specialized GUI models, general-purpose foundation models, and multi-component agentic frameworks. These include visionaction models such as UI-TARS-1.5-7B [28] and OpenAI CUA 4o [26], generalist LLM baselines such as Claude 4 Sonnet [4], and recent agentic pipelines that coordinate reasoning and tool use, including GTA-1-7B w/ o3 [43], Jedi-7B w/ o3 [39], Agent S2.5 w/ o3 [3], and CoAct-1 [31]. These systems operate in zero-shot, unguided setting, receiving only the task description without any human demonstration. In contrast, Aloha is explicitly provided the demonstration trajectory. Because these settings differ fundamentally in assumptions, supervision, and evaluation protocol, the reported numbers are not directly comparable. Instead, they serve to position Aloha within the broader landscape of GUI-agent paradigms. Evaluation Metric. The official OSWorld benchmark reports continuous score in [0,1], where partial credit is granted using task-specific reward functions embedded in the closed evaluation environment. Since these reward functions are not publicly available, they cannot be reproduced outside the official runner. Consequently, we evaluate all systemsincluding our reproduction of unguided baselinesusing strict binary success metric: task is counted as successful only if the final state exactly matches the goal, with no partial credit for intermediate progress. This yields more conservative assessment of Alohas capabilities relative to OSWorlds partially-credited scoring. 4.3 Experimental Results Table 11 and Figure 12 report Alohas performance across the ten OSWorld application categories. Aloha demonstrates strong generalization to diverse real-world software, achieving the highest success rates on Chrome (91.3%), OS operations (83.3%), and Thunderbird (80.0%). It also performs reliably on development and media applications, including VS Code (73.9%), LibreOffice Writer (69.6%), GIMP (65.4%), and VLC (64.7%). More complex office-style workflows such as Calc (57.4%) and Impress (42.6%) show moderate difficulty, while tasks requiring cross-application coordination remain the most challenging (Multi-apps, 37.6%). Across all 361 evaluated tasks, Aloha successfully completes 217, corresponding to an overall success rate of 60.1%, demonstrating robust performance under diverse GUI environments and heterogeneous task structures. Aloha Performance Across OSWorld Categories Strong coverage on everyday productivity workflows, with near-perfect success in browser tasks. Category Solved Total Chrome OS Thunderbird LibreOffice Writer GIMP VLC VS Code LibreOffice Calc LibreOffice Impress Multi-apps 42.0 20.0 12.0 16.0 17.0 11.0 17.0 27.0 20.0 35.0 46.0 24.0 15.0 23.0 26.0 17.0 23.0 47.0 47.0 93. Rate 91.3% 83.3% 80% 69.6% 65.4% 64.7% 73.9% 57.4% 42.6% 37.6% Overall 217 361 60.1% OSWorld covers 361 real desktop tasks; Aloha currently automates 217 of them end-to-end. Figure 11: ShowUI-Aloha demonstrates consistently stronger end-to-end task success than prior unguided and agentic GUI agents across broad set of OSWorld-style real-world tasks. 10 Teaching Mode Improves Performance over Strong Baseline Agents Human-taught demonstrations enable ShowUI-Aloha to achieve higher end-to-end task success than unguided and agentic models on OSWorld-style tasks. Model Type Score UI-TARS-1.5-7B OpenAI CUA 4o Claude 4 Sonnet GTA-1-7B Jedi-7B Agent S2.5 CoAct-"
        },
        {
            "title": "Specialized\nSpecialized\nGeneral\nAgentic\nAgentic\nAgentic\nAgentic",
            "content": "29.4 31.3 43.9 48.6 50.6 54.2 56.4 ShowUI-Aloha Human Guided 60.1 OSWorld baseline models report graded benchmark scores; Aloha uses binary end-to-end success rate, making its performance strictly harder to achieve. Figure 13: ShowUI-Aloha outperforms strong prior GUI agents, including fully autonomous and agentic systems, on OSWorld-style tasks. Human-taught task traces enable consistent improvements in end-to-end task success under user-oriented evaluation protocol, highlighting reliable and scalable pathway toward real-world desktop automation. 100 91 ) % ( R c S 80 60 40 20 65 70 43 38 83 80"
        },
        {
            "title": "Calc",
            "content": "Impress"
        },
        {
            "title": "Writer",
            "content": "Multi-apps OS"
        },
        {
            "title": "VSCode",
            "content": "Figure 12: Aloha OSWorld Task success rate in each category. 4.4 Error Analysis Despite the overall strong performance, Aloha still exhibits several characteristic failure modes. The most frequent error arises from failed or incorrect element selection, particularly in applications such as GIMP and LibreOffice Impress where multiple visually similar icons exist in dense toolbars. Because the underlying model receives only limited textual or structural descriptions of each icon, it occasionally confuses semantically related actions. Enhancing visual-text alignment and incorporating richer 11 metadata could mitigate this ambiguity. Many failures arise from the agents imprecise drag-selection when editing text. Without semantic awareness of text boundaries, the motor-level controller often selects too little, too much, or nothing at all before deleting or typing. These slight inaccuracies lead to leftover characters, malformed inputs, and incomplete value replacement, revealing the sensitivity of text manipulation to precise cursor-drag behavior. These failures show that while Aloha reliably executes most deterministic GUI workflows, its remaining weaknesses center on fine-grained element localization and precise drag-based text editing. Addressing these challenges will require stronger icon-level semantic grounding and more adaptive recovery mechanisms for subtle selection and editing drift. Breakdown of Failure Modes in Unsuccessful Trials Element localization dominates the error distribution, revealing the clearest opportunity for targeted improvement. Failure Mode Element localization errors Text / field editing errors Misaligned action execution Stalled or looping trajectories Other failures Share 53.5% 16.0% 14.6% 8.3% 7.6% Percentages computed across 144 failed trials. 53.5% 7.6% 8.3% 16.0% 14.6%"
        },
        {
            "title": "Element localization\nText editing\nMisaligned actions\nTrajectory stalls",
            "content": "Other Figure 14: Breakdown of Failure Modes in Unsuccessful Trials 4.5 Ablation Study To assess the contribution of core components in Aloha, we conduct an ablation study on representative subset of 30 OSWorld tasks evenly covering all ten application categories. Each variant uses the same evaluation protocol and environment as the main experiments. We focus on two major factors: (1) the role of human demonstration traces (TeachTrace), and (2) the role of the planners temporal memory (PlannerMemory), which conditions each action on both the current step and the sequence of previous decisions. For the planner ablation, we disable all contextual functions, reducing the agent to one-step decision-maker with no accumulated task history. Impact of Human Teaching and Planner Memory Ablation on 30-task OSWorld subset shows both components are critical for stable long-horizon execution. Variant Success Rate (%) Step-Norm Full (ours) TeachTrace PlannerMemory 63.3 36.7 50.0 0.89 0.6 0.7 Success Rate is exact task completion; Step-Norm is mean normalized progress (ReachedStep / TraceStep) across 30 OSWorld tasks. Figure 15: Impact of Human Teaching and Planner Memory. Removing the human TeachTrace produces the largest degradation, dropping success from 63.3% to 36.7% and reducing normalized progress from 0.89 to 0.56. This confirms that demonstration-driven procedural grounding is the primary contributor to Alohas reliability on multi-step OSWorld tasks. Disabling PlannerMemory also yields substantial decline (50.0% success, 0.68 Step-Norm), indicating that temporally aware planning is essential for maintaining task-state consistency and avoiding drift 12 in longer sequences. Taken together, the ablations show that Alohas improvements arise from the combination of human-taught trajectories and memory-equipped plannerneither component alone is sufficient for robust, generalizable GUI automation. Figure 16: Additional examples of Aloha executing complex real-world tasks. From left to right: (1) automated air-ticket booking involving multi-step UI navigation and structured form filling; (2) advanced Excel operations such as matrix transposition and cell-range manipulation; and (3) batch editing of slide backgrounds in PowerPoint. These diverse tasks demonstrate Alohas ability to generalize beyond simple click-and-type patterns and reliably follow high-level workflows across heterogeneous applications."
        },
        {
            "title": "5 Conclusion",
            "content": "This work presents ShowUI-Aloha, practical framework that converts raw human desktop demonstrations into structured, executable trajectories for GUI agents. Aloha integrates lightweight cross-platform recorder, learner that distills interaction logs into intent-aligned traces, and planneractor system that uses temporal memory to operate low-level OS actuatorsenabling the agent to learn via abstraction rather than memorization. Evaluated on broad set of OSWorld-style tasks, Aloha demonstrates robust multi-step execution across diverse real-world desktop applications. single demonstration often generalizes to an entire task group sharing the same workflow logic, and ablations confirm the importance of both human-derived traces and temporally conditioned planning. Limitations. Remaining challenges include fine-grained icon disambiguation, noise-sensitive drag-based text selection, and reliance on at least one demonstration per workflow family. Future Work. Expanding task-group coverage, improving icon-level and text-structure understanding, and scaling toward few-shot or demonstration-free generalization are promising directions. Turning Alohas structured traces into compact visionlanguageaction models may further reduce runtime dependence on demonstrations. Because ShowUI-Aloha is fully open-sourced, researchers can readily swap VLMs, actuators, or planners, or reuse individual components such as the recorder. We hope Aloha serves as foundation for developing demonstration-driven, memory-aware GUI agents capable of reliable operation in complex desktop environments."
        },
        {
            "title": "References",
            "content": "[1] Tamer Abuelsaad, Deepak Akkil, Prasenjit Dey, Ashish Jagmohan, Aditya Vempaty, and Ravi Kokku. Agent-e: From autonomous web navigation to foundational design principles in agentic systems, 2024. [2] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164, 2024. [3] Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. Agent s2: compositional generalist-specialist framework for computer use agents. arXiv preprint arXiv:2504.00906, 2025. [4] Anthropic. Claude opus 4 & claude sonnet 4 system card. Technical report, Anthropic, 2025. [5] Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. [6] Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, and Lichao Sun. Gui-world: video benchmark and dataset for multimodal gui-oriented understanding, 2025. [7] Wentong Chen, Junbo Cui, Jinyi Hu, Yujia Qin, Junjie Fang, Yue Zhao, Chongyi Wang, Jun Liu, Guirong Chen, Yupeng Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [8] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024. [9] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [10] Antonino Furnari, Sebastiano Battiato, Kristen Grauman, and Giovanni Maria Farinella. Next-active-object prediction from egocentric videos. Journal of Visual Communication and Image Representation, 49:401411, November 2017. [11] Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, Peiyi Wang, Xiangwu Guo, Hengxu Wang, Luowei Zhou, and Mike Zheng Shou. Assistgui: Task-oriented pc graphical user interface automation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1328913298, June 2024. [12] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. [13] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use, 2024. [14] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem AlShikh, and Ruslan Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pages 161178. Springer, 2024. [15] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: large language model-based web navigating agent, 2024. [16] Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang. Sheetcopilot: Bringing software productivity to the next level through large language models, 2023. [17] Jiaxiang Li, Siliang Zeng, Hoi-To Wai, Chenliang Li, Alfredo Garcia, and Mingyi Hong. Getting more juice out of the sft data: Reward learning from human demonstration improves sft for llm alignment, 2024. [18] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use, 2025. [19] Yanda Li, Chi Zhang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, and Yunchao Wei. Appagent v2: Advanced agent for flexible mobile interactions, 2024. [20] Kevin Qinghong Lin, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan Wang, and Mike Zheng Shou. Videogui: benchmark for gui automation from instructional videos. arXiv preprint arXiv:2406.10227, 2024. [21] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [22] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. 14 [23] Guangyi Liu, Pengxiang Zhao, Liang Liu, Zhiming Chen, Yuxiang Chai, Shuai Ren, Hao Wang, Shibo He, and Wenchao Meng. Learnact: Few-shot mobile gui agent with unified demonstration benchmark. arXiv preprint arXiv:2504.13805, 2025. [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [25] Dang Nguyen, Jian Chen, Yu Wang, Gang Wu, Namyong Park, Zhengmian Hu, Hanjia Lyu, Junda Wu, Ryan Aponte, Yu Xia, et al. Gui agents: survey. arXiv preprint arXiv:2412.13501, 2024. [26] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. [27] Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale, 2024. [28] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [29] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. [30] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. [31] Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, Ran Xu, and Caiming Xiong. Coact-1: Computer-using agents with coding as actions. arXiv preprint arXiv:2508.03923, 2025. [32] Ddac Surs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1188811898, 2023. [33] Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Tucker Balch, and Manuela Veloso. Adaptagent: Adapting multimodal web agents with few-shot learning from human demonstrations, 2024. [34] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [35] Shuai Wang, Weiwen Liu, Jingxuan Chen, Yuqi Zhou, Weinan Gan, Xingshan Zeng, Yuhan Che, Shuai Yu, Xinlong Hao, Kun Shao, et al. Gui agents with foundation models: comprehensive survey. arXiv preprint arXiv:2411.04890, 2024. [36] Zifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, and Li Yi. Genh2r: Learning generalizable human-to-robot handover via scalable simulation, demonstration, and imitation, 2024. [37] Qinchen Wu, Difei Gao, Kevin Qinghong Lin, Zhuoyu Wu, Xiangwu Guo, Peiran Li, Weichen Zhang, Hengxu Wang, and Mike Zheng Shou. Gui action narrator: Where and when did that action take place?, 2024. [38] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [39] Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. [40] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Jing Hua Toh, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2025. [41] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction, 2025. [42] Shuo Yang, Wei Zhang, Weizhi Lu, Hesheng Wang, and Yibin Li. Learning actions from human demonstration video for robotic manipulation, 2019. [43] Yan Yang, Dongxu Li, Yutong Dai, Yuhao Yang, Ziyang Luo, Zirui Zhao, Zhiyuan Hu, Junzhe Huang, Amrita Saha, Zeyuan Chen, Ran Xu, Liyuan Pan, Caiming Xiong, and Junnan Li. Gta1: Gui test-time scaling agent. arXiv preprint arXiv:2507.05791, 2025. [44] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions, 2024. [45] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [46] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms, 2024. [47] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. [48] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents as smartphone users, 2023. [49] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and Duyu Tang. Android in the zoo: Chain-ofaction-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024. [50] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore"
    ]
}