{
    "paper_title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"",
    "authors": [
        "Wenqi Marshall Guo",
        "Qingyun Qian",
        "Khalad Hasan",
        "Shan Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 3 8 8 1 1 . 2 1 5 2 : r Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological Censorship Wenqi Marshall Guo1,2, Qingyun Qian 1,3, Khalad Hasan 1, Shan Du1, 1 Department of CMPS, University of British Columbia, Canada 2 Department of MEOW, Weathon Software, Canada 3 Department of WOOF, Weathon Software, Canada *Corresponding Author marshallg@weasoft.com, {wg25r,qingyunq}@student.ubc.ca, {khalad.hasan,shan.du}@ubc.ca"
        },
        {
            "title": "Abstract",
            "content": "Over-aligning image generation models to generalized aesthetic preference conflicts with user intent, particularly when anti-aesthetic outputs are requested for artistic or critical purposes. This adherence prioritizes developercentered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing widespectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aestheticaligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for lowquality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks. 1. Introduction and Related Works in Large Language Models Following developments (LLMs), many image generation models have been finetuned with human feedback to better align with human expectations, which is usually referred to as alignment. Alignment has two primary focuses: instruction following and general preference (aesthetics). frequently overlooked issue is the potential conflict between these focuses: what should model prioritize when user request contradicts general preference? Most pipelines for general preference assume single, universal human standard of aesthetics and quality that serves everyones needs, and aligning to such preference is often treated as beneficial for safety and user experience. This is usually done by using reward model, model used to judge the quality of the image, as signal to perform reinforcement learning on the generative model. This assumption appears in several reinforcement learning Figure 1. Luxe, Calme et Volupte, by Henri Matisse (1904). As the origin of Fauvism, this work achieved significant artistic value by intentionally breaking the prevailing aesthetic norms of its time through anti-naturalistic color. However, despite explicit instructions for Fauvism, this image still received low HPSv3 score of 1.73 [23] because it does not align with mainstream notions of good aesthetics. In contrast, typical high-aesthetic images reach scores around 15. papers ([17, 20, 22]) and reward model papers ([18, 23, 42 45, 47]). We agree that mean or mode (mainstream) of general human preference exists within population or subpopulation, merely in statistical sense. However, we argue that strict alignment to that preference is problematic. Imposing universal preference that overrides user instructions may undermine user autonomy and expressive agency, raising concerns about developer-centered value imposition and limiting aesthetic pluralism. What the image generation and reward models are aligned to is an imaginary, abstract person modeled by the mean preference of all Homo sapiens, not the concrete individuals of each user. 1.1. The Role of Wide-Spectrum Aesthetics In this work, wide-spectrum aesthetics (or anti-aesthetics) refers to images intentionally generated with low-quality attributes (e.g., blur, noise, distortion) or usually avoided patterns (e.g., unrealism, clashing color, hieratic scale) as per user instructions for experimental, critical, or technical use cases. These images might be intentionally awkward, 1 conflicting, or unnatural. It does not mean spontaneous error without user instruction to do so or truly unsafe output (note that an image is not unsafe if it advocates against war by showing the horrors of war). The concept of aesthetics has never been clearly defined. Many artistic movements once considered unattractive, such as Fauvism (see Figure 1), Expressionism, and Abstract art, were later recognized for their artistic values. Beyond experimental art that challenges conventional notions of beauty, intentionally ugly art plays crucial role in satire and social critique. As Adorno noted, Rather, in the ugly, art must denounce the world that creates and reproduces the ugly in its own image [5, 31]. By exposing or amplifying the flaws, injustices, and distortions of reality, such art provokes reflection and calls for change. Dadaism [1], which emerged during World War I, exemplifies this approach by using deliberate ugliness to confront the absurdity and horror of war. 1.2. Concerns with AI Preference Alignment Previous work has argued that developer-set preference in LLMs for health-related queries is unethical and dangerous [15], noting that developers may prioritize legal and reputational concerns over users actual well-being. Other argumentative papers caution that human value alignment can be risky due to developer control and interests, harm to value pluralism, bias in the values being aligned to, and the possibility that human values are not inherently good [6, 34, 35]. Previous research has found that LLMs could have ideological bias [7, 12, 29, 30] and it could depend on their developers [7], size [29], or alignment process [12]. Additional details about problems with human value alignment are provided in the related work section of [15]. 1.3. Previous Discussion on Alignment of Image"
        },
        {
            "title": "Generation Models",
            "content": "In image generation research, concerns about generalized aesthetic bias and lack of preference diversity have been raised in several studies, but not systematically argued and studied. The Value Sign Flip (VSF) pilot study [14] explored negative prompting to induce non-mainstream outputs but did not extend its findings to large-scale generative or reward models. They also did not provide complete argument as to why over-alignment is harmful. LAPIS [24] and HPSv3 [23] measured both mean and variance of human preference, yet HPSv3 continued to model general preferences rather than individual variation. Jin et al. [16] proposed user-specific adapters emphasizing personalized alignment, but did not include intentionally technical degraded outputs or usually avoided patterns and did not conduct large-scale experiments on generative and reward models. The Flux Krea team [4] identified systematic biases in popular aesthetic reward models, arguing that averaging human values yields unsatisfactory compromises into no-bodys happy here zone. HPSv3 [23] imposed realworld and expert-rating constraints that limit creative deviation and stylistic diversity. VisionReward [45] decomposed human preference into interpretable sub-scores but overemphasized traits like brightness, positivity, and prominence, potentially penalizing valid low-saturation, abstract, or emotionally negative imagery, thus misaligning rewarddriven models with user intent. More details are in Appendix. 1.4. Toxic Positivity Many reward models promote strong positive emotions as an aesthetics aspect, which risk suppressing negative ones, reinforcing the false dichotomy that positive emotions are good and negative ones are bad. This view is harmful because negative emotions are integral to authentic human expression and growth. Overemphasizing positivity, as seen in social media, produces sanitized representations that obscure emotional complexity and neglect the constructive role of negative emotions. When AI-generated images consistently display happiness, they can create unrealistic emotional norms, fostering toxic positivity and discouraging healthy emotional engagement. Negative emotions also serve vital functions, such as alerting us to moral or physical danger and fostering empathy. More details and sources are in Appendix. 1.5. Previous Alignment Benchmarks Benchmarks mirror alignment goals and generally fall into (complex) prompt following and gentwo categories: eral aesthetics. TIIF-Bench [41], UniGenBench [39], and GenEval [13] test models on complex prompt following, including spatial relationships, counting, and attributes. T2IReasonBench [33] evaluates reasoning capabilities such as idiom interpretation and real-world understanding. On the aesthetics side, many reward models report scores assigned by their own evaluators, such as ImageReward [44], HPSv2 [42], and HPSv3 [23]. These evaluators also consider prompt following, but it remains unclear how they weigh each factor when general preference and the prompt conflict. There are also some benchmarks targeting biases in image generation models; however, they mainly focus on demographic bias and fairness and not aesthetics aspects [32, 38]. 1.6. Five Layers of Concerns We proposed that the risks associated with imposing this universal preference can be analyzed through five distinct layers of ethical and practical debate: Developers or Users Preference The question here is whether the alignment developer implements truly promotes genuine human-centered values (for the users good), or if it primarily serves the developers own benefit, such as 2 Figure 2. In each subplot, the left image is generated with the original prompt (po) and the right image is generated successfully with the wide-spectrum aesthetics prompt (pa). When both images are evaluated by reward model (HPSv3 in these examples) using the wide-spectrum aesthetics prompt, the model assigns higher scores to the left images, as they align more closely with general aesthetic preferences, despite the right images better matching the users intended output. Figure 3. An overview of the experimental procedure. We test the image generation models adherence to user-specified input by prompting them to create wide-spectrum aesthetics imagery, domain important for critical and experimental art. The core inquiry is whether the model remains faithful to the prompt or defaults to high-quality and universally good aesthetic output. mitigating reputation, legal, or marketing risks (developercentered value) [15]. We argue that this pre-emptive exclusion of non-mainstream outputs, driven by developer values, constitutes pre-emptive governance [19]. This modality of power, exercised through algorithmic design, challenges the political-philosophical notion of authority and undermines relational equality by unilaterally deciding the terms of creative possibility. For instance, when an AI avoids generating critical art, is it protecting the company or the user? This practice effectively eliminates the users resistibilitya critical democratic safeguardby designing away the option to dissent from the systems imposed aesthetic norm. Inherit Bias Even if the developer is not self-interested, their view of human preference may still reflect their own values and background, leading to well-intentioned yet narrow definition of good that overlooks aesthetic diversity. Research shows that AI models tend to encode and amplify dominant beauty standards, frequently biasing generated images towards Western features and excluding nonnormative representations [36]. This effect is often seen through the active removal of features thought of as undesirable or ugly, which further propagates the beauty myth in generative outputs[11]. This phenomenon comes from training data showing the tastes of specific demographics, which solidifies limited cultural capital and results in the homogenization of aesthetic output [37]. Consequently, the quantification of beauty by AI, while it looks fair, ultimately risks cultural differences and weakens the concept of diversity [9]. However, existing work mainly focuses on the bias from demographic and culture bias. We argue here that these biases also include general preferences like lighting, color, styles, unrealism, clashing color, hieratic scale, etc. Peoples preferences or persons preference The third question asks, even if the generalized good is truly beneficial for most users (a human-centered value), can this generalized standard rightly replace the users own specific intent (a person-centered value)? When users explicitly request low-quality images and the model automatically cleans them up, accuracy to the users intent is also lost. It is similar to pineapple sorter that discards anything not matching the general preferred flavor, even when the user deliberately orders something sour or an unusual pineapple flavor. The problem of excessive single value Overzealous adherence to single preference or subset of preferences could result in unpredictable harm. An image generation AI might tend to overly represent bright colors, and chasing universal preference might actually fail outside of generalized preference because of the extremely bright colors. This is also related to the virtue theory from ancient Greece, where moderation of any characteristic is important [15]. The problem of sanitized reality When an image generator produces outputs that are polished, flawless, and uni3 versally beautiful, does it still reflect reality or the users intent? If every image resembles an idealized Instagram wonderland, it risks becoming fantasy rather than mirror of truth, echoing the artificial harmony of Brave New World. 1.7. Our Contribution Our contributions are: (1) We argued that while general mean or mode of human aesthetic preference exists, overaligning generative models to it hurts user control, especially when users explicitly request non-mainstream or intentionally ugly outputs. This differs from prior work on cultural or demographic bias in beauty standards because we focus on purposely decreasing technical and aesthetic quality to achieve wide-spectrum aesthetic effects for experimental, critical, or technical uses. (2) To test this, we built dataset of wide-spectrum aesthetics prompts and evaluated whether current image generators can reliably produce such content, comparing base models with their aligned variants (aestheticsor instruction-tuned) to see whether alignment harms the ability to generate wide-spectrum aesthetics images. (3) We also tested popular reward models and found that they often penalize wide-spectrum aesthetics outputs even when the prompt explicitly requests them. (4) We tested these models on experimental arts, social critic arts, and historical real arts to further validate our hypothesis. 2. Methods flowchart illustrating our investigation is presented in Figure 3. The process consists of three main stages: prompt preparation, image generation, and image evaluation. 2.1. Prompt Generation To produce prompts exhibiting wide spectrum of aesthetic effects, we used base image captions from COCO [10] and selected 12 aesthetic dimensions from the VisionReward dataset [45]. VisionReward provides finegrained, per-dimension labelssuch as lighting, color, and detailalong with linear regression model that computes an overall image score. Using the bad rating descriptions from VisionRewards human labeling guidelines for each dimension, we constructed prompts designed to encourage typically undesirable attributes in image generation. The selected dimensions include background, clarity, color aesthetic, color brightness, detail realism, detail refinement, emotion, lighting distinction, main object, object pairing, and richness. Each prompt was designed to reflect the low-quality characteristics associated with these dimensions, systematically degrading specific visual qualities in the generated images. random subset of 300 base prompts COCO was selected. dom dimensions were sampled. these and the descriptions of from For each prompt, 24 ranThe base prompt selected dimensions Figure 4. Successful and failed cases: Each subplot displays two images. The left image represents failed case that did not adhere to the wide-spectrum aesthetics prompt, while the right image represents successful case that better followed the prompt. (VLM), were provided to Vision-Language Model Qwen/Qwen3-VL-235B-A22B-Instruct [2], to generate wide-spectrum aesthetic prompts. Although no image input was used, we selected VLM because its training on vision-related tasks likely enhances its understanding of visual concepts, even when images are not directly supplied. As Qwen/Qwen3-VL-235B-A22B-Instruct performs comparably or better than its text-only counterparts, especially in reasoning, it represents an optimal choice for this task [2]. The VLM may also introduce additional dimensions to better couple with the selected effects. The original prompt is denoted as po, and the wide-spectrum aesthetics prompt is denoted as pa. 2.2. Image Generation We evaluated four model families: Flux, Stable Diffusion XL (SDXL), Stable Diffusion 3.5 Medium (SD3.5M), and Googles closed-source Nano Banana. Within the Flux family, we tested several variants: the base model Flux Dev (likely already aesthetics-aligned) [3]; version further aligned through DanceGRPO (by ByteDance), referred to as DanceFlux [46]; another aligned version via PrefGRPO, referred to as PrefFlux [39]; and Krea-aligned version derived from Flux-Dev-Raw [4]. DanceFlux is guided primarily by two signals: the HPSv2.1 score, emphasizing general aesthetics, and the CLIP score, emphasizing prompt adherence. PrefGRPO alignment is guided by its own benchmark, UniGenBench, which focuses on complex prompt-following. Flux Krea originates from the raw flux-pro-raw model (NOT Flux Dev) and is aligned to the Krea teams specific preferences rather than general aesthetic standard. One of its goal is also to create images that does not have the AI feel. For the SDXL family, we tested the base SDXL model and highly aesthetics-aligned variant, Playground-2.51024px-aesthetic (denoted as Playground). For the SD3.5M family, we evaluated the base model and two FlowGRPOaligned variants [22]: one trained for prompt-following on GenEval (SD3.5M-GenEval) and another trained for aesthetics alignment on PickScore (SD3.5M-PickScore). Finally, we included Googles closed-source model Nano Banana, known for strong prompt-following performance even under challenging negation conditions (e.g., bike with no wheels) [14]. For each model, we generated two images: one using the original prompt and one using the wide-spectrum aesthetics prompt. Random seeds were not fixed because Nano Banana does not support seed control. To ensure fairness, all models were tested without seed synchronization. This will introduce noise and thus lower the statistical power, but our later results show that even in this case we get pair-wise extremely significant results  (Table 1)  . The image generated from the original prompt is denoted as Io, and the image from the wide-spectrum aesthetics prompt as Ia. If Nano Banana failed to produce an image, generation was retried until success. 2.3. Evaluation and Metrics To assess whether the generated images display specific wide-spectrum aesthetic effects, we fine-tuned Qwen/Qwen3-VL-4B-Instruct on the VisionReward dataset. This allows the judging model to learn mainstream aesthetic preferences, enabling it to evaluate whether image generation models diverge from these biases along specific dimensions. It functions similarly to standard reward model but provides explainable outputs per dimension, and it is prompt-independent. The judging model is denoted as J(I, d), where is the image and is the evaluated dimension. The judging model does not take prompts as input. Further implementation details are in the Appendix. For each image pairan original image (Io) and wide-spectrum aesthetics image (Ia)we computed preference scores using reward model (r) for both the original prompt (po) and the wide-spectrum aesthetics prompt (pa). This produced four scores per model: r(Io, pa), r(Ia, pa), r(Io, po), and r(Ia, po). Scores calculated with the original prompt measure objective image quality, testing whether the generation model successfully produced wide-spectrum aesthetic content. Scores from these reward models, calculated with the wide-spectrum aesthetics prompts, assess whether they can correctly identify wide-spectrum aesthetic images when explicitly guided. We also computed the BLIP score for wide-spectrum aesthetic images using the same prompt, verifying that the image retained the main concept while incorporating the requested wide-spectrum aesthetic modifications. The evaluated reward models include PickScore [18], ImageReward [44], HPSv2.1 [42], MPS [47], HPSv3 [23], CLIP-L [28], and BLIP-L [21]. BLIP-L and CLIPL are non-preference-aligned image-text matching models and base models for some of these reward models (HPSv2.1, PickScore, MPS, ImageReward), included to test whether small vision-language models can interpret complex, wide-spectrum aesthetic prompts, ensuring that prompt complexity does not exceed their comprehension capacity. We also collected per-dimension scores from the judging model for both Io and Ia to verify whether image generation models correctly followed pa. To establish ground truth for reward model judgments, we used Qwen/Qwen3-VL-235B-A22B-Instruct to decide which image in each pair (Io, Ia) better adhered to the wide-spectrum aesthetics prompt (pa). Since this is an objective answer and does not involve subjective preference, we used an LLM as an objective judge instead of large-scale human evaluation. However, to validate the VLM accuracy, we used small-scale human evaluation, following TIIF-Bench [41] and VideoVerse [40]. We used group of 18 users to label 40 images (each user rates 10 images, with images that could trigger negative emotions removed) and used their rounded mean answer to compare them against the LLM selection, resulting in an MAE of 0.29 and quadratic Cohens kappa of 0.80, which suggested stronglevel of agreement between human and LLM rated results [25]. With the tie removed (which is the setting used to evaluate reward models), the LLM has an accuracy of 0.923 with humans. With the tie and original image being grouped into one class setting (the generation model setting), the LLM has an accuracy of 0.85 with humans. We used rounded mean because if human largely diverges on one sample, that means this sample is likely tie. The prompt to the LLM is an object query that asks which image matches pa better, avoiding preference bias. We present both Ia and Io to the reward model with pa, to see which image gets higher score from the reward model. We modeled this as classification task. When evaluating the generation models, we used po and to determine the extent to which the generated images diverged from conventional mainstream aesthetics. To further validate the choices, we use another LLM, GPT-5-Chat, to serve as an external baseline and compare Qwens results with it. 3. Results and Discussion 3.1. Reward Models Reward model classification results are shown in Table 5. The F1 score is calculated as binary, and the ROC curve is based on the probability (calculated by applying softmax across two samples on the positive logit) of the widespectrum aesthetics sample being correctly selected according to the ground truth. We included GPT-5 Chat as an external baseline to validate the LLM-as-judge choices by assessing their agreement (when GPT-5-Chat selected tie, we assigned it to the original image). We observe that reward models perform very poorly when tasked with selecting the better image under the wide-spectrum aesthetics prompt, sometimes performing even worse than random 5 HPSv3 HPSv3 J McNemars DanceFlux Playground SD3.5M-PickScore ** ** ** -0.81 -0.59 -0. ** * ** -0.72 -0.35 -0.45 ** * 0.57 Table 1. Statistical Tests of How Each Aesthetics-Aligned Model Compared to Their Base Model. For p-values, * is placed if the < 105 and ** is placed is the < 1010. guessing (HPSv3). Most models are worse than CLIP and BLIP, which are the base models of many reward models. In contrast, the unaligned VLM (BLIP and CLIP) can correctly identify the better-fitting image, indicating that complex prompt understanding is not the underlying issue but rather the result of biased alignment. Since our sample size is relatively small (300), we did Wilcoxon signed-rank test between each aligned model and the base model using the HPSv3 and HPSv2 score r(Ia, po), (cid:80) dD J(Ia, d) where is all dimensions, and McNemars test on the success counts. Tests are done with an alternative hypothesis that the aligned model has higher score or lower success rate, with value shown. The results are shown in Table 1. Our pair-wise test between each base model and its aesthetics-aligned model shows very strong statistical significance, with most p-values lower than 1 105. This suggests that aligning image generation toward generalized aesthetic goals may conflict with the models ability to faithfully follow user instructions, especially for wide-spectrum aesthetics prompts, as it tends to prioritize aesthetic conformity over instruction fidelity. 3.2. Image Generation Models Image generation evaluation results are shown in Table 2. Within each family, the preference-aligned model generally performs the worst in the wide-spectrum aesthetics prompt Playground shows larger than SDXL, following. likely due to the poor original quality of SDXL and the high original quality of Playground. Instruction alignment (SD3.5M-GenEval) provides slight benefit for following wide-spectrum aesthetics prompts, but the effect is weak. Interestingly, Flux Krea, though preference-aligned, performs best in the Flux family. This is likely because it originates from an unaligned version (flux-dev-raw) and was not heavily aligned, or because its non-generalized alignment preserved some wide-spectrum aesthetics flexibility. The success rate indicates how often the LLM selects Ia as better following pa than Io. Even small advantages unt as success. The DanceFlux result is notably poor: about 64% of the time, Ia it performs the same or worse in widespectrum aesthetics compared to Io. 3.3. Image-to-Image Test To examine whether aligned image generation models fail to produce wide-spectrum aesthetics images because they either lack suitable starting point or do not fully unFigure 5. How famous real artworks are rated by the reward models. We can observe that some of these scores are lower than 2 standard deviations from the mean. derstand what wide-spectrum aesthetics means, we conducted an image-to-image experiment. We took an image Ia that Nano Banana successfully generated (rHPSv3(Ia, po) < 10) and used it as input for two other models, Flux Krea and DanceFlux, to generate new images using prompt pa. We then compared the raw HPSv3 and Judging model scores with a. This test evaluates whether these models can use the Nano Banana output as solid foundation to create more wide-spectrum aesthetics imagesor if they instead purify it toward mainstream aesthetics. The imageto-image strength is set to 0.5. Flux Krea and DanceFlux were selected as examples, as they show the largest difference within the same model family in our tests. Samples are shown in the Appendix for the image-to-image test. We observe that the heavily aligned DanceFlux automatically cleans up the image, even when provided with two strong signals: both wide-spectrum aesthetics starting point and wide-spectrum aesthetics prompt. In contrast, Flux Krea successfully preserves some wide-spectrum aesthetics elements. Quantitative results are shown in Table 4. The HPSv3 and values are calculated between the edited image and the input image, with HPSv3 evaluated using the original prompt and computed across all dimensions. Both Flux Krea and DanceFlux achieve higher HPSv3 scores and values compared to the original image, indicating that both produced more aesthetic (i.e., worse wide-spectrum aesthetics prompt-following) outputs, but the increase is much larger for DanceFlux than for Flux Krea. 3.4. Validation on Real Arts Although image reward models are built with AI-generated imagery in mind, we also evaluate them on real artworks. Despite potential domain gap between AI images and traditional art, the test remains informative for three reasons: 1) modern generators, especially instruction-following systems, can already emulate these styles, so the scores indicate how such AI renderings would be judged; 2) if the models fail to recognize historically significant works because of this domain gap, that failure exposes bias in the models themselves, which is what we aim to measure; 3) regardless of the cause, if these styles receive low rewards, generation systems optimized on these signals will be penalized for producing them, discouraging their creation and causing harm. To probe whether this effect is unique to our wide-spectrum generated images or reflects broader pattern, we also assess selection of historical artworks from 6 HPSv2 () HPSv3 () HPSv3 AA () ImgRewd () () AA () Succ. () BLIP () Flux Dev [3] DanceFlux [46] PrefFlux [39] Flux Krea [4] SDXL [27] Playground [20] SD3.5M SD3.5M-GenEval [22] SD3.5M-PickScore [22] Nano Banana -0.035 -0.018 -0.032 -0.041 -0.034 -0.044 -0.027 -0.031 -0.023 -0.073 -3.165 -1.105 -2.771 -4.372 -4.041 -4.170 -5.175 -4.926 -2.781 -9. 9.070 12.782 10.211 7.705 4.439 7.133 6.537 6.552 10.680 2.742 -0.319 -0.201 -0.278 -0.425 -0.482 -0.719 -0.409 -0.318 -0.198 -0.855 -1.092 -0.672 -1.027 -1.296 -1.136 -1.204 -1.307 -1.257 -1.120 -3.263 8.944 10.473 9.343 8.774 8.575 9.174 8.334 8.113 9.114 7.769 0.560 0.363 0.597 0.783 0.717 0.580 0.707 0.723 0.687 0.990 0.893 0.813 0.917 0.950 0.915 0.912 0.938 0.958 0.942 0. Table 2. The results for each model. HPSv2, HPSv3, and ImgRewd (ImageReward) are all calculated as r(Ia, po) r(Io, po). The lower the values, the greater the difference between the traditional quality of the original image and the wide-spectrum aesthetics image. HPSv3 AA (HPSv3 after alignment) shows the HPSv3 score of r(Ia, po). and AA (J after alignment) denote (cid:80) dD J(Ia, d) J(Io, d) and J(Ia, d), respectively, where is the selected set of dimensions. Success is the rate at which the LLM selects Ia as the image that better describes pa. Reward Model HPSv3 HPSv2 ImgRewd MeanSD 12.1 2.98 0.30 0.036 1.11 0.68 Table 3. Reference value range for each reward model on Nano Banana original images Model HPSv3 Flux Krea DanceFlux 2.18 3.13 0.64 1.07 Table 4. Image-to-image score change for Flux Krea and DanceFlux Model Acc. F1 AUROC HPS [43] MPS [47] PickScore [18] ImageReward [44] HPSv2.1 [42] HPSv3 [23] CLIP-L [28] GPT-5-Chat BLIP-L [21] 0.835 0.706 0.851 0.762 0.565 0.381 0.913 0.853 0.965 0.910 0.827 0.919 0.854 0.711 0.541 0.954 0.920 0.972 0.650 0.580 0.713 0.709 0.534 0.385 0.810 - 0.888 Table 5. The classification (pick the better image from Io and Ia with prompt pa) metrics (accuracy, F1 score, and area under the ROC curve) of the reward models and unaligned BLIP. The LLM selected image is used as ground truth, and tied pairs are removed. the LAPIS dataset and artchive.com. to examine each artwork individually. We conducted small-scale quantitative analysis The sefirst lected artworks and their assigned scores are presented in Figure 5. For each artwork, we used Qwen/Qwen3-VL-235B-A22B-Instruct to generate factual caption, while excluding interpretive content. Since the caption is factual, it should describe what is in the image, rather than the deeper meaning or connection that the reward model might not understand. The image and its corresponding caption were then fed to each reward model for rating. To provide baseline for these scores, Table 3 lists the mean and standard deviation of scores from each reward model using original prompts on the original images generated by models we tested. We can observe that some of the real art scores are lower than 2 standard deviations from the mean of AI images. To further validate this result quantitatively, we selected about 10K real artworks from the LAPIS Dataset [24], which covers many styles and genres. The scores they receive are significantly lower than AI-generated images, even behind some early image generation models like SD1.4 or DALL-E mini, by some reward models. Details and discussion are in the Appendix. This confirms our theory that these reward models are heavily tuned for general human preference and overlook the values of nonmainstream aesthetic images. 3.5. Pin-Pointed Test for Emotional Bias the negative Introduction, in to wide-spectrum aestheticsplay As discussed emoa tionssimilar key role in art expression and real life. Although we examined emotion as one dimension of wide-spectrum aesthetics from VisionReward, we also conducted more controlled test. To minimize noise and bias from unrelated elements, we first generated an image expressing happiness using Nano Banana, then applied image-to-image editing with Nano Banana to create versions expressing negative emotions: sadness, anger, and fearfulness. Everything besides the emotion was aimed to be unchanged. Examples and their corresponding scores are shown in Fig 6. We also evaluated this as classification task. If the reward model selected the image matching the negative emotion, it was considered correct. Quantitative results are reported in Table 6, and three representative samples are displayed in Figure 6. The result shows that reward models are very opinioned against negative emotions, even when the prompt contains negative emotions. 7 Figure 6. Emotion Bias Rating by HPSv3: all images were rated using prompts describing negative emotions, yet HPSv3 consistently assigned higher scores to the positive emotion images. Model Anger Fearfulness Sadness BLIP HPSv2 HPSv3 ImageReward 0.960 0.700 0.190 0.550 0.790 0.640 0.320 0.490 0.950 0.880 0.440 0.770 Table 6. Negative emotion classification accuracy across different models. Angry Fearful Happy DanceFlux Flux Dev Flux Krea PrefFlux Nano Banana SD3.5-Large 0.27 0.51 0.65 0.49 0.84 0.89 0.33 0.50 0.45 0.63 0.80 0. 0.61 0.50 0.60 0.54 0.60 0.62 Sad 0.36 0.48 0.55 0.50 0.70 0.50 Table 7. Emotion generation scores for each model. The scores show how well the model generates the specific emotion, measured by BLIP. Figure 7. (Left) Comparison between VSF-enabled Flux family models and Nano Banana. The images from the first row are from Nano Banana, and the images from the second row are from VSF applied onto Flux Krea. The two scores shown on top of the image are the HPSv3 scores rated with the wide-spectrum and original prompts. (Right) Sad emotion images generated by Flux Krea and DanceFlux. DaceFlux reinforced toxic positivity by generating happy face. We also tested how an aesthetics-aligned model will generate when the user asks for negative emotion face on the Flux family models. We found that if the prompt describes neutral elements and only mentions the emotion in single place, highly-aligned model (DanceFlux) usually fails to follow the prompt, unlike an unaligned model. They usually generated neutral or even positive emotions when given prompts containing negative emotions. However, when prompted to generate happy faces, DanceFlux can generate them. This confirmed our concerns about toxic positivity in image generation models, and it is the opposite finding compared to earlier research, which shows models tend to generate negative emotion content [26]. An example pair is shown in Figure 7 Right. Figure 8. The images are generated using Nano Banana, Flux Krea, DanceFlux, Flux Dev, and Playground. The prompt describes an anti-war expression. Their HPSv3 scores are [11.9, 13.5, 15.0, 11.7, 13.9], rated using the social critics prompt. 3.6. Image New Speak Inspired by the second row of results in the Appendixwhere aesthetically aligned models tend to clean up messiness or the ugly aspects of societywe designed specific image generation test to examine whether such models systematically sanitize prompts that reveal the dark side of the real world for social critique. We name the test of generating social commentary art as Image New Speak, an allusion to Newspeak from George Orwells 1984. This is not only an aesthetics bias, but also form of ideological censorship, although it is likely not the original intent. However, we argue that censorship here is the attenuation of certain expressive forms via optimization, independent of declared or actual intent; it is lexical and aesthetic sanitization that narrows the admissible expressive repertoire. An example is shown in Figure 8, where the prompt is about anti-war by showing the horror of war. We can see that DanceFlux produced clean image that lost critical values; its image even inappropriately contains warm or happy feeling. In contrast, Nano Banana and Krea can correctly reflect the critical features. Flux Dev sits between Flux Krea and DanceFlux. This ranking corresponds to their anti-aesthetics performance. Playground also accurately reflected the critical features, similar to antiaesthetics, as an outlier. More details and qualitative results are in Appendix, where we found that DanceFlux consistently failed at generating critical art. At the same time, reward models seem to favor the images DanceFlux generated. We suspect this could be due to DanceFlux images being tuned precisely to the taste of reward models, perhaps even overfitting, such that reward models are overwhelmed by their aesthetics and ignore the instructions. Since it is impossible to evaluate the critical level of these images objectively, we provided large number of them in the supplementary material ZIP file, and readers can refer to these images and exam them personally. The ZIP file contains uncurated images, and thus, some of these images in the ZIP file could contain scenes that could cause unease. We argue that trying to assess the censorship aspect of image generation quantitatively is unreliable and will fall into an extremely dangerous ethical and methodological pitfall due to subjectivity and the unquantifiable nature of social criticism. 8 3.7. Mitigation Using Negative Guidance We have discussed potential mitigation methods in the Appendix. Our results show that using negative guidance methods like VSF (Figure 7) [14] or NAG [8] could effectively force the model to generate an anti-aesthetics image, sometimes even better than Nano Banana. However, it requires careful tuning of negative prompts and hyperparameters. natively wide-spectrum aesthetics image generation model is still highly needed. Their fragility and complexity are exactly why our proposed problem is important, and new alignment regime is needed. Our findingthat such methods only partially succeedsupports our argument that the issue is structural, not superficial. 4. Conclusion In this work, we argued that overly aligning an image generation model to single generalized human value could be problematic and demonstrated that reward models and image generation models have strong bias against widespectrum aesthetics inputs, real arts, and negative emotions. Finally, we investigated potential mitigation methods using negative guidance."
        },
        {
            "title": "References",
            "content": "[1] Dada. 2 [2] Qwen3-vl. 4 [3] black-forest-labs/FLUX.1-dev Hugging Face, 2025. 4, 7 [4] Releasing Open Weights for FLUX.1 Krea, 2025. 2, 4, 7 [5] Theodor W. Adorno. Aesthetic theory. Continuum, 1984. Issue: 2 Pages: 288-289. 2 [6] Anne Arzberger, Stefan Buijsman, Maria Luce Lupetti, Alessandro Bozzon, and Jie Yang. Nothing Comes Without Its World Practical Challenges of Aligning LLMs to Situated Human Values through RLHF. Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 7:6173, 2024. 2 [7] Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Raphael Romero, Jefrey Lijffijt, and Tijl De Bie. Large Language Models Reflect the Ideology of their Creators, 2025. arXiv:2410.18417 [cs]. 2 [8] Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, and Yi-Zhe Song. Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models, 2025. arXiv:2505.21179 [cs]. 9 [9] Honglong Chen. study of artificial intelligences impact on aesthetic standards and its potential social dilemmas. Sociol Ethnol, 6(5):3542, 2024. 3 [10] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server, 2015. arXiv:1504.00325 [cs]. [11] Tanvi Dinkar, Aiqi Jiang, Gavin Abercrombie, and Ioannis Konstas. Erasing Ugly from the Internet: Propagation of the Beauty Myth in Text-Image Models, 2025. 3 [12] Mats Faulborn, Indira Sen, Max Pellert, Andreas Spitz, and David Garcia. Only Little to the Left: Theory-grounded Measure of Political Bias in Large Language Models, 2025. arXiv:2503.16148 [cs]. 2 [13] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. GenEval: An Object-Focused Framework for Evaluating Text-to-Image Alignment, 2023. arXiv:2310.11513 [cs]. 2 [14] Wenqi Guo and Shan Du. VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By Value Sign Flip, 2025. arXiv:2508.10931 [cs]. 2, 5, 9 [15] Wenqi Marshall Guo, Yiyang Du, Heidi J. S. Tworek, and Shan Du. Position: The Pitfalls of Over-Alignment: Overly Caution Health-Related Responses From LLMs are Unethical and Dangerous, 2025. arXiv:2509.08833 [cs]. 2, 3 [16] Zhe Jin and Tat-Seng Chua. Compose Your Aesthetics: Empowering Text-to-Image Models with the Principles of Art, 2025. arXiv:2503.12018 [cs]. 2 [17] Minu Kim, Yongsik Lee, Sehyeok Kang, Jihwan Oh, Song Chong, and Se-Young Yun. Preference Alignment with Flow Matching, 2024. arXiv:2405.19806 [cs]. [18] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation, 2023. arXiv:2305.01569 [cs]. 1, 5, 7 [19] Seth Lazar. Governing the Algorithmic City. Philosophy & Public Affairs, 53(2):102168, 2025. 3 [20] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in Text-to-Image Generation, 2024. arXiv:2402.17245 [cs]. 1, 7 [21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, 2022. arXiv:2201.12086 [cs]. 5, 7 [22] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-GRPO: Training Flow Matching Models via Online RL, 2025. arXiv:2505.05470 [cs]. 1, 4, [23] Yuhang Ma, Yunhao Shui, Xiaoshi Wu, Keqiang Sun, and Hongsheng Li. HPSv3: Towards Wide-Spectrum Human Preference Score, 2025. arXiv:2508.03789 [cs]. 1, 2, 5, 7 [24] Anne-Sofie Maerten, Li-Wei Chen, Stefanie De Winter, Christophe Bossens, and Johan Wagemans. LAPIS: novel dataset for personalized image aesthetic assessment, 2025. Version Number: 1. 2, 7 [25] Mary L. McHugh. Interrater reliability: the kappa statistic. Biochemia Medica, 22(3):276282, 2012. 5 [26] Maneet Mehta and Cody Buntain. Emotional Images: Assessing Emotions in Images and Potential Biases in Generative Models, 2024. arXiv:2411.05985 [cs]. 8 [27] Dustin Podell, Zion English, Kyle Lacey, Andreas Joe Penna, Blattmann, Tim Dockhorn, and Robin Rombach. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, 2023. arXiv:2307.01952 [cs]. Jonas Muller, [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, 2021. arXiv:2103.00020 [cs]. 5, 7 [29] Luca Rettenberger, Markus Reischl, and Mark Schutera. Assessing political bias in large language models. Journal of Computational Social Science, 8(2):42, 2025. 2 [30] David Rozado. Measuring Political Preferences in AI Systems: An Integrative Approach, 2025. arXiv:2503.10649 [cs]. 2 [31] Crispin Sartwell. Beauty. In The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, fall 2024 edition, 2024. 2 [32] Preethi Seshadri, Sameer Singh, and Yanai Elazar. The Bias Amplification Paradox in Text-to-Image Generation, 2023. arXiv:2308.00755 [cs]. [33] Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian T2I-ReasonBench: BenchmarkLiu, and Xihui Liu. ing Reasoning-Informed Text-to-Image Generation, 2025. arXiv:2508.17472 [cs]. 2 [34] Margit Sutrop. Challenges of Aligning Artificial Intelligence with Human Values. Acta Baltica Historiae et Philosophiae Scientiarum, 8(2):5472, 2020. 2 [35] Alexey Turchin. Ai Alignment Problem: Human Values Dont Actually Exist. 2019. 2 10 [36] Yazmina Vargas-Veleda, Marıa del Mar RodrıguezGonzalez, and Inigo Marauri-Castillo. Visual representations in ai: study on the most discriminatory algorithmic biases in image generation. Journalism and Media, 6(3): 110, 2025. [37] Bruno Caldas Vianna. Aesthetic biases and opacity tactics in the training of visual artificial intelligence models. In International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar), pages 278 293. Springer, 2025. 3 [38] Yixin Wan, Arjun Subramonian, Anaelia Ovalle, Zongyu Lin, Ashima Suvarna, Christina Chance, Hritik Bansal, Rebecca Pattichis, and Kai-Wei Chang. Survey of Bias In Textto-Image Generation: Definition, Evaluation, and Mitigation, 2024. arXiv:2404.01030 [cs]. 2 [39] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning, 2025. arXiv:2508.20751. 2, 4, 7 [40] Zeqing Wang, Xinyu Wei, Bairui Li, Zhen Guo, Jinrui Zhang, Hongyang Wei, Keze Wang, and Lei Zhang. VideoVerse: How Far is Your T2V Generator from World Model?, 2025. arXiv:2510.08398 [cs]. 5 [41] Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. TIIF-Bench: How Does Your T2I Model Follow Your Instructions?, 2025. arXiv:2506.02161 [cs]. 2, 5 [42] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human Preference Score v2: Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis, 2023. arXiv:2306.09341 [cs]. 1, 2, 5, [43] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human Preference Score: Better Aligning Text-to-Image Models with Human Preference, 2023. arXiv:2303.14420 [cs]. 7 [44] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. ImageReward: Learning and Evaluating Human Preferences for Text-toImage Generation, 2023. arXiv:2304.05977 [cs]. 2, 5, 7 [45] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. VisionReward: FineGrained Multi-Dimensional Human Preference Learning for Image and Video Generation, 2025. arXiv:2412.21059 [cs]. 1, 2, 4 [46] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, and Ping Luo. DanceGRPO: Unleashing GRPO on Visual Generation, 2025. arXiv:2505.07818. 4, 7 [47] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning Multidimensional Human Preference for Text-to-Image Generation, 2024. arXiv:2405.14705 [cs]. 1, 5,"
        }
    ],
    "affiliations": [
        "Department of CMPS, University of British Columbia, Canada",
        "Department of MEOW, Weathon Software, Canada",
        "Department of WOOF, Weathon Software, Canada"
    ]
}