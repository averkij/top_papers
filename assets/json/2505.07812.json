{
    "paper_title": "Continuous Visual Autoregressive Generation via Score Maximization",
    "authors": [
        "Chenze Shao",
        "Fandong Meng",
        "Jie Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR."
        },
        {
            "title": "Start",
            "content": "Chenze Shao 1 Fandong Meng 1 Jie Zhou 1 5 2 0 2 2 1 ] . [ 1 2 1 8 7 0 . 5 0 5 2 : r Abstract Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into discrete space, which can introduce significant information loss. To tackle this issue, we introduce Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well generative model approximates the true distribution. Within this framework, all we need is to select strictly proper score and set it as the training objective to optimize. We primarily explore class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https: //github.com/shaochenze/EAR. 1. Introduction Autoregressive large language models (Achiam et al., 2023; Touvron et al., 2023; Team et al., 2023; Bai et al., 2023) have demonstrated remarkable scalability and generalizability in understanding and generating discrete text, which has inspired the exploration of autoregressive generation on other data modalities. However, autoregressive models equipped with cross-entropy loss are limited to handle discrete tokens 1Pattern Recognition Center, WeChat AI, Tencent Inc. Correspondence to: Chenze Shao <chenzeshao@tencent.com>, Fandong Meng <fandongmeng@tencent.com>, Jie Zhou <withtomzhou@tencent.com>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). from finite vocabulary. Therefore, for continuous modalities such as visual data, Visual AutoRegressive modeling (VAR1) typically resort to quantization-based approaches (van den Oord et al., 2017; Razavi et al., 2019; Esser et al., 2021; Yu et al., 2024a) to cast the data into discrete space. Discrete visual representation based on vector quantization provides support for autoregressive generation, yet the primary concern lies in the information loss due to quantization errors. During visual generation, quantization errors degrades the reconstruction quality of discrete image tokenizers, which upper-bounds the generation quality (Rombach et al., 2022). Moreover, discrete representations compromise the models perception of low-level details, restricting its ability to capture continuous variations and subtle differences. Consequently, in terms of visual understanding, the performance of discrete tokenizers often lags behind that of continuous tokenizers (Wu et al., 2024; Xie et al., 2024). Given the limitations associated with vector quantization, there is growing interest in continuous visual autoregressive generation. However, without finite vocabulary, it is generally intractable to explicitly predict the likelihood over continuous spaces and train with likelihood maximization. Prior to this work, autoregressive generation in continuous spaces has been explored through GIVT (Tschannen et al., 2023) and diffusion loss (Li et al., 2024). Nevertheless, the expressive capability of GIVT is confined to the pre-defined family of Gaussian mixtures (Tschannen et al., 2023), and the per-token diffusion procedure necessitates multiple denoising iterations to recover the token distribution, which significantly increases the inference latency (Li et al., 2024). In this work, we introduce Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules (Brier, 1950; Good, 1952; Gneiting & Raftery, 2007), which provide powerful statistical tools capable of evaluating how well generative model approximates the true distribution. Specifically, scoring rules are functions to assess the quality of probability distribution based on the observed sample. scoring rule is considered strictly proper if it encourages the model to make honest predictions. In other words, the expected score 1Here, VAR refers to the autoregressive modeling of visual content, not limited to next scale prediction (Tian et al., 2024). Continuous Visual Autoregressive Generation via Score Maximization is maximized only when model predictions follow the true distribution, and any deviation from the truth will result in decrease in expected score. The intrinsic property of strictly proper scoring rules makes them well-suited training objectives for generative models. prominent example is the cross-entropy loss used in discrete autoregressive models, which corresponds to the maximization of logarithmic score (Good, 1952). Within the Continuous VAR framework, all we need is to select strictly proper score for continuous variables and set it as the training objective to optimize. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from this framework, where they are respectively aligned with the logarithmic score (Good, 1952) and the Hyvarinen score (Hyvarinen, 2005). Under the Continuous VAR framework, we primarily explore class of training objectives based on the energy score (Szekely, 2003), which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. The associated energy loss incentivizes the model to generate samples close to the target label, while maintaining the diversity between independent samples. The model architecture remains largely analogous to discrete Transformer (Vaswani et al., 2017), with the key difference being the substitution of the softmax layer with small MLP generator, both of which transform the hidden representation into distribution. Similar to Generative Adversarial Networks (Goodfellow et al., 2014), the MLP generator is an implicit generative model that takes random noises as additional inputs, and the predictive distribution is implicitly represented by its sampling process. Experiments on the ImageNet 256256 benchmark (Deng et al., 2009) show that our approach achieves stronger visual generation quality than the traditional autoregressive Transformer that uses discrete tokenizer. Compared to diffusionbased methods, our approach exhibits substantially higher inference efficiency, as it does not require multiple denoising iterations to recover the target distribution. 2. Related Work Visual Autoregressive Generation. Early efforts approached visual autoregressive generation by treating the image as sequence of pixels (Gregor et al., 2014; Parmar et al., 2018; van den Oord et al., 2016a;b). To mitigate the expensive cost of autoregressive modeling at the pixel level, van den Oord et al. (2017) introduced the vector quantization technique to represent an image as set of discrete tokens, paving the way for more effective autoregressive image generation with both causal (Razavi et al., 2019; Esser et al., 2021; Ramesh et al., 2021; Yu et al., 2022; Sun et al., 2024a; Tian et al., 2024) and masked Transformers (Chang et al., 2022; Li et al., 2023; Chang et al., 2023). However, the information loss incurred during the quantization process becomes bottleneck for the generation quality. Consequently, recent focus has shifted towards finding better image tokenizer (Yu et al., 2022; Mentzer et al., 2024; Yu et al., 2024a;b; Weber et al., 2024). In parallel, there is growing interest in employing continuous tokenizers for autoregressive image generation, with Gaussian mixture models (Tschannen et al., 2023) and diffusion models (Li et al., 2024) being used to represent the token distribution. Our approach advances this direction by establishing universal framework for predicting continuous tokens. Strictly Proper Scoring Rules. Strictly proper scoring rules, initially introduced in Brier (1950) for the verification of weather forecasts, have since evolved into comprehensive theoretical framework for evaluating probabilistic forecasts. In the realm of deep learning, the most extensively applied scoring rule is the logarithmic score (Good, 1952), which is closely linked with maximum likelihood estimation, cross-entropy loss, and perplexity evaluation. The Brier score is also widely used for training classification networks (Shoemaker, 1991; Hung et al., 1996; Kline & Berardi, 2005; Hui & Belkin, 2021) and evaluating their calibration (Lakshminarayanan et al., 2017; Ovadia et al., 2019; Gruber & Buettner, 2022). Recently, Shao et al. (2024) proposed using scoring rules as the training objective for autoregressive language modeling. In the continuous space, the Hyvarinen score (Hyvarinen, 2005) plays an important role in score matching, which gives rise to score-based diffusion models (Song & Ermon, 2019; Song et al., 2021). The energy score has also been employed in generative modeling, with applications spanning image generation (Bellemare et al., 2018), speech synthesis (Gritsenko et al., 2020), time series prediction (Pacchiardi et al., 2024; Pacchiardi & Dutta, 2022), and self-supervised learning (Vahidi et al., 2024). In this work, we focus on leveraging scoring rules to enable the autoregressive modeling of continuous data. 3. Continuous Visual Autoregressive"
        },
        {
            "title": "Generation",
            "content": "In this section, we begin by introducing the essential background of strictly proper scoring rules. Following that, we present the Continuous VAR framework, which enables continuous visual autoregressive generation via score maximization. For simplicity of notation, we assume setting of unconditional generation, and the conclusion can be extended to conditional generation scenarios. 3.1. Strictly Proper Scoring Rules In statistical decision theory, scoring rules serve as quantitative measures to assess the quality of probabilistic predic2 Continuous Visual Autoregressive Generation via Score Maximization tions, by assigning numerical score based on the predicted distribution and the observed sample x. Let represents the sample space and be the set of probability measures on . scoring rule takes values in the extended real line = [, ], indicating the reward or utility of predicting when sample is observed: S(p, x) : (cid:55) R. (1) The role of scoring rules is to assess whether the prediction honestly represents the underlying sample distribution q. This is reflected in the expected score with respect to q, denoted as S(p, q): S(p, q) = Exq[S(p, x)]. (2) proper scoring rule should encourage the model to make honest predictions. Formally, scoring rule is proper if the expected score is maximized when the model reports true probabilities: S(p, q) S(q, q), p, P. (3) It is strictly proper when the equality holds if and only if = q. The strict propriety means that the score maximizer is unique, where any deviation from the truth will result in decrease in expected score. The study of scoring rules, which dates back to the Brier score (Brier, 1950) for the verification of weather forecasts, has evolved into comprehensive framework offering wealth of useful scores, such as the logarithmic score (Good, 1952), Brier score (Brier, 1950), and spherical score (Roby, 1965). For continuous variables, the choices expand to strictly proper scores such as the energy score (Szekely, 2003), CRPS (Matheson & Winkler, 1976), and Hyvarinen score (Hyvarinen, 2005), as well as proper scores like the kernel score (Eaton, 1981) and Variogram score (Scheuerer & Hamill, 2015). Gneiting & Raftery (2007) provides comprehensive literature review on scoring rules. 3.2. Continuous VAR via Score Maximization The property of strictly proper scoring rules naturally align with the objective of generative models. With loss function that promotes the maximization of strictly proper score, the model will be trained to approximate the data distribution. direct approach is to take the negative of strictly proper score as the loss: LS(p, x) = S(p, x). (4) For example, maximizing the logarithmic score S(p, x) = log p(x) recovers the cross-entropy loss. The emphasis on the strict propriety of the scoring rule is crucial. Unlike strictly proper scores which guarantee unique optimizer, proper but not strictly proper scores results in loss function with multiple potential minimizers, making it challenging for the model to converge to the correct distribution. trivial but telling example is the constant-valued score. While being technically proper, it does not offer meaningful guidance for model training. When dealing with intricate samples like long texts, videos, or high-resolution images, direct generation poses significant challenges, which necessitates breaking down the process into several steps for autoregressive modeling. In this case, the direct calculation of Equation 4 is not always feasible, but we can evaluate scoring rules at each time step to calculate the following sequence loss (Shao et al., 2024): LS(p, x) = (cid:88) t=1 S(p(x<t), xt). (5) The expected loss is minimized only when every expected score S(p(x<t), q(x<t)) is maximized, which consequently encourages honest sequential predictions = q. However, for continuous-valued generative models, the explicit likelihood estimation is sometimes intractable due to the lack of finite vocabulary, which makes the score calculation also infeasible. Under these circumstances, we can adopt an unbiased estimator of Equation 5 as the loss function. Since the expectation of loss remains unchanged, the model will still be trained towards approximating the true distribution. 3.3. Examples Here, we revisit the previous methodologies of continuous visual autoregressive generation, namely GIVT and diffusion loss. We will show that these methods can be derived from the perspective of score maximization, falling within our Continuous VAR framework. Example 1 (GIVT, Tschannen et al., 2023). Generative Infinite-Vocabulary Transformers (GIVT) is perhaps the first visual Transformer that directly generates vector sequences with real-valued entries. The loss function for GIVT is still the cross-entropy, which corresponds to the maximization of the logarithmic score S(p, x) = log p(x). To estimate the likelihood, GIVT employs an invertible flow model (Dinh et al., 2015) to simplify the latent distribution, and then approximates it with Gaussian Mixture Model (GMM). Recently, this methodology has been adapted for speech synthesis (Lin & HE, 2025). The primary limitation of GIVT lies in its expressive capability, which is constrained by the capacity of GMM and its assumption of channel-wise independence. Example 2 (Diffusion Loss, Li et al., 2024). Recently, diffusion loss is proposed to model the per-token distribution by diffusion procedure, which has soon gained widespread 3 Continuous Visual Autoregressive Generation via Score Maximization applications such as video generation (Deng et al., 2024; Liu et al., 2024), text-to-image generation (Fan et al., 2024; Yu et al., 2025), speech synthesis (Turetzky et al., 2024), and multi-modal generation (Sun et al., 2024b). Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020), also known as score-based generative models (Song & Ermon, 2019; Song et al., 2021), require multiple denoising iterations to recover the target distribution, which results in significant inference latency for per-token diffusion procedures. From the perspective of score matching2, one needs to estimate the gradients of the data distribution to reverse diffusion process with Langevin dynamics, which is facilitated by maximizing the Hyvarinen score (Hyvarinen, 2005): S(p, x) = (2 tr(2 log p(x)) + log p(x)2), (6) where tr() denotes the trace of matrix. The expected score is equivalent to the score matching objective up to constant, which shows that the score is strictly proper: S(p, q) = Exq[x log p(x) log q(x)2] + C(q). (7) The diffusion training objective is an estimation of S(p, q) through denoising score matching over multiple noise scales (Vincent, 2011; Song & Ermon, 2019). It implies that the per-token diffusion loss also falls within our Continuous VAR framework with respect to the Hyvarinen score. 4. Energy-based Autoregressive Generation Under the Continuous VAR framework, we develop Energy-based AutoRegression (EAR) approach via maximizing the energy score (Szekely, 2003). The estimation of energy score does not require explicit likelihood estimations but merely the capability to sample from the model distribution. This reduction of constraints facilitates the design of more expressive energy Transformer. Moreover, the energy Transformer is highly efficient in inference, capable of predicting the next continuous token in single forward pass. Further details are elaborated below. 4.1. Energy Loss The energy score is family of strictly proper scoring rules for continuous variables in Rd. To avoid symbol confusion, we denote the samples drawn from the model distribution as x, and the samples from the data distribution as y. Let α (0, 2), the energy score is defined as: S(p, y) = E[x1 x2α] 2E[x yα], (8) 2Please note that the concept of score differs between scoring rules and score matching. In scoring rules, score is measure used to assess the quality of probabilistic predictions. In score matching, score refers to the gradient log p(x). where x1, x2, Rd are independent samples with distribution p. The expected energy score is associated with the generalized energy distance α(p, q) by constant: α(p, q) = 2E[xyα] E[x1 x2α] E[y1 y2α] = S(p, q) + C(q). (9) For α (0, 2), α(p, q) 0 with equality to zero if and only if = (Szekely, 2003; Szekely & Rizzo, 2013), which implies that the energy score is strictly proper. Note that the distance 2(p, q) = E[x] E[y]2 is minimized as long as their expectations match, so the energy score at α = 2 is proper but not strictly proper. The energy score can be unbiasedly estimated using two independent samples x1, x2 drawn from the distribution p. In this way, the energy loss is defined as: L(p, y) = x1 yα + x2 yα x1 x2α, (10) which incentivizes the model to generate samples close to the target label, while maintaining the diversity between independent samples. Notably, the energy loss does not require explicit likelihood estimations but merely the capability to sample from the model distribution, which gives much flexibility in the following architecture design of energy Transformer. 4.2. Energy Transformer Figure 1 illustrates the architecture of energy Transformer, where the energy loss is employed to supervise each autoregressive generation step. The continuous-valued energy Transformer remains largely analogous to discrete Transformer (Vaswani et al., 2017), with the key difference being the substitution of the softmax layer with small MLP generator, both of which transform the hidden representation into distribution. Similar to Generative Adversarial Networks (Goodfellow et al., 2014), the MLP generator is an implicit generative model, whose predictive distribution is implicitly represented by its sampling process. The energy Transformer is designed to accept continuous tokens as inputs. In the embedding layer, the lookup table is replaced with linear projection, which maps each token of size dtoken to dmodel. The representations extracted by Transformer are then mapped to dmlp, serving as inputs for the MLP generator. The MLP generator also takes random noise ϵ as an additional input to perturb the representation for the sampling purpose. The random noise of size dnoise is drawn from uniform distribution ranging from [0.5, 0.5], which is then embedded to size dmlp. The MLP generator consists of few residual blocks that gradually inject noises into the prediction. Each residual block contains two-layer FFN network with SiLU activation (Elfwing et al., 2018), and the noise is incorporated via 4 Continuous Visual Autoregressive Generation via Score Maximization Figure 1. Comparison between the discrete-token standard Transformer and our continuous-token energy Transformer. At the input side, the embedding lookup table is replaced with linear projection. At the output side, the softmax classification layer is replaced with small MLP generator, which takes random noise ϵ as input to perturb the hidden state. adaptive layer normalization (Peebles & Xie, 2023), which perturbs the prediction with shift, scale, and gate layers. Specifically, assuming that the input to the i-th residual block is hi, its output is given by: hi ϵ = (1 + scale(ϵ)) LN (hi) + shif t(ϵ), hi+1 = hi + gate(ϵ) (hi ϵ), (11) where shif t(), scale(), and gate() are linear transformations that interpret the input noise as perturbation signals, LN () represents layer normalization, and () denotes two-layer feed-forward neural network with intermediate dimension of dmlp. Finally, the MLP generator concludes by predicting the next continuous token via linear layer. 4.3. Other Techniques In this section, we present several techniques that have proven effective in improving the visual generation quality of EAR. Temperature. The temperature hyperparameter τ is widely used in the sampling process of generative models, which trades diversity for accuracy. For EAR, we can incorporate temperature hyperparameters during both training and inference, denoted as τtrain and τinfer, respectively. During training, the energy loss is composed of two components: yα and x1 x2α, where the latter measures the diversity of the generated outputs. Therefore, we can assign weight τtrain < 1 to x1 x2α in the energy loss, which can enhance the generation quality with short period of fine-tuning. However, this approach is not applicable for τtrain > 1, as it would cause the loss function unbounded and hackable. During inference, directly modifying the scale of the noise would corrupt the generated images. Instead, we propose to only scale the shif t(ϵ) by temperature τinfer, while keeping scale(ϵ) and gate(ϵ) unchanged. Classifier-Free Guidance. We employ Classifier-Free Guidance (CFG, Ho & Salimans, 2022) to improve the quality of conditional generation. At training time, we replace the condition with dummy token for 10% of the samples. At inference time, the Transformer model is run with both the given condition and the dummy token, providing two outputs hc and hu. The combination of the two outputs = cfg hc + (1 cfg) hu is fed to the MLP generator, where cfg is the guidance scale. Following Chang et al. (2023), we linearly increase the guidance scale during the autoregressive generation. We sweep the optimal guidance scale for each model. Masked Autoregressive Generation. Masked autoregressive models can be regarded as type of autoregressive model that predicts set of unknown tokens based on existing tokens (Chang et al., 2022; Li et al., 2023; Chang et al., 2023; Li et al., 2024). They supports bidirectional attention, which facilitates more effective representation learning compared to causal attention. Consistent with Li et al. (2024), we find that masked autoregressive generation performs better than causal generation. During training, we randomly sample masking ratio in the range of [0.7, 1.0]. During inference, we generate tokens in random order, progressively reducing the masking ratio from 1.0 to 0 fol5 Continuous Visual Autoregressive Generation via Score Maximization Table 1. Model comparisons on ImageNet 256256 conditional generation. Metrics include Frechet Inception Distance (FID), Inception Score (IS), Precision (Pre) and Recall (Rec). or indicate lower or higher values are better. Type Model #Params w/o guidance FID IS Pre Rec FID w/ guidance IS Pre Rec GAN BigGAN (Brock et al., 2019) GigaGAN (Kang et al., 2023) StyleGan-XL (Sauer et al., 2022) Diff AR ADM (Dhariwal & Nichol, 2021) LDM-4 (Rombach et al., 2022) DiT-XL/2 (Peebles & Xie, 2023) L-DiT-7B (Alpha-VLLM, 2024) VDM++ (Kingma & Gao, 2023) VQGAN (Esser et al., 2021) RQ-Transformer (Lee et al., 2022) LlamaGen-3B (Sun et al., 2024a) MaskGIT (Chang et al., 2022) MAGE (Li et al., 2023) MAGVIT-v2 (Yu et al., 2024a) VAR-d30 (Tian et al., 2024) GIVT (Tschannen et al., 2023) MAR (Li et al., 2024) EAR EAR-B EAR-L EAR-H 112M 569M 166M 6.95 3.45 - 554M 10.94 400M 10.56 9.62 675M 5.06 7B 2.40 2B 1.4B 3.8B 3.1B 227M 230M 307M 2.0B 304M 943M 205M 474M 937M 15.78 7.55 - 6.18 6.93 3.65 - 5.67 2. 5.46 3.69 3.16 224.5 225.5 - 101.0 103.5 121.5 153.3 225.3 78.3 134.0 - 182.1 195.8 200.5 - - 227.8 155.9 183.4 204.2 0.89 0.84 - 0.69 0.71 0.67 0.70 - - - - 0.80 - - - 0.75 0.79 0.76 0.77 0.76 0.38 0.61 - 0.63 0.62 0.67 0.68 - - - - 0.51 - - - 0.59 0. 0.57 0.59 0.61 - - 2.30 4.59 3.60 2.27 2.28 2.12 - - 2.18 - - 1.78 1.92 3.35 1.55 2.83 2.37 1.97 - - 265. 186.7 247.7 278.2 316.2 267.7 - - 263.3 - - 319.4 323.1 - 303.7 253.3 273.8 289.6 - - 0.78 0.82 0.87 0.83 0.83 - - - 0.84 - - - 0.82 0.84 0. 0.82 0.81 0.81 - - 0.53 0.52 0.48 0.57 0.58 - - - 0.54 - - - 0.59 0.53 0.62 0.54 0.57 0.59 lowing cosine schedule. By default, we use 64 generation steps in this schedule. Learning Rate for MLP Generator. In our experiments using regular learning rates, we found that the model failed to converge. Given that the Transformer backbone remains unchanged, we hypothesize the MLP generator may need smaller learning rate to ensure its training stability (Singh et al., 2015; Howard & Ruder, 2018; Xu et al., 2025). To address it, we adjust the MLP generators learning rate by applying constant multiplier λ < 1 throughout the training. Empirically, setting λ = 0.25 strikes balance between training efficiency and stability. 5. Experiments 5.1. Settings We evaluate visual generation capability on the classconditional ImageNet 256256 benchmark (Deng et al., 2009). We use Frechet Inception Distance (FID, Heusel et al., 2017) as the main metric, and also provide Inception Score (IS, Salimans et al., 2016) and Precision/Recall (Kynkaanniemi et al., 2019) as secondary metrics. We follow the evaluation suite of Dhariwal & Nichol (2021). We use the decoder-only Transformer architecture following the implementation in ViT (Dosovitskiy et al., 2021) for masked autoregressive generation. The class condition is represented as 64 class tokens at the start of the decoder sequence. We use the discrete VQ-16 tokenizer (Rombach et al., 2022) for the standrard Transformer and the continuous KL-16 tokenizer (Li et al., 2024) for our energy Transformer. The stride of both tokenizers is 16. Following Li et al. (2024), we explore the scaling behavior of Energy-based AutoRegression (EAR) with three sizes of energy Transformer, referred to as EAR-B, EAR-L, and EAR-H, respectively. They respectively have 24, 32, 40 Transformer blocks and width of 768, 1024, and 1280. The MLP generators account for approximately 15% of the parameter size of energy Transformer, which respectively have 6, 8, 12 blocks and width of 1024, 1280, and 1536. The random noise for the MLP generator has size of dnoise = 64, independently drawn from uniform distribution [0.5, 0.5] at each time step. We by default set α = 1 to calculate the energy loss. We train our model for total of 800 epochs, where the first 750 epochs use the standard energy loss and the last 50 epochs reduces the temperature τtrain to 0.99. The inference temperature τinf er is set to 6 Continuous Visual Autoregressive Generation via Score Maximization Figure 2. The speed/quality trade-off for EAR and MAR. The number of autoregressive steps is fixed at 64. For MAR, we vary the number of diffusion steps (10, 20, 25, 30, 40, 50) to generate outputs under different inference latencies. For EAR, the curve is obtained by using different model sizes (EAR-B, EAR-L, EAR-H). The inference time is measured on single A100 GPU. 0.7. Our models are optimized by the AdamW optimizer (Loshchilov & Hutter, 2019) with β1 = 0.9, β2 = 0.95. The batch size is 2048. The learning rate is 8e-4 and the constant learning rate schedule is applied with linear warmup of 100 epochs. We use weight decay of 0.02, gradient clipping of 3.0, and dropout of 0.1 during training. Following Peebles & Xie (2023); Li et al. (2024), we maintain the exponential moving average of the model parameters with momentum of 0.9999. 5.2. Main Results In Table 1, we compare EAR with popular image generation models, including GANs, diffusion models, and VQbased autoregressive models. Notably, EAR-B obtains strong FID of 2.83 with only 205M parameters, and EAR-H achieves competitive FID of 1.97, while maintaining relatively modest model size among the leading systems. The scalability of EAR suggests that the generation quality could be further boosted through scaling. Our approach is most closely aligned with MAR (Li et al., 2024), as they both model the distribution of continuous tokens with an MLP module on top of masked autoregressive Transformer. They can both be viewed as instances under the Continuous VAR framework, with EAR and MAR maximizing the energy score and the Hyvarinen score, respectively. Figure 2 illustrates the inference latency (average time to generate an image) and generation quality (measured by FID) of the two methods. EAR is significantly more efficient in inference, capable of producing high-quality image in roughly 1s, while MAR takes nearly 10 times longer to produce images of comparable quality. The efficiency advantage stems from the difference in probabilistic modeling. Trained with the diffusion loss, MAR necessitates multiple 7 Figure 3. FID curves of the continuous-valued energy Transformer (205M) and the discrete-valued standard Transformer (196M). The guidance scale is 3.0. denoising iterations to recover the target distribution. Conversely, the energy-style supervision enables EAR to make predictions within single forward computation. The continuous tokenizer we use exhibits strong reconstruction quality of 1.22 FID. In contrast, VQ tokenizer with the same model architecture only achieves reconstruction FID of 5.87 (Rombach et al., 2022), which could become bottleneck in generation quality. In Figure 3, we compare our continuous-valued energy Transformer with discrete-valued Transformer that uses the VQ tokenizer. The results show that continuous tokenization with the energy loss consistently outperforms discrete tokenization with the cross-entropy loss, highlighting the great potential of Continuous VAR. For causal autoregressive modeling, our findings align with Li et al. (2024) that both continuousand discrete-valued Transformers can only achieve FID scores around 20. We hypothesize that causal models may suffer from overfitting due to the absence of random masking mechanism during traininga key feature of masked autoregressive modeling that enhances generalization. 5.3. Importance of Being Strictly Proper In the energy loss presented in Equation 10, the exponential coefficient α was empirically set to 1 in previous experiments. While any choice of α (0, 2) ensures the strict propriety, energy losses with α < 1 invariably induce rapid training collapse due to gradient instability. For example, the gradient of x1 x2α can be expressed as 1xi x1x22α (xi x1x2α 2) . When trainθ ing begins, independent samples x1 and x2 are typically nearly indistinguishable, which causes the denominator x1 x22α to approach zero exponentially faster than the numerator when α < 1, resulting in unbounded gradient magnitudes that destabilize optimization. = (cid:80)n 1xi 2) θ α(xi i=1 Continuous Visual Autoregressive Generation via Score Maximization Additionally, while the energy score remains proper at α = 2, its non-strict propriety (only expectation alignment Ep[x] = Eq[y] is enforced) proves insufficient for effective training. As evidenced in Table 2, training with α = 2 fails to generate meaningful contents (FID > 100), whereas leveraging strictly proper energy scores with α [1, 2) achieves decent generation quality. It validates our claim on the importance of being strictly proper for scoring rules, which guarantees unique global minimum. Table 2. The performance of EAR-B when being trained with different exponential term α in the energy loss. The number of training epochs is 400. The guidance scale is 3.0. α FID IS 1.0 3.55 230.3 1.25 3.73 223.1 1.5 4.10 212. 1.75 4.32 204.2 2.0 188.1 6.4 Figure 4. Generation quality of the Gaussian Transformer under different standard deviations during inference. The model size is 184M. cfg is disabled since it does not work well here. 5.4. Importance of Being Expressive Figure 5. The results of varying learning rates for EAR-B. While the logarithmic score also serves as strictly proper scoring rule, it necessitates explicit knowledge of the predictive probability density, which poses challenge for continuous-valued models. To obtain an explicit likelihood estimation, it generally requires constraining the predictive distribution of the model, as exemplified in Tschannen et al. (2023). For instance, consider model that parameterizes Gaussian distribution with mean vector µ and covariance matrix Σ. The corresponding negative log-likelihood objective becomes: = log (xµ, Σ) = 1 2 (x µ)T Σ1(x µ) + 2 log(2π) + 1 2 log Σ. (12) This objective reduces to the Mean Squared Error (MSE) loss under the common assumption of channel independence with fixed standard deviation σ. We employ this MSE loss to train Gaussian Transformer and experiment with different σ during inference. The results are depicted in Figure 4. As seen, an appropriate variance selection yields non-trivial generation quality, but the performance gap compared to EAR remains substantial, suggesting that the token distribution is complex and challenging to be explicitly represented using predefined distributions. Our proposed energy Transformer addresses this challenge through its inherently expressive architecture: the model implicitly defines the predictive distribution through its sampling process, which enables the automatic learning of complex data distributions without restrictive prior assumptions. 8 5.5. Ablation Study Effect of Learning Rate. We observed that the model failed to converge when using standard learning rates, and reducing the learning rate specifically for the MLP generator was found effective to enhance training stability. As illustrated in Figure 5, when the entire model was trained with global learning rate lr = 8e 4, the training process eventually collapsed after several epochs. By selectively lowering the learning rate for the MLP generator to λ = 0.25, the training process is stabilized. While reducing the learning rate of the entire model also enables successful training, this approach results in relatively lower training efficiency. Effect of Noise. In our experiments, we observed that both the type and dimension of random noise can affect model performance. We experimented with Uniform noise and Gaussian noise, setting the noise dimension, dnoise, to 32, 64, and 128. As shown in Table 3, uniform noise consistently outperforms Gaussian noise, and dnoise = 64 performs better than other settings. Therefore, we adopt the 64-dimensional uniform noise in EAR. Table 3. The results of varying random noises for EAR-B. The number of training epochs is 400. The guidance scale is 3.0. dnoise w/o cfg w/ cfg 32 9.87 3.89 Uniform 64 7.95 3. 128 7.04 4.34 32 9.45 4.01 Gaussian 64 7.79 3. 128 7.17 4.51 Continuous Visual Autoregressive Generation via Score Maximization Figure 6. Samples of EAR-H under different gudiance scales. We fix the random seed and apply the constant cfg schedule during sampling. Figure 7. The results of varying classifier-free guidance scales for EAR models. Figure 8. The results of varying temperatures τtrain (left) and τinfer (right) for EAR-B. Effect of CFG. Classifier-free guidance (Ho & Salimans, 2022) plays crucial role in the inference stage of EAR. Figure 7 illustrates the variations of FID and Inception Score across different cfg scales. The image quality, as measured by the Inception Score, consistently improves with increasing cfg. However, the FID metric reaches its optimal value around cfg=3.0, as excessive guidance scale can compromise the generation diversity. Figure 6 illustrates the sampling outputs of EAR-H under different guidance scales, where larger cfg scale generally produces more fine-grained images. Effect of Temperature. We employ temperature hyperparameters τtrain and τinfer to trade diversity for accuracy. Figure 8 shows the impact of varying temperatures during the fine-tuning and inference of EAR-B. These results induce temperature combination of τtrain = 0.99 and τinfer = 0.7. 6. Conclusion This paper introduces Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The Continuous VAR framework is grounded in strictly proper scoring rules, and we primarily explore class of energy-based training objectives, which is likelihood-free and induces an expressive enery Transformer architecture. Our experimental results demonstrate competitive performance in both generation quality and inference efficiency, while leaving substantial room for future improvements. Promising research directions include: 1) architectural optimization of the energy Transformer, 2) incorporation of alternative strictly proper scoring rules as training objectives, 3) extension to more continuous modalities such as video and audio, and 4) continuous language modeling through the conversion of discrete text into latent vector representations."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. 9 Continuous Visual Autoregressive Generation via Score Maximization"
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alpha-VLLM. Large-dit-imagenet. 2024."
        },
        {
            "title": "URL",
            "content": "https://github.com/Alpha-VLLM/ LLaMA2-Accessory/tree/main/ Large-DiT-ImageNet. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Bellemare, M. G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B., Hoyer, S., and Munos, R. The cramer distance as solution to biased wasserstein gradients, 2018. URL https://openreview.net/ forum?id=S1m6h21Cb. Brier, G. W. Verification of forecasts expressed in terms of probability. Monthly weather review, 78(1):13, 1950. Brock, A., Donahue, J., and Simonyan, K. Large scale GAN training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=B1xsqj09Fm. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1131511325, June 2022. Chang, H., Zhang, H., Barber, J., Maschinot, A., Lezama, J., Jiang, L., Yang, M.-H., Murphy, K. P., Freeman, W. T., Rubinstein, M., Li, Y., and Krishnan, D. Muse: Text-to-image generation via masked generative transformers. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 40554075. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/ v202/chang23b.html. Deng, H., Pan, T., Diao, H., Luo, Z., Cui, Y., Lu, H., Shan, S., Qi, Y., and Wang, X. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image In 2009 IEEE Conference on Computer Vidatabase. sion and Pattern Recognition, pp. 248255, 2009. doi: 10.1109/CVPR.2009.5206848. Dhariwal, P. and Nichol, A. Diffusion models beat gans In Ranzato, M., Beygelzimer, on image synthesis. A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 87808794. Curran Associates, Inc., 2021. URL https://proceedings.neurips. cc/paper_files/paper/2021/file/ 49ad23d1ec9fa4bd8d77d02681df5cfa-Paper. pdf. Dinh, L., Krueger, D., and Bengio, Y. NICE: non-linear independent components estimation. In Bengio, Y. and LeCun, Y. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings, 2015. URL http://arxiv.org/abs/1410.8516. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=YicbFdNTTy. Eaton, M. method for evaluating improper prior distributions. Technical report, University of Minnesota, 1981. Elfwing, S., Uchibe, E., and Doya, K. Sigmoidweighted linear units for neural network function Neulearning. approximation in reinforcement ral Networks, 107:311, 2018. ISSN 0893-6080. https://doi.org/10.1016/j.neunet.2017.12.012. doi: https://www.sciencedirect.com/ URL science/article/pii/S0893608017302976. Special issue on deep reinforcement learning. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1287312883, June 2021. Fan, L., Li, T., Qin, S., Li, Y., Sun, C., Rubinstein, M., Sun, D., He, K., and Tian, Y. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. Gneiting, T. and Raftery, A. E. Strictly proper scoring rules, prediction, and estimation. Journal of the American statistical Association, 102(477):359378, 2007. Good, I. J. Rational decisions. Journal of the Royal Statistical Society: Series (Methodological), 14(1):107114, 1952. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, 10 Continuous Visual Autoregressive Generation via Score Maximization Y. Generative adversarial nets. In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and Weinberger, K. (eds.), Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips. cc/paper_files/paper/2014/file/ 5ca3e9b122f61f8f06494c97b1afccf3-Paper. pdf. Gregor, K., Danihelka, I., Mnih, A., Blundell, C., and Wierstra, D. Deep autoregressive networks. In Xing, E. P. and Jebara, T. (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 12421250, Bejing, China, 2224 Jun 2014. PMLR. URL https://proceedings.mlr.press/v32/ gregor14.html. Gritsenko, A., Salimans, T., van den Berg, R., Snoek, J., and Kalchbrenner, N. spectral energy distance for parallel speech synthesis. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1306213072. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 9873eaad153c6c960616c89e54fe155a-Paper. pdf. Gruber, S. and Buettner, F. Better uncertainty calibration via proper scores for classification and beyond. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 86188632. Curran Associates, Inc., 2022. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by two time-scale update rule converge to local nash equilibrium. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. Information Process- (eds.), Advances in Neural ing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 8a1d694707eb0fefe65871369074926d-Paper. pdf. Ho, J. and Salimans, T. Classifier-free diffusion guidURL https://arxiv.org/abs/ ance, 2022. 2207.12598. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper_files/paper/2020/file/ 4c5bcfec8584af0d967f1ab10179ca4b-Paper. pdf. Howard, J. and Ruder, S. Universal language model fine-tuning for text classification. In Gurevych, I. and Miyao, Y. (eds.), Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 328339, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1031. URL https: //aclanthology.org/P18-1031. Hui, L. and Belkin, M. Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=hsFN92eQEla. Hung, M., Hu, M., Shanker, M., and Patuwo, B. Estimating posterior probabilities in classification problems with neural networks. International Journal of Computational Intelligence and Organizations, 1(1):4960, 1996. Hyvarinen, A. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research, 6(24):695709, 2005. URL http://jmlr. org/papers/v6/hyvarinen05a.html. Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10124 10134, 2023. Kingma, D. P. and Gao, R. Understanding diffusion objectives as the ELBO with simple data augmentation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. net/forum?id=NnMEadcdyD. Kline, D. and Berardi, V. Revisiting squared-error and cross-entropy functions for training neural network classifiers. Neural Computing and Applications, 14:310318, 12 2005. doi: 10.1007/s00521-005-0467-y. Kynkaanniemi, T., Karras, T., Laine, S., Lehtinen, J., and Aila, T. Improved precision and recall metric for assessing generative models. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 0234c510bc6d908b28c70ff313743079-Paper. pdf. 11 Continuous Visual Autoregressive Generation via Score Maximization Lakshminarayanan, B., Pritzel, A., and Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Lee, D., Kim, C., Kim, S., Cho, M., and Han, W.-S. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Li, T., Chang, H., Mishra, S., Zhang, H., Katabi, D., and Krishnan, D. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 21422152, June 2023. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Lin, W. and HE, C. Continuous autoregressive modeling with stochastic monotonic alignment for speech synthesis. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=cuFzE8Jlvb. Liu, H., Liu, S., Zhou, Z., Xu, M., Xie, Y., Han, X., Perez, J. C., Liu, D., Kahatapitiya, K., Jia, M., et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. Loshchilov, I. and Hutter, F. Decoupled weight decay regIn International Conference on Learning ularization. Representations, 2019. URL https://openreview. net/forum?id=Bkg6RiCqY7. Matheson, J. E. and Winkler, R. L. Scoring rules for continuous probability distributions. Management science, 22 (10):10871096, 1976. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: VQ-VAE made simple. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=8ishA3LxN8. Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J., Lakshminarayanan, B., and Snoek, J. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. Information Processin Neural (eds.), Advances ing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 8558cb408c1d76621371888657d2eb1d-Paper. pdf. Pacchiardi, L. and Dutta, R. Likelihood-free inference with generative neural networks via scoring rule minimization, 2022. URL https://arxiv.org/abs/2205. 15784. Pacchiardi, L., Adewoyin, R. A., Dueben, P., and Dutta, R. Probabilistic forecasting with generative networks via scoring rule minimization. Journal of Machine Learning Research, 25(45):164, 2024. URL http://jmlr. org/papers/v25/23-0038.html. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Image transShazeer, N., Ku, A., and Tran, D. In Dy, J. and Krause, A. (eds.), Proceedformer. ings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 40554064. PMLR, 1015 Jul 2018. URL https://proceedings.mlr.press/v80/ parmar18a.html. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 41954205, October 2023. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toIn Meila, M. and Zhang, T. (eds.), image generation. Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 88218831. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/ v139/ramesh21a.html. Razavi, A., van den Oord, A., and Vinyals, O. Generating diverse high-fidelity images with vq-vae-2. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Paper. pdf. Roby, T. B. Belief states: preliminary empirical study. Behavioral Sci, 10(3):255270, 1965. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1068410695, June 2022. 12 Continuous Visual Autoregressive Generation via Score Maximization Salimans, T., Goodfellow, I., Zaremba, W., Cheung, ImV., Radford, A., Chen, X., and Chen, X. proved techniques for training gans. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. URL https://proceedings.neurips. cc/paper_files/paper/2016/file/ 8a3363abe792db2d8761d6403605aeb7-Paper. pdf. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Scheuerer, M. and Hamill, T. M. Variogram-based proper scoring rules for probabilistic forecasts of multivariate quantities. Monthly Weather Review, 143(4):13211334, 2015. Shao, C., Meng, F., Liu, Y., and Zhou, J. Language generation with strictly proper scoring rules. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=LALSZ88Xpx. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modIn Ineling through stochastic differential equations. ternational Conference on Learning Representations, 2021. URL https://openreview.net/forum? id=PxTIG12RRHS. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation, 2024a. URL https: //arxiv.org/abs/2406.06525. Sun, Y., Bao, H., Wang, W., Peng, Z., Dong, L., Huang, S., Wang, J., and Wei, F. Multimodal latent language arXiv preprint modeling with next-token diffusion. arXiv:2412.08635, 2024b. Szekely, G. J. E-statistics: The energy of statistical samples. Bowling Green State University, Department of Mathematics and Statistics Technical Report, 3(05):118, 2003. Szekely, G. J. and Rizzo, M. L. Energy statistics: class of statistics based on distances. Journal of Statistical Planning and Inference, 143(8):12491272, 2013. ISSN 0378-3758. doi: https://doi.org/10.1016/j.jspi.2013.03. 018. URL https://www.sciencedirect.com/ science/article/pii/S0378375813000633. Shoemaker, P. note on least-squares learning procedures and classification by neural network models. IEEE Transactions on Neural Networks, 2(1):158160, 1991. doi: 10.1109/72.80304. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Singh, B., De, S., Zhang, Y., Goldstein, T., and Taylor, G. Layer-specific adaptive learning rates for deep networks, 2015. URL https://arxiv.org/abs/ 1510.04609. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 22562265, Lille, France, 07 09 Jul 2015. PMLR. URL https://proceedings. mlr.press/v37/sohl-dickstein15.html. Song, Y. and Ermon, S. Generative modeling by estimating In Wallach, H., gradients of the data distribution. Larochelle, H., Beygelzimer, A., d'Alche-Buc, F., Fox, E., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips. cc/paper_files/paper/2019/file/ 3001ef257407d5a371a96dcd947c7d93-Paper. pdf. Tian, K., Jiang, Y., Yuan, Z., PENG, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=gojL67CfS8. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Tschannen, M., Eastwood, C., and Mentzer, F. Givt: Generative infinite-vocabulary transformers. arXiv:2312.02116, 2023. Turetzky, A., Shabtay, N., Shechtman, S., Aronowitz, H., Haws, D., Hoory, R., and Dekel, A. Continuous speech synthesis using per-token latent diffusion. arXiv preprint arXiv:2410.16048, 2024. Vahidi, A., Schosser, S., Wimmer, L., Li, Y., Bischl, B., Hullermeier, E., and Rezaei, M. Probabilistic selfsupervised representation learning via scoring rules minIn The Twelfth International Conference imization. 13 Continuous Visual Autoregressive Generation via Score Maximization Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xu, S., Bu, Z., Zhang, Y., and Barnett, I. hessianinformed hyperparameter optimization for differential learning rate, 2025. URL https://arxiv.org/ abs/2501.06954. Yu, H., Luo, H., Yuan, H., Rong, Y., and Zhao, F. Frequency autoregressive image generation with continuous tokens. arXiv preprint arXiv:2503.05305, 2025. Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vectorquantized image modeling with improved VQGAN. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=pfNyExj7z2. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Gupta, A., Gu, X., Hauptmann, A. G., Gong, B., Yang, M.-H., Essa, I., Ross, D. A., and Jiang, L. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024a. URL https://openreview.net/forum? id=gzqrANCF4g. Yu, Q., Weber, M., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. An image is worth 32 tokens for reconstruction and generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024b. URL https://openreview.net/forum? id=tOXoQPRzPL. on Learning Representations, 2024. URL https:// openreview.net/forum?id=skcTCdJz0f. van den Oord, A., Kalchbrenner, N., Espeholt, L., kavukcuoglu, k., Vinyals, O., and Graves, A. CondiIn tional image generation with pixelcnn decoders. Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016a. URL https://proceedings.neurips. cc/paper_files/paper/2016/file/ b1301141feffabac455e1f90a7de2054-Paper. pdf. van den Oord, A., Kalchbrenner, N., and Kavukcuoglu, K. Pixel recurrent neural networks. In Balcan, M. F. and Weinberger, K. Q. (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 17471756, New York, New York, USA, 2022 Jun 2016b. PMLR. URL https://proceedings.mlr. press/v48/oord16.html. van den Oord, A., Vinyals, O., and kavukcuoglu, k. Neural discrete representation learning. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 7a98af17e63a0ac09ce2e96d03992fbc-Paper. pdf. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Vincent, P. connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661 1674, 2011. doi: 10.1162/NECO 00142. Weber, M., Yu, L., Yu, Q., Deng, X., Shen, X., Cremers, D., and Chen, L.-C. Maskbit: Embedding-free image generation via bit tokens, 2024. URL https: //arxiv.org/abs/2409.16211. Wu, Y., Zhang, Z., Chen, J., Tang, H., Li, D., Fang, Y., Zhu, L., Xie, E., Yin, H., Yi, L., et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 14 Continuous Visual Autoregressive Generation via Score Maximization A. Additional Results Table 4. Model comparisons on ImageNet 512512 conditional generation. The cfg scale is set to 4.0."
        },
        {
            "title": "Type Model",
            "content": "#Params w/o guidance IS FID w/ guidance IS FID"
        },
        {
            "title": "Diff",
            "content": "AR ADM (Dhariwal & Nichol, 2021) DiT-XL/2 (Peebles & Xie, 2023) VDM++ (Kingma & Gao, 2023) 554M 23.24 675M 12.03 2.99 2B MaskGIT (Chang et al., 2022) MAGVIT-v2 (Yu et al., 2024a) GIVT (Tschannen et al., 2023) MAR (Li et al., 2024) EAR EAR-B 227M 307M 304M 481M 205M 7.32 3.07 8.35 2.74 7.75 58.1 105.3 232.2 156.0 213.1 - 205. 141.5 7.72 3.04 2.65 - 1.91 - 1.73 3.38 172.7 240.8 278.1 - 324.3 - 279. 227.0 Table 5. The effect of attention masking on EAR-B. The number of training epochs is 400. Type w/o guidance IS FID w/ guidance IS FID Causal Bidirection 17.83 7.95 78.6 130.5 8.10 3.55 144.5 230."
        }
    ],
    "affiliations": [
        "Pattern Recognition Center, WeChat AI, Tencent Inc."
    ]
}