{
    "paper_title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
    "authors": [
        "Gonzalo Ariel Meyoyan",
        "Luciano Del Corro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 8 8 2 3 1 . 1 0 6 2 : r BERTology View of LLM Orchestrations: Tokenand Layer-Selective Probes for Efficient Single-Pass Classification"
        },
        {
            "title": "Gonzalo Ariel Meyoyan",
            "content": "Departamento de Computación, FCEyN Universidad de Buenos Aires Buenos Aires, Argentina gmeyoyan@dc.uba.ar Luciano Del Corro ELIAS Lab, Departamento de Ingeniería Universidad de San Andrés Victoria, Argentina delcorrol@udesa.edu.ar"
        },
        {
            "title": "Abstract",
            "content": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full tokenlayer hidden-state tensor, rather than committing to fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form single representation for classification. We instantiate this template with direct pooling, 100Kparameter scoring-attention gate, and downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving nearserving latency and avoiding the VRAM and latency costs of separate guard-model pipeline."
        },
        {
            "title": "Introduction",
            "content": "Modern LLM deployments are rarely single model in isolation. In production, serving LLM is surrounded by auxiliary components for safety moderation, jailbreak detection, policy compliance, and retrieval filtering (Inan et al., 2023; Han et al., 2024; Ghosh et al., 2024). While effective, this orchestration pattern is expensive: each additional classifier adds its training and evaluation pipeline, deployment surface, and inference compute, often requiring an extra model invocation per request. natural question is whether these side tasks can reuse computation we already pay for. Recent work has shown that strong moderation signals can be extracted without deploying separate guard LLM, either by inspecting the serving models output distribution (Hu et al., 2024) or by using training-free decision rules in latent space (Chrab aszcz et al., 2025). Work on lightweight safety heads has demonstrated that small classifier attached to model can provide efficient safeguards (Xuan et al., 2025). These directions share an important thesis: the serving LLM already contains information useful for moderation, and the core opportunity is to exploit it with minimal additional parameters and latency. These approaches reduce orchestration cost by reusing the serving LLM for moderation, either by reading signals from its output distribution (e.g., logit-based readouts) or by applying lightweight decision rules to fixed latent representation. However, transformer does not produce single representation: it computes depth-indexed sequence of hidden states that are transformed across layers. Classical BERTology work argues that transformer layers behave like pipeline, with different depths encoding different abstractions (Tenney et al., 2019; Jawahar et al., 2019; de Vries et al., 2020; Rogers et al., 2020). This suggests that there may exist an intermediate depth where multiple tasks share useful common ancestor representation, and that effective reuse should discover where the signal is most separable rather than commit to fixed layer or token priori. This shifts the question from whether we can reuse the serving models computation to where in the computation we should read out: which tokens and which layers provide the most discriminative features for given classification objective? Motivated by this view, we treat moderation and NLU classification as representation-selection problems over the full LT hidden-state tensor, and learn lightweight, aggregators that select and combine information across both tokens and layers while keeping the serving LLM frozen. We operationalize this idea with layer-selective probes trained on the hidden states produced during the serving LLMs forward pass. We train lightweight classifier that aggregates information across all tokens and all layers. Concretely, we propose two-stage aggregation architecture: (i) token-level aggregation within each layer to produce layer summary vector, followed by (ii) layerlevel aggregation to produce single representation that is fed to linear classification head. We instantiate this template with three expressive mechanisms: direct pooling, scoring attention gate that learns importance weights with very few parameters, and multi-head self-attention variant with aggressive dimension downcasting to control cost. We evaluate our probes on safety moderation and sentiment classification, using relatively small Llama-3.2-3B-Instruct as the serving backbone. Across ToxicChat and WildGuardMix, our probes match or surpass strong guard baselines while requiring orders of magnitude fewer trainable parameters, and they remain competitive on IMDB, SST2 and Emotion. Beyond accuracy, we show that reusing hidden states yields simple deployment path: classification can be performed alongside generation with single model call, avoiding the latency and overhead of separate guard model."
        },
        {
            "title": "2 Related Work",
            "content": "Orchestration and Guard Models. Production LLM systems typically complement the serving model with auxiliary classifiers for content moderation, policy compliance, and safety (Inan et al., 2023; Han et al., 2024; Ghosh et al., 2024). These guard models improve safety but add latency, memory, and deployment complexity. An alternative is safety aligning the main model, but overdoing it risks degrading the models overall capabilities, an effect sometimes referred to as the alignment tax (Huang et al., 2025). Our approach provides additional safety without training big models and avoiding complex deployment pipelines, requiring only 35M additional trainable parameters versus several billion for separate guard model, while reusing the forward pass already performed by the serving LLM. Reusing Computation for Classification. Recent work extracts classification signals from the LLM. MULI (Hu et al., 2024) trains sparse classifier on first-token logits; LPM (Chrab aszcz et al., 2025) applies distance-based rules to vector. For safety probing, ShieldHead (Xuan et al., 2025) attaches head to final-layer hidden states, OmniGuard (Verma et al., 2025) probes single layer, and Zhou et al. (2024) analyze how safety signals emerge across depth. These methods reduce orchestration cost but commit to fixed layer or token, underutilizing signals distributed across the model. In contrast, we introduce an approach to search for signals across layer and token hidden states. Layer Selection and Multi-Layer Aggregation. Interpretability work shows that transformer layers encode different abstractions: lower layers capture syntactic patterns while higher layers encode semantics (Tenney et al., 2019; Jawahar et al., 2019). Subsequent analyses suggest that taskrelevant information is distributed across depth and that combining layers outperforms single-layer selection (de Vries et al., 2020; Rogers et al., 2020). There must be for each task, common ancestor representation that is useful for both. This motivates our design choice: rather than commit to fixed readout position, we learn aggregators that discover which layers and tokens are most informative for given task. We propose principled, datadriven way to select and combine representations for classifier by learning lightweight aggregators over tokens and layers, rather than committing to fixed depth or pooling strategy. In practice, we depart from prior probing work along two axes: we learn joint tokenlayer readout over the full hidden-state tensor, rather than fixing either single layer (e.g., final/selected) or single position (e.g., first token). This yields probes that can be attached to frozen serving LLM and run in the same forward pass, achieving strong accuracy with orders of magnitude fewer trainable and inference parameters than standalone classifiers or guard-model pipelines."
        },
        {
            "title": "3 Probe Architecture",
            "content": "Formally, given frozen decoder-only LLM with layers and an input prompt tokenized into tokens, the model produces hidden states h(l) RT at each layer {0, . . . , 1}, where is the hidden dimension (with l=0 denoting the embedding output). Our goal is to learn lightweight classifier Cθ that predicts label (e.g., safe/unsafe, positive/negative) from these representations, i.e., = Cθ({h(l)}L1 l=0 ). The LLM remains frozen and we train only θ, enabling retrofitting to deployed models while reusing the serving forward pass. The main challenge is to Figure 1: View of our probing architecture. The aggregation method featured here is the scoring attention gate aggregate information across both tokens (T ) and layers (L) to extract discriminative features for classification."
        },
        {
            "title": "3.1 Two-Stage Aggregation Architecture",
            "content": "Our architecture addresses this dimensionality challenge with two-stage aggregation scheme. The LLM produces three-dimensional hidden-state tensor of shape (layers tokens hidden dimension). To avoid feeding this tensor directly to lightweight classifier, we first aggregate across tokens within each layer to obtain fixed-size layer summaries, then aggregate across layers to produce single d-dimensional vector that is consumed by standard classification head. Overview. Figure 1 summarizes our pipeline. Given hidden states {h(l)}L1 l=0 , we (Stage 1) aggregate tokens within each layer to obtain layer summaries v(l) Rd, (Stage 2) aggregate across layers to produce single vector Rd, and then apply linear head to obtain class logits. We use the same aggregation mechanism for both stages (Section 3.2). Stage 1: Token-level aggregation. For each layer l, we reduce h(l) RT to layer summary using A(l) token: v(l) = A(l) token(h(l)), v(l) Rd, yielding {v(l)}L l=0. Stage 2: Layer-level aggregation. We then aggregate across depth with Alayer: This module learns task-specific layer weighting from data, avoiding manual layer selection. Classification head. Finally, logits are produced by linear head, logits = Woutv + bout, Wout RCd, trained with cross-entropy."
        },
        {
            "title": "3.2 Aggregation Mechanisms",
            "content": "The core design choice is the aggregation operator that compresses representations across tokens (Stage 1) and across layers (Stage 2). This operator must balance expressiveness with overhead: it should extract task-relevant signal while adding minimal parameters and inference cost. We study three mechanisms spanning simple toexpressive spectrum, from fixed pooling to learned attention. Here, we use the same mechanism for Stage 1 and Stage 2 to keep the architecture uniform and to enable controlled comparisons. Let RN denote the input to an aggregation block, where {T, L} depends on the stage. The block outputs fixed-size vector Rd summarizing the most informative content in X. Direct pooling. The simplest aggregation strategy. It applies fixed pooling operator over valid (non-padding) positions. Max pooling selects the largest activation per dimension, v[j] = max iV X[i, j], while mean pooling averages activations, v[j] = 1 (cid:88) iV X[i, j], = Alayer({v(l)}L l=0), Rd. where denotes valid positions. Scoring attention gate. To add learnable weighting with minimal parameters, this approach learns to assign scalar importance scores to each input (token or layer) using single linear projection, si = tanh(wX[i, :]), {1, . . . , }, mask padding by setting si = , and normalize with softmax: α = softmax(s), = (cid:88) i=1 αi X[i, :]. For token-level aggregation, we use one gate per layer (L total); for layer-level aggregation, we use single shared gate. Parameter count: (L+1)d trainable parameters. Multi-head self-attention. We use multi-head self-attention (Vaswani et al., 2017) as more expressive alternative. We implement attention with PyTorch scaled_dot_product_attention, which can leverage FlashAttention when available (Dao, 2023). To control cost, we downcast QKV projections from to dinner < (e.g., d/16 or d/32), split into heads with dhead = dinner/H. For each head h, we compute Attnh(Qh, Kh, Vh) = softmax (cid:19) (cid:18) QhK dhead Vh, concatenate heads, project back to dimensions, which we then pool (mean or max) over the sequence dimension to obtain Rd. As with the scoring gate, we use L+1 MHA modules (one per layer for Stage 1, one for Stage 2), for total of (L+1) 4d dinner parameters."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We use Llama-3.2-3B-Instruct Model. (Grattafiori et al., 2024) to simulate serving LLM.1 Notably, this compact 3B backbone is substantially smaller than many guard-model and classifier baselines, yet it already exposes hidden representations that support strong moderation and NLU probes on our benchmarks. In many real deployments, the serving model is substantially larger, and established scaling laws suggest that language modeling and downstream capability tend to improve with model size and data/compute (Kaplan 1Code available at https://anonymous.4open. science/r/LLM-Probing-Classifierset al., 2020; Hoffmann et al., 2022). Since our probes read out from the serving models internal representations without modifying the backbone, stronger backbones offer natural path to improved probe accuracy at essentially unchanged relative probe overhead; we leave systematic backbone sweep to future work. We focus on the Llama family for comparability and practical relevance, as it underlies widely used open guard-model baselines and is common in recent safety evaluations. We treat the scaling argument as hypothesis rather than guarantee, and empirically validating crossbackbone transfer is an important next step. Datasets. We evaluate on five public benchmarks spanning two settings: safety moderation and sentiment classification. ToxicChat. ToxicChat (Lin et al., 2023) contains 10,166 humanLLM prompts (Zheng et al., 2023) labeled for toxicity, including explicit toxic content and jailbreak-style prompts. We use the authors train/test split and restrict the test set to humanannotated samples. WildGuardMix. WildGuardMix (Han et al., 2024) is large safety benchmark (89K examples) mixing multiple sources and synthetic data across diverse harm categories (e.g., violence, hate, self-harm, sexual content, privacy), with both direct requests and adversarial jailbreaks. We use the standard train/test split. IMDB / SST-2 / Emotion. IMDB (Maas et al., 2011) is binary sentiment over movie reviews; following Palma et al. (2025), we use their 12,500example subset (6,250/6,250). SST-2 (Socher et al., 2013) is binary sentiment over short snippets (standard split). Emotion (Saravia et al., 2018) is multi-class emotion classification benchmark; following Palma et al. (2025), we use their 8,000example subset. Metrics. We report F1 for ToxicChat and WildGuardMix, and accuracy for IMDB, SST-2, and Emotion; for ToxicChat we additionally report AUPRC when available."
        },
        {
            "title": "4.2 Hyperparameter Search",
            "content": "A practical benefit of our probes is their small trainable footprint, which makes extensive hyperparameter exploration feasible. We explored approximately 100 configurations per dataset on the validation set, scale of experimentation that would be prohibitive when fine-tuning multi-billion parameter guard models. This allows us to select optimal configurations and analyze how sensitive results Method F1(%) AUPRC Added Params (M) Extra LM call Standalone classifiers (extra model call) ToxicChat-T5-large 82.2 Reuse baselines (same serving pass) MULI (logits) 77.8 Ours (same serving pass) Direct pooling Scoring attention Multi-head self-attn 73.53 0.68 80.49 1.17 84.51 0.43 0. 0.829 0.812 0.005 0.854 0.008 0.898 0.005 780 0.13 0.003 0.10 35 Table 1: ToxicChat (in-distribution). All Reuse methods (MULI and ours) attach probe to frozen Llama-3.2-3B serving model and reuse the same forward pass; we therefore report added parameters beyond the 3B backbone. Standalone baselines require an additional model invocation. From (Lin et al., 2023). Method F1(%) AUPRC Added Params (M) Extra LM call Standalone guard models / APIs (extra model call) OpenAI Moderation Llama Guard, Llama-Guard2 Aegis-Guard-D GPT-4 WildGuard ShieldHead (Llama3.1-8B) ShieldHead (Gemma2-27B) 0.631 0.626 61.4 61.6 47.1 70.0 68.3 70.8 64.3 67.7 Ours (same serving pass; trained on WildGuardMix) Direct pooling Scoring attention Multi-head self-attn 0.565 0.706 0.798 53.33 64.81 72.88 7,000 8,000 8,000 1,700,000 7,000 91 0.003 0.10 35 Table 2: ToxicChat (out-of-distribution). Guard models / APIs require an additional model invocation. Our probes attach to frozen Llama-3.2-3B serving model and reuse the same forward pass; we therefore report added parameters beyond the 3B backbone. From (Lin et al., 2023). From (Han et al., 2024). AUPRC from (Inan et al., 2023). From (Xuan et al., 2025). are to different hyperparameter values. We present such analysis in Appendix A."
        },
        {
            "title": "4.3 Safety Classification Results",
            "content": "To reduce peak GPU memory during training, we optionally precompute and cache hidden states from the frozen LLM before training the probe. This decouples backbone inference from classifier training, allowing larger batch sizes under VRAM constraints. At inference time, the pipeline is unchanged: single forward pass of the serving LLM with the probe attached. For each dataset and aggregation mechanism, we search standard training hyperparameters (learning rate, batch size, weight decay) as well as mechanism-specific choices. For MHA, we vary the number of heads and the downcasting factor; for pooling and MHA, we also vary the pooling operator (mean vs. max). Full ranges and selected settings are reported in Appendix A. We evaluate whether safety signals in frozen serving LLM can be read out with lightweight probes that run in the same forward pass as generation. To make the deployment trade-off explicit, we group baselines by whether they require an extra LM call at inference (standalone guard models / APIs) versus reuse methods that attach small head to the serving model (MULI and ours). For all reuse methods, we report added parameters beyond the frozen Llama-3.2-3B-Instruct backbone. ToxicChat: in-distribution  (Table 1)  . When trained and evaluated on ToxicChat, learned token layer aggregation improves over fixed readouts. Direct pooling reaches 73.53 F1, while the scoringattention gate improves to 80.49 F1 with 0.10M added parameters. The multi-head self-attention (MHA) probe achieves 84.51 F1 and 0.898 AUPRC, improving over the logit-reuse baseline MULI (77.8 F1 / 0.829 AUPRC) and the standalone Method F1(%) Added Params (M) Extra LM call Standalone guard models / APIs (extra model call) OpenAI Mod API Llama-Guard2 Llama3.1-AegisGuard WildGuard 12.1 70.4 82.1 88.9 Reuse baselines (same serving pass) MULI (logits, Llama-3.2-3B) 83.79 Ours (same serving pass) Direct pooling Scoring attention Multi-head self-attn 82.84 0.06 85.98 0.61 88.55 0.30 8,000 8,000 7,000 0.13 0.003 0.10 35 Table 3: WildGuardMix. Reuse methods attach probe to serving Llama-3.2-3B model and reuse the forward pass; we report added parameters beyond the backbone. Standalone baselines require an model invocation. Method IMDB SST-2 Emotion Added Params (M) Extra LM call Standalone classifiers / truncation baselines (extra model call) DeBERTa V3 Large RoBERTa Large SentriLlama 3.2 (3B) Instruct 90.38 95.99 95.94 95.34 94.30 95.79 Reuse baselines (same serving pass) MULI (logits, Llama-3.2-3B) Prompting (extra model call) Llama 3.2 (3B) Zero-shot Llama 3.2 (3B) Few-shot Llama 3.2 (3B) Chain-of-Thought 86.50 77.59 76.06 91.54 93.19 83.97 85.28 93.06 87.65 84.16 82.20 64. 44.55 33.40 56.05 418 355 0.0031.2 0.130.77 0 0 0 Ours (same serving pass) Direct pooling Scoring attention Multi-head self-attn 94.07 0.04 95.05 0.11 95.15 0. 92.29 0.18 94.42 0.64 95.39 0.32 69.18 0.64 84.58 0.67 87.68 1.09 0.0030.018 0.100.11 3535.5 Table 4: Sentiment and emotion classification on IMDB, SST-2, and Emotion. For Reuse methods (MULI and ours), we report added parameters beyond frozen Llama-3.2-3B serving model and no extra model call at inference. Baseline and prompting results from Palma et al. (2025). SentriLlama truncates the backbone at selected layer and trains classifier head; the reported trainable range varies by classifier and number of classes. ToxicChat-T5 classifier (82.2 F1 / 0.885 AUPRC), while avoiding an extra model invocation. These gains are obtained with aggressive attention downcasting, suggesting that the improvement is driven primarily by where we read out (across tokens and layers) rather than by large additional capacity. ToxicChat: cross-dataset generalization  (Table 2)  . To test robustness under distribution shift, we train on WildGuardMix and evaluate on ToxicChat. Pooling drops to 53.33 F1, indicating that learned selection is important for transfer. The scoring-attention gate reaches 64.81 F1 / 0.706 AUPRC with 0.10M parameters, and MHA reaches 72.88 F1 / 0.798 AUPRC. In this setting, our probes outperform several guard-model and API baselines reported in Table 2 while requiring no additional model call at inference. Because these baselines vary in architecture and training data, we treat such cross-dataset comparisons as indicative of the computeaccuracy trade-off rather than controlled head-to-head matches. The key result is that probes trained on broad safety mixture transfer to ToxicChat without deploying separate guard LLM. WildGuardMix  (Table 3)  . On the larger and more heterogeneous WildGuardMix benchmark, simple reuse is strong: direct pooling achieves 82.84 F1, and learned aggregation provides consistent gains. The scoring-attention gate reaches 85.98 F1 with 0.10M added parameters, and MHA achieves 88.55 F1, approaching the strongest standalone guard baseline in the table (WildGuard at 88.9 F1) while training only 35M parameters and still running in the same serving pass. Compared to MULI (83.79 F1), which reads out from first-token logits, both learned aggregation variants perform better, supporting the hypothesis that safety cues are distributed across layers and token positions and are better captured by joint tokenlayer selection. Takeaway. Across both datasets, method ranking is consistent (pooling < scoring-attention < MHA). Overall, the benefit is not only reuse of the serving computation, but learning where in the tensor the safety signal is most separable, reducing the latency and VRAM overhead associated with second LLM in the orchestration pipeline."
        },
        {
            "title": "4.4 Sentiment Analysis Results",
            "content": "Table 4 reports accuracy on IMDB, SST-2, and Emotion. Overall, we find that frozen Llama-3.2-3B-Instruct already encodes strong sentiment/emotion cues in its hidden-state tensor, and that lightweight readout trained on these representations substantially outperforms prompting while avoiding an extra model invocation. Reuse vs. prompting. Prompting the same backbone is markedly weaker: zero-shot and few-shot prompting perform poorly on all three datasets, and even chain-of-thought remains far below trained probes. In contrast, our reuse probes deliver high accuracy with single serving pass, showing that supervised readouts can reliably extract sentiment information from the internal representations without changing the backbone. Effect of learned tokenlayer aggregation. Within our probe family, learned aggregation consistently wins over pooling. Direct pooling is already strong on IMDB and SST-2 but underperforms significantly on Emotion (69.18), suggesting that simple summaries may miss task-relevant structure. The scoring-attention gate provides large jump with minimal additional capacity, using only 0.100.11M added parameters, indicating that learning where to read across tokens and layers captures much of the signal. The more expressive MHA probe yields the best overall reuse performance, and is particularly beneficial for the multi-class Emotion task. Comparison to standalone classifiers and logit reuse. Against standalone classifiers (DeBERTa, RoBERTa), our MHA probe is competitive on IMDB and SST-2 and achieves the best result on Emotion. Compared to the logit-reuse baseline MULI, which reads out from first-token logits, our probes are stronger, especially on Emotion (87.68 vs. 64.05), suggesting that hidden states and token layer aggregation provide richer signal than logits alone. Relation to SentriLlama. SentriLlama reports strong results by truncating the backbone at selected layer and training classifier head. In contrast, our probes operate on an intact frozen serving model and can run alongside generation, which is the common requirement in orchestration settings. Despite this constraint, our scoring-attention and MHA probes remain competitive across datasets while keeping inference to single serving pass. Takeaway. For recurring sentiment and emotion classification inside an LLM orchestration, reuse probes offer favorable trade-off: large gains over prompting, improvements over logitonly reuse, and accuracy competitive with taskspecific classifiers while preserving single-pass serving pipeline."
        },
        {
            "title": "5 Layer Attention Analysis",
            "content": "A central motivation of our approach is that taskrelevant information is distributed across depth, and that committing to fixed readout position may underutilize available signal. To probe this, we visualize the layer-aggregation weights produced by the scoring-attention gate on ToxicChat, stratified by label and prediction correctness (Figure 2). No single layer dominates across all groups. For classified toxic prompts, attention is spread over later layers (L17L28), with layers receiving weight above the uniform baseline. This pattern is consistent with features emerging progressively through depth, suggesting that restricting the readout to single layer can discard evidence from nearby layers. In contrast, correctly classified nontoxic prompts place most mass on the final layers (L27L28), with smaller contribution from the embedding layer (L0). These class-conditional patterns can be interpreted as the probe emphasizing different depths where representations are most separable for each content type. Toxic prompts tend to elicit informative intermediate-to-late representations starting around L17, whereas non-toxic prompts rely more heavily on the final layers. Regarding misclassifications, toxic prompts predicted as non-toxic concentrate attention on L28, resembling the correctly classified non-toxic profile; conversely, misclassified non-toxic prompts exhibit the more distributed pattern typical of toxic predictions. Thus, errors are associated with layerweight profiles that align more with the predicted class than with the ground-truth label, which may reflect atypical representations, label noise, or genuinely ambiguous cases."
        },
        {
            "title": "These findings align with classical BERTology",
            "content": "Figure 2: Attention weights from the scoring-attention probe on ToxicChat, stratified by label and correctness. The dashed line marks uniform 1/L ( 0.034). Toxic prompts attend to later layers (L17L28), while non-toxic prompts concentrate on layers L0 and L27L28. Misclassifications resemble the predicted-class profile. Approach Throughput Avg. Latency Peak GPU (MB) (samples/s) (ms/sample) Base (3B) + Pooling probe + Scoring probe + MHA probe GuardServe (8B+3B) 37.84 33.72 32.36 24.83 8. 26.43 29.66 30.90 40.27 123.21 6497.63 6497.63 6749.00 6968.92 22768.72 Table 5: Latency benchmark on ToxicChat test prompts using Llama-3.2-3B-Instruct (2000 samples, max length 512, batch size 1, max_new_tokens=1). Probes reuse the serving pass; GuardServe runs LlamaGuard 3 and then the 3B model. work showing that transformer layers encode different abstractions (de Vries et al., 2020; Jawahar et al., 2019), and extend it to decoder-only LLMs: safety-relevant features are not localized to single depth but distributed across the network, motivating learned aggregation over fixed readouts. Similar layer-wise attention patterns emerge for sentiment classification (Figure 4 in Appendix C)."
        },
        {
            "title": "Inference Efficiency Analysis",
            "content": "We quantify the systems overhead of our probes relative to standard production pattern that runs separate guard model before invoking the serving LLM. We report end-to-end latency, throughput, and peak GPU memory, since these directly determine serving cost and tail-latency budgets. Deployment scenarios. Our baseline is guardthen-serve pipeline in which Llama-Guard-38B (Inan et al., 2023) screens prompts and the model (Llama-3.2-3B-Instruct) is invoked after guard decision. In contrast, our probes are attached to the model and compute the label from hidden states produced in the forward pass as generation, eliminating an extra model invocation. Results. Table 5 shows clear separation between single-pass reuse and two-model pipeline. Relative to the serving-only baseline, pooling adds negligible overhead and the scoring-attention gate remains close. The MHA probe is slower, reflecting the additional attention computation, but still substantially faster than the guard-then-serve pipeline. Throughput follows the same trend. Peak memory is dominated by whether second model is loaded. All probe variants remain near the serving footprint (6.57.0 GB), whereas the guard-then-serve pipeline requires 22.8 GB due to the additional 8B guard model. Overall, these measurements show that (i) reuse-based classification preserves single-model serving profile, and (ii) within our probe family, pooling and scoringattention offer the best latencyaccuracy trade-off, while MHA provides higher accuracy at moderate but still single-pass overhead."
        },
        {
            "title": "7 Conclusions and Future Work",
            "content": "We showed that lightweight probes trained on frozen serving LLMs hidden states can support moderation and sentiment/emotion classification without deploying separate classifier model. By casting classification as representation selection over the full hidden-state tensor and using two-stage tokenlayer aggregation, our approach avoids committing to single layer or token while remaining parameter-efficient. Across ToxicChat and WildGuardMix, our probes approach or match strong guard-model baselines with far fewer trainable parameters, and they are competitive on IMDB, SST-2, and Emotion. System-wise, reuse enables single-pass pipeline that reduces latency, VRAM, and orchestration complexity relative to guard-then-serve deployments. key next step is to validate robustness across backbones."
        },
        {
            "title": "Limitations",
            "content": "Generalization Across Models and Architectures. We evaluated our approach exclusively on Llama-3.2-3B-Instruct. While this demonstrates that even compact models encode rich hidden representations, generalization to other model families (e.g., GPT, Gemma, Mistral) and sizes (1B to 70B+) remains unexplored. Different architectures may encode task-relevant information differently across layers, potentially requiring architecturespecific tuning of our aggregation mechanisms. Furthermore, our approach inherits the semantic understanding, capabilities, and biases of the underlying frozen LLM. Since we train probes on the models hidden representations without modifying the base model, any biases present in the LLMs pretraining data or limitations in its semantic understanding will directly affect classification performance. Sequence Length Constraints. Our experimental setup was limited by VRAM constraints when processing longer sequences with larger batch sizes. Longer input sequences (e.g., full documents, multi-turn conversations) may present memory challenges that require optimization techniques. Training Data Requirements. Our smallest dataset (SST-2) contained 7,000 examples. The minimum dataset size required to effectively train these probes remains unclear, particularly for the more parameter-intensive multi-head attention variants. Tasks with limited labeled data may require investigation of few-shot or transfer learning approaches. Response Generation Limitation. Unlike safety fine-tuning or guard models that can generate explanatory rejections (e.g., cannot help with that request because...), our classification approach can only detect harmful content and interrupt generation. While this may be addressable through conditional re-prompting after detection, we did not evaluate such strategies. This limits the user experience compared to models that provide contextual explanations during content moderation."
        },
        {
            "title": "Ethical Considerations",
            "content": "Misuse potential. Understanding where safety signals are encoded in LLM hidden states could theoretically inform adversarial attacks designed to evade detection. However, similar information is available in prior interpretability work, and we believe the benefits of efficient moderation outweigh this risk. Bias and disparate impact. Because probes read out from frozen backbone, they inherit biases and representation gaps from the underlying LLM and training data. This can lead to disparate error rates across demographic groups. Privacy and data handling. Our experiments optionally cache hidden states to reduce trainingtime GPU memory. Hidden representations may encode sensitive information present in prompts. Cached activations should therefore be treated as sensitive data, with appropriate access controls and limited retention."
        },
        {
            "title": "References",
            "content": "Maciej Chrab aszcz, Filip Szatkowski, Bartosz Wójcik, Jan Dubinski, Tomasz Trzcinski, and Sebastian Cygert. 2025. Do llms understand the safety of their inputs? training-free moderation via latent prototypes. Preprint, arXiv:2502.16174. Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. Preprint, arXiv:2307.08691. Wietse de Vries, Andreas van Cranenburgh, and Malvina Nissim. 2020. Whats so special about BERTs layers? closer look at the NLP pipeline in monolingual and multilingual models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 43394350, Online. Association for Computational Linguistics. Shaona Ghosh, Prasoon Varshney, Erick Galinkin, and Christopher Parisien. 2024. Aegis: Online adaptive ai content safety moderation with ensemble of llm experts. Preprint, arXiv:2404.05993. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Seungju Han, Kavel Rao, Allyson Ettinger, Liwei Jiang, Bill Yuchen Lin, Nathan Lambert, Yejin Choi, and Nouha Dziri. 2024. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. Preprint, arXiv:2406.18495. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, pages 36873697, Brussels, Belgium. Association for Computational Linguistics. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 16311642. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. Bert rediscovers the classical nlp pipeline. Preprint, arXiv:1905.05950. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Sahil Verma, Keegan Hines, Jeff Bilmes, Charlotte Siska, Luke Zettlemoyer, Hila Gonen, and Chandan Singh. 2025. Omniguard: An efficient approach for ai safety moderation across modalities. Preprint, arXiv:2505.23856. Zitao Xuan, Xiaofeng Mao, Da Chen, Xin Zhang, Yuhan Dong, and Jun Zhou. 2025. ShieldHead: Decodingtime safeguard for large language models. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1812918143, Vienna, Austria. Association for Computational Linguistics. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, and Yongbin Li. 2024. How alignment and jailbreak work: Explain LLM safety through intermediate hidden states. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 24612488, Miami, Florida, USA. Association for Computational Linguistics. Johannes Welbl, Aidan Clark, and 1 others. 2022. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556. Zhanhao Hu, Julien Piet, Geng Zhao, Jiantao Jiao, and David Wagner. 2024. Toxicity detection for free. Preprint, arXiv:2405.18822. Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Zachary Yahn, Yichang Xu, and Ling Liu. 2025. Safety tax: Safety alignment makes your large reasoning models less reasonable. Preprint, arXiv:2503.00555. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. Preprint, arXiv:2312.06674. Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019. What Does BERT Learn about the Structure of Language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 36513657, Stroudsburg, PA, USA. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. 2023. ToxicChat: Unveiling hidden challenges of toxicity detection in real-world user-AI conversation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 46944702, Singapore. Association for Computational Linguistics. Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies, pages 142150. Dario Di Palma, Alessandro De Bellis, Giovanni Servedio, Vito Walter Anelli, Fedelucio Narducci, and Tommaso Di Noia. 2025. Llamas have feelings too: Unveiling sentiment and emotion representations in llama models through probing. Preprint, arXiv:2505.16491. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842866. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. 2018. CARER: Contextualized affect representations for emotion recognition. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Hyperparameter Search Range Learning rate Batch size Max epochs Optimizer Weight decay LR scheduler Pooling method Attention heads Downcast factor [105, 103] {8, 16, 32, 64} 10 (with early stopping) AdamW (β1=0.9, β2=0.999) [0, 0.05] Cosine annealing {mean, max} {4, 8, 16} {4, 8, 16, 32, 64} Table 6: Hyperparameter search ranges. The downcast factor determines the inner attention dimension as dinner = d/factor, where = 3072 for Llama-3.2-3B."
        },
        {
            "title": "Sensitivity",
            "content": "We conducted hyperparameter search for each dataset and aggregation mechanism. key advantage of our lightweight probes is that this scale of experimentation (approximately 100 configurations per dataset) would be prohibitive when finetuning multi-billion parameter guard models. Beyond finding optimal configurations, this enables systematic sensitivity analysis: characterizing how performance varies across the design space."
        },
        {
            "title": "All",
            "content": "using performed search was train/validation split, with the test set held out for final evaluation. We employed early stopping based on validation F1 score for safety datasets and validation accuracy for sentiment datasets. A.1 Search Ranges Table 6 summarizes the hyperparameter ranges explored across all experiments. A.2 Experimental Setup All experiments were conducted on single NVIDIA RTX 3090 GPU (24GB VRAM) with 96GB system RAM. To reduce peak GPU memory during training, we pre-extracted and cached hidden states from the frozen LLM before training probe classifiers, allowing larger batch sizes under VRAM constraints. A.3 Sensitivity Analysis Figure 3 shows PR-AUC sensitivity to hyperparameter choices on ToxicChat, revealing that learning rate is the dominant factor for Pooling and Scoring Attention, while Self-Attention is comparatively robust across settings. Figure 3: Sensitivity of PR-AUC to hyperparameter choices on ToxicChat across 100 configurations. Each row corresponds to an aggregation method; horizontal lines denote medians. Self-Attention sustains high PR-AUC (0.750.90) with low variance across settings. Pooling and Scoring Attention are highly sensitive to learning rate, spanning 0.20.9. Dataset Downcast Accuracy (%) F1 (%) ROC AUC PR AUC Params (M) ToxicChat SST-2 4 8 16 32 64 4 8 16 32 64 96.06 0.33 95.89 0.09 95.57 0.20 96.13 0.19 95.91 0.12 89.88 5.74 95.44 0.22 91.95 1.47 95.39 0.32 93.70 2.39 84.13 0.86 82.93 1.40 83.02 0.39 84.51 0.43 82.80 1. 90.95 4.57 95.52 0.20 92.51 1.24 95.31 0.33 94.01 2.04 0.983 0.002 0.981 0.002 0.981 0.001 0.981 0.001 0.981 0.001 0.990 0.001 0.992 0.001 0.992 0.000 0.992 0.001 0.991 0.000 0.906 0.008 0.902 0.012 0.889 0.004 0.898 0.006 0.904 0.002 0.988 0.003 0.992 0.001 0.992 0.000 0.992 0.001 0.991 0.000 283 142 71 35 283 142 71 35 18 Table 7: Effect of attention downcasting on ToxicChat and SST-2 performance. Results show mean std across 3 runs. Lower downcasting factors (larger dinner) increase parameter count. Downcast 32 is optimal for ToxicChat, while downcast 8 achieves best results on SST-2. All experiments use the same hyperparameters configuration from our main results, with only the downcast parameter being altered."
        },
        {
            "title": "Downcasting",
            "content": "We evaluate the effect of dimension downcasting on the multi-head self-attention aggregation mechanism using the ToxicChat dataset. The results are shown on Table 7. The downcasting factor determines the inner dimension dinner = d/factor for the QKV projections, where is the hidden dimension (d = 3,072 for Llama-3.2-3B). Figure 4: Layer-wise post-softmax attention weights on SST-2, stratified by ground-truth label (Positive vs. Negative) and prediction correctness. Similar to toxicity detection (Figure 2), correctly classified samples exhibit distinct class-conditional attention patterns: positive sentiment concentrates on later intermediate layers (L17L28), while negative sentiment shows concentrated attention weights on the embedding and final layers (L0, L28). Layer Attention on SST-2 Figure 4 shows layer-wise attention patterns for sentiment classification on SST-2, paralleling our toxicity analysis. The task-specific attention distributions further validate that different features related to sentiment classification emerge at different layer depths, supporting our learned aggregation approach over single layer selection."
        }
    ],
    "affiliations": [
        "Departamento de Computación, FCEyN Universidad de Buenos Aires",
        "ELIAS Lab, Departamento de Ingeniería Universidad de San Andrés"
    ]
}