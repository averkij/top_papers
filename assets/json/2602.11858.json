{
    "paper_title": "Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception",
    "authors": [
        "Lai Wei",
        "Liangbo He",
        "Jun Lan",
        "Lingzhong Dong",
        "Yutong Cai",
        "Siyuan Li",
        "Huijia Zhu",
        "Weiqiang Wang",
        "Linghe Kong",
        "Yue Wang",
        "Zhuosheng Zhang",
        "Weiran Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent \"Thinking-with-Images\" methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into a training-time primitive, thereby internalizing the benefits of agentic zooming into a single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this region-grounded supervision back to the full image. After training on such data, the smaller student model improves \"single-glance\" fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, a hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with a dual-view protocol that quantifies the global--regional \"zooming gap\". Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks, and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when \"Thinking-with-Images\" is necessary versus when its gains can be distilled into a single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 2 1 ] . [ 1 8 5 8 1 1 . 2 0 6 2 : r Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception Lai Wei1,2,3,, Liangbo He2,, Jun Lan2,, Lingzhong Dong1,2, Yutong Cai1, Siyuan Li2, Huijia Zhu2, Weiqiang Wang2, Linghe Kong1, Yue Wang3, Zhuosheng Zhang1, Weiran Huang1,4, 1School of Computer Science, Shanghai Jiao Tong University 2Ant Group 3Zhongguancun Academy 4Shanghai Innovation Institute Figure 1: Average scores across multimodal perception benchmarks. ZwZ-4B/7B/8B demonstrate competitive performance compared with current SOTA MLLMs (e.g., Gemini-3-Flash, Kimi-K2.5, Qwen3-VL-235B). Abstract Multimodal Large Language Models (MLLMs) excel at broad visual understanding but still struggle with fine-grained perception, where decisive evidence is small and easily overwhelmed by global context. Recent Thinking-with-Images methods alleviate this by iteratively zooming in and out regions of interest during inference, but incur high latency due to repeated tool calls and visual re-encoding. To address this, we propose Region-to-Image Distillation, which transforms zooming from an inference-time tool into training-time primitive, thereby internalizing the benefits of agentic zooming into single forward pass of an MLLM. In particular, we first zoom in to micro-cropped regions to let strong teacher models generate high-quality VQA data, and then distill this regiongrounded supervision back to the full image. After training on such data, the smaller student model improves single-glance fine-grained perception without tool use. To rigorously evaluate this capability, we further present ZoomBench, hybrid-annotated benchmark of 845 VQA data spanning six fine-grained perceptual dimensions, together with dual-view protocol that quantifies the globalregional zooming gap. Experiments show that our models achieve leading performance across multiple fine-grained perception benchmarks (Figure 1), and also improve general multimodal cognition on benchmarks such as visual reasoning and GUI agents. We further discuss when Thinking-with-Images is necessary versus when its gains can be distilled into single forward pass. Our code is available at https://github.com/inclusionAI/Zooming-without-Zooming. Equal Contribution. This work was done during the first authors internship at Ant Group. Project Lead. Correspondence to: Weiran Huang <weiran.huang@sjtu.edu.cn> 1 Figure 2: Zooming without Zooming. Thinking-with-Images models rely on iterative tool-based cropping and re-encoding at inference, incurring high latency. Our Region-to-Image Distillation performs zooming only during training to synthesize region-grounded supervision on the full image, enabling single-pass fine-grained perception without test-time tool use."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) achieve strong general visual understanding and reasoning (Bai et al., 2025a; Comanici et al., 2025), yet they remain brittle on fine-grained perception tasks where the decisive evidence is small and easily overwhelmed by the global context (Zhang et al., 2025b; Wang et al., 2025c; Liu et al., 2025a), such as tiny text and subtle attributes. In standard single-pass setting, models must retrieve such micro evidence from thousands of visual tokens, making it difficult to reliably isolate minute evidence amid clutter, distractors, and redundant information. To mitigate this, recently popular paradigms like Thinking-with-Images (Wu & Xie, 2024; Zheng et al., 2025b; Zhang et al., 2025f; Wu et al., 2025a) equip MLLMs with agentic tool use to iteratively zoom in and out regions of interest and re-encode them during inference. Its effectiveness largely comes from reducing interference: once micro-crop is isolated, the model can devote its capacity to small region, turning needle-in-a-haystack problem into straightforward recognition. However, this comes with substantial latency due to repeated tool calls and multiple visual encoding passes, limiting real-time usage. This motivates us natural question: can we obtain the accuracy benefits of zooming in while keeping inference to single forward pass? We achieve this by introducing novel and easy-to-implement Region-to-Image Distillation method. Concretely, we decouple zooming in from inference-time (i.e., the agentic Thinking-with-Images process) and repurpose it as training-time primitive. Firstly, we employ advanced models (large, cloud-hosted) to generate perception-centric questions and high-consensus answers only on micro-crops, where fine details are unambiguous and hallucinations are minimized. Then, these region-grounded data are distilled back to the full image, with explicit visual grounding via bounding-box overlays to avoid referential ambiguity. Training on such distilled supervision teaches the model (smaller, localized) to suppress global noise and pay attention to micro-level evidence directly from the full image. Therefore, the model internalizes the benefits of agentic zooming in into single glance (one forward pass on the full image) at inference time without iterative tool use. This can also be summarized as an idea of Zooming without Zooming shown in Figure 2. Here, the first Zooming refers to the training-time primitive: we zoom into micro-regions to synthesize fine-grained training data and thus our model can zoom in and out in mind. In contrast, the second Zooming denotes the inference-time tool-use we seek to bypass. To pave the way for efficiently building high-quality benchmark at low cost, as well as rigorously evaluate fine-grained acuity in this regime, we present ZoomBench. It is challenging benchmark of 845 high-quality 2 VQA samples built with our synthesis pipeline plus human verification, spanning six perceptual dimensions and covering diverse question formats. Unlike from prior benchmarks, ZoomBench naturally supports dual-view evaluation protocol to quantify the global-regional zooming gap, along with interpretability analyses based on relative attention maps (Zhang et al., 2025b) for deeper inspection. We conduct extensive experiments to validate the effectiveness of our region-to-image synthetic data. By leveraging reinforcement learning (Guo et al., 2025) on just 74K data, our modes (ZwZ) achieve substantial performance gains across all evaluated fine-grained perception benchmarks, including our proposed ZoomBench. Remarkably, ZwZ-8B outperforms similarly sized baselines, and remains competitive with or even exceeds significantly larger state-of-the-art MLLMs (e.g., Kimi-K2.5, Qwen3-VL-235B, and Gemini3) on many challenging perceptual tasks despite using much smaller backbones (Figure 1). Our models also consistently surpass many agentic Thinking-with-Images models such as DeepEyes (Zheng et al., 2025b) and Thyme (Zhang et al., 2025f), while avoiding their iterative tool-use latency (Figure 5). Beyond in-distribution perception, our approach further enhances models general multimodal cognition, improving out-of-distribution generalization on visual reasoning, AIGC detection, and GUI agent benchmarks. Diving deeper, our dual-view evaluation and attention map analysis show that ZwZ better localizes task-relevant micro evidence from the full image, effectively narrowing the globalregional zooming gap. Finally, we discuss the comparison and boundary between ZwZ and Thinking with Images, clarifying when tool-based image operations are necessary and when their benefits can be distilled into single forward pass. To summarize, our key contributions are the following: We propose Region-to-Image Distillation, which utilizes the expertise of MLLMs on cropped region to synthesize training data conditioned on the full image, thereby distilling the benefits of tool-based zooming in into single glance at inference . We introduce ZoomBench, diverse and rigorous benchmark for fine-grained multimodal perception, featuring broad coverage across multiple scope dimensions, humanAI hybrid construction pipeline, and various evaluation protocols. Experiments show that our method substantially improves fine-grained perception, exceeding tool-based agentic models while achieving significantly lower inference latency."
        },
        {
            "title": "2 Related Works",
            "content": "Fine-Grained Multimodal Perception. Recent advancements in fine-grained multimodal perception have shifted towards Thinking-with-Images paradigm, where MLLMs actively acquire local information during inference rather than relying solely on global image encoding (Yu et al., 2025c; Wei et al., 2025d; Zhang et al., 2025e; Jiang et al., 2025a; Hu et al., 2024; Zhou et al., 2024; Zhang et al., 2025d; Wei et al., 2025c; Zhang et al., 2025h; 2026; Ma et al., 2026; Wang et al., 2025h; Hou et al., 2025; Wu et al., 2025b). Notably, DeepEyes (Zheng et al., 2025b) and Mini-o3 (Lai et al., 2025) employ reinforcement learning to incentivize the usage of visual tools such as Zoom in (Crop) and Search, while Thyme (Zhang et al., 2025f) and PixelReasoner (Wang et al., 2025d) enable models to generate code or perform pixel-space operations to manipulate visual inputs dynamically. Other training-free approaches (Liu et al., 2024; Hu et al., 2024; Mondal et al., 2024; Shen et al., 2025; Zhang et al., 2025b; Liu et al., 2025b; Fu et al., 2025; Peng et al., 2025; Luan et al., 2024) employ tree search strategies or attention mapping to let the model zoom in the key region during inference. Though effective, this paradigm suffers from significant inference overhead. The requirement for multiple visual encoding passes or iterative tool call loops increases latency, making these methods less suitable for real-time applications. Besides, some works leverage specialized textual reasoning formats (Wang et al., 2025e;b) or latent visual reasoning (Yuan et al., 2025) to enhance perception, but these methods often require heavy format-specific supervision and careful training to induce the desired reasoning behavior, making data construction harder to scale (further discussed in Section 6.1). To this end, we propose Region-to-Image Distillation approach which aims to internalize the benefits of visual tools through pure RL training, without the need for explicit tool invocation or recursive processing during inference. Multimodal Synthetic Data. While datasets like Visual Genome (Krishna et al., 2017), VStar (Wu & Xie, 2024), Rexverse (Jiang et al., 2024), Thyme-SFT/RL (Zhang et al., 2025f), VGR-SFT (Wang et al., 2025e), and Visual Probe (Lai et al., 2025) involve valuable fine-grained perception data, their utility is significantly constrained by limited sample sizes, low difficulty for current MLLMs, or homogeneous image, task distributions, largely due to their reliance on manual annotation. This scarcity necessitates automated and scalable synthesis pipelines to derive high-quality VQA data from vast, unlabeled image corpora (Shi et al., 2025; Cai et al., 2025; Gao et al., 2025; Lin et al., 2026b;a; Wei et al., 2025b). Recent methods such as Genixer (Zhao et al., 2024), 3 MM-Evol (Luo et al., 2025), and Oasis (Zhang et al., 2025c) utilize proprietary MLLMs (e.g., GPT-4 (Hurst et al., 2024)) to synthesize large-scale visual instructions. However, they predominantly rely on global-to-global synthesis pipeline based on traditional teacher-to-student distillation, where teacher models are prompted with full images to generate QA pairs based on further workflows. This can easily lead to hallucinatory artifacts and poor perceptual grounding, as even advanced MLLMs struggle to discern minute details when facing coarse-grained image. Unlike these prior methods, our Region-to-Image Distillation targets the zooming gap in MLLMs by adopting region-centric synthesis strategy: we generate QA pairs from scratch using micro-crops as anchors to ensure grounded factuality, while training the model on the full image. Note that our method is also compatible with the synthesis workflows in these prior works (i.e., other synthesis workflows can be applied to QA pair generation on micro crops). Besides, some methods take different synthesis routes but are less aligned with natural-image VQA. For example, Multimodal Self-Instruct (Zhang et al., 2024a) synthesizes VQA data from programmatically generated, non-natural images. MED (Bai et al., 2025c) constructs minimally edited image pairs with semantically aligned descriptions to train MLLMs for fine-grained difference detection. Other approaches design self-supervised proxy tasks to generate data with verifiable ground truth (Zeng et al., 2025; Tao et al., 2025; Wu et al., 2025c). These synthetic settings can introduce distribution shift, limiting their generalization to real-world natural-image QA. Perception Benchmarks. The evaluation of fine-grained perception has led to several pioneering benchmarks (Tong et al., 2024; Yue et al., 2024; Wei et al., 2024). However, CV-Bench (Tong et al., 2024), VStar (Wu & Xie, 2024), and HR-Bench (Wang et al., 2025g) are limited by narrow task coverage, single evaluation protocol, or overly templated question styles. CountQA (Tamarapalli et al., 2025), RC-Bench (Niu et al., 2025), ColorBench (Liang et al., 2025), and GroundingME (Li et al., 2025d) only target specific fine-grained capabilities such as counting, Document OCR, color perception, and visual grounding. MME-RealWorld (Zhang et al., 2024b) and TreeBench (Wang et al., 2025b) covers broader sub-tasks, but they highly rely on labor-intensive and consuming manual construction. Our proposed benchmark, ZoomBench, addresses these shortcomings by leveraging our proposed Region-to-Image Distillation method to reduce human annotation cost. Beyond standard benchmarking, we also include two evaluation protocols for deeper analysis: (1) dual-view evaluation and (2) relative attention map evaluation."
        },
        {
            "title": "3 Zooming without Zooming",
            "content": "To instantiate the Zooming without Zooming idea, we make two primary contributions as follows: (1) our Region-to-Image Distillation method, which internalizes the benefits of tool-based zooming during training, and (2) the ZoomBench, which is designed to rigorously and comprehensively evaluate this capability. 3.1 Region-to-Image Distillation We propose Region-to-Image Distillation (R2I) as shown in Figure 3, an automated and scalable synthesis pipeline capable of deriving high-veracity VQA data from vast, unlabeled image corpora. It aims to distill fine-grained, region-grounded supervision into full-image VQA instances, enabling single-pass fine-grained perception at test time without any tool use. In fine-grained perception, the decisive evidence is often confined to micro-region in the image I, Setup. which is small and easily overwhelmed by the global context. Our goal is to optimize the student model by distilling the teachers regional-view expertise on into the students global-view predictions on I, which is defined as Region-to-Image Distillation. We explain the details of our proposed method as follows. (1) Zoom-in Synthesis on the Cropped Region. Given an image pool Draw, we define zoom-in function (I) that generates set of candidate bounding boxes {B1, B2, . . . , Bn} using an object recognition and segmentation system. For each box Bi, we extract the corresponding region Ri by cropping and resizing. Importantly, is object-centric: it proposes regions, each covering at least one visible object in the image, so these extracted regions are semantically meaningful and more likely to contain informative visual evidence. To ensure the synthetic data specifically target fine-grained challenges, we enforce that: Rtarget = , where τ is sparsity threshold (e.g., τ = 0.1). This filtering ensures that the evidence is hidden in the global view. For each micro-region Rtarget, teacher model serves as question generator to propose perception-centric questions QR that are strictly answerable from alone. These questions rely on fine-grained cues that are easily overlooked from the global view, such as tiny text, symbols, subtle attributes or small-instance counts. To obtain high-veracity labels without manual effort, we utilize teacher models as answer generators to derive the pseudo-label via majority voting for each question QR. Note that triplet (R, Q, A) is retained only if the answer generators reach high Ri Area(Bi) Area(I) < τ (cid:110) (cid:111) 4 Figure 3: Overview of Region-to-Image Distillation. We synthesize fine-grained VQA pairs on zoomed-in micro-crops using strong teachers with consensus filtering, then distill them to the full image via box-overlay grounding and an augmented prompt, enabling improved single-pass inference without test-time zooming. consensus, substantially reducing hallucinated or invalid questions. (2) Zoom-out Distillation to the Full Image. After obtaining reliable region-level supervision, we zoom out to distill these QA pairs into their original full-images, forming fine-grained training instances. However, question that is unambiguous when isolated within may become referentially ambiguous in the global image (please see Appendix 11.1 for examples). To resolve this referential uncertainty, we apply grounding transformation G(I, Q, B) (also can be viewed as 1, an inverse transformation of P) that yields an augmented training pair (I, Q). Specifically, overlays the visual bounding box onto and appends spatial constraint to Q. Consequently, the training triplet is formulated as (I, Q, A) (e.g., the Augmented VQA in Figure 3), where and ensure the question remains precisely anchored to the intended region. Furthermore, we perform difficulty filtering on the synthetic data using smaller multimodal model, removing overly easy samples and yielding the final training dataset Dsyn. Finally, the student training objective is formulated as: max θ (I,Q,A)Dsyn, (cid:98)Aπθ (I,Q) (cid:104) r( (cid:98)A, A) (cid:105) , (1) where r( (cid:98)A, A) is task reward and πθ is the student policy. Intuitive Explanation. natural question then arises: can the model still improve its performance at inference time when the bounding box is removed? To answer this, we can treat as privileged information (Vapnik & Vashist, 2009; Pechyony & Vapnik, 2010) available only during training. This learning paradigm has been both theoretically and empirically verified in prior works (Sharmanska et al., 2014; Collier et al., 2022; Bartolo et al., 2026; Jiang et al., 2025b). Formally, the model learns conditional distribution P(A I, Q, B) (nearly equivalent to P(a I, Q) as we use I, as an operational way to encode I, B) during training. Through visual grounding (box overlays), acts as structural hint that successfully forces the models internal attention mechanism to align with the micro-region (see Section 5.2). Crucially, as shown in our experiments (Section 4), this alignment indeed successfully generalizes to cases where is removed. General View. Algorithm 1 abstracts our method as tool-action distillation. In particular, given raw image I, we first apply tool-call action () to obtain an altered observation (cid:98)I (e.g., zoomed-in crop). We then use teacher generator to synthesize questionanswer pairs (Q, A) conditioned on (cid:98)I, so that the questions explicitly require the visual evidence revealed or emphasized by . Next, we map each question back to the 5 Algorithm 1 general view of our method. Require: Raw image pool Draw; tool-call action (); question-answer generator Ensure: Distilled dataset Dsyn = {(I, (cid:98)Q, A)} 1: Dsyn 2: for each image Draw do 3: 4: 5: 6: Dsyn Dsyn {(I, (cid:98)Q, A)} 7: end for 8: return Dsyn (cid:98)I = (I) (Q, A) ((cid:98)I) I, (cid:98)Q = 1((cid:98)I, Q) // conduct tool-call action (e.g., zoom-in) // generate question-answer pairs based on (cid:98)I // inverse transformation Table 1: Comparison of ZoomBench with other fine-grained multimodal perception benchmarks. MCQ denotes multiple-choice-question and OQ means open question. Difficulty (higher is better) is measured as 1 accuracy of Qwen2.5-VL-7B on each benchmark. Our benchmark emphasizes hybrid high-veracity collection, automatic evidence annotation, diverse challenging questions, along with two unique evaluation protocols. Question Collection Question Format Evidence Annotation Dimension Difficulty Dual-View Interpretability Data Construction Evaluation Protocol Benchmarks CV-Bench VStar HR-Bench MME-RealWorld TreeBench FINERS-4k Manual, Templated Manual, Templated Hybrid, Templated Manual Hybrid Manual MCQ MCQ MCQ MCQ MCQ MCQ, OQ ZoomBench (Ours) Hybrid (R2I) MCQ, OQ Manual Auto 4 2 6 10+ 10 4 24.7 19.9 29.6 37.4 63.0 33.0 57.5 original image domain via an inverse transformation 1, producing re-anchored question (cid:98)Q such that (I, (cid:98)Q) is unambiguous and still refers to the same evidence used to answer ((cid:98)I, Q). This yields distilled dataset Dsyn = {(I, (cid:98)Q, A)} that can train student to solve the task directly from the full image, thereby internalizing the benefits of the tool action into single forward pass of the student. Here, (cid:98)Q denotes the original question augmented with the tool/action description when necessary (e.g., explicitly stating how we perform the tool-call action on the image), while in cases without ambiguity we simply have (cid:98)Q = Q. Note that in our implementation, for stronger spatial grounding, we further convert (I, (cid:98)Q) into (I, Q) by (i) overlaying the bounding box on to form I, and (ii) appending an explicit constraint prompt to form Q. Although we instantiate as zooming in as it is the most widely used tool among Thinking with Images actions, the same framework can distill many other tool actions, such as flipping (Xu et al., 2025), 3D grounding (Han et al., 2025; Chen et al., 2025), and calling expert models (Lu et al., 2025; Zhou et al., 2025). We further discuss this in Section 6.2. Generally, by distilling the teachers regional-view expertise into the students global-view predictions, the student learns to gaze at critical evidence from the full image that would otherwise be overlooked (just like zoom-in in mind), thereby internalizing the benefits of zooming while retaining single-pass inference efficiency. More implementation details (box proposal, filtering thresholds, teacher choices, and prompting) are deferred to Appendix 8. 3.2 ZoomBench Construction. Existing multimodal fine-grained perception benchmarks (Tong et al., 2024; Wu & Xie, 2024; Wang et al., 2025g; Zhang et al., 2024b; 2025d; Wang et al., 2025b) often suffer from limited task coverage, overly templated question styles, low difficulty, or expensive manual construction. To pave the way for building high-quality (diverse, challenging, and easy-to-scale) benchmark at low cost, we repurpose our Region to Image Distillation method. Concretely, we utilize powerful MLLM to propose questions and candidate answers based on cropped micro regions, then mapped to the full images to form challenging perception tasks without any spatial grounding (e.g., bounding boxes). The cropped regions automatically serves as the evidence regions to answer the questions from the full images. After that, we incorporate human6 Figure 4: Category distribution across six fine-grained dimensions of our benchmark (left) and ZoomBench data statistics: distribution of image resolutions (middle) and crop-to-image area ratios (right). in-the-loop verification stage: human annotators are tasked only with checking the validity, difficulty, and correctness of the model-generated question-answer pairs against both the full images and cropped regions. This hybrid approach significantly reduces the labor-intensive burden of from-scratch labeling (creating questions and answers) while maintaining high quality. Statistics. The resulting ZoomBench comprises 845 diverse and challenging QA pairs with high-resolution images sourced from diverse image datasets (detailed in Appendix 8.1 and Appendix 8.2). We cluster these questions and find that they fall into six popular and major categories in fine-grained perception: (1) Fine-Grained Counting: targeting small and densely packed objects; (2) OCR: focusing on the text and symbol recognition; (3) Color Attributes: discerning subtle color variations in parts of the object; (4) Structural Attributes: examining geometric and shape-related properties such as object structure and part layout; (5) Material Attributes: recognizing material composition and surface properties (e.g., metal, wood, glass, fabric); and (6) Object Identification: distinguishing specific object types and species, such as flags, brands, landmarks, and notable figures. Evaluation Protocols. Our benchmark adopts hybrid evaluation format that combines multiple-choice questions (for cases where answer spaces are naturally discrete or where strict normalization is hard) and open questions with canonical target answers (please see Appendix 9.3 for details of the scoring system). Moreover, ZoomBench provides both the full image and the corresponding key-region crop for each instance, which supports deeper analysis in the subsequent section (Section 5). These include the proposed dual-view protocol to measure the zooming gap, and attention-map-based (Zhang et al., 2025b) study to interpretably demonstrate the internalized zooming ability. Table 1 further distinguishes the difference between ZoomBench and other existing ones. We also provide data statics demonstration in Figure 4, construction details in Appendix 8.1, and several examples of our benchmark in Appendix 11.2."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we describe the experimental settings and present main experimental results. We aim to evaluate the effectiveness of our proposed Region-to-Image Distillation method, focusing on whether our model can internalize zooming expertise to achieve fine-grained perception in single forward pass. 4.1 Experimental Settings Benchmarks. We consider 3 groups of benchmarks. (i) General perception: HR-Bench (Wang et al., 2025g), VStar (Wu & Xie, 2024), CV-Bench (Tong et al., 2024), MME-RealWorld (Zhang et al., 2024b), and our ZoomBench (full image setting). (ii) Specific perception: ColorBench (Liang et al., 2025) for color understanding and robustness, and CountQA (Tamarapalli et al., 2025) for counting in the wild. (iii) Out-of-distribution (OOD) generalization: MMStar (Chen et al., 2024) for general multimodal understanding, and BabyVision (Chen et al., 2026a) for visual reasoning. Model Training. We train Qwen3-VL-4B (Bai et al., 2025a), Qwen2.5-VL-7B (Bai et al., 2025b), and Qwen3VL-8B with reinforcement learning (DAPO (Yu et al., 2025a)) only on our 74K synthetic data (see Appendix 9 for details) without any SFT. We denote the resulting models as ZwZ (Zooming without Zooming), highlighting that they perform fine-grained perception from the full image in single glance. 7 Table 2: Main results on various benchmarks. We report accuracy (%) for each model. Among open-source models (except GPT-5.1 and Gemini-3-Flash), the best results are highlighted in bold, and the second-best are underlined. ZwZ consistently improves over the corresponding Qwen-VL baselines, achieving the best overall average among open-source models. Models GPT-5.1 Gemini-3-Flash Qwen3-VL-4B Qwen2.5-VL-7B Qwen3-VL-8B MiMo-VL-7B-RL MiniCPM-V-4.5 (9B) GLM-4.5V (108B) Qwen3-VL-235B-A22B Kimi-K2.5 (1T) ZwZ-4B (Ours) ZwZ-7B (Ours) ZwZ-8B (Ours) ZoomBench HR-4K HR-8K VStar CV-B. MME-RW-en MME-RW-cn GP-Avg CountQA ColorB. MMStar BabyVision General Perception Specific Perception OOD Generalization Closed-Source Models 67.00 87.88 65.25 85.00 70.16 86.39 84.22 89.57 64.04 74. 55.57 72.62 Open-Source Models 78.25 71.62 78.88 74.38 69.88 81.63 84.50 81.87 81.75 75.38 84.38 72.88 67.88 74.63 72.88 63.62 74.88 81.62 75.38 80.10 78.53 86.39 81.15 70.16 83.25 87.96 85. 84.95 75.34 85.44 84.31 80.25 87.59 86.72 89.18 63.47 60.80 65.96 63.40 58.16 66.04 67.07 71.51 Our Models 79.50 73.25 82.00 92.67 88.48 91.10 87.90 79.83 87. 68.52 66.21 69.87 63.63 58.30 66.67 59.78 56.23 60.71 65.29 68.40 68.09 66.96 70.59 47.22 59.29 40.24 42.49 37.87 45.09 42.60 49.23 49.11 56.33 55.74 55.62 58. 64.78 79.37 69.07 64.99 70.83 68.71 62.99 71.90 74.61 75.50 76.31 72.25 77.64 31.41 66.88 83.43 85.47 71.60 83. 28.14 18.91 28.99 28.27 23.43 35.93 40.58 52.81 30.82 20.72 32.40 81.63 76.36 82.77 82.80 79.75 84.59 85.62 86.61 83.08 80.82 83.59 69.73 61.93 70.93 73.53 67.87 75.87 76.33 81.80 71.13 63.40 73. 13.92 34.51 13.66 12.89 12.89 16.24 14.95 15.72 18.30 33.25 16.24 15.98 16.75 Avg 59.44 75.10 61.52 56.82 62.86 61.98 56.99 65.04 67.55 71. 66.86 62.42 68.12 Table 3: Comparison of different datasets. The best results are highlighted in bold, and the second-best are underlined. Model trained on our synthetic dataset achieves superior performance. Training Data Size Synthetic? ZoomBench HR-4K HR-8K VStar CV-B. MME-RW-en MME-RW-cn CountQA ColorB. MMStar BabyVision General Perception Specific Perception OOD Generalization Qwen3-VL-8B - 47K + DeepEyes data 55K + Thyme-RL data 37K + TreeVGR-RL data + Oasis data 500K + MM-Self-Instruct data 65K + our data 10K + our data 74K - 37.87 45.80 40.93 43.02 37.51 37.04 52.90 58.11 78.88 84.75 82.75 82.88 81.50 81.38 82.88 84. 74.63 80.12 78.05 81.00 77.62 78.75 81.38 82.00 86.39 88.48 91.62 90.05 83.77 82.72 91.62 91.10 85.44 88.62 88.29 83.71 87.07 86.32 87.78 87.40 65.96 68.71 71.14 69.12 64.96 68.02 68.43 69.87 66.67 71.03 71.17 69.73 70.26 67.48 69.63 70.59 28.99 30.56 30.96 23.49 27.23 28.73 33.97 32. 82.77 82.94 83.98 79.05 81.78 83.13 83.19 83.59 70.93 72.67 74.87 71.60 70.53 72.00 72.53 73.13 12.89 10.57 13.66 13.66 13.40 15.21 16.75 16.75 Avg 62.86 65.84 66.13 64.30 63.24 63.71 67.37 68.12 Baselines. We compare against 6 kinds of baselines: (a) closed-source frontier MLLMs, including GPT5.1 (OpenAI, 2025) and Gemini-3-Flash (Google, 2025); (b) strong open-source MLLMs, including Qwen3-VL235B (Bai et al., 2025a), GLM-4.5V (Hong et al., 2025b), Kimi-K2.5 (Team et al., 2026), MiniCPM-4.5-V (Yu et al., 2025b), and MiMo-VL-7B-RL (Li et al., 2025b); (c) Thinking-with-Images agentic models, including Pixel-Reasoner (Wang et al., 2025d), DeepEyes (Zheng et al., 2025b), DeepEyesV2 (Hong et al., 2025a), Thyme (Zhang et al., 2025f), Mini-o3 (Lai et al., 2025), SenseNova-MARS (Chng et al., 2025), and SkyworkR1V4 (Zhang et al., 2025g); (d) Qwen3-VL with official tool use, implemented following the tool-use recipes provided in the official Qwen3-VL Githubs cookbook; (e) models trained on existing open-sourced multimodal datasets, including synthetic data (Oasis (Zhang et al., 2025c), Multimodal-Self-Instruct (Zhang et al., 2024a)) as well as human-curated data (DeepEyes (Zheng et al., 2025b), Thyme-RL (Zhang et al., 2025f), TreeVGR-RL (Wang et al., 2025b)); (f) models trained with synthetic data from proxy tasks, including AGILE-7B (Zeng et al., 2025), DiG-8B (Tao et al., 2025), and Image-Jigsaw-7B (Wu et al., 2025c). Note that we do not include other training-free methods (Liu et al., 2024; Mondal et al., 2024; Hu et al., 2024; Shen et al., 2025; Zhang et al., 2025b; Liu et al., 2025b; Fu et al., 2025; Peng et al., 2025) for comparison because they are compatible with our method without any conflict. Besides, we all use the Instruct version of Qwen3-VL models with different sizes (4B, 8B, 235B) for training and evaluation. 4.2 Results Leading Performance across Various Benchmarks. As shown in Table 2, training on our data yields consistent gains on all benchmarks, not limited to perception tasks. Notably, ZwZ-8B improves the Qwen3VL-8B baseline from 61.52 to 68.12 average score with large gains on ZoomBench (37.87 58.11), and similar improvements are also observed for ZwZ-4B/7B with different scales. On the average of general perception benchmarks, ZwZ-4B and ZwZ-8B also surpass all open-source models including GLM-4.5V, Qwen3-VL235B, and Kimi-K2.5 with much larger sizes, and remains competitive with the SOTA closed-source model Gemini-3. This clearly indicates that our data effectively distills the expertise of teacher models (GLM-4.5V 8 Table 4: We compare our models (single forward pass) with agentic models on several perception benchmarks. The best results are highlighted in bold, and the second-best are underlined. Models VStar HR-4K HR-8K MME-RW-en Avg Base Models (Single Forward Pass) Qwen3-VL-4B Qwen2.5-VL-7B Qwen3-VL-8B 80.1 78.5 86.4 78.3 71.6 78.9 72.9 67.9 74.6 Thinking-with-Images Agentic Models Pixel-Reasoner (7B) DeepEyes (7B) Thyme (7B) DeepEyesV2 (7B) Mini o3 (7B) SenseNova-MARS-8B Skywork-R1V4 (30B) 84.3 90.1 82.2 81.8 88.2 92.2 88.0 72.6 75.1 77.0 77.9 77.5 83.1 82.8 66.1 72.6 72.0 73.8 73.3 78.4 79.8 Qwen3-VL with Official Tool Use Qwen3-VL-4B+tool Qwen3-VL-8B+tool 88.0 90. 81.3 82.3 74.4 78.0 Our Models (Single Forward Pass) ZwZ-4B ZwZ-7B ZwZ-8B 92.7 88.5 91.1 81.8 75.4 84. 79.5 73.3 82.0 63.5 58.3 66.0 64.4 64.1 64.8 64.9 65.5 67.9 71.4 65.4 66.0 68.5 66.2 69.9 73.7 69.1 76. 71.9 75.5 74.0 74.6 76.1 80.4 80.5 77.3 79.1 80.6 75.9 81.9 Figure 5: Benchmark accuracy versus inference speed. Our models achieve higher accuracy than base models and agentic baselines while retaining single-pass efficiency. and Qwen3-VL-235B) at cropped region via zooming to student models (ZwZ), and even enables the students to outperform the teachers when given the full image as global input. Beyond perception-centric benchmarks, ZwZ also reveals strong out-of-distribution performance on MMStar and BabyVision, indicating that learning to reliably retrieve micro evidence from cluttered global inputs benefits broader multimodal understanding and visual reasoning, rather than overfitting to narrow perception patterns. Surpassing Existing Open Datasets. To isolate the contribution of our Region-to-Image distilled supervision, we fine-tune the same backbone (Qwen3-VL-8B) using DAPO on several representative public datasets, including human-curated non-synthetic data (DeepEyes, Thyme-RL) and large-scale synthetic data (Oasis, MM-Self-Instruct). Table 3 shows that our distilled samples (10K, 74K) demonstrate strong data efficiency and outperform all alternatives, including datasets that are an order of magnitude larger (e.g., Oasis 500K), suggesting that the key factor is high quality and fine granularity rather than sheer data volume. Outperforming Proxy Task-Based Data Synthesis Methods. Beyond comparisons with large-scale datasets, we evaluate ZwZ against methods that similarly employ synthetic data generated from proxy tasks (e.g., image jigsaw) with verifiable and reliable ground truth. These results validate that our method yield stronger, more generalizable performance, and is more practical and effective to improve small size (under 10B) models than training with proxy synthetic objectives. Exceeding Thinking-with-Images Models. We further compare ZwZ with representative agentic baselines that explicitly zoom-in during inference. Table 4 shows that our single-pass models are competitive with, and often surpass, strong agentic systems. In particular, ZwZ-8B achieves the best overall average among the listed methods, demonstrating that our approach can exceed the accuracy benefits of inferencetime zooming while avoiding iterative tool calls. Internalizing Tool-Use Benefits with Lower Latency. We also implement the official Qwen3-VL tool-use pipeline and compare against it  (Table 4)  . ZwZ surpasses the tool-use counterpart while using only single forward pass, directly suggesting that Region-to-Image Distillation effectively internalizes the gains of tool-based zooming into model weights. Moreover, Figure 5 reports the accuracyspeed trade-off, where inference speed is defined as the inverse of the average per-sample inference time on ZoomBench, and benchmark accuracy is the average score in Table 4. ZwZ attains higher accuracy with substantially lower inference cost (around 10 faster inference speed) than agentic and tool-use baselines. 4.3 Ablation Study In this section, we conduct series of ablation experiments on Qwen3-VL-8B to investigate the impact of key components in our Region-to-Image (R2I) distillation framework. Note that we use 10K data for efficient comparison in our ablation study. 9 Table 5: Comparison with proxy task-based synthetic data methods on perception benchmarks. Table 6: Ablation study using different data synthesis strategies. Benchmarks AGILE-7B DiG-8B Image-Jigsaw-7B ZwZ-7B ZwZ-8B Strategies Zoom-Bench HR-4K HR-8K VStar CountQA ColorBench HR-Bench-4K HR-Bench-8K VStar MMStar 73.0 70.5 80.6 - - 70.4 81.2 72.7 - 71.1 80.6 - 75.4 73.3 88.5 63.4 84.4 82.0 91.1 73. Direct Synthesis 40.95 82.12 78.12 84.82 32. R2I + bbox-in-image + bbox-in-question + no-bbox 52.90 46.98 46.27 82.28 79.25 81.50 81.38 77.12 80.62 91.62 89.53 88. 33.97 31.53 28.27 83.45 83.19 82.62 83.09 Avg 66.94 70.89 67.84 68. Figure 6: Generalization to real-world tasks. ZwZ-8B consistently improves over Qwen3-VL-8B on AIGC detection and GUI agent benchmarks. Figure 7: Dual-view evaluation. ZwZ-8B exhibits strong performance on both Global-View and Regional-View with the smallest zooming gap. Effectiveness of Region-to-Image Distillation. We first compare our Region-to-Image Distillation method with the Direct Synthesis method, which nearly represents the traditional global-to-global synthesis and teacher model to student model distillation approaches (e.g., Genixer (Zhao et al., 2024), MM-Evol (Luo et al., 2025), Oasis (Zhang et al., 2025c)). In this setting, the teacher model is prompted with the full image to generate VQA pairs to train the student model. As shown in Table 6, our method (R2I + bbox-in-image) significantly outperforms Direct Synthesis across all benchmarks, which confirms that generating supervision on micro-crops effectively bypasses the hallucinatory tendencies of teacher models when facing global views, thereby providing higher-quality training signals. Importance of Visual Grounding. critical challenge in R2I is the referential ambiguity when distilling crop-derived answers back to the full image. One straightforward way is designing an agentic workflow to verify whether each synthesized QA pair is strictly unambiguous at the full-image level. However, this persample validity checking is costly and would also filter out substantial portion of the data. An alternative is to query the MLLM to rewrite each question conditioned on the full image to remove ambiguity. However, in our manual inspection, many rewritten questions introduce severe hallucinations. Thus, we ablate three grounding strategies (including our final chosen strategy): (a) R2I + no-bbox: Directly distilling the cropbased VQA pairs to the full image without any spatial guidance. (b) R2I + bbox-in-question: Providing the target bounding box coordinates as text (e.g., [x1, y2, x2, y2]) within the prompt. (c) R2I + bbox-in-image (ours): Overlaying the bounding box directly onto the image. As illustrated in Tabel 6, both the no-bbox and bbox-in-question setting performs worse than the bbox-in-image format. We posit that overlaying boxes on images acts as strong privileged information. Besides its original purpose to resolve referential uncertainty, it also effectively forces the models visual encoder and cross-modal projector to learn direct mapping between the global context and the specific micro-region compared the putting bbox in question or without adopting bbox. Crucially, although these boxes are only present during training, our results (where no boxes are provided at test time) demonstrate that the model effectively internalizes this zoom-in (or called gazing) capability, achieving superior fine-grained perception in single glance. 4.4 Real World Tasks Generalization. To evaluate broader applicability, we further test on real-world tasks that require strong multimodal grounding, including AIGC detection (LOKI (Ye et al., 2024), FakeCLUE (Wen et al., 2025)) and GUI agents (ScreenSpot Pro (Li et al., 2025c) and OSWorldG (Xie et al., 2025)). As shown in Figure 6, ZwZ-8B consistently outperforms Qwen3-VL-8B across these benchmarks. These results demonstrate that our approach enhances not only fine-grained perception but also foundational multimodal capabilities across diverse real-world applications. Table 7: Detailed dual-view results on ZoomBench. We report Global-View, Regional-View, and their difference (Zooming Gap) across six perceptual dimensions. Note that row-wise averages are weighted by the number of samples in each dimension, since different dimensions contain different amounts of data. For the averaged scores, the best are highlighted in bold, and the second-best are underlined. Models Dual-View ZoomBench Counting OCR Color Structure Material Identification Qwen3-VL-8B MiniCPM-4.5-V GLM-4.5V Qwen3-VL-235B Kimi-K2.5 GPT-5.1 Gemini-3-Flash ZwZ-8B Average Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap Global Regional Zooming Gap 29.90 48.04 18.14 31.86 45.10 13.24 32.84 57.84 25.00 37.75 58.33 20.58 44.61 62.25 17.64 34.80 52.45 17.65 42.16 68.14 25. 40.69 54.90 14.21 36.83 55.88 19.05 47.97 38.02 69.11 64.05 21.14 26.03 47.97 44.63 78.05 67.77 30.08 23.14 63.41 50.41 78.05 74.79 14.64 24.38 61.79 46.69 82.11 71.49 20.32 24.80 67.48 53.31 80.49 71.90 13.01 18.59 54.47 48.76 78.86 71.49 24.39 22.73 80.49 54.55 89.43 74.38 19.83 8.94 67.48 68.60 81.30 81.40 13.82 12.80 61.38 50.62 79.67 72.16 18.29 21.54 37.42 74.19 36.77 46.45 71.61 25.16 52.90 81.29 28.39 58.06 78.71 20.65 62.58 79.35 16.77 49.03 78.71 29.68 61.94 87.10 25. 58.71 76.13 17.42 53.39 78.39 25.00 39.34 63.93 24.59 49.18 75.41 26.23 47.54 80.33 32.79 42.62 77.05 34.43 65.57 83.61 18.04 57.38 77.05 19.67 72.13 81.97 9.84 57.38 78.69 21.31 53.89 77.25 23.36 43.33 68.33 25.00 43.33 71.67 28.34 63.33 86.67 23.34 55.00 78.33 23.33 60.00 75.00 15.00 53.33 76.67 23.34 73.33 80.00 6. 55.00 75.00 20.00 55.83 76.46 20.63 Avg 37.87 63.08 25.21 42.60 65.33 22.73 49.23 73.61 24.38 49.11 72.07 22.96 56.33 73.25 16.92 47.22 70.06 22.84 59.29 78.34 19.05 58.11 73.37 15.26 49.97 71.14 21."
        },
        {
            "title": "5 Deeper Analysis on ZoomBench",
            "content": "Going beyond standard benchmarking, we leverage ZoomBench to more carefully diagnose fine-grained perception under two additional evaluation protocols. 5.1 Dual-View Evaluation Motivated by Zhang et al. (2025b), we introduce dual-view evaluation protocol to explicitly measure the zooming gap in ZoomBench. Each instance is evaluated in two conditions: (1) Global-View, where the model answers from the full image; and (2) Regional-View, where the model is given the corresponding microcropped region. The Regional-View accuracy serves as an empirical upper bound (whether the model can answer when the evidence is explicitly visible), while the Global-View accuracy measures whether the model can find and use the same evidence from the full image. We define the zooming gap as the GlobalRegional performance difference; larger gap indicates that the sample is solvable given the evidence, yet the model fails to reliably pay attention to it under realistic global inputs, which is precisely the fine-grained perception (not recognition) bottleneck targeted by ZoomBench. Figure 7 and Table 7 summarize the dual-view results and the breakdown over six perceptual dimensions. From Figure 7, we observe consistent and substantial zooming gap for vanilla MLLMs across scales. For 11 Table 8: Attention map bounding-box coverage on ZoomBench. Higher is better. Models QwenVL-4B QwenVL-7B QwenVL-8B ZwZ-4B ZwZ-7B ZwZ-8B Coverage (%) 17. 12.44 17.39 21.45 (4.11) 13.39 (0.95) 21.64 (4.25) example, Qwen3-VL-8B drops from 63.08% (Regional) to 37.87% (Global), yielding 25.21% gap, which suggests that many failures stem from perceptual oversight rather than insufficient recognition or reasoning. Moreover, even much larger models (e.g., GLM-4.5V, Qwen3-VL-235B, Kimi-K2.5, and GPT-5.1) still exhibit notable gaps, indicating that scaling parameters alone does not eliminate the fine-grained retrieval bottleneck. In contrast, ZwZ-8B largely narrows this gap to only 15.26%, smallest among the tested models. It also achieves the second highest Global-View accuracy (58.11%) that nearly rivals the Regional-View performance of the vanilla Qwen3-VL-8B baseline. dimension-wise analysis of Table 7 further reveals several key insights: (1) Counting emerges as the most challenging dimension. Unlike other dimensions where Regional-View substantially improves performance, Counting remains difficult even after zooming in, suggesting that the task requires not only precise localization of densely packed objects but also sophisticated reasoning to avoid overor under-counting. (2) The largest average zooming gaps appear in Structure and Material, suggesting that subtle surface/texture attributes are especially prone to attention dilution under global inputs. (3) In contrast, OCR and Identification show smaller gaps, implying these more salient, discrete cues are comparatively easier to exploit. Generally, these results demonstrate that our Region-to-Image Distillation effectively teaches the model to focus on the needle in the haystack directly from the global view and effectively bridges the gap between global and regional perception without requiring test-time tool calling. 5.2 Attention Map Coverage Analysis To further evaluate whether the model grounds its predictions on task-relevant image regions from view of interpretability, we measure how much of the Relative Attention (Zhang et al., 2025b) falls inside the annotated key-region bounding box for each VQA sample in ZoomBench. For each image-question pair, we compute relative attention map Arel over the visual token grid (see Appendix 12 for details). Note that in ZoomBench, each sample is annotated with ground-truth bounding box = (x1, y1, x2, y2) on the original image, indicating the minimal key region required to answer the question. Thus, we map the box to the visual token grid to obtain B, and then define the Relative-Attention Coverage Ratio as the fraction of total relative-attention mass inside the key region: Coverage(B) = (i,j) Arel(i, j) (i,j) Arel(i, j) . (2) higher coverage value indicates that the models question-relevant visual evidence is more concentrated within the annotated key region. Results. Table 8 reports the average bounding-box coverage of relative-attention mass on ZoomBench. Across all model scales, our ZwZ variants consistently achieve higher coverage than their corresponding Qwen-VL baselines, indicating that the models question-relevant attention is more concentrated within the annotated key region. This also suggests that our method enhances the models ability to find (pay attention to) and utilize micro-level evidence during inference, and thereby narrowing the GlobalRegional zooming gap discussed in Section 5.1. We also present some demonstrations of attention map comparison in Appendix 12."
        },
        {
            "title": "6 Discussion and Future Direction",
            "content": "In this section, we firstly compare with different recently popular multimodal thinking paradigms. Then, we further talk about when to use agentic workflows like Thinking with Images and when not to. 6.1 Comparison of Different Multimodal Thinking Paradigms Currently, there are many works that focus on developing new multimodal thinking paradigms beyond standard textual reasoning. We summarize several representative ones as follows. (1) Textual CoT with Additional Tags or Structured Traces. line of work injects explicit intermediate structures into the reasoning trace, such as special tokens and bounding-box, line coordinates for visual 12 grounding (Yuan et al., 2025; Zhang et al., 2025a; Yang et al., 2025c; Zhu et al., 2026; Yang et al., 2025b; Chen et al., 2026b; Li et al., 2025e). While such formats can improve faithfulness, controllability, and clear interpretability on targeted tasks, these training data are often expensive to curate. They also risk format overfitting: the model may over-rely on particular reasoning template that is not universally helpful, raising an adaptivity issue, i.e., the model must learn when to follow the template versus answer directly under out-of-distribution (OOD) inputs. (2) Thinking with Images: Tool-Based Agentic Reasoning. Agentic TwI methods (Zheng et al., 2025b; Wang et al., 2025e; Zhang et al., 2025f) (we mainly compare with) call external tools (zoom, rotate, detectors, retrievers, etc.) during inference to obtain improved observations or auxiliary signals. They are flexible and can handle open-world settings with clear interpretability, but are typically slower due to multi-step interaction and require solving an adaptive tool-use problem: deciding whether, when, and how to invoke tools. mismatched or overly-used tool policy can degrade performance in many scenarios. (3) Thinking with Images: Native Image Generation. Another direction focuses on developing unified multimodal models (Deng et al., 2025; Li et al., 2025a; Zhang et al., 2025i) or world models (Zhao et al., 2025; Wiedemer et al., 2025) that use native image generation as scratchpad (e.g., drawing or editing images) during reasoning. This mutlimodal reasoning paradigm is also called thinking with generating images. This can provide additional visual cues, but inference is very slow (image generation is expensive) and training typically needs paired interleaved image-text interaction data, making large-scale data construction challenging. Moreover, current unified models (e.g., Bagel (Deng et al., 2025)) still lag behind state-of-the-art textual-CoT-based multimodal LLMs (e.g., Qwen3-VL (Bai et al., 2025a)) in multimodal understanding tasks. (4) Thinking with Images: Implicit Latent Reasoning. To solve the slow inference problem in thinking with generating images, many implicit approaches replace explicit image-generation steps with latent-space multimodal reasoning (Wang et al., 2025f; Dong et al., 2025; Wang et al., 2026; Wu et al., 2026; He et al., 2025; Yang et al., 2025d). Though promising, currently shifting pretrained model from default textual reasoning to latent reasoning (generating images in the latent space) typically requires large-scale and diverse interleaved image-text post-training data, which can be costly. This is similar to what is needed to train unified models, and also faces an adaptivity issue to avoid harming OOD performance. In addition, this reasoning paradigm also lacks clear visual interpretability, and current models trained in the paradigm remain behind strong multmodal LLM baselines. For instance, Monet-7B (using latent CoT) reports 71.0 on HR-Bench-4K and 83.3 on VStar, compared with DeepEyes (tool-base agenetic model; 75.1 / 90.1) and ZwZ-7B (text-only CoT model; 75.4 / 88.5) under the same benchmarks. (5) Textual CoT with Stronger Perception (Ours). We summarize the limitations of prior methods along three practical axes: (i) test-time cost, (ii) data/engineering overhead, and (iii) adaptivity. We find that many image-space operations can be described and constrained in certain language patterns, allowing pure-text reasoning to cover broad set of perceptual image operations without the need of additional Thinking with Images during inference. Therefore, in our implementation, we choose to maintain the simplest and the most widely used paradigm, i.e., textual CoT, and focus on improving perception under single-pass inference via reinforcement learning. This post-training recipe is simple, efficient and scalable: (1) synthetic VQA pairs are generated from images alone, making data construction largely automatic and easy to scale; (2) the model does not rely on SFT for external guidance, thereby maintaining OOD performance; (3) it also guarantees advanced performance after post-training once we get strong pretrained multimodal model (current SOTA MLLMs are usually based on textual CoT for reasoning, e.g., Qwen3-VL). It achieves good tradeoff among inference speed, clear interpretability, and strong performance. 6.2 Rethinking Thinking with Images Note that our superior results of Region-to-Image Distillation (R2I) on various benchmarks actually do not argue against Thinking-with-Images (denoted as TwI). Rather, they suggest practical criterion (shown in Table 9): TwI is most valuable when the action produces unpredictable information gain. Information-Gain Actions: TwI Remains Essential. TwI is well motivated when image actions result to new information. For example, web search or retrieval (Wu et al., 2025a) is essential as it brings in new images or documents. In these cases, the tools outputs are totally unpredictable from the current view of image and cannot be replaced by purely internalized weights, because the agent must interact with an external source to gain more information. 1. Note that zooming in can sometimes yield information gain. When the original image is too high-resolution to be fed into the MLLM directly, it is typically downsampled, which can erase fine details in small regions. Zooming in such regions preserves their effective resolution, recovering visual evidence that is missing in the downsampled global view and thus providing additional information compared to not zooming in. In our data synthesis and training, we encode images at full resolution without downsampling. 13 Table 9: Distillability of common TwI tool/actions under the information-gain criterion. Information-neutral actions are usually predictable based on current image view and the MLLMs strong cognition. Therefore, we argue that these benefits can be internalized into the MLLM by training solely on textual CoT via our method (Algorithm 1). Category Tool/Action for Images (examples) Predictable? Information-gain actions (unpredictable external information) Web search / retrieval Bring in new, unseen images from external sources (Wu et al., 2025a) Information-neutral actions (reformat/reveal existing evidence) Zoom in (crop)1 Zoom out Flip Rotate Denoise 2D grounding 3D grounding Draw images Zooming in key regions for easier perception (Zheng et al., 2025b) Zooming out for detecting adversarial AIGC (Xu et al., 2025) Horizontal/vertical flipping (Xu et al., 2025) Rotation for viewpoint normalization (Zhang et al., 2025f) Remove Gaussian noise (Xu et al., 2025) 2D object detection and segmentation (Lu et al., 2025; Zhou et al., 2025) Depth estimator; 3D object detection and segmentation (Chen et al., 2025) Draw sketches or auxiliary lines (Wei et al., 2025a; Zhao et al., 2025) Information-Neutral Actions: Distill instead of Acting. In contrast, many common TwI actions concentrate on image editing (e.g., zooming (Zheng et al., 2025b), flipping (Xu et al., 2025), rotating (Zhang et al., 2025f), objection detection and segmentation (Lu et al., 2025; Zhou et al., 2025), drawing auxiliary constructs (Wei et al., 2025a; Zhao et al., 2025; Wu et al., 2025b; Wiedemer et al., 2025)) that are often predictable based on current image view and the MLLMs strong cognition. They typically do not add new information; they mainly reduce perceptual difficulty by making existing evidence easier to access. For such cases, thinking with images at inference time can be inefficient and unnecessary, and the benefits can often be internalized into the MLLM (we assume that MLLM has enough knowledge capacity) through training with textual CoT alone, which is exactly what our method (R2I) does (Algorithm 1). 6.3 Rethinking Agentic Workflows (Test-Time Scaling) Diving deeper, we further analyze whether the popular and classical parallel divide-and-conquer agentic workflows (e.g., ReAct (Yao et al., 2022)), which is also similar to test-time scaling techniques (Yuksekgonul et al., 2026; Zheng et al., 2026), can also be internalized into LLMs single forward pass. In our view, if the result of each process in the workflow is exactly part of the final answer, parallelization primarily improves wall-clock speed and can sometimes reduce cost by delegating simpler processes to cheaper LLMs. It therefore remains practically useful and essential. For example, in agentic systems such as Cursor (Cursor, 2024), Claude Code (Anthropic, 2025), and OpenClaw (OpenClaw, 2026), two agents can work concurrently on frontend (using cheaper LLM) and backend code (using more expensive but much stronger LLM), and each process produces an artifact that directly constitutes part of the final deliverable. In contrast, if intermediate outcome of parallel process does not appear in the final output but merely serves as auxiliary reference (e.g., using agent debate in MoltBook (MoltBook, 2026) to solve problems), then the benefits of such parallelization workflow can be internalized through training (e.g., DeepSeek-R1 (Guo et al., 2025) internalizes the reflection-based workflow into its own thinking pattern, and MM-UPT (Wei et al., 2025b) internalizes the benefits of majority voting into the models single forward pass). This also resembles the System 2 to System 1 Distillation. 6.4 Limitation current limitation is that few perception-related tasks like spatial reasoning and multi-object perception have not been widely incorporated into our approach, and thus we have not evaluated performance on benchmarks such as TreeBench (Wang et al., 2025b) that largely focus on these abilities. However, our methodology can be readily extended to this domain by synthesizing training data through spatial reasoning or multi-object searching tools (Chen et al., 2025; Wu et al., 2025b; Wang et al., 2025g) based on Algorithm 1. 6.5 Future Direction promising direction is to develop unified and dynamic agent policy (Wang et al., 2025a; Bai et al., 2026). Based on our analysis, this agent can (i) default to single-pass inference with enhanced perceptual skills, (ii) decide when and how to invoke tools, and (iii) prioritize information-gain TwI actions, while using information-neutral operations only sparingly. This would combine the efficiency of our method with the open-world capability of agentic TwI."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose Zooming without Zooming, Region-to-Image Distillation framework that converts inference-time zooming (in and out) to training-time primitive. By synthesizing high-veracity VQA supervision on microcrops and distilling it back to full images with explicit grounding, our approach teaches MLLMs to recover fine-grained evidence from global inputs in single forward pass, eliminating the latency of tool-based Thinking-with-Images inference. To rigorously evaluate this capability, we introduce ZoomBench along with dual-view protocol that quantifies the globalregional zooming gap. Experiments show that our models consistently improve fine-grained perception across benchmarks, outperform agentic and official tool-use baselines with much lower inference cost, and improve general multimodal cognition, yielding notable gains on visual reasoning and real-world tasks. Analyses further confirm that our models better pay attention to task-relevant micro regions and substantially narrow the zooming gap. References Anthropic. Claude code. https://github.com/anthropics/claude-code, 2025. Hongbo Bai, Yujin Zhou, Yile Wu, Chi-Min Chan, Pengcheng Wen, Kunhao Pan, Sirui Han, and Yike Guo. Glance-or-gaze: Incentivizing lmms to adaptively focus search via reinforcement learning. arXiv preprint arXiv:2601.13942, 2026. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, et al. Qwen3-vl technical report. arXiv preprint arXiv:2511.21631, 2025a. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. Tianyi Bai, Yuxuan Fan, Jiantao Qiu, Fupeng Sun, Jiayi Song, Junlin Han, Zichen Liu, Conghui He, Wentao Zhang, and Binhang Yuan. Hallucination at glance: Controlled visual edits and fine-grained multimodal learning. arXiv preprint arXiv:2506.07227, 2025c. Matthias Bartolo, Dylan Seychell, Gabriel Hili, Matthew Montebello, Carl James Debono, Saviour Formosa, and Konstantinos Makantasis. Enhancing object detection with privileged information: model-agnostic teacher-student approach. arXiv preprint arXiv:2601.02016, 2026. Mengzhang Cai, Xin Gao, Yu Li, Honglin Lin, Zheng Liu, Zhuoshi Pan, Qizhi Pei, Xiaoran Shang, Mengyuan Sun, Zinan Tang, et al. Opendataarena: fair and open arena for benchmarking post-training dataset value. arXiv preprint arXiv:2512.14051, 2025. Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, et al. Sam 3: Segment anything with concepts. arXiv preprint arXiv:2511.16719, 2025. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 35583568, 2021. Liang Chen, Weichu Xie, Yiyan Liang, Hongfeng He, Hans Zhao, Zhibo Yang, Zhiqi Huang, Haoning Wu, Haoyu Lu, et al. Babyvision: Visual reasoning beyond language. arXiv preprint arXiv:2601.06521, 2026a. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? Advances in Neural Information Processing Systems, 37:2705627087, 2024. Meida Chen, Qingyong Hu, Zifan Yu, Hugues Thomas, Andrew Feng, Yu Hou, Kyle McCullough, Fengbo Ren, and Lucio Soibelman. Stpls3d: large-scale synthetic and real aerial photogrammetry 3d point cloud dataset. arXiv preprint arXiv:2203.09065, 2022. Shuhang Chen, Yunqiu Xu, Junjie Xie, Aojun Lu, Tao Feng, Zeying Huang, Ning Zhang, Yi Sun, Yi Yang, and Hangjie Yuan. Cogflow: Bridging perception and reasoning through knowledge internalization for visual mathematical problem solving. arXiv preprint arXiv:2601.01874, 2026b. 15 Siyi Chen, Mikaela Angelina Uy, Chan Hee Song, Faisal Ladhak, Adithyavairavan Murali, Qing Qu, Stan Birchfield, Valts Blukis, and Jonathan Tremblay. Spacetools: Tool-augmented spatial reasoning via double interactive rl. arXiv preprint arXiv:2512.04069, 2025. Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, et al. Sensenova-mars: Empowering multimodal agentic reasoning and search via reinforcement learning. arXiv preprint arXiv:2512.24330, 2025. Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, et al. Meta clip 2: worldwide scaling recipe. arXiv preprint arXiv:2507.22062, 2025. Mark Collier, Rodolphe Jenatton, Effrosyni Kokiopoulou, and Jesse Berent. Transfer and marginalize: Explaining away label noise with privileged information. In International conference on machine learning, pp. 42194237. PMLR, 2022. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Cursor. Cursor: The ai code editor. https://cursor.com, 2024. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Shuai Dong, Siyuan Wang, Xingyu Liu, Chenglin Li, Haowen Hou, and Zhongyu Wei. Interleaved latent visual reasoning with selective perceptual modeling. arXiv preprint arXiv:2512.05665, 2025. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. Xin Gao, Xiaoyang Wang, Yun Zhu, Mengzhang Cai, Conghui He, and Lijun Wu. Closing the data loop: Using opendataarena to engineer superior training datasets. arXiv preprint arXiv:2601.09733, 2025. Google. Gemini 3. https://blog.google/products-and-platforms/products/gemini/gemini-3/, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yi Han, Cheng Chi, Enshen Zhou, Shanyu Rong, Jingkun An, Pengwei Wang, Zhongyuan Wang, Lu Sheng, and Shanghang Zhang. Tiger: Tool-integrated geometric reasoning in vision-language models for robotics. arXiv preprint arXiv:2510.07181, 2025. Zefeng He, Xiaoye Qu, Yafu Li, Tong Zhu, Siyuan Huang, and Yu Cheng. Diffthinker: Towards generative multimodal reasoning with diffusion models. arXiv preprint arXiv:2512.24165, 2025. Jack Hong, Chenxiao Zhao, ChengLin Zhu, Weiheng Lu, Guohai Xu, and Xing Yu. Deepeyesv2: Toward agentic multimodal model. arXiv preprint arXiv:2511.05271, 2025a. Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, et al. Glm-4.5v and glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning. arXiv preprint arXiv:2507.01006, 2025b. Xinhai Hou, Shaoyuan Xu, Manan Biyani, Moyan Li, Jia Liu, Todd Hollon, and Bryan Wang. Codev: Code with images for faithful visual reasoning via tool-aware policy optimization. arXiv preprint arXiv:2511.19661, 2025. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. 16 Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Chaoya Jiang, Yongrui Heng, Wei Ye, Han Yang, Haiyang Xu, Ming Yan, Ji Zhang, Fei Huang, and Shikun Zhang. Vlm-r3: Region recognition, reasoning, and refinement for enhanced multimodal chain-of-thought. arXiv preprint arXiv:2505.16192, 2025a. Jun-Peng Jiang, Yu Xia, Hai-Long Sun, Shiyin Lu, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, De-Chuan Zhan, and Han-Jia Ye. Multimodal tabular reasoning with privileged structured information. arXiv preprint arXiv:2506.04088, 2025b. Qing Jiang, Gen Luo, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, and Lei Zhang. Chatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017. Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search. arXiv preprint arXiv:2509.07969, 2025. Ang Li, Charles Wang, Deqing Fu, Kaiyu Yue, Zikui Cai, Wang Bill Zhu, Ollie Liu, Peng Guo, Willie Neiswanger, Furong Huang, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025a. Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, et al. Xiaomi mimo-vl-miloco technical report. arXiv preprint arXiv:2512.17436, 2025b. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. In Proceedings of the 33rd ACM International Conference on Multimedia, pp. 87788786, 2025c. Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, et al. Groundingme: Exposing the visual grounding gap in mllms through multi-dimensional evaluation. arXiv preprint arXiv:2512.17495, 2025d. Zejun Li, Yingxiu Zhao, Jiwen Zhang, Siyuan Wang, Yang Yao, Runzhou Zhao, Jun Song, Bo Zheng, and Zhongyu Wei. Mixture-of-visual-thoughts: Exploring context-adaptive reasoning mode selection for general visual reasoning. arXiv preprint arXiv:2509.22746, 2025e. Yijun Liang, Ming Li, Chenrui Fan, Ziyue Li, Dang Nguyen, Kwesi Cobbina, Shweta Bhardwaj, Jiuhai Chen, Fuxiao Liu, and Tianyi Zhou. Colorbench: Can vlms see and understand the colorful world? comprehensive benchmark for color perception, reasoning, and robustness. arXiv preprint arXiv:2504.10514, 2025. Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, and Lijun Wu. Mmfinereason: Closing the multimodal reasoning gap via open data-centric methods. arXiv preprint arXiv:2601.21821, 2026a. Honglin Lin, Chonghan Qin, Zheng Liu, Qizhi Pei, Yu Li, Zhanping Zhong, Xin Gao, Yanfeng Wang, Conghui He, and Lijun Wu. Scientific image synthesis: Benchmarking, methodologies, and downstream utility. arXiv preprint arXiv:2601.17027, 2026b. Peng Liu, Haozhan Shen, Chunxin Fang, Zhicheng Sun, Jiajia Liao, and Tiancheng Zhao. Vlm-fo1: Bridging the gap between high-level reasoning and fine-grained perception in vlms. arXiv preprint arXiv:2509.25916, 2025a. Xianjie Liu, Yiman Hu, Yixiong Zou, Liang Wu, Jian Xu, and Bo Zheng. Hide: Rethinking the zoom-in method in high resolution mllms via hierarchical decoupling. arXiv preprint arXiv:2510.00054, 2025b. Zuyan Liu, Yuhao Dong, Yongming Rao, Jie Zhou, and Jiwen Lu. Chain-of-spot: Interactive reasoning improves large vision-language models. arXiv preprint arXiv:2403.12966, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Meng Lu, Ran Xu, Yi Fang, Wenxuan Zhang, Yue Yu, Gaurav Srivastava, Yuchen Zhuang, Mohamed Elhoseiny, Charles Fleming, Carl Yang, et al. Scaling agentic reinforcement learning for tool-integrated reasoning in vlms. arXiv preprint arXiv:2511.19773, 2025. Bozhi Luan, Hao Feng, Hong Chen, Yonghui Wang, Wengang Zhou, and Houqiang Li. Textcot: Zoom in for enhanced multimodal text-rich image understanding. arXiv preprint arXiv:2404.09797, 2024. Run Luo, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Yongbin Li, Minzheng Wang, Pengpeng Zeng, et al. Mmevol: Empowering multimodal large language models with evol-instruct. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 1965519682, 2025. Jinlong Ma, Yu Zhang, Xuefeng Bai, Kehai Chen, Yuwei Wang, Zeming Liu, Jun Yu, and Min Zhang. Beyond unimodal shortcuts: Mllms as cross-modal reasoners for grounded named entity recognition. arXiv preprint arXiv:2602.04486, 2026. MoltBook. Moltbook. https://www.moltbook.com, 2026. Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh, and Godawari Sudhakar Rao. Kam-cot: Knowledge augmented multimodal chain-of-thoughts reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 1879818806, 2024. Junbo Niu, Yuanhong Zheng, Ziyang Miao, Hejun Dong, Chunjiang Ge, Hao Liang, Ma Lu, Bohan Zeng, Qiahao Zheng, Conghui He, et al. Native visual understanding: Resolving resolution dilemmas in vision-language models. arXiv preprint arXiv:2506.12776, 2025. OpenAI. Gpt-5. https://openai.com/index/gpt-5-1/, 2025. OpenClaw. Openclaw. https://github.com/openclaw/openclaw, 2026. Dmitry Pechyony and Vladimir Vapnik. On the theory of learnining with privileged information. Advances in neural information processing systems, 23, 2010. Ruotian Peng, Haiying He, Yake Wei, Yandong Wen, and Di Hu. Patch matters: Training-free fine-grained image caption enhancement via local perception. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 39633973, 2025. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35: 2527825294, 2022. Viktoriia Sharmanska, Novi Quadrianto, and Christoph Lampert. Learning to transfer privileged information. arXiv preprint arXiv:1410.0389, 2014. Haozhan Shen, Kangjia Zhao, Tiancheng Zhao, Ruochen Xu, Zilun Zhang, Mingwei Zhu, and Jianwei Yin. ZoomEye: Enhancing multimodal LLMs with human-like zooming capabilities through tree-based image exploration. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 66026618, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main.335. URL https://aclanthology.org/2025.emnlp-main.335/. Yucheng Shi, Quanzheng Li, Jin Sun, Xiang Li, and Ninghao Liu. Enhancing cognition and explainability of multimodal foundation models with self-synthesized data. arXiv preprint arXiv:2502.14044, 2025. Jayant Sravan Tamarapalli, Rynaa Grover, Nilay Pande, and Sahiti Yerramilli. Countqa: How well do mllms count in the wild? arXiv preprint arXiv:2508.06585, 2025. Zhou Tao, Shida Wang, Yongxiang Hua, Haoyu Cao, and Linli Xu. Dig: Differential grounding for enhancing fine-grained perception in multimodal large language model. arXiv preprint arXiv:2512.12633, 2025. 18 Kimi Team, Tongtong Bai, Yifan Bai, Yiping Bao, et al. Kimi k2.5: Visual agentic intelligence. arXiv preprint arXiv:2602.02276, 2026. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, visioncentric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. Vladimir Vapnik and Akshay Vashist. new learning paradigm: Learning using privileged information. Neural networks, 22(5-6):544557, 2009. Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, et al. Adatooler-v: Adaptive tool-use for images and videos. arXiv preprint arXiv:2512.16918, 2025a. Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025b. Haochen Wang, Yuhao Wang, Tao Zhang, Yikang Zhou, Yanwei Li, Jiacong Wang, Jiani Zheng, Ye Tian, Jiahao Meng, Zilong Huang, et al. Grasp any region: Towards precise, contextual pixel understanding for multimodal llms. arXiv preprint arXiv:2510.18876, 2025c. Haozhe Wang, Alex Su, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025d. Jiacong Wang, Zijian Kang, Haochen Wang, Haiyong Jiang, Jiawen Li, Bohong Wu, Ya Wang, Jiao Ran, Xiao Liang, Chao Feng, et al. Vgr: Visual grounded reasoning. arXiv preprint arXiv:2506.11991, 2025e. Qixun Wang, Yang Shi, Yifei Wang, Yuanxing Zhang, Pengfei Wan, Kun Gai, Xianghua Ying, and Yisen Wang. Monet: Reasoning in latent visual space beyond images and language. arXiv preprint arXiv:2511.21395, 2025f. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 79077915, 2025g. Yubo Wang, Juntian Zhang, Yichen Wu, Yankai Lin, Nils Lukas, and Yuhan Liu. Forest before trees: Latent superposition for efficient visual reasoning. arXiv preprint arXiv:2601.06803, 2026. Yuji Wang, Wenlong Liu, Jingxuan Niu, Haoji Zhang, and Yansong Tang. Vg-refiner: Towards tool-refined referring grounded reasoning via agentic reinforcement learning. arXiv preprint arXiv:2512.06373, 2025h. Jingxuan Wei, Caijun Jia, Qi Chen, Honghao He, Linzhuang Sun, Conghui He, Lijun Wu, Bihui Yu, and Cheng Tan. Geoint-r1: Formalizing multimodal geometric reasoning with dynamic auxiliary constructions. arXiv preprint arXiv:2508.03173, 2025a. Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, and Weiran Huang. Diff-erank: novel rank-based metric for evaluating large language models. arXiv preprint arXiv:2401.17139, 2024. Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, and Lichao Sun. Unsupervised post-training for multi-modal llm reasoning via grpo. arXiv preprint arXiv:2505.22453, 2025b. Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, and Weiran Huang. Advancing multimodal reasoning via reinforcement learning with cold start. arXiv preprint arXiv:2505.22334, 2025c. Yana Wei, Liang Zhao, Kangheng Lin, En Yu, Yuang Peng, Runpei Dong, Jianjian Sun, Haoran Wei, Zheng Ge, Xiangyu Zhang, et al. Perception in reflection. arXiv preprint arXiv:2504.07165, 2025d. Siwei Wen, Junyan Ye, Peilin Feng, Hengrui Kang, Zichen Wen, Yize Chen, Jiang Wu, Wenjun Wu, Conghui He, and Weijia Li. Spot the fake: Large multimodal model-based synthetic image detection with artifact explanation. arXiv preprint arXiv:2503.14905, 2025. Thaddaus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025a. Junfei Wu, Jian Guan, Kaituo Feng, Qiang Liu, Shu Wu, Liang Wang, Wei Wu, and Tieniu Tan. Reinforcing spatial reasoning in vision-language models with interwoven thinking and visual drawing. arXiv preprint arXiv:2506.09965, 2025b. Linquan Wu, Tianxiang Jiang, Yifei Dong, Haoyu Yang, Fengji Zhang, Shichaang Meng, Ai Xuan, Linqi Song, and Jacky Keung. Lavit: Aligning latent visual thoughts for multi-modal reasoning. arXiv preprint arXiv:2601.10129, 2026. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Penghao Wu, Yushan Zhang, Haiwen Diao, Bo Li, Lewei Lu, and Ziwei Liu. Visual jigsaw post-training improves mllms. arXiv preprint arXiv:2509.25190, 2025c. Tianbao Xie, Jiaqi Deng, Xiaochuan Li, Junlin Yang, Haoyuan Wu, Jixuan Chen, Wenjing Hu, Xinyuan Wang, Yuhui Xu, Zekun Wang, et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. Zhengzhuo Xu, Chong Sun, SiNan Du, Chen Li, Jing Lyu, and Chun Yuan. Vacot: Rethinking visual data augmentation with vlms. arXiv preprint arXiv:2512.02361, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Shuo Yang, Yuwei Niu, Yuyang Liu, Yang Ye, Bin Lin, and Li Yuan. Look-back: Implicit visual re-focusing in mllm reasoning. arXiv preprint arXiv:2507.03019, 2025b. Siqi Yang, Zilve Gao, Haibo Qiu, Fanfan Liu, Peng Shi, Zhixiong Zeng, Qingmin Liao, and Lin Ma. Learning when to look: disentangled curriculum for strategic perception in multimodal reasoning. arXiv preprint arXiv:2512.17227, 2025c. Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025d. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, et al. Loki: comprehensive synthetic data detection benchmark using large multimodal models. arXiv preprint arXiv:2410.09732, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Tianyu Yu, Zefan Wang, Chongyi Wang, Fuwei Huang, Wenshuo Ma, Zhihui He, Tianchi Cai, Weize Chen, Yuxiang Huang, Yuanqian Zhao, et al. Minicpm-v 4.5: Cooking efficient mllms via architecture, data, and training recipe. arXiv preprint arXiv:2509.18154, 2025b. Xuan Yu, Dayan Guan, and Yanfeng Gu. Zoom-refine: Boosting high-resolution multimodal understanding via localized zoom and self-refinement. arXiv preprint arXiv:2506.01663, 2025c. Haobo Yuan, Yueyi Sun, Yanwei Li, Tao Zhang, Xueqing Deng, Henghui Ding, Lu Qi, Anran Wang, Xiangtai Li, and Ming-Hsuan Yang. Visual reasoning tracer: Object-level grounded reasoning benchmark. arXiv preprint arXiv:2512.05091, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and 20 reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, et al. Learning to discover at test time. arXiv preprint arXiv:2601.16175, 2026. Yu Zeng, Wenxuan Huang, Shiting Huang, Xikun Bao, Yukun Qi, Yiming Zhao, Qiuchen Wang, Lin Chen, Zehui Chen, Huaian Chen, et al. Agentic jigsaw interaction learning for enhancing visual perception and reasoning in vision-language models. arXiv preprint arXiv:2510.01304, 2025. Chi Zhang, Haibo Qiu, Qiming Zhang, Yufei Xu, Zhixiong Zeng, Siqi Yang, Peng Shi, Lin Ma, and Jing Zhang. Perceptual-evidence anchored reinforced learning for multimodal reasoning. arXiv preprint arXiv:2511.18437, 2025a. Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422, 2025b. Letian Zhang, Quan Cui, Bingchen Zhao, and Cheng Yang. Oasis: One image is all you need for multimodal instruction data synthesis. arXiv preprint arXiv:2503.08741, 2025c. Lu Zhang, Jiazuo Yu, Haomiao Xiong, Ping Hu, Yunzhi Zhuge, Huchuan Lu, and You He. Finers: Fine-grained reasoning and segmentation of small objects with reinforcement learning. arXiv preprint arXiv:2510.21311, 2025d. Wenqi Zhang, Zhenglin Cheng, Yuanyu He, Mengna Wang, Yongliang Shen, Zeqi Tan, Guiyang Hou, Mingqian He, Yanna Ma, Weiming Lu, et al. Multimodal self-instruct: Synthetic abstract image and visual reasoning instruction using language model. arXiv preprint arXiv:2407.07053, 2024a. Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025e. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024b. Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025f. Yifan Zhang, Liang Hu, Haofeng Sun, Peiyu Wang, Yichen Wei, Shukang Yin, Jiangbo Pei, Wei Shen, Peng Xia, Yi Peng, et al. Skywork-r1v4: Toward agentic multimodal intelligence through interleaved thinking with images and deepresearch. arXiv preprint arXiv:2512.02395, 2025g. Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. arXiv preprint Evaluating and steering modality preferences in multimodal large language model. arXiv:2505.20977, 2025h. Yu Zhang, Mufan Xu, Xuefeng Bai, Kehai Chen, Pengfei Zhang, Yang Xiang, and Min Zhang. Instruction anchors: Dissecting the causal dynamics of modality arbitration. arXiv preprint arXiv:2602.03677, 2026. Zefeng Zhang, Xiangzhao Hao, Hengzhu Tang, Zhenyu Zhang, Jiawei Sheng, Xiaodong Li, Zhenyang Li, Li Gao, Daiting Shi, Dawei Yin, et al. Cooper: unified model for cooperative perception and reasoning in spatial intelligence. arXiv preprint arXiv:2512.04563, 2025i. Henry Hengyuan Zhao, Pan Zhou, and Mike Zheng Shou. Genixer: Empowering multimodal large language model as powerful data generator. In European Conference on Computer Vision, pp. 129147. Springer, 2024. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, et al. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 17021713, 2025. Tong Zheng, Chengsong Huang, Runpeng Dai, Yun He, Rui Liu, Xin Ni, Huiwen Bao, Kaishen Wang, Hongtu Zhu, Jiaxin Huang, et al. Parallel-probe: Towards efficient parallel thinking via 2d probing. arXiv preprint arXiv:2602.03845, 2026. 21 Yaowei Zheng. Mathruler. https://github.com/hiyouga/MathRuler, 2025. Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025a. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025b. Qiji Zhou, Ruochen Zhou, Zike Hu, Panzhong Lu, Siyang Gao, and Yue Zhang. Image-of-thought prompting for visual reasoning refinement in multimodal large language models. arXiv preprint arXiv:2405.13872, 2024. Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv preprint arXiv:2509.01656, 2025. Zhihao Zhu, Jiafeng Liang, Shixin Jiang, Jinlan Fu, Ming Liu, Guanglu Sun, See-Kiong Ng, and Bing Qin. Analyzing reasoning consistency in large multimodal models under cross-modal conflicts. arXiv preprint arXiv:2601.04073, 2026."
        },
        {
            "title": "Appendix",
            "content": ""
        },
        {
            "title": "Implementation Details of Our Method",
            "content": "We present the implementation details of our data synthesis method to construct the high=quality training dataset and ZoomBench. 8.1 Region-to-Image Distillation Algorithm 2 Region-to-Image Distillation (R2I) Require: Raw image pool Draw; proposal function (); question generator Tgen; teacher ensemble {T1, . . . , Tm}; sparsity // propose candidate boxes via detection/segmentation // initialize // zoom in: crop region from // generate region-answerable questions // collect teacher answers // answer on the crop to reduce hallucination end for if HIGHCONSENSUS(a) then CONSENSUSMAP(a) // e.g., majority vote (I, Q) GROUNDTOFULL(I, B, Q) // zoom out: overlay on to form and add spatial constraint to form // store full-image training triplet (I) for each box and AREA(B)/AREA(I) τ do threshold τ; max questions per crop CROP(I, B) QR Tgen(R; K) for each question QR do Ensure: Distilled dataset Dsyn 1: Dsyn 2: for each image Draw do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: [ ] for each teacher Tj {T1, . . . , Tm} do aj Tj(R, Q); {aj} end if end for Dsyn Dsyn {(I, Q, A)} 15: 16: 17: 18: 19: end for 20: Dsyn REJECTIONSAMPLING(Dsyn) 21: return Dsyn end for To ensure broad visual diversity and high information density, we curate pool of high-resolution images (mostly over 800 800 pixels) from several large-scale datasets, including SA-1B (Kirillov et al., 2023), LAION (Schuhmann et al., 2022), MetaCLIP (Chuang et al., 2025), Visual Genome (Krishna et al., 2017), CC12M (Changpinyo et al., 2021), and STPLS3D (Chen et al., 2022). For datasets lacking native object-level annotations, we implement an object recognition and segmentation system. In particular, we employ Qwen3VL-235B (Bai et al., 2025a) to generate comprehensive object inventories based on the images, which are subsequently processed by SAM3 (Carion et al., 2025) to produce precise bounding boxes and cropped regions R. To specifically target fine-grained challenges, we filter these candidates to retain only regions much smaller than the full images I, satisfying the area ratio Area(R)/Area(I) < 0.1. For data synthesis, we firstly zoom in (crop and then resize by 2) to synthesize VQA pairs directly for objects in each cropped region R. In particular, we use Qwen3-VL-235B (Bai et al., 2025a) as the question generator, and Qwen3-VL-235B (Bai et al., 2025a) together with GLM-4.5V (Hong et al., 2025b) as two independent answer generators. These two models are strong yet cost-effective for data synthesis. To obtain reliable pseudo-labels, we sample four responses per answer generator (8 in total) and keep QA pair only when the majority answer reaches strict consensus (> 6/8); otherwise, we discard it. After distilling back to get the augmented vqa I, Q, A, we also filter out data that Qwen3-VL-8B can answer correctly over 2 times in 4 trials based on I, Q. In total, we collect 74K samples for training. The overall pipeline of our method is also shown in Algorithm 2. 8.2 Benchmark Construction For benchmark construction, we adopt more powerful MLLM, Gemini-2.5-Pro (Comanici et al., 2025), as both the question generator and answer generator using the Region-to-Image Distillation pipeline. Note that we split images into training and benchmark partitions, ensuring no overlap of images or QA pairs to avoid data leakage. To ensure benchmarks validity, we further perform human verification. Each example is independently checked by 3 paper authors (PhD-level researchers), who are provided with the full image, the corresponding cropped region, and the model-generated QA pair. Annotators verify (not annotate) that (i) the question is unambiguous and answerable, and (ii) the final answer is correct under both the full-image and cropped-region views. Each annotator spends only about 10 hours to verify roughly 650 raw samples (1,960 raw samples in total for 3 annotators). After careful verification and selection (filtering out very easy questions from the view of annotators), we finally remain 845 examples to form the Zoom-Bench, each consisting of high-resolution full image and small-ratio cropped view showing the key region for the question (i.e., the dual-view evaluation). Note that HR-Bench (Wang et al., 2025g), TextVQA-gt-bbox (Zhang et al., 2025b), and TreeBench (Wang et al., 2025b) include evaluation settings that resemble our dual-view protocol. However, HR-Bench constructs its two views by cropping the same 8K image to 4K; since the crop covers large portion of the original image (around 25% by area), the resulting globallocal gap is typically small. TextVQA-gt-bbox and TreeBench (Wang et al., 2025b) provides human-annotated ground-truth box for each question, which is costly to obtain at scale. Moreover, TreeBench (Wang et al., 2025b) mainly targets on the evaluation of Thinking with Images models and thus does not conduct the dual-view evaluation; HR-Bench (Wang et al., 2025g) and TextVQA-gt-bbox (Zhang et al., 2025b) are relatively easy for current MLLMs (e.g., Qwen2.5-VL-7B, not the most advanced model, already achieves 84.9% on TextVQA-gt-bbox and 68.5% on HR-Bench-8K), making them less suitable for testing advanced and rapidly evolving models. We also demonstrate the data statistics of our benchmark in Figure 4. Besides, the benchmark comprises 224 open-ended questions with canonical target answers and 621 multiple-choice questions, covering 6 distinct dimensions."
        },
        {
            "title": "Implementation Details of Experiments",
            "content": "9.1 RL Training Settings We adopt the EasyR1 (Zheng et al., 2025a) framework for multi-modal reinforcement learning, which is based on DAPO (Yu et al., 2025a). Specifically, we set the rollout group to 8, training epoch to 1, and use AdamW optimizer (Loshchilov & Hutter, 2017) with learning rate of 1 106, weight decay of 1 102, rollout temperature of 1.0, and gradient clipping at maximum norm of 1.0. Other hyperparameters follow the default settings provided in the EasyR1 framework. 9.2 Reward Modeling in Reinforcement Learning To provide the verifiable reward for reinforcement learning (Guo et al., 2025), we implement multi-stage tiered reward system. This system prioritizes deterministic evaluation and falls back to heuristic or semantic methods only when necessary, ensuring balance between accuracy and computational efficiency. The evaluation pipeline follows three-step hierarchy: Rule-based Matching. The system first attempts to evaluate the response using exact or symbolic matching via tools like mathruler (Zheng, 2025). If the extracted answer matches the ground truth exactly, maximum reward (i.e., 1) is assigned. Continuous Counting Reward. For tasks involving numerical counting or estimation where binary 0/1 reward is too sparse, we employ distance-based continuous reward for the predicted value ypred and the ground truth ygt. This stage is triggered if the rule-based matching fails but both the prediction and ground truth can be parsed as numerical values. Specifically, for non-zero ground truth values, the score is defined as: (cid:32) = max 0, 1 (cid:33) ypred ygt ygt In the specific case where ygt = 0, we apply smooth exponential decay to handle the singularity: = eypredygt This continuous formulation ensures that the agent is penalized less for near misses, which significantly accelerates convergence in mathematical reasoning tasks by smoothing the optimization landscape. LLM-as-a-Judge. LLM (e.g., Qwen3-8B (Yang et al., 2025a)) as judge to provide binary 0/1 reward. If neither rules nor numerical parsing can resolve the correctness, we utilize an efficient We also plot the training reward curve for Qwen3-VL-8B on our dataset in Figure 8 and observe consistent, steady increase over time, indicating that the dataset is highly learnable for the model. 24 (3) (4) Figure 8: The training reward of Qwen3-VL-8B. 9.3 Scoring System in Benchmark Evaluation For evaluation of all the tested benchmarks, we adopt simplified reward pipeline that mirrors the one used in reinforcement learning, but removes the continuous counting reward to keep the metric strictly verifiable and comparable across methods. In particular, we first apply rule-based matching (e.g., mathruler (Zheng, 2025)) to check exact/symbolic equivalence and assign binary score (1 for correct, 0 otherwise). If rule-based matching fails, we fall back to an LLM-as-a-Judge using more powerful model (e.g., Qwen3-30B (Yang et al., 2025a)) to determine correctness and output binary 0/1 score. 9.4 Inference Protocol and Latency For direct-answer models (including ours), inference consists of single forward pass on the full image. Besides, for inference efficiency, we do not open the thinking mode for GLM-4.5V and Kimi-K2.5. For agentic baselines, we follow their official tool-use configurations (crop/zoom operations and the maximum number of steps) and report end-to-end wall-clock latency measured under the same hardware and batch size."
        },
        {
            "title": "10 Prompts.",
            "content": "Prompt of LLM-as-a-Judge. The prompt used for LLM-as-a-Judge in RL reward modeling and benchmark evaluation is shown as follows. Your task is to judge whether the response expresses the same meaning as the answer of question. The question is: {question} The answer is: {gt} The response is: {response} Please check and compare them and then judge. Consider them equivalent if: Different phrasings of the same answer (e.g., 5 people vs five people) Slight variations in description that refer to the same entity Minor differences in precision that dont change the core answer (such as two colors that are similar: e.g., brown and dark brown, white and off-white) Do NOT consider them equivalent if: They refer to different objects, numbers, or concepts They represent different interpretations of the question. If the response is correct, your output should be Yes. Otherwise, your output should be No. Put your output within boxed{}. Prompt of Question Generation. We also present the prompt used for question generation in our Region-toImage Distillation. 25 You are an expert specialist in generating Visual Question Answering (VQA) datasets. Your task is to generate three high-quality and valid questions based solely on the provided image. Reference Examples (use these for inspiration on questioning angles): {examples str} Core Generation Rules: 1. Image-based Questions: All questions must be answerable by examining the image provided. The answer to each question must be identical, accurate, and concise. It can be short, factual, and concrete string (e.g., number, noun, or text). 2. Content Relevance: Question types include, but are not limited to: Object Identification: Identify the exact sub-component or item (e.g., What is the person holding? Answer: Apple). OCR: Recognizing text within the image. 3. Quality Control: If the image is of such low quality that it is impossible to generate meaningful questions, return an empty JSON list: []. 4. Diversity of Questions: Aim for diverse range of questions. This includes counting, spatial relationships, scene recognition, anomaly detection, shape, material, structure, etc. Output Format: Please carefully observe the image and generate your response in the following JSON format: [ ] {\"question\": \"Question 1\"}, {\"question\": \"Question 2\"}, {\"question\": \"Question 3\"}"
        },
        {
            "title": "11 Case Study",
            "content": "We present some ambiguous cases and benchmark cases generated by MLLMs during our benchmark annotating process. 11.1 Ambiguous Cases Below we show two cases that are unambiguous in the cropped image but become ambiguous in the full image due to information loss from cropping. Therefore, we introduce explicit visual grounding by overlaying the target bounding box on the image to construct our training dataset. 26 Ambiguous Case 1. Ambiguous Case 2. Question: How many tennis balls are visible in the image? A. 1 B. 2 C. 3 D. 4 Answer: for the cropped image and for the full image Question: What is the name of the island group labeled in the bottom right corner of the map? (Answer the full text as written) A. I. BALEARES B. I. DE AZORES C. I. DA MADEIRA D. I. DE LAS CANARIAS Answer: for the cropped image. As for the full image, it should be CANARIAS. 27 Ambiguous Case 3. Question: What brand name is written on the orange fender of the bicycle? A. City Cycle B. Lipton C. ICE TEA D. Bike Share Answer: for the cropped image and for the full image. 11.2 Benchmark Cases We also provide one representative case for each category (including counting, OCR, color, structure, material, and identification) in our benchmark. Note that all questions are generated by MLLMs via our Region-to-Image Distillation method and are of high quality. Fine-Grained Counting. OCR. Question: How many fish are visible on the left side of the mermaid? A. 5 B. 4 C. 3 D. 2 Answer: Question: What word is visible on the poster behind the character that starts with WANTED? A. ROOST B. COST C. CONST D. GHOST Answer: 28 Color Attributes. Structural Attributes. Material Attributes. Question: What is the color of the valve handle attached to the hydrant? A. red B. blue C. yellow D. black Answer: Question: What type of headwear is on the statue? A. wreath B. tiara C. headdress D. crown Answer: Question: What is the primary material of the bottle shown in the image? A. glass B. ceramic C. metal D. plastic Answer: 29 Object Identification. Question: What flag is displayed on the stern of the boat? A. Greek flag B. Turkish flag C. Russian flag D. Ukrainian flag Answer: C"
        },
        {
            "title": "12 Relative Attention Map Computation",
            "content": "We follow Zhang et al. (2025b) to compute relative attention maps that highlight image regions that are specifically relevant to answering given question, rather than reflecting question-agnostic or baseline attention patterns. Setup and notation. Given an image-question pair (x, q), an MLLM first encodes the image into grid of visual tokens (patch tokens) produced by the vision encoder (e.g., ViT). Let N2 denote the number of visual grid cells. Depending on the architecture, the ViT tokens are either (i) directly projected into the LLM input space (e.g., by an MLP), or (ii) resampled by Transformer connector (e.g., Q-Former) into image tokens that are then fed to the LLM together with the text tokens. Let be the number of LLM layers and the number of attention heads per LLM layer. If Transformer connector is present, let Lc and Hc be its number of layers and heads. We use greedy decoding and denote the special starting answer token position by (i.e., the first position at which the model starts generating the answer). (1) Answer-to-token attention in the LLM. We extract the softmax cross-attention from the starting answer token to the image tokens in each LLM layer: We then average over heads to obtain Ast(x, q) RLH1T. ˆAst(x, q) = 1 h=1 (h) st (x, q) RL1T. (5) (6) (2) Token-to-image attention in the visual connector. For models with Transformer connector (e.g., Qwen-VL series), we extract the connectors softmax cross-attention from each image token to the N2 ViT grid tokens: We average over heads: Ati(x) RLcHcTN2 . ˆAti(x) = 1 Hc Hc h=1 (h) ti (x) RLcTN2 . (7) (8) For architectures without Transformer connector (e.g., when an MLP directly maps ViT tokens into LLM ˆAti(x) to the identity mapping from the N2 visual grid tokens to themselves (i.e., image tokens), we set = N2 and ˆAti(x) is an identity matrix for each connector layer index). (3) Answer-to-image attention. We combine the LLM answer-to-token attention and the connector tokento-image attention via tensor product to obtain an answer-to-image attention map over the N2 visual grid cells: Asi(x, q) RLLc1N2 (m,k) (9) si where {1, . . . , L} and {1, . . . , Lc} index the LLM and connector layers, respectively, and the righthand side is standard matrix multiplication over the shared dimension T. Finally, we reshape the last dimension N2 back to an grid to obtain spatial map. (m) st (x, q) ˆA (x, q) = ˆA (k) ti (x), , 30 (4) Relative attention. Softmax attention may include components that are not specific to the question (e.g., global register tokens or generic image summarization). To emphasize question-relevant attention, we normalize Asi(x, q) by the answer-to-image attention computed with fixed generic instruction q: where the division is element-wise. Following Zhang et al. (2025b), we use fixed such as Write general description of the image. for all samples. In practice, to avoid numerical issues we compute Arel(x, q) = Asi(x, q) Asi(x, q) , (10) Arel(x, q) = Asi(x, q) Asi(x, q) + ϵ , (11) with small constant ϵ (e.g., 106). (5) Layer selection to obtain single map. The relative attention defined above yields tensor Arel(x, q) RLLc1N2 . For downstream coverage computation, we convert it into single map by selecting fixed LLM layer. Specifically, we use the relative attention from the 24-th LLM layer: (12) where denotes the connector layer index (for architectures with visual connector); if no connector is used, we set Lc = 1 and = 1. We keep this layer choice fixed for all samples and all experiments. rel(x, q) = (x, q) RNN, = 24, (m,k) rel 12.1 Demonstrations of Attention Map We also provide qualitative attention-map comparisons between our ZwZ-8B and the base Qwen3-VL-8B. ZwZ-8B concentrates more relative attention on the annotated key region, indicating improved localization of task-relevant evidence. Object Identification. Original Image Qwen3-VL-8B ZwZ-8B Question: What symbol is visible on the trams front, to the left of the number 8275? A. European Union flag - blue background with yellow stars B. No Smoking symbol - red circle with diagonal line through cigarette C. International Symbol of Access (wheelchair symbol) - white wheelchair figure on blue background D. Emergency Exit symbol - green rectangle with white running figure Answer: 31 Structural Attributes. Original Image Qwen3-VL-8B ZwZ-8B Question: What is the shape of the blue sign on the pole? A. circle B. square C. inverted triangle D. rectangle Answer: OCR. Original Image Qwen3-VL-8B ZwZ-8B Question: What is the text written on the side of the van, visible near the top? A. Indigo 0 820 20 15 55 B. Indigo 0 820 20 15 50 C. Indigo 0 820 20 14 40 D. Indigo 0 820 20 16 60 Answer:"
        }
    ],
    "affiliations": [
        "Ant Group",
        "School of Computer Science, Shanghai Jiao Tong University",
        "Shanghai Innovation Institute",
        "Zhongguancun Academy"
    ]
}