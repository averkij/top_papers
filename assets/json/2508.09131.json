{
    "paper_title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer",
    "authors": [
        "Zixin Yin",
        "Xili Dai",
        "Ling-Hao Chen",
        "Deyu Zhou",
        "Jianan Wang",
        "Duomin Wang",
        "Gang Yu",
        "Lionel M. Ni",
        "Lei Zhang",
        "Heung-Yeung Shum"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-guided color editing in images and videos is a fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, a training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instruction-based editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility."
        },
        {
            "title": "Start",
            "content": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer ZIXIN YIN, The Hong Kong University of Science and Technology, International Digital Economy Academy XILI DAI, The Hong Kong University of Science and Technology (Guangzhou) LING-HAO CHEN, Tsinghua University, International Digital Economy Academy DEYU ZHOU, The Hong Kong University of Science and Technology (Guangzhou), StepFun JIANAN WANG, Astribot DUOMIN WANG, StepFun GANG YU, StepFun LIONEL M. NI, The Hong Kong University of Science and Technology (Guangzhou), The Hong Kong University of Science and Technology LEI ZHANG, International Digital Economy Academy HEUNG-YEUNG SHUM, The Hong Kong University of Science and Technology Fig. 1. Text-conditioned color editing. Our method, ColorCtrl with FLUX.1-dev, edits colors across multiple materials while preserving light-matter interactions. For example, in the fourth case, the balls color, its water reflection, specular highlights, and even small droplets on the glass have all been changed. It also enables fine-grained control over the intensity of specific descriptive terms. Text-guided color editing in images and videos is fundamental yet unsolved problem, requiring fine-grained manipulation of color attributes, including albedo, light source color, and ambient lighting, while preserving physical consistency in geometry, material properties, and light-matter interactions. Existing training-free methods offer broad applicability across editing tasks but struggle with precise color control and often introduce visual inconsistency in both edited and non-edited regions. In this work, we present ColorCtrl, training-free color editing method that leverages the attention mechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By disentangling structure and color through targeted manipulation of attention maps and value tokens, our method enables accurate and consistent color editing, along with word-level control of attribute intensity. Our method modifies only the intended regions specified by the prompt, leaving unrelated areas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate that ColorCtrl outperforms existing training-free approaches and achieves state-of-the-art performances in both edit quality and consistency. Furthermore, our method surpasses strong commercial models such as FLUX.1 Kontext Max and GPT-4o Image Generation in terms of consistency. When extended to video models like CogVideoX, our approach exhibits greater advantages, particularly in maintaining temporal coherence and editing stability. Finally, our method also generalizes to instructionbased editing diffusion models such as Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility. Here is the project website."
        },
        {
            "title": "Introduction",
            "content": "Film industry-level color changing in images and videos based on textual instructions is fundamental yet challenging task in deep learning and visual editing. Here, color encompasses not only object albedo (i.e., intrinsic surface color independent of material properties), but also the color of light sources and ambient illumination. Color editing is an ill-conditioned problem, as it requires explicit or implicit 3D reconstruction of the entire scene, including correct illumination. During editing, it is essential to modify 5 2 0 2 3 1 ] . [ 2 1 3 1 9 0 . 8 0 5 2 : r 2 Zixin et al. only the intended color attributes while preserving material properties, ensuring accurate reflections and refractions, and keeping non-editing regions unchanged. Traditional image processing methods have been widely commercialized in professional software such as Photoshop, serving billions of users worldwide. However, the steep learning curve and significant manual effort required make it difficult to achieve widespread accessibility. Moreover, such tools are not well-suited for automated batch processing and present challenges when applied to video-related tasks. Recently, diffusion models have demonstrated remarkable capabilities in generating high-quality images that adhere to physical principles of color and illumination. This has spurred growing interest in leveraging their generative power to tackle the above challenges, with controllability emerging as critical factor. Although many methods [Magar et al. 2025; Zhang et al. 2025] finetune diffusion models for controllable editing, they usually require large-scale datasets and complex training pipelines, and are often constrained to narrow domains or specific edit types. On the other hand, training-free methods [Jiao et al. 2025] have gained popularity for wide range of image editing tasks due to their generality and ease of use. Despite their success in many scenarios, they still struggle with fine-grained color control and often introduce inconsistencies in edited and non-edited regions, making precise color editing an unresolved challenge. The recent architectural shift in diffusion models from U-Net [Rombach et al. 2022] to the Multi-Modal Diffusion Transformer (MMDiT) [Esser et al. 2024] opens up new opportunities for training-free color editing, for two main reasons: (1) the new architecture allows for scaling up both data and model capacity, enabling better adherence to physical priors; and (2) the improved fusion of text and vision modalities supports more flexible and precise attentioncontrol editing strategies. In this work, we propose ColorCtrl, training-free, open-world color editing method that effectively leverages pre-trained MM-DiT models to perform natural and precise color modifications, while preserving all other visual attributes, such as geometry, material, reflection and refraction, light source position, illumination direction, and light intensity, as shown in Fig. 1. For example, when editing the color of ball, ColorCtrl accurately adjusts not only the ball itself but also its reflection in the water, the specular reflections on both sides, and even the small water droplets on the glass surface, as demonstrated in the fourth example of Fig. 1. Furthermore, ColorCtrl supports fine-grained, word-level control over the strength of color attributes while preserving the other visual attributes mentioned above. Thanks to our precise and robust control, ColorCtrl requires no manual parameter tuning and can be directly applied across all attention layers and inference timesteps. To verify the effectiveness of our method, we conduct extensive experiments showing that ColorCtrl outperforms existing trainingfree approaches on both Stable Diffusion 3 Medium (a.k.a., SD3) [Esser et al. 2024] and FLUX.1-dev [Labs 2024]. To better understand the gap between open-source and commercial models, we further compare ColorCtrl with two strong commercial baselines: FLUX.1 Kontext Max [Black Forest Labs 2025] and GPT-4o Image Generation (a.k.a., GPT-4o) [OpenAI 2025]. ColorCtrl achieves more natural color editing and substantially better preservation of visual consistency, including background and structural fidelity. In addition, our method is model-agnostic and can be seamlessly extended to video models such as CogVideoX [Yang et al. 2024], where the performance gap becomes even more pronounced. Finally, ColorCtrl can be integrated into instruction-based editing models such as Step1X-Edit [Liu et al. 2025a] and FLUX.1 Kontext dev [Black Forest Labs 2025], demonstrating strong compatibility. In summary, the main contributions can be listed as follows. We propose ColorCtrl, training-free method for color editing that supports modification of albedo, light source color and ambient lighting, while preserving the consistency of geometry, material and light-matter interaction. We present extensive experiments demonstrating that ColorCtrl achieves state-of-the-art performance among training-free methods on MM-DiT-based models. Compared to commercial models (i.e., FLUX.1 Kontext Max and GPT-4o Image Generation), our method delivers significantly better consistency preservation and produces more natural edits. ColorCtrl generalizes well across multiple MM-DiT-based models, including video and instruction-based editing models, highlighting its broad applicability and extensibility."
        },
        {
            "title": "2 Related Work",
            "content": "Text-to-image and video generation. Diffusion models with UNet backbone [Guo et al. 2024; Ho et al. 2020; Rombach et al. 2022] have largely replaced early GAN systems [Reed et al. 2016; Wang et al. 2023; Yu et al. 2023] due to superior image fidelity. However, the U-Net design scales poorly, leading to the adoption of Diffusion Transformers (DiT) [Peebles and Xie 2023]. Among these, MMDiT [Esser et al. 2024] has emerged as widely adopted backbone for recent state-of-the-art models [AI 2024; Black Forest Labs 2025; Esser et al. 2024; Kong et al. 2024; Labs 2024; Liu et al. 2025b; Yang et al. 2024], such as SD3 [Esser et al. 2024] and FLUX.1-dev [Labs 2024] for image generation, as well as CogVideoX [Yang et al. 2024] for video generation. In this work, we propose an attention-control scheme that plugs into any MM-DiT model. Text-guided editing. Training-free text-guided editing methods based on pre-trained diffusion models offer flexibility and efficiency. Existing approaches fall into two groups: (i) sampling-based methods, which steer generation by injecting controlled noise or inversion [Huberman-Spiegelglas et al. 2024; Jiao et al. 2025; Kulikov et al. 2024; Xu et al. 2023; Yan et al. 2025]; (ii) attention-based methods, which modify attention maps, starting with Prompt-to-Prompt [Hertz et al. 2023] and its image/video variants [Cai et al. 2025; Cao et al. 2023; Chen et al. 2024; Liu et al. 2024; Rout et al. 2025; Wang et al. 2025; Xu et al. 2025]. Among these, DiTCtrl is the only method exploring attention control in MM-DiT, but ColorCtrl differs in key ways: DiTCtrl only applies mask extraction during long video generation, not color editing, while our improved method is more robust. Additionally, DiTCtrls attribute re-weighting disrupts attention consistency, causing inconsistent geometries, while our method avoids this. Furthermore, ColorCtrl works across all layers and timesteps, unlike DiTCtrl, which requires careful layer selection. Different from DiTCtrl, we operate in attention maps rather than key and value tokens. More recently, methods [Brooks et al. 2023; Liu et al. 2025a] like FLUX.1 Kontext [Black Forest Labs 2025] and GPT-4o [OpenAI 2025] have shown convenient creative workflows in training with synthetic instruction-response pairs for fine-tuning diffusion model for image editing. Despite advancements in text-guided editing, we demonstrate that current methods still face significant challenges in accurately changing colors while preserving geometry, material properties and light-matter interaction. Also, all aforementioned training-free methods rely on selectively manipulating specific inference steps or attention layers, which limits their robustness and consistency with respect to the source. In contrast, our approach requires no manual selection of steps or layers."
        },
        {
            "title": "3 Method",
            "content": "We first formulate the color editing problem in Sec. 3.1, followed by revisit of the attention mechanism in MM-DiT blocks in Sec. 3.2. Sec. 3.3 describes our approach for preserving geometry, material properties, and light-matter interactions. In Sec. 3.4, we introduce method for preserving colors in non-editing regions, which are automatically detected by the model. Together, Sec. 3.3 and Sec. 3.4 constitute the core of our method. Finally, Sec. 3.5 introduces wordlevel control over color attributes via attention re-weighting, serving as an optional add-on for fine-grained attribute control."
        },
        {
            "title": "3.1 Task Formulation: Text-guided Color Editing",
            "content": "Rendering process. To provide clearer and more precise description of the conditions required for film industry-level color changing, we define the rendering process of frame by = (cid:0)ùê∫, ùêø, ùê¥, ùëÜ, ùê∂(cid:1), (1) where the notations of rendering inputs are defined as, () - the rendering function, ùê∫ = {ùê∫1, . . . , ùê∫ùëÅ } - set of object geometries, e.g., mesh topology for ùëÅ objects, ùêø = {ùêøenv, ùêø1, . . . , ùêøùêæ } - the ambient illumination ùêøenv and ùêæ light sources (each with position, intensity, and spectrum), ùê¥ = {ùê¥1, . . . , ùê¥ùëÅ } - the albedos (base colors) of each object, ùëÜ = {ùëÜ1, . . . , ùëÜùëÅ } - material parameters (roughness, specularity, and normal maps) for each object that are color-independent, ùê∂ - camera intrinsics and extrinsics. Editing task. According to the predefined image rendering process in Eq. (1), we formulate the consistent editing task in this work as follows. Given source image and text prompt pairs ùëû before and after editing1 that specifies which objects or lights to recolor and the desired target color, we aim to learn the following editing function ùëì () ùëì : (I, ùëû) ÀÜI = (cid:0)ùê∫, ÀÜùêø, ÀÜùê¥, ùëÜ, ùê∂(cid:1), (2) such that ÀÜI satisfies: (C1) Geometry/view consistency - ùê∫ and ùê∂ remain fixed with the source image I, preserving object layout and perspective. 1An example of ùëû: white fox. orange fox. (source and target prompt respectively). Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer 3 Fig. 2. Pipeline of ColorCtrl. (a) Visualizes the attention mechanism in MM-DiT blocks. (b) Enables effective color editing while maintaining structural consistency. (c) Preserves colors in non-editing regions. (d) Applies attribute re-weighting to specific word tokens. Symbols in the source branch have no superscript. Symbols with superscript indicate the target branch, and hats (e.g., ÀÜùëâ , ÀÜùëÄ) denote edited outputs. (C2) Illumination consistency - light positions and scalar intensities remain fixed. Specifically, changes may occur only on the target spectral components (i.e., RGB channels) of ùêø. (C3) Material consistency - the object material ùëÜ remain fixed. The edits apply only to the albedo components ùê¥ of specific objects. In the scope of the defined editing task in Eq. (2), the editing function ùëì () needs to internally infer the underlying scene parameters and localize the editing objects, lights, and regions to perform precise, constrained color changes."
        },
        {
            "title": "3.2 Preliminaries: Attention in MM-DiT Blocks",
            "content": "The attention mechanism for text-vision fusion in MM-DiT [Esser et al. 2024] differs from that in U-Net [Rombach et al. 2022]. In U-Net, cross-attention is used for text guidance, while self-attention focuses on visual content interactions, separately. In contrast, MMDiT integrates text and vision by concatenating their tokens together and processing them jointly via sole self-attention. As illustrated in Fig. 2 (a), vision and text tokens ùíõ are fed into the ùëñ-th MM-DiT block at timestep ùë° during inference. After modulation, the block produces an attention map ùë¥ and value tokens ùëΩ , which are used to generate the updated tokens ÀÜùëß for the next block and timestep. The resulting attention map ùë¥ can be divided into four parts: vision-to-vision (upper-left), vision-to-text (upper-right), text-to-vision (lower-left), and text-to-text (lower-right). These quadrants correspond to different query-key token pairings. For example, the text-to-vision region represents attention scores computed between query tokens from the text modality ùë∏ text and key tokens from the vision modality ùë≤ vision. Similarly, the value tokens ùëΩ are composed of vision part ùëΩ vision and text part ùëΩ text. The functional roles of each region are discussed in detail in the following sections."
        },
        {
            "title": "3.3 Structure Preservation",
            "content": "Following [Cao et al. 2023; Hertz et al. 2023], we divide the editing process into source branch and target branch. The source branch follows the original generation process, producing source image and storing intermediate attention outputs. The target branch reuses 4 Zixin et al. Method Fix seed FireFlow [Deng et al. 2025] RF-Solver [Wang et al. 2025] SDEdit [Meng et al. 2022] DiTCtrl [Cai et al. 2025] UniEdit-Flow [Jiao et al. 2025] Ours Canny SSIM 0.5787 0.6078 0.6711 0.6353 0.8119 0.8016 0.8473 SD BG Preservation PSNR 20.44 19.19 23.30 27.78 35.40 36.31 42.93 SSIM 0.8411 0.8461 0.8906 0.8699 0.9812 0.9774 0.9960 Clip Similarity Whole Edited 29.17 28.63 27.43 25.98 26.21 26.08 28.32 27.54 27.24 25.96 24.47 24.67 24.67 26. Canny SSIM 0.7180 0.8322 0.8394 0.8285 0.8306 0.8498 0.9196 FLUX.1-dev BG Preservation PSNR 22.32 35.87 36.25 32.62 34.48 37.57 39.49 SSIM 0. 0.9717 0.9715 0.9605 0.9791 0.9777 0.9936 Clip Similarity Whole Edited 27. 25.84 25.83 25.54 25.89 25.78 27.34 25.76 23.56 23.58 23.17 23.58 23.44 24.90 Table 1. Quantitative image results compared with training-free methods on PIE-Bench. Results for FireFlow on SD3 are omitted due to their consistency being worse than that obtained using fixed seeds. Fig. 3. Top row: SD3 results; bottom row: FLUX.1-dev results. (a) The edit prompt is white fox orange fox. Left to right: source image, our full method, without color preservation, with swapped text-to-text part in structure preservation, and with swapped ùëâ text in color preservation. (b) The generation prompt is white fox in forest, and the token for mask extraction is fox. From left to right: the mask extracted from vision-to-text parts, and from text-to-vision parts. these stored variables to generate edited results. Starting from fixed random seed, the model can generate the desired object without applying any editing method. However, the resulting image often diverges significantly from the source, making it unsuitable for meaningful editing tasks that require structural consistency. To address this, all subsequent modifications are applied exclusively on the target branch, aiming to improve consistency without disrupting the intended edit. The process of keeping ùê∫, ùëÜ, ùê∂, light positions, and scalar intensities fixed (as in constraints (C1)-(C3)) is referred to as structure preservation. We observe that the vision-to-vision part of the attention map inherently encodes rich knowledge about the parts of the scene that must remain unchanged. Given source attention map ùë¥, its vision-to-vision part is transferred from ùë¥ to the corresponding part in the target attention map ùë¥, producing an edited attention map ÀÜùë¥ that fully respects the structure preservation constraints, as described in Fig. 2 (b)."
        },
        {
            "title": "3.4 Color Preservation",
            "content": "Despite enforcing structure preservation, we observe that undesired changes such as color shifts can still occur in regions unrelated to the edit, as shown in Fig. 3 (a). To further localize the edit and reduce inconsistencies, edits should be confined to the intended regions, while preserving all other areas. Motivated by the approach of Cai et al. [2025], we first extract binary mask ùíé from visionto-text parts of attention maps with threshold ùúñ, which indicates the target editing region. In contrast to Cai et al. [2025], which averages the vision-to-text and text-to-vision parts to obtain the mask, we use only the vision-to-text parts, as they provide superior spatial localization for the target object, unlike the text-to-vision parts (see Fig. 3 (b)). Based on ùíé, we copy the value tokens from the non-editing regions in the vision part of the source ùëΩ into the corresponding regions of the target ùëΩ , yielding the final value tokens ÀÜùëΩ , as shwon in Fig. 2 (c). We refer to this procedure as color preservation. As shown in Fig. 3 (a), including the text part of value tokens ùëΩ text during value exchange significantly weakens text guidance in FLUX.1-dev, and leads to severe artifacts in SD3."
        },
        {
            "title": "3.5 Attribute Re-Weighting",
            "content": "So far, our method enables powerful color editing capabilities. However, the granularity of control via text prompts remains limited. For example, if the user specifies color like dark yellow, there is no way to explicitly control the degree of darkness. Existing reweighting techniques in U-Net-based models primarily follow two approaches: (1) Scaling the text embedding of specific tokens before the diffusion process [Perry 2023]. However, this method is designed for CLIP-based [Radford et al. 2021] text encoders and is incompatible with MM-DiT-based models, which typically use T5 [Raffel et al. 2020]. (2) Scaling the attention scores of specific tokens in the cross-attention map [Hertz et al. 2023]. This is also inapplicable to MM-DiT, which relies solely on self-attention. Moreover, both approaches fail to maintain structural consistency during scaling, limiting their utility in high-fidelity editing tasks. To support more fine-grained and user-friendly control in MMDiT-based models, we introduce mechanism to modulate the strength of specific attribute words. Specifically, we scale the attention scores in the text-to-vision parts of the attention map corresponding to the selected word tokens before the softmax operation, as illustrated in Fig. 2 (d). This modification can be applied either directly on the original source attention maps or on the target attention maps that have already undergone structure preservation, allowing flexible integration in above editing pipeline."
        },
        {
            "title": "4 Experiments\n4.1 Setup",
            "content": "Baselines. We compare our method against several training-free approaches built upon MM-DiT, including FireFlow [Deng et al. 2025], RF-Solver [Wang et al. 2025], SDEdit [Meng et al. 2022], DiTCtrl [Cai et al. 2025], FlowEdit [Kulikov et al. 2024], and UniEdit-Flow [Jiao et al. 2025]. We focus exclusively on MM-DiT-based baselines, as prior work [Deng et al. 2025; Jiao et al. 2025] has shown that U-Netbased methods perform significantly worse. Accordingly, we exclude methods that cannot be adapted to the MM-DiT architecture. For commercial baselines, we compare with FLUX.1 Kontext Max [Black Forest Labs 2025] and GPT-4o Image Generation [OpenAI 2025]. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer 5 Fig. 4. Qualitative image results compared with training-free methods and commercial models on PIE-Bench. The top three rows are generated using FLUX.1-dev, while the bottom two are generated using SD3. Best viewed with zoom-in. Implementation. We conduct experiments on SD3 [Esser et al. 2024] and FLUX.1-dev [Labs 2024] for image generation, and on CogVideoX-2B [Yang et al. 2024] for video generation. For instructionbased image editing, we use Step1X-Edit [Liu et al. 2025a] and FLUX.1 Kontext dev [Black Forest Labs 2025]. For FLUX.1-dev, Step1X-Edit, and FLUX.1 Kontext dev, we apply attention control to the single-stream attention layers, following Deng et al. [2025]. Unless otherwise noted, we use the Euler sampler and adopt UniEditFlow [Jiao et al. 2025] for image inversion. Maintaining balance between fidelity to the original image and the strength of the applied edits is well-known trade-off in generative editing. To ensure fair comparison across methods, we carefully tune the hyperparameters for each baseline. Additional details are provided in Appendix A. Benchmark. While prior editing methods [Cai et al. 2025; Cao et al. 2023; Hertz et al. 2023] typically lack standardized benchmark evaluation, we adopt prompts from the Change Color task in PIEBench [Ju et al. 2024], which includes 40 editing pairs, to better showcase the capabilities of our method. Although our approach is fully compatible with inversion methods, we adopt noise-to-image setting to better isolate and evaluate editing performance, removing the influence of reconstruction and inversion. This also allows us to reuse the same set of benchmark prompts for video diffusion models, enabling consistent and fair comparison across both image and video domains. To further scale up the evaluation, we introduce new benchmark called ColorCtrl-Bench, consisting of 300 prompt pairs in the same style as PIE-Bench (see Appendix A.4 for details). For all baselines, we adopt fixed sampler and identical random seeds to ensure that source images are consistent, enabling reliable comparison across methods. Evaluation protocol. Unlike the original PIE-Bench, which evaluates structural similarity using structural distance [Tumanyan et al. 2022], we adopt the Structural Similarity Index (SSIM) [Wang et al. 2004] computed on Canny edge maps [Canny 1986], following the approach of Zhao et al. [2023], for more accurate assessment. To evaluate the preservation of non-edited regions (a.k.a., BG Preservation), we compute PSNR and SSIM exclusively on those regions, which are annotated using Grounded SAM 2 [Ren et al. 2024] with dilation. Semantic alignment of the edits is assessed using CLIP similarity [Radford et al. 2021], applied to both the entire image and the edited regions."
        },
        {
            "title": "4.2 Comparison with Training-Free Methods (Images)",
            "content": "Tab. 1 and Tab. 5 report benchmark results on both SD3 and FLUX.1dev, comparing our method with other training-free baselines. Our method achieves state-of-the-art performance, delivering superior results in both preserving source content and executing accurate edits. Fig. 4 further supports this finding: other methods exhibit limited capacity for color editing and often introduce visual inconsistencies, while ours produces coherent and faithful edits. Additional results are in Appendix B."
        },
        {
            "title": "4.3 Comparison with Commercial Models (Images)",
            "content": "Tab. 2 and Tab. 6 compare our method (based on SD3 and FLUX.1dev) with two commercial models: FLUX.1 Kontext Max [Black Forest Labs 2025] and GPT-4o Image Generation [OpenAI 2025]. Despite being based on open-source models, our approach achieves superior layout and detail consistency, as well as better preservation of non-edited regions. While the CLIP similarity scores of our method are slightly lower, visual results in Fig. 4 reveal that the commercial models often rely on over-saturated, unrealistic edits to better align with prompts. For example, in the top row, FLUX.1 Kontext Max recolors the entire mouse, including its magic wand, in solid purple, while GPT-4o produces dark, dissonant shade. In contrast, our method applies harmonious color. In the second row, only our method preserves the shirts semi-transparency, whereas the commercial models render it as opaque yellow, ignoring material properties. In the third row, our method respects the muted tone of green tea and edits the ice cream to natural reddish-brown red tea color. The commercial models, however, apply an unnaturally 6 Zixin et al. Method FLUX.1 Kontext Max [Black Forest Labs 2025] GPT-4o Image Generation [OpenAI 2025] Ours Canny SSIM 0.7305 0.6240 0.8473 SD3 BG Preservation Clip Similarity PSNR 31.97 23.69 42.93 SSIM 0.9254 0.8134 0.9960 Whole 28.93 29.51 28.32 Edited 27.30 28.08 26.96 Canny SSIM 0.7607 0.7431 0.9196 FLUX.1-dev BG Preservation Clip Similarity PSNR 26.77 23.71 39.49 SSIM 0.9165 0.8755 0.9936 Whole 28.36 28.84 27.34 Edited 26.10 26.46 24.90 Table 2. Quantitative image results compared with commercial models on PIE-Bench. Method Fix seed FireFlow [Deng et al. 2025] RF-Solver [Wang et al. 2025] SDEdit [Meng et al. 2022] DiTCtrl [Cai et al. 2025] FlowEdit [Kulikov et al. 2024] UniEdit-Flow [Jiao et al. 2025] Ours Canny SSIM 0. 0.7517 0.7677 0.7880 0.6912 0.7852 0.7645 0.8651 PIE-Bench [Ju et al. 2024] BG Preservation PSNR SSIM Clip Similarity Whole Edited 20.73 30.74 35.41 35.46 27.70 34.24 38.00 38. 0.8912 0.9605 0.9730 0.9689 0.9435 0.9704 0.9772 0.9916 27.97 25.42 25.40 24.64 27.02 26.13 25.35 27. 26.63 24.18 24.08 23.21 25.57 24.97 24.13 25.96 Canny SSIM 0.6671 0.7957 0.8102 0.8281 0.7270 0.8639 0. 0.8885 ColorCtrl-Bench BG Preservation PSNR SSIM Clip Similarity Whole Edited 19.49 28.78 32.64 34.30 26.14 32.64 35.33 38.68 0. 0.9323 0.9562 0.9626 0.9165 0.9774 0.9613 0.9893 28.01 25.26 25.36 24.74 26.54 25.95 25.39 27.32 26. 24.12 24.16 23.46 25.14 23.63 24.14 26.16 Table 3. Quantitative video results compared with baselines on PIE-Bench and ColorCtrl-Bench. Fig. 5. Qualitative video results compared with training-free methods on PIE-Bench. Prompt is green tea red tea. Each shows three frames. pure red. In the final row, although the prompt requests yellow kitten, no naturally occurring cat has truly pure yellow coat. Our method instead generates kitten with the closest plausible fur color, aligned with real-world appearances, unlike the commercial models, which apply flat, high-saturation tones that appear unnatural. These results demonstrate that higher CLIP similarity does not necessarily indicate better edit quality. It often stems from prompt overfitting while compromising realism and consistency. Overall, our method consistently produces more faithful, controllable edits, even built on open-source models."
        },
        {
            "title": "4.4 Comparison with Training-Free Methods (Videos)",
            "content": "Thanks to the training-free nature of our method, it can be seamlessly extended to video editing tasks. Since FLUX.1 Kontext Max and GPT-4o do not support video editing, we compare only with other training-free methods. FlowEdit is excluded as it only applies to rectified flow models rather than diffusion-based models. Tab. 3 presents benchmark results on CogVideoX-2B [Yang et al. 2024]. Similar to image editing, our method outperforms all baselines. Notably, the performance gap becomes even more pronounced due to the added temporal dimension. Visualization results in Fig. 5 further highlight the effectiveness of ColorCtrl. Consistent with its performance in image editing, our method also handles challenging cases in video, such as accurately reflecting the color change of the ice cream in the bowls reflection. Please refer to Appendix for additional comparisons."
        },
        {
            "title": "4.5 Attribute Re-Weighting Analysis",
            "content": "Although DiTCtrl [Cai et al. 2025] includes mechanism for attribute re-weighting, both the paper and results in Tab. 1 suggest Fig. 6. Examples of attribute re-weighting. The top two rows are generated using FLUX.1-dev, while the bottom one are generated using SD3. limitations. Specifically, the method struggles to achieve the intended color changes while maintaining consistency with the source image. Moreover, their approach of scaling attention weights after the softmax operation violates the assumption that attention scores should sum to one, which leads to incorrect attention behavior. Since none of the baselines can smoothly adjust attribute strength while simultaneously satisfying the three constraints ((C1)-(C3)), we present results of our method alone in Fig. 6. These results show that our method not only supports re-weighting single attribute within the same prompt, but also allows adjusting attribute strength across different prompts (second row). Moreover, our method can re-weight multiple attributes simultaneously (third row). Overall, these results demonstrate that our method enables smooth and controllable transitions in attribute strength, while preserving structural consistency across the image and maintaining color fidelity in non-edited regions, on both SD3 and FLUX.1-dev."
        },
        {
            "title": "4.6 Ablation Study",
            "content": "According to Tab. 4, starting from fixed random seed generation, we observe the highest CLIP similarity due to the lack of consistency constraints. However, this comes at the cost of very low scores in Canny SSIM, as well as PSNR and SSIM in non-edited regions, Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer 7 Fig. 7. Examples of results generated with Step1X-Edit (left) and FLUX.1 Kontext dev (right). Green arrows: first edit using the editing model. Blue arrows: second edit using the editing model. Orange arrows: second edit using the editing model with ColorCtrl. Top left: red diamond is added to the neck, then changed to blue. Bottom left: an orange cap is added, then changed to purple. Top right: blue butterflies are added, then changed to yellow. Bottom right: flashlight with blue light is added, then changed to green. Method Fix seed + Structure Preservation + Color Preservation (Ours) Canny SSIM 0.5787 0.7312 0.8473 SD3 BG Preservation PSNR 20.44 24.77 42.93 SSIM 0.8411 0.9201 0.9960 Clip Similarity Whole 29.17 28.41 28.32 Edited 27.54 27.29 26.96 Canny SSIM 0.7180 0.9019 0.9196 FLUX.1-dev BG Preservation Clip Similarity PSNR 22.32 30.20 39.49 SSIM 0.8877 0.9719 0. Whole 27.72 27.44 27.34 Edited 25.76 25.22 24.90 Table 4. Ablation study evaluating the effectiveness of each component on PIE-Bench. is also compatible with instruction-based editing diffusion models, such as Step1X-Edit [Liu et al. 2025a] and FLUX.1 Kontext dev [Black Forest Labs 2025]. Given real input image and target editing instruction, the model performs edits accordingly. However, performing second round of editing using the original model alone often leads to structural inconsistencies, such as distortions or shifts in shadows and edges. By incorporating our method, the model can further refine color edits while preserving structural fidelity. As shown in Fig. 7, compared to using the base model alone, our approach achieves better outline consistency and improved preservation of subtle visual cues like shadows. Discussion. Our method integrates structure preservation, regional color preservation, and word-level attribute intensity control into unified, training-free pipeline. These components work together to enable precise and consistent text-driven color editing. Each part of the attention computation plays distinct role: the vision-to-vision part of ùë¥ preserves structure, the vision-to-text part is used for mask extraction, the text-to-vision part enables attribute re-weighting, the vision part of ùëΩ supports color preservation, and the text-to-text region of ùë¥ along with the text part of ùëΩ provides crucial textual guidance. As demonstrated by the degradation observed in Fig. 3 (a) when the text-to-text region of ùë¥ or the text part of ùëΩ is altered, preserving the integrity of these parts is essential for maintaining robust and coherent generation that aligns with the target prompt."
        },
        {
            "title": "5 Conclusion",
            "content": "We have introduced ColorCtrl, training-free method for textguided color editing that achieves fine-grained, physically consistent control over albedo, light source color, and ambient illumination. Our method is designed to edit only the intended visual attributes specified in the prompt, leaving all unrelated regions untouched. Built upon diverse MM-DiT-based diffusion models, such as SD3 and FLUX.1-dev, our approach enables fine-grained control over attribute intensity, while preserving geometry, material properties, Fig. 8. Examples of real image editing. Left: results generated with SD3. Right: results generated with FLUX.1-dev. indicating poor structural and visual consistency. Introducing the structure preservation component significantly improves all consistency metrics, suggesting that the geometric and material attributes are effectively maintained, as also illustrated in Fig. 3. Adding the color preservation component completes our method, which further enhances consistency, particularly in non-edited regions, while sacrificing almost no CLIP similarity. Overall, the results validate the effectiveness of each component in our method."
        },
        {
            "title": "4.7 Compatibility and Discussion",
            "content": "Real image editing. To apply ColorCtrl to real images, we integrate UniEdit-Flow [Jiao et al. 2025] for inversion and replace the original editing module with our method. The results, shown in Fig. 8, demonstrate that our approach generalizes well to real-world inputs on both SD3 and FLUX.1-dev, preserving fine-grained consistency (e.g., subtle fabric wrinkles and shadows) while delivering strong editing performance. Notably, in the top row, even when editing black clothing, ColorCtrl accurately distinguishes material shading from cast shadows, resulting in illumination-consistent edits. Generalization to instruction-based editing diffusion models. In addition to text-to-image and text-to-video models, ColorCtrl 8 Zixin et al. and light-matter interactions. ColorCtrl not only outperforms prior training-free methods and achieves state-of-the-art results but also delivers stronger consistency than commercial models, i.e., FLUX.1 Kontext Max and GPT-4o Image Generation, on both quantitative and qualitative evaluations. Moreover, its model-agnostic design generalizes naturally to video diffusion models (i.e., CogVideoX) and instruction-based editing diffusion models (i.e., Step1X-Edit and FLUX.1 Kontext dev), highlighting its broad applicability. We believe ColorCtrl paves the way for scalable, high-fidelity, and controllable color editing in both research and practical deployment."
        },
        {
            "title": "References",
            "content": "Stability AI. 2024. Stable Diffusion 3.5. https://github.com/Stability-AI/sd3.5. Accessed: May 2025. Black Forest Labs. 2025. FLUX.1 Kontext: Flow Matching for In-Context Image Generation and Editing in Latent Space. https://bfl.ai/announcements/flux-1-kontext. Accessed: 2025-06-13. Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1839218402. Minghong Cai, Xiaodong Cun, Xiaoyu Li, Wenze Liu, Zhaoyang Zhang, Yong Zhang, Ying Shan, and Xiangyu Yue. 2025. Ditctrl: Exploring attention control in multimodal diffusion transformer for tuning-free multi-prompt longer video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 77637772. John Canny. 1986. computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelligence 6 (1986), 679698. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. 2023. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF international conference on computer vision. 2256022570. Minghao Chen, Iro Laina, and Andrea Vedaldi. 2024. Training-free layout control with cross-attention guidance. In Proceedings of the IEEE/CVF winter conference on applications of computer vision. 53435353. Yingying Deng, Xiangyu He, Changwang Mei, Peisong Wang, and Fan Tang. 2025. FireFlow: Fast Inversion of Rectified Flow for Image Semantic Editing. In Forty-second International Conference on Machine Learning. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. 2024. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. In The Twelfth International Conference on Learning Representations. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohenor. 2023. Prompt-to-Prompt Image Editing with Cross-Attention Control. In The Eleventh International Conference on Learning Representations. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems 33 (2020), 68406851. Jonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. 2024. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1246912478. Guanlong Jiao, Biqing Huang, Kuan-Chieh Wang, and Renjie Liao. 2025. UniEditFlow: Unleashing Inversion and Editing in the Era of Flow Models. arXiv preprint arXiv:2504.13109 (2025). Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. 2024. PnP Inversion: Boosting Diffusion-based Editing with 3 Lines of Code. In The Twelfth International Conference on Learning Representations. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. 2024. HunyuanVideo: Systematic Framework For Large Video Generative Models. CoRR (2024). Vladimir Kulikov, Matan Kleiner, Inbar Huberman-Spiegelglas, and Tomer Michaeli. 2024. Flowedit: Inversion-free text-based editing using pre-trained flow models. arXiv preprint arXiv:2412.08629 (2024). Black Forest Labs. 2024. Flux. https://github.com/black-forest-labs/flux. Accessed: May 2025. Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. 2025a. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761 (2025). Shaoteng Liu, Tianyu Wang, Jui-Hsien Wang, Qing Liu, Zhifei Zhang, Joon-Young Lee, Yijun Li, Bei Yu, Zhe Lin, Soo Ye Kim, et al. 2025b. Generative video propagation. In Proceedings of the Computer Vision and Pattern Recognition Conference. 1771217722. Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. 2024. Video-p2p: Video editing with cross-attention control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 85998608. Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, and Yedid Hoshen. 2025. LightLab: Controlling Light Sources in Images with Diffusion Models. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers. 111. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. 2022. SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. In International Conference on Learning Representations. OpenAI. 2025. GPT 4o Image Generation. https://openai.com/index/introducing-4oimage-generation/. Accessed: 2025-06-13. William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision. 41954205. Damian Perry. 2023. Compel: Prompt Parser and Conditioning Tuning for Diffusion Models. https://github.com/damian0815/compel. Accessed: 2025-07-01. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. Generative adversarial text to image synthesis. In International conference on machine learning. PMLR, 10601069. Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. 2024. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159 (2024). Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1068410695. Litu Rout, Yujia Chen, Nataniel Ruiz, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng Chu. 2025. Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations. In The Thirteenth International Conference on Learning Representations. Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2022. Splicing vit features for semantic appearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1074810757. Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. 2023. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1797917989. Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. 2025. Taming Rectified Flow for Inversion and Editing. In Forty-second International Conference on Machine Learning. Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600612. Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, Charles Ling, and Boyu Wang. 2025. Unveil inversion and invariance in flow transformer for versatile image editing. In Proceedings of the Computer Vision and Pattern Recognition Conference. 2847928489. Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. 2023. Inversion-Free Image Editing with Natural Language. CoRR (2023). Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, and Linfeng Zhang. 2025. Eedit: Rethinking the spatial and temporal redundancy for efficient image editing. arXiv preprint arXiv:2503.10270 (2025). Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. 2024. Cogvideox: Textto-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072 (2024). Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn Wong, and Baoyuan Wang. 2023. Talking head generation with probabilistic audio-to-visual diffusion priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 76457655. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2025. Scaling in-the-wild training for diffusion-based illumination harmonization and editing by imposing consistent light transport. In The Thirteenth International Conference on Learning Representations. Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. 2023. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in Neural Information Processing Systems 36 (2023), 1112711150. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer 9 Method Fix seed FireFlow [Deng et al. 2025] RF-Solver [Wang et al. 2025] SDEdit [Meng et al. 2022] DiTCtrl [Cai et al. 2025] FlowEdit [Kulikov et al. 2024] UniEdit-Flow [Jiao et al. 2025] Ours Canny SSIM 0.6805 0.6625 0.7329 0.7643 0.8465 0.8261 0.8503 0.8775 SD3 BG Preservation PSNR SSIM Clip Similarity Whole Edited 16.93 13.87 18.69 27.24 31.65 30.77 32. 38.16 0.8303 0.7664 0.8596 0.9287 0.9723 0.9622 0.9725 28.27 27.45 26.70 25.22 25.25 25.53 25.24 0. 28.07 26.65 25.95 25.19 23.77 23.82 24.24 23.81 26.69 Canny SSIM 0. 0.8553 0.8832 0.8430 0.8699 0.8704 0.8725 0.9324 FLUX.1-dev BG Preservation PSNR SSIM Clip Similarity Whole Edited 19.54 32.80 36.25 29.47 31.72 29.29 34.85 37. 0.8552 0.9594 0.9736 0.9428 0.9707 0.9620 0.9676 27.70 25.05 24.89 24.45 24.90 25.09 25.04 0.9901 26. 26.25 23.69 23.49 22.83 23.53 23.52 23.61 25.26 Table 5. Quantitative image results compared with training-free methods on ColorCtrl-Bench. Results for FireFlow on SD3 are omitted due to consistency worse than that obtained using fixed seeds. Implementation Details Inference Settings A.1 For benchmarking, we use 28 inference steps for both SD3 [Esser et al. 2024] and FLUX.1-dev, with the classifier-free guidance (CFG) scale [Ho and Salimans 2021] set to 7.5. All images are generated at resolution of 1024 1024. For CogVideoX-2B, we use 50 inference steps and set the CFG scale to 6, generating videos at 49 frames with 720 480 resolution. fixed random seed of 42 is used for all benchmark experiments. For real image editing, we use the latest inversion method from UniEdit-Flow [Jiao et al. 2025] and set the CFG scale to 1 according to the method. The target object for mask extraction is determined using the blended_word keywords provided by PIE-Bench [Ju et al. 2024] and ColorCtrl. We use mask threshold of ùúñ = 0.1, which consistently yields strong performance across SD3, FLUX.1-dev, and CogVideoX2B. Despite being relatively coarse, this threshold is sufficient due to the strong global adaptation ability of modern generative models, which enables them to propagate edits from sparse color cues to semantically aligned regions. For Step1X-Edit [Liu et al. 2025a] and FLUX.1 Kontext dev [Black Forest Labs 2025], we use their official code and setting with resolution 1024 1024. Image generation is performed on an RTX 4090 GPU, while video generation uses an A100 GPU. A.2 Sampling Details To avoid redundant computation and accelerate inference, the source branch is first executed to cache attention maps and value tokens for reuse, following similar strategy to that in Wang et al. [2025]. During this stage, the editing mask ùëö is also computed to indicate the regions to be modified. In the actual editing phase, the cached features and mask are directly loaded, eliminating the need to recompute the source branch. This approach ensures that the editing process remains as efficient as standard sampling methods, without introducing any additional computational cost. A."
        },
        {
            "title": "Implementation of Compared Methods",
            "content": "Several compared methods lack official implementations for SD3 or CogVideoX-2B, or do not provide compatible sampling code. To ensure fair comparison, we reimplement these methods on SD3, FLUX.1-dev, and CogVideoX-2B by faithfully following their original designs and carefully tuning hyperparameters to match the reported performance. Detailed implementations are as follows: Fig. 9. Examples of FireFlow and RF-Solver with difference end timesteps on SD3. The selected end timestep refers to the setting used in the benchmark evaluation, while the higher end timestep denotes larger value chosen for comparison purposes. DiTCtrl [Cai et al. 2025]: For SD3-based image editing, we set the editing range from timestep 2 to 17, modifying the last 5 blocks. For FLUX.1-dev, we edit from timestep 2 to 11 across the last 6 blocks. For video editing with CogVideoX-2B, the official implementation is used. During editing, key (ùë≤ )and value (ùëΩ ) tokens from the source branch are copied into the attention layers of the target branch. FlowEdit [Kulikov et al. 2024]: We adopt the official image editing setting for FLUX-1-dev. For SD3, we rescale the ùëõ_ùëöùëéùë• factor to account for the change in inference steps. Specifically, since the number of steps has changed from 50 to 28, we set ùëõ_ùëöùëéùë• = 28 50 33 18, while keeping all other parameters unchanged. UniEdit-Flow [Jiao et al. 2025]: The official version supports SD3 and FLUX.1-dev but provides the ùúî parameter only for CFG = 1. Following the similarity transformation introduced in the paper, we use ùúî = 5 7.5 0.6 and set ùõº = 0.6 for SD3, ùõº = 0.85 for FLUX.1-dev, and ùõº = 0.8 for video generation, which yields comparable performance to the original. FireFlow [Deng et al. 2025]: In SD3, we observe that it is difficult to select suitable end timestep for FireFlow, as setting it too high often leads to artifacts and generation failures, as shown in Fig. 9. To mitigate quality degradation caused by excessive editing steps, we restrict editing to timesteps 0 through 3 across all blocks in SD3. In contrast, FLUX.1-dev does not exhibit this issue, and using the same range (timesteps 0 to 3) yields results comparable to those reported in the original paper. For video generation, the editing ends at timestep 9. During editing, value tokens (ùëΩ ) are copied from the source to the target. RF-Solver [Wang et al. 2025]: RF-Solver exhibits similar issue to FireFlow when applied to SD3, where higher editing timesteps lead to artifacts and degraded generation quality, as illustrated in Fig. 9. To address this, we limit editing to timesteps 0 to 7 in the second half of the SD3 blocks. For FLUX.1-dev, we set the 10 Zixin et al. Method FLUX.1 Kontext Max [Black Forest Labs 2025] GPT-4o Image Generation [OpenAI 2025] Ours Canny SSIM 0.8016 0.6988 0.8775 SD3 BG Preservation PSNR 30.40 21. 38.16 SSIM 0.9152 0.8030 0.9896 Clip Similarity Whole 28.23 28.24 28.07 Edited 26.83 26.66 26.69 Canny SSIM 0.8032 0.7727 0.9324 FLUX.1-dev BG Preservation Clip Similarity PSNR 27.18 23.05 37.96 SSIM 0.8854 0.8371 0.9901 Whole 27.56 27.65 26.53 Edited 26.24 26.24 25.26 Table 6. Quantitative image results compared with commercial models on ColorCtrl-Bench. end timestep to 4, which yields stable results without noticeable artifacts. Value tokens (ùëΩ ) are copied from the source to the target. For video generation, editing is performed up to timestep 9. SDEdit [Meng et al. 2022]: We set ùë°0 = 0.6 and apply editing to both generated and real input images or videos. The same parameter is applied across SD3, FLUX.1-dev, and CogVideoX-2B. FLUX.1 Kontext Max [Black Forest Labs 2025]: Results are obtained using the official API, with source images and instruction prompts taken from PIE-Bench [Ju et al. 2024]. GPT-4o Image Generation [OpenAI 2025]: Results are generated through the official client using the source images and instruction prompts from PIE-Bench. A.4 ColorCtrl-Bench Construction Here, we provide details on the construction of ColorCtrl-Bench. Following similar approach to that in Ju et al. [2024], we use GPT to generate dataset of tuples, each consisting of source prompt, target prompt, subject token, and an instruction, consistent with the format used in PIE-Bench. These tuples represent an image before and after color editing operation, along with an instruction describing the transformation and subject token indicating the object to be modified. The exact template is shown in Fig. 17. A.5 Evaluation Details For editing region masks, we first detect the target object with Grounded SAM 2 [Ren et al. 2024], using the blended_word in the PIE-Bench and ColorCtrl-Bench as the detection keyword. The raw mask is then dilated to compensate for the uncertainty of the boundary, slightly enlarging the edited region. marginally eroded mask is still adequate for BG preservation metrics, and marginally dilated foreground does not bias CLIP similarity scores. Hence, this dilation neither undermines the consistency check nor suffers from the boundary instability of Grounded SAM 2. For video evaluation, we compute each metric frame-wise and report the average over all frames as the final score for the video. More Results and Analysis B.1 More Results of Image Editing Tab. 5 and Tab. 6 present quantitative results of image editing tasks on ColorCtrl-Bench, comparing our method against both trainingfree baselines and commercial models. Visual examples are shown in Fig. 13. Consistent with our findings on PIE-Bench, ColorCtrl outperforms other methods in maintaining object consistency and achieves accurate color edits with proper illumination and reflection. The similar conclusion drawn from this larger-scale benchmark further highlights the robustness and effectiveness of our approach. Fig. 15 presents additional image editing results on PIE-Bench, comparing our method with both training-free and commercial Fig. 10. Examples of image editing results. Fig. 11. Additional qualitative video results compared with trainingfree models on ColorCtrl-Bench. Left: gray sofa brown sofa. Right: brown suitcase black suitcase. Each shows three frames. models. In the first column, only ColorCtrl successfully preserves the texture of the rocks, while other methods either fail to change the color or introduce texture distortions. In the third column, FLUX.1 Kontext Max and GPT-4o alter the pose and fabric wrinkles, whereas other training-free methods fail to perform the intended edit. Fig. 10 shows additional image results with FLUX.1-dev. B.2 More Results of Video Editing Fig. 14 and Fig. 11 present video editing results on ColorCtrl-Bench, while Fig. 16 shows additional results on PIE-Bench. Our method consistently outperforms all baselines, achieving superior color editing accuracy and better structural consistency across frames. Training-Free Text-Guided Color Editing with Multi-Modal Diffusion Transformer 11 Fig. 13. Qualitative image results compared with training-free methods and commercial models on ColorCtrl-Bench. The left four columns are generated using FLUX.1-dev, while the right three are generated using SD3. Best viewed with zoom-in. Fig. 15. Additional qualitative image results compared with trainingfree methods and commercial models on PIE-Bench. The left three columns are generated using FLUX.1-dev, while the right two are generated using SD3. Best viewed with zoom-in. B.3 Limitations The generation quality and the precision of text-guided localization in our method are fundamentally limited by the capabilities Fig. 14. Qualitative video results compared with training-free models on ColorCtrl-Bench. Left: yellow bus green yellow. Right: green backpack yellow backpack. Each shows three frames. of the underlying generative models. As shown in Fig. 12, failure case with SD3 demonstrates that the model fails to correctly detect red trees and instead modifies all trees, including green ones that should remain unchanged. Similarly, in the example with FLUX.1-dev, the model misinterprets lipstick and edits the casing rather than the lipstick itself. As foundation models continue to improve, we expect the performance and applicability of our method to advance accordingly. Fig. 12. Examples of failure cases. Left: results generated with SD3. Right: results generated with FLUX.1-dev. Furthermore, editing real images and videos remains challenging due to the limitations of current inversion and reconstruction techniques. Although our method performs reliably on data that lies within the training distribution of the generative model, handling real-world inputs requires first mapping them accurately into the latent space of the model. This mapping process is still difficult and highly sensitive to the quality of the inversion. 12 Zixin et al. Fig. 16. Additional qualitative video results compared with trainingfree models on PIE-Bench. Left: white shirt blue shirt. Right: white lamb blue lamb. Each shows three frames. Fig. 17. The prompt used to generate ColorCtrl-Bench with GPT."
        }
    ],
    "affiliations": [
        "Astribot",
        "International Digital Economy Academy",
        "StepFun",
        "The Hong Kong University of Science and Technology",
        "Tsinghua University"
    ]
}