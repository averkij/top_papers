{
    "paper_title": "OmniScientist: Toward a Co-evolving Ecosystem of Human and AI Scientists",
    "authors": [
        "Chenyang Shao",
        "Dehao Huang",
        "Yu Li",
        "Keyu Zhao",
        "Weiquan Lin",
        "Yining Zhang",
        "Qingbin Zeng",
        "Zhiyu Chen",
        "Tianxing Li",
        "Yifei Huang",
        "Taozhong Wu",
        "Xinyang Liu",
        "Ruotong Zhao",
        "Mengsheng Zhao",
        "Xuhua Zhang",
        "Yue Wang",
        "Yuanyi Zhen",
        "Fengli Xu",
        "Yong Li",
        "Tie-Yan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as \"AI Scientists.\" However, existing AI Scientists predominantly formulate scientific discovery as a standalone search or optimization problem, overlooking the fact that scientific research is inherently a social and collaborative endeavor. Real-world science relies on a complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish a genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, a framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) a structured knowledge system built upon citation networks and conceptual correlations; (2) a collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering a sustainable and scalable innovation ecosystem."
        },
        {
            "title": "Start",
            "content": "OmniScientist: Toward Co-evolving Ecosystem of Human and AI Scientists Chenyang Shao1,2 Dehao Huang2 Yu Li1 Keyu Zhao1 Weiquan Lin2 Yining Zhang2 Qingbin Zeng1 Zhiyu Chen2 Tianxing Li1 Yifei Huang2 Taozhong Wu2 Xinyang Liu1 Ruotong Zhao1 Mengsheng Zhao2 Xuhua Zhang2 Yue Wang2 Yuanyi Zhen2 Fengli Xu1,2, Yong Li1,2, Tie-Yan Liu2 1Department of Electronic Engineering, BNRist, Tsinghua University 2Zhongguancun Academy {fenglixu, liyong07}@tsinghua.edu.cn omniscientist.ai"
        },
        {
            "title": "Abstract",
            "content": "With the rapid development of Large Language Models (LLMs), AI agents have demonstrated increasing proficiency in scientific tasks, ranging from hypothesis generation and experimental design to manuscript writing. Such agent systems are commonly referred to as AI Scientists. However, existing AI Scientists predominantly formulate scientific discovery as standalone search or optimization problem, overlooking the fact that scientific research is inherently social and collaborative endeavor. Real-world science relies on complex scientific infrastructure composed of collaborative mechanisms, contribution attribution, peer review, and structured scientific knowledge networks. Due to the lack of modeling for these critical dimensions, current systems struggle to establish genuine research ecosystem or interact deeply with the human scientific community. To bridge this gap, we introduce OmniScientist, framework that explicitly encodes the underlying mechanisms of human research into the AI scientific workflow. OmniScientist not only achieves end-to-end automation across data foundation, literature review, research ideation, experiment automation, scientific writing, and peer review, but also provides comprehensive infrastructural support by simulating the human scientific system, comprising: (1) structured knowledge system built upon citation networks and conceptual correlations; (2) collaborative research protocol (OSP), which enables seamless multi-agent collaboration and human researcher participation; and (3) an open evaluation platform (ScienceArena) based on blind pairwise user voting and Elo rankings. This infrastructure empowers agents to not only comprehend and leverage human knowledge systems but also to collaborate and co-evolve, fostering sustainable and scalable innovation ecosystem. Through OmniScientist, we aim to transition AI agents from mere task executors to genuine scientists capable of understanding scientific norms, participating in collaboration, and driving the evolution of the scientific ecosystem. 5 2 0 2 1 2 ] . [ 1 1 3 9 6 1 . 1 1 5 2 : r a"
        },
        {
            "title": "3.1 Data Foundation .",
            "content": "."
        },
        {
            "title": "3.3 Research Ideation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.4 Experiment Automation .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.6 Paper Review .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Building Co-evolution Systems of Human / AI scientists",
            "content": "4.1 Protocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1. From External Users to Internal Participants . . . . . . . . . . . . . . . . . 4.1.2 Centralized Hub Enabling Multi-participant Engagement . . . . . . . . . 4.1.3 From Data Provenance to Contribution Provenance . . . . . . . . . . . . . 4.2 Closed-loop Multi Agent System . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Case Study: Variance Reduction in STDE via Closed-Loop Experiment . . . . . . 4.4 Human-AI Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Case Study: HLE Challenge via Human-AI Collaboration . . . . . . . . . . . . . . 5 Evaluation through ScienceArena 5.1 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Design: Elo-Based Real-Time Ranking . . . . . . . . . . . . . . . . . . . . . . . 5.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Citation matters for literature review . . . . . . . . . . . . . . . . . . . . . 5.3.2 Balancing novelty and feasibility in ideation . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5.3.3 Combining discriminative judgment with conciseness in paper review . . .",
            "content": "6 Discussion 6.1 Limitations . 6.2 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Conclusion 3 5 6 6 11 13 15 17 19 20 20 21 22 23 25 30 30 31 31 33 34 36 36 36"
        },
        {
            "title": "Introduction",
            "content": "The practice of science has always evolved with its tools, from the telescope and the microscope to the computer and the algorithm. Today, large language models (LLMs) represent the next major transformation. Across disciplines, LLM-powered agents are beginning to assist in tasks once reserved for human researchers: reviewing vast literatures, proposing hypotheses, writing reports, and even designing experiments. As these capabilities deepen, fundamental question arises: Can AI evolve from mere tool into genuine participant in the scientific ecosystem? Current efforts to build such AI Scientists [1, 2] have made significant strides. Systems like AlphaEvolve [3] perform iterative optimization through explicit mathematical modeling and codebased exploration of search spaces, while OpenAI Deep Research [4] conducts broad information retrieval and synthesis guided by specified research topic. Systems such as Virtual Lab [5] and Future House [6] represent further step toward automation, integrating more comprehensive AIdriven research workflows and coordinating multiple tools to accomplish complex scientific tasks. However, despite their sophistication, these approaches predominantly formulate scientific discovery as standalone search or optimization problem, overlooking fundamental reality: scientific research is inherently social and collaborative endeavor supported by complex institutional infrastructure. Due to the absence of these critical dimensions, current systems operate as isolated tools, struggling to establish genuine research ecosystem or to interact deeply with the human scientific community. Integrating the human research infrastructure is essential for advancing AI scientific intelligence. Centuries of scientific progress have yielded not just static facts, but sophisticated cognitive and structural framework. For instance, citation networks transform isolated findings into traceable lineage of ideas, revealing the evolutionary path of scientific thought; peer-review mechanisms act as rigorous quality controls ensuring reliability; and collaborative protocols regulate the exchange of contribution and credit. These structures provide the necessary environment for science to evolve. Without explicitly modeling and encoding these underlying mechanisms, AI scientists remain efficient executors but fail to inherit the dynamic, self-correcting nature of human scientific research. In this paper, we take the first step by introducing OmniScientist1, comprehensive framework that explicitly encodes human research infrastructure into the life cycle of AI-driven research. OmniScientist goes beyond simple task automation; it simulates complete scientific environment. At its core lies robust data foundation, built upon millions of full-text publications and metadata. This forms dynamic scientific network that captures citation relationships and knowledge context, serving as the systems cognitive bedrock. Building on this foundation, the literature review module employs multi-agent architecture to conduct iterative, semantically guided exploration, ensuring that agents possess comprehensive awareness of the research landscape. Guided by this context, the research ideation process leverages principles from the science of science [7] to explore and refine concepts within the citation network, generating novel hypotheses that are both contextually grounded and methodologically rigorous. For experiment automation, the system employs an iterative multi-agent loop that generates, evaluates, and refines experimental strategies, enabling selfoptimization through rigorous feedback mechanisms. Following experimentation, scientific writing is supported by an integrated framework that synthesizes related work, generates figures, and refines text according to standardized academic norms, producing coherent, publication-ready manuscripts. Finally, the system incorporates paper review mechanism that functions as quality control gate, evaluating submissions through in-depth comparison with prior work to provide objective and actionable feedback. Together, these components do not function in isolation but as an interconnected ecosystem covering the entire lineage of scientific research. Furthermore, to transform these functional modules into cohesive and governed ecosystem, we introduce two critical infrastructural innovations. First, we propose the Omni Scientific Protocol (OSP), standardized collaboration backbone designed to orchestrate the complex interactions between multiple AI agents and human researchers. Rather than treating humans as passive observers, OSP allows researchers to seamlessly participate in the execution and collaboration processes, providing timely feedback, strategic suggestions, or course corrections whenever the system requires high-level human intuition. To maintain scientific integrity, OSP further incorporates granular contribution tracking system. This mechanism records the provenance of every idea, dataset, and experimental result, attributing credit to specific agents or human participants, thereby establishing 1http://omniscientist.ai:31300/ Figure 1: Overview of Our OmniScientist System transparent model of authorship and accountability akin to the contributor roles in modern science. Second, addressing the persistent challenge of evaluating open-ended scientific discovery, we develop ScienceArena2, an open benchmarking platform designed to simulate the community-driven nature of scientific validation. Unlike static metrics, ScienceArena employs blind, pairwise voting mechanism where human experts evaluate anonymized research outputs based on scientific rigor and novelty. By aggregating these preferences into dynamic Elo ratings, the platform establishes living leaderboard that reflects evolving community standards, effectively allowing human judgments to actively shape the evolutionary direction of AI scientific agents. Collectively, this work marks paradigm shift from designing isolated research tools to constructing comprehensive scientific ecosystem. By explicitly encoding the infrastructure of human research into the AI workflow, OmniScientist empowers LLM agents to evolve from mere task executors into autonomous participants within this community. Looking ahead, we envision future where AI Scientists autonomously refine their capabilities through continuous evolution within the ecosystem, while collaborating with human researchers to collectively expand the boundaries of knowledge. 2ScienceArena.ai"
        },
        {
            "title": "2 Research Scope",
            "content": "An AI Scientist refers to system that employs artificial intelligence, particularly LLMs, to emulate scientific research activities. Its objective is to perform various stages of scientific investigation with minimal human intervention, including generating novel hypotheses, designing experimental protocols, conducting experiments or simulations, analyzing results, and even drafting research manuscripts. In essence, AI Scientists aim to endow AI with the creativity and reasoning capabilities of human scientists, enabling autonomous or semi-autonomous scientific discovery. AI Scientists represent significant shift in the role of AI, moving from computational tool toward an originator of scientific knowledge. typical AI Scientist system consists of two main components: intelligent planning and automated execution. The planning component is often implemented with LLMs, serving as the systems cognitive core. It excels at extracting knowledge from literature and data, performing reasoning, and planning experimental procedures. The execution component operates as the systems hands and instruments, using code, simulation tools, or robotic platforms to carry out experiments and collect data according to the planned procedures. This design can form closed loop in which the AI Scientist repeatedly executes the core cycle of the scientific method: generating hypotheses, conducting experiments, measuring outcomes, and refining the hypotheses. This loop allows the system to continuously learn and improve from feedback, analogous to human scientists iteratively testing and refining their theories. In recent years, multiple systems and platforms worldwide have explored the AI Scientist vision, with technical approaches converging into three main directions. These directions collectively reveal the evolution of AI research from automated execution to method-driven reasoning and reflect differing perspectives on whether AI can serve as an independent agent of scientific discovery. The first direction emphasizes fully automated workflows. Representative systems include the AI Scientist [1] developed by Sakana AI in Japan and Westlake Universitys DeepScientist [8]. Sakana AIs AI Scientist is among the earliest systems to claim fully automated scientific discovery. The system uses LLM as its central control unit, orchestrating tasks across different stages through predefined templates. In 2025, its AI Scientist version 2 [9] achieved notable milestone: three papers generated autonomously by the AI were submitted to an ICLR Workshop, and one of these was even accepted. DeepScientist advances the discovery-driven research paradigm by formalizing scientific discovery as Bayesian optimization problem, enabling the AI to refine its actions through multilevel experimental loops. The system has achieved SOTA performance in several AI-related tasks, demonstrating the feasibility of machine-led scientific research, although its applications remain largely confined to computational and simulated environments rather than experimental natural sciences. Overall, these systems demonstrate that closed-loop AI research process is achievable, yet they operate largely under constrained and predefined task context. The second direction involves human-AI collaborative paradigms, exemplified by DeepMinds AI Co-Scientist [10]. Unlike fully automated systems, this approach emphasizes complementary collaboration between AI and human scientists. The system employs multiple dedicated agents, each fulfilling distinct cognitive role in the research process, such as hypothesis generation, critical evaluation, ranking, and evolutionary optimization. Through mechanisms such as Elo scoring and iterative feedback, the agents collectively form socially structured scientific reasoning process, simulating interactions in real research teams. Experiments show that this group intelligence can propose testable hypotheses in complex domains such as biomedical research, some of which have been published in leading journals. The work demonstrates the potential of human-AI collaboration to enhance discovery. The third direction encompasses knowledge-augmented research platforms, represented by FutureHouses AI research ecosystem [6] and DPTechnologys Bohrium platform [11]. FutureHouse emphasizes open interfaces and modular agent architecture, including agents such as Crow for literature assistance, Falcon for review synthesis, Owl for novelty assessment, and Phoenix for experimental planning. These agents collaborate in pipeline to help researchers identify key knowledge, detect innovation opportunities, and generate executable experimental plans from extensive literature. The platform integrates open databases and domain-specific corpora, ensuring high transparency and traceability in research knowledge retrieval, analysis, and validation. Bohrium focuses on crossstage integration through the concept of Science Navigator, unifying literature comprehension, 5 computational simulation, and experimental execution into single research operating system. The platform can answer natural language scientific queries, perform simulations, and execute experiments, creating closed loop between information processing and experimental workflows. These systems extend AI capabilities from textual understanding to scientific computation and experimental interfacing, illustrating the potential for AI Scientists to integrate with real research facilities. Current applications are mainly in materials science and chemistry, but the architectural concepts provide reference for future AI research clouds. In summary, existing AI Scientist systems mark crucial transition from static tools to more autonomous research agents. They are increasingly capable of undertaking laborious research workflows across expansive digital and physical environments. However, most current approaches still predominantly formulate scientific discovery as standalone search or optimization problem. They often operate without the support of the foundational infrastructure that sustains human scientific research. Our OmniScientist fills this critical gap by explicitly encoding these underlying mechanisms into the AI workflow. By embedding these structural dimensions, OmniScientist advances the paradigm from merely optimizing research efficiency to constructing scientific ecosystem, enabling AI agents to navigate rigorous scientific norms and evolve from standalone executors into autonomous participants in the scientific discovery."
        },
        {
            "title": "3 Key Designs",
            "content": "3.1 Data Foundation We construct dynamic and comprehensive research knowledge base that not only supports the full spectrum of scientific activities, ranging from literature review to ideation, but also mirrors the collaborative fabric of human scientific ecosystems. First, we incorporate the OpenAlex open-access academic graph, one of the most comprehensive scholarly knowledge networks. The dataset contains approximately 269 million paper metadata records, along with their citation relationships. For each paper, the metadata fields include, but are not limited to: title, abstract, authors, affiliations, publication year, venue, DOI, references, citation counts, keywords, subject area, and open-access status. This structured information provides foundational map of the scholarly landscape, encoding not just academic content but also the web of attribution, influence, and topic lineage critical to scientific collaboration. Second, we integrate the arXiv open-access paper repository, providing approximately 2.6 million PDF full-text documents, covering over 90% of AI-related publications. This resource supports deep semantic reading and content-based reasoning beyond metadata, empowering the system to perform actions similar to those of human researchers conducting comprehensive literature surveys. In addition, to capture actionable scientific outputs and experimental artifacts, we collect 102,679 full-text papers from the top ten AI conferences over the past decade, along with 116,970 referenced baseline models and 68,316 related datasets. Each entry encapsulates not only the textual description of methods and results but also implementation resources, datasets, hyperparameters, and evaluation metrics. This structure enables the system to trace scientific workflows, reproduce experiments, and explore methodological innovations. The knowledge base is organized as directed, labeled graph comprising four core node types: Paper, Author, Concept, and Resource (datasets, models, tools). The semantic structure is captured through edges such as CITES (Paper to Paper), WRITTEN_BY (Paper to Author), USES (Paper to Resource), and CENTERS_ON (Paper to Concept). As shown in Figure 2 (right), this graph schema represents not only data but also the underlying structure of scientific progress. To further model the interpretive layer of scientific discourse, we attach citation_contexts to CITES edges, preserving the textual rationale behind citations. This allows the system to move beyond structural links to reason over authors intents and comparative judgments. However, scientific knowledge is not static. Like human ecosystems that rely on editorial boards, peer review, and collaborative curation, our data foundation must remain dynamic, self-improving, and structurally coherent. We therefore deploy multi-agent refinement pipeline (Figure 2, left) that continuously diagnoses, enriches, and validates the graph. 6 Figure 2: The Multi-Agent Refinement Pipeline (left) and the Refined Data Structure (right). Metric Metadata Completeness Score Metadata Correctness Score Retrieval Accuracy (QA Benchmark) 1.000 0.997 0.880 Table 1: Performance Metrics of the Multi-Agent Refinement Pipeline 0.965 0.951 0. Original OpenAlex Refined Database Diagnose Agent: This agent initiates the cycle by auditing the current state of the knowledge graph for quality issues. It prioritizes these issues and formulates refinement tasks, effectively creating to-do list of data curation actions. Search Agent: Once issues are identified, the Search Agent queries external scholarly databases and APIs and infers the right answer. It also parse full-text papers to extract hidden semantic metadata: e.g., detecting if paper mentions using particular dataset or codebase, or if it cite another work positively or negatively. Normalization Agent: The agent standardizes the result of the Search Agent to ensure that identical entities are not duplicated under different names (e.g., ImageNet dataset vs. Image Net). Coding Agent: The Coding Agent takes the normalized information and integrates it into the knowledge graph. It acts as the database editor, merging duplicate entities and inserting new nodes or relations in alignment with the graph schema. ReviewAgent: The final agent in the pipeline acts as quality control and validation layer. The Review Agent evaluates the modifications made to the knowledge graph in the current iteration, checking for accuracy and coherence. If certain new edges are found to be erroneous or low-confidence, the Review Agent can remove them and store them for human review. Performance Metrics. We conducted small-batch evaluation (n = 1000) to assess improvements in structure and retrieval. Metadata completeness increased from 0.965 to 1.000, and correctness from 0.951 to 0.997. On benchmark of 100 QA pairs probing inter-paper relationships, retrieval accuracy improved from 0.70 to 0.88. These gains reflect the systems enhanced ability to surface relational evidence critical for knowledge reuse and synthesis. 7 Figure 3: Case Study: Semantic Relation Capture Case Study: Semantic Relation Capture. Figure 3 illustrates how the refined scientific network uncovers subtle, author-intended conceptual links that traditional metadata or keyword searches cannot reveal. In this example, Paper cites Paper and Paper through two distinct CITES relations. Viewed independently, and appear unrelated: they address different topics, share no metadata, and do not cite each other. Under conventional keyword retrieval, they remain in separate semantic regions. The refined KG exposes deeper alignment. The citation_context from to emphasizes that Inceptions multi-branch structure provides lightweight, skip-style pathways that bypass parts of the computation. The citation_context from to highlights that Highway Networks use learnable gates to regulate information flow through shortcut routes. Considered together, these contexts show that both works investigate mechanisms for routing information across layers to mitigate degradation in deep models; one achieves this through fixed architectural branches, while the other employs parameterized gating. They also reveal methodological contrast: Paper treats shortcut pathways as architectural design, whereas Paper frames them as trainable control mechanisms. By representing citation contexts as structured relational properties, the refined knowledge graph makes this hidden conceptual bridge explicit, significantly enhancing the granularity of the literature review agent. Together, the structured knowledge base and refinement pipeline form dynamic substrate for agent collaboration, scientific inference, and cross-agent memory. By mirroring the protocols and epistemic structures of human scientific research, our data foundation sets the stage for an AI research ecosystem capable of cumulative innovation and sustained interaction with the human scientific community. 3.2 Literature Review Current general-purpose literature review products such as OpenAI DeepResearch exhibit several limitations when applied to scientific research contexts. First, they suffer from low-quality information sources. The open web contains heterogeneous and noisy information with limited factual verification; such content may include erroneous, misleading, or overly speculative claims that are incompatible with the rigor, accuracy, and norms required for scientific inquiry. Second, their retrieval depth is shallow. These systems predominantly rely on keyword-based or embedding-based semantic search, which is insufficient for conducting complex scientific literature retrieval. Third, they demonstrate limited scientific rigor and weak reasoning discipline. As these products were not originally designed 8 Figure 4: Deep Research Framework Diagram for research workflows, their summaries and inferences often fail to meet the standards of precision, structure, and methodological soundness expected in scholarly practice. To address these issues, OmniScientist introduces semantically grounded and rigor-oriented automated literature review pipeline specifically designed for scientific research. To mitigate the problem of low-quality information sources, we construct local scientific paper database together with curated scientific networks (as detailed in Section 3.1), ensuring that the review relies only on verified, peer-reviewed research outputs. To improve the precision and completeness of retrieval, we build an Elasticsearch service on top of the local database, enabling multi-field querying across titles, abstracts, author metadata, and other structured fields. More importantly, we leverage the constructed scientific network to further enhance both the breadth and depth of retrieval. Specifically, the initial set of candidate papers is obtained from the embedding-based Elasticsearch search. Papers linked through citation and reference relationships in the scientific network are then added to the candidate pool. relevance verification mechanism is applied to filter this pool, and the retained papers are further expanded layer by layer along their citation and reference links. This process goes beyond keyword matching to emulate how human researchers trace the genealogy of ideas, effectively implementing breadth-first search (BFS) that navigates the structural infrastructure of the field until predefined retrieval depth or coverage is reached. This network-augmented retrieval process is implemented as flexible tool, allowing balance to be struck between retrieval depth and efficiency. Starting from user-defined research query, the literature review system executes retrieval, filtering, parsing, and synthesis through coordinated multi-agent orchestration. The detailed workflow is as follows: Research Plan Generation. Upon receiving research topic or problem statement, the Planning Agent synthesizes an initial research plan, identifying research objectives, decomposed sub-questions, expected methodological directions, and key conceptual dimensions. These elements serve as early semantic anchors, grounding the entire pipeline and ensuring that subsequent retrieval is guided by structured intent. Keyword Extraction and Literature Retrieval. Based on the research plan, the Literature Agent extracts domain-specific concepts and generates an expanded keyword set consisting of explicit terms, latent concepts, and semantically related expressions derived through LLMbased inference. These terms are compiled into retrieval templates used by the ElasticSearch. Retrieval operates across multiple indexed paper fields with field-specific weighting: titles and abstracts receive higher relevance emphasis, while body text and authorship metadata broaden semantic coverage. This multi-field semantic retrieval mechanism significantly increases recall and precision. Additionally, the Literature Agent can optionally invoke the scientific network retrieval tool to expand the candidate set based on citation and reference relationships within the scientific network. 9 Relevance Ranking and Filtering. Retrieved documents are further evaluated by the Literature Agent using multi-dimensional relevance and quality scoring mechanism. In addition to semantic relevance that captures topical alignment, methodological similarity, task-structure proximity, and latent domain adjacency, the agent also considers the scholarly quality of each paper. The evaluation incorporates factors such as citation impact, venue prestige, and empirical rigor. By integrating both conceptual relevance and scientific influence, the system prioritizes literature that is highly relevant and reliably impactful, ensuring that only strong and valuable papers progress to the next stage. PDF Parsing and Key Point Extraction. Each retained paper undergoes structured PDF parsing to identify section headers, abstracts, methodological components and experimental results. The LLM extracts the papers core contributions, innovations and empirical findings, refining these outputs through iterative self-verification. Cross-paper reasoning identifies conceptual variants, methodological connections, and experimental discrepancies, enabling more integrated understanding of the literature landscape. Draft Construction via Sketchboard Writing. Extracted insights are progressively incorporated into the Sketchboard, structured drafting workspace managed by the Writing Agent. Through iterative refinement cycles, the agent develops coherent paragraphs, maintains narrative consistency, and enforces stylistic alignment across sections. The process transforms preliminary notes into complete and analytically grounded literature review in Markdown format. This workflow is supported by decoupled multi-agent architecture composed of Planning Agent, Literature Agent, and Writing Agent, coordinated by top-level Agent Orchestrator responsible for dependency management, task scheduling, and global quality control. The modularity of this design allows for seamless integration of future specialized agents, such as mathematical reasoning or experimental analysis modules. To exemplify the practical application value of the scientific network retrieval tool, we conducted case study. This experiment, detailed in Figure 5, evaluates the quality of surveys generated by four distinct methods: (1) Base LLM (gpt-5) generating survey from the query alone, without external retrieval; (2) WebSearch enabled gpt-5, representative of the general-purpose systems we critiqued; (3) the Deep Research system, state-of-the-art commercial agent (o4 mini DeepResearch) ; and (4) Tool-Augmented gpt-5, where the base LLM was equipped with the retrieval results from the scientific network tool. The generated surveys were evaluated by Gemini-2.5-pro across five dimensions: Relevance, Completeness, Depth, Logical Consistency, and Usefulness, with each dimension scored on 110 scale. Figure 5: Evaluation of Survey Quality Across Multiple Dimensions for Different Models The results compellingly demonstrate that retrieval quality is the primary determinant of survey quality. The Tool-Augmented model, representing perfect, relation-aware retrieval, achieves the highest scores across all dimensions, confirming this clear upper bound. The Deep Research system, while the strongest baseline, substantially outperforms the Base and WebSearch approaches, particularly in Completeness, Depth, and Logic. Moreover, we observe substantial performance gap between the Deep Research system and the Tool-Augmented model. This gap reinforces our central claim: even the most advanced commercial systems, which do not incorporate relation-aware retrieval, remain unable to reconstruct the full set of logically connected publications underlying scientific topic. Their failure to capture these structural relations leads to surveys that are noticeably less complete and less coherent. By contrast, the scientific-network-augmented retrieval pipeline in our OmniScientist system is specifically designed to mitigate this deficiency by identifying more comprehensive and structurally grounded set of relevant papers."
        },
        {
            "title": "3.3 Research Ideation",
            "content": "The generation of novel research ideas plays critical role in advancing scientific research. While recent advancements in LLMs have shown potential for this task, previous work in research ideation has suffered from several key limitations. Primarily, existing approaches have relied on simplistic methods, such as keyword co-occurrence or semantic similarity [12, 13]. These techniques typically focus on identifying statistical associations within the literature or static concept representations, thereby overlooking the complex, contextual, and co-occurrence-based relationships that researchers construct. Some LLM-driven methods, on the other hand, propose and iteratively refine research ideas by leveraging relevant literature retrieved through techniques like semantic similarity [14]. However, these methods fail to fully utilize the valuable network of scientific concepts, which reveals the nuanced relationships among literature arising from shared concepts. While some works do attempt to leverage this concept network [15, 16], they are typically limited to retrieving only first-order neighbors directly related to the query. This narrow focus neglects deeper, higher-order relationships and more specific connections between concepts, which are essential for generating more comprehensive insights. In order to enable LLMs to effectively leverage the valuable network of scientific concepts, integrating their internal knowledge with human research achievements, we propose Deep Ideation framework. Within this framework, the LLM iteratively queries the scientific network through an explore-expandevolve workflow, dynamically acquiring human knowledge embedded within the network. In parallel, the Idea Stack tracks the progression of ideas, offering an overarching perspective on the evolving research process, much like how human researchers refine their ideas over time through accumulated insights. The generated ideas are continuously refined through review feedback that aligns with the level of human expertise, ultimately resulting in high-quality idea proposal. The overall process is illustrated in Figure 6. Figure 6: Overview of our Deep Ideation framework. In this figure, we set the maximum size of the keyword set to 4. Key Components of Deep Ideation Framework There are four key components in the Deep Ideation Framework: the Scientific Network, the Relation Analysis Module, the Keyword Selection Module, and the Idea Formulation Module. The Scientific Network is constructed based on the co-occurrence relationship of concepts in the literature. Here, we define concepts as the keywords found in the literature. Initially, the literatures title, abstract, and introduction are input into the LLM, and we prompt the LLM to extract relevant keywords from these sections. These keywords are then treated as nodes in the network. Subsequently, co-occurring keywords within the same literature are connected by edges. 11 Relation Analysis Module is responsible for summarizing how co-occurring literature, through the process of human authorship, construct connections between keywords. Specifically, it analyzes the relationships between keywords and their neighboring terms as established in the literature, capturing the way these terms are linked in the context of scientific research. Keyword Selection Module plays crucial role in steering the ideation process by selecting the most significant and impactful keywords to expand the initial set. Beyond merely refining the keyword collection, this module actively shapes the direction of the evolving idea, ensuring that it remains focused on the most promising avenues for both novelty and feasibility. Idea Formulation Module addresses key gap in many existing approaches [12], which often focus solely on keyword combinations without providing complete, structured idea proposal. This module plays critical role in synthesizing the selected keywords into coherent and scientifically grounded idea proposal, transforming set of keywords into fully formed concept. Explore-Expand-Evolve Workflow Explore: The process begins with an initial set of keywords K0 = {k1, k2, . . . , kn}, which are refined by identifying and analyzing their neighboring terms within the scientific network. To obtain the neighboring keywords, we define (K0) as the set of neighboring keywords for all ki K0. Since the number of neighbors for each keyword may be large, we limit the selection to the neighboring terms, where is predefined maximum number. This gives us set of neighboring keywords for each ki: (K0) = {N (k1), (k2), . . . , (kn)} Each (ki) is limited to the neighbors. The Relation Analysis Module then analyzes the relationships between each pair of selected keywords (ki, kj) and their common co-occurrence across multiple papers. Given that multiple papers can share co-occurring keywords, the relationship R(ki, kj) between two keywords is derived by considering all the papers where both ki and kj appear, represented by P(ki, kj): R(ki, kj) = g(ki, kj, P(ki, kj)) where P(ki, kj) = {p1, p2, . . . , pt} represents the set of papers that both ki and kj co-occur in, and is function that aggregate the relationship together. Expand: Following the exploration and relation analysis, the Keyword Selection Module is tasked with selecting the most significant keyword knew to add to the current set Kt, where knew (K0). The selection is based on comprehensive analysis of the relationship between the new keyword and the existing set of keywords. This new keyword is chosen by evaluating the relationships R(knew, ki) for each ki Kt, where the relationship between the newly selected keyword and an existing keyword is considered: R(knew, ki) = g(knew, ki, P(knew, ki)) The Keyword Selection Module outputs the selected keyword knew, the reason for the selection (based on its relationship to the current keyword set), and its connection to the existing keyword. The selected keyword is then added to the current keyword set: Kt+1 = Kt {knew} Subsequently, this updated set of keywords Kt+1 and the Idea Stack, which contains all previous research iterations (including keyword sets and idea proposals), are input into the Idea Formulation Module. The Idea Formulation Module synthesizes the selected keywords into coherent idea proposal, which includes the research background, research idea, and general implementation approach. The idea proposal at time is generated as: 12 Pt = LLM (Kt+1, prompt) where the prompt represent the prompt template for idea formulation module. The Idea Stack records each rounds progress, tracking keyword evolution, idea development, and evaluations, thus mirroring the iterative nature of human research. Evolve: The Evolve Mechanism triggers when the keyword set reaches predefined length Lmax. At this point, the focus shifts to evolving the keyword set or the idea proposal. The Router determines whether the focus should be on refining the keyword set or on adjusting the idea proposal. The Router decision is formalized as: Next Action = (cid:26)Keywords Evolve"
        },
        {
            "title": "Idea Proposal Evolve",
            "content": "if Router == Evolve(Kt) if Router == Evolve(Pt) During the evolution phase, the keywords in Kt are dynamically replaced based on insights from previous iterations. This evolution is represented by: Kt+1 = (Kt {kold}) {knew} or Pt+1 = LLM (Kt+1, prompt) The idea proposal is refined by incorporating new findings and emerging research trends, while the keyword set is updated iteratively to adapt to the evolving research context. This ensures that the generated ideas continue to evolve, progressively becoming more novel and feasible. Critic Model The Critic Model drives the iterative refinement of ideas in the Deep Ideation framework by providing expert-level evaluative feedback on generated proposals. This feedback loop ensures continuous improvement by aligning with domain-specific evaluation standards. Although LLMs can be used for reviews, they lack the nuanced, expert-level reasoning required for deep evaluation. To address this, we developed Scientific Reasoning Simulation prompt that enables the LLM to mimic the cognitive process of human reviewers, assessing novelty and feasibility based on existing research. This simulated reasoning is used to fine-tune the LLM, aligning its feedback with expert review standards. Overall, we presented the Deep Ideation framework, which integrates LLMs with scientific networks to generate novel and scientifically grounded research ideas. By leveraging the relationships between keywords in scientific literature, our method ensures that generated ideas are both innovative and anchored in existing knowledge. The iterative workflow, enhanced by the Idea Stack, enables continuous idea refinement, mirroring the cognitive process of human researchers. Additionally, the critic model, trained on real-world feedback, provides critical evaluative input to ensure that the ideas are novel and feasible. 3.4 Experiment Automation In the typical scientific research workflow, validating novel idea necessitates rigorous experimentation against appropriate datasets and comparison with relevant baseline methods. While recent automated science platforms like FunSearch [17] and AlphaEvolve [18] have excelled at optimizing solutions for specific problems, they often overlook this critical preceding step of resource selection. The few existing works dedicated to recommending datasets or baselines, such as DataFinder [19] and DataHunter [20], suffer from several key limitations. First, they typically rely solely on selfdescriptions (e.g., the papers abstract) for representation. However, the true academic positioning of baseline or dataset is defined by both its self-description and, crucially, how other papers cite and describe it (its citation context). Second, these methods generally focus on recommending either baselines or datasets in isolation, ignoring the critical synergistic relationship between them. To address these gaps, OmniScientist proposes novel two-stage framework for joint baseline and dataset recommendation. First, we construct comprehensive representation for both baselines and datasets by extracting and combining their self-descriptions with their broader citation contexts. We then fine-tune an embedding model on this rich representation to perform coarse recall. To achieve 13 Figure 7: Illustration of Collective Perception Augmented Retrieval fine-grained reranking, we explicitly model the interactions by extracting \"paper-baseline-paperdataset\" citation chains. These paths are used to construct reasoning chains, upon which we fine-tune Large Language Model (LLM) to generate an explainable final ranking. To implement the coarse recall stage, we construct rich representation for each candidate (baseline or dataset) that moves beyond simple self-description, see Figure 7. We introduce collective perception signal by first extracting all citation contexts for given target from the experimental sections of papers in our corpus. Since the raw contexts can be numerous and noisy, we use large language model to synthesize them into concise summary. We then create the final target representation by concatenating its first-person self-description with this third-person collective perception. We finetune bi-encoder retriever on these concatenated representations using contrastive loss objective, training it to pull query towards its true associated baselines and datasets. In the fine-grained reranking stage, our objective is to leverage the synergistic relationship between baselines and datasets using reasoningaugmented reranker, see Figure 13. For each candidate from the recall stage, we extract interaction chains from our scholarly knowledge graph. For example, to recommend baseline for query paper p, we find paths such as paper (p) dataset (d) paper(p) baseline (b). This chain explicitly connects the query paper to the candidate baseline via shared dataset (d) that was also used by another paper (p). We then finetune large language model as listwise reranker, training it to take the query, the candidate, and its evidential chains as input. The models task is to generate an explicit reasoning chain that justifies the candidates relevance, resulting in final, interpretable, and precise ranking. Figure 8: Construction of Chain-Derived Dataset/Baseline Pool Analysis Once the appropriate datasets and baselines are identified, the system can proceed to the experiment execution phase. To facilitate efficient iterative optimization of the proposed idea, we constructed multi-agent system for automated experimentation. This system comprises four specialized agents: Evolution Agent: Responsible for generating new method code variants based on parent programs. Sample Agent: Constructs contextual prompts for the next iteration based on the evolutionary history and performance metrics. Evaluation Agent: Executes the code and measures various performance metrics. Feedback Agent: Analyzes execution errors or suboptimal performance and translates them into actionable suggestions for improvement. 14 Figure 9: The overall framework of scientific writing 3.5 Scientific Writing Previous works on AI scientists have developed relatively mature pipeline for automated scientific paper writing. Typically, this process begins with either human-defined or LLM-generated paper structure, followed by content generation and final refinement[1, 9, 2, 21]. However, this workflow presents two major limitations. First, it fails to effectively learn from existing literature, resulting in limited ability to emulate the linguistic style and structural conventions specific to given subfield. Second, it places insufficient emphasis on visual contentwhile such systems can often generate data-related figures, they struggle to produce methodological or conceptual diagrams, which are crucial for clearly conveying research ideas. These limitations arise because existing methods do not fully encode the established methodologies of human scientific writing into the AI scientist workflow. To address the aforementioned issues, OmniScientist proposes multi-agent framework that explicitly encodes two critical components of human scientific writingthe deep learning from related literature and the emphasis on high-quality visual communication (figures). As shown in Figure 9, this framework consists of four synergistic agent subsystems: Outline Agent, Figure Agent, Writing Agent, and Refinement Agent. To tackle the problem of insufficient learning from related works, the Outline Agent employs an LLM to summarize the most relevant papers, analyzing their writing strategies and structural patterns to derive data-driven writing summary that informs the overall composition process. To improve the visual quality of figures, the Figure Agent combines image generation models with Python-based script executors to produce methodology diagrams and data visualizations, respectively. Also, Vision-Language Model (VLM) is employed to evaluate and iteratively refine the generated images, ensuring both accuracy and aesthetic coherence. Together, these agents enable OmniScientist to autonomously generate coherent, visually enhanced, and fieldadaptive research papers. The modular design promotes interpretability, extensibility, and domain transferability, making OmniScientist powerful foundation for fully automated scientific writing. The details of each agent are as follows: Outline Agent: The Outline Agent analyzes the most relevant papers to learn their writing styles and structural patterns, generating an optimized writing summary. Based on the research idea, literature review, experiment results, and formatting requirements, it constructs paper outline that defines section titles and details for each section. It also produces 15 (a) Page 1 (b) Page 2 (c) Page 8 (d) Page 10 Figure 10: Case of Scientific Writing 16 high-level guidance specifying tone, style, and figure design preferences to guide the entire writing process. Figure Agent: The Figure Agent uses an LLM to generate figure list from the outline, detailing each figures section, description, required data, and type (method or data). For method figures, it then refines the textual description and invokes an image generation model; for data figures, it then writes and executes Python scripts to produce visualizations. Each figure is then verified by Vision-Language Model (VLM) to ensure accuracy and visual quality. Writing Agent: The Writing Agent generates text section by section, utilizing the outline, generated figures, literature review, experiment results, and research ideas as input. If the author provides bibliography (BIB) file alongside the literature review, the agent will use those existing references; otherwise, it will generate the BIB file based on the content of the literature review. For each section, it first proposes structure and then generates the content as LaTeX-formatted output. The Agent subsequently evaluates the quality of the generated text and provides feedback; if the quality is deemed insufficient, the writing process is repeated. Refinement Agent: The Refinement Agent initiates quality assessment, checking for length, format, and consistency issues, as well as its adherence to the provided guidance. Following this, it applies single, comprehensive revision based on high-level feedback to the entire document. Afterward, it compiles the paper in LaTeX, automatically fixes compilation errors, and employs VLM-based evaluation to assess figure quality and layout aesthetics, yielding polished, publication-ready paper. Overall, OmniScientist provides an intelligent multi-agent framework that successfully completes the automated scientific paper writing task. By integrating literature-informed structuring, highquality figure generation, LLM-driven writing, and VLM-based refinement, it effectively encodes the complexity of the human scientific writing process and ensures the generated output meets standards of coherence and visual clarity. Crucially, this modular architecture facilitates seamless collaboration among specialized agents and promotes interoperability with both human researchers and other AI Scientist components, laying critical foundation for building robust AI research ecosystem. Figure 10 shows some pages of demo case created by our paper writing framework. 3.6 Paper Review Recent advancements in Large Language Models (LLMs) have catalyzed the development of automated scholarly paper review (ASPR) systems[2231], transitioning from general-purpose models to specialized agents. Current state-of-the-art approaches range from finetuned specialist models, such as OpenReviewer[32] and DeepReviewer[33] designed to emulate expert tone, to hierarchical decomposition frameworks like TreeReview[34] that enhance efficiency through structured questioning. Concurrently, sophisticated multi-agent systems, including MAMORX[35] and Agent Reviewers[36], simulate academic review committees with specialized, multimodal roles. Despite this progress, significant gaps remain. Current systems, while incorporating external knowledge retrieval or agent-based discussion, often fail to provide fine-grained, verifiable traceability for their judgments. This creates process that is difficult to inspect and undermines trust. Furthermore, existing iterative mechanisms are typically model-centricemploying reinforcement learning with judge-models or autonomous self-correctionrather than being designed for explicit human-in-theloop (HITL) collaboration. Finally, while multimodal analysis has emerged, it frequently remains high-level, such as the Figure Critic role, and fails to detect subtle, fine-grained inconsistencies. Critically, these systems often ignore the real-world challenge of parsing artifacts, including OCR or Markdown errors, making them less reliable in practice. To address these limitations, we propose Traceable and Interactive Multi-Agent Review (TIMAR) system, based on the detailed design of the multi-agent paper review system presented. The overall system is illustrated in Figure 11. TIMAR implements academic peer review as structured, evidence-driven, five-stage workflow. The process begins with (A) Input Pre-processing, where the manuscript is parsed, and multimodal elements (figures and tables) undergo fine-grained consistency verification (e.g., cell-by-cell checks) to identify and tolerate parsing artifacts. Next, (B) Retrieval Augmentation builds dedicated evidence pool by selecting key references from 17 Figure 11: Overview of TIMAR (Traceable and Interactive Multi-Agent Review) framework. the paper and enabling both broad (Breadth) and targeted (Depth) retrieval from external source. This evidence informs (C) Parallel Review, where three distinct agentsNovelty, Rigor, and Clarityindependently generate initial, evidence-bound assessments. These drafts are then fed into the (D) Merge and Iterative Revision stage, which functions as collaborative nexus. Here, an internal multi-agent debate and external Human-in-the-Loop (HITL) feedback (e.g., preference inputs) are treated as explicit instructions to refine the review. Finally, the (E) Final Report and Annotation stage synthesizes the converged text, automatically annotating key judgments with traceable citations [n] that link directly to the manuscript or the retrieved evidence pool. This architecture moves beyond simple automation to become governed process, generating transparent and robust critiques for human collaboration. It directly tackles the core limitations we identified by delivering: (1)Detailed and auditable fact-checking with full explainability; (2)A novel framework for multi-round Human-in-the-Loop (HITL) interaction; and (3) Reliable, in-depth analysis of multimodal elements, featuring explicit artifact tolerance. Verifiable Traceability and Explainability. The TIMAR framework moves beyond opaque judgments, achieving verifiable traceability through two core mechanisms. First, it applies an evidencedriven discipline to the parallel review agents (Novelty and Rigor). This discipline requires retrieval actions to gather external proof, preventing claims from being assessed in isolation. For example, it requires Breadth Depth retrieval sequence to verify strong \"first-ever\" claims against both broad academic fields and specific prior works.Second, the final output stage generates traceable conclusionevidencecitation chain. The system automatically marks key judgments in the review text (e.g., underlining) and links them with numbered citation ...[n]. This citation links directly to the evidence source, whether it is specific location in the manuscript (e.g., Section 3, Table 2) or retrieved paper from the systems evidence pool.This two-pronged approach makes the agents entire reasoning process transparent, allowing users to trace any conclusion back to its original evidence. Human-in-the-Loop Iterative Refinement. Instead of relying on autonomous self-correction, our system introduces an iterative refinement process explicitly designed for human-AI collaboration. This process is centered on the Merge and Iterative Revision stage, which functions as central hub for collaboration. This stage first merges the parallel reviews and generates an internal Debate. This debate, which produces optimistic and skeptical viewpoints, does not automatically resolve 18 conflicts. Instead, its purpose is to highlight the most important or disputed points for human operator (such as an Area Chair) to resolve. This mechanism simulates the collaborative decision-making process inherent in human editorial committees, keeping the AI agent aligned with evolving community standards. Crucially, the system is designed to accept User Preferences (e.g., questionnaires or structured feedback) as explicit revision instructions. This external guidance allows human to steer the reviews focus, tone, or depth in subsequent rounds. During this refinement loop, the system is prevented from performing new retrieval, ensuring that revisions remain grounded in the existing evidence base and focus on finalizing the review. Robust Multimodal and Artifact Analysis. To ensure high reliability, the framework performs robust analysis of multimodal elements and structural artifacts before the main review begins. This task is handled by the \"Input Pre-processing\" stage, which performs fine-grained verification to build trustworthy input. This stages methods include first-image-then-text evaluation pipeline, establishing visual baseline for figures before assessing their textual descriptions for consistency. It also uses \"cell-by-cell consistency\" verification for tables, which cross-validates the semantic content of tables image against its parsed text representation (e.g., HTML) to catch subtle data inconsistencies. Most importantly, the entire system follows strict artifact tolerance principle. This core rule requires that agents do not penalize paper quality for parsing failures, such as OCR errors, malformed formulas, or table misalignments. This crucial distinction enables the system to reliably operate on real-world documents and differentiate between author error and parser error."
        },
        {
            "title": "4 Building Co-evolution Systems of Human / AI scientists",
            "content": "The preceding sections have detailed the individual functional modules of OmniScientist, ranging from literature review and ideation to automated experimentation. However, functioning in isolation, these components remain fragmented tools rather than coherent intelligence. To transcend this fragmentation and construct unified scientific ecosystem, effective mechanisms are required to orchestrate these diverse capabilities and bridge the gap between human and machine researchers. In this section, we present the architectural backbone that connects these discrete modules. We first introduce the Omni Scientific Protocol (OSP), which acts as the connective tissue for communication, collaboration, and credit attribution. Building on this foundation, we then detail the Closed-loop Multi-Agent System and Human-AI Collaboration Mechanism. To validate the efficacy of the OSP, we present two comprehensive case studies: one demonstrating the systems capability for autonomous discovery in stochastic derivative estimation via the closed-loop workflow, and another highlighting the synergistic potential of human-AI collaboration in solving complex reasoning challenges. 4.1 Protocol For complex and large-scale AI Scientist Ecosystem that integrates numerous functional modules, protocol capable of connecting all components and integrating all capabilities is of paramount importance. The general-purpose Model Context Protocol (MCP) [37] provides interfaces for agentto-tool interactions, while the Agent-to-Agent (A2A) [38] protocol establishes the foundation for inter-agent communication. Together, these provide the core infrastructure for scientific research protocols. Building upon them, the Scientific Context Protocol (SCP) [39] has been proposed to provide standardized research workflow through centralized SCP Hub. This enables efficient coordination between science-oriented applications and external research assets such as laboratory instruments, databases, LLMs, and specialized computational models. However, scientific research is not merely data transmission workflow. It is reasoning-aware coordination process, deeply interwoven with human intuition, collaborative debate, rigorous provenance tracking, and intellectual credit attribution. truly effective protocol must enable diverse agents, tools, data sources, and even human researchers to co-evolve within unified scientific context, thereby supporting the systemic research capabilities envisioned for an AI Scientist Ecosystem. Yet, under this setting, existing protocols reveal four fundamental deficiencies: 1. Human-is-External: Current protocols treat human scientists as users or operators external to the system, rather than as the highest-level cognitive agents within it. Human-AI interactions remain fragmented and non-protocolized, occurring through ad-hoc interruptions, overrides, or external interventions. 19 2. Collaboration-is-Dark: Essential collaborative activities, such as group discussions, peer reviews, and mentorship, take place outside the protocol layer (e.g., via Slack, WeChat, or in-person meetings). Consequently, the most critical phase of consensus formation remains opaque and untraceable, breaking the provenance chain of scientific reasoning at its very origin. 3. Credit-is-Ambiguous: Current protocols focus solely on whether task is completed, ignoring who contributed intellectually. While data provenance can answer where did this data come from? it fails to answer where did this discovery originate? or who deserves credit for this insight?. To address these foundational issues, we propose the Omni Scientific Protocol (OSP), novel protocol framework specifically designed for scientific research scenarios. OSP unifies Human-AI collaboration (H-AI) and intellectual credit attribution, enabling transparent, accountable, and semantically grounded coordination across the entire scientific workflow."
        },
        {
            "title": "4.1.1 From External Users to Internal Participants",
            "content": "OSP fundamentally redefines the role of the human scientist at the protocol level. Instead of being treated as an external operator, the human is positioned as an internal participant: the highest-level decision-making entity within the ecosystem. To enable this, OSP introduces Unified Participant Model. In this model, the protocol no longer distinguishes between AI and human entities. Both are abstracted as common type called Participant. Human scientists (Human_Participant) and AI scientists (AI_Scientist_Participant) hold equal protocol-level status and can both send and receive messages symmetrically within the same communication fabric. This design marks fundamental shift in the nature of humanAI interaction. Communication is no longer hierarchical, where humans issue commands through interfaces, but peer to peer, where AI agents can proactively initiate asynchronous negotiations with human participants. As result, human intuition, judgment, and decision making are no longer opaque operations external to the system. They become integral, auditable, and traceable components within the protocol itself. To support this new form of asynchronous, long-horizon humanAI interaction, we designed set of protocol performatives to replace the simple request and inform actions in traditional protocols. These performatives are specifically designed to capture the critical humanAI negotiation events that occur in scientific research activities: REQUEST_REVIEW(artifact_id, criteria): The AI agent proactively sends this request to one or more Human_Participant entities, asking for review of critical scientific artifact, such as draft survey paper or piece of algorithmic code. Upon sending this request, the agents task state transitions to WAITING_FOR_HUMAN, awaiting expert feedback. REQUEST_DECISION(task_id, options): When an AI agent encounters crucial branching point during exploration (for example, discovering two distinct but potentially feasible synthesis pathways), it can use this performative to request high-level decision from Human_Participant, providing the relevant information (options) in structured format. APPROVE(artifact_id, version_hash): standard approval receipt returned by Human_Participant. This protocol message, including the version hash, is permanently recorded in the provenance chain and serves as formal validation for subsequent steps. REJECT(artifact_id, returned by Human_Participant. The reason field, which can be structured data or natural language, is itself valuable scientific information and can trigger the AI agent to reflect, re-plan, or initiate new round of exploration. reason): standard rejection receipt In this way, human approval or rejection is no longer merely click in user interface, but referable, legally significant protocol event that guarantees full-chain traceability in scientific workflows. 4.1.2 Centralized Hub Enabling Multi-participant Engagement After redefining the notion of participant, OSP introduces centralized Hub as the foundational infrastructure of the protocol to address the challenge of multi-party scientific collaboration. In real-world research, collaboration is rarely one-to-one, linear process. Instead, it inherently exhibits complex many-to-many (M-to-N) structure. single research project (Project) often involves multiple human scientists (e.g., PIs, PhD students, collaborators) and variety of AI agents (e.g., data analysis agents, literature review agents). Existing peer-to-peer or bus-style agent communication protocols lack mechanisms to effectively manage such multi-party scientific teams. To this end, the Hub is not merely message broker; it serves three critical roles: 1. Identity and Project Registry: The Hub manages the unified registration of both Human_Participant (human scientists) and AI_Agent_Participant (AI agents). More importantly, it registers and maintains the definition of each research Project. This design is essential, as it establishes clear participant boundaries and scopes of work for every research activity. Without explicit project demarcation, attribution of contributions would be impossible. 2. Message Exchange and Distribution Center: The Hub functions as the central node for the exchange, routing, and archival of all protocol-level communications. Within OSP, any participant may transmit protocol message (e.g., REQUEST_REVIEW) directly to the Hub. The Hub then routes this message, based on the associated project context, to one or more appropriate recipients. This star-shaped topology transforms conventional communication mesh into scalable and manageable 1 model, dramatically improving both extensibility and robustness. 3. Immutable Process Recorder: Because all human-AI, AI-AI, and even human-human interactions (as the protocol expands) must be mediated through the Hub, it naturally becomes the single source of truth for the entire scientific workflow. It enforces the recording of every critical step, decision, and cognitive action, thereby providing what we term forced auditability, the fundamental technical guarantee for contribution provenance and accountability. In summary, the introduction of the Hub architecture transforms research activities from isolated one-to-one interactions into manageable many-to-many collaboration framework. It is not merely communication backbone but the anchor of identity, project, and process integrity, forming the foundational layer upon which humanAI collaboration and contribution tracing can be built. 4.1.3 From Data Provenance to Contribution Provenance Building upon the collaborative framework unified and managed by the Hub, OSP establishes complete mechanism for the transition from data provenance to contribution provenance. After all, any scientific platform that fails to clearly define intellectual contributions can hardly gain the trust and adoption of real-world researchers. Unlike traditional protocols, OSP does not rely on single, superficial log file to determine contribution. Instead, it leverages the project management corethe Hubto construct and maintain unified Long Scientific Context. Since every activity, discussion, and procedural execution by all participants (both human and AI) must go through the Hub, it can capture and record every aspect of projects lifecycle. This context encompasses not only the final research outputs but, more importantly, all intermediate data and related resources generated throughout the scientific process, such as cited literature, executed experiments, generated logs, charts, written code, and even discussion records among team members. To enable contribution attribution, we define the ScholarlyObject as the fundamental carrier within the protocol. It represents the smallest unit of intellectual value within scientific activity (for example, Hypothesis, CodeBlock, or Artifact) and serves as the container of contribution. The key mechanism that enables provenance tracking is that every ScholarlyObject must carry an immutable ContributionLedger. This ledger is chronological record of intellectual actions, documenting each Participant (human or AI) who performed an action (such as create, refine, propose, or approve) along with the corresponding timestamp. simplified example of ContributionLedger is shown below, illustrating the evolution of hypothesis: 21 \" ContributionLedger \": [ \" participant_id \": \" Human_A_ID ( PhD Student Bieber )\", \" action \": \" PROPOSE_HYPOTHESIS \", \" timestamp \": \"...\" \" participant_id \": \" AI_Reviewer_ID ( Review Agent )\", \" action \": \" refine_statement \", \" timestamp \": \"...\" \" participant_id \": \" Human_B_ID ( Advisor Frank )\", \" action \": \" APPROVE \", \" timestamp \": \"...\" { }, { }, { } ] By binding each ScholarlyObject to its ContributionLedger, OSP establishes an unbroken chain of contribution. When downstream agent is assigned to verify given hypothesis, it is protocolenforced to reference the original object and its complete ledger. This ensures that, regardless of how automated the subsequent research process becomes, the final scientific result (Result) can always be transparently traced back to all contributorssuch as Human_A, AI_Reviewer, and Human_B. 4.2 Closed-loop Multi Agent System Existing work in the AI Scientist domain, such as FunSearch [17] and AlphaEvolve [18], has primarily focused on the deep, evolutionary refinement of algorithms. These systems typically construct an evolutionary framework to iteratively improve target algorithm, effectively framing scientific discovery as standalone search and optimization problem. However, this isolationist paradigm overlooks the extensive collaborative mechanisms and supporting infrastructures, such as related papers, that constitute the foundation of human research. Consequently, existing AI scientists operate as solitary entities that fail to form cohesive scientific ecosystem. By relying heavily on internal, pre-existing knowledge and ignoring the collective intelligence embedded in the scientific community, these systems are susceptible to converging on local optima, missing opportunities for more transformative, knowledge-grounded innovations. To address this gap, we propose Closedloop Multi-Agent System that integrates the specialized capabilities of DeepResearch, Ideation, and Automated Experimentation, see Figure 12. This framework is designed to balance deep algorithmic evolution with broad, knowledge-driven exploration, ensuring that innovation is both grounded in existing science and empirically validated. This integrated system operates as synergistic collective of specialized agents, each managing critical phase of the scientific discovery lifecycle. The workflow commences with the DeepResearch Agent. Given an initial research topic or problem, this agent leverages our curated scientific network and advanced literature review pipeline to conduct comprehensive survey. It synthesizes the current state-of-the-art, identifies established methodologies, and pinpoints unresolved challenges or gaps in the literature. This synthesized knowledge is then passed to the Ideation Agent, which utilizes the Deep Ideation framework to explore the scientific concept network, generating novel research hypotheses that are explicitly grounded in the context of prior work. Once high-potential idea is formulated, the Experiment Agent takes responsibility. As detailed in our Experiment Design module, this agent executes the necessary experiments to validate or invalidate the hypothesis. Crucially, this process is not linear but closed loop. The empirical results, error logs, and performance data generated by the Experiment Agent are fed back into the system. This new information serves as critical input for the Ideation Agent to refine or discard the hypothesis, or for the DeepResearch Agent to initiate new, more targeted literature search to understand anomalous results. This iterative cycle of research, ideation, and experimentation allows the system to autonomously navigate the scientific landscape, progressively refining its understanding and converging on genuinely novel discoveries. Figure 12: Overview of Closed-loop Multi Agent System"
        },
        {
            "title": "4.3 Case Study: Variance Reduction in STDE via Closed-Loop Experiment",
            "content": "To demonstrate the practical efficacy and distinct advantages of our closed-loop multi-agent system, we conducted case study targeting the improvement of highly influential, state-of-the-art method: the Stochastic Taylor Derivative Estimator (STDE) [40]. This method, recognized as best paper at NIPS 2024, provides an efficient amortization technique for arbitrary differential operators. While powerful, the accuracy of STDE is fundamentally reliant on standard Monte Carlo (MC) sampling to estimate complex expectations. The well-known limitation of MC sampling is its probabilistic convergence rate of O(1/ ), which can introduce significant variance and thus limit the precision of the solution, especially in high-dimensional settings. Our objective was to leverage our AI Scientist frameworks to discover and implement principled modification that significantly reduces this estimation error. We first deployed AlphaEvolve, which represents the paradigm of internal algorithmic evolution. It diligently explored variations of the existing STDE architecture, focusing on optimizing hyperparameters, testing different neural network architectures for the score function, and experimenting with alternative optimizers. In parallel, we deployed our OmniScientist system. Its DeepResearch Agent began by conducting broad literature survey on \"variance reduction,\" \"stochastic derivative estimation,\" and \"high-dimensional integration.\" The system identified powerful, long-standing field of research, Quasi-Monte Carlo (QMC) methods, that was not utilized in the original STDE paper. The Ideation Agent hypothesized that the core MC sampler in STDE could be directly replaced with QMC sampler. QMC methods utilize low-discrepancy sequences that cover the sample space more uniformly than the pseudorandom numbers of standard MC. This deterministic design leads to superior theoretical convergence rate, often approaching O(log(N )k/N ), which translates directly to lower estimation error for the same computational budget. Figure 13: Code of Quasi-Monte Carlo Generated by OmniScientist experimental The results, summarized in Table 2 for AllenCahnTwobody the equation, this validate knowledge-driven approach. Table 2: Error Comparison for AllenCahnTwobody equation Method 100 1000 10000 100000 STDE Alphaevolve Omniscientist 0.008730 0.007859 0.006780 0.002620 0.001654 0.000579 0.003440 0.002059 0.000572 0.002500 0.003041 0.001210 23 The AlphaEvolve variant, despite its extensive search, yielded only marginal improvement over the STDE baseline. It successfully fine-tuned the existing method but remained confined within its original conceptual boundaries, thus failing to address the fundamental bottleneck In stark contrast, the OmniScientist-proposed solution, which introduced the of MC variance. external concept of Quasi-Monte Carlo sampling, achieved dramatic and consistent reduction in solution error across all tested dimensions. This case study demonstrates the superior capability of OmniScientist to achieve significant scientific breakthroughs by actively seeking, integrating, and applying external knowledge from the broader scientific literature."
        },
        {
            "title": "4.4 Human-AI Collaboration",
            "content": "Human expertise remains indispensable in the scientific discovery process, even as AI scientist systems continue to advance. Human scientists possess domain knowledge, methodological intuition, and practical experience that allow them to recognize blind spots, detect conceptual drift, and identify low-value or erroneous reasoning patterns that AI systems may overlook. Incorporating human insights therefore not only compensates for the limitations of automated reasoning but also stimulates new ideas and alternative perspectives, ultimately enhancing the scientific workflow. Existing AI scientist ecosystems provide only limited support for such collaboration. In systems such as CRISPR-GPT [41], human feedback is allowed through lightweight user-proxy that accepts natural language instructions, yet the underlying interaction protocol remains coarse and under-specified. Other systems, such as Virtual Lab [42], introduce more explicit points of human involvement, including goal setting and review of intermediate outcomes. However, these workflows largely position humans as external supervisors rather than integrated team members, and their rigidly predefined interaction patterns restrict flexibility during complex scientific exploration. Building on the unified protocol described in Section 4.1, we propose more systematic and adaptable approach to human-AI collaboration. Our design seeks to support fine-grained human intervention while preserving the autonomy and exploratory capabilities of AI scientists. Specifically, our system accommodates the following key scenarios: Multi-human participation. Many existing systems assume single human paired with multi-agent AI team, which limits collaborative potential. In real scientific practice, however, creative progress often emerges from the complementary expertise of multiple human researchers. Our system explicitly supports multi-human interaction, enabling experts from different fields to contribute diverse perspectives and engage collaboratively with the AI team. Human validation of critical outputs. When the AI scientist produces consequential artifacts such as draft papers, algorithmic designs, or experimental conclusions, it can request human review. Such feedback is treated not as simple annotation but as part of the scientific record that shapes subsequent planning, refinement, or redirection. Human involvement in decision points. During exploration, the AI scientist may encounter branching pathways or ambiguous solution strategies. In these cases, it can query human experts for guidance. Human judgments rooted in experience, intuition, or theoretical understanding help steer the system toward meaningful and scientifically coherent trajectories. Formal approval and rejection. Human scientists may explicitly approve or reject AIgenerated outcomes. These decisions carry procedural weight: rejection triggers structured reflection, re-planning, or re-execution, and both the decision and its rationale are recorded in the research context to ensure transparency and reproducibility. Through these mechanisms, our system supports rich spectrum of collaboration patterns ranging from low-level iterative discussions to high-level review and strategic decision-making. The result is flexible, protocol-driven framework that integrates human expertise with autonomous AI reasoning, better aligning AI scientist ecosystems with the complex, dynamic, and inherently collaborative nature of real scientific research."
        },
        {
            "title": "4.5 Case Study: HLE Challenge via Human-AI Collaboration",
            "content": "To investigate the potential of human-AI collaboration in solving complex scientific research problems, we designed and conducted delicate case study based on the Humanitys Last Exam (HLE)[43]. Specifically, we constructed three controlled experimental conditions corresponding to three tasksolving modes: (i) AI Solo Mode, in which the model solves the questions independently, (ii) Human Solo Mode, in which human participants solve questions on their own, and (iii) Human-AI Collaboration Mode, in which humans and an LLM jointly solve questions through structured interaction. The Human-AI Collaboration Mode was designed as an interactive setting inspired by Tree of Thoughts (ToT)[44]. Participants engaged in multi-round interactions with the model. In each round, the LLM generated three reasoning paths or intermediate results for the participant to evaluate. Participants could choose among these paths or provide feedback to guide further refinement. The model then produced three new reasoning paths in the next round. This process continued until the participant submitted the final answer. The system automatically recorded the entire interaction process, response time, and answer correctness. The study recruited 10 PhD-level participants. Each participant completed 10 HLE questions spanning computer science and artificial intelligence. For every individual, five questions were completed in Human Solo Mode and five in Human-AI Collaboration Mode. Question assignment followed cyclic matrix design, ensuring that each question was answered by five participants in the Solo condition and by another five in the Collaboration condition. This allocation guaranteed balance from both the question and participant perspectives and enabled systematic comparison between individual and collaborative performance. The 10 HLE questions have been carefully selected from the computer science/AI category, ensuring that they cover various sub-fields such as machine learning, spatial recognition, database and query processing, algorithms and data structures, etc. in order to fully assess the reasoning potential of the three modes across different domains. For Human-AI Collaboration Mode and AI Solo Mode, we choose GPT-5 [45] as the LLM to process questions independently or collaboratively with human participants. The experimental platform provided unified frontend interface through which human participants interacted with the system. All interaction data and results were automatically recorded and stored as timestamped JSON files for subsequent analysis and evaluation. This setup enabled fine-grained control over the collaborative process and offered structured data foundation for examining the dynamic mechanisms of synergic human-AI collaboration in complex tasks. Above in the picture 14 are the result of our case study. The results demonstrate that human-AI collaboration significantly outperforms solo human efforts, which highlights the effectiveness of integrating human expertise with AI capabilities, particularly in complex scientific tasks. The collaborative modes higher accuracy suggests that iterative human-AI interactions, as implemented in the study, effectively leverage the strengths of both parties. In contrast, the LLM modes consistent accuracy of 0.0 implies the limitations of AI when operating independently, emphasizing the need for human guidance to enhance AI performance in nuanced scientific contexts. Figure 14: HLE average accuracy across modes Taking Examples from the Case Study To further explain the potential of human-AI collaboration, we thoroughly review two of the questions as examples in the HLE case study. The first example in both Human-AI Collaboration Mode and AI Solo Mode has been presented in 15, 16 and 17. In AI Solo Mode of this example, the LLM outputs wrong answer because of reasoning mistakes and then stop reasoning. But when it comes to Human-AI Collaboration Mode, in the first turn, the three diverse reasoning paths and process represents three different stages of the reasoning process towards the final answer of the question. And the request of verification from human review directs the second-turn collaboration to further enhance the validity of the answer. 25 Figure 15: Case study: No.1 Example of Human-AI Collaboration Mode (Part I) The second example in both Human-AI Collaboration Mode and AI Solo Mode has been presented in 18, 19 and 20. In AI Solo Mode of this example, the LLM generates false answer because of reasoning mistakes. But when it comes to Human-AI Collaboration Mode, in the iterative process, the human feedback always help LLM choose the best reasoning path out of three possible ideas to solve the question, which eventually leads to correct answer. These two examples from the case study implies that the Human-AI Collaboration Mode ensures that not only could LLM fully utilize its reasoning ability by generating multiple diverse reasoning paths or intermediate results but also the human review inserts scientific insight and verification to assure the accuracy of both the answer and the reasoning process. Besides, the validity and efficiency of process is guaranteed by the flexibility of human feedback and the mastery and understanding of the whole context from human participants. Overall, Human-AI Collaboration is proved to be Figure 16: Case study: No.1 Example of Human-AI Collaboration Mode (Part II) Figure 17: Case study: No.1 Example of AI Solo Mode 27 Figure 18: Case study: No.2 Example of Human-AI Collaboration Mode (Part I) crucial to enhance the ability and efficiency by inserting human insight into the context, which will significantly empower the AI scientist ecosystem. Figure 19: Case study: No.2 Example of Human-AI Collaboration Mode (Part II) Figure 20: Case study: No.2 Example of AI Solo Mode"
        },
        {
            "title": "5 Evaluation through ScienceArena",
            "content": "In the previous sections, we established the architectural foundation for co-evolutionary scientific ecosystem. However, robust ecosystem requires not only creation mechanisms but also rigorous evaluation infrastructure to distinguish genuine discovery from hallucination and to guide the evolutionary direction of AI agents. To this end, we introduce ScienceArena, an open evaluation platform designed to mirror the peer-review and feedback dynamics of the scientific community, serving as the final critical piece that completes our infrastructural framework. By leveraging crowd-sourced expert comparisons and dynamic Elo ratings, ScienceArena functions as living testbed, ensuring that AI Scientists are evaluated against the evolving standards of human scientific consensus, thereby guiding their continuous evolution."
        },
        {
            "title": "5.1 Motivation",
            "content": "Evaluating the research capabilities of AI Scientist systems in open-ended environments remains highly challenging, primarily due to the absence of unified, adaptive, and extensible evaluation framework capable of systematically assessing reasoning, creativity, and methodological competence. Existing benchmarks are mostly constrained to static tasks or fixed datasets. For example, DeepResearch Bench [46] consists of 100 PhD-level research tasks designed by domain experts across 22 fields (including science, technology, and finance) to assess the functional capabilities of deep research models. Similarly, IdeaBench [47] constructs benchmark based on 2,374 target papers in biomedical research and their 29,408 cited references, challenging models to generate research ideas grounded in the cited literature that match or surpass the novelty of the target publications. Although these static benchmarks are carefully designed, they do not align well with users actual experiences and needs in real scientific workflows. For instance, users typically do not have access to all relevant references when they have not yet formed concrete research idea. As result, such benchmarks fail to capture model performance in realistic, multi-stage scientific reasoning scenarios. Moreover, current evaluation methodologies still rely heavily on LLM-as-a-Judge. For instance, DeepResearch Bench [46] uses Gemini-2.5-Pro for scoring, while IdeaBench [47] depends on GPT4o. However, in complex scientific research contexts, judgments made by LLMs often diverge from those of human users [48, 49]. To address these two limitations, we draw inspiration from both the LMArena platform [50] and the human peer-review process to develop ScienceArena.ai3, benchmarking platform specifically designed for automated scientific research systems. Following the principles of LMArena, we abandon static evaluation questions and instead delegate the creation of evaluation queries to broad user base. Human users dynamically submit authentic research questions, and model outputs are evaluated through anonymous, pairwise comparisons. We further incorporate an Elo-based dynamic leaderboard to clearly reveal the capability ranking across different domains. In addition, we draw inspiration from the peer-review process, which relies on expert judgment. Accordingly, we invite domain experts: PhD students and faculty members, to participate in large-scale voting, enabling rigorous characterization of the capability boundaries of different AI Scientist systems. Track Organization: Considering the capabilities of existing AI Scientists platforms and the generality across research domains, we defined six tracks: literature review, ideation, hypothesis generation, reviewer, paperQA, and authorQA. Domain-specific functionalities that are more specialized, such as protein molecule design, have not been included at this stage due to the high complexity of evaluation and the significant integration challenges. Based on the preference data collected from the platform, we further conduct in-depth analyses to identify the common characteristics of highly rated AI responses, providing design insights and guidance for future AI Scientist development. Sciencearena.ai aims to serve as fundamental infrastructure for the evaluation and evolution of AI-driven scientific intelligence, more than leaderboard, it is an open ecosystem for the comparison, co-creation, and collective advancement of AI Scientists. 3https://sciencearena.ai/"
        },
        {
            "title": "5.2 Design: Elo-Based Real-Time Ranking",
            "content": "To support dynamic and continuous evaluation in the ScienceArena, we employ the Elo rating system as foundation and extend it with suite of algorithmic and architectural enhancements designed specifically for an open, high-throughput model evaluation environment. Our design introduces stabilized cold-start calibration, pairwise update decay, and activity-aware rating regression to improve robustness under uneven comparison frequencies, while fully asynchronous event-processing pipeline enables real-time rating updates even under substantial user traffic. Formally, each candidate model is associated with scalar rating R, initialized to common baseline R0 = 1000. When two models and receive user preference judgment, the system computes the expected win probability of under the standard Elo formulation: EA = 1 1 + 10(RB RA)/400 . Given the observed outcome SA {0, 1}, the rating update is performed as: A = RA + K(SA EA), where is tunable update coefficient. = RB + K((1 SA) (1 EA)), (1) (2) Based on these Elo definitions, we further incorporate cold-start sensitivity window for newly submitted models. During this phase, the effective rating difference is scaled as = (RB RA),  > 1, which temporarily amplifies the advantage signal and allows the system to converge more rapidly to stable estimate despite limited early comparisons. To avoid disproportionate influence from repeated matchups between the same pair of models, we also apply pairwise decay factor to the update magnitude. Specifically, the effective learning rate becomes Keff = nAB , 0 <  < 1, where nAB denotes the number of prior encounters between models and B. This mechanism discourages oscillatory rating shifts arising from oversampled pairs and promotes broader comparison coverage across the model population. Additionally, we employ an activity-based regression mechanism that gradually adjusts the rating of an inactive model toward the global mean according to = Ri (Ri R),  > 0, ensuring that outdated models do not dominate the leaderboard and that the rankings reflect temporal relevance. In addition to algorithmic refinements, ScienceArena system employs an asynchronous, messagedriven update architecture. Each user comparison is emitted as an asynchronous event and routed through lightweight message-queuebased pipeline to dedicated rating-update worker. This design decouples the rating computation process from the front-end interaction loop, enabling millisecondlevel update latency even under high user concurrency. Once the update is computed, the leaderboard is immediately re-sorted, ensuring that the displayed rankings consistently reflect the most recent observational evidence. Through the combination of online Elo-style updates, cold-start calibration, pairwise decay, temporal regularization, and an asynchronous execution infrastructure, the resulting ranking system achieves robustness, scalability, and responsiveness in large-scale human-in-the-loop scientific evaluation. It supports continuous model submissions, high-frequency preference judgments, and self-correcting competitive environment in which model quality is continuously refined based on real-user scientific preferences. 5.3 Evaluation Based on the voting data collected from the ScienceArena platform, we conduct comprehensive analysis of user preferences across different tracks. This allows us to identify the characteristics of high-quality responses and to understand what types of answers are more likely to be well-received by users. Building on these findings, we further summarize several insights and design recommendations for the AI Scientist system as follows."
        },
        {
            "title": "5.3.1 Citation matters for literature review",
            "content": "The analysis of the Literature Review track reveals that citation usage plays decisive role in shaping evaluators preferences. Three complementary dimensions: quantity, density, and depth, collectively determine how citations influence the perceived quality of response. Quantity: the visual signal of academic authority. Across submissions, responses containing larger number of citations consistently receive higher preference scores. Even when individual references are discussed only briefly, the visual abundance of citation markers (e.g., [Author, Year]) tends to enhance the impression of academic credibility. This suggests visual bias in human evaluation: evaluators implicitly associate frequent referencing with expertise, comprehensiveness, and scholarly authority. In other words, citation quantity functions not merely as an informational measure, but also as stylistic cue that signals intellectual effort and coverage breadth. Figure 21: One case of Literature Review: The number of citations has an almost decisive effect on voting, especially when there is clear disparity in citation counts between responses. In the figure, the response on the right contains only five citations, which is far too few for literature review. Density: the structural rhythm of scholarly writing. Beyond sheer numbers, the distribution of citations within the text strongly affects perceived coherence. High-rated responses typically exhibit even citation density, where each major paragraph is anchored by one or more references that substantiate the corresponding argument. Such structured integration enhances both readability and logical flow, making the response appear as genuine academic synthesis rather than loosely connected summary. In contrast, citation clusters, where multiple sources are concentrated in single paragraph, often lead to perception of imbalance or superficiality. The most successful entries therefore balance breadth and order, ensuring that citations rhythmically support the narrative structure. Depth: the interpretive integration of references. third dimension concerns how deeply citations are integrated into reasoning. Some participants adopt selective and interpretive strategy, citing fewer but more representative or seminal works. This approach enables deeper conceptual connections, stronger contrastive reasoning, and richer interpretive insight. However, despite these qualitative merits, such selective responses often underperform when compared with more citation-rich ones; this again reflects the evaluator bias favoring apparent comprehensiveness over analytical depth. The challenge, therefore, lies in balancing citation depth with citation quantity, ensuring that interpretive richness is not overshadowed by the perceived authority of volume. Taken together, these three dimensions suggest that in literature review writing (whether human or model generated), citations serve not only as evidential anchors but also as aesthetic and evaluative signals. The best-performing responses achieve an equilibrium: sufficient quantity to convey breadth, consistent density to maintain structure, and adequate depth to demonstrate understanding. Future AI systems designed for scholarly synthesis should thus focus on improving citation integration 32 Figure 22: One case of Literature Review: Citation depth matters. deeper understanding and interpretation of citations can also make response competitive. In the figure, the response on the left uses table to summarize the various methods in clear and intuitive way, performing horizontal comparisons across aspects such as intervention stage and the models used, which significantly enhances the depth of the literature review. strategies, emphasizing not just how many references are cited, but how meaningfully they are woven into the argument. 5.3.2 Balancing novelty and feasibility in ideation In the Ideation track, evaluators preferences are influenced not only by the apparent creativity of an idea but also by the careful balance between novelty and feasibility. good ideation response is one that demonstrates originality while remaining grounded in practical constraints, presenting an idea that is both imaginative and actionable. Overall, human evaluators tend to reward innovation that operates within feasible boundaries rather than unconstrained speculation. Novelty: Generating ideas that meaningfully extend the frontier of existing research. Highscoring responses often exhibit substantial novelty in how they define or approach research problem. Many outstanding entries creatively establish connections between previously unlinked domains or propose entirely new perspectives on familiar challenges. In some cases, participants even formulate new research questions that extend beyond the current scientific agenda, demonstrating an ability to rethink what counts as meaningful problem in the first place. Such originality is most compelling when it is situated within the broader scientific landscape. High-quality novel ideas not only introduce something new but also articulate how they depart from, extend, or reconfigure existing lines of work. By framing the contribution against prior literature, the ideas distinctiveness becomes clearer and its novelty more convincingly established. Feasibility: From conceptual inspiration to technical credibility. Feasibility reflects whether an idea appears implementable or verifiable within realistic research or engineering setting. Highquality ideation responses do not merely propose innovative concepts; they also outline full sequence of substeps for task decomposition, articulate complete methodological pathway, and describe experimental validation in detail. Importantly, the most convincing responses explain specific technical mechanisms rather than remaining at high-level conceptual description. Such concreteness enhances readers confidence in the ideas plausibility and fosters sense of scientific credibility. Conversely, responses that stay at the conceptual level, without indicating how the idea could be operationalized, are often perceived as hollow creativity. Thus, feasibility functions as the structural backbone of ideation: it anchors imaginative thinking to methodological realism. Trade-off and implications. Taken together, the Ideation track results reveal consistent pattern: the best-performing responses balance novelty with feasibility. Overemphasizing novelty risks detachment from reality, while prioritizing feasibility too heavily may result in conservative, incremental 33 Figure 23: One case of Ideation: Situate the idea within the context of the existing literature to highlight its novelty. Figure 24: One case of Ideation: Ideas accompanied by detailed and practical experimental plan are more likely to be favored. ideas. The most preferred submissions achieve both, presenting an innovative conceptual leap while providing credible path toward realization. This trade-off underscores broader evaluative principle: users favor actionable innovation, not pure speculation. In other words, the most successful ideas are those that appear doable yet new. For future AI systems designed for creative scientific ideation, developing mechanisms that can dynamically balance novelty generation with feasibility reasoning, producing ideas that are simultaneously visionary and credible, will be key to achieving genuinely high-quality outputs. 5.3.3 Combining discriminative judgment with conciseness in paper review In the Paper Review track, evaluators value reviews that demonstrate both professional judgment and conciseness of expression. High-quality reviews are not defined by length or comprehensiveness alone; instead, they reflect the reviewers ability to critically assess the scientific contribution, identify principal strengths and limitations, and provide actionable recommendations. Overall, 34 human evaluators favor responses that convey informed and authoritative assessments, emphasizing substantive evaluation over extraneous commentary. Conciseness and focus. The results from the paper review track show that high-quality reviews are typically concise and focused rather than exhaustive. While many model-generated reviews attempt to cover every possible aspect of paper, ranging from methodology and experiments to presentation and typos, this excessive comprehensiveness often dilutes the evaluative signal. Readers may find it difficult to discern the reviewers main judgment amid flood of minor comments, and the response may further come across as impersonal or AI-like. By contrast, concise reviews convey evaluative precision: they emphasize the most critical strengths and weaknesses and briefly mention secondary issues. This clarity of focus reflects mature form of academic judgment, where the reviewer is capable of distinguishing what truly determines the papers quality from what merely decorates it. Figure 25: One case of Paper Review: Conciseness of responses strongly influences user preferences. The review generated by the model on the right is more than four times longer than the one on the left, containing 7,256 words and total of 42,537 characters. Discriminative judgment. Another defining feature of strong reviews is their ability to distinguish high-quality submissions from weaker ones. Low-performing reviews often rely on vague or noncommittal language such as an interesting paper that could be improved, offering little actionable insight. Effective reviews, in contrast, demonstrate clear evaluative direction. They identify authentic innovation rather than superficial novelty, assess whether the presented evidence sufficiently supports the claims, and explicitly discuss how methodological limitations influence the credibility of the work. critical component of strong evaluative judgment is the reviewers ability to position the submission within the context of the existing literature. High-quality reviews not only recognize the most relevant and up-to-date works in the field but also articulate how the paper compares to them methodologically and conceptually. By conducting such horizontal comparisons, reviewers can more accurately determine whether the contribution represents meaningful advancement, modest incremental step, or reinvention of well-established ideas. By articulating well-reasoned, literature-grounded, and decisive assessments, strong reviews enhance both the reliability and interpretability of the evaluation process, ensuring that the review serves not merely as commentary but as substantive act of scholarly judgment. Taken together, the findings from the paper review track show that the most effective reviews combine professional discriminative judgment with concise writing style. More text does not imply better reviewing; what matters is the reviewers ability to deliver clear, well-supported, and confident evaluation grounded in solid understanding of the relevant literature. By focusing on the issues that truly determine scholarly quality and providing targeted, insightful feedback rather than overwhelming detail, such reviews meaningfully contribute to the refinement of the work and uphold rigorous academic standards."
        },
        {
            "title": "6.1 Limitations",
            "content": "Although OmniScientist shows encouraging progress toward automated scientific reasoning, its current capabilities remain constrained in the AI domain. This limitation permeates nearly every stage of the system. The underlying data backbone includes primarily arXiv full texts, which strongly overrepresent AI and computer science, while many other scientific disciplines rely on journal-first publication practices and are not well captured by arXiv. Key sources such as Nature, Science, and other domain-specific journals are not yet integrated, leading to limited coverage of non-AI literature. The ideation module is similarly tuned to AI-centric topics, and the automated experiment agent currently supports only computational workflows, limited to tasks such as configuring development environments, preparing datasets, running training and inference scripts, performing hyperparameter sweeps, and generating quantitative evaluation results for machine learning models. Disciplines requiring wet-lab experimentation in chemistry or biology, or physical-world interactions with scientific instruments, remain outside the systems current operational scope. Even the reviewing component has been optimized using AI-domain manuscripts, which restricts its generalizability and reduces review quality when applied to submissions from other fields. Another limitation concerns the trade-off between efficiency and resource cost. OmniScientists modules still require significant computational resources and considerable processing time, making it challenging to efficiently complete particularly complex tasks or meet tight timelines. 6.2 Future Work Broaden the interdisciplinary data base. Future work will expand OmniScientists data foundation beyond AI-centric preprints to cover other scientific domains. This includes integrating repositories such as BioRxiv and PubMed, as well as high-impact subscription-based journals including Nature, Science, and Cell. We will implement automated ingestion pipelines for these sources, perform metadata harmonization, and establish domain-specific indexing schemes to ensure accurate and efficient retrieval across disciplines. For subscription-based high-impact journals, we will explore potential collaborations with publishers to ensure lawful access and integration, given the copyright restrictions on their content. This expansion will provide robust, high-quality foundation for cross-domain scientific reasoning and literature-based discovery. Enhance support for wet experiments. OmniScientist will be extended to execute physical experiments by integrating with laboratory instruments and robotics platforms. Planned capabilities include automated experimental setup, data acquisition, and control of instrument parameters, alongside computational workflows. We will also develop standardized interfaces for domain-specific experimental protocols in physics, chemistry, and biology, enabling the system to perform iterative experimental cycles that combine in-silico simulations with real-world validations."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we introduce OmniScientist, comprehensive framework that transitions AI from isolated task automation to cohesive scientific ecosystem. By explicitly encoding the foundational infrastructure of human research into the AI workflow, we empower LLM agents to evolve beyond mere executors. Instead, they function as autonomous participants capable of internalizing scientific norms, collaborating within governed environment, and producing knowledge that is rigorously grounded in the genealogy of human scientific research. Looking ahead, OmniScientist serves as blueprint for the next generation of scientific discovery, where artificial and human intelligence do not operate in silos but co-evolve. We envision future where this ecosystem continuously refines itself through community-driven evaluation, fostering symbiotic partnership that accelerates the pace of innovation. Ultimately, this work points toward new era of research, where AI agents actively drive the evolution of the scientific enterprise itself, collectively expanding the boundaries of human knowledge."
        },
        {
            "title": "References",
            "content": "[1] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Nicolaus Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. ArXiv, abs/2408.06292, 2024. [2] Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. Ai-researcher: Autonomous scientific innovation. ArXiv, abs/2505.18705, 2025. [3] Alexander Novikov, Ngn Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav M. Kozlovskii, Francisco J. R. Ruiz, Abbas Mehrabian, M. Pawan Kumar, Abigail See, Swarat Chaudhuri, George Holland, Alex Davies, Sebastian Nowozin, Pushmeet Kohli, Matej Balog, and Google Deepmind. Alphaevolve: coding agent for scientific and algorithmic discovery. ArXiv, abs/2506.13131, 2025. [4] OpenAI."
        },
        {
            "title": "Introducing",
            "content": "deep research. introducing-deep-research/, Feb 2025. Accessed: 2025-10-29. https://openai.com/index/ [5] Kyle Swanson, Wesley Wu, Nash L. Bulaong, John E. Pak, and James Y. Zou. The virtual lab: Ai agents design new sars-cov-2 nanobodies with experimental validation. bioRxiv, 2024. [6] Future House. Future house platform. https://platform.futurehouse.org/, 2025. Accessed: 2025-10-29. [7] Santo Fortunato, Carl T. Bergstrom, Katy Brner, James A. Evans, Dirk Helbing, Stasa Milojevic, Alexander Michael Petersen, Filippo Radicchi, Roberta Sinatra, Brian Uzzi, Alessandro Vespignani, Ludo Waltman, Dashun Wang, and Albert Laszl Barabsi. Science of science. Nature, 214:12, 2018. [8] Yixuan Weng, Minjun Zhu, Qiujie Xie, Qiyao Sun, Zhen Lin, Sifan Liu, and Yue Zhang. Deepscientist: Advancing frontier-pushing scientific findings progressively. arXiv preprint arXiv:2509.26603, 2025. [9] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. [10] Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, et al. Towards an ai co-scientist. arXiv preprint arXiv:2502.18864, 2025. [11] DeepSe Technologies / Bohrium. Bohrium: Ai for science science navigator platform. https://www.bohrium.com/, 2025. Accessed: 2025-11-06. [12] Jamshid Sourati and James Evans. Accelerating science with human-aware artificial intelligence. Nature human behaviour, 7(10):16821696, 2023. [13] Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):9598, 2019. [14] Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. arXiv preprint arXiv:2410.07076, 2024. [15] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 279299, 2024. [16] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 67096738, 2025. 37 [17] Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, Pawan Kumar, Emilien Dupont, Francisco JR Ruiz, Jordan Ellenberg, Pengming Wang, Omar Fawzi, et al. Mathematical discoveries from program search with large language models. Nature, 625(7995):468475, 2024. [18] Alexander Novikov, Ngn Vu, Marvin Eisenberger, Emilien Dupont, Po-Sen Huang, Adam Zsolt Wagner, Sergey Shirobokov, Borislav Kozlovskii, Francisco JR Ruiz, Abbas Mehrabian, et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. [19] Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, and Graham Neubig. Datafinder: arXiv preprint Scientific dataset recommendation from natural language descriptions. arXiv:2305.16636, 2023. [20] Michael Frber and Ann-Kathrin Leisinger. Datahunter: system for finding datasets based on scientific problem descriptions. In Proceedings of the 15th ACM Conference on Recommender Systems, pages 749752, 2021. [21] Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Michael Moor, Zicheng Liu, and Emad Barsoum. Agent laboratory: Using llm agents as research assistants. arXiv preprint arXiv:2501.04227, 2025. [22] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. [23] Weixin Liang, Yuhui Zhang, Hancheng Cao, Binglu Wang, Daisy Yi Ding, Xinyu Yang, Kailas Vodrahalli, Siyu He, Daniel Scott Smith, Yian Yin, et al. Can large language models provide useful feedback on research papers? large-scale empirical analysis. NEJM AI, 1(8):AIoa2400196, 2024. [24] Mike DArcy, Tom Hope, Larry Birnbaum, and Doug Downey. Marg: Multi-agent review generation for scientific papers. arXiv preprint arXiv:2401.04259, 2024. [25] Pawin Taechoyotin and Daniel Acuna. Remor: Automated peer review generation with llm reasoning and multi-objective reinforcement learning. arXiv preprint arXiv:2505.11718, 2025. [26] Sihang Zeng, Kai Tian, Kaiyan Zhang, Yuru Wang, Junqi Gao, Runze Liu, Sa Yang, Jingxuan Li, Xinwei Long, Jiaheng Ma, et al. Reviewrl: Towards automated scientific review with rl. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1694216954, 2025. [27] Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, and Linyi Yang. Cycleresearcher: Improving automated research via automated review. arXiv preprint arXiv:2411.00816, 2024. [28] Zhaolin Gao, Kiant Brantley, and Thorsten Joachims. Reviewer2: Optimizing review generation through prompt generation. arXiv preprint arXiv:2402.10886, 2024. [29] Jianxiang Yu, Zichen Ding, Jiaqi Tan, Kangyang Luo, Zhenmin Weng, Chenghua Gong, Long Zeng, Renjing Cui, Chengcheng Han, Qiushi Sun, et al. Automated peer reviewing in paper sea: Standardization, evaluation, and analysis. arXiv preprint arXiv:2407.12857, 2024. [30] Zhenzhen Zhuang, Jiandong Chen, Hongfeng Xu, Yuwen Jiang, and Jialiang Lin. Large language models for automated scholarly paper review: survey. Information Fusion, page 103332, 2025. [31] Eftekhar Hossain, Sanjeev Kumar Sinha, Naman Bansal, Alexander Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Ram Pavan Kumar Guttikonda, Mousumi Akter, Md Mahadi Hassan, et al. Llms as meta-reviewers assistants: case study. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 77637803, 2025. 38 [32] Maximilian Idahl and Zahra Ahmadi. Openreviewer: specialized large language model for generating critical scientific paper reviews. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations), pages 550562, 2025. [33] Minjun Zhu, Yixuan Weng, Linyi Yang, and Yue Zhang. Deepreview: Improving llm-based paper review with human-like deep thinking process. arXiv preprint arXiv:2503.08569, 2025. [34] Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, and Ngai Wong. Treereview: dynamic tree of questions framework for deep and efficient llm-based scientific peer review. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1566215693, 2025. [35] Pawin Taechoyotin, Guanchao Wang, Tong Zeng, Bradley Sides, and Daniel Acuna. Mamorx: Multi-agent multi-modal scientific review generation with external knowledge. In Neurips 2024 Workshop Foundation Models for Science: Progress, Opportunities, and Challenges, 2024. [36] Kai Lu, Shixiong Xu, Jinqiu Li, Kun Ding, and Gaofeng Meng. Agent reviewers: Domainspecific multimodal agents with shared memory for paper review. In Forty-second International Conference on Machine Learning. [37] Anthropic. Introducing the model context protocol. https://www.anthropic.com/news/ model-context-protocol, 2024. Accessed: 2025-11-09. [38] Google. Agent2agent (a2a) protocol latest documentation. https://a2a-protocol.org/ latest/, 2025. Accessed: 2025-11-09. [39] Open Science Lab. Scientific intelligence context protocol (scp). https://github.com/ open-sciencelab/scp, 2025. Accessed: 2025-11-09. [40] Zekun Shi, Zheyuan Hu, Min Lin, and Kenji Kawaguchi. Stochastic taylor derivative estimator: Efficient amortization for arbitrary differential operators. Advances in Neural Information Processing Systems, 37:122316122353, 2024. [41] Yuanhao Qu, Kaixuan Huang, Ming Yin, Kanghong Zhan, Dyllan Liu, Di Yin, Henry Cousins, William Johnson, Xiaotong Wang, Mihir Shah, et al. Crispr-gpt for agentic automation of gene-editing experiments. Nature Biomedical Engineering, pages 114, 2025. [42] Kyle Swanson, Wesley Wu, Nash Bulaong, John Pak, and James Zou. The virtual lab of ai agents designs new sars-cov-2 nanobodies. Nature, pages 13, 2025. [43] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed Shaaban, John Ling, Sean Shi, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. [44] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [45] OpenAI. Gpt-5 is here, 2025. Accessed: 2025-11-17. [46] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents, 2025. [47] Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, Albert Huang, Eric Xie, Stefan Bekiranov, and Aidong Zhang. Ideabench: Benchmarking large language models for research idea generation. Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2, 2024. [48] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, Nitesh V. Chawla, and Xiangliang Zhang. Justice or prejudice? quantifying biases in llm-as-a-judge. ArXiv, abs/2410.02736, 2024. 39 [49] Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. Humans or LLMs as the judge? study on judgement bias. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 83018327, Miami, Florida, USA, November 2024. Association for Computational Linguistics. [50] LMArena: Find the best AI for you. https://lmarena.ai/, 2025. Accessed: 2025-11-07."
        }
    ],
    "affiliations": [
        "Department of Electronic Engineering, BNRist, Tsinghua University",
        "Zhongguancun Academy"
    ]
}