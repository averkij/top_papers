{
    "paper_title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering",
    "authors": [
        "Rachneet Kaur",
        "Nishan Srishankar",
        "Zhen Zeng",
        "Sumitra Ganesh",
        "Manuela Veloso"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated charts, those requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, a novel agentic framework that explicitly performs visual reasoning directly within the chart's spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using a library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieve the highest scores across varying visual and reasoning complexity levels, and (c) serves as a plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 4 1 5 4 0 . 0 1 5 2 : r ChartAgent: Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering"
        },
        {
            "title": "Rachneet Kaur Nishan Srishankar Zhen Zeng\nSumitra Ganesh Manuela Veloso",
            "content": "J.P. Morgan AI Research {rachneet.kaur, nishan.srishankar, zhen.zeng}@jpmorgan.com {sumitra.ganesh, manuela.veloso}@jpmorgan.com"
        },
        {
            "title": "Abstract",
            "content": "Recent multimodal LLMs have shown promise in chart-based visual question answering, but their performance declines sharply on unannotated chartsthose requiring precise visual interpretation rather than relying on textual shortcuts. To address this, we introduce ChartAgent, novel agentic framework that explicitly performs visual reasoning directly within the charts spatial domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively decomposes queries into visual subtasks and actively manipulates and interacts with chart images through specialized actions such as drawing annotations, cropping regions (e.g., segmenting pie slices, isolating bars), and localizing axes, using library of chart-specific vision tools to fulfill each subtask. This iterative reasoning process closely mirrors human cognitive strategies for chart comprehension. ChartAgent achieves state-of-the-art accuracy on the ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries. Furthermore, our analyses show that ChartAgent is (a) effective across diverse chart types, (b) achieves the highest scores across varying visual and reasoning complexity levels, and (c) serves as plug-and-play framework that boosts performance across diverse underlying LLMs. Our work is among the first to demonstrate visually grounded reasoning for chart understanding using tool-augmented multimodal agents."
        },
        {
            "title": "Introduction",
            "content": "Charts, including bar plots, pie charts, line graphs, and their many variants, are foundational tools for communicating quantitative information across domains such as finance, science, and journalism (Chishtie et al., 2022; Srivastava et al., 2025). Enabling computational systems to answer naturallanguage questions about charts, referred to as chart visual question answering (Chart VQA), remains an essential yet challenging problem in multimodal machine learning research (Masry et al., 2022; Xu et al., 2023; Xia et al., 2024; Wang et al., 2024b). Recent advances in multimodal large language models (MLLMs) have driven substantial progress in general visual reasoning tasks (Liu et al., 2023d; Hurst et al., 2024; Li et al., 2024). However, their performance degrades significantly on Chart VQA, especially when dealing with charts that lack explicit textual annotations of key values or labels, commonly referred to as unannotated charts (Xu et al., 2023; Xia et al., 2024; Islam et al., 2024) (see Appendix for examples). These scenarios demand accurate visual grounding and interpretation (e.g., estimating numerical values from graphical elements), setting where even state-ofthe-art (SoTA) MLLMs often struggle. To address these shortcomings, we draw inspiration from how humans reason with charts. Humans typically process graphical elements sequentially, interpreting axes, legends, and segments, and often add annotations to support intermediate reasoning, such as tracing bars and lines to compare values, circling or shading pie slices to judge proportions, and highlighting legends or markers to align categories. Building on these cognitive strategies, we propose ChartAgent, novel agentic framework explicitly designed for visually grounded reasoning in the chart domain (see Figure 1). At the core of ChartAgent lies multi-turn interaction loop that progressively decomposes chart queries into subtasks that are primarily visual (e.g., legend detection, axis localization, pie segmentation) and occasionally numerical (e.g., arithmetic operations), while simultaneously manipulating and interacting with chart images through precise, modular perception tools (e.g., segmentation, detection, localization) tailored to fulfill these subtasks, thereby augmenting MLLM reasoning with chartspecialized visual capabilities. To the best of our Figure 1: Comparison of our work with the existing SoTA. (a) ChartAgent performs visually grounded reasoning in the chart domain. For this unannotated chart, GPT-4o fails to produce the correct answer, whereas ChartAgent succeeds. (b) ChartAgent performance on unannotated charts and numeric QA compared with the top-10 SoTA. knowledge, and complementary to existing chart VQA approaches that rely on prompting or finetuning MLLMs (Masry et al., 2025, 2024; Han et al., 2023; Liu et al., 2023b), this work is the first to demonstrate visually grounded reasoning for chart understanding through tool-augmented multimodal agents, achieving SoTA performance. Importantly, the perception tools are designed to generate interpretable visualizations (see Figures 8, 9) that the agent can inspect. This allows it to dynamically adjust its strategy, such as tuning parameters or switching to alternative tools, when the outputs are unsatisfactory. Our key contributions are: Multimodal Agent for Charts: We introduce ChartAgent, the first framework to augment MLLM reasoning with chart-specialized visual capabilities for Chart VQA, systematically demonstrating visually grounded reasoning in charts via tool-augmented multimodal agent. Modular Vision Tool Library with SelfVerification: An agent-compatible library of chart-specialized perception tools covering 40+ chart types, generating interpretable visual outputs that not only support grounded reasoning in ChartAgent but also enable visual selfverification mechanism, allowing the agent to inspect intermediate results and adaptively adjust reasoning and tool use. State-of-the-Art Performance: ChartAgent achieves new SoTA, surpassing 30+ baselines by up to 16.07% absolute gain overall and 17.31% on unannotated, numerically intensive queries, evaluated on the well-established ChartBench and ChartX datasets spanning 40+ chart types. In-Depth Analysis: We conduct extensive analyses to demonstrate the effectiveness of ChartAgent. Specifically, we show that (a) it is effective across diverse chart types, (b) it achieves the highest scores across varying visual and reasoning complexity levels of chartQA pairs, and (c) it serves as plug-and-play framework that enhances performance across different base MLLMs, thereby validating both effectiveness and generalization. We also present failure mode analysis highlighting common errors."
        },
        {
            "title": "2 Related Work",
            "content": "We review related work in three areas: chart VQA, MLLMs and visual grounding, and agentic frameworks. See Appendix for an extended review. Chart VQA Chart VQA interprets charts to answer natural-language queries. Early synthetic datasets like FigureQA (Kahou et al., 2017) and DVQA (Kafle et al., 2018) focused on visual reasoning but lacked real-world diversity. Subsequent datasets, PlotQA (Methani et al., 2020), ChartQA (Masry et al., 2022), and EvoChart (Huang et al., 2024), introduced realistic, complex charts, while newer benchmarks such as ChartBench (Xu et al., 2023), ChartX (Xia et al., 2024), and CharXiv (Wang et al., 2024b) added broader variety and numerically intensive queries. Chart-specific MLLMs (Zhang et al., 2024; Masry et al., 2023; Liu et al., 2024a; Masry et al., 2024) improved instruction tuning and visionlanguage alignment, and ChartOCR (Luo et al., 2021) combined vision tools with rule-based parsing. Yet, recent studies (Xu et al., 2023; Razeghi et al., 2024; Islam et al., 2024) show sharp drops on unannotated charts, exposing weak visual grounding. Our work addresses this by equipping MLLMs with chart-specialized, visually grounded reasoning. MLLMs and Visual Grounding Generalpurpose MLLMs such as GPT-4 (Achiam et al., 2023), GPT-4o (Hurst et al., 2024), Gemini (Team et al., 2023), LLaVA (Liu et al., 2023d), and Visual CoT (Shao et al., 2024) have advanced visual reasoning. For precise visual grounding, models are augmented with tools or visual prompts: Visual ChatGPT (Wu et al., 2023), MM-ReAct (Yang et al., 2023b), ViperGPT (Surís et al., 2023), and VisProg (Gupta and Kembhavi, 2023) use structured tools, Visual Sketchpad (Hu et al., 2024b) refines plans iteratively, and Set-of-Marks (Yang et al., 2023a) enriches inputs with annotations. Inspired by these, our approach combines iterative reasoning, visual prompting, and modular vision tools to strengthen visual grounding in charts. Agentic Frameworks Agent-based AI systems, defined by perception, cognition, and action, have advanced with LLM integration. The ReAct framework (Yao et al., 2023) structures interactions into iterative reasoning, action, and observation, while platforms such as AutoGen (Wu et al., 2024a), CrewAI (cre), LangChain (Lan), LangGraph (lan), and AutoGPT (aut) support practical implementations. MLLM agents extend this paradigm to robotics (Nasiriany et al., 2024; Hori et al., 2025), vision-language reasoning (Liu et al., 2025; Yang et al., 2023b), and GUI navigation (Verma et al., 2024; He et al., 2024). Similarly, ChartAgent integrates multimodal reasoning with modular, chartoriented vision tools in an agentic framework."
        },
        {
            "title": "Visually Grounded Reasoning in Charts",
            "content": "Given multimodal query consisting of chart image and natural-language question about the chart, the goal is to generate an answer that accurately reflects the information conveyed in the chart. Building on human strategies for chart comprehension, such as highlighting legend entries to clarify category mappings, sketching guide lines across bars or axes to compare values, or shading portions of pie chart to approximate proportions, we propose ChartAgent. As illustrated in Figure 2, ChartAgent is novel agentic framework that equips MLLMs with structured visual reasoning capabilities for charts, by decomposing queries into visual subtasks and directly interacting with chart images in their spatial domain through specialized vision tools to accomplish these subtasks. These tools are supported by interpretable intermediate visualizations that enable adaptive refinement of reasoning and grounding until confident answer is reached or the iteration limit is exhausted. Visually Grounded Chart Reasoning The foundation of ChartAgent is structured, iterative ReAct (Yao et al., 2023)-style multi-turn interaction loop within the charts visual environment, which at each time step generates sequence of Thought, Action, and Observation phases to guide the agent in interpreting charts and answering user queries. Thought (Reasoning): The MLLM evaluates the current state st, which includes the multimodal query along with previous thoughts, actions, and observations, to derive the next subtask (goal) gt that guides the subsequent action toward answering the users query. These sub-goals primarily involve visual perception tasks (e.g., segmenting chart elements, detecting and annotating legends, or localizing axes), but may also include numerical operations (e.g., interpolation, arithmetic). Action (Chart Tool Execution): Based on the subtask gt from the Thought phase, the agent selects and executes an appropriate tool achart-tool from modular chart-specialized library (see App. Table 5) that directly manipulates the chart image. Examples include pie segmentation, bar isolation, legend detection, axis tick localization, and interpolation. Each tool returns structured outputs (e.g., numeric estimates, labels, detected coordinates) and, when applicable, interpretable intermediate or final visualizations (e.g., segmentation masks with labels, colored overlays for pie slices, bar height markers, annotated legends, or bounding boxes) (see App. Figures 8, 9), which support the agents subsequent visual self-verification. , ot+1). Observation (Visual Self-Verification and Adaptive Tool Use): Based on the invoked action , ChartAgent receives new perceptionachart-tool friendly visualizations and outputs ot+1. The multimodal state is then updated as st+1 = ChartAgent then inter- (st, gt, achart-tool prets and verifies these multimodal outputs, particularly for perception-related tools, by visually inspecting the provided visualizations to assess their accuracy. If verification reveals unsatisfactory results (e.g., incomplete segmentation, mismatched legend associations, overly small pie slices, incorrect colors, negative bar heights, or outputs inconsistent with axis values), the agent adaptively adjusts its tool use in the next iteration + 1, for instance by invoking an alternative tool or tweaking parameters such as detection thresholds. This iterative correction loop mimics human-like debugging, enabling ChartAgent to reason and ground with visualizations it generated on the chart, thereby ensurFigure 2: ChartAgent. The (A) orchestrator extracts chart metadata and routes annotated charts with textual shortcuts and qualitative QA to the base MLLM, while unannotated charts and numeric queries trigger the ReActstyle loop. The system includes (B) library of universal and chart-specific tools, (C) metadata for parameterizing tool usage and retrieving chart-type-specific ICL examples, and (D) few-shot ICL retrieval. Using these components as the (E) input, ChartAgent performs (F) iterative visual reasoning, supported by (G) visual self-verification of intermediate outputs. When tool-based reasoning is unreliable, (H) the agent falls back to the base MLLM. ing improved chart VQA capabilities (see Sec. 5.3 for analysis of recovery behavior effectiveness). If tool outputs remain insufficient after multiple tries, ChartAgent recognizes the limits of its perception capabilities with available tools, an essential feature for trustworthy agent design. Chart Interaction and Manipulation The effectiveness of ChartAgent hinges on the careful design of modular library of perception and numeric tools tailored for chart understanding (a detailed taxonomy is provided in App. and Table 5). Inspired by primitive visual tasks in natural image domains (e.g., object detection, segmentation, relational inference), we define analogous primitive tasks for the chart domain, treating chart elements (e.g., bars, pie slices, lines, legends, tick marks, and axis labels) as fundamental visual objects. By targeting shared components such as legends, axes, ticks, bar segments, and pie slices, these tools enable broad generalization across diverse chart formats (see App. for the 40+ chart types supported in ChartAgent). Guided by this perspective, we designed structured, chart-domain-specific set of primitive tools, organized into two categories: 1. Universal chart tools: General-purpose perception tools applicable across chart types, such as segmentation, legend detection, axis localization, and numeric interpolation. 2. Chart-specific tools: Tools specialized for particular chart types (e.g., pie, bar, line, box), targeting subtasks unique to their visual structures. Each tool is deliberately scoped to remain clear and distinct, avoiding overly fine-grained or excessively complex functionalities, thereby ensuring robust implementations with modern vision techniques. Architecture and Components Chart Metadata Extraction and Orchestration: ChartAgent begins with an LLM-based orchestrator (e.g., GPT-4o) that extracts comprehensive chart metadata, including chart type, title, legend details, axis labels and tick marks, annotation status (annotated or unannotated), and concise visual description (see App. L.1.3). This metadata is critical for orchestrating the smart routing mechanism, which first determines whether perception tools are necessary for the user task. For annotated charts containing explicit textual shortcuts (e.g., numerical annotations or clear labels) or for queries requiring mainly qualitative reasoning, direct reasoning by the base MLLM is often sufficient. In such cases, the orchestrator routes the query directly to the MLLM balancing accuracy and computational efficiency. In contrast, for unannotated charts (see App. A), where accurate interpretation of graphical elements, such as barheight/pie-area estimation, or legend association, is essential, the orchestrator initiates deeper, iterative routine of visual reasoning to derive the answer. In the unannotated case, the extracted metadata is also used to retrieve appropriate chart-type-specific few-shot in-context learning (ICL) examples and to parameterize subsequent tool usage. Chart Tools Implementation: Chart tools are implemented as Python functions callable by ChartAgent. Some of these tools internally leverage SoTA computer vision and OCR methods, such as Segment Anything (SAM) (Kirillov et al., 2023), Semantic SAM (Li et al.), Tesseract (tes), and EasyOCR (eas). They also handle edge cases (e.g., rotated text, fuzzy label matching for legends or axis ticks, and filtering small, background, or overlapping segments) and return structured outputs (e.g., numeric values, bounding boxes, text labels) along with visualizations (e.g., segmentation masks with labels or bounding box annotations; see details and examples in App. F.2 and Figures 8, 9) that are explicitly designed to facilitate ChartAgents visual self-verification. See App. for detailed tool descriptions, App. L.1.2 for prompt details, and Sec. 5.3 for an analysis of the effectiveness of these chart-specialized visual tool designs. ICL: ChartAgent uses few-shot (12) ICL examples that are specifically retrieved based on the chart type identified during metadata extraction. If no ICL examples exist for the detected chart type, then none are added. Each ICL example consists of complete ReAct trajectory that successfully answers sample queries (see App. L.1.4)."
        },
        {
            "title": "Agentic",
            "content": "Multimodal Framework: ChartAgent uses GPT-4o (gpt-4o-2024-08-06) as the base MLLM. With its plug-and-play design, ChartAgent benefits from advances in both perception tools and MLLM reasoning, enabling seamless integration and sustained cumulative performance gains. We also experiment with other MLLMs to validate this generalization; see Sec. 5.2. ChartAgent is built on AutoGen (Wu et al., 2024a), which enables tool orchestration; see App. for the structured set of prompts. After each ReAct cycle, ChartAgent evaluates the updated multimodal state st+1 and decides whether to continue or terminate with final answer. If satisfactory results cannot be achieved after multiple iterations, the agent gracefully falls back to direct MLLM reasoning (see Sec. 5.3 for evaluation). The maximum number of ReAct iterations is set to 15. Further implementation details in App. G."
        },
        {
            "title": "4 Experimental Protocol and Details",
            "content": "Datasets We benchmark on two widely used datasets: ChartBench (Xu et al., 2023), which spans 9 chart categories and 42 subtypes, including standard charts (bar, line, pie) and complex ones (area, radar, box, scatter, node, and combinations), with 3,800 chartQA pairs (76.2% unannotated). We evaluate two QA types: (1) Numeric QA, requiring precise value extraction, and (2) Relationship QA, involving structural reasoning (e.g., connectivity in graphs), with 96.7% numeric QA. ChartX (Xia et al., 2024), which covers 18 chart types, ranging from standard to domain-specific formats (e.g., treemaps, heatmaps, candlestick charts), with 1,152 chartQA pairs (61.7% unannotated). The questions span (1) Numeric QA, and (2) Value Comparison / Global Perception QA, which involves reasoning over relative or extremum-based patterns, with 71.9% numeric QA. Both benchmarks are visually grounded, requiring models to reason about chart logic (e.g., bar heights, pie-slice areas) beyond OCR. Their high proportion of unannotated charts and numeric QA makes them particularly well-suited for evaluating complex visual reasoning. See App. and for dataset details. Baselines We evaluate against 42 baseline models to ensure comprehensive comparison: (A) Proprietary MLLMs: GPT-4o, GPT-4o-mini, Claude 3 Haiku, Gemini 1.5; (B) Open-Weight General-Purpose MLLMs: BLIP-2, CogAgent, CogVLM, DeepSeek-VL2, DocOwl1.5, InstructBLIP, InternVL3, LLaMA-3.2, LLaVA1.6/1.5/OneVision, mPLUG-Owl3, Phi-3 Vision, Pixtral, Qwen2-VL, Qwen-VL-Chat, SmolVLM, (C) Chart-Specific SPHINX-V, VisualGLM; MLLMs: ChartGemma, ChartInstruct, ChartLLaMA, ChartVLM, DePlot, MatCha, OneChart, TinyChart, UniChart. Concurrent Works: We adTable 1: Comparison of accuracy (%). Red: Best, Blue: Second best. All values correspond to the highest performance achieved across zero-shot and CoT prompting styles for each MLLM. Ann./Unann. denote Annotated and Unannotated charts. RL QA: Relationship QA; VC/GC QA: Value Comparison & Global Conception QA. Model Chart Types Question Types Overall Model Chart Types Question Types Overall Ann. Unann. Numeric QA RL QA Avg. Ann. Unann. Numeric QA VC/GC QA Avg. Proprietary Multimodal Large Language Models GPT 4o (Hurst et al., 2024) GPT 4o-mini (GPT, 2024) Claude 3 Haiku (Anthropic, 2024a) Gemini 1.5 (Team et al., 2024) 94.33 84.83 84.58 89.72 36.15 25.19 26.04 27.27 Open-weights Multimodal Large Language Models BLIP-2 (Li et al., 2023) CogAgent (Hong et al., 2023) CogVLM (Wang et al., 2023) DeepSeek-VL2 (Wu et al., 2024c) DocOwl1.5 (Hu et al., 2024a) InstructBLIP (Dai et al., 2023) InternVL3 (Zhu et al., 2025) LLama3.2 (Grattafiori et al., 2024) Llava1.6 (Liu et al., 2024b) Llava1.5 (Liu et al., 2023c) LlaVA-OneVision (Li et al., 2024) mPLUG-Owl3 (Ye et al., 2024) Phi3-vision (Abdin et al., 2024) Pixtral (Agrawal et al., 2024) Qwen2-VL (Wang et al., 2024a) Qwen-VL-Chat (Bai et al., 2023) SmolVLM (Marafioti et al., 2025) SPHINX-V (Lin et al., 2025) VisualGLM (GLM et al., 2024) Chart-related Models ChartGemma (Masry et al., 2025) ChartInstruct (Masry et al., 2024) ChartLlama (Han et al., 2023) ChartVLM (Xia et al., 2024) DePlot (Liu et al., 2023a) MatCha (Liu et al., 2023b) OneChart (Chen et al., 2024) TinyChart (Zhang et al., 2024) UniChart (Masry et al., 2023) 3.67 69.92 64.83 90.75 67.50 3.92 72.67 87.58 35.58 26.75 13.25 31.08 86.92 66.58 78.42 27.17 47.75 35.91 4.83 75.92 55.17 38.25 61.00 70.08 59.50 56.78 77.33 53.50 2.92 11.62 11.62 30.31 23.58 5.92 30.92 36.38 9.92 7.00 10.50 12.65 40.77 28.73 43.50 6.54 14.46 12.30 7.65 22.42 20.19 11.42 23.92 28.15 9.69 26.81 32.77 15.96 91.00 89.50 73.00 53. 4.00 27.00 21.50 33.50 44.50 24.50 57.00 50.00 42.00 16.50 37.00 46.50 52.00 63.50 83.00 21.00 58.00 0.5 58.00 35.00 22.00 39.50 11.50 78.50 17.50 62.76 28.50 34.50 54.53 44.03 44.53 47.08 3.16 30.03 28.42 49.39 37.45 5.29 44.11 52.11 18.03 13.24 11.37 18.47 55.32 40.50 54.53 13.05 24.97 19.76 6.76 39.32 31.24 19.89 35.63 41.39 25.42 36.81 46.84 27.82 Proprietary Multimodal Large Language Models GPT 4o GPT 4o-mini Claude 3 Haiku Gemini 1.5 84.84 71.95 63.57 68.09 39.44 33.94 25.77 31.41 52.05 42.51 35.99 40.22 Open-weights Multimodal Large Language Models BLIP-2 CogAgent CogVLM DeepSeek-VL2 DocOwl1.5 InstructBLIP InternVL3 LLama3.2 Llava1.6 Llava1.5 LlaVA-OneVision mPLUG-Owl3 Phi3-vision Pixtral Qwen2-VL Qwen-VL-Chat SmolVLM SPHINX-V VisualGLM 1.13 46.15 46.38 66.74 42.53 10.41 65.84 78.51 26.24 18.55 20.14 23.98 59.95 64.93 76.24 24.66 28.51 27.37 9.28 Chart-related Models ChartGemma ChartInstruct ChartLlama ChartVLM DePlot MatCha OneChart TinyChart UniChart 45.93 27.38 30.54 46.83 60.63 28.28 54.48 57.01 24.66 1.69 24.93 24.23 35.63 24.37 8.87 36.62 39.86 18.17 14.51 12.82 18.31 41.69 38.17 42.96 20.42 22.11 20.70 13.10 28.87 17.75 21.55 29.01 34.51 17.04 37.14 33.38 18. 0.72 27.05 24.28 43.84 26.81 7.37 44.20 50.36 16.55 10.63 13.89 14.49 41.06 41.55 51.81 11.59 19.93 14.49 4.47 27.54 20.29 18.72 35.75 41.30 18.24 41.61 36.11 16.06 52.50 41.50 42.94 46.69 3.11 30.28 29.03 50.28 37.06 4.22 43.39 52.22 16.69 13.06 9.94 16.92 55.89 39.53 52.94 12.61 23.14 18.08 3.92 39.56 31.75 18.81 36.97 39.33 25.86 35.22 47.86 27.44 69.14 63.89 51.23 58. 3.40 48.46 54.32 57.10 42.90 14.81 57.10 65.74 33.33 29.94 20.06 35.80 68.21 66.05 65.74 48.77 36.42 45.67 29.94 55.56 24.38 41.05 36.11 52.78 29.32 51.50 58.64 33.95 56.86 48.52 40.28 45.48 1.48 33.07 32.73 47.57 31.34 9.46 47.83 54.69 21.27 16.06 15.62 20.49 48.70 48.44 55.73 22.05 24.57 23.26 11.63 35.42 21.44 25.00 35.85 44.53 21.35 44.33 42.45 21.09 Multimodal Agentic Framework (Ours) Multimodal Agentic Framework (Ours) ChartAgent 94.33 60.81 70.91 91. 71.39 ChartAgent 84.84 44.16 55.93 69. 59.69 (a) ChartBench (76.2% unannotated charts; 96.7% numeric QA) (b) ChartX (61.7% unannotated; 71.9% numeric QA) ditionally include recently released models whose knowledge cutoffs are later than the dataset release or whose launch dates are concurrent with ours: GPT-o3/o4-mini/4.1/5/5-mini, Gemini 2.0 Flash, Claude 3.7 Sonnet/3.5 Sonnet/3.5 Haiku, and Mistral. We compare zero-shot and Chain-of-Thought (CoT) prompting; see App. L.2 for the corresponding prompts. Further details in App. and Table 4. Evaluation Metrics We use accuracy as the primary evaluation metric, computed via two-step procedure. First, GPT-4o standardizes both the models response and the ground truthstripping units (e.g., for million, for billion), converting scales, removing symbols, and formatting numbers consistently (see App. H). If responses are numeric, we then apply an arithmetic correctness check with strict 5% relative error tolerance; for non-numeric responses, we perform an exact string match after standardization. Prior work often uses the LLM-as-a-Judge paradigm (Masry et al., 2023, 2022; Xia et al., 2024; Xu et al., 2023), but we find it suboptimal for numerically precise answers under 5% tolerance, as LLMs may inconsistently Figure 3: (a) Left: ChartAgent vs. concurrent works: overall accuracy () and average absolute error (). (b) Right: Effectiveness of visual self-verification: enabled 70% successful recoveries when invoked. enforce thresholds or miss small deviations (see App. J.5). See App. L.3 for evaluation prompts."
        },
        {
            "title": "5.1 Performance",
            "content": "Comparison to State-of-the-art Table 1 presents comparative analysis of ChartAgent against 32 baselines on the ChartBench and ChartX benchmarks, stratified by annotation status and QA type. ChartAgent consistently outperforms all competTable 2: Accuracy on unannotated charts (%) by chart type. Red: Best, Blue: Second best. Abbreviations: Over: Overlay Stack: Stacked Mul: Multi Sing: Single Hor: Horizontal Vert: Vertical B-L: Bar-Line L-L: Line-Line Dir: Directed Undir: Undirected Combo: Combination. See App. for examples of each chart type. Model Area Horizontal Bar 3D Bar Vertical Bar Box Combo Line Node Pie Radar Scatter Avg. Over Stack Mul Sing Stack Mul Stack Mul Sing Stack Hor Vert Stock B-L L-L Mul Sing Dir Undir Mul Ring Sector Mul Fill Sing 3D Proprietary Multimodal Large Language Models GPT 4o Gemini 1.5 21.0 5.0 18.0 4.0 24.0 59.0 28.0 52.0 10.0 7. 20.0 14.0 6.0 4.0 38.0 73.0 39.05 49.0 12.0 5.0 20.0 26.0 13.0 18.0 63.0 24. Open-weights Multimodal Large Language Models 35.0 41.0 37.0 75.0 91.0 7.0 28.0 91.0 91.0 48.0 59.26 5.0 3.0 1.0 32.0 14. 34.0 29.52 22.0 20.0 7.0 1.0 6.0 0.0 63.0 45.0 36.15 27.27 DeepSeek-VL2 InternVL3 LLama3.2 Phi3-vision Pixtral Qwen2VL 29.0 25.0 46.0 27.0 26.0 57.0 Chart-related Models 11.0 16.0 21.0 37.0 10.0 18.0 25.0 57.0 45.0 80.0 58.0 91.0 43.0 78.0 25.0 51.0 87.0 97.0 8.0 19.0 11.0 8.0 6.0 17.0 36.0 38.0 31.0 40.0 30.0 40. 8.0 1.0 4.0 7.0 5.0 7.0 58.0 44.0 71.0 86.0 39.0 94.0 82.0 80.0 89.0 92.0 89.0 97.0 13.0 16.0 10.0 30.0 10.0 24.0 11.0 3.0 16.0 23.0 6.0 6.0 9.0 15.0 16.0 29.0 4.0 13.0 51.0 60.0 49.0 48.0 39.0 64. 8.0 46.0 48.0 51.0 31.0 27.0 24.0 30.0 56.0 62.0 42.0 46.0 63.0 87.0 42.0 31.0 55.0 66.0 84.0 39.0 19.0 24.0 17.0 32.0 68.0 37.0 46.0 80.0 85.0 80.0 36.0 52.0 58.0 51.0 59.0 86.0 0.0 0.0 5.0 2.0 2.0 1.0 6.0 2.0 4.0 14.0 21.0 12.0 15.0 9.0 25.0 21.0 28.0 9. 5.0 13.0 21.0 24.0 24.0 6.0 8.0 17.0 10.0 11.0 26.0 66.0 8.0 9.0 13.0 9.0 11.0 9.0 44.0 25.0 46.0 73.0 72.0 47.0 30.31 30.92 36.38 40.77 28.73 43.50 DePlot TinyChart 18.0 32.0 2.0 22. 43.0 74.0 71.0 88.0 13.0 13.0 34.0 37.0 9.0 15.0 66.0 76.0 78.0 82. 7.0 21.0 20.0 20.0 3.0 2.0 0.0 4.0 48.0 45.0 14.0 63.0 84.0 46.0 50.0 51.0 91.0 22.0 73.0 35.0 4.0 1. 3.0 20.0 5.0 21.0 2.0 10.0 2.0 8.0 3.0 4.0 2.0 27. 28.15 32.77 Multimodal Agentic Framework (Ours) ChartAgent 30.0 38.0 79.0 76. 82.0 20.0 6.0 88.0 88.0 76. 89.0 83.0 64.0 67.0 65.0 63.0 81.0 91.0 91.0 18.0 94.0 80. 22.0 20.0 6.0 64.0 60.81 ing methods, showing particularly strong gains on unannotated charts and numeric QAthe dominant categories across both datasets. On ChartBench, ChartAgent achieves 71.39% overall accuracy, +16.07% absolute gain over the secondbest model (Phi-3 Vision), including 60.81% on unannotated charts (+17.31% over Qwen2-VL) and 70.91% on numeric QA (+15.02% over Phi-3 Vision). similar trend is observed on ChartX, where ChartAgent attains 59.69% overall accuracy (+2.83% absolute gain over GPT-4o), with top scores on unannotated (44.16%) and numeric QA (55.93%). Furthermore, Figure 3(a) and App. Table 14 present results comparing ChartAgent with 10 additional concurrent works on newly curated dataset designed to ensure fair comparison and mitigate potential data leakage (see App. J.6). ChartAgent outperforms all concurrent models by significant margin, achieving +10.48% absolute accuracy gain over the second-best model (GPT5) and 5.72-point reduction in average absolute error relative to GPT-o3. Overall, these results establish ChartAgent as the new SoTA in Chart VQA, with major gains in numeric QA on unannotated charts, highlighting the value of visually grounded agentic reasoning for charts. Performance by Chart Type Table 2 compares ChartAgent with the top-10 baselines on unannotated charts, stratified by chart type on ChartBench (see App. J.1 for the full table and ChartX results). On ChartBench, ChartAgent achieves the largest gains on Bar (particularly horizontal and stacked variants, up to +65%), Box (up to +69%), Combination (Bar-Line, Multi-Line, up to +23%), and Pie (Ring, Sector, up to +62%) charts, while performance is weaker on 3D and Radar charts due to challenges in handling slanted, depth-distorted axes and radial coordinate structures. On ChartX, major gains are observed for Bubble, Ring, and Treemap charts, whereas MultiAxes and Radar charts remain the most challenging. Overall, these results underscore ChartAgents robustness across wide range of chart types."
        },
        {
            "title": "5.2 Effectiveness of ChartAgent",
            "content": "Performance Across Visual and Reasoning Complexity Levels We analyze ChartAgents performance across difficulty levels, stratified by (1) the visual complexity of charts and (2) the reasoning complexity of chartQA pairs, each categorized into three levels: Easy, Medium, and Hard. Visual complexity reflects the perceptual effort required to interpret chart, while reasoning complexity measures the depth of reasoning needed to answer question. See App. for details and statistics, and App. L.4 for corresponding prompts. Figure 4(a) compares ChartAgent with the top-10 baselines on unannotated charts, stratified by these complexity levels on ChartBench (see App. J.4 for full results). All models show consistent decline from Easy to Hard across both dimensions, confirming that visual clutter and multi-step reasoning increase Chart VQA difficulty. ChartAgent achieves the best performance at all levels except visually Hard, with notable gains on visually Easy (+18%) and Medium (+20.1%) charts, and reasoning Easy (+21.2%) and Medium (+20.8%) tasks. Visually Hard charts (17.9%) remain challenging due to 3D, radar, and overlapping structures that obscure segment boundaries and axis references. However, on reasoning Hard tasks involving multi-step numerical reasoning, ChartAgent still delivers +6.9% gain, underscoring its robustness across most difficulty levels. These results demonstrate ChartAgents strong generalization across varying visual and reasoning complexities in chartQA pairs. Figure 4: Analysis of ChartAgent Performance. (a) Left: Stratified by visual complexity of charts and reasoning complexity of chartQA pairs on unannotated charts, compared with top-10 SoTA. (b) Middle: ChartAgent performance on unannotated+numeric chartQA when instantiated with different base MLLMs. (c) Right: Ablation study comparing ChartAgent with ReAct using no tools and ReAct with natural imagebased generic tools. Plug-and-Play Generalization Across MLLMs ChartAgent follows plug-and-play design, enabling seamless integration with any MLLM to provide chart-specialized, visually grounded reasoning. To assess generalization beyond GPT-4o as the base MLLM, we evaluate ChartAgent with three additional models: GPT-4o-mini, Claude 3 Haiku, and Pixtral, covering both closedand open-source variants. Figure 4(b) compares the performance of ChartAgent+Base MLLM versus the Base MLLM alone on unannotated and numeric Chart VQA. ChartAgent consistently outperforms its corresponding base models, yielding absolute accuracy gains of +26.7% on GPT-4o, +23.9% on GPT-4omini, +28.4% on Claude 3 Haiku, and +12.2% on Pixtral. Thus, ChartAgent serves as an effective plug-and-play framework that enhances performance across diverse MLLMs, demonstrating both robustness and generalization."
        },
        {
            "title": "5.3 Additional Analysis",
            "content": "Effectiveness of Visual Self-Verification and Recovery We evaluated ChartAgents ability to detect unsatisfactory tool outputs and recover using its visual self-verification mechanism. Figure 3(b) and App. Table 15 summarize these results. Across 30 randomly sampled trajectories from ChartBench, tool outputs were correct and required no recovery in 50% of cases. In the remaining 50%, ChartAgent correctly flagged unsatisfactory outputs and triggered its self-verification mechanism, recovering successfully 70% of the time and failing 30%, with the latter contributing to 15% overall error rate attributable to unresolved toollevel failures. Thus, ChartAgents visual self-verification mechanism is both frequently invoked and often effective, enhancing its robustness in the presence of imperfect tool outputs. ages and rely on generic tools such as cropping and zooming (Zheng et al., 2025; Su et al., 2025; Jegham et al., 2025; Hu et al., 2024b; Gupta and Kembhavi, 2023; Surís et al., 2023). While effective for object localization or text spotting, these tools lack the fine-grained capabilities required for structured, quantitative reasoning in charts, motivating the need for chart-specialized visual tools. We compare three ReAct-style agents, all using GPT-4o as the base MLLM with visual selfverification: (i) ReAct (No Tools), (ii) ReAct + Natural Image Tools, with generic natural-image operations, and (iii) ChartAgent. Figure 4(c) shows that ChartAgent outperforms both variants by +32.6% over ReAct (No Tools) and +30.0% over ReAct + Image Tools overall, and by +38.8% and +37.8% respectively on the unannotated + numeric subset. These findings highlight the limitations of generic tools and the necessity of chart-specialized visual grounding. See App. J.3 for further details. Fallback Behavior and Common Triggers We conducted manual analysis of 30 randomly selected ChartBench trajectories (unannotated, numeric QA) to understand when and why ChartAgent reverts to the base MLLM (i.e., GPT4o). The fallback rate was relatively low (below 10%) and was typically triggered by: (1) bar charts with negative or axis-inconsistent bar-height estimates; (2) OCR tools returning None for legends or axis labels; and (3) edge-point detection or interpolation tools producing empty or axis-inconsistent outputs. In such cases, the agent identified toolbased reasoning as unreliable and reverted to the base MLLM, rare but effective fail-safe mechanism that helps maintain robustness. See App. J.8 for further details on fallback behavior."
        },
        {
            "title": "5.4 Failure Mode Analysis",
            "content": "Ablation Study Prior frameworks for visually grounding MLLMs primarily focus on natural imWe conducted failure mode analysis to identify common errors in ChartAgent, which fall into two main categories: (1) Perception-based failures. These stem from visual misinterpretations such as: (1.1) OCR obstruction from overlays or dense elements; (1.2) Poor color contrast (e.g., white text on yellow background); (1.3) Legend occlusion over key regions; (1.4) Element invisibility where lines or markers blend with background; (1.5) Segmentation errors caused by axis lines overlapping chart elements; (1.6) Overlapping series obscuring category distinctions; and (1.7) Axis interpretation issues in 3D or multi-axis charts with distorted or inconsistent scales across multiple axis. (2) Reasoning-based failures. (2.1) Incorrect tool choice (e.g., using area instead of height); (2.2) Ambiguous queries (e.g., missing denominators in multi-ring pies); (2.3) Label duplication across hierarchy levels (e.g., Netflix as both parent and child); and (2.4) Subtype confusion between visually similar area-chart variants. See App. and Figures 12a, 12b for visual examples. Most failures are perception-driven, originating from tool-level errors rather than high-level reasoning or planning."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduced ChartAgent, novel multimodal agentic framework for visually grounded reasoning in charts. Inspired by human cognitive strategies of iterative reasoning and annotation-based visual comprehension, ChartAgent employs multi-turn, tool-augmented interaction loop to achieve SoTA performance on well-established benchmarks spanning 40+ chart types, surpassing 40+ baselines with particularly strong gains on unannotated charts and numeric QA. Comprehensive analyses demonstrate its robustness across varying visual and reasoning complexity levels, its plug-and-play generalization across different MLLMs, and the effectiveness of each system component, supported by detailed failure mode analysis."
        },
        {
            "title": "7 Limitations and Broader Perspective",
            "content": "Limitations and future work: The current approach only handles the question-answering task and can be extended to additional tasks such as summarization, description, and fact-checking. Evaluation has been limited to single charts without context; future work can extend this to charts with context, slides, and multiple charts. We evaluate only with GPT-4o, the current SOTA model for chart QA, but in the future, the agentic framework we propose can be integrated with any multimodal LLM to enhance its perceptive ability for chart understanding, making it agnostic to new model releases. The in-context learning (ICL) examples for the agent are entirely textual and not multimodal. Incorporating multi-modal ICLsuch as imagescould potentially improve the agents accuracy. However, this would also increase the context length of the prompt, introducing trade-off between richer supervision and computational overhead. Future work should explore multi-modal ICL while carefully considering the implications of longer context lengths on performance and efficiency. Additionally, while we adhered to established benchmarks for consistency, we did not evaluate our method on newly constructed or strictly held-out dataset to check for potential data leakage into the OpenAI or Claude models. Another limitation is the monetary cost associated with using OpenAIs or Claudes models, which we discuss in more detail in Appendix J.10. Broader perspective: Prior work has highlighted the new and unpredictable risks associated with using automated agents in sensitive contexts (Wright, 2024). We advise against using this framework or MLLM agents to automate critical chartor imagerelated tasks without human oversight. Additionally, the resources accompanying this study will be responsibly released for research purposes only. Datasets and code: The benchmarks used in this study are publicly available and were curated by previous research. Specifically, we include the following datasets: ChartBench (Xu et al., 2023), ChartX (Xia et al., 2024), and ChartQAunannotated (Islam et al., 2024). We abide by their terms of use. To support reproducibility and facilitate future research, we will make the resources associated with this study available upon acceptance."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to thank David Westera of J.P. Morgan AI Research for his valuable discussions and feedback on this work."
        },
        {
            "title": "Disclaimer",
            "content": "This paper was prepared for informational purposes by the Artificial Intelligence Research group of JPMorgan Chase & Co and its affiliates (\"J.P. Morgan\") and is not product of the Research Department of J.P. Morgan. J.P. Morgan makes no representation and warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful."
        },
        {
            "title": "References",
            "content": "AutoGPT. Significant-Gravitas/Auto-GPT. 2025-05-01. https://github.com/ Accessed: CrewAI. https://github.com/crewAIInc/crewAI. Accessed: 2025-05-01. EasyOCR. EasyOCR. Accessed: 2025-05-01. https://github.com/JaidedAI/ LangChain. https://github.com/langchain-ai/ langchain. Accessed: 2025-05-01. LangGraph. https://github.com/langchain-ai/ langgraph. Accessed: 2025-05-01. Tesseract OCR. tesseract-ocr/tesseract. 05-01. https://github.com/ 2025Accessed: Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Qin Cai, Martin Cai, Caio César Teodoro Mendes, Weizhu Chen, and 96 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, and 1 others. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, and 1 others. 2024. Pixtral 12b. arXiv preprint arXiv:2410.07073. Jaided AI. 2020. Easyocr. https://github.com/ JaidedAI/EasyOCR. Anthropic. 2024a. The claude 3 model family: Opus, sonnet, haiku. Anthropic. 2024b. Model card addendum: Claude 3.5 haiku and upgraded claude 3.5 sonnet. Accessed: 2025-07-09. Anthropic. 2025. Claude 3.7 sonnet system card. Accessed: 2025-07-09. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: versatile vision-language model for understanding, localizaarXiv preprint tion, arXiv:2308.12966. text reading, and beyond. Jinyue Chen, Lingyu Kong, Haoran Wei, Chenglong Liu, Zheng Ge, Liang Zhao, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. 2024. Onechart: Purify the chart structural extraction via one auxiliary token. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 147155. Jawad Chishtie, Iwona Anna Bielska, Aldo Barrera, Jean-Sebastien Marchand, Muhammad Imran, Syed Farhan Ali Tirmizi, Luke Turcotte, Sarah Munce, John Shepherd, Arrani Senthinathan, and 1 others. 2022. Interactive visualization applications in population health and health services research: systematic scoping review. Journal of medical Internet research, 24(2):e27534. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Preprint, arXiv:2305.06500. Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, and 40 others. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. Preprint, arXiv:2406.12793. Google. 2025. Gemini 2.0 flash model card. Accessed: 2025-07-09. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Tanmay Gupta and Aniruddha Kembhavi. 2023. Visual programming: Compositional visual reasoning without training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1495314962. 2024. GPT4o-mini. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/. Accessed: 2025-05-01. Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023. Chartllama: multimodal llm for chart understanding and generation. Preprint, arXiv:2311.16483. Nidhal Jegham, Marwan Abdelatti, and Abdeltawab Hendawi. 2025. Visual reasoning evaluation of grok, deepseek janus, gemini, qwen, mistral, and chatgpt. arXiv preprint arXiv:2502.16428. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. Webvoyager: Building an end-toend web agent with large multimodal models. arXiv preprint arXiv:2401.13919. Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, and 1 others. 2024. Metagpt: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations. Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. Cogagent: visual language model for gui agents. Preprint, arXiv:2312.08914. Chiori Hori, Motonari Kambara, Komei Sugiura, Kei Ota, Sameer Khurana, Siddarth Jain, Radu Corcodel, Devesh Jha, Diego Romeres, and Jonathan Le Roux. 2025. Interactive robot action replanning using multimodal llm trained from human demonstration videos. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and 1 others. 2024a. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding. arXiv preprint arXiv:2403.12895. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. 2024b. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Muye Huang, Lai Han, Xinyu Zhang, Wenjun Wu, Jie Ma, Lingling Zhang, and Jun Liu. 2024. Evochart: benchmark and self-training approach towards arXiv preprint real-world chart understanding. arXiv:2409.01577. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. 2018. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656. Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. 2017. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, and 1 others. 2023. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. 2024. VisualWebArena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 881905, Bangkok, Thailand. Association for Computational Linguistics. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326. Li, Zhang, Sun, Zou, Liu, Yang, Li, Zhang, and Gao. Semantic-sam: Segment and recognize anything at any granularity. arxiv 2023. arXiv preprint arXiv:2307.04767. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, and 1 others. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR. Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, and Enamul Hoque. 2024. Are large vision language models up to the challenge of chart comprehension and reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 33343368, Miami, Florida, USA. Association for Computational Linguistics. Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. 2025. Draw-andunderstand: Leveraging visual prompts to enable MLLMs to comprehend what you want. In The Thirteenth International Conference on Learning Representations. Fangyu Liu, Julian Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. 2023a. DePlot: One-shot visual language reasoning by plot-to-table translation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1038110399, Toronto, Canada. Association for Computational Linguistics. Fangyu Liu, Francesco Piccinno, Syrine Krichene, Chenxi Pang, Kenton Lee, Mandar Joshi, Yasemin Altun, Nigel Collier, and Julian Eisenschlos. 2023b. MatCha: Enhancing visual language pretraining with math reasoning and chart derendering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1275612770, Toronto, Canada. Association for Computational Linguistics. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. 2024a. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 12871310. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023c. Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b. Llavanext: Improved reasoning, ocr, and world knowledge. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023d. Visual instruction tuning. Advances in neural information processing systems, 36:34892 34916. Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, and 1 others. 2025. Llava-plus: Learning to use tools for creating multimodal agents. In European Conference on Computer Vision, pages 126142. Springer. Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. 2021. Chartocr: Data extraction from charts images via deep hybrid framework. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 19171925. Andrés Marafioti, Orr Zohar, Miquel Farré, Merve Noyan, Elie Bakouch, Pedro Cuenca, Cyril Zakka, Loubna Ben Allal, Anton Lozhkov, Nouamane Tazi, Vaibhav Srivastav, Joshua Lochner, Hugo Larcher, Mathieu Morlon, Lewis Tunstall, Leandro von Werra, and Thomas Wolf. 2025. Smolvlm: Redefining small and efficient multimodal models. Preprint, arXiv:2504.05299. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. ChartQA: benchmark for question answering about charts with visual and logical reasoning. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2263 2279, Dublin, Ireland. Association for Computational Linguistics. Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. 2023. UniChart: universal vision-language pretrained model for chart comprehension and reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1466214684, Singapore. Association for Computational Linguistics. Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. 2024. ChartInstruct: Instruction tuning for chart comprehension and reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1038710409, Bangkok, Thailand. Association for Computational Linguistics. Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. 2025. ChartGemma: Visual instruction-tuning for chart reasoning in the wild. In Proceedings of the 31st International Conference on Computational Linguistics: Industry Track, pages 625643, Abu Dhabi, UAE. Association for Computational Linguistics. Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. 2020. Plotqa: Reasoning over scientific plots. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 15271536. MistralAI. 2025. Mistral small 3. Srija Mukhopadhyay, Adnan Qidwai, Aparna Garimella, Pritika Ramu, Vivek Gupta, and Dan Roth. 2024. Unraveling the truth: Do vlms really understand charts? deep dive into consistency and robustness. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1669616717. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, and 1 others. 2024. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. In Forty-first International Conference on Machine Learning. OpenAI. 2025a. Introducing gpt-4.1 in the api. Accessed: 2025-07-09. OpenAI. 2025b. Openai o3 and o4-mini system card. Accessed: 2025-07-09. Yasaman Razeghi, Ishita Dasgupta, Fangyu Liu, Vinay Venkatesh Ramasesh, and Sameer Singh. 2024. Plot twist: Multimodal models dont comprehend simple chart details. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 59225937, Miami, Florida, USA. Association for Computational Linguistics. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551. Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. 2024. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Ray Smith. 2007. An overview of the tesseract ocr In ICDAR 07: Proceedings of the Ninth engine. International Conference on Document Analysis and Recognition, pages 629633, Washington, DC, USA. IEEE Computer Society. Archita Srivastava, Abhas Kumar, Rajesh Kumar, and Prabhakar Srinivasan. 2025. Enhancing financial vqa in vision language models using intermearXiv preprint diate structured representations. arXiv:2501.04675. Zhaochen Su, Peng Xia, Hangyu Guo, Zhenhua Liu, Yan Ma, Xiaoye Qu, Jiaqi Liu, Yanshu Li, Kaide Zeng, Zhengyuan Yang, and 1 others. 2025. Thinking with images for multimodal reasoning: Foundations, methods, and future frontiers. arXiv preprint arXiv:2506.23918. Dídac Surís, Sachit Menon, and Carl Vondrick. 2023. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11888 11898. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, and 1 others. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, and 1 others. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Tucker Balch, and Manuela Veloso. 2024. Adaptagent: Adapting multimodal web agents with few-shot learning from human demonstrations. arXiv preprint arXiv:2411.13451. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. 2024a. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. Preprint, arXiv:2409.12191. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023. Cogvlm: Visual expert for pretrained language models. Preprint, arXiv:2311.03079. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, and 1 others. 2024b. Charxiv: Charting gaps in realistic chart understandIn The Thirty-eight Coning in multimodal llms. ference on Neural Information Processing Systems Datasets and Benchmarks Track. Webb Wright. 2024. Ai agents with more autonomy than chatbots are coming. some safety experts are worried. Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen White, Doug Burger, and Chi Wang. 2024a. Autogen: Enabling next-gen LLM applications via multi-agent conversations. In First Conference on Language Modeling. Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, and Yuyu Luo. 2024b. Chartinsights: Evaluating multimodal large language models for low-level chart question answering. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1217412200. Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, and 8 others. 2024c. Deepseek-vl2: Mixture-of-experts visionlanguage models for advanced multimodal understanding. Preprint, arXiv:2412.10302. Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, and 1 others. 2024. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, and 1 others. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972. Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. 2023. Chartbench: benchmark for complex visual reasoning in charts. arXiv preprint arXiv:2312.15915. Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023a. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441. Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. 2023b. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint arXiv:2303.11381. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. 2024. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. Preprint, arXiv:2408.04840. Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. 2024. Tinychart: Efficient chart understanding with program-of-thoughts learning and visual token merging. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 18821898. Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. Gpt-4v (ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. 2025. Deepeyes: Incentivizing\" thinking with images\" via reinforcement learning. arXiv preprint arXiv:2505.14362. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, Yue Cao, Yangzhou Liu, Xingguang Wei, Hongjie Zhang, Haomin Wang, Weiye Xu, and 32 others. 2025. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. Preprint, arXiv:2504.10479."
        },
        {
            "title": "Table of Contents",
            "content": "A Annotated vs. Unannotated Charts"
        },
        {
            "title": "C Datasets",
            "content": "15"
        },
        {
            "title": "D Chart Types Supported in ChartAgent",
            "content": ""
        },
        {
            "title": "H Examples of Response Standardization",
            "content": "for Accuracy Evaluation"
        },
        {
            "title": "L Prompts",
            "content": "21 22 24 27 27 35 37 A.1 Examples Figure 5 illustrates this distinction, showing representative examples of both annotated and unannotated charts from the datasets. A.2 How ChartAgent Handles Annotated vs."
        },
        {
            "title": "Unannotated Charts",
            "content": "Given chart, ChartAgent first classifies it as annotated or unannotated using an LLM-based orchestrator (e.g., GPT-4o). On uniformly sampled subset, this classification step achieves 100% accuracy. ChartAgent dynamically adapts its execution pathway based on the annotation type. For annotated chartswhere text extraction alone is sufficientthe agent directly forwards the query to the base model (e.g., GPT-4o), which already achieves over 90% accuracy (see Table 1). This approach ensures both high performance and computational efficiency. For unannotated charts, however, ChartAgent triggers its full ReAct-style loop. Here, the agents iterative reasoning and specialized visual tool use become essential to accurately extract values and answer queries, as detailed in Section 3."
        },
        {
            "title": "B Related Work",
            "content": "A Annotated vs. Unannotated Charts B.1 Chart Visual Question Answering An annotated chart contains explicit textual annotations or shortcuts. For instance, in bar charts, exact values may be printed above or inside the bars; in pie charts, percentage labels may appear alongside slices. In some cases, answers to questions may even be embedded in the title or legend. Generally, an annotated chart includes values visibly placed near the relevant graphical elements, though the information may also appear elsewhere within the chart image, such as in captions or legends. These textual cues allow models like GPT-4o to directly extract information from the image, often producing correct answers without requiring complex visual reasoning. In contrast, unannotated charts lack such explicit value indicators. Consequently, the model must infer values by interpreting graphical featuressuch as bar heights, pie slice angles, or positions along axes. These tasks demand finegrained visual perception and structured reasoning, often exceeding the capabilities of general-purpose LLMs or MLLMs alone. Chart visual question answering (Chart VQA) aims to automatically interpret visual charts to answer natural-language queries. Early datasets such as FigureQA (Kahou et al., 2017) and DVQA (Kafle et al., 2018) introduced synthetic charts designed to evaluate specific reasoning skills but lacked real-world diversity. This gap was subsequently addressed by more comprehensive datasets like PlotQA (Methani et al., 2020), ChartQA (Masry et al., 2022), and EvoChart (Huang et al., 2024), which incorporated complex, real-world charts coupled with natural-language queries. Recent benchmarks such as ChartBench (Xu et al., 2023), ChartX (Xia et al., 2024), and CharXiv (Wang et al., 2024b) have further expanded the complexity and diversity of tasks, covering wide range of chart types and numeric-intensive queries. These benchmarks reflect growing trend toward datasets that demand sophisticated visual comprehension combined with nuanced quantitative reasoning. Advancements in chart-focused multimodal large language models (MLLMs) (Zhang et al., 2024; Masry et al., 2023; Han et al., 2023; Wu et al., 2024b; Mukhopadhyay et al., 2024; Liu Figure 5: Examples of annotated (top) vs. unannotated (bottom) charts. An annotated chart contains explicit textual annotations or shortcuts, whereas an unannotated chart lacks such explicit value indicators. For instance, in the first column (top), the bar chart includes printed bar values, while in the corresponding bottom chart, the values must be inferred through visual interpretation. et al., 2024a; Masry et al., 2024) have demonstrated notable progress by leveraging instructiontuned datasets and vision-language alignment methods. Alternatively, ChartOCR (Luo et al., 2021) combines computer vision tools and rule-based techniques, such as keypoint detection and chartspecific rules, for enhanced chart understanding. However, recent studies (Xu et al., 2023; Razeghi et al., 2024; Islam et al., 2024) reveal persistent limitations, particularly in precise numerical interpretation tasks involving unannotated chartsvisualizations lacking textual shortcuts such as numeric annotations or labels. In particular, (Xu et al., 2023) showed significant performance drop when transitioning from annotated charts (containing textual cues) to unannotated charts, highlighting models dependence on optical character recognition (OCR) rather than genuine visual reasoning. Addressing this limitation requires enhanced visual grounding capabilities that enable accurate interpretation and numerical reasoning directly from graphical elements (e.g., bar heights, segment areas). Our approach specifically targets this challenge by enhancing MLLMs with modular, specialized vision tools tailored explicitly to the chart domain, thereby significantly improving visual reasoning and grounding in Chart VQA. B.2 General-Purpose Multimodal LLMs and"
        },
        {
            "title": "Visual Grounding",
            "content": "While recent chart-specific multimodal models have made notable progress, broader developments in general-purpose multimodal large language models (MLLMs)such as GPT-4 (Achiam et al., 2023), GPT-4o (Hurst et al., 2024), Gemini (Team et al., 2023), and LLaVA (Liu et al., 2023d), Visual CoT (Shao et al., 2024)have significantly advanced general visual reasoning and understanding across various tasks and domains. However, these general-purpose MLLMs also face challenges when tasks demand precise visual grounding and fine-grained interpretation of visual information. To address these limitations, recent approaches have explored augmenting language and multimodal models with external tools or visual prompting. For instance, ToolFormer (Schick et al., 2023) integrates text-based language models with external APIs, demonstrating improved reasoning through external knowledge retrieval. Simiarly, Visual ChatGPT (Wu et al., 2023) and MMReAct (Yang et al., 2023b) enhance text-only ChatGPT with vision expert tools for multimodal tasks. For MLLMs, ViperGPT (Surís et al., 2023) and VisProg (Gupta and Kembhavi, 2023) generate executable code via LLMs to perform sequences of tool invocations, though their execution follows fixed plan without flexibility for dynamic adaptation based on intermediate tool outcomes. In contrast, methods like Visual Sketchpad (Hu et al., 2024b) explicitly incorporate intermediate visual results into iterative reasoning, enabling dynamic refinement of action plans based on observed outcomes. Additionally, visual prompting methods such as Set-of-Marks (SoM) (Yang et al., 2023a) augment input images with visual annotations (e.g., bounding boxes or segmentation masks), providing richer context to LLMs for informed reasoning. Inspired by SoM, our approach similarly presents the multimodal agent with explicit visualizations of intermediate tool outputs, enabling visual inspection and informed decision-making at each reasoning step. Motivated by these advancements, our work extends multimodal LLM capabilities specifically into the chart domain, combining iterative reasoning, dynamic visual prompting, and modular external tools. Unlike fixed-sequence approaches, our framework enables adaptive replanning and precise visual grounding, effectively addressing complex chart interpretation tasks. B.3 Agentic Frameworks The concept of agentsentities capable of perception, cognition, and actionhas long been foundational in artificial intelligence research. Traditional agents perceive their environment, reason about possible actions, and execute these actions to achieve specific goals. Recent advances in large language models (LLMs) have inspired new generation of LLM-based agents, leveraging powerful reasoning capabilities and dynamic interactions with external tools. notable example of aligning LLM reasoning explicitly with the agent paradigm is the ReAct framework (Yao et al., 2023), which organizes model interactions into iterative cycles of reasoning (cognition), action execution (action), and observing results (perception). This structured loop allows LLM-based agents to refine their decisions dynamically, closely mirroring traditional agent definitions. Several software frameworks and platforms now support the practical implementation of LLMbased agents, enabling seamless integration of external tool usage within iterative reasoning loops. Examples include AutoGen (Wu et al., 2024a), CrewAI (cre), LangChain (Lan), LangGraph (lan), and AutoGPT (aut), each providing flexible infrastructures to orchestrate sophisticated LLM-driven workflows. Extending this agentic paradigm into multimodal settings has further expanded agent capabilities across diverse applications. Multimodal agents effectively handle tasks in software engineering (Jimenez et al., 2024; Hong et al., 2024), robotics (Nasiriany et al., 2024), general visionlanguage reasoning (Liu et al., 2025; Yang et al., 2023b), and GUI navigation (Xie et al., 2024; Koh et al., 2024; Zheng et al., 2024; Verma et al., 2024). These frameworks dynamically combine visual perception with iterative LLM reasoning, adjusting action plans based on multimodal feedback. Chart VQA introduces unique challenges that specifically require chart-oriented perception and numeric reasoning capabilities. Our proposed ChartAgent explicitly adopts the ReAct agentic framework (Yao et al., 2023), integrating iterative multimodal reasoning with carefully designed modular perception tools specifically tailored for chart understanding tasks. The practical implementation of our agent leverages AutoGen (Wu et al., 2024a), providing flexible infrastructure for orchestrating dynamic interactions between the multimodal LLM and external tools, enabling effective iterative refinement and visual grounding."
        },
        {
            "title": "C Datasets",
            "content": "To evaluate our agents ability to understand charts, we design experiments that require complex visual reasoning, specifically focusing on question answering over unannotated charts, where accurate numerical interpretation and output precision are critical. We evaluate ChartAgent on two well-established and widely used chart QA benchmarks: ChartBench (Xu et al., 2023) and ChartX (Xia et al., 2024). These benchmarks are visually groundedmodels must interpret the visual logic of the chart to answer questions, without relying solely on OCR. They are designed to assess chart comprehension and data reliability through complex reasoning, and the majority of their charts are unannotated (see Appendix A), making them ideal for testing visual understanding. C.1 ChartBench ChartBench (Xu et al., 2023) comprises charts from 9 major categories and 42 subcategories, with unannotated charts present across all 9 categories and over 75% of images being unannotated. It includes both regular chart types (line, bar, pie) and diverse, complex types such as area, box, radar, scatter, node, and combination charts (e.g., bar+line, bar+pie). The test set originally contained 2,100 images (50 per subcategory), but we discarded 4 subcategories with corrupted or incorrect ground-truth labels, yielding final set of 1,900 images. We use two subsets of the ChartBench QA data: Numeric Question Answering (NQA) and Value Extraction (VE), resulting in 3,800 imageQA pairs. ChartBench includes two primary types of questions: 1) Numeric QA questions requiring precise numerical extraction (e.g., What is the value of India in 2021? or How much more is than B?); 2) Relationship QA questions involving relational understanding (e.g., Is node connected to node B? or Is node directed toward node B?). C.2 ChartX ChartX (Xia et al., 2024) comprises charts from 18 categories, including regular types such as line, bar, and pie charts, as well as fine-grained and domainspecific charts such as ring charts, radar charts, box plots, 3D-bar charts, histograms, treemaps, rose charts, bubble charts, multi-axes charts, area charts, heatmaps, funnels, and candlestick charts. The dataset includes 1,152 image-question pairs in the test set, with more than 60% of the images being unannotated. ChartX includes two primary types of questions: 1) Numeric QA questions that require precise numerical extraction; 2) Value Comparison and Global Perception QA questions that require relative or extremum-based reasoning (e.g., identifying the highest, lowest, or most relevant entity), where exact values are not necessary. Examples of global perception questions include: Which country has the highest GDP?, Which region planted the most trees?, Are there more trees planted in 2021 in region or region B? It is important to note that ChartX is much harder dataset, both in terms of questions and chart samples. The questions are more varied and open-ended; for example, How many countries have CO2 emissions greater than or equal to 350 million metric tons? and How many nonprofits received donations in the range of 50K to 100K? require computing all entries and then applying careful numeric filtering, which increases error susceptibility. The chart samples themselves are also more challenging: significant fraction are occluded charts, where legends often overlap bars or chart elements of interest; many multi-axis plots involve three or more Y-axes; and in some cases grid lines are the same color as the bar or the box in box plots, making it difficult to distinguish regions of interest even after segmentation. Overall, ChartX presents substantially more challenging testbed. C.3 Dataset Statistics Table 3 presents the chart type, annotation, and QA type distribution across the two evaluation datasets, ChartBench and ChartX. key observation is the dominance of unannotated charts, which constitute over 76% of ChartBench and over 61% of ChartX. As discussed in Appendix A, such unannotated samples require visual extraction of values from chart elements rather than relying on textual annotations or shortcuts, thereby posing greater difficulty. Another important characteristic is the prevalence of numeric QA, comprising more than 94% in ChartBench and nearly 72% in ChartX. Taken together, these properties underscore that both datasets serve as rigorous testbeds for evaluating chart reasoning systems under visually demanding and numerically intensive conditions. Note that we did not use the popular ChartQA (Masry et al., 2022) dataset, as all charts are annotated and MLLM performance on it already exceeds 85% due to strong OCR capabilities. We also excluded the CharXiv (Wang et al., 2024b) dataset, as it lacks numerically precise questions and primarily focuses on descriptive and reasoningbased queries. Thus, ChartBench and ChartX were selected for evaluation as they emphasize unannotated charts and require models to demonstrate true visual understanding and numerical reasoning beyond text extraction. See Appendix for visualizations of the diverse chart types included in our benchmark datasets."
        },
        {
            "title": "D Chart Types Supported in ChartAgent",
            "content": "ChartAgent supports wide range of chart types across both the ChartBench and ChartX datasets. Specifically, ChartBench contains 9 major categories and 38 subcategories of charts (excluding 4 with corrupted or incorrect ground-truth labels), while ChartX comprises 18 types organized into three subcategoriesgeneral, fine-grained, and domain-specific. Figure 6 illustrates examples of each ChartX chart type, and Figure 7 presents the corresponding examples from the ChartBench dataset. Table 3: Dataset Statistics. Chart type, annotation, and QA type distribution in the evaluation datasets. (a) ChartBench (3800 Image-QA pairs): Over 75% unannotated charts; approximately 95% numeric QA. Chart Type % Annotated % Unannotated Regular Types Extra (Diverse/Complex) Types ChartBench 23.80% 76.20% 11.90% 31.00% 11.90% 7.10% 7.10% 9.50% 7.10% 4.80% 11.90% Line Bar Pie Area Box Radar Scatter Node Combination QA Type % Numeric QA % Non-Numeric QA ChartBench 94.74% 5.26% (b) ChartX (1152 Image-QA pairs): Over 60% unannotated charts; over 70% numeric QA. Chart Type % Annotated % Unannotated Regular Types Extra (Diverse/Complex) Types ChartX 38.28% 61.72% 17.36% 17.36% 8.68% 4.34% 4.34% 4.34% 4.34% 4.34% 4.34% 4.34% 4.34% 4.34% 4.34% 4.51% 4.34% 4.34% Line Bar Pie Area Box Radar Ring 3D-Bar Histogram Treemap Rose Bubble Multi-axes Heatmap Funnel Candlestick QA Type % Numeric QA % Non-Numeric QA"
        },
        {
            "title": "ChartX",
            "content": "71.88% 28.12% Figure 6: Chart types in the ChartX dataset: 18 types organized into three subcategoriesgeneral, fine-grained, and domain-specific chart types, with the percentage of data in each subcategory indicated. Over 60% of the data is unannotated, making ChartX robust testbed for visual reasoning in charts. Figure 7: Chart types in the ChartBench dataset: 9 major types with 38 subtypes (excluding 4 subtypes with corrupted or incorrect ground-truth labels). Annotated subtypes are marked in green, and unannotated subtypes are marked in red. Over 75% of the data is unannotated, making ChartBench robust testbed for visual reasoning in charts."
        },
        {
            "title": "E Baselines",
            "content": "Table 4 summarizes the model architecture details of all baseline MLLMs compared in our experiments, including both proprietary and open-weight modelscovering general-purpose as well as chart-specific open-weight MLLMs. See Appendix G.2 for implementation details and Appendix L.2 for prompts. Table 4: Model architectures of baseline MLLMs considered in our experiments, including both proprietary and open-weight modelscovering general-purpose and chart-based open-weight MLLMs. We report the model version (for proprietary models) or the underlying component architectures (for open-weight models), along with the name and parameter sizes of the vision encoder and language model (where applicable), and official access links. Concurrent works with knowledge cutoff dates after the release of our benchmark datasets (ChartBench, ChartX) are highlighted in orange . Model Version Link GPT 4o (Hurst et al., 2024) GPT 4o-mini (GPT, 2024) Claude3 Haiku (Anthropic, 2024a) GPT o3 (OpenAI, 2025b) GPT o4-mini (OpenAI, 2025b) GPT 4.1 (OpenAI, 2025a) Gemini 2.0 flash (Google, 2025) Claude3.7 Sonnet (Anthropic, 2025) Claude3.5 Sonnet (Anthropic, 2024b) Claude3.5 Haiku (Anthropic, 2024b) gpt-4o-2024-08-06 gpt-4o-mini-2024-07-18 claude-3-haiku-20240307 o3-2025-04-16 o4-mini-2025-04-16 gpt-4.1-2025-04-14 gemini-2.0-flash-001 claude-3-7-sonnet-20250219 Anthropic/claude-3.7-sonnet claude-3-5-sonnet-20240620 Anthropic/claude-3.5-sonnet claude-3-5-haiku-20241022 Anthropic/claude-3.5-haiku OpenAI/gpt4o OpenAI/gpt4o-mini Anthropic/claude-3-haiku OpenAI/o3 OpenAI/o4-mini OpenAI/gpt4.1 Google/gemini-2.0-flash (a) Proprietary Multimodal Large Language Models Model Vision Encoder Language Model Link Name Size Name Size GeneralPurpose MLLMs EVA-CLIP ViT EVA2-CLP-E EVA2-CLP-E SigLIP-SO400M 878M DeepSeek MoE 16.4B deepseek-ai/deepseek-vl2-small Salesforce/blip2-opt-2.7b THUDM/cogagent-vqa-hf THUDM/cogvlm-vqa-hf OPT Vicuna1.5 Vicuna1.5 2.7B 7B 7B 1.1B 11B 11B BLIP-2 (Li et al., 2023) CogAgent (Hong et al., 2023) CogVLM (Wang et al., 2023) DeepSeek-VL2 (Wu et al., 2024c) DocOwl1.5-Chat (Hu et al., 2024a) ViT/L InstructBLIP (Dai et al., 2023) InternVL3 (Zhu et al., 2025) LLama3.2 (Grattafiori et al., 2024) ViT Llava1.6 (Liu et al., 2024b) Llava1.5 (Liu et al., 2023c) LlaVA-OneVision (Li et al., 2024) mPLUG-Owl3 (Ye et al., 2024) Phi3-vision (Abdin et al., 2024) Pixtral (Agrawal et al., 2024) Qwen2-VL (Wang et al., 2024a) Qwen-VL-Chat (Bai et al., 2023) SmolVLM (Marafioti et al., 2025) SPHINX-V (Lin et al., 2025) VisualGLM (GLM et al., 2024) Mistral (MistralAI, 2025) BLIP2-QFormer InternViT CLIP ViT CLIP ViT SigLIP SigLIP CLIP ViT Pixtral-ViT ViT ViT-bigG SigLIP ViT-H SAM BLIP2-QFormer ChartGemma (Masry et al., 2025) ChartInstruct (Masry et al., 2024) ChartLlama (Han et al., 2023) ChartVLM (Xia et al., 2024) DePlot (Liu et al., 2023a) MatCha (Liu et al., 2023b) OneChart (Chen et al., 2024) TinyChart (Zhang et al., 2024) UniChart (Masry et al., 2023) UniChart Pix2Struct SAM-base 304M Llama2 188M Vicuna 300M Qwen2.5 630M Llama3.1 304M Mistral 304M Vicuna1.5 894M Qwen2 400M Qwen2 428M Phi-3 Mini 400M Mistral-Nemo 675M QwenLM 1.9B QwenLM 428M SmolLM2 636M Llama2 188M ChatGLM Mistral-small 22.2B Chartrelated MLLMs PaliGemma 3B 201M LLama2 LLaVA1.5 13B 282M Vicuna1.5 Pix2Struct 282M Pix2Struct 282M 86M OPT TinyLLaVA 3.1B Donut-base 201M mPLUG-DocOwl/DocOwl1.5 LAVIS/instrucblip OpenGVLab/InternVL3-2B meta-llama/Llama-3.2-11B-Vision 7B 7B 1.5B 8B 6.74B llava-hf/llava-v1.6-mistral-7b-hf 7B liuhaotian/llava-v1.5-7b 494M lmms-lab/llava-onevision-quen2-0.5b-ov 7B 3.8B 12B 7.6B 7.7B 1.7B 13B 6.2B mPLUG/mPLUG-Owl3-7B-240728 microsoft/Phi-3-vision-128k-instruct mistralai/Pixtral-12B-2409 Qwen/Qwen2-VL-7B-Instruct Qwen/Qwen-VL-Chat HuggingFaceTB/SmolVLM-Instruct AFeng-x/Draw-and-Understand THUDM/visualglm-6b mistralai/Mistral-Small-Instruct-2409 7B 7B ahmed-masry/chartgemma ahmed-masry/ChartInstruct-LLama2 tingxueronghua/ChartLlama-code U4R/ChartVLM-base google/deplot google/matcha-chartqa 125M kppkkp/OneChart mPLUG/TinyChart-3B-768 vis-nlp/UniChart (b) Open-weight Multimodal Large Language Models"
        },
        {
            "title": "F Taxonomy of Tools in ChartAgent",
            "content": "Table 5 provides summary and description of the key vision and analytical tools used in ChartAgent. Table 5: Taxonomy of Tools in ChartAgent. Summary of key vision and analytical tools used in ChartAgent. Chart Type Chart Tool Description Universal Tools annotate_legend get_marker_rgb clean_chart_image segment_and_mark axis_localizer All Detects legend coordinates, crops the legend, and annotates it with numeric labels. Returns the cropped and annotated legend image along with label mappings. Retrieves the dominant RGB color of legend marker, either by label (from an annotated legend image) or by associated text. Detects and removes the title and legend (if present) from the chart image to avoid interference with downstream visual analysis such as OCR, segmentation, or edge detection. Segments an input image using the specified model and applies post-processing to clean the masks. This includes multi-step filtering pipeline that removes small, duplicate, composite, and background-dominated masks. Returns labeled image with drawn contours and optional numbered labels, along with cleaned list of segmentation masks. Uses Segment Anything (ViT-H) as the default segmentation model (Kirillov et al., 2023). Localizes the specified axis (x-axis, left y-axis, or right y-axis) by detecting its numeric tick values and mapping them to corresponding pixel positions in the chart image. Uses Tesseract OCR (Smith, 2007) and EasyOCR (AI, 2020). interpolate_pixel_to_value Maps pixel coordinate to its corresponding axis value using linear interpolation between known axis ticks and their pixel positions. arithmetic Performs specified arithmetic operation between two numeric inputs. Supports operations such as addition, subtraction, multiplication, division, percentage, and ratio. Chart-specific Tools Pie, Treemap Bar, Combination compute_segment_area get_bar compute_bar_height get_boxplot Box compute_boxplot_entity Line, Area, Scatter, Combination Radial Bar get_edgepoints get_radial analyze_radial_geometry estimate_radial_value Computes the area of chart segment by: (1) counting discrete visual elements of specified color, (2) counting pixels of specified color, or (3) counting pixels within segment identified by specific label ID. Detects and returns the bounding box of bar in chart image that matches specified color and/or axis label. It segments bar regions using model, filters by color if provided, locates the target axis label using OCR if specified, and selects the closest matching bar accordingly. Computes the height or length of bar in value space by mapping its pixel coordinates to axis values using OCR-based axis detection and localization. Detects and returns boxplot segments filtered by color, axis label, or segmentation indices. Handles both horizontal and vertical boxplot orientations and supports fuzzy matching for axis-aligned labels and approximate color filtering. Computes statistical entity (e.g., max, min, median, Q1, Q3, range, or interquartile range) of boxplot by mapping its pixel coordinates to value space using axis localization. Computes edge points of chart segment filtered by color, axis label, or segmentation indices. The edge is determined by scanning perpendicular to the center of the matched label. Supports both vertical and horizontal chart orientations and optionally handles lineplot dots. Useful for identifying segment bounds for downstream value extraction. Computes the coordinates for the radial bar segment of interest using either colorbased filtering or segmentation mask labels. Estimates the radial geometry of radial bar chart for the segment of interest. Identifies the chart center, detects the outer circle representing the maximum value, and computes the maximum radial extent (i.e., radius) of the contour of interest. Estimates the value of radial segment in radial bar chart by scaling its radial length relative to the outermost circle. The reference value for the outer circle is provided externally (e.g., by an LLM), with default of 100."
        },
        {
            "title": "These tools are categorized into two broad",
            "content": "groups: (1) Universal types. annotation localization tools, which are applicable These include across all chart (annotate_legend), legend (axis_localizer), axis marker color detection (get_marker_rgb), (clean_chart_image), chart segmentation into value space terpolation from pixel (interpolate_pixel_to_value), and arithmetic operations (arithmetic). (segment_and_mark), cleaning (2) Chart-specific are use and plots types. tools, which deFor signed for particular chart pie and treemap charts use example, compute_segment_area; bar charts use compute_bar_height; get_bar and box compute_boxplot_entity; area, and scatter charts use get_edgepoints; and radial bar charts use get_radial, analyze_radial_geometry, and estimate_radial_value. For combination charts (e.g., bar+line or bar+pie), combination of the relevant chart-specific tools is applied. get_boxplot line, F.1 Underlying Models Powering ChartAgent"
        },
        {
            "title": "Tools",
            "content": "While the specific function-calls are abstracted away from the agent, some of the universal tools above require the custom model: (1) Semantic segmentation: Segment Anything Model v1 is used by the segment_and_mark tool extract the chart contents (foreground) from the background, generate masked segments, and remove any unneeded content as well as duplicates. Segment Anything uses (ViT-H) (Kirillov et al., 2023) model as an image encoder (641M parameters), alongside prompt encoder, and mask decoder. (2) Optical"
        },
        {
            "title": "Character",
            "content": "Recognition: Tesseract (Smith, 2007) is used for quick OCR and localization tasks, such as extracting the chart xand ytick data in axis_localizer and legend informaTesseract is tion in annotate_legend. light-weight and uses combination of image preprocessing techniques for character extraction and an LSTM-based OCR engine. (3) Optical Character Recognition: EasyOCR (AI, 2020) is used as backup for Tesseract OCR which can fail in more complicated and noisy images. The tools that utilize EasyOCR are the same as above. EasyOCR uses VGG16based CRAFT (Character Region Awareness for Text Detection, 138M parameters) for text detection followed by CRNN (Convolutional Recurrent Neural Network, 83M parameters) model for recognition. F.2 Tool Outputs and Intermediate Visualizations for Self-Verification in ChartAgent Our chart-specialized tools are carefully designed to produce clear, perception-friendly visualizations and outputs that ChartAgent can interpret for selfverification. Figures 8 and 9 show illustrative intermediate visualizations and final outputs from our universal and chart-specific tools, respectively, and also highlight the variations that these tools are able to robustly handle. To support explicit visual inspection, tool outputs include overlays, highlights, or annotations that are optimized to be easily interpretable by the base MLLM (e.g., colored segment overlays in pie charts, bar height markers, annotated legends). These custom-designed artifacts allow ChartAgent to reason over visual evidence grounded in the charts. When outputs appear semantically inconsistent or visually incorrect (e.g., pie segments too small, mismatched colors, negative bar heights, or responses contradicting axis values), ChartAgent engages in recover-and-retry processtweaking tool parameters or invoking alternative tools. This iterative correction loop mimics human-like debugging, ensuring robust reasoning and accurate interpretation in the chart domain. These visualizations are therefore critical for enabling ChartAgent to assess intermediate results and adapt its behavior in subsequent steps. quantitative evaluation of the effectiveness of this visual self-verification is provided in Section 5.3. Note that some tools generate additional outputs not displayed herefor example, the annotate_legend tool also produces cropped legend image, an annotated cropped legend image, and bounding-box mapping between detected markers/text and their (x, y, w, h) coordinates. In this figure, however, we highlight only the key output (the annotated cropped legend image) to focus on the most relevant artifacts for visual self-verification. In contrast, some tools produce Figure 8: Illustrative examples of key intermediate and final output visualizations for universal tools in ChartAgent. These visualizations are critical to facilitating visual self-verification in ChartAgent. Such tool observations enable ChartAgent to perceptually assess the outputs and refine its tool usage in the next iterationeither by adjusting tool parameters or invoking different tool if the intermediate results indicate incorrect or unexpected behavior. Note the diverse variations that our tools are capable of handling robustly. only numeric outputs, such as arithmetic and interpolate_pixel_to_value, which are not included here. Complete inputoutput specifications for each chart-specialized tool are provided in Table 5 and Section L.1.2."
        },
        {
            "title": "ChartAgent",
            "content": "G.1 ChartAgent is built using the AutoGen 0.2.26 running on Python 3.9 and conframework, Figure 9: Illustrative examples of key intermediate and final output visualizations for chart-specific tools in ChartAgent. These visualizations enable visual self-verification in ChartAgent, allowing it to refine tool usage through perceptual assessment and iterative correction. We intentionally present some easier examples here for illustration, to help readers quickly follow the process. However, ChartAgent tools are capable of handling wide range of cases, including more difficult and complex ones, as demonstrated by the overall results. figured to perform maximum of 15 reasoning iterations per task. The GPT-4o model (gpt-4o-2024-08-06) is used as the base multimodal LLM for reasoning in ChartAgent, with the temperature set to 0.0 to ensure deterministic outputs. Further, for variants of ChartAgent with different base MLLMs, we use GPT-4o-mini (gpt-4o-mini-2024-07-18), Claude 3 Haiku (claude-3-haiku-20240307), and Pixtral-12B2409. For reproducibility, the random seed is set to 42. All experiments are conducted on Linux machine using an AWS g4dn.xlarge instance, which is equipped with single NVIDIA T4 GPU with 16 GB of memory. For segmentation tasks, we use the Segment Anything (ViT-H) (Kirillov et al., 2023) model as the default, which has size of 2.56 GB and 641 million parameters. For OCR tasks, we utilize Tesseract OCR (Smith, 2007) and EasyOCR (AI, 2020). This setup facilitates efficient execution of both model inference and vision tool pipelines. All ChartAgent prompts are provided in Appendix L.1. G.2 Baselines Similar to the ChartAgent setup, all applicable baselines were run with temperature setting of 0.0 to ensure deterministic outputs, with the random seed fixed at 42 for reproducibility. All proprietary baseline models, as well as open-weight general-purpose baseline models, were evaluated using both zero-shot and Chain-of-Thought (CoT) prompting styles. All baseline prompts are provided in Appendix L.2. For chart-based baseline models such as DePlot (Liu et al., 2023a) and OneChart (Chen et al., 2024), which output structured tables rather than direct answers, we apply zero-shot GPT-4o call to extract the final answer (see Appendix L.2.4 for the corresponding prompt)."
        },
        {
            "title": "H Examples of Response Standardization",
            "content": "for Accuracy Evaluation As part of our two-step accuracy evaluation (Section 4), we use GPT-4o to standardize both the models response and the ground truth answer, before applying an arithmetic or string-matching correctness check. Below are representative examples of the standardization operations applied: (1) Converting Scales e.g., for thousand, for million, for billion ground truth: 3000 response: 4K 4000 ground truth: 15% 15 response: 0.15 times 15% 15 ground truth: 2000m 2000 response: 2.5km 2500m 2500 ground truth: 48 hours 48 response: 2 days 48 hours 48 (2) Stripping Units e.g., $, %, K, M, B, etc. ground truth: 5 response: 5K 5 ground truth: 15 response: 10% (3) Removing Symbols response: 1,000 1000 (4) Standardizing Number Formats ground truth: 7 response: seven 7 These standardizations of the ground truth and response ensure that formatting differences do not lead to incorrect evaluations during the subsequent arithmetic correctness check or stringmatching step. Prompts for both evaluation strategiesnamely, our standardization-based accuracy computation and the LLM-as-a-Judge baseline evaluationare provided in Appendix L.3."
        },
        {
            "title": "I Complexity Analysis",
            "content": "To examine ChartAgent performance under varying levels of difficulty, we divide all chartQA samples across our evaluation datasets into difficulty levels based on (a) the visual complexity of charts and (b) the reasoning complexity of chartQA pairs. This stratification enables us to analyze performance trends across distinct categories of challenge. Each dimension is categorized into three levels: Easy, Medium, and Hard. Visual complexity reflects the effort needed to interpret the chart image. Easy charts (e.g., single bar or line plots) contain few elements and clean layouts. Medium charts (e.g., multiseries line plots, grouped/stacked bar charts) introduce moderate clutter and overlapping elements. Hard charts (e.g., radar charts, 3D plots, or heavily layered visuals) are highly cluttered and visually demanding. Table 6: Complexity Label Statistics. Distribution of difficulty levels stratified by (a) visual complexity of charts and (b) reasoning complexity of chartQA pairs in the evaluation datasets. Rows correspond to reasoning complexity; columns correspond to visual complexity. Each dimension has three levels: Easy , Medium , Hard . Reasoning Complexity Visual Complexity Easy Medium Hard Total Reasoning Complexity Easy Medium Hard Total 37.38% 35.88% 1.43% 74.69% 8.86% 6.40% 16.02% 0.76% 7.07% 1.24% 9.29% 0.98% 39.12% 51.81% 9.07% 100% Easy Medium Hard Total Visual Complexity Easy Medium Hard Total 44.27% 20.83% 2.60% 67.71% 5.90% 22.74% 9.38% 5.82% 9.55% 0.52% 7.55% 3.12% 54.17% 31.51% 14.32% 100% (a) ChartBench Dataset (b) ChartX Dataset Figure 10: Complexity dimensions in chartQA pairs. Representative examples are shown for (a) visual complexity of charts and (b) reasoning complexity of chartQA pairs, each categorized into Easy, Medium, and Hard levels. (a) For visual complexity: Easy charts (e.g., single bar or line plots) have few elements and clean layouts; Medium charts (e.g., multi-series line or stacked bar plots) add moderate overlap; Hard charts (e.g., radar charts, 3D plots, or heavily layered visuals) are highly cluttered. (b) For reasoning complexity: Easy chartQA pairs involve direct lookup; Medium pairs require comparisons or proportions; Hard pairs need complex multi-step reasoning. similar trend is observed for reasoning complexity: although Easy dominates, both datasets include substantial portions of Medium and Hard reasoning tasks, ensuring coverage of non-trivial scenarios. Further, Figure 10 illustrates representative examples spanning different chart types and subtypes across the Easy, Medium, and Hard levels for both visual and reasoning complexity. The prompts used to label chart images and chartQA pairs into these stratified levels are provided in Appendix L.4. Reasoning complexity captures the cognitive effort required to answer question using the chart. Easy chartQA pairs involve direct value lookup. Medium pairs require comparisons, ratios, or proportions. Hard pairs demand multi-step reasoning, arithmetic aggregation, or complex logical inference. Table 6 reports the distribution of visual and reasoning complexity across our evaluation datasets, ChartBench and ChartX. Both datasets provide coverage across all three categories. The majority of charts fall under visually Easy or Medium categories, with fewer than 15% classified as visually Hard. ChartX contains larger fraction of visually Hard charts, making it slightly more challenging overall in terms of clutter and layout. A"
        },
        {
            "title": "J Expanded Discussion on Results",
            "content": "J.1 Performance by Chart Type Table 7 shows the accuracy on unannotated charts by chart type. Table 7: Accuracy on unannotated charts (%) by chart type. Red: Best, Blue: Second best. Abbreviations: Over: Overlay Stack: Stacked Mul: Multi Sing: Single Hor: Horizontal Vert: Vertical B-L: Bar-Line L-L: Line-Line Dir: Directed Undir: Undirected Combo: Combination. See App. for examples of each chart type. Model Area Horizontal Bar 3D Bar Vertical Bar Box Combo Line Node Pie Radar Scatter Over Stack Mul Sing Stack Mul Stack Mul Sing Stack Hor Vert Stock B-L L-L Mul Sing Dir Undir Mul Ring Sector Mul Fill Sing 3D Proprietary Multimodal Large Language Models GPT 4o GPT 4o-mini Claude 3 Gemini 1.5 21.0 23.0 15.0 5.0 18.0 7.0 5.0 4.0 24.0 59.0 13.0 27.0 12.0 32.0 28.0 52.0 10.0 7.0 7.0 7. Open-weights Multimodal Large Language Models BLIP-2 CogAgent CogVLM DeepSeek-VL2 DocOwl1.5 InstructBLIP InternVL3 LLama3.2 Llava1.6 Llava1.5 LlaVA-OneVision mPLUG-Owl3 Phi3-vision Pixtral Qwen2VL QwenVLChat SmolVLM SPHINX-V VisualGLM 0.0 14.0 21.0 29.0 19.0 5.0 25.0 46.0 7.0 1.0 9.0 11.0 27.0 26.0 57.0 6.0 7.0 7.0 6.0 Chart-related Models ChartGemma ChartInstruct ChartLlama ChartVLM DePlot MatCha OneChart TinyChart UniChart 25.0 20.0 20.0 16.0 18.0 3.0 0.0 32.0 15. 0.0 2.0 3.0 11.0 8.0 7.0 16.0 21.0 7.0 5.0 2.0 2.0 37.0 10.0 18.0 8.0 3.0 2.0 3.0 8.0 6.0 2.0 8.0 2.0 1.0 6.0 22.0 5.0 1.0 3.0 15.0 3.0 4.0 17.0 25.0 57.0 21.0 69.0 3.0 11.0 45.0 80.0 58.0 91.0 11.0 12.0 12.0 8.0 7.0 9.0 9.0 20.0 43.0 78.0 25.0 51.0 87.0 97.0 4.0 8.0 12.0 17.0 17.0 3.0 2.0 1.0 21.0 54.0 23.0 72.0 2.0 15.0 24.0 78.0 43.0 74.0 8.0 29.0 27.0 67.0 71.0 88.0 24.0 59.0 4.0 6.0 3.0 8.0 3.0 1.0 19.0 11.0 8.0 7.0 12.0 1.0 8.0 6.0 17.0 2.0 3.0 4.0 4.0 9.0 1.0 7.0 10.0 13.0 0.0 2.0 13.0 7. Multimodal Agentic Framework (Ours) 20.0 20.0 25.0 14.0 5.0 15.0 18.0 36.0 20.0 5.0 38.0 31.0 18.0 6.0 12.0 15.0 40.0 30.0 40.0 6.0 12.0 16.0 2.0 21.0 17.0 12.0 29.0 34.0 8.0 16.0 37.0 11.0 6.0 7.0 5.0 4.0 73.0 38.0 56.0 19.0 51.0 67.0 39.05 49. 4.0 4.0 3.0 8.0 0.0 4.0 1.0 4.0 1.0 3.0 10.0 2.0 7.0 5.0 7.0 3.0 1.0 10.0 1.0 3.0 7.0 7.0 7.0 9.0 1.0 2.0 15.0 0.0 2.0 11.0 11.0 58.0 39.0 3.0 44.0 71.0 7.0 5.0 11.0 11.0 86.0 39.0 94.0 5.0 14.0 9.0 4.0 36.0 36.0 14.0 60.0 66.0 18.0 69.0 76.0 32.0 4.0 9.0 16.0 82.0 78.0 11.0 80.0 89.0 19.0 9.0 7.0 15.0 92.0 89.0 97.0 17.0 26.0 26.0 6.0 86.0 85.0 20.0 85.0 78.0 40.0 80.0 82.0 60. 12.0 2.0 6.0 5.0 3.0 4.0 4.0 13.0 6.0 4.0 16.0 10.0 1.0 4.0 7.0 2.0 30.0 10.0 24.0 5.0 0.0 4.0 5.0 6.0 6.0 7.0 8.0 7.0 11.0 11.0 21.0 1.0 20.0 26.0 13.0 12.0 8.0 5.0 13.0 18.0 1.0 3.0 6.0 8.0 7.0 7.0 3.0 11.0 17.0 7.0 4.0 1.0 16.0 23.0 6.0 6.0 3.0 5.0 1.0 4.0 8.0 12.0 6.0 7.0 9.0 15.0 16.0 29.0 4.0 13.0 1.0 0.0 7.0 7.0 7.0 4.0 6.0 1.0 5.0 5.0 27.0 9.0 9.0 5.0 3.0 23.0 20.0 20.0 17.0 3.0 17.0 0.0 3.0 2.0 8.0 3. 63.0 57.0 62.0 24.0 3.0 22.0 2.0 51.0 32.0 1.0 60.0 49.0 0.0 2.0 14.0 16.0 48.0 39.0 64.0 2.0 28.0 2.0 0.0 22.0 5.0 1.0 7.0 0.0 1.0 0.0 4.0 6.0 35.0 41.0 37.0 75.0 91.0 29.0 36.0 19.0 50.0 88.0 24.0 23.0 28.0 50.0 75.0 7.0 28.0 91.0 91.0 71.0 91.0 48.0 59.26 5. 9.0 5.0 4.0 3.0 4.0 0.0 0.0 20.0 20.0 6.0 21.0 16.0 10.0 19.0 24.0 20.0 9.0 46.0 48.0 51.0 31.0 8.0 15.0 23.0 23.0 74.0 42.0 3.0 23.0 2.0 27.0 24.0 30.0 56.0 62.0 42.0 46.0 63.0 87.0 42.0 39.0 7.0 16.0 15.0 11.0 3.0 1.0 7.0 7.0 38.0 2.0 10.0 14.0 15.0 14.0 10.0 52.0 31.0 55.0 66.0 84.0 39.0 19.0 24.0 17.0 32.0 68.0 37.0 46.0 80.0 85.0 80.0 6.0 6.0 9.0 20.0 7.0 23.0 62.0 5.0 15.0 13.0 10.0 46.0 7.0 16.0 22.0 63.0 3.0 6.0 2.0 0.0 5.0 5.0 5.0 31.0 36.0 24.0 68.0 32.0 27.0 24.0 13.0 68.0 18.0 16.0 18.0 10.0 41.0 3.0 37.0 40.0 30.0 95.0 13.0 48.0 45.0 14.0 63.0 84.0 16.0 14.0 13.0 18.0 16.0 12.0 62.0 38.0 90.0 65.0 46.0 50.0 51.0 91.0 22.0 16.0 25.0 13.0 37.0 36.0 5.0 31.0 24.0 36.0 47.0 26.0 52.0 58.0 45.0 22.0 36.0 41.0 51.0 59.0 86.0 22.0 54.0 54.0 53. 38.0 26.0 38.0 10.0 73.0 19.0 60.0 35.0 33.0 3.0 1.0 7.0 1.0 3.0 3.0 1.0 0.0 2.0 2.0 0.0 5.0 1.0 0.0 0.0 0.0 2.0 2.0 1.0 2.0 0.0 2.0 1.0 0.0 2.0 2.0 1.0 4.0 0.0 0.0 1.0 3.0 32.0 7.0 9.0 14.0 2.0 18.0 7.0 6.0 14.0 1.0 2.0 4.0 4.0 8.0 1.0 10.0 14.0 21.0 12.0 2.0 2.0 3.0 5. 2.0 8.0 8.0 7.0 3.0 1.0 0.0 20.0 0.0 34.0 16.0 12.0 29.52 6.0 22.0 20.0 8.0 3.0 1.0 13.0 11.0 3.0 0.0 7.0 1.0 2.0 9.0 25.0 15.0 8.0 3.0 9.0 25.0 5.0 11.0 1.0 23.0 21.0 28.0 9.0 3.0 12.0 16.0 4.0 8.0 3.0 15.0 5.0 5.0 1.0 7.0 21.0 1.0 6.0 2.0 9.0 13.0 2.0 4.0 6.0 13.0 15.0 5.0 13.0 21.0 14.0 10.0 2.0 0.0 2.0 7.0 24.0 24.0 6.0 17.0 10.0 8.0 1.0 3.0 2.0 13.0 11.0 9.0 1.0 24.0 12.0 7.0 3.0 17.0 11.0 26.0 66.0 8.0 9.0 13.0 9.0 11.0 9.0 10.0 8.0 3.0 9.0 14.0 16.0 14.0 8.0 4.0 2.0 4.0 7. 3.0 8.0 0.0 2.0 2.0 2.0 0.0 10.0 4.0 8.0 6.0 0.0 4.0 2.0 0.0 0.0 8.0 4.0 3.0 4.0 11.0 6.0 3.0 2.0 0.0 4.0 1.0 63.0 43.0 51.0 45.0 3.0 20.0 16.0 44.0 20.0 11.0 25.0 46.0 16.0 14.0 16.0 6.0 73.0 72.0 47.0 5.0 14.0 7.0 8.0 29.0 4.0 14.0 14.0 2.0 10.0 2.0 27.0 11. Avg. 36.15 25.19 26.04 27.27 2.92 11.62 11.62 30.31 23.58 5.92 30.92 36.38 9.92 7.00 10.50 12.65 40.77 28.73 43.50 6.54 14.46 12.30 7.65 22.42 20.19 11.42 23.92 28.15 9.69 26.81 32.77 15.96 ChartAgent 30. 38.0 79.0 76.0 82.0 20.0 6.0 88. 88.0 76.0 89.0 83.0 64.0 67.0 65.0 63.0 81.0 91.0 91. 18.0 94.0 80.0 22.0 20.0 6.0 64.0 60. (a) ChartBench Dataset (9 major chart types, 42 subtypes; 26 unannotated) Model Area Bar 3D Bar Box Bubble Candlestick Heatmap Histogram Line Multi-Axes Radar Ring Rose Treemap Average Proprietary Multimodal Large Language Models GPT 4o GPT 4o-mini Claude 3 Haiku Gemini 1.5 26.0 16.0 26.0 26.0 35.19 32.41 25.0 40.74 22.0 34.0 20.0 22. 40.0 42.0 22.0 48.0 Open-weights Multimodal Large Language Models BLIP-2 CogAgent CogVLM DeepSeek-VL2 DocOwl1.5 InstructBLIP InternVL3 LLama3.2 Llava1.6 Llava1.5 LlaVA-OneVision mPLUG-Owl3 Phi3-vision Pixtral Qwen2VL QwenVLChat SmolVLM SPHINX-V VisualGLM 0.0 16.0 20.0 24.0 14.0 6.0 24.0 40.0 16.0 12.0 8.0 14.0 38.0 34.0 28.0 24.0 26.0 18.0 16.0 Chart-related Models ChartGemma ChartInstruct ChartLlama ChartVLM DePlot MatCha OneChart TinyChart UniChart 32.0 8.0 12.0 12.0 16.0 12.0 9.3 22.0 16.0 0.9 23.15 31.48 41.7 24.07 3.7 36.11 37.0 19.4 11.1 12.0 30.6 41.7 45.4 53.70 17.59 23.15 20.4 8.33 36.11 16.67 18.52 26.85 52.78 18.5 69.52 47.22 23.15 2.0 30.0 30.0 24.0 20.0 20.0 30.0 30.0 24.0 18.0 12.0 24.0 38.0 22.0 38.0 18.0 20.0 20.0 24.0 26.0 12.0 38.0 28.0 14.0 18.0 5.26 28.0 14.0 Multimodal Agentic Framework (Ours) 44.0 48.0 38.0 50.0 2.0 20.0 16.0 34.0 18.0 10.0 38.0 26.0 12.0 16.0 10.0 12.0 40.0 42.0 42.0 20.0 14.0 16.0 22.0 0.0 30.0 28.0 36.0 32.0 14.0 44.0 30.0 26.0 36.0 16.0 24.0 54.0 54.0 42.0 20.0 28.0 20.0 10.0 30.0 26.0 28.0 34.0 22.0 12.0 20.41 28.0 12.0 28.0 6.0 16.0 26.0 32.0 16.0 10.87 24.0 4.0 78.0 66.0 48.0 8. 2.0 48.0 34.0 62.0 44.0 0.0 66.0 58.0 30.0 6.0 36.0 18.0 58.0 62.0 60.0 28.0 50.0 30.0 8.0 42.0 56.0 44.0 42.0 32.0 32.0 39.58 62.0 26.0 50.0 50.0 50.0 25.0 0.0 50.0 50.0 50.0 50.0 25.0 50.0 25.0 50.0 0.0 0.0 25.0 50.0 50.0 50.0 50.0 0.0 0.0 75.0 25.0 0.0 25.0 50.0 25.0 50.0 0.0 25.0 75.0 42.55 34.04 27.66 44. 2.1 19.15 17.02 38.3 42.55 2.1 53.19 70.2 14.9 8.5 6.4 19.1 46.8 44.7 65.96 21.28 17.02 21.3 8.51 31.91 21.28 8.51 42.55 63.83 8.5 63.04 51.06 42.55 53.92 39.22 33.33 33.33 2.0 30.39 25.49 54.9 35.29 17.6 49.02 69.6 25.5 20.6 20.6 22.5 52.0 43.1 61.76 28.43 31.37 28.4 18.63 42.16 28.43 24.51 44.12 70.59 29.4 77.0 46.08 29.41 18.0 8.0 6.0 18. 0.0 10.0 12.0 14.0 12.0 8.0 16.0 16.0 4.0 8.0 6.0 4.0 22.0 14.0 18.0 6.0 8.0 10.0 8.0 8.0 4.0 10.0 16.0 16.0 6.0 24.0 16.0 12.0 30.0 28.0 22.0 20.0 6.0 26.0 26.0 26.0 24.0 8.0 24.0 26.0 18.0 20.0 8.0 16.0 40.0 32.0 26.0 36.0 20.0 30.0 16.0 22.0 8.0 28.0 24.0 22.0 14.0 9.3 24.0 12.0 30.0 35.0 15.0 30. 0.0 15.0 15.0 20.0 5.0 0.0 30.0 25.0 10.0 5.0 10.0 5.0 35.0 20.0 15.0 10.0 5.0 5.0 10.0 10.0 5.0 15.0 30.0 5.0 10.0 30.0 10.0 10.0 34.0 26.0 20.0 30.0 4.0 24.0 16.0 26.0 10.0 6.0 32.0 28.0 10.0 10.0 8.0 8.0 36.0 24.0 34.0 6.0 16.0 18.0 4.0 18.0 10.0 16.0 18.0 6.0 10.0 11.11 16.0 8.0 44.83 24.14 10.34 20. 0.0 17.24 27.59 17.2 3.45 10.3 3.45 20.7 3.4 6.9 0.0 10.3 13.8 31.0 13.79 13.79 0.0 13.8 6.90 6.90 10.34 13.79 13.79 13.79 10.3 3.57 6.90 6.90 39.44 33.94 25.77 31.41 1.69 24.93 24.23 35.63 24.37 8.87 36.62 39.86 18.17 14.51 12.82 18.31 41.69 38.17 42.96 20.42 22.11 20.70 13.10 28.87 17.75 21.55 29.01 34.51 17.04 37.14 33.38 18.87 ChartAgent 32.0 50.0 30.0 33.33 70.0 50. 50.0 36.17 64.71 16.0 30.0 50. 28.0 65.52 44.16 (b) ChartX Dataset (18 chart types in total; 14 unannotated) J.2 Analysis of Tool Usage in ChartAgent Generic tools such as crop/zoom are insufficient To gain deeper insight into the internal decisionmaking process of ChartAgent, we examine how it selects visual tools across different chart types. Table 8 summarizes the most frequently used tools for each chart type, reflecting tool-usage patterns observed in agent trajectories (see Appendix Table 5 for detailed descriptions of each tools functionality). This analysis demonstrates that ChartAgent strategically adapts its tool usage to the structural and semantic properties of different chart types. Overall, Further, Figure 11 illustrates the percentage of times ChartAgent employs each tool tool usage is across chart types. strongly chart-type dependent. Universal tools (e.g., annotate_legend, get_marker_rgb, clean_chart_image) are employed consistently across nearly all chart types, whereas chartspecific tools (e.g., get_boxplot for boxplots or analyze_radial_geometry for radial bars) are invoked only when structurally required. Combination charts exhibit the highest diversity of tool usage, reflecting the need to simultaneously process multiple chart modalities (e.g., bar and line elements). Interestingly, several tools show nearly identical usage percentages, suggesting they are frequently used together in agent trajectories. For example, annotate_legend and get_marker_rgb exhibit very similar distributions across chart types: once the legend is localized, the agent almost always proceeds to extract the corresponding marker color. Such patterns indicate that certain tools are implicitly coupled in the decision-making process, with ChartAgent invoking them in conjunction to complete semantically linked subtasks. J.3 Ablation Study Prior agentic frameworks in natural image VQA rely heavily on generic tools like cropping and zooming. While effective for object localization or text spotting in natural images, these tools lack the capabilities required for structured, quantitative reasoning over charts. Chart-based QA tasks often demand operations such as axis parsing, color-based segmentation, pixel-to-value interpolation, and arithmetic reasoning, which cannot be supported by coarse manipulations like cropping or zooming. This motivates the design of chartspecialized tools tightly integrated into the reasoning loop. because: They cannot extract or match RGB values to identify legend categories. They cannot segment visual elements (e.g., pie slices, bars) based on color or structure. They cannot compute pixel areas or interpolate numerical values from axes. As result, agents using only natural image tools often produce reasoning traces filled with irrelevant observations, ultimately lowering accuracy. In contrast, chart-specialized tools (e.g., axis parsing, bar/pie segmentation, legend detection, numeric estimation) allow precise grounding of reasoning steps and enable recovery via visual selfverification. To understand the contribution of chartspecialized visual tools in our framework, we conduct an ablation study comparing three variants of the ReAct agent, all implemented with GPT-4o as the underlying reasoning model and equipped with visual self-verification: (i) ReAct (No Tools): reasoning without any visual tools; (ii) ReAct + Natural Image Tools: reasoning augmented with generic natural-image tools such as crop and zoom; and (iii) ChartAgent (Ours): reasoning supported by chart-specialized tools designed for fine-grained chart understanding. Table 9 presents the comparison across the three variants. We report both average accuracy and performance on the more challenging subset of unannotated/numeric chart questions. The results highlight several key observations: ReAct without tools underperforms even GPT-4o + CoT. While ReAct provides reasoning structure, without visual grounding it accumulates errors, producing misleading traces. Generic tools provide marginal gains. Crop/zoom adds limited context but cannot handle structured quantitative reasoning, resulting in only minor improvements over no tools. Chart-specialized tools are critical. The large performance jump with ChartAgent demonstrates the necessity of type-specific visual grounding and self-verification mechanisms for robust chart QA. Table 8: Most frequently used tools across chart types. Tool-usage patterns observed in agent trajectories (see Appendix Table 5 for tool descriptions). Chart Type (Chart Subtypes) Pie (Ring, Sector, Multi-Ring), Treemap Bar (Horizontal/Vertical Single/Multi/Stacked, Histogram, 3D) Box (Horizontal/Vertical) Area (Overlay, Stacked) Line (Single/Multi) Scatter (Bubble, 3D) Radial Bar, Rose Combination (Bar-Line, Line-Line), Multi-Axes Chart Tools Used annotate_legend get_marker_rgb clean_chart_image segment_and_mark compute_segment_area arithmetic annotate_legend get_marker_rgb clean_chart_image segment_and_mark get_bar compute_bar_height axis_localizer interpolate_pixel_to_value clean_chart_image segment_and_mark get_boxplot compute_boxplot_entity axis_localizer interpolate_pixel_to_value annotate_legend get_marker_rgb clean_chart_image segment_and_mark get_edgepoints axis_localizer interpolate_pixel_to_value arithmetic annotate_legend get_marker_rgb clean_chart_image get_edgepoints axis_localizer interpolate_pixel_to_value annotate_legend get_marker_rgb clean_chart_image segment_and_mark get_edgepoints axis_localizer interpolate_pixel_to_value annotate_legend get_marker_rgb clean_chart_image segment_and_mark get_radial analyse_radial_geometry estimate_radial_value annotate_legend get_marker_rgb clean_chart_image segment_and_mark get_bar compute_bar_height get_edgepoints axis_localizer interpolate_pixel_to_value Figure 11: Tool-use statistics across benchmark datasets. Percentage of times ChartAgent employs given tool when solving queries for each chart type. As expected, universal tools are used broadly across all chart types, whereas chart-specific tools are invoked selectively depending on the chart type detected by the ChartAgent orchestrator. Table 9: Ablation study on the role of tools in chart VQA. Chart-specialized tools enable strong gains, especially for unannotated charts & numeric QA. Red: Best. Method ReAct + No Tools ReAct + Natural Image Tools ChartAgent (Ours) Overall Acc. (%) Unannotated & Numeric Acc. (%) Tool Type None 38.84 19. Generic 41.35 20.50 Chartspecialized 71.39 58. This ablation study confirms that generic naturalimage tools are fundamentally inadequate for chart reasoning. By equipping the agent with comprehensive taxonomy of chart-specialized tools, integrated into an iterative ReAct loop with visual self-verification, ChartAgent achieves state-of-theart performanceparticularly excelling on unannotated charts and numeric QA where prior methods fail. J.4 Visual and Reasoning Complexity"
        },
        {
            "title": "Analysis",
            "content": "Table 10 presents the accuracy on unannotated charts by visual complexity of the charts and reasoning complexity of the chartQA pairs. J.5 Accuracy vs. LLM-as-a-Judge We found that GPT often relaxes the 5% margin condition, leading to inflated performance compared to arithmetic accuracy, which strictly enTable 10: Accuracy by Complexity Levels. Accuracy (%) on unannotated charts stratified by visual complexity of the charts and reasoning complexity of the chartQA pairs. Red: Best, Blue: Second best. Model Visual Complexity Reasoning Complexity Overall Model Visual Complexity Reasoning Complexity Overall Easy Medium Hard Easy Medium Hard Average Easy Medium Hard Easy Medium Hard Average Proprietary Multimodal Large Language Models Proprietary Multimodal Large Language Models GPT 4o GPT 4o-mini Claude 3 Haiku Gemini 1.5 57.16 39.93 40.53 46.36 28.25 20.22 21.17 20.83 17.59 9.45 10.42 6.19 44.06 32.06 33.17 36.43 Open-weights Multimodal Large Language Models BLIP-2 CogAgent CogVLM DeepSeek-VL2 DocOwl1.5-Chat InstructBLIP InternVL3 LLama3.2 Llava1.6 Llava1.5 LlaVA-OneVision mPLUG-Owl3 Phi3-vision Pixtral Qwen2VL QwenVLChat SmolVLM SPHINX-V VisualGLM Chart-related Models ChartGemma ChartInstruct ChartLlama ChartVLM DePlot MatCha OneChart TinyChart UniChart 3.16 13.23 15.17 43.08 43.08 9.83 49.27 58.01 15.66 8.50 11.17 18.81 55.83 45.39 66.02 8.98 23.06 20.26 12.74 39.68 38.96 17.84 44.90 50.36 17.48 52.21 53.03 30.83 2.45 11.78 9.94 25.39 15.45 4.02 22.67 28.86 7.69 6.19 9.39 9.67 36.08 22.94 36.69 5.65 10.42 8.44 5. 15.66 12.05 9.26 15.11 19.54 6.81 15.61 24.71 10.28 4.56 6.51 11.07 19.54 10.10 4.56 21.17 14.33 5.21 6.84 14.01 10.42 22.48 11.73 15.64 4.23 10.75 9.44 4.23 8.47 8.79 4.56 9.77 9.77 2.61 5.22 16.94 3.26 3.06 13.44 12.94 37.00 29.72 6.67 37.89 45.28 12.78 7.83 11.39 14.89 50.11 35.39 54.44 7.61 17.83 15.22 9.56 28.72 25.67 13.61 31.56 37.78 13.22 34.17 40.00 21.06 20.84 9.94 10.33 9. 3.44 8.41 9.18 16.63 10.33 3.82 16.83 14.15 2.68 6.31 10.71 8.99 19.69 14.53 17.40 3.25 9.37 7.07 3.44 7.46 7.27 5.93 6.31 5.16 1.72 4.29 16.83 3.06 13.72 9.39 9.39 1.08 1.08 5.78 8.66 12.64 8.66 5.05 12.27 20.58 5.05 2.89 4.33 5.05 19.49 12.27 21.66 5.78 2.17 3.24 3.25 9.75 9.03 7.58 7.58 9.03 1.81 2.82 15.88 7.22 36.15 25.19 26.04 27. 2.92 11.62 11.73 30.31 23.58 5.92 30.92 36.38 9.92 7.00 10.50 12.65 40.73 28.73 43.50 6.54 14.46 12.30 7.65 22.42 20.19 11.42 23.92 28.15 9.69 26.81 32.77 15.96 GPT 4o GPT 4o-mini Claude 3 Haiku Gemini 1.5 42.11 36.22 25.69 36.84 47.77 39.28 32.59 31.25 22.70 22.09 16.56 20. 49.86 43.21 34.07 44.60 Open-weights Multimodal Large Language Models BLIP-2 CogAgent CogVLM DeepSeek-VL2 DocOwl1.5-Chat InstructBLIP InternVL3 LLama3.2 Llava1.6 Llava1.5 LlaVA-OneVision mPLUG-Owl3 Phi3-vision Pixtral Qwen2VL QwenVLChat SmolVLM SPHINX-V VisualGLM Chart-related Models ChartGemma ChartInstruct ChartLlama ChartVLM DePlot MatCha OneChart TinyChart UniChart 0.93 26.06 26.62 42.41 28.79 8.05 40.25 49.23 19.20 14.55 12.69 21.67 46.74 45.82 51.39 19.19 23.22 21.67 10. 31.89 22.60 20.12 33.13 49.22 17.95 55.73 39.93 25.69 1.78 25.89 23.21 36.61 23.66 8.04 41.52 37.95 18.75 14.29 15.63 16.96 41.07 39.73 40.18 24.10 25.44 20.08 15.18 32.58 17.85 22.76 28.57 26.78 19.64 25.35 32.14 13.83 3.07 21.47 20.85 20.86 16.56 11.66 22.70 23.93 15.33 14.72 9.20 13.49 32.52 20.86 28.83 17.79 15.33 19.63 15.34 17.79 7.97 22.69 21.47 15.95 11.65 13.38 22.08 12.26 1.38 30.47 27.98 47.37 32.41 8.03 46.26 49.31 21.32 16.34 16.89 21.33 53.74 49.58 55.13 23.82 25.76 25.20 14. 37.67 24.37 22.43 35.45 45.70 20.77 45.55 44.04 26.31 31.54 24.48 17.01 18.26 2.48 18.67 19.92 24.07 17.43 8.71 28.21 31.95 13.27 12.45 7.88 15.35 26.97 27.39 32.37 15.76 18.25 16.59 8.71 18.25 11.61 19.08 23.65 28.21 13.69 36.77 22.82 11.61 22.22 24.07 17.59 16.67 0.93 20.37 21.30 22.22 12.96 12.04 23.15 25.93 18.52 12.96 10.19 14.81 34.26 24.07 24.07 19.44 18.52 14.81 17. 23.14 9.25 24.07 19.44 11.11 12.03 6.45 21.29 10.18 39.44 33.94 25.77 31.41 1.69 24.93 24.23 35.63 24.37 8.87 36.62 39.86 18.17 14.51 12.82 18.31 41.69 38.17 42.96 20.42 22.11 20.70 13.10 28.87 17.75 21.55 29.01 34.51 17.04 37.14 33.38 18.87 Multimodal Agentic Framework Multimodal Agentic Framework ChartAgent (Ours) 83.98 56.77 17.92 71.33 41. 28.52 60.81 ChartAgent (Ours) 50.93 49.91 24. 54.14 38.17 27.78 44.16 (a) ChartBench Dataset (b) ChartX Dataset forces this threshold. This observation is important to share with the community, as most recent Chart VQA papers (Xu et al., 2023; Xia et al., 2024; Masry et al., 2022) rely directly on GPT-based accuracy for evaluation. Table 11 reports the comparison between our standardized accuracy evaluation and the corresponding LLM-as-a-Judge results on the ChartBench dataset. Table 11: Accuracy vs. LLM-as-a-Judge. Results on the ChartBench dataset. All values represent accuracy in percentage. Model Accuracy LLM-as-a-Judge Gap (%) Gemini 2.0 flash GPT 4o-mini DeepSeek-VL2 ChartLlama ChartInstruct GPT 4o SPHINX-V TinyChart CogVLM ChartGemma 69.90 42.24 49.39 19.89 31.24 51.47 19.76 46.84 28.11 39. 76.45 48.47 55.16 24.42 35.68 55.63 23.79 50.82 31.68 42.76 -6.55 -6.24 -5.76 -4.53 -4.45 -4.16 -4.03 -3.97 -3.58 -3.45 J.6 Concurrent Works The ChartBench dataset was released on December 26, 2023, and ChartX on February 19, 2024. Table 12 shows the split of models with knowledge cutoff dates before versus after each dataset release. Since datasets may have leaked into the training data of models with knowledge cutoff dates after release, we report these concurrent model results separately. J.6.1 Performance of Concurrent Works on"
        },
        {
            "title": "Public Benchmarks",
            "content": "Table 13 presents the accuracy comparison for concurrent works with knowledge cutoff dates after the dataset releases. We suspect that benchmark data (ChartBench and ChartX, released in December 2023 and February 2024, respectively) may have been included in the training data of GPT-o3 and GPT-o4-mini (knowledge cutoff: May 2024). In several cases, particularly with GPT-o3, we observed that the model produced correct answers despite incorrect Table 12: Knowledge Cutoffs and Concurrent Works. Comparison of model and dataset release dates relative to ChartBench and ChartX, showing whether models were trained before or after these benchmarks. Model / Dataset Knowledge Cutoff Claude 3 Haiku Claude 3 Sonnet GPT-4o GPT-4o-mini GPT-o1 ChartBench Dataset ChartX Dataset Claude 3.5 Sonnet GPT-o3 GPT-o4-mini GPT-4.1 Claude 3.5 Haiku Gemini 2.0 Claude 3.7 Sonnet Mistral-Small Aug 1, 2023 Aug 1, 2023 Sept 30, 2023 Sept 30, 2023 Sept 30, 2023 Dec 26, 2023 Feb 19, 2024 Apr 1, 2024 May 31, 2024 May 31, 2024 May 31, 2024 Jul 1, 2024 Aug 1, 2024 Nov 1, 2024 Mar 17, 2025 Relative to ChartBench / ChartX Before both Before both Before both Before both Before both After both After both After both After both After both After both After both After both reasoning steps or tool outputs. For example, even when the agent misidentified key visual elements or generated invalid intermediate outputs, the final answer was still correct. We also noted this behavior in instances where it was humanly very difficult to provide the exact answer, yet GPT-o3 and GPTo4-mini produced outputs with decimal-level precision. Such patterns suggest possible memorization or exposure to similar instances during training. While preliminary, these observations provide strong evidence of potential data leakage from public benchmarks into newer models. To strengthen this analysis, we curated new held-out internal dataset that mirrors the complexity of ChartBench and ChartX, enabling more rigorous evaluation. J.6.2 Performance of Concurrent Works on the Internal Dataset We created new dataset with 125 chartQA pairs that we are confident were not included in the training data of newer models, and conducted evaluations for fairer comparison of these models against ChartAgent. Table 14 reports the overall accuracy (within 5% margin) and average numeric error on this curated dataset. Clearly, ChartAgent outperforms all newer models by significant margin in both accuracy and average error, reinforcing its effectiveness as chart-focused visually-grounded reasoning framework. J.7 Visual Self-Verification and Recovery"
        },
        {
            "title": "Behavior",
            "content": "In addition to analyzing difficulty-based trends, we studied whether ChartAgent could detect unsatisfactory tool outputs and recover using its visual self-verification mechanism. We manually evaluated 30 randomly selected agent trajectories from the ChartBench dataset to assess this behavior. The results are summarized in Table 15. In 50% of the sampled cases, the tool outputs were correct, and no recovery was needed. In the remaining 50%, the agent correctly identified the tool outputs as unsatisfactory and triggered its self-verification mechanism. Among these, 70% resulted in successful recovery, leading to correct final answers. The remaining 30% failed to recover, contributing to 15% overall error rate attributable to unresolved tool-level failures. These findings demonstrate that ChartAgents visual self-verification mechanism is both frequently invoked and often effective, enhancing robustness in the presence of imperfect tool outputsespecially critical for unannotated chart understanding. J.8 Fallback Analysis: When ChartAgent"
        },
        {
            "title": "Reverts to the Base Model and Common\nTrigger Conditions",
            "content": "We conducted manual analysis of 30 randomly selected agent trajectories from ChartBench, focusing on unannotated charts and numeric QA, to better understand when and why the agent reverts to the base model (GPT-4o). We found that the fallback rate was relatively lowless than 10% across the sample. The most common reasons for fallback included the following: Bar charts: When the computed bar height was negative or highly inconsistent with the axis values, indicating failure in visual estimation, the agent abandoned tool-based reasoning and allowed GPT4o to attempt direct response. OCR-based tools returning None: For example, if legend or axis label detection failed to locate any relevant entities, the agent deemed the output unsatisfactory and reverted to GPT-4o. Line charts: When edge-point detection or interpolation tools produced empty outputs or values that were highly inconsistent with the axis, the agent once again defaulted to GPT-4o. In all such cases, the agent judged tool-based reasoning to be unreliable and defaulted to the base model. While rare, this fallback mechanism serves Table 13: Accuracy on Concurrent Works (Public Benchmarks). Comparison of accuracy (%) on concurrent works with knowledge cut-off dates after the release of the datasets. All values correspond to the highest performance achieved across zero-shot and CoT prompting styles for each MLLM. Ann./Unann. denote Annotated and Unannotated charts. RL QA: Relationship QA; VC/GC QA: Value Comparison & Global Conception QA. Model Chart Types Question Types Overall Model Ann. Unann. Numeric QA RL QA Avg. Chart Types Question Types Overall Ann. Unann. Numeric QA VC/GC QA Avg. Proprietary Multimodal Large Language Models Proprietary Multimodal Large Language Models GPT o3 GPT o4-mini GPT 4.1 Gemini 2.0 flash Claude 3.7 Sonnet Claude 3.5 Sonnet Claude 3.5 Haiku 98.18 98.50 97.33 97.79 97.75 96.50 90.67 76.56 71.73 67.00 58.31 60.38 56.23 38.58 82.55 79.14 75.61 71.81 71.64 68.14 53.89 98.44 99.00 94.00 41.00 82.00 83.50 75.50 83.39 80.18 76.58 69.90 72.18 68.95 55.03 GPT o3 GPT o4-mini GPT 4.1 Gemini 2.0 flash Claude 3.7 Sonnet Claude 3.5 Sonnet Claude 3.5 Haiku 91.18 91.18 92.99 89.37 89.37 87.78 80.32 71.13 72.68 69.58 58.31 60.28 57.32 40.70 79.59 80.92 77.90 68.72 69.81 67.39 50.97 76.85 76.85 80.25 74.07 75.62 73.15 68.52 78.82 79.77 78.56 70.23 71.44 69.01 55.90 Open-weights Multimodal Large Language Models Open-weights Multimodal Large Language Models Mistral 91.75 43.23 57.08 90. 58.55 Mistral 84.84 48.59 59.06 71. 62.50 Multimodal Agentic Framework Multimodal Agentic Framework ChartAgent (Ours) 94.33 60. 70.91 91.00 71.39 ChartAgent (Ours) 84.84 44. 55.93 69.14 59.69 (a) ChartBench Dataset (b) ChartX Dataset Table 14: Accuracy on Concurrent Works (Internal Benchmarks). Overall average accuracy (within 5% margin) and average error across models on the curated internal dataset. Red: Best, Blue: Second best. Model Accuracy (%) Avg. Error (%) ChartAgent (ours) GPT 5 GPT 5-mini Claude 3.7 Sonnet GPT o4-mini Gemini 2.0 GPT-4.1 GPT-o3 Claude 3.5 Haiku Mistral o1 GPT-4o 85.19 74.71 73.18 69.71 69.68 67.24 66.61 62.93 42.11 38.54 33.07 22.02 3.42 24.09 11.24 15.52 21.88 21.07 24.32 9.14 37.31 38.74 44.31 64.34 as valuable fail-safe. J.9 Inference Time Comparison: ChartAgent vs. GPT-4o We conducted preliminary timing analysis on representative subset of samples across various chart types to compare the inference time between ChartAgent and GPT-4o. On average: single GPT-4o call with chain-of-thought reasoning took approximately 12 seconds per query. In contrast, full ChartAgent trajectoryincluding multi-step tool invocations and self-verificationtook approximately 125 seconds per query. Table 15: Visual self-verification and recovery outcomes in ChartAgent trajectories."
        },
        {
            "title": "Metric",
            "content": "Cases where recovery was needed (i.e., tool output deemed unsatisfactory)"
        },
        {
            "title": "Value",
            "content": "50%"
        },
        {
            "title": "Successful recoveries among needed cases",
            "content": "70%"
        },
        {
            "title": "Correct final answers following recovery",
            "content": "Cases where tool error propagated to final answer (i.e., remained incorrect) 70% 15% This increase in inference time is expected due to the agentic design, which involves iterative reasoning, multiple visual perception tool executions, and verification steps. We note that the runtime can be significantly reduced in practice by optimizing tool efficiencymany intermediate outputs currently used for visualization and debugging can be streamlined or skipped entirely in deployment settings. Despite the additional overhead, we believe the substantial accuracy improvementsparticularly on unannotated and numerically dense chartsjustify the increased computational cost in settings where precision is critical. J.10 Monetary Cost Analysis Our approach incurs monetary costs due to the use of OpenAIs GPT-4o (Hurst et al., 2024) as the base reasoning model. We spent approximately $2000 to run ChartAgent on both datasets, covering 4952 chart image and QA pairs across diverse chart typesresulting in an average cost of approximultiple Y-axes with different scales) make it visually hard to map chart elements to the correct reference values. 2) Reasoning-based failures. (2.1) Unit mismatches: The agent sometimes multiplies values based on axis labels (e.g., reading 160 as 160,000 due to in thousands), which may not match the ground truth. (2.2) Incorrect tool selection: Occasionally, the agent chooses the wrong measurement toolfor instance, computing area instead of heightleading to incorrect results despite correct region localization. (2.3) Question ambiguity: Some questions, such as those from multi-ring pie charts in ChartBench, lack clear context (e.g., undefined denominators), resulting in ambiguous interpretation. We plan to address such cases in future work by enabling the agent to detect ambiguity and proactively request user clarification when necessary. (2.4) Label duplication: Charts with the same label used at multiple hierarchy levels (e.g., parent and child segments both labeled Netflix) confuse the model during segment selection and reasoning. See Appendix for examples. (2.5) Subtype misclassification in area charts: Overlay and stacked area charts can appear visually similar, and misclassifying them leads to incorrect answer logic (e.g., value subtraction errors), even if all other steps are executed correctly See Figure 12 for illustrations of common failure modes (12a) and qualitative failure cases where ChartAgent produces incorrect responses (12b). Overall, most failures are perception-driven, originating from chart tool errors rather than complex reasoning or planning. mately $0.40 per sample. This cost can be substantially reduced by using smaller models such as GPT-4o-mini, or eliminated entirely with opensource models like Pixtral, Llama, or Qwen, since our framework is designed to be plug-and-play. For example, switching from GPT-4o to GPT-4o-mini would reduce the average cost per sample by more than 15 (to roughly $0.025), making large-scale evaluation far more economical. Thus, monetary cost should not be considered serious limitation, as our approach can seamlessly adapt to free or low-cost models as well."
        },
        {
            "title": "K Details on Failure Mode Analysis",
            "content": "ChartAgent encounters two main categories of failure: visual perception challenges and reasoning ambiguities. 1) Perception-based failures. (1.1) OCR obstruction by visual overlays: Black overlays or dense chart elements often cover axis or legend text, preventing accurate OCR extraction. (1.2) Poor color contrast: Labels in white placed over fluorescent yellow or similarly bright backgrounds are difficult for vision tools to detect. (1.3) Legend occlusion: In some charts, the legend overlaps with key visual elementssuch as bars of interesthindering accurate region detection. (1.4) Chart element invisibility: Median lines in box plots that share the same color as the box become indistinguishable, making it hard to extract correct values. (1.5) Segmentation failure due to axis overlap: Axis lines overlapping with chart elements confuse the segmentation tool and result in incorrect extraction. (1.6) Overlap-induced indistinguishability: When multiple data series substantially overlap in charts (e.g., radar plots, line charts, scatterplots with dense clusters, or filled regions), subtle differences between categories become imperceptible. This occurs due to coincident paths, stacked fills, or saturation effects, preventing reliable detection of fine-grained deviations. (1.7) Axis interpretation failures: When unusual or complex axes (e.g., 3D distorted axes, (a) Illustrations of common failure modes in ChartAgent. (b) Qualitative failure cases where ChartAgent produces incorrect responses. Figure 12: Failure Mode Analysis. Examples where ChartAgent fails to produce the correct response due to visual perception challenges or reasoning ambiguities. (A) Perception-based failures include OCR obstruction by overlays, poor color contrast, key chart element occlusions (e.g., legends blocking bars), chart element invisibility, difficult segmentation (e.g., overlapping axes or cluttered regions), overlap confusion, 3D depth distortion, and multiple Y-axis mapping errors. (B) Reasoning-based failures include label duplication, ambiguous questions (e.g., undefined denominators) and misclassification of visually similar chart subtypes (e.g., stacked vs. overlay area)."
        },
        {
            "title": "L Prompts",
            "content": "We present the prompts used for ChartAgent L.1, baselines L.2, evaluation L.3, and complexity analysis L.4. Note that some low-level prompt details are omitted below for space constraints. L.1 ChartAgent Prompts ChartAgent comprises structured set of prompts that specify reasoning, tool usage, metadata extraction, and in-context learning (ICL). For clarity, we first present the overall concatenated prompt, followed by its individual components: the System Prompt (L.1.1), Chart Tool Definitions (L.1.2), Chart Metadata Extraction Prompt (L.1.3), and ICL Examples (L.1.4)."
        },
        {
            "title": "ChartAgent Prompt",
            "content": "SYSTEM PROMPT [L.1.1] Instruction: To support your analysis, several Python-based tools are available in tools.py and will be pre-imported for you. Bounding boxes follow the format [x, y, w, h], where and denote the horizontal and vertical coordinates of the upper-left corner, and and represent the width and height of the box. Use the provided tools for precise numeric analysis by extracting properties such as area, height, and other quantitative attributes of chart components. Execute one tool at time, and wait for its output before proceeding. If the output seems uncertain, you may re-run the tool with adjusted parameters or switch to different tool. Below are the tools defined in tools.py: TOOL DEFINITIONS PROMPT [L.1.2] Below are examples demonstrating how to use the tools to address user requests. You may refer to them for guidance. IN CONTEXT LEARNING EXAMPLES PROMPT [L.1.4] Note for readers: Only ICL examples corresponding to the chart type detected in the chart metadata extraction stage are retrieved and used. For example, if the chart is detected as pie chart, only pie chart ICL examples are included. GOAL: Using the tools above, reason about how to solve # USER REQUEST # and generate step-by-step actions (each action is Python Jupyter notebook code block) to solve the request. You may need to use the tools above to process chart images and numerical values, and to make decisions based on the visual and numerical outputs of previous code blocks. The Jupyter notebook has already executed the following code to import the necessary packages: ```python from PIL import Image from IPython.display import display from tools import * The generated actions should fully resolve the user request # USER REQUEST #. Assume the request is reasonable and solvable; do your best to solve it. If you believe you have the answer, output ANSWER: <your answer> and end with TERMINATE. Here is the chart metadata: CHART METADATA [L.1.3] Note for readers: Extracted during the chart metadata extraction stage using the Chart Metadata Extraction Prompt (L.1.3). Input: <chart image> # USER IMAGE #: {entry[\"image\"]} <question> # USER REQUEST #: {entry[\"query\"]} Instruction (continued): Now please generate only THOUGHT 0, and ACTION 0 in RESULT. If no action needed, also reply with ANSWER: <your answer> and ends with TERMINATE in the RESULT. # RESULT #: Agent: THOUGHT 0: should annotate the legend in the chart image to correctly identify the marker of interest. ACTION 0: ```python legend_image, labeled_legend, bbox_map = annotate_legend(chart_image, legend_texts) display(labeled_legend) print (bbox_map) ``` Instruction (continued): OBSERVATION: Execution success. The output is as follows: <output> Please generate the next THOUGHT and ACTION. If you can get the answer, please also reply with ANSWER: <your answer> and end with TERMINATE. Trajectory continues as the agent generates THOUGHT 1, ACTION 1, . . . until termination. For visualization purposes, prompts containing code are formatted differently here; in practice, however, all prompts are provided as plain text inputs to the MLLM. The same prompt template is used across all chart VQA samples and datasets in this work. L.1.1 System Prompt The system prompt establishes the agents role and high-level objectives. It instructs the model to follow structured reasoning, invoke tools where appropriate, and return answers in well-defined format."
        },
        {
            "title": "System Prompt",
            "content": "You are helpful multimodal AI assistant for answering questions about chart images, including numeric QA, free-form QA, and multiple-choice QA. You operate in Python Jupyter notebook environment and can: Suggest Python code (in executable code blocks) to process images, text, or data. Use variables and states from previous cells. Provide complete code, not partial snippets. The notebook imports Image from the PIL package and display from the IPython.display package. Display all image outputs using display(). [MORE INSTRUCTIONS ...] (The actual system prompt contains additional detailed guidelines.) Execution Instructions: Execute one tool at time and wait for results. If an error occurs, fix it and re-run. If unsure about output, try different parameters or tools. For each turn, you should first do \"THOUGHT\", based on the chart images and textual question you see. If you think you get the answer to the initial user request, you can reply with: ANSWER: <your answer> and end with TERMINATE. L.1.2 Chart Tool Definitions The following are the Python-based tools available to ChartAgent, along with their inputs, outputs, and expected behaviors. An abridged parameter set is shown for some tools to save space and aid readability."
        },
        {
            "title": "Tool Definitions Prompt",
            "content": "```python"
        },
        {
            "title": "Universal Tools",
            "content": "def annotate_legend (image: PIL.Image.Image, legend: dict[str]) tuple[PIL.Image.Image, PIL.Image.Image, dict[int, tuple[str, tuple[int,int,int,int]]]]: \"\"\" Detects legend coordinates, crops the legend, and annotates it with numeric labels. Args: image (PIL.Image.Image): Input chart image legend (dict[str]): Legend strings Returns: legend_image (PIL.Image.Image): Cropped legend image labeled_legend (PIL.Image.Image): Annotated legend image with numeric label mappings bbox_mapping (dict[int, (str, (int, int, int, int))]): Maps numeric labels to (text, bounding box coordinates [x_min,y_min,x_max,y_max]). Example: image = PIL.Image.open(\"chart_image.png\") legend_image, labeled_legend, bbox_mapping = annotate_legend(image, legend={\"Legend1\", \"Legend2\"}) display(labeled_legend) \"\"\" def get_marker_rgb (image: PIL.Image.Image, bbox_mapping: dict[int, int, text_of_interest: str, label_of_interest: int, distance_between_text_and_marker: int) tuple[int, int, int]: tuple[int, tuple[str, int, int]]], \"\"\" Retrieves the dominant RGB color of legend marker, either by label (from an annotated legend image) or by associated text. Args: image (PIL.Image.Image): Input legend image bbox_mapping (dict): Mapping of label numbers to (text, bbox) tuples. The bounding box is (x_min, y_min, x_max, y_max). text_of_interest (str, optional): The legend text whose marker color should be retrieved. If provided, fuzzy matching is applied. label_of_interest (int, optional): The label number in bbox_mapping whose marker color should be retrieved. distance_between_text_and_marker (int): Approximate distance in pixels between the legend text and its marker (default: 5). Returns: tuple[int, int, int]: The (R, G, B) color of the detected marker. Examples: # Example 1: Using text label image = Image.open(\"chart_image.png\") legend_image, labeled_legend, bbox_mapping = annotate_legend(image) rgb_color = get_marker_rgb(legend_image, bbox_mapping, text_of_interest=\"Rock\") print(\"Detected RGB color:\", rgb_color) # Output: (0, 0, 255) # Example 2: Using label number rgb_color = get_marker_rgb(legend_image, bbox_mapping, label_of_interest=5) print(\"Detected RGB color:\", rgb_color) # Output: (255, 0, 0) \"\"\" def clean_chart_image (image: PIL.Image.Image, title: str, legend: dict[str]) PIL.Image.Image: \"\"\" Cleans chart image by removing title and legend if provided. Args: image (PIL.Image.Image): Input PIL image of the chart title (str): Title to remove (None to skip title removal) legend (dict[str]): Legend strings to remove (None to skip legend removal) Thresholds and expand values control removal bounding boxes. Returns: cleaned_image (PIL.Image.Image): Cleaned chart image Example: image = PIL.Image.open(\"chart_image.png\") cleaned_image = clean_chart_image(image, title=\"Title\", legend={\"Legend1\", \"Legend2\"}) display(cleaned_image) \"\"\" def segment_and_mark (image: PIL.Image.Image, segmentation_model: str, min_area: int, iou_thresh_unique: float, iou_thresh_composite: float, white_ratio_thresh: float, remove_background_color: bool) tuple[PIL.Image.Image, list[dict]]: \"\"\" Segments an input image using the specified model and applies post-processing to clean the masks through multi-step filtering pipeline that removes small, duplicate, composite, and background-dominated masks. Returns labeled image with drawn contours and optional numbered labels, along with cleaned list of segmentation masks. Args: image (PIL.Image.Image): The input chart image to be segmented segmentation_model (str): Segmentation model (Segment Anything (\"SAM\") by default) min_area (int): Minimum pixel area to keep mask (5000 default) iou_thresh_unique (float): IoU threshold for duplicate removal (0.9 default) iou_thresh_composite (float): IoU threshold for composite mask removal (0.98 default) white_ratio_thresh (float): White pixel ratio to discard mask (0.95 default) remove_background_color (bool): If True, remove background color pixels Returns: labeled_image (PIL.Image.Image): Segmented and labeled image with drawn contours and numeric labels cleaned_masks (list[dict]): Cleaned list of segmentation masks Example: image = Image.open(\"chart_image.png\") labeled_image, cleaned_masks = segment_and_mark(image) display(labeled_image) print(f\"Total masks: {len(cleaned_masks)}\") \"\"\" def axis_localizer (image: PIL.Image.Image, axis: ple[list[float], list[int]]: str, axis_threshold: float, axis_tickers: list) tu- \"\"\" Localizes the specified axis (x-axis, left y-axis, or right y-axis) by detecting its numeric tick values and mapping them to corresponding pixel positions in the chart image. Uses Tesseract OCR and EasyOCR. Args: pil_image (PIL.Image.Image): Input chart image axis (str): Axis to localize; (x-axis), (left y-axis), or right_y (right y-axis) axis_threshold (float): Fraction of the image to scan for tick labels along the axis direction (0.2 default) axis_tickers (list or None): Optional pre-supplied axis tick strings to improve matching Returns: axis_values (list[float]): Detected numeric tick values (e.g., [0, 200, 400, 600]) axis_pixel_positions (list[int]): Corresponding pixel positions (e.g., [950, 850, 750, 650]) Example: axis_values, axis_pixel_positions = axis_localizer(image, axis='y', axis_threshold=0.2, axis_tickers=[\"200\", \"400\", \"600\", \"800\", \"1000\", \"1200\", \"1400\"]) print(axis_values, axis_pixel_positions) \"\"\" def interpolate_pixel_to_value (pixel: float, axis_values: list[float], axis_pixel_positions: list[int]) float: \"\"\" Maps pixel coordinate to its corresponding axis value using linear interpolation between known axis ticks and their pixel positions. Args: pixel (float or int): Pixel coordinate to map axis_values (list[float]): Numeric axis values (e.g., [0, 200, 400, 600]) axis_pixel_positions (list[int]): Pixel positions corresponding to axis_values (e.g., [950, 850, 750, 650]) Returns: float: Interpolated axis value corresponding to the given pixel Example: axis_values = [0, 200, 400, 600] axis_pixel_positions = [950, 850, 750, 650] val = interpolate_pixel_to_value(800, axis_values, axis_pixel_positions) print(val) # Expected interpolation between 200 and 400 \"\"\" def arithmetic (a: float, b: float, operation: str) float: \"\"\" Performs specified arithmetic operation between two numeric inputs. Supports operations such as addition, subtraction, multiplication, division, percentage, and ratio. Args: (float): First operand (float): Second operand operation (str): Arithmetic operation to perform. Supported: \"add\", \"subtract\", \"multiply\", \"divide\", \"percentage\", \"ratio\" (\"percentage\" by default) Returns / Raises: float: Result of the arithmetic operation ValueError: If an unsupported operation is provided or division by zero occurs Example: total = 1200 part = 300 result = arithmetic(part, total, operation=\"percentage\") print(\"Percentage:\", result) # Output: 25.0 \"\"\" Pie Chart Treemap Chart-specific Tools def compute_segment_area (image: PIL.Image.Image, filter_rgb: tuple[int,int,int], measure: str, masks: list, filter_segment: list) tuple[PIL.Image.Image, int]: \"\"\" Computes the area of chart segment by: (1) counting discrete visual elements of specified color, (2) counting pixels of specified color, or (3) counting pixels within segment identified by specific label ID. Commonly used for pie charts and tree maps. Args: image (PIL.Image.Image): Input chart image (cleaned if necessary) filter_rgb (tuple[int,int,int], optional): RGB values to filter by; if None, uses full chart measure (str): Method to measure area \"pixels\" or \"discrete-dots\" masks (list, optional): Segmentation masks (SAM-style) filter_segment (list, optional): Segment label numbers to include in pixel counting Returns / Raises: visualization (PIL.Image.Image): Image with detected/filtered areas highlighted int: Computed area (discrete-dots or pixels) ValueError: If measure is unsupported Examples: # Example 1: Full pie chart area (discrete-dots) image = Image.open(\"pie_chart.png\") vis, area = compute_segment_area(image, measure=\"discrete-dots\") print(area) # e.g., 500 # Example 2: Area of RGB-colored section (pixels) rgb_interest = (255, 0, 0) # red vis, area = compute_segment_area(image, filter_rgb=rgb_interest, measure=\"pixels\") print(area) # e.g., 5000 # Example 3: Area of specific segments via masks labeled_img, masks = segment_and_mark(image) vis, area = compute_segment_area(image, measure=\"pixels\", masks=masks, filter_segment =[3,5,7]) print(area) # e.g., \"\"\""
        },
        {
            "title": "Bar Chart",
            "content": "def get_bar (image: PIL.Image.Image, rgb_of_interest: tuple[int,int,int], ticker_label: str, segmentation_model: str, bar_orientation: str ) tuple[int, int, int, int]: \"\"\" Detects and returns the bounding box of bar in chart image that matches specified color and/or axis label. It segments bar regions using model, filters by color if provided, locates the target axis label using OCR if specified, and selects the closest matching bar accordingly. Commonly used for bar charts. Args: image (PIL.Image.Image): Input chart image rgb_of_interest (tuple[int,int,int], optional): RGB color of target bar ticker_label (str, optional): Axis label text of interest segmentation_model (str): Segmentation model for detection (\"SAM\" default) bar_orientation (str): \"vertical\", \"horizontal\", or \"vertical-right\" (\"vertical\" default) Returns: tuple[int,int,int,int]: Bounding box (x, y, w, h) if bar is found, else None Examples: # Example 1: Vertical bar plot image = Image.open(\"bar_chart.png\") bbox = get_bar(image, rgb_of_interest=(100,128,45), ticker_label=\"2016\") print(bbox) # e.g., (50, 100, 30, 200) # Example 2: Combination bar-line plot bbox = get_bar(image, rgb_of_interest=(100,128,45), ticker_label=\"2016\", bar_orientation = \"vertical-right\") print(bbox) \"\"\" def compute_bar_height (image: PIL.Image.Image, bar_of_interest: axis_threshold: float, x_axis_tickers: list, y_axis_tickers: list, x_axis_title: str, y_axis_title: str) float: tuple[int,int,int,int], bar_orientation: str, \"\"\" Computes bars value (height or length) by mapping its pixel bounding box to axis values using OCR-based axis localization. Supports left/right y-axes for vertical bars and the x-axis for horizontal bars. Commonly used for bar charts. Args: image (PIL.Image.Image): Input chart image bar_of_interest (tuple[int,int,int,int]): Bounding box (x, y, w, h) of the bar bar_orientation (str): \"vertical\", \"vertical-right\", or \"horizontal\" (\"vertical\" default) axis_threshold (float): Fraction of the image scanned for tick labels during axis localization (0.15 default) x_axis_tickers (list or None): Optional pre-read x-axis tick labels y_axis_tickers (list or None): Optional pre-read y-axis tick labels x_axis_title (str or None): X-axis title, if available y_axis_title (str or None): Y-axis title, if available Returns: float: Estimated bar value (height for vertical; length for horizontal) Examples: # Example 1: Vertical bar on left y-axis image = Image.open(\"bar_chart.png\") bar = (120, 210, 35, 180) # (x, y, w, h) from get_bar() bar_height = compute_bar_height(image, bar, bar_orientation=\"vertical\") print(bar_height) # Example 2: Horizontal bar (value from x-axis) bar = (100, 70, 150, 25) bar_length = compute_bar_height(image, bar, bar_orientation=\"horizontal\", x_axis_tickers =[\"200\", \"400\", \"600\", \"800\", \"1000\", \"1200\", \"1400\"]) print(bar_length) \"\"\""
        },
        {
            "title": "Box Plot",
            "content": "def get_boxplot (image: PIL.Image.Image, masks: box_labels_of_interest: list, boxplot_orientation: str, axis_threshold: float) list: rgb_of_interest: list, tuple[int,int,int], ticker_label: str, \"\"\" Detects and returns boxplot segments filtered by color, axis label, or segmentation indices. Handles both horizontal and vertical boxplot orientations and supports fuzzy matching for axis-aligned labels and approximate color filtering. Commonly used for box plots. Args: image (PIL.Image.Image): Input chart image masks (list): List of segmentation masks rgb_of_interest (tuple[int,int,int] or None): RGB color to filter segments ticker_label (str or None): Axis label (e.g., \"Tuesday\") to filter segments box_labels_of_interest (list or None): Segmentation mask indices to select boxplot_orientation (str): \"vertical\" or \"horizontal\" (\"vertical\" default) axis_threshold (float): Fraction of image scanned for axis values (0.15 default) Returns: list[tuple[int,int,int,int]]: Final filtered (x, y, w, h) segments Examples: # Example 1: Filter by RGB color (vertical boxplot) image = Image.open(\"box_plot.png\") boxplot_of_interest = get_boxplot(image, masks=masks, rgb_of_interest=(106, 184, 209)) print(boxplot_of_interest) # Example 2: Filter by ticker label (vertical boxplot) boxplot_of_interest = get_boxplot(image, masks=masks, ticker_label=\"Tuesday\") print(boxplot_of_interest) # Example 3: Filter by segmentation indices (horizontal boxplot) boxplot_of_interest = get_boxplot(image, masks=masks, box_labels_of_interest=[3, 7], boxplot_orientation=\"horizontal\") print(boxplot_of_interest) \"\"\" def compute_boxplot_entity (image: PIL.Image.Image, boxplot_of_interest: plot_orientation: str, entity_of_interest: str, axis_threshold: float, x_axis_tickers: float: list[tuple[int,int,int,int]], boxlist) list, y_axis_tickers: \"\"\" Computes statistical entity (e.g., max, min, median, Q1, Q3, range, or interquartile range) of boxplot by mapping its pixel coordinates to value space using axis localization. Commonly used for box plots. Args: image (PIL.Image.Image): Input chart image boxplot_of_interest (list[tuple[int,int,int,int]]): Bounding boxes of the boxplot segments boxplot_orientation (str): \"vertical\" or \"horizontal\" (\"vertical\" default) entity_of_interest (str): One of \"median\", \"max\", \"min\", \"range\", \"iqr\", \"q1\", \"q3\", \"q2\" (\"median\" default) axis_threshold (float): Fraction of image scanned for tick labels during axis localization (0.15 default) x_axis_tickers (list or None): Optional pre-read x-axis tick labels y_axis_tickers (list or None): Optional pre-read y-axis tick labels Returns: float: Computed value of the requested boxplot entity Examples: # Example 1: Median value of vertical boxplot image = Image.open(\"box_plot.png\") boxplot_segments = [(120, 150, 40, 80), (120, 250, 40, 70)] # from get_boxplot() median_val = compute_boxplot_entity(image, boxplot_segments, entity_of_interest=\"median\") print(median_val) # Example 2: Maximum value (Q1) of horizontal boxplot boxplot_segments = [(100, 70, 120, 30), (250, 70, 140, 30)] max_val = compute_boxplot_entity(image, boxplot_segments, \"horizontal\", \"max\") print(max_val) # Example 3: Interquartile range (IQR) of vertical boxplot boxplot_segments = [(130, 160, 30, 70), (130, 260, 30, 90)] iqr_val = compute_boxplot_entity(image, boxplot_segments, entity_of_interest=\"iqr\") print(iqr_val) \"\"\" Line Area Scatter Plots def get_edgepoints (image: PIL.Image.Image, masks:list, ticker_label: str, mask_labels_of_interest: list, chart_orientation: str, lineplot_get_dot: bool, axis_threshold: float) list[tuple[int,int]]: tuple[int,int,int], rgb_of_interest: \"\"\" Computes edge points of chart segment filtered by color, axis label, or segmentation indices. The edge is determined by scanning perpendicular to the center of the matched label. Supports both vertical and horizontal chart orientations and handles lineplot dots. Useful for identifying segment bounds for downstream value extraction. Commonly used for line, area, and scatter plots. Args: image (PIL.Image.Image): Input chart image masks (list or None): SAM masks with bbox and segmentation fields rgb_of_interest (tuple[int,int,int] or None): Target RGB color for filtering ticker_label (str or None): Axis label (e.g., \"Q3\") for filtering mask_labels_of_interest (list or None): SAM mask indices to select chart_orientation (str): \"vertical\" or \"horizontal\" (\"vertical\" default) lineplot_get_dot (bool): Whether to get edge points for lineplot dots (True) or area chart segments (False) (False default) axis_threshold (float): Portion of the image scanned for axis localization (0.15 default) Returns: list[tuple[int,int]]: Edge points perpendicular to the label center: Vertical: [(x, top_y), (x, bottom_y)] Horizontal: [(left_x, y), (right_x, y)] Examples: # Example 1: RGB color + ticker label (vertical area chart) image = Image.open(\"area_chart.png\") edge_points = get_edgepoints(image, rgb_of_interest=(106, 184, 209), ticker_label=\"Q2\") print(edge_points) # Example 2: SAM mask index + ticker label (vertical area chart) edge_points = get_edgepoints(image, masks, ticker_label=\"A\", mask_labels_of_interest=[3]) print(edge_points) # Example 3: RGB color + ticker label (line plot) image = Image.open(\"line_plot.png\") line_dots = get_edgepoints(image, rgb_of_interest=(237, 0, 209), ticker_label=\"Q1\", lineplot_get_dot = True) print(line_dots) \"\"\""
        },
        {
            "title": "Radial Bar Plot",
            "content": "def get_radial (image: PIL.Image.Image, rgb_of_interest: tuple[int,int,int], ticker_label: str, segmentation_model: str) tuple[int,int,int,int]: \"\"\" Computes the coordinates for the radial bar segment of interest using either color-based filtering or segmentation mask labels. Commonly used for radial bar plots. Args: image (PIL.Image.Image): Input chart image rgb_of_interest (tuple[int,int,int] or None): Target RGB color of segment ticker_label (str or None): Axis label (e.g., \"Q3\") for filtering segmentation_model (str): Segmentation model to use; \"color\" for color-based filtering or \"SAM\" for Segment Anything (\"color\" default) Returns: tuple[int,int,int,int]: Bounding box (x, y, w, h) representing the segments radial coordinates Example: image = Image.open(\"radial_bar_plot.png\") radial_coords = get_radial(image, rgb_of_interest=(106, 184, 209), ticker_label=\"Q2\") print(radial_coords) \"\"\" def analyze_radial_geometry (image: PIL.Image.Image, contour_of_interest: np.ndarray) tuple[PIL.Image.Image, int, int, float, float]: \"\"\" Estimates the radial geometry of radial bar chart for the segment of interest. Identifies the chart center, detects the outer circle representing the maximum value, and computes the maximum radial extent (i.e., radius) of the contour of interest. Commonly used for radial bar plots. Args: image (PIL.Image.Image): Input chart image contour_of_interest (np.ndarray): Contour representing the segment of interest Returns: PIL.Image.Image: Image with detected outer circle and chart center marked int: X-coordinate of the circles center int: Y-coordinate of the circles center float: Outer circle radius (r_outer) float: Maximum radius from center to the contour (r_max) Example: image_radial_geometry, center_x, center_y, r_outer, r_max = analyze_radial_geometry(image, contour_of_interest=contour) #contour from get_radial() display(image_radial_geometry) print(\"Center coordinates:\", center_x, center_y, \"Outer circle radius:\", r_outer, \"Max radius:\", r_max) \"\"\" def estimate_radial_value (image: PIL.Image.Image, center_x: int, center_y: int, r_outer: int, r_max: int, reference_circle_value: float) float: \"\"\" Estimates the value of radial segment in radial bar chart by scaling its radial length relative to the outermost circle. The reference value for the outer circle is provided externally (e.g., by an LLM), with default of 100. Args: image (PIL.Image.Image): Input chart image center_x (int): X-coordinate of the circle center center_y (int): Y-coordinate of the circle center r_outer (int): Radius of the outer circle r_max (int): Maximum radius from the center to the contour reference_circle_value (float): Value corresponding to the outer reference circle (default: 100) Returns: float: Estimated value of the radial segment Example: radial_value = estimate_radial_value(image, center_x=250, center_y=250, r_outer=200, r_max=150, reference_circle_value=100) #center_x, center_y, r_outer, r_max from analyze_radial_geometry() print(\"Estimated value:\", radial_value) \"\"\" ``` L.1.3 Chart Metadata Extraction The metadata extraction prompt guides the agent to identify essential chart components, such as chart type, axis ranges, and legend entries. This metadata is then used to retrieve and condition the appropriate ICL examples, and to parameterize subsequent tool calls."
        },
        {
            "title": "Chart Metadata Extraction Prompt",
            "content": "Instruction: You are vision-language model tasked with analyzing data visualization chart image. Extract and return the following information as JSON dictionary using the exact keys specified below. chart_type: e.g., pie chart, multi-ring pie chart, bar chart, line chart, box plot, etc. title: Exact chart title as shown. legend: List or dictionary of all legend entries. highlevel_legend_categories and finegrained_legend_subcategories: If the chart shows category hierarchy, list both, even if names overlap. legend_embedded: true if legend is within the chart; false if outside. axis/ axis/ right-y axis/ color-bar labels: Axis labels (strings). May be empty. axis/ axis/ right-y axis/ color-bar/ radial axis ticker values: Tick values (List). May be empty. annotation_type: Either \"annotated\" or \"unannotated\". \"annotated\" if numeric values are written directly in the chart. \"unannotated\" if such values are not shown in the chart. visual_description: Concise summary of the charts visual structure. Only output the JSON object. Input: Chart image corresponding to the current sample."
        },
        {
            "title": "In Context Learning",
            "content": "L.1.4 We provide ICL examples corresponding to each major chart type. At inference time, only the examples matching the detected chart type are retrieved and used. For instance, if chart is classified as pie chart during the metadata extraction stage, only pie chart ICL examples are appended to the prompt. If no ICL examples exist for the detected chart type, then no ICL is added."
        },
        {
            "title": "ICL Example Prompt",
            "content": "Below is an example of how to use the tools to solve user query with vertical grouped bar chart. [MORE INSTRUCTIONS ...] The actual ICL prompt contains additional detailed guidelines, including selfverification, recovery strategies, changing tool parameters, using alternative tool options in case of incorrect trajectories, and fallback options to the base MLLM. # USER REQUEST #: <An image here> According to this chart, what is the GDP growth rate for India in 2020? # USER IMAGE stored in chart_image, as PIL image. THOUGHT 0: First, will display and read the image. ACTION 0: ```python display(image=chart_image) ``` OBSERVATION: Execution success. The chart image is displayed: <image displayed here> THOUGHT 1: Next, will read the chart components such as title, legend, and axes. ACTION 1: ```python title_text = \"Annual GDP Growth Rate\" legend_texts = {\"Singapore\", \"India\", \"United States\", \"United Kingdom\", \"China\"} x_axis_tickers = [\"2016\", \"2017\", \"2018\", \"2019\", \"2020\"] y_axis_tickers = [\"0\", \"1.50\", \"3.0\", \"4.50\", \"6.0\", \"7.50\", \"9.0\", \"10.50\"] x_axis_title = \"Year\" y_axis_title = \"GDP Growth Rate (%)\" print(title_text, legend_texts) ``` OBSERVATION: Execution success. Title, legend, and axes are recorded. THOUGHT 2: Now, should annotate the legend in the chart image so that can correctly identify the marker corresponding to the category of interest \"India\". ACTION 2: ```python legend_image, labeled_legend, bbox_mapping = annotate_legend(chart_image, legend_texts) display(labeled_legend) print (bbox_mapping) ``` OBSERVATION: Execution success. The output is labeled image of legend elements and their bounding boxes. Labeled legend image is displayed: <image displayed here> THOUGHT 3: will now use the labeled_legend to determine the bounding box label corresponding to the legend marker of interest India. Markers are small colored circles or squares placed to the left of their respective labels. The India marker is red, with the number 5 above it, so assign 5 as the label_of_interest. ACTION 3: ```python label_of_interest = 5 print(label_of_interest) ``` OBSERVATION: Execution success. Output: label_of_interest = 5 THOUGHT 4: will now extract the RGB color of the India legend marker using its bounding box label. ACTION 4: ```python rgb_of_interest = get_marker_rgb(legend_image, bbox_mapping, label_of_interest=5) print (\"Detected RGB color:\", rgb_of_interest) ``` OBSERVATION: Execution success. Output: Detected RGB color: (145, 20, 45) THOUGHT 5: will now clean the chart image by removing both the title and the legend to avoid interference with bar identification using segmentation and color matching. This ensures that extraneous elements do not affect contour detection or segmentation. If the legend is too close to the xor y-axis, will not crop the legend to avoid removing parts of the axis; in that case, only the title will be removed. ACTION 5: ```python cleaned_image = clean_chart_image(chart_image, title = title_text, legend=legend_texts) display(cleaned_image) ``` OBSERVATION: Execution success. Cleaned image displayed without title and legend: <image displayed here> THOUGHT 6: will now detect the bar of interest in the cleaned_image corresponding to India (marker color = rgb_of_interest) and the year 2020 (ticker_label). The bar orientation is vertical. ACTION 6: ```python bar_of_interest = get_bar(cleaned_image, rgb_of_interest=rgb_of_interest, ticker_label =\"2020\", bar_orientation=\"vertical\") print(bar_of_interest) ``` OBSERVATION: Execution success. Output: bar_of_interest = (120, 210, 35, 180) THOUGHT 7: Now, will compute the value height of the detected bar (bar_of_interest) by mapping its pixel height to the axis scale. Since this is vertical bar chart, will apply threshold of 0.2 to automatically detect the axis region in the left 20% of the image. will then use the x_axis_tickers and y_axis_tickers (already extracted in previous steps) to perform the mapping accurately. ACTION 7: value_height = compute_bar_height(cleaned_image, bar_of_interest, bar_orientation=\" vertical\", axis_threshold=0.2, x_axis_tickers=x_axis_tickers, y_axis_tickers= y_axis_tickers, x_axis_title=x_axis_title, y_axis_title=y_axis_title) print(\"Bar value height:\", value_height) OBSERVATION: Execution success. Output: Bar value height: 4.2 ANSWER: The GDP growth rate for India in 2020 is 4.2. TERMINATE. L.2 Baseline Prompts To benchmark ChartAgent, we compare against several baseline prompting strategies. We apply zeroshot (L.2.1) and chain-of-thought (CoT) (L.2.2) prompts across all proprietary and open-weight MLLM baselines. In addition, we include ReAct prompt (L.2.3) for ablation studies, comparing ChartAgent with ReAct-style agent to isolate the effect of chart-specialized visual tools. Finally, we use tabular question-answering prompt (L.2.4) for few chart-based baselines that output structured tables instead of direct answers. L.2.1 Zero-shot The zero-shot prompt provides only minimal task instructions, requiring the model to answer directly from the chart without intermediate reasoning or tool use. Zero-shot Prompt Instruction: You are data analyst skilled at analyzing chart data. Carefully examine the chart and answer the Users question with single word or short phrase. Input Format: <chart image> {entry[\"image\"]} <question> {entry[\"query\"]} L.2.2 Chain-of-Thought The chain-of-thought (CoT) prompt encourages the model to reason step by step before providing its final answer, resulting in more structured and coherent reasoning compared to zero-shot prompting. Chain-of-Thought (CoT) Prompt Instruction: You are data analyst skilled at analyzing chart data. Analyze the users chart, carefully examine it, think step by step, and answer the users question. Provide your final answer in the exact format: My final answer is {answer here}. Input Format: <chart image> {entry[\"image\"]} <question> {entry[\"query\"]} L.2.3 ReAct The ReAct prompt (Yao et al., 2023) combines reasoning traces with action steps, allowing the model to interleave thought, tool/code invocation, and observations until final answer is reached. We use this prompt in our ablation studies to isolate the contribution of chart-specialized visual tools in our framework."
        },
        {
            "title": "ReAct Prompt",
            "content": "Instruction: You are data analyst skilled at analyzing chart data. Your task is to analyze the provided chart and answer the users question. Carefully examine the chart, reason step by step, and invoke actions (e.g., tool calls or code) when helpful. Follow this exact format: Thought: ... Action: ... Observation: ... (repeat Thought/Action/Observation as needed) Final Answer: ... If no action is needed, go directly to Final Answer. Input Format: <chart image> {entry[\"image\"]} <question> {entry[\"query\"]} L.2.4 Tabular Question-Answering For few chart-based baselines that output structured tables rather than direct answers, we apply tabular question-answering prompt. This prompt instructs the GPT-4o model to use the extracted table together with the users question to produce concise answer. Tabular Question-Answering Prompt Instruction: Given the data table and the users question, use the table to determine and provide the answer in single word or short phrase. Input Format: <extracted_table> {entry[\"extracted_table\"]} <user_question> {entry[\"query]} L.3 Evaluation Prompts Recall that we evaluate model predictions using two strategies: (1) standardization-based accuracy computation, and (2) GPT-Accuracy metric based on the LLM-as-a-Judge paradigm. The first method uses GPT-4o to standardize responses before applying an arithmetic-based correctness check, with strict 5% relative error tolerance for numeric responses and string matching for non-numeric ones. The second method prompts an LLM to assess correctness directly, also applying 5% tolerance for numeric responses. The prompts used for both evaluation strategies are provided in L.3.1 and L.3.2, respectively. L.3.1 Accuracy The following prompt is used to standardize both the ground truth and predicted responses before performing the accuracy check. GPT-4o is instructed to remove units (e.g., for thousand, for million, for billion), convert scales, eliminate symbols, and standardize number formats. Once standardized, numeric responses are evaluated arithmetically using strict 5% relative error tolerance, while non-numeric responses require string match."
        },
        {
            "title": "Prompt for Standardizing Ground Truth and Predicted Responses",
            "content": "Instruction: You are given question, ground truth answer, and models predicted answer. Your task is to determine whether the prediction is correct. Follow these steps exactly: 1. If both answers are numeric, first extract the numeric portion of each value. Normalize both answers carefully by checking both the <groundtruth answer> and <predicted answer> values in context. If both values include the same unit (e.g., K, M, B, or their full-word equivalents like thousand, million, billion), do not scale. Simply strip the unit and compare the base numbers. Example: 20K vs 21K compare 20 vs 21. If one value has unit and the other is already scaled (e.g., 21K vs 21000), convert the unit-based value to its full numeric form before comparison. Example: 21K vs 21000 compare 21000 vs 21000. If only one value includes unit and the other is not clearly scaled, strip the unit and compare only the numeric parts (without scaling either). Examples: 16M vs 16 compare 16 vs 16 500M vs 500 compare 500 vs Apply scaling only when the context requires it, based on whether the values represent scaled vs. unscaled forms of the same quantity. Additional examples: 20K vs 21K compare 20 vs 21 20K vs 21000 compare 20000 vs 21000 16M vs 16 compare 16 vs 16 500 vs 500M compare 500 vs 500 142.6 vs 1,350 million compare 142.6 vs 1350 170.0 vs 160 million compare 170 vs 160 20B vs 21 compare 20 vs 21 Convert number words (e.g., ten, eleven, forty-two) to digits. Remove commas, currency symbols, and surrounding text. Ignore case when processing text. 2. If the answers are not numeric, return the string response. Return JSON object in the following format: { \"ground_truth_filtered\": <normalized ground truth value>, \"response_filtered\": <normalized predicted value> } Input Format: <question> {entry[\"query\"]} <groundtruth answer> {entry[\"ground_truth\"]} <response> {original_response} L.3.2 LLM-as-a-Judge The following prompt is used to evaluate response correctness using the LLM-as-a-Judge baseline, also referred to as GPT-Accuracy in prior literature (Xu et al., 2023; Masry et al., 2022; Xia et al., 2024). The LLM (GPT-4o) is shown the question, ground truth, and model prediction, and is asked to assess whether the prediction is correct, with 5% error tolerance applied to numeric answers. While flexible, this method may be imprecise for fine-grained numeric evaluation, as discussed in Sections 4 and J.5. LLM-as-a-Judge Prompt for Evaluating Response Correctness Instruction: Given multiple QA pairs and the corresponding predictions, evaluate the correctness of each prediction. Return only single word: \"True\" or \"False\". If the ground truth answer is numeric value (with or without units), allow 5% error tolerance when comparing against the prediction. Input Format: <question> {entry[\"query\"]} <groundtruth answer> {entry[\"ground_truth\"]} <response> {original_response} L.4 Complexity Analysis Prompts Each chartquestion pair in our dataset is annotated with two types of complexity labels: visual complexity and reasoning complexity. The prompts used to generate these labels are shown in L.4.1 and L.4.2, respectively. L.4.1 Visual Complexity The following prompt categorizes charts by visual complexityEasy, Medium, or Hardbased solely on the visual effort needed to interpret the information presented in the chart image."
        },
        {
            "title": "Visual Complexity Rating Prompt",
            "content": "Instruction: You are data analyst, skilled at visually interpreting charts. You are shown only chart image, with no additional context or question. Your task is to assess the visual complexity of the chart based solely on what you see. Think step by step: Count the number of visible elements (e.g., data points, lines, bars, rings, legends, labels, colors, gridlines). Assess how cluttered or clean the layout appears. Judge whether understanding the chart requires visual comparisons, dense reading, or reasoning over multiple components. Examine the clarity of legends, axis ticks, text placement, and grouping. Charts tend to be visually complex if they include: Multiple nested or layered elements 3D perspectives or overlapping dimensions Low contrast or overlapping visual elements Radar/polar charts with filled or intersecting shapes Multi-axis designs (e.g., dual Y-axes) Overlaid plot types (e.g., bars + lines) Dense scatter plots with tightly packed points Stacked formats requiring segment comparison Ambiguous or visually similar elements that are hard to distinguish When in doubt especially in the presence of distortion, depth, layering, or ambiguity prefer labeling as Hard. Respond in the following JSON format: { \"label\": \"Easy / Medium / Hard\", \"reasoning\": \"Step-by-step explanation of how you reached your conclusion.\" } Input Format: <chart image> {entry[\"image\"]} L.4.2 Reasoning Complexity The following prompt categorizes chartquestion pairs by reasoning complexityEasy, Medium, or Hardbased solely on the level of reasoning needed to interpret and answer the question using the chart image."
        },
        {
            "title": "Reasoning Complexity Rating Prompt",
            "content": "Instruction: You are data analyst, skilled at solving visual questions over chart images. You are shown chart image and corresponding question. Your task is to assess the reasoning complexity required to answer the question correctly. Think step by step: 1. Identify the key visual elements referenced by the question. 2. Determine the number of distinct reasoning steps needed to answer it. 3. Evaluate the complexity of each step, considering: The need for precise perception (e.g., color or shape differentiation, relative positioning) Cross-referencing multiple regions, axes, or visual types Complex chart features (e.g., stacked vs. overlayed areas, 3D perspective) Occlusion or ambiguity in label visibility (e.g., overlapping text or hidden legends) Requirement for highly precise numerical interpretation (especially in visually challenging layouts) Typically Hard cases include: Area charts requiring distinction between stacking vs overlay 3D charts with visual distortion or unclear projections Double-ring pie charts requiring ring disambiguation Radar charts with overlapping regions Multi-axis charts with axis disambiguation needs Perceptually ambiguous cases (e.g., boxplots with red boxes and red medians) Multi-step numerical comparison questions (e.g., \"How much higher is than Y?\") Charts with occluded or obscured labels or legends Typically Medium cases include: Multi-bar or multi-line charts with separated groups Node-link diagrams requiring structured inspection Two-step quantitative reasoning (e.g., compute and compare and Y) Tasks involving comparison or arithmetic over multiple extracted values Typically Easy cases include: Annotated charts where answers can be read off directly Questions solvable via clearly readable text Simple selection (e.g., identifying the maximum value) Respond in the following JSON format: { \"label\": \"Easy / Medium / Hard\", \"reasoning\": \"Step-by-step explanation of how you reached your conclusion.\" } Input Format: <chart image> {entry[\"image\"]} <question> {entry[\"query\"]}"
        }
    ],
    "affiliations": [
        "J.P. Morgan AI Research"
    ]
}