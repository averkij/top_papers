{
    "paper_title": "Medal S: Spatio-Textual Prompt Model for Medical Segmentation",
    "authors": [
        "Pengcheng Shi",
        "Jiawei Chen",
        "Jiaqi Liu",
        "Xinglin Zhang",
        "Tao Chen",
        "Lei Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Medal S, a medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal S achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. A lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal S offers two prompting modes: a text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and a hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, a two-stage inference strategy, and post-processing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal S outperforms SAT with a DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal S achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal S will be publicly available at https://github.com/yinghemedical/Medal-S."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 0 0 3 1 . 1 1 5 2 : r Medal S: Spatio-Textual Prompt Model for Medical Segmentation Pengcheng Shi1, Jiawei Chen1,4, Jiaqi Liu1, Xinglin Zhang1, Tao Chen1,3, and Lei Li1,2((cid:0)) 1 Medical Image Insights, Shanghai, China 2 University of Washington, Seattle, WA, USA 3 University of Waterloo, Waterloo, ON, Canada 4 Xian Jiaotong University, Xian, China xinglinzh@gmail.com lenny.lilei.cs@gmail.com Abstract. We introduce Medal S, medical segmentation foundation model that supports native-resolution spatial and textual prompts within an end-to-end trainable framework. Unlike text-only methods lacking spatial awareness, Medal achieves channel-wise alignment between volumetric prompts and text embeddings, mitigating inaccuracies from resolution mismatches. By preserving full 3D context, it efficiently processes multiple native-resolution masks in parallel, enhancing multi-class segmentation performance. lightweight 3D convolutional module enables precise voxel-space refinement guided by both prompt types, supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy modalities in the BiomedSegFM dataset. Medal offers two prompting modes: text-only mode, where model predictions serve as spatial prompts for self-refinement without human input, and hybrid mode, incorporating manual annotations for enhanced flexibility. For 24-class segmentation, parallel spatial prompting reduces inference time by more than 90% compared to sequential prompting. We propose dynamic resampling to address target-patch ratio imbalance, extending SAT and nnU-Net for data augmentation. Furthermore, we develop optimized text preprocessing, two-stage inference strategy, and postprocessing techniques to improve memory efficiency, precision, and inference speed. On the five-modality average on the validation set, Medal outperforms SAT with DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Medal achieves excellent performance by harmonizing spatial precision with semantic textual guidance, demonstrating superior efficiency and accuracy in multi-class medical segmentation tasks compared to sequential prompt-based approaches. Medal will be publicly available at https://github.com/yinghemedical/Medal-S. Keywords: Medical Segmentation Foundation Model Spatial and Textual Prompts. 1 Project Lead 2 ((cid:0))Corresponding author P. Shi et al. Fig. 1. Left: Example renders from the BiomedSegFM challenge dataset (original images and segmentation masks) covering five imaging modalities: CT, MRI, microscopy, PET, and ultrasound. Top-right: Sample text prompts. Bottom-right: Key challenges include (1) multi-modal heterogeneity, (2) multi-class segmentation, and (3) targetpatch ratio imbalance, causing spatio-textual misalignment, sequential inference inefficiency, and FP/FN errors. Our solutions: channel-wise prompt alignment (2.3), parallel spatial prompts (2.3), and dynamic resampling (2.5)."
        },
        {
            "title": "Introduction",
            "content": "Medical image segmentation, the precise delineation of anatomical structures and pathologies within medical volumes, is fundamental to computational healthcare. Despite its importance, challenges persist due to the diversity of imaging modalities and anatomical variations. Recent advances in foundation models, notably the Segment Anything Model (SAM) [13] and its successor, SAM 2 [19], have transformed natural image segmentation by introducing promptable models that generalize across various image distributions and tasks. However, directly applying these models to medical volumes is hindered by the intrinsic differences between natural and medical images. Adaptations of foundation models for medical image segmentation have followed distinct strategies, each with inherent trade-offs. Early approaches, such as MedSAM [16], extended SAMs 2D capabilities to medical images, primarily using bounding box prompts. Similarly, ScribblePrompt [23], 2D model, improved segmentation accuracy for unseen labels and image types by supporting flexible annotation styles, including bounding boxes, clicks, and scribbles. To overcome the limitations of 2D methods and leverage 3D spatial information, subsequent models incorporated 3D spatial prompts. For example, SAMMed3D [22], SegVol [5], and VISTA3D [9] introduced dedicated 3D prompting mechanisms. VISTA3D [9] enabled automatic and interactive 3D segmentation with spatial prompts, facilitating efficient inspection and editing by clinicians. SegVol [5] expanded prompt types to include spatial and semantic cues, imMedal 3 proving precision and semantic disambiguation. Beyond this, MedSAM2 [18] advanced 3D segmentation by using intuitive 2D interactions to generate full 3D segmentations. MedSAM2 [18], akin to SAM 2 [19], supports segmentation of 3D medical images and videos, primarily using bounding box prompts and memory-conditioned features. Most recently, nnInteractive [6], built on the nnU-Net [11] framework, introduced comprehensive 3D interactive open-set segmentation method. This approach supports diverse spatial prompts, including points, scribbles, boxes, and novel lasso prompt, leveraging 2D interactions to produce complete 3D segmentations with superior performance. Despite these advancements, current medical segmentation models face significant limitations with spatial prompts. Models like SegVol [5] and SAM-Med3D [22] often rely on multiple downsampling operations for spatial prompts, while VISTA3D [9] downsamples spatial point prompts, leading to substantial loss of voxel-level details. In contrast, nnInteractive [6] incorporates spatial prompts at the native resolution, preserving 3D spatial context. Moreover, existing spatial prompting methods process multiple classes sequentially rather than in parallel, reducing inference efficiency and limiting the models ability to learn features across interrelated anatomical regions. On the text prompt front, models like CLIP-Driven Universal Model [15] and SegVol [5] utilize simple semantic classes as prompts, while VISTA3D [9] employs similar class-based prompts. However, such categorical prompting often lacks flexibility in practice. More recent models, such as SAT [28] and BioMedParse [27], adopt text-only prompting paradigms. However, BioMedParse, 2D model, remains limited in handling 3D medical images. These approaches often sacrifice 3D spatial context and lack spatial prompts, hindering self-iterative refinement and real-world correction capabilities. CAT [10] attempts to integrate spatial anatomical information with text prompts but embeds cropped regions after multiple downsampling steps, failing to utilize spatial prompts at native resolution. Additionally, its complex contrastive learning approach for inter-class relationships is less streamlined. This fragmentation creates tension between preserving native-resolution spatial prompts and achieving efficient multimodal processing. Ideally, spatial prompts for different classes and their corresponding text should maintain one-to-one channel-wise correspondence at the native resolution. In text-guided controllable generation, several works have successfully integrated native-resolution segmentation masks with text prompts. Prior works like MakeAScene [7] and SpaText [1] have demonstrated effective fusion of segmentation masks and text for controllable generation. ControlNet [25] further enhances this through spatial conditioning of diffusion models. However, such joint textual and native-resolution spatial prompts approaches remain unexplored for medical image segmentation. To address these challenges, we introduce Medal S, medical segmentation foundation model that natively supports both spatial and textual prompts in an end-to-end framework. Medal aligns with initiatives like ScaleMAI [14] and supports datasets including RadGenome-Chest CT [26] and RadGPT [2]. Our key contributions are: 4 P. Shi et al. novel channel-wise alignment between volumetric prompts and text embeddings through text embedding transformation and lightweight 3D convolution, addressing spatial prompt-text misalignment and enabling precise simultaneous refinement. Parallel spatial prompting at native resolution with simultaneous 3D spatial/textual input for multi-class segmentation. Maintains full fidelity and provides more than 10 faster inference (24 classes) vs. sequential processing. Dynamic resampling for target-patch ratio imbalance (building upon SAT [28] and nnU-Net [11]), with optimized text preprocessing, two-stage inference, and post-processing, achieving fast inference, memory efficiency, and excellent performance. Comprehensive support for 243 classes across CT, MRI, PET, ultrasound, and microscopy (BiomedSegFM dataset), featuring both text-only self-refinement and hybrid manual annotation modes for enhanced clinical flexibility."
        },
        {
            "title": "2 Method",
            "content": "Our proposed Medal framework presents novel approach to universal medical image segmentation by synergistically integrating spatial prompts with textdriven feature adaptation. As illustrated in Fig. 2, the framework consists of three key components: (1) An image encoder that extracts multi-scale visual features, (2) text encoder that processes prompt embeddings, and (3) query decoder that fuses visual and textual features to produce adapted embeddings. Parallel spatial promptswhether simulated, predicted, or annotatedare processed at native resolution and aligned with spatio-textual features through channel-wise alignment. The framework supports iterative self-refinement for precise segmentation, offering both robustness and flexibility in medical segmentation."
        },
        {
            "title": "2.1 Prompt Encoder",
            "content": "The prompt encoder comprises two components: foreground spatial prompt encoding and textual prompt encoding, with implementations inspired by nnInteractive [6] and SAT [28] respectively. Foreground spatial prompt encoding To enhance the models focus on target foreground regions, we generate foreground spatial prompt Sf R1HW by aggregating parallel spatial prompts Sp RN HW (obtained from either previous predictions or user annotations) through binary thresholding operation: Sf = (cid:33) S(i) (cid:32) (cid:88) i=1 (1) Medal 5 Fig. 2. Medal framework pipeline. Multi-scale visual features from the image encoder and text embeddings from the text encoder are fused by query decoder into adapted embeddings. Parallel spatial prompts (simulated, predicted, or annotated) are processed at native resolution and aligned via channel-wise matching, maintaining full fidelity. This achieves greater than 10 speedup for 24-class segmentation versus sequential processing (see Fig. 4) and supports iterative self-refinement for precise segmentation. where H() is the Heaviside step function: H(x) = (cid:40) 1 0 if > 0 otherwise (2) The resulting Sf is concatenated with the input image as additional channels to the U-Net encoder, similar to the native resolution prompt in nnInteractive. Textual prompt encoding We employ frozen pre-trained text encoder Φtext from the SAT framework to process medical terminology prompts = {t1, . . . , tN }: zj = Φtext(tj), where zj represents the embedding for anatomical target tj. zj Rd (3)"
        },
        {
            "title": "2.2 Spatial Prompt Generation",
            "content": "We introduce spatial prompt generation method Gprompt that enhances segmentation robustness for both interactive applications and autonomous refinement. 6 P. Shi et al. The generator produces realistic coarse segmentations from ground truth masks 0, 1N HW D, where represents semantic channels and (H, W, D) denote spatial dimensions. The method outputs two complementary binary prompts: single-channel global foreground prompt Sf 0, 11HW and multichannel class-specific prompt Sp 0, 1N HW D. The generation process applies controlled stochastic transformations through five key parameters. The drop probability range [pmin drop] [0, 1]2 regulates false negative simulation by removing mask blocks, while the add probability range [pmin add ] controls false positive generation through block additions. Channel-level variations are introduced via pchan-zero, which nullifies entire channels in Sp, and pzero determines the probability of returning empty prompts. The block size set Z3 defines the possible 3D block dimensions for these transformations. drop, pmax add, pmax 1 As detailed in Algorithm 1, the process begins by potentially returning empty prompts when random sample falls below pzero. Otherwise, Meff is created by randomly zeroing out channels in according to pchan-zero. The single-channel prompt Sf is generated by channel summation and binarization of Meff. For block operations, the method samples block dimensions [bh, bw, bd] B, establishing transformation grid. Mutually exclusive drop and add masks (Bdrop and Badd) are generated using probabilistically sampled parameters, upsampled to full resolution, and applied to both prompt types. The multi-channel prompt additionally incorporates class-specific variations through random channel assignment of modified blocks. Final outputs undergo binarization to maintain strict 0,1 values. This approach systematically simulates diverse input conditions ranging from coarse segmentations to imperfect user annotations, significantly improving model generalization across varying input qualities while maintaining anatomical plausibility. The stochastic yet controlled transformations enable robust handling of real-world scenarios where prompt quality may vary substantially."
        },
        {
            "title": "2.3 Query Decoder",
            "content": "Our query decoder builds upon the architectures of SAM [13] and SAT [28], while incorporating key design principles from DETR [3] and MaskFormer [4] for segmentation tasks. Departing from conventional approaches that compress dense prompts into low-dimensional mask embeddings (e.g., SAM-Med3D [22], SegVol [5], and VISTA3D [9]), our method introduces novel preservation of the complete 3D spatial context in volumetric prompts. This preservation proves particularly vital for 3D medical imaging applications, where the maintenance of native spatial resolution directly impacts diagnostic accuracy. The decoders architecture establishes precise channel-wise alignment between volumetric prompts and text embeddings, effectively mitigating the accuracy degradation typically caused by resolution mismatches. This design enables efficient parallel processing of multiple native-resolution masks, yielding significant improvements in multi-class segmentation performance. Furthermore, we Medal 7 Algorithm 1 Spatial Prompt Generation Require: {0, 1}N HW D, [pmin drop, pmax 1 drop], [pmin add, pmax add ], pchan-zero, pzero [0, 1], add > 0 then c=1 Meff,c > 0) return 01HW D, 0N HW Ensure: Sf {0, 1}1HW D, Sp {0, 1}N HW 1: if Random(0, 1) < pzero then 2: 3: end if 4: Meff ChannelMask(M, pchan-zero) 5: Sp Meff 6: Sf ((cid:80)N 7: if pmax 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end if 19: Sf (Sf > 0), Sp (Sp > 0) 20: return Sf , Sp drop > 0 or pmax [bh, bw, bd] RandomChoice(B) [nh, nw, nd] [H/bh, W/bw, D/bd] pd U(pmin add, pmax add ) Bdrop Random(nh, nw, nd) < pd Badd (Random(nh, nw, nd) < pa) (Bdrop) Udrop, Uadd Upsample(Bdrop, (H, W, D)), Upsample(Badd, (H, W, D)) Sf Sf (1 Udrop) Uadd Ckeep AssignBlocksToChannels(Bdrop, ) Cadd AssignBlocksToChannels(Badd, ) Sp Sp Ckeep Cadd drop), pa U(pmin drop, pmax incorporate lightweight 3D convolutional module that jointly optimizes voxelspace features using both prompt modalities while maintaining their channelwise alignment, ensuring accurate and robust 3D segmentation across targets with varying semantics. The query decoder operates on per-voxel features RCHW D, where (H, W, D) denote the voxel grid dimensions and represents the feature channels. These features are derived through progressive upsampling of visual encoder outputs with skip connections in U-Net-style architecture [20]. Concurrently, the decoder receives adapted text embeddings RN produced by transformer-based query decoder [21], where indicates the number of semantic queries (corresponding to anatomical targets). The query decoder adapts text embeddings RN (with as the text dimension) using multi-scale visual features RCV HV WV DV according to: = Φquery(V, Z) The core innovation of our approach lies in the spatial prompt refinement module. This module enhances per-voxel features through an interaction mechanism between adapted text embeddings and parallel spatial prompts Sp RN HW (which originate from either previous predictions or user an8 P. Shi et al. notations during inference). The refinement process begins with the computation of spatio-textually aligned features Fa RCHW via: Fa = TSp where RCN denotes the transposed adapted text embeddings. We then concatenate Fa with the original features along the channel dimension, resulting in R2CHW tensor. This combined representation is processed by lightweight 3D convolutional module inspired by nnU-Nets native-resolution skip connection architecture [11], producing refined features Fr RCHW D: Fr = Conv([F; Fa]) The final per-voxel prediction RN HW is obtained through voxelwise correlation between queries and refined features, followed by sigmoid activation σ(): = σ (TFr) This approach produces multi-channel probability map with dedicated channels for each anatomical structures and pathological region, facilitating complete 3D volumetric segmentation. 2."
        },
        {
            "title": "Iterative Inference",
            "content": "Our query decoder employs an iterative inference approach inspired by Masked Autoencoders (MAE) [8]. The algorithm progressively refines predictions through multiple iterations, where each output P(t) RN HW serves as the spatial prompt Sp for subsequent iterations. The random masking mechanism facilitates prediction in unprompted regions, with complementary masked predictions aggregated to improve robustness against input noise. As detailed in Algorithm 2, each iteration consists of four key components: (1) feature enhancement through cross-attention between text queries and spatial prompts, (2) rounds of random block masking using block sizes = 4, 8, (3) parallel prediction of both masked and unmasked regions, and (4) prediction averaging across all rounds. Medal-S supports two distinct prompting strategies. The text-only mode initializes with zero tensors and relies solely on text prompts with self-refinement, while the hybrid mode incorporates external spatial cues such as manual annotations when configured. The inference pipeline orchestrates prompt generation and iterative refinement through coordinated function calls, dynamically updating spatial prompts to enhance segmentation accuracy throughout the process."
        },
        {
            "title": "2.5 Dynamic Resampling",
            "content": "Dynamic resampling addresses the challenge of varying segmentation target sizes relative to fixed patch size in medical image segmentation. When the target Algorithm 2 Iterative Query Decoder Inference Require: Voxel features RCHW D, queries RN , initial prompt Sp RN HW D, iterations , parameters Θ, block sizes = {4, 8}, repetitions = 1 Medal 9 Conv([F; E(t)]; Θ) E(t) TS(t1) F(t) P(t) σ(TF(t) ) S(t) P(t) Psum 0 for = 1 to do Ensure: Prediction RN HW 1: P(0) 0 2: for = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end for 22: return P(T ) RandomChoice(B) Nh H/b, Nw W/b, Nd D/b Nselected max(1, (Nh Nw Nd)/2) Mb RandomMask(Nh, Nw, Nd, Nselected) Upsample(Mb, (H, W, D)) Mc 1 P1 Model(T, P(t), M; Θ) P2 Model(T, P(t), Mc; Θ) Ppatch P1 Mc + P2 Psum Psum + Ppatch end for P(t) Psum/R size significantly exceeds the patch size, partial visibility of the target within patch can lead to false positives (FP), as the model lacks global context and may misinterpret background noise as part of the target. Conversely, when the target is much smaller than the patch, the imbalance between foreground and background can result in false negatives (FN), as the model struggles to focus on small, critical regions. To mitigate these issues, we propose dynamic resampling strategy that adjusts the voxel spacing of the input image based on the physical size of the smallest foreground connected component or the smallest class-specific target, ensuring balanced representation within the fixed patch size used by the model. Our approach begins by identifying the smallest foreground connected component or the smallest target class in each case, which serves as the reference for resampling. Ideally, each target would have tailored resampling rate to optimize its representation within the patch. However, to balance computational efficiency during training and inference, we focus on the smallest target to determine the resampling parameters. The core idea is to adjust the current spacing = [sx, sy, sz] of the image to target spacing = [tx, ty, tz] such that the physical dimensions of the target align with the patch size = [px, py, pz]. The adjusted spacing for each dimension is computed as: P. Shi et al. = max min (cid:16) (cid:16) (cid:17) ti, piαti di (cid:17) ti, piαti di , , if si > ti, otherwise, where is the adjusted spacing, di is the image dimension, and α is scale factor. This formula ensures that the physical size of the resampled image fits within the patch while preserving sufficient detail. To prevent excessive resampling, we impose constraints such that remains within practical bounds, depending on the target class and inference stage."
        },
        {
            "title": "2.6 Two-stage Inference",
            "content": "We propose two-stage inference strategy to optimize computational efficiency and segmentation accuracy for medical imaging, particularly for localized regions like focal lesions or anatomical structures. The coarse-to-fine strategy first performs low-resolution segmentation to identify regions of interest (ROIs), followed by high-resolution refinement to capture fine details. This approach is efficient for datasets with small foreground regions, reducing inference time compared to full high-resolution processing. The strategy trains two models: one for coarse segmentation at voxel spacing of (1.5, 1.5, 3.0) and another for high-resolution segmentation at (1.0, 1.0, 1.0). In the first stage, images are processed using sliding window with crop size of (224, 224, 128), corresponding to physical field of view of approximately (336, 336, 384). This coarse resolution enables rapid ROI detection. The output mask highlights potential target regions, but if no foreground is detected, the strategy defaults to full-volume high-resolution inference to avoid missing subtle targets. In the second stage, the ROI is extracted based on the coarse segmentations non-zero predictions, scaled by factor (1.1 to 1.5) to include context. The image is resampled to target spacing of (1.0, 1.0, 1.0) with crop size of (192, 192, 192). To manage memory, the physical volume = (cid:81)3 i=1 si di (where si and di are voxel spacing and dimension size) is constrained by threshold Vthreshold = (1.8)3 (cid:81)3 i=1 ci, with ci as the crop size. If exceeded, voxel spacing is adjusted to satisfy si di 1.9 ci ti, where ti is the target spacing, ensuring memory usage stays below 32 GB. The second stage refines segmentation using the coarse predictions as spatial prompts, enhancing accuracy for small or intricate structures like synaptic clefts or micro-lesions. sliding window approach ensures high precision despite increased computational cost. The strategy allows flexible use of either stage: the first for rapid analysis or the second for high-resolution segmentation when resources permit. This adaptability balances efficiency and accuracy, making the approach suitable for diverse medical imaging applications. Medal"
        },
        {
            "title": "2.7 Text Prompts Preprocessing",
            "content": "To effectively preprocess text prompts from the BiomedSegFM dataset, systematic approach is employed to extract modality and class-specific identifiers, enabling dynamic resampling and post-processing strategies. The methodology begins by parsing the text prompts JSON to generate class mapping, which assigns unique identifiers to anatomical structures and lesions across modalities (CT, MRI, US, PET, Microscopy). This mapping is constructed by extracting modality information from dataset prefixes and standardizing class names, such as mapping \"Left renal structure\" to \"Left kidney\" or \"Myocardium\" to \"Heart\". The resulting class mapping, stored as JSON file, ensures each class within modality has unique identifier, facilitating consistent encoding. Next, variant mapping is created to handle diverse terminologies in the prompts. This mapping accounts for anatomical and lesion variants, incorporating directionality (e.g., \"left\" or \"right\") and suffixes (e.g., \"lesions\", \"tumors\"). For instance, \"hepatic lesions\" is mapped to \"Liver lesions\" using predefined rules and regular expressions to detect directional patterns. The variant mapping prioritizes longer, more specific terms to avoid partial matches, ensuring \"Brainstem\" is distinguished from \"Brain\". This preprocess yields comprehensive variant mapping JSON, covering all prompt variations. For training and inference, text preprocessing function extracts modality and class information from each prompt. Given sentence and instance label (0 for anatomy, 1 for lesion), the function identifies the modality (e.g., CT, MRI, US, PET, or Microscopy) by matching keywords. It then retrieves the class identifier cid and canonical name cname from the class mapping, using the variant mapping to handle term variations. The function prioritizes longer matches to ensure specificity, formalized as: (cid, cname) = arg max kK (cid:0)len(k) s, m (cid:1) , is the modalitywhere is the set of terms (class names and variants), specific class dictionary for label l, and len(k) is the term length. Directional patterns are detected using regular expressions to refine matches, such as distinguishing \"Left kidney\" from \"Kidney\". The extracted modality and class identifier cid are input to text encoder, producing embeddings that guide dynamic resampling and post-processing. For example, the extracted modality enables the text encoder to distinguish between different modalities, while resampling strategies leverage cid to apply classspecific target spacing, or post-processing leverages cid to apply class-specific segmentation refinements. This streamlined pipeline ensures robust handling of diverse text prompts, enhancing segmentation accuracy."
        },
        {
            "title": "2.8 Loss Function",
            "content": "We employ combined loss = LBCE + LDice, standard in medical image segmentation. For classes and voxels: 12 P. Shi et al. LBCE ="
        },
        {
            "title": "1\nM C",
            "content": "(cid:88) n, [sn,c log pn,c + (1 sn,c) log(1 pn,c)] LDice = 1 2 (cid:80) n, cpn,csn,c n,c + (cid:80) n,c s2 n,c p2 n,c (cid:80) where pn,c and sn,c are the predicted probability and ground truth (0 or 1) for class at voxel c. This combination optimizes both pixel-level accuracy and region-based overlap."
        },
        {
            "title": "2.9 Post-processing",
            "content": "Our post-processing method refines segmentation results by suppressing spurious predictions while preserving anatomically plausible structures, improving upon nnU-Net [11]. Unlike nnU-Net, which retains only the largest connected component across all classes in single operation, our approach processes each class independently and leverages probability maps to prioritize components based on both probability and size. As outlined in Algorithm 3, given probability map RN HW D, where is the number of classes, the segmentation map R1HW is derived by computing the maximum probability across classes, pmax = maxj=1,...,N Pj, and the corresponding class index, cmax = arg maxj=1,...,N Pj. Voxels are assigned class labels lj {1, . . . , } where pmax 0.5, i.e., = lcmax if pmax 0.5, otherwise = 0 (background). For each class {1, . . . , }, binary mask Ml = (S = l) is created. Connected components in Ml are labeled using 6-connectivity, yielding labeled image Cl and component sizes Σl = {(ci, si)}. The mean probability for component ci is computed as the average of Pl over voxels where Cl = ci. Among the top three largest components, those with mean probabilities within τ = 0.1 of the maximum and above 0.86 are retained. If none qualify, the highest-probability component is kept if it is among the two largest and its size is at least 0.6 times the largest; otherwise, the largest component is selected. The refined mask updates by setting = 0 where Ml . This method enhances nnU-Net by processing classes individually and using probabilities to guide component selection, improving multi-class segmentation robustness."
        },
        {
            "title": "3.1 Data and Evaluation Methodology",
            "content": "The development set builds upon the CVPR 2024 MedSAM on Laptop Challenge [17], incorporating additional 3D cases sourced from publicly available datasets5. This collection encompasses various standard 3D imaging modalities, 5 Complete dataset details can be found at https://medsam-datasetlist.github. io/ Algorithm 3 Post-processing Require: RN HW D, labels {1, . . . , }, background = 0, threshold τ = 0.1, connectivity = 6 Medal 13 Ensure: Refined segmentation R1HW 1: pmax maxj=1,...,N Pj, cmax arg maxj=1,...,N Pj 2: 0, S[pmax 0.5] lcmax 3: for = 1 to do 4: Ml (S = l) Cl, Σl ConnectedComponents(Ml, k) 5: 6: if Σl = then continue 7: end if SortBySize(Σl)[: 3] 8: PT {(ci, mean(Pl[Cl = ci])) ci } 9: 10: pmax max{pi (ci, pi) PT } 11: {ci (ci, pi) PT , (pmax pi) τ, pi > 0.86} 12: if 2 then 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end for 25: return cmax arg maxci {pi (ci, pi) PT } T2 [: 2] if cmax T2 and Σl(cmax)/Σl(T [0]) > 0.6 then end if S[Ml (Cl = cmax) (Cl = [0]) (Cl K) end if ] else else including Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Positron Emission Tomography (PET), Ultrasound, and Microscopy. The hidden test set was collaboratively developed by the community, consisting exclusively of previously unpublished cases. All annotations were either supplied by data contributors or generated by the challenge organizers using 3D Slicer [12] and MedSAM2 [18]. Participants have the option to either use the full training set or participate in the coreset track, which permits model development using only 10% of the total training cases. The text-guided segmentation task evaluates both semantic and instance segmentation performance. Semantic segmentation assessment employs two metrics: the Dice Similarity Coefficient (DSC) for measuring region overlap and Normalized Surface Distance (NSD) for evaluating boundary accuracy. Instance segmentation performance is quantified using the F1 score at 0.5 overlap threshold, along with DSC scores for correctly identified instances. runtime constraint of 60 seconds per class is enforced - submissions exceeding this limit will receive zero scores for all DSC and NSD metrics on the affected test cases. 14 P. Shi et al."
        },
        {
            "title": "3.2\nData preprocessing Consistent with MedSAM [16], all medical images were\nconverted to npz format and normalized to an intensity range of [0, 255]. For CT\nscans specifically, we performed Hounsfield unit normalization using standard\nwindow settings: soft tissues (width:400, level:40), lung (width:1500, level:-160),\nbrain (width:80, level:40), and bone (width:1800, level:400). Following this nor-\nmalization, the intensity values were linearly scaled to the target range of [0, 255].\nFor non-CT imaging modalities, we first truncated intensity values at the 0.5th\nand 99.5th percentiles before applying the same rescaling procedure. Images\nthat already had native intensity values within the [0, 255] range underwent no\nadditional preprocessing steps.",
            "content": "Table 1. Training protocols. Pre-trained Model Batch size Patch size Spacing Total steps Optimizer Initial learning rate (lr) Lr decay schedule Training time Loss function Number of model parameters SAT 4 224224128 (1.5, 1.5, 3.0) 108600 AdamW 1e-4 cosine 168 hours BCE+Dice 221M Table 2. Training protocols for the 2nd model Pre-trained Model Batch size Patch size Spacing Total steps Optimizer Initial learning rate (lr) Lr decay schedule Training time Loss function Number of model parameters SAT 8 192192192 (1.0, 1.0, 1.0) 91300 AdamW 1e-4 cosine 160 hours BCE+Dice 221M Training Protocols To handle large-scale datasets for fast preprocessing and data loading, the dynamic resampling strategy mentioned in Section 2.5 is deMedal 15 veloped based on the latest version of the resampling function from the nnUNet [11] framework. The resampling function in the latest version of nnU-Net significantly improves the efficiency of resampling large-scale data by leveraging CPU processing. Additionally, the latest Blosc2 compression format from nnUNet is adopted to compress npz files, achieving balance between file storage size and read speed during dataloader operations. For the preprocessing of 3D images, dataloader, and data augmentation, most of the functions and code from nnU-Net are retained with appropriate modifications. For the text dataloader and simultaneous training across multiple datasets, different sampling rates are set for each dataset, along with adjustments to the positive-negative sample ratio and padding alignment for varying batch text prompt lengths. Multi-GPU training is primarily based on the SAT [28]. The optimal model selection criteria are also based on SAT, as the learning rate curve decreases gradually with each epoch; by default, the model from the final iteration is adopted. The training configurations are as follows: for the 224224128 input size, we use batch size of 2 per GPU across 2 GPUs (effective batch size of 4), while for the 192192192 input size, we employ batch size of 2 per GPU across 4 GPUs (effective batch size of 8). The complete training process takes 7 days (168 hours) to complete. Detailed environment settings and training protocols are presented in Table 3,Table 1, and Table 2. Environment settings The development environments and requirements are presented in Table 3. Table 3. Development environments and requirements. Ubuntu 22.04.4 LTS (Jammy Jellyfish) Intel(R) Xeon(R) Platinum 8468 CPU @2.10GHz 2TB DDR (1.8TB available) System CPU RAM GPU (number and type) Eight NVIDIA H100 80GB HBM3 CUDA version Programming language Python 3.10.16 Deep learning framework torch 2.2.0, torchvision 0.17.0 12."
        },
        {
            "title": "4.1 Quantitative Results on Validation Set",
            "content": "Table 4 presents validation results on the all-data track. Our Medal model consistently outperforms the CAT and SAT baselines, as well as the single-stage ablation Medal (w/o stage1), across nearly all metrics and modalities. It falls 16 P. Shi et al. Table 4. Quantitative evaluation results of the validation set on the all-data track. Modality Method CT MRI Microscopy PET Ultrasound Average Semantic Segmentation Instance Segmentation DSC CAT 72.11 BiomedParse-V 85.12 67.80 SAT Medal (w/o stage1) 80.36 81.87 Medal (w/o post) 81.90 Medal 54.15 CAT 73.96 BiomedParse-V SAT 56.10 Medal (w/o stage1) 62.90 61.96 Medal (w/o post) 61.95 Medal - CAT - BiomedParse-V - SAT - Medal (w/o stage1) - Medal (w/o post) - Medal - CAT - BiomedParse-V - SAT - Medal (w/o stage1) - Medal (w/o post) - Medal 85.94 CAT 90.50 BiomedParse-V SAT 85.58 Medal (w/o stage1) 77.99 82.37 Medal (w/o post) 82.45 Medal 70.73 CAT 83.19 BiomedParse-V SAT 69.83 Medal (w/o stage1) 73.75 75.40 Medal (w/o post) 75.44 Medal DSC TP 37.17 67.49 39.54 47.14 49.97 50.94 28.13 70.53 27.28 65.11 66.48 66.41 36.28 65.52 42.43 70.49 72.12 72.39 27.79 71.85 78.63 72.92 70.89 72.11 - - - - - - 32.34 68.85 46.97 63.91 64.87 65.46 NSD 72.27 89.65 67.26 79.28 81.59 81.61 61.93 86.64 66.69 71.11 71.11 70.94 - - - - - - - - - - - - 83.60 91.35 79.24 73.04 79.16 79.48 72.60 89.21 71.06 74.48 77.28 77.34 F1 29.93 51.19 25.17 29.66 38.46 39.97 13.75 53.17 12.28 33.62 47.50 46.99 3.13 19.39 20.06 33.03 33.82 33.44 10.98 31.32 42.00 28.30 33.89 32.57 - - - - - - 14.45 38.77 24.88 31.15 38.42 38.24 Medal 17 below the concurrent BiomedParse-V in semantic segmentation but achieves comparable performance in instance segmentation. On average, Medal achieves 75.44 DSC, 77.34 NSD, 38.24 F1, and 65.46 DSC TPrepresenting gains of 4.71 / 4.74 / 23.79 / 33.12 over CAT and 5.61 / 6.28 / 13.36 / 18.49 over SAT. These improvements arise from two core innovations. Medal advances beyond SAT through channel-wise spatial-textual alignment using lightweight 3D convolutions, preserving full-resolution fidelity. Combined with parallel prompt processing and optimized resampling, this enables superior boundary delineation and instance separation. In ultrasound, Medal lags behind SAT (82.45 vs. 85.58 DSC) due to large target-to-patch ratios, revealing that the current dynamic resampling strategy still has room for improvement to better adapt to more complex input sizes, spacings, and target proportions. While Medal demonstrates the above advantages, it still lags behind BiomedParse-V in certain aspects, indicating room for further improvement in future work. Ablation on two-stage inference. The two-stage inference paradigm: lowresolution predictions from stage 1 provide global spatial prompts that effectively guide fine-grained refinement in stage 2. Ablation studies reveal average DSC gains of 1.69, NSD gains of 2.86, F1 gains of 7.09 and DSC TP gains of 1.55, with up to 13.37 F1 improvement in MRIdemonstrating that coarse-to-fine spatial guidance is essential for precise localization in complex, multi-organ scenes. Ablation on post-processing. Medal with post-processing yields only marginal average gains over Medal (w/o post): +0.04 DSC, +0.06 NSD, - 0.18 F1, +0.59 DSC TP. This suggests that the current post-processing is not universally effective and may disrupt refined predictions in some cases. In summary, Medal establishes new standard in universal medical segmentation by synergistically integrating global spatial cues with precise textual alignment in two-stage, full-resolution frameworkdelivering superior accuracy and efficiency across diverse anatomies and imaging modalities."
        },
        {
            "title": "4.2 Qualitative results on validation set",
            "content": "As illustrated in Fig. 3, the qualitative results on the validation set provide insights into the performance of our proposed method, Medal S, across different modalities. Due to the unavailability of the CAT all-data Docker and the lack of updated qualitative results for CAT, direct comparison with CAT on the all-data track for the validation set is not feasible. Consequently, our analysis focuses on comparing Medal predictions against ground truth annotations across five modalities, presenting both successful and challenging segmentation cases for each. The qualitative results reveal that Medal performs effectively in segmenting multi-class targets and regions with substantial volume across various modalities. In such cases, the model accurately delineates boundaries and captures structural details, benefiting from its channel-wise alignment of volumetric prompts and text embeddings, as well as its ability to process spatial prompts at native 18 P. Shi et al. Fig. 3. Comparison of Medal and ground truth results on the validation set for five different modalities. For each modality, we present both good segmentation results and bad segmentation results. resolution. However, segmentation quality diminishes for smaller lesions, particularly in datasets with significant foreground-background imbalance or blurred boundaries, such as those involving tumors. These challenging cases often exhibit ambiguous edges and complex textures, which pose difficulties for precise segmentation. contributing factor to these failures is the inherent noise in the labels of such data, which increases segmentation difficulty. Small lesions and imbalanced datasets amplify the impact of label inaccuracies, making it harder for the model to distinguish between foreground and background. While Medal Ss dynamic resampling and optimized preprocessing mitigate some of these issues, further improvements in handling noisy labels and refining boundary detection for small, ambiguous targets are necessary to enhance performance in these scenarios."
        },
        {
            "title": "4.3 Spatial Prompt Ablation Study",
            "content": "As shown in Fig. 5, we conduct case study to analyze the impact of spatial prompts under three distinct settings: 1) without any spatial prompts, 2) using the Stage-1 segmentation prediction as the spatial prompt for Stage-2, and 3) usMedal 19 Fig. 4. Efficiency comparison of spatial prompting strategies. (a) Inference runtime and (b) peak GPU memory consumption versus the number of classes. Parallel prompting achieves minimal time complexity with respect to the number of classes, resulting in greater than 10 speedup for 24-class segmentation over the sequential approach, whose runtime grows substantially. While parallel prompting requires moderately more memory, it remains within practical limits and offers favorable trade-off for drastic time savings in multi-class scenarios. Table 5. Quantitative ablation study on the effect of spatial prompts. Variant w/o spatial prompts w/ stage1 spatial prompts w/ GT spatial prompts DSC 83.50 83.98 87.23 NSD 78.15 79.16 84.50 ing the ground-truth (GT) segmentation mask as the spatial prompt for Stage-2. The results qualitatively demonstrate that the model can dynamically adapt its predictions based on the presence and quality of spatial prompts. Specifically, when provided with more accurate spatial prompts (e.g., GT masks), the model generates significantly refined outputseffectively suppressing small-scale noise, resolving category confusion, and enhancing spatial continuity. These observations are quantitatively supported by the results in Table 5, where using GT spatial prompts yields the highest DSC and NSD, confirming that the model effectively leverages superior spatial prompts to produce more accurate segmentation results. Based on the comprehensive analysis of spatial prompt effectiveness in Fig. 5 and Table 5, which is an analysis result for single test data example, we further demonstrate the efficiency of our proposed parallel spatial prompting mechanism through runtime and memory evaluation. As shown in Fig. 4, our parallel spa20 P. Shi et al. Fig. 5. Qualitative comparison of Medal with different spatial prompt configurations. From left to right: (a) Input Image, (b) Ground Truth, (c) Medal without spatial prompts, (d) Medal with Stage-1 prediction as spatial prompts, and (e) Medal with GT masks as spatial prompts. Each configuration is visualized in axial, coronal, sagittal views and 3D rendering, demonstrating the progressive improvement in segmentation quality with better spatial prompts - particularly in noise reduction, confusion resolution, and continuity enhancement. Medal 21 tial prompting approach achieves remarkable balance between performance and efficiency when evaluated on the same single case. While providing significant segmentation quality improvements as evidenced by the 83.98 DSC and 79.16 NSD in Table 5, the parallel method only introduces moderate computational overhead compared to the no-prompt baseline. Crucially, as the number of classes increases from 1 to 24, the parallel approach maintains reasonable runtime (40.63s vs 31.75s baseline) and memory usage (12.5GB vs 9.49GB baseline), while sequential prompting exhibits substantial time growth (435.1s), becoming practically infeasible for multi-class segmentation tasks. This efficiency advantage, combined with the proven effectiveness in enhancing segmentation quality, makes our parallel spatial prompting strategy highly promising for clinical applications where both accuracy and computational efficiency are paramount."
        },
        {
            "title": "4.4 Results on final testing set",
            "content": "The quantitative evaluation results on the test set for the all-data track, as shown in Table 6, demonstrate that our method Medal achieves Dice score of 58.06 and an NSD of 59.11, both of which are higher than those of the improved baseline model SAT (DSC: 54.13, NSD: 52.97). This indicates that Medal is promising approach. However, there remains performance gap compared to the current leading method, BiomedParse-V (DSC: 74.97, NSD: 77.47), suggesting potential for further optimization in the future. Table 6. Quantitative evaluation results of the testing set on the all-data track. Method Semantic Segmentation CAT BiomedParse-V SAT Medal DSC 33.04 74.97 54.13 58. NSD 31.53 77.47 52.97 59.11 Instance Segmentation DSC TP 4.66 44.76 29.59 17.86 F1 1.94 23.80 14.19 10."
        },
        {
            "title": "4.5 Limitation and future work",
            "content": "While Medal demonstrates strong performance across multiple modalities, certain limitations warrant further exploration. The dynamic resampling strategy, although effective in addressing target-patch ratio imbalances, requires additional refinement to better handle modalities like ultrasound, where large target sizes challenge the current patch-based approach. Expanding the variety of spatial prompts could further enhance performance. For instance, incorporating diverse spatial prompts, such as those in nnInteractive, including 2D spatial cues, could improve flexibility and precision in capturing complex anatomical structures. 22 P. Shi et al. Future work will focus on optimizing Medal for challenging datasets, particularly those involving instance segmentation with small lesions, significant foreground-background imbalances, or blurred boundaries, such as tumor data. Additionally, addressing datasets with high number of classes and intricate anatomical relationships will be priority. To align with clinical scenarios, we aim to develop robust solutions for diverse, complex datasets with numerous lesions, ensuring the models applicability in real-world medical imaging tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "This study introduces Medal S, novel medical image segmentation foundation model that achieves superior performance across diverse modalities. Its end-toend trainable framework uniquely harmonizes spatial precision with semantic textual guidance by supporting native-resolution spatial and textual prompts. key innovation is the channel-wise alignment of spatial prompts and text embeddings, which successfully mitigates inaccuracies from resolution mismatchesa critical limitation of text-only methods. Crucially, this parallel processing of spatial prompts yields 10 speedup for 24-class segmentation over sequential methods, with even greater efficiency gains for more classes. This advancement is further enabled by lightweight 3D convolutional module for precise voxel-space refinement. Medal demonstrates superior efficiency and accuracy in multi-class 3D medical segmentation. On the five-modality average on the validation set, it outperforms SAT with DSC of 75.44 (vs. 69.83), NSD of 77.34 (vs. 71.06), F1 of 38.24 (vs. 24.88), and DSC TP of 65.46 (vs. 46.97). Supporting up to 243 classes across CT, MRI, PET, ultrasound, and microscopy, Medal offers two versatile prompting modes: text-only for self-refinement and hybrid for incorporating manual annotations. Future work will focus on improving robustness for complex, imbalanced datasets and small lesions, and refining the dynamic resampling strategy to address the current performance lag in ultrasound. Overall, Medal represents significant advancement for high-speed, memory-efficient, and accurate medical image segmentation. Acknowledgements We thank all the data owners for making the medical images publicly available and CodaLab [24] for hosting the challenge platform. Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article."
        },
        {
            "title": "References",
            "content": "1. Avrahami, O., Hayes, T., Gafni, O., Gupta, S., Taigman, Y., Parikh, D., Lischinski, D., Fried, O., Yin, X.: Spatext: Spatio-textual representation for controllable image generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1837018380 (2023) 3 Medal 23 2. Bassi, P.R., Yavuz, M.C., Hamamci, I.E., Er, S., Chen, X., Li, W., Menze, B., Decherchi, S., Cavalli, A., Wang, K., et al.: Radgpt: Constructing 3d image-text tumor datasets. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 2372023730 (2025) 3 3. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: Endto-end object detection with transformers. In: European conference on computer vision. pp. 213229. Springer (2020) 6 4. Cheng, B., Schwing, A., Kirillov, A.: Per-pixel classification is not all you need for semantic segmentation. Advances in neural information processing systems 34, 1786417875 (2021) 5. Du, Y., Bai, F., Huang, T., Zhao, B.: Segvol: Universal and interactive volumetric medical image segmentation. In: Advances in Neural Information Processing Systems. vol. 37, pp. 110746110783 (2024) 2, 3, 6 6. Fabian, I., Maximilian, R., Lars, K., Stefan, D., Ashis, R., Florian, S., Benjamin, H., Tassilo, W., Moritz, L., Constantin, U., Jonathan, D., Ralf, F., Klaus, M.H.: nninteractive: Redefining 3D promptable segmentation. arXiv preprint arXiv:2503.08373 (2025) 3, 4 7. Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., Taigman, Y.: Makea-scene: Scene-based text-to-image generation with human priors. In: European Conference on Computer Vision. pp. 89106. Springer (2022) 3 8. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1600016009 (2022) 8 9. He, Y., Guo, P., Tang, Y., Myronenko, A., Nath, V., Xu, Z., Yang, D., Zhao, C., Simon, B., Belue, M., Harmon, S., Turkbey, B., Xu, D., Li, W.: VISTA3D: unified segmentation foundation model for 3D medical imaging. In: Proceedings of the IEEE/CVF International Conference on Computer Vision and Pattern Recognition (2024) 2, 3, 6 10. Huang, Z., Jiang, Y., Zhang, R., Zhang, S., Zhang, X.: Cat: Coordinating anatomical-textual prompts for multi-organ and tumor segmentation. In: Globerson, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J., Zhang, C. (eds.) Advances in Neural Information Processing Systems. vol. 37, pp. 35883610 (2024) 11. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature methods 18(2), 203211 (2021) 3, 4, 8, 12, 15 12. Kikinis, R., Pieper, S.D., Vosburgh, K.G.: 3D Slicer: platform for subject-specific image analysis, visualization, and clinical support, pp. 277289. Springer (2013) 13 13. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollar, P., Girshick, R.: Segment anything. In: Proceedings of the International Conference on Computer Vision. pp. 4015 4026 (2023) 2, 6 14. Li, W., Bassi, P.R., Lin, T., Chou, Y.C., Zhou, X., Tang, Y., Isensee, F., Wang, K., Chen, Q., Xu, X., et al.: Scalemai: Accelerating the development of trusted datasets and ai models. arXiv preprint arXiv:2501.03410 (2025) 3 15. Liu, J., Zhang, Y., Chen, J.N., Xiao, J., Lu, Y., Landman, B., Yuan, Y., Yuille, A., Tang, Y., Zhou, Z.: Clip-driven universal model for organ segmentation and tumor detection. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 2115221164 (2023) 3 16. Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medical images. Nature Communications 15, 654 (2024) 2, 24 P. Shi et al. 17. Ma, J., Li, F., Kim, S., Asakereh, R., Le, B.H., Nguyen-Vu, D.K., Pfefferle, A., Wei, M., Gao, R., Lyu, D., Yang, S., Purucker, L., Marinov, Z., Staring, M., Lu, H., Dao, T.T., Ye, X., Li, Z., Brugnara, G., Vollmuth, P., Foltyn-Dumitru, M., Cho, J., Mahmutoglu, M.A., Bendszus, M., Pflüger, I., Rastogi, A., Ni, D., Yang, X., Zhou, G.Q., Wang, K., Heller, N., Papanikolopoulos, N., Weight, C., Tong, Y., Udupa, J.K., Patrick, C.J., Wang, Y., Zhang, Y., Contijoch, F., McVeigh, E., Ye, X., He, S., Haase, R., Pinetz, T., Radbruch, A., Krause, I., Kobler, E., He, J., Tang, Y., Yang, H., Huo, Y., Luo, G., Kushibar, K., Amankulov, J., Toleshbayev, D., Mukhamejan, A., Egger, J., Pepe, A., Gsaxner, C., Luijten, G., Fujita, S., Kikuchi, T., Wiestler, B., Kirschke, J.S., de la Rosa, E., Bolelli, F., Lumetti, L., Grana, C., Xie, K., Wu, G., Puladi, B., Martín-Isla, C., Lekadir, K., Campello, V.M., Shao, W., Brisbane, W., Jiang, H., Wei, H., Yuan, W., Li, S., Zhou, Y., Wang, B.: Efficient medsams: Segment anything in medical images on laptop. arXiv:2412.16085 (2024) 12 18. Ma, J., Yang, Z., Kim, S., Chen, B., Baharoon, M., Fallahpour, A., Asakereh, R., Lyu, H., Wang, B.: Medsam2: Segment anything in 3d medical images and videos. arXiv preprint arXiv:2504.03600 (2025) 3, 13 19. Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle, R., Rolland, C., Gustafson, L., Mintun, E., Pan, J., Alwala, K.V., Carion, N., Wu, C.Y., Girshick, R., Dollár, P., Feichtenhofer, C.: Sam 2: Segment anything in images and videos. In: International Conference on Learning Representations (2025) 2, 3 20. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention. pp. 234241 (2015) 7 21. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems 30 (2017) 22. Wang, H., Guo, S., Ye, J., Deng, Z., Cheng, J., Li, T., Chen, J., Su, Y., Huang, Z., Shen, Y., et al.: Sam-med3d: towards general-purpose segmentation models for volumetric medical images. In: European Conference on Computer Vision. pp. 5167. Springer (2024) 2, 3, 6 23. Wong, H.E., Rakic, M., Guttag, J., Dalca, A.V.: Scribbleprompt: fast and flexible interactive segmentation for any biomedical image. In: European Conference on Computer Vision. pp. 207229. Springer (2024) 2 24. Xu, Z., Escalera, S., Pavão, A., Richard, M., Tu, W.W., Yao, Q., Zhao, H., Guyon, I.: Codabench: Flexible, easy-to-use, and reproducible meta-benchmark platform. Patterns 3(7), 100543 (2022) 22 25. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image diffusion models. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 38363847 (2023) 3 26. Zhang, X., Wu, C., Zhao, Z., Lei, J., Tian, W., Zhang, Y., Xie, W., Wang, Y.: Development of large-scale grounded vision language dataset for chest ct analysis. Scientific Data 12(1), 1636 (2025) 3 27. Zhao, T., Gu, Y., Yang, J., Usuyama, N., Lee, H.H., Kiblawi, S., Naumann, T., Gao, J., Crabtree, A., Abel, J., et al.: foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nature Methods 22, 166176 (2025) 28. Zhao, Z., Zhang, Y., Wu, C., Zhang, X., Zhou, X., Zhang, Y., Wang, Y., Xie, W.: Large-vocabulary segmentation for medical images with text prompts. NPJ Digital Medicine 8(1), 566 (2025) 3, 4, 6,"
        }
    ],
    "affiliations": [
        "Medical Image Insights, Shanghai, China",
        "University of Washington, Seattle, WA, USA",
        "University of Waterloo, Waterloo, ON, Canada",
        "Xian Jiaotong University, Xian, China"
    ]
}