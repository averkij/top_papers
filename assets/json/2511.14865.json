{
    "paper_title": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications",
    "authors": [
        "Dwipam Katariya",
        "Snehita Varma",
        "Akshat Shreemali",
        "Benjamin Wu",
        "Kalanand Mishra",
        "Pranab Mohanty"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations."
        },
        {
            "title": "Start",
            "content": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications Snehita Varma Capital One, AI Foundations San Francisco, USA Dwipam Katariya Capital One, AI Foundations McLean, USA Akshat Shreemali Capital One, AI Foundations New York, USA Benjamin Wu Capital One, AI Foundations San Jose, USA Kalanand Mishra Capital One, AI Foundations San Jose, USA Pranab Mohanty Capital One, AI Foundations San Jose, USA 5 2 0 2 8 1 ] . [ 1 5 6 8 4 1 . 1 1 5 2 : r ABSTRACT Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, transformerbased framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations. CCS CONCEPTS Computing methodologies Machine learning; Knowledge representation and reasoning; Information systems Recommender systems; Temporal data. KEYWORDS personalization, ads targeting, user sequence modeling, financial services, transformers, joint modeling, recommender systems Contributed to the study when at Capital One. Now at NVIDIA Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). 2025 Copyright held by the owner/author(s). Correspondence to: Dwipam Katariya <dwipam.katariya@capitalone.com> 1The modeling referenced in this research is not reflective of actual, granular activities or tasks unique to any specific organization and rather are common across the financial services industry ACM Reference Format: Dwipam Katariya, Snehita Varma, Akshat Shreemali, Benjamin Wu, Kalanand Mishra, and Pranab Mohanty. 2025. FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications. In the 19th ACM Conference on Recommender Systems (RecSys 25), September 2226, 2025, Prague, Czech Republic. ACM, New York, NY, USA, 10 pages."
        },
        {
            "title": "1 INTRODUCTION\nFinancial Service (FS) enterprises maintain a rich history of userâ€™s\nmultichannel interactions, with millions of users generating bil-\nlions of interactions daily across several digital products and phys-\nical channels. Users transact with credit and debit cards, browse\ndigital channels, talk to bank associates to accomplish tasks that\nmay or may not directly signal product interests and even get ex-\nposed to the related ads on external search engine before an actual\npurchase, as illustrated in Figure 1. (See Appendix B for more de-\ntails on customer journey). Sequential recommendation systems\n[4â€“7, 10, 15, 19, 30, 34, 37, 46] have used explicit and implicit sig-\nnals [11, 32, 35, 43, 48, 52, 53] such as clicks, dislikes/likes, and\nothers, combining longitudinal and heterogeneous contextual data.\n(See Appendix A for more on related work). However, prior work\nhas not explored other dynamic and non-digital signals such as\ntransaction and payments history, despite their dominance in FS.\nRepresenting such temporal multi-channel, multi-product behavior\nis important for accurate real-time personalization, specifically at\nscale under low-latency constraints for high throughput applica-\ntions. This requires a principled design of sequential representations\nand optimization of architectural and embedding strategies [33, 50].\nFS also has a varied content mix with various objectives, as shown\nin Table 1. Hence, optimizing solely for Click-Through-Rate (CTR)\nrisks adverse selection, promoting low-utility, clickbait items. Sim-\nilarly, optimizing solely for Conversion-Rate (CVR) may degrade\nuser experience. Furthermore, a mix of content is served at differ-\nent touchpoints by interrelated products such as feed-style ranking\nand placement-style ads targeting. Since feed-style personalization\nmay globally rank locally personalized ads content, it demands\ncross-product awareness. This proliferates product-specific models\nwhich are effective in isolation but result in increased maintenance\ncost and technical debt [2]. Moreover, such siloed models fail to\nleverage valuable cross-product interactions, leading to suboptimal\nperformance [41]. Recent research [36, 38, 41, 42, 45] has explored\nthe integration of search and recommendation systems, primar-\nily within the domains of e-commerce, travel, and digital media.\nHowever, this work does not sufficiently address the FS sector,",
            "content": "CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Katariya et al. Figure 1: Illustrative example of users journey across channels, products and pages where large enterprises operate multiple interrelated products beyond search and recommendation systems. Moreover, FS still relies on tree-based approaches for all Machine Learning (ML) modeling tasks [3, 22, 39], primarily due to regulatory alignment (e.g., explaining loan application denials). This approach necessitates extensive feature engineering by domain experts, process that is both time-consuming and complex. The evolving nature of these feature sets further complicates the management of large-scale ML systems, making it difficult to maintain and scale models efficiently. Hence, we present FinTRec, unified transformer-based framework for personalized advertising and servicing needs for FS that: a) considers the users raw sequential and heterogeneous context eliminating the need for feature engineering, b) balances both business value and user experience with multi-objective function, c) addresses infrastructure complexity and cost in order to enable real-time inference by relying on proprietary foundational model (FM) to capture the users years worth of transactions, payments, statements and other context and, d) when fine-tuned (FT) for product adaptation, enables improved local optimization through cross-product awareness, reduces technical debt and deployment complexity."
        },
        {
            "title": "2 TASK DEFINITION\nClick-Through-Rate as Next item Prediction: Let ğ‘– âˆˆ IT represent\nthe candidate item shown to a user ğ‘¢ âˆˆ U at time ğ‘¡ âˆˆ T . Hence,\nË†ğ‘CTR (ğ‘–, ğ‘¢)ğ‘¡ = P(clickğ‘– = 1 | ğ¼ğ‘¡ âˆ’1, ğ‘¢ğ‘¡ ).\nConversion-Rate: Traditionally, conversion rate (CVR) is defined\nas the probability of conversion given a user clicks. However, in\nFS platforms, exposure alone, without a click, can influence con-\nversions. Additionally, users may convert without any platform\nexposure, arriving via external sources like search engines or direct\nlinks. Hence, ğ‘CVR(ğ‘–, ğ‘¢)ğ‘¡ = P(convert = 1 | ğ‘–, ğ‘¢ğ‘¡ ).\nFinal Ranking: CTR reflects short-term engagement, while CVR\ncaptures long-term value. Optimizing solely for CTR can promote\nlow-quality or clickbait content with limited business impact. In\nPlatform Generated Content (PGC), where content supply is platform-\ndriven, ranking must adapt quickly to shifting business priorities\ninfluenced by economic and policy changes. To support this, we\nintroduce an urgency signal ğ‘¢ğ‘  (ğ‘–) âˆˆ Râ‰¥0, quantifying the strate-\ngic importance of promoting item(i) at a given time(t) set at the",
            "content": "campaign owner level. The final Ranking Score(RS) is computed as: RS(ğ‘–, ğ‘¢)ğ‘¡ = ğœ†ğ‘¢ğ‘  ğ‘¢ğ‘  (ğ‘–)ğ‘¡ + ğœ†ğ‘ğ‘¡ğ‘Ÿ ğ‘CTR(ğ‘–, ğ‘¢)ğ‘¡ + ğœ†ğ‘ğ‘£ğ‘Ÿ ğ‘CVR(ğ‘–, ğ‘¢)ğ‘¡ ğ‘£ (ğ‘–)ğ‘¡ (1) where: ğ‘CTR(ğ‘–, ğ‘¢)ğ‘¡ : predicted probability of click after calibrating Ë†ğ‘CTR (ğ‘–, ğ‘¢)ğ‘¡ , ğ‘CVR(ğ‘–, ğ‘¢)ğ‘¡ : predicted probability of conversion, ğ‘£ (ğ‘–): present value(PV) per conversion in dollars, ğ‘¢ğ‘  (ğ‘–): urgency score, ğœ†ğ‘¢ğ‘  R0: stakeholder-controlled urgency weighting coefficient, ğœ†ğ‘ğ‘¡ğ‘Ÿ R0: CTR weighting coefficient, ğœ†ğ‘ğ‘£ğ‘Ÿ R0: CVR weighting coefficient. This formulation enables flexible balancing of longterm value optimization and short-term business priorities, while preserving ranking interoperability. The urgency term acts as soft override, allowing stakeholders to elevate the prominence of specific item categories without entirely bypassing user-preference signals (e.g., fed policy change affecting savings account APY). Since, PV is realized only when conversion occurs, ğ‘CVR(ğ‘–, ğ‘¢)ğ‘¡ ğ‘£ (ğ‘–)ğ‘¡ estimates the expected value per conversion. It should be noted that other signals do not have monetary value associated with them."
        },
        {
            "title": "3 METHODOLOGY\n3.1 Data Pre-processing\nClickstream activities are stored at the most granular level of web\nand mobile activity, along with the timestamp. Individual trans-\nactions - such as payments, ATM usage, call center events - are\nalso stored in a tabular format at the interaction level and serve\nas valuable context signals for online ad targeting and personal-\nization [8, 20]. A notable property of this data is its composition\nof two distinct signal types: dynamic and static [20, 47]. Dynamic\nsignals, such as transactions, payments and money transfers, are\ncharacterized by their frequent changes over time. In contrast, Static\nsignals, like a userâ€™s product ownership, maintain a persistent state\nover a longer temporal horizon. Table 2 illustrates sample data\nsources used for this study. A fundamental requirement for an accu-\nrate real-time recommendation is low latency feature fetching and\ntransformation. This is particularly challenging when leveraging\nan extensive history of user interactions, including offline touch-\npoints such as transactions and payments. Given the impracticality\nof performing these transformations in real-time, due to both the",
            "content": "2For brevity, Ë†ğ‘CTR (ğ‘–, ğ‘¢ )ğ‘¡ will be referred as pCTR and ğ‘CVR(ğ‘–, ğ‘¢ )ğ‘¡ will be referred as pCVR FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Table 1: Content mix served on FS digital products Content Type Illustrative Examples Feedback Marketing Servicing Third-party open new accounts - credit card, loan, bank etc.. enroll in paperless statement, check savings balance etc.. Click 4x miles on Expedia, 5x cashback on Etsy, etc.. Convert Click / Convert sequences based on marketing opt outs. Feature transformation: Continuous context features undergo standardization. Categorical features like product enrollments, due to their multi-valued nature, are multi-hot encoded. Other categorical attributes (e.g., channel type, intra-page structured elements, layout, placements, device type) are tokenized as illustrated in Figure 3. FM embeddings are incorporated without further transformation. This process yields three fixed-dimensional context feature vectors - static context vector (ğ¹ğ‘  ), dynamic context vector (ğ¹ğ‘‘ ) , FM embedding vector (ğ¹ğ‘“ ğ‘š) per impression. For each user ğ‘¢, their tokenized interaction history is represented as time-ordered sequence of items ğ‘†ğ‘¢ = (ğ‘–ğ‘¡ ğ‘™, . . . , ğ‘–ğ‘¡ 0), where ğ‘™ is the sequence length. Each item ğ‘–ğ‘¡ in ğ‘†ğ‘¢ is associated with its context vector {ğ¹ğ‘ , ğ¹ğ‘‘, ğ¹ğ‘“ ğ‘š } ğ¹ğ‘ . Figure 3: Illustrative example of intra-page tokenization. Tokens are grouped and arranged by their timestamps across pages"
        },
        {
            "title": "3.2 Model Architecture\nA decoder-only architecture is used for pCTR (Figure 5b), whereas\nan encoder-only architecture is used for pCVR (Figure 5a). Encoder\narchitectures are widely adopted for modeling conversion behav-\nior, while decoder architectures are commonly used for next click",
            "content": "Figure 2: Yellow circle represent nonconversion events including impressions. Red circles represent conversions. For each conversion, item #1 queries data back to 60 days, while item #2 and item #3 queries data back to 30 and 15 days respectively. Then the data is temporally split for training/validation/test volume and heterogeneous dimensionality of the data, we rely on our proprietary FM to aggregate dynamic heterogeneous context to point-in-time representation. Our legacy Random Forest (RF) model [3, 22, 39] processes expertengineered features derived from the dynamic and static context and clickstream data (e.g., total web/mobile logins and product page visits in 7/14/30 days, avg. conversion rates in 7/14/30 days since fraud reported, utlization rate etc.). critical limitation of the RF approach is its reliance on flattening user interaction histories into time-aligned snapshots per impression. This simplification overlooks fine-grained temporal dependencies and evolving user preference signals. Another drawback is the models difficulty in handling high dimensional embeddings - such as 768-dimensional FM embeddings - which can lead to unstable feature sampling and degraded performance. To address this, we applied PCA to reduce the embeddings to 32 dimensions before training. In contrast, FinTRec directly leverages raw user interaction histories and 768-dim FM embeddings. It segments these histories into sequences, each culminating in conversion/no-conversion and/or click/no-click event, thereby rigorously preserving chronological order and temporal dependencies across sessions, channels and products. Due to content-specific regulatory requirements such as 30 days marketingopt out notices, and varied conversion time window (e.g., enrolling in credit monitoring is quicker conversion time frame compared to enrolling in savings account) requires careful content-specific attribution process. Our data processing pipeline allows for flexible conversion attribution as illustrated in Figure 2 and filters user CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Katariya et al. Table 2: Sample data sources used for this study Data Source Clickstream Heterogeneous Context Product Enrollment Tenures ........ Description Impressions and clicks across pages, users, platforms and products. Multiple impressions and clicks are grouped together per user session spanning from login to logout/idle > 30 mins 768-dim batched sequential representation of last 3 years transactions, payments, statements etc. with proprietary encoder-only FM Products that user own User relationship with all the products (in days) ........ Type Dynamic Dynamic Static Static prediction, as the sequential dependency of user clicks and immediate feedback naturally aligns with the autoregressive properties of decoders. Although conversion modeling involves long-term attribution and delayed feedback, there is no theoretical limitation that precludes the use of decoders for conversion or encoders for click prediction. In practice, however, the prevailing choice is often influenced by practical considerations, such as data storage schemas and serving infrastructure, which tend to favor encoders for conversion modeling and decoders for click prediction. Sequence Processing: For both pCTR and pCVR models, the pipeline first aggregates ğ¹ğ‘‘ since users most recent digital session into dynamic context vector ğ¹ğ‘‘ (ğ‘¡ ) . ğ‘†ğ‘¢ tokens are mapped to dense vectors via an input embedding layer and fused with the resulting item token embeddings at (t) with ğ¹ğ‘‘ (ğ‘¡ ) to form single, combined embedding vector. Given that ğ¹ğ‘  is time-invariant and ğ¹ğ‘“ ğ‘š are inherently sequential, the pCVR model utilizes ğ¹ğ‘  and ğ¹ğ‘“ ğ‘š corresponding to the latest interaction (ğ‘¡ 0). In contrast, the pCTR model retains ğ¹ğ‘  and and ğ¹ğ‘“ ğ‘š for all the time points within the sequence. Order and Temporal Encoding: To preserve the intra-session interaction order, we apply positional encodings ğ‘ğ‘¡ as proposed by [40]. To capture temporal dynamics, we construct multivariate timestamp ğ‘’ğ‘¡ representation by decomposing timestamp into dayofweek, weekofmonth, hourofday etc. We then compute an element-wise dot product between the ğ‘ğ‘¡ and ğ‘’ğ‘¡ , and add the result to the input sequence embeddings resulting in Ë†ğ‘†ğ‘¢ . Causal Decoder Block: For pCTR task, We utilize causal decoder-only architecture, where self-attention is masked based on timestamps rather than token positions, in contrast to the default formulation in [40] resulting in: hğ‘¡ = Decoder( Ë†ğ‘†ğ‘¢ ). This forces strict time ordered look-back constraint. Bi-Directional Encoder Block: For pCVR, We adopt bi-directional encoder block [40] resulting in hğ‘¢ = Encoder( Ë†ğ‘†ğ‘¢ ). We then, pool the representations from â„ (ğ‘¡ ğ‘™ ) to â„ (ğ‘¡ 0) to capture global summary of the interaction sequence: Static and FM Context Fusion: The decoders output hğ‘¡ is concatenated with [ğ‘“ğ‘“ ğ‘š (ğ‘¡ ) , ğ‘“ğ‘  (ğ‘¡ ) ] and [ğ‘“ğ‘“ ğ‘š (0) , ğ‘“ğ‘  (0) ] for encoders output hğ‘¢ . This combined representation is passed through multi-layer perceptron (MLP) followed by Sigmoid (in case of pCTR) and Softmax (in case of pCVR). Loss Functions: pCVR: For training the Random Forest model with down-sampled negatives, we use the standard Binary Classification loss followed by item-specific calibration. Transformers is trained with BCE and does not require calibration. pCTR: We adopt modified Next Item Loss (as shown in eq.2) that includes only clicked items. Since certain item must be exposed regardless of clicks, the model is not optimized for predicting the immediate next item, but instead focuses on learning from positively engaged signals. Next Item Loss = 1 (cid:205)ğ‘¢ Pğ‘¢ ğ‘™ğ‘œğ‘” ğ‘¢ ğ‘¡ Pğ‘¢ (cid:32) exp( Ë†ğ‘¦ğ‘¢,ğ‘¡ [ğ‘¦ğ‘¢,ğ‘¡ +1]) (cid:205)ğ¾ ğ‘—=1 exp( Ë†ğ‘¦ğ‘¢,ğ‘¡ [ ğ‘—]) (cid:33) (2) where, U: set of users, Pğ‘¢ : time steps for user such that click occurred at t+1, ğ‘¦ğ‘¢,ğ‘¡ +1: the ground-truth clicked item at time t+1, Ë†ğ‘¦ğ‘¢,ğ‘¡ Rğ¾ : the logits (predicted scores) over the item vocabulary at time t"
        },
        {
            "title": "3.3 Product Adaptation FT\nFirst, we collapse customer interactions across all the existing prod-\nucts and align them on a single flat timeline [20], followed by\npre-training a FinTRec-decoder-only model. We then adopt Low-\nRank Adaptation (LoRA) [17] for new product adaptation. During\nfine-tuning, only the low-rank product-specific matrices are up-\ndated. To incorporate product-specific semantics, we extend the\npre-trained token embedding matrix with new learnable embed-\ndings. Let ğ¸ âˆˆ Rğ‘‰ Ã—ğ‘‘ denote the original token embeddings for a vo-\ncabulary of size ğ‘‰ and hidden size ğ‘‘. We introduce ğ¸new âˆˆ Rğ‘‰new Ã—ğ‘‘ ,\nrepresenting embeddings for ğ‘‰new new product(s). The combined\nembedding matrix becomes:",
            "content": "ğ¸ = (cid:21) (cid:20) ğ¸ ğ¸new R(ğ‘‰ +ğ‘‰new ) ğ‘‘ In our setup, ğ¸ remains frozen during fine-tuning, while ğ¸new is optimized jointly with the LoRA parameters. Formally, let ğœƒLoRA represent all LoRA parameters across selected transformer layers, and ğ¸new the trainable token embeddings. The model is fine-tuned by minimizing task-specific loss over labeled training data (ğ‘†ğ‘¢, ğ¹ğ‘, ğ‘¦): min ğœƒLoRA,ğ¸new (ğ‘“base (ğ‘†ğ‘¢ ; ğ¹ğ‘ ; ğœƒfrozen) + ğ‘“LoRA (ğ‘†ğ‘¢ ; ğ¹ğ‘ ; ğœƒLoRA, ğ¸new), ğ‘¦) Here, ğ‘“base is the frozen backbone model, and ğ‘“LoRA represents the learned low-rank adaptations and new token embeddings. This formulation ensures that only small subset of model parameters along with new tokens are updated, enabling scalable and efficient adaptation to new products. Linear Probing (LP) enables lightweight fine-tuning by updating only the final dense layer, while keeping all other model parameters frozen. Full Fine-Tuning (F-FT) All the model parameters of the FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic base transformer layers are updated. Token embedding extension is also applied to both LP-FT and F-FT. Since different products have different output dimensions, new output adapter is added by replacing the base model head. Output adapter is replaced for all three FT strategies. For F-FT and LoRA-FT, we also unfreeze parameters in the dense layer that correspond to ğ¹ğ‘  and ğ¹ğ‘“ ğ‘š. The goal of LP-FT was to set lower bound on the maximum performance gain we could achieve with minimal updates; hence, the dense layers corresponding to ğ¹ğ‘  and ğ¹ğ‘“ ğ‘“ ğ‘š were kept frozen. (a) pCVR encoder-only architecture (b) pCTR decoder-only architecture Figure 5: The pCVR model employs an encoder-only architecture (Figure 5a), while the pCTR model utilizes causal decoder-only architecture (Figure 5b). Only positive samples are used for training pCTR, hence (t-2) is skipped from final loss calculation Recall@topk is given as: Recall@top-k = 1 (cid:205)ğ‘¢ Pğ‘¢ ğ‘¢ ğ‘¡ Pğ‘¢ {ğ¼ğ‘¢,ğ‘¡ +1 Ë†ğ‘…ğ‘› ğ‘¢,ğ‘¡ } (3) as such, Ë†ğ‘…ğ‘› ğ‘¢,ğ‘¡ is the top-k recommended items at time for user u. Product Adaptation: The effectiveness of our product adaptation approach was evaluated using leave-product-out validation scheme. Specifically, product-specific interactions were held out during the pre-training phase before being subjected to the dedicated product adaptation strategy outlined in Section 3.3. We experiment our framework with both feed-style and placement-style products. We further tightly align our training with in-production inference in order to compute model performance metrics. For example, the PGC Marketing and Servicing model is inferred at the page level, while mobile homepage is inferred at each login. Training Specifications: The FinTRec model was implemented using the PyTorch framework. We utilized NVIDIA 8 A10G GPUs and 512 GB of RAM. We adopted Distributed Data Processing (ddp) from PyTorch Lightning for parallelization. Each experiment ran for 50 epochs using the AdamW optimizer with learning rate 1ğ‘¥104, weight decay of 1ğ‘¥105 and batch size of 64 per epoch. Model with the lowest validation log loss was selected. We did not employ early stopping. We utilize 10 multi-head attention blocks with hidden dimensions of 256 and input/output embedding dimension of 512."
        },
        {
            "title": "4 OFFLINE RESULTS\nTable 3 reports FinTRecâ€™s performance on the pCVR model for PGC\nmarketing content. FinTRec (0.0439) substantially outperforms the\nproduction-grade RF baseline (0.0984) and RF + FM embeddings",
            "content": "Figure 4: Dynamic context (ğ¹ğ‘‘ ) and Clickstream sequence (ğ‘†ğ‘¢ ) are processed to form sequential user representations ( Ë†ğ‘†ğ‘¢ ). Static (ğ¹ğ‘  ) and FM embeddings (ğ¹ğ‘“ ğ‘š) are time-aligned with Ë†ğ‘†ğ‘¢"
        },
        {
            "title": "3.4 Experimental Setup\nWe use various data sources from the internal data ecosystem as\nmentioned in Table 1. For model training, we apply a temporal split\nof 90 days for training, 7 days for validation, and 7 days for testing\non clickstream data. We focus our study on 30M+ userâ€™s sequences,\n200+ items, 1BN+ interactions, and 10+ data sources across mobile\nand web channels.\nMetrics: We adopt widely used evaluation metrics: log loss (for\npCVR) and ranking recall@topk (for pCTR). Log Loss: quantifies\nthe uncertainty in predictions required for trustworthy monetary\nvalue assessments. Ranking Recall: 99% of sessions display a max-\nimum of five items (across products) at any given time. An effec-\ntive personalization system aims to elevate relevant content to the\nhighest possible rank within this limited display. Ranking Recall\ndirectly quantifies this performance, measuring how effectively\nrelevant items are surfaced in such constrained visual interfaces.",
            "content": "CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Katariya et al. Table 3: Experimental Results of FinTRec for pCVR task on PGC marketing content. Best results are boldfaced and highlighted Model Log Loss RF + Feature Set RF + Feature Set (In Prod) 0.0984 0.0938 + FM Embeddings FinTRec w/o time embeddings w/o FM embeddings context window=1 context window=5 context window=50 context window=120 0.0439 0.0481 0.0605 0.1135 0.0844 0.0478 0.0439 Table 4: Experimental Results of FinTRec when FT for product adaptation with pCTR objective. Recall@1,5 shows relative lift in (%) over the product-specific baseline. Best results are boldfaced and highlighted. Second best are underlined. Product Model Recall@1 Recall@5 PGC Servicing (placement-style) Mobile Homepage (feeds-style) Third-Party Marketing (placement-style) (Product-Specific) FinTRec F-FT LP-FT LoRA-FT (Product-Specific) FinTRec F-FT LP-FT LoRA-FT (Product-Specific) FinTRec F-FT LP-FT LoRA-FT 0.00 -5.24 +26.85 +11.41 +24.21 0.00 -8.64 +13.85 +5.36 +14.11 0.00 -32.44 +25.63 +8.17 +23.11 0.00 +2.90 +20.09 +3.62 +20.11 0.00 -16.34 +5.15 +1.16 +5.16 0.00 -24.14 +17.28 +3.52 +15.75 LoRA-FT and LP-FT, required <5% and <1% of the pretrained models parameters and <10% and <5% of its training time (0.0938). Removing temporal encodings (0.0481) or FM embeddings (0.0605) degrades performance, demonstrating the importance of temporal dynamics and financial context. Long-range dependencies are prominent in FS, as demonstrated by the increased accuracy with larger context window, factor often overlooked or subdued by expert-engineered features. Product Adaptation: Table 4 presents pCTR model gains across PGC Servicing, Mobile Homepage, and Third-Party Marketing. The Product-Specific model is model developed in isolation based on the current framework, and incorporates both static and dynamic signals, as well as FM Embeddings. F-FT achieves the highest lift (up to +26.85% Recall@1). LoRA-FT reaches comparable performance (+24.21% Recall@1) with <5% parameter updates, and LP-FT offers smaller gains with <1% parameter fine-tuning, underscoring the importance of full temporal adaptation. LoRA-FT strikes balance between model performance and training cost by improving code base standardization. FinTRec without fine-tuning under performs in most cases, due to: a) differences in pCTR across products , specifically negatively affecting Third-Party more than PGC Servicing, where pCTR for PGC Servicing is marginally higher and b) lack of product-specific temporal alignment."
        },
        {
            "title": "7 ONLINE A/B EXPERIMENTS\nWe conduct offline simulations to assess a modelâ€™s potential im-\npact on downstream business outcomes before it is deployed to\nproduction. Our simulation methodology, inspired by established",
            "content": "FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Table 5: Log Loss vs A/B test performance. Log Loss and PV shows relative lift in %. Act. PV shows actual realized PV in production. Est. PV when simulated offline pre-deployment is calculated based on final RS. Feature set corresponds to our proprietary user context features set as illustrated in Table 2. Our simulation do not guarantee the magnitude of log loss improvement resulting in equal magnitude of PV improvement. Control Test Log Loss(%) Act. PV(%) (Est. PV(%)) RF RF + Features Set RF + Features Set RF + Feature Set RF + Features Set + FM Embeddings FinTRec + Features Set + FM Embeddings -7.00 -4. -55.38 +3.75 (<+6.08) +10.00 (<+37.50) TBD (<+41.50) Table 6: Visit quantification for explainability. AUROC is obtained from model trained with important touch points. Relative AUROC in % is reported compared to the best performing model from the Table 3. (%) is rounded to the nearest integer. Visit Importance AUROC(%) Most Important Second Most Important First Two Important Visit -4.00% -5.00% -2.00% Visit importance identified with the Attention Tracing methods described in [1, 51] and averaged across both the methods approaches from Netflix [29] and Yahoo [24], systematically explores the search space of weighting coefficients (ğœ†ğ‘¢ğ‘ , ğœ†ğ‘ğ‘¡ğ‘Ÿ , ğœ†ğ‘ğ‘£ğ‘Ÿ ) to determine an optimal balance for the ranking score defined in Eq. 1. This process generates sensitivity curve (Figure 6) that illustrates the trade-off between estimated Clicks and PV. The final coefficients are selected by product owners to align with current business objectives, typically by setting the values to match historically observed CTR and then noting the resulting increase in PV. Table 5 compares log loss reductions(%) with both actual and estimated total PV lifts from live A/B tests(%) and offline simulations. Using the latest dataset, we compare the top-ranked items from the current and new models using our final ranking objective, and estimate the lift in total PV. This evaluation shows that lower log loss generally correlates with higher user engagement, though the relationship is not strictly proportional. Adding the relevant feature set to the RF, the baseline yields -7.00% log loss reduction and +3.75% realized PV gain with offline estimate showing up to +6.08% PV gain. Further incorporating FM embeddings improves PV by +10.00%, with simulations estimating up to +37.50% lift. The largest log loss reduction (-55.38%) is achieved with FinTRec, with projected simulated PV gain up to +41.50%. While actual PV impact is pending, we conclude the offline results support FinTRec deployment. Furthermore, we also monitor other guardrail metrics like Fraud Rate (eg: % Frauds reported in 30 days), 2hr. Call Rate post user session, Credit Card On-Time Payments which may affect regulatory findings. In addition to guardrail metrics, tracking stakeholder-defined metrics is also essential to account for variations in PV, urgency scores and external factors like interest rates. For instance, business can decide thresholds for impression rates on specific content categories. This allows to detect unintended - a) model consequences early b) cannibalization due to content relatedness. Figure 6: Change in expected clicks and PV based on offline simulation runs."
        },
        {
            "title": "8 CONCLUSION\nWe introduces FinTRec and show superior performance over tradi-\ntional and widely used tree-based baseline. We further show the\neffectiveness of incorporating time, long-range sequences, and im-\nplicit feedback via FM embeddings from years of transactional and\nother offline context. FM Embeddings enable real-time inference by\neliminating the need for costly history retrieval. Fine-tuning across\nproducts further enhances performance through cross-product\nknowledge sharing, reduces infrastructure overhead, and improves\ncodebase standardization. Historical A/B tests, simulations and ex-\nplainability study confirms FinTRecâ€™s production readiness. Limita-\ntions and Future Work: a) pCTR and pCVR models are developed\nin separate codebases. Hence, future work will explore scalable ar-\nchitectures and unified frameworks for click and conversion models\nto further mitigate technical debt, maintenance, and training costs\nb) Our current FM embeddings are nightly batched and therefore\nstale, necessitating solutions for same day interaction awareness\nwithout incurring additional latency c) Although we applied ex-\nplainability methods for visit importance in budget allocation and\ninitial regulatory analysis, use-case-specific frameworks for finer\nalignment are yet to be fully explored. Broader Impact Statement:\nWhile studied in context of Financial Services and Banking, this\nwork is also applicable to diverse industries such as e-commerce,\nretail, travel, and media. We invite other researchers to expand our\nstudy further.",
            "content": "CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Katariya et al."
        },
        {
            "title": "9 AUTHOR BIO\nDwipam Katariya(Mclean, VA), Snehita Varma(San Francisco, CA)\nand Akshat Shreemali(New York, NY) are Data Science Manager\nat Capital One. Kalanand Mishra(San Jose, CA) is Director of Data\nScience at Capital One. Pranab Mohanty(Seattle, WA) is Sr. Director\nof Applied Research at Capital One. They all are part of recom-\nmendation and personalization team within the AI Foundations\norg. Benjamin Wu worked as Data Science Manager at Capital One,\nbefore joining NVIDIA as Sr. Solutions Architect.",
            "content": "ACKNOWLEDGMENTS We thank Aditya Sasanur and Neil Kumar for their contribution to ML experiments. We thank Jesse Zymet for thought provoking discussion and insights for highlighting value of the implicit feedback data. We are thankful to our collaborators from Foundation Models team. We thank Giri Iyengar for the support and sponsoring the study. REFERENCES [1] Barkan, O., Hauon, E., Caciularu, A., Katz, O., Malkiel, I., Armstrong, O., and Koenigstein, N. Grad-sam: Explaining transformers via gradient selfattention maps. Proceedings of the 30th ACM Intl Conf. on Information and Knowledge Management (2021), 6. [2] Bhattacharya, M., Ostuni, V., and Lamkhede, S. Joint modeling of search and recommendations via an unified contextual recommender (unicorn). In Proceedings of the 18th ACM Conference on Recommender Systems (2024), pp. 793 795. [3] Borisov, V., Broelemann, K., Kasneci, E., and Kasneci, G. Deeptlf: robust deep neural networks for heterogeneous tabular data. International Journal of Data Science and Analytics 16, 1 (2023), 85100. [4] Cao, P., and Lio, P. Genrec: Generative personalized sequential recommendation. arXiv preprint arXiv:2407.21191 (2024). [5] Chang, J., Gao, C., Zheng, Y., Hui, Y., Niu, Y., Song, Y., Jin, D., and Li, Y. Sequential recommendation with graph neural networks. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval (2021), pp. 378387. [6] Chen, Q., Zhao, H., Li, W., Huang, P., and Ou, W. Behavior sequence transformer for e-commerce recommendation in alibaba. In Proceedings of the 1st international workshop on deep learning practice for high-dimensional sparse data (2019), pp. 14. [7] Chen, Y.-C., Chen, Y.-L., and Hsu, C.-H. G-transrec: transformer-based nextitem recommendation with time prediction. IEEE Transactions on Computational Social Systems (2024). [8] Chitsazan, N., Sharpe, S., Katariya, D., Cheng, Q., and Rajasethupathy, K. Dynamic customer embeddings for financial service applications. arXiv preprint arXiv:2106.11880 (2021). [9] Covington, P., Adams, J., and Sargin, E. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems (New York, NY, USA, 2016), RecSys 16, Association for Computing Machinery, p. 191198. [10] de Souza Pereira Moreira, G., Rabhi, S., Lee, J. M., Ak, R., and Oldridge, E. Transformers4rec: Bridging the gap between nlp and sequential/session-based recommendation. In Proceedings of the 15th ACM conference on recommender systems (2021), pp. 143153. [11] Fan, Z., Ou, D., Gu, Y., Fu, B., Li, X., Bao, W., Dai, X.-Y., Zeng, X., Zhuang, T., and Liu, Q. Modeling users contextualized page-wise feedback for clickthrough rate prediction in e-commerce search. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (Feb. 2022), WSDM 22, ACM, p. 262270. [12] Garuti, F., Luetto, S., Sangineto, E., and Cucchiara, R. Large-scale transformer models for transactional data. In CEUR WORKSHOP PROCEEDINGS (2024), pp. 242 247. [13] Gorishniy, Y., Rubachev, I., Khrulkov, V., and Babenko, A. Revisiting deep learning models for tabular data. Advances in neural information processing systems 34 (2021), 1893218943. [14] Grbovic, M., and Cheng, H. Real-time personalization using embeddings for search ranking at airbnb. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (2018), pp. 311320. [15] Hansen, C., Hansen, C., Maystre, L., Mehrotra, R., Brost, B., Tomasi, F., and Lalmas, M. Contextual and sequential user embeddings for large-scale music recommendation. In Proceedings of the 14th ACM Conference on Recommender Systems (2020), pp. 5362. [16] Hidasi, B., and Karatzoglou, A. Recurrent neural networks with top-k gains for session-based recommendations. In Proceedings of the 27th ACM international conference on information and knowledge management (2018), pp. 843852. [17] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR 1, 2 (2022), 3. [18] Huang, X., Khetan, A., Cvitkovic, M., and Karnin, Z. Tabtransformer: Tabular data modeling using contextual embeddings. arXiv preprint arXiv:2012.06678 (2020). [19] Kang, W.-C., and McAuley, J. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) (2018), IEEE, pp. 197206. [20] Katariya, D., Origgi, J. M., Wang, Y., and Caputo, T. Timesync: Temporal intent modelling with synchronized context encodings for financial service applications. arXiv preprint arXiv:2410.12825 (2024). [21] Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A., Kliushkina, N., Araya, C., Yan, S., and Reblitz-Richardson, O. Captum: unified and generic model interpretability library for pytorch. [22] Kotios, D., Makridis, G., Fatouros, G., and Kyriazis, D. Deep learning enhancing banking services: hybrid transaction classification and cash flow prediction approach. Journal of big Data 9, 1 (2022), 100. [23] Li, C., Liu, Z., Wu, M., Xu, Y., Zhao, H., Huang, P., Kang, G., Chen, Q., Li, W., and Lee, D. L. Multi-interest network with dynamic routing for recommendation at tmall. In Proceedings of the 28th ACM international conference on information and knowledge management (2019), pp. 26152623. [24] Li, L., Chu, W., Langford, J., and Wang, X. Unbiased offline evaluation of contextual-bandit-based news article recommendation algorithms. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining (New York, NY, USA, 2011), WSDM 11, Association for Computing Machinery, p. 297306. [25] Li, Z., Yang, C., Chen, Y., Wang, X., Chen, H., Xu, G., Yao, L., and Sheng, M. Graph and sequential neural networks in session-based recommendation: survey. ACM Computing Surveys 57, 2 (2024), 137. [26] Liu, C., Cao, J., Huang, R., Zheng, K., Luo, Q., Gai, K., and Zhou, G. Kuaiformer: Transformer-based retrieval at kuaishou. arXiv preprint arXiv:2411.10057 (2024). [27] Luetto, S., Garuti, F., Sangineto, E., Forni, L., and Cucchiara, R. One transformer for all time series: Representing and training with time-dependent heterogeneous tabular data. arXiv preprint arXiv:2302.06375 (2023). [28] Lv, F., Li, M., Guo, T., Yu, C., Sun, F., Jin, T., and Ng, W. Xdm: Improving sequential deep matching with unclicked user behaviors for recommender system. In International Conference on Database Systems for Advanced Applications (2022), Springer, pp. 364376. [29] McInerney, J., Brost, B., Chandar, P., Mehrotra, R., and Carterette, B. Counterfactual evaluation of slate recommendations with sequential reward interactions. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (New York, NY, USA, 2020), KDD 20, Association for Computing Machinery, p. 17791788. [30] Moreira, G. d. S. P., Rabhi, S., Ak, R., Kabir, M. Y., and Oldridge, E. Transformers with multi-modal features and post-fusion context for e-commerce session-based recommendation. arXiv preprint arXiv:2107.05124 (2021). [31] Mukund Sundarajan, Ankur Taly, Q. Y. Axiomatic attribution for deep networks. ICML (2017). [32] Ouyang, W., Zhang, X., Li, L., Zou, H., Xing, X., Liu, Z., and Du, Y. Deep spatio-temporal neural networks for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2019), pp. 20782086. [33] Pancha, N., Zhai, A., Leskovec, J., and Rosenberg, C. Pinnerformer: Sequence modeling for user representation at pinterest. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining (2022), pp. 37023712. [34] Pi, Q., Bian, W., Zhou, G., Zhu, X., and Gai, K. Practice on long sequential user behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining (2019), pp. 26712679. [35] Ren, K., Fang, Y., Zhang, W., Liu, S., Li, J., Zhang, Y., Yu, Y., and Wang, J. Learning multi-touch conversion attribution with dual-attention mechanisms for online advertising. In Proceedings of the 27th acm international conference on information and knowledge management (2018), pp. 14331442. [36] Shi, T., Xu, J., Zhang, X., Zang, X., Zheng, K., Song, Y., and Yu, E. Unified generative search and recommendation. arXiv preprint arXiv:2504.05730 (2025). [37] Sun, F., Liu, J., Wu, J., Pei, C., Lin, X., Ou, W., and Jiang, P. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management (2019), pp. 14411450. [38] Sun, Z., Si, Z., Zang, X., Leng, D., Niu, Y., Song, Y., Zhang, X., and Xu, J. Kuaisar: unified search and recommendation dataset. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (2023), pp. 54075411. FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic [39] Taha, A. A., and Malebary, S. J. An intelligent approach to credit card fraud IEEE access 8 detection using an optimized light gradient boosting machine. (2020), 2557925587. [40] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023. [41] Wang, J., Zhang, Y., and Chen, T. Unified recommendation and search in ecommerce. In Information Retrieval Technology: 8th Asia Information Retrieval Societies Conference, AIRS 2012, Tianjin, China, December 17-19, 2012. Proceedings 8 (2012), Springer, pp. 296305. [42] Xie, J., Liu, S., Cong, G., and Chen, Z. Unifiedssr: unified framework of sequential search and recommendation. In Proceedings of the ACM Web Conference 2024 (2024), pp. 34103419. [43] Xie, R., Ling, C., Wang, Y., Wang, R., Xia, F., and Lin, L. Deep feedback network for recommendation. In Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence (2021), pp. 25192525. [44] Yang, J., Yi, X., Cheng, D. Z., Hong, L., Li, Y., Wang, S., Xu, T., and Chi, E. H. Mixed negative sampling for learning two-tower neural networks in recommendations. [45] Yao, J., Dou, Z., Xie, R., Lu, Y., Wang, Z., and Wen, J.-R. User: unified information search and recommendation model based on integrated behavior sequence. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management (2021), pp. 23732382. [46] Yuan, W., Wang, H., Yu, X., Liu, N., and Li, Z. Attention-based context-aware sequential recommendation model. Information Sciences 510 (2020), 122134. [47] Zhang, D., Wang, L., Dai, X., Jain, S., Wang, J., Fan, Y., Yeh, C.-C. M., Zheng, Y., Zhuang, Z., and Zhang, W. Fata-trans: Field and time-aware transformer for sequential tabular data. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (2023), pp. 32473256. [48] Zhang, H., Meng, C., Guo, W., Guo, H., Zhu, J., Zhao, G., Tang, R., and Li, X. Time-aligned exposure-enhanced model for click-through rate prediction. arXiv preprint arXiv:2308.09966 (2023). [49] Zhang, H., Wang, S., Zhang, K., Tang, Z., Jiang, Y., Xiao, Y., Yan, W., and Yang, W.-Y. Towards personalized and semantic retrieval: An end-to-end solution for ecommerce search via embedding learning. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (2020), pp. 24072416. [50] Zhang, W., Li, D., Liang, C., Zhou, F., Zhang, Z., Wang, X., Li, R., Zhou, Y., Huang, Y., Liang, D., et al. Scaling user modeling: Large-scale online user representations for ads personalization in meta. In Companion Proceedings of the ACM Web Conference 2024 (2024), pp. 4755. [51] Zhengxuan Wu, Thanh-Son Nguyen, Y. C. O. Structured self attention weights encode semantics in sentiment analysis. Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP (2020), 255264. [52] Zhou, G., Mou, N., Fan, Y., Pi, Q., Bian, W., Zhou, C., Zhu, X., and Gai, K. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence (2019), vol. 33, pp. 59415948. [53] Zhou, G., Zhu, X., Song, C., Fan, Y., Zhu, H., Ma, X., Yan, Y., Jin, J., Li, H., and Gai, K. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining (2018), pp. 10591068. RELATED WORK Recent advancements in recommendation systems have shown the importance of using sequential machine learning methods for representing users Point-of-Interest (POI) based on their past interactions and automatically recommending interested items. Users provide explicit and implicit feedback from interactions such as clicks, conversions, likes, dislikes, comments, shares, and/or dwell time. However, utilizing all user historical information poses challenges in capturing evolving user preferences over time. This has led to the emergence of session-aware recommendation (SAR) [25] models that adapt dynamically to ongoing user behavior. SAR has witnessed widespread adoption within academia as well as the industry, growing over 2x YoY since FY 2019 [25]. For instance, SAR has been widely studied within the e-commerce, travel, and entertainment industries. Recently, transformers have shown superior performance for SAR over RNNs and other traditional Machine Learning Models [6, 19, 26, 33, 37]. Although plethora of research addresses peculiarities of transformers for SAR [19, 37], these academic efforts often fail to address certain industrial challenges, which has limited their effectiveness in driving success in large-scale recommendation systems [26]. For example, AliBaba [6] introduced behavior sequence transformer for e-commerce recommendation and demonstrated the superiority of sequential actions over an embedding and MLP paradigmwhere raw features are embedded into low-dimensional vectors and passed into an MLP for final recommendation. However, while the model showed improvements, the study did not sufficiently address industrial challenges like scalability and inference latency for large-scale recommendation systems. Pinterest studied the effectiveness of deploying model that leverages users historical actions within largescale recommendation system, focusing on an industrial solution capable of handling billions of pins (items) and 400M+ MAU. Their work highlighted the complexities of SAR, including using streaming infrastructure to capture the latest user activity and managing data to encode the user state in real-time [33]. Furthermore, the retrieval taska precursor to the final recommendationis responsible for selecting thousands of candidate items from billions of options. Applying Transformers directly to this large-scale operation in real-time is computationally prohibitive [26]. To address these challenges, specialized architecture, KuaiFormer, was introduced by Kuaishou. Besides SAR studied in the e-commerce, travel, and entertainment domains, SAR has also been deployed in wealth management and, more recently, has started to gain adoption in retail banking. Several competitions on Kaggle held by Santander [16] highlight the growing need for recommending relevant retail banking products to users. Although these studies have been discussed in the realm of model architectures, datasets, evaluation, and bias detection, there are limited studies available on industrial applications of transformers in the Financial Services domain. And unlike digital-only platforms, users in FS also interact across various channels, for example, by logging into mobile and/or web applications, using their credit/debit cards, physical locations, etc., generating rich history of users sequential interaction [20]. TIMeSynC [20] addressed this through data processing model architecture, yet failed to address the real-time aspect of data retrieval. Customer interactions across these channels are not only explicit but also leave trails of their implicit feedback. While several studies have focused on modeling positive feedbacksuch as clicks or conversions using sequential modelsnegative signals such as skips and exposure without engagement have been often overlooked [11]. DIN [53] made an early attempt to capture the diversity of user interests by designing an adaptive activation mechanism, allowing the model to selectively attend to relevant historical behaviors based on current context. However, DIN is limited in its ability to model the evolution of latent interests over time [52]. DIEN [52] extended DIN by adding latent temporal interest extractor and an explicit user behavior extractor, followed by GRU-based sequential layer to learn dynamic user representations. Despite these improvements, DIEN still does not explicitly account for negative feedback signals. DSTN [32] and DFN [43] highlighted the importance of modeling both users positive and negative (exposure, unclicked) feedback for CTR prediction tasks. Exposure data is abundant, and recent studies demonstrated the usefulness of using such data in behavioral sequential tasks. For instance, XDM [28] demonstrated the effectiveness of exposure data to help guide CARS@RecSys 25, Sep 2226, 2025, Prague, Czech Republic Katariya et al. explored unifying search and recommendation systems, primarily within the domains of e-commerce, travel, and digital media. However, this body of work does not sufficiently address the financial services (FS) sector, where enterprises often operate multiple interrelated products beyond just search and recommendation systems. In the FS industry, personalization efforts and advertising strategies have similarly relied on distinct, product-specific models, thereby overlooking potential gains from shared signals across products. Notably, naive attempts to merge signals from diverse products have revealed fundamental trade-off: improvements in one task frequently degrade performance in others [41]. This challenge is compounded by the heterogeneity of sequential signals across productseach product may depend on distinct features, with only partial overlapsresulting in redundant feature engineering and inefficient inference pipelines. Hence, in this study, we address all the gaps above and propose FinTRec tailored to unifying personalization and ads targeting in FS. CUSTOMER JOURNEY Users interact with FS through multiple channels and products to complete their tasks. As they navigate digital platforms, they are exposed to personalized ads and service messages, delivered via mechanisms such as placement-based targeting or feed-style personalization. However, digital exposure is only part of the journeyusers also engage through offline touch-points, including visits to physical locations or interactions with call center agents. An illustrative user journey Figure 1 shows seven touch-points over month: four via digital platforms, one physical location, one transaction, and one external source (e.g., Google Search). High-value outcomes, such as product acquisitions, are not always attributable to digital ads alone but may result from this broader interaction landscape. It further gets complicated by FSs diverse and interrelated product offerings  (Table 1)  , necessitating accurate capture of contextual information such as channel, page type, placement, and layout. Channels include email, SMS, mobile, and web applications. Ads can appear across hundreds of pagese.g., the homepage, credit card pages, or other product-specific sectionseach with multiple placements (e.g., welcome modules, account details, recent transactions, action buttons, or marketing messages). Interaction data is stored at granular level across disparate databases, using various schema and formats. Each event is timestamped, enabling alignment along users timeline for sequential modeling tasks. positive explicit clicks. DFN [43], DSTN [32], DIEN [52], and XDM [28] all incorporated users positive and exposure feedback independently, ignoring mutual interplay and contextual dependencies between them. Moreover, they also overlooked the influence of spatial page context on user behavior. RACP [11] addressed this gap by jointly modeling both the exposure and click sequences, capturing their interactions. RACP used hierarchical attention architecture enabling the model to learn both intra-page context (how items on the same page influence each other) and inter-page interest shifts across sessions. Despite notable advancements, all these studies overlook the effect of sequence truncation in multi-feedback settings, undermining the cross-correlation between different feedback signals. TEM4CTR [48] employs three-stage framework by first searching unclicked records close to clicked records, followed by an attention mechanism to extract relevant exposure information, and finally an interest extraction module to jointly extract users latent interest representations from both the clicked and unclicked sequences for the CTR prediction task. However, these studies do not address other implicit feedback behavior data generated from cross-channel behaviors that are usually observed and dominant in the FS domain. Recent studies have shown the importance of combining cross-channel behavior for real-time intelligent personalization. [9, 14, 15, 23, 34, 44, 49] studied pre-trained transfer learning approaches and demonstrated the efficacy of embedding data streams combined with the final model to be suitable for realworld systems. However, FS has several such data streams, and combining them for low latency inference is yet to be studied. This requires combining pre-trained embeddings with sequential data. Tab-Transformer [18] contextualized categorical features but lacked contextualization across other feature types [13]. Both FTTransformer and Tab-Transformer studied the architectural changes on several datasets including bank marketing datasets; however, they lacked interaction signals from other heterogeneous data such as payments, transactions, etc., that are dominant in FS [13]. Due to various data sources and multi-modality, developing embeddings on individual data streams and using them in downstream models has been previously discussed [8, 12, 27, 33]. [8] discussed clickstream embeddings as an input for fraud, intent prediction, and tasks in FS. Similarly, [27] demonstrated the impact of payments and transaction embeddings on Fraud as downstream task. Thus, these studies focused on single data stream representation and lacked combining representation from different data streams for accurate marketing and ads recommendation in FS. Furthermore, its common practice to perform feature engineering and convert raw sequences into tabular data [3, 22] for machine learning algorithms such as Gradient Boosted Trees, more specifically in the FS and banking domain [39] due to their higher explainability for closer regulatory alignment. However, this requires extensive feature engineering on longitudinal tabular data leading to long hours spent in developing accurate features. Traditionally, organizations have developed product-specific machine learning models to deliver personalized user experiences. While effective in isolation, this approach often results in fragmented infrastructure and substantial technical debt [2]. Moreover, such siloed models fail to leverage valuable cross-product interactions, leading to suboptimal performance [41]. Recent research [36, 38, 41, 42, 45] has"
        }
    ],
    "affiliations": [
        "Capital One, AI Foundations"
    ]
}