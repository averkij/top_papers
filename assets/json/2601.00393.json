{
    "paper_title": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos",
    "authors": [
        "Yuxue Yang",
        "Lue Fan",
        "Ziqi Shi",
        "Junran Peng",
        "Feng Wang",
        "Zhaoxiang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we propose NeoVerse, a versatile 4D world model that is capable of 4D reconstruction, novel-trajectory video generation, and rich downstream applications. We first identify a common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumbersome training pre-processing. In contrast, our NeoVerse is built upon a core philosophy that makes the full pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-of-the-art performance in standard reconstruction and generation benchmarks. Our project page is available at https://neoverse-4d.github.io"
        },
        {
            "title": "Start",
            "content": "NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos Yuxue Yang1, 2 Lue Fan1 (cid:0) Ziqi Shi1 Junran Peng1 Feng Wang2 Zhaoxiang Zhang1 (cid:0) 1NLPR & MAIS, CASIA 2CreateAI {yangyuxue2023, lue.fan}@ia.ac.cn https://neoverse-4d.github.io 6 2 0 2 1 ] . [ 1 3 9 3 0 0 . 1 0 6 2 : r Figure 1. Illustration of NeoVerse. NeoVerse reconstructs 4D Gaussian Splatting (4DGS) from monocular videos in feed-forward manner. These 4DGS can be rendered from novel viewpoints to provide degraded rendering conditions for generating high-quality and spatial-temporally coherent videos."
        },
        {
            "title": "Abstract",
            "content": "In this paper, we propose NeoVerse, versatile 4D world model that is capable of 4D reconstruction, noveltrajectory video generation, and rich downstream applications. We first identify common limitation of scalability in current 4D world modeling methods, caused either by expensive and specialized multi-view 4D data or by cumIn contrast, our Neobersome training pre-processing. Verse is built upon core philosophy that makes the full (cid:0) Corresponding Authors. Project Lead. 1 pipeline scalable to diverse in-the-wild monocular videos. Specifically, NeoVerse features pose-free feed-forward 4D reconstruction, online monocular degradation pattern simulation, and other well-aligned techniques. These designs empower NeoVerse with versatility and generalization to various domains. Meanwhile, NeoVerse achieves state-ofthe-art performance in standard reconstruction and generation benchmarks. 1. Introduction 4D world modeling holds transformative potential in many fields, such as digital content creation, autonomous driving, and embodied intelligence. Recent approaches have made strides from both 3D side [10, 29, 47, 54, 67, 81] and 4D side [9, 12, 18, 24, 27, 42, 48, 56, 58, 62, 64, 80, 87] with principle of hybrid reconstruction and generation. This paradigm typically involves two stages: reconstructing 3D/4D representation [7, 25, 35, 72, 82] of the scene, and then, using the geometric prior to guide generation models [22, 37, 55, 59, 75]. Such reconstruction-generation hybrid paradigm has widely recognized promising features, including spatiotemporal consistency and precise viewpoint control. However, the current solutions usually have limitations in terms of scalability. The limitation of scalability manifests in two main aspects. (1) Limited data scalability. Some methods, such as ViewCrafter [81], utilize videos of static scenes to create multi-view training data and learn to generate videos in novel trajectories. Although effective, they cannot be extended to 4D scenes. Some other methods, such as SynCamMaster [3], CamCloneMaster [46], and ReCamMaster [2], depend on specialized, hard-to-capture multiview dynamic videos to learn novel trajectory generation. Such non-scalable data limits the models generalization and versatility. (2) Limited training scalability. Another line of work [9, 18, 27, 80] utilizes more flexible data types but usually necessitates cumbersome offline preprocessing stage to create training data. For example, TrajectoryCrafter generates training data using heavy video depth estimator [25] in an offline manner. Similarly, previous work FreeSim [18] pre-reconstructs the Gaussian field to prepare training input, which utilizes offline reconstruction [11, 13] and may even rely on extra 3D detection methods [17, 36, 73, 79]. Such an offline curation usually leads to significant computational burden, storage consumption, inflexible training scheme tuning, and even disables online augmentations. The two kinds of limitations erect barrier to leveraging the cheap and diverse in-the-wild monocular videos, constraining the potential for building more powerful models. To address these challenges, we propose NeoVerse. The core philosophy of NeoVerse is making the full pipeline scalable to diverse in-the-wild monocular videos, enhancing generalization and versatility of 4D world models. To implement our vision, we first propose feed-forward 4DGS model, built upon VGGT [60]. This model not only Gaussianizes VGGT but also features bidirectional motion modeling mechanism, which is crucial for efficient online reconstruction (Sec. 3.2) and applications requiring time control. We then incorporate this feed-forward model into the generation training process. During each training iteration, it efficiently reconstructs 4D scenes using sparse key frames from monocular videos in an online manner. In addition, efficient online monocular degradation simulations, including Gaussian culling and average geometry filter, are proposed to simulate degraded rendering patterns in novel trajectories and offer conditions for generation. Combining them together makes the whole training process scalable to diverse in-the-wild monocular videos (up to 1M clips) in terms of both training efficiency and technical feasibility. We summarize our contributions as follows. We propose NeoVerse, 4D world modeling approach, which is scalable to and enhanced by diverse in-the-wild monocular videos. NeoVerse is versatile, enabling many applications, including 4D reconstruction, multiview video generation, video editing, stabilization, super-resolution, etc. NeoVerse achieves state-of-the-art results in both reconstruction and generation tasks. We will make the source code publicly available to decentralize general 4D world models by leveraging cheap and diverse in-the-wild monocular videos. 2. Related Works Feed-forward Gaussian Reconstruction. Recent stereo and 3D geometry foundation models [31, 34, 38, 39, 44, 60, 65, 68, 71, 76, 82] can estimate dense depth, point maps, and even camera parameters in single forward pass, thereby driving shift in Gaussian Splatting from per-scene optimization to generalizable feed-forward reconstruction. For static scenes, pose-free models such as NoPoSplat [76] reconstruct 3D Gaussians directly from sparse, unposed multi-view images, and AnySplat [31] further extends this paradigm to casually captured, long uncalibrated image sequences. For dynamic scenes, 4DGT [71], StreamSplat [68], and MoVieS [38] push feed-forward GS into 4D; however, each method still retains specific constraints: 4DGT is trained on posed monocular videos and adopts largely uni-directional temporal modeling strategy, MoVieS similarly assumes known camera poses during training and inference, while StreamSplat focuses on frame-by-frame modeling. Reconstruction-based Video Generation. Recent methods such as GEN3C [54], DaS [21], See3D [47], ViewCrafter [81], Difix3D+ [67], GS-DiT [5], Voyager [29], Uni3C [9], FreeSim [18], TrajectoryCrafter [80], See4D [45], PostCam [12], Light-X [41] follow hybrid reconstruction+generation paradigm, where 3D/4D representation is first reconstructed and then used as geometric guidance for generative video model. GEN3C [54] builds depth-based 3D feature cache whose renderings condition video diffusion model for 3D-consistent, posecontrollable synthesis; ViewCrafter [81] adopts pointconditioned video diffusion framework to extend single2 or sparse-view inputs into long-range, high-fidelity novelview sequences; Difix3D+ [67] applies single-step diffusion enhancer to rendered novel views to correct artifacts in underconstrained regions and distill the improvements back into NeRF/3DGS representations; and TrajectoryCrafter [80] formulates camera-controllable video generation for monocular videos as trajectory redirection, conditioning dual-stream diffusion backbone on point-cloud renderings and source frames to follow user-specified camera paths. Despite their strong spatialtemporal consistency and viewpoint controllability, these reconstructionbased approaches are mostly tailored to static or quasi-static scenes and rely on curated data or heavyweight offline reconstruction, limiting scalability to in-the-wild monocular videos. 3. Methodology This section is organized as follows. In Sec. 3.1, we first propose an efficient pose-free feed-forward 4DGS reconstruction model, which reconstructs 4DGS from monocular videos. In Sec. 3.2, we introduce how to combine reconstruction part and generation and make the full pipeline scalable. Sec. 3.3 contains the training scheme and Sec. 3.4 elaborates on inference strategies. 3.1. Pose-Free Feed-Forward 4DGS Reconstruction Our feed-forward model is partially built upon VGGT [60] backbone. For simplicity, we mainly introduce how we make VGGT dynamic and Gaussianized. Bidirectional motion modeling. Given monocular video {I RHW 3}T t=1, VGGT extracts the frame-wise features using the pretrained DINOv2 [50]. These features, concatenated with camera tokens and register tokens, are fed into series of Alternating-Attention blocks [60], obtaining so-called frame features. While this process effectively aggregates spatial information, they are insufficient for motion modeling due to temporal unawareness. We introduce bidirectional motion-encoding branch. Different from uni-directional motion in 4DGT [71], the bidirectional prediction distinguishes the instantaneous velocity between + 1 and 1. Such distinction facilitates temporal Gaussian interpolation between two consecutive timestamps. t=1 and {F t}T Specifically, for the frame features {F t}T t=1, we copy and slice them into two parts along the temporal dimension: {F t}T 1 t=2. Then we obtain forward motion features using the first part as queries and the second part as keys and values. Similarly, the backward motion features are encoded conversely. Formally, we have {F fwd {F bwd t=1 = CrossAttn(q = {F t}T 1 t=2 = CrossAttn(q = {F t}T t=1 ; k, = {F t}T t=2; k, = {F t}T 1 }T 1 }T t=2), t=1 ), (1) and bwd where fwd are forward motion features from timestamp to + 1, and backward motion features from to 1. These features will be utilized to predict bidirectional linear and angular velocity of Gaussian primitives. Gaussianizing VGGT. We first define 4D Gaussians as {(µi, αi, ri, si, shi, τ i, v+ , , ω+ , ω )}T HW i= , (2) where each Gaussian is parameterized by: 3D position µi, opacity αi, rotation ri, scale si, and spherical harmonics coefficients shi, as inherited from 3D Gaussians [35]. For bidirectional motion modeling, we introduce forward and , backward velocities v+ , and forward and backward angular velocities ω+ , ω . In addition, we adopt life span τ following the common practice in 4DGS. The 3D positions {µi} is obtained by backprojecting pixel depth to 3D space using predicted depth and camera parameters. For the other attributes, {(µi, αi, ri, si, shi, τ i} are predicted from the frame features, while the dynamic attributes {v+ , ω , ω+ } are predicted from the bidirectional motion features. , 3.2. Reconstruction-guided Video Generation In this subsection, we introduce how to combine the reconstruction and generation in scalable training pipeline. Efficient on-the-fly reconstruction from sparse key frames. Although the proposed feed-forward 4DGS reconstruction is efficient, it can still be the bottleneck of training efficiency if we conduct on-the-fly reconstruction with long video input. To boost the training efficiency, we propose reconstruction from sparse key frames. Given long video input with frames, we only take key frames as reconstruction input but render from all the frames since the rendering process is extremely efficient compared with network computation. However, such an operation requires interpolating the Gaussian field at nonkeyframes. Thanks to our bidirectional motion modeling, such interpolation can be implemented as follows. Given non-key-frame query timestamp tq, we transfer nearest key-frame Gaussian at timestamp to tq following µi(tq) = (cid:40) µi + v+ µi + i tq t, tq t, tq t, tq < t, ri(tq) = (cid:40) ri ϕ(ω+ ri ϕ(ω tq t), tq t), tq t, tq < t, αi(tq) = αiexp(γ d(tq, t) 1 1τi ), (3) (4) (5) where we assume the real-world motion in short interval between two adjacent input frames is approximately linear. 3 Figure 2. Framework of NeoVerse. In the reconstruction part, we propose pose-free feed-forward 4DGS reconstruction model (Sec. 3.1) with bidirectional motion modeling. The degraded renderings in novel viewpoints from 4DGS are input to the generation model as conditions. During training, the degraded rendering conditions are simulated from monocular videos (Sec. 3.2), and the original videos themselves serve as targets. to carefully simulate degradation renderings paired with ground-truth monocular frames. Therefore, we propose three techniques to simulate the degradation rendering patterns based on monocular videos. (1) Visibility-based Gaussian Culling for occlusion simulation. Given the camera pose trajectory predicted from the sparse key frames, we apply random transform to the trajectory to obtain novel trajectory. constraint is applied to this transform to ensure new camera poses still roughly point to the scene center. Using depth, we can easily identify those Gaussians that are occluded from the transformed new camera poses. We then simply cull those invisible Gaussian primitives and render the remaining Gaussian primitives back into the original viewpoints, resulting degradation pattern demonstrated in Fig. 3 (a). (2) Average Geometry Filter for flying-edge-pixel and distortion simulation. In addition to the occlusion, another typical degradation pattern is the flying pixels in depthdiscontinuous edges. The network has tendency to produce average depth value at those edges to minimize regression loss, as also confirmed by [69]. From first-principles perspective, we propose to use average filter to create such averaged depth patterns. Specifically, we render depth in the transformed novel trajectory and apply an average filter in the rendered depth map. We then adjust the center position of each Gaussian according to the average filtered depth value. When such modified Gaussians are rendered back into the original views, the flying-pixel pattern appears as shown in Fig. 3 (b). We can further apply larger filter kernel to simulate spatially broader distortions shown in Fig. 3 Figure 3. Training pairs with degradation simulation. Angular velocities ω are represented in the axis-angle representation, and ϕ() converts it to quaternion. The opacity of the Gaussian is represented by time-varying function to ensure natural transition between input frames. To handle non-uniform keyframe intervals, we model opacity decay with normalized temporal distance d(tq, t) = Tk+1Tk 1, where [Tk, Tk+1] is the keyframe interval containing query timestamp tq. The life span τ is constrained in the range of (0, 1) with sigmoid function, and γ is hyper-parameter that controls the decay speed. When τ approaches 1, the exp() tends towards 1, indicating αi(tq) αi; otherwise, αi(tq) decays rapidly. tqt Monocular degradation simulation. Our generation model is expected to generate high-quality novel views from low-quality novel view renderings, necessitating such training pairs. For multi-view or static datasets [40, 78], we can easily get such training pairs as in ViewCrafter [81]. for in-the-wild monocular videos, we need However, (c), caused by potential depth error. All three kinds of degradations in Fig. 3 are simulated with fundamental principles in geometry relation and depth learning, and designed to be simple yet effective, enabling the utilization of in-the-wild monocular videos. Degraded rendering conditioning. We use the obtained degraded renderings as conditions for generation and the original videos as targets. The rendered conditions include multiple modalities, including RGB images, depth maps, and masks binarized from opacity maps to indicate the empty regions. Pluker embeddings of the original trajectory are also computed to provide explicit 3D camera motion information [9]. We introduce control branch to incorporate them into the generation model like [29, 32, 74, 83]. During training, we only train the control branch while freezing the video generation model, not only for training efficiency, but more importantly, to make NeoVerse accessible to powerful distillation LoRAs [23] to speed up the generation process. 3.3. Training Scheme We partition the training into two stages: 1) reconstruction model training; 2) generation model training with on-the-fly reconstruction and degradation simulation. Reconstruction. We train our feed-forward 4DGS reconstruction model with multi-task loss on various static and dynamic 3D datasets: Lrecon = Lrgb +λ1Lcamera +λ2Ldepth +λ3Lmotion +λ4Lregular, (6) where Lrgb is the photometric loss between rendered and ground-truth images, including an L2 loss and LPIPS [84] loss. The camera loss Lcamera and depth loss Ldepth supervise the predicted camera parameters and depth maps following VGGT [60]. Notably, Ldepth also contains the supervision for rendered depth from Gaussians. The motion loss Lmotion = (cid:80) adds supervision on the predicted bidirectional velocities, where ˆv+ are the ground-truth forward and backward velocities computed from some dynamic 3D datasets [8, 20, 33, 49, 57, 86]. To prevent the Gaussians from becoming erroneously transparent, we introduce regularization loss Lregular = (cid:80) 1 Ai, where Ai is rendered accumulated opacity map. +ˆv and ˆv v+ ˆv+ Generation. For generation model training, we adopt Rectified Flow [16] and Wan-T2V [59] 14B to model the denoising diffusion process. The whole training process is performed on monocular videos. Given monocular video, we first utilize on-the-fly reconstruction from sparse key frames to obtain 4DGS and simulate degradation renderings as conditions crender. For the video latent x1 and 5 sampled noise x0 (0, I), the training objective of generation model fθ is formulated as Lgen = Ex1,x0,crender,ctext,tfθ(xt, t, crender, ctext) vt2 2, (7) where xt is linear interpolation between x1 and x0 at timestamp t, vt = x1 x0 is ground-truth velocity. ctext is the text condition extracted from the video caption using language model like umT5 [14]. Renderings crender are input into the generation model through control branch like [32, 83]. 3.4. Inference Reconstruction and global motion tracking. Given monocular video, our feed-forward model outputs 4DGS and camera parameters of each frame. Before rendering conditions from novel trajectory, we can optionally aggregate Gaussians from multiple timestamps into single timestamp for more complete representation. For better aggregation, we conduct motion separation by global motion tracking. The motivation of global motion tracking is to identify those objects undergoing both static and dynamic phases in clip, which should be regarded as the dynamic part and cannot be easily identified using predicted instantaneous velocity. Taking Gaussian primitive as example, given world-to-camera poses {P t}T t=1, camera intrinsics {Kt}T t=1, and Gaussian position µi for Gaussian i, we project the Gaussian center to each frame and compute its projected pixel coordinates pi,t and depth di,t. Let Dt[pi,t] and Vt[pi,t] are the sampled depth and velocity at pixel pi,t. We define visibility-weighted maximum velocity magnitude at the global video level as mi,t = max{V + mi = max t=1,...,T [pi,t]2, [pi,t]2}, 1(di,t Dt[pi,t]) mi,t, (8) where mi,t is the maximum velocity magnitude at frame t, 1() is function indicating whether the Gaussian is visible, and mi is the visibility-weighted maximum velocity magnitude across all frames. Finally, we separate the Gaussians into static set and dynamic set according to mi with threshold η. Temporal aggregation, interpolation, and generation. With separated dynamic part and static part, we conduct two different Gaussian temporal aggregation strategies for each part, respectively. The static part is simply aggregated across all frames, while the dynamic part is aggregated only from couple of nearby frames to avoid motion drifting errors. In some cases, we may need to interpolate Gaussians into an intermediate timestamp between two adjacent discrete frames. typical case is creating slow-motion videos and bullet-time shots. Our bidirectional motion mechanism sufficiently supports such tasks happening in short time interval. In practice, we use similar techniques in Sec. 3.2 for interpolation. After the optional aggregation and interpolation, we render the resulting Gaussians into any desired novel trajectory. The renderings, along with other conditions, are sent to the generation model to generate videos. 4. Experiments 4.1. Implementation For reconstruction, we follow the learning rate schedule of VGGT [60]. We resize all input videos to have longest edge of 560 pixels. GSplat [77] is adopted as the Gaussian Splatting rendering backend. For the generation, the video resolution is fixed at 336 560 and the length is set to 81 frames. The training is conducted on 32 A800 GPUs, where the first stage trains 150K iterations and the second stage trains 50K iterations. More training details can be found in the supplementary material. Datasets. We collect 18 public datasets following CUT3R [63], including Arkitscenes [4], DL3DV [40], PointOdyssey [86], Kubric [20], Waymo [57], SpatialVID [61], GFIE [26], etc. Besides the above datasets, we further curate large-scale self-collected monocular video dataset from the internet, containing over 1M videos from diverse scenarios. More details about datasets are provided in the supplementary material. 4.2. Quantitative Evaluation Reconstruction benchmark. Our reconstruction results on both static and dynamic datasets are shown in Table 1 and Table 2, respectively. Our reconstruction part achieves state-of-the-art performance among all metrics. Recent reprints MoVieS [38] and StreamSplat [68] are not listed in the table because they are neither open-sourced nor provide detailed evaluation protocol. Our detailed evaluation protocols are provided in the supplementary material. Method VRNeRF [70] (16 views) SSIM LPIPS PSNR Scannet++ [78] (32 views) SSIM LPIPS PSNR NoPoSplat [76] Flare [85] AnySplat [31] Ours 11.27 12.62 18.02 20.73 0.408 0.597 0.705 0.766 0.620 0.623 0.366 0.352 8.69 12.19 22.79 25.34 0.312 0.619 0.773 0. 0.614 0.611 0.217 0.195 Table 1. Quantitative comparison with other static reconstruction models. Method MonST3R [82] 4DGT [71] Ours PSNR 17.42 30.09 32. ADT [51] SSIM LPIPS PSNR DyCheck [19] SSIM LPIPS 0.554 0.909 0. 0.534 0.178 0.120 9.32 9.94 11.56 0.103 0.208 0.293 0.710 0.639 0.558 Table 2. Quantitative comparison with other dynamic reconstruction models. : indicate the method takes camera poses as input. performance. We conduct more analysis in the section of qualitative evaluation. Runtime evaluation. Table 3 also shows the efficiency evaluation of both the reconstruction stage and the generation stage. Thanks to our intentional design of condition injection in Sec. 3.2, our generation process gets significantly accelerated by the off-the-shelf distillation technique [15]. More importantly, as discussed in Sec. 3.2, our bidirectional motion design enables more efficient reconstruction from sparse key frames without loss of generation performance. 4.3. Qualitative Evaluation and Analysis For an intuitive understanding, we conduct rich qualitative evaluations and analysis, leading to the following findings. Rendering quality. Fig. 5 and Fig. 6 demonstrate the rendering quality comparison. Our model not only achieves better visual quality but is also more faithful to input observations. Instead, other methods may predict unreal artifacts such as regions indicated by yellow boxes in Fig. 5. It Pose prediction accuracy. is noteworthy that our model also has better pose prediction accuracy. In Fig. 5, the compared method [31] shows field of view (images with red boundaries) inconsistent with the ground truth, which is caused by inaccurate pose prediction. Trajectory controlability vs. generation quality. An intriguing and fundamental phenomenon we can find in Fig. 4 is that related work usually demonstrates trade-off between generation quality and trajectory controllability. Specifically, TrajectoryCrater, reconstruction-generation hybrid method similar to our NeoVerse , shows good trajectory controllability and exhibits consistent trajectories with our method, while its generation quality is inferior. This is mainly caused by its non-scalable training pipeline, stopping the model from seeing diverse in-the-wild videos, such as very challenging human activities in Fig. 4. In contrast, the purely generation-based method ReCamMaster shows good visual generation quality, but cannot achieve precise trajectory control, which is crucial in some downstream tasks such as simulation. Generation benchmark. In Table 3, we compare the generation performance with related work TrajectoryCrafter [80] and ReCamMaster [2], demonstrating better Artifact suppression. Another reason for our superiority over the similar reconstruction-based TrajectoryCrafter is that our degradation simulations  (Fig. 3)  enable artifact 6 Figure 4. Generation with large camera motions on challenging in-the-wild videos. We compare our method against other related work on Pan left (left) and Move right (right) cases. Our NeoVerse achieves better generation quality while maintaining precise camera controllability. Yellow boxes highlight artifacts. Method Frames Inference Time (s) Recon. Gen. Total TrajectoryCrafter [80] ReCamMaster [2] Ours (11 key frames) Ours (21 key frames) Ours (41 key frames) Ours (full frames) 49 81 81 81 81 38 - 2 3 5 10 121 168 159 168 18 18 18 20 21 23 28 Subj. Consist. Back. Consist. Temp. Flick. Motion Smooth. Aesth. Quality Imag. Quality 83.02 88.21 88.43 88.73 89.10 89.42 88.58 91.60 92.27 92.43 92.65 92. 94.71 96.56 96.77 96.76 96.67 96.51 97.64 98.86 98.80 98.71 98.63 98.67 44.63 44.29 44.55 44.59 44.89 44. 54.59 58.87 59.75 60.01 60.37 61.51 Table 3. VBench [30] results for novel view generation. We randomly collect 100 unseen in-the-wild videos, each with 4 different camera trajectories, resulting in total of 400 test cases. For fair comparison of inference time, we resize all videos to 336 560 resolution and report the average results over all test cases. The runtime evaluation is conducted on an A800 GPU. suppression. In contrast, the generation quality of TrajectoryCrafter is significantly decreased by ghosting patterns from inaccurate reconstruction. Contextually grounded imagination. Fig. 4 also demonthat our NeoVerse can conduct contextually strates grounded imagination for non-observed regions, such as the second singer and crowded people. We give credit to our design scalability to diverse in-the-wild videos. 4.4. Ablation Study Method PSNR SSIM LPIPS w/o Regularization w/o Bidirectional Motion Reconstruction part w/ Generation 10.86 11.27 11.56 14. 0.244 0.285 0.293 0.323 0.576 0.570 0.558 0.501 Table 4. Ablation experiments on DyCheck. w/. Generation indicates our full pipeline, which gains significant performance improvements over the pure reconstruction part. Motion modeling. In Table 4, we remove the motion modeling mechanism by skipping Eq. (1) and predicting motions directly from frame features. The performance drop reveals the effectiveness of our modeling mechanism. Opacity regularization. In Sec. 3.3, we introduce opacity regularization to avoid the model learning shortcut, which is outputting transparent primitives for the regions in similar colors to the predefined background color. This technique is proven effective in Table 4. Degradation simulation. As discussed in Sec. 3.2, large camera motions often result in degraded renderings containing flying edge pixels and distortions. Fig. 7 demonstrates the necessity of our online degradation simulation. Without training on simulated degraded samples, the generation model tends to trust the geometric artifacts in the condition, leading to ghosting effects or blurred outputs. By incorporating degradation simulation, the model learns to suppress these artifacts and hallucinate realistic details in occluded or distorted regions. 7 Figure 5. Qualitative comparison with state-of-the-art methods in static scenes. Red boundaries indicate inconsistent renderings due to inaccurate pose prediction. Yellow boxes indicate artifacts. Figure 7. Effectiveness of degradation simulation. The model learns to suppress artifacts and hallucinate realistic details in occluded or distorted regions through degradation simulation. Figure 6. Qualitative comparison with state-of-the-art methods in dynamic scenes. Yellow boxes indicate artifacts. Note that the black regions in our prediction are not error but mainly caused by partial observations of input frames. Global motion tracking. Fig. 8 showcases the importance of global motion tracking when identifying the dynamic instances. Without the global tracking, some dynamic objects are mistakenly identified as static due to partial static state. 4.5. Applications superiority of NeoVerse is the support for rich downstream applications other than the novel trajectory video generation. Due to the limited space, here we briefly introduce several typical applications, leaving more details in the supplementary materials. 3D tracking. By associating nearest Gaussian primitives between consecutive frames using predicted 3D flow, our NeoVerse achieves 3D tracking shown in Fig. 9. Figure 8. Visualization about global motion tracking and aggregation. (a) Input video. (b) Aggregated static Gaussians separated by predicted velocities. (c) Aggregated static Gaussians separated with global motion tracking. Figure 9. Visualization of 3D tracking. For better visualization, we only show the Gaussian centers. Video editing. Since our model has binary mask condition and textual condition, it can edit videos with the help of video segmentation model [53], demonstrated in Fig. 10. [4] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. 6, 1 [5] Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, FuYun Wang, and Hongsheng Li. Gs-dit: Advancing video generation with pseudo 4d gaussian fields through efficient dense 3d point tracking. arXiv preprint arXiv:2501.02690, 2025. 2 [6] Michael Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: synthetic dataset of bodies exhibiting detailed lifelike animated motion. In CVPR, pages 87268737, 2023. 1 [7] Aleksei Bochkovskii, Ama AG Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. 2 [8] Yohann Cabon, Naila Murray, and Martin Humenberger. Virtual kitti 2. arXiv preprint arXiv:2001.10773, 2020. 5, 1 [9] Chenjie Cao, Jingkai Zhou, Shikai Li, Jingyun Liang, Chaohui Yu, Fan Wang, Xiangyang Xue, and Yanwei Fu. Uni3c: Unifying precisely 3d-enhanced camera and human motion controls for video generation. arXiv preprint arXiv:2504.14899, 2025. 2, 5 [10] Luxi Chen, Zihan Zhou, Min Zhao, Yikai Wang, Ge Zhang, Wenhao Huang, Hao Sun, Ji-Rong Wen, and Chongxuan Li. Flexworld: Progressively expanding 3d scenes for flexiableview synthesis. arXiv preprint arXiv:2503.13265, 2025. [11] Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, and Li Zhang. Periodic vibration gaussian: Dynamic urban scene reconstruction and real-time rendering. arXiv preprint arXiv:2311.18561, 2023. 2 [12] Yipeng Chen, Zhichao Ye, Zhenzhou Fang, Xinyu Chen, Xiaoyu Zhang, Jialing Liu, Nan Wang, Haomin Liu, and Guofeng Zhang. Postcam: Camera-controllable novel-view video generation with query-shared cross-attention. arXiv preprint arXiv:2511.17185, 2025. 2 [13] Ziyu Chen, Jiawei Yang, Jiahui Huang, Riccardo de Lutio, Janick Martinez Esturo, Boris Ivanovic, Or Litany, Zan Gojcic, Sanja Fidler, Marco Pavone, et al. Omnire: Omni urban scene reconstruction. arXiv preprint arXiv:2408.16760, 2024. 2 [14] Hyung Won Chung, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151, 2023. 5 [15] LightX2V Contributors. Lightx2v: Light video generation inference framework. https://github.com/ ModelTC/lightx2v, 2025. 6 [16] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. Figure 10. Video editing. Left: The white car is edited to be red. Right: The mirror teapot is edited to be transparent. Video stabilization. By smoothing the predicted camera trajectory, our model achieves effective video stabilization, as demonstrated in the teaser Fig. 1. Video super-resolution The Gaussian representation in NeoVerse supports flexible rendering resolution without the significant loss of appearance information. Thus, NeoVerse can achieve video super-resolution by generation with larger rendering resolution, also demonstrated in Fig. 1. Others. Moreover, NeoVerse is also capable of other applications such as background extraction  (Fig. 8)  , image to world  (Fig. 1)  . We leave more demonstrations in the supplementary materials. 5. Conclusion and Limitations In this paper, we introduce NeoVerse, 4D world model that overcomes key scalability limitations in previous arts, building training pipeline scalable to in-the-wild monocular videos. Thus, the generalization and versatility of NeoVerse are significantly enhanced by the diverse in-the-wild data, enabling various downstream applications. Extensive experiments demonstrate state-of-the-art performance in both reconstruction and generation tasks. Limitations. NeoVerse correct it cannot underlying 3D information. be trivially applied to data without 3D information trainlike 2D cartoons. ing resources, our curated dataset is not future work. that Due to the constraints of (1M clips) large. We leave more data for data with Therefore, requires"
        },
        {
            "title": "References",
            "content": "[1] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, Aron Monszpart, Victor Prisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose relative to single image. In ECCV, pages 690708. Springer, 2022. 1 [2] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. In ICCV, 2025. 2, 6, 7 [3] Jianhong Bai, Menghan Xia, Xintao Wang, Ziyang Yuan, Xiao Fu, Zuozhu Liu, Haoji Hu, Pengfei Wan, and Di Zhang. Syncammaster: Synchronizing multi-camera video generation from diverse viewpoints. In ICLR, 2025. 2 9 In Forty-first international conference on machine learning, 2024. 5 [17] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Embracing Single Stride 3D Object Detector with Sparse Transformer. In CVPR, 2022. [18] Lue Fan, Hao Zhang, Qitai Wang, Hongsheng Li, and Zhaoxiang Zhang. Freesim: Toward free-viewpoint camera simulation in driving scenes. In CVPR, pages 1200412014, 2025. 2 [19] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: reality check. NIPS, 35:3376833780, 2022. 6, 2 [20] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: In CVPR, pages 37493761, scalable dataset generator. 2022. 5, 6, 1 [21] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, et al. Diffusion as shader: 3d-aware video diffusion for versatile video generation control. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 112, 2025. 2 [22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2 [23] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 5 [24] Tao Hu, Haoyang Peng, Xiao Liu, and Yuewen Ma. Ex-4d: Extreme viewpoint 4d video synthesis via depth watertight mesh. arXiv preprint arXiv:2506.05554, 2025. [25] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying Shan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, pages 20052015, 2025. 2 [26] Zhengxi Hu, Yuxue Yang, Xiaolin Zhai, Dingye Yang, Bohan Zhou, and Jingtai Liu. Gfie: dataset and baseline for gaze-following from 2d to 3d in indoor environments. In CVPR, pages 89078916, 2023. 6, 1 [27] Jiaxin Huang, Sheng Miao, Bangbang Yang, Yuewen Ma, and Yiyi Liao. Vivid4d: Improving 4d reconstruction from In ICCV, pages monocular video by video inpainting. 1259212604, 2025. 2 [28] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR, pages 28212830, 2018. 1 [29] Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, et al. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. arXiv preprint arXiv:2506.04225, 2025. 2, 5 [30] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchIn CVPR, pages mark suite for video generative models. 2180721818, 2024. [31] Lihan Jiang, Yucheng Mao, Linning Xu, Tao Lu, Kerui Ren, Yichen Jin, Xudong Xu, Mulin Yu, Jiangmiao Pang, Feng Zhao, et al. Anysplat: Feed-forward 3d gaussian splatting from unconstrained views. arXiv preprint arXiv:2505.23716, 2025. 2, 6 [32] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 5 [33] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. In CVPR, pages 1322913239, 2023. 5, 1 [34] Nikhil Keetha, Norman Muller, Johannes Schonberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, et al. Mapanything: Universal feed-forward metric 3d reconstruction. arXiv preprint arXiv:2509.13414, 2025. 2 [35] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. TOG, 42(4):1391, 2023. 2, 3 [36] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: learning birds-eye-view representation from lidar-camera via spatiotemporal transformers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 2 [37] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 2 [38] Chenguo Lin, Yuchen Lin, Panwang Pan, Yifan Yu, Honglei Yan, Katerina Fragkiadaki, and Yadong Mu. Movies: Motion-aware 4d dynamic view synthesis in one second. arXiv preprint arXiv:2507.10065, 2025. 2, [39] Haotong Lin, Sili Chen, Junhao Liew, Donny Chen, Zhenyu Li, Guang Shi, Jiashi Feng, and Bingyi Kang. Depth anything 3: Recovering the visual space from any views. arXiv preprint arXiv:2511.10647, 2025. 2 [40] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep In CVPR, pages 2216022169, learning-based 3d vision. 2024. 4, 6, 1 [41] Tianqi Liu, Zhaoxi Chen, Zihao Huang, Shaocong Xu, Saining Zhang, Chongjie Ye, Bohan Li, Zhiguo Cao, Wei Li, Hao Zhao, et al. Light-x: Generative 4d video rendering with camera and illumination control. arXiv preprint arXiv:2512.05115, 2025. 2 [42] Tianqi Liu, Zihao Huang, Zhaoxi Chen, Guangcong Wang, Shoukang Hu, Liao Shen, Huiqiang Sun, Zhiguo Cao, Wei Li, and Ziwei Liu. Free4d: Tuning-free 4d scene generation with spatial-temporal consistency. arXiv preprint arXiv:2503.20785, 2025. 2 10 [43] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi. Hoi4d: 4d egocentric dataset for category-level humanobject interaction. In CVPR, pages 2101321022, 2022. [44] Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, and Chunchao Guo. Worldmirror: Universal 3d world reconstruction with any-prior prompting. arXiv preprint arXiv:2510.10726, 2025. 2 [45] Dongyue Lu, Ao Liang, Tianxin Huang, Xiao Fu, Yuyang Zhao, Baorui Ma, Liang Pan, Wei Yin, Lingdong Kong, See4d: Pose-free 4d generaWei Tsang Ooi, et al. arXiv preprint tion via auto-regressive video inpainting. arXiv:2510.26796, 2025. 2 [46] Yawen Luo, Jianhong Bai, Xiaoyu Shi, Menghan Xia, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Tianfan Xue. Camclonemaster: Enabling reference-based camera control for video generation. In SIGGRAPH Asia, 2025. 2 [47] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. In CVPR, pages 20162029, 2025. 2 [48] Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, and Qifeng Chen. Follow-your-creation: Empowering 4d creation through video inpainting. arXiv preprint arXiv:2506.04590, 2025. 2 [49] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution highdetail dataset and benchmark for scene flow, optical flow and stereo. In CVPR, pages 49814991, 2023. 5, 1 [50] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3, [51] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In ICCV, pages 2013320143, 2023. 6, 2 [52] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn ICCV, pages sion transformers for dense prediction. 1217912188, 2021. 1 [53] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 8 [54] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera control. In CVPR, pages 61216132, 2025. 2 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. [56] Chenxi Song, Yanming Yang, Tong Zhao, Ruibo Li, and Chi Zhang. Worldforge: Unlocking emergent 3d/4d generation in video diffusion model via training-free guidance. arXiv preprint arXiv:2509.15130, 2025. 2 [57] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception In CVPR, for autonomous driving: Waymo open dataset. pages 24462454, 2020. 5, 6, 1 [58] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExIn Eutreme monocular dynamic novel view synthesis. ropean Conference on Computer Vision, pages 313331. Springer, 2024. 2 [59] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 2, 5, 1 [60] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In CVPR, pages 5294 5306, 2025. 2, 3, 5, 6 [61] Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, et al. Spatialvid: large-scale video dataset with spatial annotations. arXiv preprint arXiv:2509.09676, 2025. 6, [62] Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, and Kun Gai. Cinemaster: 3d-aware and controllable framework for cinematic text-to-video generation. In SIGGRAPH, pages 110, 2025. 2 [63] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In CVPR, pages 10510 10522, 2025. 6 [64] Qisen Wang, Yifan Zhao, Peisen Shen, Jialu Li, and Jia Li. Chronosobserver: Taming 4d world with hyperspace diffusion sampling. arXiv preprint arXiv:2512.01481, 2025. 2 [65] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d viIn CVPR, pages 2069720709, 2024. 2, sion made easy. 1 [66] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of visual slam. In IROS, pages 49094916, 2020. 1 [67] Jay Zhangjie Wu, Yuxuan Zhang, Haithem Turki, Xuanchi Ren, Jun Gao, Mike Zheng Shou, Sanja Fidler, Zan Gojcic, and Huan Ling. Difix3d+: Improving 3d reconstructions with single-step diffusion models. In CVPR, pages 26024 26035, 2025. 2, [68] Zike Wu, Qi Yan, Xuanyu Yi, Lele Wang, and Renjie Streamsplat: Towards online dynamic 3d reconLiao. struction from uncalibrated video streams. arXiv preprint arXiv:2506.08862, 2025. 2, 6 11 [83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 5 [84] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep In CVPR, pages 586595, features as perceptual metric. 2018. 5 [85] Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, and Gordon Wetzstein. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. In CVPR, pages 2193621947, 2025. 6 [86] Yang Zheng, Adam Harley, Bokui Shen, Gordon Wetzstein, and Leonidas Guibas. Pointodyssey: large-scale In ICCV, synthetic dataset for long-term point tracking. pages 1985519865, 2023. 5, 6, [87] Jingqiu Zhou, Lue Fan, Linjiang Huang, Xiaoyu Shi, Si Liu, Zhaoxiang Zhang, and Hongsheng Li. Flexdrive: Toward trajectory flexibility in driving scene gaussian splatting reIn CVPR, pages 15491558, construction and rendering. 2025. 2 [69] Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, et al. Pixel-perfect depth with semantics-prompted diffusion transformers. arXiv preprint arXiv:2510.07316, 2025. 4 [70] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia, Aayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo Porzi, Peter Kontschieder, Aljaˇz Boˇziˇc, et al. Vr-nerf: HighIn SIGGRAPH Asia, fidelity virtualized walkable spaces. pages 112, 2023. 6, 2 [71] Zhen Xu, Zhengqin Li, Zhao Dong, Xiaowei Zhou, Richard Newcombe, and Zhaoyang Lv. 4dgt: Learning 4d gaussian transformer using real-world monocular videos. arXiv preprint arXiv:2506.08015, 2025. 2, 3, 6 [72] Honghui Yang, Di Huang, Wei Yin, Chunhua Shen, Haifeng Liu, Xiaofei He, Binbin Lin, Wanli Ouyang, and Tong He. Depth any video with scalable synthetic data. arXiv preprint arXiv:2410.10815, 2024. 2 [73] Yuxue Yang, Lue Fan, and Zhaoxiang Zhang. Mixsup: Mixed-grained supervision for label-efficient lidar-based 3d object detection. arXiv preprint arXiv:2401.16305, 2024. 2 [74] Yuxue Yang, Lue Fan, Zuzeng Lin, Feng Wang, and Zhaoxiang Zhang. Layeranimate: Layer-level control for animation. In ICCV, pages 1086510874, 2025. [75] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2 [76] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. In ICLR, 2024. 2, 6 [77] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, et al. gsplat: An open-source library for gaussian splatting. Journal of Machine Learning Research, 26(34):117, 2025. 6 [78] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In ICCV, pages 1222, 2023. 4, 6, 1, 2 [79] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. CenterIn Proceedings of based 3d object detection and tracking. the IEEE/CVF conference on computer vision and pattern recognition, pages 1178411793, 2021. 2 [80] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocular videos via diffusion models. In ICCV, 2025. 2, 3, 6, 7 [81] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 2, 4 [82] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and MingHsuan Yang. Monst3r: simple approach for estimatarXiv preprint ing geometry in the presence of motion. arXiv:2410.03825, 2024. 2, 12 NeoVerse: Enhancing 4D World Model with in-the-wild Monocular Videos"
        },
        {
            "title": "Supplementary Material",
            "content": "We provide videos on the project page1 to vividly present qualitative results for an enhanced view experience. A. Implementation Details Reconstruction model. The transformer decoders in the bidirectional motion-encoding branch follow the design of DUSt3R [65], where each decoder block consists of selfattention layer for intra-frame spatial modeling and crossattention layer for inter-frame temporal modeling. Finally, two DPT [52] heads are employed to predict the forward and backward motions, respectively. Here, we define the forward/backward velocities {v+ } as the 3D displacements from the current frame to the next/previous frame in the camera coordinate. , Generation model. The multiple encoders for multimodal conditions are implemented with 1) VAE [59] encoder for RGB images and depth maps, 2) convolutional layers with 8 spatial and 4 temporal compression ratio for masks and pluker embeddings. During the generation training stage, only convolutional layers are trainable while the VAE encoder is frozen. B. Training Details To ensure compatibility with the patch size of DINOv2 [50] in the reconstruction model (14 downsampling) and the VAE in the generation model (8 compression), we resize all input videos to have longest edge of 560 pixels during reconstruction training, and fixed resolution of 336 560 during generation training. Reconstruction model. We train the reconstruction model on combination of static and dynamic 3D datasets. For each training iteration, we sample key frames (where 2 8) and 1 intermediate target frames between adjacent key frames. While only the key frames are processed by the reconstruction model to predict Gaussians, the supervision loss is computed on all 2N 1 frames. We utilize cosine learning rate schedule with peak learning rate of 1 104 and warmup 5K iterations. To enhance the models robustness to temporal direction, we apply random temporal reversal augmentation with probability of 0.5. The weights for the multi-task loss (Eq. 6 in the main paper) are set as follows: λ1 = 5.0 (camera), λ2 = 1.0 (depth), λ3 = 1.0 (motion), and λ4 = 0.1 (regularization). 1https://neoverse-4d.github.io Dataset Dynamic Depth Pose Flow Real ① PointOdyssey [86] DynamicReplica [33] Kubric [20] Spring [49] VKITTI2 [8] Waymo [57] ② TartanAir [66] BEDLAM [6] MVS-Synth [28] GFIE [26] ③ HOI4D [43] CoP3D ④ DL3DV [40] Scannet++ [78] ARKitScenes [4] HyperSim [4] MapFree [1] ⑤ SpatialVID [61] Monocular Videos Clip 131 483 5.7K 37 50 798 369 10.4K 120 3.0K 2.8K 6.4K 853 4.5K 457 460 371.3K 1M Table S1. Training Datasets. We categorize existing datasets into 5 groups based on their data characteristics. Group ①④ are used in reconstruction training, while group ⑤ is used in generation training. : we only use videos for generation training. Generation model. For the generation model, we use constant learning rate of 1 105 and batch size of 1 per GPU. To enable efficient on-the-fly reconstruction, we randomly sample 11 21 keyframes from each video clip to reconstruct the 4DGS representation. Additionally, we employ mask drop strategy where we randomly set all masks to 0 (indicating all degraded renderings need inpainting) with probability of 0.2 to improve model robustness. C. Dataset Details We summarize the datasets used in our training in Table S1. Our training data is categorized into five groups: ① Dynamic datasets with 3D flow for velocity supervision. ② Dynamic datasets with depth and camera poses. ③ Dynamic datasets with incomplete 3D information (e.g., only camera poses or depth). ④ Static datasets (we assume 3D flow is zero). ⑤ Monocular videos. We train the reconstruction model on ① to ④, while the generation model is trained on ⑤. Though SpatialVID provides 3D information, we dont use it for reconstruction training due to its unstable depth quality. D. Evaluation Protocol Following AnySplat, we perform test-time pose alignment to facilitate fair comparison, without introducing groundtruth poses during inference. Figure S1. Failure cases. Top: Text generation failure. Bottom: Novel view generation on 2D data. Static reconstruction. We evaluate static reconstruction performance on VRNeRF [70] and Scannet++ [78]. VRNeRF: We select 6 scenes captured with pinhole cameras. For each scene, we randomly sample 16 views as input for reconstruction and 8 novel views for testing. Scannet++: We evaluate on all 50 scenes in the test set. We utilize 32 input views for reconstruction and evaluate on 16 novel views. Dynamic reconstruction. For dynamic reconstruction on ADT [51], we follow 4DGT [71] to evaluate the same 4 scenes: Apartment release multiuser cook seq141 M1292 Apartment release multiskeleton party seq114 M1292 Apartment release meal skeleton seq135 M1292 Apartment release work skeleton seq137 M1292 For each sequence, we sample clip of 64 consecutive frames. We use 32 frames (stride 2) as input and the remaining 32 interleaved frames for testing. For DyCheck [19], we evaluate 5 scenes (apple, block, paper-windmill, spin, teddy). We sample 64 consecutive timestamps for each scene, using 32 frames (stride 2) from casually-captured video (camera 0) for reconstruction and the complete 64 frames from another fixed-camera video (camera 1) for testing. E. Limitations and Failure Cases Although our method can handle various challenging scenarios, there are some limitations as shown in Fig. S1. Similar to many video diffusion models, our method occasionally struggles to render legible and correct text (Top two rows). Besides, our method relies on extracting 3D clues from videos. It struggles with data lacking 3D geometry, 2 Figure S2. Image to world. Starting from single view, NeoVerse can reconstruct 3D scene, generate an exploration video, and iteratively expand the visible area. Figure S3. Single-view to multi-view generation. Starting from single front-view video, NeoVerse can generate multi-view consistent videos. such as 2D cartoons. For instance, as the camera moves to the right side of 2D cartoon character (Bottom two rows), the model may fail to generate the correct 3D profile (e.g., revealing the other side of face), as the input video lacks inherent 3D structure. F. Additional Qualitative Results Image to world. Our NeoVerse allows for exploration in captured image by iteratively generating new views and reconstructing the scene. As illustrated in Fig. S2, given single starting image, we can generate spatially coherent video trajectory. This generated video is then used to reconstruct larger Gaussian Splatting scene, effectively outpainting the 3D world. Single-view to multi-view Fig. S3 demonstrates the capability of generating multi-view consistent videos from single-view video through iterative application of NeoVerse."
        }
    ],
    "affiliations": [
        "CreateAI",
        "NLPR & MAIS, CASIA"
    ]
}