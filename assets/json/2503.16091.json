{
    "paper_title": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event Forecasting for Treatment Adherence",
    "authors": [
        "Abdullah Mamun",
        "Diane J. Cook",
        "Hassan Ghasemzadeh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways to developing an on-demand intervention tool, enabling timely and personalized support. With the increasing popularity of smartphones and wearables, it is now easier than ever to develop and deploy smart activity monitoring systems. However, effective forecasting systems for treatment adherence based on wearable sensors are still not widely available. We close this gap by proposing Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI is a knowledge-guided adherence forecasting system that leverages smartphone sensors and previous medication history to estimate the likelihood of forgetting to take a prescribed medication. A user study was conducted with 27 participants who took daily medications to manage their cardiovascular diseases. We designed and developed CNN and LSTM-based forecasting models with various combinations of input features and found that LSTM models can forecast medication adherence with an accuracy of 0.932 and an F-1 score of 0.936. Moreover, through a series of ablation studies involving convolutional and recurrent neural network architectures, we demonstrate that leveraging known knowledge about future and personalized training enhances the accuracy of medication adherence forecasting. Code available: https://github.com/ab9mamun/AIMI."
        },
        {
            "title": "Start",
            "content": "AIMI: Leveraging Future Knowledge and Personalization in Sparse Event Forecasting for Treatment Adherence Abdullah Mamun,a,b, Diane J. Cookc, Hassan Ghasemzadehb aSchool of Computing and Augmented Intelligence, Arizona State University, Phoenix, AZ 85054, USA bCollege of Health Solutions, Arizona State University, Phoenix, AZ 85054, USA cSchool of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164, USA 5 2 0 2 0 ] . [ 1 1 9 0 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Adherence to prescribed treatments is crucial for individuals with chronic conditions to avoid costly or adverse health outcomes. For certain patient groups, intensive lifestyle interventions are vital for enhancing medication adherence. Accurate forecasting of treatment adherence can open pathways to developing an on-demand intervention tool, enabling timely and personalized support. With the increasing popularity of smartphones and wearables, it is now easier than ever to develop and deploy smart activity monitoring systems. However, effective forecasting systems for treatment adherence based on wearable sensors are still not widely available. We close this gap by proposing Adherence Forecasting and Intervention with Machine Intelligence (AIMI). AIMI is knowledge-guided adherence forecasting system that leverages smartphone sensors and previous medication history to estimate the likelihood of forgetting to take prescribed medication. user study was conducted with 27 participants who took daily medications to manage their cardiovascular diseases. We designed and developed CNN and LSTM-based forecasting models with various combinations of input features and found that LSTM models can forecast medication adherence with an accuracy of 0.932 and an F-1 score of 0.936. Moreover, through series of ablation studies involving convolutional and recurrent neural network architectures, we demonstrate that leveraging known knowledge about future and personalized training enhances the accuracy of medication adherence forecasting. Code available: https://github.com/ab9mamun/AIMI. Key words: Time-Series Forecasting, Machine Learning, Medication Adherence, Mobile Health 1. Introduction Approximately 6.7 million adults are affected by heart failure (HF) in the U.S. [40]. The number of American adults with HF has increased by 31.4% in the last 10 years [17]. It is deadly disease with 5-year mortality rate of 50% and 10-year excess mortality rate of 36-40% [52, 14]. Medication adherence is crucial for many HF patients as well as individuals suffering from hypertension, hyperlipidemia, and diabetes [47, 20, 15, 5]. Despite the risks, people often fail to adhere to the prescribed medications [45, 27]. Lifestyle interventions, such as smartphone apps and reminders, can improve medication adherence, but low-cost reminder solutions are not as effective as intensive interventions in increasing medication adherence [3, 26, 11, 26]. In this study, we aim to design an effective intervention system by accurately forecasting whether person is likely to miss their prescribed medication. Treatment adherence forecasting was addressed in different prior publications in different application domains. An adherence prediction system with Long Short-Term Memory (LSTM) networks was developed in [18] that can predict adherence to specific injection-based treatments with 77.35% accuracy. In that study, Internet-connected sharps bins were used to collect treatment events. Another study developed adherence forecasting among individuals with psychiatric disorders with an AUC of 0.87 [28]. Several publications from the past addressed the forecasting of health events such as hospital readmission, development of heart failure, and other adverse health outcomes [10, 41, 55]. Disease progression and prediction of health outcomes were previously addressed using Corresponding author. Email: a.mamun@asu.edu. electronic health records (EHR) [1]. LSTM-based models were proposed for forecasting daily step counts [36, 37]. [21] forecasts the medical service demand with hybrid model based on ARIMA and self-adaptive filtering method. Using future knowledge to improve prediction is relatively new idea, and prior work in this area is very limited [46]. Healthcare datasets collected in the wild face difficulties related to data noise, missing data, and natural human variance [39]. In this case, any additional available information should be considered for inclusion in the feature set. However, current literature does not provide enough evidence to draw reliable conclusion about the impact of factors such as time context, location, or future knowledgei.e., known future eventson persons treatment adherence. We hypothesize that medication adherence can be affected by persons temporal and behavioral context, such as the day of the week, location, and activity. Consider the situation where someone is in the middle of trip or went to bed later than usual, possibly because they are sleeping late the next morning. In these two cases, they may not take the medication at the prescribed time. Although adherence forecasting using past electronic health records has been explored in the past [29], forecasting such events using sensor or physical activity data or future knowledge features did not get sufficient attention. This research aims to bridge this gap by formulating sparse event forecasting problem and exploring contextaware and knowledge-guided deep learning solutions. We propose smart health system, AIMI(Adherence Forecasting and Intervention with Machine Intelligence), that considers not only the current activity and history of adherence but also the time-context and known information of future prescriptions. To evaluate our method, we conducted user study with N=27 participants who take medications to manage cardiovascular diseases. Then Convolutional Neural Networks (CNN) and LSTM models were trained and evaluated as potential forecasting models for the AIMI system. Furthermore, we include an incremental learning algorithm for training the system in resource-constrained training environment, e.g., limited memory. This incremental learning not only facilitates on-device learning in healthcare systems but also ensures the security of the data as it eliminates the need to transfer any data to third-party servers to train neural networks. In such training settings, the performance of neural networks is prone to worsen for specific instances after they are trained on new instances [9] which we overcome with personalization of the final model. In summary, our main contributions are: i) investigating the challenges of predicting and forecasting treatment adherence, ii) proposing the AIMI system for forecasting treatment adherence using sensor data, adherence history, and future knowledge features, iii) providing implementations of our proposed method using CNN and LSTM models and systematic comparison of the models for this task, iv) conducting user study and evaluating the AIMI on the collected dataset, v) proposing an incremental learning algorithm for resource-constrained environments, and vi) determining the impact of knowledge guidance and personalization of machine learning models for treatment adherence forecasting. 2. Background and Related Work 2.1. Treatment Adherence and Personalization Treatment adherence can be defined in few possible ways. One form of nonadherence is not taking the medication at the prescribed time. Whether person will adhere to the prescribed medications can be predicted with baseline questionnaires [44]. Another form of nonadherence can be violating medical temporal constraint (MTC) associated with the medicine, such as recommendation to take particular medication after meal or before meal. [50] predicts the violation of medical temporal constraint with behavioral prediction model, HERBERT, that utilizes personalized training process. Predicting or forecasting the health events of person differs from the same statistical variables that are affected by the actions of multiple entities of the world. generic classifier may achieve strong performance on one set of participants data but suffer for other participants [49]. That is why fine-tuning the generic models is necessary for some participants for such challenging problems [2]. Prior work [49] demonstrates that the classification accuracies for an individual in stress classification problem can be improved by margin of up to 39% after employing personalized training. 2.2. Forecasting with Deep Learning Time-series forecasting is complex task that has been around for considerable length of time [4]. In earlier work, language model-based forecaster predicted world events including climate and geopolitical conflicts [57]. 2 While transformers [54] have excelled in many different areas including natural language processing [13, 25], image and video data processing [7, 24], and language generation [6, 53, 12], they may not perform well for forecasting stationary time-series data. Non-stationary transformers can overcome this issue by re-incorporating nonstationarity on feed-forward layers or using De-stationary attention [35]. Recurrent networks have shown promise for event forecasting [8, 56]. Forecasting can be univariate or multivariate, depending on how many variables we want to jointly predict. The LSTNet model, combination of CNN, LSTM, and autoregressive layer-based neural network, has proven to be effective in multivariate forecasting [30]. However, the LSTNet method scales poorly because the input layers size depends on the number of variables (e.g. power generation in power plants) we want to forecast. There are important questions to answer in the area of forecasting with future knowledge regarding their impact and effective ways to incorporate them. Qi et al. [46] proposed knowledge-guided transformer to predict the demand for products of Alibaba. Li et al. [32] followed similar process to predict molecular properties of matter. Additionally, Liu et al. [34] provided knowledge-guided decoding for academic knowledge graph completion. 2.3. Classical Methods of Forecasting Before deep learning was established, various classical methods were proposed to solve natural and urban problems with forecasting. In particular, Gagliardi et al. [16] introduced Markov chain model to forecast short-term water demand in different areas. Another popular forecasting technique is imitation learning, where models are trained to mimic an experts decisions [42]. Lee and Tong [31] proposed forecasting algorithm based on ARIMA and genetic programming and validated their method using three different datasets. Often, deep recurrent models demonstrate superior performance over autoregressive models and linear regression models [37, 51] but in specific settings, ARIMA or seasonal ARIMA can still achieve better performance than LSTM or GRU models [33]. 2.4. Challenges of Time-Series Sensor Data Wearable sensor datasets in free-living environments are often accompanied by numerous challenges, such as low sampling rates, missing data or missing sensors, and noisy or inconsistent labels. Alternative imputation methods are available for recovering missing sensor data, such as CNN-based [38], GAN-based[23], clustering-based [22], etc. Another common challenge is the selection of channels. An efficient channel selection process was presented by [48]. One way to overcome the challenge of low sampling rate is by extracting statistical features [43]. 3. Materials and Methods 3.1. Sparse Event Forecasting with Future Knowledge Given dataset, = (S , E), containing wearable sensor readings, , and sparse event data, for particular participant, the goal is to forecast future values of E. The sensor and event data are collected over seconds. The sparse event data were sampled at much lower rate than the sensor data, i.e. νS >> νE, where ν represents the sampling frequency of variable. Therefore, for the same time interval, there will be more sensor data samples than sparse events. Assuming there are data points of sensor data and data points of event data within the duration of , let us denote those data points as 1..N = (S 1, 2, ..., N), and E1..T = (E1, E2, ..., ET ). Here, Ei represents the action of taking medication at time step i, and it can take value of either 0 or 1. To accomplish the forecasting task, we assume that the value of at time point + is correlated with the present and past values of and E. This can mean either i) the past and can partially influence the future values of E; or ii) the future value of is affected by an external factor that also affects the present and past values of and E. This forecasting problem can be written as the following equation if f1 is the forecasting function. = f1(S 1..N, E1..T ) Consider scenario in which we have additional knowledge about the future that is outside the dataset, D. An example of knowledge feature is the scheduled time for meeting or sales promotion. In the context of this paper, knowledge feature is the prescribed time to take medication. Let us consider one knowledge parameter, K. One (1) +1 ˆE f1 3 Figure 1: An overview of the AIMI system with the architecture of the LSTM-based forecasting model. The shapes of the data for batch size 32 and forecasting with sensor, event, and knowledge features are indicated in parentheses. distinct property of this knowledge parameter is that it can be known in advance for even future time steps. Let be the number of time units into the future (time + L) for which external knowledge is available at time . That means, at time , we can have the values for future knowledge features, KT +1..T +L. The knowledge parameters can potentially be added as an extra parameter to the forecasting function of Equation 1. Then, the next evolution of the forecaster would be as follows: ˆE f2 +1 = f2(S 1..N, E1..T , K1..T +L) (2) In Equation 2, we use knowledge parameters K1..T +L, among which only the values for the future time steps, i.e. KT +1,..,T +L will be considered future knowledge, whereas, K1..T will be considered present and past knowledge. The overall coverage of this paper will be to find out how future knowledge can improve the performance of forecaster when integrated into the feature set of neural network. Furthermore, this work also investigates i) whether the performance is negatively affected if the low-sampled feature set is removed from Equation 1, and ii) whether the performance drop can be recovered using future knowledge. These scenarios are presented in Equation 3 and Equation 4. ˆE T +1 = f3(S 1..N) ˆE f4 +1 = f4(S 1..N, KT +1..T +L) (3) (4) Note that in Equation 4, is not present in the feature set. However, only Ks future time steps, or KT +1..T +L, are used here. This formulation ignores past knowledge features as well. We implement this solution with machine learning model and evaluate the model on medication adherence dataset. Each forecasting function is combination of data processing and machine learning model, M. The sensor data, , and the event data, E, may not be in the usable shape for the model, M. Therefore, the forecasting function will process the data by cleaning it, creating features, addressing imbalances, and handling related issues. Data processing details are discussed in Section 3.3. The architecture of the LSTM model is displayed in Fig. 1, and readers are referred to for the details of the CNN model used for the experiments. 3.2. Dataset To validate the method described in Section 3.1, medication adherence dataset is chosen [44]. This dataset is derived from clinical study that was conducted with people who are at risk for atherosclerotic cardiovascular disease and take medications for their condition. The study was approved by an Institutional Review Board (IRB) and all participants provided informed consent. Study participants performed their regular activities while their smartphone sensors continuously recorded data that reflected their activities. The details of the data used for this study are described in Section 3.3 and Table 1. Medication events were collected from electronic pillboxes. The sensor data are sampled at 1Hz. For most participants, the medication events are collected at 1 event/day, though for some participants, these were collected at 2 events/day, as prescribed by their physicians. Initially, 27 participants were available to form the dataset for this paper. Among 4 them, data from 2 participants were removed due to unresolvable collection errors. From the remaining 25 participants, randomly 3 participants data are held out for future testing purposes. Thus, we used 22 participants data for training and testing the models used in the experiments. For each participant, the sensor data and the event data are saved in separate files. sensor data file has sensor data sampled at rate of 1Hz including yaw, pitch, roll, 3-axial acceleration, 3-axial rotation, location coordinates, altitude, horizontal and vertical accuracies, and speed features. However, there was 5-second pause after every 5 seconds of data collection. Therefore, 5 sensor readings were collected every 10 seconds. An event data file contains the medication event data for each day for person. If the person takes medication, it is recorded with the timestamp. If no record is found for day, it implies that the person did not take medication on that day. Moreover, the prescribed times are also present for those days. For some participants, more than one medication was prescribed for each day but for other participants, only one medication was prescribed. Therefore, usually, the medication events are sampled at 1-2 samples/day frequency. 3.3. Data Preprocessing To make these data usable for neural network training, the sensor data and the event data are first combined to create merged data file. This merged data file contains sensor data for each second, the previous medication event, and the next medication event. The timestamps of the previous and next medication events were included, as well as the previous and next prescribed times. The merged data file is then processed to add more features, such as contextual features (day of the week, hour of the day) and additional derived features. For example, whether any medication was taken in the last 2, 3, 6, 12, or 24 hours and whether any medication is taken the next hour. The medication event of the next hour is chosen as the target variable that we want our neural network model to forecast. To create training data points, sliding window was slid over the time series data with duration of 3600 seconds and 50% overlap. For each sensor data entry, the reading was recorded with both local and UTC timestamps. These were converted to relative timestamps by subtracting the current timestamp from the timestamp of the prescribed time for the next medication event. Relative timestamps were calculated using prescribed time, which is future knowledge. Hence, the relative timestamps were considered future knowledge features. In the dataset, sensor data were collected at 1Hz frequency with 5-second pause after every 5 seconds of data collection. In other words, 5 sensor readings were collected every 10 seconds on average. The length of the sliding windows was set to 1800 samples, which is 3600 seconds of data. The prescribed times are the same for patient across different days. Whether the person takes the medication, medication next hour, may depend on the next prescribed hour but not the other way around. Thus, these relative timestamps got the information regarding the future knowledge, prescribed time, and had to be chosen as future knowledge features. For the experiments conducted, the final feature set that was used is presented in Table 1. logical flow of the process is illustrated in Fig. 2. 3.4. Training Process During the initial experiments for model choosing and the first trial, the models were trained on an Intel(R) Core(TM) i7-7500 CPU with clock speed of 2.7 GHz and 16 GB RAM (random access memory). Then the LSTM experiments were repeated on computation node of supercomputer with 32 cores, 64 GB RAM, and an NVIDIA A100 GPU. The models were trained in an incremental manner to keep the training pipeline compatible with training environments with limited computing resources. We used total data from 22 participants to train the models. The largest data file of the Merged data with more features step, as shown in Fig. 2, for single participant was around 1.18 gigabytes (GB), whereas the total RAM of our CPU-based training machine was 16 GB. Hence, the training was done in 6 sessions to cover all 22 participants. At first, the 22 participant IDs were shuffled and then separated for 6 different sessions. Those sessions used 1, 4, 4, 4, 5, and 5 participants training data for training and the same 1, 4, 4, 4, 4, and 5 participants test data for testing, respectively. Each session saved the model weights after the training was complete and the next session (if any) loaded that weight before starting the training. Thus, after each training session, the cumulative number of participants training data that was used would be 1, 5, 9, 13, 17, and 22, respectively. This incremental training is further described in Algorithm 1. The whole process was repeated 3 times with three different random orders of the participants. 5 Figure 2: An illustration of the AIMI systems data processing method and separation of training and test data. 3.5. Evaluation Method In this work, training data and test data were separated in chronological order for each of the 22 participants, as shown in Fig. 2. Accuracy and macro average precision, recall, and F-1 scores were chosen for the evaluation of the performance. The training and test sets were divided before shuffling or balancing the data. ADASYN was used for balancing the data by the generation of synthetic data points near the existing data points of the minority class [19]. The training set and the test set were processed and balanced independently but through similar processes. This ensures that there is no data leakage between the training and the test set. Moreover, the feature sets were carefully considered so that any information related to the target forecast variable, medication event of the next hour, could not influence the feature variables. Algorithm 1: Incremental training process for large dataset on machine with limited memory. Find size for the data, that the machine can handle in one training session; Divide the training data, Dtrain into disjoint chunks, Chunk[1]...Chunk[n] so that Dtrain = Chunk[1] Chunk[2] ... Chunk[n] and ize(Chunk[i]) for 1 n; 1; while do Create and compile model M; if 2 then Load weights from file model weights to model M; end Train model with Chunk[i]; Save weights of model to file model weights; + 1; end 6 Table 1: Dataset features. Here, the Relative timestamp and Next prescribed hour features embed knowledge of the future. Medication next hour is the forecast variable. Type N=numeric, B=boolean. Type Dimension 14 3 3 3 2 3 39 1 1 1 7 24 Feature High-resolution features (H) Yaw, Pitch, Roll Rotation rate (x,y,z) Acceleration (x,y,z) Latitude, longitude Altitude, horizontal accuracy, speed Low-resolution features (L) Last medication event Last prescribed time Last event hour Day of the week Hour of the day Medication events of last 2, 3, 6, 12, and 24 hours Future knowledge features (K) Relative timestamp Next prescribed hour Medication next hour (target)"
        },
        {
            "title": "N\nB\nB",
            "content": "B 5 26 2 24 1 4. Results 4.1. Comparison Between CNN and LSTM The main goal of this study is to present the importance of using future knowledge in the feature set. To accomplish this, we test two candidate models and proceed with the one that performs better in the first round of experiments. We have tested our method with LSTM and CNN models. The initial analysis found that LSTM provided overall higher and more stable performance in forecasting. Moreover, the performance of CNN did not improve much with the increasing number of epochs. We decided that both CNN and LSTM models would be trained for approximately the same duration. The total duration of training was tuned by setting the number of epochs. It was observed that the LSTM model needed about 10 epochs to reach an accuracy and F-1 score close to 90% when trained on 22 participants data with all features. The designed CNN model had 189,972 parameters compared to the 651 parameters of the LSTM model. However, the CNN model trained faster per epoch. To train the CNN model for 10 epochs on the first participants data took around 151.88 seconds and that time was 573.58 seconds for the LSTM model. That suggests that the CNN model can be trained for around 38 epochs within time that can train the LSTM model for 10 epochs. Therefore, the CNN model was trained for 38 epochs. It took the CNN model 571.02 seconds to train on the first participants data with 38 epochs, and 330.54 seconds on average per participant to finish training on 22 participants data. Table 2: Compute time needed with CPU for training the LSTM models. = high-resolution, = low-resolution, and = future knowledge features. #Train users 22 22"
        },
        {
            "title": "Input",
            "content": "Feature dim. 14 H+K 40 H+L 53 H+L+K 79 #Param Epochs Train hours 1.89 1.93 2.40 2.47 139 347 451 659 10 10 10 10 It was found that the LSTM model provided better accuracies, precisions, recalls, and F-1 scores than the CNN model in the later training phases. The accuracies and F-1 scores of these two models are compared in Fig. 3. Each 7 (a) Accuracies (b) F-1 scores Figure 3: Comparison of the CNN and LSTM models accuracies and macro average F-1 scores in different training phases. The training phase labels, 1-1, 5-4, 9-4, 13-4, 17-4, and 22-5, indicate the numbers of training and test participants for those phases. LSTM model was trained for 10 epochs for every experimental session. The time taken to complete the training is presented in Table 2. Although efforts were made so that no other user-initiated heavy-duty applications were running while training the models, compute time can depend on other processes or background services that might be running on the machine. 4.2. Improving the Performance with Knowledge Features In order to find the importance of future knowledge, ablation studies on the features were performed. We train the models with few different sets of features and present their results in Table 3. From the experiments, it can be noticed that among all four different sets of features, H+L+K (high-sampled+low-sampled+knowledge) usually performs better as we increase the size of the training data. In Table 4, it is noticed that between the results for feature sets and H+K, H+K consistently gets higher precision, recall, and F-1 scores than alone. when both models were trained with 22 participants training data and tested on 6 participants test data. The situation of Equation 3 and Equation 4 is simulated in the rows with the and H+K features. In Table 3, we see that when the model is trained with 22 participants data, they get an F-1 score of 0.442 with only sensor data, but the F-1 score is 0.936 when future knowledge is added. This is an improvement of almost 112%. This implies that by adding only few knowledge parameters, well-designed neural network can possibly forecast medication event variable without even using the past values of that same variable. Another set of experiments was conducted to validate the hypothesis and similar findings were observed. In Table 4, H+K features provided an accuracy as high as 0.936 along with all three other metrics, precision, recall, and F-1 score values greater than or equal to 0.936. 4.3. Importance of Personalization The performance of the fully trained LSTM model with sensor, event, and knowledge features can be the last row of Table 3. By the word, fully, we mean that the model was trained on all 22 participants training data. The result reported in Table 3 is on the last 5 participants test data out of those 22 participants. Furthermore, this model was last trained at the 22-5 phase with Participants 18 to 22. In the fourth phase (10 to 13 phase), the participant IDs in the training data were 10 to 13. For specific trial, when fully-trained model is tested again on Participants 10 to 13, we noticed drop in the F-1 score from 0.936 to 0.683. This happened because the fourth phase model forgot the best parameter values for the participants when it was trained with different participants in the fifth and sixth phases. When this model was trained again on the training data of Participants 10 to 13, we saw that the F-1 score improved from 0.683 to 0.928. The result of this experiment is depicted in Fig. 4. However, when the same experiment was repeated for Participants 14 to 17, the results were not similar. The three F-1 scores for the trial of this experiment on Participants 14 to 17 after Phase 5, after Phase 6, and after retraining on the data of Participants 14 to 17 are 0.931, 0.914, and 0.897 respectively. Nonetheless, these results suggest that personalization should be considered for participant behavioral action forecasting problems. 8 Table 3: Ablation experiments for training in 6 iterations. Four LSTM models are independently trained for different sets of features: H= highresolution, L= low-resolution, and K= future knowledge features. The p-value for the effect of on F-1 scores is 0.00006 << standard threshold of 0.05. #Train users 1 1 1 1 5 5 5 5 9 9 9 9 13 13 13 13 17 17 17 17 22 22 22 22 #Test users 1 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 5 5"
        },
        {
            "title": "Precision Recall",
            "content": "F-"
        },
        {
            "title": "0.617\nH\nH+K\n0.869\nH+L\n0.510\nH+L+K 0.937\n0.549\nH\nH+K\n0.805\nH+L\n0.659\nH+L+K 0.912\n0.475\nH\nH+K\n0.909\nH+L\n0.782\nH+L+K 0.896\n0.494\nH\nH+K\n0.845\nH+L\n0.833\nH+L+K 0.919\n0.605\nH\nH+K\n0.906\nH+L\n0.834\nH+L+K 0.894\n0.516\nH\nH+K\n0.932\nH+L\n0.829\nH+L+K 0.898",
            "content": "0.189 0.385 0.015 0.405 0.447 0.847 0.652 0.895 0.321 0.871 0.824 0.848 0.166 0.878 0.787 0.888 0.665 0.855 0.794 0.849 0.455 0.888 0.798 0.867 0.521 0.562 0.333 0.600 0.328 0.719 0.820 0.936 0.441 0.961 0.715 0.970 0.333 0.801 0.921 0.958 0.659 0.977 0.899 0.959 0.526 0.989 0.903 0.946 0.237 0.449 0.028 0.469 0.375 0.733 0.705 0.913 0.361 0.914 0.765 0.904 0.222 0.838 0.847 0.922 0.616 0.912 0.842 0.901 0.442 0.936 0.837 0.903 (a) Accuracies (b) F-1 scores Figure 4: The performance of the model drops for the test data of Participants 10 to 13 when the model is trained with 9 more additional participants. When the model is trained with the training data of Participants 10 to 13 again, the performance recovers on the test data of those participants. 4.4. Forecasting with Location and Future Knowledge Features In Table 5, the forecasting accuracy and macro average precision, recall, and F-1 scores are presented for location and future knowledge features. The final two phases of test results suggest that location may be very important factor in forecasting adherence when combined with future knowledge. If we compare the result with the results of Table 3, we notice that when the model has access to all sensors, the performance improves very quickly as we feed more data to the model. When it has only the location features, it needs more data to learn the patterns and to achieve an F-1 score close to 0.90. 9 Table 4: Ablation experiments for training in 5 iterations. Participants for each phase are chosen differently than the experiments of Table 3. H= high-resolution, L= Low-resolution, and K= future knowledge features. The p-value for the effect of on F-1 scores is 0.001 << standard threshold of 0.05. Therefore, our results are statistically significant. #Train users 1 1 8 8 12 12 16 16 22 22 #Test users 1 1 7 7 4 4 4 4"
        },
        {
            "title": "Precision Recall",
            "content": "F-1 H+K H+K H+K H+K H+K 0.905 0.952 0.403 0.879 0.525 0.880 0.490 0.936 0.516 0.922 0.575 0.577 0.241 0.879 0.526 0.880 0.489 0.943 0.549 0.932 0.574 0.599 0.406 0.879 0.524 0.880 0.490 0.937 0.518 0.922 0.575 0.588 0.292 0.879 0.518 0.880 0.485 0.936 0.428 0. 5. Discussion 5.1. Clinical Significance and Novelty The clinical significance of this work lies in its potential to improve medication adherence in patients with heart failure (HF) by addressing critical and often overlooked aspect of disease management: the forecasting of adherence based on temporal and contextual factors. HF affects millions of people in the U.S., with high morbidity and mortality rates, yet non-adherence to prescribed medications remains common challenge that leads to poor outcomes. By leveraging temporal and behavioral data, including contextually relevant information such as time of day, location, and activities, this study seeks to identify patterns that may predict non-adherence, which can inform more effective, personalized interventions. This work closes this gap in adherence forecasting by introducing model that uses future contextual knowledge, which has not been thoroughly explored in current literature, to enhance prediction accuracy. Through the proposed CNN and LSTM-based approaches, this research provides methodological advancement by addressing the challenges of sparse event forecasting and evaluating model performance on real-world healthcare data. Ultimately, this research has the potential to contribute to the development of tailored intervention systems, enabling healthcare providers to proactively address adherence barriers, thus improving patient outcomes and reducing healthcare costs associated with heart failure. Table 5: Performance of forecasting with location features. Two LSTM models are independently trained for different sets of features: Loc = location, and K= future knowledge features. The p-value for the effect of on F-1 scores is 0.00013 << standard threshold of 0.05. #Train users 1 1 5 5 9 9 13 13 17 17 22 #Test users 1 1 4 4 4 4 4 4 4 4"
        },
        {
            "title": "Precision Recall",
            "content": "F-1 Loc Loc+K Loc Loc+K Loc Loc+K Loc Loc+K Loc Loc+K Loc Loc+K 0.500 0.543 0.501 0.756 0.500 0.889 0.503 0.906 0.502 0.876 0.501 0.925 10 0.181 0.329 0.168 0.578 0.166 0.872 0.336 0.858 0.168 0.847 0.334 0.870 0.667 1.000 0.333 0.602 0.333 0.911 0.667 0.978 0.333 0.921 0.667 1. 0.250 0.402 0.223 0.589 0.221 0.888 0.446 0.913 0.223 0.879 0.445 0.930 5.2. Summary of Our Contribution and Findings This paper highlights the importance of medication adherence in chronic cardiovascular conditions, reviewing prior work and providing an overview of time-series forecasting using deep learning and classical ML algorithms, along with key challenges and solutions. We designed forecasting system, AIMI, to solve the treatment adherence problem. To develop and evaluate the system in free-living settings, we conducted user study with individuals with cardiovascular diseases. We then trained CNN and LSTM models to complete the system development. Our LSTM-based models achieve an accuracy of 0.932 and an F-1 score of 0.936 on the test set. We found that the location and future knowledge features play an important role in improving performance. Moreover, in freeliving settings, personalization of the trained models is also an important step to overcome the forecasting systems parameter forgetting challenge. When the model is trained with only one users data, the data distribution it has seen is very small and the models learning is somewhat random. Adding more data to the training set improves the performance. Moreover, when more features are added to the feature set, it requires more data to train the model properly. If we look at the results of the last few rows of Table 3 when the model is fully-trained, we see that the results are improving with the inclusion of features. 5.3. Limitations and Future Works The experiments of this paper show the importance of using future knowledge in the feature set for time-series forecasting. Nonetheless, the study has few limitations. First of all, the dataset is highly skewed towards one class label. Medication events are sparse events and adherence varies highly across individuals. The number of representative samples for both classes in the eight-week study could not be ensured. Moreover, as the problem addresses hourly forecasting and the medication event usually takes place only once day, the negative samples outnumber the positive samples by factor of almost 24 in some cases. For this reason, the test set was balanced for justified evaluation of the forecasting models. future direction of this work is the deployment of the forecasting model in smartphone application where the model will track the sensor data and recent medication events and forecast whether person may miss their medication. The smartphone app can then notify the user if the model forecasts they might miss their medication. clinical trial may be performed to measure the efficacy of the system. 6. Conclusion Forecasting sparse events is challenging task, especially when the data is collected in an uncontrolled environment. This particular area is hardly explored in deep learning research. Moreover, the use of future knowledge is relatively new concept that has not been applied in sparse events forecasting to the best of our knowledge. This paper aimed to contribute to this gap by evaluating CNN and LSTM models using high-sampled and low-sampled features, with and without future knowledge. The accuracies, and macro average precision, recall, and F-1 scores of the knowledge-guided models tend to outperform the knowledge-free models after the model is trained with certain size of data. When only high-sampled features and future knowledge data are available, the improvement in the F-1 score using future knowledge improves by almost 112%. This paper also addresses the importance of personalization in behavioral sparse event forecasting. It has been shown that model may forget the best weight parameters for particular participants as it keeps training on additional participants data. Hence, retraining with the target participants data is important before making predictions in deployed system. We invite more researchers to join this area so that the still unanswered questions can be answered with substantial theoretical and empirical evidence. We will share public version of the medication adherence dataset in the near future after removing sensitive information."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the National Institutes of Health under grant R21NR015410 and by the National Science Foundation under grant CNS-2227002. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding organizations."
        },
        {
            "title": "References",
            "content": "[1] Alaa, A.M., van der Schaar, M., 2019. Attentive state-space modeling of disease progression. Advances in neural information processing systems 32. [2] Azghan, R.R., Glodosky, N.C., Sah, R.K., Cuttler, C., McLaughlin, R., Cleveland, M.J., Ghasemzadeh, H., 2023. Personalized modeling and detection of moments of cannabis use in free-living environments, in: 2023 IEEE 19th International Conference on Body Sensor Networks (BSN), IEEE. pp. 14. [3] de Barros, K.A.A.L., da Silva Praxedes, M.F., Ribeiro, A.L.P., Martins, M.A.P., 2023. Effect and usability of mobile health applications for medication adherence in patients with heart failure: systematic review. International Journal of Medical Informatics , 105206. [4] Benidis, K., Rangapuram, S.S., Flunkert, V., Wang, Y., Maddix, D., Turkmen, C., Gasthaus, J., Bohlke-Schneider, M., Salinas, D., Stella, L., et al., 2022. Deep learning for time series forecasting: Tutorial and literature survey. ACM Computing Surveys 55, 136. [5] Brown, M.T., Bussell, J.K., 2011. Medication adherence: Who cares?, in: Mayo clinic proceedings, Elsevier. pp. 304314. [6] Brown, T.B., 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 . [7] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., Joulin, A., 2021. Emerging properties in self-supervised vision transformers, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660. [8] Chaudhary, A., Mishra, R., Gupta, H.P., Shukla, K.K., 2021. Jointly prediction of activities, locations, and starting times for isolated elderly people. IEEE Journal of Biomedical and Health Informatics 27, 22882295. [9] Chen, H.J., Cheng, A.C., Juan, D.C., Wei, W., Sun, M., 2020. Mitigating forgetting in online continual learning via instance-aware parameterization. Advances in Neural Information Processing Systems 33, 1746617477. [10] Choi, E., Schuetz, A., Stewart, W.F., Sun, J., 2017. Using recurrent neural network models for early detection of heart failure onset. Journal of the American Medical Informatics Association 24, 361370. [11] Choudhry, N.K., Krumme, A.A., Ercole, P.M., Girdish, C., Tong, A.Y., Khan, N.F., Brennan, T.A., Matlin, O.S., Shrank, W.H., Franklin, J.M., 2017. Effect of reminder devices on medication adherence: the remind randomized clinical trial. JAMA internal medicine 177, 624631. [12] Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.W., Sutton, C., Gehrmann, S., et al., 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research 24, 1113. [13] Devlin, J., Chang, M.W., Lee, K., Toutanova, K., 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . [14] Drozd, M., Relton, S.D., Walker, A.M., Slater, T.A., Gierula, J., Paton, M.F., Lowry, J., Straw, S., Koshy, A., McGinlay, M., et al., 2021. Association of heart failure and its comorbidities with loss of life expectancy. Heart 107, 14171421. [15] Fitzgerald, A.A., Powers, J.D., Ho, P.M., Maddox, T.M., Peterson, P.N., Allen, L.A., Masoudi, F.A., Magid, D.J., Havranek, E.P., 2011. Impact of medication nonadherence on hospitalizations and mortality in heart failure. Journal of cardiac failure 17, 664669. [16] Gagliardi, F., Alvisi, S., Kapelan, Z., Franchini, M., 2017. probabilistic short-term water demand forecasting model based on the markov chain. Water 9, 507. [17] Go, A.S., Mozaffarian, D., Roger, V.L., Benjamin, E.J., Berry, J.D., Blaha, M.J., Dai, S., Ford, E.S., Fox, C.S., Franco, S., et al., 2014. Heart disease and stroke statistics2014 update: report from the american heart association. circulation 129, e28e292. [18] Gu, Y., Zalkikar, A., Liu, M., Kelly, L., Hall, A., Daly, K., Ward, T., 2021. Predicting medication adherence using ensemble learning and deep learning models with large scale healthcare data. Scientific Reports 11, 18961. [19] He, H., Bai, Y., Garcia, E.A., Li, S., 2008. Adasyn: Adaptive synthetic sampling approach for imbalanced learning, in: 2008 IEEE international joint conference on neural networks (IEEE world congress on computational intelligence), IEEE. pp. 13221328. [20] Hood, S.R., Giazzon, A.J., Seamon, G., Lane, K.A., Wang, J., Eckert, G.J., Tu, W., Murray, M.D., 2018. Association between medication adherence and the outcomes of heart failure. Pharmacotherapy: The Journal of Human Pharmacology and Drug Therapy 38, 539545. [21] Huang, Y., Xu, C., Ji, M., Xiang, W., He, D., 2020. Medical service demand forecasting using hybrid model based on arima and self-adaptive filtering method. BMC Medical Informatics and Decision Making 20, 114. [22] Hussein, D., Belkhouja, T., Bhat, G., Doppa, J., 2024. Sensor-aware data imputation for time-series machine learning on low-power wearable devices. ACM Transactions on Design Automation of Electronic Systems URL: http://dx.doi.org/10.1145/3698195, doi:10.1145/3698195. [23] Hussein, D., Bhat, G., 2024. Sensorgan: novel data recovery approach for wearable human activity recognition. ACM Transactions on Embedded Computing Systems 23, 128. [24] Islam, M.M., Bertasius, G., 2022. Long movie clip classification with state-space video models, in: European Conference on Computer Vision, Springer. pp. 87104. [25] Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al., 2023. Mistral 7b. arXiv preprint arXiv:2310.06825 . [26] Kini, V., Ho, P.M., 2018. Interventions to improve medication adherence: review. Jama 320, 24612473. [27] Kleinsinger, F., 2018. The unmet challenge of medication nonadherence. The Permanente Journal 22. [28] Koesmahargyo, V., Abbas, A., Zhang, L., Guan, L., Feng, S., Yadav, V., Galatzer-Levy, I.R., 2020. Accuracy of machine learning-based prediction of medication adherence in clinical research. Psychiatry research 294, 113558. [29] Kumamaru, H., Lee, M.P., Choudhry, N.K., Dong, Y.H., Krumme, A.A., Khan, N., Brill, G., Kohsaka, S., Miyata, H., Schneeweiss, S., et al., 2018. Using previous medication adherence to predict future adherence. Journal of managed care & specialty pharmacy 24, 11461155. [30] Lai, G., Chang, W.C., Yang, Y., Liu, H., 2018. Modeling long-and short-term temporal patterns with deep neural networks, in: The 41st international ACM SIGIR conference on research & development in information retrieval, pp. 95104. [31] Lee, Y.S., Tong, L.I., 2011. Forecasting time series using methodology based on autoregressive integrated moving average and genetic programming. Knowledge-Based Systems 24, 6672. [32] Li, H., Zhao, D., Zeng, J., 2022. Kpgt: Knowledge-guided pre-training of graph transformer for molecular property prediction. arXiv preprint arXiv:2206.03364 . 12 [33] Liu, X., Lin, Z., Feng, Z., 2021. Short-term offshore wind speed forecast by seasonal arima-a comparison against gru and lstm. Energy 227, 120492. [34] Liu, X., Mao, S., Wang, X., Bu, J., 2023. Generative transformer with knowledge-guided decoding for academic knowledge graph completion. Mathematics 11, 1073. [35] Liu, Y., Wu, H., Wang, J., Long, M., 2022. Non-stationary transformers: Exploring the stationarity in time series forecasting, in: Advances in Neural Information Processing Systems. [36] Mamun, A., Leonard, K.S., Buman, M.P., Ghasemzadeh, H., 2022a. Multimodal time-series activity forecasting for adaptive lifestyle intervention design, in: 2022 IEEE-EMBS International Conference on Wearable and Implantable Body Sensor Networks (BSN), IEEE. pp. 14. [37] Mamun, A., Leonard, K.S., Petrov, M.E., Buman, M.P., Ghasemzadeh, H., 2024. Multimodal physical activity forecasting in free-living clinical settings: Hunting opportunities for just-in-time interventions. arXiv preprint arXiv:2410.09643 . [38] Mamun, A., Mirzadeh, S.I., Ghasemzadeh, H., 2022b. Designing deep neural networks robust to sensor failure in mobile health environments, in: 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), IEEE. pp. 24422446. [39] Marino, M., Lucas, J., Latour, E., Heintzman, J.D., 2021. Missing data in primary care research: importance, implications and approaches. Family Practice 38, 199202. [40] Martin, S.S., Aday, A.W., Almarzooq, Z.I., Anderson, C.A., Arora, P., Avery, C.L., Baker-Smith, C.M., Barone Gibbs, B., Beaton, A.Z., Boehme, A.K., et al., 2024. 2024 heart disease and stroke statistics: report of us and global data from the american heart association. Circulation 149, e347e913. [41] Michailidis, P., Dimitriadou, A., Papadimitriou, T., Gogas, P., 2022. Forecasting hospital readmissions with machine learning, in: Healthcare, MDPI. p. 981. [42] Minor, B., Doppa, J.R., Cook, D.J., 2015. Data-driven activity prediction: Algorithms, evaluation methodology, and applications, in: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 805814. [43] Mirzadeh, S.I., Ardo, J., Fallahzadeh, R., Minor, B., Evangelista, L., Cook, D., Ghasemzadeh, H., 2019. Labelmerger: Learning activities in uncontrolled environments, in: 2019 First International Conference on Transdisciplinary AI (TransAI), IEEE. pp. 6467. [44] Mirzadeh, S.I., Arefeen, A., Ardo, J., Fallahzadeh, R., Minor, B., Lee, J.A., Hildebrand, J.A., Cook, D., Ghasemzadeh, H., Evangelista, L.S., 2022. Use of machine learning to predict medication adherence in individuals at risk for atherosclerotic cardiovascular disease. Smart Health 26, 100328. [45] Mykyta, L., Cohen, R.A., 2023. Characteristics of adults aged 1864 who did not take medication as prescribed to reduce costs: United states, 2021 . [46] Qi, X., Hou, K., Liu, T., Yu, Z., Hu, S., Ou, W., 2021. From known to unknown: Knowledge-guided transformer for time-series sales forecasting in alibaba. arXiv preprint arXiv:2109.08381 . [47] Qin, X., Hung, J., Knuiman, M.W., Briffa, T.G., Teng, T.H.K., Sanfilippo, F.M., 2023. Evidence-based medication adherence among seniors in the first year after heart failure hospitalisation and subsequent long-term outcomes: restricted cubic spline analysis of adherence-outcome relationships. European Journal of Clinical Pharmacology 79, 553567. [48] Sah, R.K., Cleveland, M.J., Ghasemzadeh, H., 2023. Stress monitoring in free-living environments. IEEE Journal of Biomedical and Health Informatics . [49] Sah, R.K., Cleveland, M.J., Habibi, A., Ghasemzadeh, H., 2022. Stressalyzer: Convolutional neural network framework for personalized stress classification, in: 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), IEEE. pp. 46584663. [50] Seegmiller, P., Gatto, J., Mamun, A., Ghasemzadeh, H., Cook, D., Stankovic, J., Preum, S.M., 2023. Actsafe: Predicting violations of medical temporal constraints for medication adherence. arXiv preprint arXiv:2301.07051 . [51] Siami-Namini, S., Tavakoli, N., Namin, A.S., 2018. comparison of arima and lstm in forecasting time series, in: 2018 17th IEEE international conference on machine learning and applications (ICMLA), IEEE. pp. 13941401. [52] Taylor, C.J., Ordonez-Mena, J.M., Roalfe, A.K., Lay-Flurrie, S., Jones, N.R., Marshall, T., Hobbs, F.R., 2019. Trends in survival after diagnosis of heart failure in the united kingdom 2000-2017: population based cohort study. bmj 364. [53] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al., 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 . [54] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017. Attention is all you need. Advances in neural information processing systems 30. [55] Vieira, A., Sousa, I., Doria-Nobrega, S., 2023. Forecasting daily admissions to an emergency department considering single and multiple seasonal patterns. Healthcare Analytics 3, 100146. [56] Zhang, S., Qian, Y., Wang, T., Zhang, J., Qin, X., 2024. Forecasting events within temporal intervals using first occurrence distributions, in: 2024 International Joint Conference on Neural Networks (IJCNN), IEEE. pp. 110. [57] Zou, A., Xiao, T., Jia, R., Kwon, J., Mazeika, M., Li, R., Song, D., Steinhardt, J., Evans, O., Hendrycks, D., 2022. Forecasting future world events with neural networks. Advances in Neural Information Processing Systems 35, 2729327305. 13 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 A. Architecture of the CNN model Although the code for the full pipeline is shared separately, here we share the code for the CNN model for convenience. The CNN model used in the main paper contained series of convolutional layers followed by flatten layer and dense layer. from tensorflow.keras.models import Model from tensorflow.keras.layers import Flatten, Dense from tensorflow.keras.layers import Conv2D from tensorflow.keras.layers import Reshape, Input, Add, Activation from tensorflow.keras import activations def create_model_cnn(window_size, feature_size): input_frame = Input(shape=(window_size, feature_size, 1)) = Conv2D(10, (60, 1), strides=(30,1), activation=relu)(input_frame) = Conv2D(10, (1, 1), strides=(1,1), activation=relu)(x) = Conv2D(10, (1, 1), strides=(1,1), activation=relu)(x) = Conv2D(10, (1, 1), strides=(1,1), activation=relu)(x) = Conv2D(10, (1, 1), strides=(1,1), activation=relu)(x) = Conv2D(10, (1, 1), strides=(1,1), activation=relu)(x) = Flatten()(x) = Dense(1, activation=relu)(x) x2 = Reshape((window_size*feature_size*1,))(input_frame) x2 = Dense(1)(x2) = Add()([x, x2]) output = Activation(activations.sigmoid)(z) return Model(input_frame, output) The model also included skip connection from the input feature to the last hidden layer through dense layer in between. logical diagram of the architecture of the CNN model that uses all high-resolution, low-resolution, and knowledge features is presented in Fig. 5. The CNN models for other feature sets have similar architecture except for the input shape of the first layer, which eventually affects the output and input shape of most layers. 14 Figure 5: The architecture of the CNN model."
        }
    ],
    "affiliations": [
        "College of Health Solutions, Arizona State University, Phoenix, AZ 85054, USA",
        "School of Computing and Augmented Intelligence, Arizona State University, Phoenix, AZ 85054, USA",
        "School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA 99164, USA"
    ]
}