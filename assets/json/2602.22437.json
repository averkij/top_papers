{
    "paper_title": "veScale-FSDP: Flexible and High-Performance FSDP at Scale",
    "authors": [
        "Zezhou Wang",
        "Youjie Li",
        "Zhiqi Lin",
        "Jiacheng Yang",
        "Cong Xie",
        "Guanyu Feng",
        "Zheng Zhong",
        "Ziyue Huang",
        "Hongyu Zhu",
        "Zhi Zhang",
        "Yanghua Peng",
        "Xin Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDP's fixed element- or row-wise sharding formats conflict with the block-structured computations. In addition, today's implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, a redesigned FSDP system that couples a flexible sharding format, RaggedShard, with a structure-aware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As a result, veScale-FSDP achieves 5~66% higher throughput and 16~30% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 5 2 ] . [ 1 7 3 4 2 2 . 2 0 6 2 : r veScale-FSDP: Flexible and High-Performance FSDP at Scale Zezhou Wang, Youjie Li,, Zhiqi Lin, Jiacheng Yang, Cong Xie, Guanyu Feng, Zheng Zhong, Ziyue Huang, Hongyu Zhu, Zhi Zhang, Yanghua Peng, Xin Liu"
        },
        {
            "title": "ByteDance Seed",
            "content": "Equal Contribution, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "Fully Sharded Data Parallel (FSDP), also known as ZeRO, is widely used for training large-scale models, featuring its flexibility and minimal intrusion on model code. However, current FSDP systems struggle with structure-aware training methods (e.g., block-wise quantized training) and with non-element-wise optimizers (e.g., Shampoo and Muon) used in cutting-edge models (e.g., Gemini, Kimi K2). FSDPs fixed elementor row-wise sharding formats conflict with the blockstructured computations. In addition, todays implementations fall short in communication and memory efficiency, limiting scaling to tens of thousands of GPUs. We introduce veScale-FSDP, redesigned FSDP system that couples flexible sharding format, RaggedShard, with structureaware planning algorithm to deliver both flexibility and performance at scale. veScale-FSDP natively supports efficient data placement required by FSDP, empowering block-wise quantization and non-element-wise optimizers. As result, veScale-FSDP achieves 566% higher throughput and 1630% lower memory usage than existing FSDP systems, while scaling efficiently to tens of thousands of GPUs. Date: February 27, 2026 Correspondence: youjie.li@bytedance.com and pengyanghua.yanghua@bytedance.com Project Page: https://github.com/volcengine/veScale"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have become transformative technology in everyday applications. Driven by the scaling law [11], LLMs now reach billions of parameters and achieve human-level performance. Training such giant models requires parallelization techniques that distribute the model and optimizer states across thousands of GPUs [9]. Among these, Deepspeed ZeRO [25] or Fully Sharded Data Parallel (FSDP) [17, 20, 34] is one of the most fundamental techniques. FSDP is often the first choice because of its efficient yet flexible data-parallel programming paradigm and decoupling from model architecture. When additional scaling is needed [16, 27], FSDP can be combined with other parallelisms. However, existing FSDP/ZeRO systems struggle with modern structure-aware training. State-of-the-art models use non-element-wise optimizers such as Shampoo [7] and Muon [10], and block-wise quantized training like DeepSeek-V3 [15], all of which require atomic tensor blocks. The core limitation is that existing FSDP frameworks shard parameters, gradients, and optimizer states either element-wise [25, 34] or rowwise [17, 20], producing sharding boundaries that often misalign with the required block sizes. Consequently, 1 model developers must intrusively modify their models or optimizers to match tensor boundaries, or system developers must handle complex boundary checks, padding, and additional communication logic. Beyond inflexibility, current FSDP systems fall short of our production throughput and memory targets, where we aim to extract every bit of hardware efficiency. GPU Memory is the tighter constraint: in shared clusters, jobs run out of memory or will operate at the memory limit incurring expensive device-side frees, prompting over-provisioning that leaves GPU resources wasted. These demands become even more critical when scaling training to over 10K GPUs and trillions of parameters. Few existing FSDP systems can scale to this level while maintaining efficiency. Deepspeed ZeRO [25] pioneered the FSDP research but suffers from fragmented AllGather operations [8] and inefficient memory management [33]. PyTorch FSDP1 [34] addresses some AllGather inefficiency, but incurs slow ReduceScatter [35] and does not solve memory overhead [33]. PyTorch FSDP2 [20] improves memory management [6] but introduces high tensor copy overhead. Meanwhile, both FSDP1 and FSDP2 suffer from slow collectives due to unaligned communication buffer [18, 32]. MegatronFSDP [17] further improves performance but requires extra padding, increasing both communication and memory costs. To this end, we reinvent PyTorch FSDP2 and present veScale-FSDP, combining both flexibility and performance at scale: For flexibility, veScale-FSDP introduces novel sharding format, RaggedShard, which supports arbitrary sharding granularity with custom block sizes for structure-aware training, while seamlessly composing with existing PyTorch DTensor sharding formats. For performance, veScale-FSDP introduces planning algorithm that rearranges RaggedShard tensors to maximize communication efficiency while respecting their desired sharding granularities. We formulate planning as NP-hard optimization problem and use practical polynomial-time heuristics that achieve high-quality solutions in practice. veScale-FSDP further provides high-performance primitive, Distributed Buffer (DBuffer), that backs RaggedShard tensors with slices of global buffer, not only enabling zero-copy access and minimal communication overhead but also reducing memory fragmentation via batched memory allocations. Our extensive evaluations demonstrate that veScale-FSDP outperforms all existing FSDP systems on both dense and sparse LLMs across different scales, achieving 566% higher throughput and 1630% lower memory usage, while scaling efficiently to tens of thousands of GPUs. In addition, case studies show that veScale-FSDP is able to natively accommodate both non-element-wise optimizers like Muon [10] and block-wise quantization methods like 8-bit Adam [2]. veScale-FSDP has been battle-tested in production and is portable without relying on internal infrastructure. RaggedShard code is open sourced at https://github.com/volcengine/veScale."
        },
        {
            "title": "2.1 Structure–Aware Training",
            "content": "Structureaware training is the core technique behind the top-tier models such as Gemini [28] and DeepSeek [15], and becomes increasingly important, including: Matrix Optimizers. Matrix-based optimizers such as Shampoo [7] and Muon [10] can deliver faster convergence rate. The calculation is conducted on the matrix with the original 2D shape, requiring the full matrix to be present locally on each device and then be computed only on chosen devices. Block-wise Quantization. Training with quantized model [15] and optimizer states [2] is widely used to improve system efficiency. Block-wise quantization is one of the prevailing techniques to preserve both quality and efficiency of training, but requires slicing the tensors into 2D blocks for the calculation of the scaling factors. Sharding parameters without care would end up in sharded blocks across devices, incurring high complexity in either model design or system development. 2 Figure 1 The DTensor for flexible communication and computation. Here shows an example of DTensors executing sharded matrix multiplication on device. The darken part in each DTensor indicates the materialized local tensor on this device. Table 1 Interleaved copy time (ms) compared to their corresponding collectives in FSDP2 for GPT-OSS-120B on 64 GPUs. Shard(0) is the default parameter sharding mode and Shard(1) is used when Shard(0) incurs large padding. Gather side Reduce side AllGather Copy-Out ReduceScatter Copy-In Shard(0) Shard(1) 43.71 ms 44.35 ms 5.22 ms 13.72 ms 94.24 ms 95.36 ms 12.37 ms 23.14 ms"
        },
        {
            "title": "2.2 DTensor and JaggedTensor\nDistributed Tensor (DTensor) [14, 30] is a promising primitive of PyTorch that provides the opportunity\ntowards structure–aware training. It represents a global tensor distributed across devices, where each device\nholds a local sharded tensor. DTensor supports three sharding formats (placement): Shard(dim) that evenly\nshards a global tensor along a tensor dimension, Replicate that replicates the global tensor, and Partial. It\nalso enables users to switch between these placement via redistribute with implicit collective communications.\nAdditionally, DTensors can be computed directly by operators like matmul, as shown in Figure 1. However, a\nfundamental limitation prevents structure-aware training: the Shard format cannot represent the block-wise\nsharding needed for quantization or the uneven sharding required by matrix optimizers.",
            "content": "JaggedTensor/NestTensor on single device [21, 22, 29] are PyTorch/TensorFlow primitives that represent single-device tensors whose last dimension is jagged. For example, 2D tensor in which each row may have different length. While these primitives still fail to represent the block-level granularity as the atomic unit, they offer useful hint for how veScale-FSDP can support structureawareness in distributed training setting."
        },
        {
            "title": "2.3 ZeRO and FSDPs\nDeepSpeed ZeRO [25] pioneered this line of FSDP research. Its core idea is to concatenate a layer of\ntensors (parameters, gradients, and optimizer states) and then shard each concatenated tensors across devices,\nwhere tensors can be irregularly sharded across device boundary. ZeRO only unshards a layer using AllGather\nbefore forward and backward pass, and reduces the layer gradients using ReduceScatter back to different\ndevices. Such sharding design is limited in element-wise plain tensor and cannot support structure-aware\ntraining.",
            "content": "FullyShardedDataParallel (FSDP1) [34] is the first PyTorch-native ZeRO, following the same sharding format and limitation, but it is optimized in performance. fully_shard (FSDP2) is the second PyTorch-native ZeRO, representing the state-of-the-art FSDP in the community. It replaces the concatenated shard design with per-parameter sharding, representing each tensor as Shard(0) DTensor. This exposes maximal DTensor flexibility for FSDP parameters in communication, computation, and model checkpointing. However, this even sharding format is still far from enabling structureaware training. Moreover, this per-parameter design introduces performance overhead from copying parameters in interleaved memory addresses, as shown in Figure 2 and Table 1. 3 Figure 2 The fundamental overhead in FSDP2. (AllGather is shown; ReduceScatter is reverse process.) Figure 3 veScale-FSDP overview. Megatron-FSDP [17] is the most recent FSDP prototype that pursues speed. It forgoes FSDP2s design and rolls back to FSDP1s concatenated sharding to avoid the copying overhead, while heavily optimizing performance. However, Megatron-FSDP develops special mechanism to enforce concatenation-sharded tensor become Shard(0) DTensor, such that the model checkpointing can use DTensor. This mechanism inserts padding into the concatenation so that tensors are sharded row-wise along device boundaries rather than element-wise. Without careful padding planning, the concatenation size can grow significantly, increasing both memory usage and communication volume. Moreover, row-wise sharding still falls short of supporting structure-aware training."
        },
        {
            "title": "3 Overview",
            "content": "To address both challenges of flexibility and performance, we present veScale-FSDP, novel FSDP that combines the best of worlds. Figure 3 provides the overview. Model developers are given the freedom to develop sophisticated large models (e.g., with sparse MoE structures) and structure-aware optimizers (e.g., with non-element-wise operators) for achieving unprecedented model quality. Meanwhile, the model/optimizer 4 Figure 4 Flexibility comparison of different sharding formats. can be simply parallelized with PyTorch-native API fully_shard like FSDP2, without intrusively hacking model/optimizer code. During parallelization, complex operators of models/optimizers can still enjoy singledevice semantics, thanks to proposed sharding format, dubbed RaggedShard that offers the flexibility to express arbitrary sharding granularity and arbitrary distribution across devices for each DTensor (4). Under the hood, RaggedShard DTensors are grouped for bucketed communication. Toward optimal performance, their layouts are rearranged via proposed planning algorithm which is derived from NP-hard optimization problem. The planned layouts are then mapped to Distributed Buffer (DBuffer), new primitive that achieves zero-copy and minimal overhead (5), enabling efficient scaling up to 10K GPUs in real production deployments."
        },
        {
            "title": "4 RaggedShard for Flexibility",
            "content": "This section proposes novel and general sharding format RaggedShard to enable flexibility of FSDP for complex model and structure-aware optimizers. Existing sharding formats. The second format is the Row-wise (Even) Shard, where tensor is evenly partitioned along dimension, with equal-sized shards assigned to each device. This design improves flexibility by enabling non-element-wise computations on sharded tensors and allowing dimension redistribution via All2All collectives. However, it still faces challenges with block-wise quantization, as evenly divided shards are not guaranteed to align with block boundaries. This row-wise sharding format serves as the foundation of FSDP2. Inspired by the JaggedTensor/NestedTensor on single device [21, 22, 29], we The RaggedShard format. propose RaggedShard format for DTensor to offer the flexibility to express arbitrary sharding granularity in contiguous memory (the atomic non-shardable block consisting of contiguous elements or rows or planes) and arbitrary sharding distribution (the number of blocks per device). For simple example in Figure 4, setting RaggedShards granularity as one tensor row gives Row-wise RaggedShard with different number of rows across devices. similar concept has been prototyped in the model checkpointing mechanism of Megatron-FSDP. The most flexible sharding format is the Block-wise RaggedShard, where the sharding granularity is defined as tensor block with customizable shape. For example, tensor may be partitioned into three 2D blocks, with one block placed on device 0 and two blocks on device 1 (see Figure 4). This format not only supports non-element-wise computation and efficient redistribution but also enables block-wise quantization with perfect alignment between quantization blocks and shard boundaries. In fact, the block-wise RaggedShard generalizes 5 Figure 5 Composability of RaggedShard with existing even Shard for 2D parallelism like FSDPEP (Expert Parallel). Figure 6 Grouped communication of RaggedShard DTensors. all previous sharding formats through different choices of block size. Composing with existing sharding formats. DTensor has been widely used to express tensor partitions in parallelization strategies such as tensor parallelism (TP) [26] and expert parallelism (EP) [12]. It allows tensors to be represented using replicated, partial-value, or evenly sharded placements along selected dimension. RaggedShard extends this capability as an additional DTensor placement. To support combinations of multiple parallelization strategies, RaggedShard needs to compose cleanly with existing DTensor placements. RaggedShard is orthogonal to both replicated and partial-value placements and veScale-FSDP specially handles the Shard placement. In practice, TP uses Shard(0) and Shard(1) for columnand row-wise tensor parallelism; EP can be encoded as Shard(0) along the expert dimension. By convention, EP/TP is applied before FSDP. In PyTorch, however, the DTensor placement list is organized in the opposite order of conceptual application (see Figure 5): tensor shown with placements (RaggedShard, Shard(0)) is partitioned as Shard(0) followed by RaggedShard. veScale-FSDP reconciles this by: (i) for Shard(0), introducing dedicated placement StridedRaggedShard that carries reordering/stride metadata and performs Meanwhile, RaggedShard, as an extended DTensor placement, can offers the checkpointing capability by directly reusing DTensor-based checkpointing stacks (e.g., PyTorch Distributed Checkpoint [23]) for failure recovery and also inheriting their optimizations such as communication-free sharded checkpointing."
        },
        {
            "title": "5 Grouped RaggedShard for Performance",
            "content": "This section discusses how to group RaggedShard DTensors for efficient communication, as well as its NPhardness for optimality and the underlying optimization. Challenges for efficient communication. As well known in the systems community, collective communication relies on tensor bucketing or grouping to maximize network utilization [13, 34]. The same applies to RaggedShard DTensors in FSDP. However, efficiently grouping RaggedShard tensors is non-trivial and naively approaches can lead to significant inefficiencies. Figure 6(a) illustrates three inefficient factors: i) Sharded block that can happen when tensors are just concatenated back-to-back and put into communication buffer, without realizing that sharding boundary is within certain block, which breaks the abstraction of Block-wise RaggedShard and incurs extra communication for quantization; ii) Non-contiguous tensor memory that can 6 happen when two ends of communication buffers are padded (to align collective preferred unit size [18, 32] or equal size across devices [19]), without realizing that such padding is within certain tensor, which breaks the memory contiguousness and incurs interleaved copy overhead (e.g., similar to the copy-out after AllGather in Figure 2); and iii) Imbalanced load that can happen when different tensor sizes or block sizes or padding sizes are not aggregated to be equal across devices, which breaks the symmetry in collective communication (esp., Ring Algorithm) and ends up in underutilized networking. Towards efficient communication. To efficiently group RaggedShard DTensors, we propose two-step approach that addresses the above challenges: first permute tensors, and then pad between them rather than padding within individual tensors, as illustrated in Figure 6(b). The key idea is to balance tensor and block sizes across devices while aligning block boundaries in the sharded communication buffer so that blocks are placed contiguously. This approach inevitably introduces some padding overhead, which must be carefully minimized to reduce both memory usage and communication volume. Formally, the proposed approach can be formulated as an Optimization problem formulation. optimization problem. Let = T1, T2, . . . , Tn denote set of RaggedShard DTensors, which are sharded across devices. Each DTensor has block size of gt, total tensor size (in elements) et, and hence ut = et/gt sharding blocks. We allocate global communication buffer and place each as contiguous memory interval [ℓt, rt). The decision variables are the per-device buffer size and the interval endpoints {ℓt, rt}tT . Our goal is to minimize subject to the three factor constraints: Non-Sharded Block, Contiguous Tensor Memory, and strict Balanced Load (Figure 6(b)): min S,{ℓt,rt}tT s.t. rt ℓt = et rt mS, , rt ℓt rt ℓt, = , kS ℓt kS rt (kS ℓt) 0 , = 1, . . . , (mod gt), This optimization problem is NP-hard, as it can be reduced from the classic Partition problem [5]. Although it can be formulated as an Integer Linear Programming (ILP) problem and solved using off-the-shelf solvers, such methods are impractical at scale. In practice, ILP solvers often take tens of minutes to generate plan and may even trigger system timeouts. Given that user-defined FSDP wrapping can yield hundreds of parameter groups with diverse sharding block sizes, and deployments may span up to hundreds of thousands of devices, we instead design polynomial-time heuristic algorithm that achieves near-optimal efficiency in practice. veScale-FSDP proposes polynomial-time dynamic-programming (DP) Heuristic-guided solution. buffer-layout algorithm, guided by permutation heuristics that exploit the regularity of transformer models. The optimization difficulty comes from tensor permutation: in principle, any permutation of tensors could be mapped into the buffer, and finding the global optimum would require exploring all permutations. Fortunately, in practice, transformer parameters are highly structured: linear weights dominate the total parameter count, and sharding blocks are often consistent across layers. To leverage this regularity, we explore three simple permutations: (i) default order of tensors; (ii) sorted order by sharding block sizes, and (iii) sorted order by tensor shapes. Our statistics show that these orders yield optimal or near-optimal results, so we adopt the default order for simplicity and ease of debugging. Given the tensor order, the proposed algorithm applies DP procedure to place tensors into smallest global buffer while enforcing the aforementioned three factor constraints. It enjoys time complexity of O(T 2m log(E) log(T m)). Algorithm 1 presents the detail. The core idea is case analysis of how each tensor aligns with shard boundaries in any valid layout: (1) it lies entirely within single local shard; (2) it straddles two adjacent shards, but doesnt contain full shard; (3) it fully contains at least one shard. If every tensor falls into cases (1)(2), feasibility is monotone in the shard size S: whenever layout exists for S, it also exists for + (where is the base alignment quantum). 7 Algorithm 1 Structure-aware planning for grouped communication of RaggedShard DTensors. 1: Input: ordered tensor list ; per-tensor sharding block size gt (collectively = {gt}); per-tensor size et; per-tensor number of blocks ut; number of devices m; collective preferred unit size gcoll. 2: Output: minimal per-device buffer size . 3: 4: function CheckValidShard(S) 5: 6: 7: 0 while < et do for all do 8: 9: 10: 11: 12: 13: // Monotonic in i, skip intermediate calculation max{i [l, et) : dp(t, i; S) = dp(t, l; S)} Add interval [l, r] into dp result. + end while end for return: dp(tlast, etlast ; S) 14: 15: end function 16: gcoll 17: for all G.sorted() do 18: LeastCommonMultiple(g, g) min{k : CheckValidShard(k g)} min(S, S) 19: 20: 21: end for 22: return Because every shard includes an inter-tensor boundary, we can always absorb additional as padding. If any tensor is in case (3), the feasible shard sizes must be multiples of = LCM{ gt is in case (3) }. In this regime, feasibility is monotone over multiples: if kL is feasible, then (k+1)L is also feasible. We therefore enumerate the case-(3) set and binary-search the minimal over the corresponding multiples, as shown in Line 17-21. To avoid exponentially enumerating all Case-(3) subsets, we sort tensors by element count and enumerate only the granularity prefixes, yielding 2-approximation. Inside the feasibility checker, we define dp(t, i) as the minimum number of devices (shards) required to store all atomic units up to the i-th unit of tensor t. Although the index space is as large as numerical space, dp(t, i) is monotone within tensor: dp(t, i) dp(t, i+1). So within each tensor there are at most distinct values. In Line 8-9, we exploit this by batching contiguous indices into segments and skipping dp(t, ) evaluations at intermediate indices, achieving the stated time complexity. Distributed Buffer (DBuffer) Beyond grouping RaggedShard DTensors, the underlying communication buffer also plays vital role in achieving high performance of communication, computation, and memory efficiency. To this end, veScale-FSDP proposes new primitive, Distributed Buffer (DBuffer), to optimize the performance of grouped DTensors. Figure 7 shows the design. First, inspired by DTensor, DBuffer provides global buffer semantics over an -dimensional device topology, with sharding specification along each dimension, abstracting away the complexity of -D communication and operations. Second, DBuffer takes group of tensors and executes group-level operators rather than per-tensor operators. For example, before communication, each tensor may need to launch its own CUDA kernels for add, scale, zero, or copy (which may differ across tensors), incurring fragmented compute overhead and blocking communication. With DBuffer, identical kernels across tensors are fused before communication, reducing blocking time. Third, DBuffer offers zero-copy before and after communication by leveraging RaggedShards planning algorithm and providing persistent address mapping to each tensors data pointer, minimizing memory footprint and fragmentation. Lastly, DBuffer uses in-place communication and computation. 8 Figure 7 Distributed Buffer (DBuffer) for high performance communication. 2D DBuffer for AllGathering parameters is shown; Reversely, 2D DBuffer redistributing from (Partial, Partial) to (Replicate, Shard) implements 2D gradient reduction with ReduceScatter and AllReduce."
        },
        {
            "title": "6 Evaluation",
            "content": "Our evaluation answers the following questions: How much does veScale-FSDP improve end-to-end training performance over all baseline systems (6.1)? How well does veScale-FSDP scale to large device counts (6.2), in terms of weak scaling, strong scaling, and model size scaling? How are 8-bit Adam and Muon optimizer enabled by veScale-FSDPs customizable sharding granularity and RaggedShard DTensor, in both performance and development velocity (6.3)? How does veScale-FSDP planner minimize padding, and what is the algorithm overhead (6.4)? How much does each component of veScale-FSDP contribute to the training performance (6.5)? Hardware: We ran all experiments on GPU cluster; each node contains 8GPUs and connected with proprietary high-speed interconnect. Implementation: veScale-FSDP is implemented with 7.6 lines of code (LoC) in Python, transparently replacing the backend of FSDP2 while using the same PyTorch-native fully_shard API. veScale-FSDP serves as plug-and-play Python module, compatible with standard PyTorch distributed runtimes and wide range of PyTorch versions. Baselines: We compare veScale-FSDP against state-of-the-art open-source frameworks: DeepSpeed ZeRO v0.17.6 [25], PyTorch 2.7.1 FullyShardedDataParallel (FSDP1) [34], PyTorch 2.7.1 fully_shard (FSDP2) [20], and Megatron-FSDP. For fairness, all frameworks are configured to use ZeRO-3 with mixed precision (i.e., FP32 master weights and BF16 forward/backward). Unless otherwise specified, veScale-FSDP employs element-wise sharding granularity and is compatible with standard training workflows. Workloads: For the end-to-end comparison with the baselines (6.1), we evaluate two state-of-the-art opensource models, LLaMA-3-70B [4] and GPT-OSS-120B [1], as well as an internal MoE model. Under weak scaling, each device is statically assigned one batch; the sequence length is 4096 for the dense LLaMA model and 8192 for the MoE models. We use the AdamW optimizer by default. To avoid out-of-memory (OOM) errors for the baselines on GPT-OSS, we also report results using the SGD optimizer. 9 Figure 8 FSDP training performance. Top row: normalized aggregate throughput (tokens/s). Bottom row: peak per-GPU memory (GB). We sweep FSDP (ZeRO-3) at 128/256 GPUs and HSDP with 2and 4-way replication (2*256, 4*256 GPUs)."
        },
        {
            "title": "6.1 End-to-End Performance",
            "content": "Figure 8 compares the performance of veScale-FSDP to baselines on three representative models on 1024 GPUs. Throughput: on the MoE models, veScale-FSDP is 1166% faster than all baselines. On LLaMA-3-70B veScaleFSDP is 5% faster than DeepSpeed, FSDP1, and FSDP2, and slightly ahead of Megatron-FSDP. The higher throughput arises from optimized communication overlapping, DBuffer-based zero-copy collectives, and flexible sharding granularities that avoid padding overhead. In contrast, DeepSpeed emits fragmented collectives [8], while FSDP1 exhibits communication bubbles where data movement operations block NCCL progress, under-utilizing the network in both systems. FSDP2 relies on the per-parameter DTensor even-sharding format that introduces interleaved copy-out after all-gather and interleaved copy-in before reduce-scatter; together these copies can consume up to 14% of training iteration and hence reduce throughput. In addition, FSDP1 and FSDP2 overlook NCCL address alignment caveat, leading to substantial degenerate communication performance in certain cases [32]. Although Megatron optimized for zero-copy collectives, its fixed Stride(0) sharding granularity, where to remain consistent with upstream DTensor Shard(0) semantic for distributed checkpointing, induces 33% buffer padding inflation in MOE models thus slows the collectives [17]. Our experiments show that veScale-FSDP achieves linear scalability; detailed analysis appears in 6.2. Memory: Across benchmarks, veScale-FSDP reduces peak reserved memory by 1630%. The memory saving stems from deterministic, batched DBuffer memory management: we explicitly manage stream dependencies for predictable memory deallocation, and we batch allocations to reduce fragmentation. By contrast, DeepSpeed and FSDP1 inherit non-deterministic deallocations from implicit record_stream [6], which often prevents the caching allocator from reusing buffers, inflating peak reserved memory by 20%. Relative to FSDP2 per-parameter eager allocation, our batched policy yields further 12% reduction. Megatrons padding-inflated buffers not only degrade collective efficiency but also raise peak memory by 33% in MoE experiments; its mixed-precision support persists low-precision buffers, consuming 24% more memory than veScale-FSDP in the LLaMA-3 experiments. Lower reserved memory translates directly into higher end-to-end efficiency: under high memory pressure, the PyTorch caching allocator issues device frees that synchronize with the driver and stall training. In terms of scalability, veScale-FSDP memory footprint decreases monotonically as the FSDP group size increases and grows only marginally with the replication factor, matching scaling expectations. notable exception appears with GPT-OSS: FSDP2 trains at 128 devices but OOMs at 256. The per-parameter sharding design in DTensor requires padding to enforce even splits along the sharded dimension; with 128 experts spread over 256 devices, the all-gather buffer effectively doubles, exhausting 10 (a) Weak scaling to 10K GPUs (b) Strong scaling to 10K GPUs Figure 9 Scalability of veScale-FSDP in up to 10K GPUs and 2.4T model size. (c) Model scaling on 1K GPUs memory. While FSDP2 allows custom sharding along other dimensions, it requires manual padding and thus doubles the interleaved-copy overhead, making it prohibitively expensive (recall Table 1)."
        },
        {
            "title": "6.2 Scalability and Composability\nThe flexibility of RaggedShard also enables seamless integration with complementary parallelization strategies\nsuch as expert parallelism (EP) [12]. Combining these techniques allows veScale-FSDP to efficiently scale\ntraining to internal models with up to 2.4T parameters on as many as 10K GPUs, as shown in Figure 9. Note\nthat we evaluate scalability of MoE, because MoE workloads are often more challenging to scale under FSDP:\nsparse expert computation lowers per-GPU compute while requires substantial all-gather/reduce-scatter traffic,\nmaking communication and padding overheads more significant.",
            "content": "Weak scaling: Figure 9a presents the weak scaling performance of veScale-FSDP. We train an 800B-parameter MoE internal model on 1K to 8K GPUs while keeping the input size fixed at 2K16K tokens per GPU. Across all input sizes, veScale-FSDP demonstrates near-linear scalability as the GPU count increases. This is expected since the communication cost of FSDP and the computation cost per GPU remain constant with respect to the number of GPUs, depending only on the model and input sizes. These results confirm the efficiency of veScale-FSDP on large-scale GPU clusters. Strong scaling: We further evaluate the strong scaling performance of veScale-FSDP by fixing the global batch size to 16M128M tokens and tuning expert and sequence parallelism configurations for each setting. Figures 9b show the resulting throughput across different GPU numbers. veScale-FSDP scales linearly with 128M-token global batch up to 10K GPUs, while still delivering 3.4 throughput gain from 1K to 8K GPUs at 16M-token global batch. When number of GPUs is small, each GPU processes enough tokens to fully overlap communication with computation, yielding near-linear scaling. However, as the GPU count continues to increase, fewer tokens are assigned per GPU per iteration, causing FSDP communicationincluding parameter all-gather and gradient reduce-scatterto dominate runtime. To mitigate this overhead, we adopt cross-node expert parallelism, which further reduces FSDP communication time. This optimization introduces higher computation cost due to token exchange and reduced kernel efficiency, resulting in the performance drop at very large scales. Model scaling: We also evaluate model scaling by fixing the GPU count to 1K and increasing the model size from 400B to 2.4T parameters. With model sparsity constant and 8K training tokens per GPU, we scale both depth (number of layers) and width (intermediate dimensions) proportionally. Figure 9c reports the effective Model FLOPS Utilization (MFU) per GPU as model size grows. Enabled by efficient memory management of DBuffer (5), veScale-FSDP can train 2.4T-parameter models on only 1K GPUs without any performance degradation. In fact, MFU slightly improves with larger models due to the increased compute intensity and better utilization of GPU resources. 11 Figure 10 Training convergence with veScale-FSDP on 64 GPUs (FSDP size 64) for 8-bit Adam and distributed Muon. (a) 8-bit Adam (b) Distributed Muon Algorithm 2 RaggedShard Distributed Muon 1: for all in 2D parameter tensors do 2: 3: 4: 5: 6: 7: 8: grad(w) MomentumUpdate(g, m) // Choose compute device via load balancing SelectRoot() // Unshard to root via redistribute Redistribute(u, RaggedShard(r)) // Muon update: NewtonSchulz on full tensor. NewtonSchulz(o) // Redistribute update back. Redistribute(o, placement(u)) 9: 10: 11: 12: η 13: end for 6.3 8-bit Adam and Muon Optimizer We show the flexibility of RaggedShard DTensor using two examples: 8-bit Adam and distributed Muon optimizer. 8-bit Adam optimizer. 8-bit Adam [3] applies block-wise INT8 quantization to the gradient statistics, substantially reducing optimizer-state memory. To enable 8-bit Adam, veScale-FSDP exposes orig_param_policy interface that lets users set the quantization granularity per parameter. In our setup, we use 32 32 blocks and assign matrix parameters to 32-row block granularity. With this layout, each device quantizes its local shard independently without any communication, and block boundaries are perfectly preserved by RaggedShard. In contrast, existing FSDP systems do not natively track such block boundaries, so enabling block-wise 8-bit Adam often requires intrusive system changes or manual collectives to exchange quantization metadata, incurring both complexity and overhead. We implement the 8-bit Adam using veScale-FSDP with few lines of code and provide the evaluation in Figure 10a. We compare 8-bit Adam under distributed data parallelism (DDP) and veScale-FSDP. The loss curves track closely, with occasional spikes characteristic of reduced-precision optimizer states. The small difference stems from the gradient-reduction schedule: DDP uses bucketed all-reduce, whereas veScale-FSDP performs layer-wise reduce-scatter. (Note that the loss curves in Figure 10 are not directly comparable: for 8-bit Adam we use smaller learning rate to mitigate overflow/underflow in reduced precision.) Distributed Muon optimizer. The matrix-sign preconditioner (e.g., NewtonSchulz) of Muon requires the full 2D parameter matrix with its original shape. Algorithm 2 sketches the distributed Muon optimizer enabled by RaggedShard. Thanks to RaggedShards capability to support uneven sharding, users can write Muons parameter-gather step in clean SPMD way: after redistribution, only the root rank holds the full 2D parameter, so the NewtonSchulz update becomes no-op on other ranks. As lines 47 show, the algorithm selects root via load balancing and unshards to it using the standard DTensor redistribute with 12 Figure 11 Padding overhead (extra padding bytes over total parameter size) of RaggedShard communication. Lines compare the sharding granularities (1/16/128 parameter row size) versus FSDP sharding size (number of GPUs). (a) DeepSeek-v3-671B (b) GPT-OSS-120B RaggedShard placement. Lines 89 run the Muon matrix iteration only on the root that holds the full tensor. Finally, lines 1012 redistribute the update back to the original device and apply it. Therefore, users do not need to handle the complex logic of communication and can further overlap communication with computation via asynchronous redistribute. In addition, our optimized Muon reaches 47.3% MFU on 256 GPUs by exploiting the communication-computation overlapping and using torch.compile to further increase compute density. We implement distributed Muon using veScale-FSDP with few lines of code and provide the evaluation in Figure 10b. We compare the loss curves of Muon with AdamW: the two Muon runs (veScale-FSDP and DDP) match closely, and Muon converges faster than AdamW, stabilizing around 0.01 lower loss after training 80B tokens, which is consistent with prior results [31]."
        },
        {
            "title": "6.4 Planning Quality",
            "content": "A major design objective of the planning algorithm (Algorithm 1) is to enable arbitrary granularity of RaggedShard, while minimizing padding overhead and thus reducing communication volume. The quality of planning algorithm can be directly evaluated by padding size. We evaluate it by benchmarking DeepSeek-v3671B and GPT-OSS-120B across varying device counts. Following the DeepSeek-style quantization scheme, we quantize only the FFN weights (most parameters) and sweep the row granularity of expert-MLP matrices over 128, 16, 1. The 128-row setting reproduces DeepSeeks 128128 tiling (i.e., weights can be sliced into 128128 blocks). We then report the resulting relative padding ratios and analyze the root cause of the extra padding. Figure 11a and 11b show that with 1 and 16 row granularities, veScale-FSDP keep padding overhead less than 3% across all FSDP sizes for both models. With 128 rows, DeepSeek-v3 remains mostly below 3% with mild growth, whereas GPT-OSS exhibits step-like fluctuations with spikes up to 18%. GPT-OSS fuses all experts into single parameter tensor, whereas DeepSeek-V3 materializes each expert as separate parameter; this enables per-expert padding between MLPs and thus relaxes the global padding constraint. The fluctuation behavior is expected: each matrix must be partitioned across the shard group in discrete quanta determined by (i) the granularity unit (e.g., rows) and (ii) NCCLs even-input alignment for high-performance collectives. Effective shard sizes are therefore rounded up to the least common multiple of these granularities; when the group size crosses multiple, the per-device shard size jumps, producing the observed spikes. Lastly, we also evaluate the overhead of planning algorithm itself: the algorithm runtime is less than 0.3 seconds across all experiments, which is one-time and negligible in distributed training initialization."
        },
        {
            "title": "6.5 Performance Breakdown",
            "content": "To quantify the benefit of each component, we ablate veScale-FSDP by disabling one component at time and report the resulting throughput, normalized to the full system. We run this study on 32 GPUs when training GPT-OSS-style model with 8-bit Adam. 13 Table 2 Component ablation for 8-bit Adam, in terms of normalized throughput when disabling each component. N/A means impossible without intrusively changing model/optimizer code or managing custom collectives. veScale-FSDP Component Normalized Throughput 100.0% Combined Disable DBuffer only 92.8% Disable Planning Algorithm only 65.4% Disable RaggedShard only N/A Table 2 shows that DBuffer and the Planning Algorithm account for most of the realized speedups: disabling them reduces throughput to 92.8% and 65.4%, respectively. In contrast, RaggedShard is not just an optimization; it is the abstraction that makes block-wise 8-bit Adam usable without intrusive model/optimizer changes or hand-written collectives. Specifically: DBuffer. Disabling DBuffer drops throughput by 7.2%, reflecting the copy-in/copy-out overhead around collectives when communication buffers require copy. Planning Algorithm. Disabling the planning drops throughput by 34.6% because quantization blocks are no longer guaranteed to be fully contained within devices local shard. The system then falls back to DTensor redistribution to assemble the required optimizer states before per-block quantization, incurring substantial extra communication overhead. RaggedShard DTensor. Disabling RaggedShard makes it effectively non-runnable: users must either (i) carefully change every model and optimizer tensor so that 32 32 block boundaries align with shard boundaries, or (ii) manually implement complex collectives (e.g., per-block metadata exchange and state gathering) to recover block-wise semantics. We therefore report this setting as N/A to indicate its not meaningfully usable."
        },
        {
            "title": "7 Lessons Learned",
            "content": "During the deployment of veScale-FSDP for real industrial workloads that use more than 10K GPUs, we summarize the key lessons we have learned in the following. Lesson-1: Small-scale workloads can predict large-scale performance. The performance of FSDP-based workloads can be accurately estimated using each layers computation time and FSDP communication time. Computation occurs entirely within each GPU, and FSDP communication time remains largely unchanged when the number of GPUs increases. This observation is validated by our weak scaling experiments (6). In practice, we profile the performance of veScale-FSDP on around 64 GPUs and extrapolate to thousands of GPUs, achieving similar results. This extrapolation assumes that the profiling run exercises network behavior similar to the target scale: comparable network topology, identical collective algorithms/protocols, and sufficiently large workload to reach bandwidth saturation. To further improve predictability at large scales, we use additional parallelization (e.g., HSDP/EP) to cap the collective group size, preventing excessively large collectives whose latency can vary more. Lesson-2: Design system abstractions on the shoulders of giants. DTensor provides powerful abstraction that already supports wide range of parallelization techniques. By designing new abstractions on top of DTensor, we can seamlessly integrate existing parallelization strategies. In our work, RaggedShard placement is implemented as an optional placement on DTensor, enabling easy collaboration of established infrastructure such as tensor and expert parallelism, as well as mature training tools like distributed checkpointing [23]. This approach minimizes engineering effort while contributing to shared ecosystem that benefits the broader community. In fact, RaggedShard has already appeared as planned feature on the official roadmap [24] of PyTorch. Lesson-3: Decoupled model definition with system optimization matters. The rapid evolution of model architectures demands frequent updates to model definitions. However, existing frameworks such as MegatronLM tightly couple system-level parallelization optimizations with model code, making it difficult for researchers to modify or extend architectures. In veScale-FSDP, we decouple model definition from the system framework, allowing researchers to focus on model design while maintaining linear scalability across up to 10K GPUs. This separation greatly simplifies model development and accelerates architectural innovation."
        },
        {
            "title": "8 Conclusion",
            "content": "veScale-FSDP is scalable training system that combines high flexibility with high performance through the RaggedShard abstraction and structure-aware planning algorithm that maximizes GPU utilization. Experiments demonstrate that veScale-FSDP seamlessly integrates with emerging techniques such as Muon optimizers and significantly outperforms existing systems, achieving 566% higher throughput and 1630% lower memory usage, while scaling efficiently to tens of thousands of GPUs."
        },
        {
            "title": "9 Acknowledgments",
            "content": "veScale-FSDP would not have been possible without the tremendous support and collaboration of our teammates and colleagues. We sincerely thank them (in no particular order; this list is not exhaustive): veScale members: Hongrui Zhan, Ziyi Zhang, Hao Feng ByteDance teammates: Jianyu Jiang, Chenyuan Wang, Cesar Andres Stuardo Moraga, Juntao Zhao, Bin Jia, Chengye Li, Zhongkai Zhao, Shixiong Zhao, Tiantian Fan, Hanshi Sun, Wenlei Bao, Shixun Wu, Zhekun Zhang, Yanbo Liang, Li-wen Chang, Jun Wang, Cheng Li, Li Han, Heng Zhang, Zhenbo Sun, Bo Liu, Xiaonan Nie, Ru Zhang, Hao Gong, Zuquan Song, Yucheng Nie, Jiawei Wu, Hongpeng Guo, Xinyi Di Equally important, we thank everyone on the TorchTitan team and Edward Z. Yang for the insightful discussions and collaboration within the open-source community."
        },
        {
            "title": "References",
            "content": "[1] Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. [2] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. arXiv preprint arXiv:2110.02861, 2021. [3] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers via block-wise quantization. In International Conference on Learning Representations, 2022. [4] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [5] Michael Garey and David S. Johnson. Complexity results for multiprocessor scheduling under resource constraints. SIAM journal on Computing, 4(4):397411, 1975. [6] Andrew Gu, Wei Feng, and Yanli Zhao. [rfc] per-parameter-sharding fsdp, 2023. URL https://github.com/ pytorch/pytorch/issues/114299. [7] Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor optimization. In International Conference on Machine Learning, pages 18421850. PMLR, 2018. [8] Halilakin. Deepspeed is slower than fsdp, 2024. URL https://github.com/deepspeedai/DeepSpeed/issues/ 5047#issuecomment-1926275502. [9] Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi Huang, Yangrui Chen, Zhi Zhang, Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, et al. {MegaScale}: Scaling large language model training to more than 10,000 {GPUs}. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), pages 745760, 2024. [10] Keller Jordan, Yuchen Jin, Vlado Boza, Jiacheng You, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan.github.io/posts/muon/. [11] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [12] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020. [13] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating data parallel training. arXiv preprint arXiv:2006.15704, 2020. [14] Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, and Li-Wen Chang. veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD, 2025. URL https://arxiv.org/abs/2509.07003. [15] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [16] Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, and Xin Liu. VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo, 2025. URL https://arxiv.org/abs/2508.02317. [17] Megatron. Mcore custom fully sharded data parallel (fsdp). Technical report, 2025. [18] NVIDIA NCCL. Regarding the allgather bandwidth with different byte alignment under different protocols, 2025. URL https://github.com/NVIDIA/nccl/issues/413. [19] NVIDIA NCCL. Nccl: Collective operations, 2025. URL https://docs.nvidia.com/deeplearning/nccl/ user-guide/docs/usage/collectives.html. [20] Pytorch. Fully sharded data parallel (fsdp2). Technical report, 2024. 16 [21] PyTorch. Pytorch jaggedtensor, 2025. URL https://docs.pytorch.org/FBGEMM/fbgemm_gpu/overview/ jagged-tensor-ops/JaggedTensorOps.html. [22] PyTorch. Pytorch nestedtensor, 2025. URL https://docs.pytorch.org/docs/main/nested.html. [23] PyTorch Team. Distributed checkpoint, 2025. URL https://docs.pytorch.org/docs/stable/distributed. checkpoint.html#distributed-checkpoint-torch#-distributed-checkpoint. [24] PyTorch Team. Meta pytorch team 2026 h1 roadmaps, 2026. URL https://dev-discuss.pytorch.org/t/ meta-pytorch-team-2026-h1-roadmaps. [25] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [26] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. arXiv preprint Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053, 2019. [27] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. [28] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [29] TensorFlow. Tensorflow ragged tensors, 2025. URL https://www.tensorflow.org/guide/ragged_tensor. [30] The PyTorch Team. PyTorch DTensor (Distributed Tensor). https://pytorch.org/docs/stable/distributed. tensor.html, 2024. [31] Kaiyue Wen, David Hall, Tengyu Ma, and Percy Liang. Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046, 2025. [32] Bowen Wu, Wei Cui, Carlo Curino, Matteo Interlandi, and Rathijit Sen. Terabyte-scale analytics in the blink of an eye. arXiv preprint arXiv:2506.09226, 2025. [33] Jane Xu. Fsdp & cudacachingallocator, 2024. URL https://dev-discuss.pytorch.org/t/ fsdp-cudacachingallocator-an-outsider-newb-perspective/1486. [34] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. [35] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, URL Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. https://github.com/pytorch/pytorch/blob/a4925c0ce004cf883fdd1b248d71676769524934/torch/ distributed/fsdp/_runtime_utils.py#L695C1-L773C1. Fsdp1 post backward reduce, 2025."
        }
    ],
    "affiliations": [
        "Bytedance"
    ]
}