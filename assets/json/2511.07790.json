{
    "paper_title": "CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis",
    "authors": [
        "Rochana R. Obadage",
        "Sarah M. Rajtmajer",
        "Jian Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 9 7 7 0 . 1 1 5 2 : r CC30K: CITATION CONTEXTS DATASET FOR REPRODUCIBILITY-ORIENTED SENTIMENT ANALYSIS Rochana R. Obadage Old Dominion University Norfolk, VA, USA rochana@cs.odu.edu Sarah Rajtmajer The Pennsylvania State University University Park, PA, USA smr48@psu.edu Jian Wu Old Dominion University Norfolk, VA, USA j1wu@odu.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited papers perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing research gap in resources for computational reproducibility studies. The dataset was created through pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https: // github. com/ lamps-lab/ CC30k . Keywords Citations Reproducibility Replicability Machine Learning Classification RAG LLM Fine Tuning Science of Science"
        },
        {
            "title": "1 Introduction",
            "content": "Citation contexts are textual fragments in scholarly documents that surround and contain citations to prior work (see Fig. 1). Citation context can tell us why the work was cited, offer insight into the authors perspectives on the work, and highlight relationships between the cited and citing works. Here, we explore one particular opportunity inherent to citation contexts, namely, the opportunity to mine citation contexts for signals of reproducibility. In computational fields such as Machine Learning (ML) and Artificial Intelligence (AI), researchers often share and reuse code to compare models against benchmark datasets or build upon and extend learning frameworks. In the process, they often need to reproduce (using the same methods and datasets) or replicate (using the same methods and different datasets) the results reported in another paper.1 The outcome of this process can be reported in their paper as citation contexts. Despite norms of code and data sharing and researchers advanced technical expertise, automatic reproducibility assessment of published work remains pressing challenge for the computer sciences broadly [3, 4, 5] and the AI community in particular [6, 7, 8, 9]. It was empirically shown using small dataset that these citation contexts may serve as promising signal of the reproducibility of the cited study [10]. Moreover, writ large, analyzing these contexts can help 1Throughout this work, we adopt the definition of reproducibility from [1] where finding is deemed reproducible if consistent results are obtained using the same input data, computational steps, methods, and code, and conditions of analysis. Similarly, we adopt the definition of replicability from [1], which refers to obtaining consistent results across studies aimed at answering the same scientific question, with each study collecting its own data. These definitions align with those adopted by ACM [2]. identify patterns, trends, and gaps in reproducibility to advance our understanding of scientific transparency and best practices [11]. R.R. Obadage et al., 2025 Figure 1: Examples of citation context with different reproducibility-oriented sentiments [10]. To facilitate the study of reproducibility assessment and its relation with citation context, we develop the CC30k dataset comprising 30,734 citation contexts from scientific literature in ML  (Table 2)  . Each context identifies cited paper through its citation mark and is labeled into one of three categories reflecting reproducibility-oriented sentiments (ROS): Positive, Negative, and Neutral. The CC30k dataset was created using pipeline. We extracted citation contexts, parsed citation marks and sentence structures, and filtered citation contexts to eliminate ambiguity caused by multiple citation markers within the same context. We used crowdsourcing to annotate the contexts via Amazon Mechanical Turk [12]. Each context was labeled by three independent annotators, with majority voting employed to determine the final label. We implemented comprehensive and careful worker selection mechanism, including using pilot dataset and multiple metrics to build pool of qualified annotators. Verifying the crowdsourced results indicates that it is possible to obtain high-quality ROS labels on citation contexts in AI papers without relying on annotators with domain-specific backgrounds. Our contributions are as follows: We introduce CC30k, large-scale dataset of 30,734 citation contexts, each labeled according to its reproducibility-oriented sentiment. We demonstrate why traditional sentiment classifiers fail to capture reproducibility-oriented sentiment. To illustrate the utility of CC30k, we fine-tune several large language models (LLMs) on the dataset for the task of ROS classification. We compare CC30k with existing datasets for both sentiment analysis and citation analysis. We analyze and categorize citation contexts based on their citation mark formats."
        },
        {
            "title": "2 Related Work",
            "content": "Existing datasets in sentiment analysis and citation context analysis have yet to incorporate signals of reproducibility and replicability (see Table 1). Traditional sentiment analysis datasets, such as IMDb Movie Reviews [13] or SemEval 2017 Task 4 [14], are designed for sentiment classification tasks in web-based text data such as product reviews [15] or social media posts [16]. These datasets played crucial role in advancing sentiment analysis models and enabling the development of general sentiment analysis tools. Many of these datasets have become core components in widely adopted software packages and are routinely featured on leaderboards such as those maintained on the paperswithcode [17] platform. Citation context datasets such as the ACL-ARC subset [18], SciCite [19], and the Critical Citation Contexts Corpus [20] typically focus on broader citation functions (e.g., background, uses, compares or contrasts, motivation, continuation, future), citation intents (e.g., background, method, result), or on citations that indicate scholarly critique or debate, without specifically addressing reproducibility. While these resources have enriched bibliometric analyses and enabled deeper understanding of citation roles, they lack the specificity required to evaluate reproducibility-oriented sentiments in scholarly texts. The CC30k dataset bridges these domains through identifying reproducibility-oriented sentiments in citation contexts. 2 CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis Table 1: Comparison of CC30k with existing datasets for sentiment (above the middle line) and citation analysis (below the middle line). Dataset Name Internet Movie Database (IMDb) [13] SemEval-2017 Task 4 [14] Purpose Evaluate sentiment analysis models on complex movie reviews Comprehensive benchmark for various sentiment analysis subtasks Size Data Category reviews 65,000 50,000 tweets classification for Applications Sentiment movie reviews Sentiment classification, topicspecific sentiment, Tweet quantification Sentiment analysis for customer reviews of airlines Sentiment analysis for short, informal social media posts Educational content evaluation, code-mixed language processing Training tools for automatic retrieval of critical citations Bibliometrics and citation analysis Bibliometric search and citation sentiment detection Classification of incidental vs. influential citations Evaluation of citation roles and impact on citing papers Precise citation generation, verifiable content creation Bibliometrics and citation intent studies Research in citation intent classification Citation context generation, academic writing assistance Reproducibility and assessment prediction Twitter US Airline Sentiment [21] 14, tweets Sentiment140 [22] 1.6M tweets SentiGrad [23] Critical Citation Contexts Corpus [20] IMS Citation Corpus [24] Context-Enhanced Citation Sentiment Detection [25] 3C Shared Task (2021) [26] Citation FPAI [27] 6,064 youtube ments com2,008 1,741 3,000 2,120 citation texts citation texts citation texts citation texts citation texts conconconcon- concon1,969 10,969 10,000 wikipedia citations citation texts citation texts citation texts citation texts 17,210 30,734 conconcon- SCiFi [28] ACL-ARC Subset [18] SciCite [19] MCG-S2ORC [29] CC30k (ours)"
        },
        {
            "title": "3 Methods",
            "content": "3.1 Overview Evaluate sentiment analysis models with imbalanced sentiment classes Evaluate models on large-scale real-world social media data Sentiment analysis of code-mixed comments using classification Study of critical citations to understand mechanisms and develop detection tools Citation faceted classification schemes Sentiment analysis of citation contexts considering broader contexts classification Citation context based on influence Citation analysis using classification scheme based on function, polarity, aspects, and influence Verifiable generation with finegrained citations Citation intent classification (e.g., background, method, result) Classify citation intents in academic papers Multi-sentence citation context generation Reproducibility-oriented sentiment analysis (Pos/Neg/Neutral) The raw dataset contains 41,244 citation contexts without any ROS labels, introduced in our previous study [10]. We built small dataset consisting of 1,937 citation contexts whose ROS labels were manually assigned by domain experts. However, the dataset contained only 23 samples labeled with negative ROS, tiny fraction of the labeled samples. Because manual labeling is time-consuming and thus does not scale well, we explored the assistance of large language models (LLMs). However, both LLMs and existing generic sentiment analysis models struggled to accurately label ROS (see Table 7). Therefore, here we explore the crowdsourcing method. Fig. 2 provides schematic illustration of the data preparation workflow. 3.2 Dataset Production The data production pipeline  (Fig. 2)  contains several key terms: Original studies (cited papers): Original research papers whose results are reproduced in reproducibility studies. Reproducibility studies (rep-studies): Papers documenting attempts to reproduce reported results from original studies. 3 R.R. Obadage et al., 2025 Figure 2: The data production process. Citing papers: Papers citing original studies, by including citation mark in sentence, which is called citation context. Here, we only study explicit citation context, which includes citation marks [30]. Data preparation included the following key steps: 1. Data collection: We collected metadata and PDF files for reproducibility studies (145 rep-studies) and their associated original studies (130 original papers). We primarily sourced rep-studies from the Machine Learning Reproducibility Challenge (MLRC) competitions [31] and additional resources, as detailed in Table 2. 2. Citation context collection: We obtained citation contexts about original studies from citing papers using Semantic Scholar Graph API (S2GA) [32], and identified total of 13,314 citing papers referencing the original studies, resulting in 41,244 citation contexts (collected January 2024). 3. Crowdsourcing for ROS labeling: After filtering out citation contexts with ambiguous citation marks (see sections 3.3 and 3.4), we annotated citation contexts through crowdsourcing, yielding 25,829 labeled contexts, each categorized as Positive, Negative, or Neutral. 4. Crowdsourced label verification: To assess the quality of crowdsourced labels, we manually validated 1% (244 citation contexts) of the dataset using stratified sampling. 5. Negative augmentation: To address the class imbalance, we added 4,905 negative citation contexts by combining labels from human experts and supervised machine learning labeler. 4 CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis Table 2: Data sources for selected reproducibility studies with the year of reproduction, number of rep-studies (#Rep-S.), number of original studies (#Org-S.), number of citations (#Cit.), and number of citation contexts (#contexts). Data Source ICLR [33, 34] NeurIPS [33, 34] MLRC [33, 34] MLRC [33, 34] MLRC [33, 34] TSR [35] Total Year 2019 2019 2020 2021 2022 #Rep-S. 4 10 23 47 45 16 145 #Org-S. 4 10 22 41 37 16 130 #Cit. 651 3,364 3,224 2,596 2,881 598 13,314 #Contexts 2,102 9,908 10,798 6,958 9,869 1,609 41,244 3.3 Citation mark analysis citation mark, or inline citation, refers to complete entry in bibliography elsewhere in the paper [36]. It may appear in parentheses, brackets, or as superscript number. Citation marks have many styles, usually associated with the bibliographic style adopted throughout the paper. For example, the citation marks associated with the APA style include up to three authors last names followed by the publication year. In the IEEE style, it includes only an Arabic number in square brackets. citation context may cite several papers in several places. To avoid confusion for labelers in the crowdsourcing and ensure that only uniquely identifiable (e.g., Table 3: Items (a) and (f)) citation contexts were included in the samples to be annotated, we analyzed citation marks present within citation contexts and exclude those with multiple, inconsistent, or ambiguous citation marks (e.g., Table 3: Items (c) and (e)) (citation context filtering processes are further discussed in Section 3.4). This process involves visually examining various citation formats, parsing citation marks using regular expressions, recognizing the appearance of citation marks in citation contexts, and establishing filtering rules. By programmatically examining the entire citation context collection (41,244), we found that the most common text-based formats in our dataset are MLA, APA, and IEEE. We categorize citation contexts based on the number of citation marks they contain: Contexts with single citation mark: For example, the IEEE format (e.g., \"[1]\") or MLA/APA format (e.g., \"(Han, 2022)\"). Contexts with multiple citation marks: Examples include multiple IEEE citations (e.g., [1][2][3]), or multiple MLA/APA citations (e.g., (Zhang et al., 2022; Höllein et al., 2022; Xie et al., 2022)). Table 3: Identified citation mark formats via regular expression-based parsing and their descriptions. Contexts with formats (c) and (e) were excluded from this study. Item Citation Mark Format (a) (b) [1] [1][2][3] (c) (d) (e) (f) (g) (h) (i) (j) [1] and [2] [1,2,3] [1,2,3] text text text [4] (Yao, 2022) (Yao et al., 2022) (Zhang et al., 2022; Höllein et al., 2022; Xie et al., 2022) (Iwasawa & Matsuo, 2021) (Zhang et al., 2022; Iwasawa & Matsuo, 2021; Xie et al., 2022) Description single numeric citation mark, commonly used in IEEE and similar styles. Multiple numeric citation marks appearing consecutively, representing multiple references. Numeric citation marks separated by and\" indicating distinct references. range of numeric citation marks listed together, representing multiple references within single bracket. combination of grouped numeric citation marks followed by another distinct numeric citation mark, often within the same context. textual citation with single author, often used in MLA or APA styles. textual citation indicating multiple authors, abbreviated using et al.. Multiple textual citations with et al., separated by semicolons. Often used in APA or similar styles. textual citation listing two authors explicitly, separated by &. Common in styles like APA. combination of textual citations, including both et al. and explicit &, separated by semicolons. 5 R.R. Obadage et al., 2025 Contexts with multiple citation marks are more complex and require special parsing and handling, especially when citation marks are mixed or placed in different parts of the sentence. full list of citation mark formats identified in our data and examples is shown in Table 3. 3.4 Cleansing citation contexts for crowdsourcing By identifying various styles of citation marks, resolving citation marks referring to the cited (original) paper from multiple citation marks, and analyzing sentence structures, we were able to provide clean citation contexts to crowdworkers. We created filtered dataset containing only citation contexts that uniquely identify the related cited paper (see Appendix2). regular expression-based approach (see our GitHub repository3) was applied to obtain citation contexts in which cited papers are unambiguously identified (Table 3: Item (e) is problematic and cannot uniquely identify cited paper because grouped and separate numeric citation marks appear together, making it unclear which cited paper the text refers to). This process reduced the number of citation contexts from 41,244 to 25,829  (Table 4)  . Table 4: Summary of scenarios in which citation context contains citation marks referring to uniquely identified cited papers. Scenario Description Example Contexts with single citation mark These citation contexts are straightforward and easily identifiable (Ya et al., 2022; Dai & Hang, 2021) or [1] or [1, 2, 3] Contexts with multiple APAlike citation marks These contexts have more than one citation mark in single context, which could be challenging for workers without additional details (Ya et al., 2022; Dai & Hang, 2021) text text (Zhao et al., 2022) Contexts that contain only the first authors name These contexts only contain the first author name of the cited paper within the context Zhao argues that... or According to Zhang... Total 3.5 Crowdsourcing annotation 3.5.1 Task Design Count 20, 4,871 128 25,829 We employed crowdworkers recruited through Amazon Mechanical Turk to annotate the dataset (25,829 citation contexts) with three ROS categories. Each context was assigned to three workers; each worker was asked to assign label independently to citation context. We applied majority voting (2/3 or 3/3 agreement) to determine the final label for citation context. To progressively monitor the labeling process, we posted the entire job in 26 batches, each containing about 1000 citation contexts. For each labeling task, we recorded the time in seconds spent on the task, the label, and the workers ID. 3.5.2 Types of ROS labels and descriptions Positive: The context suggests successful reproducibility or replicability, such as the reuse of data, code, or concepts from the cited paper. It may include terms such as reproduce, replicate, or repeat the experiments, or references to the software or processes from the cited paper being used for pre-processing or comparison. Negative: The context hints about irreproducibility or irreplicability, such as the unavailability of the cited papers data or code, or unsuccessful attempts to obtain the results. Neutral: The context simply mentions (cites) paper without providing any hints about reproducibility. These contexts lack any indication of attempts to run the implementation or verify the results. 3.5.3 Worker selection mechanism To ensure collecting high-quality labels, we implemented an iterative worker selection mechanism using pilot dataset of 20 citation contexts for which we had ground truth labels, by in-house domain experts. This pilot dataset included 10 actual citation contexts from our finalized dataset and 10 dummy citation contexts, the labels of which could easily be determined. 2https://github.com/lamps-lab/CC30k/blob/main/appendix.pdf 3https://github.com/lamps-lab/CC30k/blob/main/notebooks/R001_AWS_Labelling_Dataset_Preprocessing_ Mturk.ipynb 6 CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis Figure 3: Crowdsourcing task interface using Crowd HTML elements. In the worker selection stage, we posted the pilot dataset as separate task, requiring at least 10 workers to participate. To select more reliable workers, we restricted participation to Mechanical Turk Masters [37], program under MTurk for selecting specialized group of workers. To build large pool of qualified workers, we repeatedly posted the pilot dataset in multiple batches, allowing only new workers who had not annotated in previous batches to annotate new batch. Through this iterative process, we built pool of 138 workers. We evaluated the workers responses against our ground truth labels and selected 16 workers who: labeled at least 15 citation contexts from the pilot dataset; achieved an accuracy of over 90% in their answers; and correctly labeled all the dummy citation contexts. We designed graphic interface for the labeling task shown in Fig. 3 using the Mechanical Turk Crowd HTML Elements. 3.6 Augmenting Negative Citation Contexts During the crowdsourcing stage, we identified severe imbalance in the distribution of ROS labels, with negative contexts representing only 1% of the dataset  (Table 6)  , due to the scarcity of explicit negative citation contexts. To address this issue, we applied an AI-assisted approach to add more negative citation contexts to balance the positive ROS labels. Using S2GA, we first collected 692,604 citation contexts from 21,757 computer science papers published after 2017, consistent with the lower bound year used in our crowdsourced set. We then classified these contexts using an ensemble 7 model consisting of 5 transformer models (SPECTER [38], SciBERT [39], DistilBERT [40], BioBERT [41], and BlueBERT [42]), each fine-tuned using the crowdsourcing labeled data. Before classifying the 600k contexts, we evaluated4 all models, including the ensemble, on the test set reported in Table 8. The ensemble model achieved weighted average F1 score of 0.81 and produced 43,790 negative candidates. R.R. Obadage et al., 2025 Figure 4: The pipeline of augmenting negative citation contexts; AML: augmented and machine labeled, AHV: augmented and human-validated. From these, we manually verified random sample of 5,578 contexts, and ultimately identified 1,055 negative citation contexts. However, this is still insufficient to balance the positive labels. From the remaining verified 4,523 non-negative contexts, we randomly selected 1,055 as additional ground truth samples. Using this data (a total of 2,110 samples) we fine-tuned several binary classifiers, including SciBERT, RoBERTa, DistilBERT, DeBERTa, and GPT-4o in zero-shot and few-shot settings, to identify actual negatives. RoBERTa achieved the highest F1 score of 0.67, and we used this model as verification step to filter negative candidates with high confidence. Specifically, we fine-tuned RoBERTa [43, 44] binary classifier (negative vs. non-negative) and applied the model to the remaining 38,212 candidate negatives, resulting in 3,850 high-confidence (>0.99) candidates. Combined with the manually verified subset, this process yielded 4,905 new negative contexts  (Fig. 4)  , which were added to the CC30k dataset. We also introduced metadata field, label_type, which indicates the origin of the label. label may be obtained through crowdsourcing (crowdsourced), through human validation of negatively augmented examples (augmented_human_validated), or through automatic assignment via the augmentation pipeline without human validation (augmented_machine_labeled). The statistics of the final dataset is presented in Table 6, and the metadata schema is shown in Table 5."
        },
        {
            "title": "4 Data Records",
            "content": "The CC30k dataset is in structured format to facilitate easy use for analysis and modeling. The dataset is stored as single CSV file, with each row representing unique citation context and its labels and associated metadata of the cited and the citing paper. The GitHub repository5 includes README file with detailed instructions on how to use the dataset and description of its structure. The dataset contains CSV file with 37 columns described in the Table 5. We report the average word count and the average number of sentences per context, for each sentiment category  (Table 6)  . Fig. 6 shows these distributions. 4.1 Labeling Outcomes Fig. 5 shows the distribution of citation contexts and citing papers across cited papers, along with the fraction of positive and negative citation contexts for crowdsourced portion. The subfigures in Fig. 5 reveal highly skewed distribution across the three dimensions. In Fig. 5(a), the majority of cited papers are cited in no more than 100 citation contexts, while few (2.36%) are cited in more than 2000 contexts, which are likely to be works that impact large number of 4https://github.com/lamps-lab/CC30k/blob/main/notebooks/R001_Extend_CC25k_Dataset.ipynb 5https://github.com/lamps-lab/CC30k/blob/main/README.md CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis Table 5: The fields in the CC30k dataset. Metadata of citing and cited papers are collected from S2GA. Column Name input_index input_context input_file_key input_first_author worker_id_w1 work_time_in_seconds_w1 worker_id_w2 work_time_in_seconds_w2 worker_id_w3 work_time_in_seconds_w3 label_w1 label_w2 label_w3 batch majority_vote majority_agreement rs_doi rs_title rs_authors rs_year rs_venue rs_selected_claims rs_reproduced_claims reproducibility_label org_doi org_title org_authors org_year org_venue org_paper_url org_citations citing_doi citing_year citing_venue citing_title citing_authors label_type Description Unique ID for each citation context. Citation context that workers are asked to label. Identifier linking the context to rep-study. Name or identifier of the first author of the cited paper. Unique ID of the first worker who labeled this citation context. Time (in seconds) the first worker took to label the citation context. Unique ID of the second worker who labeled this citation context. Time (in seconds) the second worker took to label the citation context. Unique ID of the third worker who labeled this citation context. Time (in seconds) the third worker took to label the citation context. Label assigned by the first worker. Label assigned by the second worker. Label assigned by the third worker. Batch number for the posted Mechanical Turk job. Final label based on the majority vote among workers labels (reproducibilityoriented sentiment). Indicates how many of the three workers agreed on the final majority vote. Digital Object Identifier (DOI) of the reproducibility study paper. Title of the reproducibility study paper. List of authors of the reproducibility study paper. Publication year of the reproducibility study paper. Venue (conference or journal) where the reproducibility study was published. Number of claims selected from the original paper for reproducibility study (by manual inspection). Number of selected claims that were successfully reproduced (by manual inspection). Reproducibility label assigned to the original paper based on the number of rs_reproduced_claims (reproducible, not-reproducible, partiallyreproducible [if 0 < rs_reproduced_claims < rs_selected_claims] ). DOI of the original (cited) paper that was assessed for reproducibility. Title of the original (cited) paper. List of authors of the original (cited) paper. Publication year of the original (cited) paper. Venue where the original (cited) paper was published. URL to access the original (cited) paper. Number of citations received by the original (cited) paper (collected by the date 2024/02/15). DOI of the citing paper that cited the original (cited) paper. Publication year of the citing paper. Venue where the citing paper was published. Title of the citing paper. List of authors of the citing paper. label augmented_machine_labeled crowdsourced or augmented_human_validated or source: papers. similar trend is evident in Fig. 5(b). Fig. 5(c) shows that the majority of cited papers receive less than 5% negative ROS citations, whereas positive citation contexts exhibit wider distribution. R.R. Obadage et al., 2025 Figure 5: Distribution of citation contexts, citing papers, and sentiment proportions across cited papers (crowdsourced portion): (a) Citation context count distribution across cited papers, (b) Citing paper count distribution across cited papers, (c) Distribution of the fraction of positive and negative citation contexts among all citation contexts in paper. Table 6: Summary Statistics of CC30k Dataset after Negative Augmentation (30,734 Contexts) C:Crowdsourced, AHV: Augmented and Human-Validated, AML: Augmented and Machine Labeled Category Positive (C) Neutral (C) Negative (C) Negative (AHV) Negative (AML) Total # Contexts 5,102 20,448 279 1,055 3,850 30, % Avg. Words Avg. Sent. 16.63% 66.65% 0.91% 3.26% 12.55% 100% 36.55 36.03 35.84 38.89 39.06 36.59 1.33 1.29 1.20 1.22 1.26 1."
        },
        {
            "title": "5 Data Quality Assessment",
            "content": "Table 6 shows that the final dataset contains crowdsourced data, augmented and human-validated (AHV), and augmented and machine-labeled (AML). The AHV data are human-validated and used as gold standard to train/evaluate supervised models. The AML portion only contains citation contexts with negative ROS labels. Here, we focus on assessing the quality of the crowdsourced data by manually validating the labels using subset containing 1% of samples selected from the crowdsourced portion. To avoid selection bias due to the imbalanced distribution across three ROS labels, we adopted stratified sampling approach, in which we selected 10 majority-voted citation contexts from each batch of 1,000 citation contexts with 4 positive, 3 negative, and 3 neutral labels. Specifically, we randomly sampled 3 Neutral 10 CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis Figure 6: Word count per context distribution for different categories: Neutral, Positive, and Negative (based on the entire CC30k dataset). Table 7: Macro average performance metrics of five popular open-source sentiment classification models available on HuggingFace (3-class) compared to ground truth from MTurk labels (support: Negative = 279, Neutral = 20,448, Positive = 5,102; total = 25,829). Model RoBERTa-based model [45] BERT-based model [46] BERTweet [47] BERT AutoTrain [48] BERT sbcBI [49] Precision Recall 0.44 0.37 0.48 0.50 0.53 0.35 0.36 0.38 0.36 0.34 F1-Score 0.37 0.36 0.40 0.37 0.30 contexts per batch (2 with majority voting agreement of 2/3 and 1 with an agreement of 3/3), 4 Positive contexts per batch (2 with agreement of 2/3 and 2 with agreement of 3/3), and 3 Negative contexts per batch (2 with agreement of 2/3 and 1 with agreement of 3/3). This results in subset of 244 citation contexts. Table 8 provides summary of the comparison between the manually-validated and the majority-voted labels. Table 8: Comparison of ground truth labels with Mechanical Turk annotations for the selected sample Label Category Manually-validated Majority-voted 55 Negative 102 Positive 87 Neutral 244 Total 55 99 75 229 The discrepancy between the two total numbers is due to 15 mislabels, as determined by majority voting, resulting in an overall accuracy of 93.85% for the crowdsourced data. All the mislabeling was limited to scenarios with the majority voting agreement of 2/3. No mislabels were found in cases where all three workers agreed on label (3/3 11 R.R. Obadage et al., 2025 agreement). To further assess the quality of the crowdsourced labels, we calculated the precision, recall, and F1-score for each label category (see Table 9). The macroand weighted-average F1-scores are both 94%, indicating high quality of the crowdsource data. The overall percent agreement among annotators was 99.35%. We also computed Krippendorffs α (ordinal) to quantify inter-annotator reliability. According to the Landis & Koch [50] scale, α = 0.29 would be interpreted as fair agreement. However, this lower α value is primarily due to the extreme skew in label distribution, with the majority of items labeled as Neutral. Table 9: Evaluation metrics for annotation quality Label Category Negative Neutral Positive Macro Avg. Weighted Avg. Precision Recall 1.00 0.86 0.97 0.94 0.94 0.89 0.96 0.95 0.93 0.94 F1-Score 0.94 0.91 0.96 0.94 0.94 Support 55 87 102"
        },
        {
            "title": "6 Potential Uses of the Dataset",
            "content": "6.1 Baseline performance of off-the-shelf sentiment classifiers Using the labeled citation contexts, we first evaluate the performance of five open-source sentiment classification models on HuggingFace [51]. Performance measures in Table 7, reveal that these models perform poorly when evaluated on the crowdsourced portion of citation contexts in our dataset. The macro-averaged F1-scores for all models are below 0.41. The results highlight the limitations of off-the-shelf sentiment models in addressing complex and domain-specific tasks. Our CC30k dataset fills this gap by providing specialized resource tailored to ROS predictions. 6.2 Fine-tuning large language models To showcase the practical utility of CC30k for downstream NLP applications, we conducted series of experiments in which we fine-tuned base LLMs on our dataset for the task of ROS classification. All models are evaluated using the same test set containing 244 samples with ground truth labels described in section 5. We selected two open-source LLMs, LLaMA-3-8B6 and Qwen-1.5-7B7, for fine-tuning, and one commercial LLM, GPT-4o, for Retrieval-Augmented Generation (RAG). We evaluated eight scenarios for each LLM, including two direct inference (base model) in zero-shot and few-shot (n=5 i.e., five randomly selected examples per category, 15 total) prompting methods, and six fine-tuned models with three training set sizes (3k, 9k, 15k) under both zero-shot and few-shot prompting methods. For example, 3k means we selected 3,000 citation contexts with labels from CC30k in total, with 1,000 from each ROS category. Across models we observe that fine-tuning on CC30k yields consistent and substantial gains over base direct inference in most settings  (Fig. 7)  . The magnitude of the gains depends on the base model, the prompting method, and the fine-tuning data size. Here, we summarize performance gains in the form of weighted average F1-scores for each LLM. Qwen1.57B: In the base setting, the zero-shot score was 0.436, while the few-shot score reached 0.539. Fine-tuning with few-shot prompting improved performance to 0.625 with 3k training samples and 0.695 with 9k training samples, but the score dropped to 0.428 with 15k training samples. The best observed F1-score was 0.695, achieved with 9k training samples under few-shot prompting. LLaMA 38B: Fine-tuning with few-shot prompting achieved 0.671 with 3k training samples, which is the highest F1-score, followed by 0.658 with 9k training samples, with lower scores observed with 15k training samples. GPT-4o (with RAG): The F1 score peaked at 0.786 with 3k training samples under zero-shot prompting. The second best F1-score is 0.767 with 9k training samples under few-shot prompting. In summary, fine-tuning on CC30k generally improves performance relative to direct base model inference by 5% to 27%. For Qwen and LLaMA, the most evident improvements appear when fine-tuning is combined with few-shot prompting (e.g., Qwen: 0.539 0.695; LLaMA: 0.549 0.671). . This demonstrates that parameter updates informed by CC30k plus small number of in-context examples can be complementary. For GPT-4o, however, RAG fine-tuning 6https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct 7https://huggingface.co/Qwen/Qwen1.5-7B CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis Figure 7: Reproducibility-Oriented Sentiment Classification Performance of Fine-Tuned LLMs with Varying Training Sample Sizes from the CC30k Dataset. 3k\" indicates 3,000 citation contexts selected from CC30k, with 1,000 from each ROS category. Dashed lines connect few-shot inference scenarios to show model performance trends across increasing fine-tuning data sizes (e.g., the red dashed line tracks Qwen performance in the few-shot settings). produced very large gains even with zero-shot inference (0.514 0.786 at 3k RAG). These observations indicate that CC30k is effective for improving the LLMs performance on ROS classification. However, the best setting is model-dependent, and our dataset provides sufficient number of samples for fine-tuning. In many cases, few-shot prompting improves performance over zero-shot, consistent with prior findings that in-context examples help guide model outputs by conditioning model behavior on task-specific demonstrations [52, 53]. However, when the model is equipped with strong external context via RAG, zero-shot inference can match or even exceed few-shot performance (the GPT-4o 3k RAG zero-shot case is an example: 0.786 zero-shot vs. 0.703 few-shot). An explanation is that retrieval surfaces precise supporting passages that eliminate the need for additional in-prompt examples. These phenomena are aligned with several empirical and theoretical analyses of in-context learning and with the benefits of retrieval-augmented approaches [54, 55]. We note that the performance improvements are not strictly monotonic with more fine-tuning data for every model. For example, Qwens few-shot F1-score increases from 3k to 9k but then drops at 15k; LLaMA shows similar pattern. One plausible contributor is label noise in our training contexts: crowdsourced annotations contain noise due to the ambiguity in the text, subtle inter-annotator differences, and overlapping categories, and label noise is known to cause unstable fine-tuning patterns and occasional degradation as training sample sizes grow if noise is not explicitly addressed. Thus, the non-monotonic behavior observed can be explained primarily by the model sensitivity to noisy supervision during parameter updates. For GPT-4o, which we could only use in retrieval-augmented generation (RAG) setup due to the model weights not being publicly available, additional interactions between retrieval and in-context signals with imperfect labels may also contribute. Survey and empirical work [56, 57] on learning with label noise supports this interpretation. 6.3 Other Usages In addition to improving the performance of LLMs via fine-tuning, the citation contexts in the CC30k dataset can be further linked to citing and cited papers, putting ROS-labeled citation contexts in the citation graph. CC30k can then be used to study patterns in citation practices in AI and potentially other disciplines. Such analyses can reveal disciplinary differences in reproducibility practices and concerns, and provide insights into how reproducibility is discussed and referenced in scholarly publications. Because certain citation contexts may contain explanations of why cited paper is not reproducible/replicable, CC30k can support identifying key factors influencing AI reproducibility/replicability. This can provide researchers and policymakers with actionable insights into improving reproducibility in science. In particular, because certain citation contexts mention data and software availability/accessibility, CC30k can then be used for reproducibility-aware data and/or software recommendation [58]. 13 R.R. Obadage et al.,"
        },
        {
            "title": "7 Conclusion",
            "content": "In conclusion, the CC30k dataset is built to address the challenges of reproducibility-oriented sentiment analysis within scientific literature in AI. By providing carefully labeled, reproducibility-oriented, high-quality dataset of 30,734 citation contexts, this work fills critical gap in resources for studying reproducibility and replicability in AI research. The creation of the dataset, supported by robust crowdsourcing pipeline and thorough validation, ensures its reliability and utility for potentially impactful applications. Our evaluation of existing sentiment analysis models highlights the limitations of generic approaches in this domain, underscoring the need for specialized datasets such as CC30k. By making the dataset publicly available, we encourage further exploration and refinement of ROS analysis, promoting trustworthiness in scholarly communications."
        },
        {
            "title": "References",
            "content": "[1] Engineering National Academies of Sciences, Medicine, et al. Reproducibility and replicability in science. 2019. [2] Association for Computing Machinery. Artifact Review and Badging - Current, 2020. Accessed 02-19-2025. [3] Christian Collberg and Todd A. Proebsting. Repeatability in computer systems research. Commun. ACM, 59(3):6269, February 2016. [4] Alexandre Hocquet and Frédéric Wieber. Epistemic issues in computational reproducibility: software as the elephant in the room. European Journal for Philosophy of Science, 11(2):38, April 2021. [5] Benjamin Antunes and David R.C. Hill. Reproducibility, replicability and repeatability: survey of reproducible research with focus on high performance computing. Computer Science Review, 53, August 2024. [6] Edward Raff. step toward quantifying independently reproducible machine learning research. In Advances in Neural Information Processing Systems, volume 32, 2019. [7] Edward Raff, Michel Benaroch, Sagar Samtani, and Andrew L. Farris. What do machine learning researchers mean by \"reproducible\"?, 2024. [8] Odd Erik Gundersen and Sigbjørn Kjensmo. State of the art: Reproducibility in artificial intelligence. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. [9] Odd Erik Gundersen, Malte Helmert, and Holger Hoos. Improving reproducibility in ai research: Four mechanisms adopted by jair. J. Artif. Int. Res., 81, January 2025. [10] Rochana R. Obadage, Sarah M. Rajtmajer, and Jian Wu. Short: Can citations tell us about papers reproducibility? case study of machine learning papers, 2024. [11] Kevin C. Elliott. taxonomy of transparency in science. Canadian Journal of Philosophy, 52(3):342355, 2022. [12] Amazon Mechanical Turk. Amazon Mechanical Turk. https://www.mturk.com/, 2005. [Accessed 09-012024]. [13] Maas, Andrew L. et al. Learning word vectors for sentiment analysis, June 2011. [14] Sara Rosenthal, Noura Farra, and Preslav Nakov. SemEval-2017 task 4: Sentiment analysis in Twitter. pages 502518, August 2017. [15] Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 04, page 168177, 2004. [16] Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng. Why we twitter: understanding microblogging usage and communities. In Proceedings of the 9th WebKDD and 1st SNA-KDD 2007 Workshop on Web Mining and Social Network Analysis, WebKDD/SNA-KDD 07, page 5665, 2007. [17] Meta AI Research. Papers with Code - Machine Learning Datasets. https://paperswithcode.com/ datasets?mod=texts&page=1, 2019. [Accessed 02-13-2025]. [18] David Jurgens, Srijan Kumar, Raine Hoover, Dan McFarland, and Dan Jurafsky. Measuring the evolution of scientific field through citation frames. Transactions of the Association for Computational Linguistics, 6:391406, 07 2018. [19] Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. Structural scaffolds for citation intent classification in scientific publications, 2019. [20] Frédérique Bordignon, Philippe Gambette, and Karla Avanço. Corpus of critical citations contexts, February 2024. [21] Kaggle. Twitter US Airline Sentiment kaggle.com. https://www.kaggle.com/datasets/crowdflower/ twitter-airline-sentiment, 2015. [Accessed 09-01-2025]. 14 CC30k: Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis [22] Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. CS224N project report, Stanford, 1(12):2009, 2009. [23] Pampa Saha Ghosh and Souvik Sengupta. Sentigrad: new hindi-english code mixed sentiment analysis dataset with preliminary results and open challenges. In 2024 DABCon, pages 15, 2024. [24] Charles Jochim and Hinrich Schütze. Towards generic and flexible citation classifier based on faceted classification scheme. In Proceedings of COLING 2012, pages 13431358, December 2012. [25] Awais Athar and Simone Teufel. Context-enhanced citation sentiment detection. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597601, June 2012. [26] Suchetha N. Kunnath, David Pride, Drahomira Herrmannova, and Petr Knoth. 3c shared task (2021). https: //kaggle.com/competitions/3c-shared-task-influence-v2, 2021. Kaggle. [27] Myriam Hernández-Alvarez, José M. Gomez Soriano, and Patricio Martínez-Barco. Citation function, polarity and influence classification. Natural Language Engineering, 23(4):561588, 2017. [28] Shuyang Cao and Lu Wang. Verifiable generation with subsentence-level fine-grained citations, 2024. [29] Anand, Avinash et al. Context-Enhanced Language Models for Generating Multi-paper Citations, page 8094. 2023. [30] Chenrui Guo, Haoran Cui, Li Zhang, Jiamin Wang, Wei Lu, and Jian Wu. SmartCiteCon: Implicit citation context extraction from academic literature using supervised learning. In Proceedings of the 8th International Workshop on Mining Scientific Publications, pages 2126, 05 August 2020. [31] Akhil Pandey Akella, David Koop, and Hamed Alhoori. Laying foundations to quantify the \"effort of reproducibility\". In Proceedings of the 2023 ACM/IEEE Joint Conference on Digital Libraries, JCDL 23, page 5660, 2024. [32] Kinney, Rodney Michael et al. The semantic scholar open data platform, 2023. [33] Koustuv Sinha. ML Reproducibility Challenge 2023 MLRC2023 reproml.org. https://reproml.org/, 2023. [Accessed 03-02-2024]. [34] RESCIENCE C. ReScience C. https://rescience.github.io/read/, 2023. [Accessed 10-02-2024]. [35] Kehinde Ajayi, Muntabir Hasan Choudhury, Sarah M. Rajtmajer, and Jian Wu. study on reproducibility and replicability of table structure recognition methods, 2023. [36] Deepank Gupta, Bob Morris, Terry Catapano, and Guido Sautter. new approach towards bibliographic reference identification, parsing and inline citation matching. In Contemporary Computing, pages 93102, 2009. [37] Amazon Mechanical Turk. Simplified Masters Qualifications. https://blog.mturk.com/ simplified-masters-qualifications-137d77647d1c, 2015. [Accessed Dec 31, 024]. [38] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel Weld. SPECTER: Document-level representation learning using citation-informed transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 22702282, Online, July 2020. [39] Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: pretrained language model for scientific text. In EMNLPIJCNLP, pages 36153620, November 2019. [40] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter, 2020. [41] Lee, Jinhyuk et al. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240, 09 2019. [42] Yifan Peng, Shankai Yan, and Zhiyong Lu. Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on ten benchmarking datasets. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 5865, August 2019. [43] Liu, Yinhan et al. Roberta: robustly optimized bert pretraining approach, 2019. [44] Brian Cheang, Bailey Wei, David Kogan, Howey Qiu, and Masud Ahmed. Language representation models for fine-grained sentiment classification, 2020. [45] Jochen Hartmann, Mark Heitmann, Christina Schamp, and Oded Netzer. The power of brand selfies. Journal of Marketing Research, 2021. [46] Seethal. Seethal/sentiment-analysis-generic-dataset Hugging Face. https://huggingface.co/Seethal/ sentiment_analysis_generic_dataset, 2022. [Accessed 10-02-2024]. R.R. Obadage et al., 2025 [47] Juan Manuel Pérez, Juan Carlos Giudici, and Franco Luque. pysentimiento: python toolkit for sentiment analysis and socialnlp tasks, 2021. [48] Souvick Das. Souvikcmsa/BERT-sentiment-analysis Hugging Face huggingface.co. https://huggingface. co/Souvikcmsa/BERT_sentiment_analysis, 2022. [Accessed 10-02-2024]. [49] BI team. sbcBI/sentiment-analysis-model Hugging Face huggingface.co. https://huggingface.co/ sbcBI/sentiment_analysis_model, 2022. [Accessed 10-02-2024]. [50] Landis and Koch. The measurement of observer agreement for categorical data. Biometrics, 33(1):159 174, March 1977. [51] Hugging Face Team. Models - Hugging Face huggingface.co. https://huggingface.co/models, 2024. [Accessed 01-16-2025]. [52] Brown, Tom et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc., 2020. [53] Lampinen, Andrew et al. Can language models learn from explanations in context? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 537563, December 2022. [54] Liu, Jiachang et al. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100114, May 2022. [55] Gao, Yunfan et al. Retrieval-augmented generation for large language models: survey, 2024. [56] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: survey. IEEE Trans Neural Netw Learn Syst, 34(11):81358153, October 2023. [57] Xuefeng Liang, Xingyu Liu, and Longshan Yao. Reviewa survey of learning from noisy labels. ECS Sensors Plus, 1(2):021401, jun 2022. [58] Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee Giles. Context-aware citation recommendation. In Proceedings of the 19th International Conference on World Wide Web, WWW 10, page 421430, 2010."
        }
    ],
    "affiliations": [
        "Old Dominion University",
        "The Pennsylvania State University"
    ]
}