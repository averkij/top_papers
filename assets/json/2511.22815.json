{
    "paper_title": "Captain Safari: A World Engine",
    "authors": [
        "Yu-Cheng Chou",
        "Xingrui Wang",
        "Yitong Li",
        "Jiahao Wang",
        "Hanting Liu",
        "Cihang Xie",
        "Alan Yuille",
        "Junfei Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of a scene under user-controlled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, a pose-conditioned world engine that generates videos by retrieving from a persistent world memory. Given a camera path, our method maintains a dynamic local memory and uses a retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, a new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through a multi-stage geometric and kinematic validation pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in a 50-participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is a powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as a challenging new benchmark for future world-engine research."
        },
        {
            "title": "Start",
            "content": "Captain Safari: World Engine Yu-Cheng Chou1 Xingrui Wang1 Yitong Li2 Cihang Xie3 Alan Yuille1 Jiahao Wang1 Hanting Liu1 Junfei Xiao1(cid:66) 1Johns Hopkins University 2Tsinghua University 3UC Santa Cruz https://johnson111788.github.io/open-safari/ 5 2 0 2 8 2 ] . [ 1 5 1 8 2 2 . 1 1 5 2 : r Figure 1. Captain Safari is pose-aware world engine that generates long-horizon, 3D-consistent FPV videos from any user-specified camera trajectory. By retrieving pose-aligned world memory, it keeps geometry stable across large viewpoint changes and reconstructs crisp, well-formed structures while faithfully tracking aggressive 6-DoF motion."
        },
        {
            "title": "Abstract",
            "content": "World engines aim to synthesize long, 3D-consistent videos that support interactive exploration of scene under usercontrolled camera motion. However, existing systems struggle under aggressive 6-DoF trajectories and complex outdoor layouts: they lose long-range geometric coherence, deviate from the target path, or collapse into overly conservative motion. To this end, we introduce Captain Safari, pose-conditioned world engine that generates videos by retrieving from persistent world memory. Given camera path, our method maintains dynamic local memory and uses retriever to fetch pose-aligned world tokens, which then condition video generation along the trajectory. This design enables the model to maintain stable 3D structure while accurately executing challenging camera maneuvers. To evaluate this setting, we curate OpenSafari, new in-the-wild FPV dataset containing high-dynamic drone videos with verified camera trajectories, constructed through multi-stage geometric and kinematic validation (cid:66) Corresponding author: Junfei Xiao (xiaojf97@gmail.com) Preprint, work in progress. pipeline. Across video quality, 3D consistency, and trajectory following, Captain Safari substantially outperforms state-of-the-art camera-controlled generators. It reduces MEt3R from 0.3703 to 0.3690, improves AUC@30 from 0.181 to 0.200, and yields substantially lower FVD than all camera-controlled baselines. More importantly, in 50participant, 5-way human study where annotators select the best result among five anonymized models, 67.6% of preferences favor our method across all axes. Our results demonstrate that pose-conditioned world memory is powerful mechanism for long-horizon, controllable video generation and provide OpenSafari as challenging new benchmark for future world-engine research. 1. Introduction Simulating coherent 3D worlds through controllable video generation has long been foundational challenge for augmented reality, embodied AI, and virtual agents [810, 13 16, 19, 20, 23, 26, 29, 40, 4345, 50, 51, 57]. Classical game engines and physics simulators offer explicit geometry and precise control, but require heavy manual authoring and expensive computation [7, 28, 32]. Moreover, while they may achieve visual realism in specialized domains, they still fall short in capturing the richness and diversity characteristic of real world, such as natural scenes [27, 35, 58]. In contrast, modern video diffusion models synthesize high-fidelity, diverse videos from text or images, yet typically operate as feed-forward clip generators without persistent world state: they struggle with longrange 3D consistency, complex trajectory following, and faithful reconstruction of diverse scenes [18, 24]. In this work, we move toward bridging this gap with Captain Safari, world engine that enables pose-conditioned modeling of 3D-consistent and diverse environments, surpassing the limitations of traditional game engines in terms of generality, diversity, and interactivity. Contemporary video world models face three intertwined challenges. First, long-horizon consistency is limited by the temporal window of context frames; models often forget distant scenery or violate spatial coherence, leading to abrupt appearance changes that break the realism and continuity of the generated environment [8, 14, 45]. Second, achieving complex camera maneuvers under strict 3D consistency remains difficult: existing poseor trajectory-conditioned methods typically work well only for slow, near-forward motions [16, 34, 49]. When the path involves fast 6-DoF movement, strong parallax, or sharp turns, models exhibit trade-offeither dampening motion and restricting viewpoint changes to preserve geometry, or committing to the requested path at the cost of distortions, flicker, and structural drift. Third, current approaches underrepresent complex outdoor layouts. Much of the works focuses on structured, constrained settings (e.g., indoor tours, driving scenes, or real-estate videos), and models are seldom stress-tested in in-the-wild FPV scenarios where the camera weaves around buildings, vegetation, and varied terrain with substantial parallax [6, 22, 59, 60]. As result, methods that look competitive in simplified environments often fail to preserve geometry and appearance when confronted with truly diverse, complex outdoor scenes. To address these issues, we introduce Captain Safari, pose-aware world engine that explicitly maintains persistent notion of world state to uphold long-horizon 3D consistency across strong parallax. Because storing and propagating full long-term state is computationally prohibitive, we develop retrieval mechanism that selects and aggregates only the most informative scene cues, thereby providing strong geometric guidance without incurring prohibitive cost. Crucially, this retrieval is pose-aware: given the target camera pose, it assembles pose-aligned world prior that steers the generation process, enabling accurate tracking of aggressive camera maneuvers while preserving 3Dconsistent structure in complex environments. Furthermore, to close the gap in complex outdoor layouts and aggressive camera motion, we curate OpenSafari, large-scale dataset of high-dynamic FPV drone videos with verified camera poses. Much of the literature targets structured, constrained settings (e.g., indoor tours, driving or real-estate videos), and even outdoor datasets typically In contrast, OpenSafeature slow, near-forward motion. fari comprises in-the-wild FPV flights that weave around buildings and vegetation across uneven terrain, exhibiting large parallax, rapid 6-DoF maneuvers, and sharp viewpoint changes. Paired with verified camera trajectories, these videos present diverse, cluttered outdoor scenes and longrange motion, challenging models to maintain 3D consistency while faithfully tracking complex maneuvers. We evaluate Captain Safari along three axes: video quality, 3D consistency, and trajectory following. Across these criteria, our method consistently outperforms contemporary camera-controlled video generators on OpenSafari: Table 1 reports clear gains in 3D consistency and accurate tracking under complex maneuvers, while maintaining strong perceptual quality. Importantly, large-scale human study  (Table 2)  shows that Captain Safari receives 67% of votes in five-way comparisons, indicating that the improvements are perceptually salient. Qualitative comparisons (Fig. 4 and Fig. 5) further demonstrate stable geometry under longrange path and faithful adherence to sharp 6-DoF camera turns in cluttered outdoor scenes. In summary, our contributions are: 1. We present Captain Safari, the first camera-controlled video generation method to enforce long-horizon 3D consistency while tracking aggressive FPV maneuvers. 2. We propose pose-guided, long-horizon retrieval that efficiently reconciles strict 3D consistency with accurate tracking of complex maneuvers. 3. We curate OpenSafari, large-scale in-the-wild FPV dataset with verified camera poses, featuring diverse, cluttered outdoor scenes and rapid 6-DoF motion that stress-test geometry-consistent camera control. 4. In OpenSafari, our pose-aware retrieval notably improves video quality, 3D consistency, and trajectory alignment, also achieving 67% human preference rate. 2. Related Work 2.1. 3D-Consistent World Models Early image-to-3D approaches reconstruct geometry indirectly via multi-view consistency or implicit fields, but often fail to maintain coherent structure across large view changes [13, 21, 43, 50]. Recent efforts integrate 3D reasoning into the generative process. DiffusionGS [5] injects Gaussian Splatting into the diffusion denoiser, enforcing view consistency and enabling single-stage, scalable 3D generation. GenEx [23] and EvoWorld [40] extends this idea from static reconstruction to dynamic world creation, generating explorable 360 panoramic environments 2 Figure 2. Method overview. Captain Safari builds local world memory and, given query camera pose, retrieves pose-aligned tokens that summarize the scene. These tokens then condition video generation along the user-specified trajectory, preserving stable 3D layout. grounded in physical priors. Complementary to these generative reconstructions, Geometry Forcing [44] and Memory Forcing [14] explicitly couple training signals with geometric supervision and spatio-temporal memory, ensuring consistency during long rollouts. Meanwhile, open-world models such as Wonderland [20], WonderWorld [51], WonderTurbo [26], and EvoWorld [40] further integrate geometryindexed or adaptive memories to maintain persistent world states across interactions. However, these approaches still use implicit, clip-bound memories, whereas we introduce an explicit pose-indexed world memory retrieved on demand for camera-controlled generation. 2.2. Camera Controlled Video Generation Early T2V/I2V models learned camera motion implicitly and struggle to reliably repeat explicit trajectories [11, 42, 61]. Recent work such as CameraCtrl [10] treats camera parameters as explicit conditions, encoding camera extrinsics and trajectories or enforcing path constraintsto improve controllability and accuracy [2, 25, 41, 47, 55]. MotionPrompting [9] implement compositional control by pointtrack conditioning and MotionPro [56] use path-alignment losses that lower rotational and translational error; trainingfree control is also achieved by fitting lightweight pointcloud and using noise-layout prior to steer denoising [11]. Scene-preserving geometric priors further strengthen cliplevel consistency. Cami2V[57] treats camera pose as physical prior and exploits epipolar and multiview constraints; RealCam-I2V [19] recovers metric depth with DepthAnything v2 [48] to reconstruct scale-stable scene; PoseTraj[16] employs pose-aware pretraining to obtain rotation-aligned motion. Compared with parameter-only conditioning, these priors reduce within-clip layout drift and better preserve local geometry under view changes. Further, recent work links camera control with world modeling. CVD [17], Cavia [46], and WoVoGen [22] jointly synthesize multi-view and multi-trajectory videos from shared scene representation, enforcing cross-path consistency. Meanwhile, methods that conditioning from explicit renderable 3D representations (e.g., 3D Gaussians) can anchor geometry, improve cross-view 3D consistency and path adherence [15, 29, 33, 52, 53]. However, these approaches typically build one-off 3D scenes, whereas we unify long-horizon camera control with persistent poseindexed world memory shared across trajectories. 3. Captain Safari We introduce Captain Safari, memory-guided video generation framework. Sec. 3.1 presents an implicit world memory for stable scene representation, while Sec. 3.2 describes pose-conditioned retrieval system that maps camera views to world tokens, guiding DiT-based generator for coherent outputs along arbitrary trajectories. 3.1. Implicit Memory of World Geometry Problem setup. We represent video as = {It}T t=0, where It is the frame at time step t. On the same time axis we define camera poses = {(Rt, Tt)}T t=0 and obtain 3D-aware memory feature mt at each time step using pretrained geometry encoder. All memory features form global memory bank = {mt}T t=0. Given text prompt p, the camera poses C, and target 3 clip time step = [t0, t1], together with its associated local world memory Mlocal M, our goal is to synthesize video segment ˆVT that (i) aligns with p, (ii) respects the prescribed poses {(Rt, Tt)}tT , and (iii) maintains coherent 3D world across viewpoints. Local world memory. Directly conditioning on the full memory bank for every clip would be computationally expensive and dominated by temporally distant observations. Instead, for each target clip time step = [t0, t1] we define local memory Mlocal = {mτ τ [ks, ke]} whose endpoints are sampled under t0 ks t0, max(ks, t0)+1 ke min(ks+L, t1), (1) where is fixed bound and all time steps are integers. These constraints enforce that: (i) the memory window starts at most seconds before the clip entrance t0, tying it to nearby observations; (ii) its duration is at most L, which keeps the conditioning set compact; and (iii) its end time ke always touches or overlaps t0 while remaining within [t0, t1], ensuring that each clip is supported by temporally compatible world prior. All Mlocal are constructed as such dynamic clip-aligned window of the shared bank M, so neighboring clips naturally share overlapping memory entries, constraining computation while coupling their generations to 3D-consistent underlying world. Pose-retrieved memory. Within given clip time step , we treat the local memory Mlocal as static hypothesis of the surrounding world built from key frames. Each time step τ provides pose token pτ (derived from (Rτ , Tτ )) and set of 3D-aware memory tokens mτ,1, . . . , mτ,M . The collection {(pτ , mτ,1:M )}τ forms an implicit world table: pose token indicates where the camera has observed the scene, while memory tokens encode what the world looks like from those configurations. For any target time step , we derive its camera pose to query pose token pt, embed it as qt = ϕp(pt), and use dedicated retrieval module to read from this static table in pose-dependent manner. Concretely, qt is concatenated with bank of learnable query tokens and processed into retrieval queries, which perform cross-attention over the encoded memory mem (defined in Sec. 3.2), yielding set of world tokens wt = Agg (cid:16) (cid:17) CrossAttn(Qt, mem) . (2) corresponding to the updated learnable queries. These pose-aligned world tokens wt are directly used as the reconstructed memory at pose t. Thus, all frames in access local memory through pose-conditioned queries instead of raw time indices, encouraging multi-view observations to remain tied to consistent static 3D world. 3.2. Memory Retrieval and Conditioning Memory retriever design. As shown in Figure 2, given the local memory, we represent each time step τ by pose 4 token pτ and its associated memory tokens mτ,1:M . Our retriever is designed to (i) jointly encode posememory pairs into coherent world representation, and (ii) extract, for any query pose, compact set of pose-aligned tokens that summarize the most relevant parts of this local world."
        },
        {
            "title": "We first embed pose and memory features into a shared",
            "content": "space and form joint sequence per time step: ˆXτ = (cid:2)ϕp(pτ ), ϕm(mτ,1), . . . , ϕm(mτ,M )(cid:3), (3) where ϕp and ϕm denote learnable embeddings for pose and memory tokens, respectively. stack of transformer blocks (MemEnc) with 3D-aware positional encoding refines these sequences, (cid:1), Xτ = MemEnc(cid:0) ˆXτ and we obtain the encoded local world memory by concatenation mem = (cid:2) Xks , . . . , Xke optionally masked to exclude padded or non-key entries. (5) (4) (cid:3), For target time step t, we derive the query pose token pt, embed it as qt = ϕp(pt), and concatenate it with learnable query tokens r1, . . . , rM , ˆQt = [qt, r1, . . . , rM ]. (6) This sequence is refined by transformer blocks sharing the same architecture as MemEnc, denoted as QryEnc, yielding pose-aware retrieval queries Qt = QryEnc(cid:0) ˆQt (cid:1). (7) We then perform cross-attention from Qt to the encoded memory mem, Yt = Qt + CrossAttn(Qt, mem), (8) and take the subset of tokens in Yt corresponding to the learnable queries as the retrieved world tokens wt = [wt,1, . . . , wt,M ], (9) which form pose-aligned world feature for time t. During training, linear head maps wt back to the original memory space to reconstruct the target memory tokens at the query pose. Stacking multiple retrieval blocks iteratively refines both the queries and the retrieved tokens, enabling the model to softly route each query pose to the most relevant subset of past observations, instead of relying on rigid temporal neighborhood or single nearest frame. Memory-conditioned DiT. For given target clip time step , the retriever consumes Mlocal and the query pose pt and outputs pose-aligned set of world tokens wt RM dm , which summarize the static local world relevant to this segment. These tokens are mapped into the DiT hidden space by the memory embedding MLP WT = ϕw(wt) RM D. (10) Figure 3. OpenSafari. new in-the-wild FPV dataset with rigorously verified camera trajectories, designed to stress-test geometryconsistent, camera-controllable video generation. We curate clips through compact, multi-stage pipeline that filters, reconstructs, and verifies trajectories, yielding clean, motion-rich videos with reliable camera paths. The latent clip is encoded as single spatio-temporal token sequence RLzD, obtained by patchifying all frames in VT . At each DiT layer l, we first apply selfattention over the full sequence and then inject the world tokens through dedicated memory cross-attention: (l+1) = + CrossAttn(Z l, WT , WT ). (11) The clip-level world tokens WT are reused as keys and values across all layers, providing stable, 3D-consistent prior that shapes the denoising of every spatio-temporal token. 4. OpenSafari 4.1. Video Data Curation Existing camera-conditioned datasets do not match our target regime. RealEstate10K [59] focuses on slow, mostly indoor real-estate walkthroughs with gentle motion and clean, quasi-static scenes, while Minecraft [4] is synthetic voxel world with simplified geometry and engine-constrained dynamics. Neither captures aggressive, in-the-wild 6-DoF drone flight with strong parallax, large elevation changes, and complex outdoor layouts that truly stress long-horizon 3D consistency. We therefore propose OpenSafari, new dataset of real-world FPV-style drone videos with verified camera trajectories tailored to this challenging setting. We construct Safari-FPV from FPV-style drone videos collected on AirVuz1 and YouTube2, and retain only clips that pass strict multi-stage preprocessing pipeline. As shown in Figure 3, we: (i) download the highest available resolution for each URL and discard sources below the target resolution; (ii) normalize all videos to 720p, 24 fps, and fixed 16:9 center crop, removing letterboxing and black borders so that subsequent camera estimation operates on clean field of view; (iii) run scene detection to obtain single-shot segments; (iv) split segments into fixed-length videos via uniform temporal slicing. We then filter videos with single diagnostic based on motion. Specifically, we run RAFT [36] to estimate opticalflow magnitude; videos with too little motion are removed, while videos with stable, coherent motion are kept to emphasize informative, parallax-rich trajectories rather than static views. Only videos satisfying the motion constraint enter the final dataset. This yields large-scale, in-thewild drone corpus explicitly tailored to stress-test geometryaware, trajectory-following video generation. 4.2. Camera Trajectory Reconstruction For each curated video, we estimate camera intrinsics and extrinsics at 4 fps using Hierarchical Localization [30, 31]. We extract local features, build exhaustive image pairs within each video, run feature matching, and reconstruct COLMAP-style SfM model; from this model we export 1https://www.airvuz.com/ 2https://www.youtube.com/ 5 Table 1. Benchmark camera-controlled video generation. Captain Safari ranks first in 3D consistency and trajectory following with competitive video quality. Compared to the ablated variant without memory, Captain Safari substantially improves 3D consistency and trajectory following, with only slight trade-off in video quality. (Recon. = reconstruction rate. CosSim = cosine similarity.) Model Video Quality 3D consistency Trajectory Following FVD LPIPS MEt3R Recon. AUC@30 AUC@15 CosSim Geometry Forcing [44] Real-CamI2V [19] Wan2.2-5B-Control-Camera [38] Captain Safari w/o Mem. Captain Safari 2662.75 1585.61 1387.75 998.47 1023.46 0.667 0.513 0.545 0.504 0.512 0.4834 0.3703 0.3932 0.3720 0.3690 0.877 0.923 0.767 0.912 0.968 0.168 0.174 0.181 0.193 0. 0.056 0.051 0.054 0.068 0.068 0.429 0.296 0.420 0.508 0.563 Table 2. Human preference. Users overwhelmingly prefer Captain Safari across all criteria, capturing 67% of total votes. The memoryremoved variant ranks distant second, while baselines competitors receive single-digit preference. Model Video Quality 3D consistency Trajectory Following Average Geometry Forcing [44] Real-CamI2V [19] Wan2.2-5B-Control-Camera [38] Captain Safari w/o Mem. Captain Safari 0.20% 4.20% 3.20% 25.00% 67.40% 0.00% 6.40% 3.80% 24.20% 65.60% 0.20% 4.40% 6.40% 20.00% 69.00% 0.13% 5.00% 4.47% 23.07% 67.33% per-frame camera parameters as initial trajectories. To obtain deployment-ready data, we apply three-stage verification-and-fix pipeline to every reconstructed trajectory. First, database check consumes SfM statistics (inlier counts and ratios) to flag potentially unreliable transitions. Next, sgeometric check revisits suspicious pairs using stored keypoints and matches, recomputes essential matrices, and thresholds symmetric epipolar errors. Last, kinematics check analyzes the pose sequence for translation spikes, rotation jumps, forward-direction flips, and higher-order smoothness violations, using robust MADbased scores to detect implausible motion. The per-transition decisions are fused into binary badindex, which drives strict policy. If bad transitions are sparse and localized, we invoke targeted fix: we linearly interpolate camera centers and apply SLERP to rotations with capped interpolation angle, optionally extrapolating at video boundaries. The fixed segments are then revalidated by the same database/geometric/kinematics criteria. If post-fix validation succeeds, the trajectory is exported into the final dataset. If the bad-index is too dense, violations are too severe, or fixed trajectories still fail verification, the entire video is discarded. The resulting OpenSafari couples high-dynamic, in-thewild FPV drone video with rigorously verified camera trajectories. It departs from existing benchmarks by emphasizing aggressive 6-DoF motion, strong parallax, and complex outdoor layouts, while enforcing strict geometric and kinematic validation. This makes OpenSafari challenging testbed for camera-controllable video generation. 5. Experiments 5.1. Implementation Details Training recipe. We adopt two-stage recipe. We first warm up the pose-conditioned memory retriever using pose-aligned memory tokens mt. We then jointly train the retriever and DiT end-to-end, updating the DiT via LoRA [12]. Memory cross-attention is initialized from the corresponding context cross-attention weights, and other new layers use standard initialization. Dataset. We extract overlapping clips with 1 stride, yielding 51,997 training candidates. diversity-based trajectory filter removes clips with near-static motion, resulting in 11,481 final training clips. We additionally construct non-overlapping test set of 787 clips for evaluation. For each clip, we generate single descriptive caption using Qwen2.5-VL-7B [3] and use it as the text condition. Configuration and notation. We generate = 5 clips at 24 fps from = 15 videos. Camera poses and memory features are sampled at 4 fps. For target 5 clip with interval [t0, t1], we use the terminal pose pt1 as the query. The memory window is limited to = 5 s. We use Wan2.2-Fun-5B-Control-Camera [38] as our base DiT with hidden dimension = 3072. Retriever and DiT are trained with 1 and 5 epochs, respectively. For each video, we extract 3D-aware memory feature from pretrained StreamVGGT [62]. We select four layers {4, 11, 17, 23}; at each layer, the feature contains 782 tokens. Concatenating across the four layers yields = 4 782 and dm = 1024 memory tokens per frame. 6 Figure 4. Qualitative comparisons. Left: Baselinesincluding the memory-removed variantexhibit abrupt popping/vanishing of the school bus, and GF is low-quality. Captain Safari alone renders the bus smoothly exiting the frame. Right: Baselines distort or lose field marking, with Wan2.2 collapsing under large camera motion, affirming the challenge of 3D consistency under rapid trajectories. Captain Safari preserves crisp markings and coherent layout while following the fast 6-DoF path. 5.2. Benchmark Metrics. We evaluate video generation along three complementary axes: video quality, 3D consistency, and trajectory following. For video quality, we report FVD [37] and LPIPS [54]. For 3D consistency, we use MEt3R [1], computed between GT and generated videos at matched time steps and reconstruction rate that measures the percentage of frames successfully registered in the recovered 3D model [30, 31]. For trajectory following, we report camera relocation accuracy (AUC [39]) and the cosine similarity between the flattened camera pose, capturing how the model adheres to the desired camera parameters over time. Baselines. We compare against representative cameracontrollable video generation models, including Geometry Forcing [44], Real-CamI2V [19, 57], and Wan2.2-5BControl-Camera [38], which cover geometry-constrained, reconstruction-driven, and large-scale diffusion-based approaches to trajectory-conditioned video synthesis. Human Study. We conduct human study with 50 participants. Each participant is presented with 10 cases, where each case contains the GT video and five anonymized model-generated videos (three baselines, our model, and its ablated variant). For every case, participants are asked to select the best video under three criteria: Video Quality, 3D Consistency, and Trajectory Following. In total, the study collects 50 10 3 = 1, 500 human preference votes. 5.3. Generation Quality As shown in Table 1, our Captain Safari attains substantially lower FVD (1023.46 vs. 1387.75) and slightly improved LPIPS score (0.512 vs. 0.513) compared to the SOTA baseline, demonstrating more stable temporal dynamics and sharper spatial details. Moreover, the human study in Table 2 indicates that 67.40% of participants prefer our videos over all competing methods, highlighting the perceptual realism and overall fidelity of our generations. Qualitative comparisons in Figure 4 further reveal that Captain Safari produces visually compelling, realistic, and highly authentic scene dynamics. These findings are also consistent with the samples shown in Figure 1, where our method delivers vivid, coherent, and natural-looking drone videos that closely resemble real-world captures. 5.4. 3D Consistency Captain Safari achieves state-of-the-art 3D consistency. As shown in Table 1, our method lowers MEt3R by 0.0013 (0.3690 vs. 0.3703) and raises the reconstruction rate by 0.045 (0.968 vs. 0.923) copared to the strongest baseline. Consistently, the human study in Table 2 shows that 65.60% of participants prefer Captain Safari for 3D consistency, substantially surpassing all competing approaches. Qualitative visualizations further confirm these quantitative gains. In Figure 1, structures such as the Greek-style columns remain geometrically stable across large viewpoint changes. In Figure 4, our model produces (left) school bus that smoothly moves out of the frame, and (right) preserves crisp, globally consistent field markings on the soccer pitch, whereas baselines exhibit distortions and disappearance. Figure 5 and Figure 1 further show that our reconstructions yield sharper facades and well-formed windows 7 Figure 5. Scene reconstruction and camera trajectory. With pose-aligned memory, Captain Safari reconstructs well-structured building facade (the memory-removed variant blurs/warps it), demonstrating the benefit of memory. It also preserves fine detailsparked cars and the tree on their roofsthat Wan2.2-5B fails to retain. Meanwhile, Real-CamI2V follows only short path, whereas Captain Safari covers the full trajectory with stable 3D structure, highlighting the challenge of maintaining 3D consistency under fast motion. without collapsing geometry. Together, these results validate that the implicit world memory and pose-conditioned retrieval of Captain Safari effectively stabilize the underlying 3D world under aggressive camera motion. 5.5. Trajectory Following Captain Safari delivers the most accurate trajectory following among all competing models. As shown in Table 1, our method achieves the highest AUC@30 (0.200) and AUC@15 (0.068), along with the best cosine similarity (0.563), outperforming the strongest baseline by clear margins. The human study in Table 2 further reinforces this observation, with 69.00% of participants identifying our model as the most faithful to the target camera path. Figure 5 provides clear visualization of these improvements. Captain Safaris predicted trajectory closely aligns with the ground-truth path, while the ablated variant deviates and flies over the rooftop, and RealCam-I2V fails to follow the intended forward motion, advancing only slightly rather than committing to the prescribed trajectory. Furthermore, our method demonstrates stable and coherent generation under challenging viewpoint changes with complex camera maneuvers in Figure 1. These results highlight the effectiveness of our memory-augmented, pose-conditioned design for precise trajectory adherence. 5.6. Ablation Study Our results highlight the importance of the proposed poseconditioned world memory. As shown in Table 1, adding memory yields substantial improvements in both 3D consistency and trajectory following. These gains confirm that retrieving pose-aligned world features at the target frame provides the model with an explicit understanding of what the scene should look like, enabling stable geometry and accurate motion alignment. Qualitative comparisons in Figure 4 and Figure 5 further illustrate these effects. With memory, the generated scenes preserve global structure, maintain consistent geometry across viewpoints. In contrast, the ablated variant often drifts and exhibits geometric inconsistencies. Together, these results validate the effectiveness of our memoryaugmented design in stabilizing the underlying 3D world and guiding precise camera motion. 6. Conclusion We introduced Captain Safari, pose-conditioned world engine built on world memory that enables long-range, 3D-consistent video generation under complex FPV trajectories. Together with OpenSafari, our curated dataset of inthe-wild drone videos with verified camera poses, this establishes rigorous benchmark for controllable video generation. Captain Safari markedly improves 3D consistency and trajectory accuracy over prior methods while maintaining strong visual fidelity. Although the system incurs nontrivial inference overhead, future work will explore realtime world engines with lightweight memory and faster generative backbones. We hope Captain Safari and OpenSafari encourage further research in persistent world models and long-horizon controllable video generation."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Asim, Christopher Wewer, Thomas Wimmer, Bernt Schiele, and Jan Eric Lenssen. Met3r: Measuring multi-view consistency in generated images. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 60346044, 2025. 7 [2] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d: In European Trajectory-conditioned text-to-4d generation. Conference on Computer Vision, pages 5372. Springer, 2024. 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 6 [4] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022. 5 [5] Yuanhao Cai, He Zhang, Kai Zhang, Yixun Liang, Mengwei Ren, Fujun Luan, Qing Liu, Soo Ye Kim, Jianming Zhang, Zhifei Zhang, et al. Baking gaussian splatting into diffusion denoiser for fast and scalable single-stage image-to-3d generation and reconstruction. arXiv preprint arXiv:2411.14384, 2024. 2 [6] Yaru Cao, Zhijian He, Lujia Wang, Wenguan Wang, Yixuan Yuan, Dingwen Zhang, Jinglin Zhang, Pengfei Zhu, Luc Van Gool, Junwei Han, et al. Visdrone-det2021: The vision meets drone object detection challenge results. In Proceedings of the IEEE/CVF International conference on computer vision, pages 28472854, 2021. [7] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning arXiv preprint from rgb-d data in indoor environments. arXiv:1709.06158, 2017. 1 [8] Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, and Chuang Gan. Adaworld: Learning adaptable world models with latent actions. arXiv preprint arXiv:2503.18938, 2025. 1, 2 [9] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Yusuf Aytar, Michael Rubinstein, Chen Sun, et al. Motion prompting: Controlling video generation with motion trajectories. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 112, 2025. 3 [10] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 1, 3 [11] Chen Hou and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. 3 [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [13] Ronghang Hu, Nikhila Ravi, Alexander Berg, and Deepak Pathak. Worldsheet: Wrapping the world in 3d sheet for view synthesis from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1252812537, 2021. 1, 2 [14] Junchao Huang, Xinting Hu, Boyao Han, Shaoshuai Shi, Zhuotao Tian, Tianyu He, and Li Jiang. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft. arXiv preprint arXiv:2510.03198, 2025. 2, 3 [15] Tianyu Huang, Wangguandong Zheng, Tengfei Wang, Yuhao Liu, Zhenwei Wang, Junta Wu, Jie Jiang, Hui Li, Rynson WH Lau, Wangmeng Zuo, et al. Voyager: Long-range and world-consistent video diffusion for explorable 3d scene generation. arXiv preprint arXiv:2506.04225, 2025. 3 [16] Longbin Ji, Lei Zhong, Pengfei Wei, and Changjian Li. Posetraj: Pose-aware trajectory control in video diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2277622785, 2025. 1, 2, 3 [17] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. 3 [18] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. 2 [19] Teng Li, Guangcong Zheng, Rui Jiang, Shuigen Zhan, Tao Wu, Yehao Lu, Yining Lin, Chuanyun Deng, Yepan Xiong, Min Chen, et al. Realcam-i2v: Real-world image-to-video generation with interactive complex camera control. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2878528796, 2025. 1, 3, 6, 7 [20] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 798810, 2025. 1, 3 [21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. [22] Jiachen Lu, Ze Huang, Zeyu Yang, Jiahui Zhang, and Li Zhang. Wovogen: World volume-aware diffusion for conIn Eutrollable multi-camera driving scene generation. ropean Conference on Computer Vision, pages 329345. Springer, 2024. 2, 3 [23] Taiming Lu, Tianmin Shu, Junfei Xiao, Luoxin Ye, Jiahao Wang, Cheng Peng, Chen Wei, Daniel Khashabi, Rama Chellappa, Alan Yuille, et al. Genex: Generating an explorable world. arXiv preprint arXiv:2412.09624, 2024. 1, 2 9 [24] Andrew Melnik, Michal Ljubljanac, Cong Lu, Qi Yan, Weiming Ren, and Helge Ritter. Video diffusion models: survey. arXiv preprint arXiv:2405.03150, 2024. 2 [25] Chong Mou, Mingdeng Cao, Xintao Wang, Zhaoyang Zhang, Ying Shan, and Jian Zhang. Revideo: Remake video with motion and content control. Advances in Neural Information Processing Systems, 37:1848118505, 2024. 3 [26] Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Weijie Wang, Haoyun Li, Guosheng Zhao, Jie Li, Wenkang Qin, Guan Huang, and Wenjun Mei. Wonderturbo: Generating arXiv preprint interactive 3d world in 0.72 seconds. arXiv:2504.02261, 2025. 1, 3 [27] Ava Pun, Gary Sun, Jingkang Wang, Yun Chen, Ze Yang, Sivabalan Manivasagam, Wei-Chiu Ma, and Raquel Urtasun. Neural lighting simulation for urban scenes. Advances in Neural Information Processing Systems, 36:1929119326, 2023. 2 [28] Santhosh Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel Chang, et al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238, 2021. [29] Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Muller, Alexander Keller, Sanja Fidler, and Jun Gao. Gen3c: 3d-informed world-consistent video generation with precise camera conIn Proceedings of the Computer Vision and Pattern trol. Recognition Conference, pages 61216132, 2025. 1, 3 [30] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In CVPR, 2019. 5, 7 [31] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning feature matching with graph neural networks. In CVPR, 2020. 5, 7 [32] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: In Proceedings of platform for embodied ai research. the IEEE/CVF international conference on computer vision, pages 93399347, 2019. 1 [33] Manuel-Andreas Schneider, Lukas Hollein, and Matthias Nießner. Worldexplorer: Towards generating fully navigable 3d scenes. arXiv preprint arXiv:2506.01799, 2025. 3 [34] Xincheng Shuai, Henghui Ding, Zhenyuan Qin, Hao Luo, Xingjun Ma, and Dacheng Tao. Free-form motion control: Controlling the 6d poses of camera and objects in video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1244912458, 2025. 2 [35] Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye Ren, and Raquel Urtasun. Scenegen: Learning to generate realistic traffic scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 892901, 2021. 2 [36] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In European conference on computer vision, pages 402419. Springer, 2020. [37] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 7 [38] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 6, 7 [39] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 7 [40] Jiahao Wang, Luoxin Ye, TaiMing Lu, Junfei Xiao, Jiahan Zhang, Yuxiang Guo, Xijun Liu, Rama Chellappa, Cheng Peng, Alan Yuille, et al. Evoworld: Evolving panoramic world generation with explicit 3d memory. arXiv preprint arXiv:2510.01183, 2025. 1, 2, 3 [41] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances in Neural Information Processing Systems, 36:75947611, 2023. 3 [42] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3 [43] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74677477, 2020. 1, [44] Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, and Jiang Bian. Geometry forcing: Marrying video diffusion and 3d representation for consistent world modeling. arXiv preprint arXiv:2507.07982, 2025. 3, 6, 7 [45] Jianzong Wu, Liang Hou, Haotian Yang, Xin Tao, Ye Tian, Pengfei Wan, Di Zhang, and Yunhai Tong. Vmoba: Mixtureof-block attention for video diffusion models. arXiv preprint arXiv:2506.23858, 2025. 1, 2 [46] Dejia Xu, Yifan Jiang, Chen Huang, Liangchen Song, Thorsten Gernoth, Liangliang Cao, Zhangyang Wang, and Hao Tang. Cavia: Camera-controllable multi-view video arXiv preprint diffusion with view-integrated attention. arXiv:2410.10774, 2024. 3 10 [60] Yunsong Zhou, Michael Simon, Zhenghao Peng, Sicheng Mo, Hongzi Zhu, Minyi Guo, and Bolei Zhou. Simgen: Simulator-conditioned driving scene generation. Advances in Neural Information Processing Systems, 37:4883848874, 2024. 2 [61] Zhenghong Zhou, Jie An, and Jiebo Luo. Latent-reframe: Enabling camera control for video diffusion models without training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1277912789, 2025. [62] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 6 [47] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. 3 [48] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2024. 3 [49] Xiaoda Yang, Jiayang Xu, Kaixuan Luan, Xinyu Zhan, Hongshun Qiu, Shijun Shi, Hao Li, Shuai Yang, Li Zhang, Checheng Yu, et al. Omnicam: Unified multimodal video generation via camera control. arXiv preprint arXiv:2504.02312, 2025. 2 [50] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. 1, 2 [51] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 59165926, 2025. 1, [52] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. Trajectorycrafter: Redirecting camera trajectory for monocarXiv preprint ular videos via diffusion models. arXiv:2503.05638, 2025. 3 [53] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. 3 [54] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 7 [55] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generaIn Proceedings of the Computer Vision and Pattern tion. Recognition Conference, pages 20632073, 2025. 3 [56] Zhongwei Zhang, Fuchen Long, Zhaofan Qiu, Yingwei Pan, Wu Liu, Ting Yao, and Tao Mei. Motionpro: precise moIn Proceedtion controller for image-to-video generation. ings of the Computer Vision and Pattern Recognition Conference, pages 2795727967, 2025. 3 [57] Guangcong Zheng, Teng Li, Rui Jiang, Yehao Lu, Tao Wu, and Xi Li. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957, 2024. 1, 3, [58] Mengqi Zhou, Yuxi Wang, Jun Hou, Shougao Zhang, Yiwei Li, Chuanchen Luo, Junran Peng, and Zhaoxiang Zhang. Scenex: Procedural controllable large-scale scene generation. arXiv preprint arXiv:2403.15698, 2024. 2 [59] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Learning arXiv preprint and Noah Snavely. view synthesis using multiplane images. arXiv:1805.09817, 2018. 2, 5 Stereo magnification:"
        }
    ],
    "affiliations": [
        "Johns Hopkins University",
        "Tsinghua University",
        "UC Santa Cruz"
    ]
}