{
    "paper_title": "What Matters in Transformers? Not All Attention is Needed",
    "authors": [
        "Shwai He",
        "Guoheng Sun",
        "Zheyu Shen",
        "Ang Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using a similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that a large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved a 48.4\\% speedup with only a 2.4\\% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose a method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90\\% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: \\url{https://github.com/Shwai-He/LLM-Drop}."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 6 6 8 7 5 1 . 6 0 4 2 : r WHAT MATTERS IN TRANSFORMERS? NOT ALL ATTENTION IS NEEDED Shwai He Guoheng Sun Zhenyu Shen Ang Li University of Maryland, College Park {shwaihe,ghsun,zyshen,angliece}@umd.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "While scaling Transformer-based large language models (LLMs) has demonstrated promising performance across various tasks, it also introduces redundant architectures, posing efficiency challenges for real-world deployment. Despite some recognition of redundancy in LLMs, the variability of redundancy across different architectures in transformers, such as MLP and Attention layers, is under-explored. In this work, we investigate redundancy across different modules within Transformers, including Blocks, MLP, and Attention layers, using similarity-based metric. Surprisingly, despite the critical role of attention layers in distinguishing transformers from other architectures, we found that large portion of these layers exhibit excessively high similarity and can be pruned without degrading performance. For instance, Llama-2-70B achieved 48.4% speedup with only 2.4% performance drop by pruning half of the attention layers. Furthermore, by tracing model checkpoints throughout the training process, we observed that attention layer redundancy is inherent and consistent across training stages. Additionally, we further propose method that jointly drops Attention and MLP layers, allowing us to more aggressively drop additional layers. For instance, when dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90% of the performance on the MMLU task. Our work provides valuable insights for future network architecture design. The code is released at: https://github.com/CASE-Lab-UMD/LLM-Drop."
        },
        {
            "title": "INTRODUCTION",
            "content": "Transformer-based large language models (LLMs) have significantly advanced AI research, achieving remarkable performance across various domains OpenAI (2024); Team (2024). However, scaling these models also introduces redundant architectures, namely overarching, leading to inefficiencies that complicate their real-world deployment Frantar et al. (2023); Sun et al. (2023), e.g., inflating deployment costs and resource demands. For instance, while the deployment cost of Llama-2-70B exceeds 128GB in FP16 precisionsurpassing the capacity of single A100 GPUit can be reduced in depth without significantly impacting performance Gromov et al. (2024). Although several previous works have been proposed to promote LLM efficiency via removing redundant parameters or architectures Frantar et al. (2023); Sun et al. (2023), these approaches often employ universal techniques that overlook the unique characteristics of transformer architectures. Specifically, transformer Vaswani (2017) architectures are composed of multiple stacked blocks, each containing an MLP layer and an Attention layer, which serve distinct functions and exhibit corresponding different levels of redundancy Wang et al. (2023); Shi et al. (2023) This motivates deeper investigation into the specific redundancies within Transformers, with the goal of identifying and addressing the most critical modules. In this work, we systematically explore the redundancy in three key Transformer components: Block, MLP, and Attention. Using similarity-based metric Gromov et al. (2024); Men et al. (2024) , we evaluate the importance of each component and progressively drop those identified as redundant. We first apply Block Drop approach but observe that removing entire blocks leads to significant performance degradation. This suggests need for more fine-grained strategy. Equal contribution Corresponding author 1 Upon further examination, we explore the separate pruning of MLP and Attention layers. Our findings reveal that while dropping MLP layers negatively affects performance, substantial portion of Attention layers, i.e., the core of Transformer architectures which distinguish it from other mainstream architectures (e.g., RWKV Peng et al. (2023) and Mamba Gu & Dao (2024)), can be pruned without degrading the models performance. For instance, dropping 50% of the Attention layers in Llama-2-70B Touvron et al. (2023) results in comparable performance to the full model, indicating high degree of redundancy in these layers. Building on these insights, we propose more flexible approach, Joint Layer Drop, which targets both MLP layers and Attention layers. By combining the importance scores of these layers, we find that jointly dropping low-importance Attention and MLP layers yields better performance under high sparsity conditions compared to pruning only one type of layer. Our work demonstrates that the redundancy in Attention layers is not only significant but also consistent across different training stages, indicating that this redundancy is an inherent property of Transformer architectures. These findings open the door to more efficient Transformer designs, reducing both memory (e.g., KV-Cache) and computational costs (e.g., inference speed), while maintaining performance. In summary, our key contributions are as follows: Through an in-depth analysis of redundancy in three key Transformer componentsBlock, MLP, and Attentionwe uncover surprising level of redundancy within the Attention. We propose Attention Drop, simple yet effective algorithm for efficiently removing redundant Attention layers in training-free manner. Additionally, we introduce Joint Layer Drop, which further improves the performance at high dropping ratios by jointly targeting both Attention and MLP layers. Our extensive experiments demonstrate the effectiveness of dropping Attention, for instance, removing 50% of the attention layers in Llama-2-70B results in only 2.4% performance reduction while achieving up to 48.4% speedup. We further show that the attention layers remain consistently high redundancy throughout the training process, indicating it as an inherent property and providing valuable insights for future architecture design."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Large Language Models Although Transformer-based Large Language Models (LLMs) have demonstrated promising performance across various tasks, their deployment costs remain significant challenge for practical usage Sun et al. (2023); Lin et al. (2024); Gromov et al. (2024). Transformer Vaswani (2017) models consist of multiple blocks, which include Attention layers and MLP layers. Attention layers compute the contextual information between input tokens with quadratic complexity concerning the input sequence length Li et al. (2020). KV-Cache Pope et al. (2022) mitigates the computational issue but results in excessive memory costs Zhang et al. (2023). MLP layers Liu et al. (2021); Mai et al. (2022) transform each token independently, using an up-projection followed by down-projection, and contribute most of the model parameters. Recent works have revealed that not all blocks or layers are equally important Men et al. (2024); Chen et al. (2024), which urges us to reflect on the structured redundancy within LLMs and the potential design of more compact architectures. Model Compression LLMs can be compressed to promote their efficiency in memory and computation. Quantization Frantar et al. (2023); Lin et al. (2024) and Pruning Sun et al. (2023); Frantar & Alistarh (2023) are the most widely used techniques to compress LLMs. Specifically, quantization transforms the data type into low-bit but remains potentially redundant architecture and parameters. Pruning can be categorized into unstructured pruning Kusupati et al. (2020); Sanh et al. (2020) and structured pruning Zhuang et al. (2020); Kwon et al. (2020). While unstructured pruning maintains better performance than structured pruning, it cannot be effectively applied to hardware, limiting its practical usage. Our methods, Block Drop and Layer Drop, focus on removing structured modules rather than fine-grained parameters, creating hardware-friendly efficient architectures while 2 maintaining comparable performance. Additionally, Block Drop and Layer Drop are orthogonal to quantization, and their integration with quantization significantly enhances efficiency."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "In this section, we present the methodology for identifying and removing redundant modules in LLMs. We begin by introducing similarity-based metric to assess redundancy across both Attention and MLP layers. Based on the insights gained from this analysis, we develop two targeted techniques, i.e., MLP Drop and Attention Drop, to efficiently eliminate redundant components while preserving model performance."
        },
        {
            "title": "3.1 PRELIMINARIES",
            "content": "Similarity-based Drop To assess the redundancy of modules in LLMs, we employ similaritybased metric that evaluates the importance of each module by measuring the similarity between its input and output Gromov et al. (2024). The underlying hypothesis is that redundant modules produce outputs that are similar to their inputs, implying minimal transformation. In contrast, important modules are expected to significantly alter their inputs and thus should be preserved. The similarity between the hidden states of the input and output of module is quantified using cosine similarity. The importance score of the module is computed as: = 1 CosineSim(X, ). (1) Modules with higher cosine similarity exhibit lower importance scores, indicating redundancy. We identify and prune the modules with the lowest importance scores according to predefined pruning ratio. complete evaluation of the metrics effectiveness is provided in Appendix B. Block Drop Transformer models are composed of stacked blocks, where each block shares common architecture and can be viewed as subnetwork. To reduce complexity, we first consider dropping entire blocks that are deemed unimportant. As shown in Figure 2, Transformer blocks operate sequentially, with each blocks output feeding into the next. To evaluate redundancy, we compute the similarity between the input and output of each block. For the l-th block, the importance score is calculated as: Sl = 1 CosineSim(X B, B), (2) and where denote the input and output of the l-th block, respectively. Since the similarity scores are computed locally, we can offload irrelevant modules to save memory. By iteratively computing the importance scores for each block from shallow to deep, we can identify and drop blocks with the lowest scores, thus saving memory and computational resources. 3.2 MOTIVATION Figure 1: Visualization of Block Drop. where we use to denote the blocks with high similarity scores (i.e., low importance scores). The dropped blocks are blurred. Figure 2: Importance scores of Blocks. Block Drop is an aggressive technique that risks removing essential layers, as it overlooks the internal fine-grained architectures within each block. transformer block consists of both an Attention layer and an MLP layer. These layers perform distinct functions, with the Attention layer facilitating contextual information flow between tokens and the MLP layer transforming the token representations. Given their distinct roles, we assess the redundancy of each layer separately by measuring the importance scores of Attention and MLP layers individually. Specifically, we leverage multiple calibration datasets to measure the importance scores, ranging from the pretraining dataset (e.g., C4 Raffel et al. (2020)) to instruction fine-tuning datasets (e.g., CodeAlpaca-20k1, 1https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k 3 MathInstruct Yue et al. (2024) and LIMA Zhou et al. (2024)). Figure 2 and 3 illustrate the varying trend of importance scores for Attention layers compared to MLP layers across multiple datasets. This observation motivates us to consider the varying levels of redundancy between MLP and Attention layers and to develop more fine-grained dropping techniques accordingly, namely, MLP Drop and Attention Drop. (a) MLP (b) Attention Figure 3: Importance scores for MLP and Attention layers, where we use various calibration datasets for comprehensive analysis. 3.3 LAYER DROP. Figure 4: Visualization of Layer Drop, where we visualize dropping either MLP or Attention Layers. Given the residual connection, we take LayerNorm together with the corresponding layers. The dropped layers with high similarity scores (i.e., low importance scores) are blurred. MLP Drop As illustrated in Figure 4, each MLP layer follows LayerNorm operation and involves residual connection, ensuring that part of the input is preserved in the final output. Given the input of the LayerNorm before MLP at the l-th Block, the output = l + MLP(LayerNorm(X (3) Since the output contains both the residual and the MLP transformation, evaluating similarity based solely on the MLPs output can be misleading. To address this, we consider the MLP layer and its associated LayerNorm as single unit and compute their importance score as follows: can be formulated as: )). Sl = 1 CosineSim(X , ). (4) By treating these layers as single entity, we ensure more accurate measure of importance. MLP Drop removes both the unimportant MLP and associated LayerNorm layers. Further validation of this approach can be found in Appendix B. Attention Drop of the l-th Attention layer is computed as: Similarly, Attention layers also operate within residual connection. The output = l + Attention(LayerNorm(X A)), (5) 4 is the inputs of the corresponding LayerNorm layers and where overall outputs that involve residual connections. Like MLP Drop, we assess both the Attention layer and its associated LayerNorm as single unit. The importance score for the Attention layer is: Sl = 1 CosineSim(X A, A). (6) Layer Drop, for both MLP and Attention layers, is performed in one-shot manner, calculating importance scores once and removing redundant layers in single step. This approach avoids the resource-intensive and time-consuming iterative pruning process. The effectiveness of this simple one-shot technique is evaluated in Appendix B. Implementation and Loading the Pruned Model . After removing redundant layers, the pruned model can be easily loaded using existing libraries, such as Huggingface Transformers Wolf et al. (2020), with only minor adjustments to the model configuration. Additional implementation details are provided in Appendix A."
        },
        {
            "title": "INVESTIGATION OF DROPPING DIFFERENT TARGET MODULES",
            "content": "In this section, we conduct comprehensive investigation into the effects of dropping different target modules. To quantify the trade-off between performance degradation and speedup, we introduce new metric, i.e., Speedup Degradation Ratio (SDR), defined as: γ = Avg. Speedup , (7) where Avg. represents the percentage change in average performance across the evaluated tasks, and Speedup denotes the corresponding percentage of speedup achieved by each method. Therefore, γ measures the amount of performance degradation incurred for each 1% increase in speedup. lower γ value indicates that the model achieves speedup with minimal performance loss, making it more efficient. In contrast, higher γ value suggests that the performance loss is substantial relative to the speedup gained, implying less favorable trade-off. Table 1 and Figure 5 summarize the results of dropping different target modules, such as Block, MLP, and Attention layers. Specifically, Table 1 shows the performance impact of dropping fixed number of modules (e.g., 4 and 8 layers), while Figure 5 extends this analysis by evaluating broader range of dropping ratios (0% to 100%). Block and MLP Drop: Significant Performance Degradation with Moderate Speedup Block Drop and MLP Drop both lead to notable performance declines across both models, despite achieving moderate speedups. For example, Dropping 4 blocks results in 2.4% average performance decline (from 68.2 to 65.8) for Llama-2-13B, with speedup of 1.11, corresponding to γ of 0.22. However, dropping 8 blocks causes 7.5% performance drop (down to 60.7), with only modest speedup of 1.24 and higher γ of 0.31. Similarly, MLP Drop exhibits comparable trend, with small decline at 4 layers (1.3%, γ = 0.32), but much larger drop at 8 layers (6.3%, γ = 0.79). These results suggest that while Block and MLP Drop provide moderate speedup, they do so at the cost of significant performance degradation, especially at higher drop ratios. Attention Drop: Minimal Performance Impact with High Efficiency Surprisingly, despite the critical role of attention layers in Transformer architectures, dropping attention layers is highly effective. Both Llama-2-13B and Mistral-7B maintain over 99% of their original performance even after dropping 8 attention layers, as shown in Figure 5(c). For example, Attention Drop maintains near-baseline performance even after dropping 8 layers (69.8 vs. 70.3), with speedup of 1.23 and low γ of 0.02. Dropping 12 attention layers results in only slight performance decline (67.3), with significant speedup of 1.40 and γ of 0.08. The superior performance of Attention Drop persists when compared to other compression techniques in Appendix B. These results demonstrate that attention layers are highly redundant, and their removal has minimal impact on model accuracy, making Attention Drop highly efficient pruning strategy. 5 Table 1: Experimental Results of Dropping Different Modules, where we drop the fixed number (e.g., 4 and 8) of modules on Llama-2-13B and Mistral-7B. Here, Block, MLP, and Attn are corresponding modules. Rows with averaged performance lower than 95% of the original performance are grayed. Llama-2-13B Method ARC-C BoolQ HellaSwag MMLU OBQA PIQA RTE WinoGrande Avg. () SpeedUp () γ () Baseline Block-4 Block-8 MLP-4 MLP-8 Attn-4 Attn-8 Attn-16 Attn-20 59.9 54.8 48.0 54.9 49.2 58.8 58.2 56.4 53. 80.7 73.3 56.8 76.1 63.4 80.4 80.5 79.2 76.9 82.2 80.6 75.3 80.4 75.6 82.0 82.2 81.9 78.6 55.1 54.8 53.8 54.8 54.5 54.7 54.5 48.2 51.5 45.6 45.8 41.2 45.4 42.2 46.2 47.0 47.4 44.4 80.5 79.1 75.3 79.5 76.0 80.5 80.5 79.5 77.6 Mistral-7B 65.0 60.3 59.9 66.4 59.2 67.9 64.3 59.9 59. 77.0 77.5 75.6 77.3 75.1 77.2 77.4 76.2 77.1 68.2 65.8 60.7 66.9 61.9 68.5 68.1 66.1 64.9 1.00 1.11 1.24 1.04 1.08 1.05 1.13 1.29 1.40 0.22 0.31 0.32 0.79 -0.05 0.01 0.07 0.08 Method ARC-C BoolQ HellaSwag MMLU OBQA PIQA RTE WinoGrande Avg. () SpeedUp () γ () Baseline Block-4 Block-8 MLP-4 MLP-8 Attn-4 Attn-8 Attn-12 61.5 53.1 40.0 53.2 36.7 61.0 60.2 57.2 83.7 80.4 71.6 80.3 71.8 83.5 82.7 76.8 83.2 77.5 63.9 77.7 33.6 82.9 82.3 80.2 62.5 61.6 60.0 61.7 53.3 62.5 62.2 59. 43.8 40.0 30.6 40.0 30.6 44.6 44.2 41.8 82.0 77.6 69.3 77.6 68.0 82.0 81.3 79.1 66.8 70.0 63.9 67.5 66.8 64.6 66.8 66.1 78.5 76.6 69.7 77.3 66.6 78.0 78.8 77.7 70.3 67.1 58.6 66.9 53.4 69.9 69.8 67.3 1.00 1.14 1.32 1.03 1.06 1.10 1.23 1.40 0.23 0.37 1.13 2.82 0.04 0.02 0.08 (a) Block Drop (b) MLP Drop (c) Attention Drop Figure 5: Performance with respect to Dropping Ratios. The solid lines represent the impact of dropping the modules with the lowest importance scores in Mistral-7B and Llama-2-13B, and the dotted lines represent the performances of the baseline and random guessing. Larger Models Show Consistent Robustness to Attention Drop To verify the consistency of our findings on larger models, we take Llama-2-70B into consideration, since it also comes from the Llama family and has larger model size. Specifically, we drop the modules with different dropping ratios ranging from 5% to 60% on Table 3. Similar to the findings in smaller models, Llama-270B also showcases sensitivity to Block Drop and MLP Drop, where dropping only 10% to 20% of blocks or MLP layers leads to significant performance drop. Table 2: Experimental results on Llama-3, where Llama-3-8B and Llama-3-70B are included. Rows with averaged performance lower than 95% of the original performance are grayed. Method HellaSwag MMLU OBQA WinoGrande Avg. () SpeedUp () γ () Llama-3-8B Baseline Attn-4 Attn-8 Attn-12 Attn-16 Attn-20 82.2 81.6 81.1 79.4 71.2 42.2 65.5 65.1 65.1 63.9 38.2 23.0 45. 44.8 45.0 42.2 39.4 30.6 77.7 78.2 78.4 77.8 72.8 58.7 67.6 67.4 67.4 65.8 55.4 38.6 1.00 1.07 1.16 1.26 1.38 1.52 0.03 0.01 0.07 0.32 0.56 Baseline 88.0 78. 48.4 85.4 75.1 1.00 Llama-3-70B In contrast, Attention Drop performs much better on Llama-2-70B. Specifically, when dropping 40 out of 80 attention layers, Llama-2-70B achieves speedup of 1.48 and γ of 0.05. similar trend is observed in Llama-3, as shown in Table 2. This robustness indicates that larger models can also tolerate the removal of significant proportion of Attention layers without degrading performance. Attn-4 Attn-8 Attn-16 Attn-32 Attn-40 Attn-48 87.9 87.8 87.8 87.9 85.2 81.2 78.7 78.5 78.7 78.6 77.1 73.9 49.0 48.8 48.6 48.8 48.0 47.4 85.2 85.2 84.9 85.3 82.8 81. 75.2 75.1 75.0 75.2 73.3 71.0 1.04 1.10 1.17 1.35 1.43 1.55 -0.03 0.00 0.01 0.00 0.00 0.07 The results clearly indicate that Attention Drop is the most efficient method for pruning, allowing for significant speedup with minimal impact on performance. In the following sections, we will examine the efficiency improvements achieved through Attention Drop and further investigate the layerwise importance of attention layers to gain deeper insights into model architecture. For additional comparisons between Attention Drop and other compression methods, please refer to Appendix B. Table 3: Block Drop and Layer Drop on Larger Models, where we drop series of numbers (from 4 to 48) of modules on Llama-2-70B. Rows with averaged performance lower than 95% of the original performance are grayed. Llama-2-70B Method ARC-C BoolQ HellaSwag MMLU OBQA PIQA RTE WinoGrande Avg. () SpeedUp () γ () Baseline Block-4 Block-8 Block-16 BlockMLP-4 MLP-8 MLP-16 MLP-32 Attn-4 Attn-8 Attn-16 Attn-32 Attn-40 Attn-48 67.4 63.8 59.1 44.6 35.1 65.4 64.4 57.5 40.6 67.2 67.3 67.8 67.2 63.7 58. 83.8 80.4 77.5 64.6 58.8 84.0 83.9 53.6 61.9 84.0 83.8 83.9 84.8 82.8 73.7 87.1 84.6 81.3 69.9 56. 86.1 84.9 81.6 64.2 87.0 86.9 87.2 87.2 84.4 80.6 68.5 60.2 55.1 29.3 25.7 68.7 68.7 69.1 59.8 68.6 68.5 68.5 68.4 66.2 56. 48.6 48.0 46.2 40.0 36.8 46.6 47.6 46.0 29.8 48.8 48.4 49.0 49.6 46.8 45.0 82.5 81.6 81.0 75.2 71. 82.9 81.7 79.2 64.2 82.5 82.9 83.0 81.8 80.1 79.8 69.3 71.1 68.2 51.6 54.5 68.2 66.8 58.8 52.7 69.3 69.0 68.2 67.5 66.8 59. 83.7 78.0 73.2 59.7 55.3 83.4 82.2 81.7 72.7 83.3 82.6 82.8 83.5 81.3 81.0 73.9 71.0 67.7 54.4 49. 73.2 72.5 65.9 55.7 73.8 73.7 73.8 73.8 71.5 66.9 1.00 1.07 1.14 1.30 1.67 1.04 1.05 1.08 1.17 1.06 1.12 1.21 1.35 1.48 1.62 0.41 0.44 0.65 0.37 0.18 0.28 1.00 1.07 0.02 0.02 0.00 0.00 0.05 0."
        },
        {
            "title": "5 EFFICIENCY OF ATTENTION DROP",
            "content": "In this section, we evaluate the efficiency of Attention Drop in terms of both memory usage and inference speed. Specifically, we examine the reduction in memory overhead due to the key-value (KV) cache and measure the speedup during the entire generation phase. The results demonstrate that Attention Drop provides substantial improvements in both efficiency metrics while maintaining high performance. KV-cache Memory Reduction Given the auto-regressive nature of LLMs, where outputs are generated token by token, the KV-cache is used to store intermediate representations of input sequences. This cache helps accelerate inference by preventing redundant computations but comes with significant memory cost, especially with longer sequence lengths or larger batch sizes. Our proposed Attention Drop method efficiently removes unimportant attention layers, reducing the corresponding KV-cache. Table 4 provides comparison of 16-bit precision KV-cache memory usage before and after Attention Drop for various models, where we use 8 Nvidia RTX A6000 Ada 7 GPUs for the 70B models and 4 Nvidia RTX A6000 Ada GPUs for other smaller models. As shown, Attention Drop results in substantial memory savings across all tested models. For instance, in Llama-2-13B, the KV-cache is reduced from 52GB to 26GB, 50% reduction. Note that the reported results are based on resource-constrained scenarios. In resource-sufficient cases, where larger batch sizes and longer sequence lengths can be applied, the memory usage savings from Attention Drop become even more significant. These reductions are beneficial for both memory-constrained and memory-sufficient environments, allowing for more efficient model deployment. Table 4: Comparison of KV-cache sizes before and after Attention Drop across different models, with sequence length of 2048. Since only Llama-2-13B does not use grouped-query attention, the KV-cache for each token is significantly larger compared to other models. Model Batch Size Mistral-7B Llama-2-13B Llama-2-70B Llama-3-8B Llama-3-70B 64 32 32 64 32 wo/Attn Drop w/Attn Drop Layers KV-cache Layers KV-cache 32 40 80 32 80 16GB 52GB 20GB 16GB 20GB 20 20 40 20 40 10GB 26GB 10GB 10GB 10GB Speed Measurement We also evaluate the run-time speed improvements achieved through Attention Drop. The inference speed is measured throughout the entire generation process, starting from the input prompt to the generation of the final token. To ensure that the results accurately reflect the speed improvements, we follow two key principles in our setup: (1) all operations are performed on single Nvidia RTX A6000 Ada GPU, avoiding any communication overhead caused by multi-GPU setups; (2) we increase the batch sizes to maximize GPU utilization for each model. Specifically, for Llama-2-70B, we employ 4-bit quantization due to its large model size, while noting that Attention Drop is orthogonal to quantization shown in C. For Llama-2-13B and Mistral-7B, we use 16-bit precision. In terms of sequence length, we use an input sequence of 2048 tokens and autoregressively generate an additional 2048 tokens. This setup allows us to capture the full inference process, ensuring that both the prefill (initial processing of the input sequence) and the generation (token-by-token inference) stages are included in the speed measurements. The speed-up ratios achieved through Attention Drop are presented in Tables 1, 2, and 3. Our results show that Attention Drop provides up to 40% speed-up while retaining more than 95% of the original models performance. Additionally, as demonstrated in Table 1, the γ values for Attention Drop are significantly lower than those for MLP Drop and Block Drop, especially at higher speed-up ratios. This indicates that Attention Drop achieves more efficient trade-off between speed and performance, making it superior method for model acceleration."
        },
        {
            "title": "6 VISUALIZATION EXAMPLES OF LAYER IMPORTANCE",
            "content": "In this section, we visualize the importance scores and the corresponding dropping order of pretrained models. We then trace back through historical checkpoints to explore the dynamics of importance scores throughout the training process. 6.1 DEEPER MODULES WITH HIGHER REDUNDANCY Based on Figure 2, 3 and 10, we observe that the deeper layers (excluding the last ones) often exhibit excessively low importance across Block, MLP, and Attention modules. To further analyze the dropped modules, we visualize the dropped layers or blocks with different dropping ratios. Figure 6 visualizes the remaining and dropped layers/blocks as the number of dropped modules increases. Llama-2-13B and Mistral-7B exhibit similar patterns in Layer Drop and Block Drop: initially, both models tend to drop the deeper layers, followed by the shallower ones. These findings are consistent with Xu et al. Men et al. (2024), which suggests that deeper layers tend to be more redundant. Larger models (e.g., Llama-2-70B) also showcase similar trend, which is shown in Appendix C. 8 (a) Block Drop (b) MLP Drop (c) Attention Drop Figure 6: Visualization of Dropping Order for Block Drop and Layer Drop. We visualize the remaining layers and blocks under different dropped numbers, where yellow areas represent the retained layers/blocks and red areas indicate the dropped portions."
        },
        {
            "title": "6.2 CONSISTENT REDUNDANCY OF ATTENTION LAYERS THROUGHOUT TRAINING",
            "content": "Given that the deep layers exhibit high redundancy, to investigate how such pattern is achieved, we revisit the historical checkpoints to track the dynamic changing of layer-wise importance scores. Specifically, we use checkpoints released by MAP-Neo-7B Zhang et al. (2024), due to the released continuous checkpoints during training stages. Figure 7 presents the importance scores of Blocks and Layers at different training stages, where Attention layers demonstrate consistently lower importance scores than MLP and Block at all training stages. While the importance scores for MLP layers and Blocks gradually increase as training progresses, the importance scores of Attention layers change much more slowly. Given the consistently higher redundancy of attention layers throughout training, we believe that this pattern arises from the inherent properties of attention layers. Figure 7: Visualization of Importance Scores in Checkpoints during the Pre-training Process of MAP-Neo-7B, where lighter areas represent low importance scores (i.e., high similarity scores). We present the entire Training Process (checkpoints for every 500B trained tokens). We independently visualize the importance scores at the module index 0, since they are significantly higher."
        },
        {
            "title": "JOINT LAYER DROP FURTHER ENHANCES THE PERFORMANCE",
            "content": "While significant proportion of attention layers exhibit high redundancy, our findings also show that some MLP layers have low importance. To further optimize model efficiency, we introduce Joint Layer Drop, which combines both Attention Drop and MLP Drop strategies. This approach leverages the redundancy in both attention and MLP layers to enhance the overall performance of the model. Methodology: Combining Attention and MLP Drop The Joint Layer Drop method is implemented by first calculating the importance scores for both attention layers (Sl ) individually. These scores are computed based on similarity-based metric that identifies redundant layers, as previously discussed. Once the importance scores are obtained for each type of layer, we concatenate the scores into single array: = [Sl ]. From this combined set of importance scores, we drop the layers with the lowest values, regardless of whether they are attention or MLP layers. This joint approach allows us to remove the most redundant components from both layer types simultaneously, enhancing the models efficiency while maintaining performance. A) and MLP layers (Sl A, Sl Superior Performance with Joint Layer Drop As demonstrated in Figure 8, Joint Layer Drop consistently achieves better performance than either Attention Drop or MLP Drop alone. The process begins by exclusively dropping attention layers, which are typically more redundant than MLP layers. This continues until the number of dropped attention layers exceeds 14 for Mistral-7B and 18 for Llama-2-13B. As result, in the initial stages of pruning, the performance of Joint Layer Drop overlaps with that of Attention Drop. However, as the dropping ratio increases and the more redundant attention layers are pruned, MLP layers start to become the next most redundant components. At this point, Joint Layer Drop begins to remove MLP layers, leading to further reductions in redundant layers without significant performance loss, e.g., after dropping 31 layers (Attention + MLP), Llama-2-13B still retains 90% of the performance on the MMLU task. (a) Llama-2-13B (b) Mistral-7B Figure 8: Accuracy Curves of Dropping Different Target Modules, where we consider dropping single types of modules and Joint dropping (Attn + MLP). In the line of Joint Drop, represents the step where the MLP is dropped, while the represents the step where the Attention is dropped."
        },
        {
            "title": "8 CONCLUSION AND DISCUSSIONS",
            "content": "Insights for Future Network Architecture Design Despite the success of scaling up LLMs, our work provides valuable insights for scaling down models to achieve more efficient architectures. One key insight is the high redundancy in attention layers, particularly in deeper layers, which suggests that future works could reduce the number of attention layers without sacrificing performance, rather than maintaining parity with MLP layers. Moreover, unlike MLP layers, attention layers exhibit consistent redundancy as training progresses. This consistency may pose bottleneck in training large models. To address this, future research could explore replacing attention layers with alternative mechanisms or develop new training techniques that capitalize on this redundancy to further enhance language model capacity. Limitations While our proposed dropping techniques improve efficiency in the models we evaluated, there are limitations. key area for future work is testing the applicability of these techniques across broader range of models, such as vision transformers and vision-language models. Furthermore, our methods focus on post-training dropping without involving retraining, which could potentially recover or even improve performance after pruning. Retraining these models could unlock even greater efficiency in more compact architectures. Conclusion In this work, we systematically revisited transformer architectures by investigating the effects of dropping three types of structures: Blocks, MLP layers, and Attention layers. Our findings reveal that attention layers display significant redundancy and can be removed in large proportions without compromising performance. To build on this, we introduced Joint Layer Drop, method that further increases both dropping ratios and performance by targeting redundant layers across both MLP and Attention layers. This study empirically demonstrates the potential for creating more compact and efficient transformer models, providing valuable insights for future network design within the NLP community. By exploring structured redundancy, we open up new avenues for designing more efficient, scalable models that maintain high performance even under resource constraints."
        },
        {
            "title": "REFERENCES",
            "content": "Winogrande: An adversarial winograd schema challenge at scale. 2019. AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. Iñigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. Efficient intent detection with dual sentence encoders. arXiv preprint arXiv:2003.04807, 2020. Xiaodong Chen, Yuxuan Hu, and Jing Zhang. Compressing large language models by streamlining the unimportant layer. arXiv preprint arXiv:2403.19135, 2024. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A. Roberts. The unreasonable ineffectiveness of the deeper layers, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. 11 Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened llama: simple depth pruning for large language models. ICLR Workshop on Mathematical and Empirical Understanding of Foundation Models (ME-FoMo), 2024. URL https://openreview.net/forum?id=18VGxuOdpu. Aditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In Proceedings of the International Conference on Machine Learning, July 2020. Se Jung Kwon, Dongsoo Lee, Byeongwook Kim, Parichay Kapoor, Baeseong Park, and Gu-Yeon Wei. Structured compression by weight encryption for unstructured pruning and quantization. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19061915, 2020. doi: 10.1109/CVPR42600.2020.00198. Rui Li, Jianlin Su, Chenxi Duan, and Shunyi Zheng. Linear attention mechanism: An efficient attention for semantic segmentation, 2020. Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, and Wenhu Chen. Long-context llms struggle with long in-context learning. arXiv preprint arXiv:2404.02060, 2024. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for llm compression and acceleration. In MLSys, 2024. Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. Pay attention to mlps, 2021. Florian Mai, Arnaud Pannatier, Fabio Fehr, Haolin Chen, Francois Marelli, Francois Fleuret, and James Henderson. HyperMixer: An MLP-based Green AI Alternative to Transformers. arXiv preprint arXiv:2203.03691, 2022. Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering, 2018. OpenAI. Gpt-4 technical report, 2024. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference, 2022. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by fine-tuning, 2020. URL https://arxiv.org/abs/2005.07683. Dachuan Shi, Chaofan Tao, Ying Jin, Zhendong Yang, Chun Yuan, and Jiaqi Wang. UPop: Unified and progressive pruning for compressing vision-language transformers. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pp. 3129231311. PMLR, 2023. Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. Ashish Vaswani. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings of ICLR. Tiannan Wang, Wangchunshu Zhou, Yan Zeng, and Xinsong Zhang. EfficientVLM: Fast and accurate vision-language models via knowledge distillation and modal-adaptive pruning. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, pp. 1389913913, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.873. URL https://aclanthology.org/2023.findings-acl.873. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Qun Liu and David Schlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL https://aclanthology.org/2020.emnlp-demos.6. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building math generalist models through hybrid instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=yLClGs770I. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. Map-neo: Highly capable and transparent bilingual large language model series, 2024. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023. 13 Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Tao Zhuang, Zhixuan Zhang, Yuheng Huang, Xiaoyi Zeng, Kai Shuang, and Xiang Li. Neuron-level structured pruning using polarization regularizer. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 98659877. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_ files/paper/2020/file/703957b6dd9e3a7980e040bee50ded65-Paper.pdf."
        },
        {
            "title": "A IMPLEMENTATION DETAILS",
            "content": "Models We utilize Llama-2 Touvron et al. (2023) and Mistral Jiang et al. (2023) as the default models, given their competitive performance and wide usage. We also evaluated the completely opensource model MAP-Neo Zhang et al. (2024) to explore the redundancy variations in the modules of the entire pre-training phase. Additionally, we experimented with the newly released Llama-3 AI@Meta (2024) to verify the effectiveness of model dropping on the latest models. Datasets. For the calibration dataset, we used the validation set of the C4 dataset Raffel et al. (2019), with 256 samples and an input sequence length of 2,048, following the setup in Sun et al. (2023). To evaluate the performance of the model, we report the results of the following tasks: BoolQ Clark et al. (2019), OBQA Mihaylov et al. (2018), PIQA Bisk et al. (2019), RTE Wang et al. (2019), ARC-C Clark et al. (2018), HellaSwagZellers et al. (2019), MMLU Hendrycks et al. (2021), WinoGrande ai2 (2019) and GSM8K Cobbe et al. (2021). Please refer to Table 5 for detailed information. The evaluation code is based on EleutherAI LM Evaluation Harness Gao et al. (2023). Table 5: Experimental Settings for Evaluation Tasks. performed with respect to the length of the input. Norm refers to the normalization Task Number of few-shot Metric BoolQ RTE OBQA PIQA MMLU WinoGrande GSM8K HellaSwag ARC-C 0 0 0 0 5 5 5 10 25 Accuracy Accuracy Accuracy (Norm) Accuracy (Norm) Accuracy Accuracy Exact Match Accuracy (Norm) Accuracy (Norm)"
        },
        {
            "title": "B ABLATION STUDIES",
            "content": "One-Shot v.s. Iterative One-shot and iterative approaches are the two most common methods for model compression. In the one-shot approach, importance scores are computed once, and the model is pruned in single step. In contrast, the iterative method computes importance scores and prunes the model incrementally over multiple iterations. In Figure 9, we empirically compare Iterative Dropping and One-Shot Dropping, where in Iterative Dropping, layers are removed one by one in each iteration. Figure 9: Ablation Study on Dropping Strategies, i.e., Iterative and One-Shot, where One-Shot Dropping achieves comparable performance with Iterative Dropping. As shown in Figure 9, Iterative Dropping achieves performance that is merely comparable to OneShot Dropping, without offering any significant enhancement. Given the simplicity and efficiency, One-Shot Dropping emerges as the superior choice. Table 6: Ablation Study on the Residual Connection, where we report the average performance of Mistral-7B on MMLU, WinoGrande, HellaSwag, and OpenbookQA. denotes the number of dropped modules. The notation \"w/ res\" indicates the involvement of the residual connection, while \"w/o\" indicates dropping without considering it. Attn Drop MLP Drop w/o res w/ res w/o res w/ res 39.4 37.7 36.8 32.2 32.0 65.0 65.3 65.4 63.4 62. 31.2 31.1 31.1 30.8 30.9 64.5 61.9 55.6 49.9 42.1 4 8 12 16 20 Residual Connection The involvement of the residual connection ensures more accurate estimation by accounting for the overall inputs and outputs. To explore its impact on performance, we also consider dropping modules without involving the residual connection. In this case, the importance scores are measured solely by the inputs and outputs of the Attention or MLP layers. As shown in Table 6, the involvement of the residual connection is essential for Layer Drop. Calibration Datasets Figure 2 and Figure 3 demonstrates the robustness of the importance scores across different datasets. In Figure 10, we further verify that the importance scores remain relatively stable across various modules of Mistral-7B as the sample size increases. This stability indicates that both Block Drop and Layer Drop maintain consistency regardless of the number of samples. Consequently, we confirm that using 256 samples is sufficient for computing similarity, which serves as the standard adopted for all our experiments. Dropping Metrics We selected the Reverse Order and Relative Magnitude metrics proposed by ShortGPT Men et al. (2024) and applied them to Attention Drop. Additionally, we considered the random dropping of attention layers. Our experiments were conducted using the Mistral-7B model, and the reported performance is averaged across five different random seeds. Notably, in Table 7, our metric, Cosine Similarity, consistently outperformed the others. 16 (a) Block (b) MLP (c) Attention Figure 10: The impact of sample quantity on the importance scores of Block and MLP. Table 7: Ablation study of Attention Drop across different metrics. Metric Attn-4 Attn-8 Attn-12 Random Reverse Order Relative Magnitude Cosine Similarity 61.5 66.9 67.0 67.0 49.6 66.9 66.8 66.9 39.4 61.5 62.3 64. Comparison with other Compression Techniques We first compare our method with published sparse models pruned by Shortened LLaMA Kim et al. (2024) in Table 8. Specifically, we prune the original Vicuna-13B-v1.3 model Chiang et al. (2023) using our proposed Joint Drop technique to maintain the same parameter budget. While Shortened LLaMA involves post-compression retraining, training-free Joint Drop performs better on average performance. We also compare our approach with the mainstream pruning method Wanda Sun et al. (2023) in Table 9. Under the same parameter budget, our methods outperform Wanda with unstructured sparsity. Additionally, Wanda contributes to fine-grained sparsity, which is not hardware-friendly and has limited practical usage. Table 8: Comparison with Shortened LLaMA Kim et al. (2024), where Joint Layer Drop achieves significantly higher speedup. Method HellaSwag MMLU OBQA Winogrande Avg. () SpeedUp () Joint Layer Drop Shortened LLaMA-PPL Shortened LLaMA-Taylor 76.0 75.3 76. 49.6 47.7 47.0 42.4 44.2 42.4 74.9 74.0 76.3 60.7 60.3 60.6 1.45 1.23 1.22 Table 9: Comparison with Wanda Sun et al. (2023) on Llama-2-13B under the same parameter budget. Taking performance into account, we apply unstructured sparsity for Wanda, while our proposed Attention Drop outperforms it in both performance and efficiency. Method HellaSwag MMLU OBQA Winogrande Avg. () SpeedUp () Wanda Attn-4 Wanda Attn-8 Wanda Attn-12 82.3 82.0 82.4 82.2 82.4 82.7 54.8 54.7 54.7 54.5 54.8 54.4 45.2 46.2 45.8 47.0 46.2 48.0 77.6 77.2 77.6 77.4 77.4 76. 65.0 65.0 65.1 65.3 65.2 65.4 1.00 1.05 1.00 1.13 1.00 1."
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "Dropping Order on Larger Models We present the dropping order of Block Drop and Layer Drop for the 70B Llama models in Figure 11. Similar to smaller models, larger models also tend to drop deeper layers first. While the dropping order of Blocks differs between Llama-2-70B and Llama-3-70B, we believe this is attributed to different training techniques, e.g., different numbers of training tokens. (a) Block (b) MLP (c) Attention Figure 11: Visualization of Dropping Order for Block Drop and Layer Drop on Larger Models, i.e., Llama-2-70B and Llama-3-70B. Figure 12: Accuracy Curves on GSM8k. Performance on Knowledge Intensive Tasks To assess the impact of Attention Drop on more complex technical tasks, we evaluated Llama-2-7B and Mistral-7B, and two corresponding instruction fine-tuned models, MetaMath-7B-V1.0 and MetaMath-Mistral-7B Yu et al. (2023). The results in Figure 12 indicate that, except for Llama-2-7B-Math, which is MetaMath-7B-V1.0, all the models do not experience significant performance degradation when dropping fewer than 8 Attention layers. We speculate that this is because Llama-2-7B-Math is initialized with Llama-2-7B and undergoes instruction fine-tuning to improve its mathematical ability. Llama-2-7B-Base exhibits poor performance in mathematics, and the ability obtained solely through fine-tuning appears to 18 be superficial. Therefore, when dropping Attention layers, Llama-2-7B-Maths ability rapidly deteriorates. We select the first sample from the test set of GSM8K2 as the input and display the MetaMath-Mistral7Bs raw output in Table 10. Even when dropping 10 attention layers, the model could still give the correct answer of this question, but it failed to adhere to the correct output format. However, when dropping 12 attention layers, the model is no longer able to produce the correct answer of this question. Table 10: Output Cases of Model with Dropping Some Attention Layers, where correct answers are marked in green , while incorrect answers are marked in red . Note that #### is used to extract the correct answer. Question Golden Answer Method Baseline Attn-2 Attn-4 Attn-6 Attn-8 AttnAttn-12 Janets ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers market? Janet sells 16 - 3 - 4 = 16-3-4=99 duck eggs day.nShe makes 9 * 2 = $9*2=1818 every day at the farmers market.n #### 18 Output Janets ducks lay 16 eggs per day.n She eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses 3 + 4 = 3+4=77 eggs per day.nTherefore, she sells 16 - 7 = 16-7=99 eggs at the farmers market.nShe sells each egg for $2, so she makes 9 $2 = $9*2=1818 per day.n #### 18 Janets ducks lay 16 eggs per day.nShe eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses 3 + 4 = 3+4=77 eggs per day.nTherefore, she sells 16 - 7 = 16-7=99 eggs at the farmers market.nShe sells each egg for $2, so she makes 9 $2 = $9*2=1818 per day.n #### 18 Janets ducks lay 16 eggs per day.nShe eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses total of 3 + 4 = 3+4=77 eggs per day.nTherefore, she sells 16 - 7 = 16-7=99 eggs at the farmers market.nShe sells each egg for $2, so she makes 9 $2 = $9*2=1818 per day.n #### 18 Janets ducks lay 16 eggs per day.nShe eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses total of 3 + 4 = 3+4=77 eggs per day.nTherefore, she sells 16 - 7 = 16-7=99 eggs at the farmers market.nShe sells each egg for $2, so she makes 9 $2 = $9*2=1818 per day.n #### Janets ducks lay 16 eggs per day.nShe eats 3 eggs for breakfast and bakes 4 eggs for muffins, so she uses total of 3 + 4 = 3+4=77 eggs per day.nTherefore, Janet sells 16 - 7 = 16-7=99 eggs at the farmers market.nShe sells each egg for $2, so Janet makes 9 $2 = $9*2=1818 per day at the farmers market.n #### 18 The total number of eggs the ducks lay is 16.nJane eats 3 eggs for breakfast, so there are 16 - 3 = 13 eggs left.nJane bakes 4 eggs for muffins, so there are 13 - 4 = 9 eggs left.nJane sells the remaining 9 eggs at $2 each, so she makes 9 * $2 = $9*2=18} Dividing the total number of eggs to the number of eggs used for personal use, we get 16 - 3 - 3 = 11 eggs for selling.nIf each egg is sold for $2, then the total amount earned from selling is 11 * 2 = $22.nTherefore, she makes $22 every day at the farmers market.n #### 22 Attention Drop is Orthogonal to Quantization Given that quantization simplifies data types and enhances efficiency in memory usage and inference speed, we integrate module dropping with quantization to verify whether the quantized models can maintain the performance achieved by Attention Drop. Specifically, we use the mainstream AWQ algorithm Lin et al. (2024) for 4-bit quantization, following its default settings, which involve using 128 samples from the Pile dataset Gao et al. (2020) as the calibration dataset. As shown in Table 11, the integration of quantization still maintains the performance of Attention Drop, i.e., only less than 1% difference in average performance. 2https://huggingface.co/datasets/openai/gsm8k 19 Table 11: Integration of Module Dropping and Quantization. w/Quant denotes quantized models. Method ARC-C HellaSwag OBQA WinoGrande Avg. Baseline w/Quant Attn-4 w/Quant Attn-8 w/Quant Baseline w/Quant Attn-4 w/Quant Attn-8 w/Quant 59.9 59.5 58.8 58.0 58.2 57.7 61.5 61.2 61.0 61.0 60.2 60.1 Llama-2-13B 82.2 81.7 82.0 81.7 82.2 81.9 45.6 45.8 46.2 46.0 47.0 47.0 Mistral-7B 83.2 82. 82.9 82.8 82.3 82.0 43.8 42.8 44.6 43.6 44.2 43.8 77.0 77.1 77.2 76.2 77.4 77.0 78.5 78. 78.0 77.6 78.8 77.5 66.2 66.0 66.1 65.5 66.2 65.9 66.8 66.1 66.6 66.3 66.4 65.9 Table 12: Performance on Long In-Context Task. Method Baseline Attn-2 Attn-4 Attn-6 Attn-8 MLP-2 MLP-4 MLP-6 MLP-8 Context Token Length 7k 4k 9k 14k 2k 26 33 28 24 15 29 21 9 70 68 63 63 50 64 57 41 42 75 71 68 65 53 71 64 48 76 78 75 72 58 70 65 47 48 81 77 73 69 64 78 66 48 Avg. 65.6 65.4 61.4 58.6 48.0 62.4 54.6 38.6 39.0 Attention Drop on Long In-Context Task We evaluate the performance of Attention Drop on the long in-context benchmark. Following LongICLBench Li et al. (2024), we present the results on BANKING77 Casanueva et al. (2020), with the only distinction being that we sample 100 examples from the test set. BANKING77 is banking-domain intent detection dataset comprising 77 classes. We evaluate from 1-shot/label to 5-shot/label, resulting in contextual lengths of 2k, 4k, 7k, 9k, 14k . We employ the togethercomputer/LLaMA-2-7B-32K3 for Layer Drop, which enlarges the context window of Llama-2 to 32k using position interpolation. From the results in Table 12, we observe that Attention Drop maintains performance and outperforms MLP Drop. 3https://huggingface.co/togethercomputer/LLaMA-2-7B-32K"
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}