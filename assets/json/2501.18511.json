{
    "paper_title": "WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in Post-Training",
    "authors": [
        "Benjamin Feuer",
        "Chinmay Hegde"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language model (LLM) post-training, from DPO to distillation, can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples and code are available at https://github.com/penfever/wildchat-50m."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 1 1 5 8 1 . 1 0 5 2 : r WILDCHAT-50M: Deep Dive Into the Role of Synthetic Data in Post-Training Benjamin Feuer1, Chinmay Hegde1 1 NYU Correspondence to: bf996@nyu.edu https://github.com/penfever/wildchat-50m"
        },
        {
            "title": "Abstract",
            "content": "Language model (LLM) post-training can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples. Our work is available at https://github.com/ penfever/wildchat-50m. 1. Introduction Large language model (LLM) post-training encompasses broad suite of algorithmic techniques, and is an active area of current research. Improvements in LLM post-training have led to many breakthrough accomplishments, ranging from recent developments in test-time scaling from OpenAI and Deepseek (OpenAI et al., 2024; DeepSeek-AI et al., 2025) to new algorithms for efficiently aligning LLMs to human preferences (Rafailov et al., 2024). All of them rely on synthetic data during post-training, sometimes in the form of judgments through LLM judges or pairwise comparative outputs. More recently, simple SFT on large model outputs (also called distillation) has proven powerful tool enabling reasoning models (DeepSeek-AI et al., 2025). Unfortunately, the open source ecosystem supporting post-training in general, and data curation in particular, is in its infancy, with industry labs capabilities far outstripping 1 that of most academic labs (Weber et al., 2024; Feuer et al., 2024b; Ivison et al., 2023). stark challenge for smaller labs, especially in academia, has been the difficulty of acquiring publicly available synthetic datasets at large scale. This has posed barriers for researchers who are interested in conducting careful comparative analyses of the synthetic data quality (SDQ) of data generating models (DGMs), as measured by standard academic ground-truth and LLM-as-a-judge benchmarks. To close this gap and better understand the downstream effects of DGM choice on synthetic data quality, we develop WILDCHAT-50M, which is the largest and most diverse publicly available dataset of chat transcripts to date. We also show that WILDCHAT-50M is particularly effective source for post-training data for LLMs. Our core contributions in this work are as follows: 1. We introduce WILDCHAT-50M, the largest publicly available dataset of chat transcripts. Our dataset consists vast corpus of synthetically generated chat transcripts using 50 different open-weight models. ranging in size from 0.5B to 104B parameters, each participating in over 1M multi-turn conversations. Each model participates in 2 or 3 turns per conversation on average, resulting in approximately dataset comprising over 125 million chat transcripts in aggregate. 2. We conduct thorough comparative analysis on the runtime and VRAM efficiency of these models, as well as analyze the distinctive qualities of different model outputs. Our analysis may inform researchers on how to scale up post-training data even further in the future. 3. We demonstrate the power of our dataset by using it as the basis of RE-WILD, novel data mix for supervised fine-tuning (SFT) of LLMs. When we fine-tune Llama-3.1 8B Base on RE-WILD, wehow that our models outperform the SFT mix proposed in Tulu-3 (Lambert et al., 2024), along with several other existing, strong SFT baselines on range of post-training benchmarks. The rest of this paper is organized as follows. Section 2.1 describes high-level details of how the WILDCHAT50M dataset was constructed. Section 2.2 provides deep dive into the generation process of this dataset, along with technical aspects such as model response similarity and throughput efficiency. Section 3 lists and analyzes the results of our SFT experiments. Section 4 overviews related work, and Section 6 gives concluding discussion. 2. The WILDCHAT-50M Dataset 2.1. Data Collection We begin with brief description of the data collection process for WILDCHAT-50M, detailing our source of prompts and the technical details of how responses were collected. Although, to our knowledge, there are no large-scale diverse chat transcripts dataset of synthetically generated LLM responses, there are at least two recent large chat datasets available on which one could potentially base such dataset: WildChat-1M from AllenAI, and LMSys-Chat-1M from LMSys (Zhao et al., 2024; Zheng et al., 2024). Although both have their strengths, we chose to focus on the former because of its rich variety of use-cases (including those which are potentially toxic), its diverse regional and temporal dimensions, and its relatively low levels of contamination for commonly used test sets (Zhao et al., 2024; Lambert et al., 2024). Technical details. Our data collection process was conducted over period of approximately two months on 12x8 H100 shared research cluster. We estimate the total GPU costs of our data collection at 10,000 H100-hours. The homogeneity of the nodes in our study and codebase allow us to make controlled comparisons on important considerations, such as the VRAM efficiency and runtime of each model. All responses and judgments are generated using VLLM (Kwon et al., 2023), highly performant and stable framework for LLM inference. Models are distributed across up to 8 GPUs; we do not conduct infererence using more than one node for any model. We first minimize the number of GPUs required per model, and then heuristically maximize the size of the context window given that number of GPUs and the capacity of the model, resulting in wide range of context windows, depending on the model architecture (2048 tokens to 20,000 tokens). The largest models in our data collection process were queried using FP8 quantization, with checkpoints provided by Neural Magic (Kurtic et al., 2023). All other models were run in bfloat16, using their native checkpoints. We do not further ablate the effect of this quantization on output quality, as this has been studied and reported in prior work (Jin et al., 2024). 2.2. Dataset Analysis WILDCHAT-50M currently includes 19 unique pre-trained models and 35 post-trained model variants, for total of 54 checkpoints represented. With the exception of the responses in the original WildChat dataset, which are sourced from various GPT checkpoints, all responses in WILDCHAT50M are derived from models sourced from HuggingFace; the release dates range from July 2023 to November 2024, and the parameter counts range from 0.5B to 104B. For comprehensive list of all the LLMs we used in the study, please refer to Sec. C. We attempted to select diverse set of models; our main limiting factor was compatibility with our hardware setup, and with VLLM as an inference engine. Naming Conventions. In order to make this paper more readable, we will employ certain naming conventions for the models we describe, enumerated below. We will sometimes utilize abbreviations for some particularly common model names: Qwen2.5-72B-Instruct := Q72, Llama-3.1-8B-Instruct := L8I, Llama-3.3-70B := L70, Qwen2-7B-Instruct := Q7, Cohere-Command-RPlus-104B := CRP, AI21-Jamba-Mini-1.5-52B := JMB. Our model names will follow the following general convention: {SFT target : DGM}. Sometimes we will not specify the SFT target model name; in that case, it will always be Llama-3.1-8B-Base := L8B. Several times, we just report benchmarks for model as is and not do any model post-training; in this case, the naming convention is just { Model name }: None. Most of our experiments were conducted on SFT models trained on 250,000 (250k) conversations. If we used quantity other than 250k, we note it in the model name. Analysis of Throughput Efficiency. Our first comparison describes the relative efficiency of inference across the models in our study. We consider two measures of throughput efficiency; average combined input and output tokens per second (Tok/s), and average time elapsed in seconds per 1000 conversations processed (Time). We compute our averages over random subset of 5000 conversations. The slowest model in our study is Qwen2.5-72B-Instruct with context window length of 20,000, averaging 3,163 Tok/s, and the fastest is Llama-2-7B-Chat, with context window of 2,048, averaging 37,357 Tok/s, more than 10 times faster. Input is significantly faster than output; the mean ratio over all unique pretrained models is 4.68 to 1, with large standard deviation of 3.3. Both Time (σ = 0.90, ρ = 0.73) and Tok/s (σ = 0.41, ρ = 0.80) are strongly correlated with simple proxy for model efficiency, the product of context window length and number of parameters. Analysis of Response Similarity. To the best of our knowl2 edge, the degree of similarity between diverse human respondents to LLM chat prompts has not been rigorously quantified at scale. However, this problem has been studied in the domain of abstractive summarization, where it can be assumed that the similarity would be considerably higher (Maynez et al., 2020; Iskender et al., 2021; Lin & Hovy, 2002; van Halteren & Teufel, 2003; Jing et al., 1998). For summarization tasks, there is no one truth, evidenced by low agreement between humans in producing gold standard summaries by sentence selection, low overlap measures between humans when gold standard summaries are created by reformulation in the summarizers own words, and assigning information overlap between them. It would be natural to assume, therefore, that LLMs that share neither pre-training nor post-training data would likewise may produce substantively different responses to prompts. However, our results show that this does not appear to be the case; LLM responses are unusually similar to one another. See Sec. for deeper analysis of this result, as well as every score (with associated standard deviation) for every model. 3. SFT Experiments We now show that WILDCHAT-50M can be leveraged by researchers as very valuable dataset for studying data curation strategies for LLMs post-training. Our core experiments focus on the SFT stage of post-training (also referred to as instruction tuning). While other forms of post-training (such as tuning to human preferences) are also interesting, we leave their thorough analysis to future work. Following recent work such as Lambert et al. (2024), we focus on curating an SFT data mixture using human-in-theloop process, in contrast with an automated curation process such as that of Xu et al. (2023a). Unlike both of those works, we do not curate new prompts; only new responses to them. 3.1. RE-WILD: new data mixture for SFT Following the recent work of Lambert et al. (2024), we design our SFT data mixture, that we call RE-WILD, using combination of WildChat data with particularly high quality DGM that generates the responses and datasets designed to boost performance on world knowledge benchmarks. Later in this section, we describe the empirical process by which we determined which DGMs had high SDQ, and how. The specific composition of our mix can be found in Tab. 1;. The datasets in this composition were chosen heuristically to emphasize complementary skillsets (math, worldknowledge, and chat/instruction following). Training. We conduct our SFT experiments using modiSource Num. Convs WildChat-Q72 MMLU Auxiliary Train Tulu 3 Persona Hub Algebra 246,750 99,800 20,000 Table 1. Data blending in RE-WILD. Our data blend is simpler than Tulu 3, consisting of just three sources, and is around 40% the size of the Tulu 3 SFT blend. The datasets were chosen heuristically to emphasize complementary skillsets (math, worldknowledge, and chat/instruction following). MMLU Auxiliary Train data is from Hendrycks et al. (2021), Tulu 3 Persona Hub Algebra is from Lambert et al. (2024). fied version of the Axolotl framework (Lian, 2025). We use the AdamW optimizer (Loshchilov & Hutter, 2017) with learning rate of 2e-5, single epoch, and cosine learning rate scheduler, with eight steps of gradient accumulation, in bf16 precision. We also utilize several techniques to optimize training speed, such as gradient checkpointing, flash attention, and in some cases, FSDP (full shard, autowrap). Some artifacts (configurations, logs) are available on our GitHub repo, and others will be made available soon. Each of our SFT runs utilizes one 4xH100 node. The average time to fine-tune model for 250,000 conversations takes approximately 5.5 hours. Evaluation. Benchmarking LLM alignment is challenging task, because of the open-ended nature of the objective and the large number of potential confounds (Lambert et al., 2024), as well as the fact that evaluation hyperparameters are not generally standardized across reported results. To deal with the latter issue, we employ Evalchemy, recently introduced evaluation framework that is standardized, popular, reliable, and validated by reproduction reports for all benchmarks (Guha et al., 2024). Evalchemy itself utilizes the LM Evaluation Harness from Eleuther AI (Gao et al., 2024). In order to make comparisons with past and future work easier, we select benchmarks which are popular and prominent in the recent research literature. We break down the concept of alignment into subcategories such as generalist chat capability, world knowledge, and instruction following. Following recent work, we select mix of ground-truth benchmarks and LLM-judge benchmarks, in order to balance out the potential confounds inherent to each evaluation method (Feuer et al., 2024a; White et al., 2024). For generalist chat capabilities, we use MixEval, AlpacaEval2, and MTBench with GPT-4o-mini as the judge LLM (Ni et al., 2024; Dubois et al., 2024; Zheng et al., 2023). In some cases, we also report the average score over all of our benchmarks. For AlpacaEval2, we report length-controlled win rate. For instruction following and world knowledge, we utilize the recent HuggingFace OpenLLM Leaderboard 3 Figure 1. RE-WILD outperforms strong baselines, on average, across nine benchmarks. In particular, it exhibits strong performance on generalist chat and instruction following benchmarks. MT Bench scores here are divided by 10, so that the scale is similar to our other evaluations. For the exact numeric scores for all models, please refer to our GitHub repository. Figure best viewed in color. Model MTBench AlpacaEval BBH GPQA MATH MUSR IFEval MMLU Pro MixEval Q72 L8I L70 Q7 CRP JMB 6.86 6.26 6.23 6.03 6.05 6.05 41.00 21.12 24.91 17.26 13.44 25. 48.07 45.83 46.57 48.72 49.27 47.36 29.32 30.35 29.97 29.90 30.82 28.28 5.62 4.49 4.08 2.93 4.16 3.93 40.27 37.32 39.44 42.11 38.79 37.50 36.78 38.26 33.83 29.39 28.47 25.69 30.38 32.54 30.85 30.34 31.64 29. 64.50 65.30 64.60 61.30 60.30 57.40 Table 2. The choice of data generating model has strong and unpredictable effects on downstream benchmark performance. We compare the performance of six different DGMs from four different model families, ranging in size from 0.5B to 104B parameters, each fine-tuned on 250k samples from DGM. We find large degree of variance in benchmark performance, with no one model dominating. 4 2 (Fourrier et al., 2024). For IFEval, we report prompt-level strict accuracy because it is more challenging and therefore better exhibits separation between models; however, we also include instance-level loose accuracy in our artifacts, where we observe generally similar trends. Finally, in some figures and tables, we report the average performance for all benchmarks (MixEval, AlpacaEval2-LC, MTBench / 10, OpenLLM LB 2) as Avg. Baselines. Following (Lambert et al., 2024), we utilize strong baseline checkpoints also utilized in that paper; Tulu 3 SFT, Magpie Align SFT from Xu et al. (2024), and Ultrachat from (Cui et al., 2023). 3.2. Key Findings RE-WILD is strong SFT data mix. Our first result is that RE-WILD constitutes particularly attactive data mixture for SFT. In the spider chart shown in Fig. 1, we show that RE-WILD outperforms several strong SFT baselines on aggregate; in particular, it excels on generalist chat tasks as well as instruction following tasks. Because prior work has shown that LLM judges can introduce implicit biases into their judgments, we include mix of ground truth and LLM judged benchmarks (Feuer et al., 2024a). We find that REWILD performs well on both measures, indicating robust support for the claim that RE-WILD provides superior model for generalist chat and instruction following. How much data from WILDCHAT-50M should be used? Scaling up dataset size is generally acceptable method for improving SFT model performance in most settings. But what sort of scaling laws apply for partially synthetic datasets such as WILDCHAT-50M? We ablate the effect of data scaling at 100k, 250k and 500k samples across four DGMs. In Fig. 2, we see that average performance steadily improves with scale, as expected, for most models. In this figure, GPT is used to denote the original WildChat dataset, which were generated using blend of different GPT checkpoints, primarily 3.5. The upper asymptote for performance (if it exists) is beyond the maximum we have encountered in our experiments. How much does the choice of data generating model impact downstream performance? Can we be certain that it is not the prompts, or perhaps some quirk of our training procedure, that have led to improved SFT performance using RE-WILD? To evaluate this concern, we compare the performance of six unique pretrained models from four distinct model families, including Qwen-2.5-72B-Instruct from Alibaba, Llama-3.3-70B-Instruct from Meta, Command-RPlus from Cohere, and Jamba-1.5-Mini from AI21 (Qwen Team, 2024; Dubey et al., 2024; Duran et al., 2023; Lieber et al., 2024). The models range from 7B to 104B active parameters. The choice of DGM has large effect, even when controlling for potential confounds such as the numFigure 2. Data scaling improves SFT performance. The effect is, however, somewhat dependent on SDQ for DGMs such as GPT 3.5, the benefits taper off relatively quickly, but for the other three DGMs we consider, they continue to increase. Avg is the average performance over (MixEval, AlpacaEval2-LC, MTBench / 10, OpenLLM LB 2). ber of parameters for the model and the size of the context window. Furthermore, we find that no model dominates the benchmark, and that parameter count is not perfect indicator of data quality. On three of the nine benchmarks we consider, the best performing model has fewer than 10B parameters. See Tab. 2. How much does the length of the context window impact performance? Qwen-2.5-72B supports large context window of over 131K tokens, which we were able to take advantage of during data generation. We experiment with truncating all Qwen-2.5-72B responses so that they are no longer than Llama-3.3-70B responses (with context window of 8,192 tokens compared to 20,000 for Qwen in our experiments). Surprisingly, we find that the effect on the SFT model is slightly positive (.404 vs .400 averaged over 9 benchmarks). This is perhaps because we use context window of 8,192 tokens in our SFT base model, Llama-3.1-8B. Do models benefit from blending DGMs? One impetus behind creating very large prompt-response dataset like that of Zhao et al. (2024) is the intuition that more samples and more interactions will generally lead to better models. In the case of prompt diversity, this appears to be true (Feuer et al., 2024a), perhaps because it makes the model more robust to inconsistencies in prompting. But do the benefits of heterogeneous scaling extend to responses? In other words, do models trained on blended DGM responses outperform the sum of their parts? We conduct experiments to test this hypothesis, the results of which can be found in Sec. D, summarized here. We find that blending offers no benefit; the whole behaves almost exactly as the sum of its parts. This 5 finding indicates that SDQ depends primarily on prompt diversity, and it is most effective to optimize, rather than generalize, responses. benchmark results when controlling for dataset size, and appears also to extend to larger post-trained checkpoint in the same model family; see Sec. for the complete results. Are models with strong performance on certain benchmarks better teachers for those benchmarks? We evaluate this question by comparing Llama-3.18B-Base model that is fine-tuned on Qwen-2.5-72B responses, to Llama3.1 8B-Base model fine-tuned on Llama-3.3-70B responses, and then comparing the two source models directly. We report the agreement rate between model checkpoints; we say that there is agreement if the fine-tuned model and the base model are both better or both worse than their counterparts, and there is not agreement otherwise. Across six benchmarks, we report an agreement rate of .5, which is at chance level, indicating that fine-tuned models do not necessarily inherit the strengths and weaknesses of their synthetic data generators. Is styling inherited during SFT? Effective use of presentational styling elements such as HTML tags, attributes and inline properties can have strong effect on texts readability and clarity. We investigate whether such formal styling behaviors are inherited from the DGM during the SFT process. We select subset of 80 turns from MTBench and examine the behavior of two DGMs (Qwen 2.5 72B and Llama 3.3 70B) and their finetunes. We convert the model responses from Markdown (which they commonly use in their responses) to HTML and report the absolute and proportional frequency of each styling tag. Absolute frequency is the raw count for each feature (see rows 1:4). Proportional frequency, for rows A, B, and feature , is given by A[F ] B[F ]. The closer this quantity is to 1, the more similar the model responses. We report the raw results from this experiment in Tab. 3. Overall, we observe that SFTs track the styling of their DGMs very closely indeed; across all style features, the mean proportional frequency (MPF) of Qwen SFT compared to Qwen-2.5-72B is .91 across all features, and for Llama, this score is 1.05. When we compare the SFTs to each other, by contrast, it is 2.61, indicating that the responses are much less similar. Do models learn better from DGMs in the same model family? Recent work from Tajwar et al. (2024) has shown that approaches that use on-policy sampling or attempt to push down the likelihood on certain responses (i.e., employ negative gradient) outperform offline and maximum likelihood objectives. In this section, we inquire whether this finding extends not only to direct on-policy sampling, but something we might call approximate on-policy sampling, where pretrained base model is fine-tuned on its own post-trained outputs, or those of model from similar family. Indeed, when we experiment with Qwen-2-7B and Llama-3-8B, we find that this approach produces stronger 3.3. Why do some models outperform others as sources of synthetic data? We showed above that the choice of DGM can have dramatic effect on the performance of downstream LLMs finetuned on their responses. We now explore possible explanations for why SDQ varies so dramatically, even between superficially similar models. We begin by eliminating some of the more obvious explanations. For example, we showed above that model parameter count and response length is not reliably predictive of performance, so these are likely not primary factors contributing to data quality. Inspection of the benchmark results in Tab. 2 shows greater variance in performance on chat-quality and instruction following benchmarks (such as IFEval, MixEval, MTBench, and AlpacaEval). natural explanation for this observation would be that SDQ is domain-specific, and is inherited from the DGM. If this was the case however, then benchmark performance of fine-tunes would generally agree with those of the DGMs on those benchmarks. In reality, benchmark performance is not reliably inherited from the DGM. On the AlpacaEval leaderboard, Qwen 2.5-72B and Llama 3.3-70B are essentially tied; but Qwen 2.5-72B is superior DGM (as measured by AlpacaEval performance) (Dubois et al., 2024). potential resolution to this phenomenon lies in noting that LLM judges utilize range of judgment criteria, both explicit and implicit (Feuer et al., 2024a). Therefore, it is possible that supervised fine-tuning can significantly improve models (generalist) benchmark score by improving on only one or two criteria (e.g., comprehensiveness and readability), while its DGM, whose overall score may be already higher, comparatively underperforms because of limitations in some other criteria such as factuality. In order to determine whether this is indeed the case in our evaluations, we conduct an experiment using 80 turns from MT Bench drawn from four models (i) Q72 : None, (ii) L70 : None (DGMs), (iii) L8B : Q72, and (iv) L8B : L70 each fine-tuned on 500k samples from WILDCHAT-50M. We provide the complete conversations in the appendix for reference. In Tab. 4, we provide raw results in the form of MTBench score as well as PrefRt, which stands for Preference Rate, or the rate at which models responses are preferred over anothers (100 means always preferred, 0 means never preferred). L8L is the rate at which models response is preferred over L8B : L70, L8Q is the rate at which models 6 Model Strong Em Ol Ul h1 h2 h4 Q72 : None L70 : None L8B : L70 L8B : Q72 PF L8B : L70, L70 : None PF L8B : Q72, Q72 : None PF L8B : Q72, L8B : L70 741 406 331 808 0.815 1.090 2.441 53 38 34 60 0.895 1.132 1.765 83 76 75 77 0.987 0.928 1. 230 116 109 260 0.940 1.130 2.385 24 30 27 20 0.900 0.833 0.741 0 38 13 0 0.342 1.000 0.000 135 22 32 149 1.455 1.104 4.656 26 4 3 33 0.750 1.269 11.000 1217 1222 1210 1237 0.990 1.016 1. len 6492 6411 6207 6585 0.968 1.014 1.061 Table 3. SFT models strongly inherit formal stylistic elements from their DGMs. This table indicates how frequently certain Markdown stylistic elements appear in LLM responses (converted to HTML tags for greater clarity). The columns are names of HTML tags and inline properties, and the cells are frequency counts. PF stands for proportional frequency, the ratio of the first and second model listed (order here is presumed to be arbitrary). response is preferred over L8B : Q72, L70B is the rate at which models response is preferred over L70: None, and Q72B is the rate for Q72 : None. Model MTBench PrefRt-L8L PrefRt-L8Q PrefRt-L70B PrefRt-Q72B FlipCt L8B : L70 L8B : Q72 L70 : None Q72 : None 6.38 6.72 7.64 7.83 N/A 40 52.5 61.25 22.5 N/A 38.75 43.75 8.75 16.25 N/A 25 15 13.75 21.25 N/A N/A N/A 5 6 We can see that L8B : Q72 outperforms L8B : L70, and Q72 : None outperforms L70 : None, both in terms of average MTBench score, and in terms of win rate. Interestingly, the proportional improvement is similar in both comparisons, suggesting degree of heritability to LLM judge preferences. When we manually inspected random sample of model outputs, we often agreed with the judge (the reader may form their own opinions by referring to examples in Sec. E). The MT bench prompts, responses and complete judgments are available in our GitHub repository. When we include the DGMs themselves in the comparisons, the conclusions are somewhat different. L70 : None outperforms L8B : Q72 on both metrics. In particular, on 5 of the 80 turns, L70 : None flips the ranking (rather than simply breaking tie), compared to L8B : L70. We note first that such judgment flips are quite rare. We manually inspect these 5 flips and find that on 4 of the 5, the LLM judge cited factuality as key reason for L8B : L70s low score. In other words, even though the style of L8B : Q72 is still generally preferred by the judge, the superior factuality of L70 : None drives an overall change in rank. We analyze the same phenomenon through another lens in Fig. 3 and Fig. 4). Here, we visualize the frequency of common words in the judgments where the score differed between models. Common negations of words were counted separately (although these were rare). We discover that L8B : L70 responses are more commonly associated with words such as lacks, few, misleading and concise, whereas L8B : Q72 responses are more commonly associated with words such as appropriate, comprehensive, complete and detailed. One limitation of this analysis is that it fails to capture common context that tends to accompany these words in the judgments; for example, the reason that clearer is more common in L8B : L70 judgments is that the judge frequently Table 4. LLM judge preferences for DGMs and SFTs. For column name definitions, we refer the reader to the main text, section Sec. 3. Overall, we observe that Qwen responses are generally preferred by LLM judges, and that the rate at which they are preferred is similar from DGM to SFT. Last but not least, we note that reversals of judgment from SFT to DGM (FlipCt) are uncommon. employed semantic variations of phrases like could have been clearer, not because the responses themselves were clearer. The same applies to critical. Figure 3. Key words more common in L8B : L70 judgments. The more negative tone of these judgments emphasizes words like clearer (as in, could have been clearer), lacks, convoluted and repetitive. From these results, it seems that SDQ on set of generalist chat prompts such as WildChat is largely function of domain-agnostic factors, such as the comprehensiveness of the model response, the clarity of the structure, and the tone and stylistic tendencies of the language. 7 5. Limitations There are few key limitations of this work which we wish to highlight here. Because of practical constraints, we were not able to report results using other post-training approaches other than SFT. It is likely that the relative effect of DGM choice would differ depending on the post-training regime. Although our benchmark suite is standardized, balanced and large, it does not encompass all use cases. In particular, we did not evaluate performance on highly specialized tasks, such as coding or legal reasoning. We consider both of these limitations as pointing towards useful directions for future work. It might also be beneficial to consider the differential advantages and disadvantages of diversifying DGMs on smaller, more focused datasets, with and without ground truth. 6. Conclusions In this work, we make several valuable practical and empirical contributions to the fields understanding of synthetic data. In particular, we provide robust empirical evidence that the choice of DGM is an extremely important factor in downstream SFT model performance on generalist chat benchmarks; simply by selecting good DGM, we compensate for small dataset size and outperform more complex methods and carefully curated SFT mixes. Equally important, we provide novel insight into why certain DGMs produce much higher SDQ than others; our experiments indicate that comprehensiveness, clarity, tone and prompt responsiveness are highly heritable during the SFT process, even on generalist data, unlike skills such as world knowledge or mathematics, which are heritable only when data is curated for that particular purpose. Finally, we provide novel comparative insights into LLMs, reporting high degree of similarity in the prompt responses of diverse LLMs. Taken together with the prior observation, the distinction between high and low SDQ may be subtle, and worthy of future research."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work. In particular, we wish to acknowledge the fact that we are building upon dataset (WildChat) which is known to contain examples of user inputs on potentially upsetting topics, including but not limited to sex, violence, and bigoted claims (Zhao et al., 2024). Following the authors of that work, we will require approval before researchers are allowed to access our datasets, and will release our data under the AI2 ImpACT License, which explicitly forbids certain use cases for this data. Figure 4. Key words more common in L8B : Q72 judgments. These judgments tended to be more positive; emphasis was placed on words like appropriate, necessary, comprehensive and accurate. 4. Related Work Our work explores LLM post-training, research area which has witnessed dynamic growth since 2022. SFT, sometimes referred to as instruction tuning (Mishra et al., 2022; Wei et al., 2022; Sanh et al., 2022; Wang et al., 2022; Longpre et al., 2023), in which language models are trained on samples including task instructions and their corresponding responses, has been shown to allow LLMs to generalize better to unseen tasks. Initially those samples were drawn from traditional NLP tasks with verified ground truth answers (Wang et al., 2023b). However, over time, it has become clear that more heterogeneous approach, both to prompt and model responses, tends to lead to superior outcomes, and that combining instruction tuning datasets, either strategically or randomly, can lead to strong results (Taori et al., 2023; Conover et al., 2023; Wang et al., 2023a). Unfortunately, post-training is an area where open science continues to trail closed advances made in frontier industry labs (Chiang et al., 2024). Models trained on open data underperform those trained on closed data, both in the pretraining and in the post-training stage. Numerous algorithmic advances have been introduced into the recent literature which attempt to serve as scalable instruction tuning methods (Tunstall et al., 2023; Xu et al., 2023b; Zhou et al., 2023; Yasunaga et al., 2024). Despite the variety (and abundance) of algorithmic approaches, many fail to scale to the high-data regime (Feuer et al., 2024a). Therefore, in this work we turn to the more basic question of the quality of datasets used in post-training. While there is still considerable ground to be covered, we hope that the size and diversity of our dataset, combined with its strong performance on SFT benchmarks, will encourage researchers to use it as basis for future work. 8 In addition, our data will be subject to any additional restrictions from the licenses of the models we utilize. We also preserve, without modification, the original user inputs from WildChat (Zhao et al., 2024), and therefore in our work there remains the possibility that users may have inadvertently included personal information within their conversations which was not detected by the (considerable) safeguards put in place by the WildChat authors. We will make this clear to users upon dataset release."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is supported in part by the AI Research Institutes program supported by NSF and USDA-NIFA under Award No. 2021-67021-35329, Google Cyber NYC gift grant, and NSF grant # 2154119. The authors gratefully acknowledge the support of NYU ITs High Performance Computing resources, services, and staff. The authors also gratefully acknowledge compute support provided by the National Artificial Intelligence Research Resource (NAIRR) Pilot, Groq LPU Inference Engine, and the Empire AI Consortium."
        },
        {
            "title": "References",
            "content": "Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., et al. Nemotron-4 340b technical report. arXiv preprint, 2024. AI, ., :, Young, A., Chen, B., Li, C., Huang, C., Zhang, G., Zhang, G., Wang, G., Li, H., Zhu, J., Chen, J., Chang, J., Yu, K., Liu, P., Liu, Q., Yue, S., Yang, S., Yang, S., Xie, W., Huang, W., Hu, X., Ren, X., Niu, X., Nie, P., Li, Y., Xu, Y., Liu, Y., Wang, Y., Cai, Y., Gu, Z., Liu, Z., and Dai, Z. Yi: Open foundation models by 01.ai, 2025. URL https://arxiv.org/abs/2403.04652. Banerjee, S. and Lavie, A. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Goldstein, J., Lavie, A., Lin, C.-Y., and Voss, C. (eds.), Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pp. 6572, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. URL https://aclanthology.org/ W05-0909/. Cai, Z., Cao, M., Chen, H., Chen, K., Chen, K., Chen, X., Chen, X., Chen, Z., Chen, Z., Chu, P., Dong, X., Duan, H., Fan, Q., Fei, Z., Gao, Y., Ge, J., Gu, C., Gu, Y., Gui, T., Guo, A., Guo, Q., He, C., Hu, Y., Huang, T., Jiang, T., Jiao, P., Jin, Z., Lei, Z., Li, J., Li, J., Li, L., Li, S., Li, W., Li, Y., Liu, H., Liu, J., Hong, J., Liu, K., Liu, K., Liu, X., Lv, C., Lv, H., Lv, K., Ma, L., Ma, R., Ma, Z., Ning, W., Ouyang, L., Qiu, J., Qu, Y., Shang, F., Shao, Y., Song, D., Song, Z., Sui, Z., Sun, P., Sun, Y., Tang, H., Wang, B., Wang, G., Wang, J., Wang, J., Wang, R., Wang, Y., Wang, Z., Wei, X., Weng, Q., Wu, F., Xiong, Y., Xu, C., Xu, R., Yan, H., Yan, Y., Yang, X., Ye, H., Ying, H., Yu, J., Yu, J., Zang, Y., Zhang, C., Zhang, L., Zhang, P., Zhang, P., Zhang, R., Zhang, S., Zhang, S., Zhang, W., Zhang, W., Zhang, X., Zhang, X., Zhao, H., Zhao, Q., Zhao, X., Zhou, F., Zhou, Z., Zhuo, J., Zou, Y., Qiu, X., Qiao, Y., and Lin, D. Internlm2 technical report, 2024. URL https://arxiv.org/abs/2403.17297. Chiang, W.-L., Zheng, L., Sheng, Y., Angelopoulos, A. N., Li, T., Li, D., Zhang, H., Zhu, B., Jordan, M., Gonzalez, J. E., et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint, 2024. Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., Introducing the worlds and Xin, R. first truly open instruction-tuned llm. URL, 2023. https://www.databricks.com/blog/2023/04/12/dollyfirst-open-commercially-viable-instruction-tuned-llm. Free dolly: Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M. Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint, 2023. Dai, W., Lee, N., Wang, B., Yang, Z., Liu, Z., Barker, J., Rintamaki, T., Shoeybi, M., Catanzaro, B., and Ping, W. Nvlm: Open frontier-class multimodal llms, 2024. URL https://arxiv.org/abs/2409.11402. DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Bao, H., Xu, H., Wang, H., Ding, H., Xin, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Wang, J., Chen, J., Yuan, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Ye, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Zhao, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Xu, Y., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. GLM, T., :, Zeng, A., Xu, B., Wang, B., Zhang, C., Yin, D., Zhang, D., Rojas, D., Feng, G., Zhao, H., Lai, H., Yu, H., Wang, H., Sun, J., Zhang, J., Cheng, J., Gui, J., Tang, J., Zhang, J., Sun, J., Li, J., Zhao, L., Wu, L., Zhong, L., Liu, M., Huang, M., Zhang, P., Zheng, Q., Lu, R., Duan, S., Zhang, S., Cao, S., Yang, S., Tam, W. L., Zhao, W., Liu, X., Xia, X., Zhang, X., Gu, X., Lv, X., Liu, X., Liu, X., Yang, X., Song, X., Zhang, X., An, Y., Xu, Y., Niu, Y., Yang, Y., Li, Y., Bai, Y., Dong, Y., Qi, Z., Wang, Z., Yang, Z., Du, Z., Hou, Z., and Wang, Z. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. URL https://arxiv.org/abs/2406.12793. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint, 2024. Dubois, Y., Galambosi, B., Liang, P., and Hashimoto, T. B. Length-controlled alpacaeval: simple way to debias automatic evaluators, 2024. URL https://arxiv. org/abs/2404.04475. Duran, E., Ambrosi, E., and Benaich, N. The ai language gap: Bridging humans and machines in the age of large language models, 2023. URL https://cohere.com/research/papers/ the-ai-language-gap.pdf. Feuer, B., Goldblum, M., Datta, T., Nambiar, S., Besaleli, R., Dooley, S., Cembalest, M., and Dickerson, J. P. Style outweighs substance: Failure modes of llm judges in alignment benchmarking, 2024a. URL https://arxiv.org/abs/2409.15268. Feuer, B., Xu, J., Cohen, N., Yubeaton, P., Mittal, G., and Hegde, C. Select: large-scale benchmark of data curation strategies for image classification, 2024b. URL https://arxiv.org/abs/2410.05057. and Wolf, T. Fourrier, C., Habib, N., Lozovskaya, A., Szafer, llm leaderboard Open K., https://huggingface.co/spaces/ 2. open-llm-leaderboard/blog, 2024. Accessed: 2025-01-13. Ganesan, K. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks, 2018. URL https://arxiv.org/abs/1803.01937. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Noach, A. L., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL, 2024. https://zenodo.org/records/12608602. Guha, E., Raoof, N., Mercat, J., Frankel, E., Keh, S., Grover, S., Smyrnis, G., Vu, T., Marten, R., Saad-Falcon, J., Choi, C., Arora, K., Merrill, M., Deng, Y., Suvarna, A., Bansal, H., Nezhurina, M., Choi, Y., Heckel, R., Oh, S., Hashimoto, T., Jitsev, J., Shankar, V., Dimakis, A., Sathiamoorthy, M., and Schmidt, L. Evalchemy, November 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. Iskender, N., Polzehl, T., and Moller, S. Reliability of human evaluation for text summarization: Lessons learned and challenges ahead. In Belz, A., Agarwal, S., Graham, Y., Reiter, E., and Shimorina, A. (eds.), Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pp. 8696, Online, April 2021. Association for Computational Linguistics. URL https: //aclanthology.org/2021.humeval-1.10/. Ivison, H., Wang, Y., Pyatkin, V., Lambert, N., Peters, M., Dasigi, P., Jang, J., Wadden, D., Smith, N. A., Beltagy, I., et al. Camels in changing climate: Enhancing lm adaptation with tulu 2. arXiv preprint, 2023. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., d. l. Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint, 2023. Jin, R., Du, J., Huang, W., Liu, W., Luan, J., Wang, B., and Xiong, D. comprehensive evaluation of quantization strategies for large language models, 2024. URL https: //arxiv.org/abs/2402.16775. Jing, H., Barzilay, R., McKeown, K. R., and Elhadad, M. Summarization evaluation methods: Experiments and analysis. In Radev, D. R. and Hovy, E. H. (eds.), Working Notes of the AAAI Spring Symposium on Intelligent Text Summarization, pp. 6068, 1998. 10 Kurtic, E., Kuznedelev, D., Frantar, E., Goin, M., and Alistarh, D. Sparse fine-tuning for inference acceleration of large language models, 2023. URL https: //arxiv.org/abs/2310.06927. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., Smith, N. A., Wang, Y., Dasigi, P., and Hajishirzi, H. Tulu 3: Pushing frontiers in open language model post-training, 2024. URL https://arxiv.org/abs/2411.15124. Lian, W. e. a. Axolotl. https://github.com/ axolotl-ai-cloud/axolotl, 2025. Accessed: 2025-01-13. Lieber, O., Lenz, B., Bata, H., Cohen, G., Osin, J., Dalmedigos, I., Safahi, E., Meirom, S., Belinkov, Y., ShalevShwartz, S., Abend, O., Alon, R., Asida, T., Bergman, A., Glozman, R., Gokhman, M., Manevich, A., Ratner, N., Rozen, N., Shwartz, E., Zusman, M., and Shoham, Y. Jamba: hybrid transformer-mamba language model, 2024. URL https://arxiv.org/ abs/2403.19887. Lin, C.-Y. and Hovy, E. Manual and automatic evaluation of summaries. In Proceedings of the Document Understanding Conference (DUC), 2002. Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint, 2023. Loshchilov, I. and Hutter, F. Fixing weight decay regularization in adam. CoRR, abs/1711.05101, 2017. URL http://arxiv.org/abs/1711.05101. Maynez, J., Narayan, S., Bohnet, B., and McDonald, R. On faithfulness and factuality in abstractive summarization. In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 19061919, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main. 173. URL https://aclanthology.org/2020. acl-main.173/. Meng, Y., Xia, M., and Chen, D. Simpo: Simple preference optimization with reference-free reward. arXiv preprint, 2024. Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Crosstask generalization via natural language crowdsourcing instructions. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1, pp. 34703487, Dublin, Ireland, Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long. 244. URL, May 2022. https://aclanthology.org/2022.acllong.244. Mistral, A. I. Ministraux: Pushing the boundaries of efficient transformer design. URL, 2024. Retrieved 2024-1117. https://mistral.ai/news/ministraux/. NexusFlow AI. Athene: Pioneering the Future of AI Workflows, 2025. URL https://nexusflow.ai/ blogs/athene. Accessed: 2025-01-29. Ni, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neubig, G., and You, Y. Mixeval: Deriving wisdom of the crowd from llm benchmark mixtures, 2024. URL https:// arxiv.org/abs/2406.06565. OpenAI, :, Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., Iftimie, A., Karpenko, A., Passos, A. T., Neitz, A., Prokofiev, A., Wei, A., Tam, A., Bennett, A., Kumar, A., Saraiva, A., Vallone, A., Duberstein, A., Kondrich, A., Mishchenko, A., Applebaum, A., Jiang, A., Nair, A., Zoph, B., Ghorbani, B., Rossen, B., Sokolowsky, B., Barak, B., McGrew, B., Minaiev, B., Hao, B., Baker, B., Houghton, B., McKinzie, B., Eastman, B., Lugaresi, C., Bassin, C., Hudson, C., Li, C. M., de Bourcy, C., Voss, C., Shen, C., Zhang, C., Koch, C., Orsinger, C., Hesse, C., Fischer, C., Chan, C., Roberts, D., Kappler, D., Levy, D., Selsam, D., Dohan, D., Farhi, D., Mely, D., Robinson, D., Tsipras, D., Li, D., Oprica, D., Freeman, E., Zhang, E., Wong, E., Proehl, E., Cheung, E., Mitchell, E., Wallace, E., Ritter, E., Mays, E., Wang, F., Such, F. P., Raso, F., Leoni, F., Tsimpourlas, F., Song, F., von Lohmann, F., Sulit, F., Salmon, G., Parascandolo, G., Chabot, G., Zhao, G., Brockman, G., Leclerc, G., Salman, H., Bao, H., Sheng, H., Andrin, H., Bagherinezhad, H., Ren, H., Lightman, H., Chung, H. W., Kivlichan, I., OConnell, I., Osband, I., Gilaberte, I. C., Akkaya, I., Kostrikov, I., Sutskever, I., Kofman, I., Pachocki, J., Lennon, J., Wei, J., Harb, J., Twore, J., Feng, J., Yu, J., Weng, J., Tang, J., Yu, J., Candela, J. Q., Palermo, J., Parish, J., Heidecke, J., Hallman, J., Rizzo, J., Gordon, J., Uesato, J., Ward, J., Huizinga, J., Wang, J., Chen, K., Xiao, K., Singhal, K., Nguyen, K., Cobbe, K., Shi, K., Wood, K., Rimbach, K., Gu-Lemberg, K., Liu, K., Lu, K., Stone, K., Yu, K., Ahmad, L., Yang, L., Liu, L., Maksin, L., Ho, L., Fedus, L., Weng, L., Li, L., McCallum, L., Held, L., Kuhn, L., Kondraciuk, L., Kaiser, L., Metz, L., Boyd, M., Trebacz, M., Joglekar, M., Chen, M., Tintor, M., Meyer, M., Jones, 11 M., Kaufer, M., Schwarzer, M., Shah, M., Yatbaz, M., Guan, M. Y., Xu, M., Yan, M., Glaese, M., Chen, M., Lampe, M., Malek, M., Wang, M., Fradin, M., McClay, M., Pavlov, M., Wang, M., Wang, M., Murati, M., Bavarian, M., Rohaninejad, M., McAleese, N., Chowdhury, N., Chowdhury, N., Ryder, N., Tezak, N., Brown, N., Nachum, O., Boiko, O., Murk, O., Watkins, O., Chao, P., Ashbourne, P., Izmailov, P., Zhokhov, P., Dias, R., Arora, R., Lin, R., Lopes, R. G., Gaon, R., Miyara, R., Leike, R., Hwang, R., Garg, R., Brown, R., James, R., Shu, R., Cheu, R., Greene, R., Jain, S., Altman, S., Toizer, S., Toyer, S., Miserendino, S., Agarwal, S., Hernandez, S., Baker, S., McKinney, S., Yan, S., Zhao, S., Hu, S., Santurkar, S., Chaudhuri, S. R., Zhang, S., Fu, S., Papay, S., Lin, S., Balaji, S., Sanjeev, S., Sidor, S., Broda, T., Clark, A., Wang, T., Gordon, T., Sanders, T., Patwardhan, T., Sottiaux, T., Degry, T., Dimson, T., Zheng, T., Garipov, T., Stasi, T., Bansal, T., Creech, T., Peterson, T., Eloundou, T., Qi, V., Kosaraju, V., Monaco, V., Pong, V., Fomenko, V., Zheng, W., Zhou, W., McCabe, W., Zaremba, W., Dubois, Y., Lu, Y., Chen, Y., Cha, Y., Bai, Y., He, Y., Zhang, Y., Wang, Y., Shao, Z., and Li, Z. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. Qwen Team. Qwen2.5: tion models, September 2024. https://qwenlm.github.io/blog/qwen2.5/. party of foundaAlibaba, 2024. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Sanh, V., Webson, A., Raffel, C., Bach, S., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Scao, T. L., Biderman, S., Gao, L., Wolf, T., and Rush, A. M. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. Tajwar, F., Singh, A., Sharma, A., Rafailov, R., Schneider, J., Xie, T., Ermon, S., Finn, C., and Kumar, A. Preference fine-tuning of llms should leverage suboptimal, on-policy data, 2024. URL https://arxiv.org/abs/2404. 14367. Team, T. M. G., Hardin, C., Dadashi, R., Bhupatiraju, S., Sifre, L., Rivi`ere, M., Kale, M. S., Love, J., Tafti, P., and Hussenot, L. and et al. Gemma, 10:34740, 2024. https://www.kaggle.com/m/3301. Teknium, R., Quesnelle, J., and Guang, C. Hermes 3 technical report. arXiv preprint, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint, 2023. Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., et al. Zephyr: Direct distillation of lm alignment. arXiv preprint, 2023. van Halteren, H. and Teufel, S. Examining the consensus between human summaries: initial experiments with factoid analysis. In Proceedings of the HLT-NAACL 03 Text Summarization Workshop, pp. 5764, 2003. URL https://aclanthology.org/W03-0508/. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S., Arunkumar, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Pal, K. K., Patel, M., Moradshahi, M., Parmar, M., Purohit, M., Varshney, N., Kaza, P. R., Verma, P., Puri, R. S., Karia, R., Doshi, S., Sampat, S. K., Mishra, S., S. Reddy, A., Patro, S., Dixit, T., and Shen, X. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 50855109, United Arab Emirates. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.emnlp-main.340. URL, December 2022. Abu Dhabi. https://aclanthology.org/2022.emnlp-main.340. Wang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu, K., Wadden, D., MacMillan, K., Smith, N. A., Beltagy, I., et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36::7476474786, 2023a. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions, 2023b. URL https://arxiv.org/abs/2212.10560. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama model, 2023. Weber, M., Fu, D., Anthony, Q., Oren, Y., Adams, S., Alexandrov, A., Lyu, X., Nguyen, H., Yao, X., Adams, V., Athiwaratkun, B., Chalamala, R., Chen, K., Ryabinin, M., 12 alignment, 2023. URL https://arxiv.org/abs/ 2305.11206. Dao, T., Liang, P., Re, C., Rish, I., and Zhang, C. Redpajama: an open dataset for training large language models, 2024. URL https://arxiv.org/abs/2411. 12372. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners, 2022. URL https:// arxiv.org/abs/2109.01652. White, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain, S., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S., Hegde, C., LeCun, Y., Goldstein, T., Neiswanger, W., and Goldblum, M. Livebench: challenging, contaminationfree llm benchmark, 2024. URL https://arxiv. org/abs/2406.19314. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint, 2023a. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions, 2023b. URL https://arxiv.org/abs/2304.12244. Xu, Z., Jiang, F., Niu, L., Deng, Y., Poovendran, R., Choi, Y., and Lin, B. Y. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. ArXiv, abs/2406.08464, 2024. URL https://api.semanticscholar. org/CorpusID:270391432. Yasunaga, M., Shamis, L., Zhou, C., Cohen, A., Weston, J., Zettlemoyer, L., and Ghazvininejad, M. Alma: Alignment with minimal annotation, 2024. URL https: //arxiv.org/abs/2412.04305. Zhao, W., Ren, X., Hessel, J., Cardie, C., Choi, Y., and Deng, Y. Wildchat: 1m chatgpt interaction logs in the wild, 2024. URL https://arxiv.org/abs/2405.01470. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. URL https: //arxiv.org/abs/2306.05685. Zheng, L., Chiang, W.-L., Sheng, Y., Li, T., Zhuang, S., Wu, Z., Zhuang, Y., Li, Z., Lin, Z., Xing, E. P., Gonzalez, J. E., Stoica, I., and Zhang, H. Lmsys-chat-1m: largescale real-world llm conversation dataset, 2024. URL https://arxiv.org/abs/2309.11998. Zhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X., Efrat, A., Yu, P., Yu, L., Zhang, S., Ghosh, G., Lewis, M., Zettlemoyer, L., and Levy, O. Lima: Less is more for 13 Model avg rouge1 std rouge1 avg rougeL std rougeL avg meteor std meteor 0.37 Mixtral-8x7B-Instruct 0.37 Llama-3.1-Nemotron-70B-Instruct 0.34 Qwen2.5-72B-Instruct 0.34 Mistral-7B-wizardlm 0.34 Mistral-7B-sharegpt-vicuna 0.33 Mistral-7B-Base-SFT-IPO 0.33 internlm2 5-20b-chat 0.33 Llama-3.1-70B-Instruct 0.33 Llama-3.3-70B-Instruct 0.32 Llama-2-7b-chat-hf 0.32 Mistral-7B-Base-SFT-CPO 0.32 Qwen2-7B-Instruct 0.31 Llama-3-8B-ShareGPT-112K Qwen2.5-Coder-32B-Instruct 0.31 Llama-3-8B-Magpie-Pro-SFT-200K 0.30 0.30 google gemma-2-9b-it 0.29 AI21-Jamba-1.5-Mini 0.28 OpenHermes-2-Mistral-7B 0.27 Llama-3-Base-8B-SFT-ORPO 0.27 google gemma-2-27b-it 0.27 OpenHermes-2.5-Mistral-7B 0.26 Mistral-7B-Base-SFT-SLiC-HF 0.25 Mistral-7B-Base-SFT-KTO 0.24 Ministral-8B-Instruct-2410 0.23 Llama-3-Base-8B-SFT-RDPO 0.19 Mistral-7B-Base-SFT-RRHF 0.11 0.06 0.09 0.07 0.06 0.11 0.08 0.10 0.09 0.09 0.07 0.08 0.09 0.10 0.15 0.10 0.13 0.09 0.07 0.04 0.06 0.13 0.05 0.13 0.06 0. 0.19 0.23 0.17 0.22 0.18 0.17 0.15 0.16 0.17 0.19 0.17 0.15 0.18 0.15 0.18 0.16 0.17 0.16 0.14 0.13 0.15 0.14 0.13 0.12 0.13 0.08 0.06 0.05 0.05 0.06 0.03 0.05 0.03 0.05 0.04 0.07 0.04 0.04 0.07 0.05 0.09 0.06 0.08 0.06 0.03 0.02 0.04 0.07 0.04 0.07 0.03 0.02 0.20 0.20 0.17 0.16 0.18 0.19 0.20 0.16 0.20 0.20 0.18 0.17 0.15 0.18 0.17 0.14 0.14 0.15 0.22 0.15 0.13 0.13 0.11 0.13 0.19 0.16 0.05 0.05 0.05 0.05 0.04 0.06 0.06 0.06 0.05 0.06 0.05 0.06 0.06 0.05 0.09 0.06 0.06 0.07 0.05 0.03 0.04 0.07 0.04 0.09 0.03 0.04 Table 5. Intra-LLM response similarity in RE-WILD. Here we report intra-llm response similarity scores. The method we use to obtain these scores is described in Sec. 2.2. A. Intra-LLM Response Similarity In our experiment, we collect 500 responses from 25 randomly selected models and compute their similarity to set of reference responses (randomly sampled from 4 other models in the set of 25) using three traditional NLP similarity metrics: ROUGE-1, ROUGE-L, and METEOR (Banerjee & Lavie, 2005; Ganesan, 2018). These metrics are computed as F1-score over unigrams, F1-score using the longest common subsequence, and weighted F1-score giving 9:1 weightage for precision over recall with chunking penalty. In Tab. 5, we include the extended results for response similarity. To see relevant citation for any particular model in this table, please refer to Sec. C. Overall, we find high similarity across models, albeit with fairly high degree of variance; for example, Mixtral-8x7BInstruct has high ROUGE-1 similarity of 0.37, while Ministral-8B-Instruct-2410 has 0.24, among the lowest scores. Considering the diversity of both prompts and models, this level of similarity suggests that LLMs produce much more regular and predictable output than humans. On model-by-model level, we can interpret measures like these as signal as to how generic and how consensus-driven any particular LLMs response is. We also observe that the larger models tend to generate more similar responses; in some sense, they are closer to consensus response to the prompt. B. Ablation on the Effect of On-Policy DGMs In Tab. 6, we provide extended results on the effects of fine-tuning using DGMs that are highly similar to the fine-tune targets (and therefore, in some limited sense, on-policy). For the analysis of the results here, please refer to our main paper, Sec. 3. The naming convention used in this table is described in Sec. 2.2, with one new abbreviation; Qwen-2-7B-Base := Q7B. 14 Model MTBench AlpacaEval BBH GPQA MATH MUSR IFEval MMLU Pro MixEval Avg L8B : L8I L8B : Q7 L8B : L70 Q7B : L8I Q7B : Q7 Q7B : Q72 Q7 : None L8I : None 6.26 6.03 6.23 6.51 6.69 7.25 7.17 7.20 21.12 17.26 24.91 15.87 27.09 36.68 33.14 30.84 0.46 0.49 0.47 0.51 0.54 0.54 0.55 0.51 0.30 0.30 0.30 0.29 0.31 0.32 0.33 0. 0.04 0.03 0.04 0.17 0.19 0.21 0.19 0.12 0.37 0.42 0.39 0.43 0.45 0.43 0.45 0.40 0.38 0.29 0.34 0.35 0.40 0.34 0.42 0.42 0.33 0.30 0.31 0.39 0.42 0.43 0.40 0.38 0.65 0.61 0.65 0.61 0.69 0.65 0.73 0.74 0.36 0.35 0.36 0.39 0.43 0.42 0.44 0. Table 6. Models learn more effectively from highly similar DGMs. In this table, we report the complete, extended results from our experiments on the effect of diversifying both DGM and SFT-target. Both Llama and Qwen benefit from more similar upstream models. C. List of All LLMs in the Study, with Citations We attempt to use model naming conventions consistent with those on HuggingFace; that way, in order to find any particular model, it should only be necessary to Google its name. We do not include the complete HuggingFace links because they might de-anonymize this work. Where available, we include citation to the work where the model was first introduced into the literature. NVLM-D-72B, from Dai et al. (2024) Llama-3.3-70B-Instruct, Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct from Dubey et al. (2024) Yi-1.5-34B-Chat, from AI et al. (2025) c4ai-command-r-plus-08-2024, from Duran et al. (2023) mistral-7b-sft-beta, from Tunstall et al. (2023) Llama-3-8B-Magpie-Align-v0.2, Llama-3-8B-Magpie-Pro-SFT-200K-v0.1, Llama-3-8B-OpenHermes-243K, Llama3-8B-ShareGPT-112K, Llama-3-8B-Tulu-330K, Llama-3-8B-Ultrachat-200K, Llama-3-8B-WildChat, Llama-3-8BWizardLM-196K, from Xu et al. (2024) Athene-70B, from NexusFlow AI (2025) Qwen2-7B-Instruct, Qwen2.5-14B-Instruct, Qwen2.5-72B-Instruct, Qwen2.5-Coder-32B-Instruct from Qwen Team (2024) glm-4-9b-chat from GLM et al. (2024) AI21-Jamba-1.5-Mini from Lieber et al. (2024) gemma-2-27b-it, gemma-2-9b-it from Team et al. (2024) internlm2 5-20b-chat from Cai et al. (2024) Llama-2-7b-chat-hf, Llama-2-13b-chat-hf from Touvron et al. (2023) Ministral-8B-Instruct-2410, Mistral-Nemo-Instruct-2407, Mixtral-8x7B-Instruct-v0.1 from Mistral (2024); Jiang et al. (2023) Llama-3.1-Nemotron-70B-Instruct-HF from Adler et al. (2024) Mistral-7B-magpie-v1.0, Mistral-7B-sharegpt-vicuna-v1.0, Mistral-7B-tulu, Mistral-7B-wizardlm-v1.0 from Feuer et al. (2024a) 15 Llama-3-Base-8B-SFT-CPO, Llama-3-Base-8B-SFT-DPO, Llama-3-Base-8B-SFT-IPO, Llama-3-Base-8B-SFT-KTO, Llama-3-Base-8B-SFT-ORPO, Llama-3-Base-8B-SFT-RDPO, Llama-3-Base-8B-SFT-RRHF, Mistral-7B-Base-SFTCPO, Mistral-7B-Base-SFT-DPO, Mistral-7B-Base-SFT-IPO, Mistral-7B-Base-SFT-KTO, Mistral-7B-Base-SFTRDPO, Mistral-7B-Base-SFT-RRHF, Mistral-7B-Base-SFT-SLiC-HF, Mistral-7B-Base-SFT-SimPO from Meng et al. (2024) OpenHermes-2-Mistral-7B, OpenHermes-2.5-Mistral-7B from Teknium et al. (2024) D. Ablations on Blending Data-Generating Models Our extended results on the effect of blending DGMs can be found at Tab. 7. The analysis of this table can be found in our main paper, as well as explanations of the abbreviation conventions for model names. Model MTBench Alpaca Eval (LC) BBH GPQA MATH MUSR IFEval MMLU Pro MixEval Avg L8B : Q7 (500k) L8B : L8I (500k) L8B : L8I + Q7 (500k) L8B : Q72 (500k) L8B : L70 (500k) L8B : Q72 + L70 (500k) 6.33 6.52 6.43 6.51 6.39 6.82 19.51 21.03 18.57 41.67 27.38 39.93 0.48 0.46 0.47 0.48 0.46 0.48 0.28 0.32 0.28 0.29 0.31 0.29 0.04 0.05 0.05 0.05 0.04 0.04 0.40 0.39 0.41 0.39 0.36 0. 0.33 0.42 0.34 0.39 0.38 0.38 0.30 0.32 0.32 0.30 0.32 0.31 0.64 0.66 0.64 0.66 0.65 0.65 0.35 0.37 0.36 0.37 0.36 0.36 Table 7. Effects of blending DGMs. Where we blend models, we always draw random sample of approximately equal size from each DGM, and denote the mixture as DGM + DGM B. e.g., L8I + Q7. In this table, it can be seen that blends benchmark between their DGMs, not above them (as measured by Avg). E. Output Samples In this appendix section, we provide complete sample responses from pair of DGMs in conveniently human-readable format, along with the prompt(s) in the conversation; Llama-3.3-70B and Qwen-2.5-72B-Instruct are the models shown here. For more samples, including sample judgments, please refer to our GitHub repository. The responses may be more convenient to read in the repository than they are in this document, as they are HTML formatted; however, we include them here so as not to inconvenience the reader. Please note that some of the output samples may contain statements that are offensive, toxic or otherwise distasteful to some readers. Please exercise caution when reading both prompts and responses. The content of the prompts and responses does not necessarily reflect the beliefs of the authors."
        }
    ],
    "affiliations": [
        "NYU"
    ]
}