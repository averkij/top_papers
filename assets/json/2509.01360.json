{
    "paper_title": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision",
    "authors": [
        "Che Liu",
        "Zheng Jiang",
        "Chengyu Fang",
        "Heng Guo",
        "Yan-Jie Zhou",
        "Jiaqi Qu",
        "Le Lu",
        "Minfeng Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate a large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, a unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets a new state-of-the-art in zero-shot image-to-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver a promising signal to the medical imaging community, positioning M3Ret as a step toward foundation models for visual SSL in multimodal medical image understanding."
        },
        {
            "title": "Start",
            "content": "M3Ret: Unleashing Zero-shot Multimodal Medical Image Retrieval via Self-Supervision Che Liu1,2, Zheng Jiang1,3, Chengyu Fang1,3, Heng Guo1,4, Yan-Jie Zhou1,4, Jiaqi Qu1,4, Le Lu1, Minfeng Xu1,4 1DAMO Academy, Alibaba Group 2Imperial College London 3Tsinghua University 4Hupan Lab Equal contribution, Corresponding author Medical image retrieval is essential for clinical decision-making and translational research, relying on discriminative visual representations. Yet, current methods remain fragmented, relying on separate architectures and training strategies for 2D, 3D, and video-based medical data. This modality-specific design hampers scalability and inhibits the development of unified representations. To enable unified learning, we curate large-scale hybrid-modality dataset comprising 867,653 medical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. Leveraging this dataset, we train M3Ret, unified visual encoder without any modality-specific customization. It successfully learns transferable representations using both generative (MAE) and contrastive (SimDINO) self-supervised learning (SSL) paradigms. Our approach sets new state-of-the-art in zero-shot imageto-image retrieval across all individual modalities, surpassing strong baselines such as DINOv3 and the text-supervised BMC-CLIP. More remarkably, strong cross-modal alignment emerges without paired data, and the model generalizes to unseen MRI tasks, despite never observing MRI during pretraining, demonstrating the generalizability of purely visual self-supervision to unseen modalities. Comprehensive analyses further validate the scalability of our framework across model and data sizes. These findings deliver promising signal to the medical imaging community, positioning M3Ret as step toward foundation models for visual SSL in multimodal medical image understanding. Date: September 3, 2025 Correspondence: {gh205191, eric.xmf}@alibaba-inc.com"
        },
        {
            "title": "1 Introduction",
            "content": "5 2 0 2 1 ] . [ 1 0 6 3 1 0 . 9 0 5 2 : r Figure 1 Recall@5 on zero-shot image-to-image retrieval across four medical datasets. The proposed M3Ret, pretrained respectively with generative (MAE [1]) and contrastive (SimDINO [2]) self-supervision, achieves superior or comparable performance on ChestXray14 (X-ray) [3], Fetal Planes (Ultrasound) [4], Hyper Kvasir (Endoscopy) [5], and CT-RATE (CT) [6], outperforming language-supervised (BMC-CLIP [7]) and visual SSL (UniMiSS+ [8]) baselines. In medical imaging, retrieving similar images given query image plays vital role in supporting clinical decision-making and advancing translational research [9]. Such tasks depend heavily on high-level and discriminative visual representations [10, 11, 12, 13]. However, most existing approaches focus on 2D images and rely extensively on language supervision [14, 15, 7], limiting their applicability to image-only datasets and reducing generalizability. Even self-supervised learning (SSL) methods, which avoid textual labels, often adopt modality-specific designs, training separate models for 2D and 3D data with architectures and training strategies tailored to each [16, 17, 18, 19]. For example, 3D models typically operate on cropped sub-volumes instead of full scans, hindering volume-level retrieval that relies on global context. This fragmentation impairs the development of unified and transferable representations, making retrieval across imaging modalities, for instance, between 2D image and 3D volume, practically infeasible. This challenge arises from the inherent heterogeneity of medical imaging: 2D and 3D scans differ in spatial dimensionality; grayscale modalities such as X-rays contrast sharply with RGB video (e.g., endoscopy); and temporal dynamics in videos are absent from static images. These disparities prompt fundamental question: Can we learn unified visual representation without relying on modality-specific design? To address this, we propose M3Ret, unified framework for visual representation learning in zero-shot multimodal medical image retrieval. To enable generalizable pretraining, we collect large-scale hybrid modality dataset comprising 867,653 clinical imaging samples, including 2D X-rays and ultrasounds, RGB endoscopy videos, and 3D CT scans. To the best of our knowledge, this is the largest real-world hybrid medical imaging dataset to date. Built upon this dataset, M3Ret enables joint training across diverse imaging modalities without any modality-specific design. It leverages two widely adopted SSL paradigms, MAE [1] and SimDINO [2], to learn transferable visual representations across modalities. We make three key contributions: (1) We show, for the first time, that diverse medical modalities, X-rays, CT scans, ultrasounds, and endoscopy videos, can be trained together in single unified framework without any modality-specific modifications. (2) We achieve superior performance on zero-shot image-to-image retrieval across broad range of datasets, covering diverse imaging modalities and tasks, including global categories (various diseases or anatomical regions), region-specific abnormalities, and cross-modal retrieval, outperforming various baselines, including SSL methods and supervised models with language, mask, or category supervision. Fig. 1 provides representative illustration of the results. (3) We conduct an in-depth analysis of key SSL design factors, and demonstrate that performance gains are primarily driven by scaling data, model capacity, and compute, echoing the bitter lesson [20]. Our results demonstrate that unified representation learning across medical imaging modalities is not only feasible but also highly effective, paving the way toward general-purpose foundation models for medical image understanding."
        },
        {
            "title": "2 Related Work",
            "content": "Medical Image Retrieval. Medical image retrieval focuses on retrieving the most relevant medical images given query input, facilitating tasks such as case comparison, lesion matching, and diagnostic support [9, 21, 22]. Early approaches primarily targeted 2D images with text-based queries [14, 15, 23], while image-to-image retrieval remains relatively underexplored. Some studies [24, 25, 26] utilize small, narrowly scoped datasets, such as binary COVID-19 classification (e.g., with or without infection), which limits both generalizability and scalability. Other modalities, like sketch-based retrieval [27], attempt to combine visual and textual cues, but often suffer from limited clinical usability due to complex input requirements. In 3D medical imaging, BIMCV-R [28] is among the first to address retrieval but relies solely on text queries, making it incompatible with image-only datasets. 3D-MIR [29] uses image-based queries, but its slice-wise processing fails to capture full 3D spatial context. Moreover, it supports only global category-level retrieval and does not consider fine-grained, region-level semantics. Importantly, all aforementioned methods rely on supervised learning with paired training data, which poses significant scalability challenges due to the cost and effort of annotation. Self-Supervised Learning for Medical Imaging. Self-supervised learning (SSL) has emerged as promising alternative to supervised training in the medical domain, enabling the learning of visual representations without annotation. Many existing methods [16, 30, 31] adopt visual-only SSL approaches, but are typically tailored to specific modalities, such as 2D chest X-rays. This limits their ability to generalize to other imaging types or anatomical regions, such as abdominal or brain scans, and to 3D modalities like CT or MRI. For 3D medical imaging, most SSL frameworks either process cropped sub-volumes [17, 18, 32, 33, 34, 35, 36, 37], which restrict the models capacity to learn global anatomical context, or use heavily downsampled full volumes [38], which may lose subtle tissue patterns. These design choices hinder the learning of comprehensive and discriminative features across the entire 3D volume. Figure 2 Comparison of visual representation learning strategies in medical imaging. (a) BMC-CLIP [7] relies on 2D image-text pairs (e.g., X-rays and reports), limiting generalization beyond paired data and modalities. (b) VoCo [35] learns from 3D sub-volumes with random cropping via contrastive learning, but neglects global context. (c) Our proposed approach learns unified visual representations from heterogeneous medical imaging modalities (2D, 3D, and videos) using purely visual SSL (e.g., MAE [1], SimDINO [2]), without any extra supervision signals such as text or modality-specific design. Multimodal Representation Learning in Medical Imaging. Multimodal learning in medical imaging has primarily focused on aligning medical images with associated textual data, such as radiology reports [14, 15, 23, 39, 6, 40]. Models like BMC-CLIP [7] leverage large-scale image-text pairs and achieve strong performance in tasks such as retrieval. However, their dependence on text supervision limits their use in scenarios where textual annotations are unavailable. Another line of work explores multimodal learning across different variants of single imaging modality [41], such as T1-weighted, T2-weighted, and FLAIR sequences in brain MRI. These approaches still operate within single modality (e.g., MRI) and typically require data from the same anatomical region, which restricts generalization to other body parts and imaging modalities. Only few recent efforts aim to learn representations across heterogeneous imaging modalities. For instance, UniMiSS [37] and UniMiSS+ [8] propose cross-modal learning between 2D X-rays and 3D CT scans. Yet, they rely on strictly aligned scans from the same anatomical region and use complex alternating training strategies, which limit scalability to more diverse and unpaired datasets. MedCoSS [42] proposes continual SSL framework to perform multimodal representation learning, but requires multi-stage sequential training pipeline that involves clinical reports. Similarly, UniMedI [43] aligns 2D and 3D modalities using additional report supervision, but it depends on paired image-text data, further constraining its applicability in broader clinical contexts."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we present our unified pretraining framework M3Ret, which enables visual SSL across diverse medical imaging modalities. As shown in Fig. 2, existing methods either rely on language supervision or are restricted to single modalities with limited global context. In contrast, our framework enables joint training on 2D, 3D, and video data using purely visual signals, without any modality-specific design or additional supervision. We first describe the unified patchification that supports heterogeneous medical inputs in modality-agnostic manner, and then introduce two SSL paradigms, MAE [1] and SimDINO [2], used for visual representation learning."
        },
        {
            "title": "3.1 Unified Patchification for Multimodal Medical Images",
            "content": "Our framework aims to standardize the input format across various medical imaging modalities, including 2D and 3D grayscale and RGB images, to enable the use of shared encoder. Each input image or volume is represented as 4D tensor RCHW S, where denotes the number of channels, and represent the height and width, which are fixed at 256 pixels, and corresponds to the number of slices in CT scans or the number of frames in endoscopy videos. We define the preprocessing and reshaping operations for each modality as follows: X-ray and Ultrasound: Each 2D grayscale image is resized to 256 256, duplicated along the channel dimension to match the RGB format, and replicated along the slice dimension to obtain 4D tensor R32562564. Endoscopy: Given RGB video data, we randomly sample = 16 frames from single video, resulting in R325625616. CT: Volumes are resized to = 64 slices. Each grayscale scan (C = 1) is duplicated along the channel axis to match RGB format, yielding R325625664. We apply 4D patchification with patch size = (Cp, Hp, Wp, Sp) = (3, 16, 16, 4) universally, which partitions into non-overlapping spatiotemporal patches (patching is applied over H, , and S; channel dimension is fixed). This yields sequence of visual tokens = Patchify(X) RN D, where is the number of tokens and is the patch embedding dimension."
        },
        {
            "title": "3.2 Masked Autoencoder (MAE)",
            "content": "Masked Autoencoders (MAE) [1] aim to reconstruct the pixel-level content of masked patches from the remaining visible ones. Let the input be 4D tensor RCHW S. The input is partitioned into non-overlapping visual patches using unified patchification strategy. Each patch is flattened and linearly projected into D-dimensional embedding, forming the patch embedding matrix RN D. fixed mask ratio α (0, 1) (e.g., α = 0.75) determines the fraction of patches to be masked. The remaining (1 α)N visible patches are denoted as Pvis R(1α)N and are processed by an encoder function fθ : R(1α)N R(1α)N D, where θ represents the learnable parameters of the encoder and is the output dimensionality of the latent representations. To reconstruct the masked patches, decoder gϕ takes the encoder output along with learned embeddings for the masked patches Pmask, and outputs the reconstructed patches in pixel space: ˆXmask = gϕ(fθ(Pvis), Pmask). Here, ϕ denotes the learnable parameters of the decoder, and ˆXmask represents the reconstructed pixel values of the masked patches. The model is trained to minimize the mean squared error (MSE) between the reconstructed and original pixel values of the masked patches. Let {1, . . . , } be the set of indices corresponding to the masked patches. The training objective is given by: LMAE = 1 αN (cid:88) iM ˆXi Xi2 2, where Xi and ˆXi denote the original and reconstructed pixel values of patch i, respectively. Notably, MAE does not utilize [CLS] token. In downstream zero-shot image retrieval tasks, we use the average pooling of all visual patch embeddings from the encoder as the image representation to compute cosine similarity between images."
        },
        {
            "title": "3.3 SimDINO",
            "content": "SimDINO [2] aims to learn discriminative visual representations by aligning features extracted from different augmented views of the same input. Given an input image X, two types of augmented views are generated, local vc(X) and global vg(X), as detailed in Appendix B. We sample two global views for every modality, plus 10 local views for 2D inputs and 4 for 3D or video inputs to avoid GPU out-of-memory issues. Each view is partitioned into non-overlapping visual patches with unified patchification strategy, and [CLS] token is prepended. All views pass through the student encoder fstudent, whereas only global views feed the teacher encoder fteacher. The resulting [CLS] embeddings, zc = cls student(vc(X)), zg = cls teacher(vg(X)), present discriminative visual representations. The total loss combines an alignment term with coding-rate regularization [44, 45]: LSimDINO = 1 2 zc zg2 2 1 2 (cid:16) log det + ϵ2 Γ (cid:17) where Γ = Cov[zc] Rdd denotes the covariance matrix of the student [CLS] embeddings in batch, with being the embedding dimension. We set ϵ = 0.5, and update the teacher parameters using exponential moving average (EMA) of the student parameters, where the momentum starts at 0.996 and is gradually increased to 1.0 using cosine schedule during pretraining. During zero-shot image retrieval tasks, SimDINO uses the original (non-augmented) image to compute its representation by concatenating the [CLS] token with the average-pooled patch embeddings. The cosine similarity between these combined representations is then used to rank image pairs."
        },
        {
            "title": "4.1 Task Configuration",
            "content": "To comprehensively evaluate our models performance in zero-shot medical image-to-image retrieval, we design four distinct task settings, each targeting specific level of granularity or imaging modality. These tasks are illustrated below and aim to assess the models capability across various retrieval scenarios. We report Recall@K, Median Rank (MdR), and Mean Rank (MnR) as evaluation metrics. Details of the datasets used in each setting are provided in Appendix C. Category-level Retrieval: This setting evaluates the models basic capability to retrieve images based on global categories. The input query image may belong to one or multiple categories at the image level, such as specific diseases (e.g., atelectasis) or anatomical regions (e.g., head). retrieval is considered correct if the returned sample shares at least one positive category with the query image. For this task, we use the following public datasets: ChestXray14 [3] for X-ray modality, Fetal Planes [4] for Ultrasound, and Kvasir Capsule [47] and Hyper Kvasir [5] for Endoscopy. Progressive Regional Retrieval Tasks: To evaluate the models understanding of fine-grained regional information, we design two increasingly challenging tasks using the RadGenome-ChestCT [48] dataset, which is extended from CT-RATE [6] dataset with anatomical region grounded reports (We use the term CT-RATE consistently throughout the descriptions for its simplicity). These tasks assess performance at different levels of spatial granularity: (1) Regional Abnormality Retrieval: This task evaluates the models ability to retrieve images based on coarse regional annotations. Each query image is labeled with regional abnormality statuses (e.g., the aorta labeled as normal or abnormal). retrieval is considered correct if the returned image matches the querys abnormality status for the corresponding regions. (2) Lesion Size Retrieval: This more challenging task requires retrieval based on precise lesion size annotations. Each query image is labeled with regional lesion descriptions (e.g., abdomen with hypodense lesion, 20 mm). retrieval is correct if the returned image matches the lesion size annotations for the relevant regions. Together, these tasks assess the models ability to capture increasingly fine-grained pathological information, from coarse abnormality status to precise lesion characterization. Cross-modal Retrieval: This setting evaluates the models ability to generalize across imaging modalities, including those not seen during pretraining. Given query image, the task is to retrieve an image from different modality that shares the same category or semantic content. For example, given an X-ray image of the head, the desired output is corresponding head CT scan. In addition to cross-modal retrieval between modalities seen during pretraining (e.g., X-ray and CT), we also explore more challenging scenario: retrieving MRI images based on CT queries. This task is particularly demanding because the model has never been exposed to MRI data during pretraining, making it true test of generalization to unseen modalities. For instance, given CT image of the abdomen, the desired output is corresponding abdomen MRI. These tasks highlight the models ability to link seen and unseen modalities with semantic consistency, as well as its robustness in handling novel imaging modalities."
        },
        {
            "title": "4.2 Baselines",
            "content": "We select four strong baseline methods that cover diverse supervision learning strategies, including text, mask, and disease category supervision, as well as visual-only SSL, to enable comprehensive comparisons. These baselines include models pretrained with 2D images, 3D volumes, and joint 2D-3D inputs. For all experiments, we use their official pretrained weights and inference code. DINOv2 [49]: DINOv2 is self-supervised learning model that learns powerful visual features from large, curated dataset of 142 million images without relying on textual annotations. It employs self-distillation and ViT architecture to learn versatile visual features. SigLIP2 [50]: SigLIP2 is multilingual vision-language model that learns generalizable visual representations from image-text pairs. It is trained on the extensive web-crawled dataset, which contains 10 billion images with their corresponding captions in over 100 languages. The training process combines sigmoid loss for image-text matching with self-supervised techniques, including self-distillation and masked prediction, to enhance performance on both global and dense prediction tasks. DINOv3 [51]: DINOv3 is self-supervised learning model that significantly scales up its predecessor, DINOv2, by training on massive dataset of 1.7 billion images with various scale ViT architecture. BMC-CLIP [7]: BMC-CLIP is trained on 24M image-text pair dataset (BIOMEDICA), restricted to 2D image inputs and reliant solely on text supervision. Its reliance on large-scale text-image data collection incurs significant costs and limits scalability. VoCo [35]: VoCo is pretrained on 160K CT scans to learn volumetric representations but uses cropped subvolumes as input rather than full 3D volumes, which limits its ability to capture global context. It supports only 3D CT inputs, making it unsuitable for 2D-based tasks or cross-modal retrieval. Since VoCo provides multiple official pretrained weights1, we select the model with comparable scale to ours, VoCo-Omni-B, as the baseline. Notably, this version of VoCo is pretrained using both mask annotations and segmentation task in parallel with contrastive learning. Merlin [38]: Merlin is pretrained on over 6 million CT-EHR pairs with full supervision, using EHR codes as disease categories, and is restricted to 3D CT inputs. Its reliance on paired CT-EHR data limits its applicability to other modalities or datasets lacking such annotations. CT-FM [52]: CT-FM is large-scale, 3D visual foundation model pre-trained on 148,000 3D CT scans using contrastive learning in subvolume-level. The model is designed to perform diverse and complex tasks across 1https://github.com/Luffy03/Large-Scale-Medical Table 1 Comparison of category-level retrieval. The best and second-best results are highlighted in bold and underlined. indicates models using language supervision. Modality Dataset Metric DINOv2-B [49] DINOv3-7B [51] SigLIP2 [50] BMC-CLIP [7] UniMiSS+ [8] M3Ret (MAE) M3Ret (SimDINO) Modality Dataset Metric DINOv2-B [49] DINOv3-7B [51] SigLIP2 [50] BMC-CLIP [7] M3Ret (MAE) M3Ret (SimDINO) X-ray ChestXray14 Ultrasound Fetal Planes R@1 R@5 R@10 MnR MdR 6.246 0.798 0.311 6.248 0.794 0.304 6.629 0.760 0.248 6.459 0.777 0.284 6.353 0.783 0.298 6.552 0.762 0.247 6.277 6.276 6.635 6.479 6.384 6. 0.658 0.649 0.599 0.631 0.648 0.603 0.345 0.674 0.812 6.136 6. 0.983 R@1 R@5 R@10 MnR MdR 5.414 0.993 0.923 5.448 5.423 5.447 / 5.435 5.460 5.442 5.460 / 0.971 0.934 0.947 / 0.881 0.955 0.994 0.987 0.991 / 0.966 0.990 0.997 0.995 0.996 / 0.981 0. 5.429 5.456 5.399 5.445 Kvasir Capsule Hyper Kvasir Endoscopy R@1 R@5 R@10 MnR MdR 0. 0.898 0.784 0.776 0.720 0.743 0.606 0.647 0.913 0.895 0.904 0.843 0.872 0.939 0.927 0.936 0.901 0.913 5.132 5.191 5.245 5.186 5.360 5.302 5.038 5.106 5.134 5.117 5.289 5. R@1 R@5 R@10 MnR MdR 6.801 0.736 0.396 6.865 0.731 0.387 6.755 0.731 0.374 0.390 6.817 0.706 0.412 0.668 0.673 0.657 0.654 0.687 6.788 6.850 6.775 6.842 0.753 0.747 6.592 6.617 6.563 6. 0.448 0.690 Table 2 Comparison of retrieval at the levels of regional abnormality status and lesion size. and indicate supervised pretraining with segmentation masks, and disease categories, respectively."
        },
        {
            "title": "Lesion Size",
            "content": "R@1 R@5 R@10 MnR MdR R@1 R@5 R@10 MnR MdR 10.666 0.044 CT-FM [52] 10.760 0.038 CT-CLIP [6] 10.773 0.037 VoCo [18] 10.711 0.040 UniMiSS+ [8] 10.572 0.054 Merlin [38] M3Ret (MAE) 10.679 0.046 M3Ret (SimDINO) 0.058 10.666 10.760 10.773 10.711 10.571 10.679 0.173 0.151 0.150 0.159 0.198 0.178 9.493 9.649 9.662 9.591 9.311 9.461 9.495 9.650 9.662 9.593 9.312 9.463 0.031 0.022 0.021 0.028 0.043 0. 0.007 0.004 0.004 0.006 0.010 0.008 0.273 0.243 0.241 0.255 0.059 0.045 0.043 0.050 10.544 10.544 0.303 0.276 0.076 0. 0.046 0.200 0.303 0.076 9.298 9. 0.014 imaging modalities, including segmentation, triage, retrieval, and semantic understanding. CT-CLIP [6]: CT-CLIP is contrastive language-image pre-training framework focused on 3D medical imaging, specifically CT scans. It is trained on the CT-RATE dataset, which pairs 25,692 non-contrast 3D chest CT scans with their corresponding radiology reports. The model learns to align 3D visual features with textual representations from the reports, enabling it to understand the semantic content of medical images. This allows CT-CLIP to be applied to various downstream tasks, such as multi-abnormality detection and case retrieval, without requiring task-specific training. UniMiSS+ [8]: UniMiSS+ is pretrained in purely self-supervised manner using Chest X-ray and CT data. It aims to learn 2D and 3D representations, but the pretraining explicitly aligns 2D and 3D scans from the same body part, hence it is limited to cases where both modalities originate from the same anatomical region."
        },
        {
            "title": "4.3 Experimental Results",
            "content": "Zero-shot retrieval across modalities. We evaluate M3Ret on four datasets covering different modalities, ChestXray14 (X-ray), Fetal Planes (ultrasound), Kvasir Capsule and Hyper Kvasir (endoscopy), under zeroshot image-to-image retrieval settings. As shown in Table 1, M3Ret with SimDINO pretraining consistently outperforms or matches BMC-CLIP [7], strong baseline pretrained on 24M image-text pairs with language Table 3 Cross-modal retrieval performance across tasks and metrics. The best and second-best results are highlighted in bold and underlined. and indicate supervised pretraining with segmentation masks, and disease categories, respectively. Task Metric UniMiSS+ VoCo Merlin M3Ret (MAE) M3Ret (SimDINO) Task Metric UniMiSS+ M3Ret (MAE) M3Ret (SimDINO) Task Metric UniMiSS+ M3Ret (MAE) M3Ret (SimDINO) CT MRI MRI CT CT MRI R@1 R@5 R@10 MnR MdR R@1 R@5 R@10 MnR MdR 8.790 0.160 8.677 0.258 7.871 0.370 8.591 0.301 0.315 0.342 0.463 0.329 0.380 0.424 0.548 0.397 8.802 8.692 7.895 8. 0.207 0.196 0.236 0.246 0.370 0.676 9.126 7.116 0.303 0.561 9.142 7.056 0.626 0.525 0. 0.710 0.641 0.677 7.005 7.552 7.285 6.977 7.554 7.274 0.252 0.495 0. 7.746 7.767 0.424 CT X-ray X-ray CT CT X-ray R@1 R@5 R@10 MnR MdR R@1 R@5 R@10 MnR MdR 7.720 0.204 8.778 0.595 0.336 0.417 0. 7.745 8.774 0.327 0.262 0.513 0.487 0.572 0.548 7.837 7. 7.799 7.900 0.552 0.502 0.625 0.581 7.403 7.851 7.396 7.836 0.344 0.310 0. MRI X-ray X-ray MRI MRI X-ray R@1 R@5 R@10 MnR MdR R@1 R@5 R@10 MnR MdR 9.890 0.012 9.717 0.055 10.597 9.792 10.599 9. 9.895 9.718 0.153 0.200 0.101 0.130 0.073 0.217 0.192 0.242 0.044 0. 0.152 0.306 0.373 8.787 8.798 0. 0.382 0.482 8.383 8.388 supervision. It also surpasses UniMiSS+ [8], domain-specific SSL method trained on chest X-rays and CT scans. Furthermore, M3Ret with SimDINO outperforms MAE variants on 3 out of 4 datasets, suggesting it learns more discriminative and transferable features by aligning views rather than reconstructing pixels. These results highlight M3Rets ability to extract modality-agnostic visual representations without requiring any modality-specific architecture or paired supervision. Fig. 5 displays top 3 retrieved examples computed by M3Ret and BMC-CLIP [7] for qualitative comparison. Retrieval based on local abnormality. We further assess M3Ret on fine-grained retrieval tasks involving regional abnormality status and lesion size  (Table 2)  . M3Ret with SimDINO notably outperforms supervised baselines such as VoCo [35], which relies on 160K pixel-level organ and tumor annotations, and Merlin, which is trained with disease-level supervision. Our approach achieves these results using purely self-supervised learning, without any task-specific design or annotations. This demonstrates that generic visual SSL can implicitly capture localized pathological cues, enabling strong performance on fine-grained retrieval without the need for region-level supervision. Emerging cross-modal generalization. To explore cross-modal alignment capabilities, we benchmark M3Ret on three retrieval setups: CT MRI, CT X-ray, and MRI X-ray  (Table 3)  . M3Ret achieves superior performance in nearly all configurations. Remarkably, in CT X-ray retrieval, it outperforms UniMiSS+ [8], despite the latter being pretrained on explicitly paired CT/X-ray images. More impressively, M3Ret generalizes well to MRI-related tasks without ever seeing MRI data during pretraining. These findings demonstrate that visual SSL can learn generalizable visual representations, and that our unified SSL framework effectively encodes diverse imaging modalities into shared latent space, without requiring paired samples or explicit cross modal alignment, while preserving semantic consistency."
        },
        {
            "title": "4.4 Analysis",
            "content": "Effect of the Number of Local Views. We investigate how varying the number of local crops for 2D images affects the performance of M3Ret pretrained with SimDINO. Specifically, we evaluate settings with {2, 4, 6, 8, 10} local views, as shown in Fig. 3 (a). Across different numbers of local crops, the performance remains stable Figure 3 Recall@5 for ChestXray14 and CT-RATE datasets. (a) Performance under varying numbers of local crops. (b) Effect of embedding strategies. (c) Comparison of different patch sizes. M3Ret (SimDINO) shows stable performance across local views and embedding choices, while smaller patch size offers better granularity and improves representation learning capacity. without noticeable oscillations, suggesting that learned visual representation via SSL is not sensitive to the number of local views. Interestingly, this observation contrasts with findings from several SSL studies on natural images [53, 54], which report that increasing the number of local views enhance representation learning on RGB images. possible explanation lies in the nature of medical images: even when pretraining across multiple imaging modalities, samples from the same modality or anatomical region often exhibit highly similar structures, differing only in subtle local patterns. As result, additional local crops may provide limited incremental benefit for representation learning in this domain. Effect of the Embedding Strategy. We further assess the robustness of our model with respect to different embedding strategies, including average pooling over all visual tokens except the [CLS] token (Avg.), mixed strategy (Mix.) that concatenates the average-pooled visual tokens with the [CLS] token, and using the [CLS] token alone. As illustrated in Fig. 3 (b), performance remains consistent across all embedding methods on both ChestXray14 and CT-RATE datasets. This stability indicates that our framework is not sensitive to the choice of embedding approach, highlighting the robustness of the learned representations. Effect of Patch Size. To explore the effect of feature granularity, we pretrain M3Ret using three different patch sizes: [884], [16164], and [32324] (The first dimension is always kept as 3 and is omitted here for simplicity). This allows us to examine impact of patch granularity on representation quality. As shown in Fig. 3 (c), we observe clear trend: smaller patch sizes consistently improve performance across various downstream tasks. This suggests that finer-grained features are more effective for representation learning in medical imaging. These results also align with findings from visual SSL on natural images [49, 53], indicating that fine-grained features are beneficial across both natural and medical domains. Model Parameter Scaling. We examine scalability with respect to model size by comparing three ViT variants: ViT-T, ViT-S, and ViT-B. As shown in the top of Fig. 4, M3Ret achieves consistent performance gains across four downstream datasets with various modalities as model capacity increases, highlighting its scalability. Pretraining Data Scaling. To assess scalability with respect to dataset size, we pretrain on 20%, 60%, and 100% of the dataset. The bottom of Fig. 4 shows performance consistently improves, demonstrating that M3Ret effectively leverages more training data. Our analysis also reveals power law scaling trends, with fitted slopes and intercepts computed following [38]. These results further support the scalability of M3Ret with respect to both model capacity and data size. Unimodal and Multimodal Visual Pretraining. We analyze the impact of unimodal versus multimodal pretraining on retrieval performance across different visual modalities. Overall, multimodal pretraining tends to outperform unimodal pretraining, leading to improved retrieval metrics across most datasets. However, there are cases where unimodal pretraining performs competitively or slightly better in certain settings. Notably, multimodal pretraining does not degrade performance significantly compared to unimodal pretraining, even in cases where unimodal models show superior results. This suggests that multimodal pretraining offers significant benefits in enhancing representation quality without introducing any issues such as modality collapse or conflicts during training. The results in Table 4 demonstrate that incorporating multiple modalities in visual pretraining strengthens the models ability to capture diverse and complementary features, thereby improving Figure 4 Recall@5 across four downstream tasks. Top: effect of model size scaling using ViT-T, ViT-S, and ViT-B. Bottom: effect of increasing pretraining data ratio (20%, 60%, 100%). M3Ret consistently benefits from larger models and more data, demonstrating its scalability. Red dashed lines show the fitted power laws. Figure 5 Qualitative comparison of top 3 retrieval examples. The top two rows show two X-ray query images presenting pleural effusion and pneumothorax, respectively. The bottom two rows display two ultrasound query images capturing the fetal brain and thorax, respectively. The top 3 retrieved results based on embedding similarity are listed in each row. representation quality without negative side effects."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we demonstrate that unified visual representation learning across heterogeneous modalities is both feasible and effective, without relying on modality-specific design. It can be accomplished through Table 4 Ablation study results comparing unimodal and multimodal pretraining for category-level retrieval. The best results are highlighted in bold."
        },
        {
            "title": "Unimodal\nMultimodal",
            "content": "X-ray ChestXray"
        },
        {
            "title": "Fetal Planes",
            "content": "R@1 R@5 R@10 MnR MdR 6.314 0.785 0.295 6.343 0.650 0.323 0.665 0. 6.259 6.220 R@1 R@5 R@10 MnR MdR 0.991 0.926 0.991 0.980 0.980 5.449 5.468 5.427 5. 0."
        },
        {
            "title": "Endoscopy",
            "content": "R@1 R@5 R@10 MnR MdR 0.534 0.825 0.918 0.901 5.411 5.431 5.340 5.353 0. 0.834 R@1 R@5 R@10 MnR MdR 6.780 0.360 6.754 0.673 0.673 0.755 0.736 6. 6.672 0.432 CT"
        },
        {
            "title": "Lesion Size",
            "content": "R@1 R@5 R@10 MnR MdR 0.053 9.343 0.053 0.296 0.295 9.344 0.194 9.340 9. 0.195 R@1 R@5 R@10 MnR MdR 0.075 10.543 10.562 10.543 10.562 0.076 0.015 0. 0.045 0.043 the integration of large-scale, real-world multimodal dataset and purely visual SSL, with both SimDINO and MAE proving successful. Our unified model, M3Ret, achieves superior in-domain performance and demonstrates surprising generalization to unseen modalities such as MRI and to fine-grained pathological cues, all without paired data, segmentation masks, or language supervision. Notably, we outperform strong baselines such as DINOv3-7B, which uses more than 2000 times the data of our work, and BMC-CLIP, which relies on text supervision. Additionally, we find that M3Ret pretrained with SimDINO provides better performance than the MAE-pretrained variant in most scenarios, and is robust to hyperparameter choices and scales effectively with model and data sizes. Our results provide clear answer to the question posed at the start of this work, and may encourage future efforts toward building scalable, general-purpose foundation models for medical image understanding."
        },
        {
            "title": "References",
            "content": "[1] K. He, X. Chen, S. Xie, Y. Li, P. Dollár, and R. Girshick, Masked autoencoders are scalable vision learners, in Proceedings of CVPR, 2022, pp. 16 00016 009. [2] Z. Wu, J. Zhang, D. Pai, X. Wang, C. Singh, J. Yang, J. Gao, and Y. Ma, Simplifying dino via coding rate regularization, arXiv preprint arXiv:2502.10385, 2025. [3] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers, Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases, in Proceedings of CVPR, 2017, pp. 34623471. [4] X. P. Burgos-Artizzu, D. Coronado-Gutiérrez, B. Valenzuela-Alcaraz, E. Bonet-Carne, E. Eixarch, F. Crispi, and E. Gratacós, Evaluation of deep convolutional neural networks for automatic classification of common maternal fetal ultrasound planes, Scientific Reports, vol. 10, 2020. [5] H. Borgli, V. Thambawita, P. H. Smedsrud, S. Hicks, D. Jha, S. L. Eskeland, K. R. Randel, K. Pogorelov, M. Lux, D. T. D. Nguyen et al., Hyperkvasir, comprehensive multi-class image and video dataset for gastrointestinal endoscopy, Scientific data, vol. 7, no. 1, p. 283, 2020. [6] I. E. Hamamci, S. Er, C. Wang, F. Almas, A. G. Simsek, S. N. Esirgun, I. Doga, O. F. Durugol, W. Dai, M. Xu et al., Developing generalist foundation models from multimodal dataset for 3d computed tomography, arXiv preprint arXiv:2403.17834, 2024. [7] A. Lozano, M. W. Sun, J. Burgess, L. Chen, J. J. Nirschl, J. Gu, I. Lopez, J. Aklilu, A. W. Katzer, C. Chiu, A. Rau, X. Wang, Y. Zhang, A. S. Song, R. Tibshirani, and S. Yeung-Levy, Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature, ArXiv, vol. abs/2501.07171, 2025. [8] Y. Xie, J. Zhang, Y. Xia, and Q. Wu, Unimiss+: Universal medical self-supervised learning from cross-dimensional unpaired data, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 12, pp. 10 02110 035, 2024. [9] H. Müller, N. Michoux, D. Bandon, and A. Geissbuhler, review of content-based image retrieval systems in medical applicationsclinical benefits and future directions, International journal of medical informatics, vol. 73, no. 1, pp. 123, 2004. [10] R. Datta, D. Joshi, J. Li, and J. Z. Wang, Image retrieval: Ideas, influences, and trends of the new age, ACM Computing Surveys (Csur), vol. 40, no. 2, pp. 160, 2008. [11] J. Choe, H. J. Hwang, J. B. Seo, S. M. Lee, J. Yun, M.-J. Kim, J. Jeong, Y. Lee, K. Jin, R. Park et al., Contentbased image retrieval by using deep learning for interstitial lung disease diagnosis with chest ct, Radiology, vol. 302, no. 1, pp. 187197, 2022. [12] A. Qayyum, S. M. Anwar, M. Awais, and M. Majid, Medical image retrieval using deep convolutional neural network, Neurocomputing, vol. 266, pp. 820, 2017. [13] Z. Li, X. Zhang, H. Müller, and S. Zhang, Large-scale retrieval for medical image analytics: comprehensive review, Medical Image Analysis, vol. 43, pp. 6684, 2018. [14] S. Zhang, Y. Xu, N. Usuyama, J. Bagga, R. Tinn, S. Preston, R. Rao, M. Wei, N. Valluri, C. Wong et al., Largescale domain-specific pretraining for biomedical vision-language processing, arXiv preprint arXiv:2303.00915, 2023. [15] O. Pelka, S. Koitka, J. Rückert, F. Nensa, and C. Friedrich, Radiology objects in context (roco): multimodal image dataset, in CVII-STENT/LABELS@MICCAI, 2018. [16] F. Perez-Garcia, H. Sharma, S. Bond-Taylor, K. Bouzid, V. Salvatelli, M. Ilse, S. Bannur, D. C. Castro, A. Schwaighofer, M. P. Lungren, M. T. Wetscherek, N. Codella, S. L. Hyland, J. Alvarez-Valle, and O. Oktay, Rad-dino: Exploring scalable medical image encoders beyond text supervision, ArXiv, vol. abs/2401.10815, 2024. [17] Y. Tang, D. Yang, W. Li, H. R. Roth, B. A. Landman, D. Xu, V. Nath, and A. Hatamizadeh, Self-supervised pre-training of swin transformers for 3d medical image analysis, Proceedings of CVPR, pp. 20 69820 708, 2021. [18] L. Wu, J. Zhuang, and H. Chen, Voco: simple-yet-effective volume contrastive learning framework for 3d medical image analysis, in Proceedings of CVPR, 2024, pp. 22 87322 882. [19] F. Haghighi, M. R. H. Taher, M. B. Gotway, and J. Liang, Dira: Discriminative, restorative, and adversarial learning for self-supervised medical image analysis, in Proceedings of CVPR, 2022, pp. 20 82420 834. [20] R. Sutton, The bitter lesson, Incomplete Ideas (blog), 2019. [Online]. Available: http://www.incompleteideas. net/IncIdeas/BitterLesson.html [21] T. W. Cai, J. Kim, and D. D. Feng, Content-based medical image retrieval, in Biomedical information technology. Elsevier, 2008, pp. 83113. [22] A. Kumar, J. Kim, W. Cai, M. Fulham, and D. Feng, Content-based medical image retrieval: survey of applications to multidimensional and multimodality data, Journal of digital imaging, vol. 26, pp. 10251039, 2013. [23] J. Rückert, L. Bloch, R. Brüngel, A. Idrissi-Yaghir, H. Schäfer, C. S. Schmidt, S. Koitka, O. Pelka, A. B. Abacha, A. G. S. de Herrera, H. Müller, P. A. Horn, F. Nensa, and C. M. Friedrich, Rocov2: Radiology objects in context version 2, an updated multimodal image dataset, Scientific Data, vol. 11, 2024. [24] F. K. Jush, T. Truong, S. Vogler, and M. Lenga, Medical image retrieval using pretrained embeddings, arXiv preprint arXiv:2311.13547, 2023. [25] H. H. Lee, A. Santamaria-Pang, J. Merkow, O. Oktay, F. Pérez-García, J. Alvarez-Valle, and I. Tarapov, Regionbased contrastive pretraining for medical image retrieval with anatomic query, arXiv preprint arXiv:2305.05598, 2023. [26] B. Hu, B. Vasu, and A. Hoogs, X-mir: Explainable medical image retrieval, in Proceedings of WACV, 2022, pp. 440450. [27] K. Kobayashi, L. Gu, R. Hataya, T. Mizuno, M. Miyake, H. Watanabe, M. Takahashi, Y. Takamizawa, Y. Yoshida, S. Nakamura et al., Sketch-based semantic retrieval of medical images, Medical Image Analysis, vol. 92, p. 103060, 2024. [28] Y. Chen, C. Liu, X. Liu, R. Arcucci, and Z. Xiong, Bimcv-r: landmark dataset for 3d ct text-image retrieval, in Proceedings of MICCAI, 2024. [29] A. B. Abacha, A. Santamaría-Pang, H. H. Lee, J. T. Merkow, Q. Cai, S. T. Devarakonda, A. Islam, J. Gong, M. P. Lungren, T. Lin, N. Codella, and I. Tarapov, 3d-mir: benchmark and empirical study on 3d medical image retrieval in radiology, ArXiv, vol. abs/2311.13752, 2023. [30] S. Azizi, B. Mustafa, F. Ryan, Z. Beaver, J. Freyberg, J. Deaton, A. Loh, A. Karthikesalingam, S. Kornblith, T. Chen et al., Big self-supervised models advance medical image classification, in Proceedings of ICCV, 2021, pp. 34783488. [31] M. R. H. Taher, M. B. Gotway, and J. Liang, Representing part-whole hierarchies in foundation models by learning localizability composability and decomposability from anatomy via self supervision, in Proceedings of CVPR, 2024, pp. 11 26911 281. [32] X. Tao, Y. Li, W. Zhou, K. Ma, and Y. Zheng, Revisiting rubiks cube: self-supervised learning with volume-wise transformation for 3d medical image segmentation, in Proceedings of MICCAI. Springer, 2020, pp. 238248. [33] Z. Zhou, V. Sodha, J. Pang, M. B. Gotway, and J. Liang, Models genesis, Medical Image Analysis, vol. 67, p. 101840, 2021. [34] Y. Jiang, M. Sun, H. Guo, X. Bai, K. Yan, L. Lu, and M. Xu, Anatomical invariance modeling and semantic alignment for self-supervised learning in 3d medical image analysis, in Proceedings of ICCV, 2023, pp. 15 859 15 869. [35] L. Wu, J. Zhuang, and H. Chen, Large-scale 3d medical image pre-training with geometric context priors, arXiv preprint arXiv:2410.09890, 2024. [36] H.-Y. Zhou, C. Lu, C. Chen, S. Yang, and Y. Yu, unified visual information preservation framework for self-supervised pre-training in medical image analysis, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 7, pp. 80208035, 2023. [37] Y. Xie, J. Zhang, Y. Xia, and Q. Wu, Unimiss: Universal medical self-supervised learning via breaking dimensionality barrier, in Proceedings of ECCV, 2021. [38] L. Blankemeier, J. P. Cohen, A. Kumar, D. V. Veen, S. J. S. Gardezi, M. Paschali, Z. Chen, J.-B. Delbrouck, E. P. Reis, C. A. M. Truyts, C. Bluethgen, M. E. K. Jensen, S. Ostmeier, M. Varma, J. M. J. Valanarasu, Z. Fang, Z. Huo, Z. Nabulsi, D. Ardila, W.-H. Weng, E. A. Junior, N. Ahuja, J. A. Fries, N. H. Shah, A. Johnston, R. D. Boutin, A. Wentland, C. P. Langlotz, J. Hom, S. Gatidis, and A. S. Chaudhari, Merlin: vision language foundation model for 3d computed tomography, Research Square, 2024. [39] W. Lin, Z. Zhao, X. Zhang, C. Wu, Y. Zhang, Y. Wang, and W. Xie, Pmc-clip: Contrastive language-image pre-training using biomedical documents, in Proceedings of MICCAI. Springer, 2023, pp. 525536. [40] Y. Zhang, P. Hager, C. Liu, S. Shit, C. Chen, D. Rueckert, and J. Pan, Towards cardiac mri foundation models: Comprehensive visual-tabular representations for whole-heart assessment and beyond, arXiv preprint arXiv:2504.13037, 2025. [41] S. Rui, L. Chen, Z. Tang, L. Wang, M. Liu, S. Zhang, and X. Wang, Brainmvp: Multi-modal vision pre-training for brain image analysis using multi-parametric mri, arXiv preprint arXiv:2410.10604, 2024. [42] Y. Ye, Y. Xie, J. Zhang, Z. Chen, Q. Wu, and Y. Xia, Continual self-supervised learning: Towards universal multi-modal medical data representation learning, in Proceedings of CVPR, 2024, pp. 11 11411 124. [43] X. He, Y. Yang, X. Jiang, X. Luo, H. Hu, S. Zhao, D. Li, Y. Yang, and L. Qiu, Unified medical image pre-training in language-guided common semantic space, ArXiv, vol. abs/2311.14851, 2023. [44] Y. Ma, H. Derksen, W. Hong, and J. Wright, Segmentation of multivariate mixed data via lossy data coding and compression, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 29, no. 9, pp. 15461562, 2007. [45] Y. Yu, K. H. R. Chan, C. You, C. Song, and Y. Ma, Learning diverse and discriminative representations via the principle of maximal coding rate reduction, Advances in neural information processing systems, vol. 33, pp. 94229434, 2020. [46] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in Proceedings of ICLR, 2021. [47] P. H. Smedsrud, V. L. Thambawita, S. Hicks, H. L. Gjestang, O. O. Nedrejord, E. Næss, H. Borgli, D. Jha, T. J. D. Berstad, S. L. Eskeland, M. Lux, H. N. Espeland, A. Petlund, D. T. D. Nguyen, E. Garcia-Ceja, D. Johansen, P. T. Schmidt, E. Toth, H. L. Hammer, T. de Lange, M. Riegler, and P. Halvorsen, Kvasir-capsule, video capsule endoscopy dataset, Scientific Data, vol. 8, 2020. [48] X. Zhang, C. Wu, Z. Zhao, J. Lei, Y. Zhang, Y. Wang, and W. Xie, Radgenome-chest ct: grounded vision-language dataset for chest ct analysis, ArXiv, vol. abs/2404.16754, 2024. [49] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, arXiv preprint arXiv:2304.07193, 2023. [50] M. Tschannen, A. Gritsenko, X. Wang, M. F. Naeem, I. Alabdulmohsin, N. Parthasarathy, T. Evans, L. Beyer, Y. Xia, B. Mustafa et al., Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features, arXiv preprint arXiv:2502.14786, 2025. [51] O. Siméoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa et al., Dinov3, arXiv preprint arXiv:2508.10104, 2025. [52] S. Pai, I. Hadzic, D. Bontempi, K. Bressem, B. H. Kann, A. Fedorov, R. H. Mak, and H. J. Aerts, Vision foundation models for computed tomography, arXiv preprint arXiv:2501.09001, 2025. [53] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in Proceedings of ICCV, 2021, pp. 96509660. [54] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, Unsupervised learning of visual features by contrasting cluster assignments, Advances in neural information processing systems, vol. 33, pp. 99129924, 2020. [55] S. Bai, K. qin Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin, Qwen2.5-vl technical report, 2025. [56] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei et al., Qwen2.5 technical report, arXiv preprint arXiv:2412.15115, 2024."
        },
        {
            "title": "Acknowledgement",
            "content": "The LaTeX template is built upon Metas original template."
        },
        {
            "title": "C Downstream Task Configuration",
            "content": "C.1 Dataset Setup C.2 Implementation Details"
        },
        {
            "title": "A Pretraining Data Distribution",
            "content": "15 15 17 17 18 19 20 20 In this section, we illustrate the distribution of body parts in the pretraining dataset across four modalities: X-ray, ultrasound, endoscopy, and CT, as shown in Fig. 6. For better visualization, we only include body parts with more than 1,000 samples. The overall modality distribution is summarized in Table 5. Table 5 Sample distribution across four imaging modalities in our collected dataset. Total 867,653 X-ray 286,240 Ultrasound 299, CT 233,088 Endoscopy 48,"
        },
        {
            "title": "B Details of Data Augmentation in SimDINO",
            "content": "Algorithm 1: Data Augmentation for 2D Images Input: Image R1HW , global crop scale sg, local crop scale sl, number of local crops Nl Output: list of augmented crops: {X (g1), (g2), (l1), . . . , (lNl)} Define global crop size Sg = (sg H, sg ) Define local crop size Sl = (sl H, sl ) Transformations: Global Transform1: RandCrop Resize RandFlip RandShiftIntensity Gaussian blur Normalize Global Transform2: RandCrop Resize RandFlip RandShiftIntensity Contrast adjust Normalize Local Transform: RandCrop Resize Normalize Apply augmentations: (g1) global_transform1(X) (g2) global_transform2(X) for = 1 to Nl do (li) local_transform(X) Expand channel to = 3 after each crop return {X (g1), (g2), (l1), . . . , (lNl)} Figure 6 Distribution of body parts in the pretraining dataset. Top Left: X-ray. Top Right: Ultrasound. Bottom Left: Endoscopy. Bottom Right: CT. Algorithm 2: Data Augmentation for Videos Input: Video R3HW S, global crop scale sg, local crop scale sl, number of local crops Nl Output: Augmented crops list Get number of frames: shape(X)[2] if > 16 then Randomly sample 16 indices: indices randint(0, S, 16) Temporal slice: X[:, :, :, indices] Define spatial crop sizes: Sg = (sg H, sg W, 1), Sl = (sl H, sl W, 1) Apply the same transformation logic as in Data Augmentation for 2D Images, but preserve the temporal axis (S) Return two global and Nl local augmented views Algorithm 3: Data Augmentation for 3D Volumes Input: Volume R1HW S, crop scales sg, sl, number of local crops Nl Output: list of augmented crops Set appropriate sizes: Global crop size: Sg = (sg H, sg W, sg S) Local crop size: Sl = (sl H, sl W, sl S) Resize targets vary per case Define transformation pipelines: Global Transform1: RandCrop Resize RandFlip RandShiftIntensity Gaussian blur Normalize Global Transform2: RandCrop Resize RandFlip RandShiftIntensity Contrast adjust Normalize Local Transform: RandCrop Resize Normalize Apply augmentations: (g1) global_transform1(X) (g2) global_transform2(X) for = 1 to Nl do (li) local_transform(X) Expand channel to = 3 after each crop return {X (g1), (g2), (l1), . . . , (lNl)} The data augmentation method for 2D images is designed for grayscale modalities such as X-ray and ultrasound. It generates two global views and multiple local crops. Each transformation includes spatial cropping, optional noise or contrast adjustments, and normalization. The augmented image is then duplicated across the channel dimension to form three channels and expanded along the slice dimension to match the shared input format. The data augmentation method for RGB endoscopy videos, which is similar to the 2D case in spatial dimensions, processes temporal stack of frames while preserving the slice dimension (typically = 16). The augmentations are applied only to the spatial axes. The data augmentation method for 3D volumes of varying depth. It chooses transformation parameters and resizing targets accordingly. All volumes are normalized and augmented in 3D. After transformation, the single-channel input is expanded to three channels to align with the shared encoder input convention."
        },
        {
            "title": "C Downstream Task Configuration",
            "content": "C.1 Dataset Setup In this study, we utilize several publicly available and private datasets to evaluate the performance of our models across various medical imaging modalities. The detailed specifications for each dataset are shown in Table 6, including its composition, the number of images used for testing, and the organs or disease categories it encompasses. Category-level Retrieval: For this task, we use the following public datasets: ChestXray14 [3] for X-ray modality, Fetal Planes [4] for Ultrasound, and Kvasir Capsule [47] and Hyper Kvasir [5] for Endoscopy. ChestX-ray14 [3] is widely recognized medical imaging dataset that comprises 112,120 frontal-view Xray images obtained from 30,805 unique patients. The dataset includes fourteen common thoracic disease labels, covering diverse range of conditions, including Atelectasis, Effusion, Infiltration, Pneumothorax, Edema, Emphysema, Fibrosis, Pleural Thickening, Cardiomegaly, Nodule, Mass, Hernia, Consolidation, and Pneumonia. To ensure clarity and focus in our evaluation, we excluded images associated with multiple abnormal categories, resulting in refined test set of 7,992 samples. This selection process aimed to minimize label ambiguity and facilitate more precise assessment of the models diagnostic capabilities. Fetal Planes [4] is comprehensive collection of routinely acquired maternal-fetal screening ultrasound images, sourced from two distinct hospitals. Each image in the dataset has been meticulously annotated by expert maternal-fetal clinicians and categorized into six classes: Abdomen, Brain, Femur, Thorax, Cervix, and Other. For the purpose of our evaluation, we excluded the \"Other\" class to refine the dataset and constructed test set comprising 8,188 images. This curated test set was used to assess the models ability to accurately classify fetal anatomical planes, ensuring focused and meaningful evaluation of its performance. Kvasir Capsule [47] stands as the largest publicly available PillCAM dataset in the field of gastrointestinal imaging. The dataset comprises 47,238 labeled images and spans total of 117 videos, capturing anatomical landmarks, pathological findings, and normal observations. For our experiments, we processed the labeled images by grouping consecutive frames into video segments. Specifically, we aggregated sequential images to form video clips, ensuring that each clip contained at least 16 frames. Finally, we obtained total of 344 video segments, which were subsequently used for our evaluations. HyperKvasir [5] is comprehensive image and video dataset capturing the gastrointestinal tract. The dataset comprises 373 videos featuring various findings and anatomical landmarks. Each video was manually reviewed by medical professionals specializing in gastroenterology, resulting in total of 171 annotated findings. For our study, we utilized Qwen2.5 [55] to classify the findings into 13 distinct categories. We excluded any data that could not be reliably categorized, ultimately constructing refined test set containing 364 samples. Progressive Regional Retrieval Tasks: To evaluate the models ability to handle fine-grained regional information, we design three increasingly challenging tasks, all using the RadGenome-ChestCT [48] dataset. RadGenome-ChestCT [48] is large-scale, region-guided 3D chest CT interpretation dataset based on the CT-RATE [6] dataset, which is the first publicly available dataset that pairs 3D medical images with corresponding textual reports. RadGenome-ChestCT extends the original CT-RATE of over 25,692 non-contrast 3D chest CT volumes and reports from more than 20,000 patients by incorporating organ-level segmentation masks covering 197 categories, multi-granularity grounded reports linked to specific anatomical regions, and 1.3 million grounded Visual Question Answering (VQA) pairs. These enhancements provide rich intermediate reasoning visual clues and enable models to associate textual explanations with corresponding visual evidence. In our experiments, we evaluated the model on test set of 25,687 CT images to assess its performance in interpreting complex regional information. To ensure label quality, we employed Qwen2.5 [55] as an auxiliary annotator to clean and harmonize the original labels, resolving inconsistencies and ambiguities in the annotations. This process yielded refined taxonomy of 21 well-defined abnormality categories, which were subsequently used for performance evaluation. Cross-modal Retrieval: In addition to the public datasets, we also utilized private datasets for cross-modal retrieval tasks. From the private MRI test set, we sampled 6,000 images representing 12 body parts. Similarly, we sampled 4,500 CT images encompassing 9 body parts and 9,500 X-ray images covering 19 body parts from the respective private test sets. These private datasets allowed us to evaluate the models generalizability across diverse imaging modalities and anatomical structures. Downstream Classification: To further evaluate the quality and transferability of the learned representations, we conduct downstream classification tasks on two datasets previously introduced: the publicly available RadGenome-ChestCT [48] for multi-label disease classification (18 categories) and private MRI dataset for organ classification (12 categories). For the RadGenome-ChestCT dataset, we follow the data split protocol established in [48], which consists of 24,123 training samples and 1,564 test samples. This split ensures consistent and fair comparison with prior work. For the private MRI dataset, we perform stratified train-test split with 4:1 ratio to maintain class distribution balance across both subsets, resulting in dedicated training set and an independently held-out test set for unbiased evaluation. All models are trained on the respective training sets using frozen features to assess the effectiveness of the pre-trained representations in linear probing setup. By leveraging these datasets, we aim to provide comprehensive evaluation of our models capabilities in handling medical imaging data across various modalities and pathological contexts. C.2 Implementation Details With the pretrained vision encoder, we compute the embeddings for each data sample, then we calculate the cosine similarity between the query and candidate samples to evaluate their relevance. The recall list is Table 6 Overview of downstream datasets used in the study. Task Category Category-level Retrieval Modality Dataset X-ray ChestXray14 Fetal Planes Ultrasound Kvasir Capsule Endoscopy Endoscopy Hyper Kvasir Test Size Classes/Regions 7,992 8,188 344 364 14 classes 5 anatomical planes 13 abnormalities 13 abnormalities Progressive Regional Retrieval CT-RATE Cross-modal Retrieval Downstream Classification Private MRI Private CT Private X-ray CT-RATE Private MRI CT MRI CT X-ray CT MRI 25, 21 abnormalities 6,000 4,500 9,500 1,564 1,200 12 body parts 9 body parts 19 body parts 18 abnormalities 12 body parts Table 7 Comparison of classification performance on the CT-RATE and MRI datasets. The best and second-best results are highlighted in bold and underlined, respectively. denotes models using language supervision, and and indicate supervised pretraining with segmentation masks and disease categories. Method CT-RATE In-house MRI F1 Score Precision Accuracy AUROC F1 Score Precision Accuracy AUROC 0.105 VoCo [18] 0.156 UniMiSS+ [8] 0.306 CT-FM [52] 0.136 CT-CLIP [6] / Merlin [38] M3Ret (MAE) 0.178 M3Ret (SimDINO) 0.313 0.810 0.810 0.803 0.819 / 0.824 0.636 0.702 0.763 0.720 / 0. 0.259 0.472 0.494 0.324 / 0.355 0.404 0.829 / / 0.949 0.785 0.404 0.828 / / 0.948 0.782 0.427 0.828 / / 0.948 0.785 0.847 0.982 / / 0.997 0. 0.840 0.954 0.954 0.624 0.955 0. 0.797 then examined to determine whether the retrieved samples belong to the same disease category or anatomy, thereby verifying if they are true positive samples. This process ensures that the retrieval results align with the intended clinical or anatomical context. Specifically, in the context of cross-modal retrieval, key challenge arises from the fact that different modalities have different descriptions about varying field-of-views or anatomical body parts, typically reflected by the BodyPartExamined tag within the DICOM (Digital Imaging and Communications in Medicine) format. To correlate images depicting the same body part across different modalities, we leverage Qwen2.5 [56] to establish tag mapping between anatomical regions across different modalities. This mapping enables systematically identify whether two samples from different modalities correspond to the same anatomical region, thereby facilitating accurate determination of positive and negative samples."
        },
        {
            "title": "D Downstream Classification Performance",
            "content": "To further validate the quality and transferability of the learned representations, we evaluate M3Ret on downstream classification tasks. We benchmark our model on two distinct datasets: CT-RATE [6] for multi-label disease classification (18 categories) and our in-house MRI dataset for body part classification (12 body parts). As shown in Table 7, M3Ret with SimDINO pretraining consistently outperforms other methods, achieving the best results across F1 Score, Precision, Accuracy, and AUROC. This highlights that SimDINO learns more discriminative features than MAE, leading to superior performance. Interestingly, Merlin also performs well on the MRI task despite having never been trained on MRI data. This suggests that its cross-modal generalization capability may be largely attributed to learning robust representations of anatomical structures, which are transferable across modalities. Note that CT-FM [52] and CT-CLIP [6] were not evaluated on the MRI task, as their pretraining is limited to CT data. Similarly, Merlin was excluded from the CT-RATE evaluation, as its supervised pretraining with disease labels would create an unfair comparison with the other self-supervised methods. These results underscore the strength of our approach in learning modality-agnostic, transferable features that are highly effective for diverse downstream applications."
        },
        {
            "title": "E Limitations and Future Work",
            "content": "We pretrain M3Ret on mainstream medical imaging modalities, including 2D grayscale images (X-ray, ultrasound), RGB videos (endoscopy), and 3D volumetric scans (CT), which cover broad range of clinical scenarios. However, the medical imaging domain encompasses wider variety of modalities such as PET, SPECT, functional MRI (fMRI), and whole-slide histopathology images, each with distinct characteristics and clinical value. Even within CT imaging, different acquisition protocols, such as varying radiation doses or contrast phases, may introduce distribution shifts. Due to the high cost, limited availability, and data heterogeneity associated with these modalities, collecting large-scale, high-quality datasets for unified pretraining remains major challenge. In future work, we aim to extend our framework to incorporate more diverse and fine-grained modalities, and explore transfer strategies to adapt unified representations to data-scarce or rare imaging types."
        },
        {
            "title": "F Broader Impact",
            "content": "This work delivers key finding to the medical imaging and broader medical imaging computing communities: visual SSL enables scalable and effective representation learning across heterogeneous medical imaging modalities without relying on modality-specific architectural design or paired supervision. By demonstrating strong performance and generalization from purely visual signals, our findings highlight the potential of visual SSL as unified approach for medical image understanding. Our results also emphasize the critical role of data scale. We show that increasing the quantity of training data yields substantial performance gains, underscoring the need for future efforts to prioritize large-scale, high-quality medical image collection and curation. We hope this work encourages the community to shift focus toward building diverse multimodal datasets for learning unified visual representations that can benefit wide range of clinical and scientific applications. To date, self-supervised learning and medical image retrieval technologies in medical image analysis have not been associated with negative societal outcomes. Our training process adheres to strict data anonymization and privacy standards, ensuring that no sensitive personal information is exposed. This framework does not introduce any foreseeable risks related to ethical or societal impact."
        },
        {
            "title": "G More Visualization Results",
            "content": "We present additional results for regional abnormality retrieval and cross-modal retrieval tasks. Our proposed model, M3Ret, achieves superior performance across nearly all configurations. The results of regional abnormality retrieval are illustrated in Figure 7. Our approach achieves these results using purely self-supervised learning (SSL), without relying on any task-specific design or annotations. This is significant advancement, as it demonstrates the capability of generic visual SSL to implicitly capture localized pathological cues, which are often critical for fine-grained medical image analysis. By leveraging self-supervision, our method avoids the need for region-level supervision, which is typically labor-intensive and requires expert annotations. Besides this, the model demonstrates strong performance in retrieving fine-grained details, suggesting that the learned representations inherently encode meaningful regional information. This ability to capture localized features without explicit guidance highlights the potential of SSL to serve as powerful tool for medical imaging tasks, where annotated data is often scarce or expensive to obtain. The results of cross-modal retrieval are shown in Figures 8, 9, 10, 11, 12, 13. Notably, it outperforms UniMiSS+ [8], even though the latter is pretrained on explicitly paired CT and X-ray images, which are typically expected to provide strong foundation for cross-modal alignment. This underscores the robustness of our approach. Even more impressively, M3Ret demonstrates strong generalization capabilities to MRIrelated tasks, despite never having been exposed to MRI data during pretraining. This highlights the models ability to transfer knowledge across diverse medical imaging modalities without requiring modality-specific fine-tuning or explicit domain adaptation. These findings suggest that self-supervised learning (SSL) in the visual domain can effectively learn generalizable visual representations that transcend specific imaging modalities. Furthermore, our unified SSL framework successfully encodes diverse imaging modalitiessuch as CT, X-ray, and MRIinto shared latent space. This is achieved without relying on paired samples or explicit cross-modal alignment strategies, which are often resource-intensive and challenging to obtain in real-world scenarios. Importantly, the framework preserves semantic consistency across modalities, enabling meaningful comparisons and alignments between different types of medical images. These results not only validate the effectiveness of our approach but also open new avenues for leveraging self-supervised learning in multimodal medical imaging applications, where labeled data is scarce or difficult to acquire. In summary, our work contributes to advancing the field of medical imaging by demonstrating that single, unified SSL framework can effectively handle multiple imaging modalities while maintaining high performance and semantic coherence. This has significant implications for improving diagnostic tools and workflows in clinical settings, particularly when dealing with heterogeneous datasets. Figure 7 Qualitative results of top 3 retrieval examples in regional abnormality retrieval. Figure 8 Qualitative results of top 3 retrieval examples in CT X-ray Task. Figure 9 Qualitative results of top 3 retrieval examples in X-ray CT Task. Figure 10 Qualitative results of top 3 retrieval examples in CT MRI Task. Figure 11 Qualitative results of top 3 retrieval examples in MRI CT Task. Figure 12 Qualitative results of top 3 retrieval examples in X-ray MRI Task. Figure 13 Qualitative results of top 3 retrieval examples in MRI X-ray Task."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Imperial College London",
        "Tsinghua University"
    ]
}