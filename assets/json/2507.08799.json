{
    "paper_title": "KV Cache Steering for Inducing Reasoning in Small Language Models",
    "authors": [
        "Max Belitsky",
        "Dawid J. Kopiczko",
        "Michael Dorkenwald",
        "M. Jehanzeb Mirza",
        "Cees G. M. Snoek",
        "Yuki M. Asano"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it a more robust and practical solution for controlled generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 9 9 7 8 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Max Belitsky\nVIS Lab\nUniversity of Amsterdam",
            "content": "Dawid J. Kopiczko FunAI Lab University of Technology Nuremberg"
        },
        {
            "title": "Michael Dorkenwald\nVIS Lab\nUniversity of Amsterdam",
            "content": "M. Jehanzeb Mirza CSAIL MIT Cees G.M. Snoek VIS Lab University of Amsterdam Yuki M. Asano FunAI Lab University of Technology Nuremberg"
        },
        {
            "title": "Abstract",
            "content": "We propose cache steering, lightweight method for implicit steering of language models via one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach leverages GPT-4o-generated reasoning traces to construct steering vectors that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of hyperparameter stability, inference-time efficiency, and ease of integration, making it more robust and practical solution for controlled generation."
        },
        {
            "title": "Introduction",
            "content": "The ability of large language models to perform complex reasoning is key driver of their increasing utility. However, this potential is not always spontaneously realized, especially in smaller models which may possess latent reasoning capabilities that require specific guidance to activate. Traditional methods for uncovering these abilities, such as supervised fine-tuning or few-shot prompting with chain-of-thought examples, can be effective but often demand significant data or intricate prompt design. The question then arises: can we develop more lightweight interventions to unlock and steer these inherent reasoning processes post-training? One promising direction is activation steering (Turner et al., 2024; Panickssery et al., 2023), which aims to guide model behavior by directly modifying its internal hidden states. While promising for its ability to influence outputs without retraining, activation steering often requires continuous interventions at each token generation step throughout the decoding process to be effective (Wehner et al., 2025). This continuous manipulation can introduce instability, making the outcomes highly sensitive to hyperparameter choices (e.g., targeted layers, intervention strength) and potentially leading to degradation in generation quality. To address these issues, we introduce method called cache steering. Our approach operates by making targeted, one-time modification directly to the key-value cache of Transformer model, Correspondance to max.belitsky@student.uva.nl and dawid.kopiczko@utn.de Preprint. Under review. Figure 1: Comparison of activation steering (left) and cache steering (right). Activation steering modifies the models hidden states at each generation step, requiring repeated interventions that propagate through the model and can lead to instability if not carefully tuned. In contrast, cache steering modifies only the stored key and value tensors from the prompt in one-shot fashion. These modified representations influence future generations implicitly, leading to more stable and efficient inference. typically after the cache has been populated by an initial prompt. By applying steering vectors, derived from reasoning traces generated by capable teacher model like GPT-4o, to these cached key and value representations, we can guide the reasoning trajectory of smaller models. This single intervention, applied before token generation begins, effectively steers the model towards more explicit, multi-step reasoning without altering model weights or requiring complex prompt modifications. It results in several key advantages: improved stability and robustness to hyperparameter variations, significantly reduced computational overhead at inference time as no ongoing interventions are needed, and seamless integration with standard Transformer inference pipelines. We demonstrate that our method improves reasoning structure and, in many cases, task accuracy on multiple benchmarks, including GSM8K, ARC-Challenge, CSQA, and PIQA. Overall, our key contributions are as follows: We propose cache steering, novel technique for controlling model behavior via one-shot modification of the KV cache at inference time. We demonstrate that cache steering enables distillation of reasoning styles from larger models such as GPT-4o into smaller models without fine-tuning or prompt augmentation. We conduct extensive evaluations across reasoning benchmarks, comparing cache steering against activation steering and CoT prompting, and analyze its efficiency and stability."
        },
        {
            "title": "2 Related Work",
            "content": "Reasoning and Chain-of-Thought prompting. widely adopted approach to enhance reasoning abilities in LLMs involves demonstrating example solutions to the problem (in-context learning or ICL) that contain step-by-step reasoning process (Chain-of-Thought or CoT) in prompt to the language model (Brown, 2020; Wei et al., 2022), technique known as few-shot prompting. Zero-shot variants of CoT prompting simplify this approach by adding instructions such as \"Lets think step by step\" to elicit step-by-step reasoning without the need for example demonstrations (Kojima et al., 2022). Recent work shows that reinforcement learning can lead to remarkable reasoning capabilities, which can be effectively distilled into smaller models through supervised fine-tuning (Guo et al., 2025). These findings suggest that it is not enough to just trigger CoT reasoning in the language models, but the style of the reasoning matters. This motivates our approach, which aims to directly steer small models toward reasoning behavior reminiscent of larger teacher models via cache-level interventions. The code is available at https://github.com/MaxBelitsky/cache-steering. 2 Activation steering. Activation steering, also known as representation engineering, is technique used to control the generation process of LLMs implicitly by manipulating their intermediate activations during decoding, typically through linear interventions (Panickssery et al., 2023; Turner et al., 2024). Multiple works have applied activation steering to induce or suppress specific behaviors in models without retraining. The examples include sentiment, topic and style control (Turner et al., 2024); function steering (Todd et al., 2023; Postmus and Abreu, 2024), removing or inducing refusal behavior, (Lee et al., 2024), toxicity reduction (Turner et al., 2024), truthfulness (Wang et al., 2025), editing factual knowledge (Yin et al., 2024), reasoning (Zhang and Viteri, 2025; Galichin et al., 2025) and other (Wehner et al., 2025). In its most basic form, activation steering involves two steps: vector extraction and injection of the vector into the activations of the model at inference. The vector extraction stage involves computing steering vector, which is commonly done by aggregating activations from pairs of positive prompts with desired behavior and negative or sometimes neutral prompts, forming contrastive set = {(p+ )}. The most common aggregation method is Differencein-Means (Wehner et al., 2025), which is identical to Mean-of-Differences when the vectors are paired: 1 ), ..., (p+ 0 ), (p+ , 0 , 1 , sl ="
        },
        {
            "title": "1\nN",
            "content": "(cid:88) fl(p+) fl(p) (p+,p)C where fl represents the part of the Transformer model (e.g., the whole decoder layer) at layer and is the number of examples in the contrastive dataset C. To steer the models output, the steering vector is added to the activations of specific layers during inference: = hl + csl where hl represents the activations at layer before steering, sl is steering vector extracted from layer l, and is coefficient that determines the strength of the steering. It is important to mention that the vector can be extracted and applied to different token positions, layers, and parts of the model, which are treated as hyperparameters or design choices. Usually, it is common practice to perform grid search to determine the layers to apply steering to and the value of the steering strength coefficient (Turner et al., 2024; Lee et al., 2024; Wang et al., 2025; Dong et al., 2024; Panickssery et al., 2023; Wang et al., 2024; Stolfo et al., 2024; Zhang and Viteri, 2025; Postmus and Abreu, 2024). While activation steering offers tool for model control, it typically requires continuous intervention during generation (Wehner et al., 2025), which can be expensive and can lead to unstable generations. In contrast, our work extends the activation steering paradigm to the key-value (KV) cache, enabling one-shot intervention that is both more efficient and more stable at inference time. Cache manipulation. Another emerging line of research explores the idea of modifying the keyvalue (KV) cache from the memory and efficiency perspective (Li et al., 2024; Liu et al., 2024a; Ge et al., 2023; Mu et al., 2023). These approaches aim to reduce the memory footprint or compress contextual representations through KV cache manipulation. Building on this idea, Liu et al. (2024b) introduced method for augmenting the KV cache to improve the performance on tasks that require reasoning abilities. The authors use differentiable \"coprocessor\", which allows augmenting the KV cache as pre-generation step instead of modifying activations directly during the forward pass. However, in order to augment the KV cache, the method requires training separate model, which makes this method less practical than pure activation steering methods introduced in the previous subsection. In contrast, our approach aims to use the KV cache as target for behavioral control in small models without training auxiliary modules."
        },
        {
            "title": "3 Cache Steering",
            "content": "We introduce cache steering, lightweight method for inducing structured reasoning in language models by applying steering vectors directly to the key-value cache. Unlike traditional activation steering methods, which modify intermediate hidden states during generation, our approach modifies the cached keys and values associated with specific tokens, enabling one-shot intervention that can be precomputed and reused. This technique is compatible with standard inference APIs and does not require model fine-tuning or prompt engineering."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Transformer-based language models rely on the self-attention mechanism, which operates on sets of query, key, and value vectors to compute contextualized token representations. For given input sequence, the attention output at layer is computed as: Attention(Ql, Kl, l) = softmax (cid:32) Ql(Kl) Dh (cid:33)"
        },
        {
            "title": "V l",
            "content": "where Ql, Kl, RT HDh are the query, key, and value tensors at layer l, is the sequence length, is the number of attention heads, and Dh is the dimensionality of each head. During autoregressive decoding, the model stores the keys Kl and values corresponding to previously processed tokens, which is known as key-value (KV) cache. These cached tensors are used to efficiently compute attention for each new token without recomputing representations for the entire sequence. Importantly, these cache entries can be precomputed and reused across multiple examples (such as caching system prompt), which is especially useful in scenarios involving large models or repeated inference over similar inputs. This makes the KV cache potential target for behavioral interventions, offering compatibility with real-world settings. 3.2 Extracting key-value steering vectors 1 , 0 , 0 ), (p+ Similarly to activation steering, we construct contrastive set of prompt pairs = {(p+ )} to extract the key-value steering vectors. We refer to prompts that demonstrate the desired behavior as positive and the prompts without such behavior as negative. We discuss the details of how the positive and negative prompts are constructed for the reasoning induction task in Section 3.5. 1 ), ..., (p+ , For each contrastive pair of examples, we make forward pass and extract the keys and values vectors from the designated token position (typically the final token of the input prompt). The vectors are then aggregated using the Mean-of-Differences method: Sk = 1 (cid:88) fl(p+) fl(p) (p+,p)C Sv = 1 (cid:88) fl(p+) fl(p) (p+,p)C where fl is Transformer layer, Sk RHDh are the resulting steering tensors at layer l, with denoting the number of attention heads and Dh their dimension. By taking the difference between positive and negative examples and averaging across multiple contrastive pairs, we aim to isolate directional signal associated with target behavior while minimizing the amount of noise introduced by information from individual examples. RHDh and Sv 3.3 Applying key-value steering vectors At inference time, we perform standard forward pass on the input prompt to populate the KV cache. Then, at each layer l, we modify the cached key and value vectors at target token position of the KV cache as follows: = + cvSv where Kl, RHDh are the original cached key and value vectors at layer l, and Sk RHDh are the steering vectors, and ck, cv are scalar coefficients controlling the steering strength. Then the generation proceeds as usual using the modified cache. = Kl + ckSk , Sv 3.4 Intuition Cache steering differs fundamentally from traditional activation steering in how and when it influences the models behavior during generation. visual depiction of the key differences can be observed in Figure 1. Below, we outline the core intuitions behind our approach. No modification of queries. Activation steering typically alters the hidden states that produce the queries, keys, and values. Changing the queries can potentially degrade the models ability to 4 condition on its input. In contrast, cache steering modifies only the stored keys and values from past tokens, leaving the queries unchanged, which can help the model to retain more accurate representation of the input context during decoding. Steering the past vs. steering the present. At specific timestep t, activation steering explicitly affects the current hidden state of the layer l, which then propagates through all subsequent layers +1 to +N , where is the number of layers in the model. This can lead to cascading effects, where small interventions compound across layers, known as \"oversteering\", which can negatively affect the generation quality after the intervention. There is extensive evidence that this makes activation steering very sensitive to specific hyperparameters like steering strength or the application layer (Turner et al., 2024; Lee et al., 2024; Wang et al., 2025; Dong et al., 2024; Panickssery et al., 2023; Wang et al., 2024; Stolfo et al., 2024; Zhang and Viteri, 2025; Postmus and Abreu, 2024). In contrast, cache steering modifies the fixed key and value representations of past tokens. These values are not further processed through the layers, but are instead directly attended to by future queries. This means cache steering can be applied to many (or all) layers without aggregation of the effects, which leads to implicit effects on the future hidden states. Alignment with source behavior. Crucially, our steering vectors are extracted from the keys and values of prompts containing CoT reasoning traces. In standard few-shot prompting scenarios, these key-value pairs would reside in the past cache and would be attended to during generation. By applying steering vectors to the KV cache at inference time, we hope to transfer the style of these examples without needing to include them explicitly. 3.5 Implementation details Contrastive set construction. To extract steering vectors, we construct contrastive dataset consisting of paired prompts. Each pair includes positive example (containing explicit chain-ofthought reasoning) and negative example (containing only the final answer). Each contrastive prompt is created using few-shot in-context learning (ICL) examples. Specifically, both the positive and negative prompts include ICL examples followed by question and generation prompt. The positive and negative prompts differ only in the presence of reasoning steps in the ICL examples (see Appendix for more details and an illustrative example). Extraction and application positions. We extract key and value vectors from the final token of the prompt, which typically corresponds to the last token of the generation prompt depending on the models chat template (e.g. \"nn\" in \"assistantnn\"). During inference, we aim to apply cache steering to the same logical position in the prompt as used during extraction. However, due to the autoregressive decoding mechanism (see Section 3.1), the KV cache is populated only after each token is processed. To ensure alignment, we append neutral offset token (e.g., newline or whitespace) to the prompt, so that the KV representation of the final token can be used in the generation of the next tokens. This ensures the cache steering affects the intended location. Details on token alignment and cache offset are provided in Appendix F. Hyperparameters. As with activation steering, the steering strength coefficients of key and value vectors are treated as hyperparameters. Since we are interested in distilling reasoning behaviors from larger models, we additionally treat the number of contrastive pairs and the number of in-context examples in each pair as additional hyperparameters. Similarly to other steering approaches, we perform small grid search over the hyperparameters to obtain reasonable values for each modeldataset pair (Turner et al., 2024; Lee et al., 2024; Wang et al., 2025; Dong et al., 2024; Panickssery et al., 2023; Wang et al., 2024; Stolfo et al., 2024; Zhang and Viteri, 2025; Postmus and Abreu, 2024). We find that steering coefficients tend to lie within consistent ranges across tasks, suggesting robustness in the methods behavior. More on this in Section 5.2. The full list of hyperparameters can be found in Appendix G."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Datasets. We use four common reasoning benchmarks for the evaluation: GSM8K (Cobbe et al., 2021), CommonsenseQA (Talmor et al., 2018), ARC-Challenge (Clark et al., 2018), and PIQA (Bisk 5 Table 1: Comparison of baseline methods, activation steering, and cache steering across four reasoning benchmarks. We evaluate six models of different sizes on four datasets using both greedy (left side) and sampling-based (right side) decoding. Results show that cache steering consistently improves reasoning performance, often outperforming both baseline and activation steering. Combining cache steering with CoT prompting yields further gains in more than half of the cases. Numbers in parentheses denote standard deviation across 5 sampled generations per input. Dataset Model Greedy Sampling Baseline CoT Activation steering Cache steering Cache steering + CoT Baseline Cache steering ARC-c GSM8K CSQA PIQA SmolLM2-360M-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Phi-4-mini-instruct SmolLM2-360M-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Phi-4-mini-instruct SmolLM2-360M-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Phi-4-mini-instruct SmolLM2-360M-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Phi-4-mini-instruct 24.32 53.67 74.32 39.51 83.11 84.56 8.49 45.56 68.54 17.44 76.34 77.94 19.74 53.56 70.27 47.42 73.87 69.78 50.38 65.29 69.42 52.12 80.03 78. 26.62 53.75 77.13 37.20 84.98 86.69 10.39 46.10 71.57 24.94 77.56 74.68 22.11 54.71 72.56 44.31 74.04 70.11 52.61 64.96 76.93 53.43 81.61 79.59 24.06 53.84 74.23 40.69 84.64 86.18 7.66 45.41 68.38 23.81 76.50 75. 19.66 54.14 69.12 45.95 73.30 69.29 49.62 61.48 72.31 53.43 80.25 80.74 27.13 55.03 79.27 40.36 85.58 87.97 8.95 46.32 67.17 18.04 75.81 78.47 21.95 55.20 72.32 46.03 75.27 70.00 51.31 63.76 73.34 54.57 82.86 79. 25.26 56.14 79.52 38.82 85.24 86.77 10.39 47.16 72.10 25.47 77.86 75.74 22.52 53.56 72.48 45.37 74.37 70.52 52.50 66.43 76.88 55.55 83.13 80.25 24.16 (1.13) 24.52 (0.87) 52.29 (0.81) 53.16 (1.44) 74.64 (0.36) 77.71 (0.82) 38.05 (0.21) 35.96 (0.93) 82.66 (0.28) 85.09 (0.64) 83.46 (0.56) 87.2 (0.62) 7.87 (0.4) 8.08 (0.38) 43.71 (0.83) 43.88 (1.22) 68.22 (0.43) 67.57 (1.22) 16.94 (1.08) 16.48 (0.4) 75.94 (0.62) 75.22 (0.58) 77.48 (0.62) 77.1 (0.66) 20.02 (1.77) 21.31 (0.67) 51.45 (0.64) 50.78 (0.73) 70.09 (0.78) 70.40 (1.1) 45.67 (1.18) 42.36 (1.11) 73.92 (0.37) 74.27 (1.01) 68.22 (0.62) 67.52 (1.08) 48.12 (0.66) 50.72 (1.03) 65.02 (0.88) 62.48 (1.57) 68.35 (0.28) 71.83 (0.66) 51.82 (0.68) 53.86 (0.44) 79.08 (0.39) 83.41 (0.52) 77.19 (0.63) 79.46 (0.68) et al., 2020). These datasets span arithmetic reasoning, commonsense inference, scientific questions, and physical commonsense reasoning. For each dataset, we generate elaborate step-by-step answers to subset of questions from the corresponding training sets using GPT-4o, which are then used in positive examples in the contrastive set. The details on the specific prompt used to generate these steps and the generation procedure can be found in the Appendix J. Steering vectors are computed using the training set, while evaluation is performed on the corresponding test sets. Models. We evaluate cache steering on small instruction-tuned models from four families: Llama3.2 (1B and 3B variants), SmolLM2 (360M), Qwen2 (0.5B), and Phi-4-mini (3.8B) (Grattafiori et al., 2024; Team, 2024; Allal et al., 2025; Abouelenin et al., 2025). Additionally, we add the Llama-3.1 (8B) model to evaluate how cache steering scales beyond the smallest models. The list with the full model names and URLs can be found in Appendix H. Decoding strategies. Since cache steering affects internal representations, which result in shift in output logits, we evaluate our approach using both deterministic and stochastic decoding. For sampling-based decoding, we assess the consistency of steering effects by generating the response with 5 different seeds and comparing that to the baseline generations using the same setup. The generation arguments can be found in Appendix E. Answer extraction and metrics. Answer correctness is determined using task-specific heuristics. For GSM8K, we extract the final number mentioned in the output using digit pattern matching (Wang et al., 2023; Wang and Zhou, 2024). For multiple-choice tasks (ARC, PIQA, CSQA), we develop 6 Table 2: Cache steering consistently increases the length of generated outputs across tasks. We report the average number of generated tokens under three conditions: baseline decoding, CoT prompting, and cache steering. Results are shown for multiple model sizes averaged across reasoning benchmarks. Cache steering leads to significantly longer outputs, exceeding even CoT-prompted completions, suggesting that the intervention encourages more elaborate reasoning, even without explicit prompting."
        },
        {
            "title": "Cache Steering",
            "content": "SmolLM2-360M-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Phi-4-mini-instruct 73.5 (94.1) 100.0 (59.5) 156.0 (36.7) 121.8 (50.7) 160.2 (37.7) 107.8 (33.5) 194.8 (27.7) 167.8 (52.1) 174.8 (18.6) 161.8 (27.5) 181.0 (29.6) 211.0 (20.7) 294.2 (52.4) 225.0 (50.6) 297.2 (61.0) 291.2 (122.4) 284.5 (95.8) 328.8 (132.7) 4-stage extraction pipeline that uses soft string matching against known answer choices, with failover to constrained decoding. More details on the answer extraction process can be found in Appendix C. In several experiments, we compare cache steering to activaComparison to activation steering. tion steering. More specifically, we use the CAA method (Panickssery et al., 2023), which is one of the most popular methods for activation steering. In all experiments, we make the best effort to provide fair comparison. The details of how activation steering vectors are extracted and applied can be found in Appendix D."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Inducing reasoning via cache steering To evaluate the effectiveness of cache steering in inducing reasoning behavior, we compare it against several baselines: standard greedy decoding (no intervention), CoT prompting (appending Lets think step by step to the prompt), and activation steering. We also evaluate hybrid approach that combines CoT prompting with cache steering. As shown in Table 1 (greedy part), cache steering consistently outperforms the baseline and often leads to performance gains over the CoT prompting. Furthermore, the combination of CoT prompting with cache steering leads to additional gains in more than half of the cases, indicating the complementary nature of both techniques. Notably, cache steering surpasses activation steering in almost all cases. Additionally, we report the mean number of generated tokens per model, averaged over all datasets, in Table 2 (the individual results for each dataset-pair can be found in Appendix O). Cache steering leads to longer outputs, exceeding even CoT-prompted completions, suggesting that the intervention encourages more elaborate reasoning, even without explicit prompting. Taking into account the results from both tables, we can conclude that cache steering leads to more elaborate reasoning. Importantly, even though cache steering is effective at inducing reasoning, this alone does not guarantee higher accuracy on the benchmarks. In some cases, it induces reasoning traces that are well-structured and plausible but arrive at incorrect conclusions (e.g., due to question ambiguity or alternative valid interpretations). We also observe that failures or the models to tie complex reasoning back to the choice labels (i.e., generating the correct answer but the wrong label) sometimes cause correct reasoning traces to be scored as incorrect. We encourage readers to examine qualitative outputs in the code repository, where the steering effect is visible in the structure and style of responses. We also provide several qualitative examples in Appendix A. Qualitative examination. To verify that cache steering indeed induces reasoning behaviors, we present several representative completions in Appendix A. In nearly all cases, cache steering causes the model to produce multi-step, logically structured answers, even when the baseline outputs are shallow or abrupt. Figure 2: Cache steering ablations on ARC-c (Llama-3.2-1B-Instruct). Accuracy remains stable across contrastive set sizes and key/value steering strengths, with optimal performance around ck = 0.3 and cv = 6. Fewer in-context examples (e.g., 1-shot) yield better steering, likely due to reduced noise. Overall, the method is robust to range of hyperparameters. Figure 3: Cache steering introduces negligible overhead compared to activation steering. We report average time per token (in milliseconds) for cache steering, activation steering, and the baseline (no intervention) on single H100 GPU, using batch sizes of 1 and 16. Unlike activation steering, which requires continuous intervention, cache steering adds virtually no runtime cost over baseline inference. Stability under sampling. The right side of Table 1 reports results under sampling-based decoding, comparing cache steering to the baseline across multiple models and tasks. We observe that cache steering produces consistent performance improvements or maintains parity with the baseline, indicating that the intervention leads to stable and meaningful shifts in the models logits. Rather than injecting noise or introducing erratic behavior, cache steering systematically biases the model toward more structured reasoning even under stochastic generation. The relatively low standard deviations across runs further support the robustness of the effect. 5.2 Ablation studies We conduct ablation experiments on Llama-3.2-1B-Instruct and the ARC-c dataset to assess the sensitivity of cache steering to the primary hyperparameters: 1) number of contrastive pairs used to extract steering vectors, 2) number of few-shot examples per contrastive example, and 3) steering strength coefficients ck and cv. The results for all the ablation studies can be found in Figure 2. Vector extraction. We vary the number of contrastive pairs from 100 to 1000. The accuracy remains relatively stable across this range, with only minor fluctuations (from 53.1% to 55.7%). This suggests that even small contrastive sets can yield effective steering vectors, though performance tends to improve slightly with larger sets. We also vary the number of ICL examples per prompt from 1 to 10. Interestingly, the best result is achieved with single example (55.8%), and performance dips slightly at 3-shot (52.4%) before recovering. This non-monotonic trend suggests that reasoning signals may be sensitive to specific examples in the training data. Vector application. More importantly, we observe that cache steering is robust to steering strength variation. Varying the key coefficient ck between 0.0 and 0.4 results in only minor changes, with 8 Table 3: Overview of reasoning styles used in the style transfer experiment. Each style reflects distinct structure."
        },
        {
            "title": "Style Name",
            "content": "Stepwise Reasoning Strategy + Execution Causal Chain Annotated Deduction Analogical Reasoning Step 2: Structure Step 1: ... Strategy: ... If ..., then ... [Premise] [Inference] This is similar to... ... Solution: ... Therefore... Thus, we can infer... Table 4: Percentage of generated outputs that exhibit the intended structure when steered using style-specific vector. Results demonstrate that cache steering can reliably induce distinct reasoning styles, although its effectiveness varies across styles. Metric (%)"
        },
        {
            "title": "Stepwise\nReasoning",
            "content": "Strategy+ Execution"
        },
        {
            "title": "Matching Style",
            "content": ""
        },
        {
            "title": "Causal\nChain",
            "content": ""
        },
        {
            "title": "Analogical\nReasoning",
            "content": "15 90 the best performance at ck = 0.3 (56.4%). Varying the value coefficient cv from 1 to 10 shows peak around cv = 6 (55.0%) and gradual decline afterward, with performance dropping below 52% beyond cv = 8. Although extreme hyperparameters can lead to slight performance drops, cache steering remains stable to local changes to the coefficients. In contrast, activation steering often exhibits high sensitivity, with small shifts in coefficient values leading to catastrophic generation failures (Panickssery et al., 2023; Turner et al., 2024; Da Silva et al., 2025). We show the sensitivity of activation steering to hyperparameters on smaller subset of ARC-c in Appendix N. 5.3 Computational overhead Cache steering requires only one-time cache modification prior to generation, unlike activation steering, which in most cases requires continuous intervention for the steering to be effective (Wehner et al., 2025). As shown in Figure 3, cache steering achieves latency comparable to the baseline (no intervention), while activation steering incurs substantially higher time per token, especially at larger batch sizes. These findings underscore the practical efficiency of cache steering, making it well-suited for real-world deployment scenarios. Full experimental details are provided in Appendix M. 5.4 Style Transfer To explore whether cache steering can be used to distill distinct reasoning styles from teacher model, we evaluate how the stylistic form of the reasoning traces used to extract the vectors affects the response structure. For this experiment, we select subset of 20 questions from the ARC-Challenge dataset, each paired with its correct multiple-choice answer. For each question, we construct six distinct reasoning traces that arrive at the same answer but differ in their structure. The description of the six styles can be found in Table 3. We extract one steering vector per style. During inference, we apply each style-specific steering vector to set of 20 questions from the test set of the ARC-Challenge dataset to examine how it affects the resulting output structure. For this experiment, we use the SmolLM-360M-Instruct model due to its small size. Table 4 reports the percentage of generated responses that match the intended structure for each style-specific steering vector. The results indicate that cache steering successfully induces the correct structure in almost all test cases for Stepwise Reasoning, Causal Chain, and Analogical Reasoning styles. In these cases, the outputs consistently adhere to the patterns from the test data used to extract the steering vectors, suggesting that the stylistic signal is robustly captured and transferred through the key-value cache intervention. By contrast, style transfer is less reliable for the Strategy + Execution and Annotated Deduction formats. Only half of the generations reflect the strategy-execution structure, and just 3 out of 20 9 Figure 4: Example outputs from the SmolLM-360M-Instruct model on single ARC-Challenge question, using different style-specific steering vectors. Each generation reflects the structure of the steering traces used to construct the corresponding vector. match the annotated deduction style. To understand these failure modes, we perform qualitative analysis of the outputs. In the case of Annotated Deduction, we hypothesize that this format is underrepresented in the models pretraining distribution. While most of the completions exhibit partial stylistic artifacts, such as starting with phase or word in square brackets (e.g., [Farms] in Wyoming were ...), they lack the structured logical progression seen in the positive examples used during vector extraction. In most cases, the steering signal appears to nudge the model in the direction of the desired style, but is either not strong enough to elicit full adherence or too strong to generate meaningful text. similar pattern emerged with the Strategy + Execution format. Although all the responses begin with the correct discourse marker (e.g., Strategy:), half of the generated samples repeat the same marker in loop (e.g. Strategy:, Strategy:, ...). We attribute this breakdown to possible oversteering: since we do not explicitly adapt the steering coefficients for each style, the default values are likely too high for this specific case, leading to degenerate outputs. Figure 4 provides qualitative examples of the opening segments of responses generated for single ARC-Challenge question under each of the five steering styles. These illustrate that the rhetorical differences between generations are not only detectable but often pronounced. For instance, all responses produced using the analogical steering vector begin with Just like ..., while the causal chain examples adhere to conditional logic pattern. These observations confirm that stylistic signals are indeed encoded in the KV cache and can be carried over to any prompt using cache steering. Taken together, these results suggest that cache steering can be used not only to induce reasoning in general, but to exert more fine-grained control over its form. This opens up future work on controllable reasoning strategies, such as choosing style to improve model interpretability, user alignment, or explanation quality."
        },
        {
            "title": "6 Limitations",
            "content": "As with any method, cache steering has limitations. In this work, we focus exclusively on inducing reasoning behavior in small LLMs. While results are promising, it remains to be seen how well cache steering generalizes to larger models, different domains, or tasks beyond reasoning (e.g., instruction following or safety alignment). Extending cache steering to these broader use cases requires further empirical and theoretical investigation."
        },
        {
            "title": "7 Conclusions",
            "content": "We introduced cache steering, novel technique for guiding the reasoning behavior of language models via one-shot modifications to the key-value (KV) cache. By leveraging contrastive examples and GPT-4o-generated CoT reasoning traces, our method induces structured reasoning in small models without requiring fine-tuning, prompt engineering, or continuous intervention during generation. Unlike traditional activation steering, cache steering modifies only the stored representations of past tokens, leading to improved stability, compatibility with standard inference pipelines, and efficiency at inference time. Through experiments on GSM8K, ARC-Challenge, CSQA, and PIQA, we demonstrate that cache steering can reliably induce reasoning behavior and, in some cases, improve task accuracy. In addition to inducing reasoning, we show that cache steering enables control over the style of reasoning. While cache steering is not yet fully plug-and-play, and its effectiveness depends on coefficients and steering vector selection, it offers promising and lightweight mechanism for behavior control in language models. We hope this work opens up new directions for controllable generation, reasoning style transfer, and low-cost distillation techniques in the key-value space."
        },
        {
            "title": "Acknowledgments",
            "content": "This research was supported in part by the European Laboratory for Learning and Intelligent Systems (ELLIS). The ELLIS Amsterdam Unit sponsored research visit to the Fundamental AI Lab (FunAI) at the University of Technology Nuremberg (UTN). We also acknowledge the University of Amsterdam (UvA) for providing the computational resources that made this research possible."
        },
        {
            "title": "References",
            "content": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, et al. Smollm2: When smol goes bigdata-centric training of small language model. arXiv preprint arXiv:2502.02737, 2025. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. David Chanin. steering-vectors: Steering vectors for transformer language models in pytorch/huggingface. https://pypi.org/project/steering-vectors/, 2025. Accessed: 2025-05-13. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Patrick Queiroz Da Silva, Hari Sethuraman, Dheeraj Rajagopal, Hannaneh Hajishirzi, and Sachin Kumar. Steering off course: Reliability challenges in steering language models. arXiv preprint arXiv:2504.04635, 2025. Weilong Dong, Xinwei Wu, Renren Jin, Shaoyang Xu, and Deyi Xiong. Contrans: Weak-to-strong alignment engineering via concept transplantation. arXiv preprint arXiv:2405.13578, 2024. 11 Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Rogov, Elena Tutubalina, and Ivan Oseledets. have covered all the bases here: Interpreting reasoning features in large language models via sparse autoencoders. arXiv preprint arXiv:2503.18878, 2025. Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. arXiv preprint arXiv:2307.06945, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Bruce Lee, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Erik Miehling, Pierre Dognin, Manish Nagireddy, and Amit Dhurandhar. Programming refusal with conditional activation steering. arXiv preprint arXiv:2409.05907, 2024. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, and Minyi Guo. Clusterkv: Manipulating llm kv cache in semantic space for recallable compression. arXiv preprint arXiv:2412.03213, 2024a. Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, and Arthur Szlam. Deliberation in latent space via differentiable cache augmentation. arXiv preprint arXiv:2412.17747, 2024b. Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens. Advances in Neural Information Processing Systems, 36:1932719352, 2023. Nina Panickssery, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Matt Turner. Steering llama 2 via contrastive activation addition. arXiv preprint arXiv:2312.06681, 2023. Joris Postmus and Steven Abreu. Steering large language models using conceptors: Improving addition-based activation engineering. arXiv preprint arXiv:2410.16314, 2024. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. Alessandro Stolfo, Vidhisha Balachandran, Safoora Yousefi, Eric Horvitz, and Besmira Nushi. Improving instruction-following in language models through activation steering. arXiv preprint arXiv:2410.12877, 2024. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018. Qwen Team. Qwen2 technical report. arXiv preprint arXiv:2412.15115, 2024. Eric Todd, Millicent Li, Arnab Sen Sharma, Aaron Mueller, Byron Wallace, and David Bau. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023. Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan Vazquez, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. URL https://arxiv. org/abs/2308.10248, 2024. Tianlong Wang, Xianfeng Jiao, Yinghao Zhu, Zhongzhi Chen, Yifan He, Xu Chu, Junyi Gao, Yasha Wang, and Liantao Ma. Adaptive activation steering: tuning-free llm truthfulness improvement method for diverse hallucinations categories. In Proceedings of the ACM on Web Conference 2025, pages 25622578, 2025. Weixuan Wang, Jingyuan Yang, and Wei Peng. Semantics-adaptive activation intervention for llms via dynamic steering vectors. arXiv preprint arXiv:2410.12299, 2024. Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting. URL https://arxiv. org/abs/2402.10200, 2024. Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. Advances in Neural Information Processing Systems, 36:7476474786, 2023. Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, and Mario Fritz. Taxonomy, opportunities, and challenges of representation engineering for large language models. arXiv preprint arXiv:2502.19649, 2025. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Fangcong Yin, Xi Ye, and Greg Durrett. Lofit: Localized fine-tuning on llm representations. Advances in Neural Information Processing Systems, 37:94749506, 2024. Jason Zhang and Scott Viteri. Uncovering latent chain of thought vectors in large language models. In Workshop on Neural Network Weights as New Data Modality, 2025."
        },
        {
            "title": "A Qualitative examples",
            "content": "Here we show qualitative examples from different model-dataset pairs. We try to include cases when: 1) both baseline and steering answers are correct; 2) only the steering answer is correct; 3) only the baseline answer is correct. A.1 Llama-3.2-1B-Instruct on ARC-c Llama-3.2-1B-Instruct on ARC-c: both baseline and steering are correct Prompt: scientist investigated how well plants native to Maryland would grow in desert conditions. The scientist placed equal amounts of desert soil into 10 identical containers. Each container held two of the same kind of plant. The scientist investigated total of 10 different kinds of plants. The scientist placed all of the containers into greenhouse adjusted to produce hot, desert-like conditions. The scientist gave the plants equal, but small amounts of water for the next five weeks. Most of the plants did not survive after the five week investigation. Which statement best explains why 16 of the plants did not survive? Choices: A: Plants do not grow in desert conditions. B: Few plants survive severe changes to their environment. C: Plants native to Maryland are not good for scientific studies. D: Plants native to Maryland are only able to survive in Maryland. Baseline Response: The correct answer is B: Few plants survive severe changes to their environment. The statement that best explains why 16 of the plants did not survive is that they were placed in desert-like environment with hot, desert-like conditions, and then given small amounts of water for five weeks. This severe change to their environment would likely be stressful for the plants, making it difficult for them to survive. The fact that most of the plants did not survive suggests that the conditions were too harsh for them, rather than that the plants themselves are not good for scientific studies or that they are only able to survive in Maryland. So the correct choice is B. Steered Response: To understand why 16 of the plants did not survive, we need to consider the factors that affect plant growth in desert conditions. Firstly, plants native to Maryland are not adapted to desert conditions. They are typically found in temperate climates with moderate temperatures and adequate rainfall. Desert plants, on the other hand, are adapted to hot, dry conditions with limited water availability. Secondly, the desert soil used in the containers was not suitable for the plants. Desert soil is often high in salt and lacks essential nutrients, which can be detrimental to plant growth. Thirdly, the amount of water provided to the plants was small, which can be stressful for most plants. Even if the plants were adapted to desert conditions, the lack of water would still be significant challenge. Now, lets consider the options: A: Plants do not grow in desert conditions. This statement is incorrect because plants can grow in desert conditions, but they require specific adaptations and conditions to survive. B: Few plants survive severe changes to their environment. This statement is partially correct, as the severe changes to the environment (hot, dry conditions and limited water) would indeed be challenging for most plants. However, it does not explain why 16 of the plants did not survive. C: Plants native to Maryland are not good for scientific studies. This statement is incorrect because plants native to Maryland can be valuable subjects for scientific studies, especially when they are adapted to specific conditions. D: Plants native to Maryland are only able to survive in Maryland. This statement is incorrect because plants native to Maryland can survive in other environments, including desert conditions, as long as they are adapted to those conditions. The correct answer is B: Few plants survive severe changes to their environment. This is because the combination of hot, dry conditions and limited water provided to the plants would be challenging for most plants to survive. So the correct choice is B. Llama-3.2-1B-Instruct on ARC-c: incorrect baseline, correct steering Prompt: college campus has so few parking spaces that cars are often lined up waiting to park during class hours. Which of the following describes how the college could best solve the need for more parking spaces using the universal systems model? Choices: A: The college could build an additional parking lot, gather parking data, and then receive input from 14 community meetings. B: The college could limit access to parking lots during busy hours, hire security officers to help restrict parking, and then gather parking data. C: The college could gather parking data, receive input from community meetings, build an additional parking lot, and then gather more parking data. D: The college could hire security officers to help restrict parking, gather parking data, receive input from community meetings, and then gather more data. Baseline Response: The correct answer is B: The college could limit access to parking lots during busy hours, hire security officers to help restrict parking, and then gather parking data. This option describes how the college could best solve the need for more parking spaces using the universal systems model. The universal systems model is framework for understanding and managing complex systems, and in this case, it involves: Identifying the problem (limited parking spaces) Gathering data on the current situation (parking data) Receiving input from stakeholders (community meetings) Taking action to address the problem (limiting access to parking lots during busy hours and hiring security officers) This approach is good example of how the universal systems model can be applied to solve complex problem like limited parking spaces. So the correct choice is B. Steered Response: To solve the need for more parking spaces on college campus, the college should consider the universal systems model, which involves several key components. First, the college should gather parking data. This involves collecting information about the number of cars parked, the types of vehicles, and the time of day when parking is most popular. This data will help the college understand the current parking situation and identify areas where additional parking spaces are needed. Next, the college should receive input from community meetings. This involves engaging with students, faculty, and staff to gather their opinions and concerns about parking. By listening to their feedback, the college can better understand the needs and priorities of the community. Building an additional parking lot is crucial step in addressing the parking shortage. However, it should be done in conjunction with the gathering of parking data and the community meetings. This ensures that the new parking lot is designed and constructed to meet the needs of the community, and that it is accessible and convenient for everyone. Finally, the college should gather more parking data to evaluate the effectiveness of the new parking lot and make adjustments as needed. This will help the college to refine its parking management strategies and ensure that the parking lot is meeting its intended purpose. In summary, the correct answer is the one that involves gathering parking data, receiving input from community meetings, building an additional parking lot, and then gathering more data to evaluate its effectiveness. Choices A, B, and do not accurately describe the universal systems model. Choice involves gathering data and then implementing solution, but it does not involve community meetings. Choice involves restricting access to parking during busy hours, but it does not involve gathering data or receiving input from the community. Choice involves gathering data, but it does not involve building an additional parking lot or receiving input from the community. So the correct choice is C. Llama-3.2-1B-Instruct on ARC-c: correct baseline, incorrect steering Prompt: In 1903 Mary Anderson invented the first windshield wiper. How did this invention most likely help people? Choices: A: It helped people use less gas. B: It kept people from driving too fast. C: It made cars easier for people to buy. D: It made cars safer to drive in bad weather. Baseline Response: The correct answer is D: It made cars safer to drive in bad weather. Mary Andersons windshield wiper invention was designed to help people see while driving, especially in rainy or snowy conditions. By keeping the windshield clear of rain and snow, drivers could see the road ahead more clearly, reducing the risk of accidents. This was particularly important in the early days of automobile travel, when roads were often poorly maintained and visibility was limited. Andersons 15 invention helped to improve road safety and reduce the number of accidents on the roads. So the correct choice is D. Steered Response: To understand how Mary Andersons windshield wiper invention helped people, we need to consider the context and purpose of the invention. Mary Andersons windshield wiper was designed to improve visibility while driving, especially in rainy or foggy conditions. The primary goal of the invention was to reduce the amount of time spent stopping and starting the vehicle, which was significant challenge in early automobiles. In the early 1900s, driving was hazardous activity, especially in poor weather conditions. Drivers had to constantly stop and start the engine, which was time-consuming and often resulted in accidents. The windshield wiper helped to reduce this time by allowing drivers to clear the windshield of rain and debris more quickly. Therefore, the correct answer is that Mary Andersons windshield wiper invention most likely helped people by reducing the time spent stopping and starting the vehicle, which was significant challenge in early automobiles. So the correct choice is B."
        },
        {
            "title": "B Contrastive data construction",
            "content": "All prompts are formatted using the chat template from the models tokenizer configuration. This ensures consistency with how the models are typically used during chat-style inference. The example of single contrastive pair with 1 in-context example: Figure 5: The example of single contrastive pair with 1 in-context example. The positive example (left) includes both the reasoning trace and the final answer. The negative example (right) includes only the final answer. To ensure that in-context examples are semantically similar to the target question, we embed all training questions using the all-MiniLM-L6-v2 sentence embedding model (Reimers and Gurevych, 2019) and retrieve the top-n most similar examples based on cosine similarity."
        },
        {
            "title": "C Answer extraction",
            "content": "We explored using an LLM to extract answer labels from model outputs, which is common practice in recent activation steering studies (Panickssery et al., 2023; Wang et al., 2023; Wehner et al., 2025). However, due to computational constraints, we were unable to use sufficiently large model to ensure high-quality extraction. In particular, smaller judges often rely on their own knowledge to infer the correct answer, rather than faithfully extracting it from the generated output. This compromises the reliability of evaluation in cases where steering affects reasoning without necessarily correcting the final answer. To minimize the amount of false positive answers, we opted for rule-based pipeline that is transparent, fast to run at scale, and robust enough for comparative analysis in our setting. GSM8K. For GSM8K, we extract the final numeric answer using digit-based pattern match. Specifically, we select the last number mentioned in the models output. This approach has been used 16 in prior work (Wang et al., 2023; Wang and Zhou, 2024). While this method introduces both false positives (e.g., trailing numbers in explanations) and false negatives (e.g., answers embedded in text), these effects tend to cancel out over large-scale evaluation. Multiple-choice tasks. For multiple-choice datasets, we require more structured extraction due to the open-ended nature of the model outputs. Therefore, we adopt the approach from Wang and Zhou (2024) and augment it further with more rigorous extraction process. We develop multi-stage extraction pipeline designed to recover answer labels with high precision and robustness. For all experiments, we append the string \"So the correct choice is\" to the end of the prompt and allow the model to generate 10 additional tokens. The output is then processed in the following stages: 1. Regex-based label extraction: We apply sequence of 12 regular expressions to identify explicit label mentions (e.g., A, option C, (B)) both immediately following the answer prefix and in the whole answer string as fallback. Each pattern is designed to handle common formats seen across models and datasets. The full list of regex patterns can be found in the code. 2. String match fallback: If no regex match is found, we scan the span after \"So the correct choice is\" for an exact string match with any of the raw answer choices. To avoid false positive matches, we only accept matches when exactly one choice matches unambiguously. We also filter out cases containing negation cues (e.g., not, incorrect, wrong) to exclude completions like So the correct answer is not B. 3. Invalid and multi-label detection: We detect multi-answer completions (e.g., both and C) or noncommittal outputs (none of the above) and label them as [incorrect]. Any remaining answers that do not match valid choice are marked as [invalid]. 4. Constrained decoding for fallback resolution: For all completions marked as [invalid], we discard the answer span after \"So the correct choice is\" and repeat decoding with constraints: we re-append \"So the correct choice is\" to the prompt and sample single token, masking the logits to allow only valid label tokens. This final step guarantees that valid label is recovered for every input. Despite this multi-stage approach, errors in label extraction are still common. In particular, semantically correct answers may be incorrectly marked due to minor phrasing differences or ambiguous generation formats. Additionally, the constrained decoding stage forces the model to generate valid label even if the generated text is not semantically meaningful, which was especially common in activation steering experiments. Therefore, to mitigate this, we discard the results if the accuracy before the constrained decoding stage is significantly lower than after."
        },
        {
            "title": "D Activation steering evalutaion",
            "content": "To make fair comparison of activation steering to cache steering, we chose similar setting: greedy decoding, 200 contrastive samples, each with 5 few-shot examples. Similarly to cache steering implementation, the vectors were extracted from the last token position, aggregated with Differencein-Means method. The vectors were applied continuously to each new token during decoding. All these choices adhere to the current best practices in activation steering research (Wehner et al., 2025). To extract and apply the steering vector, we used the steering-vectors Python library (Chanin, 2025) that implements the most popular activation steering method CAA (Panickssery et al., 2023). First we performed grid search on subset of data over [0.5, 1, 3] and middle layers of each model: for SmolLM2-360M-Instruct [13, 14, 15, 16, 17, 18, 19], for Llama-3.2-1B-Instruct [6, 7, 8, 9, 10], for Llama-3.2-3B-Instruct [13, 14, 15], for Llama-3.1-8B-Instruct [15, 16, 17], for Phi-4-mini-instruct [15, 16, 17], for Qwen2-0.5B-Instruct [11, 12, 13]. The selected hyperparameters were inspired by the numbers reported in Turner et al. (2024). Then, activation steering with the best parameters was evaluated on the full test set to obtain the final results."
        },
        {
            "title": "E Sampling parameters",
            "content": "For sampling experiments, we used the parameters specified in the generation config of the model. If the generation config was not available, we used temperature: 0.6, top_p: 0.9, top_k: 50."
        },
        {
            "title": "F Aligning cache position",
            "content": "Through extensive experimentation, we discovered that for most datasets, cache steering is most effective if the vectors are applied to the same token from which the vectors were extracted. In case of an instruction-tuned model, such token can be the last token of the generation prompt (e.g., the newline character after \"assistant\"). Therefore, when we extract the steering vectors from such token, the steering effect is most pronounced if we apply the vector to the same token. Even though the actual positions of the extraction and application tokens in the corresponding sequences are different, in both sequences the tokens play similar role. We think of these tokens as information aggregation tokens. In cases when the desired application token is last in the sequence, we append special token to the prompt in order to be able to apply the intervention to the correct position. In cases when cache steering is applied to any other position, this procedure is not needed."
        },
        {
            "title": "G List of hyperparameters",
            "content": "Table 5: Hyperparameters used for the experiments for each task-dataset pair. Task Model Contrastive Samples In-context Examples ck cv GSM8K CSQA ARC-c PIQA HuggingFaceTB/SmolLM2-360M-Instruct meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B-Instruct Qwen/Qwen2-0.5B-Instruct meta-llama/Llama-3.1-8B-Instruct microsoft/Phi-4-mini-instruct meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B-Instruct HuggingFaceTB/SmolLM2-360M-Instruct Qwen/Qwen2-0.5B-Instruct meta-llama/Llama-3.1-8B-Instruct microsoft/Phi-4-mini-instruct meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.2-1B-Instruct HuggingFaceTB/SmolLM2-360M-Instruct Qwen/Qwen2-0.5B-Instruct meta-llama/Llama-3.1-8B-Instruct microsoft/Phi-4-mini-instruct meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B-Instruct HuggingFaceTB/SmolLM2-360M-Instruct Qwen/Qwen2-0.5B-Instruct meta-llama/Llama-3.1-8B-Instruct microsoft/Phi-4-mini-instruct 200 100 100 100 100 100 100 300 400 200 100 100 400 200 300 400 200 200 200 200 200 200 200 5 5 5 5 5 5 5 10 12 10 5 5 10 10 10 10 10 10 10 10 10 10 10 10 0 0 0 0 0 0 0 0 0.2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 3 2 1 10 4 6 4 10 10 6 6 6 10 6 6 10 6 8"
        },
        {
            "title": "H Models Used",
            "content": "We evaluate our method using multiple open-source language models from different model families. Below, we list their Hugging Face model hub URLs. 18 SmolLM2-360M-Instruct URL: https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct License: Apache 2.0 LLaMA-3.2-1B-Instruct URL: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct License: Llama 3.2 Community License LLaMA-3.2-3B-Instruct URL: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct License: Llama 3.2 Community License Qwen2-0.5B-Instruct URL: https://huggingface.co/Qwen/Qwen2-0.5B-Instruct License: Apache 2.0 Phi-4-mini-instruct URL: https://huggingface.co/microsoft/Phi-4-mini-instruct License: MIT Llama-3.1-8B-Instruct URL: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct License: Llama 3.1 Community License"
        },
        {
            "title": "I Reproducibility",
            "content": "I.1 Experiments reproducibility To ensure consistency and facilitate detailed analysis, we implemented several reproducibility safeguards throughout our experimental pipeline. Sample tracking via UUIDs. At test time, we assign each input unique identifier (UUID) derived deterministically from hash of the input text. This allows us to track and compare individual examples across experiments with different settings (e.g., sampling, steering variants, decoding strategies), and ensures the integrity of input data over time. The UUID makes it easy to locate the same question across logs, qualitative outputs, and evaluation reports, and is sensitive to minor changes in the question itself. Deterministic runs. For all runs involving stochastic generation (e.g., sampling-based decoding), we set the random seed at the beginning of each run to guarantee reproducibility. In all experiments, we used the chat template predefined by the model to Llama chat template tokenize the input text. However, we noticed that specifically in the Llama models, the current date is added to the system prompt, making it impossible to fully reproduce the results. Therefore, we modify the chat template of the Llama models to exclude the current date from the system prompt."
        },
        {
            "title": "J Generation of reasoning data",
            "content": "To generate reasoning data, we used GPT-4o model via OpenAIs Chat Completions API. We used the following instruction prompt to elicit detailed CoT-style responses: You are given question and corresponding answer to that question. Your task is to think step by step and provide the reasoning steps to get the answer. Separate each reasoning step with <reasoning> </reasoning> tags. The question: {question}. The correct answer: {answer}. The obtained reasoning steps were further parsed with regular expressions."
        },
        {
            "title": "K Hardware specifications",
            "content": "All experiments were run on the internal cluster (not in the cloud). These are the specifications of the hardware: 1 NVIDIA H100 GPU, 94GiB of memory 16 AMD 4th GEN EPYC CPUs The time to run each experiment varied per model, dataset, whether it was baseline experiment, cache steering or activation steering experiment, the amount of training data used, etc. On average, single run for almost all model-dataset pairs took less than 1 hour to run, with the exception of Llama-3.2-3B model on PIQA dataset, which took under 2 hours, and activation steering experiments, which took under 6 hours per experiment. The full research project required more compute than the experiments reported in the paper since significant part of the project was experimentation and empirical analysis."
        },
        {
            "title": "L Software environment",
            "content": "Python 3.11.11 transformers: 4.49.0 torch: 2.5."
        },
        {
            "title": "M Computational overhead experiment details",
            "content": "To compare the computational efficiency of cache steering, activation steering, and the model without any intervention, we measure the per-token generation time under both methods using subset of 100 examples from the ARC-Challenge dataset. We conduct experiments using two different batch sizes: 1 (single-example inference) and 16 (batched inference), to reflect both interactive and throughput-oriented use cases. All runs are executed on the same hardware using greedy decoding. Timing is measured from the beginning of generation (post-prompt forward pass) to the completion of the final token. All results are averaged over three runs."
        },
        {
            "title": "N Sensitivity of activation steering to hyperparameters",
            "content": "Figure 6: Sensitivity of activation steering to hyperparameters on ARC-c dataset using Llama-3.2-1B. The results are obtained from the activation steering grid search described in D."
        },
        {
            "title": "O Length of generated outputs",
            "content": "Table 6: Cache steering consistently increases the length of generated outputs across tasks. We report the average number of generated tokens under three conditions: baseline decoding, CoT prompting, and cache steering. Results are shown for multiple model sizes across four reasoning benchmarks. The results indicate that cache steering leads to longer answers on average across all tasks and models, except for GSM8K. We hypothesize that the reason for that is that this dataset is classic benchmark for evaluation of reasoning methods, and all models are already trained on this dataset and generate CoT responses even without explicit instructions."
        },
        {
            "title": "ARC",
            "content": "CSQA GSM8K PIQA SmolLM2-360M-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Phi-4-mini-instruct SmolLM2-360M-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Phi-4-mini-instruct SmolLM2-360M-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Phi-4-mini-instruct SmolLM2-360M-Instruct Qwen2-0.5B-Instruct Llama-3.1-8B-Instruct Llama-3.2-1B-Instruct Llama-3.2-3B-Instruct Phi-4-mini-instruct 22 75 178 152 190 110 19 53 104 46 105 62 214 187 185 147 171 142 39 85 157 142 175 117 188 147 196 171 208 159 104 151 134 146 181 222 216 179 146 167 215 210 204 173 196 203 220 315 242 315 283 301 369 295 181 340 283 251 443 222 188 207 150 179 345 289 327 449 407"
        }
    ],
    "affiliations": [
        "CSAIL MIT",
        "FunAI Lab University of Technology Nuremberg",
        "VIS Lab University of Amsterdam"
    ]
}