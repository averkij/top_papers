{
    "paper_title": "Kinetics: Rethinking Test-Time Scaling Laws",
    "authors": [
        "Ranajoy Sadhukhan",
        "Zhuoming Chen",
        "Haizhong Zheng",
        "Yang Zhou",
        "Emma Strubell",
        "Beidi Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We rethink test-time scaling laws from a practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-$N$, long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals a new Kinetics Scaling Law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics Scaling Law suggests that test-time compute is more effective when used on models above a threshold than smaller ones. A key reason is that in TTS, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose a new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling because, unlike training, where parameter scaling saturates, test-time accuracy continues to improve through increased generation. The code is available at https://github.com/Infini-AI-Lab/Kinetics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 3 3 3 5 0 . 6 0 5 2 : r Kinetics: Rethinking Test-Time Scaling Laws Ranajoy Sadhukhan, Zhuoming Chen, Haizhong Zheng, Yang Zhou, Emma Strubell, Beidi Chen {rsadhukh,zhuominc,haizhonz,yangzho6,estrubel,beidic}@andrew.cmu.edu Carnegie Mellon University We rethink test-time scaling laws from practical efficiency perspective, revealing that the effectiveness of smaller models is significantly overestimated. Prior work, grounded in compute-optimality, overlooks critical memory access bottlenecks introduced by inference-time strategies (e.g., Best-of-N , long CoTs). Our holistic analysis, spanning models from 0.6B to 32B parameters, reveals new Kinetics scaling law that better guides resource allocation by incorporating both computation and memory access costs. Kinetics suggests that test-time compute is more effective when used on models above parameter threshold than with smaller ones. key reason is that in test-time scaling, attention, rather than parameter count, emerges as the dominant cost factor. Motivated by this, we propose new scaling paradigm centered on sparse attention, which lowers per-token cost and enables longer generations and more parallel samples within the same resource budget. Empirically, we show that sparse attention models consistently outperform dense counterparts, achieving over 60 points gains in low-cost regimes and over 5 points gains in high-cost regimes for problem-solving accuracy on AIME, encompassing evaluations on state-of-the-art MoEs. These results suggest that sparse attention is essential for realizing the full potential of test-time scaling where, unlike training, accuracy has yet to saturate as function of computation, and continues to improve through increased generation. Github: https://github.com/Infini-AI-Lab/Kinetics Website: https://infini-ai-lab.github.io/Kinetics (a) Kinetics scaling law (b) Sparse Kinetics Figure 1 (a) Pareto Frontier for Qwen3 series on AIME24 (Long-CoTs). Previous test-time scaling laws (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024) focus solely on compute optimality, neglecting the significant bottleneck of memory access in long-sequence generation. This leads to suboptimal resource utilization. By incorporating memory access, Kinetics reduces resource demands by up to 3 to achieve the same accuracy. (b) Top-K Sparse attention on AIME24 (Best-of-N ). Inspired by Kinetics, we show that sparse attention models scale significantly better than dense models, achieving over 50-point improvements in AIME24 in the low-cost regime and consistently outperforming dense models in the high-cost regime, in addition to substantial efficiency gains. B200 seconds represent the amount of work performed by single NVIDIA B200 at full utilization per second. indicates equal contributions. 1 (a) Attention Cost Dominates (b) Generated Tokens vs. Cost (c) Block top-k Attention Figure 2 (a) Inference cost is dominated by attention (i.e. Softmax( qkT )v), which is 10-1000 more than model parameter computation (linear modules, including those in MLPs and WQ, WK , WV , WO), sparse attention fundamentally mitigates this bottleneck. (b) Under the same resource constraints, sparse attention can generate significantly more tokens than dense models, which has been demonstrated to enhance the effectiveness of test-time scaling. (c) Simple block sparse attention yields substantial gainsimproving accuracy by 45 points in the low-cost regime, and achieving equivalent accuracy while using 8.58 fewer resources."
        },
        {
            "title": "Introduction",
            "content": "Test-time scaling (TTS) has recently emerged as powerful strategy (e.g., Best-of-N , Long-CoT (Wei et al., 2022)) for enhancing the reasoning capabilities of large language models (LLMs) (Guo et al., 2025; Jaech et al., 2024; Qwen-Team, 2025), particularly in scenarios where agents interact with complex environments, e.g., writing code, browsing the web (Nakano et al., 2021; Yao et al., 2023a) or reinforcement learning (RL) with LLMs-in-the-loop (Huang et al., 2022; Driess et al., 2023; Chen et al., 2025a). These capabilities, however, introduce substantial inference-time costs, making it critical to understand performance scaling in this new paradigm. Existing scaling law studies (Brown et al., 2024; Snell et al., 2024; Wu et al., 2024) primarily focus on floating-point operations (FLOPs) while ignoring memory access costs, which are often the dominant factor in determining wall-clock latency in TTS regimes. As shown in Figure 1a, this gap can lead to sub-optimal deployment decisions. In Section 3, we introduce the Kinetics scaling law for TTS, derived from cost model that explicitly incorporates memory access costs. This new perspective reveals markedly different conclusions about Paretooptimal strategies for allocating test-time compute (Figure 1a). Specifically, we find that: (1) prior scaling laws consistently overestimate the effectiveness of small models enhanced with inference-time strategies; and (2) computational resources are best spent first on increasing model size up to critical threshold (empirically, around 14B parameters) before investing in test-time strategies, such as Best-of-N sampling or Long-CoTs. Guided by Kinetics, our approach yields up to 3 reduction in resource demands to reach the same accuracy on NVIDIA B200 hardware. Our roofline analysis (Yuan et al., 2024a) across suite of state-of-the-art reasoning models reveals that the shift in optimal test-time compute strategies arises because test-time strategies (e.g., Best-of-N , LongCoTs) disproportionately increase attention costs rather than parameter costs (Figure 2a). Our Iso-cost analysis (Kumar et al., 2024) shows that the quadratic growth of attention with generation length, combined with the disproportionate scaling of KV memory relative to model parameters, drives preference for scaling up model size over generations. This imbalance is further exacerbated by MoE architectures (Shazeer et al., 2017; Du et al., 2021; Fedus et al., 2022; AI@Meta, 2025; Dai et al., 2024; Jiang et al., 2024), which reduce active parameter count without alleviating attention overhead. Building on the above analyses, in Section 4 we introduce new scaling paradigm, centered on sparse attention, which fundamentally reshapes the scaling law and significantly enhances the scalability of TTS (Figures 1b and 2b). According to our Sparse Kinetics scaling law, computational resources are best allocated to test-time strategies rather than reducing sparsity. As more computing is invested at test time, higher sparsity becomes increasingly critical to fully leverage the benefits of these strategies. Guided by this principle, we find that sparse attention increases problem-solving rates by up to 60 points in the low-cost regime and over 5 points in the high-cost regime on AIME (Figure 1b) and LiveCodeBench, encompassing state-of-the-art MoEs. 2 In Section 5, we demonstrate the practicality of Sparse Kinetics using simple block-sparse attention mechanism (Figure 2c). This approach achieves up to 3.2 33.3 speedup on H200 GPUs. While sparsity has traditionally been employed either for regularization in small models (Tibshirani, 1996; Molchanov et al., 2017) or to reduce computation in over-parameterized networks (Mishra et al., 2021; Chen et al., 2021; Hoefler et al., 2021; Dao et al., 2021; Frantar and Alistarh, 2023; Liu et al., 2023), our work introduces fundamentally different perspective: sparsity as central enabler of efficient and scalable test-time compute. In contrast to pretraining, where scaling is exhibiting diminishing returns (Sutskever, 2024), TTS continues to benefit from increased token generation and more optimized inference paths. Our work highlights the importance of considering hardware concerns alongside model architecture in order to develop practical understanding of scaling laws. We hope this study will guide and encourage future co-design of model architectures, test-time strategies, and hardware systems to fully unlock the next wave of LLM scaling."
        },
        {
            "title": "2 Related Work and Problem Settings",
            "content": "In this section, we first review several lines of related work relevant to Kinetics. Then we introduce cost model accounting for computation and memory access, followed by roofline analysis uncovering key departure from traditional scaling laws. Finally, we outline the experimental setup used in the subsequent analysis. Notation is summarized in Table 1. Scaling Laws. Prior work (Kaplan et al., 2020; Hoffmann et al., 2022; Kumar et al., 2024) has extensively examined the scaling laws of pretraining, exploring the trade-off between model size and the number of training tokens under fixed FLOPs budget. More recently, studies such as (Snell et al., 2024; Wu et al., 2024; Brown et al., 2024; Beeching et al.) have extended this analysis to test time scaling, with focus on compute-optimality. While these works offer strong theoretical foundation, they largely overlook the critical bottleneck posed by memory access in current inference systems. Test-Time Scaling. Recent LLMs such as DeepSeek-R1 (Guo et al., 2025), OpenAI-o1/o3 (Jaech et al., 2024), and QwQ (Qwen-Team, 2025) generate extended CoT reasoning (Wei et al., 2022) to solve complex problems, including those from AIME (MAA, 2024, 2025). Parallel search through repeated sampling (Brown et al., 2024), majority voting (self-consistency) (Wang et al., 2022), and reward-model (Wu et al., 2024; Feng et al., 2023; Snell et al., 2024) (e.g., Best-of-N , weighted voting, tree search) aims to improve reasoning accuracy. Strategies such as (Fu et al., 2024; Arora and Zanette; NovaSky-Team, 2025) and hybrid models (Lieber et al., 2024; Paliotta et al., 2025; Wang et al., 2025) have been proposed to reduce the cost of test-time scaling."
        },
        {
            "title": "Note",
            "content": "Advanced test-time strategies shift evaluation from token-centric metrics (e.g., perplexity, latency) to task-level throughputthe number of tasks completed per unit time. This shift is especially relevant for reasoning tasks, where intermediate steps may vary widely depending on the strategy, yet the ultimate utility hinges almost entirely on the correctness of the final output. In contrast, traditional tasks like chat completions focus on token-level quality and throughput. Sparse Attention. significant line of prior work has focused on overcoming the quadratic computational bottleneck of attention mechanisms during LLM training by leveraging the natural sparsity of attention matrices (Child et al., 2019; Kitaev et al., 2020; Daras et al., 2020; Zaheer et al., 2020; Beltagy et al., 2020; Yuan et al., 2025). More recently, sparse attention has experienced resurgence in the context of LLM inference, where methods such as (Zhang et al., 2023; Xiao et al., 2024; Tang et al., 2024; Liu et al., 2024b; Chen et al., 2024; Hu et al., 2025) restrict the memory access of the key-value (KV) cache during generation while maintaining strong performance. These advances form strong and steady foundation for our exploration of new test-time scaling paradigm. Extended related work is discussed in Appendix E."
        },
        {
            "title": "Cost Model",
            "content": "We first calculate the inference cost for the cases where the batch size is 1, and then extend to more general case in TTS. Finally, we propose our cost model using equivalent FLOPs. We assume the model weight and 3 Table 1 Notation Used throughout the Paper."
        },
        {
            "title": "Symbol Description",
            "content": "Lout Task (set) T, N, NT Model C, CTTS() Cost function n, nT B, BT Algorithm A"
        },
        {
            "title": "Lin",
            "content": "# Gen tokens Reasoning trials Max # tokens KV budget Prompt length KV size / token Parameters GQA ratio KV cache are stored and calculated using the same precision. Computation. As discussed in (Brown et al., 2024), the computation of transformer architecture layer consists of two parts: linear modules and self-attention, which is given by: Ccomp = 2P Lout (cid:124) (cid:123)(cid:122) (cid:125) model parameters computation (MLP and WQ,WK ,WV ,WO) + r(2Lin + Lout)LoutD (cid:123)(cid:122) (cid:125) self-attention (cid:124) Softmax(qkT / d)v Memory Access. Memory access also consists of two parts, model parameters and KV cache: Cmem = 2P Lout (cid:124) (cid:123)(cid:122) (cid:125) model parameter access (MLP and WQ,WK ,WV ,WO) + 2LinLoutD + L2 (cid:124) (cid:123)(cid:122) KV cache (prompt + decoding) Softmax(qkT / d)v outD (cid:125) In real serving scenarios, large batch size will be used (DeepSeekAI, 2025) with growing GPU VRAM (Tirumala and Wong, 2024) and model parallelism (Pope et al., 2023). The access to the model parameters will be amortized across requests in batch; Figure 3 shows parameter access time is negligible when the batch size is large. Thus, we only consider the second term (i.e., KV cache loading) in our cost function. Furthermore, in the cases that we have reasoning trials, the prompt cache access (Juravsky et al., 2024; Zheng et al., 2024) is also shared across these trials. Thus, Ccomp(N ) = 2P Lout + 2rN LinLoutD + rN L2 Cmem(N ) = 2LinLoutD + L2 outD outD (1) (2) eFLOPs. We propose eFLOPs (equivalent FLOPs) to capture both compute and memory access cost, eFLOPs = Ccomp + Cmem 1 (3) where is the arithmetic intensity of hardware, which reflects that modern accelerators usually have much larger computation capacity over memory bandwidth, and the gap is growing over the years (Sadhukhan et al., 2024). In this work, we use = 562.5 (unit: FLOPs / GB) from the NVIDIA B200 (Tirumala and Wong, 2024). Figure 3 Latency breakdown for different model sizes (Qwen3 series) and context lengths (batch size 4K). Combining Equations (1) to (3), we obtain the final cost model: CTTS = 2N Lout (cid:123)(cid:122) (cid:125) (cid:124) linear modules computation + 2rN LinDLout + rN DL2 + 2ILinDLout + IN DL (4) (cid:124) (cid:123)(cid:122) self-attention computation out (cid:125) (cid:124) (cid:123)(cid:122) KV access out (cid:125) 1Max cost model max(Ccomp, Cmem I) also works here and favor our claims more since most of the time Cmem dominates the cost. We choose to use an additive cost model because Ccomp mainly comes from linear layers while Cmem mainly comes from the self-attention layer. The parallelization of these components during decoding remains an active area of research (Zhu et al., 2024). We discuss this max cost model in Appendix A.1. 4 where P, r, are hyper-parameters determined by the model . In MoE models, stands for the number of active parameters rather than total parameters. Analysis. Our key insight is attention-related cost dominates in long CoTs. We show this by estimating the ratio of attention-related cost to parameter-related cost Φ: Φ = 2rLinD + (rD + ID)Lout 2P As shown in Figure 2a, in the regime of long CoTs, where the generation length exceeds 4096 tokens, the cost of attention surpasses that of model parameters by factor of 10-1000. While multi-head latent attention (MLA; Liu et al., 2024a) reduces KV memory access by constant factor (similar to in GQA), it is insufficient for achieving true scalability due to several limitations: (1) MLA does not reduce attention computation; (2) the gap between FLOPs and memory bandwidth is expected to widen in the future; and (3) emerging fine-grained MoEs (AI@Meta, 2025; Dai et al., 2024; Snowflake-Team, 2024) drastically reduce FLOPs in linear layers by factor of 10-20, further increasing the relative cost of attention. Under the context of Long-CoTs being widely adopted, we can safely assume generated length Lout Lin or at least proportional to Lin. Hence, the bottleneck of inference is shifted from linear term LoutP to the quadratic term L2 outD, motivating our Kinetics scaling law, akin to kinetic energy: Ek = 1 2 mv2. Experimental Setup. Tasks: we focus on three challenging reasoning benchmarks: AIME24 (MAA, 2024), AIME25 (MAA, 2025), math datasets spanning algebra, combinatorics, and geometry, and LiveCodeBench (Jain et al., 2024)2, which includes complex programming problems from recent coding competitions. Models: We evaluate performance across various model sizes of the Qwen3 (Yang et al., 2025) and DeepSeek-R1-DistilledQwen (Yang et al., 2024b; Guo et al., 2025) series. Test-time Strategies: To eliminate the confounding effects introduced by the specific implementations of test-time strategies, such as the quality of reward models, we adopt two representative yet straightforward approaches: Long-CoTs, practical and widely used method in state-of-the-art reasoning models, and the oracle Best-of-N (repeated sampling (Brown et al., 2024)), which measures the solving rate for verifiable problems and suggests an upper bound via TTS. Hardware: We use the specifications of NVIDIA B200 as hardware reference to study the latest serving scenarios. Experiments details are presented in Appendix D."
        },
        {
            "title": "3 Rethinking Test-time Scaling Laws",
            "content": "In Section 3.1, we first introduce Kinetics, derived from empirical investigations across the Qwen3 model series. Then, we explore the underlying reasons for the divergence between Kinetics and prior scaling laws through an Iso-Cost analysis in Section 3.2."
        },
        {
            "title": "3.1 Kinetics",
            "content": "In this section, We study the scaling behavior of the Qwen3 (Yang et al., 2024a,b) considering the following problem: For each fixed maximum inference budget, eFLOPs per question, what is the Pareto frontier of achievable accuracy across different LLM configurations? With the refined cost model in Section 2, we first formulate the objective of the test-time scaling law, focusing on the tradeoff between model size and the number of generated tokens. Kinetics (for dense models). Given problem instance and total inference budget C, our goal is to explore the optimal tradeoff between two key factors: the choice of language model , and the number of reasoning trials or the maximum generation length n. More precisely, (N, n), = arg max(N,n),M Acc(N, n, ; ) s.t. CTTS(N, n, ; ) (5) Let Acc(N, n, ; ) denote the problem-solving rate of model on task , using reasoning trials, each with maximum reasoning length of n.3 2For LiveCodeBench, we sample 50 problems from the v5 subset (24 hard, 16 medium, 10 easy). 3For fairness, we do not schedule resources across tasks, but consider resource upper bound for all the tasks. 5 Figure 4 AIME24 Pareto Frontier (Long-CoTs). Evaluations of Qwen3 series models. By controlling the maximum allowed generation lengths, we control the incurred inference cost in eFLOPs (ab for our scaling law) or FLOPs (cd for previous scaling law) and measure the accuracy (Pass@1) in AIME24. The optimal model is marked with different colors in (ac). The optimal generation length is presented in (bd). (a) Accuracy (eFLOPs) (b) Accuracy (FLOPs) (c) Optimal Models Figure 5 AIME24 Pareto Frontier (Best-of-N ). We control the incurred inference cost in eFLOPs (a) or FLOPs (b) and measure the solving rate (coverage) in AIME24 for various models by varying the maximum allowed number of reasoning trials. We use the curve envelopes to project the optimal models in (c). In the Long CoTs scenario, where NT = 1, we vary nT to evaluate the model performance under different costs. We present our results in Figure 4. Kinetics highlights two important findings compared to the previous scaling law, which focused on merely FLOPs: Efficiency of small models is overestimated. As shown in Figures 3 and 4 (ac), smaller models, despite having fewer parameters, are not as efficient as commonly assumed. For example, the 14B model outperforms both the 4B and 8B models even at low accuracy levels (e.g., below 40%), and the 0.6B model only lies on the Pareto frontier in regions where accuracy is negligible. In contrast, under previous scaling laws, models of all sizes span meaningful portion of the Pareto frontier. Extending CoTs is more effective than enlarging parameters only for models beyond critical scale (empirically, 14B). Kinetics reveals that under constrained compute budgets, allocating resources to model scaling yields greater returns than increasing CoT length. As illustrated in Figure 4 (bd), only the 14B and 32B models benefit from generating CoTs longer than 10K tokens; for smaller models (e.g., 1.7B and 4B), switching to larger model is more advantageous when Lout < 5K. This suggests that, in practice, most of the available compute should be devoted to increasing model size rather than lengthening generations (Figure 4 (d)). In contrast, previous scaling laws assumed that longer CoTs provided consistent benefits across all model sizes, recommending model scaling only after CoT performance gains had plateaued. In the Best-of-N setting, we fix the maximum number of generated tokens at nT , and vary the number of reasoning trials to evaluate the problem-solving rate (i.e., the probability that at least one trial produces correct answer). We have similar observations in Figures 5a to 5c. Under the previous scaling laws (Figure 5b), 6 (a) KV v.s. Parameters (b) Iso-eFLOPs (c) Iso-FLOPs Figure 6 Explanation of the New Scaling Law. Left: Analysis across four LLM families reveals consistent trend of disproportionately slower KV memory growth relative to model size. For the Qwen3 series in particular, doubling model parameters results in only 1.18 increase in KV cache size. Middle and Right: We compare the Iso-Cost landscapes under the proposed cost model (b) and the traditional model (c). the most cost-effective strategy to achieve high accuracy is to apply repeated sampling using smaller models. Kinetics (Figure 5a) reveals that deploying 14B model with fewer reasoning trials is more efficient. We also observe critical size of 14B. For models smaller than 14B, increasing compute is best allocated toward model scaling rather than additional trials. For models at or above 14B, however, further computation is more effectively spent on increasing the number of reasoning trials, up to diminishing returns. The above observations are consistent in DeepSeek-R1-Distilled-Qwen series, while the critical model size becomes 7B. Experiments on AIME25 and LiveCodeBench as well as the analysis of DeepSeek-R1-Distilled-Qwen are presented in Appendix B. 3.2 Iso-Cost Study We attribute the above divergence between Kinetics and previous scaling laws to two reasons. Disproportionation between KV memory size and model parameters . Smaller models tend to require significantly more KV cache relative to their parameter size. For example, Qwen3-0.6B demands 3.5GB of KV cache to store 32K tokens, despite the model itself occupying only 1.2GB. In contrast, Qwen3-32B uses just 8GB of KV cache for the same sequence length. Empirically, doubling model parameters results in only 1.18 increase in KV cache size. As shown in Figure 6a, this phenomenon is consistently observed across model families such as OPT (Zhang et al., 2022) (1.55), Qwen2.5 (Yang et al., 2024b) (1.46), and LLaMA3 (Grattafiori et al., 2024) (1.27). Shift from linear to quadratic cost model. Under this revised model, increasing generation length incurs substantially higher cost than scaling model size; consequently, the tradeoff between model capacity and token budget shifts meaningfully. For instance, under the linear LP model, the cost of generating 8K tokens with 14B model (which is usually insufficient to solve complex tasks) is treated as equivalent to generating 24K tokens with 4B model (sufficient to complete most tasks). However, under the L2D model, the same 14B@8K generation is only comparable in cost to 4B@9K generation. This tighter bound makes it much harder for smaller models to compensate for their limited capacity through extended generation alone. Thus, only if the gap in model capacities is small enough (e.g., 32B only improves the accuracy by 3% on AIME24 compared to 14B), the benefits of extending generation length might be more effective than directly enlarging model parameters. Figures 6b and 6c show an Iso-Cost analysis comparing two cost models. Under Kinetics, the cost grows quadratically with Lout, while the KV cache scales sub-linearly with model parameters . As result, when total budget is low, the Iso-eFLOPs contours tend to stretch horizontally, favoring larger model sizes over longer generation lengths. This implies that increasing model size is more efficient use of resources than generating longer outputs. In contrast, the traditional FLOPs-based model leads to steeply vertical contours, encouraging longer generation before increasing model size. 7 (a) Best-of-N Gen. (b) Best-of-N Budget (c) Optimal Model Selection Comparison Figure 7 (ab) Tradeoff Between Generated Tokens and KV Budget. We empirically investigate how to balance the tradeoff between generating more tokens and allocating larger KV cache budget, which may yield more accurate but potentially shorter outputs. Using Qwen3-8B as representative model, we fit curves to characterize this tradeoff. For Best-of-N , we find that for every doubling of the total compute cost, the optimal KV budget increases by factor of 1.18, while the total number of generated tokens increases by 1.74. When the KV budget is small, the computational cost is dominated by model parameter-related computation rather than KV cache access. We incorporate model-specific constant (212) into the fitted curve to account for this effect. (c) Optimal Model Selection with Sparse Attention. Compared to the scaling law for the dense models, small models (0.6B, 1.7B, 4B) are more effective with sparse attention. In other words, they occupy more space in the Pareto Frontier (Figure 8a)."
        },
        {
            "title": "4 Test-time Scaling with Sparse Attention",
            "content": "Based on our findings in Section 3, we propose new scaling paradigm centered on sparse attention. We begin by presenting simple approach for identifying oracle resource allocation in sparse attention models, which we use to plot the Pareto frontier in Section 4.1. We then analyze the resulting changes in the scaling law and show that sparse attention models with massive tokens generated at test time, no matter sequentially via Long-CoTs or in parallel via Best-of-N , can lead to significantly higher problem-solving rates in Section 4.2."
        },
        {
            "title": "4.1 Oracle Resource Allocation with Sparse Attention Models",
            "content": "Problem statement. Let denote the corresponding sparsity algorithms (e.g., top-k, block top-k and local. Our goal is to explore the optimal tradeoff among three factors: model , KV budget B, and number of trials, and the maximum generation length (N, n). Specifically, (N, n), M, = arg max(N,n),M,B Acc(N, n, B, A, ; ) s.t. CTTS(N, n, B, A, ; ) (6) The cost function CTTS differs from the one in Equation (4) as it incorporates sparse attention mechanisms (which reduces the quadratic term L2D back to linear term LBD). This modified cost model is discussed in detail in Appendix A.2. Oracle resource allocation: We present method to obtain the optimal schedule between generation parameters (N, n) and the KV budget for each task, establishing an upper bound on achievable performance and enabling analysis of the core tradeoff between TTS strategies and sparsity. We begin by solving the subproblem for each individual task : max Acc(NT , nT , BT , A, ; ) s.t. CTTS(NT , nT , BT , A, ; ) (7) Empirically, we discretize the search space. For instance, in Best-of-N , we discretize the space of and by producing search grid: = {N0, N1, . . . , Ni} {B0, B1, . . . , Bj} For each pair (Na, Bb) G, we compute the corresponding cost CT,(a,b) and accuracy AccT,(a,b). We use (NT , BT ) which maximizes the accuracy under the cost constraint as an approximation for Equation (7). By combining the optimal configurations (NT , BT ) for all tasks , we obtain solution to the overall problem in Equation (6). Similar discretizations also applies for Long-CoTs. Thus we find the oracle resource allocation. We present how we obtain the oracle resource allocation in detail in Appendix D.2. 8 (a) Best-of-N Scaling (b) Best-of-N 32B (c) Best-of-N 30B-A3B (d) Long-CoTs Scaling (e) Long-CoTs 32B (f ) Long-CoTs 30B-A3B Figure 8 Sparse Attention Boosts Test-Time Scaling (AIME24). In (a)(d), we show that sparse attention models significantly improve the cost-accuracy trade-off under both inference strategies, ultimately achieving higher problem-solving rates at lower computational budgets. In (b)(e), we analyze individual model performance (32B) and observe that sparse attention provides notable gains. In low-cost regimes, it can enhance problem-solving rates by 5060 percentage points. Even in high-cost regimes, sparse models maintain an advantage of around 5 points, while reaching these performance levels much earlier. In (c)(f ), we show consistent conclusions for MoE models (Qwen3-30B-A3B). We use an oracle algorithm =top-k here to present an upper bound of sparse attention. For reference, workload of 105 Tera-eFLOPs corresponds to approximately 22 seconds of full utilization on single B200."
        },
        {
            "title": "4.2 Sparse Kinetics",
            "content": "Sparse attention fundamentally reshapes Kinetics and significantly enhances the scalability of TTS. We show Sparse Kinetics with an oracle algorithm =top-k to demonstrate the extraordinary potential of sparse attention. We present three important findings below. Sparse attention significantly enhances problem-solving performance. As shown in Figures 8a, 8b, 8d and 8e, compared to dense baselines, for both of the inference strategies and models of various sizes, sparse attention models improve problem-solving rates by up to 60 points in the low-cost regime and over 5 points in the high-cost regime. From an efficiency perspective, dense models require over 10 more eFLOPs to match the same solving rate. These findings underscore sparse attention as key enabler for unlocking the full benefits of test-time scaling. Sparse attention becomes increasingly valuable in high-cost scenarios. We investigate the tradeoff between KV budget and generation tokens. For Best-of-N , we analyze how the optimal KV budget and the number of generated tokens scale with cost across reasoning trials. As shown in Figures 7a and 7b, Our analysis reveals consistent trend: allocating additional compute toward generating more tokens is generally more effective than expanding the KV cache. In Best-of-N frontier, doubling the cost leads to only 1.18 increase in KV budget, compared to 1.74 increase in total generated tokens. Sparse attention reshapes Kinetics. As shown in Figure 7c, applying sparse attention significantly improves the efficiency of smaller models (0.6B, 1.7B, 4B), allowing them to re-emerge on the Pareto frontier across broader range. Sparse attention reduces attention cost from quadratic cost term (L2D) to linear one (LBD), making it negligible or comparable when compared to the cost of computing with model parameters (LP ). Further results for AIME25 and LiveCodeBench are presented in Appendix C, where we also ablate the performance of sparse attention on tasks with different difficulties. (a) Best-of-N Scaling (block top-k) (b) Long-CoTs Scaling (block top-k) (c) Sparse Algorithm Comparison Figure 9 Block top-k attention. In (a) and (b), we illustrate the optimality of block top-k sparse attention in terms of TTS on AIME24 dataset. Although upper bounded by the oracle top-k attention performance, block top-k achieves good trade-off between effectiveness and tractability. Although easy to implement, the performance of local attention is poor (c). Discussion: MoE models. The emerging MoEs reduce the computation cost by factor of 16 (Yang et al., 2025) to 128 (AI@Meta, 2025), further exacerbating the bottleneck in attention. We present the advantages of sparse scaling on Qwen3-30B-A3B in Figures 8c and 8f."
        },
        {
            "title": "5 Experimental Validation",
            "content": "In this section, we demonstrate the practicality of Sparse Kinetics through block top-k attention. We first show the scaling of block top-k attention, which is even comparable to the oracle top-k attention. Then we report empirical improvements in task throughput (number of tasks performed per unit time) using our block-sparse implementation. In addition, we conduct ablation studies with alternative sparsification strategies, such as local attention, to highlight the importance of the KV selection mechanism."
        },
        {
            "title": "5.1 Block Top-k Attention",
            "content": "While top-k attention offers attractive theoretical scaling, it is computationally intractable in practice. Instead, we adopt block topk attention for two key reasons. First, it exploits temporal locality in attention patterns (Sun et al., 2024b) to retrieve semantically related key-value (KV) blocks, reaching high accuracy. Second, its localized retrieval is hardware-efficient and integrates seamlessly with paged attention (Kwon et al., 2023), enabling high-throughput decoding. Block top-k attention is proved efficiently implementable in massive prior work (Tang et al., 2024; Sun et al., 2024a; Xu et al., 2025; Zaheer et al., 2020; Yuan et al., 2025). In practice, we compute representative vector for each KV block by averaging its key vectors, and use these to score the relevance of blocks to each query. Importance scores are shared across query heads within group, following the Grouped Query Attention (GQA) scheme. The definition of block top-k attention is introduced in detail in Appendix D.3. Figure 10 Throughput improvement with block top-k attention. Performance and Scaling. First, as shown in Figures 9a and 9b, block top-k attention demonstrates comparable scaling to the oracle top-k attention, improving accuracy by 45 points in the low-cost regime and achieving equivalent accuracy while using 8.58 fewer resources compared to dense attention. More accuracy evaluations across various benchmarks (including with MoE models) are presented in Appendix C. Second, we 10 compare block top-k with local attention in Figure 9c. Although local attention is more efficient due to its static sparsity pattern, it performs significantly worse. Its poor test-time scaling prevents it from outperforming dense attention except in very low-accuracy regimes. 5."
        },
        {
            "title": "Implementation and Empirical Results",
            "content": "Implementation. To demonstrate the practical efficiency gain of sparse attention, we build our attention backend on Flashinfer (Ye et al., 2025) and torch compile4. Alongside the paged KV cache, we introduce an auxiliary data structure to store block-level average key vectors. The KV block size is chosen such that the memory load from the block-average vectors and the selected top-k KV blocks remains balanced. This design enables sub-quadratic KV loading cost as the number of reasoning tokens increases. Rather than constructing full end-to-end serving system, we estimate the overall model execution time using per-layer latency and throughput measurements (He and Zhai, 2024). Results. We illustrate the benefit of block top-k attention across different model sizes on H200 nodes (8 GPUs per node) with batch size and context length of (4096, 16384) and (2048, 32768). Here we assume uniform workload of tasks with similar context lengths and generation lengths. As shown in Figure 10, block top-k attention substantially improves inference throughput, particularly for smaller models. For instance, the Qwen3-0.6B model achieves 23.6 33.3 increase in throughput. This improvement reflects the growing inefficiency of dense attention at longer contexts, which disproportionately affects smaller models. The substantial improvement in throughput highlights the potential for corresponding gains in task-level throughput, when appropriately co-designed with inference systems and test-time strategies. We leave this direction for future work."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "This work introduces the Kinetics Scaling Law based on the insight that attention costs rather than parameter counts are the dominant factor in TTS. We demonstrate that sparse attention fundamentally reshapes the scaling landscape, enabling longer generations and significantly higher accuracy. We envision the Kinetics as foundational tool for guiding end-to-end design across LLM serving, agent frameworks, and reinforcement learning environments. Sparse Kinetics may signal new paradigm, enabling continued progress even beyond pretraining plateaus. While our analysis centers on NVIDIA GPUs, the underlying principle that scaling memory bandwidth is more challenging and costly than scaling FLOPs applies broadly across hardware platforms. Ultimately, this study highlights the need for co-designing model architectures, test-time inference techniques, and hardware infrastructure as critical step toward enabling the next wave of scalable LLM deployment. Limitations, Future Scope, and Broader Impact Limitations. Our experiments primarily focus on Qwen3 (Yang et al., 2025) and DeepSeek-R1-DistilledQwen (Guo et al., 2025), two state-of-the-art pretrained reasoning model series, evaluated from the inference perspective. However, the effects of training and post-training strategies are not fully explored and may influence the performance gaps and robustness to sparse attention mechanisms. In addition, our cost analysis assumes cloud-based serving environment, where computational resources are typically sufficient and large batch sizes are feasible. In contrast, local deployment scenarios, such as those using ollama,5 often face limited VRAM where access to model parameters can dominate inference costs. Smaller models may be more appropriate in such settings, and our findings may not fully extend to these use cases. Future Scope. Our sparse scaling law offers valuable insights for enriching the applications of sparse attention algorithms and the design space of test-time scaling strategies. On one hand, except for top-k, currently we only discuss simple variant, i.e., block top-k, and have already demonstrated strong scalability. There exist more advanced sparse attention algorithms (Tang et al., 2024; Chen et al., 2024; Yuan et al., 2025; Lin et al., 4https://docs.pytorch.org/tutorials/intermediate/torch compile tutorial .html 5https://github.com/ollama/ollama (a) gen length vs Nopt correlation (top-k attention) (b) gen length vs Nopt correlation (block top-k attention) Figure 11 Correlation between Generation Length and Number of Trials. Longer generations correlate strongly with the optimal number of trials (Nopt), serving as proxy for problem difficulty. (a) shows this trend for top-k and (b) block top-k attention on the AIME24 dataset using the Qwen3-8B model. 2025) which may push test-time scaling to much higher boundary. On the other hand, test-time scaling algorithms are proposed to adaptively allocate computation to tasks, or even to tokens (Arora and Zanette; Mohtashami et al., 2023; Ma et al., 2025b,a). Extending them towards to new resource allocation problems in sparse attention is critical to reach the limit of Sparse Kinetics. For instance, since generation length strongly correlates with the optimal number of trials under sparse attention (as shown in Figure 11), it can be used as dynamic signal to adjust the number of trials and KV budget. Moreover, sparse attention drastically reduces inference cost, enabling more reasoning trials and longer generations. This unlocks greater flexibility in configuring TTS strategies within fixed resource budget. Broader Impact. This work aims to contribute to the understanding of efficiency and scalability challenges in the test-time scaling era, spanning model architecture, system-level implementation, and hardware design. We highlight the central role of sparsity in addressing these challenges. In addition, by shifting the focus from token-level metrics to task-level throughput, we open up broader opportunities for algorithmsystem co-design and emphasize the real-world utility of generative AI for society. Our study is algorithmic in nature and does not target specific applications. While large language models can be misused in harmful ways, this work does not introduce new capabilities or risks beyond those already present in existing systems. Test-time scaling can consume substantial amount of energy, raising concerns about the environmental sustainability of widespread deployment. By promoting sparse attention, our work hopes to help to reduce the carbon footprint and energy consumption of inference systems and support the broader goal of sustainable AI."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Yong Wu, Xinyu Yang and Harry Dong for providing us constructive feedback on our paper and computing resources of NVIDIA. This work was partially supported by Google Research Award, Amazon Research Award, Intel, Li Auto, Moffett AI, and CMU CyLab Seed funding. This material is also based upon work supported by the National Science Foundation under Grant No. 2326610. Any opinions, findings, and conclusions or recommendations expressed are those of the authors and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "title": "References",
            "content": "AI@Meta. Llama 4 model card. 2025. https://github.com/meta-llama/llama-models/blob/main/models/llama4/ MODEL CARD.md. Daman Arora and Andrea Zanette. Training language models to reason efficiently, 2025. URL https://arxiv. org/abs/2502.04463. Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models. https://huggingface. co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Ruisi Cai, Yuandong Tian, Zhangyang Wang, and Beidi Chen. Lococo: Dropping in convolutions for long context compression. arXiv preprint arXiv:2406.05317, 2024. Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher Re. Scatterbrain: Unifying sparse and low-rank attention. Advances in Neural Information Processing Systems, 34:1741317426, 2021. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, and Philipp Krahenbuhl. Reinforcement learning for long-horizon interactive llm agents. arXiv preprint arXiv:2502.01600, 2025a. Mouxiang Chen, Binyuan Hui, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Jianling Sun, Junyang Lin, and Zhongxin Liu. Parallel scaling law for language models. arXiv preprint arXiv:2505.10475, 2025b. https://arxiv.org/abs/2505.10475. Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, et al. Magicpig: Lsh sampling for efficient llm generation. arXiv preprint arXiv:2410.16179, 2024. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR, abs/2307.08691, 2023. doi: 10.48550/ARXIV.2307.08691. https://doi.org/10.48550/arXiv.2307.08691. Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations (ICLR), 2021. https://arxiv.org/abs/2112.00029. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros Dimakis. Smyrf: Efficient attention using asymmetric clustering. arXiv preprint arXiv:2010.05315, 2020. DeepSeek-AI. Deepseek open infra index. 2025. https://github.com/deepseek-ai/open-infra-index/blob/main/ 202502OpenSourceWeek/day 6 one more thing deepseekV3R1 inference system overview.md. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems, 35:3031830332, 2022. 13 Danny Driess, Minh Nguyen, Fei Xia, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Dehao Chen, Yonghui Wu, and Jeff Dean. Glam: Efficient scaling of language models with mixture-of-experts. arXiv preprint arXiv:2112.06905, 2021. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(1):52325270, 2022. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pages 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, and Hao Zhang. Efficiently serving llm reasoning programs with certaindex. arXiv preprint arXiv:2412.20993, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. CoRR, abs/2312.00752, 2023. doi: 10.48550/ARXIV.2312.00752. https://doi.org/10.48550/arXiv.2312.00752. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. https://openreview.net/forum?id=uYLFoz1vlAC. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Jiaao He and Jidong Zhai. Fastdecode: High-throughput gpu-efficient llm serving using heterogeneous pipelines. arXiv preprint arXiv:2403.11421, 2024. Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Journal of Machine Learning Research, 22(241): 1124, 2021. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, and Amir Gholami. Kvquant: Towards 10 million context length llm inference with kv cache quantization. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 12701303. Curran Associates, Inc., 2024. https://proceedings.neurips.cc/ paper files/paper/2024/file/028fcbcf85435d39a40c4d61b42c99a4-Paper-Conference.pdf. Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, Xusheng Chen, Tao Xie, and Yizhou Shan. Efficient long-decoding inference with reasoning-aware attention sparsity. arXiv preprint arXiv:2502.11147, 2025. Wenlong Huang, Fei Fei, and Chelsea Finn. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Jordan Juravsky, Bradley Brown, Ryan Ehrlich, Daniel Fu, Christopher Re, and Azalia Mirhoseini. Hydragen: High-throughput llm inference with shared prefixes. arXiv preprint arXiv:2402.05099, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165. PMLR, 2020. http://proceedings.mlr.press/v119/katharopoulos20a.html. Nikita Kitaev, (cid:32)Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In The International Conference on Machine Learning (ICML), 2020. Tanishq Kumar, Zachary Ankner, Benjamin Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Re, and Aditi Raghunathan. Scaling laws for precision. arXiv preprint arXiv:2411.04330, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention, 2023. https://arxiv.org/abs/2309.06180. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 1927419286. PMLR, 2023. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation, 2024. https://arxiv.org/abs/ 2404.14469. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: hybrid transformer-mamba language model, 2024. https://arxiv.org/abs/2403.19887. Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, and Mingyu Gao. Twilight: Adaptive attention sparsity with hierarchical top-p pruning. arXiv preprint arXiv:2502.02770, 2025. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024a. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024a. Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, et al. Retrievalattention: Accelerating long-context llm inference via vector retrieval. arXiv preprint arXiv:2409.10516, 2024b. Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 2213722176. PMLR, 2023. Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024c. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025a. 15 Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025b. MAA. American invitational mathematics examination 2024, 2024. https:// artofproblemsolving.com/wiki/index.php/American Invitational Mathematics Examination?srsltid= AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf Xt. MAA. American invitational mathematics examination 2025, 2025. https:// artofproblemsolving.com/wiki/index.php/American Invitational Mathematics Examination?srsltid= AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf Xt. Pierre-Emmanuel Mazare, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Herve Jegou, and Matthijs Douze. Inference-time sparse attention with asymmetric indexing. arXiv preprint arXiv:2502.08246, 2025. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. Specinfer: Accelerating generative large language model serving with tree-based speculative inference and verification. arXiv preprint arXiv:2305.09781, 2023. Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. Amirkeivan Mohtashami, Matteo Pagliardini, and Martin Jaggi. Cotformer: chain-of-thought driven architecture with budget-adaptive computation cost at inference. arXiv preprint arXiv:2310.10845, 2023. Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural networks. In Proceedings of the 34th International Conference on Machine Learning, pages 24982507. PMLR, 2017. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Reiichiro Nakano, Jacob Hilton, Jeffrey Wu, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, and Edoardo Ponti. The sparse frontier: Sparse attention trade-offs in transformer llms. arXiv preprint arXiv:2504.17768, 2025. Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeleton-of-thought: Prompting llms for efficient parallel generation. arXiv preprint arXiv:2307.15337, 2023. NovaSky-Team. Sky-t1: Train your own o1 preview model within $450. https://novasky-ai.github.io/posts/sky-t1, 2025. Accessed: 2025-01-09. Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Y. Li, Aviv Bick, J. Zico Kolter, Albert Gu, Francois Fleuret, and Tri Dao. Thinking slow, fast: Scaling inference compute with distilled reasoners, 2025. https://arxiv. org/abs/2502.20339. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5:606624, 2023. Qwen-Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. https://qwenlm.github.io/blog/ qwq-32b/. Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, and Beidi Chen. Magicdec: Breaking the latency-throughput tradeoff for long context generation with speculative decoding. arXiv preprint arXiv:2408.11049, 2024. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Re, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with single gpu. In International Conference on Machine Learning, pages 3109431116. PMLR, 2023. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Snowflake-Team. Snowflake arctic. https://github.com/Snowflake-Labs/snowflake-arctic, 2024. Apache 2.0 License. Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, and Beidi Chen. Shadowkv: Kv cache in shadows for high-throughput long-context llm inference. arXiv preprint arXiv:2410.21465, 2024a. Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding, 2024b. https://arxiv.org/abs/2404.11912. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290, 2024c. Ilya Sutskever. Sequence to sequence learning with neural networks: what decade. In NeurIPS (Keynote talk), 2024. https://www.youtube.com/watch?v=1yvBqasHLZs. Ruslan Svirschevski, Avner May, Zhuoming Chen, Beidi Chen, Zhihao Jia, and Max Ryabinin. Specexec: Massively parallel speculative decoding for interactive llm inference on consumer devices. Advances in Neural Information Processing Systems, 37:1634216368, 2024. Jihoon Tack, Jack Lanchantin, Jane Yu, Andrew Cohen, Ilia Kulikov, Janice Lan, Shibo Hao, Yuandong Tian, Jason Weston, and Xian Li. Llm pretraining with continuous concepts. arXiv preprint arXiv:2502.08524, 2025. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series (Methodological), 58(1):267288, 1996. Ajay Tirumala and Raymond Wong. Nvidia blackwell platform: Advancing generative ai and accelerated computing. In 2024 IEEE Hot Chips 36 Symposium (HCS), pages 133. IEEE Computer Society, 2024. Junxiong Wang, Wen-Ding Li, Daniele Paliotta, Daniel Ritter, Alexander M. Rush, and Tri Dao. M1: Towards scalable test-time compute with mamba reasoning models, 2025. https://arxiv.org/abs/2504.10449. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35: 2482424837, 2022. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2024. https://arxiv.org/abs/2309.17453. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang 17 Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. https://arxiv.org/abs/2505.09388. Shinn Yao, Jiaming Zhao, Dian Yu, et al. React: Synergizing reasoning and acting in language models. Advances in Neural Information Processing Systems (NeurIPS), 2023a. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36: 1180911822, 2023b. Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, and Luis Ceze. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. https://arxiv.org/abs/2501.01005. Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: distributed serving system for Transformer-Based generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22), pages 521538, Carlsbad, CA, July 2022. USENIX Association. ISBN 978-1-939133-28-1. https://www.usenix.org/conference/osdi22/presentation/yu. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen, Guangyu Sun, and Kurt Keutzer. Llm inference unveiled: Survey and roofline model insights, 2024a. https://arxiv.org/abs/2402.16363. Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024b. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. Amir Zandieh, Insu Han, Majid Daliri, and Amin Karbasi. Kdeformer: Accelerating transformers via kernel density estimation. In International Conference on Machine Learning, pages 4060540623. PMLR, 2023. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in Neural Information Processing Systems, 37:6255762583, 2024. Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, et al. Nanoflow: Towards optimal large language model serving throughput. arXiv preprint arXiv:2408.12757, 2024."
        },
        {
            "title": "Table of Contents",
            "content": "Cost Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 Max Cost Model v.s. Additive Cost Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Details about Sparse Attention Cost Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Kinetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Additional Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Additional Reasoning Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Sparse Kinetics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Additional Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Additional Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Experimental Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Estimate Cost, Optimal Generation Tokens, Accuracy and Solving Rate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Oracle Resource Allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Top-K Attention and Block Top-K Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Extended Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "A Cost Model",
            "content": "In this section, we delve into the cost models used in the Kinetics. We show empirically that adopting max cost model does not alter the scaling behavior and outline methods for calculating the cost of sparse attention models. Figure 12 AIME24 Pareto Frontier (Long-CoTs) with Max Cost Models. (a)(b) is the original plot with the additive cost model. (c)(d) is the corresponding plot using max cost models. Compared to the original plots, the overall trend is similar except that larger models span slightly broader region on the Pareto frontier. For example, the 14B model now consistently outperforms the 4B model with noticeable gap around accuracy 0.3 and maintains dominance thereafter. In contrast, under the additive cost model in Figure 4(a), the two models alternate in performance until accuracy exceeds 0.4. This suggests that, when evaluated using max cost model, larger models appear slightly more efficient relative to their performance under additive cost models. 19 Figure 13 AIME24 Pareto Frontier (Best-of-N ) with Max Cost Models. We re-plot Figure 5a using max cost models. The Pareto Frontier is very similar under different cost models. A.1 Max Cost Model v.s. Additive Cost Model Max cost model is widely used in performance modeling (Yuan et al., 2024b). It assumes that computation and memory operations can be fully overlapped with each other and only considers the bottleneck operation for cost measurement. Cmax-cost = max(Ccomp, Cmem I) where Ccomp denotes the compute cost, Cmem the memory cost per access, and the memory intensity. In this section, we analyze the Kinetics using the max cost model. For clarity, we refer to the cost model Ccomp + Cmem I, which is used in the main paper, as the additive cost model. We draw two conclusions from empirical results under the max cost model: Kinetics for dense models still holds. We re-plot Figure 4(a)(b) and Figure 5a under the measurement of max cost models in Figures 12 and 13. We find except that in Long-CoTs scenarios, large models become slightly more effective in low-cost regime (with accuracy0.3), the overall trends are very close to the plots with additive cost models. Sparse attention solves problems more cost-effectively. We re-plot Figures 8a and 8d in Figures 14a and 14b. Under the max cost models, in Long-CoTs, the accuracy and efficiency gaps increase from 47.5 points and 11.21 to 52.8 points and 15.71, respectively. In Best-of-N , the gaps widen from 65 points and 10.67 to 69.4 points and 19.64. These results indicate that under the max cost model, our claim that sparse attention can enhance problem-solving performance is strengthen. Compared to dense attention models, sparse attention models tend to have more balanced memory and compute costs. Thus omitting one of them via max cost model will favor sparse attention models. A.2 Details about Sparse Attention Cost Model Sparse attention models follow different cost functions due to the sparsification of KV memory access. In this paper, we focus on algorithms that impose uniform KV budget (denoted as B) per attention head for each decoded token. We consider Lin for the sake of simplicity. Under this setting, the cost model for sparse attention is given by: Csparse = 2N Lout + 2rN DBLout (cid:125) (cid:124) (cid:123)(cid:122) compute + 2IN DBLout (cid:123)(cid:122) (cid:125) memory (cid:124) . (8) In practical implementations, we must also account for the overhead associated with retrieving or searching KV memory, denoted as Csearch, which depends on the specific sparse attention algorithm A. For example, in block top-k selection, the search cost is: Csearch = 2N LinDLout + rN DL2 2Block-Size (cid:123)(cid:122) compute (cid:124) out + (cid:125) 2ILinDLout + IN DL2 2Block-Size (cid:123)(cid:122) memory (cid:124) out (cid:125) . (9) 20 (a) Long-CoTs (b) Best-of-N Figure 14 Sparse attention scales significantly better under max cost models. We re-plot Figures 8a and 8d using max cost models. Compared to the original plots, the performance and efficiency gaps between sparse attention models and dense models become more pronounced. In Long-CoTs, the accuracy and efficiency gaps increase from 47.5 points and 11.21 to 52.8 points and 15.71, respectively. In Best-of-N , the gaps widen from 65 points and 10.67 to 69.4 points and 19.64. In our work, we choose the Block-Size in such way that Csparse and Csearch are roughly balanced, so that the sparse attention cost increases sub-linearly with generation length. For local attention and oracle top-k attention, we assume no search overhead, i.e., Csearch = 0. Many sparse attention algorithms skip the first layer (Tang et al., 2024; Chen et al., 2024; Zhang et al., 2023), resulting in only minor increase in total cost. For the Qwen3 series, this additional overhead is bounded by 3.57% for the 0.6B model and by 1.56% for the 32B model."
        },
        {
            "title": "B Kinetics",
            "content": "In this section, we further verify Kinetics for dense models proposed in Section 3 with extended experimental results of different benchmarks and model series. B.1 Additional Benchmarks We evaluate on AIME25 in Figures 15 and 16a to 16c and LiveCodeBench6in Figures 17 and 18a to 18c (excluding the 0.6B model), following the setting described in Section 3. The empirical results support the Kinetics: across both benchmarks, the 0.6B and 1.7B models are consistently less effective, and the Pareto frontier is almost always dominated by the 14B models. B.2 Additional Reasoning Models In Figures 19 and 20a to 20c, we evaluate DeepSeek-R1 Distilled Qwen models (abbreviated as DS models) (Guo et al., 2025) on AIME24. The DeepSeek series models further demonstrate that previous scaling lawsthose based on FLOPssignificantly overestimate the effectiveness of the 1.5B model. As predicted by the Kinetics, increasing the number of generated tokens for the 1.5B model is less effective than scaling up the model size, such as using the 7B or larger variants. Interestingly, we observe shift in the emerging model size: unlike Qwen3, where the 14B model dominates, the 7B model becomes the dominant choice in the DeepSeek series. In Figures 19, 20a and 20c, the 7B model spans most of the Pareto frontier, and Figure 19 shows that 7B models with Long-CoTs are more efficient and effective than 14B models with short generations. We attribute this to an architectural outlier in the DeepSeek-R1 (Qwen2.5) model series. As shown in Table 2, the DeepSeek-R1 7B model is significantly more 6For LiveCodeBench dataset, we have sampled 50 examples from the v5 subset consisting 167 examples. Our subset comprises 24 hard, 16 medium and 10 easy examples respectively. 21 Figure 15 AIME25 Pareto Frontier (Long-CoTs). We conduct the same experiments as Figure 4. (a) Accuracy (eFLOPs) (b) Accuracy (FLOPs) (c) Optimal Models Figure 16 AIME25 Pareto Frontier (Best-of-N ). We conduct the same experiments as Figures 5a to 5c. Figure 17 LiveCodeBench Pareto Frontier (Long-CoTs). We conduct the same experiments as Figure 4. 22 (a) Accuracy (eFLOPs) (b) Accuracy (FLOPs) (c) Optimal Models Figure 18 LiveCodeBench Pareto Frontier (Best-of-N ). We conduct the same experiments as Figures 5a to 5c. Figure 19 AIME24 Pareto Frontier (Long-CoTs). We conduct the same experiments as Figure 4 on DeepSeek Distilled Qwen series. (a) Accuracy (eFLOPs) (b) Accuracy (FLOPs) (c) Optimal Models Figure 20 AIME24 Pareto Frontier (Best-of-N ). We conduct the same experiments as Figures 5a to 5c on DeepSeek Distilled Qwen series. 23 KV memory-efficient than the Qwen3-8B model. Unlike most model series illustrated in Figure 6a, where KV cache size typically grows sublinearly with respect to model parameters, DeepSeek-R1 shows deviation from this trend: the 14B model has approximately 3.4 more KV memory than the 7B model, while having only 2 more parameters. Table 2 KV memory Size for Qwen3 and DeepSeek-R1 Distilled models (per 32K tokens, unit: GB). Qwen3 Qwen3-1.7B Qwen3-8B Qwen3-14B Qwen3-32B"
        },
        {
            "title": "DeepSeek",
            "content": "3.5 DS-1.5B 0.875 4.5 DS-7B 1.75 6 DS-14B 8 DS-32B 8 This finding highlights the importance of concrete model architecture design, rather than focusing solely on the number of model parameters. Whether KV memory size is directly related to reasoning performance remains an open question, which we leave for future investigation."
        },
        {
            "title": "C Sparse Kinetics",
            "content": "We present additional results supporting the Sparse Kinetics across multiple tasks and demonstrate how these insights enable scalable test-time scaling with sparse attention. C.1 Additional Benchmarks Beyond AIME24, we evaluate our approach on LiveCodeBench (Jain et al., 2024) and AIME25 (MAA, 2025). LiveCodeBench features complex programming problems from recent coding contests, while AIME25 consists of challenging math problems. In both cases, sparse attentionparticularly oracle top-kconsistently outperforms dense attention. Block top-k attention, tractable alternative, closely matches the performance of the oracle. For LiveCodeBench, we sample 50 problems from the v5 subset (24 hard, 16 medium, 10 easy). As shown in Figure 21, oracle top-k attention can achieve 10 speedup in high-accuracy regimes and improves coverage by 4050% in low-cost regimes. Conversely, the tractable alternative, Block top-k yields 56 speedup and 3040% coverage gains. We further show how the benefits of sparse attention scale with problem difficulty (Figures 21g to 21i). Figure 22 confirms similar trends for AIME25, with substantial gains in both accuracy and efficiency under sparse attention. We present the evaluations of MoE models (Qwen3-30B-A3B) in Figures 23a and 23b, where we draw consistent conclusions on the scalability of sparse attention. C.2 Additional Analysis Fixing model (e.g., Qwen3-8B), we investigate the tradeoff between generating more tokens through Best-of-N and increasing the KV budget in Figures 24a to 24d. As the figures suggest, on AIME25, each doubling of total compute cost increases the optimal KV budget by 1.13, while generated tokens grow by 1.67; on LiveCodeBench, these factors are 1.14 and 1.89, respectively. We find that although the concrete numbers depend on the types of tasks, the overall results confirm our suggestions in the main paper that allocating compute toward generating more responses is generally more effective than expanding KV budget, highlighting the scalability of sparse attention."
        },
        {
            "title": "D Experimental Details",
            "content": "In this section, we explain the details about our experiments. 24 (a) Best-of-N Scaling Comparison (b) Best-of-N Top-K Sparse Scaling (c) Best-of-N Block Top-K Scaling (d) Long-CoTs Scaling Comparison (e) Long-CoTs Top-K Sparse Scaling (f ) Long-CoTs Block Top-K Scaling (g) Best-of-N Scaling (Easy) (h) Best-of-N Scaling (Medium) (i) Best-of-N Scaling (Hard) Figure 21 LiveCodeBench Sparse Scaling. We evaluate Sparse Kinetics for Qwen3-14B model using oracle top-k and block-top-k attention on the LiveCodeBench dataset. (a)(d) compare block-top-k and oracle top-k with dense scaling under Best-of-N and Long-CoTs TTS settings. (b)(e) show cost-accuracy trade-offs for top-k attention. (c)(f ) show trade-offs for block-top-k attention. (g)(h)(i) compare the oracle top-k scaling for easy, medium and hard difficulty questions. D.1 Estimate Cost, Optimal Generation Tokens, Accuracy and Solving Rate When empirically measuring cost, one major challenge is the difficulty of controlling the actual generation length. Although it is possible to set an upper bound on the number of generated tokens, there is no guarantee that the model will utilize the full budget. For instance, in our Best-of-N experiments, we set the maximum number of generated tokens to 32,768, yet the average generation length was only 14K16K tokens. Furthermore, it is important to model the relationship between actual inference cost and performance metrics, such as accuracy in Long-CoTs or solving rate in Best-of-N . Relying solely on the maximum allowed generation length to estimate cost can substantially underestimate the efficiency of models that solve problems with much shorter responsesan ability that may reflect higher capability. To address this challenge, we first sample independent reasoning traces r1, r2, . . . , rS from model on task , with the maximum allowed number of tokens set to n. We slightly generalize Equation (4) as: CTTS = 2N E[Lout] + 2rN LinDE[Lout] + rN DE[L2 out] + 2ILinDE[Lout] + IN DE[L out] = aE[Lout] + bE[L2 out] + c, (10) where a, b, and are constants determined by the model architecture and test-time strategies (e.g., the value of (a) Best-of-N Scaling Comparison (b) Best-of-N Top-K Sparse Scaling (c) Best-of-N Block Top-K Scaling (d) Long-CoTs Scaling Comparison (e) Long-CoTs Top-K Sparse Scaling (f ) Long-CoTs Block Top-K Scaling Figure 22 AIME25 Sparse Scaling. We evaluate Sparse Kinetics for Qwen3-14B model using oracle top-k and block-top-k attention on the AIME25 dataset. (a)(d) compare block-top-k and oracle top-k with dense scaling under Best-of-N and Long-CoTs settings. (b)(e) show cost-accuracy trade-offs for oracle top-k attention. (c)(f ) show trade-offs for block-top-k attention. n). The expectations are estimated from the sampled traces, whose distribution is influenced by the model , the token limit n, and the task . For Long-CoTs, we fix = 1 in Equation (10) and vary n. From the sampled traces, we estimate the accuracy (Pass@1), and compute the corresponding cost by substituting the empirical values of E[Lout] and E[L2 out] measured under each n. For Best-of-N , we fix = 32,768, and estimate the solving rate (Pass@K) following the methodology of Brown et al. (2024). The corresponding cost is then computed by substituting = into Equation (10). Similarly, we can estimate the cost for sparse attention models using Equations (8) and (9). Advanced control of generation lengths is an active area of research (Yang et al., 2025; Muennighoff et al., 2025; Ma et al., 2025a), but it is beyond the scope of this paper. Optimal Generation Tokens. To address the question: Given total cost budget C, what proportion should be allocated to generating longer responses, in contrast to enlarging model sizes or reducing attention sparsity?, we project the optimal number of generation tokens from the Pareto frontier (e.g., Figures 4 and 7b). Intuitively, each point on the frontier corresponds to specific model and cost, but does not directly specify generation length, since this can vary across tasks. Estimating the optimal generation length requires further analysis. It is important to note that we do not consider inter-request resource scheduling strategies, such as early stopping or dynamic reallocation across requests (Fu et al., 2024), since we aim to ensure fairness across all inputs. Instead, the cost constraint is interpreted as the maximum allowable cost per request (not the average), even if some requests achieve saturated accuracy below that threshold. Under this assumption, the effective cost at any point on the frontier is determined by the task that incurs the maximum cost. For previous scaling laws and Sparse Kinetics studies, where the generation costs are linear to generated tokens, the optimal generation tokens is calculated with maxT NT E[Lout]. For Kinetics, the (cid:112)E[L2 out] (we only analysis Long-CoTs). This adjustment optimal generation tokens is calculated with maxT accounts for the quadratic dependence of cost on output length, better measuring the cost allocated to generating longer responses. 26 (a) Long-CoTs Block Top-K MoE (b) Best-of-N Block Top-K MoE Figure 23 AIME24 MoE Block Top-K scaling. we analyze Qwen3-30B-A3B and observe that Block Top-K yields notable gains. In low-cost regimes, it can enhance problem-solving rates by 4253 percentage points. Even in high-cost regimes, sparse models maintain an advantage of around 8 points, while reaching these performance levels much earlier. These findings are consistent with other (0.632B) models. (a) AIME25 Gen. (b) AIME25 Budget (c) LiveCodeBench Gen. (d) LiveCodeBench Budget Figure 24 Tradeoff Between Generated Tokens and KV Budget. We characterize the tradeoff between increasing generation length and allocating larger KV cache budget using Qwen3-8B. For AIME25 ((a)(b)) and LiveCodeBench ((c)(d)), we identify the optimal KV budget and generated tokens (defined as number of reasoning trials times the average generated tokens per trial) to achieve the highest problem-solving rate under every cost constraint C. D.2 Oracle Resource Allocation We describe the procedure for identifying oracle resource allocations and establishing the Pareto frontier for sparse attention models in Algorithms 1 and 2, as supplement to Section 4.1. Given fixed cost constraint C, we perform grid search over key parameters: KV budgets and either reasoning trials or maximum generation lengths. Empirically, we sweep over KV budgets {32, 64, 128, 256, 512, 1024}; reasoning trials {1, 2, 4, 8, 16, 32} (with reduced upper limit for the 14B and 32B models to save computation time); and generation lengths {2k, 4k, 6k, 8k, 10k, 12k, 14k, 16k, 18k, 20k, 22k, 24k, 26k, 28k, 30k, 32k}. By varying the cost constraint in Algorithms 1 and 2, we obtain the performance of sparse attention models under optimal resource allocation, as shown in Figures 8a to 8f and 9a to 9c. D.3 Top-K Attention and Block Top-K Attention In this section, we explain the sparse attention algorithms discussed in the main paper, namely Top-K Attention and Block Top-K Attention. During the decoding phase of large language model (LLM), the self-attention mechanism computes weighted average of past values as follows: = Softmax (cid:19) (cid:18) qK = wV, R1d, K, Rnd, R1n, (11) where is the head dimension and is the context length. The key and value matrices are given by = [k1, k2, . . . , kn], = [v1, v2, . . . , vn], where each ki, vi R1d are cached from previous decoding steps. 27 4 5 7 8 9 10 11 13 14 4 5 6 8 9 10 11 12 14 Algorithm 1: Best-of-N oracle resource allocation under cost Data: Tasks , KV budgets {B1, . . . , Bj}, trial counts {N1, . . . , Ni}, cost limit Result: Average of maximum accuracy per task under cost 1 AccumBestAcc 0 Count 0; 2 for task in do 3 for KV budget Bb do Generate max{N1, .., Ni} responses using Bb for task ; for trial count Na do compute cost c(T ) b,a ; if c(T ) b,a then Compute accuracy Acc(T ) if Acc(T ) b,a > BestAcc then b,a = Pass@Na; BestAcc Acc(T ) b,a ; end if end if end for end for AccumBestAcc += BestAcc; Count += 1; 15 16 end for 17 AvgBestAcc = AccumBestAcc/Count; 18 return AvgBestAcc; Algorithm 2: Long-CoTs oracle resource allocation under cost Data: Tasks , KV budgets {B1, . . . , Bj}, gen. lengths {n1, . . . , ni}, samples S, cost limit Result: Average of maximum accuracy per task under cost 1 AccumBestAcc 0 Count 0; 2 for task in do 3 BestAcc 0; for gen. length na do for KV budget Bb do Generate responses using (Bb, na); compute cost c(T ) b,a ; if c(T ) b,a then Compute accuracy Acc(T ) if Acc(T ) b,a > BestAcc then BestAcc Acc(T ) b,a ; b,a = Pass@1; end if end if end for end for AccumBestAcc += BestAcc; Count += 1; 15 16 end for 17 AvgBestAcc = AccumBestAcc/Count; 18 return AvgBestAcc; 28 Top-K Attention. Top-K Attention is sparsification method where only the most relevant tokens (i.e., those with the highest attention scores) are selected to compute the output. Formally, instead of computing the full softmax, we define sparse attention weight vector: wi = (cid:40) exp(si) jIK (cid:80) exp(sj ) 0 if IK, otherwise, where si = qk , IK = TopKK(s), (12) Here, IK denotes the indices of the top attention scores si. By masking out the less important positions, this approach reduces the computational and memory cost of attention from O(n) to O(K), where n. Block Top-K. Block Top-K Attention is block-level sparse attention mechanism. Instead of selecting individual tokens based on attention scores, this method selects entire blocks of tokens, thereby reducing the number of attention computations. Specifically, assume the full sequence of keys is divided into = BLOCK SIZE: BLOCK SIZE consecutive blocks, each of size = [k1, . . . , kn] {K1, K2, . . . , Km}, Ki RBLOCK SIZEd For each block Ki, we first compute the average key vector: ki ="
        },
        {
            "title": "1\nBLOCK SIZE",
            "content": "BLOCK SIZE (cid:88) ki,j j=1 Next, we compute the attention score between the query and each blocks average key: si = qk d , for = 1, 2, . . . , We then select the top = BLOCK SIZE blocks based on the scores si, denoted by the index set JK = TopKK(s). Attention is computed only over the tokens within the selected blocks. The sparse attention weights are defined as: (cid:40) exp(si) jIK (cid:80) exp(sj ) wi = 0 otherwise if IK tokens in selected blocks, For both algorithms, is the KV budget. For GQA, we conduct an average pooling across all the query heads in group, ensuring that the total number of retrieved key-value vectors does not exceed the allocated KV budget."
        },
        {
            "title": "E Extended Related Work",
            "content": "Efficient Attention. Sparse attention (Kitaev et al., 2020; Zandieh et al., 2023; Chen et al., 2021, 2024; Zhang et al., 2023; Xiao et al., 2024; Yuan et al., 2025; Nawrot et al., 2025; Child et al., 2019; Li et al., 2024; Cai et al., 2024; Mazare et al., 2025) has been comprehensively studied to reduce the attention cost when processing long sequeces. In parallel, approaches like FlashAttention (Dao et al., 2022; Dao, 2023) accelerate attention by maximizing hardware efficiency. To address the quadratic complexity of standard attention, researchers have also explored linear attention architectures (Gu and Dao, 2023; Gu et al., 2022; Katharopoulos et al., 2020; Choromanski et al., 2020). Additionally, quantization and low-precision methods (Liu et al., 2024c; Hooper et al., 2024; Lin et al., 2024b) have been broadly applied for improving inference efficiency. Efficient Inference. Orca (Yu et al., 2022), vLLM (Kwon et al., 2023), and SGLang (Zheng et al., 2024) are widely adopted to enhance the efficiency of LLM serving. Our analysis builds on the practical designs and implementations of these systems. In parallel, speculative decoding (Leviathan et al., 2023; Chen et al., 2023; Miao et al., 2023; Sadhukhan et al., 2024) has been proposed to mitigate the memory-bandwidth bottleneck during LLM decoding. Additionally, model compression and offloading (Dettmers et al., 2022; Lin et al., 29 2024a; Svirschevski et al., 2024; Sheng et al., 2023; Frantar et al., 2022) techniques are playing crucial role in democratizing LLM deployment. Efficient Test-time Strategies. Optimizing reasoning models to generate fewer tokens has been shown to directly reduce inference-time cost (NovaSky-Team, 2025; Arora and Zanette; Ma et al., 2025b). Recent work such as CoCoNut (Hao et al., 2024) and CoCoMix (Tack et al., 2025) explores conducting reasoning in latent space, thereby reducing decoding time. Methods like ParScale (Chen et al., 2025b), Tree-of-Thoughts (Yao et al., 2023b), and Skeleton-of-Thoughts (Ning et al., 2023) aim to improve efficiency by enabling parallel reasoning. Architectural innovations such as CoTFormer (Mohtashami et al., 2023) further enhance efficiency by adaptively allocating computational resources across tokens. Efficient reward-model-based (Wu et al., 2024; Snell et al., 2024; Sun et al., 2024c) test-time scaling algorithms are also comprehensively studied."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University"
    ]
}