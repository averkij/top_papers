{
    "paper_title": "Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation",
    "authors": [
        "Huajie Tan",
        "Sixiang Chen",
        "Yijie Xu",
        "Zixiao Wang",
        "Yuheng Ji",
        "Cheng Chi",
        "Yaoxu Lyu",
        "Zhongxia Zhao",
        "Xiansheng Chen",
        "Peterson Co",
        "Shaoxuan Xie",
        "Guocai Yao",
        "Pengwei Wang",
        "Zhongyuan Wang",
        "Shanghang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 0 7 3 2 . 2 1 5 2 : r Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation Huajie Tan1,2,,, Sixiang Chen1,2,, Yijie Xu2,3,, Zixiao Wang1,2, Yuheng Ji2,4, Cheng Chi2, Yaoxu Lyu1,2, Zhongxia Zhao1,2, Xiansheng Chen2, Peterson Co1,2, Shaoxuan Xie2, Guocai Yao2, Pengwei Wang2,, Zhongyuan Wang2, Shanghang Zhang1,2,(cid:66) 1 State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University 2 Beijing Academy of Artificial Intelligence, 3 University of Sydney, 4 Institute of Automation, Chinese Academy of Sciences Project website: https://robo-dopamine.github.io Figure 1. Overview of Robo-Dopamine. Robo-Dopamine integrates large-scale reward modeling with robust policy learning algorithm. (Left) We construct General Reward Model (GRM) trained on large and diverse 35M-sample dataset spanning real-world, simulation, and human-centric videos with our Dopamine-Reward, step-aware fine-grained reward modeling method. This GRM learns to predict fine-grained, relative progress between states to accurately assess task progression. (Bottom Right) The pre-trained GRM is adapted to new tasks and provides dense reward signals to our Dopamine-RL framework. By using theoretically-sound Policy-Invariant Reward Shaping method, Dopamine-RL efficiently guides the policy during online interactions without misaligning the task objective. (Top Right) Our integrated approach establishes new state-of-the-art in reward accuracy (radar chart) and demonstrates high training efficiency, significantly boosting policy success rates in both simulation and the real world (bar chart)."
        },
        {
            "title": "Abstract",
            "content": "The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process ReEqual contribution. Project leaders. (cid:66) Corresponding author: shanghang@pku.edu.cn ward Models (PRMs) are promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, 1 novel reward modeling method for learning generalpurpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, robust policy learning framework that employs theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to new task in one-shot manner from single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: Robo-Dopamine. 1. Introduction While large-scale imitation learning (IL) has substantially advanced embodied intelligence [1, 4, 5, 15, 21, 25], its reliance on static, expert-curated datasets imposes fundamental limitations [7, 24, 43, 52, 64, 65], which exhibits sub-optimal sample efficiency, poor generalization to outof-distribution (OOD) scenarios, and also struggles to acquire precise and contact-rich manipulation skills [23, 63]. In contrast, reinforcement learning (RL) offers compelling alternative [11, 29, 33, 35, 56, 58, 60]. Through continuous environmental interaction, RL enables agents to transcend the limitations of static expert data, facilitating superior generalization and the mastery of high-precision tasks. However, the primary obstacle for applying RL to realworld robotics is the design of effective reward functions. Conventional approaches falter at two extremes: sparse, binary outcome rewards [11, 33, 35, 56, 60] make exploration in long-horizon, contact-rich tasks prohibitively difficult, while handcrafted dense rewards [16, 44, 54, 55] require significant domain expertise, limiting scalability and general applicability. This dichotomy has motivated the shift towards learning-based Process Reward Models (PRMs) [2, 8, 36, 37, 59]. Despite their promise, current PRMs are hindered by two fundamental limitations. First, the underlying reward models often exhibit critical deficiencies: their task-specific design [8, 18] inherently limits generalization; uniform reward distributions [37, 59] fail to capture the varying salience of crucial sub-steps; and reliance on single-view observations [2, 8, 36, 37, 59] fails in manipulation scenes where occlusions obscure fine-grained progress only visible from wrist-level views. Second, the reward shaping algorithms utilizing these dense signals are often theoretically flawed. Naively incorporating dense rewards can induce semantic trap [41] that misguides policy optimization by inadvertently altering the optimal policy, causing the agent to prioritize high proxy rewards from intermediate steps over the true task objective. To address these, we introduce Dopamine-Reward, novel dense reward modeling method for learning generalpurpose, step-aware process reward from multi-view inputs. Dopamine-Reward directly tackles the first limitation by leveraging two key techniques: Hop-based Step-wise General Reward Model (GRM) Construction for fine-grained, structural understanding of task progression from various viewpoints, and Multi-Perspective Reward Fusion via GRM to integrate bidirectional global reward and state-wise incremental reward for more precise reward estimation, which are made possible by meticulous annotation pipeline encompassing over 3,400 hours of data, 100K trajectories, and more than 350 daily tasks, offering broad coverage, finegrained labels, and well-balanced distributions across real robots, simulations, and egocentric human videos. Building upon GRM via Dopamine-Reward, we propose robust and unified policy learning framework DopamineRL to resolve the second limitation. Dopamine-RL employs theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage the dense rewards from our GRM for highly efficient selfimprovement without altering the underlying optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments on over 10 simulation and 8 real-world tasks demonstrate the superiority of our methods: (1) Stateof-the-art Reward Accuracy. The GRM achieves over 92.8% accuracy in progress assessment, with Value-Order Consistency (VOC) score of 0.953 on rank-correlation benchmarks, outperforming established baselines. (2) High Training Efficiency. After GRM is adapted to new task in one-shot manner from single expert demonstration, the resulting reward model enables Dopamine-RL to improve policy from near-zero to 95% success rate within approximately 150 online rollouts (about one hour of real robot interaction), with some tasks reaching 100% success rate. (3) Improved Generalization. By combining step-wise structural modeling, reward fusion, and multi-view perception for robust estimation under occlusion and fine-grained state changes, our GRM provides more reliable learning signals, enabling Dopamine-RL to generalize more effectively to unseen layouts, backgrounds, and object variations. An overview of Dopamine-Reward and Dopamine-RL, together with our empirical gains in reward accuracy and policy performance, is shown in Figure 1. In summary, our main contributions are as follows: We propose Dopamine-Reward, novel reward modeling method built around General Reward Model (GRM) that provides step-aware, fine-grained, and occlusion-resilient process rewards for precise robotic manipulation. We introduce Dopamine-RL, robust policy learning framework with theoretically grounded Policy-Invariant Reward Shaping scheme, which effectively exploits dense GRM rewards to accelerate policy optimization while avoiding the semantic trap. We curate large-scale, 3,400-hour multi-view dataset with over 100K trajectories and 350 daily manipulation tasks across real robots, simulation, and egocentric human videos, offering broad coverage, fine-grained annotations, and balanced supervision for training GRM. Extensive experiments validate our framework as follow: GRM achieves state-of-the-art reward assessment (over 92.8% progress accuracy and 0.953 Value-Order Consistency score), while on 10 simulated and 8 realworld tasks, Dopamine-RL, after one-shot GRM adaptation, raises policy success from near-zero to 95% within 150 online rollouts (about one hour of robot interaction), with some tasks reaching 100% success and generalizing to unseen layouts, backgrounds, and object variations. 2. Related Work Reinforcement Learning for Robotic Skills. Reinforcement Learning (RL) has demonstrated the potential to create policies that surpass the capabilities of imitation learning [11, 28, 29, 31, 33, 35, 56, 58, 60], enabling the discovery of novel and robust strategies for complex, contact-rich and dexterous tasks. Research in this area has progressed along two principal directions. The first direction investigates various policy optimization strategies, including offline RL [20, 34, 35], online RL [13, 29, 31, 32, 58], and mixed variants [11, 28, 38]. The second direction explores the efficient application of RL to different model architectures, such as small fully-connected models [35, 38], autoregressive models [11, 33, 56], and diffusion/flow-based models [31, 58, 62]. Independent of the chosen optimization algorithm or policy architecture, more fundamental bottleneck is to design reward function that is effective and scalable in real-world RL, which has driven broad shift away from manual reward engineering [16, 44, 50, 54, 55] toward learning-based reward models. Learned Process Reward Models. In real-world RL, common practice is to train success classifier as Outcome Reward Models (ORMs) to provide binary reward signal [11, 35], which renders exploration prohibitively difficult in complex, long-horizon tasks. To mitigate sparsity, recent work leverages visionlanguage models (VLMs) as Process Reward Models (PRMs) [2, 8, 36, 37, 59], providing denser feedback by, for example, predicting progress deltas between paired observations [59] or assigning perframe progress scores with respect to language goal [37]. While several methods introduce additional structure by decomposing tasks into steps [8, 18], some open challenges remain (Section 1). First, task-specific designs may limit generalization across diverse activities [8, 18]. Second, many approaches adopt nearly uniform reward allocations, which may underweight the salience of critical sub-steps [37, 59]. In addition, current PRMs typically rely on single-view observations [2, 8, 36, 37, 59], which can impede multi-perspective state estimation and increase sensitivity to occlusions. In contrast, our method, Dopamine-Reward, aims to address these issues by learning general-purpose, step-aware reward model that explicitly fuses multi-view inputs, enabling more robust and finegrained reward estimation. 3. Method Our approach is designed to address the core challenges in real-world robotic learning by introducing two synergistic components. First, we develop Dopamine-Reward that learns general-purpose, step-aware process reward from multi-view inputs (Section 3.1). Second, we propose Dopamine-RL, robust policy learning framework built upon Dopamine-Reward, resolving the theoretical flaws in conventional reward shaping (Section 3.2). 3.1. Dopamine-Reward Modeling Method 3.1.1. General Reward Model (GRM) Construction The core of our modeling method is to build the GRM, vision-language model designed to estimate precise task progress. To ensure the model generalizes across diverse embodiments and tasks, we construct large-scale dataset structured around relative temporal transitions. This section details the three-stage GRM training data construction pipeline, from raw video segmentation to scientifically rigorous hop-based labeling strategy as follows: Step-wise task progress discretization. We treat task progress itself as the supervision signal. Given raw multiview video trajectories, we first segment each expert trajectory into sub-tasks using human-annotated multi-view keyframes {K0, K1, . . . , KN }, where K0 is the initial observation, KN is the final success observation, and each Kj is set of synchronized multi-view keyframes. To obtain dense supervision, we perform adaptive sampling within each segment. For trajectory with frames per view, we set chunk size to determine the total number of sampled points and distribute them uniformly across the segments. The number of intermediate points within segment [Kj, Kj+1] is: = (cid:22) 1 (cid:22) (cid:23)(cid:23) . (1) 3 Figure 2. The overview of our method. Our framework is composed of two core components: (a) Dopamine-Reward Modeling Method and (b) Dopamine-RL Training Framework. (a) At the heart of our reward modeling is to build the General Reward Model (GRM), vision-language model that is prompted with task description and conditioned on multi-view images of initial, goal, before, and after states to predict relative progress or regress hop. To ensure stable and accurate signal, we employ Multi-Perspective Progress Fusion, which combines incremental, forward-anchored, and backward-anchored predictions into final fused reward. (b) The Dopamine-RL framework first adapts the pre-trained GRM to novel task using single demonstration (One-Shot GRM Adaptation). Subsequently, it uses theoretically-sound Policy-Invariant Reward Shaping method to convert the GRMs dense output into reward signal that accelerates learning without altering the optimal policy. This approach is universally compatible with wide range of RL algorithms. This yields sequence of states = {s0, s1, . . . , sM }, where each state si is set of synchronous multi-view visual observations. We then define the ground-truth global progress as Φ(si) = i/M . Hop-based relative progress normalization. naive choice is to regress the progress gain Φδ(sp, sq) = Φ(sq) Φ(sp) between two states, but iterating such predictions accumulates error and can push the reconstructed Φ(s) outside [0, 1]. Instead, we introduce hop-based formulation that learns relative-relative progress. Each training sample is tuple containing task description dtask, the initial state s0, the goal state sM , BEFORE state sp, an AFTER state sq, and hop label H(sp, sq) that normalizes the progress from sp to sq relative to the full task span from s0 to sM . Given Φ(sp) and Φ(sq), we define: Φ(sq) Φ(sp) Φ(sM ) Φ(sp) if (PROGRESS) Φ(sq) Φ(sp) Φ(sp) Φ(s0) if < (REGRESS). (2) This dynamically scales the supervision into [1, 1]: for forward progress, the change is normalized by the remaining distance to the goal; for regression, by the distance alH(sp, sq) = ready covered from the initial state. key theoretical advantage is that, when global progress is reconstructed by iteratively applying predicted hops, the resulting Φ(s) is guaranteed to remain strictly within [0, 1]. detailed proof is provided in Appendix A.1. Sampling strategy and data balancing. For each trajectory, we construct balanced set of hop-based training samples. Continuous hop values are first discretized into Nhop hop bins. The temporal distance between the BEFORE state sp and AFTER state sq in each pair is then chosen from Ndis distance bins within each hop bin, yielding in total Nhop Ndis non-trivial transitions. To reduce bias toward static segments, we further introduce an additional fraction α of samples explicitly labeled as zero-hop (i.e., H(sp, sq) = 0), constructed by selecting pairs (sp, sq) whose progress change is below small threshold ϵ: Φ(sq) Φ(sp) ϵ. (3) Applying this three-stage pipeline yields dataset of 35M samples from about 3,400 hours of video and over 100K trajectories (see Appendix B). We train the GRM on this corpus to estimate hop-based relative progress between arbitrary state pairs, conditioned on the initial state, goal state, and task description. 4 3.1.2. Multi-Perspective Progress Fusion from GRM To mitigate error accumulation and ensure consistent accuracy, we fuse predictions based on GRM from three complementary perspectives: incremental prediction, forwardanchored prediction, and backward-anchored prediction. Incremental Prediction first offers fine-grained, stepby-step assessment. Refer to Equation (2), the predicted global progress Φ (st) is recursively computed from the preceding states progress Φ(st1) and the predicted hop H(st1, st). Let Φ t1,t be the estimated progress hop: Φ t1,t = (cid:40) [1 Φ(st1)] Φ(st1) if 0 if < 0. The incremental progress is then calculated as follow: Φ (st) = Φ(st1) + Φ t1,t, (4) (5) where Φ (st) is be accumulated along the trajectory, initialized with Φ(s0) = 0. While this method excels at capturing local dynamics, it is susceptible to the accumulation of prediction errors over long trajectories. To counteract this drift, we introduce extra two global perspectives. ForwardAnchored Prediction provides stable global reference by anchoring to the initial state sinit, where progress is zero: Φ (st) = H(sinit, st). (6) Conversely, Backward-Anchored Prediction is anchored to the goal state sgoal, where progress is one. This approach offers high sensitivity near task completion: and backward Φ Φ predictions tend to exhibit significant divergence in OOD scenarios or observations, whereas they remain consistent in familiar states. Consistency-Aware Weighting. We first define the mean estimated progress Φ(st) = (Φ B(st))/2. To quantify uncertainty, we calculate normalized discrepancy metric: (st) + Φ norm(st) = Φ B(st) Φ Φ(st) + ϵ (st) , (9) where ϵ is small constant for numerical stability. Normalization by Φ ensures that discrepancies are penalized more heavily during the early stages (where Φ is small), as precise guidance is critical initially. We then derive confidence weight wt (0, 1] using Gaussian kernel with sensitivity α: wt = exp (cid:0)α (norm(st))2(cid:1) . (10) Conservative State Update. To prevent the policy from exploiting erroneous estimates in OOD scenarios, we employ conservative update rule for the maintained progress state Φ(st) instead of Equation (8): wt Φ(st) = Φ(st1)+ (cid:0) Φ(st) Φ(st1) + Φ (cid:1) . (11) This mechanism acts as semantic filter: it ignores uncertain updates when wt 0 (retaining Φ(st1)) and fully trusts the estimate when consistency is high (wt 1). t1,t Φ B(st) = 1 + H(sgoal, st). (7) 3.2. Dopamine-RL Framework These three methods offer complementary strengths: local precision (incremental), initial stability (forward), and goal sensitivity (backward). We fuse them via averaging to obtain robust final progress estimate: Φ(st) = 1 3 (Φ (st) + Φ (st) + Φ B(st)) . (8) This fusion yields more accurate and drift-resistant signal, which is critical for the subsequent reward shaping. 3.1.3. Progress Consistency Checking (Optional) While the multi-perspective fusion via averaging (Equation (8)) serves as baseline, its naive application in online RL faces the risk of Out-of-Distribution (OOD) hallucination. Due to the inherent limitations of data coverage, it is impossible for the training set to encompass every corner of the state space. During RL, the policy inevitably explores unseen regions where the reward model may yield spurious high signals, leading to reward hacking. To address these, we propose bi-directional consistency checking strategy that leverages consistency as proxy for reliability, which is motivated by the observation that forward 5 Building upon Dopamine-Reward with GRM, we further introduce the Dopamine-RL framework, reinforcement learning pipeline producing high-performance policy stimulated by Dopamine-Reward, featuring three key critical attributes: minimal downstream task effort for rapid progress alignment (Section 3.2.1), fast convergence with policyinvariant guarantees (Section 3.2.2) and seamless integration with diverse RL paradigms (Section 3.2.3). 3.2.1. One-shot GRM Adaptation Dopamine-RL requires only one single human demonstration Dhuman to adapt the pre-trained GRM to novel or highprecision tasks, since the pre-trained GRM has already possessed broad prior for assessing progress. Given new task, we minimize the Mean Squared Error (MSE) between its predicted hop value, ω, and the ground-truth, Hgt: LGRM(ω) = E(sp,sq)DhumanH ω Hgt2 2, (12) where ω represents the GRMs parameters, initialized by pre-trained GRMω0. After SFT, we obtain task-adapted GRMω , poised for efficient reinforcement learning. Figure 3. Reward profiles on challenging real-world rollout. We plot the reference reward from human annotations, the VLAC baseline, and our GRM along the same trajectory. Our GRM tracks the reference signal more faithfully, sharply penalizing incorrect insertions, low positions, and misalignments, and only assigning high reward near successful task completion. Figure 4. Real-world tasks and hardware setup. Left: eight representative long-horizon manipulation tasks used to evaluate DopamineReward and Dopamine-RL, including insertion, circuit completion, folding, pick-and-place, and assembly tasks. Right: our multi-view hardware platform with the Pika teleoperation system and calibrated ZED cameras, providing synchronized wrist and third-person observations for GRM training and policy learning. 3.2.2. Policy-Invariant Reward Shaping straightforward approach to defining the dense process reward function for policy learning is to use the direct increment of this progress: r(st, at, st+1) = Φ(st+1) Φ(st). However, optimizing the standard discounted return, J(π) = Eπ[(cid:80) t=0 γtr(st, at, st+1)], with this reward is mathematically equivalent to maximizing different objective: (π) Eπ[(cid:80) t=1 γt1Φ(st) s0], as detailed in Appendix A.2. This transformed objective creates perverse incentive: it encourages the agent not to complete the task, but rather to seek and maintain states with high progress values. Consequently, the resulting policy is rewarded for stagnation, preferring safe, suboptimal state over potentially risky trajectories that lead to true task completion. To resolve the misalignment, we formulate our GRM reward rGRM that adheres to three desiderata: Optimal policy invariance. The optimal policy learned with rGRM must coincide with that under the sparse gold reward rgold (1 at task completion, 0 otherwise), so shaping guides exploration without changing task objective. Discount consistency: rGRM must be compatible with the standard exponentially discounted return and TD or Bellman updates with factor γ under memoryless (Markov) reward assumption (see Appendix A.3). Locality. At any step t, rGRM is efficiently computable from the single transition (st, at, st+1). Adherence to these desiderata uniquely determines the reward structure, we derive the reward from the continuoustime discounted potential eλtΦ(st). As detailed in Appendix A.4, the natural discrete-time, single-step increment that is consistent with this continuous form is: (st, st+1) = γΦ(st+1) Φ(st), (13) where γ = eλh. To enable autonomous learning on real robots without the need for continuous human monitoring, we automate the determination of the sparse outcome reward rgold. Specifically, we consider the task completed when the estimated progress falls within close margin of the target (i.e., Φ(st+1) 1 δ, with δ = 0.05). Thus, rgold = 1 if the completion threshold is met, and 0 other6 wise. We add the shaping term to this automated goldstandard reward to define our final reward function: 4. Experiments rGRM(st, at, st+1) = rgold + γΦ(st+1) Φ(st). (14) This form guarantees policy invariance: the cumulative discounted shaping term forms telescoping sum that collapses to constant boundary term depending only on the initial state s0. Appendix A.5 shows that the discrete-time sum and the continuous-time integral of the discounted potentials derivative converge to the same constant. : (cid:88) t=0 (cid:124) γt(γΦ(st+1) Φ(st)) (cid:123)(cid:122) Discrete PBRS Sum (cid:121)t0 (cid:17) eλtΦ(st) dt (cid:123)(cid:122) Continuous Integral (cid:125) (cid:16) dt (cid:90) 0 (cid:124) = Φ(s0) (cid:123)(cid:122) (cid:125) (cid:124) Boundary Term (15) (cid:125) = Φ(s0) (cid:125) (cid:123)(cid:122) (cid:124) Boundary Term . Since the shaping term telescopes to state-dependent constant that is independent of the subsequent policy π, the shaped Q-function is simply state-wise shift of the original one: Qπ GRM(s, a) = Qπ gold(s, a) Φ(s). (16) The shift Φ(s) is identical for all actions in given state s, so the optimal action remains unchanged: arg max GRM(s, a) = arg max Q gold(s, a). (17) This matches the standard Potential-Based Reward Shaping (PBRS) framework [41], with the GRM progress Φ serving as the potential function. 3.2.3. Universal RL-Algorithm Compatibility Dopamine-RL exhibits strong universality, seamlessly integrating with any RL algorithm, encompassing online RL, offline RL, and offline-to-online RL paradigms. It adapts effectively to both value-based methods and gradient-based approaches. By reshaping targeted reward functions to guide agent learning, Dopamine-RL is inherently agnostic to the specific RL algorithm employed. Experimental results confirm this flexibility. In simulations, we deploy under two settings: PPO[46] (Proximal Policy Optimization) algorithm and OpenVLA-OFT[26] model, and ReinFlow[61] algorithm with π0[6] model. exhibits excellent performance under both settings. In real-world settings, we combine with Cal-QL[39] (a offline-to-online Qlearning based RL algorithm) and it also delivers exceptional outcomes. Further details are shown in Appendix C. 7 We evaluate Dopamine-Reward with GRM and DopamineRL on both simulation and real-world robotic platforms, covering broad range of manipulation skills and deployment scenarios. This section summarizes our empirical findings and is organized around four questions: RQ1: How accurate is the GRM at perceiving task progress compared to VLMs and existing reward models? RQ2: How does Dopamine-RL perform in success rate, sample efficiency, and generalization against strong BC and RL baselines? RQ3: How critical is Multi-Perspective Progress Fusion for final performance? RQ4: How important is the Dopamine-RL framework for turning reward modeling into practical policy learning? 4.1. Accurate Task Progress Perception (RQ1) In this part, We assess GRMs ability to estimate task progress using two complementary protocols: video frame rank-correlation and task completion judgment. 4.1.1. Video Frame Rank-Correlation To quantitatively assess task progress perception, we follow the evaluation methodology of GVL [37] and measure the Value-Order Correlation (VOC) between the GRMs predicted progress and the ground-truth chronological order of shuffled frames. higher VOC score ([-1, 1]) indicates better understanding of temporal progress. We evaluate on diverse suite of eight datasets spanning real-world robotics (DROID [24], AGIBOT-World [7], RoboBrain-X [17]), simulation (Libero [30], RoboCasa [40], RoboTwin2.0 [9]), and egocentric human manipulation (EgoDex [19]). To test robustness to temporal granularity, we test under three distinct sampling strategies: Sparse (S) using only major keyframes, Medium (M) using uniform samples between keyframes, and Dense (D) using uniform samples across the entire trajectory. We compare our multi-view and single-view GRM against four state-of-the-art reward models: GVL [37], and VLAC [59]. As shown in Table 1, our multi-view GRM consistently achieves the highest VOC scores across all seven datasets and all sampling strategies. The performance of baseline models tends to degrade as sampling becomes denser, indicating struggle with finegrained temporal distinctions. In contrast, our model maintains exceptionally high performance, highlighting the robustness of our hop-based learning formulation and multiperspective fusion. The performance gap is most significant in complex, long-horizon tasks (e.g., LIBERO [30], RoboBrain-X [17]), where our models ability to accurately contextualize progress is paramount. Table 1. Video Frame Rank-Correlation (VOC) on Diverse Datasets. We evaluate reward models under three temporal sampling strategies: Sparse (S), Medium (M), and Dense (D). Our GRM variants (Ours-3B and Ours-8B) consistently outperform prior work. Notably, the Ours-8B (Multi-View) model sets new state-of-the-art across all benchmarks and sampling densities, showcasing exceptional robustness and progress understanding. Dataset GVL [37] VLAC-2B [59] Ours-3B (Single-View) Ours-3B (Multi-View) Ours-8B (Single-View) Ours-8B (Multi-View) . DROID [24] R AGIBOT-World [7] RoboBrain-X [17] . LIBERO [30] RoboCasa [40] RoboTwin2.0 [9] Hum. EgoDex [19] Average M 0.01 0.24 0.32 0.43 0.06 0.28 0.05 0.20 -0.30 0.17 0. 0.37 -0.04 0.19 0.04 0.12 0.07 0.12 0.33 0.38 0.02 0. 0.66 0.29 0.18 0.19 0.00 0.26 0.69 0.40 0.13 0.28 0.11 0. 0.50 0.41 0.17 0.41 0.32 0.32 -0.10 0.09 0. 0.18 0.96 0.90 0.85 0.90 0.95 0.90 0.88 0.95 0.89 0.83 0.86 0.94 0.89 0.85 0.13 0.24 0. 0.33 0.91 0.89 0.94 0.87 0.81 0.85 0.93 0. 0.81 0.87 0.99 0.97 0.91 0.95 0.98 0.95 0. 0.98 0.96 0.89 0.91 0.97 0.94 0.85 0.96 0. 0.97 0.95 0.88 0.92 0.96 0.93 0.81 0.93 0.97 0.92 0.87 0.90 0.97 0.92 0.96 0.91 0.87 0.88 0.96 0.91 0. 0.86 0.92 0.91 0.95 0.89 0.83 0.86 0.95 0. 0.83 0.89 0.99 0.97 0.92 0.94 0.99 0.96 0.99 0.97 0.91 0.93 0.98 0.95 0.88 0.86 0.96 0. 0.98 0.96 0.89 0.92 0.97 0.94 0.83 0.94 Table 2. Task Completion Classification Accuracy (as successes out of 60). Our GRM more accurately classifies the final outcomes of robot rollouts compared to both specialized reward models and large generalist models. Method Stacking Folding Clearing Average V Gemini-2.5-Pro [14] GPT-5 [42] Qwen3-VL [45] RoboBrain 2.0 [51] GVL [37] VLAC-2B [59] 50/60 51/60 43/60 38/60 25/60 19/60 45/60 48/60 41/60 35/60 27/60 21/60 51/60 52/60 43/60 41/ 15/60 21/60 Ours-8B (Single-View) Ours-8B (Multi-View) 50/60 56/60 50/60 54/60 51/60 57/60 81.1% 83.9% 76.7% 61.7% 37.2% 33.9% 83.9% 92.8% 4.1.2. Task Completion Judgment To assess the GRMs ability to make high-level judgments about task outcomes, we follow the protocol from SARM [8]. We collect 60 real-world rollouts for each of three tasks (stacking blocks, folding T-shirt, clearing desktop), with 20 successful (SE), 20 partially successful (PSE), and 20 failed (FE) episodes. We evaluate classification accuracy against reward model baselines (GVL [37], VLAC [59]) and generalist vision-language models (Gemini-2.5-Pro [14], GPT-5 [42], Qwen3-VL8B [45], RoboBrain 2.0-8B [22, 51]). More evaluation settings are shown in Appendix C.1. Results in Table 2 shows: (1) Superiority over Generalist VLMs: While large models like GPT-5 [42] and Gemini-2.5-Pro [14] achieve respectable accuracy (83%), they often struggle with spatial precisionmisclassifying near-misses (PSE) as successes. Our GRM-8B (Multi-View) significantly outperforms them (+9% vs GPT-5), demonstrating that domainspecific training on progress data is more effective than zero-shot reasoning. (2) Failure of Single-View PRMs: Existing reward models like GVL [37] and VLAC [59] perform poorly (accuracy < 40%). This is primarily because single-view models lose track of objects during occlusion (e.g., hand covering the block during stacking), leading to noisy progress curves that fail the stability check in Equation (58). (3) Impact of Multi-View Fusion: The gap between our Single-View (83.9%) and Multi-View (92.8%) variants highlights the critical role of perceptual robustness. The multi-view fusion ensures that if one view is occluded, the model can still verify progress via alternative angles, correctly distinguishing between completed task (SE) and stalled one (PSE/FE). Table 3. Policy Performance and Sample Efficiency. DopamineRL achieves significantly higher performance with fewer human demonstrations. Sample efficiency is measured by episodes needed to reach 80% of the final success rate (lower is better). Method Simulation (10 Tasks) Real-World (8 Tasks) SR (%) Rollout (#) SR (%) Rollout (#) BC (50 demos) RL + Sparse Dopamine-RL 31.5 79.9 81.0 560 395 9.8 68.0 95. 183 150 4.2. Performance, Efficiency, Generalization (RQ2) We now evaluate the Dopamine-RL framework across 10 simulation tasks (from LIBERO [30] and RoboTwin2.0 [9]) and 8 real-world tasks, whose task setups and hardware platform are illustrated in Figure 4. In our simulation experiments, Dopamine-RL is evaluated under two distinct configurations: one leveraging the PPO [46] (Proximal Policy Optimization) algorithm alongside the OpenVLA-OFT [26] model, and the other integrating the ReinFlow [61] algorithm with the π0 [6] model. For real-world implementations, we pair Dopamine-RL with Cal-QL [39] and we employ Human-in-the-Loop setup where we use just one single human demonstrations to adapt the GRM. We compare against strong baselines: Behavioral Cloning (BC) on 50 demos, and Proximal Policy Optimization [46] (PPO) using sparse reward in simulation and ConRFT [12] for real-world settings. 8 ID vs. Table 4. Generalization Performance Breakdown: OOD. The table compares success counts (out of 20 trials) for Behavioral Cloning (BC) and our framework (Ours). The final row, Avg. Relative Drop (), quantifies the average relative success rate drop from ID performance when tested on OOD settings. Condition Original (ID) Object Layout Background Avg. Drop ( %) Insert Square Circuit Cap Pen BC 7/20 4/20 2/20 3/20 57. Ours BC Ours BC Ours 19/20 5/20 20/ 8/20 19/20 15/20 15/20 16/20 19.3 3/20 1/20 2/20 60. 17/20 19/20 19/20 8.3 5/20 3/20 4/20 50.0 17/20 15/20 16/20 15. Table 5. Ablation Study Results (Average Success Rate %). Each component of the Dopamine-Reward framework is shown to be critical for achieving maximum performance. Method Variation Success Rate from Full Full Framework (Dopamine-RL) Ablations for RQ3 (Core Components) w/o Fusion (Incremental Only) w/o Fusion (Forward-Anchored Only) w/o Fusion (Backward-Anchored Only) Ablations for RQ4 (Dopamine-RL Framework) w/o Policy-Invariant Shaping w/o One-shot adaption 85.0 70.0 65.7 62.5 41.3 63.2 -15.0 -19.3 -22.5 -43.7 -21.8 As shown in Table 3, Dopamine-RL significantly outperforms all baselines in both final success rate and sample efficiency. The dense and accurate rewards from our GRM enables rapid and stable learning, achieving high performance with far fewer environment interactions. To test generalization, we evaluate the final policies under In-Distribution (ID) and Out-of-Distribution (OOD) conditions, where OOD settings include changes to object properties (Object Change), workspace layout (Layout Change), and background visuals (Background Change). Table 4 presents detailed breakdown of this analysis. The results show that while both methods experience performance drop when faced with distribution shifts, the Average Relative Drop () is significantly more pronounced for the BC baseline (50-60% degradation). In contrast, our DopamineRL framework maintains much higher proportion of its original performance, with relative drop of only 8-20%. This quantitatively demonstrates that our policy has learned more robust and generalizable understanding of the task semantics, successfully mitigating the overfitting to superficial visual features that severely impacts the BC baseline. 4.3. Ablation Studies (RQ3 & RQ4) Finally, we conduct series of ablation studies on representative subset of three real-world tasks to validate the key design choices in the Dopamine-Reward framework. The results in Table 5 confirm our hypotheses. For RQ3, we ablate the Multi-Perspective Progress Fusion in Dopamine-Reward. Removing fusion and relying on single progress estimator consistently hurts performance: the incremental-only, forward-anchored-only, and backward-anchored-only variants incur 15.0%, 19.3%, and 22.5% absolute drops, respectively. The incremental-only variant is particularly vulnerable to error drift over long horizons, confirming the importance of combining local and global progress perspectives. For RQ4, the importance of the Dopamine-RL is evident. Removing policy-invariant reward shaping leads to massive performance drop of 43.7%. The agent learns to reach 9 good-enough states and stagnates, failing to complete the tasks, which confirms the semantic trap discussed in Section 3. Besides, relying solely on zero-shot GRM, it occasionally provides incorrect rewards for corner cases in outof-distribution (OOD) tasks, such as assigning positive rewards to poor actions and negative rewards to good ones. This hinders the convergence of the policy, resulting in 21.8% drop in success rate. 5. Conclusion In this work, as named Robo-Dopamine, we tackled the critical challenges of reward design in real-world robotics by introducing Dopamine-Reward, novel approach for learning general-purpose, step-aware reward model from multiview inputs. Our core contribution, the General Reward Model (GRM), is trained on over 3,400 hours of diverse data processed via Dopamine-Reward and leverages MultiPerspective Progress Fusion to overcome perceptual limitations like occlusion. Building upon this, our DopamineRL framework employs theoretically-grounded, PolicyInvariant Reward Shaping method, which provides dense guidance to accelerate learning without altering the optimal policy, thereby systematically avoiding the common semantic trap. Extensive experiments on diverse tasks validate our approach, demonstrating state-of-the-art reward accuracy and remarkable sample efficiency, with policies improving success rates from nearly-zero to 95% in an average of only 150 interaction rollouts while exhibiting strong generalization. By combining robust multi-view reward model with principled RL framework, our work presents scalable recipe for enabling embodied agents to achieve continuous self-improvement and master complex manipulation tasks far beyond their initial demonstrations. In the future, we plan to expand our work in four potential directions, detailed in Appendix. E."
        },
        {
            "title": "References",
            "content": "[1] Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, Michael Bloesch, et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. 2 [2] Minttu Alakuijala, Reginald McLean, Isaac Woungang, Nariman Farsad, Samuel Kaski, Pekka Marttinen, and Kai Yuan. Video-language critic: Transferable reward funcarXiv preprint tions for language-conditioned robotics. arXiv:2405.19988, 2024. 2, 3 [3] Xiang An, Yin Xie, Kaicheng Yang, Wenkang Zhang, Xiuwei Zhao, Zheng Cheng, Yirui Wang, Songcen Xu, Changrui Chen, Chunsheng Wu, et al. Llava-onevision-1.5: Fully open framework for democratized multimodal training. arXiv preprint arXiv:2509.23661, 2025. 23 [4] Shuanghao Bai, Wenxuan Song, Jiayi Chen, Yuheng Ji, Zhide Zhong, Jin Yang, Han Zhao, Wanqi Zhou, Wei Zhao, Zhe Li, et al. Towards unified understanding of robot arXiv preprint manipulation: comprehensive survey. arXiv:2510.10903, 2025. 2 [5] Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open arXiv foundation model for generalist humanoid robots. preprint arXiv:2503.14734, 2025. 2 [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy π0: Groom, Karol Hausman, Brian Ichter, et al. vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550. arXiv preprint ARXIV.2410.24164. 7, 8, [7] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. 2, 7, 8, 16, 18 [8] Qianzhong Chen, Justin Yu, Mac Schwager, Pieter Abbeel, Fred Shentu, and Philipp Wu. Sarm: Stage-aware reward modeling for long horizon robot manipulation. arXiv preprint arXiv:2509.25358, 2025. 2, 3, 8, 20 [9] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. 7, 8, 17, 18 [10] Yuhui Chen, Haoran Li, and Dongbin Zhao. Boosting continuous control with consistency policy. arXiv preprint arXiv:2310.06343, 2023. 21 [11] Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. Conrft: reinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450, 2025. 2, 3 [12] Yuhui Chen, Shuai Tian, Shugao Liu, Yingting Zhou, Haoran Li, and Dongbin Zhao. Conrft: reinforced fine-tuning method for vla models via consistency policy. arXiv preprint arXiv:2502.05450, 2025. 8, [13] Zengjue Chen, Runliang Niu, He Kong, Qi Wang, Qianli Xing, and Zipei Fan. Tgrpo: Fine-tuning vision-languageaction model via trajectory-wise group relative policy optimization. arXiv preprint arXiv:2506.08440, 2025. 3 [14] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 8, 18 [15] Figure AI. Helix: vision-language-action model for generalist humanoid control. https://www.figure.ai/ news/helix, 2025. Accessed: 2025-04-18. 2 [16] Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In International conference on machine learning, pages 4958. PMLR, 2016. 2, 3 [17] FlagOpen. Robobrain-x0. https://github.com/ FlagOpen/RoboBrainX0, 2025. GitHub repository, accessed 2025-11-08. 7, 8, 16, 18 [18] Seyed Kamyar Seyed Ghasemipour, Ayzaan Wahid, Jonathan Tompson, Pannag Sanketi, and Igor Mordatch. Self-improving embodied foundation models. arXiv preprint arXiv:2509.15155, 2025. 2, 3 [19] Ryan Hoque, Peide Huang, David Yoon, Mouli Sivapurapu, and Jian Zhang. Egodex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025. 7, 8, 17, [20] Zheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Kumar, Abhishek Gupta, and Sergey Levine. Reboot: Reuse data for bootstrapping efficient real-world dexterous manipulation. arXiv preprint arXiv:2309.03322, 2023. 3 [21] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. 2 [22] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. 8 [23] Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Jim Fan, and Yuke Zhu. Dexmimicgen: Automated data generation for bimanual In 2025 dexterous manipulation via imitation learning. IEEE International Conference on Robotics and Automation (ICRA), pages 1692316930. IEEE, 2025. 2 [24] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv:2403.12945, 2024. 2, 7, 8, 16, 18 arXiv preprint [25] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 2 [26] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. 7, 8, 20 [27] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 19 [28] Kun Lei, Huanyu Li, Dongjie Yu, Zhenyu Wei, Lingxiao Guo, Zhennan Jiang, Ziyu Wang, Shiyu Liang, and Rl-100: Performant robotic manipulation Huazhe Xu. arXiv preprint with real-world reinforcement learning. arXiv:2510.14830, 2025. 3 [29] Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, et al. Simplevla-rl: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025. 2, [30] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. 7, 8, 17, 18, 20 [31] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl, 2025. URL https://arxiv. org/abs/2505.05470. 3 [32] Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025. 3 [33] Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. 2, 3 [34] Jianlan Luo, Charles Xu, Xinyang Geng, Gilbert Feng, Kuan Fang, Liam Tan, Stefan Schaal, and Sergey Levine. Multistage cable routing through hierarchical imitation learning. IEEE Transactions on Robotics, 40:14761491, 2024. 3 [35] Jianlan Luo, Charles Xu, Jeffrey Wu, and Sergey Levine. Precise and dexterous robotic manipulation via human-inthe-loop reinforcement learning. Science Robotics, 10(105): eads5033, 2025. 2, 3 [36] Yecheng Jason Ma, Vikash Kumar, Amy Zhang, Osbert Bastani, and Dinesh Jayaraman. Liv: Language-image representations and rewards for robotic control. In International Conference on Machine Learning, pages 2330123320. PMLR, 2023. 2, 3 [37] Yecheng Jason Ma, Joey Hejna, Chuyuan Fu, Dhruv Shah, Jacky Liang, Zhuo Xu, Sean Kirmani, Peng Xu, Danny Driess, Ted Xiao, et al. Vision language models are incontext value learners. In The Thirteenth International Conference on Learning Representations, 2024. 2, 3, 7, 8, 19 [38] Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar Srirama, Archit Sharma, Chelsea Finn, and Aviral Kumar. Policy agnostic rl: Offline rl and online rl fine-tuning of any class and backbone. arXiv preprint arXiv:2412.06685, 2024. 3 [39] Mitsuhiko Nakamoto, Simon Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. Advances in Neural Information Processing Systems, 36:6224462269, 2023. 7, 8, 21 [40] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. 7, 8, 17, 18 [41] Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, pages 278287. Citeseer, 1999. 2, 7 [42] OpenAI. Gpt-5 system card. System card, OpenAI, 2025. Canonical PDF version dated August 13, 2025. [43] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: In 2024 IEEE InterOpen x-embodiment collaboration 0. national Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. 2 [44] Gang Peng, Jin Yang, Xinde Li, and Mohammad Omar Khyam. Deep reinforcement learning with stage incentive mechanism of dense reward for robotic trajectory planning. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 53(6):35663573, 2022. 2, 3 [45] QwenLM. Qwen3-vl. https : / / github . com / QwenLM/Qwen3-VL, 2025. GitHub repository, accessed 2025-11-09. 8 [46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 7, 8, 20 [47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatronlm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [48] Huajie Tan, Cheng Chi, Xiansheng Chen, Yuheng Ji, Zhongxia Zhao, Xiaoshuai Hao, Yaoxu Lyu, Mingyu Cao, Junkai Zhao, Huaihai Lyu, et al. Roboos-next: unified memory-based framework for lifelong, scalable, and robust multi-robot collaboration. arXiv preprint arXiv:2510.26536, 2025. 16 [49] Huajie Tan, Xiaoshuai Hao, Cheng Chi, Minglan Lin, Yaoxu Lyu, Mingyu Cao, Dong Liang, Zhuo Chen, Mengsi Lyu, Cheng Peng, et al. Roboos: hierarchical embodied framework for cross-embodiment and multi-agent collaboration. arXiv preprint arXiv:2505.03673, 2025. 16 11 [63] Jinliang Zheng, Jianxiong Li, Zhihao Wang, Dongxiu Liu, Xirui Kang, Yuchun Feng, Yinan Zheng, Jiayin Zou, Yilun Chen, Jia Zeng, et al. X-vla: Soft-prompted transformer as scalable cross-embodiment vision-language-action model. arXiv preprint arXiv:2510.10274, 2025. 2 [64] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. 2 [65] Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, et al. Robotracer: Mastering spatial trace with reasoning in vision-language models for robotics. arXiv preprint arXiv:2512.13660, 2025. [50] Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Xiansheng Chen, Pengwei Wang, Zhongyuan Wang, and Shanghang Zhang. Reason-rft: Reinforcement fine-tuning for visual reasoning of vision language models. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 3 [51] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. 8, 19 [52] Generalist AI Team. Gen-0: Embodied foundation models that scale with physical interaction. Generalist AI Blog, 2025. https://generalistai.com/blog/preview-uqlxvb-bb.html. 2 [53] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: arXiv preprint An open-source generalist robot policy. arXiv:2405.12213, 2024. 21 [54] Bart van Marum, Aayam Shrestha, Helei Duan, Pranay Dugar, Jeremy Dao, and Alan Fern. Revisiting reward design and evaluation for robust humanoid standing and walking. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1125611263. IEEE, 2024. 2, 3 [55] Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Learning dense rewards Tomizuka, and Stefan Schaal. In 2021 IEEE Interfor contact-rich manipulation tasks. national Conference on Robotics and Automation (ICRA), pages 62146221. IEEE, 2021. 2, 3 [56] Charles Xu, Qiyang Li, Jianlan Luo, and Sergey Levine. Rldg: Robotic generalist policy distillation via reinforcement learning. arXiv preprint arXiv:2412.09858, 2024. 2, 3 [57] Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, and Song Han. Streamingvlm: Real-time arXiv preprint understanding for infinite video streams. arXiv:2510.09608, 2025. 23 [58] Hongzhi Zang, Mingjie Wei, Si Xu, Yongji Wu, Zhen Guo, Yuanqing Wang, Hao Lin, Liangzhi Shi, Yuqing Xie, Zhexuan Xu, et al. Rlinf-vla: unified and efficient framework for vla+ rl training. arXiv preprint arXiv:2510.06710, 2025. 2, 3, [59] Shaopeng Zhai, Qi Zhang, Tianyi Zhang, Fuxian Huang, Haoran Zhang, Ming Zhou, Shengzhe Zhang, Litao Liu, Sixu Lin, and Jiangmiao Pang. vision-language-action-critic model for robotic real-world reinforcement learning. arXiv preprint arXiv:2509.15937, 2025. 2, 3, 7, 8, 19 [60] Hongyin Zhang, Zifeng Zhuang, Han Zhao, Pengxiang Ding, Hongchao Lu, and Donglin Wang. Reinbot: Amplifying robot visual-language manipulation with reinforcement learning. arXiv preprint arXiv:2505.07395, 2025. 2, 3 [61] Tonghe Zhang, Chao Yu, Sichang Su, and Yu Wang. Reinflow: Fine-tuning flow matching policy with online reinforcement learning. arXiv preprint arXiv:2505.22094, 2025. 7, 8, 20 [62] Tonghe Zhang, Chao Yu, Sichang Su, and Yu Wang. Reinflow: Fine-tuning flow matching policy with online reinforcement learning. arXiv preprint arXiv:2505.22094, 2025. 3 12 Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation"
        },
        {
            "title": "Appendix",
            "content": "This supplementary material provides comprehensive details regarding the proposed method, Dopamine-Reward, and the policy learning framework, Dopamine-RL, along with additional experimental results omitted from the main manuscript due to space constraints. Section provides rigorous theoretical proofs for the bounded global progress, the existence of the semantic trap, and the optimal policy invariance, etc. Section elaborates on the composition, statistics, and sampling strategies of our 35M-sample training dataset. Section outlines detailed experimental setups for GRM, simulation and real-world evaluations. Section presents additional qualitative visualizations to demonstrate the effectiveness of our General Reward Model (GRM). Finally, Section discusses limitations and potential future research directions. A. Proof A.1. Proof of Bounded Global Progress (Proof 1) In this subsection, we provide formal proof that iteratively applying the predicted relative progress hops guarantees that the reconstructed global progress Φ(s) remains strictly within the bounds [0, 1], provided that the initial state is bounded and the model predictions lie within [1, 1]. First, we define the general recursive update rule. Based on the definition of the hop label H(sp, sq) in Equation 2, we derive the recursive update rule for estimating the global progress of the next state Φ(st) given the current state Φ(st1) and the predicted hop = H(st1, st). We assume the normalization where Φ(s0) = 0 and Φ(sM ) = 1. Rearranging the equation, the update rule is: Φ(st) = (cid:40) Φ(st1) + [1 Φ(st1)] Φ(st1) + Φ(st1) if 0 if < 0 (18) Given that the initial progress Φ(s0) = 0 and the predicted hop [1, 1], the reconstructed global progress Φ(st) satisfies Φ(st) [0, 1] for all steps t. We proceed by mathematical induction as follow: (1) Base Case (t = 0): By definition, Φ(s0) = 0, which (2) Inductive Step: Assume that for satisfies 0 [0, 1]. step 1, the hypothesis holds: 0 Φ(st1) 1. Let = Φ(st1) for brevity, where [0, 1]. We analyze the next state Φ(st) under two cases (i.e., Positive Hop and Negative Hop) based on the sign of the predicted hop H. Case 1: Positive Hop (Progress), 0 1. From Equation (18), the update is written as: Φ(st) = + H(1 G) (19) Rearranging terms to view this as convex combination: Φ(st) = + G(1 H) (20) Lower Bound: Since 0, 0, and (1 H) 0, it follows that Φ(st) 0. Upper Bound: Since 1, we substitute the maximum value of G: Φ(st) = + G(1 H) + 1 (1 H) = + 1 = 1 Thus, 0 Φ(st) 1 when 0. Case 2: Negative Hop (Regress), 1 < 0. From Equation (18), the update is: Φ(st) = + = G(1 + H) (21) Lower Bound: Since [1, 0), the term (1 + H) 0. Since 0, the product G(1 + H) 0. Upper Bound: Since < 0, the term (1 + H) < 1. Combining this with 1: Φ(st) = G(1 + H) 1 (1) = 1 Thus, 0 Φ(st) 1 when < 0. Conclusion. Since the property holds for the base case and is preserved in both update scenarios during the inductive step, we conclude that Φ(st) [0, 1] for all t. A.2. Proof of the Semantic Trap (Proof 2) In this subsection, we provide theoretical derivation demonstrating why naive dense reward formulation, defined as the direct increment of progress r(st, at, st+1) = Φ(st+1) Φ(st), fundamentally alters the reinforcement learning objective, leading to the Semantic Trap described in the main text. Consider the standard objective in reinforcement learning, which is to maximize the expected discounted return with discount factor γ (0, 1): J(π) = Eπ (cid:34) (cid:88) t= (cid:12) (cid:12) γtr(st, at, st+1) (cid:12) (cid:12) (cid:35) s0 . (22) the reward Substituting r(st, at, st+1) = Φ(st+1) Φ(st) into the cumulative return for finite horizon : progress-difference naive GT = 1 (cid:88) t=0 γt[Φ(st+1) Φ(st)]. (23) We can split the summation into two distinct terms: GT = 1 (cid:88) t=0 γtΦ(st+1) 1 (cid:88) t=0 γtΦ(st). (24) By applying variable substitution = + 1 to the first term, we rewrite it as (cid:80)T k=1 γk1Φ(sk). The second term can be expanded as Φ(s0) + (cid:80)T 1 t=1 γtΦ(st). Substituting these back into the expression for GT yields: A.3. Derivation of Exponential Discounting from Time-Consistency (Proof 3) In this subsection, we justify the use of the exponential discount factor γ = eλh. We start from the principle of timeconsistency (or memorylessness). Let D(t) : R0 (0, 1] be discount function. The memoryless property implies that the relative discount factor for an additional delay should not depend on how much time τ has already passed: D(τ + ) D(τ ) = D(), τ, 0. (30) Setting τ = and = s, this yields the Cauchy functional equation: D(t + s) = D(t)D(s). (31) γt1Φ(st) + γT 1Φ(sT ) (cid:35) GT = (cid:34)T 1 (cid:88) t=1 (cid:34) Φ(s0) + (cid:35) γtΦ(st) . 1 (cid:88) t= Transforming to the logarithmic domain with ϕ(t) = ln D(t), we obtain the linear equation ϕ(t + s) = ϕ(t) + ϕ(s), the unique continuous solution of which is ϕ(t) = λt for some constant λ 0. Thus, the discount function must take the exponential form: (25) We now group the terms for each time step [1, 1]: GT = Φ(s0) + 1 (cid:88) (γt1 γt)Φ(st) + γT 1Φ(sT ) t=1 (26) D(t) = eλt. (32) In the discrete setting with time step h, this corresponds to the discount factor γ = D(h) = eλh. This theoretical foundation ensures that our reward formulation is robust to variations in control frequency. = Φ(s0) + (1 γ) 1 (cid:88) t=1 γt1Φ(st) + γT 1Φ(sT ). A.4. Consistency of Reward Shaping in Continuous and Discrete Time (Proof 4) (27) Since the progress metric Φ(s) is bounded within [0, 1] and γ (0, 1), the term γT 1Φ(sT ) vanishes as . The infinite horizon return converges to: = lim GT = Φ(s0) + (1 γ) (cid:88) t=1 γt1Φ(st). (28) Because Φ(s0) is constant with respect to the policy π, maximizing the expected return is equivalent to: arg max π J(π) arg max π Eπ (cid:34) (cid:88) t=1 γt1Φ(st) (cid:35) s0 . (29) (cid:12) (cid:12) (cid:12) (cid:12) Conclusion: The optimization objective implicitly shifts from maximizing the change in progress to maximizing the accumulated value of progress states over time. This creates perverse incentive where the agent is encouraged to reach high-progress state quickly and stagnate there to accumulate rewards at each step, rather than completing the task. This theoretical result confirms the existence of the Semantic Trap. In this subsection, we derive the continuous-time counterpart of our discrete potential-based reward shaping term γΦ(st+1) Φ(st). We demonstrate that the discrete shaping term is mathematically equivalent to the first-order Euler discretization of specific differential equation. Furthermore, we show that its cumulative sum converges to boundary term that ensures policy invariance. A.4.1. Notation and Definitions Let = denote the discretization time step. We map the discrete time steps t, + 1 to continuous time moments t, + h. Let s(t) denote the state trajectory in continuous time. The relationship between the continuous discount rate λ > 0 and the discrete discount factor γ is defined as: γ = exp(λh). (33) Let Φ(t) := Φ(s(t)) denote the potential value along the trajectory. Its total time derivative is: Φ(t) = dt Φ(s(t)). (34) 14 A.4.2. First-Order Taylor Expansion A.4.5. Boundary Terms and Consistency We perform first-order Taylor expansion for both the potential function Φ(s(t + h)) and the discount factor γ with respect to the step size h: Using the product rule for differentiation, we observe that the integrand is exactly the total derivative of the discounted potential: Φ(s(t + h)) = Φ(s(t)) + Φ(t) + O(h2), γ = eλh = 1 λh + O(h2). (35) (36) A.4.3. Derivation of the Instantaneous Shaping Term Substituting the expansions into the discrete shaping term γΦ(st+1) Φ(st) (where st+1 s(t + h)): (cid:0)eλtΦ(t)(cid:1) = eλt Φ(t)λeλtΦ(t) = eλt( Φ(t)λΦ(t)). dt Thus, the integral can be evaluated analytically: (cid:90) dt (cid:0)eλtΦ(t)(cid:1) dt = (cid:2)eλtΦ(s(t))(cid:3)T 0 = eλT Φ(s(T )) Φ(s(0)). (42) (43) γΦ(s(t + h)) Φ(s(t)) = (1 λh)(Φ(t) + Φ(t)) Φ(t) + O(h2) (cid:16) (cid:17) Φ(t) + Φ(t) λhΦ(t) λh2 Φ(t) = Φ(t) + O(h2) (cid:17) (cid:16) Φ(t) λΦ(t) = + O(h2). Assuming Φ(s) is bounded, as , the term eλT Φ(s(T )) vanishes. The cumulative shaping reward simplifies to constant boundary term: (37) lim (cid:90) eλt( Φ λΦ)dt = Φ(s(0)). (44) Dividing by and taking the limit as 0 yields the instantaneous density of the reward shaping: lim h0 γΦ(s(t + h)) Φ(s(t)) = Φ(t) λΦ(t). (38) This result indicates that the single-step potential-based reward is, to the first order, the rectangular integration of Φ(t) λΦ(t) over the interval [t, + h]. A.4.4. From Cumulative Sum to Integral Form We now consider the cumulative discounted sum of the shaping reward over horizon . Let tk = h. The discount factor at step is γk = (eλh)k = eλtk . The discrete cumulative sum is: 1 (cid:88) k= γk [γΦ(sk+1) Φ(sk)] . (39) Substituting the first-order approximation derived above: eλtk (cid:104) h( Φ(tk) λΦ(tk)) + O(h2) (cid:105) 1 (cid:88) k=0 = 1 (cid:88) k=0 eλtk ( Φ(tk) λΦ(tk)) + O(h). (40) As 0, this Riemann sum converges to the definite integral: h0 (cid:90) 0 eλt (cid:16) Φ(t) λΦ(t) (cid:17) dt. (41) 15 This confirms that in continuous time, the total shaping reward depends only on the initial state, preserving policy invariance. A.4.6. The ODE / Euler Method Perspective Finally, we provide an intuitive interpretation using Ordinary Differential Equations (ODEs). Let us define the discounted potential as state variable y(t): y(t) := eλtΦ(s(t)). The dynamics of y(t) are governed by: dy dt = eλt( Φ(t) λΦ(t)). (45) (46) If we apply the Forward Euler method to solve this ODE numerically at discrete steps tk with step size h: y(tk+1) y(tk) + dy dt (cid:12) (cid:12) (cid:12) (cid:12)tk . Substituting the definitions back: eλ(tk+h)Φ(sk+1) eλtk Φ(sk) + eλtk ( Φ(tk) λΦ(tk)). (47) (48) Multiplying both sides by eλtk (noting that eλh = γ): γΦ(sk+1) Φ(sk) + h( Φ(tk) λΦ(tk)). (49) Rearranging terms yields: γΦ(sk+1) Φ(sk) h( Φ(tk) λΦ(tk)). (50) Conclusion: The discrete potential-based reward shaping term γΦnext Φcurr is exactly the update step of the Forward Euler method applied to the differential equation of the discounted potential. This proves that our method is structurally consistent with the underlying continuous-time physics of the task. A.5. Policy Invariance under GRM (Proof 5) In this subsection, we prove that adding the shaping term derived above preserves the optimal policy. We show this by demonstrating that the cumulative shaped reward telescopes to boundary term that is independent of the policys actions. Let the shaped reward be rGRM = renv + , where (st, st+1) = γΦ(st+1) Φ(st). The shaping component of the Q-function, Sπ(s, a), is the expected sum of discounted shaping rewards: Sπ(s, a) = Eπ (cid:34) (cid:88) t=0 γt(γΦ(st+1) Φ(st)) . (51) (cid:35) We analyze the finite horizon sum GF : 1 (cid:88) GF = γt+1Φ(st+1) 1 (cid:88) t=0 γtΦ(st) t=0 (cid:34) (cid:88) k=1 = (cid:35) γkΦ(sk) (cid:34) Φ(s0) + (cid:35) γtΦ(st) . 1 (cid:88) t=1 (52) This is telescoping sum. The intermediate terms cancel out, leaving only the boundary terms: GF = γT Φ(sT ) Φ(s0). (53) Assuming Φ [0, 1] and γ < 1, taking the limit yields limT γT Φ(sT ) = 0. Thus: (cid:88) t=0 γtF (st, st+1) = Φ(s0). (54) 0 dt (eλtΦ)dt = [eλtΦ] This matches the integral of the continuous form derived in Proof 4: (cid:82) 0 = Φ(0). Since the shaping term evaluates to Φ(s) (where is the state at = 0) regardless of the future trajectory, the Q-values are shifted by state-dependent constant: Qπ GRM (s, a) = Qπ env(s, a) Φ(s). This shift preserves the ordering of actions: Qπ GRM (s, a1) Qπ GRM (s, a2) Qπ env(s, a1) Qπ env(s, a2). (55) (56) Therefore, the optimal policy remains: π GRM = π env. B. Details of GRM Training Data In this section, we present comprehensive breakdown of the training data used to construct our General Reward Model (GRM). To ensure the model possesses robust generalizability across diverse embodiments, environments, and 16 semantic tasks, we curated massive corpus comprising over 35 million training samples derived from approximately 3,400 hours of raw video footage. This dataset aggregates diverse sources spanning real-world robotics, highfidelity simulation, and human-centric interactions. We begin by categorizing the constituent datasets (Section B.1), followed by an analysis of the embodiment diversity and task statistics (Section B.2). Finally, we detail the rigorous data processing pipeline, specifically our Stratified Relative Progress Sampling strategy (Section B.3), which transforms raw trajectories into balanced and high-quality supervision signals for reward learning. B.1. Data Sources Our training data is composed of the following established datasets and self-collected supplements: B.1.1. Real-World Datasets AGIBot-World [7]: large-scale bimanual manipulation dataset collected on the AGIBot-A2D humanoid platIt provides approximately 3.4M samples focusform. ing on high-dimensional, dual-arm coordination tasks and contact-rich interactions, serving as critical source for humanoid embodiment generalization. DROID [24]: distributed robot interaction dataset collected across multiple institutions. It contributes 8.98M samples featuring the Franka Emika Panda robot. DROID is characterized by its extreme diversity in background scenes, lighting conditions, and object instances, providing robust priors for visual perception. RoboBrain-X [17]: comprehensive skill-learning dataset covering wide range of everyday manipulation skills. We utilize subsets totaling 3.0M samples, collected on agile platforms (e.g., Agilex Piper, Galaxea R1, AGIBot-A2D). This data enriches the GRM with finegrained motion primitives. Self-Collected (Real): To bridge the domain gap between open datasets and our specific experimental setups, we collected an additional 1.1M samples with the RoboOS [48, 49]. These include specific long-horizon tasks (e.g., folding, assembly) and corner cases to enhance model robustness. Collectively, these real-world datasets ground our GRM in physical reality, effectively mitigating the sim-to-real gap often observed in reward modeling. The combination of DROIDs extreme visual diversity with the complex morphologies present in AGIBot-World and RoboBrain-X ensures that the model learns robust, embodiment-invariant representations. This allows the GRM to remain stable across varying lighting conditions, textures, and kinematic structures, which is essential for reliable deployment in unstructured environments. Figure 5. Overview of GRM training data. (Left) The hierarchical composition of our 35M-sample training corpus. The dataset is derived from episodes spanning Real-World Robotics, Simulation, and Human-Centric domains, and is further expanded via multi-view augmentation. (Right) The long-tail distribution of task categories sorted by episode count (log scale). The dataset covers broad spectrum of manipulation skills, ranging from atomic primitives (e.g., pick, push) to complex, multi-stage horizons (e.g., assemble, fold). B.1.2. Simulation Datasets LIBERO [30]: benchmark originally designed for lifelong robot learning. We incorporate 1.33M samples from its task suites (Spatial, Object, Goal, 100), which provide procedurally generated tasks with language instructions, aiding the model in aligning visual progress with semantic goals. RoboCasa [40]: large-scale simulation framework focused on everyday kitchen activities. It leverages Generative AI to create diverse assets and layouts. We use 0.52M samples to capture realistic object interactions and complex scene semantics in controlled environment. RoboTwin [9]: high-fidelity digital twin dataset designed for bimanual manipulation. We utilize paired dual-view configuration (yielding 1.68M samples from 839k trajectories), which helps the model learn geometryaware representations crucial for depth disambiguation. While real-world data offers physical fidelity, these simulation environments provide controlled testbed for learning precise semantic and geometric alignments. The procedural generation inherent in LIBERO and RoboCasa exposes the model to vast combinatorial space of task instructions and object layouts, fostering strong instruction-following capabilities. Furthermore, the clean, occlusion-free labels and paired views from RoboTwin allow the GRM to learn finegrained geometric correspondences that are often noisy or unavailable in real-world data. B.1.3. Human-Centric Datasets EgoDex [19]: large-scale egocentric video dataset capturing dexterous human hand-object interactions. With 6.61M samples, this dataset provides strong priors for understanding hand-object affordances and manipulation logic independent of robot morphology. Self-Collected (Human): We supplemented the human data with 0.57M samples of domain-specific demonstrations, ensuring coverage of tasks analogous to our robot evaluation protocols. Integrating human video data is pivotal for scaling general manipulation intelligence beyond the limits of available robot demonstrations. By leveraging the massive scale of EgoDex, our GRM acquires universal object affordance priors, learning how objects should be manipulated regardless of the specific actuator. This cross-embodiment transfer is critical for evaluating progress in novel tasks where robotspecific data may be scarce, enabling the reward model to generalize purely based on the observed state changes of the objects themselves. B.2. Statistics and Embodiment Diversity Our compiled dataset represents one of the most comprehensive collections for robotic manipulation to date. As detailed in Table 6, the final training corpus of 35 million samples is strategically balanced to maximize generalization. Real-world interaction data constitutes the majority (60%) to ensure the model is grounded in physical reality, while high-fidelity simulation (13%) and large-scale human videos (26%) provide necessary semantic breadth and affordance priors that are difficult to obtain from robots. Embodiment Agnosticism. core design principle of our GRM is robustness to morphological variations. As illustrated in Table 7, the dataset covers wide spectrum of robot embodiments, preventing the model from overfitting to specific kinematics or camera calibrations. By training on this heterogeneous mixture, the GRM learns to focus on the state changes of the objects rather than the robots motion, enabling zero-shot transfer to unseen robot configurations. Task and Instruction Diversity. Figure 5 (Right) high17 Table 6. Statistics of Raw Data Sources. The table lists the raw frame counts (in thousands, k) before augmentation. The final training set is expanded to 35M via multi-view expansion and augmentation strategies. Domain Dataset Source Raw Samples Real-World AGIBot-World [7] DROID [24] RoboBrain-X [17] Self-Collected (Real) Simulation Subtotal LIBERO [30] RoboTwin [9] RoboCasa [40] Subtotal Human EgoDex [19] Self-Collected (Human) Subtotal Total Raw Corpus 3,400k 8,983k 3,025k 1,107k 16,515k 1,330k 1,678k 523k 3,531k 6,610k 574k 7,184k 27,230k Table 7. Distribution by Robot Embodiment. Our dataset spans diverse robot hardware, from single-arm industrial robots to bimanual humanoids, ensuring strong physical generalization. Robot Embodiment Primary Data Sources Samples Franka Emika Panda DROID, LIBERO, RoboCasa AGIBot-World, RoboBrain-X AGIBot-A2D Agilex Piper RoboBrain-X, Self-Collected RoboTwin ARX-X5 RoboBrain-X, Self-Collected Galaxea UR5 Self-Collected 8,983k 3,400k 3,552k 882k 695k 433k lights the semantic diversity of the dataset. The task distribution follows natural long-tail pattern, spanning from high-frequency atomic primitives (e.g., pick, place, push) that build solid generalist foundation, to complex, multi-stage horizons (e.g., fold, assemble, pour) that challenge the models ability to track long-term progress. To further enhance semantic robustness, we employed Gemini 2.5 Pro [14] to re-annotate task instructions, thereby introducing linguistic diversity and reducing overfitting to rigid template prompts. gressive augmentation pipeline, as described in main paper. More details are as follow: B.3.1. Hop-based Relative Progress Formulation Consistent with the main paper, we define the groundtruth relative progress label H(sp, sq) using relativerelative formulation. Let trajectory consist of steps {s0, . . . , sM } with linear global progress Φ(si) = i/M . Without loss of generality, we normalize the global progress such that the initial state has Φ(s0) = 0 and the goal state has Φ(sM ) = 1. For any training pair (sp, sq): H(sp, sq) = Φ(sq) Φ(sp) 1 Φ(sp) if (PROGRESS) Φ(sq) Φ(sp) Φ(sp) if < (REGRESS). (57) This formula ensures that forward progress is normalized by the remaining distance (1 Φ(sp)), making late-stage steps statistically significant, while regression is normalized by the accumulated distance (Φ(sp)). The continuous output [1, 1] is quantized into integer percentage tokens for VLM training. See Figure 8 for the whole prompt. B.3.2. Hierarchical Score-Gap Stratified Sampling To prevent the model from overfitting to specific step sizes, we implement two-stage sampling mechanism: Score Balancing: We discretize the progress score range [100%, +100%] into Nscore uniform bins (e.g., Nscore = 25). We generate large candidate pool of random pairs and fill these bins to enforce uniform distribution of reward values. This ensures the model encounters rare events, such as significant regression (errors) or large forward jumps, as frequently as common small steps. Temporal Gap Diversification: Within each score bin, we further categorize samples based on their temporal distance = tpost tpre into Ngap bins (e.g., Ngap = 4). This method explicitly forces the model to distinguish between fast progress (large score change in short time) and slow progress (large score change over long time), effectively decoupling visual state change from the temporal duration of trajectories. B.3. Sampling Strategy and Data Balancing B.3.3. Zero-Hop Anchoring To train the General Reward Model (GRM) effectively, it is crucial to generate training pairs that cover the full spectrum of task progress dynamics. Naively sampling random pairs from trajectory typically results in long-tail distribution dominated by small, positive progress steps, leading to model bias. To address this, we use the Stratified Relative Progress Sampling strategy combined with an agStandard comparative ranking often struggles with static or near-static pairs, leading to hallucinated progress values. To mitigate this, we explicitly inject fixed ratio α (e.g., α = 5%) of Zero-Score Samples. These samples are constructed by selecting pairs (t, t+δ) where the temporal delta δ is negligible (randomly sampled within 10% of the local window). These pairs are labeled with strict score of 18 Table 8. GRM Training Hyperparameters. Hyperparameter GRM-3B GRM-8B Global Batch Size LR: {ψViT , ϕLLM } LR Scheduler Warmup Ratio Optimizer Weight Decay Max Sequence Length Tensor Parallelism (TP) Pipeline Parallelism (PP) GPU nodes 256 512 {5e6, 1e5} {5e6, 1e5} Cosine Decay Cosine Decay 0.03 AdamW 0.1 8192 1 1 168 H100 0.03 AdamW 0.1 8192 2 2 168 H100 Training Duration 8 Days 14 Days ity. For inputs, the model accepts multimodal prompt sequence. Visual inputs include the Initial State Iinit, Goal State Igoal, and two sets of multi-view observations for the transition being evaluated: Spre = {I v=1 and Spost = {I v=1, where is the number of available camera views (e.g., one front, two wrists). Textual inputs include the system prompt defining the rigorous evaluator persona and the specific task instruction dtask. For outputs, the model outputs discrete token representing the relative progress hop H(Spre, Spost) [1, 1]. post}V pre}V Training Infrastructure. We conducted large-scale training on high-performance cluster consisting of 128 NVIDIA H100 (80GB) GPUs. The training framework utilizes the Megatron-LM architecture [47] for efficient distributed training. We employed Tensor Parallelism (TP) and Pipeline Parallelism (PP) to maximize throughput. Detailed hyperparameters are provided in Table 8. The 3B model was trained for approximately 8 days, while the 8B model required 14 days to converge on the 35M-sample dataset. Evaluation Protocols. We evaluate the GRM on two distinct tasks to assess both its fine-grained temporal understanding and its high-level task success judgment via vLLM engine [27]. For Video Frame Rank-Correlation, we use Value-Order Consistency (VOC) [37] as main metric, which assesses whether the reward model correctly orders states based on temporal progress. We measure VOC on unseen test data, where details are as follow: Data Selection: For each dataset (e.g., AgiBot-World, DROID), we randomly sample 10 trajectories per subclass in each dataset from the hold-out validation set. Metric: Given two frames tA and tB where tA < tB, correct prediction requires R(stA ) < R(stB ). The VOC score is the correlation coefficient [1, 1]. Baselines: We compare with VLAC [59] and GVL [37] using their official codebase or recommended prompts. For Task Completion Judgment, to verify if the GRM can serve as reliable success detector, we collected 60 realhttps://github.com/InternRobotics/VLAC 19 Figure 6. Overview of GRM model structure. The GRM is built upon the Qwen2.5-VL architecture. It processes multimodal interleaved input sequence consisting of task text instructions and multi-view images: the initial state (sinit), the goal state (sgoal), and the paired BEFORE (spre) and AFTER (spost) observation sets. The visual signals are processed by shared Vision Encoder and Projector, then fed into the LLM Decoder, which autoregressively predicts quantized relative progress token (e.g., <score>+15%</score>) or regress token (e.g., <score>-4%</score>). 0%. This creates semantic anchor for the model, teaching it that visual similarity implies zero progress, thereby reducing false positives in stable robotic states. B.3.4. Data Augmentation and Robustness Finally, to bridge the gap between the raw data count and our effective training scale, and to ensure robustness against perceptual failures, we employ an aggressive data augmentation pipeline: Multi-View Expansion: We maximize the utility of multi-camera setups (e.g., permuting available wrist and third-person views) by treating distinct viewpoints as complementary training signals. This strategy expands the dataset size from 27.2M raw samples to 35M training samples, improving the models geometric consistency. Perceptual Robustness Training: To prevent the model from over-relying on any single input modality, we introduce Random Viewpoint Dropout (randomly masking specific camera feeds) and Context Dropout (randomly masking sinit or sgoal tokens) during training. This forces the GRM to infer progress from partial or occluded observations, significantly enhancing resilience in realworld deployments. C. Experiment Details C.1. GRM Training and Evaluation Model Details. Our GRM leverages the RoboBrain 2.0 [51] architecture, as shown in Figure 6, exploring two parameter scales: lightweight 3B variant for efficient inference and powerful 8B variant for maximum reasoning capabilworld rollouts for each of three challenging tasks: Stacking Blocks, Folding T-shirt / Pants, and Clearing Desktop. These rollouts are uniformly distributed into 20 Successful (SE), 20 Partially Successful (PSE), and 20 Failed (FE) episodes. We define the automated judgment logic following SARM [8]. Let Pt [0, 1] be the accumulated global progress at step predicted by the GRM. The trajectory classification label is determined by: Label = SE, if Pfinal > 0.8 PSE, FE, (cid:80)T if 1 otherwise. t=1 Pt ξ,"
        },
        {
            "title": "3\nT",
            "content": "T (cid:88) t=2T /3 Pt > 0.6, where ξ = 0.4 is threshold for partial progress. C.2. Simulation Experiments (58) Figure 7. Training Curve on LIBERO-Goal. The success rate of our ReinFlow agent fine-tuned with GRM-based reward shaping. The agent demonstrates stable convergence and achieves an average success rate of over 80% across the benchmark tasks. This section details the simulation experiments: the benchmark and two different RL algorithm + VLA model settings we employ to validate our methods. C.2.1. Benchmark We evaluate our framework on the LIBERO-Goal suite [30], challenging subset of the LIBERO benchmark consisting of 10 distinct long-horizon manipulation tasks. These tasks require the agent to reason about specific goal configurations and perform precise object interactions, serving as rigorous testbed for progress-based reward modeling. C.2.2. Setting 1: PPO + OpenVLA-OFT In this setting, we combine PPO (Proximal Policy Optimization) [46], stable and sample-efficient on-policy RL algorithm, with OpenVLA-OFT [26], an optimized visionlanguage-action (VLA) model, to build the reinforcement learning pipeline, implementing it via the RLinf codebase [58]. OpenVLA-OFT acts as the base policy model, refined from the OpenVLA backbone through an Optimized Fine-Tuning (OFT) recipe that integrates parallel decoding with action chunking (for high-throughput action generation), continuous action representation (avoiding lossy discretization), and an L1 regression objective (simplifying It maps multimodal training while preserving precision). inputs to continuous robot actions in one forward pass. For LIBERO-Goals long-horizon tasks, we set an action chunk size of 8 to capture temporal dependencies and reduce compounding errors. PPO optimizes the OpenVLA-OFT policy by confining updates to trust region, using clipped surrogate objective to balance exploration and exploitation. Its core optimization objective is defined as: Here, ρt(θ) = πθ(atot) πθold (atot) is the ratio between the current policy πθ and the fixed rollout policy πθold, ˆAt is the estimated advantage at timestep t, and ϵ is the clipping parameter that prevents excessive updates. We follow RLinfVLA [58]s PPO best practices: using action-level value estimation (superior for chunked policies) and partial reset (resetting sub-environments post-completion to boost sample efficiency). C.2.3. Setting 2: ReinFlow + π0 In this setting, we combine ReinFlow [61], flow-matching based reinforcement learning algorithm, and π0 [6], flowmatching based vision-language-action model to build the reinforcement learning pipeline and implement it using the RLinf [58] codebase. The training is conducted on compute node equipped with 8 NVIDIA H100 GPUs. The total training time is approximately 50 hours, achieving an average success rate of 81% across the LIBERO-Goal tasks (Figure 7). ReinFlow facilitates stable fine-tuning by injecting learnable noise into the flow matching policys deterministic path, converting it into discrete-time Markov process. Specifically, the denoising process is modified as follows: ak+1 (cid:0)ak + vθ(tk, ak, o)tk, σ2 θ(tk, ak, o)(cid:1) (60) where vθ is the velocity network and σθ is learnable noise network. This formulation allows for an exact computation of the transition probability: (cid:18) πθ(ak+1ak, o) = ln ak+1 (cid:12) (cid:12) (cid:12) (cid:12) +vθ(tk, ak, o)tk, σ2 ak θ(tk, ak, o)(cid:1) PPO(θ) = Et (cid:104) min (cid:16) ρt(θ) ˆAt, clip (ρt(θ), 1 ϵ, 1 + ϵ) ˆAt https://github.com/RLinf/RLinf (61) (cid:17)(cid:105) . (59) 20 The policy is then optimized using PPO-style objective that jointly updates the velocity and noise networks: θ, θ = arg min θ,θ (cid:34) (cid:88) i=1 θold(oi, ai) K1 (cid:88) k=0 ln π θ(ak+1 ak , oi) + α R(ai, oi)] (62) where is the advantage function, and is regularization term (e.g., entropy). Furthermore, detailed hyperparameters for the ReinFlow training are provided in Table 9. Table 9. Hyperparameters for LIBERO-Goal Experiments. Hyperparameter Algorithm Consistency Sensitivity α Discount Factor γ Batch Size Learning Rate Compute Resources Value ReinFlow 20 0.99 1792 5e-6 8 H100 C.3. Real-World Experiments This section details the real-world robotic experiments: the algorithm we use and 8 representative tasks we conduct. C.3.1. Algorithm We adopt the Consistency-based Reinforced Fine-Tuning (ConRFT) algorithm [12] in our real-world experiments. ConRFT is an Cal-QL [39] algorithm variant designed as an offline-to-online reinforcement learning (RL) method rooted in Q-learning for fine-tuning pre-trained VisionLanguage-Action (VLA) models. ConRFT extends CalQLs framework by adding consistency policy and behavior cloning loss, enabling efficient and safe adaptation for real-world robotic manipulation tasks. Key components are as follows: Algorithm Overview. ConRFT operates in two sequential phases: offline initialization and online adaptation. It maintains two data buffers: demonstration buffer (D) and replay buffer (R). The offline phase leverages small set of human demonstrations (20-30 trajectories) stored in to initialize policy via behavior cloning (BC) and calibrated Q-learning (Cal-QL) [39], ensuring stable and safe initial behavior without requiring real-world exploration. The online phase then refines this policy through interaction with the physical world, storing transitions in R. human-inthe-loop (HIL) component intervenes to correct unsafe actions during online deployment, with these corrected trajectories added back to to guide subsequent policy updates. Symmetric sampling from the combined dataset is employed to mitigate distribution shift between the offline and online stages. Policy model. The policy model is built on the OctoSmall model [53], lightweight VLA backbone selected for its balance of performance and inference efficiency. The model is trained with visual encoders and transformer backbone and its original action head is replaced with consistency policy [10], diffusion-based module that maps Gaussian-sampled random actions to expert-like behaviors, conditioned on the observation embeddings. Offline Stage. The policy is initialized exclusively using data from the demonstration buffer D, with diffusionbased consistency policy (parameterized by ψ) serving as the action head. The unified training objective integrates BC and Q-learning to balance expert imitation and reward alignment, defined as Loffline π (ψ) = βLBC π + ηLQ π, (63) where β and η are hyperparameters that balance the two loss terms. The BC loss minimizes the Euclidean distance between actions generated by the consistency policy and expert demonstrations from D, formulated as π = (s,a)D LBC mU [1,M 1] (cid:34) (cid:35) fψ(a + kmz, km Eϕ(s)) a2 , (64) in which (s, a) denotes sampling state-action pairs from the demonstration buffer, [1, 1] indicates random sampling of diffusion sub-interval, km is the diffusion step corresponding to sub-interval m, (0, I) is Gaussian noise following standard normal distribution, Eϕ(s) is the observation embedding output by the frozen Octo-Small backbone, fψ is the consistency policy module parameterized by ψ, and 2 denotes the Euclidean distance. The loss maximizes the action-value estimates from learned critic network Qθ (where θ are the critics parameters), given by π = EsD,aπψ [Qθ(s, a)] , LQ (65) with πψ indicating actions sampled from the consistency policy. The critic Qθ is updated using the Cal-QL objective: (θ) = α (EsD,aπ [max(Qθ(s, a), µ(s))] Loffline E(s,a)D [Qθ(s, a)](cid:1) + 1 E(s,a,s)D (cid:104)(cid:0)Qθ(s, a) Bπ Qθ(s, a)(cid:1)2(cid:105) (66) . In this formula, α is the conservative penalty hyperparameter, µ(s) is the value of reference policy µ that stabilizes estimates for out-of-distribution actions, (s, a, s) samples transition triples (state, action, next state) from D, Bπ is the Bellman backup operator defined as Bπ Q(s, a) = 21 r(s, a) + γEaπ Q(s, a), and Qθ is delayed target Qnetwork whose parameters θ are fixed during critic updates to ensure training stability. Online Stage. The online phase refines the policy using combined data from the demonstration buffer which is augmented with corrected trajectories from HIL interventions and the replay buffer that stores online interaction transitions. The policy update objective retains the same structure as the offline stage but with adjusted weights to prioritize reward-driven exploration over expert imitation: Lonline π (ψ) = βLBC π + ηLQ π, (67) where β is reduced and η is increased, shifting the focus toward learning from real-world feedback. The BC and losses follow the same formulations as in the offline phase, with the only difference being that data sampling is performed from instead of alone, and all symbols retain their previously defined meanings. The critic Qθ is updated via standard Bellman error loss without the conservative penalty (as online data reduces distribution shift and mitigates overestimation risks), given by (θ) = E(s,a,s)DR Lonline (cid:104)(cid:0)Qθ(s, a) Bπ Qθ(s, a)(cid:1)2(cid:105) , (68) ensuring accurate value estimation as the policy adapts to real-world dynamics. Hyperparameters Detailed hyperparameters for realworld experiments are provided in Table 10. C.3.2. Real-World Tasks In real-world experiments, we selected 8 representative tasks to verify the promotional effect of our reward model on physical robotic reinforcement learning. These tasks cover single-arm tasks, dual-arm tasks, fine-grained tasks, and long-horizon tasks, with detailed descriptions of each task provided as follows. Insert Square: The end-effector of the robotic arm grasps square block with four holes. On the table, there is target board designed for block insertion, which is equipped with four upright pegs corresponding to the four holes on the block. The robot is required to adjust the position and orientation of the block to insert it onto the pegs of the board. This task demands millimeter-level precision tolerance, categorizing it as single-arm fine-grained task. Pick and Place: white gasket and toy are placed on the table. The robotic arm needs to first grasp the toy and then place it accurately on the gasket. The initial positions of both the gasket and the toy are randomly generated within predefined range. This task is defined as single-arm finegrained long-horizon task. Complete Circuit: disconnected circuit is placed on the table, and the right arm of the dual-arm robotic system holds battery. The robot is required to first insert the battery (held by the right arm) into the battery slot, then use the Table 10. Real-World Experiment Hyperparameters. Hyperparameter Symbol Value Global Batch Size Learning Rate Reward Discount Factor Offline BC Loss Weight Offline Loss Weight Conservative Penalty Coefficient Online BC Loss Weight Online Loss Weight Compute Resources - - γ β η α β η - 256 3 104 0.98 1.0 0.1 0.1 0.5 1.0 1 RTX4090 left arm to toggle the circuit switch, thereby lighting up the bulb in the middle of the circuit. This task is classified as dual-arm fine-grained long-horizon task. Arrange Flowers: vase is placed on the table, with the left and right arms of the robotic system each holding flower. The robot needs to sequentially insert the two flowers into the vase. This task is categorized as dual-arm fine-grained long-horizon task. Fold Towel: towel is laid on the table, and the robotic arm is required to fold it neatly. This task is defined as dual-arm fine-grained long-horizon task. Build Blocks: Three building blocks are placed on the table. The robotic arm needs to stack them sequentially to form castle-like structure. This task is classified as dualarm fine-grained long-horizon task. Cap the Pen: The dual-arm robot holds pen in one arm and pen cap in the other. It needs to slowly adjust the positions and orientations of the two objects to finally fit the cap onto the pen. This task is categorized as dual-arm fine-grained task. Zip the Bag: The left arm of the robot holds bag, while the right arm clamps the zipper of the bag. The robot is required to coordinate the movements of both arms to zip up the bag completely. This task is defined as dual-arm fine-grained task. D. More Qualitative Results In this section, we provide additional qualitative visualizations to further substantiate the effectiveness and robustness of our method. We focus on three key aspects: the generalization of GRM across diverse semantic tasks, the temporal robustness of progress estimation under varying sampling intervals, and the trajectory visualization of real-world RL. GRM Predictions on Diverse Tasks. Figure 9 illustrates the predicted Hop and Progress curves generated by our GRM across variety of manipulation tasks. The results demonstrate that our model accurately captures the monotonic increase in progress for successful trajectories while effectively identifying stagnation or regression in failed attempts, validating its semantic generalization capabilities. or tracking cumulative progress in cyclic tasks like scrubbing test tube where multiple repetitions are required), thereby significantly enhancing the robustness of reward assessment. Large-Scale Generalization: We plan to scale the Dopamine-RL framework to broader range of tasks in both simulation and the real world. Specifically, we aim to validate the framework on highly dynamic tasks (e.g., throwing, catching) and long-horizon mobile manipulation scenarios to test the limits of the policy-invariant reward shaping mechanism. Multi-Modal Reward Modeling: While vision provides rich global context, fine-grained manipulation often requires non-visual cues. We intend to incorporate tactile and auditory modalities into the GRM. Tactile feedback is crucial for contact-rich tasks (e.g., sensing force during insertion), while audio can detect discrete events (e.g., the click of latch), enabling more precise reward shaping for delicate operations. Robustness to Temporal Intervals. robust reward model should remain consistent regardless of the video sampling rate. Figure 10 compares the GRMs progress estimation when inputs are sampled at different intervals (t = 10, 25, 50, 100 frames). Despite the significant variation in visual disparity between adjacent frames, our Hopbased formulation ensures that the reconstructed global progress remains consistent, highlighting the models ability to decouple physical progress from temporal duration. Real-World RL Rollout Visualization. Finally, Figure 11 visualizes the robustness of the policy learned for the Insert Block task. The policy used in this rollout was trained for approximately 20 minutes and achieved success rate of over 95%. To evaluate the policys reactivity and the reward models accuracy, we introduced an artificial disturbance during execution. As shown in the sequence, human operator manually moves the target slot while the robot is in motion (a). This intervention causes the robot to miss the target and fall into misalignment (b). Crucially, the inset plots show that our Generative Reward Model (GRM) immediately reflects this setback: the estimated Progress drops sharply, correctly identifying that the state has regressed from the goal. This accurate negative feedback guides the agent to adjust its trajectory. The robot successfully recovers from the misalignment (c), repositions itself above the target (d), aligns with the slot (e), and completes the insertion (f). This demonstrates that GRM provides dense, semantically meaningful rewards that enable the agent to recover from unexpected external perturbations. E. Future Work In future research, we plan to expand the capabilities and efficiency of Robo-Dopamine in four primary directions: Efficient GRM Inference: The current VLM-based reward model, while accurate, incurs high computational latency which can bottleneck online RL training loops. We aim to investigate low-precision quantization techniques (e.g., INT4/INT8 quantization or KV-cache compression) to significantly accelerate GRM inference and reduce memory footprint without compromising reward accuracy for RL phase. Continuous Video Stream Reasoning: We aim to further evolve the GRM from discrete frame-pair inference to continuous video stream reasoning. By incorporating historical context sequences and integrating temporal modeling architectures (e.g., Video Transformers, Temporal CNNs, or Video-LLM [3, 57]), the model will gain the ability to capture dynamic motion trends, such as inertia and trajectory, which are essential for high-dynamic tasks. Furthermore, this temporal continuity resolves state ambiguities inherent in static snapshots (e.g., distinguishing between grasping and releasing an object, 23 Figure 8. User Prompt for General Reward Model (GRM). 24 Figure 9. GRM Progress Predictions across Diverse Tasks. We visualize the frame-wise Hop (instantaneous change) and accumulated Progress predicted by GRM on unseen validation tasks. 25 Figure 10. Progress Estimation Consistency across Sampling Intervals. We plot the reconstructed progress curves for the same trajectory using different frame strides (10, 25, 50, and 100 frames). The high overlap between curves demonstrates that our GRM is robust to temporal granularity and does not simply overfit to specific frame rate. 26 Figure 11. Robustness to Artificial Disturbance during Real-World Execution. We visualize rollout of the converged policy (success rate > 95%) under human interference. Each sub-figure shows the third-person view, the ego-centric view, and the real-time GRM inference (Top: Hop, Bottom: Progress). (a) Artificial Disturbance Position: human hand intervenes and shifts the target board while the robot attempts to approach. (b) Fall Into Misalignment: The robot misses the new position. Note that the GRM Progress curve drops significantly (indicated by the red dot in the bottom inset), reflecting the failure state. (c) Misalignment Recovery: The policy reacts to the visual feedback and the drop in reward, adjusting the end-effector position. (d) Move to the top: The robot realigns directly above the target slot. (e) Align with the Slot: Precise fine-tuning before insertion. (f) Successful Insertion: The task is completed, with the progress estimation reaching its peak."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Institute of Automation, Chinese Academy of Sciences",
        "State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University",
        "University of Sydney"
    ]
}