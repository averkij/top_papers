{
    "paper_title": "WUSH: Near-Optimal Adaptive Transforms for LLM Quantization",
    "authors": [
        "Jiale Chen",
        "Vage Egiazarian",
        "Torsten Hoefler",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 5 9 0 0 . 2 1 5 2 : r WUSH: Near-Optimal Adaptive Transforms for LLM Quantization Jiale Chen Institute of Science and Technology Austria (ISTA) Vage Egiazarian Institute of Science and Technology Austria (ISTA) Torsten Hoefler ETH Zürich Jiale.Chen@ist.ac.at Vage.Egiazarian@ist.ac.at torsten.hoefler@inf.ethz.ch Dan Alistarh Institute of Science and Technology Austria (ISTA) & Red Hat AI Dan.Alistarh@ist.ac.at"
        },
        {
            "title": "Abstract",
            "content": "Quantization to low bitwidth is standard approach for deploying large language models, however, few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines Hadamard backbone with data-dependent component based on second-order moments, yielding non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats."
        },
        {
            "title": "1.1 Overview",
            "content": "Quantization of model weights (Dettmers et al., 2022; Frantar et al., 2023; Lin et al., 2024) or activations (Xiao et al., 2023; Ashkboos et al., 2024b) is now standard tool for shrinking and accelerating large language models (LLMs) (Kurtic et al., 2025), making low-precision inference feasible on wide range of hardware. central difficulty, however, is that few extreme outlier weights and activations expand the dynamic range and thereby degrade the effective resolution of low-bit representations. 1 It is known that one way to mitigate these outliers is to apply linear transforms before quantization (Jegou et al., 2008, 2010); in the case of LLM quantization, this is often in the form of rotations that spread variance more evenly across channels (Chee et al., 2023; Tseng et al., 2024a; Ashkboos et al., 2024b; Liu et al., 2025). For LLMs, Hadamard rotations have been remarkably effective, and blockwise variants aligned with quantization groups have also been shown to be useful in practice. The recent work of Egiazarian et al. (2025) shows that the block-wise Hadamard transform offers the best empirical performance for quantization among large set of existing orthogonal transforms. Yet, these transforms are typically fixed and data-agnostic: in particular, the Hadamard transform does not depend on the statistics of the underlying weights or activations. This raises natural question: if the Hadamard transform is not data-aware, in what sense can it be considered optimal for quantization? In this work, we address this question by deriving closed-form optimal linear blockwise transforms for joint weight-activation quantization, which are generally non-orthogonal and adaptive, i.e., data-aware. As opposed to prior methods such as SpinQuant (Liu et al., 2025) or FlatQuant (Sun et al., 2024), our approach is closed-form and calibration-based, as it does not require training or fine-tuning. We call our construction WUSH, and we show that it is optimal for floating-point (FP) block quantizers and asymptotically optimal for integer (INT) block quantizers. The name WUSH is mnemonic for its composition. For layer with blockwise weight W(i) and calibration activation X(i) in the i-th block, the WUSH transform Thsuw(i) is constructed as Thsuw = HS 1 (i) (i)W (i) (1) is the Cholesky decomposition of W(i)W , the matrices U(i), S(i), V(i) where (i) = (i) are from the singular value decomposition (SVD) of , and is Hadamard matrix. We will define it again in more exhaustive manner in Eq. (6). WUSH is explicitly adapted to minimize loss in second-order statistics while retaining structure amenable to efficient implementation and providing principled, data-aware alternative to heuristic Hadamard-style and blockwise rotations. (i)W (i) (i) X(i) = U(i)S(i)V (i)"
        },
        {
            "title": "1.2 Related Work",
            "content": "Post-training quantization and outliers. Generic post-training quantization schemes, such as RTN (round-to-nearest), OBQ (Frantar & Alistarh, 2022), and GPTQ (Frantar et al., 2023), are highly sensitive to heavy-tailed outliers in the weights and activations of LLMs because small number of extreme values determine the quantization scale. As result, most quantization levels are spent covering rare extremes instead of the bulk of the distribution, which leads to significant loss of accuracy. Non-uniform bitwidths and explicit outlier storage. One common strategy to mitigate outliers is to use non-uniform bitwidths. Methods such as LLM.int8() (Dettmers et al., 2022), SpQR (Dettmers et al., 2024), and QUIK (Ashkboos et al., 2024a) explicitly separate and store outliers in higher precision, while HPTQ (Chen et al., 2025) uses Huffman encoding to compress the outliers and reduce their storage costs. These approaches can achieve high 2 accuracy at low effective bitwidth, however, the resulting formats are irregular and require specialized kernels and additional engineering effort for efficient deployment. Transform-based outlier mitigation. Another line of work applies transforms to the weights and activations before quantization to reduce the impact of outliers. SmoothQuant (Xiao et al., 2023) and AWQ (Lin et al., 2024) rescale channels to stabilize the dynamic ranges of weights and activations. QuIP (Chee et al., 2023) introduces an incoherence processing step. QuIP# (Tseng et al., 2024a), QTIP (Tseng et al., 2024b), and QuaRot (Ashkboos et al., 2024b) use Hadamard transforms to spread outlier energy across channel dimensions, while SpinQuant (Liu et al., 2025) learns rotations optimized on calibration data. However, their transforms are either heuristic, costly to learn, or primarily designed for weight-only quantization, which limits their accuracy or practicality when applied to fast, per-token activation quantization in large models. Blockwise transforms and emerging FP formats. The recent FP formats, such as MXFP (MX Alliance, 2023) and NVFP (NVIDIA, 2024), have motivated blockwise transform schemes. Shao et al. (2025) indicates that blockwise Hadamard transforms can be more effective under these FP formats than full-layer transforms, but Egiazarian et al. (2025) highlights the limited gains achievable with Hadamard alone, linking this to the properties of the underlying weight and activation distributions. In contrast, our work derives closedform, data-aware optimal blockwise transform (WUSH) and analyzes how such transform interacts with both FP and INT block quantizers."
        },
        {
            "title": "1.3 Methodology",
            "content": "Problem setup. We formulate the transformed weight-activation quantization problem as follows. Define Rdindout as the weight of linear layer with din input channels and dout output channels. Define Rdindbatch as the calibration input activation with din embedding channels and dbatch tokens. Define () as quantizer that maps matrix of continuous values to quantized values (we will specify later). Let TW, TX Rdindin be the transforms applied to and X, respectively. For weight-activation quantization, the output activation becomes (TWW ) (TXX). Our goal is to choose TW, TX such that the L2 output loss ℓ = d1 outd1 batch (cid:13) (cid:13)q (TWW ) (TXX) (cid:13) (cid:13) 2 (cid:13) (cid:13) (2) is minimized under the given quantizer q. The constant d1 analysis later. outd1 batch is added to simplify the Block-independent constraint. While the weights can be pre-quantized, the activations must be transformed and quantized dynamically at inference time. To reduce the computational overhead and simplify the problem, we constrain the transforms TW and TX to be block-diagonal, and the quantizer to be an RTN (round-to-nearest) quantizer with AbsMax (the maximum absolute value within group) scales. We assume that the quantization is applied to each sub-column vector of shape 1 and that the transform block size aligns 3 with the quantization group size (d is factor of din, and is power of 2). Denote TW = diag (cid:16) TW(1), . . . , TW(din/d) (cid:17) and TX = diag (cid:16) TX(1), . . . , TX(din/d) (cid:17) (3) with TW(i), TX(i) Rdd. We partition = and = (1), . . . , without quantization as = (cid:80)din/d quantization as with W(i) Rddout with X(i) Rddbatch. Then, we can express the output (i)X(i) and the output with weight-activation (1), . . . , i=1 (cid:104) (din/d) (din/d) (cid:105) (cid:104) (cid:105) (TWW ) (TXX) = din/d (cid:88) (cid:16) i=1 TW(i)W(i) (cid:17) (cid:16) TX(i)X(i) (cid:17) . Finally, define the blockwise output loss as ℓ(i) = d1 outd1 batch (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) TW(i)W(i) (cid:17) (cid:16) TX(i)X(i) (cid:17) (i)X(i) (cid:13) 2 (cid:13) (cid:13) (cid:13) . (4) (5) We approximate the layerwise loss as ℓ (cid:80)din/d independently. i=1 ℓ(i) and minimize the blockwise loss ℓ(i) (i)W (i) = d1 outW(i)W (i) Optimal block-diagonal transforms. We will prove that there exist closed-form optimal transforms for each block under reasonable assumptions and smooth approximations, which are optimal for FP and asymptotically optimal for INT quantizations. Let (i), (i) Rdd be matrices that satisfy , batchX(i)X (i) respectively. Without loss of generality (any differences will be canceled in later steps), we let and be the lower triangular matrices from the Cholesky decomposition of (cid:1) < or the second moments d1 rank (cid:0)X(i) (cid:1) < d, we can dampen the diagonal of the second moment before the Cholesky decomposition. Let the orthogonal matrices U(i), V(i) Rdd and the diagonal matrix . Let S(i) Rdd be the singular value decomposition (SVD) of Rdd be normalized (orthogonal) Hadamard matrix where each element is 1 2 . The optimal TW(i) . In the case of rank (cid:0)W(i) (i) = U(i)S(i)V (i) batchX(i)X (i) are constructed as outW(i)W (i) (i) = and TX(i) and d1 (i)X and (i) Thsvx(i) = HS 1 (i) 2 (i)X (i) and Thsuw(i) = HS 1 (i) 2 (i)W (i) , (6) hsuw(i) = with as the identity matrix. respectively. Note that Thsvx(i) = Thsvx(i)T Remark. The Hadamard matrix is the only data-agnostic ingredient in our optimal formulation in Eq. (6), which explains why it has been empirically shown to be one of the most effective data-agnostic orthogonal transforms. 1 , which can be easily verified by calculating hsuw(i) 1. The superscript notation means () = (()1) = (())1."
        },
        {
            "title": "2 Theoretical Derivation",
            "content": "In this section, we focus on the question of finding optimal transforms that minimize the loss of one block ℓ(i). We omit the subscript (i) to simplify notation. 2.1 Problem Setup and General Proof Approach (cid:16) Probabilistic reformulation. We reformulate the problem from probabilistic perspective. We split the matrices = [w1, . . . , wdout] and = [x1, . . . , xdbatch] into columns wk, xk Rd and treat the columns as i.i.d. samples from d-dimensional independent distributions Dw and Dx, respectively. Then the matrices and satisfy = Ewww and = Exxx, respectively. We can reinterpret the blockwise loss as ℓ = Ew,x Unbiased quantization. We assume that is stochastic and unbiased quantizer, i.e., for vector α Rd, the quantization error ε (α) = (α) α is random vector with such that (TWw) (TXx) is unbiased Eε(α)ε (α) = 0. We further constrain TW = with respect to wx. Then, using the first-order approximation, we can split ℓ into two non-negative terms: (TWw) (TXx) wx (cid:17) . ℓ = Ew,x,ε(TWw),ε(TXx) = Ew,x,ε(TWw),ε(TXx) Ew,x,ε(TWw),ε(TXx) (cid:16) (cid:16) (cid:16) (cid:18) Ew,ε(TWw) (cid:18) Ew,ε(TWw) = = (cid:13) (cid:13)X (cid:13) (cid:13) (cid:13)X 1 (cid:13) (TWw + ε (TWw)) (TXx + ε (TXx)) wx (cid:17)2 xT ε (TWw) + wT Wε (TXx) + ε (TWw) ε (TXx) (cid:17)2 Wε (TXx) xT ε (TWw) + wT 2(cid:19) (cid:18) (cid:13) (cid:13) ε (TWw) (cid:13) + (cid:13) (cid:13) ε (TWw) (cid:13) 2(cid:19) + Ex,ε(TXx) (cid:18) Ex,ε(TXx) (cid:13) (cid:13)W (cid:13) (cid:13) (cid:13)W 1 (cid:13) Wε (TXx) 2(cid:19) (cid:13) (cid:13) (cid:13) ε (TXx) 2(cid:19) (cid:13) (cid:13) (cid:13) . (cid:17)2 (7) Problem reduction. We can compute the optimal TW and TX individually for each of the non-negative terms and verify if TW = . The two terms have similar forms and can be optimized in similar ways. We focus on finding the optimal TX for the second term. Define the d-dimensional random variable = Dy. Define the transformation matrix = TXW . Then, the second term becomes Ex,ε(TXx) (cid:13) (cid:13)W 1 (cid:13) ε (TXx) (cid:13) 2 (cid:13) (cid:13) (cid:16) (cid:13) (cid:13) = Ex,ε(TXx) (cid:13) (cid:13) (cid:13)T 1ε (T y)(cid:13) (cid:13) 2 (cid:13) = Ey,ε(T y) TXW (cid:17)1 (cid:16) ε TX (cid:16) (cid:17)(cid:17)(cid:13) 2 (cid:13) (cid:13) (cid:13) (8) Therefore, the two-sided quantization problem is reduced to finding the optimal Rdd that (cid:13)T 1ε (T y)(cid:13) (cid:13) 2 and compute TX = . minimizes the one-sided quantization loss Ey,ε(T y) (cid:13) Transformation parameterization. Let the unknown orthogonal matrices , Rdd and the unknown diagonal matrix Rdd be the singular value decomposition (SVD) 5 of = SR; thus, can be parameterized as = SRS1U . Denote = diag (s1, . . . , sd) and = diag (s d). Without loss of generality, assume s1 sd > 0 and 1, . . . , 1 > 0. Theorem 1 (Optimal Transform) The optimal configuration of for floating-point (FP) 1 data types is = H, = 2 , and = I. For integer (INT), the same configuration is optimal up to do(1) factor for zero-mean Gaussian/Laplacian distributed data and within factor for any distribution. Proof We provide detailed proofs for FP and INT in Sections 2.2 and 2.3, respectively. For each data type, we apply two-step proof strategy. We first provide smooth modeling of the quantizer and treat the type casting errors as random noise. Secondly, we calculate the expected quantization error with respect to the parameterized transform and minimize this expectation by solving the corresponding optimization problem. Some useful notations and basic results are as follows: Denote the k-th standard basis vector as ek {0, 1}d, where the k-th element is 1 and 0 otherwise. The second moment of is Eyyy = (cid:0)Exxx(cid:1) = X = S2U . The second moment of is EyT (T y) = (cid:0)Eyyy(cid:1) = S2U . The expected squared norm of is Ey y2 = tr (cid:0)Eyyy(cid:1) = tr (cid:0)U S2U (cid:1) = tr (cid:0)S2(cid:1). Similarly, the expected squared norm of is Ey y2 = tr (cid:0)S2(cid:1). Because 0, (tr (S))2 tr (cid:0)S2(cid:1). By Jensens inequality, tr (cid:0)S2(cid:1) d1 (tr (S))2."
        },
        {
            "title": "2.2.1 Quantization Error Modeling",
            "content": "Theorem 2 (FP Quantization Error Modeling) For vector α Rd, we can model the quantization error ε (α) = diag (η) α with η = [η1, . . . , ηd] Rd being random vector of i.i.d. samples η Dη and Eηη = 0. Proof The FP format EaMb is defined as tuple (s, e, m) with sign {0, 1}, an exponent {0, . . . , 2a 1}, and mantissa (cid:8)0, . . . , 2b 1(cid:9). Assume we do not need to consider the subnormal and special number cases. The tuple (s, e, m) represents the number xEaMb = (1)s 2e2a1+1 (cid:0)1 + 2bm(cid:1). Using the approximation 1 + () 2() for 2bm [0, 1], we define smoothed version of EaMb, named SEaMb, as xSEaMb = (1)s 2e2a1+122bm = (1)s 22b(2be+m)2a1+1 with 2be + = 2b (cid:0)log2 xSEaMb + 2a1 1(cid:1) being + bit unsigned integer (concatenating the 6 binary representations of and m). The minimum of xSEaMb is min xSEaMb = 22a1+1 and the maximum is max xSEaMb = 22b(2a+b1)2a1+1 = 22a12b+1. Denote SEaMb (x) as the quantizer that casts [ max xSEaMb , max xSEaMb] to the nearest number in the SEaMb type. Assume (, 2a 1] so that the smaller values can be represented more precisely. SEaMb (x) can be approximated using integer rounding in the logarithm space and first-order expansion. For = 0, SEaMb (x) sgn (x) 22b2b(log2x+2a11)2a1+1 = sgn (x) 22b2b log2x (cid:16)(cid:106) (cid:16) sgn (x) (cid:16) = 1 + 22b2b log2x + (cid:16)(cid:106) (cid:109) 2b log2 (cid:109) 2b log2 (cid:17) 2b log2 (cid:17) 2b log2 (cid:17) 2b ln 2 . (cid:17) 22b2b log2x2b ln 2 (9) For = 0, SEaMb (0) limx0 (cid:0)1 + (cid:0)(cid:4)2b log2 x(cid:7) 2b log2 x(cid:1) 2b ln 2(cid:1) = 0. We model the casting error SEaMb (x) = (ln 2) 2bξx where [ max xSEaMb , max xSEaMb] and the random variable ξ Uniform (cid:0)21, 21(cid:1). For quantization format that uses EaMb for values and EaMb for scales, we approximate it with scalar quantizer (x) = SEaMb (cid:0)xs1(cid:1) SEaMb (s) where R, scale R=0, max (cid:12) (cid:12) max xSEaMb. Then, (cid:12), and (cid:12) (cid:12) (cid:12)xs1(cid:12) (cid:12)xSEaMb (x) = SEaMb (cid:0)xs1(cid:1) SEaMb (s) = (cid:16) 1 + (ln 2) 2bξ (cid:17) xs1 (cid:16) (cid:17) 1 + (ln 2) 2b ξ (cid:16) = 1 + (ln 2) 2bξ (cid:17) (cid:16) (cid:17) 1 + (ln 2) 2b ξ (10) with the independent random variables ξ, ξ Uniform (cid:0)21, 21(cid:1). Define η = (cid:0)1 + (ln 2) 2bξ(cid:1) (cid:16) (cid:17) 1 + (ln 2) 2b ξ 1. The quantization error is (x) = ηx. Eηη = ξ, ξ (ln 2) 2bξ + (ln 2) 2b ξ + (ln 2)2 2bbξ ξ = 0. (11)"
        },
        {
            "title": "2.2.2 Loss Minimization",
            "content": "Using Theorem 2, the minimization objective is Ey,ε(T y) (cid:13)T 1ε (T y)(cid:13) (cid:13) 2 (cid:13) (cid:13)T 1 diag (η) y(cid:13) (cid:13) 2 (cid:13) = Ey,η = Ey,η tr (cid:16) (cid:16) (cid:17) = Ey,η tr (cid:16) yT diag (η) 1 diag (η) 1 diag (η) yyT diag (η) (cid:17) (cid:17) diag (η) (cid:17) (cid:17) S2U (cid:17) Eyyy(cid:17) 1 (cid:16) Eη diag (η) 1 (cid:16)(cid:16) (cid:16) = (cid:0)Eηη2(cid:1) tr = tr (cid:16) (12) (cid:17) 7 where represents elementwise multiplication. The lower bound of the trace term in the loss is tr = tr (cid:16) 1 (cid:16)(cid:16) (cid:18) 1 diag S2U (cid:17) (cid:17) (cid:18)(cid:13) (cid:13)SU e1 (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:17) (cid:13) (cid:13)SU ed (cid:13) , . . . , (cid:88) (cid:88) j=1 k= (cid:0)ejT 1ek (cid:1)2 (cid:13) (cid:13)SU ek (cid:13) (cid:13) 2 (cid:13) (cid:13) = = = = (cid:88) k=1 (cid:88) k=1 (cid:88) k=1 (cid:88) k=1 (cid:32) (cid:13) (cid:13)T 1ek (cid:13) (cid:13) 2 (cid:13) (cid:13)SU ek (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13)U SRS1U ek (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13)SU ek (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13)SRS1U ek (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13)RSU ek (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:16) S1RSRSU ek (cid:17)2 = d (cid:88) (cid:16) S1RSRSU ek (cid:33) (cid:17)2 k= (cid:88) (cid:32) d1 S1RSRSU ek (cid:33) k=1 (cid:16) tr = d1 (cid:16) = d1 (tr (S))2 . S1RSRSU (cid:17)(cid:17)2 2(cid:19) (cid:13) (cid:13) (cid:13) (cid:19) (rotation-invariance of L2 norm) (Cauchy-Schwarz) (Jensen) (13) The equality can be attained by choosing = H, = 1 2 , and = such that (cid:16) 1 (cid:16)(cid:16) S2U (cid:17) tr (cid:17) (cid:17) = (cid:88) k=1 (cid:13) (cid:13)SRS1U ek (cid:13) (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13)RSU ek (cid:13) (cid:13) 2 (cid:13) (cid:13) = (cid:18)(cid:13) (cid:13) (cid:13)S (cid:88) k=1 1 2 ek 2(cid:19) (cid:13) (cid:13) (cid:13) (cid:88) = (cid:88) (cid:16) sj k=1 j=1 ek (cid:17) 2 = (cid:88) (cid:88) k=1 j=1 (14) sjd1 = d1 (tr (S))2 . 8 2.2.3 Discussion Under our smooth modeling in Theorem 2, for any orthogonal (including the identity = and the Hadamard = H), the minimization objective Ey,ε(T y) (cid:13)T 1ε (T y)(cid:13) (cid:13) 2 (cid:13) Eηdiag (η)2(cid:17) = (cid:0)Eηη2(cid:1) Ey y2 = (cid:0)Eηη2(cid:1) tr (cid:0)S2(cid:1) (cid:0)Eηη2(cid:1) d1 (tr (S))2 (cid:0)Eηη2(cid:1) d1 tr (cid:0)S2(cid:1) , (cid:13)T 1 diag (η) y(cid:13) (cid:13) 2 (cid:13) = EyyT (cid:16) = Ey,η (15) so the orthogonal transforms will not be helpful for reducing the quantization error. Also note 1 that the non-trivial choices = and = 2 are both essential for reducing the error by at most times, with tr (S) s1 being the extreme case scenario. Either trivial choices of = or = will lead to the same suboptimal trace term tr (cid:0)T 1 (cid:0)(cid:0)U S2U (cid:1) I(cid:1) (cid:1) = tr (cid:0)S2(cid:1), which is the same as the case of orthogonal ."
        },
        {
            "title": "2.3.1 Quantization Error Modeling",
            "content": "We model the quantization error using pseudo noise similar to DiffQ (Défossez et al., 2022). Theorem 3 (INT Quantization Error Modeling) For vector α Rd, we can model the quantization error ε (α) = α η with η = [η1, . . . , ηd] Rd being random vector of i.i.d. samples η Dη and Eηη = 0. Proof Define the scalar quantizer of b-bit symmetric integer as (x) = (cid:0)(cid:4)xs1(cid:5) + 21(cid:1) s, where R, scale R=0, and xs1 (cid:2)2b1, 2b1(cid:1). To simplify the analysis, we model the quantizer as (x) = (cid:0)xs1 + ξ(cid:1) = + sξ with random noise ξ Uniform (cid:0)21, 21(cid:1). . Define η = 21bξ, then we For vector α Rd, we use the AbsMax scale = 21b α . The quantization error ε (α) = α η with η = [η1, . . . , ηd] Rd have sξ = η α being random vector of i.i.d. samples η Dη and Eηη = 21bEξξ = 0."
        },
        {
            "title": "2.3.2 Loss Minimization",
            "content": "Using Theorem 3, the minimization objective is Ey,ε(T y) (cid:16) (cid:13)T 1ε (T y)(cid:13) (cid:13) 2 (cid:13) 1 (cid:16) Eηηη(cid:17) (cid:17) = tr = (cid:0)Eηη2(cid:1) (cid:13) Ey y2 (cid:13)T 1(cid:13) 2 (cid:13) . = Ey,η (cid:13)T 1 y η(cid:13) (cid:13) 2 (cid:13) = (cid:0)Eηη2(cid:1) tr Ey y2 (cid:16) 1T (cid:17) Ey y2 (16) We can obtain lower bound for the term (cid:13) tr (cid:0)S2RS2R(cid:1). Because s2 1 s2 (cid:13)T 1(cid:13) = tr (cid:0)T 1T (cid:1) = tr (cid:0)U SRS2RSU (cid:1) = 2 (cid:13) and s2 1 s2 , using von Neumanns trace 9 inequality, we obtain: (cid:13)T 1(cid:13) (cid:13) 2 = tr (cid:13) (cid:16) S2RS2R(cid:17) tr (cid:0)S2S2(cid:1) . (17) Equality is attained when = I. Next, we obtain the lower and upper bounds for the term Ey y2 (cid:0)e tations as Ey y2 y(cid:1)2 = (cid:80)d (cid:80)d Ey y2 = Ey Ey maximum, and summation values, we have: . Express the expeck y(cid:1)2 (Jensens inequality) and y(cid:1)2. By the relationships among the mean, y(cid:1)2 maxk Ey (cid:0)e = Ey maxk (cid:0)e (cid:0)e k= k=1 d1Ey y2 max (cid:16) Ey T (cid:17)2 where Ey y2 = tr (cid:0)S2(cid:1) and Ey (cid:13) (cid:13)SU ek (cid:13) 2. (cid:13) Ey y2 Ey y2 max (cid:16) Ey T (cid:17)2 , (18) (cid:0)e y(cid:1)2 = T (cid:0)Eyyy(cid:1) ek = U S2U ek = Lemma 4 (Maximum Inequalities for Tail-Bounded Distributions) If X1, . . . , Xd are zero-mean Gaussian random variables, If X1, . . . , Xd are zero-mean Laplacian random variables,"
        },
        {
            "title": "E max\nk",
            "content": "X 2 min {d, (2 ln (2d) + 2)} max EX 2 ."
        },
        {
            "title": "E max\nk",
            "content": "X 2 (cid:16) 21 (ln d)2 + ln + 1 (cid:17) max EX 2 . Both inequalities hold without the need for independence. (19) (20) Proof We provide proofs for Gaussian and Laplacian distributions in Sections A.1 and A.2, respectively. Using Lemma 4, for tail-bounded Dy that is zero-mean multivariate Gaussian or Laplacian, we can further tighten the upper bound by Ey y2 min (cid:26) do(1) max (cid:16) Ey T (cid:17)2 , Ey y2 (cid:27) . (21) When = H, the marginal second moment becomes Ey (cid:13) (cid:13)SH ek and maxk Ey = H, the term (cid:13) such that (cid:13) (cid:13) = d1 = d1 tr (cid:0)S2(cid:1) = d1Ey y2, which is equalized for all k, y(cid:1)2 is minimized to d1Ey y2. Taken together, by setting = and is bounded by tr (cid:0)S2S2(cid:1) tr (cid:0)S2(cid:1) within gap of (cid:13) (cid:13) = (cid:80)d (cid:0)e y(cid:1)2 = (cid:13) Ey y2 (cid:13)T 1(cid:13) 2 (cid:13) (cid:13)SU ek j=1 s2 (cid:0)e d1 tr (cid:0)S2S2(cid:1) tr (cid:0)S2(cid:1) (cid:13) (cid:13)T 1(cid:13) 2 (cid:13) Ey y2 tr (cid:0)S2S2(cid:1) tr (cid:0)S2(cid:1) . And for tail-bounded Dy, the gap is narrowed to do(1) such that (cid:13)T 1(cid:13) (cid:13) 2 (cid:13) Ey y2 = do(1)1 tr (cid:0)S2S2(cid:1) tr (cid:0)S2(cid:1) . 10 (22) (23) By the Cauchy-Schwarz inequality, tr (cid:0)S2S2(cid:1) tr (cid:0)S2(cid:1) (cid:16) tr (cid:16)(cid:0)S2S2(cid:1) 1 2 (cid:0)S2(cid:1) 1 2 (cid:17)(cid:17)2 = (tr (S)) (24) and the equality is attained when S = 1 2 to minimize both the lower and upper bounds of the quantization loss. 1 2 . Without loss of generality, we can choose 2.3.3 Discussion (cid:13)T 1(cid:13) Consider the case of being orthogonal. (cid:13) = and Ey y2 = Ey y2 = tr (cid:0)S2(cid:1). 2 (cid:13) For any distribution Dy, the bounds are tr (cid:0)S2(cid:1) (cid:13) (cid:13)T 1(cid:13) tr (cid:0)S2(cid:1). For the 2 (cid:13) Hadamard = and tail-bounded Dy, (cid:13) (cid:13)T 1(cid:13) = do(1) tr (cid:0)S2(cid:1) with high 2 Ey y2 (cid:13) probability. Because tr (cid:0)S2(cid:1) d1 (tr (S))2 d1 tr (cid:0)S2(cid:1), an orthogonal is suboptimal, and the error bounds of an orthogonal can be at most times larger than those of the optimal , with tr (S) s1 being the extreme case scenario. Ey y"
        },
        {
            "title": "3.1 Algorithm",
            "content": "Algorithm 1: Compute WUSH Transforms and Pre-Quantize Weights Input: weights Rdindout, activations Rdindbatch, damping ratio λ Output: WUSH transform Thsuw(i) Rdd and pre-quantized weight ˆW(i) Rddout for each block = 1, . . . , din/d 1 MX d1 batchXX + λI 2 MW d1 outW + λI 3 for 1 to din/d (in parallel) do 4 j1, j2 (i 1) + 1, id 5 MX MX[j1 : j2, j1 : j2] // select the block with row/column index from j1 // same as GPTQs Hessian"
        },
        {
            "title": "6 MW ← MW[j1 : j2, j1 : j2] // select the block with row/column index from",
            "content": "(inclusive) to j2 (inclusive) j1 (inclusive) to j2 (inclusive)"
        },
        {
            "title": "7 W ′ ← Cholesky (MW)\n8",
            "content": "U , Λ Eigendecomposition (cid:0)W MXW (cid:1) Thsuw(i) HΛ 1 4 (cid:16) ˆW(i) = (i) [j1 : j2, 1 : dout]"
        },
        {
            "title": "Thsuw",
            "content": "(cid:17) 9 10 // = MW // ΛU = MXW // WUSH transform // pre-quantize weight block 11 end Algorithm 1 summarizes the procedure used to compute the blockwise transforms and prequantized weights for each linear layer. In practice, the required second-order information can be obtained either from the calibration activations or from the Hessian matrix; the 11 latter coincides, up to constant factor, with the Hessian used in GPTQ. As result, the calibration pipeline closely follows that of GPTQ: layers are processed sequentially, and after processing layer, the calibration activations are propagated through the quantized layer to provide the inputs for calibrating the next one. During inference, the forward pass is calculated as din/d (cid:88) i= ˆW (i)q (cid:0)Thsuw(i)X(i) (cid:1) . (25) with the symbol being overloaded as the new activations instead of the calibration ones. 3.2 Layer Loss Table 1: Weight-activation RTN quantization losses with identity (I), random rotation (R), Hadamard (H), WUSH without Hadamard (WUS), and WUSH transforms."
        },
        {
            "title": "Format Transform",
            "content": "Q O"
        },
        {
            "title": "Gate",
            "content": "Up"
        },
        {
            "title": "Down",
            "content": "MXFP4 NVFP4 INT"
        },
        {
            "title": "I\nR\nH\nWUS\nWUSH",
            "content": ".00547 .00435 .01105 .00407 .00384 .00761 .00390 .00379 .00724 .00627 .00446 .00357 .00334 .00334 .00330 .00276 .00449 .00439 .00339 .00710 .00556 .00545 .00576 .01068 .00914 .00860 .00405 .01201 .00773 .00720 .00722 .00656 .00568 .00561 .00475 .00435 .00423 .00501 .00498 .00560 .00562 .00226 .00236 .00244 . .00234 .00437 .00254 .00586 .00258 .00670 .00200 .00230 .00228 .00192 .00241 .00341 .00349 .00266 .00368 .00363 .00371 .00278 .00379 .00304 .00301 .00223 .00233 .00302 .00309 .01934 .00455 .17004 .00310 .00294 .00579 .00303 .00286 .00557 .21268 .01308 .00454 .00239 .00243 .00254 .00210 .00343 .00343 .00255 .05585 .00422 .00409 .05021 .01315 .00696 .00680 .01067 .12277 .00584 .00555 . .00983 .00429 .00425 .00742 We report the layerwise weight-activation RTN quantization loss (L2 loss normalized by the number of elements) for each linear layer in the 18th transformer block of Qwen3-8B, using 32 calibration samples of sequence length 2048 from the FineWeb-Edu (Penedo et al., 2024) dataset. We compare the WUSH transform with the identity (I), random rotation (R) averaged over 10 runs, Hadamard (H), and WUSH without Hadamard (WUS). Table 1 summarizes the losses for MXFP4, NVFP4, and INT4 formats, where INT4 denotes 4-bit integers with Gaussian MSE clipping and BF16 scales of group size 32. Across formats, WUSH substantially reduces the loss: WUSH consistently yields the smallest loss for MXFP4 and INT4, while WUSH and WUS are almost equally optimal for NVFP4. 12 These empirical trends are broadly consistent with our theoretical analysis, suggesting that the approximation assumptions (Section 2.1) and the smooth error models (Sections 2.2.1 and 2.3.1) capture the dominant sources of quantization loss in practice. At the same time, MXFP4 behaves as hybrid between ideal FP and INT quantization: the effective mantissa step changes uniformly, imparting INT-like behavior, especially in the subnormal regime and small exponent ranges. This helps explain why Hadamard transforms still provide measurable gains for the MXFP4 format, despite the ideal FP theory (Section 2.2.3) predicting no benefit for purely orthogonal transforms. 3.3 LLM Benchmarks We now present preliminary experiments evaluating the accuracy of models using the WUSH transform, relative to the identity (standard RTN) and Hadamard. We conduct our experiment on the LLama-3.2-3B-Instruct, LLama3.1-8B-Instruct, Qwen3-8B, and Qwen314B models. We use Platinumbench (Vendrow et al., 2025) and the LM Eval Harness (Gao et al., 2021) for evaluation. The full results for LM Eval Harness tasks are shown in Table 2. Here, we observe that WUSH achieves the highest average recovery score among all the methods tested, often by large margins. Remarkably, on large models such as Qwen3-14B, it helps narrow the accuracy gap between MXFP4 and NVFP4 to nearly 0.5%. We plot the results for Platinumbench in Figures 1 to 4. Again, we observe that WUSH provides significant accuracy improvements, especially in the case of MXFP. Figure 1: Comparison of different transforms on Qwen3-8B for both NVFP4 and MXFP4 RTN quantization on Platinum benchmark tasks. The left table shows accuracy results across the individual Platinum benchmark tasks, while the right plot shows the average accuracy scores together with their standard deviations for each transform."
        },
        {
            "title": "4 Future Work",
            "content": "A natural next step is to integrate WUSH with advanced post-training quantization schemes such as GPTQ. In this setting, the weights are updated iteratively through error propagation, while WUSH is constructed from the second-order statistics of the updated weights. Designing an algorithm that jointly updates the weights and the associated blockwise transforms is 13 Table 2: Accuracy results on the LM Eval Harness for round-to-nearest (RTN) quantization of weights and activations with identity (I), Hadamard (H), and WUSH transforms. Model Format Transform MMLU GSM8K HellaSwag WinoGrande Avg. Recovery% 73.42 70.90 69.77 71. 67.30 68.36 69.99 80.01 77.97 77.29 78.22 73.76 75.51 77.28 75.52 74.44 73.02 74. 71.24 71.48 74.06 79.84 77.38 77.76 78.82 76.68 76.29 78.18 70.09 66.77 65.59 67. 64.56 64.09 66.54 77.90 74.43 74.35 75.47 72.61 71.90 73.56 70.56 69.53 68.11 69. 67.40 67.64 69.50 74.27 73.32 72.38 73.18 71.51 71.67 73.15 71.49 67.43 65.02 67. 62.37 62.39 65.89 78.93 75.33 74.00 75.28 69.53 71.47 73.21 77.49 75.71 74.42 76. 72.64 73.42 75.45 80.81 79.03 79.29 79.80 77.83 77.13 79.44 94.33 90.95 94. 87.25 87.27 92.17 95.44 93.75 95.37 88.08 90.54 92.75 97.70 96.03 98. 93.74 94.75 97.36 97.80 98.12 98.75 96.31 95.44 98.30 BF16 t - 3 - 2 . 3 - l c s - 8 - 1 . 3 - l BF16 NVFP4 MXFP4 NVFP4 MXFP - 64.43 78.01 WUSH WUSH 61.02 59.91 61. 56.81 55.99 58.45 71.04 64.82 71.78 60.80 61.11 68.60 - 72.76 85. WUSH WUSH 68.64 66.96 68.83 62.21 63.03 66.85 80.29 77.41 78.57 69.52 75.44 75. 8 - 3 Q 4 1 - 3 Q BF16 - 72.98 90. NVFP4 MXFP4 WUSH WUSH 70.47 70.19 70.86 67.69 67.53 70. 88.40 86.35 89.33 84.23 87.04 87.95 BF16 - 77.18 91. NVFP4 MXFP4 WUSH WUSH 75.20 74.98 75.43 72.92 73.13 74. 90.22 92.04 91.78 90.22 87.41 91.87 14 Figure 2: Comparison of different transforms on Qwen3-14B for both NVFP4 and MXFP4 RTN quantization on Platinum benchmark tasks. The left table shows accuracy results across the individual Platinum benchmark tasks, while the right plot shows the average accuracy scores together with their standard deviations for each transform. Figure 3: Comparison of different transforms on Llama3.2-3B Instruct for both NVFP4 and MXFP4 RTN quantization on Platinum benchmark tasks. The left table shows accuracy results across the individual Platinum benchmark tasks, while the right plot shows the average accuracy scores together with their standard deviations for each transform. Figure 4: Comparison of different transforms on Llama3.1-8B Instruct for both NVFP4 and MXFP4 RTN quantization on Platinum benchmark tasks. The left table shows accuracy results across the individual Platinum benchmark tasks, while the right plot shows the average accuracy scores together with their standard deviations for each transform. non-trivial and will require adapting both the optimization schedule and the calibration pipeline. 15 From systems perspective, efficient kernel support for WUSH is another important direction. Unlike blockwise Hadamard rotations, which reuse single transform across all blocks, WUSH generally assigns distinct transform to each block. This per-block specialization complicates kernel reuse and fusion in existing low-level libraries. Finally, it will be useful to explore structurally simplified variants of WUSH that trade optimality for lower computational overhead. One promising avenue is to approximate the data-aware component with diagonal matrix, thereby reducing the cost of applying the transforms at inference time."
        },
        {
            "title": "5 Conclusion",
            "content": "We have studied blockwise weight-activation quantization for LLMs in the presence of heavytailed outliers and have shown that, under mild assumptions, this problem admits closed-form linear transforms for both FP and INT block quantizers. Our construction, WUSH, combines fixed Hadamard backbone with data-aware component derived from second-order statistics, yielding non-orthogonal blockwise transform that is provably optimal and remains amenable to efficient implementation. These results clarify the empirical success of Hadamard rotations and indicate that principled, data-aware block transforms can significantly narrow the gap between low-bit and full-precision models."
        },
        {
            "title": "Acknowledgment",
            "content": "We would like to thank Tijmen Blankevoort for useful discussions and for suggesting the name WUSH, which is clear improvement over our previous name (HSUW)."
        },
        {
            "title": "References",
            "content": "Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. QUIK: Towards end-to-end 4-bit inference on generative large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 33553371, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.197. URL https://aclanthology.org/ 2024.emnlp-main.197/. 2 Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlierfree 4-bit inference in rotated llms. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 100213100240. Curran Associates, Inc., 2024b. doi: 10.52202/ 079017-3180. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ b5b939436789f76f08b9d0da5e81af7c-Paper-Conference.pdf. 1, 2, 3 16 Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip: 2bit quantization of In A. Oh, T. Naularge language models with guarantees. mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 43964429. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 0df38cd13520747e1e64e5b123a78ef8-Paper-Conference.pdf. 2, 3 Jiale Chen, Yalda Shabanzadeh, Elvir Crnčević, Torsten Hoefler, and Dan Alistarh. The geometry of llm quantization: Gptq as babais nearest plane algorithm, 2025. URL https://arxiv.org/abs/2507.18553. 2 Alexandre Défossez, Yossi Adi, and Gabriel Synnaeve. Differentiable model compression via pseudo quantization noise. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=DijnKziche. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3.int8(): 8bit matrix multiplication for transformers at scale. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3031830332. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ c3ba4962c05c49636d4c6206a97e9c8a-Paper-Conference.pdf. 1, 2 Tim Dettmers, Ruslan A. Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh. SpQR: sparse-quantized representation for near-lossless LLM weight compression. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=Q1u25ahSuy. 2 Vage Egiazarian, Roberto L. Castro, Denis Kuznedelev, Andrei Panferov, Eldar Kurtic, Shubhra Pandit, Alexandre Marques, Mark Kurtz, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Bridging the gap between promise and performance for microscaling fp4 quantization, 2025. URL https://arxiv.org/abs/2509.23202. 2, 3 Elias Frantar and Dan Alistarh. Optimal brain compression: framework for In S. Koyejo, S. Mohamed, accurate post-training quantization and pruning. A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 44754488. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ 1caf09c9f4e6b0150b06a07e77f2710c-Paper-Conference.pdf. 2 Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate In The Eleventh International quantization for generative pre-trained transformers. Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=tcbBPnfwxS. 1, Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. framework for few-shot language model evaluation. Zenodo, 2021. 13 17 Herve Jegou, Matthijs Douze, and Cordelia Schmid. Hamming embedding and weak geometric consistency for large scale image search. In European conference on computer vision, pp. 304317. Springer, 2008. 2 Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1): 117128, 2010. 2 Eldar Kurtic, Alexandre Noll Marques, Shubhra Pandit, Mark Kurtz, and Dan Alistarh. give me bf16 or give me death? accuracy-performance trade-offs in llm quantization. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2687226886, 2025. 1 Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. In P. Gibbons, G. Pekhimenko, and C. De Sa (eds.), Proceedings of Machine Learning and Systems, volume 6, pp. 87100, 2024. URL https://proceedings.mlsys.org/paper_files/paper/2024/file/ 42a452cbafa9dd64e9ba4aa95cc1ef21-Paper-Conference.pdf. 1, Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: LLM quantization with learned rotations. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=ogO6DGE6FZ. 2, 3 Open Compute Project Foundation MX Alliance. OCP Microscaling Formats (MX) Specification Version 1.0. Technical specification, Open Compute Project Foundation, September 2023. URL https://www.opencompute.org/documents/ ocp-microscaling-formats-mx-v1-0-spec-final-pdf. 3 NVIDIA. Nvidia blackwell architecture technical brief. Technical report, NVIDIA, 2024. URL https://resources.nvidia.com/en-us-blackwell-architecture. 3 Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: Decanting the web for the finest text data at scale. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 3081130849. Curran Associates, Inc., 2024. doi: 10.52202/079017-0970. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 370df50ccfdf8bde18f8f9c2d9151bda-Paper-Datasets_and_Benchmarks_Track.pdf. Yuantian Shao, Peisong Wang, Yuanteng Chen, Chang Xu, Zhihui Wei, and Jian Cheng. Block rotation is all you need for mxfp4 quantization, 2025. URL https://arxiv.org/ abs/2511.04214. 3 18 Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, et al. Flatquant: Flatness matters for llm quantization. arXiv preprint arXiv:2410.09426, 2024. 2 Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. QuIP#: Even better LLM quantization with hadamard incoherence and lattice codebooks. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 4863048656. PMLR, 2127 Jul 2024a. URL https://proceedings.mlr.press/v235/ tseng24a.html. 2, 3 Albert Tseng, Qingyao Sun, David Hou, and Christopher De. Qtip: Quantization with trellises and incoherence processing. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 5959759620. Curran Associates, Inc., 2024b. doi: 10.52202/ 079017-1904. URL https://proceedings.neurips.cc/paper_files/paper/2024/file/ 6de2e84b8da47bb2eb5e2ac96c63d2b0-Paper-Conference.pdf. 3 Joshua Vendrow, Edward Vendrow, Sara Beery, and Aleksander Madry. Do large language model benchmarks test reliability? arXiv preprint arXiv:2502.03461, 2025. 13 Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. SmoothQuant: Accurate and efficient post-training quantization for large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3808738099. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/xiao23c.html. 1, 3 19 Appendix A. Maximum Inequalities for Tail-Bounded Distributions Lemma 5 (Expectation from the Survival Function) Let 0 be random variable with the density function and the survival function S(t) = (X t) with 0. Let : [0, ) be integrable and define (t) = (cid:82) 0 (u) du. Assume that limt (t) (t) = 0. Then EG (X) = (cid:82) 0 (t) (t) dt. Proof Since has density and takes values in [0, ), EG (X) = (cid:82) survival function satisfies (t) = (X t) = (cid:82) everywhere with (t) = (t). Hence, EG (X) = (cid:82) We now integrate by parts, using (t) = (t), 0 (t) (t) dt. The (u)du, so is differentiable almost 0 (t) (t) dt. 0 (t) (t) dt = (cid:82) (cid:90) (t) (t) dt = [G (t) (t)] 0 + (cid:90) 0 (t) (t) dt. (26) By definition, (0) = 0, and by assumption limt (t) (t) = 0, the boundary term [G (t) (t)] 0 = 0. Therefore, EG (X) = (cid:82) 0 (t) (t) dt. A.1 Gaussian Distribution Definition of sub-Gaussian. probability distribution of random variable is called sub-Gaussian with variance proxy σ2 if exp ((X EX) t) exp (cid:0)21σ2t2(cid:1) for all R, and the smallest such σ2 is called the optimal variance proxy. Basic properties of sub-Gaussian distributions. The variance proxy is larger than or equal to the variance: σ2 (X EX)2. For Gaussian distribution, the optimal variance proxy is the same as the variance. Chernoff bound on the tail: (X EX t) 2 exp (cid:0)21σ2t2(cid:1) for all 0. Maximum inequality. Let X1, . . . , Xd be zero-mean Gaussian random variables with variance σ2 = maxk σ2 σ2 . Denote as the smallest variance proxy for X1, . . . , Xd. Using Lemma 5, (cid:32) (cid:91) = maxk EX 2 1, . . . , σ2 (cid:90) (cid:33) (cid:19) (cid:18)"
        },
        {
            "title": "E max\nk",
            "content": "X 2 = 2 max Xk dt = 2 {Xk t} dt 0 k=1 (27) (cid:90) 0 (cid:90) (cid:33) (Xk t) dt (cid:32) (cid:88) k=1 (cid:18) max (cid:19) (Xk t) dt 2 2 (cid:90) 0 (cid:90) 0 (cid:90) 0 1 dt + (cid:90) (cid:90) dt + 2d dt + 4d = + 4dσ2 exp (cid:0)21σ2 t2 (cid:1) . 20 exp (cid:0)21σ2 t2(cid:1) dt = (cid:2)t2(cid:3)t 0 + 4d (cid:2)σ2 exp (cid:0)21σ2 t2(cid:1)(cid:3) Splitting at = σ (cid:112)2 ln (2d) gives max 2 2σ2 ln (2d) + 2σ2 = (2 ln (2d) + 2) max EX 2 (28) without needing the independence of Xk. Also, considering the trivial bound maxk 2 (cid:80)d min {d, (2 ln (2d) + 2)} maxk EX 2 maxk EX 2 , then maxk = (cid:80)d k=1 2 EX 2 k=1 . A.2 Laplacian Distribution For random variable Laplace (µ, b) and 0, (X µ t) = 2 (cid:90) 21b1 exp (cid:0)b1x(cid:1) dx = exp (cid:0)b1t(cid:1) . (29) The variance (X EX)2 = 2b2. Let X1, . . . , Xd be zero-mean Laplacian random variables with diversity b1, . . . , bd. Denote t(cid:1). Using = maxk bk = Lemma 5, . For 0, (Xk t) = exp (cid:0)b1 t(cid:1) exp (cid:0)b1 2 maxk EX 2 (cid:113) (cid:90) (cid:18) 0 (cid:90) (cid:32) (cid:88) k=1 (cid:18) max k"
        },
        {
            "title": "E max\nk",
            "content": "X 2 = 2 (cid:90) 0 (cid:90) 0 (cid:90) 2 2 0 1 dt + 2 (cid:90) (cid:90) dt + 2d dt + 2d = t2 + 2db (t + b) exp (cid:0)b1 (cid:19) max Xk dt = 2 (cid:90) 0 (cid:32) (cid:91) k=1 (cid:33) {Xk t} dt (cid:33) (Xk t) dt (Xk t) (cid:19) dt (30) exp (cid:0)b1 t(cid:1) dt = (cid:2)t2(cid:3)t (cid:1) . 0 + 2d (cid:2)b (t + b) exp (cid:0)b1 t(cid:1)(cid:3) Splitting at = ln gives"
        },
        {
            "title": "E max\nk",
            "content": "X 2 b2 (cid:16) (cid:17) (ln d)2 + 2 ln + 2 = (cid:16) 21 (ln d)2 + ln + 1 (cid:17) max EX 2 (31) without needing the independence of Xk."
        }
    ],
    "affiliations": [
        "ETH Zürich",
        "Institute of Science and Technology Austria (ISTA)",
        "Red Hat AI"
    ]
}