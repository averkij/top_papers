{
    "paper_title": "Monet: Reasoning in Latent Visual Space Beyond Images and Language",
    "authors": [
        "Qixun Wang",
        "Yang Shi",
        "Yifei Wang",
        "Yuanxing Zhang",
        "Pengfei Wan",
        "Kun Gai",
        "Xianghua Ying",
        "Yisen Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "\"Thinking with images\" has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, a training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoning: high computational cost in latent-vision alignment and insufficient supervision over latent embeddings, and address them with a three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal a limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), a reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, a high-quality text-image interleaved CoT dataset containing 125K real-world, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong out-of-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/Monet."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 5 9 3 1 2 . 1 1 5 2 : r Monet: Reasoning in Latent Visual Space Beyond Images and Language Qixun Wang1 Yang Shi1,2 Yifei Wang3* Yuanxing Zhang2 Pengfei Wan2 Kun Gai2 Xianghua Ying1 Yisen Wang1 1Peking University 2Kling Team 3Amazon AGI SF Lab"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Thinking with images has emerged as an effective paradigm for advancing visual reasoning, extending beyond text-only chains of thought by injecting visual evidence into intermediate reasoning steps. However, existing methods fall short of human-like abstract visual thinking, as their flexibility is fundamentally limited by external tools. In this work, we introduce Monet, training framework that enables multimodal large language models (MLLMs) to reason directly within the latent visual space by generating continuous embeddings that function as intermediate visual thoughts. We identify two core challenges in training MLLMs for latent visual reasoninghigh computational cost in latentvision alignment and insufficient supervision over latent embeddings, and address them with three-stage distillation-based supervised fine-tuning (SFT) pipeline. We further reveal limitation of applying GRPO to latent reasoning: it primarily enhances text-based reasoning rather than latent reasoning. To overcome this, we propose VLPO (Visual-latent Policy Optimization), reinforcement learning method that explicitly incorporates latent embeddings into policy gradient updates. To support SFT, we construct Monet-SFT-125K, high-quality textimage interleaved CoT dataset containing 125K realworld, chart, OCR, and geometry CoTs. Our model, Monet-7B, shows consistent gains across real-world perception and reasoning benchmarks and exhibits strong outof-distribution generalization on challenging abstract visual reasoning tasks. We also empirically analyze the role of each training component and discuss our early unsuccessful attempts, providing insights for future developments in visual latent reasoning. Our model, data, and code are available at https://github.com/NOVAglow646/ Monet. Recent work [10, 15, 25, 29, 49] has demonstrated that the incorporation of auxiliary images in the intermediate steps of chain-of-thought (CoT) can improve the visual reasoning of multimodal large language models (MLLMs) [1, 5, 8, 17, 30, 38]. recent trend is to acquire auxiliary images by training MLLMs to predict key region coordinates [10, 12, 32, 49], invoking visual tools such as grounding or depth estimation models [15, 25, 33], or generating executable codes [46, 47] to modify the input image. While promising, these methods are constrained by limited set of external tools, lacking the flexible, human-like visual reasoning that arises within an internal perceptual space. To emulate the flexible visual reasoning of humans, we investigate training MLLMs to reason directly in the continuous latent space. Specifically, we enable MLLMs to generate latent embeddings that serve as intermediate visual thoughts beyond textual descriptions and image embeddings, thus eliminating the need for explicit auxiliary images and overcoming the rigidity of external tools. Recent studies have begun exploring latent visual reasoning strategies [20, 23, 42]. Most existing methods simply align the generated embeddings with those of auxiliary images while applying next-token prediction (NTP) loss on text tokens during supervised fine-tuning (SFT), and directly apply GRPO [27] for reinforcement learning (RL), which leads to two key limitations: (1) poor scalability, as alignment over thousands of image tokens incurs high computational and memory costs, and using mean pooling to compress the image tokens [42] will distort detailed visual features; (2) insufficient optimization of latent embeddings, since the NTP objective in SFT can be easily overfit and the GRPO loss can only be computed for text tokens, the optimization of latent embeddings are ignored; Consequently, their improvements remain limited and task-specific. Motivated by these observations, we propose Monet1, novel training framework for multimodal latent reasoning that trains text-output MLLM (Qwen2.5-VL-7B [1]) *The work was done at MIT prior to joining Amazon. Corresponding Author 1The name Monet is derived from the great Impressionist artist, as our method enables MLLMs to think abstractly in the latent visual space. to perform latent reasoning through SFT and RL. The SFT stage aims at equipping the model with the fundamental ability to generate and reason with latent embeddings. To tackle limitation (1), instead of directly aligning latent embeddings with those of auxiliary images, we introduce dual supervision signals. First, since latent embeddings are intended to facilitate reasoning, we align the hidden representations of text tokens corresponding to crucial visual features observed from the auxiliary images, when conditioned on either auxiliary images or generated latent embeddings. Second, to preserve visual information, we use controlled attention mask so that latent embeddings can directly attend to auxiliary image embeddings. To overcome limitation (2), we optimize the alignment loss solely through latent embeddings by stopping gradients on non-latent representations. Furthermore, we propose Viusal-latent Policy Optimization (VLPO), which computes policy gradient directly for latent embeddings by estimating their output probability. Our contributions are summarized as follows: 1. We propose Monet-SFT, three-stage supervised finetuning framework that trains MLLMs to generate and reason with latent embeddings. 2. We propose VLPO, novel RL algorithm tailored for latent reasoning. Unlike GRPO, which targets text reasoning, VLPO incorporates latent embeddings into the total loss by computing an approximate probability for the latent embeddings collected during rollout. 3. We identify the limitations of existing image-text interleaved datasets: the unnecessary usage and inaccuracy of auxiliary images. To address these limitations, we further propose multi-stage data curation pipeline to construct Monet-SFT-152K, high-quality dataset with image-text interleaved chain-of-thoughts (CoT) for SFT. 4. Extensive experiments on real-world perception and reasoning benchmarks show that Monet-SFT and VLPO outperform conventional SFT + GRPO, cropping-based methods, and prior latent visual reasoning approaches. VLPO further enhances out-of-distribution (OOD) generalization on unseen tasks. 2. Related Work 2.1. Think with Images These studies can be broadly divided into two categories based on how they obtain intermediate-step auxiliary images: (1) directly emphasizing visual content from the original image, such as grounding, cropping, or re-inputting selected image tokens [3, 7, 25, 34, 44, 49]; and (2) creating new visual content beyond the original image, such as invoking external tools or code interpreters to edit the image (e.g., drawing lines, adding bounding boxes, computing depth maps) [4, 12, 15, 24, 33, 46, 47, 50], or generating new images via text-to-image models [6, 21, 41]. While these approaches significantly enhance visual perception and reasoning, they also introduce key limitations. First, models trained for specific visual tools, such as bounding box prediction, struggle to generalize to tasks requiring more complex visual operations (e.g., visual math, spatial, or graphic reasoning). Second, tool-dependent reasoning increases the cognitive burden for training. Models often fail to generate valid tool calls or executable code without extensive supervision. Finally, reliance on external tools or interpreters necessitates asynchronous, multi-turn inference, complicating deployment and increasing latency. 2.2. Reasoning in Latent Space Recent studies have explored training LLMs to reason with continuous latent embeddingseither by replacing discrete text tokens with self-generated continuous embeddings [2, 13, 14, 28, 36, 39] or by using reweighted combinations of text embeddings [23, 42]. This line of research aims to enhance reasoning flexibility by enabling exploration in continuous latent space rather than discrete linguistic space, while also shortening reasoning chains. Building on this direction, several works have extended latent reasoning to MLLMs. Li et al. [20], Yang et al. [42] propose to align generated latent embeddings with those of auxiliary images. Yang et al. [42] further compresses image embeddings with mean pooling before alignment, which may potentially distort visual information, while Li et al. [20] focuses only on cropped image regions, which cannot encode visual operations over the entire image. Pham and Ngo [23] removes auxiliary images entirely, optimizing latent embeddings via next-token prediction, but with limited gains. Moreover, these methods neglect latent-space optimization during RL. 3. Method 3.1. Overview Inference. As illustrated in Figure 1 (left), given question and its corresponding image, Monet generates textlatent interleaved reasoning chains. Specifically, it can automatically decide when to generate special token, <latent>, to initiate latent reasoning during inference. We modify the decoding process so that the representation from the last layer of the MLLM decoder is fed back as the next input embedding. After generating predefined number of latent embeddings, we insert special stop token, </latent>, prompting the model to switch back to language-based reasoning. Consistent with the findings of Li et al. [20], this fixed-length decoding strategy proves simple yet effective. We provide comprehensive analysis of the effect of in Section 4.4. Training. The training of Monet includes three-stage SFT and an RL stage. We briefly introduce the process and Figure 1. Method overview. Left: During inference, Monet can automatically decide when to start latent reasoning by outputting special start embedding. We fix the output length of the latent embeddings. Right: We propose three-stage SFT (Section 3.3) and RL (Section 3.4) framework. The SFT stages progressively warm up the model, generate high-quality latent embeddings, and distill latent reasoning ability. The RL stage further refines the model using our VLPO algorithm, specifically designed for latent reasoning. goal of each stage to provide an overview (Figure 1 right). 1. SFT Stage 1. Adapts the model to interleaved reasoning patterns. The warmed-up parameters Mwarm-up are used to initialize the teacher and student models in Stage 2. 2. SFT Stage 2. Generates high-quality target latent embeddings latent for Stage 3. The student model learns to produce these embeddings by: (2) aligning its key observational token representations with those of the fixed teacher model; and (2) making latent embeddings directly attend to auxiliary image embeddings through controlled attention flow. 3. SFT Stage 3. Trains the model to generate latent embeddings without access to ground-truth auxiliary images. Initialized with Mwarm-up, the model aligns the generated latent embeddings with the target embeddings hlatent generated in Stage 2, resulting in the model MSFT. 4. RL Stage. Further optimizes MSFT using VLPO. In the following sections, we will elaborate on the design of each component. 3.2. SFT Training Dataset Construction To train Monet, we require dataset with imagetext interleaved CoTs that provide supervision for intermediate latent thinking steps. However, existing datasets exhibit three major limitations: (1) Many samples can be trivially solved by directly observing the input image, leading the model to bypass learning meaningful feature in the intermediate images. (2) The intermediate images are sometimes inaccurate, introducing noise into training. (3) All text tokens are treated equally, overlooking those that describe crucial visual information, which could serve as useful supervision signals [11]. To overcome these issues, we design threestage data curation pipeline that ensures the necessity and correctness of auxiliary images (Figure 2). Stage 1: We collect raw text-image interleaved CoT data from ReFocus [12], CogCoM [24], Zebra-CoT [18], and Visual-CoT [26], and retain samples that Qwen2.5-VL-7B [1] incorrectly answers using only the question and the original image to ensure the necessity of auxiliary images. Stage 2: From these samples, we keep those that Qwen2.5VL-72B can correctly solve using only the auxiliary images, ensuring that the auxiliary images are accurate for reasoning. Stage 3: We use Deepseek-V3.1 [22] and Gemini 2.5 Pro [8] to identify text tokens corresponding to crucial visual observations for answering the final question to provide fine-grained supervision for learning latent embeddings. These steps yield Monet-SFT-125k, in which the auxiliary images include cropping, grounding, highlighting, and operations that create new visual references for finegrained perception and complex visual reasoning tasks. Table 1 presents the statistics of Monet-SFT-125k. 3.3. Supervised Fine-tuning The SFT stage addresses two key challenges in latent reasoning training of MLLMs: (1) Latentvisual alignment is computationally expensive, as auxiliary images contain hundreds or thousands of image embeddings. (2) The next-token-prediction objective provides weak supervision for latent embeddings. During SFT, current methods [20, 23, 42] employ cross-entropy loss on text tokens following the latent embeddings, allowing gradients to flow back to the latent embeddings. However, the model can simply memorize the following tokens instead of learning effective latent representations. To overcome these issues, we introduce three-stage SFT framework (Figure 3) that enables the model to selectively encode useful visual features from auxiliary images while focusing on the downstream reasoning process, without incurring the cost of explicit latentvisual alignment. We now describe the design of each stage in detail. 3.3.1. SFT Stage 1: Warm up. We first warm up the base model Mbase (Qwen2.5-VL-7B) by performing vanilla SFT on the image-text interleaved Figure 2. Construction pipeline of Monet-SFT-125K. Stage 1 filters hard samples (unsolvable from the original image). Stage 2 keeps those where auxiliary images lead to correct answers, ensuring their necessity and correctness. Stage 3 highlights key visual-observation tokens using advanced LLM judges, providing strong supervision for learning latent embeddings. Table 1. Statistics of the Monet-SFT-125K dataset. It contains 125K real-world, document, chart, and geometry problems with imagetext interleaved CoTs, featuring visual operations ranging from simple extractive steps such as cropping and grounding to more complex ones, including drawing auxiliary lines and creating entirely new images that represent intermediate visual states. Problem Domain Data Source Chart ReFocus [12] Real-world, chart CogCoM [24] Visual-CoT [26] Real-world, documents, chart Cropping, drawing bounding boxes Zebra-CoT visual search [18] Real-world, documents, chart Cropping, drawing bounding boxes Zebra-CoT geometry [18] Zebra-CoT count [18] Geometry 3D object counting Visual Operation Type Drawing bounding boxes, highlighting Cropping, drawing auxiliary lines and bounding boxes Drawing auxiliary lines, creating new geometric sketchpads Creating new images with 3D objects removed or added Amount 0.4K 0.5K 118.6K 2.7K 0.1 2.9K CoTs of Monet-SFT-125K, obtaining Mwarm-up (see Figure 3 upper left). This stage adapts the model to the imagetext interleaved reasoning pattern so it can effectively leverage intermediate-step images when predicting subsequent tokens. Without this adaptation, the model tends to ignore auxiliary images, and the representations of observation tokens may fail to capture sufficient visual information. Figure 4 illustrates this behavior, in which we compare the token prediction accuracy of the observation tokens with/without the auxiliary images during this warm-up stage: for the unadapted base model, using auxiliary images yields almost no improvement in predicting observation tokens, indicating poor utilization of intermediate visual cues. As the warm-up proceeds, however, the accuracy gain from auxiliary images gradually increases. This demonstrates that SFT on imagetext interleaved data drives the model to rely more on intermediate-step visual features rather than memorizing language patterns. Consequently, observation tokens encode meaningful visual information from the auxiliary images and serve as effective supervision signals. 3.3.2. SFT Stage 2: Obtain high-quality target latent embeddings. In this stage, we train the model to generate latent embeddings that capture useful visual features from auxiliary images to support visual reasoning. We initialize both teacher and student model from Mwarm-up. The teacher processes CoTs with ground-truth auxiliary images, while in the student CoT, each auxiliary image segment is followed by autoregressively-generated latent embeddings, and the auxiliary images are made visible only to these latent embeddings via modified attention mask (Figure 3 right). Now we elaborate on our key designs as follows. Alignment on Key Observation Tokens. Since latent embeddings are intended to serve the role of auxiliary images in predicting observation tokens, the hidden representations of these tokens should match those obtained when the ground-truth auxiliary images are provided. Inspired by recent textbased latent reasoning work [28, 36], we therefore align the hidden representations of observation tokens under these two conditions. Specifically, obs = {h(i,l) for each training sample, we freeze Mwarm-up and extract the observation-token representations layers when using auxiliary images, yielding from all i=1, where indexes samples and indexes layers, is the number of training samples. Let ˆh(i,l) obs denote the corresponding representations in the student CoT that uses generated latent embeddings. We fix h(i,l) obs and obs }N Figure 3. The proposed three-stage SFT pipeline: warm-up, supervised latentobservation alignment with controlled attention flow, and latent generation without auxiliary-image access. iary images. To address this, for the student CoT, we insert the auxiliary image embeddings immediately before each latent-embedding segment and apply modified attention mask that allows these image embeddings to be attended only by the latent embeddings, not by subsequent text tokens. This design offers two benefits: first, latent embeddings can directly access the visual features from auxiliary images without information loss; second, it enforces structured flow of visual information, i.e., auxiliary imageslatent embeddingsobservation tokens, encouraging the latent embeddings to selectively encode the relevant visual cues. Next-Token Prediction Loss. We Additionally apply standard next-token prediction loss on text tokens to optimize the latent embeddings. Denote the text tokens in student CoT as (i) = {yt}T (i) t=1 . For each yt, we use y<t to represent the context, which includes the question text, question image, previous text responses, and latent embeddings (note that the auxiliary images are invisible to the text tokens under our attention design). Denote the output probability distribution as p, the next-token-prediction loss is: LNTP = 1 (cid:88) (cid:88) ytT (i) log p(yt y<t). (2) Total Loss of SFT Stage 2. The total loss is: Lstage2 = LNTP + αLalign-obs, (3) in which we set α = 2.0 in all experiments. We denote the student model after training as Mstage2. After this stage, we use Mstage2 to generate the latent embeddings, denoted as h(i) latent, which serve as target latent embeddings used for the next stage. Figure 4. Prediction accuracy of the observation tokens during warm-up. Training on imagetext interleaved data encourages the model to utilize intermediate visual cues. maximize their cosine similarity through the alignment loss: Lalign-obs = 1 (cid:88) (cid:88) (cid:16) 1 cos(h(i,l) (cid:17) obs . detach(), ˆh(i,l) obs ) . l (1) To ensure that the observation alignment loss Lalign-obs can only be minimized by optimizing the latent embeddings instead of through any undesired shortcuts, we restrict gradient flow from Lalign-obs to pass solely through the generated latent embeddings to the model parameters. Implementation details are in the supplementary material. Empirically, removing this latent-only backpropagation leads to significant performance degradation (see Table 2, w/o latent-only BP). Auxiliary image latent observation Attention Flow. We observe that using the above alignment loss alone yields suboptimal performance (Table 2, w/o auxiliary img), as the observation-token representations may not encode sufficient visual information from the auxilTable 2. Ablation of the components of the SFT Stage 2. Monet-SFT denotes the full SFT pipeline. w/o latent-only BP represents without latent-only backpropagation for the alignment loss. w/o auxiliary img disables attention from latent embeddings to auxiliary-image embeddings in the student CoT. Both ablation lead to performance degradation on almost all tasks. V* HR4K HR8K MME-RealWorld VisualPuzzles Monet-SFT w/o latent-only BP w/o auxiliary img 82.20 46.07 73.30 68.50 40.13 63.88 66.00 39.00 57.63 55.03 38.67 39. 30.48 33.65 28.60 3.3.3. SFT Stage 3: Learn to generate latent embeddings without auxiliary images. After Stage 2, we obtain target latent embeddings h(i) latent that encode the information contained in auxiliary images. However, the model Mstage2 is trained in setting where latent embeddings can still attend to auxiliary images, which is different from our ultimate goal of generating latent embeddings without ground-truth auxiliary images. To close this gap, we reinitialize the model with Mwarm-up and train it to produce latent embeddings ˆh(i) latent with auxiliary images removed in the CoT (Figure 3 bottom left). We then align them with the fixed target embeddings h(i) latent using the following loss: Lalign-latent = 1 (cid:88) (cid:88) (cid:16) 1 cos(h(i,l) latent. detach(), ˆh(i,l) latent) (cid:17) . (4) Unlike previous latent-visual reasoning work [20, 42], which aligns only the final-layer representations, we align all layers to provide stronger supervision. We also apply next-token-prediction loss LN on the text tokens to let the latent embeddings benefit subsequent reasoning, which has the same form as Equation (2). Total loss of SFT Stage 3. The total loss of Stage 3 is: by maximizing: JGRP O(θ) = EQ,I,oπold oi (cid:88) (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "1 oi i=1 t=1 (cid:104) ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ε, 1 + ε) ˆAi,t (cid:105) min β KL (πθπref) , where ri,t(θ) = πθ(oi,tQ,I,oi,<t) (6) πθold (oi,tQ,I,oi,<t) , oi,<t are the response tokens before position t, ˆAi,t is the advantage computed based on the outcome rewards {r1, r2, ..., rG} of the responses, i.e., ˆAi,t = rimean({r1,r2,...,rG}) . πref is the reference model. critical limitation is that the GRPO objective can only be computed on text tokens. Since latent embeddings have no explicit probability distribution like text tokens, GRPO cannot directly optimize them, leaving the latent reasoning component largely untrained during RL. std({r1,r2,...,rG}) To address this limitation, we propose Visuallatent Policy Optimization (VLPO), novel reinforcement learning objective tailored for latent reasoning. The key idea of VLPO is to estimate the output probability of the continuous latent embeddings collected during rollout to compute ri,t(θ) for the latent embeddings. This enables latent embeddings to be optimized directly with outcome rewards, just like text tokens. Denote the latent embedding generated by πold at position of the i-th rollout as hold i,t with context [Q, I, oi,<t]. To compute πθ(hold i,t Q, I, oi,<t), we model hold i,t as sample drawn from latent Gaussian distribution whose mean is the latent embedding generated by πθ given the same context. We denote this mean by hθ i,t. Under this assumption, πθ(hold i,t Q, I, oi,<t) can be computed as: (cid:18) 1 2σ2 hold πθ(hold i,t hθ i,t Q, I, oi,<t) = exp i,t2 const (7) where σ is predefined scalar hyperparameter and const is constant. πθ(olatent Q, I, oi,<t) measures the probability that πθ chooses olatent as the action at position of response i, enabling us to compute ri,t(θ) for latent step: i,t i,t (cid:19) Lstage3 = LNTP + βLalign-latent. (5) ri,t(θ) = in which we set β = 2.0 in all our experiments. πθ πθold (cid:0)hold i,t Q, I, oi,<t (cid:0)hθ i,t Q, I, oi,<t (cid:1) (cid:1) = exp (cid:18) 1 2σ2 hold i,t hθ i,t2 (cid:19) (8) Replacing the original ri,t(θ) for latent steps yields our VLPO objective. During optimization, we fix the rollout rollout hold i,t generated by the policy. i,t and optimize hθ 3.4. VLPO: Visual-latent Policy Optimization Limitations of GRPO on latent visual reasoning. Prior latent visual reasoning works [20, 42] apply GRPO [27] after SFT. Given question and question image I, the old policy model πold generates group of responses {o1, o2, ..., oG}. Then GRPO updates the policy model πθ Reward Design. We use an accuracy reward (1 for correct answer; 0 otherwise) and format reward encouraging the model to place the final answer in boxed{}. Importantly, we do not reward latent-reasoning behavior itself, as doing so would incentivize the model to invoke latent reasoning indiscriminately. We find this simple reward scheme yields strong empirical performance. How does VLPO work? When ˆAi,t > 0, maximizing the VLPO objective will increase ri,t(θ). For text tokens, this means maximizing its output probability; For latent embeddings, Equation (8) shows that maximizing this ri,t(θ) i,t2, thereby pulling the i,t hθ equals to minimizing hold policy latent embeddings to good-action latent embedding hθ i,t that led to positive outcome reward. Thus, VLPO directly optimizes latent embeddings using reward signalsan ability that GRPO fundamentally lacks. 4. Experiment 4.1. Experiment Setup Training and Evaluation Setup. In SFT Stage 1, we train the model for 4 epochs. For SFT stage 2 and 3, we train 1000 steps (about 1 epoch). For RL training, we adopt 3.2K subset of Thyme-RL [46] and train for 1 epoch. For Monet-SFT model, we fix the training time latent size to 8; For Monet-7B (Monet-SFT + VLPO), we use training latent size of 10. At test time, we select the best latent size from {8, 10, 12, 16}. The complete training and experimental setups are provided in the supplementary material. Evaluated Benchmarks. We adopt the VLMEvalKit [9] framework for fair evaluation. We evaluate Monet on V* [40], HRBench4k [37], HRBench8K [37], MMERealWorld [45], covering fine-grained perception and reasoning tasks on real-world, chart and OCR data. To evaluate OOD capabilities, we also evaluate on VisualPuzzles [31], which includes visual logical puzzles that demands abstract visual reasoning abilities rather than prior knowledge. Baselines. We compare Monet against the following (1) Qwen2.5-VL-7B, the base model to train baselines: Monet-7B; (2) Vanilla SFT, which directly train Qwen2.5VL-7B with Monet-SFT-125K; (3) Vanilla SFT + GRPO, which applies GRPO after Vanilla SFT; (4) Deepeyes [49], representative think with images approach that performs cropping to improve perception; (5) LVR [20], recent work on latent visual reasoning that aligns the latent and auxiliary image embeddings; (6) other high-performance MLLMs, including Gemini 2.5 Pro [8], GPT-4o [16], Pangea-7B [43], and LLaVA-OneVision-72B [19]. 4.2. Main Results From Table 3 and Tabel 4, we can see that: (1) Monet substantially enhances both perception and reasoning on real-world, chart, and OCR tasks, consistently outperforming the baselines. It improves Qwen2.5-VL-7B by 4.25%9.75%, surpasses both Vanilla SFT and SFT + GRPO trained on the same data, and outperforms Deepeyes and LVR on most benchmarks. (2) Monet demonstrates strong OOD generalization. It achieves the best performance on VisualPuzzles, which mainly contains abstract visual reasoning problems that are unseen during training. 4.3. Analysis of the Role of the Components Design In this section, we present ablation studies to assess the necessity and effectiveness of each component in our training framework  (Table 5)  . We also discuss several unsuccessful attempts from our early exploration of training objectives and pipeline designs, which we hope can provide insights for future advancements in latent reasoning for MLLMs. Takeaway 1. Single-signal supervision is less effective. We remove the representation-alignment loss on observation tokens in SFT Stage 2 while retaining auxiliary-image embeddings in the student CoT under the imagelatentobservation attention flow, which reflects our initial attempt. As shown by A-SFT w/o obs. token align, removing the alignment loss causes substantial drop in performance. This indicates that supervision from observation tokens is crucial. Similarly, removing the auxiliary images visible to latent embeddings while using the alignment loss alone (w/o auxiliary img) also significantly degrades performance, showing the importance of visual supervision on the latent embeddings. These two ablations demonstrate the necessity of the dual supervisions we use. Takeaway 2. GRPO doesnt consistently improve Monet-SFT, whereas VLPO does. This is evident when comparing Monet-SFT with Monet-SFT + GRPO, indicating that GRPO is not well-suited for latent reasoning. In contrast, applying VLPO significantly enhances the performance of Monet-SFT. Takeaway 3. Latent-related losses must backpropagate only through the latent embeddings. w/o latentonly BP shows that when the alignment loss is allowed to update non-latent representations, performance drops sharply. This indicates that the model may otherwise exploit shortcut paths by minimizing the loss without actually improving the latent embeddings. Therefore, enforcing latent-only backpropagation is necessary. 4.4. Effect of the Number of the Latent Embeddings on Performance In this section, we analyze how training and test-time latent size (Ktrain and Ktest) affect performance. For Monet-SFT, we choose Ktrain {8, 10, 12}. For RL models, we take Monet-SFT with Ktrain = 8 and further apply VLPO (with RL Ktrain = 10) and GRPO. Results are shown in Figure 5. Takeaway 1. For in-distribution tasks, improvements indeed come from using abstract visual embeddings; while for OOD tasks, only VLPO incentivizes useful latent reasoning. Across all in-distribution tasks (V*, HRBench, MME-RealWorld), models perform worst when Ktest = 0. However, on the OOD task (VisualPuzzles), only the VLPO-enhanced model consistently benefits from latent reasoning, i.e., using Ktest > 0 induces better performances than Ktest = 0, indicating that SFT alone cannot Table 3. Performance on real-world perception and reasoning benchmarks. The best-performing open-source model for each dataset is highlighted in bold. Results marked with * are reported by other papers [20, 46], while the others are from our own evaluations. Model V* HRBench4K HRBench8K MME-RealWorld-Lite Overall Attribute Spatial Overall FSP FCP Overall FSP FCP Overall Reasoning Perception GPT-4o [16] 67.5* 72.2* 60.5* Proprietary Model 70.0* 48.0* 59.0* 55.5* 62.0* 49.0* 52.0* 48.3* 54.4* Qwen2.5-VL-7B [1] + vanilla SFT + vanilla SFT + GRPO Deepeyes [49] LVR [20] 76.44 81.68 78.53 83.25 80.6* 77.39 83.48 78.26 84.35 81.7* 75.00 78.95 78.95 81.58 79.0* Open-Source Model 55.75 80.25 58.5 78.28 56.75 83.25 58.75 83.75 - - 68.00 68.38 70.00 71.25 - 63.75 61.63 66.75 65.13 - 73.75 70.75 78.00 77.00 - 53.75 52.50 55.50 53.25 - 45.75 51.28 52.42 54.28 - 39.73 46.4 48.13 50.53 - Monet-7B (SFT + VLPO) Relative Improvement 83.25 +6.81 83.48 +6.09 82.89 +7. 71.00 +3.00 Our Model 85.25 +5.00 56.75 +1.00 68.00 +4.25 79.75 +6.00 56.25 +2. 55.50 +9.75 51.07 +11.34 49.62 54.41 55.18 56.63 - 58.34 +8.72 Figure 5. Effect of the number of abstract visual embeddings used during training and inference on test accuracy. The dashed line marks the accuracy of Qwen2.5-VL-7B. Table 4. Performance on the VisualPuzzles benchmark (out-ofdistribution). The best-performing open-source model for each dataset is highlighted in bold. Results marked with * are taken from the original VisualPuzzles [31] paper, while the others are obtained from our own evaluations. Model Gemini 2.5 Pro [8] GPT-4o [16] Qwen2.5-VL-7B + vanilla SFT + vanilla SFT + GRPO Deepeyes [49] Pangea-7B [43] LLaVA-OneVision-72B [19] Monet-7B Relative Improvement Overall Algorithmic Analogical Deductive Inductive Spatial VisualPuzzles Proprietary Models 60.0* 49.2* 64.0* 58.3* Open-Source Models 37.02 40.46 36.26 37.79 32.4* 34.7* Our Model 45.80 +6.09 21.80 30.81 25.12 27.01 23.7* 26.5* 30.81 +8.78 49.5* 41.3* 32.71 33.99 30.99 32.96 31.3* 30.8* 35.02 +2.31 60.0* 49.0* 47.50 46.00 43.50 41.00 38.5* 37.0* 47.50 +9.01 29.7* 27.3* 26.32 24.40 23.45 26.79 28.7* 27.3* 36.4* 26.2* 21.80 29.02 27.27 27.01 32.5* 28.7* 26.79 +0.47 25.52 +3.72 Table 5. Ablation of the components of Monet. Model V* HRBench4K HRBench8K MME-RealWorld-Lite VisualPuzzles Monet-7B (full) Monet-SFT (w/o VLPO) + GRPO w/o latent-only BP w/o auxiliary img w/o obs token align 83.25 82.20 80.10 46.07 73.30 75. 71.00 68.50 69.00 40.13 63.88 67.25 68.00 66.00 64.75 39.00 57.63 63.50 55.50 52.68 54.19 38.67 39.66 46.90 35.02 30.48 31.51 33.65 28.60 27.48 induce strong OOD generalization. Takeaway 2. For in-distribution tasks, Monet-SFT enables test-time scaling of latent embeddings; VLPO further extends this trend to OOD scenarios. On V*, HRBench, and MME-RealWorld, performance typically peaks at test-time latent size Ktest larger than the trainingtime one Ktrain. Before the peak, the performance improves as Ktest increases. Takeaway 3. VLPO improves robustness to the choice of Ktest. Compared with other models, Monet-SFT + VLPO exhibits the most stable performance as Ktest varies. Takeaway 4. GRPO mainly strengthens non-latent reasoning and provides limited benefits for latent reasoning. When Ktest = 0, Monet-SFT (Ktrain = 8) + GRPO is comparable to or better than Monet-SFT (Ktrain = 8), showing gains in non-latent reasoning; However, when Ktest > 0, Monet-SFT + GRPO underperforms the SFT model for most Ktest, suggesting minimal improvements to latent reasoning. This aligns with our analysis in Section 3.4: GRPO optimizes only text tokens and thus provides no direct supervision for latent embeddings. 5. Conclusion and Limitations We introduce Monet, framework that enables MLLMs to perform visual reasoning in the latent space. Our MonetSFT pipeline enables the model to selectively encode useful visual cues from auxiliary images into compact latent embeddings, while our VLPO algorithm explicitly optimizes these embeddings during RL. We further construct the Monet-SFT-125K dataset to support SFT. Experiments show that Monet substantially improves real-world perception and reasoning performance and exhibits strong OOD generalization on abstract visual reasoning tasks. Together, these results demonstrate Monet as promising direction toward more flexible and general multimodal reasoning. We summarize the limitations of our work as follows. First, Monet relies on multi-stage SFT pipeline, which may increase the overall training complexity and overhead. Second, we have not yet explored how different reward designs might influence latent visual reasoning in MLLMs, leaving room for exploration and further enhancement."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 3, 8 [2] Natasha Butt, Ariel Kwiatkowski, Ismail Labiad, Julia Kempe, and Yann Ollivier. Soft tokens, hard truths. arXiv preprint arXiv:2509.19170, 2025. 2 [3] Xinyan Chen, Renrui Zhang, Dongzhi Jiang, Aojun Zhou, Shilin Yan, Weifeng Lin, and Hongsheng Li. Mint-cot: Enabling interleaved visual tokens in mathematical chain-ofthought reasoning. arXiv preprint arXiv:2506.05331, 2025. 2 [4] Yang Chen, Yufan Shen, Wenxuan Huang, Shen Zhou, Qunshu Lin, Xinyu Cai, Zhi Yu, Botian Shi, and Yu Qiao. Learning only with images: Visual reinforcement learning with reasoning, rendering, and visual feedback. arXiv preprint arXiv:2507.20766, 2025. 2 [5] Zihui Cheng, Qiguang Chen, Xiao Xu, Jiaqi Wang, Weiyun Wang, Hao Fei, Yidong Wang, Alex Jinpeng Wang, Zhi Chen, Wanxiang Che, et al. Visual thoughts: unified perspective of understanding multimodal chain-of-thought. arXiv preprint arXiv:2505.15510, 2025. 1 [6] Ethan Chern, Zhulin Hu, Steffi Chern, Siqi Kou, Jiadi Su, Yan Ma, Zhijie Deng, and Pengfei Liu. Thinking with generated images. arXiv preprint arXiv:2505.22525, 2025. 2 [7] Jiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, and Youngjae Yu. Dont look only once: Towards multimodal interactive reasoning with selective visual revisitation. arXiv preprint arXiv:2505.18842, 2025. 2 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 1, 3, 7, [9] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM international conference on multimedia, 2024. 7, 1 [10] Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, and Xin Eric Wang. Grit: Teaching mllms to think with images. arXiv preprint arXiv:2505.15879, 2025. 1 [11] Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? In ICLR, 2025. 3 [12] Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. In ICML, 2025. 1, 2, 3, 4 [13] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. 2 [14] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 2 [15] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 2024. 1, [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7, 8 [17] Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, and Jiajun Zhang. Look again, think slowly: Enhancing visual reflection in vision-language models. arXiv preprint arXiv:2509.12132, 2025. 1 [18] Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, et al. Zebra-cot: dataset for interleaved vision language reasoning. arXiv preprint arXiv:2507.16746, 2025. 3, 4 [19] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 7, 8 [20] Bangzheng Li, Ximeng Sun, Jiang Liu, Ze Wang, Jialian Wu, Xiaodong Yu, Hao Chen, Emad Barsoum, Muhao Chen, and Zicheng Liu. Latent visual reasoning. arXiv preprint arXiv:2509.24251, 2025. 1, 2, 3, 6, 7, 8 [21] Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-ofthought. arXiv preprint arXiv:2501.07542, 2025. [22] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 3 [23] Tan-Hanh Pham and Chris Ngo. Multimodal chain of continuous thought for latent-space reasoning in vision-language models. arXiv preprint arXiv:2508.12587, 2025. 1, 2, 3 [24] Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, et al. Cogcom: visual language model with chain-ofmanipulations reasoning. In ICLR, 2025. 2, 3, 4 [25] Gabriel Sarch, Snigdha Saha, Naitik Khandelwal, Ayush Jain, Michael Tarr, Aviral Kumar, and Katerina Fragkiadaki. Grounded reinforcement learning for visual reasoning. arXiv preprint arXiv:2505.23678, 2025. 1, 2 [26] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Unleashing chain-of-thought reasoning in multi-modal language models. CoRR, 2024. 3, 4 [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, R. Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. 1, 6 [28] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. 2, [29] Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, et al. Realunify: Do unified models truly benefit from unification? comprehensive benchmark. arXiv preprint arXiv:2509.24897, 2025. 1 [30] Yang Shi, Jiaheng Liu, Yushuo Guan, Zhenhua Wu, Yuanxing Zhang, Zihao Wang, Weihong Lin, Jingyun Hua, Zekun Wang, Xinlong Chen, et al. Mavors: Multi-granularity video representation for multimodal large language model. arXiv preprint arXiv:2504.10068, 2025. 1 [31] Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, and Xiang Yue. Visualpuzzles: Decoupling multimodal reasoning evaluation from domain knowledge. arXiv preprint arXiv:2504.10342, 2025. 7, 8 [32] Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: Incentivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv preprint arXiv:2505.15966, 2025. 1 [33] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 1, 2 [34] Guangyan Sun, Mingyu Jin, Zhenting Wang, Cheng-Long Wang, Siqi Ma, Qifan Wang, Tong Geng, Ying Nian Wu, Yongfeng Zhang, and Dongfang Liu. Visual agents as fast and slow thinkers. In ICLR, 2025. [35] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouedec. Trl: Transformer reinforcement learning, 2020. 1 [36] Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, and Ziqian Zeng. Synadapt: Learning adaptive reasoning in large language models via synthetic continuous chain-of-thought. arXiv preprint arXiv:2508.00574, 2025. 2, 4 [37] Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In AAAI, 2025. 7 [38] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Internvl3. 5: Advancing open-source Ye, Jie Shao, et al. multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. 1 [39] Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Simcot: Supervised implicit chain-of-thought. arXiv preprint arXiv:2509.20317, 2025. [40] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In CVPR, 2024. 7 [41] Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025. 2 [42] Zeyuan Yang, Xueyang Yu, Delin Chen, Maohao Shen, and Chuang Gan. Machine mental imagery: Empower multimodal reasoning with latent visual tokens. arXiv preprint arXiv:2506.17218, 2025. 1, 2, 3, 6 [43] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: fully open multilingual multimodal llm for 39 languages. In ICLR, 2024. 7, 8 [44] Xintong Zhang, Zhi Gao, Bofei Zhang, Pengxiang Li, Xiaowen Zhang, Yang Liu, Tao Yuan, Yuwei Wu, Yunde Jia, Song-Chun Zhu, et al. Chain-of-focus: Adaptive visual search and zooming for multimodal reasoning via rl. arXiv preprint arXiv:2505.15436, 2025. 2 [45] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world arXiv preprint scenarios that are difficult for humans? arXiv:2408.13257, 2024. 7 [46] Yi-Fan Zhang, Xingyu Lu, Shukang Yin, Chaoyou Fu, Wei Chen, Xiao Hu, Bin Wen, Kaiyu Jiang, Changyi Liu, Tianke Zhang, et al. Thyme: Think beyond images. arXiv preprint arXiv:2508.11630, 2025. 1, 2, 7, 8 [47] Shitian Zhao, Haoquan Zhang, Shaoheng Lin, Ming Li, Pyvision: arXiv preprint Qilong Wu, Kaipeng Zhang, and Chen Wei. Agentic vision with dynamic tooling. arXiv:2507.07998, 2025. 1, 2 [48] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework, 2025. 1 [49] Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. DeepIncentivizing thinking with images via reinforceeyes: ment learning. arXiv preprint arXiv:2505.14362, 2025. 1, 2, 7, 8 [50] Zetong Zhou, Dongping Chen, Zixian Ma, Zhihan Hu, Mingyang Fu, Sinan Wang, Yao Wan, Zhou Zhao, and Ranjay Krishna. Reinforced visual perception with tools. arXiv preprint arXiv:2509.01656, 2025. 2 Monet: Reasoning in Latent Visual Space Beyond Images and Language"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Experimental Results We present the complete results on MME-RealWorld-Lite in Table 6. This benchmark spans wide range of task domains, such as monitoring, autonomous driving, and complex diagram data. Compared with baseline approaches, Monet achieves consistently strong performance across all data sources, demonstrating its generality in both visual reasoning and perception tasks. B. Implementation Details B.1. SFT Training latent-start We use TRL [35] to implement our SFT training. In training CoT, we allow the model to predict the spetoken <latent> while masking the cial latent-end token </latent>, the observation-start token <observation>, and the observation-end token </observation> in the SFT labels. Hyperparameters. We list the training hyperparameters of SFT in Table 7. To avoid OOM, we use batch size of 1 and set gradient accumulation steps to 16. Additionally, we restrict the total number of pixels (denoted by max total pixels) in training sequence to prevent the OOM issue caused by too many auxiliary images. Latent only backpropagation in SFT Stage 2. To ensure that gradients flow only through the latent representations to the model parameters, we replace Lalign-obs with the following surrogate loss: align-obs = 1 (cid:88) stop grad( Lalign-obs ˆh(i,L) latent ) ˆh(i,L) latent , (9) where stop grad denotes the stop-gradient operation, ˆh(i,L) latent are the generated latent embeddings in the student CoT, and is the number of layers in the MLLMs language model. By differentiating align-obs with respect to the model parameters θ: align-obs θ = Lalign-obs ˆh(i,L) latent ˆh(i,L) latent θ , (10) we can see that minimizing mizing only the generated latent embeddings. align-obs is equivalent to optiB.2. RL Training We implement our RL training on EasyR1 [48], an opensource RL training framework for multimodal LLMs. Hyperparameters. We summarize the RL training hyperparameters in Table 8. Note that we apply an accuracy threshold to retain only samples whose group accuracy falls below this threshold but remains non-zero, preventing the advantage from vanishing when the responses in group are all correct or incorrect. B.3. Training Data Construction We provide the prompt for the API models used in the stage 3 of our training data construction pipeline as below: System Prompt for Training Data Construction to of on are rely visual helpful identify which Your the (marked assistant. parts observations task You soluis tion by <latent></latent>). Wrap these essential obserin <observation>.../<observation> vations while leaving all other text unchanged. The input contains only question (with conditions) and solution chain-of-thought text no images. Identifying observations does not require access to images. Segments are delimited by <STEP_i> ... <END_STEP_i>. Rules: 1. Do NOT remove, rename, or reorder any markers. Always output all markers exactly as given. 2. Do NOT add new content to the reasoning. Inside <observation>...</observation>, 3. include as little as possible only the key tokens that truly require image observation. 4. Do NOT put from the question text or earlier <observation>...</observation>. 5. input text unchanged. information that can be inferred reasoning into If you think theres no observation, just return the C. Detailed Experimental Setup We use the VLMEvalKit [9] framework for our evaluation. We set the inference-time maximum visual tokens to be 81922828. Below is the system prompt we used for evaluation. System Prompt for Evaluation You are an expert multimodal large language model designed to reason with latent visual embeddings. Table 6. Full result on MME-RealWorld-Lite. MME-RealWorld-Lite Reasoning Perception Qwen2.5-VL-7B-Instruct + SFT + SFT + GRPO Deepeyes Monet-SFT Monet-7B (Monet-SFT + VLPO) Overall MO 30.67 39.73 52.00 46.40 59.33 48.13 46.67 50.53 48.67 50.53 51.07 46. AD 27.00 33.00 33.75 40.25 43.00 41.50 OCR 74.00 73.00 71.00 78.00 65.00 73.00 DT 70.00 65.00 66.00 70.00 69.00 75.00 overall MO 31.03 49.62 40.44 54.41 43.26 55.18 43.89 56.63 41.69 54.06 58.34 41.07 AD 31.43 36.29 37.71 38.86 44.57 48.86 OCR 90.80 88.80 89.60 90.00 76.40 85. DT 85.00 86.00 83.00 84.00 71.00 84.00 RS 39.33 48.00 45.33 51.33 54.00 54.67 Table 7. Hyperparameters for SFT. D. Case Studies D.1. Monet-SFT-125K Examples We select three examples from Monet-SFT-125K to illustrate the data structure and corresponding visual operations. These examples cover three representative types of visual operations: cropping key regions, generating new visual states, and drawing auxiliary lines or bounding boxes. As shown in Figures 6, 7, and 8, each training instance contains an imagetext interleaved CoT with intermediate auxiliary images that facilitate the visual reasoning process. Additionally, tokens that encode key observations from the auxiliary images are wrapped in <observation> and </observation>, which are used in the alignment loss during SFT Stage 2. D.2. Inference Examples We present some inference examples generated by Monet7B in this section to show the generality of tasks that Monet-7B can solve. For clarity, we dont map the generated latent embeddings to the language space as this will produce meaningless tokens. Instead, we use <latent><latent_embeddings></latent> to represent the latent reasoning segments. The tasks of the selected examples include 3D Spatial Reasoning (Figure 9), 2D transformation (Figure 10), complex diagram reasoning (Figure 11), commonsense QA (Figrue 12), fine-grained OCR (Figure 13), and math reasoning (Figure 14). Note that Monet not always uses latent thinking. For example, for the pure-text math problem in Figure 14, and Monet directly rely text-based reasoning to solve the problem. Hyperparameter learning rate batch size gradient accumulation steps weight decay SFT Stage 1 steps SFT Stage 2 steps SFT Stage 3 steps latent size SFT Stage 1 max total pixels SFT Stage 2 max total pixels SFT Stage 3 max total pixels SFT Stage 1,2,3 max pixels per img SFT Stage 2 alignment weight SFT Stage 3 alignment weight Value 0.00001 1 16 0.01 3885 (3 epochs) 1000 (1.03 epoch) 1000 (1.03 epoch) {8, 10, 12} 20002828 15002828 20002828 20002828 2.0 2.0 Table 8. Hyperparameters for RL. Hyperparameter Value learning rate batch size weight decay rollout size temperature max response length VLPO σ SFT Stage 1 steps SFT Stage 2 steps SFT Stage 3 steps latent size max pixels per img accuracy threshold 0.000001 64 0.01 8 0.5 4096 10.0 3885 (3 epochs) 1000 (1.03 epoch) 1000 (1.03 epoch) 10 20002828 0. To ensure reliable evaluation, we first apply rule-based judge (exact match) and then employ DeepSeek-V3.1 or Gemini-2.5-Pro as secondary judges. Monet-SFT-125K Example 1: Cropping the Crucial Region (from Zebra-CoT Visual Search) Question: According to the survey presented in the infographic, what percentage of parents in the U.S. delay the Varicella vaccine for their children beyond the CDCs recommendations? Put your final answer within boxed{}. Question Image: Image-text Interleaved CoT: The question asks for the percentage of parents who delay the Varicella vaccine according to CDC recommendations, based on the provided infographic. need to find the section of the infographic that lists vaccines and the percentages for delaying them past CDC recommendations. will focus on the Varicella vaccine data. The cropped image shows the specific data point for the <observation>Varicella vaccine</observation> under the Delay vaccine past CDC recommendationsrow. Therefore, the final answer is 44% . Figure 6. Example of Monet-SFT-125K: cropping the crucial region. Monet-SFT-125K Example 2: Creating New Visual States (from Zebra-CoT 3D Object Counting) Question: Consider the initial arrangement of objects presented in the image below. You are required to perform series of removal operations in the specified order: Step 1: Eliminate all objects identified as PYRAMIDs. Step 2: Eliminate all objects that are BLUE in color. Step 3: Eliminate all objects that are RED in color. After completing these steps, determine the total count of orange objects remaining. Put your final answer within boxed{}. Question Image: Image-text Interleaved CoT: will begin by analyzing the are <observation>7</observation> distinct objects in the initial configuration. My approach will be to systematically apply each removal step and visually track the remaining objects before arriving at the final count. The first operation requires the removal of all pyramid-shaped objects from the scene. This means both the <observation>purple pyramid</observation> and the <observation>orange pyramid</observation> will be taken away. To visualize the updated set of objects, will generate an image reflecting this change. shown in the problem image. set of objects initial There After removing the pyramids, as seen in the previous visual, we are left with <observation>five</observation> objects: an <observation>orange sphere</observation>, <observation>blue cube</observation>, <observation>red torus</observation>, and <observation>purple sphere</observation>. The next instruction is to remove all objects that are blue. This specifically targets the <observation>blue cube</observation>. will generate new visual to show the state after this removal. <observation>red cube</observation>, ... Having completed all specified removal operations, the final visual shows <observation>two</observation> an <observation>orange sphere</observation> and <observation>purple objects sphere</observation>. The question asks for the count of orange objects. From the two remaining objects, only one is orange. The final answer is 1 . remaining: Figure 7. Example of Monet-SFT-125K: creating new visual states. Monet-SFT-125K Example 3: Drawing Auxiliary Lines and Bounding Boxes (from CogCoM) Question:What is the sum of real users below 1 million? Put your final answer within boxed{}. Question Image: Image-text Interleaved CoT: Draw line to obtain the new image after the line is drawn. Outline each bar column, which are located at Netflix, VoD.pI, Wirtualna Polska VOD services, Player.pl, VoD.tvp.pI, CDA.pI, HBOGo.pI, lpla TV, Chili.com, NCplusgo.pl. Analyze the value of each bar in relation to x=1. The values obtained from HBOGo.pI, lpla TV, Chili.com, and NCplusgo.pl are all <observation>less than one million</observation>, with respective values of <observation>0.63, 0.59, 0.25, and 0.21.</observation> Calculate their total sum as 0.63 + 0.59 + 0.25 + 0.21 = 1.68, so the answer is 1.68. The final answer is 1.68 . Figure 8. Example of Monet-SFT-125K: drawing auxiliary lines and bounding boxes. Monet-7B Inference Example 1: 3D Spatial Reasoning (from VisualPuzzles) Question:Given pair of chairs arranged at specific angle, select the option from the four provided that matches the angle of the chairs in the question. Options: Choose from (A) (B) (C) (D) in the image. Question Image: Response by Monet-7B: will analyze the proThe problem asks to identify the option that matches the given angle between the chairs. vided image to determine the angle between the chairs and then compare it with the given choices.To accurately assess the angle between the chairs, will focus on the visual representation of the chairs in the problem image. <latent><latent_embeddings></latent>The zoomed-in view of the chairs in the problem image clearly shows the angle between them. This visual confirms the specific spatial relationship between the two chairs. Therefore, the final answer is . Figure 9. Inference example: 3D spatial reasoning. Instead of describing the angles between the chairs in language, Monet directly reasons with latent embeddings before giving the final answer. Monet-7B Inference Example 2: 2D Transformation (from VisualPuzzles) Question: The number 4 is reflected twice in <image1>. Which among the options in <image2> should appear in the field with the question mark if we do the same with the number 5? Question Image: Response by Monet-7B: To determine the correct answer, we need to identify the pattern of reflection for the number 5, similar to how the number 4 is reflected in the provided image.To better understand the reflection pattern, will generate zoomed-in view of the relevant section of the image. <latent><latent_embeddings></latent>The zoomed-in image clearly shows how number 4 is reflected. The reflection pattern for the number 4 is obtained by flipping it twice. Once horizontally, once vertically. Therefore, the final answer is . Figure 10. Inference example: 2D transformation. By generating latent embeddings, Monet successfully identifies the flipping rule of the number. Monet-7B Inference Example 3: Complex Diagram Reasoning (from MME-RealWorld-Lite) Question: Which country is the top sales country? The choices are listed below: (A) Canada (B) Germany (C) UK (D) United States (E) The image does not feature the number. Question Image: Response by Monet-7B: To determine the top sales country, need to locate the section of the image that displays sales by country. <latent><latent_embeddings></latent>The highlighted area in the image clearly shows the Top Sales Countries section. It lists various countries along with their respective sales figures. The sales figures are in millions of dollars. From the highlighted section, can see that the United States has the highest sales at 1,241.60M, followed by the UK, Germany, and other countries. This directly answers the question about the top sales country. The final answer is . Figure 11. Inference example: complex diagram reasoning. Monet-7B exhibit hierarchical reasoning pattern. Firstlt, it focus on the relevant section in the image by generating latent embeddings: The highlighted area in the image clearly shows the Top Sales Countries section. Then, it accurately identifies the contents in the Top Sales Countries section and gives the correct answer. Monet-7B Inference Example 4: Commonsense QA (from VisualPuzzles) Question: Based on the relationship between Origin and Analogy, select the option from the candidates that best completes this analogy. Options: Choose from (A) (B) (C) (D) in the image. Question Image: Response by Monet-7B: To identify the correct image that completes the analogy, will analyze the relationship between the Origin and Analogy images in the provided problem image. The goal is to find similar relationship between the Origin image and the missing Analogy image. To better understand the relationship, will focus on the Origin and Analogy images to identify the specific action being performed by the person in each image. <latent><latent_embeddings></latent> The zoomed-in view of the Origin image clearly shows person drinking wine, while the Analogy image depicts person smelling flower. This indicates that the action involves the sense of smell.Therefore, the final answer is . Figure 12. Inference example: commonsense QA. Monet-7B correctly identifies the connection between the Origin and the Analogy image, showing its commonsense reasoning capability. Monet-7B Inference Example 5: Fine-grained OCR (from MME-RealWorld-Lite) Question: Where does Joseph work? The choices are listed below: (A) Boston. (B) Miami. (C) Chicago. (D) New York. (E) The image does not feature the related information. Question Image: Response by Monet-7B: To determine where Joseph works, cle. will zoom in on the section of <latent><latent_embeddings></latent> The zoomed-in image clearly shows that Joseph Higgins is portfolio manager in New York. This directly answers the question about where Joseph works.Therefore, the final answer is . information within the artirelevant the article that mentions Joseph to clearly identify his location. need to locate his name or Figure 13. Inference example: fine-grained OCR. Monet-7B accurately identifies the key information, which is located in the middle of the rightmost region of the image. Monet-7B Inference Example 6: Math Reasoning (from VisualPuzzles) Question: Every night the wizard Tilim makes the weather forecast for the king. When Tilim gets it right he gets 3 gold coins, but when he makes mistake, he pays fine of 2 gold coins. After making the prediction for 5 days, Tilim did the math and discovered that he neither won nor lost coins. How many times did he get the weather forecast right in those 5 days? Options: (A) 1 (B) 2 (C) 3 (D) 4 Response by Monet-7B: To determine the number of correct weather forecasts, we can set up an equation based on the given conditions. Lets define: - as the number of correct forecasts (where he earns 3 gold coins per correct forecast). - as the number of incorrect forecasts (where he loses 2 gold coins per incorrect forecast) The wizard made total of 5 forecasts, so: + = 5 He neither won nor lost any gold, which means the total gain from correct forecasts equals the total loss from incorrect forecasts: 3x = 2y We now have system of two equations: 1. + = 5 2. 3x = 2y We can solve this system by expressing in terms of from the first equation: Next, substitute = 5 into the second equation: Solve for x: = 5 3x = 2(5 x) 3x = 10 2x 3x + 2x = 10 5x = 10 = 2 So, Tilim made 2 correct forecasts. To verify, we calculate y: = 5 = 5 2 = 3 Now, check the total gold: - Correct forecasts: 2 3 = 6 gold - Incorrect forecasts: 3 (2) = 6 gold - Total gold: 6+(6) = 0 gold. The calculations confirm that Tilim neither won nor lost any gold. Therefore, the number of correct forecasts is 2 . The final answer is . Figure 14. Inference example: math reasoning. Monet-7B can perform pure-text math reasoning. Since the problem is not visual-related, it doesnt activate latent thinking mode."
        }
    ],
    "affiliations": [
        "Amazon AGI SF Lab",
        "Kling Team",
        "Peking University"
    ]
}