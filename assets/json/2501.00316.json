{
    "paper_title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models",
    "authors": [
        "Mahir Labib Dihan",
        "Md Tanvir Hassan",
        "Md Tanvir Parvez",
        "Md Hasebul Hasan",
        "Md Almash Alam",
        "Muhammad Aamir Cheema",
        "Mohammed Eunus Ali",
        "Md Rizwan Parvez"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning. However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 3 ] . [ 1 6 1 3 0 0 . 1 0 5 2 : r MAPEVAL: MAP-BASED EVALUATION OF GEOSPATIAL REASONING IN FOUNDATION MODELS Mahir Labib Dihan1, Md Tanvir Hassan1, Md Tanvir Parvez2, Md Hasebul Hasan1 Md Almash Alam3 Muhammad Aamir Cheema4 Mohammed Eunus Ali1, Md Rizwan Parvez5 1Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET) 2Statistics, Islamic University Bangladesh 4Monash University 5Qatar Computing Research Institute (QCRI) 3Bangladesh Computer Council (BCC) {mahirlabibdihan, saad7557.7557a, tanvir.parvez.stat, hasebulhasan97, almash.anik.61}@gmail.com aamir.cheema@monash.edu, mohammed.eunus.ali@gmail.com, mparvez@hbku.edu.qa Equal contribution, Corresponding authors"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements in foundation models have enhanced AI systems capabilities in autonomous tool usage and reasoning. However, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MAPEVAL, benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MAPEVAL features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 700 unique multiple-choice questions about locations across 180 cities and 54 countries, MAPEVAL evaluates foundation models ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MAPEVAL, we conducted comprehensive evaluation of 28 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MAPEVAL-API, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our detailed analyses provide insights into the strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MAPEVALs critical role in advancing general-purpose foundation models with stronger geo-spatial understanding."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in foundation models, particularly large language models (LLMs) and visionlanguage models (VLMs), are significantly enhancing the capabilities of AI systems in autonomous tool usage (Qin et al., 2023; Yao et al., 2022) and reasoning (Lu et al., 2023; Wei et al., 2022). These developments facilitate the automation of everyday tasks through natural language instructions, especially in domains that require interaction with specialized tools like map services. As platforms such as Google Maps or Apple Maps have become ubiquitous for accessing various location-based services (a.k.a tools/APIs) ranging from finding nearby restaurants to determining the fastest routes between origins and destinationsthere has been growing interest in integrating 1 Figure 1: Overview of MAPEVAL. On the left, we show the annotation process, where an expert gathers either visual snapshots or textual data from Google Maps to create multiple-choice questions with ground truth labels. On the right, we depict the evaluation process and input/output for the three benchmark tasks in MAPEVAL. maps with foundation models (Xie et al., 2024; Zheng et al., 2024). couple of recent initiatives, such as WebArena (Zhou et al., 2023) and VisualWebArena (Koh et al., 2024), have introduced new tasks that involve map usage in practical scenarios. However, despite the widespread adoption of map services and the promising potential of interactions between foundation models (e.g., LLMs and VLMs) and these services, no existing studies have rigorously tested the capabilities of foundation models in location or geo-spatial reasoning. This gap is critical, as effective map-based reasoning can optimize navigation, facilitate resource discovery, and streamline logistics in everyday life. Addressing this gap is essential for advancing the practical utility of AI in real-world applications. We introduce MAPEVAL, novel benchmark designed to evaluate the geo-spatial reasoning capabilities of foundation models and AI agents in complex map-based scenarios. MAPEVAL addresses critical gap in existing benchmarks by evaluating models ability to process heterogeneous geospatial contexts, perform compositional reasoning, and interact with real-world map tools. It features three task types API, VISUAL, and TEXTUALthat require models to collect world information via map tools, deep visual understanding, and reason over diverse geo-spatial data (e.g., named entities, coordinates, operational hours, distances, routes, user reviews/ratings, map images), all of which remain challenging for state-of-the-art foundation models. Comprising 700 unique multiplechoice questions across 180 cities and 54 countries, MAPEVAL reflects real-world user interactions with map services while pushing state-of-the-art models to understand spatial relationships, map infographics, travel planning, POI search, and navigation. MAPEVAL ensures geographic diversity, realistic query patterns, and evaluation across multiple modalities. By integrating long contexts, visual complexity, API interactions, and questions requiring commonsense reasoning or recognition of insufficient information (i.e., unanswerability), it offers rigorous framework for advancing geo-spatial AI capabilities. In Fig 1, we depict an overview of MAPEVAL. With MAPEVAL, we evaluated 28 prominent foundation models, where Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro showed competitive performance overall. However, significant gaps emerged in MAPEVAL-API, with Claude-3.5-Sonnet agents outperforming GPT-4o and Gemini-1.5-Pro by 16% and 21%, respectively, and even larger disparities compared to open-source models. Our detailed analyses revealed further insights into model strengths and weaknesses. Despite these advances, all models still fall short of human performance by over 20%, especially in handling complex map images and rigorous reasoning, underscoring MAPEVALs role in advancing geospatial understanding. The benchmarking dataset and evaluation code will be open-sourced at https://github.com/MapEval."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Geo-spatial question answering presents significant challenges for foundation models (Mai et al., 2023). Early research in GeoQA (Mai et al., 2021) has focused on template-based methods (Zelle and Mooney, 1996; Chen et al., 2013; Chen, 2014; Punjani et al., 2018; Kefalidis et al., 2023), where predefined templates classify queries and retrieve information from structured databases like OpenStreetMap or DBpedia (Auer et al., 2007). While effective in certain scenarios, these methods are constrained by the static nature of the databases and the predefined templates, limiting their flexibility in handling complex or dynamic queries. There has been limited effort to assess (Roberts et al., 2023) and improve (Balsebre et al., 2024) LLMs capabilities in geospatial reasoning. Recent benchmarks such as Travel Planner (Xie et al., 2024), ToolBench (Qin et al., 2023), and API-Bank (Li et al., 2023) integrate map tools and APIs for location-based queries. While these benchmarks handle real-world tasks like itinerary planning or querying map data, the use of map APIs is limited to more straightforward use cases, such as calculating distances or identifying nearby points of interest. In addition, remote sensing research (Bastani et al., 2023; Yuan et al., 2024; Zhang et al., 2024; Lobry et al., 2020) has focused on extracting physical features from satellite imagery. While valuable for environmental monitoring and urban planning, this approach differs significantly from the task of reasoning over interactive digital map views, which involve understanding spatial relationships, map symbols, and navigation elements in dynamic, user-interactive context."
        },
        {
            "title": "3 THE MAPEVAL DATASET",
            "content": "3.1 DESIGN PRINCIPLES Reasoning. Geo-spatial reasoning in map-based tasks presents distinct challenges for foundation models, including: (a) understanding complex problem descriptions in natural language, (b) collecting relevant world information using map tools or APIs, (c) performing compositional and spatio-temporal reasoning, (d) interpreting map visuals, and (e) synthesizing information from heterogeneous geo-spatial contexts (e.g., named entities, distances, and temporal values). These tasks test the limits of state-of-the-art models, which struggle to fully grasp geo-spatial relationships, navigation complexities, and POIs. Realistic. MAPEVAL reflects real-world map usage by capturing typical user interactions with map services, such as: (a) varied usage patterns like location-based searches and travel planning, and (b) informal, often fragmented queries, without relying on perfect grammar or structure. Diversity. MAPEVAL ensures geographic diversity and broad evaluation across models and tasks: (a) capturing locations across cities and countries globally, and (b) offering wide variety of question types and contexts, which test foundation models spatial, temporal, data retrieval, and visual reasoning abilities. Long Contexts, Multi-modality, API Interactions. MAPEVAL challenges models with: (a) long geo-spatial descriptions, including POIs and navigational data, (b) complex map-specific images with location markers, and (c) API interactions, testing models abilities as language agents in realworld map-based tasks. Unanswerability, Commonsense. MAPEVAL includes questions where context is insufficient to provide an answer, testing models ability to identify missing or incomplete information, rather than making incorrect guesses. It also assesses commonsense reasoning and handling uncertainty, essential for reliable decision-making in real-world applications. Multiple Choice Questions (MCQs). We employ MCQs in MAPEVAL, similar to MMLU (Hendrycks et al., 2020), rather than open-ended queries. This approach circumvents the evaluation challenges associated with generated responses (Sai et al., 2022), allowing for more straightforward and reliable accuracy-based assessment of map-based reasoning capabilities. 3.2 TASKS Textual. The objective of MAPEVAL-TEXTUAL is to answer MCQs by decomposing complex queries and extracting relevant information from long textual contexts. These contexts describe map 3 Task Textual/API What is the direction of Victoria Falls from Harare? Question Example Type Place Info Nearby Routing Unanswerable Visual Textual/API Visual Textual/API Visual Textual/API Visual Trip Textual/API Counting Visual Is there any Hospital marked with star symbol on the tourist map of Rome? Find restaurants nearby Louvre Museum above 4.0 rating. stayed at SpringHill Suites by Marriott Portland Hillsboro. Can you recommend the nearest restaurant to my location? am driving to Brassica in Bexley Via Whittier St. After reaching Lockbourne Rd, where should go next? What is the fastest route from Times Square to Central Park by walking? Which road should follow from Wola to Mokotow to avoid flooded roads in heavy rains? Which way should be efficient while visit from Abis bus station to KONO so that Victoria park is on the way have an afternoon free in New York and plan to visit The Metropolitan Museum of Art for 3 hours, followed by 30-minute coffee break at nearby cafe, and then spend 1 hour in Central Park. Plan schedule to ensure have enough time for everything. How many hospitals are there in the left side of the river? Count 121 83 91 66 80 20 67 88 Table 1: Examples of different question categories. MAPEVAL-TEXTUAL and MAPEVALVISUAL questions are accompanied by both textual and visual context (See appendix for full qualitative example queries, contexts and evaluation model outputs during evaluations.) locations, POIs, routes, navigation details, and travel distances/times, often including user ratings or reviews. Unlike typical reading comprehension tasks, these texts combine structured data (e.g., coordinates, distances) with unstructured narratives and subjective content. The model must reason over this heterogeneous information to select the correct answer. This task evaluates the models ability to analyze fine-grained map-related information presented in text. API. In the MAPEVAL-API task, an AI agent interacts with map-based APIs to retrieve data (e.g., nearby POIs, distance calculations). The task involves generating API queries based on user questions, interpreting the returned structured data, and integrating it into reasoning processes to answer MCQs. This task evaluates the models ability to handle data retrieval, API interactions, and the synthesis of structured information in real-world, map-driven scenarios. Visual. MAPEVAL-VISUAL task requires the model to interpret and analyze map snapshots, specifically digital map views from services like Google Maps. These snapshots represent complex spatial relationships, routes, landmarks, OCR texts (e.g., rating), and symbolic elements (e.g., logos or traffic signs), which differ from typical image recognition tasks. The model must extract relevant information from the visuals, integrate it with spatial reasoning, and use it to answer MCQs. This task assesses the models ability to tackle map-specific visual contents and perform spatial reasoning. 3.3 DATASET CONSTRUCTION Data Annotation. To create high-quality benchmark dataset for MAPEVAL, we utilized Google Maps, widely adopted map service. The process of constructing the textual context presented significant challenges, particularly in ensuring accuracy and efficiency. For an example question like What are the opening hours of the British Museum? requires precise data to provide valid options and correct answer. Manually searching for the British Museum on Google Maps and looking for its opening hours can be both time-consuming and prone to errors, making this method inefficient. To address these challenges, we employed MapQaTor (Dihan et al., 2024), web interface built on Google Maps APIs, designed to streamline the collection of textual map data. MapQaTor automates data retrieval from map APIs, collecting key information like opening hours and location details to build the textual dataset (Details in Appendix B.1). For each user query, we first fetch the necessary context data using MapQaTor. Questions were then paired with their corresponding contexts, and multiple-choice options were carefully curated based on this information. The ground truth answers were derived from the same context. For MAPEVAL-API, the same questions were used as in MAPEVAL-TEXTUAL, but without textual contexts, requiring the language agents to interact with tools directly. To address consistency issues with real-time data updates, we created controlled evaluation environment. This involves caching 4 Statistics Total unique question instances - Questions with api or textual-context - Questions with visual-context Total unique countries Total unique cities Maximum textual-context length Maximum question length Maximum questions from country Maximum questions from city Average textual-context length Average question length Unique number of textual-context Unique number of visual-context Number 700 300 (42.86%) 400 (57.14%) 54 1500 107 132 44 435.63 21.41 215 270 Min, Max, Avg Choices 2, 7, 4.004 Max zoom of visual-context Min zoom of visual-context Average zoom of visual-context 21.0 8.0 15.26 Table 2: Key statistics of MAPEVAL. Lengths are in words. Visual-context means Map snapshots/images. Some questions are yes/no and some have additional complexity with 4+ choices. Figure 2: MAPEVAL category statistics. place information and simulating API interactions. Details of the pseudo-Google Maps setup are provided in Appendix C. For the visual context, we capture map snapshots from Google Maps, covering random locations across various cities and countries worldwide. Based on each snapshot, we formulate relevant questions with multiple-choice options, where the correct labels are derived directly from the map information. To maintain traceability, we save the Google Maps URL for each snapshot. Additionally, to examine model capabilities at different zoom levels, we capture snapshots at varying zoom depths1. We create the following question types for MAPEVAL: (a) Place Info: detects POIs and asks about specific details related to place (e.g., location, rating, reviews); (b) Nearby: identifies nearby places or POIs; (c) Routing: navigates between locations, considering routes and landmarks; (d) Unanswerable: when the map information (e.g., from google map) or the textual and visual context is insufficient to answer the question. Note that, in each category we formulate few questions that requires general knowledge or reasoning about locations and navigation (e.g., there are 52 commonsense QAs in MAPEVAL-VISUAL). Moreover, MAPEVAL-TEXTUAL and MAPEVAL-API exclusively feature Trip questions, which involve planning multi-stop journeys across various POIs. Due to the complexity and details of trip planning, these questions are difficult to represent in single visual snapshot. Conversely, Counting tasks are unique to MAPEVAL-VISUAL, where models count specific items or locations on mapa challenge specifically tailored to visual contexts. Quality Control and Human Performance To ensure quality, each QA pair is annotated by multiple members of our team, achieving an initial 76% mutual agreement. At least two team members then manually verify and resolve any disputes on the remaining pairs; if consensus cannot be reached (i.e., ambiguous), that pair is filtered out. To compute human scores, two team members who did not participate in the annotation process attempt to answer the questions, and their highest-scoring attempts are reported as the human performance benchmark. For MAPEVAL-API, as the questions are identical to MAPEVAL-TEXTUAL, we report the same human performance for both. 3.4 DATASET STATISTICS AND ANALYSIS The main statistics of MAPEVAL are presented in Table 2 and Figure 2. Examples of each question type and their numbers are presented in Table 12. We visualize the global distribution of locations 1Zoom levels found in map URLs indicate depth (e.g., url has zoom level 16.71), with higher values (e.g., 16 and above) showing more detail, compared to level 1 (world map)- See Appendix G. 5 (a) (b) Figure 3: Geographical Distribution of Textual and Visual Contexts. The left heatmap (a) represents the locations of places mentioned in textual contexts, while the right heatmap (b) shows the locations derived from map snapshots in visual contexts. in our dataset using coordinates  (Fig. 3)  . Table 13 (Appendix) lists all countries and their frequencies in MAPEVAL. We use OpenStreetMaps Nominatim API for reverse geocoding to determine countries from coordinates. Textual context includes the coordinates of places in it. In case of visual context, we can find the coordinates from the associated Map URL with each snapshot. For example, coordinate of an example url, is 35.7048455,139.763263. We visualize the distribution of question and textual context lengths in the Appendix (Figures 7 and 8). Overall, beyond their diversity in types, questions and contexts also vary significantly in length, reflecting varying levels of complexity and detail. Furthermore, in Appendix G.1, we illustrate the zoom level distribution in MAPEVAL-VISUAL, adding another dimension to the datasets diversity and evaluation challenges."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL PROTOCOL AND SETUP We evaluate all tasks using the accuracy metric, defined as the percentage of correct choices selected by the model. We prompt models with the respective context, question, tool usage documentations (only for MAPEVAL-API), answer format guidelines, and choices. We assess LLMs for MAPEVALTEXTUAL, VLMs for MAPEVAL-VISUAL, and ReACT agents Yao et al. (2022) (known for effective tool interaction (Zhuang et al., 2023)) built on various LLMs for MAPEVAL-API, aligning each task with appropriate model types. Appendix presents example prompts for all tasks. Our LLMs and VLMs spans both open and closed-source models. Closed-source models include Claude-3.5Sonnet, GPT-4o, GPT-4-Turbo (Achiam et al., 2023), GPT-3.5-Turbo (OpenAI, 2022), Gemini-1.5 (Pro, Flash; Reid et al. (2024)), with all except GPT-3.5-Turbo being multi-modal foundation models used in all tasks, while GPT-3.5-Turbo, which is text-only, is utilized solely in the MAPEVALTEXTUAL and MAPEVAL-API tasks. Open-source LLMs include instruct versions of Gemma2.0 (9B, 27B; Team et al. (2024)), Llama-3.2 (3B, 90B), Llama-3.1 (8B, 70B; Vavekanand and Sam (2024)) , Mistral-Nemo-7B, Mixtral-8x7B (Jiang et al., 2024), Qwen2.5 (7B, 14B, 72B; Team (2024)) , Phi-3.5-mini. For MAPEVAL-VISUAL, we considered the open-source VLMs: Qwen2VL-7B-Instruct (Wang et al., 2024), MiniCPM-Llama3-V-2 5 (Yao et al., 2024), Llama-3-VILA1.58B (Lin et al., 2023), glm-4v-9b (GLM et al., 2024), InternLm-xcomposer2 (Dong et al., 2024), paligemma-3b-mix-224 (Beyer et al., 2024), DocOwl1.5 (Hu et al., 2024), llava-v1.6-mistral-7b-hf (Liu et al., 2024b), and llava-1.5-7b-hf (Liu et al., 2024a). In MAPEVAL-API task, we concentrate our exploration on high-capacity open-source LLMs, specifically Llama-3.2-90B, Llama-3.1-70B, Mixtral-8x7B, and Gemma-2.0-9B. We limit our evaluation of open-source models in AI agents due to the tasks complexity and resource demands, the lower performance of smaller models, and the excessive number of calls for both LLMs and map APIs. 4.2 RESULTS AND ANALYSIS 4.2.1 MAPEVAL-TEXTUAL We present MAPEVAL-TEXTUAL results summaries in Table 3. Our benchmarking reveals significant insights into the current state of geo-spatial reasoning capabilities in language models. The results demonstrate clear performance hierarchy, with closed-source models generally outperforming their open-source counterparts. Claude-3.5-Sonnet leads with 66.33% overall accuracy, while Model Overall Place Info Nearby Routing Trip Unanswerable Claude-3.5-Sonnet Gemini-1.5-Pro GPT-4o GPT-4-Turbo Gemini-1.5-Flash GPT-4o-mini GPT-3.5-Turbo Llama-3.1-70B Llama-3.2-90B Qwen2.5-72B Qwen2.5-14B Gemma-2.0-27B Gemma-2.0-9B Llama-3.1-8B Qwen2.5-7B Mistral-Nemo Mixtral-8x7B Phi-3.5-mini Llama-3.2-3B Close-Source (Proprietary) LLMs 66.33 66.33 63.33 62.33 58.67 51.00 37.67 73.44 65.63 64.06 67.19 62.50 46.88 26.56 73.49 74.70 74.70 71.08 67.47 63.86 53.01 Open-Source LLMs 70.31 68.75 62.50 57.81 39.06 50.00 53.13 48.44 46.88 53.13 40.63 31. 67.47 66.27 71.08 71.08 71.08 50.60 57.83 49.40 50.60 54.22 48.19 49.40 61.00 58.33 57.00 53.67 49.00 47.33 44.00 43.33 43.33 43.00 37.00 33.00 75.76 69.70 69.70 71.21 66.67 57.58 48.48 69.70 66.67 63.64 59.09 59.09 59.09 45.45 42.42 50.00 45.45 46.97 31.82 49.25 47.76 49.25 47.76 38.81 40.30 28.36 40.30 38.81 41.79 32.84 31.34 34.33 23.88 38.81 32.84 26.87 20.90 25. Human 86.67 92.19 90.36 81.81 88. Human Performance 40.00 85.00 40.00 30.00 50.00 25.00 5.00 45.00 30.00 10.00 20.00 15.00 30.00 20.00 20.00 15.00 10.00 0.00 0.00 65.00 Table 3: MAPEVAL-TEXTUAL performances. Figure 12 visualizes the categorical accuracy. ) % ( r A 80 60 40 20 0 Sonnet G-Pro GPT-4o GPT-4 Ll3.1 MAPEVAL-TEXTUAL G-Flash Ll3.2 4o-mini Gemma2 Mixtral GPT-3.5 MAPEVAL-API Figure 4: Comparison between MAPEVAL-TEXTUAL and MAPEVAL-API . the best open-source model, Llama-3.1-70B, achieves 61.00%. However, the substantial gap between even the top-performing models and human accuracy (86.67%) underscores the challenges that remain in geo-spatial reasoning tasks. Models generally excel in Place Info, Nearby, and Routing tasks (best performance 75%), benefiting from the comprehensive context extracted by MAPEVAL-TEXTUAL. This includes textual descriptions, opening hours, distances, and routing times, enabling LLMs to easily extract relevant information and perform basic mathematical reasoning. In contrast, models struggle significantly with Trip planning scenarios (best performance 49%), indicating difficulties with complex, multi-step reasoning. This poor performance is primarily due to the challenge of aggregating multiple routes with various spatio-temporal constraints, task that remains universally difficult across model types. Performance on Unanswerable queries varies significantly, highlighting the importance of recognizing insufficient information in real-world applications. Gemini models, particularly Gemini-1.5-Pro with 85% accuracy, outperformed others in this category, where most models achieved only 0-45% accuracy. This stark contrast, along with consistent underperformance in Trip planning across all models, points to fundamental challenges in geo-spatial reasoning that transcend individual architectures. These findings validate our benchmarks ability to identify key areas for improvement in AI systems handling location-based queries and planning. Furthermore, the benchmarks results illustrate the impact of model scale, with larger models generally outperforming smaller ones. However, the performance gap between open and closed-source models suggests significant potential for advancements in open-source development, as Fig 13 highlights additional challenges for open-source models in handling longer contexts. 4.2.2 MAPEVAL-API We present the MAPEVAL-API results in Table 4, highlighting key insights into the geo-spatial reasoning abilities of language models when interacting with map APIs. The analysis shows that 7 Model Overall Place Info Nearby Routing Trip Unanswerable Claude-3.5-Sonnet GPT-4-Turbo GPT-4o Gemini-1.5-Pro Gemini-1.5-Flash GPT-3.5-Turbo GPT-4o-mini Llama-3.2-90B Llama-3.1-70B Mixtral-8x7B Gemma-2.0-9B Close-Source (Proprietary) LLMs 64.00 53.67 48.67 43.33 41.67 27.33 23.00 68.75 62.50 59.38 65.63 51.56 39.06 28. 55.42 50.60 40.96 30.12 38.55 22.89 14.46 39.67 37.67 27.67 27.00 Open-Source LLMs 54.69 53.13 32.81 35.94 37.35 32.53 18.07 14.46 Human Reference 65.15 60.61 50.00 40.91 46.97 33.33 13.64 39.39 42.42 27.27 28.79 71.64 50.75 56.72 34.33 34.33 19.40 43.28 35.82 31.34 38.81 26.87 Human (MAPEVAL-TEXTUAL ) 86. 92.19 90.36 81.81 88.06 55.00 25.00 15.00 65.00 30.00 15.00 5.00 15.00 15.00 15.00 45. 65.00 Table 4: MAPEVAL-API evaluation performance (See Figure 14 to visualize categorical accuracy) MAPEVAL-API generally underperforms compared to MAPEVAL-TEXTUAL across most task categories, with significant performance drops observed in Nearby tasks (from 74.70% to 55.42%) and Routing tasks (from 75.76% to 65.15%). Figure 4 visualizes these differences across models. While Claude-3.5-Sonnet demonstrated consistent performance, other models experienced noticeable declines, primarily due to the absence of direct context and the complexity of tool usage. This highlights the need for more advanced agent surpassing ReActs capabilities in geo-spatial domains. Interestingly, in the Trip category, MAPEVAL-API achieved notable improvement of approximately 22% in its best performance compared to MAPEVAL-TEXTUAL. This suggests that MAPEVAL-API is particularly effective at step-by-step reasoning required for solving multi-step complex problems. Claude-3.5-Sonnet led the results with an overall accuracy of 64.00%, showcasing robust performance both as tool agent and in generic graph reasoning beyond maps. substantial performance gap remains between closed-source and open-source models, with the best open-source model, Llama-3.2 90B, achieving only 39.67% overall accuracy. Similar to MAPEVALTEXTUAL, performance on Unanswerable queries showed wide variation (5% to 65%), underscoring the need for models to better identify insufficient information in real-world scenarios. 4.2.3 MAPEVAL-VISUAL We evaluate models on the MAPEVAL-VISUAL task in Table 5. As observed, closed-source models generally outperform their open-source counterparts, with Claude-3.5-Sonnet leading with an overall accuracy of 61.65%, followed by GPT-4o at 58.90% and Gemini-1.5-Pro at 56.14%. Among open-source models, Qwen2-VL-7B-Instruct tops the list with 51.63% overall accuracy. While the models perform well in Place Info tasks, achieving high accuracy of 82.64%, they struggle with more complex tasks like Counting, Nearby, and Routing, indicating areas where current models require significant improvement. However, it is crucial to understand why models with strong image reasoning capabilities still underperform on map-specific tasks. We conjecture that they are welltrained on generic images but not on detailed map data. To validate, Fig 5 plots accuracy against zoom levels, showing significant performance drop at higher zoom depths (e.g., streets, symbols, demarcations) beyond level 14, where map details become more complex. Our benchmark dataset exposes substantial performance gap between AI models and human performance, particularly in tasks that require nuanced reasoning. For instance, human performance on Routing tasks (85.18%) far surpasses the best models accuracy (50%), and similar gap is seen in the Counting task (78.41% for humans versus 47.73% for the best AI). Additionally, the dataset highlights disparities in handling uncertainty: while models like Claude-3.5-Sonnet and Gemini-1.5-Pro excel in identifying unanswerable questions, with accuracy rates of 90% and 80%, other models, especially open-source ones, struggle significantly. 4.3 QUALITATIVE ERROR ANALYSIS LLMs face challenges in spatial, temporal, and commonsense reasoning when answering locationbased queries. In spatial reasoning, they struggle with straight-line distances (Example at Listing 1), cardinal directions (e.g., East, West, North, South; Example at Listing 2), and step-by-step route planning, leading to decreased accuracy, particularly with math or counting (e.g., nearby restaurant 8 Model Claude-3-5-Sonnet GPT-4o Gemini-1.5-Pro GPT-4-Turbo Gemini-1.5-Flash GPT-4o-mini Qwen2-VL-7B-Instruct Glm-4v-9b InternLm-Xcomposer2 MiniCPM-Llama3-V-2 5 Llama-3-VILA1.5-8B DocOwl1.5 Llava-v1.6-Mistral-7B-hf Paligemma-3B-mix-224 Llava-1.5-7B-hf Overall 61.65 58.90 56.14 55.89 51.94 50. 51.63 48.12 43.11 40.60 32.99 31.08 31.33 30.58 20.05 Place info Nearby Routing Counting Unanswerable 55.56 57.78 56.67 56.67 56.47 47.78 82.64 76.86 76.86 75.21 70.25 77.69 Open-Source VLMs 71.07 73.55 50.41 60.33 46.90 43.80 42.15 37.19 22.31 Human Performance 81.67 48.89 42.22 48.89 32.22 32.22 23.33 28.89 25.56 18.89 82. 45.00 50.00 43.75 42.50 38.36 41.25 40.00 41.25 43.75 32.50 28.75 32.50 32.50 38.75 13.75 47.73 47.73 32.95 44.32 32.95 28.41 40.91 34.09 34.09 31.82 26.14 27.27 21.59 23.86 28.41 90.00 40.00 80.00 40.00 55.00 25.00 40.00 10.00 10.00 30.00 5.00 0.00 15.00 10.00 0. Human Table 5: MAPEVAL-VISUAL evaluation performance. (Fig 16 visualizes categorical accuracy). 82.23 78.41 85.18 65.00 ) % ( r A 80 60 20 0 8-11 (2.5%) Sonnet MiniCPM LLaVA-1.5 11-14 (28.3%) 14-17 (49.6%) Zoom Level (data percentage) 17-20 (17.5%) 20-23 (1.8%) GPT-4o VILA GPT-4 GLM4 G-Pro InternLM PaliGema 4o-Mini G-Flash DocOwl Qwen2-VL LLaVA-1.6 Figure 5: Accuracy by Zoom Level. counts; Example at Listing 3). Temporal reasoning issues include failing to plan trips efficiently or calculate optimal visiting times, such as errors in travel times or visit durations (Example at Listing 4). Commonsense reasoning failures occur when models cannot deduce simple conclusions from context and often hallucinates (Example at Listing 5). LLM-based agents also face challenges using map tools or APIs, particularly in Nearby and Routing queries. Misuse or misinterpretation of parameters leads to failed results, such as omitting key parameters or using incompatible values. When encountering no valid routes or results, agents may fall into infinite loops, repeatedly issuing identical requests without adjusting their approach. These issues highlight the need for better API handling and error recovery mechanisms. In visual tasks, VLMs often struggle with spatial awareness, showing confusion when POIs are visually close together or incorrectly identifying and counting POIs in map images (e.g., malls/stores). Such errors underscore the need for enhanced spatial awareness, temporal reasoning, and tool usage in foundation models (details in Appendix E)."
        },
        {
            "title": "5 ENHANCING GEOSPATIAL REASONING IN FOUNDATION MODELS",
            "content": "Calculator Integration for Complex Spatial Computations: In MAPEVAL-TEXTUAL, LLMs showed significant variability in their ability to perform spatial reasoning tasks like calculating straight-line distances (see Fig. 17), cardinal directions (see Fig. 18) and counting-related queries (see Fig. 19). For example: (i) Claude-3.5-Sonnet achieved the highest accuracy (91%) in identifying cardinal directions, while Gemma-2.0-27B scored the lowest (16.67%). (ii) In measuring straight-line distances, all models struggled, with the best accuracy being only 51.06%. (iii) Counting tasks posed challenges, even for dominating models like Claude-3.5-Sonnet, which underperformed compared to the open-source Gemma-2.0-27B (60.87% accuracy). To address these issues, we extended model capabilities by providing access to external tools (e.g., calculator) specifically 9 Model Claude-3.5-Sonnet GPT-4o GPT-4-Turbo Gemini-1.5-Pro Gemini-1.5-Flash GPT-4o-mini GPT-3.5-Turbo Llama-3.2-90B Llama-3.1-70B Mixtral-8x7B Gemma-2.0-9B Straight-Line Distance LLM 51.06 46.81 40.43 38.29 46.81 34.04 19.15 42.55 48.94 38.29 29.79 LLM+Calculator 85.11 70.21 76.59 72.34 63.83 78.72 55.32 68.90 61.7 59.57 68. Cardinal Direction LLM 91.67 62.50 58.33 62.50 58.33 29.17 20.83 66.67 66.67 33.33 37.50 LLM+Calculator 95.83 87.50 91.67 91.67 87.50 91.67 62.50 87.50 95.83 79.17 75.00 Table 6: Performance Improvement of LLMs in Straight-Line Distance and Cardinal Directions Analysis (Fig. 20 and 21 visualizes the improvement). for calculating straight-line distances and cardinal directions (Details in Appendix I). This resulted in dramatic improvement (see Table 6), with accuracies increasing by over 50% in certain cases. For instance: (i) The accuracy of Claude-3.5-Sonnet in calculating straight-line distances increased from 51.06% to 85.11%, demonstrating the utility of integrating external tools. (ii) GPT-4o-mini, which initially struggled with cardinal direction tasks, saw its performance increase from 29.17% to 91.67%, showcasing remarkable transformation with tool support. (iii) Even open-source models like Gemma-2.0-9B benefited, achieving an accuracy boost in straight-line distance tasks from 29.79% to 68.90%. These improvements highlight the challenges LLMs face when reasoning spatially without external support, especially in complex or unfamiliar contexts. By leveraging tools, models can offload computationally intensive or context-specific reasoning tasks, enabling more precise and reliable results. However, spatial reasoning is only one aspect of location-based tasks where models continue to underperform. For instance, temporal reasoning tasks, such as incorporating travel times and determining optimal visiting hours, could benefit from additional tools. Expanding tool integration in this way could improve the models performance across multiple reasoning domains, but it would also add significant complexity to the architecture, requiring the management of multiple tools for different types of reasoning. Category Place Info Nearby Routing Trip Unans. Overall Adaptive Routing of Tools and Models: In ReAct-based systems, significant challenge arises from the heavy responsibility placed on single agent to extract relevant parameters from question, call APIs with those parameters, and then provide the final answer based on API responses. This complex process often leads to issues such as parameter extraction errors, incorrect API calls, or dead loops (e.g., GPT-3.5-Turbo encountering 16 infinite iterations; see Fig. 15). These problems are particularly evident when the agent is unable to effectively reason through the task, reducing task completion rates. In fact the processing of large amount of API data even in plain text form (i.e., long contexts in MAPEVAL-TEXTUAL task) pose Table 7: Accuracy with GPT-3.5-Turbo significant challenge to LLMs (i.e., as discussed in Section 4.2.1 as well as the low performances in Table 3). To address these limitations, the CHAMELEON Framework (Lu et al., 2024) offers robust solution that adaptively breaks the task into multiple tool usage modules (e.g., multi-agent system). The integration of CHAMELEON into the MAPEVALAPI has already shown notable improvement in GPT-3.5-Turbos performance,  (Table 7)  . Besides, CHAMELEONs ability to decompose tasks and handle errors more efficiently results in fewer parameter extraction errors and prevents dead loops, significantly boosting accuracy. Another promising alternative approach would be to develop an ensemble system that combines query classifier with type-specific LLM deployment. This system would first classify incoming queries and then route them to the best-performing LLM for that particular query type achieving potential superiority. ReAct CHAMELEON 39.06 22.89 33.33 19.40 15.00 27.33 54.69 54.21 51.51 43.28 25.00 49."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce MAPEVAL, comprehensive benchmark dataset designed to assess foundation models in geo-spatial reasoning through textual, API-based, and visual evaluation modes. MAPEVAL incorporates diverse real-world scenarios to thoroughly evaluate model capabilities on 10 geo-spatial reasoning tasks. Our findings reveal that while leading models like Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro excel in certain areas, they still significantly underperform compared to human accuracy, especially when using open-source foundation models. This highlights critical areas for improvement, especially in managing complex map-based queries that require multi-step spatio-temporal reasoning, efficient tool utilization, and domain-specific knowledge. Future work could focus on developing specialized geospatial models, integrating LLMs with external tools like map APIs, and enhancing VLMs visual understanding of map images. We anticipate that MAPEVAL will catalyze ongoing research in geospatial reasoning and broader QA domains."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the reproducibility of our results, we provide the evaluation codes and the complete dataset used for our experiments at: https://github.com/MapEval. The inference process for the LLMs, including parameters such as temperature, top-k, and top-p, is part of the evaluation code. Any updates or bug fixes will be made available in the repository."
        },
        {
            "title": "LIMITATIONS",
            "content": "Our dataset does not cover all available Google Maps APIs, which limits the scope of our evaluation. Specifically, we have used five APIs from the Places and Routes categories: Text Search, Place Details, Nearby Search, Directions, and Distance Matrix. However, we did not incorporate other API categories such as Maps and Environment. This restricted API usage narrows the variety of queries we could evaluate and may leave out other valuable geospatial insights that could be gained from broader API usage. Furthermore, any future updates to the APIs we used may not be reflected in our dataset, which could impact its relevance for real-time applications, potentially making it outdated and more suitable for archival purposes. Another limitation is that the performance observed in our evaluation may not transfer to other domains or tools, as we did not explore this possibility. The generalizability of our methods remains an area for future research. Finally, different prompt formulations could lead to variations in the results, but we did not experiment with this aspect. Future work could focus on examining how different prompts affect the LLMs performance in geospatial reasoning tasks."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Soren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpedia: nucleus for web of open data. In international semantic web conference, pages 722735. Springer, 2007. Pasquale Balsebre, Weiming Huang, and Gao Cong. Lamp: language model on the map. arXiv preprint arXiv:2403.09059, 2024. Favyen Bastani, Piper Wolters, Ritwik Gupta, Joe Ferdinando, and Aniruddha Kembhavi. Satlaspretrain: large-scale dataset for remote sensing image understanding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1677216782, 2023. Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko BoË‡snjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, 11 Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. URL https: //arxiv.org/abs/2407.07726. Kurt Bollacker, Robert Cook, and Patrick Tufts. Freebase: shared database of structured general human knowledge. In AAAI, volume 7, pages 19621963, 2007. Wei Chen. Parameterized spatial sql translation for geographic question answering. In 2014 IEEE international conference on semantic computing, pages 2327. IEEE, 2014. Wei Chen, Eric Fosler-Lussier, Ningchuan Xiao, Satyajeet Raje, Rajiv Ramnath, and Daniel Sui. synergistic framework for geographic question answering. In 2013 IEEE seventh international conference on semantic computing, pages 9499. IEEE, 2013. Mahir Labib Dihan, Mohammed Eunus Ali, and Md Rizwan Parvez. Mapqator: system for efficient annotation of map query datasets, 2024. URL https://arxiv.org/abs/2412. 21015. Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model, 2024. URL https://arxiv.org/abs/ 2401.16420. Bowen Fang, Zixiao Yang, Shukai Wang, and Xuan Di. Travellm: Could you plan my new public transit route in face of network disruption? arXiv preprint arXiv:2407.14926, 2024. Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. URL https://arxiv.org/ abs/2406.12793. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024. URL https://arxiv.org/abs/2403.12895. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Nikolaos Karalis, Georgios Mandilaras, and Manolis Koubarakis. Extending the yago2 knowledge graph with precise geospatial knowledge. In The Semantic WebISWC 2019: 18th International Semantic Web Conference, Auckland, New Zealand, October 2630, 2019, Proceedings, Part II 18, pages 181197. Springer, 2019. Sergios-Anestis Kefalidis, Dharmen Punjani, Eleni Tsalapati, Konstantinos Plas, Mariangela Pollali, Michail Mitsios, Myrto Tsokanaridou, Manolis Koubarakis, and Pierre Maret. Benchmarking geospatial question answering engines using the dataset geoquestions1089. In International Semantic Web Conference, pages 266284. Springer, 2023. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. 12 Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244, 2023. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024a. URL https://arxiv.org/abs/2310.03744. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing, 58(12):85558566, 2020. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36, 2024. Gengchen Mai, Krzysztof Janowicz, Rui Zhu, Ling Cai, and Ni Lao. Geographic question answering: challenges, uniqueness, classification, and future directions. AGILE: GIScience series, 2:8, 2021. Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, Gao Cong, Yingjie Hu, et al. On the opportunities and challenges of foundation models for geospatial artificial intelligence. arXiv preprint arXiv:2304.06798, 2023. Open Geospatial Consortium. Ogc geosparql - geographic query language for rdf data. http:// www.opengeospatial.org/standards/requests/80, 2011. Document 11-052r3. OpenAI. Chatgpt, 2022. URL https://openai.com/blog/chatgpt. Dharmen Punjani, Kuldeep Singh, Andreas Both, Manolis Koubarakis, Iosif Angelidis, Konstantina Bereta, Themis Beris, Dimitris Bilidas, Theofilos Ioannidis, Nikolaos Karalis, et al. Templatebased question answering over linked geospatial data. In Proceedings of the 12th workshop on geographic information retrieval, pages 110, 2018. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Jonathan Roberts, Timo Luddecke, Sowmen Das, Kai Han, and Samuel Albanie. Gpt4geo: How language model sees the worlds geography. arXiv preprint arXiv:2306.00020, 2023. Ananya Sai, Akash Kumar Mohankumar, and Mitesh Khapra. survey of evaluation metrics used for nlg systems. ACM Computing Surveys (CSUR), 55(2):139, 2022. Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum. Yago: core of semantic knowledge. In Proceedings of the 16th international conference on World Wide Web, pages 697706, 2007. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Qwen Team. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Raja Vavekanand and Kira Sam. Llama 3.1: An in-depth analysis of the next-generation large language model, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. URL https://arxiv.org/abs/2409. 12191. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. arXiv preprint arXiv:2402.01622, 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint 2408.01800, 2024. Zhenghang Yuan, Zhitong Xiong, Lichao Mou, and Xiao Xiang Zhu. Chatearthnet: global-scale image-text dataset empowering vision-language geo-foundation models. Earth System Science Data Discussions, 2024:124, 2024. John Zelle and Raymond Mooney. Learning to parse database queries using inductive logic programming. In Proceedings of the national conference on artificial intelligence, pages 1050 1055, 1996. Zilun Zhang, Tiancheng Zhao, Yulong Guo, and Jianwei Yin. Rs5m and georsclip: large scale vision-language dataset and large vision-language model for remote sensing. IEEE Transactions on Geoscience and Remote Sensing, 2024. Huaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade Nova, Le Hou, Heng-Tze Cheng, Quoc Le, Ed Chi, et al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520, 2024. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. URL https://webarena.dev. Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset for llm question answering with external tools. Advances in Neural Information Processing Systems, 36: 5011750143, 2023."
        },
        {
            "title": "A DETAILED RELATED WORK",
            "content": "A.1 MAPEVAL-TEXTUAL Template-based GeoQA models (Zelle and Mooney, 1996; Chen et al., 2013; Chen, 2014; Punjani et al., 2018; Kefalidis et al., 2023) have predominantly followed two-step strategy for answering geographic questions: (1) classifying natural language query into predefined templates and (2) using these templates to query structured geographic knowledge sources such as PostGIS, DBpedia (Auer et al., 2007), YAGO (Suchanek et al., 2007), Freebase (Bollacker et al., 2007), and OpenStreetMap. While these approaches are effective for structured queries, they are limited by the predefined question templates and their reliance on static databases. They typically convert natural language questions into structured query language scripts. For instance, GeoQuestions1089 (Kefalidis et al., 2023) contains 1089 questions with corresponding GeoSPARQL (Open Geospatial Consortium, 2011) queries over the YAGO2geo (Karalis et al., 2019) geospatial knowledge graph. In contrast, our MAPEVAL-TEXTUAL approach shifts the focus from database querying to assessing geospatial reasoning in Large Language Models (LLMs). Annotators collect factual map services data using MapQaTor, which is then provided as context to LLMs. This setup isolates and evaluates the models ability to reason over geospatial relationships, addressing the challenge of freeform, complex map-related queries in dynamic environment. This approach allows for more holistic evaluation of LLMs, reflecting real-world usage where users interact with map tools using natural language queries. Thus, in MapEval, the responsibility lies with LLMs to answer the questions, whereas in previous works, the models were tasked with generating queries (e.g., Geoquery, GeoSPARQL), which are used to query external knowledge bases. GPT4GEO (Roberts et al., 2023) explored GPT-4s factual geographic knowledge by characterizing what it knows about the world without plugins or Internet access. Their evaluation focused on analyzing single model using templated queries about generic location and direction-oriented facts, such as routing, navigation, and planning for well-known cities and places. However, this approach is inherently constrained by the training data of GPT-4, making it incapable of answering questions about less-known places. While the findings suggest that GPT-4 shows promising geo-spatial knowledge, this approach neither establishes benchmark for geo-spatial reasoning nor incorporates real-life user queries or map services (e.g., Google Maps) as geospatial information base. Our approach employs fundamentally different evaluation and design principles. We establish benchmarking of deeper geo-spatio-temporal reasoning capabilities across multiple foundation models using real user queries rather than templates. Uniquely, our evaluation encompasses multimodal understanding, tool interactions, and answerability determination. Additionally, we provide foundation models with fine-grained map services data through both context and API access, enabling more comprehensive benchmarking of their geospatial question-answering abilities. A.2 MAPEVAL-API The MAPEVAL-API task adopts practical approach by leveraging map APIs to answer locationbased questions directly, providing more real-world scenario for evaluating the capabilities of Large Language Models (LLMs) in map-based reasoning. Recent advancements in LLMs have led to growing interest in planning tasks (Xie et al., 2024; Balsebre et al., 2024; Zheng et al., 2024; Fang et al., 2024) that involve map data. For instance, the Travel Planner (Xie et al., 2024) benchmark assessed multi-day itinerary planning using Google Maps API to determine distances, travel times, and details of nearby attractions. This task demonstrated the utility of map data in real-world planning scenarios, highlighting the potential for LLMs to integrate real-time geospatial information into decision-making. Additionally, tool-calling benchmarks such as ToolBench (Qin et al., 2023) and API-Bank (Li et al., 2023) have included location-based queries as subtask, testing the ability of LLMs to interact with APIs in structured ways. These benchmarks typically focus on simpler query types, such as retrieving distances or nearby points of interest (POIs), but they do not fully address the complexity and diversity of real-world map-based questions. 15 In contrast, MapEval-API pushes the boundaries by evaluating LLMs on wide variety of complex geospatial tasks that require not only querying map APIs but also integrating multiple pieces of information, such as travel itineraries, nearby services, and spatio-temporal reasoning. This more comprehensive evaluation of API-based reasoning challenges the models to process complex, multifaceted questions, highlighting their ability to handle nuanced map interactions and effectively synthesize data retrieved from APIs. A.3 MAPEVAL-VISUAL Prior works in geospatial analysis and map-based question answering have predominantly focused on remote sensing images (Bastani et al., 2023; Yuan et al., 2024; Zhang et al., 2024), which involve satellite or aerial imagery. These images often contain complex data about the Earths surface, including land cover, vegetation, urban infrastructure, and other environmental features. Models designed for interpreting remote sensing images (Lobry et al., 2020) typically rely on convolutional neural networks (CNNs) and other computer vision techniques for object detection, segmentation, and classification tasks. These methods often focus on identifying physical entities like roads, buildings, and natural features from high-resolution imagery. In contrast, our MAPEVAL-VISUAL approach focuses on digital map view snapshots, which are 2D representations of map services (such as Google Maps). Unlike remote sensing images, which represent physical realities captured from top-down perspective, these digital maps show geospatial information in structured, interactive format. The focus of MAPEVAL-VISUAL is to evaluate models ability to interpret and reason about these structured map views, which include not just physical features, but also symbolic and navigational information such as traffic signs, routes, landmarks, and visual cues from the map interface itself. While remote sensing image analysis typically involves extracting physical data from raw image pixels, MAPEVAL-VISUAL requires models to engage with spatial reasoning and map-based symbols, demanding different set of computational skills. In this task, the model must not only understand the spatial relationships between map features but also reason about the context provided by digital map interfaces, which include additional elements such as zoom levels, icons, and navigation markers. This distinction sets MAPEVAL-VISUAL apart from traditional remote sensing tasks and presents new challenges in the field of geospatial reasoning and map-based visual question answering."
        },
        {
            "title": "B DATA COLLECTION DETAILS",
            "content": "B.1 MAPQATOR: ANNOTATOR INTERFACE For the creation of the textual contexts and design MCQs based on that, we employed custom-built web interface named MapQaTor. As illustrated in Figure 6, this interface was central to the dataset development process, offering an intuitive, user-friendly environment that simplifies complex tasks, such as API interaction and context generation. The annotator interface is designed to reduce technical complexity for users, allowing them to concentrate on the core aspects of dataset annotation, such as selecting relevant locations, providing information on distances, durations, and directions between places, as well as identifying nearby points of interest. Its streamlined workflow facilitates efficient dataset creation by automating repetitive tasks, which not only minimizes errors but also significantly accelerates the annotation process. MapQaTor uses five key Google Maps APIs: Text Search, Place Details, Distance Matrix, Directions, and Nearby Search, based on their relevance to common map-based tasks and their ability to provide comprehensive location data. MapQaTor caches all API call responses, creating static database for evaluation purposes. This ensures consistent responses when evaluating MAPEVAL-API. Specifically, when an API call is made, the cached response is returned instead of real-time query, maintaining controlled and static evaluation environment. Once the dataset is generated, it can be easily exported in JSON format, making it readily usable for further analysis and evaluation in downstream tasks, such as model training and benchmarking. 16 B.2 FILTERING VIA LLMS: To ensure the challenge and quality of our dataset, we evaluated range of LLMs. We filtered out samples where the majority of the LLMs could easily provide the correct answer, considering these samples too easy and removing them from the dataset. Additionally, we identified samples where most LLMs failed to answer the questions based on the given context. In such cases, we re-examined the questions, correcting any inconsistencies to improve clarity and relevance. Figure 6: Screenshot of our Annotator Interface: MapQaTor"
        },
        {
            "title": "C EVALUATION DETAILS",
            "content": "C.1 PSEUDO-GOOGLE MAPS ENVIRONMENT To ensure consistency between annotation and evaluation, pseudo-Google Maps environment was developed with the following features: Caching: Information for over 13,000 locations was cached using Google Maps place ids during both annotation and evaluation stages, ensuring consistency across updates. Table 8 presents the number of data entries for each API tool in our database API Simulation: proxy interface mimics actual API interactions, enabling controlled testing while maintaining dynamic map-like attributes (e.g., travel times and place lists). Key-Query Mapping: Discrepancies between user queries and database keys were handled by storing all data using standardized place ids obtained via real API call. This method maintains static evaluation environment to preserve answer validity while simulating real-world API interactions by controlling dynamic variables like travel times, place attributes, and nearby location lists, which often change in live settings. C.2 MAPEVAL-TEXTUAL EVALUATION In this evaluation setting, we provide the LLM with pre-fetched context containing detailed information about specific locations, such as opening hours, distances between points of interest, and nearby amenities. The context is designed to simulate real-world scenario."
        },
        {
            "title": "Tool\nPlaceDetailsTool\nTravelTimeTool\nDirectionsTool\nNearbySearchTool",
            "content": "Entries (#) 13,354 1,142 317 481 Table 8: Number of data entries in the database for each API tool Listing 4 demonstrates an example of this evaluation process. The context includes details about The Metropolitan Museum of Art, including its location, opening hours, and nearby cafes. The query asks for time-optimized schedule that includes 3-hour visit to the museum, followed by 30-minute coffee break at nearby cafe, and 1 hour spent in Central Park. The available options offer different schedules, and the models are tasked with selecting the most appropriate one based on the provided context. As illustrated, models like Claude-3.5-Sonnet, Gemini1.5-Pro, and GPT-4o correctly identify Option 3 as the best fit, considering the opening hours of each location and the feasible travel times between them. In contrast, Gemma-2.0-9B selects an incorrect option, indicating misunderstanding of the cafes closing hours. This pre-fetched context evaluation allows us to test the models ability to reason over structured information and make contextually informed decisions. It highlights the importance of understanding spatial relationships, operating hours, and timing constraints, all of which are crucial in real-world trip planning tasks. C.3 MAPEVAL-API EVALUATION In this evaluation approach, we leverage Zero Shot React Agent, which utilizes dynamic toolbased framework to enhance the models ability to respond to user queries effectively. Listing 6 illustrates the structured system prompt guiding the agent in employing various available tools. This framework allows the agent to access range of functionalities, including retrieving place IDs, obtaining detailed information about locations, and estimating travel times between points of interest. The Zero Shot React Agents dynamic capabilities enable it to interact with tools in systematic manner, ensuring accurate and contextually relevant responses. For example, the agent can utilize the PlaceId tool to obtain the unique identifier for specified location, which can then be employed in subsequent actions, such as fetching detailed information with PlaceDetails or finding nearby places using NearbyPlaces. This modular approach not only simplifies complex queries but also grounds responses in real-time data. The prompt structure encourages the agent to think critically about each step, starting with the users question and leading to carefully considered action. It determines the appropriate tool to use, specifies the necessary input, and provides well-structured JSON blob for the action. The observation of the tools output informs the next steps, allowing for iterative refinement of the response. By employing this Zero Shot React Agent framework, we can assess the models proficiency in utilizing external tools to generate accurate, contextually aware responses, ultimately enhancing its effectiveness in real-world applications. C.4 MAPEVAL-VISUAL EVALUATION In this scenario, we provide Large Language Models (LLMs) with map snapshot that offers critical geospatial context necessary for answering the query. This snapshot includes default map view with clearly labeled locations and roads, aiding in the understanding of spatial relationships. The evaluation process is demonstrated in Listing 13. The input consists of two parts: an image as visual context and corresponding query with multiple answer options. In the example provided, the visual context includes map displaying several golf clubs and complex roadway network. The options represent possible answers that the evaluated models can choose from. The listing also shows the responses of various models, along with their explanations. 18 Tool Name PlaceSearch Parameters placeName, placeAddress"
        },
        {
            "title": "PlaceDetails",
            "content": "placeId Description Given place name with address our tool calls Text Search API to get list of places. Then choose the top place among them and returns its place id. Given place id our tool first searches in our database if not found, then uses Place Details API to fetch the details of the place."
        },
        {
            "title": "Directions",
            "content": "NearbySearch originId, destinationId, travelMode Given the place id of origin and destination, and travel mode our tool first searches in our database the duration (+distance) to go from origin to desIf tination by preferred travel mode. not found then queries Distance Matrix API. location, type, rankby, radius originId, destinationId, travelMode Given the place id of origin and destination, and travel mode our tool first searches in our database the available routes to go from origin to destination by preferred travel mode. If not found then queries Directions API. This tool requires the place id of the place around which to retrieve place information. Additionally, the type of places, the order in which results are listed and distance within which to return place results. It then searches the database for stored Nearby Places. If absent, it queries Nearby Search API. Table 9: Summary of the tools used in evaluation. Models like Gemini-1.5-Pro, GPT-4o-mini, and Claude-3.5-Sonnet successfully answered the query by correctly interpreting the geospatial information. On the other hand, Qwen-2VL-Chat selected an incorrect option, highlighting its difficulty in understanding spatial distances, which led to an erroneous answer. This evaluation underscores the importance of providing visual context when testing LLMs geospatial reasoning capabilities. By leveraging such visual aids, we can better assess how well these models understand and process spatial relationships in real-world scenarios FOUNDATION MODELS DETAILS Tables 10 and 11 provide comprehensive details of the open-source models utilized for dataset evaluation. FINE-GRAINED QUALITATIVE ERROR ANALYSIS MAPEVAL-TEXTUAL Commonsense Reasoning: (i) Consider scenario where the context states, {Place A} serves dinner, lunch, vegetarian food. When asked, Does {Place A} serve breakfast? many LLMs respond, There is not enough information in the context to answer, instead of simply saying No. human would deduce that since breakfast is not listed, {Place A} does not serve it. (ii) Another challenge arises in planning questions. Even when opening hours are included in the context, LLMs may plan schedules that overlook constraints, such as visiting during closed hours while satisfying other 19 Model Phi-3.5-mini-instruct Mistral-Nemo-Instruct-2407 Mixtral-8x7B-Instruct-v0.1 Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.2-3B-Instruct Llama-3.2-90B-text-preview gemma-2-27b-it gemma-2-9b-it Parameters Context Window 3.8B 7B 7B 7B 14B 72B 8B 70B 3B 90B 27B 9B 128K 128k 32K 128K 128K 128K 128k 128k 128k 128k 8.2k 8.2k Table 10: LLM model scales. Model MiniCPM-Llama3-V-2 5 Qwen2-VL-7B-Instruct Llama-3-VILA1.5-8B glm-4v-9b InternLm-xcomposer2 paligemma-3b-mix-224 DocOwl1.5 llava-v1.6-mistral-7b-hf llava-1.5-7b-hf Parameters Context Window 7B 8B 8B 4.9B 7B 3B 8B 7B 7B 8.2k 32K 8.2k 100k 96K - - - - Table 11: VLM model scales. conditions. For instance, although {Place A} is open from 9:00AM to 3:00PM, the model might schedule visit at 5:00PM, possibly due to inadequate training for this scenario. Spatial Reasoning: (i) LLMs particularly struggle with queries requiring the calculation of spatial relationships, such as cardinal directions, straight-line distances, nearest points of interest (POIs), or step-by-step route planning. For example, in Place Info, Nearby, and Routing questions, examining 50 random questions that required such computations we observed 10% decreased accuracy than others. This decline highlights the limitations of even dominant models like Gemini, which struggle with straight-line distance and direction calculations from geo-spatial data. (ii) LLMs also encounter difficulty with our domain specific questions that involve maths even in counting, especially when the count is large. For instance, in query like How many nearby restaurants have at least 4.5 rating?, LLMs often fail to provide an accurate count. Temporal Reasoning: LLMs struggle with temporal reasoning, which affects their performance on tasks like trip planning that require time manipulation. For example, when asked, want to visit A, B, and C. What is the most efficient order to visit? the model must calculate travel times and determine the optimal route but often fails. Similarly, in query like, want to visit for 1 hour. What is the latest time can leave home? the model needs to subtract the visit duration and travel time from As closing time, yet frequently makes errors in these simple time calculations. MAPEVAL-API Incorrect Tool Usage by Agents: LLM-based agents often exhibit varying degrees of errors when utilizing map tools/APIs, particularly impacting Nearby queries. This task requires complex set of arguments, and misinterpretation or improper use of these parameters frequently leads to failures in retrieving accurate results. Agents Stuck in Infinite Loops: Invalid actions and repetitive loops contribute significantly to errors, especially in Routing queries. When there are no valid routes between an origin and destination, agents often fail to reconsider their approach or stop the process. Instead, they repeatedly attempt the same query with the same parameters, resulting in deadlock and preventing progress. MAPEVAL-VISUAL Spatial Reasoning: In the Nearby category, models often exhibit confusion when multiple POIs are visually close together, leading to incorrect location selections. This indicates struggle with fine-grained spatial analysis, affecting their ability to provide reliable responses and emphasizing the need for improved spatial awareness mechanisms. Temporal Reasoning: In Routing queries, determining the fastest route requires detailed analysis of the source and destination, as well as transportation paths. VLMs often struggle with these calculations, resulting in noticeable decline in performance and underscoring the difficulties in processing geographical information effectively. Detecting and Counting: Models often struggle to accurately identify and count POIs in map images. For instance, when asked, How many shopping stores or malls are there? many proprietary VLMs may count incorrectly, with Claude-Sonnet listing an ATM as store, leading to overcounting. Conversely, they sometimes undercount, (e.g., detecting only 6 to 8 out of an actual 12 malls)."
        },
        {
            "title": "F QUALITATIVE EXAMPLES",
            "content": "Type Task Place Info Nearby Routing Unanswerable Textual/API Visual Textual/API Visual Textual/API Visual Textual/API Visual Trip Textual/API Counting Visual Question Example Which coffee shop is situated between Louvre Museum and Eiffel Tower? What is the direction and straight-line distance from Victoria Falls to Hwange National Park? Im at Baridhara Block, feeling unwell, and need some medicine. What is nearby pharmacy with good rating that is open? How many shopping malls are there within 500 radius of Berlin Cathedral? am at Toronto Zoo. Today is Sunday and its currently 8:30 PM. How many nearby ATMs are open now? Im currently staying at Horselberg-Hainich, while my friend is staying at Tungeda. After we meet up, want to visit an amusement park nearby. Can you suggest one thats close to us? want to walk from D03 Flame Tree Ridge to Aster Cedars Hospital, Jebel Ali. Which walking route involves taking the pedestrian overpass? On the driving route from Hassan II Mosque to Koutoubia via A3, how many roundabouts will encounter in total? Which restaurant is on the left side of the route from Metro El Golf to Metro Tobalaca L1? How many food stalls are there north of the overbridge at Hakaniemi? Find good coffee shop on the left side of my driving path from my home near Petaling Jaya to my office in Kuala Lumpur. How much time it would take to go to Igreja Nossa Senhora Da Conceic ao do Coroadinho - Matriz? live in Indira Road. At tomorrow 2 pm will leave my house. need to go to Military Museum to visit with friends for 2 hours and Multiplan Center to buy keyboard (which will take 20 minutes) and Sonali Bank, BUET to receive my check book (which will take 30 minutes). In which order should visit the places so that reach there on time and come back home as early as possible. will use public transport. How many restaurants or clubs are on the bottom side of Linnakatu road? Table 12: Additional Complex Examples 21 Listing 1: Example evaluation of MAPEVAL-TEXTUAL Green : Correct Answer. Red : Wrong Answer. Context: Information of Eiffel Tower: - Location: Av. Gustave Eiffel, 75007 Paris, France(48.8584, 2.2945). Information of Mont Saint-Michel: - Location: 50170 Mont Saint-Michel, France(48.6361, -1.5115). Query: What is the straight-line distance between the Eiffel Tower in Paris, France, and the Mont Saint-Michel in Normandy, France? Prompt: Please respond in the following JSON format: { \"option_no\": <option index>, // \"option_no\" refers to the number corresponding to the chosen answer from the list of options. It should be between 1 and 4. \"explanation\": \"<reason>\" } Example Prompt: Question: What is the capital of France? Option1: Berlin Option2: Paris Option3: Madrid Option4: Rome Example Response: { \"option_no\": 2, \"explanation\": \"Paris is the capital of France.\" } Provide your answer in this format. Remember to answer the question strictly based on the given context, without using any external knowledge or assumptions. Options: 1. 230 km 2. 250 km 3. 280 km 4. 310 km Calude-3.5-Sonnet: To answer this question, Ill need to calculate the straight-line distance between the two points using their given coordinates. Ill use the Haversine formula for this calculation. Eiffel Tower: (48.8584, 2.2945) Mont Saint-Michel: (48.6361, -1.5115) After performing the calculation, the approximate distance is 284 km. Based on this result, the closest option is 280 km. Gemini-1.5-Pro: cannot calculate the straight-line distance between the Eiffel Tower and Mont Saint-Michel without using external knowledge or tools. This calculation requires geographic data and mathematical formulas. GPT-4o: The straight-line distance between the coordinates of the Eiffel Tower (48.8584, 2.2945) and Mont Saint-Michel (48.6361, -1.5115) is approximately 280 km. Gemma-2.0-9B: To determine the straight-line distance, wed need to use the coordinates provided and apply distance formula. Unfortunately, cant perform calculations. **Therefore, cannot answer the question based on the given context.** 22 Listing 2: Example evaluation of MAPEVAL-TEXTUAL Green : Correct Answer. Red : Wrong Answer. Context: Information of Seurasaari Open-Air Museum: - Location: Seurasaari, 00250 Helsinki, Finland(60.1856, 24.8846). Information of Helsinki Central Station: - Location: Kaivokatu 1, 00100 Helsinki, Finland(60.1719, 24.9414). Query: What is the direction of the Seurasaari Open-Air Museum from Helsinki Central Station? Prompt: Please respond in the following JSON format: { \"option_no\": <option index>, // \"option_no\" refers to the number corresponding to the chosen answer from the list of options. It should be between 1 and 4. \"explanation\": \"<reason>\" } Example Prompt: Question: What is the capital of France? Option1: Berlin Option2: Paris Option3: Madrid Option4: Rome Example Response: { \"option_no\": 2, \"explanation\": \"Paris is the capital of France.\" } Provide your answer in this format. Remember to answer the question strictly based on the given context, without using any external knowledge or assumptions. Options: 1. East 2. West 3. North 4. South Calude-3.5-Sonnet: To determine the direction of Seurasaari Open-Air Museum from Helsinki Central Station, Ill compare their coordinates: Seurasaari Open-Air Museum: 60.1856, 24.8846 Helsinki Central Station: 60.1719, 24.9414 Analyzing the coordinates: 1. Latitude: Seurasaari (60.1856) is slightly north of Helsinki Central Station (60.1719) 2. Longitude: Seurasaari (24.8846) is west of Helsinki Central Station (24.9414) The primary direction is west, with slight northward component. Gemini-1.5-Pro: Seurasaari Open-Air Museum (60.1856, 24.8846) is located at higher latitude and almost same longitude compared to Helsinki Central Station (60.1719, 24.9414). Higher latitude corresponds to North. GPT-4o: Seurasaari Open-Air Museum is located to the north of Helsinki Central Station based on their coordinates. Gemma-2.0-9B: Seurasaari Open-Air Museum is located at 60.1856, 24.8846 and Helsinki Central Station is located at 60.1719, 24.9414. The museums coordinates are further north. 23 Listing 3: Example evaluation of MAPEVAL-TEXTUAL Green : Correct Answer. Red : Wrong Answer. Context: Nearby Restaurants of St. Lawrence Market are: 1. A&W Canada - Open: Monday: Open 24 hours, Friday: Open 24 hours, Saturday: Open 24 hours, Sunday: Open 24 hours. .... 7. Quik Sushi - Open: Open 24 hours , Tuesday: Open 24 hours, Wednesday: Open 24 hours, Thursday: Monday: 9:00AM-7:00PM , Tuesday: 11:30AM-7:30PM, Wednesday: 9:00AM-7:00PM, Thursday: 9:00AM-7:00PM, Friday: 9:00AM-7:00PM, Saturday: 9:00AM-7:00PM, Sunday: 9:00AM-7:00PM. Monday: 11:00AM-1:00AM , Tuesday: 11:00AM-1:00AM, Wednesday: 11:00AM-1:00AM, Monday: 10:30AM-9:00PM , Tuesday: 10:30AM-9:00PM, Wednesday: 10:30AM-9:00PM, .... 9. Big Pita - Open: Thursday: 10:30AM-9:00PM, Friday: 10:30AM-9:00PM, Saturday: 10:30AM-9:00PM, Sunday: 10:30AM -9:00PM. .... 14. Pizza Pizza - Open: Thursday: 11:00AM-1:00AM, Friday: 11:00AM-2:00AM, Saturday: 11:00AM-2:00AM, Sunday: 11:00AM -1:00AM. .... 17. Market Street Catch - Open: Thursday: 11:00AM-9:00PM, Friday: 11:00AM-10:00PM, Saturday: 11:00AM-10:00PM, Sunday: 11:00AM -8:00PM. .... 20. Churrasco Of The St Lawrence - Open: Monday: Closed, Tuesday: 7:00AM-7:00PM, Wednesday: 7:00AM-7:00PM, Thursday: 7:00AM -7:00PM, Friday: 7:00AM-7:00PM, Saturday: 7:00AM-7:00PM, Sunday: 10:00AM-5:00PM. Monday: 11:00AM-8:00PM , Tuesday: 11:00AM-9:00PM, Wednesday: 11:00AM-9:00PM, Query: How many nearby restaurants of St. Lawrence Market are open on Monday? Prompt: Please respond in the following JSON format: { \"option_no\": <option index>, // \"option_no\" refers to the number corresponding to the chosen answer from the list of options. It should be between 1 and 4. \"explanation\": \"<reason>\" } Example Prompt: Question: What is the capital of France? Option1: Berlin, Option2: Paris, Option3: Madrid, Option4: Rome Example Response: { \"option_no\": 2, \"explanation\": \"Paris is the capital of France.\" } Provide your answer in this format. Remember to answer the question strictly based on the given context, without using any external knowledge or assumptions. Options: 1. 2 2. 3 3.4 4. 5 Calude-3.5-Sonnet: Based on the given information, 3 nearby restaurants are open on Monday. These are: A&W Canada (open 24 hours), Quik Sushi (open 9:00 AM - 7:00 PM), and Big Pita (open 10:30 AM - 9:00 PM). All other listed restaurants are either closed on Monday or their Monday hours are not specified in the provided information. Gemini-1.5-Pro: Based on the provided information, these restaurants are open on Monday: A&W Canada, Quik Sushi, Pizza Pizza, and Market Street Catch. GPT-4o: Only A&W Canada and Quik Sushi are open on Monday. Gemma-2.0-9B: Only A&W Canada is open 24 hours on Monday. Listing 4: Example evaluation of MAPEVAL-TEXTUAL Green : Correct Answer. Red : Wrong Answer. Context: Information of The Metropolitan Museum of Art: - Location: 1000 5th Ave, New York, NY 10028, USA. - Open: Monday: 10:00AM-5:00PM, Tuesday: 10:00AM-5:00PM, Wednesday: Closed, Thursday: 10:00AM -5:00PM, Friday: 10:00AM-9:00PM, Saturday: 10:00AM-9:00-PM, Sunday: 10:00AM-5:00PM. .... Travel Time from The Metropolitan Museum of Art to American Wing Cafe on foot is 5 mins (0.3 km). Travel Time from The Metropolitan Museum of Art to The Great Hall Balcony Cafe on foot is 2 mins (0.1 km). .... Nearby Cafes of The Metropolitan Museum of Art are (sorted by distance in ascending order): 1. American Wing Cafe (1000 5th Avenue, New York) 2. The Great Hall Balcony Cafe (1000 5th Avenue 2nd Floor, New York) 3. Petrie Court Cafe (1000 5th Avenue, New York) 4. Starbucks (1142 Madison Avenue, New York) ..... Query: Today is Monday. have an afternoon free in New York and plan to visit The Metropolitan Museum of Art for 3 hours, followed by 30-minute coffee break at nearby cafe, and then spend 1 hour in Central Park (New York, NY, USA). Plan schedule to ensure have enough time for everything. will walk between places. Prompt: Please respond in the following JSON format: { \"option_no\": <option index>, // \"option_no\" refers to the number corresponding to the chosen answer from the list of options. It should be between 1 and 4. \"explanation\": \"<reason>\" } Example Prompt: Question: What is the capital of France? Option1: Berlin, Option2: Paris, Option3: Madrid, Option4: Rome Example Response: { \"option_no\": 2, \"explanation\": \"Paris is the capital of France.\" } Provide your answer in this format. Remember to answer the question strictly based on the given context, without using any external knowledge or assumptions. Options: 1. (3.00PM - 6.00PM) Visit The Metropolitan Museum of Art (6.02PM - 6.32PM) Have coffee at The Great Hall Balcony Cafe (6.38PM - 7.38PM) Spend time in Central Park. 2. (3.00PM - 6.00PM) Visit The Metropolitan Museum of Art (6.08PM - 6.38PM) Have coffee at Central Park cafe (6.42PM - 7.42PM) Spend time in Central Park. 3. (3.00PM - 6.00PM) Visit The Metropolitan Museum of Art (6.05PM - 6.35PM) Have coffee at Starbucks (6.39PM - 7.39PM) Spend time in Central Park. 4. (2.00PM - 5.00PM) Visit The Metropolitan Museum of Art (5.09PM - 5.39PM) Have coffee at Matto Espresso (5.47PM - 6.47PM) Spend time in Central Park. Calude-3.5-Sonnet: Option 3 is the most suitable schedule based on the given information. The Metropolitan Museum of Art is open on Mondays from 10:00 AM to 5:00 PM, so visiting from 3:00 PM to 6:00 PM is possible. Starbucks is open until 7:30 PM on Mondays, making it viable option for coffee break after the museum visit. Central Park is open until 1:00 AM, so spending time there from 6:39 PM to 7:39 PM is feasible. Gemini-1.5-Pro: Option3 is the only option that has time slots within the opening hours of all the places mentioned. The Metropolitan Museum of Art is open until 5:00PM on Monday, Starbucks is open until 7:30PM and Central Park is open until 1:00AM. GPT-4o: Option 3 is feasible as Starbucks is open on Monday and is nearby. The schedule allows for 3-hour visit to The Metropolitan Museum of Art, 30-minute coffee break, and 1 hour in Central Park. Gemma-2.0-9B: Option 1 provides the most feasible schedule. It allows for 3-hour visit to the museum, 30-minute coffee break at The Great Hall Balcony Cafe (which is within walking distance), and 1-hour visit to Central Park, all while ensuring enough time for travel between locations. 25 Listing 5: Example evaluation of MAPEVAL-TEXTUAL Green : Correct Answer. Red : Wrong Answer. Context: There are 3 routes from Petaling Jaya to Federal Territory of Kuala Lumpur by car. They are: 1. Via Damansara Link/Lebuhraya SPRINT/Sistem Penyuraian Trafik Kuala Lumpur Barat/E23 21 mins 15.2 km - Head east on Persiaran Tropicana toward Persiaran Tropicana .... 3. Via Damansara Link/Lebuhraya SPRINT/Sistem Penyuraian Trafik Kuala Lumpur Barat/E23 and Jalan Maarof 25 mins 15.2 km - Head east on Persiaran Tropicana toward Persiaran Tropicana .... Nearby Cafes of Petaling Jaya are (sorted by distance in ascending order): 1. Dareia Coffee - Address: Ps2-13 Lumi Tropicana, Persiaran Tropicana, Tropicana, Petaling Jaya. - Rating: 5. (7 ratings). .... Nearby Cafes of Federal Territory of Kuala Lumpur are (sorted by distance in ascending order): 1. Gwaku Coffee - Address: ULG Lobby Menara TH 1 Sentral, Jalan Stesen Sentral 5, Kuala Lumpur Sentral, Kuala Lumpur. - Rating: 4.2. (62 ratings). .... Query: Find good coffee shop on the left side of my driving path from my home near Petaling Jaya to my office in Kuala Lumpur. Prompt: Please respond in the following JSON format: { \"option_no\": <option index>, // \"option_no\" refers to the number corresponding to the chosen answer from the list of options. It should be between 0 and 4. \"explanation\": \"<reason>\" } Example Prompt: Question: What is the capital of France? Option0: Unanswerable, Option1: Berlin, Option2: Paris, Option3: Madrid, Option4: Rome Example Response: { \"option_no\": 2, \"explanation\": \"Paris is the capital of France.\" } Provide your answer in this format. Remember to answer the question strictly based on the given context, without using any external knowledge or assumptions. Options: 0. Unanswerable 1. Dareia Coffee 2. Gwaku Coffee 3. The Lemon Tree 4. Coffeeloca Calude-3.5-Sonnet: The Lemon Tree is good coffee shop option on the left side of the driving path from Petaling Jaya to Kuala Lumpur. It is located at the Ground Floor of Menara Lien Hoe, which is along Persiaran Tropicana. This cafe is on the initial part of the journey and has rating of 4 out of 5 based on 836 ratings, indicating its popular and well-liked spot. Its also described as having Moderate price level, making it suitable choice for coffee stop on the way to the office. Gemini-1.5-Pro: There is no information on which side of the driving path the cafes are located. GPT-4o: Gwaku Coffee is located on Jalan Stesen Sentral 5, which is on the driving path from Petaling Jaya to Kuala Lumpur. Gemma-2.0-9B: Gwaku Coffee is located in Kuala Lumpur Sentral, which is on the driving path from Petaling Jaya to Kuala Lumpur. 26 Listing 6: MAPEVAL-API System Prompt Respond to the human as helpfully and accurately as possible. You have access to the following tools: PlaceSearch: Get place ID for given location., args: {placeName: {description: Name of the place, title: Placename, type: string, placeAddress: {description: Address of the place, title: Placeaddress, type: string}}} PlaceDetails: Get details for given place ID., args: {placeId: {description: Place Id of the location, title: Placeid, type: string}} NearbySearch: Get nearby places around location., args: {placeId: {description: The id of the place around which to retrieve nearby places., title: Placeid, type: string}, type: {description: Type of place (e.g., restaurant, hospital, etc). Restricts the results to places matching the specified type., title: Type, type: string}, rankby: {default: distance, description: Specifies the order in which places are listed. Possible values are: (1. prominence (default): This option sorts results based on their importance. When prominence is specified, the radius parameter is required. 2. distance: This option sorts places in ascending order by their distance from the specified location. When distance is specified, radius is disallowed. In case you are not concerned about the radius, use rankby as distance.), title: Rankby, type: string}, radius: {anyOf: [{type: integer}, {type: null}], default: None, description: Defines the distance (in meters) within which to return place results., title: Radius}} TravelTime: Estimate the travel time between two places., args: {originId: {description: Place Id of Origin, title: Originid, type: string}, destinationId: {description: Place Id of Destination, title: Destinationid, type: string}, travelMode: { description: Mode of transportation (driving, walking, bicycling, transit), title: Travelmode, type: string}} Directions: Get directions/routes between two places., args: {originId: {description: Place Id of Origin, title: Originid, type: string}, destinationId: {description: Place Id of Destination, title: Destinationid, type: string}, travelMode: { description: Mode of transportation (driving, walking, bicycling, transit), title: Travelmode, type: string}} Use json blob to specify tool by providing an action key (tool name) and an action_input key (tool input). Valid \"action\" values: \"Final Answer\" or PlaceSearch, PlaceDetails, NearbySearch, TravelTime, Directions Provide only ONE action per $JSON_BLOB, as shown: { \"action\": $TOOL_NAME, \"action_input\": $INPUT } Follow this format: Question: input question to answer Thought: consider previous and subsequent steps Action: $JSON_BLOB Observation: action result ... (repeat Thought/Action/Observation times) Thought: know what to respond Action: { \"action\": \"Final Answer\", \"action_input\": \"Final response to human\" } Begin! Reminder to ALWAYS respond with valid json blob of single action. Use tools if necessary. Respond directly if appropriate. Format is Action:$JSON_BLOBthen Observation :. Thought: 27 Listing 7: Example evaluation of MAPEVAL-API Green : Correct Answer. Red : Wrong Answer. Query: Can visit The Great Pyramid of Giza at 5:00 PM Saturday? Options: 1. Yes 2. No Calude-3.5-Sonnet: To answer this question accurately, Ill need to check the opening hours of The Great Pyramid of Giza. Let me start by getting the place ID for the location. Action: { \"action\": \"PlaceSearch\", \"action_input\": { \"placeName\": \"The Great Pyramid of Giza\", \"placeAddress\": \"Al Haram, Nazlet El-Semman, Al Giza Desert, Giza Governorate, Egypt\" } } Observation: ChIJGymPrIdFWBQRJCSloj8vDIE Thought:Now that have the place ID for The Great Pyramid of Giza, Ill use it to get more details about the location, including its opening hours. Action: { \"action\": \"PlaceDetails\", \"action_input\": { \"placeId\": \"ChIJGymPrIdFWBQRJCSloj8vDIE\" } } Observation: - Location: Al Haram, Nazlet El-Semman, Al Haram, Giza Governorate 3512201, Egypt (29.9791705, 31.1342046). - Open: Monday: 8:00AM-4:30PM, Tuesday: 8:00AM-4:30PM, Wednesday: 8:00AM-4:30PM, Thursday: 8:00AM-4:30PM, Friday: 8:00AM-4:30PM, Saturday: 8:00AM-4:30PM, Sunday: 8:00AM-4:30PM. - Rating: 4.6. (27048 ratings). Thought:Based on the information provided by the PlaceDetails tool, can now answer the question. The Great Pyramid of Giza is open on Saturdays from 8:00 AM to 4:30 PM. The question asks if it can be visited at 5:00 PM on Saturday. Since 5:00 PM is after the closing time of 4:30 PM, the answer is No. Therefore, the correct option is Option 2: No. Action: { \"action\": \"Final Answer\", \"action_input\": \"2\" } Listing 8: Example evaluation of MAPEVAL-API Green : Correct Answer. Red : Wrong Answer. Query: am currently staying at Khaosan Tokyo Origami in Asakusa, Tokyo. want to visit the Sensoji Temple for 1 hour, Shibuya Crossing for 1.5 hours, Ueno Park for 2 hours, and the Tokyo Skytree for 1.5 hours. have 3.5 hours available. will leave my hostel at 7 am. How many places can visit? will walk between places. Options: 1. One 2. Two 3. Three 4. Four Calude-3.5-Sonnet: To answer this question accurately, Ill need to calculate the travel times between these locations and the time spent at each place. Lets break this down step by step. First, Ill need to get the place IDs for all the locations mentioned. Action: { \"action\": \"PlaceSearch\", \"action_input\": { \"placeName\": \"Khaosan Tokyo Origami\", \"placeAddress\": \"Asakusa, Tokyo\" } } ..... Observation: ChIJ35ov0dCOGGARKvdDH7NPHX0 Thought:Thank you for providing the place ID for Tokyo Skytree. Now that we have all the place IDs, lets calculate the travel times between these locations using the walking mode of transportation. Lets start with the travel time from Khaosan Tokyo Origami to Senso-ji Temple. Action: { \"action\": \"TravelTime\", \"action_input\": { \"originId\": \"ChIJ4ehN78GOGGARyvT_Bjj5s-c\", \"destinationId\": \"ChIJ8T1GpMGOGGARDYGSgpooDWw\", \"travelMode\": \"walking\" } } Response: value: 227}, status: LOCAL}]] [[{distance: {text: 0.3 km, value: 267}, duration: {text: 4 mins, ..... Observation: Travel Time on foot is 51 mins (3.5 km). Thought:Now that we have all the travel times and the time to be spent at each location, lets calculate how many places can be visited within the 3.5 hours available, starting at 7 am. 1. Khaosan Tokyo Origami to Senso-ji Temple: 4 minutes 2. Time at Senso-ji Temple: 1 hour 3. Total time so far: 1 hour 4 minutes At this point, its 8:04 am, and weve used 1 hour 4 minutes of the available 3.5 hours. The next destination, Shibuya Crossing, is 2 hours 52 minutes away on foot, which would exceed the remaining time (2 hours 26 minutes) even before spending any time there. Therefore, the traveler can only visit one place (Senso-ji Temple) within the given time constraints. Action: { \"action\": \"Final Answer\", \"action_input\": \"Option1\" } 29 Listing 9: Example evaluation of MAPEVAL-VISUAL Green : Correct Answer. Red : Wrong Answer. Context: Query: Which Golf Clubis located at the intersection of Springfield and Houdaille Quarry? Prompt: Choose the correct option from the image also show the answer with visible quotation and give proper explanation. Options: 1.Brooklake Country Club 2. Baltusrol Golf Club 3.East Orange Golf Course 4.Reeves-Reed Arboretum Gemini-1.5-Pro: The golf club located at the intersection of Springfield and Houdaille Quarry is \"Baltusrol Golf Club\". You can clearly see the label for Baltusrol Golf Club positioned right above Springfield Ave, with the golf flag icon situated directly at the point where Springfield Ave intersects with Houdaille Quarry. GPT-4o-mini: The correct option is \"2. Baltusrol Golf Club.\" Explanation: In the image, the location of Houdaille Quarry and Springfield can be found near the center at the junction of major roads. The Baltusrol Golf Club is marked nearby, indicating it is situated at that intersection. The other options, such as Brooklake Country Club, East Orange Golf Course, and Reeves-Reed Arboretum, do not appear to be located at the intersection of Springfield and Houdaille Quarry. Calude-3.5-Sonnet: Based on the map image, the correct answer is: \"2. Baltusrol Golf Club\" Explanation: The map shows Baltusrol Golf Club located near the intersection of Springfield Avenue and Houdaille Quarry. You can see \"Baltusrol Golf Club\" marked on the map just south of where Route 24 meets I-78, which is very close to where Springfield Avenue and Houdaille Quarry ( both visible on the map) would intersect. Therefore, Baltusrol Golf Club is the golf club located at the intersection of Springfield Avenue and Houdaille Quarry. Llama-3-VILA1.5-8B: The answer is 3.East Orange Golf Course. The location of East Orange Golf Course is at the intersection of Springfield and Houdaille Quarry, as shown in the image. The image is screenshot of map, and the location of East Orange Golf Course is marked with red pin. 30 Listing 10: Example evaluation of MAPEVAL-VISUAL Green : Correct Answer. Red : Wrong Answer. Context: Query: am presently visiting historical landmark? Mount Royal Park . Could you please inform me about the nearby Prompt: Choose the correct option from the image also show the answer with visible quotation and give proper explanation. Options: 1. Circle Stone 2.secret pool 3.Maison William Caldwell Cottingham 4.Poste de cavalerie du Service de police de la Ville de Montreal Gemini-1.5-Pro: The correct answer is \"Maison William Caldwell Cottingham\". The map shows the location of \"Mount Royal Park\". Just to the east of that marker is the \" Maison William Caldwell Cottingham\". The other options are located further away from Mount Royal Park. GPT-4o-mini: The correct option is:\"3. Maison William Caldwell Cottingham\" Explanation: Maison William Caldwell Cottingham is historical landmark located near Mount Royal Park. It reflects the architectural heritage and history of the area, providing insights into the past. The other options, while interesting, do not hold the same historical significance as Maison William Caldwell Cottingham. Calude-3.5-Sonnet: Based on the map image provided, the correct answer is:\"Circle Stone\" Explanation: The question asks about nearby historical landmark to Mount Royal Park. Looking at the map, we can see Mount Royal Park clearly labeled. Among the given options, \"Circle Stone\" is the closest historical landmark visible on the map near Mount Royal Park. Its marked with pin icon just north of the park area. The other options are either farther away or not visible on this particular map view: - \"secret pool\" is visible but seems to be farther to the northeast. - \"Maison William Caldwell Cottingham\" is visible but located to the southeast of the park. - \"Poste de cavalerie du Service de police de la Ville de Montreal\" is visible but located to the west of the park. Llama-3-VILA1.5-8B: The answer is 3.Maison William Caldwell Cottingham. Maison William Caldwell Cottingham is historic house museum located in Montreal, Quebec, Canada. It is located in the Mount Royal Park, which is popular tourist attraction in Montreal. The house was built in 1851 and is fine example of Gothic Revival architecture. It is now museum that showcases the history and culture of Montreal.. 31 Listing 11: Example evaluation of MAPEVAL-VISUAL Green : Correct Answer. Red : Wrong Answer. Context: Query: Which school is closed to Club Atletico Puerto Comercial? Prompt: Choose the correct option from the image also show the answer with visible quotation and give proper explanation. Options: 0. Unanswerable 1. Secondary school No. 25 2. PROVINCIAL SCHOOL No. 67 TOMAS ESPORA 3. Province of Buenos Aires SCHOOL No. 507 4. SCHOOL No.5 Province of Buenos Aires Gemini-1.5-Pro: The answer is \"0) Unanswerable\". The map provided does not show any schools. Therefore, it is impossible to determine which school is closed to Club Atletico Puerto Comercial based solely on the provided image. GPT-4o-mini: To determine which school is closest to Club Atletico Puerto Comercial, we need to analyze the locations of the listed schools in relation to the clubs position on the map. Analysis: - Club Atletico Puerto Comercial is located near the center bottom of the map. - The other schools must be found or deduced based on their general proximity to this location From the visible landmarks and overall arrangement on the map, PROVINCIAL SCHOOL No. 67 TOMAS ESPORA appears to be the closest to Club Atletico Puerto Comercial. Calude-3.5-Sonnet: Based on the information provided in the image, the correct answer is:0) Unanswerable Explanation: The map does not show any schools or educational institutions near Club Atletico Puerto. While it shows various landmarks and businesses, including \"Club Atletico Puerto\", there is no information about any schools visible on this map. The options provided mention specific schools, but none of these are shown or labeled on the map image. Given the limited information in the image and the absence of any school locations, the question cannot be answered based solely on whats shown here, making \"Unanswerable\" the most appropriate choice. Llama-3-VILA1.5-8B: The answer is 1) Secondary school No. 25. The image shows map of Puerto Comercial, Argentina, with various locations marked. One of the locations marked is secondary school, which is the closest school to Club Atletico Puerto Comercial. The answer is 1) Secondary school No. 25. 32 Listing 12: Example evaluation of MAPEVAL-VISUAL Green : Correct Answer. Red : Wrong Answer. Context: Query: How far is Rajpur Sonarpur in West Bengal, India, from Narayana Hospital, Chunavati, Andul Rd, Howrah, West Bengal, if take the following route: Continue on Andul Rd (42 m, 13 sec). Follow Andul Rd and NH 12 to Alipore Rd in Alipore, Kolkata (32 min, 11.3 km). Continue on Alipore Rd to Barhans (32 min, 9.5 km). Continue straight past Union Bank ATM-Garia onto Garia Main Rd/Netaji Subhash Chandra Bose Rd (7 min, 2.1 km). Pass by Jharna Ghee And Honey on the right. Prompt: Choose the correct option from the image also show the answer with visible quotation and give proper explanation. Options: 1. 23 km 2. 23.2km 3. 24.1km 4. 27.6km Gemini-1.5-Pro: The image clearly shows the total distance as \"23 km\". Therefore, the correct answer is \"1) 23 km\". GPT-4o-mini: Based on the provided information and the image, the distance from Narayana Hospital, Chunavati, Howrah, to Rajpur Sonarpur is approximately 23 km. The answer is option 1) \"23 km\" Explanation: - The route consists of several parts, with the total distance calculated and displayed in the map: - The total time shown on the map indicates travel duration of around 1 hour and 8 minutes for the distance of 23 km. Calude-3.5-Sonnet: Based on the information provided in the image, the correct answer is:\"23 km\" Explanation: The map shows route from Narayana Hospital, Chunavati (which is near the Hooghly River in the northwest of the map) to Rajpur Sonarpur (in the southeast of the map). The route is highlighted in blue. In the bottom left corner of this blue route, theres distance and time estimate clearly visible:\"1 hr 8 min 23 km\" Llama-3-VILA1.5-8B: The answer is 27.6km. The distance from Narayana Hospital, Chunavati, Andul Rd, Howrah, West Bengal, to Rajpur Sonarpur is 27.6 kilometers. The route you described involves traveling along Andul Rd, NH 12, Alipore Rd, Garia Main Rd/Netaji Subhash Chandra Bose Rd, and passing by Jharna Ghee And Honey on the right. The total distance covered is 27.6 kilometers. 33 Listing 13: Example evaluation of MAPEVAL-VISUAL Green : Correct Answer. Red : Wrong Answer. Context: Query: If traveling at constant speed of 5.4 km/h, how long will it take to travel from Point Doha International Airport, Bandara Internasional Hamad, Qatar Air Traffic Control, MB l.p, Doha to Point The Pearl Island, Doha if the distance is 19.4 km? Prompt: Choose the correct option from the image also show the answer with visible quotation and give proper explanation. Options: 1. 3.12 Minutes 2. 3.72 Minutes 3. 3.82 Minutes 4. 3.99 Minutes Gemini-1.5-Pro: The correct answer is \"25 min\". The provided options in the question (3.12, 3.72, 3.82, 3.99 minutes) are nonsensical given the travel time will take at least 25 minutes based on the provided map. Explanation: The image clearly shows that the estimated travel time from Doha International Airport to The Pearl Island is **25 minutes**. irrelevant in this case, as we have the estimated travel time provided directly from the map software. The provided speed of 5.4 km/h and distance of 19.4 km are GPT-4o-mini: 1. Convert 3.59 to minutes: 3.59 hours = 3.59 * 60 = 215.4 minutes As the distance of 19.4 km at 5.4 km/h actually corresponds to more than 3 minutes, none of the provided answer choices seem directly correct. If we need to choose the closest available option that might reflect an error in reasoning or calculation leading to proportional time in minutes. Calude-3.5-Sonnet: Distance = 19.4 km Speed = 5.4 km/h Time = 19.4 km / 5.4 km/h = 3.59 hours = 3.59 hours * 60 minutes/hour = 215.4 minutes Looking at the given options, none of them are correct as they all show times in minutes that are far too short. Llama-3-VILA1.5-8B: The answer is (1) 3.12 Minutesv. The time it takes to travel 19.4 km at constant speed of 5.4 km/h is 19.4 km / 5.4 km/h = 3.12 minutes."
        },
        {
            "title": "G DATASET STATISTICS AND ANALYSIS",
            "content": "Country Bangladesh India Japan Qatar Germany Italy South Africa France Hungary Sierra Leone Norway Zimbabwe Egypt Indonesia Vietnam Portugal Thailand Switzerland Count 132 33 24 15 10 8 7 7 7 7 7 6 6 6 6 6 6 6 Country United States Canada Australia Saudi Arabia Argentina Spain Poland Denmark Czechia Malaysia Peru Ireland Greece Nepal Belgium Morocco South Sudan Turkey Count 57 31 19 12 10 8 7 7 7 7 6 6 6 6 6 6 6 6 Country United Arab Emirates United Kingdom Pakistan China Luxembourg Brazil New Zealand Bhutan Chile Sweden Colombia Mexico Austria Netherlands South Korea Finland Russia Singapore Count 40 27 16 12 9 8 7 7 7 7 6 6 6 6 6 6 6 Table 13: Distribution of Questions Across Countries. r f b 70 50 40 30 20 10 0 300 600 900 1,200 1, Number of words r f b 330 300 270 240 210 180 150 120 90 60 30 0 20 40 60 80 100 Number of words Figure 7: Lengths (300 samples) Distribution of Textual-Context Figure 8: Distribution of Question Lengths (700 samples) G.1 ZOOM DETAILS context dataset, visual zoom levels is in In our as 11.Each Google Maps URL, value https://www.google.com/maps/@35.7048455,139.763263,16.71z?entry=ttu, where before the (e.g., 16.71) represents the zoom level. This allows us to easily extract zoom information directly from the URL, ensuring that each visual context can be accurately mapped to its respective level of detail. range paired with shown such the from 8. 21.0, as to"
        },
        {
            "title": "H EVALUATION RESULTS VISUALIZATION",
            "content": "In this section, we present the results of our evaluations through series of charts that summarize the performance of different models across various categories. These visualizations provide clear and 35 Figure 9: Zoom level 1 Figure 10: Zoom level 16 e C s f b 150 120 90 30 0 3"
        },
        {
            "title": "12\nZoom Level",
            "content": "15 18 21 Figure 11: Distribution of Zoom Levels (400 Visual Samples) concise comparison of model effectiveness in addressing textual, API-based, and visual geospatial queries. The charts are designed to highlight key trends, strengths, and limitations of the evaluated approaches. H.1 MAPEVAL-TEXTUAL Figure 12 illustrates the performance of models on MAPEVAL-TEXTUAL. ) % ( r A 80 60 40 20 0 Place Info Claude-3.5-Sonnet Llama-3.1-70B Qwen2.5-14B Llama-3.1-8B GPT-3.5-turbo Nearby Routing Trip Gemini-1.5-Pro Gemini-1.5-Flash GPT-4o-mini Qwen2.5-7B Phi-3.5-mini GPT-4o Llama-3.2-90B Gemma-2.0-27B Mistral-Nemo Llama-3.2-3B Unanswerable GPT-4-turbo Qwen2.5-72B Gemma-2.0-9B Mixtral-8x7B Figure 12: MAPEVAL-TEXTUAL categorical accuracy Figure 13 illustrates how accuracy of models in MAPEVAL-TEXTUAL changes with different context length. 36 ) % ( a c 100 80 60 40 0 0-300 (52.33%) 301-600 (26.67%) 601-900 (9.67%) 901-1200 (5.67%) 1201-1500 (5.67%) Context Length (data percentage) Claude-3.5-Sonnet Llama-3.1-70B Qwen-2.5-14B Llama-3.1-8B GPT-3.5-turbo Gemini-1.5-Pro Gemini-1.5-Flash GPT-4o-mini Qwen2.5-7B Phi-3.5-mini GPT-4o Llama-3.2-90B Gemma-2.0-27B Mistral-Nemo Llama-3.2-3B GPT-4-turbo Qwen-2.5-72B Gemma-2.0-9B Mixtral-8x7B Figure 13: Accuracy vs. Context Length (MAPEVAL-TEXTUAL ) H.2 MAPEVAL-API Figure 14 illustrates the performance of models on MAPEVAL-API. ) % ( r A 100 80 60 40 20 Place Info Nearby Routing Trip Claude-3.5-Sonnet Gemini-1.5-Pro Mixtral-8x7B GPT-4-Turbo Llama-3.1-70B Gemma-2.0-9B GPT-4o Llama-3.2-90B GPT-4o-mini Unanswerable Gemini-1.5-Flash GPT-3.5-Turbo Figure 14: MAPEVAL-API categorical accuracy Additionally in Figure 15, we can visualize the number of times agent stopped due to iteration limit. This happens when agents repeatedly calls the same api with same parameters and gets the same response. While GPT-3.5-Turbo encounters 16 infinite iterations, Claude-3.5-Sonnet doesnt face this issue. H.3 MAPEVAL-VISUAL Figure 16 illustrates the performance of models on MAPEVAL-VISUAL."
        },
        {
            "title": "I ADDITIONAL EXPERIMENT RESULTS",
            "content": "For additional experiments, we filtered questions from our textual/API dataset into three subcategories: 37 o 20 15 5 0 Sonnet G-Pro GPT-4o GPT-4 Ll3.1 G-Flash Ll3. 4o-mini Gemma2 Mixtral GPT-3.5 Figure 15: Number of times agent stopped due to iteration limit ) % ( r A 100 80 60 40 20 Place info Nearby Routing Counting Unanswerable Claude-3-5-Sonnet Gpt-4o-Mini Llama-3-VILA1.5-8B DocOwl1. Gpt-4o Gemini-1.5-Flash Glm-4v-9b Llava-v1.6-Mistral-7B Gpt-4-turbo Qwen2-VL-7B InternLm-Xcomposer2 Llava-1.5-7B Gemini-1.5-Pro MiniCPM-V-2 5 Paligemma-3B Figure 16: MAPEVAL-Visual categorical accuracy 1. Straight-Line Distance (47 questions): These questions require the computation of straightline distances, such as What is the straight-line distance between the Atomium in Brussels and the Belfry of Bruges? 2. Cardinal Direction (24 questions): These involve determining cardinal directions2, e.g., What is the direction of the Little Mermaid statue from Copenhagen Central Station? 3. Counting (23 questions): Questions involving counting entities, such as How many convenience stores are there within 400 radius of the Tokyo Tower? We visualized the accuracy of various LLMs on these subcategories under the MAPEVALTEXTUAL setting, with the following findings: 1. Straight-Line Distance: Figure 17, illustrates the accuracy on straight-line distance related questions. We can see that all models struggled, with the best accuracy being only 51.06%. 2. Cardinal Direction: Figure 18, illustrates the accuracy on cardinal-direction related questions. Here LLMs showed significant variability. While Claude-3.5-Sonnet achieved 91% accuracy, Gemma-2.0-27B scored only 16.67%. 3. Counting: Figure 19, illustrates the accuracy on counting related questions. In this case, Claude-3.5-Sonnet underperformed compared to the open-source Gemma-2.0-27B (60.87% accuracy). We identified scope for improvement in these areas and enhanced the models capabilities by integrating external tools (e.g., calculator) specifically designed for calculating straight-line distances and cardinal directions. For straight-line distances, we employed the Haversine formula3 to com2https://en.wikipedia.org/wiki/Cardinal_direction 3https://en.wikipedia.org/wiki/Haversine_formula 38 ) % ( r A 100 80 60 40 20 . 4 1 - 5 2 Q . n - 5 3 - a . 0 7 - 1 3 - l . l - 5 1 - m 4 - . 8 - 1 3 - l . 2 7 - 5 2 Q . 0 9 - 2 3 - l r - 4 - . 7 - 5 2 Q 7 8 - t . - 5 1 - m m - t M i - 4 - i - 5 . 3 - . 3 - 2 3 - l . 7 2 - 0 2 - e . 9 - 0 2 - e r - 5 3 - . Figure 17: Accuracy of LLMs on questions which needs Straight-Line Distance computation ) % ( r A 80 60 40 20 0 n - 5 . 3 - a B 0 7 - 1 . 3 - l 0 9 - 2 . 3 - l 2 7 - 5 . 2 Q 4 - o T - 4 - o - 5 . 1 - m h F - 5 . 1 - m 8 - 1 . 3 - l e - t B 7 - 5 . 2 Q 9 - 0 . 2 - e 7 8 - t B 4 1 - 5 . 2 Q m - 4 - i - 5 . 3 - o t - 5 . 3 - G 3 - 2 . 3 - l 7 2 - 0 . 2 - e Figure 18: Accuracy of LLMs on questions which needs Cardinal Direction computation pute the great-circle distance. To determine cardinal directions, we calculated the bearing4 between two geographic coordinates. Figures 20 and 21 demonstrate significant improvement in model performance with these tools. For straight-line distance-related questions, the best accuracy jumped from 51.06% to 85.11%. Similarly, for cardinal-direction questions, the top model achieved an accuracy of 95.83%, compared to the previous maximum of 91.67%. In the case of GPT-4o-mini, these enhancements led to even further progress, with the model demonstrating leap in both straight-line distance and cardinal direction accuracy, surpassing previous models. In the case of GPT-4o-mini, these enhancements led to even further progress, with the model demonstrating remarkable leap in both straight-line distance and cardinal direction accuracy. Specifically, the straight-line distance accuracy improved from 34.04% to 78.72%, while cardinal-direction accuracy increased from 29.17% to 91.67%. These results highlight the limitations of current LLMs in handling fine-grained geospatial queries independently and emphasize the value of augmenting LLM capabilities with external computational tools. Future work can explore the integration of more robust external services to address the nuances of spatial reasoning comprehensively. 4https://en.wikipedia.org/wiki/Bearing_(navigation) 39 ) % ( r A 80 60 40 20 0 ) % ( r A 100 80 40 20 0 . 7 2 - 0 2 - e . n - 5 3 - a . 2 7 - 5 2 Q u - 4 - o - t M . 4 1 - 5 2 Q M - 5 . 3 - P . - 5 1 - m . 0 7 - 1 3 - l . l - 5 1 - m o 4 - . 8 - 1 3 - l . 0 9 - 2 3 - l i - 4 - G . 7 - 5 2 Q u - 5 3 - . . 3 - 2 3 - l B 7 8 - t . 9 - 0 2 - e Figure 19: Accuracy of LLMs on Counting related questions n - 5 . 3 - a 0 7 - 1 . 3 - l h F - 5 . 1 - m 4 - B 0 9 - 2 . 3 - l r - 4 - B 7 8 - t o - 5 . 1 - m i - 4 - B 9 - 0 . 2 - e r - 5 . 3 - LLM LLM + Calculator Figure 20: Improved accuracy of LLMs after integrating calculator to compute Straight-Line Distance ) % ( r A 100 60 40 20 0 n - 5 . 3 - a 4 - G 0 7 - 1 . 3 - l 0 9 - 2 . 3 - l P - 5 . 1 - m b - 4 - h F - 5 . 1 - m 9 - 0 . 2 - e B 7 8 - t i - 4 - o t - 5 . 3 - LLM LLM + Calculator Figure 21: Improved accuracy of LLMs after integrating calculator to compute Cardinal Direction"
        }
    ],
    "affiliations": [
        "Bangladesh Computer Council (BCC)",
        "Department of Computer Science and Engineering Bangladesh University of Engineering and Technology (BUET)",
        "Monash University",
        "Qatar Computing Research Institute (QCRI)",
        "Statistics, Islamic University Bangladesh"
    ]
}