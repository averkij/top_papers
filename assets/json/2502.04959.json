{
    "paper_title": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces",
    "authors": [
        "Daniel Marczak",
        "Simone Magistri",
        "Sebastian Cygert",
        "Bartłomiej Twardowski",
        "Andrew D. Bagdanov",
        "Joost van de Weijer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Model merging integrates the weights of multiple task-specific models into a single multi-task model. Despite recent interest in the problem, a significant performance gap between the combined and single-task models remains. In this paper, we investigate the key characteristics of task matrices -- weight update matrices applied to a pre-trained model -- that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github.com/danielm1405/iso-merging ."
        },
        {
            "title": "Start",
            "content": "No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Daniel Marczak 1 2 Simone Magistri 3 Sebastian Cygert 2 4 Bartłomiej Twardowski 2 5 6 Andrew D. Bagdanov 3 Joost van de Weijer"
        },
        {
            "title": "Abstract",
            "content": "Model merging integrates the weights of multiple task-specific models into single multi-task model. Despite recent interest in the problem, significant performance gap between the comIn this bined and single-task models remains. paper, we investigate the key characteristics of task matrices weight update matrices applied to pre-trained model that enable effective merging. We show that alignment between singular components of task-specific and merged matrices strongly correlates with performance improvement over the pre-trained model. Based on this, we propose an isotropic merging framework that flattens the singular value spectrum of task matrices, enhances alignment, and reduces the performance gap. Additionally, we incorporate both common and task-specific subspaces to further improve alignment and performance. Our proposed approach achieves state-of-the-art performance across multiple scenarios, including various sets of tasks and model scales. This work advances the understanding of model merging dynamics, offering an effective methodology to merge models without requiring additional training. Code is available at https://github. com/danielm1405/iso-merging. 5 2 0 2 7 ] . [ 1 9 5 9 4 0 . 2 0 5 2 : r 1. Introduction Pre-trained models are the foundation of modern machine learning systems (Carion et al., 2020; Radford et al., 2021; Caron et al., 2021; Zhai et al., 2023). In practice, 1Warsaw University of Technology, Poland 2IDEAS NCBR, Warsaw, Poland 3Department of Information Engineering, UniItaly 4Gdansk University of Technolversity of Florence, ogy, Poland 5Computer Vision Center, Barcelona, Spain 6Department of Computer Science, Universitat Autonoma de Correspondence to: Daniel Marczak Barcelona, Spain. <daniel.marczak.dokt@pw.edu.pl>. Preprint. Under review. 1 Figure 1. Spectrum of singular values for single layer weight update matrix obtained by merging using Task Arithmetic (top) compared to our approaches: Iso-C (middle) and Iso-CTS (bottom). Task Arithmetic sums the task-specific matrices, which result in spectrum with few dominant components. Iso-C instead replaces this spectrum with uniform one, which results in significant performance improvement. Iso-CTS enhances the common subspace with task-specific subspaces and yields state-of-the-art model merging performance. they are typically fine-tuned for specialization on specific tasks (Wortsman et al., 2022b; Ilharco et al., 2022). Recently, growing body of research has focused on model merging (Li et al., 2023), which combines multiple task-specific experts into single multi-task model. Many methods have been proposed to improve the effectiveness of model merging by reducing sign conflicts (Yadav et al., 2023), by aligning gradients (Daheim et al., 2024), or through magnitudebased selection (Marczak et al., 2024). However, significant performance gap between the combined and single-task models remains. key insight from Ilharco et al. (2023) is that task vectors, defined as the offset between the flattened fine-tuned weights and the pre-trained checkpoint, from different tasks are typically close to orthogonal. This orthogonality has been seen as fundamental property enabling effective merging with reduced interference and has inspired works that enforce the orthogonality by modifying the fine-tuning proceNo Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces dure (Po et al., 2024). Most recently, Stoica et al. (2024) and Gargiulo et al. (2024) have shown that accounting for the structure of the weight update matrix, dubbed task matrix, is more effective strategy for improving the performance of model merging. In this paper, we investigate precisely what the characteristics of task matrices are that favor effective model merging. Different from previous works, we propose to analyze the alignment between task-specific and merged subspaces. Specifically, to capture the similarity between task matrices, we propose to investigate the Subspace Alignment Ratio. Through the lens of Singular Value Decomposition, our metric quantifies the similarity between subspaces spanned by the top singular vectors of task matrices. When applied to compare matrices of the merged model to the task-specific ones, this metric strongly correlates with the performance of the merged model on given task. This allows us to identify the directions amplified by multiple tasks as well as the underrepresented directions that lead to poor performance on corresponding tasks. Our goal is to design model merging technique that balances directions in the weight space across different tasks. We achieve this by flattening the singular values spectrum of the merged matrix, making it more uniform. Enforcing uniform (isotropic) spectrum significantly improves the alignment and performance of the merged model. This simple yet effective adjustment, which requires no changes to the fine-tuning procedure, leads to substantial gains in merging performance (see method Iso-C in Figure 1). However, tasks with dominant directions of smaller intensity compared to the majority of tasks and whose directions are orthogonal to the common directions may still remain underrepresented, especially when the number of tasks increases. To address this, we enhance isotropic model merging by introducing task-specific subspaces that retain unique task features while preserving shared knowledge. Our approach begins with the top singular values of the common subspace and iteratively replaces the least significant singular vectors with task-specific directions. This strategy allows us to increase the scalability of our merging approach to more tasks (see method Iso-CTS in Figure 1). The main contributions of this paper are: We show that the alignment between the subspace spanned by the principal directions of the task-specific matrices and that of the merged matrix positively correlates with the performance of the merged model.- We demonstrate that applying an isotropic scaling to singular directions of merged task matrices improves the alignment between merged and task-specific matrices. This results in simple yet highly effective technique for model merging that we call Iso-C, which outperforms most baselines. We further enhance our approach by incorporating taskspecific directions into the merged matrix resulting in Iso-CTS, merging method that achieves state-ofthe-art results, in particular for large number of tasks. 2. Related Work Model merging. Pre-trained models serve as foundation for expert models specialized in specific downstream tasks (Radford et al., 2021). Recently, model merging has emerged as promising technique to combine multiple expert models into single multi-task model. One of the pioneering works in the field, Task Arithmetic (TA) (Ilharco et al., 2023), proposed to compute task vector as difference between the expert and the pre-trained model and to then aggregate task vectors via scaled addition to create an expert in multiple tasks. The significant performance gap between individual experts and the combined model sparked an abundance of works with the aim of reducing interference when merging models. TIES (Yadav et al., 2023) proposed novel way to reduce sign conflicts between the parameters of expert models, Model Breadcrumbs (Davari & Belilovsky, 2024) removed outliers from the task vectors, and Consensus Merging (Wang et al., 2024b) removed catastrophic and selfish weights. These methods focused on per-parameter techniques to mitigate the interference, treating each parameter independently. Singular Value Decomposition of model weights. While SVD of weight matrices has been primarily used for model compression (Denton et al., 2014; Kim et al., 2016), recently its effectiveness was also identified for fine-tuning of large models. LoRA (Hu et al., 2021) uses SVD to identify the similarities of weight updates between low-rank and fullrank fine-tuning. MiLORA (Wang et al., 2024a) identifies that the bottom singular components correspond to noisy or long-tail information, while the top singular vectors contain important knowledge. Therefore, they propose fine-tuning approach that updates only the minor singular components of the weight matrix while keeping the top singular components frozen. SVFT (Lingam et al., 2024) computes outer products of its singular vectors and, during fine-tuning updates, only sparse coefficients of these combinations. SVD for model merging. The structure imposed by SVD was used for model merging in KnOTS (Stoica et al., 2024), which proposes to concatenate the task-specific low-rank adaptation matrices (LoRA) and average the right-singular vectors before SVD reconstruction to obtain the merged weights. The most similar work to us is the parallel work Task Singular Vectors (TSV) (Gargiulo et al., 2024), which measures task interference based on the interaction of sin2 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces gular vectors from different tasks and uses it to increase merging effectiveness. We share the motivation to improve model merging through SVD decomposition. However, while they focus on the orthogonalization of task-specific subspaces to reduce interference, we show that making singular values uniform in common subspace is surprisingly powerful method. Further, we show how to combine shared and task-specific subspaces for improved performance. 3. Background and Motivation In this section, we first describe the general framework of model merging and provide the notation used throughout the rest of the paper. We then motivate our approach via an analysis of the correlation between task similarity and performance improvement of the merged model. 3.1. Model Merging Model merging integrates multiple deep neural network models, each individually trained (i.e. fine-tuned) on distinct tasks starting from the same pre-trained model, into single merged model. Let θ0 denote the weights of the pre-trained network, and θt denote the fine-tuned weights for task t, with = 1, . . . , , where is the total number of tasks. We will use the notation θ(l) to identify the weights of layer for task and to denote the total number of layers in network. The objective of model merging is to find merging function , such that the model: = (θ(ℓ) θ(ℓ) 0 , {θ(ℓ) }T t=1), ℓ = 1, . . . , (1) is able to perform all tasks on which the individual models θt are trained. Building upon Task Arithmetic (TA), we define the layerwise task matrix (ℓ) as the difference between the weights of the model θt and the pre-trained model θ0 for layer ℓ: (ℓ) = θ(ℓ) θ(ℓ) 0 . (2) In the rest of the paper, the ℓ superscript is omitted when not relevant to the discussion, and all definitions refer to an arbitrary layer. The authors of Task Arithmetic propose to solve the problem of model merging by defining merging function that sums all task matrices to the pre-trained model weights: TA = θ(ℓ) θ(ℓ) 0 + α(ℓ) TA , (3) TA = (cid:80)T where α is scaling factor determined on held-out validation dataset and (ℓ) t=1 (ℓ) . The advantage of this merging strategy is that it allows for the reuse and transfer of knowledge from many fine-tuned models to the pre-trained model without requiring additional training or access to the original training data (Ilharco et al., 2023). (a) Cosine similarity between pairs of task vectors. (b) NAI vs cosine similarity between task and merged vectors. Figure 2. (a) Tasks vectors are typically close to orthogonal to each other. (b) Models with very different normalized accuracy improvements (NAI) exhibit very close cosine similarities, and the correlation between cosine similarity and NAI is low. 3.2. Cosine Similarity and Performance Improvement are Uncorrelated Starting from the definition of Task Arithmetic (TA) in Eq. (3), we aim to explore the possible reasons for the improvement achieved by TA merging over the pre-trained (or zero-shot) model across multiple tasks. To empirically quantify performance gain, we propose the Normalized Accuracy Improvement (NAI) metric, defined as: NAI(θM , θt; θ0) = Acc(θM ) Acc(θ0) Acc(θt) Acc(θ0) , (4) which quantifies the improvement of the merged model θM relative to that achieved by the task-specific model θt, both measured with respect to the zero-shot baseline θ0. Ilharco et al. (2023) hypothesize that the effectiveness of task vector summation arises from the cosine similarity between the vectorized representations of the task matrices being close to zero, i.e. vec(i), vec(j) 0 for = j, which minimizes inter-task interference. Based on this intuition, we measured the correlation between the cosine similarity of each task vector with the merged model vector and the normalized accuracy improvement NAI(θTA, θt; θ0). However, we observe no clear correlation (see Figure 2). This suggests that the underlying reason for the performance improvement of the Task Arithmetic update over the zeroshot model likely originates from other factors, which we show below can be unveiled via spectral analysis of the Task Arithmetic and task-specific matrices. 3.3. Performance Correlates with Subspace Alignment We argue that the improvement in Task Arithmetic performance derives from the relationship between the top singular 1NAI differs from Normalized Accuracy (Ortiz-Jimenez et al., 2023) which does not account for zero-shot performance. No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces (a) Normalized Accuracy Improvement (NAI) vs. Average Subspace Alignment Ratio (SARavg). (b) Average Subspace Alignment Ratios (SARavg) between pairs of task vectors. Figure 3. (a) NAI strongly correlates with SARavg (Pearson correlation coefficient ρT = 0.88). (b) Note the groups of highly aligned tasks such as {MNIST, SVHN, GTSRB} and {EuroSAT, RESISC45}. By comparing (b) and (a), the mutually aligned datasets exhibit higher alignment with the merged model and consequently achieve good performance. On the other hand, tasks with low mutual alignment, such as DTD, Cars, and SUN397, are less aligned with the merged model and achieve poor performance. vectors of TA and those of each t. Specifically, we hypothesize that the subspace of TA approximates the union of the subspaces of each t, and that the overlap of this overall subspace with each task matrix correlates with the performance improvement of the merged model. In order to empirically quantify the overlap between subspaces, we propose the Subspace Alignment Ratio (SAR) metric. Without loss of generality, we define SAR between source task matrix src and target task matrix trg, with respect to generic merged task matrix M, as: SAR(src, trg; kM) = ΠkM,trgsrcF srcF , (5) where ΠkM,trg = UkM,trgU kM,trg is the projection matrix onto the subspace spanned by the top kM left-singular vectors of trg. The columns of UkM,trg are obtained from the SVD decomposition of trg, and the number of singular vectors used (kM ) is determined from the merged task matrix by minimizing the approximation error: kM = min{k : Πk,MMF ϵMF }, (6) with ϵ = 0.05. SAR quantifies the alignment between the subspaces of two task matrices as function of the number of dominant singular vectors of the merged matrix. To provide single score measuring the overlap between two models, we denote with SARavg the Average Subspace Alignment Ratio across all layers. In Figure 3a (left, represented by stars), we plot the Normalized Accuracy Improvement achieved by TA on each task, given by NAI(θTA, θt; θ0), against the Average Subspace Alignment Ratio of each task matrix with the merged task matrix TA, i.e. SARavg(t, TA; kTA). First, we note that the alignment between task and merged matrices are notably high (ranging from 0.75 to 0.87), but vary significantly across datasets. This suggests that task vectors are well represented in the subspace identified by the task-arithmetic matrix but with different degrees of alignment and consistency depending on dataset characteristics. Furthermore, we highlight strong correlation (ρ = 0.88) between the performance improvement on individual tasks achieved by θTA and the degree of alignment of with TA. In Figure 3b, we report the average alignment ratios between pairs of tasks, i.e. SARavg(i, j; kTA). Some groups of tasks exhibit higher alignment which is due to their semantic similarity, e.g. MNIST, SVHN, and GTSRB are digit recognition datasets, while EuroSAT and RESISC45 are satellite image datasets. On the other hand, datasets such as Cars, DTD or SUN397 are less aligned to other tasks. Most importantly, tasks belonging to highly aligned groups are also highly aligned with the TA model and achieve the highest accuracy improvements (see Figure 3a). The tasks that are not aligned are underrepresented in the dominant subspace of TA, and the performance on them is low. Based on the observed correlation between performance No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces and alignment ratio, we hypothesize that merging method that aims to achieve high alignment will also achieve strong performance. Therefore, in the next section, we propose an approach called Isotropic Merging that improves alignment and, most importantly, the performance of the merged models. 4. Isotropic Merging in Common and Task-specific Subspaces In this section, we propose novel model merging method we call Isotropic Merging in Common and Task-Specific Subspaces (Iso-CTS). First, we introduce Isotropic Merging in Common Subspace (Iso-C), which is able to enhance the normalized accuracy improvement and the alignment of each task matrix using common directions identified by Task Arithmetic. Then, we show how to further enhance the performance of merged models by introducing taskspecific directions to improve merging performance on sets of many diverse tasks. 4.1. Isotropic Merging in Common Subspace In Section 3.3, we demonstrated the high alignment of each task matrix with the matrix obtained by Task Arithmetic. This alignment indicates that the span of dominant singular vectors of the merged matrix effectively covers the subspace of each task and provides good approximation of the common subspace. However, significant variability in the average alignment ratio across the dataset leads to lower accuracy improvement for less correlated tasks compared to more correlated ones. This variability stems from the skewness of the task arithmetic spectrum (Figure 1 and 7), which is concentrated in the first few singular values (which we call top or dominant), favoring more correlated tasks. Our proposed methodology, which we call Isotropic Merging in Common Subspace (Iso-C), aims to equalize the spectrum of the task arithmetic matrix in order to enhance the average subspace alignment ratio and ensure more balanced representation across tasks in the merged model. Consider the sum of task matrices TA = (cid:80) t, where Rmn. Via Singular Value Decomposition (SVD) on TA we obtain TA = ΣV , where Rmr and Rnr represent, respectively, the left and right singular vectors of TA, and Σ Rrr is the diagonal matrix containing the singular values. We denote the vector of singular values by σ = diag(Σ) Rr. To reduce the skewness towards the dominant singular vectors of TA, we propose scaling all directions of the transformation applied by the right-singular vectors to fixed value rather than using their corresponding singular values. This ensures that the final transformation is isotropic, with Algorithm 1 Iso-C: Isotropic Merging in Common Subspace Require: Task matrices 1, . . . , with Rmn 1: Sum task matrices: TA = (cid:80)T 2: Compute the SVD of TA: TA = ΣV , with t=1 Rmr, Σ Rrr, Rnr, σ = diag(Σ) Rr (Eq.7) (Eq.8) 3: Calculate isotropic factor: σ = 1 i=1 σi 4: Reconstruct the matrix: Iso-C = σU 5: return Iso-C (cid:80)r the scaling factor set to the average singular value: σ = 1 (cid:88) i= σi, (7) and merged matrix is computed using the reconstruction: Iso-C = σU . (8) We apply this operation to all network layers, and the final merged model is defined as: Iso-C = θ(ℓ) θ(ℓ) 0 + α(ℓ) Iso-C, ℓ = 1, . . . , (9) where α is chosen on held-out validation set. Applying isotropic merging results in an enhancement of the normalized accuracy improvement and the alignment of each task subspace with the top singular vectors of the task arithmetic matrix (see Figure 3a). In Algorithm 1, we give the Iso-C model merging algorithm for single layer. 4.2. Isotropic Merging in Common and Task-Specific Subspaces The effectiveness of Iso-C depends on how well the common subspace identified by the dominant singular vectors of TA approximates the subspaces of the individual tasks. The approximation error arises from how these tasks interact when summed. The top singular directions of TA capture only the dominant common variations, while singular vectors associated with near-zero singular values provide negligible information. At the same time, tasks with dominant directions of smaller intensity compared to the majority of tasks and whose directions are orthogonal to the common directions remain underrepresented. This limitation becomes more pronounced as the number of tasks increases and the tasks become more diverse. To address this limitation, we propose enhancing the range of directions used by Iso-C to ensure that the task-specific directions, which are orthogonal to those of the common subspace, are incorporated into the singular basis of the final merged matrix. We call this methodology as Isotropic Merging in Common and Task-Specific Subspaces (Iso-CTS). 5 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Algorithm 2 Iso-CTS: Isotropic Merging in Common and Task-Specific Subspaces (green shared with Iso-C) Require: Task matrices 1, . . . , with Rmn 1: Sum task matrices TA = (cid:80)T 2: Compute the SVD of TA: TA = ΣV , with t=1 Rmr, Σ Rrr, Rnr, σ = diag(Σ) Rr 3: Retain top-k singular vectors and values from common subspace: 1:k = [u1 . . . uk] 1:k = [v1 . . . vk] σcm = diag(Σ)1:k 4: Accumulate task-specific directions via projection: 5: for = 1 to do 6: = 1:k(U 1:k)t Compute SVD: = tΣtV 7: Retain first = rk 8: components of and t: 1:s = [vt,1 . . . vt,s] 1:s = [ut,1 . . . ut,s] = diag(Σt)1:s σts (Eq.10) 9: end for 10: Combine common and task-specific spaces: 1:s ] Rmr 1:s ] Rnr = [U 1:kU = [V 1:kV 1:s 1 . . . 1:s 1 . . . 11: Orthogonalize and via whitening 12: Calculate isotropic factor σ: (Eq.11) σ = 1 (cid:16) (cid:88) (cid:88) (cid:88) (cid:17) σts t,i σcm + i=1 t=1 13: Reconstruct the matrix Iso-CTS = σUV 14: return Iso-CTS i=1 (Eq.13) (Eq.12) Our approach starts with the top singular values of the common subspace and iteratively replaces the singular vectors associated with the lowest singular values with task-specific directions. The final goal is to find two orthonormal matrices Rmr and Rnr whose columns contain both common and task-specific directions. Afterward, the final matrix is reconstructed, and isotropic merging is applied. In the following, we provide detailed explanation of our proposed algorithm. Retaining components from the common subspace. We retain the top-k singular vectors associated with the subspace identified by TA: 1:k = [u1 . . . uk] 1:k = [v1 . . . vk], where 1:k, 1:k are the top-k leftand right-singular vectors from the SVD of TA. We analyze the impact of selecting in Section 5.3. Accumulating task-specific directions. We project each task-specific matrix onto the subspace orthogonal to the common subspace, i.e. the space spanned by top leftsingular directions of the common subspace 1:k: = 1:k(U 1:k)T t. (10) We then compute the SVD of = ΣtV and retain the top = rk directions for each task t: 1:s = [ut,1 . . . ut,s] 1:s = [vt,1 . . . vt,s], = 1,. . ., T. The orthogonal projection Eq. (10) guarantees that both the leftand right-singular vectors of t, representing taskspecific directions, are orthogonal to the subspace spanned by the common directions (given by 1:k). 1:s 1 . . . Combining common and task-specific matrices. After identifying the principal vectors for the common subspace and = rk principal vectors for each task, we now combine the common and task-specific directions by con1:s ] Rmr and catenating them: = [U 1:kU 1:s = [V 1:kV 1 . . . 1:s ] Rnr. Orthogonalization. There is no guarantee that the leftand right-singular task-specific vectors are orthogonal to each other, as we are only projecting each task matrix onto the common subspace. To reconstruct the final merged matrix, we must orthogonalize and V. Following Gargiulo et al. (2024), we compute the SVD of = PU ΣU and V = PV ΣV , and whiten (Schonemann, 1966): U = PU U = PV V . (11) Isotropic scaling and reconstruction. Finally, we reconstruct the final merged matrix and apply isotropic merging: Iso-CTS = σUV , (12) where σ is obtained by averaging the singular values associated with the vectors selected for both common and task-specific subspaces. Specifically, defining σcm = diag(Σ)1:k Rk, the vector of singular values associated with the common subspace identified by U1:k and V1:k, and = diag(Σt)1:s Rs, with = rk σts , the vector of singu1:s lar values associated with each task-specific subspace and , we define the scaling factor as: 1:s σ = 1 (cid:16) (cid:88) σcm + (cid:88) (cid:88) (cid:17) . σts t,i i=1 t=1 i= (13) Finally, similar to ISO-C, the merged model is defined as: Iso-CTS = θ(ℓ) θ(ℓ) 0 + α(ℓ) Iso-CTS, ℓ = 1, . . . , (14) where α is chosen on held-out validation set. 6 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Table 1. Iso-CTS achieves state-of-the-art performance for all backbones on all evaluated scenarios. We present average absolute accuracy and average normalized accuracy (in subscript) in %. The best method in bold and the second-best underlined. Method Zero-shot Fine-tuned ViT-B/32 ViT-B/16 ViT-L/14 8 tasks 14 tasks 20 tasks 8 tasks 14 tasks 20 tasks 8 tasks 14 tasks 20 tasks 48.3 92.8 57.2 90.9 56.1 91.3 55.3 94.6 61.3 92.8 59.7 93. 64.7 95.8 68.2 94.3 65.2 94.7 Weight Averaging Task Arithmetic TIES Consensus TA TSV-M Iso-C (Ours) Iso-CTS (Ours) 66.3(72.1) 70.8(76.5) 75.1(81.0) 75.0(80.8) 85.9(92.3) 86.3(92.9) 86.2(92.8) 64.3(71.1) 65.3(72.1) 68.0(74.8) 70.4(77.4) 80.1(87.9) 80.3(88.1) 81.7(89.7) 61.0(67.5) 60.5(66.8) 63.4(69.9) 65.4(72.0) 77.1(84.3) 75.5(82.5) 78.1(85.5) 72.2(76.6) 75.4(79.6) 79.7(84.3) 79.4(83.9) 89.0(93.9) 90.6(95.6) 91.1(96.1) 69.5(74.8) 70.5(75.9) 73.2(78.7) 74.4(79.9) 84.6(91.0) 84.8(91.1) 86.4(92.8) 65.3(70.4) 65.8(70.8) 68.2(73.3) 69.8(74.9) 80.6(86.5) 79.6(85.4) 82.4(88.4) 79.6(83.2) 84.9(88.7) 86.9(90.7) 86.3(90.1) 93.0(97.0) 94.2(98.3) 94.7(98.8) 76.7(81.1) 79.4(84.0) 79.5(84.1) 82.2(86.9) 89.2(94.4) 89.3(94.5) 91.0(96.3) 71.6(75.6) 74.0(78.1) 75.7(79.8) 79.0(83.2) 87.7(92.5) 87.6(92.2) 90.1(94.9) (a) Spectra of singular values for different values of interpolation coefficient (β). (b) Average Subspace Alignment Ratio (SARavg) vs. interpolation coefficient (β). (c) Normalized Accuracy Improvement (NAI) vs. interpolation coefficient (β). Figure 4. (a) Interpolating from TA (β = 0) towards Iso-C (β = 1) makes the spectrum of singular values of more uniform and increases the number of preserved components kM (Eq. (6)) denoted by dashed lines. (b) This results in an increased alignment between each task-specific model and merged model measured by SARavg. (c) As alignment increases, the performance also improves as predicted based on the strong correlation between these two properties investigated in Section 3.3. 5. Experimental Results 5.1. Experimental setup We evaluate our approaches over sets of 8, 14, and 20 datasets, following Wang et al. (2024b). We provide the details of the datasets in Appendix A.1. We consider three variants of CLIP (Radford et al., 2021) with ViT-B/32, ViTB/16 and ViT-L/14 as visual encoders (Dosovitskiy et al., 2021). We use the checkpoints fine-tuned on the tasks above, provided in (Wang et al., 2024b). If not stated otherwise, we present the results using the ViT-B/16 visual encoder. We compare our approaches with the following model merging methods: weight averaging (Wortsman et al., 2022a), Task Arithmetic (Ilharco et al., 2023), TIES-Merging (Yadav et al., 2023), Consensus TA (Wang et al., 2024b) and TSV-M (Gargiulo et al., 2024). We include the results of the zero-shot model and fine-tuned models serving as lowerand upper-bound, respectively. We compare the results based on absolute and normalized accuracy following standard practice (Wang et al., 2024b; Gargiulo et al., 2024). 5.2. Multi-task model merging Table 1 presents our main results for multi-task model merging. Iso-CTS achieves state-of-the-art results in all of the settings. Iso-C achieves very similar results to Iso-CTS in the 8 task scenario. However, Iso-CTS significantly outperforms Iso-C when merging 14 and 20 models, with improvements of up to 2.8% in absolute accuracy. This suggests that it is possible to faithfully represent small number of tasks in the common subspace. However, when the number of tasks increases, it becomes crucial to retain important directions from the task-specific subspaces in order to maximize model merging effectiveness. 5.3. Analysis and Ablations From Task Arithmetic to Isotropic Merging. We analyze what happens when interpolating between the singular values obtained by Task Arithmetic (TA) and those obtained by Iso-C, i.e. the model with the following spectra: Σβ = (1 β)ΣTA + βΣIso-C, (15) where β is an interpolation coefficient. Firstly, Figure 4a presents the change in singular values spectrum as we interpolate towards Iso-C (β 1). The skewed spectrum 7 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces (a) Normalized Accuracy Improvement (NAI) of model created by retaining components of Iso-C (associated with top-k singular vectors from TA). (b) Average Subspace Alignment Ratios (SARavg) between merged and taskspecific models for varying sets of tasks. (c) Distribution of accuracies of the merged models for varying sets of tasks. Figure 5. (a) The directions associated with the least significant singular values of TA have minor contribution to the performance of Iso-C model. (b) Task-specific directions introduced in Iso-CTS improve the Average Subspace Alignment Ratio (SARavg) between task-specific models and the merged model compared to Iso-C which uses only common subspace. (c) Higher alignment translates to higher accuracy of Iso-CTS with respect to Iso-C. achieved by Task Arithmetic becomes isotropic, i.e. the scaling factor is equal along all of the singular directions. In Figure 4b we observe steady increase in alignment between task-specific and merged models as measured by SARavg (Eq. (5)), and Figure 4c shows that as alignment increases (with β 1), the performance of the merged model improves across all tasks. These results are consistent with our findings from Section 3.3 that show strong correlation between alignment and the performance of the final model. The impact of singular directions on performance. We analyze which singular directions contribute to the improvement of individual tasks. We truncate the flattened spectrum of Iso-C, keeping the directions associated with the leftmost singular values, i.e. σi = σ for and σi = 0 for > k. Note that the leftmost directions are the ones associated with the highest singular values of TA. We plot the task-wise Normalized Accuracy Improvement (NAI, Eq. (4)) for varying in Figure 5a. We observe that the first few directions are responsible for rapid improvement on several tasks. Notably, these tasks belong to the aligned groups identified in Section 3.3 such as {MNIST, SVHN, GTSRB} and {EuroSAT, RESISC45}. Moreover, the directions associated with the least significant singular values of TA have negligible contribution to the performance. This supports our intuition for replacing less significant common directions with task-specific components in Iso-CTS (see Section 4.2). Figure 5b shows that Iso-CTS achieves higher Average Subspace Alignment Ratio (SARavg, Eq. (5)) than Iso-C. Most importantly, Figure 5c shows that thanks to the addition of task-specific directions, Iso-CTS achieves better performance across tasks. Size of the common subspace for Iso-CTS. While Iso-C operates only in the common subspace, Iso-CTS enhances it with task-specific subspaces. Therefore, we Figure 6. Iso-CTS is robust to the selected size of the common subspace as any value leads to improvement over Iso-C. These results are for the 20-task scenario. ) when merging 20 tasks. When must select the size of the common subspace (and consequently the size of each task-specific subspace given by rk ). Figure 6 plots the relationship between accuracy and the fraction of subspace assigned for the common subspace ( = 1 Iso-CTS is equivalent to Iso-C and suffers 2.8% drop in accuracy from the maximum. The optimal fraction of common subspace = 0.8, and we use this as default value for Iso-CTS across all settings. Moreover, note that Iso-CTS is quite robust to the selection of this hyperparameter any (0.0, 1.0) offers performance improvement over Iso-C while the performance for [0.5, 0.9] varies by less than 0.5% from the optimal one. 6. Conclusion In this work, we introduced an isotropic model merging framework that enhances alignment between task-specific and merged model subspaces to significantly improve the multi-task performance of the final merged model. We proposed Iso-C, which leverages Singular Value Decomposition to equalize singular values and create more balanced representation across tasks, and Iso-CTS, which further 8 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces incorporates task-specific directions to retain unique task features while preserving shared knowledge. Iso-CTS achieves state-of-the-art results across multiple model scales and task sets, demonstrating that subspace alignment is critical factor in effective model merging. These findings provide new insights into model merging and pave the way for the future development of more effective techniques for combining the knowledge of multiple models. Limitations. The common subspace is determined by Task Arithmetic, which can be suboptimal, and better methods can be developed. We consider only vision tasks, and future work could extend our findings to other domains, such as natural language processing."
        },
        {
            "title": "Impact Statement",
            "content": "This paper aims to advance the field of Machine Learning, specifically the subfield focused on merging models finetuned on different tasks to create more effective multitask model. With the growing popularity of deep learning, increasingly powerful open-source models are becoming widely available and are being adopted in both research and industry. Advances in model merging could enhance the flexibility of utilizing these models by providing an efficient way to combine their specialized capabilities. Beyond this, our paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv: 1607.06450, 2016. Bossard, L., Guillaumin, M., and Van Gool, L. Food101 Mining Discriminative Components with Random Forests. In ECCV, 2014. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. End-to-end object detection with transformers. ECCV, 2020. Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. ICCV, 2021. Cheng, G., Han, J., and Lu, X. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 2017. Clanuwat, T., Bober-Irizar, M., Kitamoto, A., Lamb, A., Yamamoto, K., and Ha, D. Deep Learning for Classical Japanese Literature. arXiv preprint arXiv: 1607.06450, 2018. Coates, A., Ng, A., and Lee, H. An Analysis of SingleLayer Networks in Unsupervised Feature Learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings, 2011. Cohen, G., Afshar, S., Tapson, J., and van Schaik, A. EMNIST: Extending MNIST to handwritten letters. In IJCNN, 2017. Daheim, N., Mollenhoff, T., Ponti, E. M., Gurevych, I., and Khan, M. E. Model merging by uncertainty-based gradient matching. In ICLR, 2024. Davari, M.-J. and Belilovsky, E. Model breadcrumbs: Scaling multi-task model merging with sparse masks. ECCV, 2024. Denton, E. L., Zaremba, W., Bruna, J., LeCun, Y., and Fergus, R. Exploiting linear structure within convolutional networks for efficient evaluation. In NeurIPS, 2014. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Gargiulo, A. A., Crisostomi, D., Bucarelli, M. S., Scardapane, S., Silvestri, F., and Rodol`a, E. Task singular vectors: Reducing task interference in model merging. arXiv preprint arXiv: 2412.00081, 2024. Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., Cukierski, W., Tang, Y., Thaler, D., Lee, D.-H., Zhou, Y., Ramaiah, C., Feng, F., Li, R., Wang, X., Athanasakis, D., Shawe-Taylor, J., Milakov, M., Park, J., Ionescu, R., Popescu, M., Grozea, C., Bergstra, J., Xie, J., Romaszko, L., Xu, B., Chuang, Z., and Bengio, Y. Challenges in Representation Learning: Report on Three Machine Learning Contests. Neural Networks, 2013. Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and Vedaldi, A. Describing textures in the wild. In CVPR, 2014. Hu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., and Chen, W. Lora: Low-rank adaptation of large language models. ICLR, 2021. 9 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Ilharco, G., Wortsman, M., Gadre, S. Y., Song, S., Hajishirzi, H., Kornblith, S., Farhadi, A., and Schmidt, L. Patching open-vocabulary models by interpolating weights. In NeurIPS, 2022. Ilharco, G., Ribeiro, M. T., Wortsman, M., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic. In ICLR, 2023. Kim, Y., Park, E., Yoo, S., Choi, T., Yang, L., and Shin, D. Compression of deep convolutional neural networks for fast and low power mobile applications. In ICLR, 2016. Krause, J., Stark, M., Deng, J., and Fei-Fei, L. 3D Object representations for fine-grained categorization. In ICCV Workshops, 2013. Krizhevsky, A. and Hinton, G. features from tiny images. Learning multiple Technical layers of Report 0, University of Toronto, Toronto, Ontario, URL https://www.cs.toronto.edu/ 2009. kriz/learning-features-2009-TR.pdf. Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradientbased learning applied to document recognition. Proceedings of the IEEE, 1998. Li, W., Peng, Y., Zhang, M., Ding, L., Hu, H., and Shen, L. Deep model fusion: survey. arXiv preprint arXiv: 2309.15698, 2023. Lingam, V., Tejaswi, A., Vavre, A., Shetty, A., Gudur, G. K., Ghosh, J., Dimakis, A., Choi, E., Bojchevski, A., and Sanghavi, S. SVFT: parameter-efficient fine-tuning with singular vectors. CoRR, abs/2405.19597, 2024. Marczak, D., Twardowski, B., Trzcinski, T., and Cygert, S. MagMax: Leveraging Model Merging for Seamless Continual Learning. In ECCV, 2024. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Y. Reading digits in natural images with unsupervised feature learning. In NeurIPS Workshops, 2011. Nilsback, M.-E. and Zisserman, A. Automated Flower Classification over Large Number of Classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 2008. Ortiz-Jimenez, G., Favero, A., and Frossard, P. Task arithmetic in the tangent space: Improved editing of pretrained models. In NeurIPS, 2023. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In CVPR, 2012. 10 Po, R., Yang, G., Aberman, K., and Wetzstein, G. Orthogonal adaptation for modular customization of diffusion models. In CVPR, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In ICML, 2021. Schonemann, P. H. generalized solution of the orthogonal procrustes problem. Psychometrika, 1966. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A., and Potts, C. Recursive deep models for semantic compositionality over sentiment treebank. In EMNLP, 2013. Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The german traffic sign recognition benchmark: multi-class classification competition. In IJCNN, 2011. Stoica, G., Ramesh, P., Ecsedi, B., Choshen, L., and Hoffman, J. Model merging with svd to tie the knots. arXiv preprint arXiv: 2410.19735, 2024. Veeling, B. S., Linmans, J., Winkens, J., Cohen, T., and Welling, M. Rotation Equivariant CNNs for Digital Pathology. In MICCAI, 2018. Wang, H., Xiao, Z., Li, Y., Wang, S., Chen, G., and Chen, Y. Milora: Harnessing minor singular components for parameter-efficient LLM finetuning. CoRR, abs/2406.09044, 2024a. Wang, K., Dimitriadis, N., Ortiz-Jimenez, G., Fleuret, F., and Frossard, P. Localizing task information for improved model merging and compression. In ICML, 2024b. Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In ICML, 2022a. Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., and Schmidt, L. Robust fine-tuning of zero-shot models. In CVPR, 2022b. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-MNIST: Novel Image Dataset for Benchmarking Machine Learning Algorithms, 2017. URL http://arxiv.org/ abs/1708.07747. Xiao, J., Ehinger, K. A., Hays, J., Torralba, A., and Oliva, A. Sun database: Exploring large collection of scene categories. IJCV, 2016. No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Yadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal, M. TIES-merging: Resolving interference when merging models. In NeurIPS, 2023. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. ICCV, 2023. 11 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces A. Additional details A.1. Datasets The 8-dataset benchmark consists of: Cars (Krause et al., 2013), DTD (Cimpoi et al., 2014), EuroSAT (Helber et al., 2019), GTSRB (Stallkamp et al., 2011), MNIST (Lecun et al., 1998), RESISC45 (Cheng et al., 2017), SUN397 (Xiao et al., 2016), and SVHN (Netzer et al., 2011). The 14-dataset benchmark builds on the preceding one, incorporating six additional datasets: CIFAR100 (Krizhevsky & Hinton, 2009), STL10 (Coates et al., 2011), Flowers102 (Nilsback & Zisserman, 2008), OxfordIIITPet (Parkhi et al., 2012), PCAM (Veeling et al., 2018), and FER2013 (Goodfellow et al., 2013). Finally, the 20-dataset benchmark includes the preceding 14 plus the following six: EMNIST (Cohen et al., 2017), CIFAR10 (Krizhevsky & Hinton, 2009), Food101 (Bossard et al., 2014), FashionMNIST (Xiao et al., 2017), RenderedSST2 (Socher et al., 2013), and KMNIST (Clanuwat et al., 2018). A.2. Implementation details Our method relies on SVD, which is defined for two-dimensional matrices Rmn. However, some weights of the neural networks are represented by vectors δ Rn, e.g. bias vectors and parameters of layer normalization (Ba et al., 2016). Therefore, following Gargiulo et al. (2024), we apply simple averaging to combine these parameters. Code to reproduce all experiments will be released upon publication of this work. B. Additional experiments B.1. Visualization of task matrix spectra When visualizing spectra of singular values of task matrices (Figure 1 and Figure 4), we selected an output projection matrix from layer ℓ = 4 of ViT/B-16 as an illustrative example. In Figure 7, we present spectra across variety of layers of ViT/B-16 for the task matrices of task-specific models, TA, Iso-C and Iso-CTS. B.2. Selection of scaling coefficient α Table 2. Optimal α value chosen on held-out validation set for different model types and numbers of tasks for Iso-C and Iso-CTS. Method Model 8 tasks 14 tasks 20 tasks Iso-C Iso-CTS ViT/32-B ViT/16-B ViT/14-L ViT/32-B ViT/16-B ViT/14-L 1.30 1.40 1. 1.50 1.60 1.90 1.00 1.00 1.30 1.20 1.20 1.50 0.90 0.80 1.00 1.10 1.10 1.20 On Figure 8, we present the relationship between the validation accuracy and scaling factor α. We observe that TA is very sensitive to the selection of α, which potentially may require more fine-grained search. On the other hand, both Iso-C and Iso-CTS are more robust to α selection, resembling the task-specific models. For reproducibility, In Table 2, we provide the optimal α value chosen on the held-out validation set for each model and number of tasks. B.3. Applying Iso to individual task matrices Flattening the skewed spectrum of singular values significantly improves the performance of the merged model, as demonstrated in Section 5.3. One may wonder if this operation might also be an effective strategy for improving single-task models. Figure 9 presents the performance of task-specific models in their original form along with their modified versions with singular value spectra of their task matrices flattened (which is equivalent to performing Iso-C for single model). We observe 3.3% drop in average performance across tasks. Therefore, the reason for the success of Iso-C lies in its ability to mitigate the negative effects of summing task matrices, not in inadvertently improving the original individual task matrices. 12 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Figure 7. Visualization of singular value spectra of different task matrices for different types of layers in ViT/B-16. No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Figure 8. TA is sensitive to the selection of α, while both Iso-C and Iso-CTS are more robust to α selection, resembling the taskspecific models. The α is chosen based on the best average performance on the validation set across tasks. The bottom right subplot denotes the optimal α for each method (Eq. (3), Eq. (9) and Eq. (14)). The model is ViT-B/16. 14 No Task Left Behind: Isotropic Model Merging with Common and Task-Specific Subspaces Figure 9. Validation Accuracy while scaling task matrices with α coefficient (Eq. (3) applied for single task). We observe performance gap between the accuracy of original and modified models for the optimal values of α (denoted by square)."
        }
    ],
    "affiliations": [
        "Computer Vision Center, Barcelona, Spain",
        "Department of Computer Science, Universitat Autonoma de Barcelona, Spain",
        "Department of Information Engineering, University of Florence, Italy",
        "Gdansk University of Technology, Poland",
        "IDEAS NCBR, Warsaw, Poland",
        "Warsaw University of Technology, Poland"
    ]
}