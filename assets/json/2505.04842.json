{
    "paper_title": "Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers",
    "authors": [
        "Kusha Sareen",
        "Morgane M Moss",
        "Alessandro Sordoni",
        "Rishabh Agarwal",
        "Arian Hosseini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Prevalent reinforcement learning~(RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RL$^V$ that augments any ``value-free'' RL method by jointly training the LLM as both a reasoner and a generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RL$^V$ boosts MATH accuracy by over 20\\% with parallel sampling and enables $8-32\\times$ efficient test-time compute scaling compared to the base RL method. RL$^V$ also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RL$^V$ achieves $1.2-1.6\\times$ higher performance when jointly scaling parallel and sequential test-time compute with a long reasoning R1 model."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 2 4 8 4 0 . 5 0 5 2 : r Preprint. Under review. Putting the Value Back in RL: Better Test-Time Scaling by Unifying LLM Reasoners With Verifiers Kusha Sareen Mila, McGill University Morgane Moss Mila, Universite de Montreal Alessandro Sordoni Microsoft Research, Mila Rishabh Agarwal Mila, McGill University Arian Hosseini Google DeepMind, Mila"
        },
        {
            "title": "Abstract",
            "content": "Prevalent reinforcement learning (RL) methods for fine-tuning LLM reasoners, such as GRPO or Leave-one-out PPO, abandon the learned value function in favor of empirically estimated returns. This hinders test-time compute scaling that relies on using the value-function for verification. In this work, we propose RLV that augments any value-free RL method by jointly training the LLM as both reasoner and generative verifier using RL-generated data, adding verification capabilities without significant overhead. Empirically, RLV boosts MATH accuracy by over 20% with parallel sampling and enables 8 32 efficient test-time compute scaling compared to the base RL method. RLV also exhibits strong generalization capabilities for both easy-to-hard and out-of-domain tasks. Furthermore, RLV achieves 1.2 1.6 higher performance when jointly scaling parallel and sequential test-time compute with long reasoning R1 model. Figure 1: Left: Scaling Sequential and Parallel Compute Jointly with GRPOV compared to baselines on the AIME24 using R1-Distill-Qwen-1.5B as the base LLM. We use Hendrycks MATH for RL fine-tuning. Each point represents the compute-optimal accuracy achieved at sequence length using 64 parallel samples. Right: Length Selection Using Joint Verifier. By iteratively increasing the generation length until chosen RLV confidence threshold is met, we can obtain the maximum accuracy at given sequential compute budget, allowing the model to dynamically allocate more sequential compute to difficult problems. Equally supervised. Corresponding authors: {kusha.sareen, agarwalr}@mila.quebec, arianhosseini@google.com 1 Preprint. Under review. Figure 2: RLV offers significant compute efficiency and performance gains over base value-free RL methods when scaling test-time compute with weighted majority voting on MATH500 (Lightman et al., 2023). For scoring solutions, we use LLM-as-a-Judge as the verifier for the base method, while the trained unified verifier for RLV. These results are based on RL fine-tuning Qwen2.5-Math-1.5B on Hendrycks MATH."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning (RL) on correctness rewards has emerged as pivotal technique for advanced reasoning capabilities of large language models (LLMs) (DeepSeek-AI, 2025). notable trend among state-of-the-art RL algorithms designed for LLMs, including GRPO (Shao et al., 2024), VinePPO (Kazemnejad et al., 2024), and Leave-one-out PPO (Chen et al., 2025b; Ahmadian et al., 2024), is their shift from the canonical PPO (Schulman et al., 2017) method by abandoning the learned value function network, opting instead to rely on empirically estimated returns. This shift reduces both computational demands and GPU memory consumption, which is crucial for scaling RL training to increasingly massive LLMs. Discarding the learned value function, while beneficial for RL training , sacrifices its potential utility at test time. Traditionally, the value function estimates expected future rewards, allowing it to serve as an outcome verifier (Cobbe et al., 2021) to assess correctness of given reasoning chain. This verification capability is valuable for scaling inference compute through parallel search strategies like Best-of-N or weighted majority voting. We argue that the potential for efficient test-time compute scaling offered by value-like signal remains largely untapped in prevalent RL methods. To capture this potential without sacrificing training scalability, we propose RLV that augments value-free methods with generative verifier (Zhang et al., 2024). Unlike traditional value functions predicting only scalar rewards, generative verifiers leverage the LLMs generation capabilities. Our core idea utilizes the abundant data generated during RL training to simultaneously train the LLM as reasoner and verifier. Specifically, we jointly optimize standard RL objectives alongside generative verification objective, framing verification as next-token prediction task conditioned on the RL-generated reasoning sequences. This enables the same LLM to serve dual function: acting as the policy generating solutions while simultaneously providing an intrinsic, generative score reflecting perceived solution correctness. Empirically, RLV demonstrates significant advantages for test-time scaling. It boosts MATH accuracy by over 20% compared to the base RL method when using parallel sampling and enables substantially more efficient test-time compute scaling, achieving 8 32 improvements, as shown with in Figure 2. Furthermore, RLV exhibits robust generalization capabilities, outperforming the base RL method not only on harder math problems in MATH2 (Shah et al., 2025) but also on out-of-domain tasks like GPQA Physics (Rein et al., 2024), as illustrated in Figure 4. The benefits of RLV extend to long CoT reasoning models when scaling both parallel and sequential compute, where it achieves 1.2 1.6 higher performance than baseline methods, consistently yielding the best results across different generation lengths and parallel sample counts (Figure 1, 6). 2 Preprint. Under review. Figure 3: Overview of RLV: (Top) During training, the LLM policy generates solutions y. This data is used for policy updates with RL and simultaneously trains the same LLM as generative verifier via supervised fine-tuning (SFT) on correctness labels by asking the model Is this solution correct? Answer Yes or No. (Bottom) At test time, the unified LLM generates solutions and also acts as verifier to assign scores for re-ranking using Best-of-N or weighted voting."
        },
        {
            "title": "2 Related Work",
            "content": "RL for Reasoning. Recently, there has been research surge in eliciting improved reasoning from LLMs via RL, including traditional RL algorithms, such as PPO (Zeng et al., 2025). Notably, one can utilize the value model trained in PPO as verifier for test-time search (Liu et al., 2023). However, the trend towards value-free RL (Shao et al., 2024; DeepSeek-AI, 2025; Kazemnejad et al., 2024; Ahmadian et al., 2024; Chen et al., 2025b) in recent LLM applications discards this possibility, and this method also involved the overhead of training separate model. Our work aims to reintegrate verification with RL, proposing simple approach where generative verifier is trained concurrently with the policy, leveraging data generated during RL training. Test-time Verification. Verification serves as powerful approach for improving LLM reasoning by scaling time-time compute, often using separate models trained via binary classification (Cobbe et al., 2021; Luo et al., 2024; Yu et al., 2024; Lightman et al., 2023; Setlur et al., 2025; Zhang et al., 2025), preference learning (Hosseini et al., 2024; Yuan et al., 2024)), or more recently using next-token prediction (Zhang et al., 2024; Mahan et al., 2024; Ankner et al., 2024). However, these separate verifiers incur significant overheads: they demand large training datasets, extra compute cycles, and substantial GPU memory during inference, potentially limiting the size of the LLM reasoner when loaded together. In contrast, we jointly train single LLM using RL and generative verification. Our method offers capable verifier essentially for free, incurring no memory and very minimal compute cost. Furthermore, as seen in Figure 2, our approach results in much better inference compute scaling than using the RL policy as verifier via LLM-as-a-Judge (Bai et al., 2022; Zheng et al., 2023; Chen et al., 2025a; Kim et al., 2024; Zhao et al., 2025)"
        },
        {
            "title": "3 Background",
            "content": "Reinforcement Learning for LLMs involves maximizing the expected reward under the LLM πθ on set of prompts , where we typically use binary correctness reward (DeepSeek-AI, 2025). To maintain stability and prevent the fine-tuned LLM from deviating too much from the base LLM πre , KL divergence penalty is often added with 3 Preprint. Under review. coefficient β, yielding the objective (θ) (Stiennon et al., 2020): (θ) = xX [JRL(θ; x)], where JRL(θ; x) = (θ; x) βDKL[πθπre ] (1) JRL(θ; x) is typically optimized using policy gradient methods, which we describe below. Proximal Policy Optimization (PPO) (Schulman et al., 2017) is canonical RL algorithm for fine-tuning LLMs, where gradient updates are constrained by clipping mechanism to prevent large changes from the previous policy. The objective is described as: JPPO(θ; x) := yπθold (.x) where pt(θ) = min (cid:0)pt(θ) ˆAt, clip(pt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:34) 1 y t=1 πθ(ytx, y<t) (ytx, y<t) πθold (cid:35) (cid:1) , (2) where ϵ is the clipping hyperparameter, and ˆAt is the advantage for token t. The advantage is typically estimated with GAE using learned value network (Schulman et al., 2018). However, for LLMs this value network can be slow, memory intensive, and inaccurate, which has resulted in state-of-the-art methods discarding it. Group Relative Policy Optimization (GRPO) (Shao et al., 2024) is variant of PPO designed to mitigate some of its drawbacks, particularly for training LLMs. key part of GRPO is that it foregoes the need for an explicit value model. Instead, it estimates the baseline for advantage calculation directly from the rewards of group of outputs {y1, y2, , yG} generated for the same prompt x. The objective function is: JGRPO(θ; x) := {yi}G i=1 πθold min (cid:0)pt(θ) ˆAi,t, clip (pt(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:1) (cid:105) , (.x) yi t=1 i=1 (cid:104) 1 1 yi ri mean({r1, r2, , rG}) std({r1, r2, , rG}) where ˆAi,t = , ri = r(x, yi) Leave-One-Out PPO (Chen et al., 2025b) also drops the value network, similar to GRPO, and estimates the advantage using leave-one-out estimator (Kool et al., 2019). Given outputs for prompt, for each output, the advantage is estimated using the average reward of the remaining 1 samples1, that is, ˆAi,t = ri 1 K1 rj. i=j VinePPO (Kazemnejad et al., 2024) improves the credit assignment in PPO by computing unbiased Monte Carlo value estimates of intermediate states, instead of relying on an inaccurate value network. Specifically, it generates generations starting from state st = y<t using the current policy πθ, and averages their reward to get the value estimate ˆVMC(st). This estimate is then plugged into the advantage calculation: ˆAt := r(x, y<t+1) + ˆVMC(st+1) ˆVMC(st), where ˆVMC(st) := 1 k=1 r(x, k) (3) Test-Time Compute Scaling prominent research direction involves performing additional computation during test-time to boost the reasoning performance of LLMs. Prevalent techniques often scale compute in parallel by sampling multiple candidate solutions and employing heuristics like majority voting to select final answer (Wang et al., 2023) or using verifier for test-time reranking with Best-of-N (Cobbe et al., 2021) or weighted voting (Uesato et al., 2022). Recently, RL has enabled scaling compute sequentially by generating long chain-of-thought (CoT), characteristic of reasoning models like R1 (DeepSeek-AI, 2025). 1This coincides with many modern implementations of Reinforce Leave-One-Out (RLOO) (Ahmadian et al., 2024), including the one in the HuggingFace RLOOTrainer. 4 Preprint. Under review. Generative Verifiers Zhang et al. (2024) pose verification as next-token prediction, where the LLM takes problem and candidate solution as input and outputs verification decision by predicting token cy, which is either Yes or No, to indicate correctness. Specifically, we train the verifier using supervised fine-tuning (SFT) loss to maximize the likelihood of predicting Yes for correct solutions y+ and No for incorrect solutions y: JVeri y(θ; x) := (4) (x,y,I,cy)DVeri where DVerify = {(x, y+, I), Yes} (cid:83) {(x, y, I), No} is class-balanced verification dataset, and corresponds to the prompt Is this solution correct? Answer Yes or No.. log πθ(cyx, y, I),"
        },
        {
            "title": "4 RLV: Unifying Verifiers with “Value-Free” RL Reasoners",
            "content": "Prevalent value-free RL methods for LLMs (3) improve training scalability but discard the value network, eliminating the intrinsic verification mechanism available in methods like PPO. This limits test-time compute scaling approaches that rely on verifier to select the final solution among several candidates. Addressing this limitation currently involves suboptimal choices: deploying separate verifier models or value networks imposes significant overhead in data curation, compute, and GPU memory (Ahmadian et al., 2024), while prompting the base LLM as verifier (LLM-as-a-Judge) has minimal overhead but is less effective due to its lack of task-specific training (Zhang et al., 2024). Training LLM reasoners with RL already produce large quantities of solution data with correctness reward labels, used solely for improving the reasoning ability of LLMs. We propose to leverage this data for an additional purpose: using these solutions generated during RL to concurrently train generative verifier within the same LLM used for reasoning. This approach, which we call RLV, efficiently builds task-specific verification capabilities while avoiding the high memory and compute costs of separate verifiers and being more effective than LLM-as-a-Judge2 based on prompting. Unified Training. We train single LLM, to perform both reasoning (problem-solving) and verification tasks. For each batch, the verification objective uses the (problem, solution, correctness reward) tuples generated during the RL process as training examples. Rather than employing separate prediction heads with regression or binary cross-entropy losses (alternatives explored in 5.4), we add the generative verification loss (JVeri in Equation 4) to the RL fine-tuning objective (JRL in Equation 1), resulting in this unified objective: JUni ied(θ) := JRL(θ; x) + λJVeri y(θ; x), where the hyperparameter λ balances the contribution of each objective. Specifically, the LLM learns to predict Yes or No token to assess correctness of given solution. (5) Test-time Scaling At test time, we use the LLM verifier to score solutions generated by itself to guide the final answer selection. This score s(x, y) quantifies the verifiers confidence as its Yes probability given the problem x, solution y, and prompt I, that is, s(x, y) := πθ(Yes x, y, I). Here, we consider three parallel sampling approaches: Majority Voting: verifier-free baseline that selects the most frequent answer. Best-of-N: Selects the solution with the highest verifier score s(x, y). Weighted Voting: Sum the verifier scores s(x, y) for solutions yielding the same final answer; select the answer with the highest cumulative score."
        },
        {
            "title": "5 Experiments",
            "content": "We aim to investigate the effectiveness and characteristics of our proposed RLVmethod, which unifies reasoner and generative verifier within single LLM. We answer several 2For Base RL (value free) LLM-as-a-Judge experiments, we use the same verification prompt as RLV and only take the likelihood of predicting Yes as the score in this setting as well, and do not generate verification CoTs for fair comparison. 5 Preprint. Under review. Figure 4: RLV outperforms the base RL method (Leave-One-Out-PPO) consistently across different number of solutions for different generalization settings with respect to the MATH training dataset. (Left) In-distribution Generalization on MATH500. (Center). Easy-to-Hard Generalization on MATH2 (Right). Out-of-Domain Generalization on Physics problems in the GPQA Diamond split. key questions about this paradigm: 1) How does parallel test-time compute scale with RLV? 2) How should the unified verifier be trained? 3) How should the unified verifier be used at test-time? 4) How does RLVinteract with sequential scaling in thinking models? Setup RL training for all our experiments utilized the Hendycks MATH dataset (Hendrycks et al., 2021) and were run on 4A100 80G Nvidia GPUs for 3 hours. Evaluations are reported on MATH500 (Lightman et al., 2023), MATH2 (Shah et al., 2025), GPQA (Rein et al., 2024), and AIME24. For experiments involving shorter chain-of-thought (CoT) reasoning, we employed the Qwen2.5 Math 1.5B model (Yang et al., 2024). We finetune it with GRPO, Leave-One-Out PPO and VinePPO with and without unified verification. Training used context window of 1024 tokens. During inference, we generated up to 1024 tokens for MATH500 and up to 2048 tokens for other test sets. To demonstrate the impact of model scaling, we also train the Qwen2.5 Math 7B model (Yang et al., 2024) with Leave-One-Out PPO. Long CoT experiments were conducted with DeepSeek-R1-Distill-Qwen-1.5B, distilled version of DeepSeek-R1. We tune it with GRPO, and use SGLang (Zheng et al., 2024) for inference purposes. The RL training process involved sampling 32 problems per online iteration, generating 5 solutions per problem. With batchsize of 8 this resulted in 32 updates per iteration. Training continued for 40 epochs. Models are trained using long chain-of-thought prompt which encloses reasoning in special tags (see Appendix D.2). 5.1 Test-Time Compute Scaling With RLV In-Distribution Generalization RLVis up to 32 more efficient and achieves 4% higher accuracy than baseline on MATH500 with 512 samples (Figure 4). Further, Figure 2 shows that across different RL methods, RLVachieves higher final accuracy levels. It also reaches strong accuracies with significantly less compute. Figure 8 shows these gains are maintained at the 7B scale. Easy-to-Hard Generalization Zhang et al. (2024) and Sun et al. (2024) demonstrate that their trained verifiers can generalize to fixed separate LLM reasoner on problem sets more challenging than those encountered during training. As illustrated in Figure 4 (center), our proposed method RLV with unified reasoner and verifier, also exhibits strong easy-tohard-generalization capacity on MATH2 (Shah et al., 2025), containing much more difficult math problems that require non-trivial combination of two distinct skills from MATH. Preprint. Under review. Figure 5: Comparing Test-Time Compute Strategies Evaluating verifier-based answer selection strategies (weighted voting, Best-of-N) and verifier-free majority voting on AIME24. The optimal strategy differs for the short CoT and long CoT tuned models. Out-of-Domain Generalization Going beyond easy-to-hard generalization, we also evaluate out-of-domain performance on GPQA Physics problems (Rein et al., 2024). RLV shows strong generalization resulting in more than 10% accuracy improvement compared to the baseline with 512 samples for weighted voting. RLV can lead to better reasoner Interestingly, we observe positive transfer from unification to better pass@1 (Chen et al., 2021) performance (without any additional testtime compute) from unified training in RLV across all these tasks, suggesting synergy between generative verification and RL objectives (see Figure 4). This improvement is observed in the Leave-One-Out PPOV models at the 1.5B and 7B scales. 5.2 Compute-Optimal Scaling: How to Use Your RLV Verifier? As outlined in 4, one can use verifier at test-time to score generated solutions. Subsequently, final answer can be selected using either the Best-of-N (BoN) strategy or Weighted Voting based on these scores. We conducted experiments on the AIME 2024 dataset with our GRPOV tuned models. Specifically, we tested variants based on Qwen2.5-Math-1.5B and R1-Distill-Qwen-1.5B, which generate short and long CoTs respectively (Figure 5). For short CoT models (Qwen2.5-Math-1.5B, left panel of Figure 5), weighted voting consistently outperforms both majority voting and Best-of-N when sampling 8 or more solutions per problem. The trend also holds for the Long CoT models (Figure 5, right). Our findings on the inference strategies are consistent with base RL experiments using the same models with LLM-as-a-Judge, where we do not train unified verifier and simply prompt the same fine-tuned reasoning model as verifier. 5.3 Complementing Long Reasoning Models with RLV An emerging technique to improve model performance at inference-time involves training them with RL to generate longer chain-of-thoughts (CoTs), simulating deeper reasoning before delivering an answer. Models employing this technique, such as DeepSeekR1 (DeepSeek-AI, 2025) can dedicate extra sequential computation during inference to self-verify, reflect, and refine their final output. However, the amount of compute they allocate to find and verify solution in their thought process is uncontrollable. Additionally, the exact mechanisms behind their reasoning are not yet fully understood. Therefore, having verifier which one can invoke reliably could further benefit inference time scaling. Our proposed method is complementary to sequential inference compute scaling. Figure 6 shows the AIME 24 success rate achieved by different methods when varying the number of solutions generated and the allowed generation length in tokens with 7 Preprint. Under review. Figure 6: Scaling Parallel Compute using RLV complements Sequential Scaling. AIME24 success rate vs number of solutions generated for Base (initial checkpoint), GRPO tuned model (no verification training) and GRPOV (unified verifier and reasoner) across varying generation lengths. GRPOV consistently outperforms GRPO, benefiting most from increased inference effort (more solutions, longer generation). For each model, we pick the inferencetime strategy (Best-of-N, Majority Vote, Weighted Vote) that gives the highest success rate. See details of budget forcing in B. budget forcing (Muennighoff et al., 2025). Notably, GRPOV method consistently achieves the highest success rate, more evident at longer generation lengths and scales well with number of sampled solutions, suggesting complementary gains to sequential scaling. Additionally, the RLVverifier allows for fine control of the number of tokens generated for given problem. In Figure 1, we predefine confidence threshold for weighted voted answers to reach. If the threshold is not met for solutions with given sequence length (from 1024, 2048 and 4096), longer sequence length is chosen until the threshold is met. This allows the model to dynamically allocate more sequential compute to more difficult problems. Figure 1 shows the AIME24 accuracy steadily increased with the average generation length. We hypothesize this method could result in large efficiency gains for very long context models (eg. 32K tokens) where correct answer is commonly reached much before model stops generating (DeepSeek-AI, 2025). 5.4 How To Train Your Unified Verifier? The literature offers several options to train unified verifier. Common approaches involve adding dedicated verification head atop the policy network, trained using classification via binary cross-entropy (BCE) (Cobbe et al., 2021; Lightman et al., 2023) or regression objectives (Stiennon et al., 2020) to predict solution scores. Generative verification, proposed by Zhang et al. (2024), suggests it can produce capable verifiers without degrading, and sometimes even improving, the policys core reasoning performance. Figure 7 compares the Reasoner accuracy (measured by pass@1) and Verifier accuracy (measured on balanced set of correct and incorrect solutions) of various verifier training approaches. Notably, Leave-One-Out PPO with separate verification heads performs poorly, both as reasoner and verifier, compared to Leave-One-Out PPOV. Overall, RLVoutperforms base RL and RL methods with separate verification heads both as reasoner and as verifier. In addition, the RLV verifier accuracy is comparable to separate verifier trained on logged data from the same RL run, showing minimal loss from joint training. key hyperparameter in unified training is the verification coefficient λ (see Equation 5), which balances the reasoning and verification objectives. Its impact is explored in Figure 7b 8 Preprint. Under review. Figure 7: (a) Comparison of Reasoner Accuracy versus Verifier Accuracy for different unified verifier training strategies. Leave-One-Out PPOV significantly outperforms Leave-OneOut PPO using LLM-as-a-Judge and separate verification heads, with binary cross-entropy (BCE) or regression (REG) on both metrics. (b) Impact of the Verification Coefficient λ on Reasoner and Verifier accuracy for RLV. GRPOV shows stark trade-off, while Leave-OneOut PPOV achieves better balance with peak reasoner performance at an intermediate λ. Figure 8: RLV Scales with Model Size. We observe consistent gains in weighted voting at 256 solutions for both 1.5B and 7B models. Additionally, training the 7B model for verification over simply using LLM-as-a-Judge results in drastically higher verifier accuracy on both MATH500 and AIME24 benchmarks. for RLV. For GRPOV, stark trade-off exists. Increasing λ significantly boosts Verifier accuracy (from 50% to 80%), but drastically decreases Reasoner accuracy (from 54% down to 35%). Prioritizing verification heavily penalizes reasoning in this setup. In contrast, for Leave-One-Out PPOV, the trade-off is more nuanced. Verifier accuracy sees consistent improvement until plateau as λ increases (50% to 80%). Crucially, Reasoner accuracy peaks around λ = 1 before slightly declining. This suggests that Leave-One-Out PPOV can achieve better balance, with optimal reasoning performance occurring at an intermediate verification coefficient where verifier accuracy is also strong."
        },
        {
            "title": "6 Discussion & Future Work",
            "content": "We proposed RLV that integrates verification into value-free RL frameworks without significant overhead, yielding substantial gains in reasoning accuracy, test-time compute efficiency, and generalization across MATH, MATH², GPQA, AIME 24 datasets. This method complements sequential scaling in long-reasoning CoT models and benefits from generative verification training, which proved superior to alternatives. Building on these findings, future work could focus on enhancing the generative verifier to produce explicit CoT explanations (Zhang et al., 2024). However, training such verifier 9 Preprint. Under review. necessitates verification-specific CoT data or RL training itself. We posit that future research could profitably investigate this direction to achieve unified framework for both solution generation and verification through RL. Further investigation into RLVs applicability across other reasoning domains and scalability with larger LLMs also remains pertinent."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We thank Khang Ngo for support in setting up infrastructure for experiments. We thank Siva Reddy and Arkil Patel for their valuable feedback on this paper and for engaging in insightful discussions. We also would like to express our gratitude to Milas infrastructure team and Microsoft Research for providing the computing resources that made this project possible."
        },
        {
            "title": "8 Author Contributions",
            "content": "KS led the project, ran almost all of the experiments and ablation studies, and wrote and edited the paper. MM was responsible for the long CoT experiments and edited the paper. AS advised the project, provided feedback on writing and wrote the initial GRPO and VinePPO code. RA conceived the project with AH, advised KS, and provided feedback on paper. AH and RA proposed several ideas and experiments, wrote the initial draft and equally advised the project. AH acted in an advisory capacity only."
        },
        {
            "title": "References",
            "content": "Arash Ahmadian, Chris Cremer, Matthias Galle, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Ust un, and Sara Hooker. Back to basics: Revisiting reinforce-style optimization for learning from human feedback in llms. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pp. 1224812267. Association for Computational Linguistics, 2024. doi: 10.18653/ V1/2024.ACL-LONG.662. URL https://doi.org/10.18653/v1/2024.acl-long.662. Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan Chang, and Prithviraj Ammanabrolu. Critique-out-loud reward models. arXiv preprint arXiv:2408.11791, 2024. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, and Sercan O. Arik. SETS: leveraging self-verification and self-correction for improved test-time scaling. CoRR, abs/2501.19306, 2025a. doi: 10.48550/ARXIV.2501.19306. URL https://doi.org/10. 48550/arXiv.2501.19306. Kevin Chen, Marco Cusumano-Towner, Brody Huval, Aleksei Petrenko, Jackson Hamburger, Vladlen Koltun, and Philipp Krahenb uhl. Reinforcement learning for long-horizon interactive llm agents, 2025b. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, 10 Preprint. Under review. Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment, 2024. URL https://arxiv.org/abs/2410. 01679. Seungone Kim, Jamin Shin, Yejin Choi, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=8euJaTveKw. Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 reinforce samples, get baseline In DeepRLStructPred@ICLR, 2019. URL https://api.semanticscholar.org/ for free! CorpusID:198489118. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, and Asli Celikyilmaz. Dont throw away your value model! generating more preferable text with value-guided monte-carlo tree search decoding. arXiv preprint arXiv:2309.15028, 2023. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision. CoRR, abs/2406.06592, 2024. doi: 10.48550/ARXIV.2406.06592. URL https://doi.org/10.48550/arXiv.2406.06592. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level googleproof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/ abs/1707.06347. 11 Preprint. Under review. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation, 2018. URL https://arxiv.org/abs/1506.02438. Amrith Setlur, Nived Rajaraman, Sergey Levine, and Aviral Kumar. Scaling test-time compute without verification or rl is suboptimal, 2025. URL https://arxiv.org/abs/ 2502.12118. Vedant Shah, Dingli Yu, Kaifeng Lyu, Simon Park, Jiatong Yu, Yinghui He, Nan Rosemary Ke, Michael Mozer, Yoshua Bengio, Sanjeev Arora, and Anirudh Goyal. Ai-assisted generation of difficult math questions, 2025. URL https://arxiv.org/abs/2407.21009. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. Learning to summarize from human feedback. CoRR, abs/2009.01325, 2020. URL https://arxiv.org/abs/2009.01325. Zhiqing Sun, Longhui Yu, Yikang Shen, Weiyang Liu, Yiming Yang, Sean Welleck, and Chuang Gan. Easy-to-hard generalization: Scalable alignment beyond human supervision, 2024. URL https://arxiv.org/abs/2403.09472. Jonathan Uesato, Nate Kushman, Ramana Kumar, H. Francis Song, Noah Y. Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback. CoRR, abs/2211.14275, 2022. doi: 10.48550/ ARXIV.2211.14275. URL https://doi.org/10.48550/arXiv.2211.14275. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. Fei Yu, Anningzhe Gao, and Benyou Wang. Ovm, outcome-supervised value models for planning in mathematical reasoning. In Kevin Duh, Helena omez-Adorno, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 858875. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-NAACL.55. URL https://doi.org/10.18653/ v1/2024.findings-naacl.55. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels, 2024. URL https://arxiv.org/abs/2412.01981. Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. HKUST-NLP Blog, 2025. URL https://hkust-nlp.notion.site/ simplerl-reason. Available at: https://hkust-nlp.notion.site/simplerl-reason. Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning, 2025. URL https://arxiv.org/abs/2501.07301. Preprint. Under review. Eric Zhao, Pranjal Awasthi, and Sreenivas Gollapudi. Sample, scrutinize and scale: Effective inference-time search by scaling verification, 2025. URL https://arxiv.org/abs/2502. 01839. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. URL https://arxiv.org/abs/2312.07104. 13 Preprint. Under review."
        },
        {
            "title": "Appendices",
            "content": "A Test-Time Compute Strategies for Baseline Similar to Figure 5, we evaluated base RL tuned (no verification training) variants based on Qwen2.5-Math-1.5B and R1-Distill-Qwen-1.5B, which generate short and long CoTs respectively, on AIME24. The compute optimal strategy for GRPOV corresponds to Bestof-N (Figure 1). For base RL, Figure 9 shows that majority voting and weighted voting significantly outperform Best-of-N selection. We use the best performing inference strategy of GRPO in comparison to GRPOV in Figure 1 and Figure 6 to be fair. Figure 9: Test-Time Compute Strategies for Base RL (No Verification Training) Methods shows different trend from Figure 5. LLM-as-a-Judge no longer provides high quality values, resulting in low Best-of-N scores. The compute optimal strategy here corresponds to majority voting and weighted voting."
        },
        {
            "title": "B Inference Details",
            "content": "Budget forcing The budget forcing implementation method can be described as follows: 1. Define the total budget for tokens, denoted by k. 2. Determine buffer number of tokens, denoted by b. 3. During inference, generate up to tokens. 4. To maintain the in-distribution characteristics of the initial generation, which will serve as the prompt for subsequent generation, truncate the generated text to the last completed sentence. If no full stops (.) are present in the initial generation, retain the entire generated text. 5. Append the conclusion tokens: ```</ think >nn``` to the resulting truncated text. 6. Use this concatenated text as the prompt for continued generation, allowing maximum of tokens. 7. The final response is the concatenation of the truncated initial generation, the conclusion tokens, and the continued generation. Thus, let G0 be the initial generation of up to tokens, and T(G0) represent the truncated version of G0 to the last completed sentence, or G0 itself if no full stops are present. Let represent the conclusion tokens: 14 Preprint. Under review. = ```<think>nn``` G1 is the continued generation, using T(G0) as the prompt, where denotes concatenation. The final response is given by: = T(G0) G1 where the total number of tokens in is constrained by the initial budget k. This method balances the appropriateness of longer generation with the preservation of in-distribution properties by managing the initial generation and subsequent continuation."
        },
        {
            "title": "C Evaluation and Metrics",
            "content": "Reliable Estimation of Best-of-N To estimate the average success rate, we perform independent trials. In each trial, we draw samples out of N(N > k) and select the best one (highest score). Finally, we average the success rates across these selected samples over trials (Hosseini et al., 2024). Best-of-k := 1 (N ) Nk i=0 (cid:19) (cid:18)N 1 αi (6) where [α0, α1, ..., αN1] are the binary correctness scores (0 or 1) for the candidate solutions sorted in decreasing order of their verifier scores."
        },
        {
            "title": "D Prompts",
            "content": "D.1 Generating Solutions to Math Problems Math prompt 1 Compute : $1 2+34+5 dots +99 100 $ . D.2 Generating Solutions to Math Problems with Long Chains of Thought Math prompt with long chain of thought Compute : $1 2+34+5 dots +99 100 $ . Think about an answer . r o g c i e mind and then provides The s n o s . . , <think> s n o s here </think >. Put your answer within boxed { { } } . n s within <think> </think> tags , a 1 2 3 4 5 15 Preprint. Under review. D.3 Verification Prompt Verification prompt Problem : Compute : $1 2+34+5 dots +99 100 $ . u n : $ ( 1 2 ) +(3 4)+ dots +(97 98) +(99 100) = 5 0 ( 1 ) = boxed { 50}. $ h o i o c ? Answer with Yes or No . 1 2 3 4 5"
        }
    ],
    "affiliations": [
        "Google DeepMind, Mila",
        "Microsoft Research, Mila",
        "Mila, McGill University",
        "Mila, Universite de Montreal"
    ]
}