{
    "paper_title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs",
    "authors": [
        "Roy Eisenstadt",
        "Itamar Zimerman",
        "Lior Wolf"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing a separation between the model's internal \"thinking\" process and the final response. A key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the model's planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate a more concise and decisive chain of thoughts. Our empirical results demonstrate that this \"overclocking\" method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 0 4 2 7 0 . 6 0 5 2 : r Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs Roy Eisenstadt Tel-Aviv University royeisen@gmail.com Itamar Zimerman Tel-Aviv University, IBM Research zimerman1@mail.tau.ac.il Lior Wolf Tel-Aviv University wolf@cs.tau.ac.il (a) (b) Figure 1: Applications of our method. (a) Monitoring the reasoning progress. (b) Overclocking it."
        },
        {
            "title": "Abstract",
            "content": "Recently, techniques such as explicit structured reasoning have demonstrated strong test-time scaling behavior by enforcing separation between the models internal thinking\" process and the final response. key factor influencing answer quality in this setting is the length of the thinking stage. When the reasoning is too short, the model may fail to capture the complexity of the task. Conversely, when it is too long, the model may overthink, leading to unnecessary computation and degraded performance. This paper explores and exploits the underlying mechanisms by which LLMs understand and regulate the length of their reasoning during explicit thought processes. First, we show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization, which is then used to reveal insights on the models planning dynamics. Second, we manipulate the internal progress encoding during inference to reduce unnecessary steps and generate more concise and decisive chain of thoughts. Our empirical results demonstrate that this overclocking method mitigates overthinking, improves answer accuracy, and reduces inference latency. Our code is publicly available. https://github.com/royeisen/reasoning_loading_bar"
        },
        {
            "title": "Introduction",
            "content": "In recent years, Large Language Models (LLMs) such as ChatGPT [21] have demonstrated remarkable capabilities across both general-purpose and domain-specific challenges, exhibiting surprising generalization abilities [13]. An emerging frontier for enhancing their performance is test-time scaling, which aims to improve model responses by dynamically allocating additional computational effort during inference [23, 28]. Project page: https://royeisen.github.io/OverclockingLLMReasoning-paper Preprint. Under review. prominent strategy for enabling effective long-form reasoning is explicit structured reasoning, which separates the models reasoning process from its final answer [27]. This is commonly implemented using special tokens that mark the start and end of the reasoning phase (e.g., <think> and </think>, which are employed by DeepSeek-R1 [7]), encouraging the model to deliberate before responding. The effectiveness of such techniques depends heavily on the length of the thinking stage [10, 30]. Too few steps may fail to capture sufficient task complexity, while too many can lead to overthinking and unnecessary computation [25]. Thus, controlling the thinking path length and understanding its underlying mechanisms are essential for balancing accuracy, efficiency, and responsiveness. While several prior works have explored the impact of reasoning length on model quality and proposed techniques to control or optimize response length through specialized training procedures [1] or prompting [29], they do not investigate the underlying mechanistic design that governs how reasoning length is determined. Moreover, none of these approaches address the unique case of structured reasoning, where the model explicitly separates holistic thinking stage from the final response. We consider this setting particularly important, as it offers valuable opportunity to better understand and exploit the thinking phase\" of reasoning models. This work tackles both of these gaps. First, we investigate whether LLMs are capable of monitoring their relative position within the thinking process. Possessing even an implicit form of progress monitoring may enable aspects of key cognitive concepts such as self-regulation [32] and metacognition [6]. To identify mechanisms that encode this information, we perform regression analysis, and show that the relative position can be captured by projections that we term progress vectors\". The extracted information is then used to create an interactive loading bar visualization, see Figure 1(a) that depicts the models progress throughout the thinking phase, making the reasoning process more transparent and easier for users to collaborate with. The ability to extract progress information does not mean that the model employs it mechanistically, unless an intervention analysis is performed. We thus manipulate the internal representation along the progress vectors and achieve clear modulation of the length of the thinking phase, showing overclocking effects. The former is depicted in Figure 1(b). Reassuringly, this modulation does not tend to be detrimental to the LLMs performance. In fact, we show that overclocking can improve the models performance by mitigating overthinking, enhancing computational efficiency, and tailoring the models reasoning depth to each tasks complexity. Our main contributions are as follows: (i) We provide the first empirical evidence that LLMs maintain an internal estimate of their relative position within the explicit thinking phase. This finding sheds light on the plausibility of planning and self-monitoring abilities of LLMs, concepts typically associated with cognitive capabilities. (ii) We identify an internal encoding of this information by learning progress vector projections, and employ these projections to expose dynamic thinking progress bar. (iii) We perform an intervention study and manipulate the progress vectors to overclock and downclock the reasoning process. Finally, (iv) We empirically demonstrate that interventions of the progress vectors improve both the efficiency and the effectiveness of strong LLMs such as DeepSeek-R1 by mitigating overthinking and reducing the generation of unnecessary reasoning steps."
        },
        {
            "title": "2 Related Work",
            "content": "Test-time scaling refers to recent trend in which the reasoning depth of an LLM is dynamically increased during inference, allowing the model to perform complex multi-step reasoning and extend its problem-solving capabilities [23, 28]. This approach heavily relies on the models ability to reason effectively over long trajectories. One notable technique for improving the effectiveness of multi-step reasoning is Chain-of-Thought (CoT) prompting [27], which encourages the model to generate sequence of intermediate reasoning steps before producing its final answer by incorporating additional guidance into the prompt. This technique has been adopted and extended in models such as DeepSeek-R1 [7], OpenAIs O1 [9], and S1 [16] which are fine-tuned on datasets containing CoT-style reasoning steps prior to the final answer. further refinement was proposed by DeepSeekR1 [7], which explicitly enforces open-ended structured reasoning through the use of dedicated <think> and </think> tokens that mark the beginning and end of the models thinking phase. The length of the reasoning chain is key factor influencing the performance of modern reasoning models. If the chain is too long, the model may engage in unnecessary computation and suffer from 2 overthinking [25, 24, 2], phenomenon in which excessive reasoning steps degrade answer quality. On the other hand, if the reasoning is too short, the model may fail to capture the complexity required to solve the task effectively [26]. To address this trade-off, state-of-the-art LLMs are increasingly designed to support adaptive thinking, where the length of the reasoning phase is dynamically adjusted based on task complexity. Two main strategies for improving the length-control the duration of models reasoning have been proposed: (i) Inference-time methods modify the prompt or inject guidance while the model is generating its chain of thought. Notable examples include Kojima et al. [13], who optimize the initial prompt, and Wu et al. [29], Jin et al. [11], who dynamically intervene during reasoning by prompting to shorten or lengthen the reasoning trajectory (ii) Training-time methods fine-tune the model with objectives and datasets that explicitly encourage regulation of the reasoning length. notable example is Length-Controlled Policy Optimization (LCPO) [1], which frames reasoning-length control as RL objective. Beyond the context of controlling reasoning-chain length, length control has long been studied in standard LLMs to manage overall output length. substantial body of work addresses this objective across applications such as summarization [14, 22]. In contrast to these methods that primarily aim to control the length of the generated answer for efficiency or user-specific preferences, our work focuses on the thinking length rather than the final output. This allows us to gain insights into the models planning, monitoring, and control capabilities, which we then leverage to improve model quality and model responsiveness. Mechanistic interpretability aims to reverse-engineer LLMs so they can be understood, trusted, and controlled [5, 19]. This is done by connecting observable behaviors to concrete neural components by assigning semantic roles to individual neurons or weights and isolating compact circuits. Prominent examples include the induction-heads [20] and mechanisms for arithmetic [12, 17, 31, 3] and incontext learning via task-vectors [8].Our work follows this line of research and aims to uncover the intrinsic mechanisms that encode models relative position within its internal reasoning process, with the goal of better understanding and even controlling the planning abilities of reasoning LLMs."
        },
        {
            "title": "3 Method",
            "content": "We investigate monitoring and control of the thinking phase in reasoning models. In Section 3.1, we show that the models hidden representations encode information about its position within the thinking phase. This allows for real-time monitoring in the form of loading bar and reveals distinct thinking patterns. In Section 3.2, we demonstrate that this internal mechanism can be modified to control the depth of the thinking phase. 3.1 Monitoring the Thinking-Phase We hypothesize that as part of learning to reason effectively, models must also implicitly learn to track their progress through the thinking phase, maintaining an estimate, for example, of how close they are to the final answer. Since progress-tracking is input-dependent, such information cannot be stored in the models static weights and must instead be encoded dynamically in the hidden representations passed between layers. We chose to extract information from the final hidden layer for two main reasons. First, this layer provides the richest token-level representations, having integrated information from all previous layers. Second, our goal is to eventually intervene in the reasoning process by manipulating the thinking progress. The final hidden layer is especially suitable for this because it is close to the token embedding space, allowing for effective intervention. Moreover, it lies downstream of the attention layers, which cache intermediate states. This means that modifying the hidden representation at this stage can influence the immediate output token without disrupting the models internal memory, thereby isolating the effect of the intervention to single step. As another design choice, we focus on models that perform explicit structured reasoning, characterized by distinct and continuous reasoning phase delimited by <think> and </think> tokens, as employed in DeepSeek-R1 [7]. This structure enables us to quantify the models progression within the thinking phase by precisely labeling each token with an interpolated value between zero and one, according to its relative position. To learn thinking progress predictor, we initially sample entries from dataset of mathematical problems. We then prompt reasoning model with single problem each time, generating an answer that contains thinking phase. For generation that ended successfully, we specifically observe its thinking trajectory = w1w2 . . . wN , which is the sequence of tokens between <think> and Figure 2: Linear vs Non-linear regression MSE loss for monitoring the thinking phase. For each model-dataset pair, 30 problems were sampled. Figure 3: MSE of Linear vs RNN for thinkingphase monitoring regression in learned and zeroshot settings. </think>, and collect the representations from the last layer, {h(k) Rd = 1, . . . , Nk} for each token in the trajectory, where is the hidden size of the reasoning model and Nk is the number of tokens in that trajectory. We pair each hidden state with its normalized position in the sequence as regression label. Formally, we construct dataset by: = {(h(k) , p(k) ) = 1, . . . , K, = 1, . . . , Nk} , (1) Rd is the hidden representation of the j-th token in the k-th thinking trajectory, and (0, 1] is the tokens relative position in its sequence. Here, denotes the number of where h(k) p(k) = Nk sampled trajectories and the total number of samples in is = (cid:80)K This allows us to optimize for progress extraction function fθ which maps the hidden representations to their relative positions in the form of regression task θ = arg minθ (h,p)D(fθ(h) p)2. k=1 Nk. (cid:80) We conduct experiments using two reasoning models - DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Llama-8B[7] - both of which are designed to explicitly separate the thinking process from final answers. We evaluate them on two benchmark datasets of mathematical problems: Math500[15] and GSM8k [4]. For each dataset, we sample 30 problems and prompt each model to generate 5 distinct responses per problem using temperature of 0.6. The prompt format includes the instruction: Please reason step by step, and put your final answer within boxed{}. <think>, as recommended by Guo et al. [7]. Following the dataset curation procedure described earlier, we obtain four distinct progress regression datasets, each corresponding to unique combination of reasoning model and problem set. We split each dataset into train and test sets using an approximate 80/20 ratio based on entire thinking trajectories, ensuring that all generations for given problem remain within the same split. We first fit linear regressor parametrized by θ Rd as the function fθ for estimating the progress property p(i) . We refer to the parameter vector θ as the thinking progress vector (TPV). = θT h(i) To assess whether the estimated progress p(i) can be more effectively captured by more complex model, we compare simple two-layer feedforward network (FFN) with our proposed TPV model. As shown in Figure 2, both models achieve low loss on the test sets, with no observable improvement from the FFN. Guided by the principle of favoring simplicity, and supported by these empirical findings, we select TPV as our method of choice for progress extraction in subsequent experiments. To improve the predictions, we leverage the models autoregressive nature, in which tokens are generated sequentially, and apply exponential smoothing over the prediction history to reduce noise. In Figure 5, we illustrate TPV predictions over problems from the Math-500 test set [15]. The light blue and dark blue points represent the predictions and smoothed predictions correspondingly, while the orange line depicts the ideal loading bar obtained through linear interpolation. Figure 5(a) presents an aggregative view over data points from several thinking trajectories, while Figures 5 (b, c) showcase TPV predictions and smoothed predictions along thinking trajectories of single problems 4 (a) MATH-500 (b) GSM8K (zero-shot) Figure 4: Qualitative analysis of RNN-based progress prediction: Predictions across time steps during the thinking phase. (a) Results on MATH-500 (in-domain); (b) results on GSM8K (zero-shot). in the Math-500 test set. As shown in the figures, both approaches, with and without smoothing, successfully predict the relative position, while the latter produces more precise results that can be used to create clearer, more interpretable loading bar. Motivated by this observation, and to better exploit the temporal structure of the progress-bar prediction task, we replace exponential smoothing with trainable sequence model. Concretely, we adopt single-layer GRU, parameterized by θRNN, whose hidden and input dimensions are both set to d, and whose output at each step is scalar. The network is trained on the sequence dataset D, using the same training samples as D, only with sequence of relative positions rather than performing single-step predictions: = {(h(k) 1 , . . . , h(k) Nk ), (p(k) 1 , . . . , p(k) Nk ) = 1, . . . , } . (2) To extend our analysis, we train the model on the MATH-500 training set and evaluate it on both the MATH-500 and GSM8K test sets, thereby covering both the learned and zero-shot regimes. The results are presented in Figure 3, showing that the RNN consistently achieves lower loss than the linear model in both settings. Although performance drops slightly in the zero-shot regime, the loss remains relatively low, suggesting that the model generalizes well across datasets. Finally, Figure 4 presents qualitative analysis of the RNNs predictions over time during the thinking phase. In both learned and zero-shot regimes, the RNN predictions (shown in purple) produce outputs that are smooth, consistent over time steps, monotonically increasing, and accurately reflect the progress of the thinking phase, especially when combined with additional exponential smoothing. 3.2 Controlling Thinking Progress crucial question that arises is whether TPVs reflect fundamental mechanism that the model uses to track its reasoning progress, or if they are merely residual artifacts that correlate with progress but do not play causal role in the computation. To address this, we conduct intervention experiments. Intervention Technique. The intervention experiments shift the hidden representation in the direction of projection vector θ by an amount α: hα = + αθ. The altered representation has new prediction value: θT hα = θT (h + αθ) = + αθ2. By performing this intervention past all attention layers, we intervene with next token prediction and refrain from editing representation values that are cached and used in consecutive decoding steps. This way, next token predictions along the thinking trajectory are influenced in two ways: (i) locally, by the intervention-edited representation values; (ii) and historically, derived from the auto-regressive process, through the tokens that were already potentially altered as result of previous interventions. 5 (b) (c) (a) Figure 5: Relative Progress Prediction Analysis: (a) We sample 1000 random tokens from the thinking trajectories across all examples in the test set of the Math-500 dataset. The x-axis denotes the ground-truth relative position within the thinking phase, while the y-axis shows the predicted progress. (b, c) Two representative examples from the Math500 test set, illustrating how the predictor estimates progress across the full thinking trajectory of single generation. The x-axis denotes the decoding step index, and the y-axis shows the predicted value in the [0, 1] range. In our experiments, we treat α as hyperparameter that determines the strength of the intervention. Setting α = 0 results in no intervention, preserving the original computation. Positive values of α induce overclocking. We hypothesize that overclocking will accelerate the models reasoning phase, making it shorter and more decisive. Overthinking. We analyze the impact of TPV intervention on different thinking patterns. Figure 1(b) compares two thinking sequences generated by the DeepSeek-R1-Distill-Qwen-32B modelwith and without intervention. The original sequence is marked by hesitation and verbosity, while the TPV-accelerated version is significantly more concise, using nearly six times fewer tokens. Notably, both trajectories arrive at the correct final answer. Example 1 illustrates mathematical problem where the original model, when prompted, delves into loop of self-verification and indecisiveness, raising repeating arguments and calculations without converging to conclusion within the limit of 2048 tokens. The accelerated version reaches the correct answer while terminating in less than half of the token limit. The corresponding predicted progress trajectories for both generations are shown in Figure 6. In addition to being shorter, the intervened trajectory shows consistently higher predicted progress beyond certain decoding step. Figure 6: Predicted progress (p) values for two thinking sequences, one from the original generation and the other from the TPV-intervened process, on the same prompt in Example 1, using DeepSeekR1-Distill-Qwen-32B. For this prompt the original generation does not converge to an answer. 6 Prompt In how many ways can 8 people sit around round table if 3 of the people Pierre, Rosa, and Thomas all want to sit together? (Two seatings are considered the same if one is rotation of the other.) Please reason step by step, and put your final answer within boxed{} TPV α = 100 (806 Thinking Tokens) <think> Okay, so have this problem: 8 people want to sit around round table, . . . Alright, lets break this down. remember that for circular permutations, the number of ways to arrange people is (n-1)! . . . think the method is correct because were treating the block as single unit, so the number of units is 6, and the circular permutations of 6 units is 5!. Then, within the block, 3! arrangements. So, 5! * 3! = 720. think thats solid. dont see any mistakes in the reasoning. So, the answer should be 720. **Final Answer** The number of ways is boxed{720}. </think> Original (Continues Past 2048 Tokens) <think> Okay, so have this problem where need to figure out how many ways 8 people can sit around round table. The catch is that Pierre, Rosa, and Thomas all want to sit together. Hmm, circular permutations can be bit tricky, but let me think through this step by step. . . . So, if consider Pierre, Rosa, and Thomas as one block But wait, . . . But hold on, want to make sure didnt miss anything. . . . Is there another way to think about this? Maybe by Wait, no, that might complicate . . . Wait, maybe made mistake. . . Wait, perhaps the first approach is wrong . . . Wait, no, think confused. . . Wait, maybe need to think differently. . . . Wait, so if fix Pierres seat, then the block is fixed relative to Pierre, . . . Wait, maybe both approaches are correct, but they are counting different things. . . . Wait, no, . . .Wait, let me think again. If we fix Pierres seat . . . Example 1: Qualitative examples of indecisive loops and overthinking in the thinking process of the base model (right), which is then mitigated in the text produced by our model (bottom left)."
        },
        {
            "title": "4 Experiments",
            "content": "This section presents systematic evaluation of the intervention technique described in Sec. 3.2, which overclocks the thinking phase of reasoning LLMs. We assess the method with respect to effectiveness, measured by improvements in answer quality, and efficiency, measured by lower computational cost. The experiments were conducted with two models, DeepSeek-R1-Distill-Qwen32B and DeepSeek-R1-Distill-LLaMA-8B, which were evaluated on two mathematical benchmarks known to be effective for reasoning LLMs. The full experimental setup is detailed in Appendix A. Baselines. To comprehensively evaluate our method, we benchmark it against the following baselines: (i) Base Model: Running the model on the problem concatenated with the instruction Please reason step by step, and put your final answer within boxed{}, as defined in DeepSeek-R1 prompting guidelines. (ii) Temperature: Same as (i) except we generate five samples and select the shortest answer. Following [7], we set the temperature to 0.6. (iii) Instruct: straightforward prompting strategy encourages the model to produce decisive answers with the following prompt: Please reason step by step, place your final answer inside boxed{}, and then immediately stop with < end_of_sentence >. Present all necessary calculations or arguments concisely, avoiding unnecessary elaboration or verbosity. < think >. 4.1 Effectiveness We measure the effectiveness of TPVs in Tables. 1, and 2 which report results for DeepSeek-R1Qwen-32B and DeepSeek-R1-LLaMA-8B respectively. Each table summarizes performance on two mathematical datasets evaluated under three token budgets. The right half of each table presents GSM-8K scores for budgets of 256, 512, and 1024 tokens, whereas the left half presents Math-500 scores for budgets of 512, 1024, and 2048 tokens. For each regime we report three key metrics to evaluate model performance: (i) #Correct - the number of problems for which the model produced correct final answer. We define the answer as the expression enclosed within the boxed{} symbol. This pattern is matched programmatically and verified manually. (ii) #Answered - the number of generations in which the model produced an answer, operationally determined by the presence of the boxed{} symbol in the output. (iii) #Ended - the number of generations that concluded naturally before hitting the enforced token limit. It is important to note that the number of answered problems can exceed the number of completed generations. This is because both models often produce the boxed{} token at the end of the thinking phase, before completing the full response, allowing truncated generations to still include valid answer. The opposite may also occur: particularly the LLaMA model occasionally fails to follow the instruction to wrap the answer in boxed{}, causing exclusion of otherwise valid answers. Table 1 reveals four notable trends: (1) The impact of α: Increasing α in our method from 5 to 100, both with and without the instruction-based acceleration used in baseline (iii), generally increases the numbers of completed, ended, and correct answers produced by the model, providing further evidence that our intervention method influences the thinking length. In particular, on Math-500, larger values of α increased the number of correct answers by 42,39,7 on token budget of 512,1024 and 2048 respectively. Similarly, on GSM-8K, the number of ended answers increases by 15, 19, and 5 for token budgets of 256, 512, and 1024, respectively. (2) Comparing Acceleration Baselines Against the Base Model: Baselines (ii) and (iii) accelerate the base model using prompting responses and temperature-based ensembling. Unsurprisingly, in most regimes both methods increase all three metrics, demonstrating that they are strong baselines against which to evaluate our overclocking approach. Specifically, on the Math-500 dataset with 512-token budget, the number of correct answers rises from 36 (base) to 54 under the temperature baseline and 53 under the instruction-based baseline, while the number of answers increases from 38 to 59 for both methods. Finally, the number of completions increases from 28 for the base model to 43 for the temperature baseline and 52 for the instruction-based baseline. Similarly, under the same 512-token limit on GSM-8K, both acceleration strategies yield improvements. The temperature-based approach increases correct responses from 221 to 237, while the instruction-driven method produces 224 correct answers, and the total answers climb from 223 to 249 with the temperature strategy and to 237 with instruction prompts. Completions also see marked rise, increasing from 28 for the base model to 43 under the temperature baseline and to 52 with instruction-based acceleration. (3) Comparing Our Method Against Baselines: Although these baselines perform strongly and the temperature-based baseline requires roughly five times more compute, our method outperforms both by producing more correct answers and more decisive responses. In particular, under low compute budgets of 256 or 512 tokens, our approach with α = 100 increases the number of correct answers on Math-500 by at least 80% in the 512 token-budget regime and boosts correct responses on GSM-8K by an average of 80% across the 256 and 512 token settings. For example, on Math-500 in the 512 token-budget regime, correct answers rise from 54 under the temperature baseline and 53 under the instruction baseline to 100 with our method, resulting in gain of more than 46 correct answers, while the number of completed answers increases from 59 under both baselines to 106, gain of 47. Remarkably, these increases in correct answers do not come at the cost of more errors, as the error rate remains unchanged. This suggests our method shortens reasoning without increasing errors, promoting more decisive thinking. Our findings for compute budgets larger than 512 generally follow the same trend, showing improvements in the number of correct answers in most regimes, without an increase in the error rate. (4) Complementary Contribution: Although our empirical findings approve that our method is more effective than the baselines, there are still cases where our method lags behind the prompting-based approach (denoted as Instruct). One prominent example is the regime of token-budget of 2048 on the Math 500, where the instruct baseline answers correctly 10% more than our method. similar trend was also observed in Table 2, with experiments conducted using LLaMA instead of the Qwen DeepSeek variants. This observation raises the question of whether the improvements are orthogonal or if the two methods directly compete. To investigate, we conducted dedicated experiments in which the instruction-based prompting technique was combined with our intervention method and compared against each individual approach. As shown in the last two rows of both tables, this hybrid approach consistently yields the best performance across most regimes, achieving an improvement of 66% on average and 285% at max over the instruction-based baseline, and 223% on average and 1416% at max over the base model. These findings suggest that our method is complementary to prompting strategies and can be effectively integrated with other acceleration techniques. 8 Table 1: Results for DeepSeek-R1-Qwen-32B on GSM-8K and Math500 at different context lengths (512, 1024, 2048 for Math500; 256, 512, 1024 for GSM-8K), showing #Correct #Cr, #Answered #An and #Ended #En for each. Method 512 Math500 2048 256 GSM-8K 512 1024 #Cr #An #En #Cr #An #En #Cr #An #En #Cr #An #En #Cr #An #En #Cr #An #En Base Temperature Instruct 36 54 53 TPV α = 5 58 TPV α = 100 100 TPV α = 5 Ins 123 TPV α = 100 Ins 148 38 59 59 58 106 126 160 28 154 43 146 52 42 162 94 201 109 240 142 256 156 149 214 165 213 254 290 101 278 119 294 177 316 98 277 167 284 218 316 279 300 280 305 280 296 331 337 245 294 296 257 268 306 330 12 39 16 23 36 34 45 16 40 26 41 26 49 9 221 33 237 15 224 14 226 29 245 34 239 40 247 223 249 237 237 256 250 263 227 285 248 237 232 233 285 252 285 248 286 261 284 298 249 299 297 299 300 298 296 300 300 293 298 300 298 Table 2: Results for DeepSeek-R1-LLaMA-8B on GSM-8K and Math500 at different context lengths (512, 1024, 2048 for Math500; 256, 512, 1024 for GSM-8K), showing #Correct #Cr, #Answered #An and #Ended #En for each. Method 512 Math500 1024 2048 GSM-8K 512 1024 #Cr #An #En #Cr #An #En #Cr #An #En #Cr #An #En #Cr #An #En #Cr #An #En Base Temperature Instruct TPV α = 5 TPV α = 100 TPV α = 5 Ins TPV α = 100 Ins 6 29 50 4 26 51 91 6 31 55 5 30 55 118 47 11 51 95 68 107 37 12 41 70 67 103 119 54 104 117 42 77 112 170 124 112 229 137 190 153 118 109 170 102 193 159 227 142 122 156 172 117 113 175 273 332 302 276 273 315 284 0 7 7 0 4 12 27 0 7 8 0 5 12 2 20 13 2 7 13 70 21 50 66 22 28 58 80 27 60 81 26 34 72 99 40 216 63 167 85 35 95 118 38 164 80 176 81 47 74 100 43 46 95 104 247 294 273 235 223 268 4.2 Efficiency We conduct series of intervention experiments on the Math-500 and GSM8K datasets by varying the intervention parameter α to overclock the models thinking phase. Figure 7 shows that increasing α consistently reduces the length of the thinking phase, making the reasoning process more efficient. These findings support our hypothesis that TPV functions as an active control signal in the models internal computation, rather than being passive correlate. In particular, when applying our method with α = 100 to the DeepSeek-R1 LLaMA model using the prompting strategy (baseline iii) on the GSM8K dataset, the average number of tokens decreases from approximately 500 to fewer than 350, resulting in 30% reduction in compute. Moreover, all positive values of α consistently accelerate the thinking phase relative to the baseline (α = 0), while also improving its effectiveness, as shown in Tables 12. For additional qualitative examples, see examples in Appendix D. 4.3 Qualitative Analysis To further evaluate the reliability of TPVs in estimating models position within its reasoning process, we test their performance under two additional conditions: (i) varied prompting strategies and (ii) differing reasoning sequence lengths. Figures 8(a-d) show that TPVs remain effective across diverse instructions, distinct from the original prompt used during training (see caption for the prompts). Figure 8(e) shows consistently low test loss across bins of different thinking sequence lengths, indicating robustness to varying reasoning depths. To better understand how progress is monitored and how it is modified during the thinking phase, we explored cases in which the behavior of the indicator is not monotonic, see Appendix B. For example it is often the case that the progress prediction increases, until noticeable drop occurs. This drop in predicted progress often coincides with self-verification behavior, where the model attempts to solve the problem again using an alternative approach. To further explore TPVs capacity to capture and explain different reasoning dynamics, we examine how predicted progress responds to specific tokens that suggest either hesitation (wait) or advancement (right). We quantify this behavior, by measuring the average effect of individual tokens on 9 Impact of α on Intervention: The x-axis represents different values of α in Figure 7: {0, 5, 25, 50, 100}, where α = 0 indicates no intervention and higher values of α amplify the amount of intervention. The y-axis shows the average number of tokens required to complete the answer. Results are obtained using DeepSeek-R1 models. Figure 8: Robustness of predicted progress under varied prompting styles and reasoning lengths. Left: Predicted and smoothed progress trajectories for the problem How many 3-digit numbers can be formed using the digits 1 through 9 (no zeros), with no repeated digits? under four different prompts: (a) Please reason step by step. (b) Think carefully and slowly. Provide detailed explanation. (c) Think quickly and provide concise answer. (d) Present all necessary calculations or arguments concisely; avoid unnecessary elaboration or verbosity. Right: (e) Mean MSE test loss from both Math500 and GSM8K datasets, grouped by thinking phase length (binned). The number of trajectories in each bin is shown above each bar. predicted progress values across the dataset, see Appendix C. The analysis shows that certain words do tend to significantly shift the predicted progress indicator in characteristic direction."
        },
        {
            "title": "5 Discussion",
            "content": "The Nelson and Narens metacognition model [18] distinguishes two interacting levels: the object level, which performs cognitive operations, and the meta level, which engages in monitoring and control over the object-level processes. We show that these two information flows, monitoring and control, are manifested in LLMs, as described in Sections 3.1, and 3.2, respectively. Similarly, Zimmerman [32] describe the process of self-regulated learning and highlights monitoring and self10 reflection as two essential components. In this context, our work shows that LLMs can estimate the number of reasoning steps required to reach desired answer, and that this estimation evolves in real time based on the tokens generated. On the applicative side, our method represents an important step toward making reasoning models more transparent and controllable. At the time of writing, millions of users interact with reasoning models such as OpenAIs O1 or DeepSeek-R1. During inference, particularly in the thinking phase, users are often left without visibility into the models internal progress or how close it is to producing an answer. By introducing an interactive progress bar that visualizes the models internal status during reasoning, we make these models more predictable, responsive, and user-friendly. Furthermore, our approach introduces plug-and-play intervention mechanism that enables real-time acceleration of the reasoning process through overclocking. This feature is especially useful for users who have an intuition about the complexity of their task and wish to adapt the models reasoning depth accordingly. This contributes to the ongoing effort to make reasoning-capable language models more interactive, adaptive, and aligned with human needs. Limitations. While our approach demonstrates promising results for monitoring and controlling thinking path lengths in LLMs, several important limitations should be acknowledged. First, our investigations are primarily focused on mathematical reasoning tasks, which represent well-structured domain with clear evaluation metrics. The applicability of our progress vector technique to more open-ended reasoning domains (e.g., ethical reasoning, creative problem-solving, or multi-step planning) remains unexplored. Specifically, while overclocking improves performance on our tested benchmarks, we have not thoroughly investigated potential negative side effects of progress vector manipulation on reasoning coherence or factual accuracy outside our test distributions. Second, our method requires access to model hidden states during inference, making it challenging to implement in black-box or API-only settings where only input-output functionality is available. Finally, our experiments focus exclusively on the length dimension of reasoning, without addressing other important aspects such as reasoning breadth, diversity of approaches considered, or uncertainty estimation."
        },
        {
            "title": "6 Conclusions",
            "content": "This paper introduces novel approach to monitoring and controlling thinking path lengths, revealing fundamental mechanisms by which LLMs regulate their reasoning processes. By identifying and manipulating the progress mechanism, we have developed two practical applications with significant implications for AI systems: (i) dynamic thinking progress visualization that enhances transparency, and (ii) an overclocking intervention technique that significantly improves both efficiency and performance. The ability to modulate reasoning depth without sacrificingand often enhancingperformance challenges conventional assumptions about the relationship between computation and accuracy in LLMs. This suggests that modern LLMs are capable of more efficient reasoning but may benefit from external guidance to avoid overthinking patterns."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This work was supported by grant from the Tel Aviv University Center for AI and Data Science (TAD). This research was also supported by the Ministry of Innovation, Science & Technology ,Israel (1001576154) and the Michael J. Fox Foundation (MJFF-022407)."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. [2] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [3] Bilal Chughtai, Lawrence Chan, and Neel Nanda. toy model of universality: Reverse engineering how networks learn group operations. In International Conference on Machine Learning, pages 62436267. PMLR, 2023. 11 [4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [5] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformer-circuits.pub/2021/framework/index.html. [6] John Flavell. Metacognition and cognitive monitoring: new area of cognitive developmental inquiry. American psychologist, 34(10):906, 1979. [7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [8] Roee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors. In Conference on Empirical Methods in Natural Language Processing, 2023. [9] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [10] Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. The impact of reasoning step length on large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 18301842, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.108. URL https://aclanthology.org/2024.findings-acl.108/. [11] Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. The impact of reasoning step length on large language models. arXiv preprint arXiv:2401.04925, 2024. [12] Subhash Kantamneni and Max Tegmark. Language models use trigonometry to do addition. In ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025. URL https://openreview.net/forum?id=CqViN4dQJk. [13] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [14] Jingun Kwon, Hidetaka Kamigaito, and Manabu Okumura. Abstractive document summarization with summary-length prediction. In Andreas Vlachos and Isabelle Augenstein, editors, Findings of the Association for Computational Linguistics: EACL 2023, pages 618624, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.findings-eacl.45. URL https://aclanthology.org/2023.findings-eacl.45/. [15] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [16] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [17] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9XFSbDPmdW. 12 [18] Thomas Nelson. Metamemory: theoretical framework and new findings. In Psychology of learning and motivation, volume 26, pages 125173. Elsevier, 1990. [19] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. Distill, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in. [20] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. [21] OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/ chatgpt, 2023. Accessed: 2025-05-11. [22] Fabian Retkowski and Alexander Waibel. Zero-shot strategies for length-controllable summarization. arXiv preprint arXiv:2501.00233, 2024. [23] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [24] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms. arXiv preprint arXiv:2505.00127, 2025. [25] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. URL https://arxiv. org/abs/2503.16419, 2025. [26] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025. [27] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [28] Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inference-time algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. [29] Tong Wu, Chong Xiang, Jiachen Wang, and Prateek Mittal. Effectively controlling reasoning models through thinking intervention. arXiv preprint arXiv:2503.24370, 2025. [30] Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. [31] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id= S5wmbQc1We. [32] Barry Zimmerman. Becoming self-regulated learner: An overview. Theory into practice, 41 (2):6470, 2002."
        },
        {
            "title": "A Experimental Setup",
            "content": "This section describes the experimental setup used for the empirical studies in Sec. 4. All experiments were conducted in PyTorch with official Hugging Face models and DeepSeek prompts. Each run used single A100 GPU and finished within at most two days. Datasets. We evaluated performance on two publicly available mathematics datasets: GSM-8K and Math-500. For Math-500, we randomly sampled 80 problems to train our loading-bar regressor and used the remaining 420 problems as the test set. For GSM-8K, we randomly sampled 330 problems, reserving the first 30 for training and the remaining 300 for testing. These datasets were chosen because they effectively assess the reasoning capabilities of LLMs, as reasoning has been shown to be remarkably effective on mathematical problems. Non-monotonic Progress Prediction We perform qualitative analysis of TPV-predicted progress trajectories and find that non-monotonic patterns often correspond to self-verification behaviors in the models reasoning. In such cases, the model reapproaches the problem using different method to confirm its solution. For instance, as illustrated in Figure 9, the model first solves combinatorics question using the counting principle, then revalidates its answer by applying the permutations formula. This second solving attempt begins at token 665indicated by vertical linewhich coincides with clear drop in the predicted progress, reflecting the models re-evaluation phase. Figure 9: math question and the associated thinking sequence generated by DeepSeek-R1-DistillQwen-32B, with each row indicating the index of its first token. The sequence is paired with its TPV-predicted progress trajectory, highlighting the effect of self-verification behavior within the models reasoning process. vertical line at token index 665 marks notable drop in predicted progress, aligning with the point where the model begins re-solving the problem."
        },
        {
            "title": "C Effect of Specific Words on Predicted Progress",
            "content": "When examining thinking sequences qualitatively, certain tokens frequently appear in contexts that suggest either hesitation or progression toward concluding the reasoning process. Words such as 14 Figure 10: Average change in values caused by significant tokens across the dataset, highlighting their impact on predicted progress. hmm, wait, and right often signal meaningful transitions in the models internal state. Some of these tokens reflect recognition of inconsistency or error, prompting the model to adjust its reasoning strategy. Others indicate the completion of reasoning step and shift toward the final solution. TPVs enable principled way to quantify this behavior. We analyze the average influence of individual tokens on predicted progress values across the dataset. As illustrated in Figure 10, tokens with the most negative impactsuch as again, hmm, wait, and problemare closely tied to hesitation or reevaluation. In contrast, tokens like right, so, and okay are associated with upward shifts in predicted progress, signaling growing confidence and momentum in the models reasoning process."
        },
        {
            "title": "D Additional Qualitative Analysis",
            "content": "To highlight the impact of our intervention in mitigating overthinking, we present qualitative analysis in Examples 24. Examples 34 demonstrate that while both the baseline model and its over-clocked variant arrive at the same answer, the over-clocked model does so more decisively and with significantly fewer thinking tokens. In contrast, Example 2 shows case where the base model does not succeed in providing an answer within the token limit, in contrast to the over-clocked variant, which provides an answer after 259 thinking tokens. 15 Prompt Suzanne walks four miles every third day. What is the fewest number of miles she can walk in February? Please reason step by step, and put your final answer within boxed{} TPV α = 100 (259 Thinking Tokens) <think> Suzanne walks 4 miles every 3 days. To find the fewest number of miles she can walk in February, we need to determine how many 3-day periods are in 28 days (a non-leap year February). 1. **Calculate the number of 3-day periods in 28 days:** 28 3 9.33 Since Suzanne cant walk fraction of 3day period, we take the integer part, which is 9. 2. **Determine the total miles walked:** 9 4 = 36 miles 3. **Check if 28 days is multiple of 3:** 28 3 = 9 with remainder of Since 28 is not multiple of 3, 9 periods of 3 days account for 27 days, leaving 1 day. Suzanne doesnt walk on the 28th day, so 36 miles is the minimum. **Final Answer** The number of ways is boxed{36}. </think> Original (Continues past 2048 tokens) <think> Okay, so need to figure out the fewest number of miles . . . First, should know how many days are in February. . . . maybe can assume its nonleap year with 28 days? Or does it matter? Let me think. . . . Let me list the days she walks: Day 1, 4, 7, 10, 13, 16, 19, 22, 25, 28. Wait, thats 10 walks. . . . Maybe she can start on later day to minimize the number of walks in February. . . . But how late can she start? . . . if she starts on day 26, for example, she would walk on day 26, then day 29, but February only has 28 days, . . . she would only walk once in February. . . . Wait, actually, the problem says she walks four miles every third day. . . . But wait, if she starts on day 26, does that mean she didnt walk on day 23? . . . she should have walked on day 23 as well. . . . If she walks every third day, that means her walking days are spaced three days apart. So, if she walks on day x, she must have walked on day - 3, - 6, etc., and will walk on day + 3, + 6, etc. . . . Wait, so starting on day 2 or 3, she has 9 walks, whereas starting on day 1, she has 10 walks. But earlier, thought she could start on day 26 and only walk once, but that seems incorrect because her walking schedule is every third day, so she cant just start on day 26 without having walked on day 23, 20, etc. . . . Example 2: An example of overthinking through first presenting an argument not aligned with the constraints of the problem and the effort of reconciling it (right); and straightforward answer presenting simple solution (left). 16 Prompt Original (1872 Thinking Tokens) Compute the 9th Fibonacci number, assuming F(1) = 1. Please reason step by step, and put your final answer within boxed{} TPV α = 100 (293 Thinking Tokens) <think> To compute the 9th Fibonacci number, start by understanding the Fibonacci sequence. The sequence is defined such that each number is the sum of the two preceding ones, with the first two numbers being 1. will list out the Fibonacci numbers up to the 9th term. Starting with F(1) = 1 and F(2) = 1, can calculate each subsequent term by adding the two previous terms: F(3) = F(2) + F(1) = 1 + 1 = 2 . . . F(9) = F(8) + F(7) = 21 + 13 = 34 By following this sequence, find that the 9th Fibonacci number is 34. </think> <think> Okay, so need to find the 9th Fibonacci number, and they mentioned that F(1) is 1. Hmm, Fibonacci numbers. . . remember they start with 0 and 1, but . . . The Fibonacci sequence is where each number is the sum of the two preceding ones. So, if F(1) is 1, then what is F(2)? think its also 1 because usually, the sequence starts with 0, 1, 1, 2, 3, 5, etc. . . . Let me write down the sequence step by step to make sure get it right. F(1) = 1 F(2) = 1 . . . F(9) = F(8) + F(7) = 21 + 13 = 34 Wait, so F(9) is 34? Let me double-check my calculations to make sure didnt make mistake. Starting from F(1) = 1: 1. F(1) = 1 2. F(2) = 1 . . . 9. F(9) = 21 + 13 = 34 Yes, that seems correct. Each time, Im adding the two previous numbers to get the next one. So, the 9th Fibonacci number is 34. . . . let me recall another way . . . remember theres Binets formula, which uses the golden ratio. The formula is: (n) = (Φn Ψn)/ 5 . . . Let me try using this formula to compute F(9) . . . First, compute . . . Now, compute . . . Let me compute this step by step: . . . . . . Similarly, compute . . . . . . Now, plug these into Binets formula: (9) = (Φ9 Ψ9)/ 5 (76.013(0.01316))/2.23607 . . . 34.0 Wow, thats exactly 34. So, using Binets formula also gives me 34 for F(9). That confirms my earlier result. Alternatively, could have used the recursive formula . . . Just to make sure, let me list out the Fibonacci numbers from F(1) to F(9): 1. F(1) = 1 2. F(2) = 1 . . . 9. F(9) = 34 Yep, thats consistent. So, Im confident that the 9th Fibonacci number is 34. </think> Example 3: An example of overthinking through excessive self-verification and redundant, duplicated computation in the thinking process of the base model (right), which is mitigated in the text produced by our model (bottom left). 17 Prompt Original (892 Thinking Tokens) Solve + = 5 , 3X - = 8. Please reason step by step, and put your final answer within boxed{} TPV α = 100 (202 Thinking Tokens) <think> To solve the system of equations: 1. Add the two equations to eliminate Y: (X + ) + (3X ) = 5 + 8 4X = 13 13 4 = 2. Substitute back into the first equation to find Y: 13 4 13 4 + = 5 = 20 4 13 = 7 4 = 5 3. Final Answer: = 13 , = 7 4 </think> <think> Okay, so have these two equations here: + = 5 and 3X - = 8. need to solve for and Y. Hmm, let me think about how to approach this. remember from class that there are couple of methods to solve systems of equations, like substitution and elimination. Maybe can use one of those here. Let me write down the equations again . . . Hmm, . . . Let me try that. Adding equation 1 and equation 2: (X + Y) + (3X - Y) = 5 + 8 Simplify the left side: + + 3X - Combine like terms: + 3X is 4X, and - is 0. So, 4X = 13 Wait, 5 + 8 is 13, so 4X = 13. Then, to solve for X, can divide both sides by 4. = 13/4 Hmm, . . . Okay, now that have X, can plug it back into one of the original equations to find Y. Let me use equation 1 because it looks simpler: + = 5. Substituting = 13/4 into equation 1: 13/4 + = 5 To solve for Y, can subtract 13/4 from both sides. = 5 - 13/4 Hmm, 5 is the same as 20/4, right? So, . . . So, = 7/4, which is 1.75 or 1 and 3/4. Let me double-check my work . . . Starting with the two equations: . . . Which simplifies to . . . Then, plugging back into equation 1: . . . Subtract 13/4: . . . That also seems correct. Just to be thorough, let me plug . . . . . . Yes, that works out. So, both equations are satisfied with = 13/4 and = 7/4. think thats it. used the elimination method by adding the two equations to eliminate Y, solved for X, then substituted back to find Y. Checked my solution in both equations, and it works. So, feel confident that is 13/4 and is 7/4. </think> Example 4: An example of overthinking through excessive self-verification and redundant, duplicated computation in the thinking process of the base model (right), which is mitigated in the text produced by our model (bottom left)."
        }
    ],
    "affiliations": [
        "IBM Research",
        "Tel-Aviv University"
    ]
}