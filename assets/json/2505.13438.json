{
    "paper_title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "authors": [
        "Penghui Qi",
        "Zichen Liu",
        "Tianyu Pang",
        "Chao Du",
        "Wee Sun Lee",
        "Min Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present a novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 8 3 4 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Optimizing Anytime Reasoning\nvia Budget Relative Policy Optimization",
            "content": "Penghui Qi12, Zichen Liu12, Tianyu Pang1, Chao Du1, Wee Sun Lee2, Min Lin1 1Sea AI Lab 2National University of Singapore (cid:135) https://github.com/sail-sg/AnytimeReasoner Figure 1: Left: We optimize anytime reasoning by sampling thinking budgets from prior distribution and maximizing the rewards at sampled budgets to push up the area under the curve. This objective naturally introduces verifiable dense rewards into the thinking process. Right: Budget Relative Policy Optimization (BRPO) leverages these dense rewards to improve advantage estimation via the Monte Carlo return (R) and an interpolated baseline that combines current progress (V1) and the average return within the rollout group (V2)."
        },
        {
            "title": "Abstract",
            "content": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present novel framework, AnytimeReasoner, to optimize anytime reasoning performance, which aims to improve token efficiency and the flexibility of reasoning under varying token budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in decoupled manner to maximize the cumulative reward. Additionally, we introduce novel variance reduction technique, Budget Relative Policy Optimization (BRPO), to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "OpenAI o1 [OpenAI, 2024] and DeepSeek-R1 [Guo et al., 2025] have shown that scaling test-time compute via RL is crucial for LLM reasoning. This involves an extensive thinking process using the chain of thought (CoT) [Wei et al., 2022] before producing an answer. RL is then employed to maximize the outcome reward provided by rule-based verifier to check the correctness of the generated answer. While RL for LLM reasoning is an active area of research, most existing work focuses on optimizing final performance based on the complete thinking process. This approach can be inefficient in both training and deployment, as long CoTs are costly, especially for online services. In our work, we focus on optimizing anytime reasoning for LLMs via RL. This is conceptually similar to the anytime algorithms introduced in Dean and Boddy [1988], Zilberstein and Russell [1995], where the system can be interrupted at any point during computation, providing the best possible solutions within the given time or resource constraint. Concretely in LLM reasoning, we assume the reasoning can be interrupted at any time, and the model should be able to summarize the best answer from the incomplete thinking process when interrupted. This capability is particularly useful for online services with limited computing resources. When there are too many requests to handle, the service can choose to interrupt in-progress requests once the thinking length is able to give sufficient accuracy, reserving longer thinking with better accuracy when resources are available. Additionally, users may want to control the cost of LLM calls, naturally leading to budgeted queries. This design can significantly extend the serving capacity and enhance the user experience. To achieve optimal performance for anytime reasoning, we propose sampling the thinking budget from prior distribution while learning, rather than using fixed, large budget as in prior work [Liu et al., 2025, Zeng et al., 2025, Luo et al., 2025]. This approach makes the model performance robust to potential interruptions in the thinking process, while incentivizing it to reach correct answers more efficiently. By achieving balance between token efficiency and thorough exploration [Qu et al., 2025], these models are also able to obtain better performance when given larger budgets. We investigate how to efficiently train LLMs with RL under sampled thinking budgets. By forcing the model to summarize the answers at predefined thinking budgets (drawn from the support of the prior distribution), we introduce verifiable dense rewards into the reasoning process. These rewards provide richer signals and better credit assignment during training [Qu et al., 2025, Cui et al., 2025]. We also propose novel variance reduction technique termed Budget Relative Policy Optimization (BRPO) that advances beyond GRPO [Shao et al., 2024] to improve training stability and efficiency under this dense reward framework. As illustrate in Figure 1 (right), we leverage rewards at previous budgets to compute the advantage function, combining with the average return of group of reasoning trajectories. Empirically, we observe that generating high-quality summary is critical for both final and anytime performance. Thus, we decouple the optimization of the thinking and summary policies, always sampling from uniform distribution to derive better summary policy, thereby improving training efficiency. We term our overall framework as AnytimeReasoner. Experimental results demonstrate that AnytimeReasoner consistently surpasses GRPO in both final and anytime performance. We conduct extensive ablation studies to evaluate the impact of each component. By independently incorporating decoupled optimization, variance reduction, and budget sampling into GRPO, we observe significant performance enhancements, underscoring the effectiveness of our methods. Notably, even when merely using the maximum token budget (without budget sampling), our method still outperforms GRPO in both standard and anytime reasoning, highlighting the robustness of our approach."
        },
        {
            "title": "2 Methodology",
            "content": "In training paradigm similar to R1-Zero [Guo et al., 2025], the model is tasked with generating comprehensive CoT within designated \"thinking box\" upon receiving question. Subsequently, the model summarizes the answer based on this thinking process. rule-based reward is then calculated according to the summarized answer. The RL objective is to maximize the expected reward: (θ) = Ex Ez question πθ( x) thinking process Ey x, z) πθ( answer [r(x, y)] (1) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) 2 (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) where represents the question, denotes the thinking process, is the summarized answer, and r(x, y) is the reward function. In previous studies [Zeng et al., 2025, Liu et al., 2025, Luo et al., 2025], the generation of thinking process and summary are typically sampled together. If the thinking process exceeds the predefined generation limit, the response is considered negative sample. We contend that this approach is impractical, particularly in online services where valid summary should be provided even if the thinking process is incomplete. We propose decoupling the generation of the thinking process and its summary, allocating separate token budgets for each. When the thinking process is halted due to budget constraints, we insert ellipses followed by </think> to prompt the model to produce summary (see Appendix A), similar to Muennighoff et al. [2025] and Qu et al. [2025]. To differentiate between the thinking and summary policies, we denote the thinking policy as πθ [r(x, y)], the objective can be and the summary policy as πϕ. By defining rϕ(x, z) = πϕ( x,z) expressed as: (θ, ϕ) = ,z X πθ( x) [rϕ(x, z)] . (2) Given that each thinking process, while incurring only small computational overhead. , multiple summaries can be sampled to better estimate the expected reward for 2.1 Optimizing Anytime Reasoning Test-time scaling [OpenAI, 2024] is crucial for enhancing the reasoning capabilities of LLMs. This concept operates on the premise that increased computational effort during the reasoning process generally leads to better performance. However, in typical RL training setups like R1-Zero-like [Guo et al., 2025], the performance on anytime reasoning is not guaranteed. The reward evaluation is based on the entire thinking process, lacking insight into whether incremental thinking consistently improves performance [Qu et al., 2025]. To optimize anytime reasoning, we propose sampling the thinking budget from prior distribution rather than using fixed token budget. Let represent the token budget for thinking, sampled from prior distribution (b = bj) for simplicity). The anytime reasoning objective is: over set of increasing budgets b1, . . . , bB} { (Pj = B Janytime(θ, ϕ) = p ,x ,z πθ( x) j=1 (cid:88) is the truncated thinking process at length of the token budget b, [rϕ(x, b)] = E ,z πθ( x) where Pjrϕ(x, bj ) , (3) z, truncate(z, b), = (cid:26) if if < . Instead of focusing solely on the final score based on the entire thinking process as in standard reasoning task, we maximize the expected score over all possible budgets with distribution . As illustrated in Figure 1, this is akin to maximizing the area under the score curve when is uniform distribution across every token budget. However, evaluating for all token budgets is impractical and 8 in our unnecessary, so we evaluate the score only at small predefined budget support (with experiments). It is important to note that this approach transforms the problem into dense reward framework, introducing verifiable dense rewards for each thinking budget. This facilitates better credit assignment during RL training and enhances the identification of each components contribution to successful reasoning process. As illustrated in Figure 2, the dense rewards for budgets prior to reaching correct answer are low. However, the cumulative return is relatively higher if the reasoning process ultimately arrives at correct answer. In contrast, the cumulative return after the first correct answer is relatively low, localizing and highlighting the tokens that contributed to the initial correct answer. This approach is distinct from typical sparse reward RL training for standard reasoning tasks, where all tokens receive the same return. Such sparse reward structures typically lead to unstable and inefficient RL training, while our dense reward approach provides more informative learning signals throughout the entire reasoning process. 3 Figure 2: By introducing dense rewards, we achieve better credit assignment during RL training. We assume uniform distribution over thinking budgets and omit the probability for simplicity. Relation to Standard Reasoning Tasks larger thinking budget is supposed to yield better performance in expectation. Since is always prefix of z, the optimal summary policy πϕ should satisfy: πθ( for any and x. Then we have: [rϕ (x, b)] x) πθ( x) [rϕ (x, z)] , Janytime(θ, ϕ) (θ, ϕ) (4) (5) This justifies the anytime reasoning objective as lower bound of the standard reasoning objective. Therefore, maximizing performance in anytime reasoning should also enhance performance in standard reasoning tasks. In an extreme case where PB = 1 (training only with full reasoning length), Janytime falls back to the standard reasoning objective 2.2 Budget Relative Policy Optimization . For detailed proof, refer to Appendix C. By defining jt = arg minj bj the thinking policy can be computed as follows: t, which represents the nearest token budget after t, the gradient for θJanytime(θ, ϕ) = ,zπθ (x) xp z (cid:88) θ log πθ(ztx, z<t) (R(x, z, jt) (x, z<t)) , (6) t=1 where R(x, z, jt) = Pjrϕ(x, bj ), j=jt (cid:88) and (x, z<t) is the variance reduction term, which should be function correlated to R(x, z, jt) but invariant with respect to zt. Typically, we set (x, z<t) = πθ( t x,z<t) [R(x, [z<t, t], jt)], representing the expected future return [Sutton and Barto, 2018]. In traditional RL, critic neural network is often used to estimate this value. However, training critic model for LLM can be both costly and noisy [Guo et al., 2025]. An alternative approach is Monte Carlo sampling, as used in VinePPO [Kazemnejad et al., 2024], but this requires significant additional computation to estimate expectations across all budgets. Another method, GRPO [Shao et al., 2024], treats generation as bandit problem and uses the average scores of multiple responses for variance reduction. In LLM generation, newly sampled tokens (actions) are consistently appended to the existing context (states). This implies that the current context (z<t) always serves as prefix for any future context ([z<t, t]). This unique property distinguishes it from traditional RL but is often overlooked. Assuming perfect summary policy that consistently extracts the best answer from the thinking process, the reward should increase monotonically with the number of generated tokens, satisfying t]). Consequently, the current reward rϕ(x, z<t) is correlated with any rϕ(x, [z<t, rϕ(x, z<t) future reward rϕ(x, [z<t, t]), particularly when is large enough to yield correct answer or when z<t . This correlation justifies its use as suitable baseline for variance reduction. Building on this insight, we introduce Budget Relative Policy Optimization (BRPO) for efficient variance reduction. Specifically, we employ the following variance reduction term: jt 1 j=1 λjt jrϕ(x, jt 1 j=1 λjt V1 = Pj, bj ) (7) (cid:80) j=jt (cid:88) (cid:80) 4 Figure 3: Left: The correlation coefficient of V1 and V2 with R(x, z, jt). Right: The normalized variance of our BRPO. We evaluate the R1-Distill-1.5B model under the scenario where λ = 0.5, 1000, 2000, ..., 8000 and { is uniform distribution over . } where the evaluated scores at previous budgets, weighted by discount factor λ, serve as the reward baseline (highlighted in red), and are multiplied by the sum of probabilities after jt to align with the scale of R(x, z, jt). As illustrated in Figure 3, when is small, the effectiveness of V1 may diminish because short thinking process z<t provides limited information. In such cases, we apply variant of GRPO as supplement. We sample set of thinking processes z1, z2, . . . , zG and compute: { R(x, zi, jt), } V2 = 1 i=1 (cid:88) (8) which represents the expected return after jt given the question x. Note that the correlation between V2 and R(x, z, jt) decreases as increases, as shown in Figure 3, due to differing prefixes (z<t) in these thinking processes. By combining V1 and V2, the overall variance reduction term is: (x, z<t) = 1 jt V1 + jt + 1 V2. (9) As demonstrated in Figure 3, our BRPO significantly outperforms GRPO in reducing variance, especially when the thinking is long. 2.3 Decoupled Optimization for Thinking and Summary In rigorous derivation, the optimization of thinking and summary policies should share the same prior budget distribution . However, an optimal summary policy is crucial when the thinking process is incomplete, and its effectiveness is significantly influenced by . An imbalanced prior distribution can lead to suboptimal summary policy. To achieve robust anytime reasoning performance, we decouple the optimization of thinking and summary policies by using different budget distribution, , for the summary policy. The decoupled gradient of the summary policy with respect to the anytime reasoning objective 3 can be computed as follows: ϕJanytime(θ, ϕ) = xp ,zπθ (x) (cid:34) (cid:88) j= yπϕ(x,z bj (cid:2)ϕ log(πϕ(yx, zbj ))r(x, y)(cid:3) ) (cid:35) . (10) as uniform distribution over the budget support In our experiments, we set . We employ distinct approach to optimize the summary policy. Specifically, for each question and thinking process bj , we sample group of summaries and use GRPO to stabilize the optimization. Typically, shared model (ϕ = θ) is used for both thinking and summary policies. In such cases, the overall gradient is: b1, . . . , bB} { θJanytime(θ) = θJanytime(θ, ϕ) (cid:12) (cid:12) 5 ϕ=θ + ϕJanytime(θ, ϕ) (cid:12) (cid:12) ϕ=θ. Figure 4: The comparison of anytime reasoning performance between GRPO and our AnytimeReasoner with various prior budget distributions. Notably, the accuracies at the maximum token budget (8000) reflect the performance in the standard reasoning task."
        },
        {
            "title": "3 Experiments",
            "content": "We implement our algorithms based on the Verl framework [Sheng et al., 2024], incorporating several key modifications as detailed in Appendix B. We employ Proximal Policy Optimization (PPO) [Schulman et al., 2017] to optimize both thinking and summary policies. For the thinking policy, we use BRPO to compute the advantage function, as detailed in Section 2.2. During training, we allocate four token budgets (B = 4) for thinking: {2000, 4000, 6000, 8000}. For each question, we sample group of 8 complete thinking processes (stopped either by </think> or when exceeding 8000 tokens). We sample 4 answers to calculate the average score at each thinking budget, which is used to compute the advantage function as in Dr. GRPO [Liu et al., 2025]. The summary length is restricted to 128 tokens. We extract the first answer and use rule-based verifier to determine the 0/1 outcome reward. As detailed in Section 2.3, we employ different prior distributions for the thinking and summary policies. Unless otherwise specified, the prior distribution for the summary policy is set to uniform distribution. We fine-tuned DeepSeek-R1-Distill-Qwen-1.5B [Guo et al., 2025] on 40,315 math problems from DeepScaleR [Luo et al., 2025] for single epoch, using batch size of 64 questions per policy iteration. Our experiments were conducted on 8 NVIDIA A100 80G GPUs, with each experiment taking approximately 30 hours to complete. During training, we evaluate the average scores of AIME2024 and AMC2022 every 20 steps and report their performance curves, sampling 32 responses for each question. After training, we assess the final model using five benchmarks: AIME2024 [Li et al., 2024a], AMC2022 [Li et al., 2024a], MATH500 [Hendrycks et al., 2021], Minerva Math [Lewkowycz et al., 2022], and Olympiad Bench [He et al., 2024], with 32 uniform token budgets ranging from 0 to 8000. We compare our methods with GRPO [Shao et al., 2024], incorporating the corrections introduced in Dr. GRPO [Liu et al., 2025]. 3.1 Main Results We consider the following prior distributions when optimizing the thinking policy by equation 3: Base: We only optimize the final performance as in standard reasoning task, namely PB = 1. Uniform: We set as uniform distribution. Linear: We assign probability proportional to the budget length, such that (b) b. 6 We evaluate the final models after training and plot the score curves under varying thinking budgets in Figure 4. For each question in AMC and AIME, we sample 320 thinking processes to compute the average score. For other datasets, we sample 80 thinking processes per question. As shown in Figure 4, all variants of our method consistently outperform GRPO by large margin across varying prior distributions. With small budgets, AnytimeReasoner-uniform excels by prioritizing optimization of these budgets. When the thinking budget is large, AnytimeReasoner with different prior distributions tends to converge to similar performance, demonstrating the robustness of our approach. Notably, even for AnytimeReasoner-base, where we optimize performance only under the maximum thinking budget as in the GRPO baseline, we still achieve significant better performance at all thinking budgets. This improvement is due to the decoupled optimization and our variance reduction technique (discussed further in Section 3.2.3). More details can be found in Appendix D.1. 3.2 Ablations To further investigate which aspects of our framework contribute to performance improvements, we conduct detailed ablations considering three factors: verifiable dense rewards (Section 3.2.1), decoupled optimization (Section 3.2.2), and variance reduction (Section 3.2.3). We report three metrics during training. Anytime Accuracy: the average accuracy over thinking budgets at {2000, 4000, 6000, 8000}. Final Accuracy: the accuracy at the maximum budget (8000). Average Thinking Length: the average thinking length under the maximum budget (8000). 3.2.1 Verifiable Dense Rewards Figure 5: Ablation on verifiable dense rewards. We investigate the effectiveness of verifiable dense rewards by modifying the objective of the thinking policy to equation 3 with linear prior distribution, while keeping the summary policy training consistent with GRPO. Specifically, we use V2 as the variance reduction term to align with GRPO and eliminate the influence of enhanced variance reduction. We also compare our method with reward shaping, where we add length penalty for correct answer as an alternative to budget sampling. In particular, the reward will be 1 for the correct answer and 0 for wrong answer. 0.2 bB As illustrated in Figure 5, incorporating dense rewards improves both the anytime and final performance. Notably, since our objective diverges from directly optimizing final performance as in the GRPO baseline, the observed improvements can be attributed to enhanced credit assignment facilitated by dense rewards. Another prominent observation is that the average thinking length is clearly shorter than the GRPO baseline under the maximum budget. This is because the thinking policy is encouraged to arrive at correct answer as quickly as possible, making the model favor shorter, correct responses. Although reward shaping with length penalty can also reduce the thinking length, it sacrifices the performance and is unstable during training. 7 Figure 6: Ablation on decoupled optimization for summary policy. 3.2.2 Decoupled Optimization To study the impact of decoupled optimization for thinking and summary policies (detailed in Section 2.3), we modify the training of summary policy in GRPO to align with AnytimeReasoner, while keeping the thinking policy training unchanged. Specifically, we sample 4 answers for each thinking budget in {2000, 4000, 6000, 8000}, applying GRPO within each summary group. This approach trains summary policy under uniformly distributed thinking budgets, while the thinking policy optimizes performance only under the maximum budget (8000). As shown in Figure 6, the decoupled GRPO clearly outperforms the vanilla GRPO, especially in the AMC benchmark. Notably, the significant improvement in the average score under sampled budgets indicates that decoupled optimization results in better summary policy for anytime reasoning. 3.2.3 Variance Reduction Figure 7: Ablation on variance reduction. To evaluate the effectiveness of our BRPO variance reduction (as detailed in Section 2.2), we modified the training of the thinking policy by incorporating BRPOs variance reduction techniques, while maintaining the summary policy training consistent with GRPO. Specifically, we set = 4 and (bB) = 1 in equation 7, aligning the objective exactly with GRPO. Figure 7 shows that our approach enhances performance on the AIME benchmark. As discussed in Section 3.2.2, the suboptimal summary policy in GRPO may constrain the potential of BRPOs effectiveness. To address this, we introduced decoupled optimization (detailed in Section 2.3) to improve the summary policy, resulting in further performance gains."
        },
        {
            "title": "4 Related Works",
            "content": "Reinforcement Learning with Verifiable Rewards Since the introduction of DeepSeek-R1 [Guo et al., 2025], growing body of research has adopted the reinforcement learning with verifiable rewards (RLVR) paradigm [Lambert et al., 2024] to improve the reasoning capabilities of large language models (LLMs). SimpleRL [Zeng et al., 2025] provides the first open-source replication of R1-Zero in mathematical domains and analyzes RL dynamics across various base models. Hu et al. [2025] demonstrate that removing the KL regularization used in RLHF [Christiano et al., 2017] improves both RL efficiency and asymptotic performance. Liu et al. [2025] identify an optimization bias in GRPO [Shao et al., 2024] and propose Dr.,GRPO, which applies Monte Carlo policy gradient method with baseline [Sutton and Barto, 2018]. While these works improve our understanding of R1-Zero-style training, they still depend on sparse outcome-based rewards, which pose challenges for credit assignment and learning efficiency [Kazemnejad et al., 2024]. In contrast, our method introduces novel policy optimization framework that leverages cheaply estimated verifiable dense rewards to improve sample efficiency and learning stability. Token Budget Efficiency of Reasoning Models Previous efforts have studied budgeted reasoning by reducing response length through prompting [Jin et al., 2024, Nayab et al., 2024, Lee et al., 2025, Ma et al., 2025] or adaptive sampling [Yang et al., 2025]. While these training-free approaches can shorten outputs, they often entail trade-off between conciseness and task performance. More recent work explores token efficiency within online RL frameworks, enabling models to jointly optimize for accuracy and brevity. Yeo et al. [2025] observe that the output lengths on harder questions tend to grow during RL training, and propose cosine-shaped reward to constrain length. Liu et al. [2025] trace this issue to optimization bias in GRPO and show that correcting it enhances token efficiency. Further, Arora and Zanette [2025] and Aggarwal and Welleck [2025] apply explicit reward shaping to target shortened or fixed outputs. Our work differs by operating in an anytime reasoning framework, where the reasoning process can be interrupted at anytime and the best-effort solution should be provided [Dean and Boddy, 1988, Zilberstein and Russell, 1995]. Despite not explicitly enforcing conciseness, our objective naturally encourages efficient reasoning, as demonstrated empirically. Connection to MRT An independent work to ours, MRT [Qu et al., 2025], optimizes test-time compute by minimizing cumulative regret relative to an oracle. Since the oracle is unknown, they employ meta-RL [Xiang et al., 2025, Beck et al., 2023] as an approximation, aiming to maximize the \"progress\" of each newly generated episode. Despite sharing similar high-level goal, our formulation fundamentally differs. Rather than minimizing regret, we optimize anytime performance by sampling the thinking budget from prior distribution, remaining tractable with standard RL techniques. These foundational distinctions lead to significant methodological differences. Firstly, our approach operates on per-token basis, instead of on episode which is ambiguous and can be hackable in RL if not well handled. Secondly, our method is grounded in principled RL, explicitly accounting for long-term returns. In contrast, MRT adopts greedy strategy, optimizing the progress of immediate next episode only. Our experimental results also significantly outperform their reported outcomes. We achieve an accuracy of 32.7% compared to their reported 30.3% on AIME 2024."
        },
        {
            "title": "5 Conclusion",
            "content": "The effectiveness of test-time scaling in LLM reasoning is commonly attributed to the generationverification gap [Xiang et al., 2025], where verifying solutions is substantially easier than generating them. During reasoning, the model engages in an iterative search process, exploring potential solutions until valid one is found. Once generated, the solution is verified for correctness, and this search-verification loop continues until confident answer is produced. In this work, we present framework that systematically exploits this generation-verification gap. Our approach is based on the key observation that verifying answers and extracting them from partial reasoning traces is easy and computationally cheap. Building on this insight, we design our framework to produce answers at some predefined thinking budgets, thereby introducing verifiable dense rewards to enhance RL training. Furthermore, we utilize these additional rewards to construct more effective variance reduction baseline than GRPO, significantly improving the stability and efficiency of RL training. By integrating these techniques, our framework achieves superior performance in both standard and anytime reasoning tasks."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv: 2503.04697, 2025. Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv: 2502.04463, 2025. Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35: 1634416359, 2022. Thomas Dean and Mark Boddy. An analysis of time-dependent planning. In AAAI, volume 88, pages 4954, 1988. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Openreasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero, 2025. Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, and Mengnan Du. The impact of reasoning step length on large language models. In ACL (Findings), 2024. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. \" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-of-thought? token complexity approach. arXiv preprint arXiv: 2503.01141, 2025. 10 Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024a. Junyan Li, Delin Chen, Tianle Cai, Peihao Chen, Yining Hong, Zhenfang Chen, Yikang Shen, and Chuang Gan. Flexattention for efficient high-resolution vision-language models. In European Conference on Computer Vision, pages 286302. Springer, 2024b. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://github.com/agentica-project/ deepscaler, 2025. Wenjie Ma, Jingxuan He, Charlie Snell, Tyler Griggs, Sewon Min, and Matei Zaharia. Reasoning models can be effective without thinking. arXiv preprint arXiv:2504.09858, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Sania Nayab, Giulio Rossolini, Marco Simoni, Andrea Saracino, Giorgio Buttazzo, Nicolamaria Manes, and Fabrizio Giacomelli. Concise thoughts: Impact of output length on llm reasoning and cost. arXiv preprint arXiv: 2407.19825, 2024. OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms/. Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement finetuning. arXiv preprint arXiv:2503.07572, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682, 2025. 11 Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv: 2504.15895, 2025. Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Shlomo Zilberstein and Stuart Russell. Approximate reasoning using anytime algorithms. In Imprecise and approximate computation, pages 4362. Springer, 1995."
        },
        {
            "title": "Table of Contents",
            "content": "A Implementation Details Tree-like Generation and Training Relation Between Standard and Anytime Reasoning Experimental Results D.1 Main Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 13 14"
        },
        {
            "title": "A Implementation Details",
            "content": "We illustrate the implementation details about how we truncate the reasoning process and prompt the model to output an answer. (a) Thinking is stopped by </think>. (b) Thinking is stopped due to out of budget. Figure 8: We decouple the generation of thinking and its summary. Given the question, the model first generates the thinking, which can be stopped by special token </think> or the budget limit. plus </think> for out of budget cases) Then we insert to prompt the model to summarize the answer. In training, these inserted tokens will be ignored when calculating the loss. (and two ellipsis Final Answer Tree-like Generation and Training Unlike previous methods with sequential question-response generation and training, our approach employs tree-like structure. In this section, we introduce how to address implementation challenges for efficient training. During generation, we use the prefix caching feature of vLLM [Kwon et al., 2023] to reuse computations. We sample complete thinking process for question x, then split it based on predefined token budgets ( in Figure 9). Each partial thinking process is appended with special end-ofthink token (</think>), and the model is prompted to output the answer directly (see Appendix for more details). i, j, { } 13 Figure 9: Our methods utilize tree-like structure for generation and training. During training, each response is typically concatenated with its corresponding question using FlashAttention [Dao et al., 2022] for speed. However, this introduces significant duplicated computation for tree-like structures, making it impractical due to high computational demands for LLM training. We implement tree structure attention mask based on FlexAttention [Li et al., 2024b]. As shown in Figure 9, we append all summaries at the end of the thinking process and record their connection positions in 1D tensor. This tensor is converted to block mask by FlexAttention, avoiding 2D tensors that can cause out-of-memory issues for long generation lengths."
        },
        {
            "title": "C Relation Between Standard and Anytime Reasoning",
            "content": "In this section, we provide proof for the inequality below: Janytime(θ, ϕ) (θ, ϕ) 1 PB Janytime(θ, ϕ). According to equation 4, we have: πθ( x) [rϕ (x, b)] Thus, it follows that: [rϕ (x, z)] , πθ( x) Janytime(θ, ϕ) = X πθ( x) ,z ,z = X (θ, ϕ). πθ( x) [rϕ(x, E (cid:20) [rϕ(x, z)] b)] (cid:21) (11) Assuming r(x, y) have: 0, which is always achievable by adding constant to each reward, we also Janytime(θ, ϕ) = = p X = PBJ ,z ,z ,z x) πθ( (θ, ϕ). [rϕ(x, (cid:20) [PBrϕ(x, B b)] (cid:21) bB )] πθ( x) πθ( x) [PBrϕ(x, z)] Combining 11 and 12, we can get Janytime(θ, ϕ) (θ, ϕ) 1 PB Janytime(θ, ϕ). This completes the proof. 14 (12) (13)"
        },
        {
            "title": "Algorithm",
            "content": "AMC22 AIME24 MATH500 Minerva OlympiadBench Avg. R1-Distill-1.5B GRPO AR-base AR-linear AR-uniform 56.4 65.0 68.4 68.6 68.5 22.3 28.9 32.7 32.1 32.2 81.1 84.7 85.5 85.6 85.6 26.3 28.9 29.6 29.6 29. 42.0 45.9 47.3 47.3 47.2 45.6 50.7 52.7 52.6 52.5 Table 1: The Final Accuracy by evaluating the maximum budget (8000) for the final models."
        },
        {
            "title": "Algorithm",
            "content": "AMC22 AIME24 MATH500 Minerva OlympiadBench Avg. R1-Distill-1.5B GRPO AR-base AR-linear AR-uniform 48.2 53.4 57.0 58.2 58.8 16.3 19.0 21.9 22.3 22.9 74.5 77.2 78.2 79.0 79.4 24.1 26.6 27.3 27.7 27. 36.0 38.8 40.2 40.9 41.2 39.8 43.0 44.9 45.6 46.0 Table 2: The Anytime Accuracy by evaluating 32 budgets (every 250 tokens) for the final models."
        },
        {
            "title": "D Experimental Results",
            "content": "D.1 Main Results We present the training curves of our AnytimeReasoner in Figure 10, corresponding to the experiments in Section 3.1. We also evaluate the performance of the models at training step of 600, and report the final accuracy in Table 1 and the anytime accuracy in Table 2. Figure 10: The training curves for main results."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Sea AI Lab"
    ]
}