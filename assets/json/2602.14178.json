{
    "paper_title": "UniWeTok: An Unified Binary Tokenizer with Codebook Size $\\mathit{2^{128}}$ for Unified Multimodal Large Language Model",
    "authors": [
        "Shaobin Zhuang",
        "Yuang Ai",
        "Jiaming Han",
        "Weijia Mao",
        "Xiaohui Li",
        "Fangyikang Wang",
        "Xiao Wang",
        "Yan Li",
        "Shanchuan Lin",
        "Kun Xu",
        "Zhenheng Yang",
        "Huaibo Huang",
        "Xiangyu Yue",
        "Hao Chen",
        "Yali Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM."
        },
        {
            "title": "Start",
            "content": "UniWeTok: An Unified Binary Tokenizer with Codebook Size 2 128 for Unified Multimodal Large Language Model Shaobin Zhuang1,2, Yuang Ai1,3, Jiaming Han1,3, Weijia Mao1,5, Xiaohui Li2 Fangyikang Wang6, Xiao Wang1, Yan Li1, Shanchuan Lin1, Kun Xu1 Zhenheng Yang1, Huaibo Huang4, Xiangyu Yue3, Hao Chen1, Yali Wang7 1ByteDance, 2Shanghai Jiao Tong University, 3MMLab, The Chinese University of Hong Kong 4Institute of Automation, Chinese Academy of Sciences, 5National University of Singapore 6Zhejiang University, 7Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences Equal contribution, Corresponding Author, Project lead"
        },
        {
            "title": "Abstract",
            "content": "Unified Multimodal Large Language Models (MLLMs) require visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within single framework. In this paper, we introduce UniWeTok, unified discrete tokenizer designed to bridge this gap using massive binary codebook (2 128 ). For training framework, we introduce Pre-Post Distillation and Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose three-stage training framework designed to enhance UniWeToks adaptability across various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM. Code: https://github.com/shallowdream204/BitDance 6 2 0 2 5 1 ] . [ 1 8 7 1 4 1 . 2 0 6 2 : r"
        },
        {
            "title": "Introduction",
            "content": "The success of the next-token prediction paradigm in Large Language Models (LLMs) [1, 63, 70, 71] has motivated extensive research into transferring this approach to unified vision-language modeling. However, achieving this unification presents significant challenges. primary obstacle is the prohibitive computational 1 Figure 1 Various resolution samples generated by our Unified MLLM based on UniWeTok, showcasing its capabilities in prompt adherence, spatial reasoning, and text rendering across various artistic styles. UniWeTok employs spatial downsampling rate of 32 and codebook size of 2128, which reduces the number of visual tokens by 75% while maintaining exceptionally high reconstruction quality. Furthermore, UniWeTok possesses strong semantic extraction capabilities and generative priors, making compressed discrete visual tokens suitable for Unified Multimodal Large Language Model. 2 cost associated with pixel-level modeling, which necessitates effective image compression. Visual tokenizers address this by employing an encoder-decoder architecture [39] to compress images into compact latent representations for subsequent reconstruction. While employing continuous tokenizers to extract image latents is common approach, it frequently suffers from error accumulation and mode collapse during autoregressive generation [86, 127]. Consequently, modeling the distribution of discrete tokens offers more robust alternative. Despite their robustness, discrete tokenizers have historically faced criticism for limited reconstruction capabilities, implying significant information loss. Recent advancements have fundamentally shifted this landscape. Through improvements to LFQ [109], BSQ [121] and GQ [127] scaled the codebook size to an unprecedented sizeexceeding 2128thereby enabling individual tokens to encapsulate massive amount of information. However, such an expansive codebook size introduces new complexities for downstream generation and understanding tasks. While recent approaches such as Infinity [31] and BitDance [3] propose solutions leveraging massive codebooks, the codebook size in text-to-image modeling remains constrained to 232. Furthermore, these approaches do not extend to multimodal understanding or the development of Unified Multimodal Large Language Models (MLLMs). This naturally leads to question: Is it feasible to construct Unified MLLM based on massive discrete visual codebook? To achieve this, we propose UniWeTok, visual discrete tokenizer that unifies robust compression, semantic extraction, and generative priors into single framework. UniWeTok maximizes token information density by achieving 32 spatial downsampling while maintaining codebook size of 2128. Building upon WeTok [127], we comprehensively advance the system across three key dimensions: (1) Training Framework: We introduce Pre-Post Distillation (PPD) loss and Generative-Aware Prior (GAP) loss to significantly enhance performance in downstream understanding and generation tasks. (2) Model Architecture: We propose SigLu activation to ensure the stable convergence of the PPD loss. Furthermore, we transition to hybrid backbone that synergizes the local inductive priors of convolutions with the global receptive field of attention mechanisms. (3) Training Pipeline: We introduce three-stage curriculum learning strategy. By adjusting resolutions and training data distributions, we enable our UniWeTok to robustly adapt to variable resolutions and perceptually sensitive scenarios, such as human faces and text. We first demonstrate that UniWeTok achieves state-of-the-art generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) in class-to-image generation while incurring significantly lower training costs compared to existing methods (Training Tokens: UniWeTok 33B vs. REPA 262B). Building on this efficiency, our Unified MLLM based on UniWeTok not only exhibits competitive capabilities in multimodal understanding but also delivers text-to-image generation quality that surpasses the prominent open-source model (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84). Furthermore, our Unified MLLM also demonstrates remarkable proficiency in image editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). Collectively, these results rigorously validate the effectiveness of UniWeTok as the visual tokenizer for Unified MLLMs modeling."
        },
        {
            "title": "2.1 Discrete Visual Tokenizer",
            "content": "VQVAE [90] and VQGAN [20] employ vector-quantization (VQ) to transform visual input into discrete tokens. But they suffer from low reconstruction quality caused by instability of the codebook utilization. To overcome these drawbacks, one line of work introduces optimization strategies or modules to improve performance [41, 77, 113, 125]. Another line of work focuses on scaling up the codebook size by grouping codebooks [6, 36, 60, 117]. ImageFolder [46], DualToken [80] and TokenFlow [69] use multiple codebooks to assist in optimizing model understanding and reconstruction capabilities. However, VQ-based tokenizers still introduce additional costs due to the lookup operation [23, 41, 108]. MAGVIT-v2 [110] introduces Lookup-Free Quantization to address extra cost and proposes the entropy loss [10, 35] to ensure the utilization of the codebook. BSQ [120] assumes independence between the bits of the binary code to eliminate unbearable computational overhead from entropy loss. WeTok [126] proposes Group-Wise Lookup-Free Quantization to mitigate the codebook entropy calculation error in BSQ. However, current tokenizers based on binary codebooks barely extract any semantic information [69], and their excessively large codebooks are detrimental to downstream generation tasks [126]. In contrast, our UniWeTok successfully achieves robust semantic 3 extraction performance based on binary codebook while ensuring that the extracted discrete tokens remain suitable for downstream generation."
        },
        {
            "title": "2.2 Unified Multimodal Large Language Model",
            "content": "With the development of LLMs [1, 5, 63, 89], MLLMs have attracted lot of research interest due to their strong multimodal understanding and reasoning capabilities [5, 15, 44, 52]. Beyond visual understanding, several recent works [18, 27, 83, 91, 97, 105, 124] attempt to integrate both visual understanding and generation within unified MLLM. Emu2 [83] enables LLMs to generate CLIP embeddings, which are decoded into images using diffusion model. Show-o [105] and Transfusion [124] integrate diffusion objectives into LLMs for image generation, but this design breaks the autoregressive paradigm and complicates the unification of the two tasks. Emu3 [93] and Chameleon [84] use VQVAE [21] as both the visual encoder and decoder, allowing unified next token prediction across images and text. Janus [13, 97] employs separate encoders for understanding and generation, resulting in distinct modalities that limit performance in multi-turn editing and interleaved generation. VILA-U [100] and UniTok [61] are trained using both pixel reconstruction and image-text alignment losses, but they struggle to converge optimally for both tasks. In contrast, we propose discrete, semantic and generation-friendly that unifies understanding and generation within single MLLM."
        },
        {
            "title": "3 Method",
            "content": "In this section, we present the proposed UniWeTok methodology. We begin by introducing the Pre-Post Distillation (PPD) and Generative-Aware Prior (GAP) in Sec. 3.1, which serve as the foundational elements of our training framework. Subsequently, we elaborate on the specific structural design in Sec. 3.2, focusing on the SigLu activation and the hybrid architecture. Finally, we demonstrate our threestage curriculum training pipeline in Sec. 3.3, critical mechanism that enables UniWeTok to generalize robustly across variable resolutions and high-sensitivity scenarios."
        },
        {
            "title": "3.1 Training Framework\nPreliminaries. WeTok [127] compress image I ∈ RH×W ×3\ninto latent feature U=E(I), U∈Rh×w×d, through the en-\ncoder E. It groups the latent features in channel dimension,\nreshape U into UG∈Rh×w×g×d′\n, where d=gd′ and g and\nd′ represent the number and channel of groups. Group-\nWise Lookup-Free Quantization (GQ) introduces codebooks\nCGQ,k={−1, 1}d′\nto perform lookup-free quantization on\neach channel of latent feature,",
            "content": "Q[i, j, k, l] = sign(UG[i, j, k, l]). (1) Notice that we introduce UQ = UG + sg[Q UG] as variable that shares the same value as but has backward gradient only with respect to UG. Here, sg[] denotes the stop-gradient operation. The UQ is reconstructed into image space ˆI = G(UQ) through the decoder G. The loss function Figure 2 Illustration of UniWeTok training framework. We introduce pre-trained semantic encoder for Pre-Post Distillation and lightweight generative model for Generative-Aware Prior. of WeTok consists of the following five parts, LWeTok = ˆI2 (cid:124) (cid:125) (cid:123)(cid:122) Recon. Loss +α UG sg[Q]2 (cid:124) (cid:125) (cid:123)(cid:122) Commitment Loss +β LLPIPS(I, ˆI) (cid:124) (cid:125) (cid:123)(cid:122) Perceptual Loss 4 +γ LGAN(I, ˆI) (cid:125) (cid:123)(cid:122) GAN Loss (cid:124) , +δ LEntropy(UG, Q) (cid:125) (cid:123)(cid:122) Entropy Loss (cid:124) (2) Figure 3 Illustration of the three-stage training pipeline of UniWeTok. where perceptual loss [119] and GAN loss are introduced for better visual quality. Entropy loss consists of token entropy loss and codebook entropy loss. The specific form of token entropy loss is as follows: LToken Entropy = 1 hw (cid:88) (cid:88) (cid:88) i= j=1 k=1 H(qG(ckUG[i, j, k])), (3) where qG(ckUG[i, j, k]) denote the conditional distribution of ckCGQ,k given UG[i, j, k]. Pre-Post Distillation. To facilitate the effective application of UniWeTok in multimodal understanding tasks, it is imperative to endow the encoder with semantic extraction capabilities. As illustrated in Fig. 2, we employ pre-trained semantic encoder ET as the teacher for distillation. Specifically, the teacher encodes the input image into semantic latents ft=ET(I), ftR1dT , where dT is the output dimension of ET. We employ cosine similarity loss for distillation. Notably, we align both UG and UQ with fT, LPre Distill = 1 LPost Distill = 1 uG fT uGfT uQ fT uQfT , uG = AttnPoolPre(UG), , uQ = AttnPoolPost(UQ), LPPD = LPre Distill + ηLPost Distill. (4) (5) (6) This alignment strategy serves to enhance the semantic extraction capability, ensuring that the model effectively captures the underlying semantic information. Generative-Aware Prior. Increasing the number of groups significantly enhances the reconstruction performance of the discrete tokenizer [127]. However, this improvement comes at the cost of increased difficulty for downstream generation tasks. To mitigate this issue, we inject Generative-Aware Prior, enabling the model to perceive the generation objective during the training phase. As illustrated in Fig. 2, we flatten the UQ into 1D sequence UBR(hw)d and feed it into randomly initialized tiny BitDance [3] model to perform next-token diffusion task, which is supervised using the MSE loss, LGAP = ϵN (0,I)UB B([Query, UB[: 1]], ϵ, t)2, (7) where is random sampled from [0, 1]. Query token allows the model not to miss the prediction of the first token and learn the distribution of the entire sequence. Consequently, the comprehensive loss function for the UniWeTok framework is defined as follows: LUniWeTok = LWeTok + θLPPD + µLGAP, (8) where θ and µ serve as hyperparameters that to regulate the weight of reconstruction, understanding and generation task during the UniWeTok training process. 5 Figure 4 Ablation study of three-stage training pipeline."
        },
        {
            "title": "3.2 Unified Discrete Tokenizer Architecture",
            "content": "Hybrid backbone. UniWeTok employs hybrid architecture for both encoder and decoder. In the encoder, input images are initially processed by stacked residual blocks, which serve to extract low-level information and perform spatial downsampling. This is followed by sequence of transformer blocks designed to capture global context. The decoder mirrors this structure symmetrically. As shown in Tab. 7, this design effectively integrates the local inductive bias of convolutional layers while minimizing the computational overhead associated with the attention mechanism. Notably, the original WeTok architectu utilizes downsample block 6 that executes spatial downsampling before channel expansion, sequential process that results in information loss. To address this, we modify the downsample block to perform channel expansion concurrently with downsampling. As shown in Tab. 4, this modification yields substantial improvement in the reconstruction capabilities. SigLu activation. During the training process, we observe notable phenomenon. As shown in Tab. 2, when we perform semantic distillation exclusively on UQ, the model fails to capture meaningful semantic information. We attribute this issue to the commitment loss, which rigidly anchors the encoder outputs UG to values near -1 or 1, making it difficult for the model to adapt its semantic representations. This constraint also creates fundamental conflict with the token entropy loss, as the latter drives the UG towards negative or positive infinity. To resolve this optimization conflict, we propose the SigLu activation function, SigLu(x) = 1 ex 1 + ex , (9) where SigLu is integrated as the final layer of the encoder. The SigLu activation inherently constrains UG to the interval [1, 1]. Under this condition, the token entropy loss becomes equivalent to the commitment loss. Consequently, we set α=0 in Eq. 2, effectively replacing the commitment term with the token entropy loss. As shown in Tab. 2, SigLu enables the model to perform semantic distillation stably. The specific model architecture could be seen in App. A."
        },
        {
            "title": "3.3 Training Pipeline",
            "content": "As illustrated in Fig. 4, our UniWeTok model is constrained to the specific image resolution defined during its pre-training. However, practical downstream applications inevitably require handling diverse range of resolutions. To bridge this gap, we propose three-stage progressive pre-training strategy. As depicted in Fig. 3, the first stage prioritizes computational efficiency by performing large-scale pre-training on general-domain dataset at fixed resolution of 256256. In the second stage, we partition computational resources to facilitate training across multiple resolutions simultaneously. Finally, the third stage employs an annealing training phase focused on perceptually sensitive domains, such as faces and text. This progressive refinement ensures that UniWeTok is optimally aligned with complex downstream understanding and generation tasks."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. We train on two datasets: (i) ImageNet [75] training set; and (ii) the general-domain dataset DataComp-1B [25]. For evaluations on ImageNet, we measure the reconstruction (rFID [32], PSNR, SSIM [95], and LPIPS [119]) and semantic extraction (zero-shot classification [72]) performance on the ImageNet 50k validation set as well as the generation performance of the generative model (gFID, IS, Precision, and Recall). Regarding general-domain, we evaluate the reconstruction performance on the ImageNet 50k validation set and the MS-COCO 2017 validation set [50]. To comprehensively assess the capabilities of UniWeTok in downstream scenarios, we train Unified MLLM based on UniWeTok and evaluate its performance on downstream understanding (SEEDB [42], POPE [47], VQAv2 [30], GQA [34], SQA [57], TQA [79], CQA [64], AI2D [37], RWQA [101], MMMU [116], MME [24]), generation (GenEval [29], DPG-Bench [33]) and editing benchmarks (GEdit [56]). Unless otherwise stated, we conduct ablation studies on the ImageNet training set. Settings. UniWeTok adopts the architecture proposed in WeTok [127] with downsampling factor of 32 and codebook size of 2128. Images are randomly cropped to target sizes for training. For ablation study, all models are trained for 250K steps with Adam [38] and consistent set of hyperparameters. For large-scale training, hyperparameters are individually tuned for each model to achieve optimal performance. For class-to-image generation and Unified MLLM, we adopt the model architecture and training setting in BitDance [3]."
        },
        {
            "title": "4.1 Ablation Studies",
            "content": "Training loss. We conduct ablation studies on the WeTok loss, the semantic distillation loss, and the prior loss. As shown in Tab. 1, incorporating the semantic distillation loss significantly enhances the semantic extraction capability of the tokenizer. Furthermore, the introduction of the prior loss improves the tokenizers 7 performance on downstream generation while maintaining its reconstruction capability. Unexpectedly, the prior loss not only preserves the models understanding capability but actually enhances it. Table 1 Ablation study of semantic distillation and prior loss. We ablate the contribution of each loss. Setting Gen. PPD GAP gFID rFID PSNR Reconstruction SSIM LPIPS(A) LPIPS(V) Top-1 Top-5 Top-10 Zero-shot Acc (%) 2.38 2.66 2. 1.33 1.12 1.18 22.67 22.32 22.27 0.66 0.65 0.63 0.11 0.12 0.12 0.20 0.21 0.22 - 46.89 48. - 73.66 75.62 - 81.11 83.84 SigLu activation. As shown in Tab. 2, the GQ poses significant challenge for semantic extraction. Post distillation almost fails to converge. Pre distillation stabilizes convergence, while it fails to ensure semantics in the discrete latents. Our proposed SigLu activation constrains the feature space, enabling effective post distillation. Table 2 SigLu activation ablation. SigLu activation makes post-distillation effective. Table 3 Pre-Post Distillation ablation. Pre-Post yields the best performance. Method Top-1 Top-5 Top-10 Pre Distill 79.55 0.53 Post Distill SigLu + Post 41.51 69.51 55.26 0.10 85.52 0.95 77.73 Distillation Loss Pre Post Zero-shot Acc (%) Top-1 Top-5 Top-10 1.01 0.52 0.10 41.51 77.73 69.51 51.32 76.70 83.26 Pre-Post Distillation. As shown in Tab. 3,we demonstrate the effectiveness of our pre-post distillation strategy. The combination of pre and post distillation yields the best performance, reaching 51.32% zero-shot Top-1 accuracy. Bottleneck channel. As shown in Tab. 4, we ablate channel widths of the bottlenecks. Doubling the channel significantly lowers rFID (from 1.58 to 1.12) and improves semantic accuracy by over 7%, proving that wider bottleneck is essential for visual compression and semantic extraction. Table 4 Bottleneck channel ablation. Extending channel width of bottleneck significantly improves reconstruction fidelity and semantic extraction capability. Setting rFID PSNR Reconstruction SSIM LPIPS(A) LPIPS(V) Top-1 Top-5 Top-10 Zero-shot Acc (%) Single Channel Double Channel 1.58 1.12 21.89 22.32 0.63 0.65 0.13 0.12 0.23 0.21 39.45 46. 68.00 73.66 76.77 81.11 Generative-Aware Prior. Tab. 5 shows that while reconstruction metrics remain stable, generative quality (gFID) improves notably from 2.66 to 2.38 with GAP and Query token, confirming that regularizing the latent space benefits downstream generation task. Table 5 Generative-Aware Prior ablation. Method GAP Query Gen. gFID rFID PSNR Reconstruction SSIM LPIPS(A) LPIPS(V) Top-1 Top-5 Top-10 Zero-shot Acc (%) 2.66 3.89 2. 1.12 1.16 1.18 22.32 22.11 22.27 0.65 0.63 0.63 0.12 0.13 0.12 0.21 0.22 0.22 46.89 48.47 48. 73.66 74.80 75.62 81.11 82.23 83.84 8 Training configurations on DataComp-1B. Tab. 6 shows that attention head is better than linear head. Moreover, scaling the batch size tripling the zero-shot accuracy. Table 6 Training configuration ablations on DataComp-1B. Attention-based semantic head and large batch size are critical for model convergence on general-domain dataset. Setting Zero-shot Acc (%) rFID PSNR SSIM LPIPS(A) LPIPS(V) Top-1 Top-5 Top-10 Reconstruction Semantic Head Architecture Linear Head Attention Head 2.91 3.10 21.96 22.02 Training Batch Size Batch Size 128 Batch Size 3.10 1.75 22.02 22.56 0.63 0.63 0.63 0.65 0.13 0.13 0.13 0. 0.24 0.24 0.24 0.22 3.89 4.09 13.85 14.39 21.93 23.31 4.09 11. 14.39 31.38 23.31 43.34 Hybrid architecture. On Datacomp-1B, Tab. 7 validates our hybrid backbone. CNNs excel at texture (rFID 1.75) but lack semantics, while Transformers capture semantics (Top-1 26.09%) but struggle with detail. Our hybrid design achieves the best of both worlds (rFID 1.35, Top-1 35.41%). Table 7 Architecture ablation. The hybrid backbone successfully synergizes the local inductive bis of convolution with the global understanding capability of attention. Backbone rFID PSNR SSIM LPIPS(A) LPIPS(V) Reconstruction Zero-shot Acc (%) Top-1 Top-5 TopCNN Only Transformer Only Hybrid (Ours) 1.75 3.38 1.35 22.56 22.02 22.54 0.65 0.65 0.66 0.12 0.14 0.13 0.22 0.26 0. 11.69 26.09 35.41 31.38 52.77 63.91 43.34 64.28 73.50 Three-stage training pipeline. As shown in Fig. 4, compared to stage 1, the model trained in stage 2 supports variable-resolution image reconstruction. Furthermore, the model trained in stage 3 demonstrates significantly improved performance in processing faces and text. Implementation details could be found in App. B."
        },
        {
            "title": "4.2 Comparison with State-of-the-Art",
            "content": "Visual Generation on ImageNet. We first evaluate the reconstruction performance of UniWeTok on the ImageNet 50K validation dataset. As shown in Tab. 8, UniWeTok compresses 256 256 resolution image into only 64 tokens, which represents 75% reduction in token count compared to other mainstream tokenizers. Following the BitDance setting, we employ UniWeTok as the visual tokenizer for autoregressive modeling. As shown in Tab. 9, UniWeTok-H achieves state-of-the-art FID of 1.38. Remarkably, our model requires training on only 33B tokens and generates just 64 tokens during inference, significantly outperforming various models that demand extensive training scales and incur higher computational costs during inference. Unified MLLM. We first compared the reconstruction performance of tokenizers trained on general-domain dataset. As shown in Tab. 10, our UniWeTok outperforms most state-of-the-art general tokenizers in reconstruction performance while utilizing only 25% of the visual token count. This reduction in sequence length allows for larger batch sizes in the subsequent training of unified MLLMs, significantly enhancing training efficiency. Given UniWeToks capability to support image compression and reconstruction at variable resolutions, we employ native resolutions for Unified MLLM training. This ensures that the inherent spatial information of the data is preserved without the distortions introduced by resizing or cropping. Following Emu3, after unified pretraining, we finetune the Unified MLLM into UniWeTok-Gen, UniWeTok-Edit and UniWeTok-Chat for better downstream performance. As shown in Tab. 11, UniWeTok-Gen demonstrates superior performance on image generation benchmarks, outperforming various diffusion-based models. As shown in Tab. 12, UniWeTok-Chat presents competitive understanding performance across broad range 9 Table 8 Reconstruction evaluation on 256 256 ImageNet 50K validation set. All models are trained on ImageNet. UniWeTok achieves SOTA results with 32 downsampling ratio. Method Tokens Ratio Codebook Size rFID PSNR Codebook Usage VQGAN [19] SD-VQGAN [74] MaskGIT [10] ReVQ [117] LlamaGen [81] ReVQ [117] TiTok [112] FlexTok [4] VAR [87] IBQ [78] Open-MAGVIT2 [58] IBQ [78] FlowMo-Lo [76] VFMTok [123] GigaTok [106] AliTok [98] UniWeTok (Ours) 16 16 16 16 16 16 16 16 16 16 16 16 256 256 16 16 16 16 16 16 16 16 256 256 256 273 8 8 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 15.5 32 16384 16384 1024 65536 16384 218 4096 64000 4096 16384 218 218 218 16384 16384 4096 2128 4.99 5.15 2.28 2.57 2.19 2.05 1.66 1.45 1.37 1.17 1.00 0.95 0.89 0.79 0.84 0.79 20.00 21.69 20.79 21.96 20.01 18.53 21.30 22.35 22.64 20.30 22.07 21.65 23.26 97% 100% 97% 96% 100% 84% 100% 100% Table 9 Comparison of class-conditional image generation on ImageNet 256256. UniWeTok achieves superior performance while employing standard causal autoregressive modeling. Method Continuous Tokens DiT-XL/2 [67] SiT-XL/2 [62] MDTv2 [26] REPA [115] MAR-B [45] MAR-L [45] MAR-H [45] NiT-XL [96] Discrete Tokens LlamaGen-L [82] LlamaGen-XL [82] LlamaGen-XXL [82] RandAR-L [66] RandAR-XL [66] RandAR-XXL [66] RAR-L [114] RAR-XL [114] RAR-XXL [114] OpenMAGVIT2-XL [59] MAGVIT-v2 [109] VAR-d20 [88] VAR-d30 [88] WeTok-AR-XL [127] UniWeTok-B UniWeTok-L UniWeTok-H Train Tokens Infer Tokens #Params FID IS Pre. Rec. 675M 675M 675M 675M 208M 479M 943M 675M 343M 775M 1.4B 343M 775M 1.4B 461M 955M 1.5B 1.5B 307M 600M 2B 1.5B 242M 527M 1.0B 2.27 2.06 1.58 1.42 2.31 1.78 1.55 2.03 3.07 2.62 2.34 2.55 2.22 2.15 1.70 1.50 1.48 2.33 1.78 2.57 1.92 2.31 2.35 1.68 1.38 278.2 277.5 314.7 305.7 281.7 296.0 303.7 265.26 256.1 244.1 253.9 288.8 314.2 322.0 299.5 306.9 326.0 271.8 319.4 302.6 323.1 276.6 284.47 288.56 284. 0.83 0.83 0.79 0.80 0.82 0.81 0.81 0.80 0.83 0.80 0.80 0.81 0.80 0.79 0.81 0.80 0.80 0.84 - 0.83 0.82 0.84 0.80 0.80 0.80 0.57 0.59 0.65 0.65 0.57 0.60 0.62 0.62 0.52 0.57 0.59 0.58 0.60 0.62 0.60 0.62 0.63 0.54 - 0.56 0.59 0.55 0.59 0.62 0.63 459B 459B 131B 262B 262B 262B 262B 197B 98M 98M 98M 98M 98M 98M 131M 131M 131M 115M 88M 82M 115M 328M 33B 33B 33B 256 256 256 256 256 256 256 64 256 256 256 256 256 256 256 256 256 256 256 256 256 256 64 64 64 10 Table 10 Zero-shot reconstruction comparison on ImageNet and MS-COCO val2017 validation set. Our UniWeTok achieves the best performance on different resolution settings. Method Ratio MS-COCO 2017 Imagenet-1k rFID PSNR SSIM rFID PSNR SSIM Cosmos [2] Show-o [104] WeTok [127] Open-MAGVIT2-I-PT [58] LlamaGen [81] WeTok [127] BSQ [121] QLIP-B [122] QLIP-L [122] TokenFlow [122] UniWeTok (Ours) WeTok [127] Cosmos [2] Open-MAGVIT2-I-PT [58] SD-VAE 1.x [73] WeTok [127] UniWeTok (Ours) 16 16 32 16 16 16 16 16 14 16 32 32 16 16 8 16 32 Resize 256 256 11.97 9.26 8.94 7.93 8.40 6.55 - - - - 6. 19.22 20.90 20.31 22.21 20.28 21.99 - - - - 22.58 Original Resolution 8.94 7.23 6.65 5.94 5.30 6.46 20.31 20.45 21.61 21.68 21.94 22.29 0.48 0.59 0.55 0.62 0.55 0.63 - - - - 0.66 0.55 0.53 0.57 0.64 0.59 0. 4.57 3.50 3.49 2.55 2.47 1.58 3.81 3.21 1.46 1.37 1.18 3.49 2.52 1.39 1.35 0.81 1.25 19.93 21.34 20.77 22.21 20.65 22.38 24.12 23.16 25.36 21.41 22.97 20.77 20.49 21.74 21.99 21.99 22.66 0.49 0.59 0.55 0.62 0.54 0.62 0.66 0.63 0.69 0.69 0. 0.55 0.52 0.56 0.63 0.58 0.65 of benchmarks. In Tab. 13, UniWeTok-Edit surpasses the diffusion model on the image editing task as an autoregressive model at similar parameter scale for the first time. The visualization results can be seen in Fig. 1,7,8,9. Table 11 Comparisons of visual generation quality on GenEval and DPG-Bench. Method # Params GenEval [29] DPG-Bench [33] Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Global Entity Attribute Relation Other Overall SDv1.5 [73] PixArt-α [12] SDv2.1 [73] SDXL [68] Playground v2.5 [43] Hunyuan DiT [48] PixArt-Σ [11] DALLE3 [8] SD3-Medium [22] SANA-1.5 [103] Chameleon [85] LlamaGen [45] EMU3-Gen [94] TokenFlow [69] Janus [97] SimpleAR [92] Transfusion [124] NextStep-1 [86] Harmon [99] Infinity [31] Janus-Pro [13] UniWeTok-Gen (Ours) 0.9B 0.6B 0.9B 2.6B 2.6B 1.5B 0.6B - 2B 4.8B 7B 0.8B 8B 13B 1.3B 1.5B 7B 14B 1.5B 8B 7B 8B 0.97 0.98 0.98 0.98 - - - 0.96 0.99 0.99 - 0.71 0.98 0.97 0.97 - - - 0.99 0.99 0.99 0.38 0.50 0.51 0.74 - - - 0.87 0.94 0.93 - 0.34 0.71 0.66 0.68 0.90 - - 0.86 0.89 0. Diffusion-based Model 0.35 0.44 0.44 0.39 - - - 0.47 0.72 0.86 0.76 0.80 0.85 0.85 - - - 0.83 0.89 0.84 0.04 0.08 0.07 0.15 - - - 0.43 0.33 0.59 0.06 0.07 0.17 0.23 - - - 0.45 0.60 0.65 Autoregressive-based Model - 0.04 0.21 0.26 0.42 0.45 - - 0.48 0.66 0.81 - 0.21 0.34 0.40 0.30 - - - 0.66 0.59 0.43 - 0.58 0.81 0.84 0.84 - - - 0.85 0.90 0. - 0.07 0.17 0.17 0.46 0.28 - - 0.74 0.79 0.78 11 0.43 0.48 0.50 0.55 - - - 0.67 0.74 0.81 0.39 0.32 0.54 0.55 0.61 0.63 0.63 0.73 0.76 0.79 0.80 0.81 74.63 74.23 74.97 79. - - 83.27 82.43 83.06 82.59 84.59 80.59 86.89 82.89 90.97 89.61 87.90 91.01 - - - - 81.76 75.43 85.21 86.68 78.72 79.22 82.33 87.38 87.97 - - - - - - - 75.39 78.60 - 80.91 81.20 88.01 88.94 88.39 88.83 - - 76.17 86.84 81.29 87.70 - - - - 86.90 88.90 87.54 91. 89.40 90.72 - - 73.49 67.81 63.18 82.57 76.96 71.11 - 86.76 80.41 74.65 84.08 83.50 75.47 74.36 86.41 78.87 86.59 87.68 80.54 90.58 89.83 83.50 80.70 88.68 84.08 84.70 - - - - - 84.76 58.40 64.84 90.22 83.15 80.60 85.22 71.20 73.38 85.46 86.41 79.68 81.97 - 86.33 - - - 85.28 - - - - - 86.6 89.32 89.48 84.19 92.92 91.11 86.63 Table 12 Comparison on multimodal understanding benchmarks. Method Pretrained-LLM SEEDB POPE VQAv2 GQA SQA TQA CQA AI2D RWQA MMMU MME-P MME-S Understanding Only Model InstructBLIP [15] IDEFICS-9B [40] LLaVA-1.5 [53] InternVL-Chat [14] mPLUG-Owl2 [107] LLaVA-1.6(HD) [54] VILA [49] Unified Model Vicuna-7B LLaMA-7B Vicuna-7B Vicuna-7B LLaMA2-7B Vicuna-7B LLaMA2-7B Persimmon-8B Fuyu-8B(HD) [7] Chameleon-MT-34B [85] LWM-7B [51] Phi-1.5-1.3B Show-o [104] LLaMA2-7B VILA-U [100] Qwen2.5-1.5B Harmon [100] Vicuna-7B EVE-7B(HD) [17] Emu3-Chat [94] Vicuna-13B TokenFlow-L [60] LLaMA-2-7B UniTok [60] Qwen3-8B UniWeTok-Chat 53.4 64.3 57.8 64.7 61. 56.3 67.1 56.8 68.2 62.6 69.3 85.9 86.4 86.2 86.5 85.5 74.1 75.2 73.8 83.9 87.6 85.0 85.2 85.0 83.2 85.6 50.9 78.5 79.3 79.4 81.8 80.8 74.2 69.6 55.8 59.3 75.3 78.6 75.1 73.9 61.1 75.8 49.2 60.5 50.1 12.5 33.8 38.4 42.2 25.9 62.0 66.8 46.1 18.2 54.8 62.9 57.0 56.1 68.7 58.2 22.8 55.7 64.2 70.2 64.9 54.8 66.6 63.3 73.7 66.6 48.3 64.5 44.8 47.7 48.7 58.3 58.9 62.6 64.9 56.8 60.3 89.2 64.7 68.6 70.0 56.6 54.1 60.3 51.6 63.1 80.3 53.7 65.1 73.9 37.4 42.1 54.8 50.3 57.8 57.4 49.2 54.8 30.6 18.4 35.3 32.7 35.1 27.9 25.1 38.9 31.6 34.4 40.0 1212.8 1510.7 1298.5 1533.0 1097.2 1336.2 1155.0 1305.7 1365.4 1448.0 1415.7 1778.0 1476.0 1622.9 1796.6 Table 13 Comparison on GEdit-Bench. G_SC, G_PQ, and G_O refer to the metrics evaluated by GPT-4.1. Model Private Gemini 2.0 [28] GPT-4o [65] Open-Source Diffusion Instruct-Pix2Pix [9] MagicBrush [118] AnyEdit [111] OmniGen [102] Step1X-Edit [55] BAGEL [16] GEdit-EN (Full set) GEdit-CN (Full set) G_SC G_PQ G_O G_SC G_PQ G_O 6.73 7.85 3.58 4.68 3.18 5.96 7.09 7.36 6.61 7.62 5.49 5.66 5.82 5.89 6.76 6.83 6.32 7.53 3.68 4.52 3.21 5.06 6.70 6. 5.43 7.67 - - - - 7.20 7.34 6.78 7.56 - - - - 6.87 6.85 5.36 7.30 - - - - 6.86 6. Open-Source Autoregressive 5.86 UniWeTok-Edit 5.89 5.09 5.78 5.92 5."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented UniWeTok, unified discrete tokenizer designed to resolve the conflict between high-fidelity reconstruction, multimodal understanding and generation. By integrating convolution-attention hybrid backbone with the SigLu activation function, UniWeTok successfully incorporates Pre-Post Distillation and Generative-Aware Prior, allowing unified discrete tokenizer with massive codebook size 2128 to extract semantic concepts, fine-grained texture details, and generative prior effectively. Our UniWeTok establishes robust and efficient baseline for future Unified MLLM works, suggesting that single, well-optimized tokenizer is sufficient to address the complex challenges inherent in Unified MLLMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Nvidia Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Samuel Huffman, Pooya Jannaty, Jingyi Jin, Seung Wook Kim, Gergely Klar, Grace Lam, Shiyi Lan, Laura Leal-Taixé, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum A. Reda, Xiao-Shuai Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne P. Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Rajan Varghese, Hao Wang, Haoxiang Wang, Hengyi Wang, Tingwei Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yuan Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos world foundation model platform for physical ai. ArXiv, abs/2501.03575, 2025. [3] Yuang Ai, Jiaming Han, Shaobin Zhuang, Xuefeng Hu, Ziyan Yang, Zhenheng Yang, Huaibo Huang, Xiangyu Yue, and Hao Chen. Bitdance: Scaling autoregressive generative models with binary tokens, 2026. URL https://github.com/shallowdream204/BitDance. [4] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oğuzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. In Forty-second International Conference on Machine Learning, 2025. [5] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [6] Zechen Bai, Jianxiong Gao, Ziteng Gao, Pichao Wang, Zheng Zhang, Tong He, and Mike Zheng Shou. Factorized visual tokenization and generation. ArXiv, abs/2411.16681, 2024. URL https://api.semanticscholar.org/ CorpusID:274281440. [7] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sağnak. Taşırlar. Introducing our multimodal models., 2023. URL https://www.adept.ai/blog/fuyu-8b. [8] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn.openai.com/papers/dall-e-3.pdf, 2(3):8, 2023. [9] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, 2023. [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11305 11315, 2022. [11] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [12] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In International Conference on Learning Representations, 2024. [13] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. [16] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [17] Haiwen Diao, Yufeng Cui, Xiaotong Li, Yueze Wang, Huchuan Lu, and Xinlong Wang. Unveiling encoder-free vision-language models. Advances in Neural Information Processing Systems, 37:5254552567, 2024. [18] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023. [19] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1286812878, 2020. [20] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [21] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, 2021. [22] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. [23] Xianghong Fang, Litao Guo, Hengchao Chen, Yuxuan Zhang, Dingjie Song, Yexin Liu, Hao Wang, Harry Yang, Yuan Yuan, Qiang Sun, et al. Enhancing vector quantization with distributional matching: theoretical and empirical study. arXiv preprint arXiv:2506.15078, 2025. [24] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji, Caifeng Shan, and Ran He. Mme: comprehensive evaluation benchmark for multimodal large language models, 2025. URL https://arxiv.org/abs/2306.13394. [25] Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. Advances in Neural Information Processing Systems, 36:2709227112, 2023. [26] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023. [27] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. [28] Google Gemini2. Experiment with gemini 2.0 flash native image generation, 2025. URL https://developers. googleblog.com/en/experiment-with-gemini-20-flash-native-image-generation/. [29] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [30] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [31] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1573315744, 2025. [32] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Neural Information Processing Systems, 2017. [33] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [34] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [35] Aren Jansen, Daniel P. W. Ellis, Shawn Hershey, R. Channing Moore, Manoj Plakal, Ashok Popat, and Rif A. Saurous. Coincidence, categorization, and consolidation: Learning to recognize sounds with minimal supervision. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 121125, 2019. [36] Mingkai Jia, Wei Yin, Xiaotao Hu, Jiaxin Guo, Xiaoyang Guo, Qian Zhang, Xiao-Xiao Long, and Ping Tan. Mgvq: Could vq-vae beat vae? generalizable tokenizer with multi-group quantization. arXiv preprint arXiv:2507.07997, 2025. [37] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251. Springer, 2016. [38] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. CoRR, abs/1412.6980, 2014. [39] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. [40] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36:7168371702, 2023. [41] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1152311532, 2022. [42] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [43] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [44] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. PMLR, 2023. [45] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. [46] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. ArXiv, abs/2410.01756, 2024. URL https://api.semanticscholar.org/ CorpusID:273025912. [47] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [48] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [49] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2668926699, 2024. [50] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, 2014. URL https://api.semanticscholar.org/CorpusID:14113767. [51] Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. World model on million-length video and language with blockwise ringattention. arXiv preprint arXiv:2402.08268, 2024. 15 [52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [53] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. [54] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae. Lee. LlavaImproved reasoning, ocr, and world knowledge., 2024. URL https://llava-vl.github.io/blog/ next: 2024-01-30-llava-next/. [55] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, and Daxin Jiang. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [56] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. [57] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. [58] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. ArXiv, abs/2409.04410, 2024. [59] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [60] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. ArXiv, abs/2502.20321, 2025. [61] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [62] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In ECCV, 2024. [63] Ben Mann, Nick Ryder, Melanie Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 1(3):3, 2020. [64] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Findings of the association for computational linguistics: ACL 2022, pages 22632279, 2022. [65] OpenAI. Introducing 4o image generation, 2025. URL https://openai.com/index/ introducing-4o-image-generation/. [66] Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In CVPR, 2025. [67] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [68] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [69] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. 2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 25452555, 2024. URL https://api.semanticscholar.org/CorpusID:274465079. [70] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018. 16 [71] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [72] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [73] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [74] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [75] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115:211 252, 2014. [76] Kyle Sargent, Kyle Hsu, Justin Johnson, Li Fei-Fei, and Jiajun Wu. Flow to the mode: Mode-seeking diffusion autoencoders for state-of-the-art image tokenization. arXiv preprint arXiv:2503.11056, 2025. [77] Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization. arXiv preprint arXiv:2412.02692, 2024. [78] Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Scalable image tokenization with index backpropagation quantization, 2025. [79] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [80] Wei Song, Yuran Wang, Zijia Song, Yadong Li, Haoze Sun, Weipeng Chen, Zenan Zhou, Jianhua Xu, Jiaqi Wang, and Kaicheng yu. Dualtoken: Towards unifying visual understanding and generation with dual visual vocabularies. ArXiv, abs/2503.14324, 2025. URL https://api.semanticscholar.org/CorpusID:277104378. [81] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. ArXiv, abs/2406.06525, 2024. [82] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [83] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [84] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [85] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [86] NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, et al. Nextstep-1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. [87] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. ArXiv, abs/2404.02905, 2024. [88] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. 2024. [89] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 17 [90] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [91] Chunwei Wang, Guansong Lu, Junwei Yang, Runhui Huang, Jianhua Han, Lu Hou, Wei Zhang, and Hang Xu. Illume: Illuminating your llms to see, draw, and self-enhance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2161221622, 2025. [92] Junke Wang, Zhi Tian, Xun Wang, Xinyu Zhang, Weilin Huang, Zuxuan Wu, and Yu-Gang Jiang. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455, 2025. [93] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [94] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [95] Zhou Wang, Alan Conrad Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13:600612, 2004. [96] Zidong Wang, Lei Bai, Xiangyu Yue, Wanli Ouyang, and Yiyuan Zhang. Native-resolution image synthesis. arXiv preprint arXiv:2506.03131, 2025. [97] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025. [98] Pingyu Wu, Kai Zhu, Yu Liu, Longxiang Tang, Jian Yang, Yansong Peng, Wei Zhai, Yang Cao, and Zheng-Jun Zha. Alitok: Towards sequence modeling alignment between tokenizer and autoregressive model. arXiv preprint arXiv:2506.05289, 2025. [99] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Zhonghua Wu, Qingyi Tao, Wentao Liu, Wei Li, and Chen Change Loy. Harmonizing visual representations for unified multimodal understanding and generation. arXiv preprint arXiv:2503.21979, 2025. [100] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [101] XAI. Realworldqa, 2024. [102] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [103] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv preprint arXiv:2501.18427, 2025. [104] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. ArXiv, abs/2408.12528, 2024. [105] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [106] Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to billion parameters for autoregressive image generation, 2025. [107] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 1304013051, 2024. 18 [108] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [109] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In ICLR, 2024. [110] Lijun Yu, José Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion tokenizer is key to visual generation, 2024. [111] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. [112] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. ArXiv, abs/2406.07550, 2024. URL https://api.semanticscholar. org/CorpusID:270379986. [113] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37: 128940128966, 2024. [114] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. In ICCV, pages 1843118441, 2025. [115] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In ICLR, 2025. [116] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [117] Borui Zhang, Qihang Rao, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Quantize-then-rectify: Efficient vq-vae training. arXiv preprint arXiv:2507.10547, 2025. [118] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. In NeurIPS, 2023. [119] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 586595, 2018. [120] Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. ArXiv, abs/2406.07548, 2024. [121] Yue Zhao, Yuanjun Xiong, and Philipp Krähenbühl."
        },
        {
            "title": "Image and video tokenization with binary spherical",
            "content": "quantization. arXiv preprint arXiv:2406.07548, 2024. [122] Yue Zhao, Fuzhao Xue, Scott Reed, Linxi Fan, Yuke Zhu, Jan Kautz, Zhiding Yu, Philipp Krahenbuhl, and De-An Huang. Qlip: Text-aligned visual tokenization unifies auto-regressive multimodal understanding and generation. ArXiv, abs/2502.05178, 2025. [123] Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, and Xiaojuan Qi. Vision foundation models as effective visual tokenizers for autoregressive image generation. arXiv preprint arXiv:2507.08441, 2025. [124] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [125] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vq-gan to 100,000 with utilization rate of 99%. Advances in Neural Information Processing Systems, 37:1261212635, 2024. 19 [126] Shaobin Zhuang, Yiwei Guo, Canmiao Fu, Zhipeng Huang, Zeyue Tian, Fangyikang Wang, Ying Zhang, Chen Li, and Yali Wang. Wetok: Powerful discrete tokenization for high-fidelity visual reconstruction. arXiv preprint arXiv:2508.05599, 2025. [127] Shaobin Zhuang, Yiwei Guo, Canmiao Fu, Zhipeng Huang, Zeyue Tian, Fangyikang Wang, Ying Zhang, Chen Li, and Yali Wang. Wetok: Powerful discrete tokenization for high-fidelity visual reconstruction. arXiv preprint arXiv:2508.05599, 2025."
        },
        {
            "title": "A Model Architecture",
            "content": "Figure 5 Detail Illustration of the UniWeTok model architecture."
        },
        {
            "title": "B More Ablation Implementation Details",
            "content": "Training loss. As shown in Tab. 14, 15, 16. SigLu activation. As shown in Tab. 17, 18, 19. Pre-Post Distillation. As shown in Tab. 20, 21, 22. Bottleneck channel. As shown in Tab. 23, 24. Generative-Aware Prior. As shown in Tab. 25, 26, 27. Training configurations on DataComp-1B. As shown in Tab. 28, 29, 30. Hybrid architecture. As shown in Tab. 31, 32, 33. BitDance-T. As detailed in Sec. 3 and 4, BitDance-T is an ultra-lightweight model with merely 8.6M parameters. Consequently, the additional computational overhead incurred during the training phase is negligible. Inference. During the inference phase, the semantic teacher and generative prior model are not required. Consequently, UniWeTok relies solely on single encoder and decoder."
        },
        {
            "title": "C More Visualization Results",
            "content": "Multimodal Understanding. More visualization results of multimodal understanding by UniWeTok-Chat could be found in Fig. ??. Image Editing. More visualization results of image editing by UniWeTok-Edit could be found in Fig. 7,8,9. 21 Table 14 w/o. PPD w/o. GAP setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize w/o. PPD w/o. GAP IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True False False False linear False Table 15 w/. PPD w/o. GAP setting. Table 16 w/. PPD w/. GAP setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize w/. PPD w/o. GAP IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize w/. PPD w/. GAP IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True linear BitDance-T True Table 17 Pre distillation setting. Table 18 Post distillation setting. Table 19 Post distillation with SigLu activation setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize pre distillation IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 False True False linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize post distillation IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 False False True linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize post distillation with SigLu IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True False True linear False 128 22 Table 20 Pre distillation setting. Table 21 Post distillation setting. Table 22 Pre-Post distillation setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize pre distillation IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True False linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize post distillation IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True False True linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize pre-post distillation IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True linear False 128 Table 23 Single bottleneck channel setting. Table 24 Double bottleneck channel setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize single bottleneck channel IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 False 16 True ViT-SO400M-16-SigLIP2-384 True True True linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize double bottleneck channel IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True linear False 128 23 Table 25 w/o. GAP w/o. Query setting. Table 26 w/. GAP w/o. Query setting. Table 27 w/. GAP w/. Query setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize w/o. GAP w/o. Query IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True linear False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize w/. GAP w/o. Query IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True linear BitDance-T False 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation prior model query token global batchsize w/. GAP w/. Query IN-1K training set [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True BitDance-T True 128 Table 28 Linear head with batchsize 128 setting. Table 29 Attention head with batchsize 128 setting. Table 30 Attention head with batchsize 1024 setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize linear head with bs128 DataComp-1B [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 0 True ViT-SO400M-16-SigLIP2-384 True True True linear BitDance-T True 128 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize attention head with bs1024 DataComp-1B [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 0 True ViT-SO400M-16-SigLIP2-384 True True True attention BitDance-T True config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize attention head with bs1024 DataComp-1B [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 0 True ViT-SO400M-16-SigLIP2-384 True True True attention BitDance-T True 1024 24 Table 31 CNN architecture setting. Table 32 Transformer architecture setting. Table 33 Hybrid architecture setting. config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize CNN architecture DataComp-1B [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 0 True ViT-SO400M-16-SigLIP2-384 True True True attention BitDance-T True 1024 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize transformer architecture DataComp-1B [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 0 True 16 True ViT-SO400M-16-SigLIP2-384 True True True attention BitDance-T True 1024 config training data image size data augmentation downsample ema (group number) (group channel) optimizer optimizer momentum weight decay learning rate schedule learning rate warmup steps cos decay end ratio total steps channel_mult channel num_res_blocks bottleneck channel double num_attn_blocks generative decoder semantic teacher SigLu activation pre distillation post distillation distill head prior model query token global batchsize hybrid architecture DataComp-1B [256, 256] random crop 32 32 True 16 8 Adam β1, β2=0.5, 0.9 0 consistent 1e-4 0 1 250250 [1,1,2,2,4,8] 128 2 True 16 True ViT-SO400M-16-SigLIP2-384 True True True attention BitDance-T True 25 Figure 6 Visualization results of multimodal understanding by UniWeTok-Chat. 26 Figure 7 Visualization results of image editing by UniWeTok-Edit part 1. 27 Figure 8 Visualization results of image editing by UniWeTok-Edit part 2. 28 Figure 9 Visualization results of image editing by UniWeTok-Edit part 3."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Institute of Automation, Chinese Academy of Sciences",
        "MMLab, The Chinese University of Hong Kong",
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences",
        "Zhejiang University"
    ]
}