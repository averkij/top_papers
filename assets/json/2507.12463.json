{
    "paper_title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior Understanding",
    "authors": [
        "Renjie Li",
        "Ruijie Ye",
        "Mingyang Wu",
        "Hao Frank Yang",
        "Zhiwen Fan",
        "Hezhen Hu",
        "Zhengzhong Tu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behavior$\\unicode{x2014}$such as motion, trajectories, and intention$\\unicode{x2014}$a comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose $\\textbf{MMHU}$, a large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. A human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide a thorough dataset analysis and benchmark multiple tasks$\\unicode{x2014}$ranging from motion prediction to motion generation and human behavior question answering$\\unicode{x2014}$thereby offering a broad evaluation suite. Project page : https://MMHU-Benchmark.github.io."
        },
        {
            "title": "Start",
            "content": "MMHU: Massive-Scale Multimodal Benchmark for Human Behavior Understanding Renjie Li1, Ruijie Ye2, Mingyang Wu1, Hao Frank Yang3, Zhiwen Fan4, Hezhen Hu4, Zhengzhong Tu1 1Texas A&M University 2Brown University 3Johns Hopkins University 4UT Austin 5 2 0 2 6 1 ] . [ 1 3 6 4 2 1 . 7 0 5 2 : r Project Page: https://MMHU-Benchmark.github.io Figure 1: We propose MMHU, large-scale dataset for human behavior understanding. We collected 57k human instances with diverse behaviors such as playing mobile phone, holding object, or using mobility devices, from diverse scenes such as in the city, school, park, and alley. We provide rich annotations including motion and trajectory, text descriptions for human motions, and recognize the behaviors that are critical to driving safety."
        },
        {
            "title": "Abstract",
            "content": "Humans are integral components of the transportation ecosystem, and understanding their behaviors is crucial to facilitating the development of safe driving systems. Although recent progress has explored various aspects of human behaviorsuch as motion, trajectories, and intentiona comprehensive benchmark for evaluating human behavior understanding in autonomous driving remains unavailable. In this work, we propose MMHU, large-scale benchmark for human behavior analysis featuring rich annotations, such as human motion and trajectories, text description Equal contribution Corresponding authors. Preprint. Under review. for human motions, human intention, and critical behavior labels relevant to driving safety. Our dataset encompasses 57k human motion clips and 1.73M frames gathered from diverse sources, including established driving datasets such as Waymo, in-the-wild videos from YouTube, and self-collected data. human-in-the-loop annotation pipeline is developed to generate rich behavior captions. We provide thorough dataset analysis and benchmark multiple tasksranging from motion prediction to motion generation and human behavior question answeringthereby offering broad evaluation suite. Our dataset will be released to promote further human-centric research in this vital area of autonomous driving."
        },
        {
            "title": "Introduction",
            "content": "Humans play an essential role in transportation systems, making the comprehensive understanding of human behaviorssuch as motion [1], intention [2, 3, 4], and trajectory [5, 6]critical for developing safe autonomous driving systems. To effectively interact with humans, autonomous vehicles must answer human-centric questions, such as What is the person doing?, Is the person going to cross the street?, and Where does the person intend to go? Failing to accurately comprehend these behaviors could lead to misinterpretations of human intent, potentially resulting in fatal accidents. While significant efforts have been devoted to understanding individual aspects of human behaviors by investigating human motion, intention, and trajectory, the absence of unified dataset limits the comprehensive evaluation of algorithms for human behavior understanding, especially in autonomous driving scenarios that mainly account for human safety. Existing driving datasets are typically designed for general driving tasks, such as depth estimation, 2D or 3D object detection, odometry, and semantic segmentation [7, 8, 9], or narrowly designed human-related tasks, such as intention prediction [10, 11, 12], motion and trajectory prediction [13], or motion reconstruction [14, 15, 16]. Moreover, with the emergence of driving-oriented vision-language models (VLMs) [17, 18, 19, 20, 21, 22, 23, 24, 25] , human behavior understanding tasks can now be approached in more integrated and flexible manner through images and text queries. However, existing training data for these VLMs are not specifically tailored to human behavior, limiting their effectiveness in capturing critical human-centric details essential for safe driving. In this work, we aim to answer three core questions regarding human behavior understanding tasks in autonomous driving scenarios: ❶ What aspects of human behavior are critical to autonomous driving? ❷ How effectively do current approaches model human behaviors in autonomous driving contexts? ❸ How can comprehensive benchmark advance the development of human behavior understanding algorithms? To this end, we propose MMHU, large-scale unified benchmark explicitly designed for comprehensively understanding various human behaviors in driving scenarios. MMHU includes rich annotations generated by human-in-the-loop annotation pipeline, enabling scalable and precise labeling from diverse data sources using only monocular video inputs. Specifically, we have collected 1.73M frames featuring 57K human instances from source videos obtained from Waymo [7], YouTube, and self-collected data. The dataset provides detailed annotations covering: (1) human motion and trajectory; (2) text descriptions of human motions generated using templates and VLMs; (3) critical human behaviors extracted via VLMs, along with question-answer (QA) pairs designed to benchmark driving-oriented and generalist VLMs. Our contributions can be summarized as follows: We introduce MMHU, unified, human-centric dataset that provides comprehensive understanding of humans behaviors in driving scenarios that can be used as benchmark for range of human-centric understanding tasks. We develop scalable, human-in-the-loop annotation pipeline employing multi-source fitting strategies to produce accurate labeling across diverse video sources, ranging from driving videos and general YouTube videos to self-collected streams. We evaluated baseline methods of human behavior understanding and analyze their performance, we further demonstrated that our dataset helps these methods achieve better performance."
        },
        {
            "title": "2 Related Works\n2.1 Human Motion",
            "content": "Human motion is essential to autonomous driving. We categorize human motion representations into 2D and 3D representations. 2D human motion [26, 27, 28, 29, 30] leverages keypoints or heatmaps to mark the local body motion on the image. For 3D representations, the SMPL series [31, 32, 33] provide compact and expressive representation via learned parameters. While most human motion datasets focus on general human motions [34, 13, 35, 36, 37], there are several datasets are specially designed for driving scenarios [14, 16, 15]. However, these datasets mainly focus on the human movements and their text description, the behaviors of humans remains unavailable. Based on the representation and datasets, several efforts have been put into human motion reconstructions from temporally aligned multi-view cameras [38], unaligned multi-view cameras [39], and monocular cameras [40, 41, 42, 43]. Other works have explored human motion generation from action labels [44] or text [45, 46]. However, due to the lack of high-quality data, there is little work that specifically generates human motion in driving scenarios. 2.2 Human Behavior Understanding Understanding human behavior in driving situations is essential for driving safety. Although there are some datasets and methods to understand human behavior and actions [5, 47, 48, 49, 50, 51, 52, 53, 54], they mainly focus on recognizing human actions in general or sports scenes. While sharing some common behavior that concerns driving safety, they mainly focus on general actions like shaking hands, dancing, or running. The behaviors specifically concerning driving safety remain unexplored. In autonomous driving, besides motion reconstruction, there are some approaches and datasets for understanding several aspect of human behaviors such as (1) human trajectory prediction [5, 55, 56, 57, 58, 59], where models are required to predict the future trajectory from previous ones, or (2) human intention prediction [60, 2, 61, 10, 62, 63], where pedestrians are simply classified into two states - crossing the street and not crossing the street. Besides the binary classification of crossing the street, there are several datasets [64, 65, 66, 12] that provide more detailed behavior labels such as stopping, glancing, or running. However, these works still focus on specified aspects of human behavior, such as the posture and action when pedestrian is crossing the street. Recently, the development of vision language models (VLMs) [17, 18, 19, 20, 21, 22, 23, 24, 25] enables question-answering based on images or videos, making human behavior understanding more flexible. There have been many specialists driving VLMs [67, 68, 69, 70, 71, 72, 73] and autonomous driving QA datasets [74, 75, 76, 73, 77, 78]. However, these models and the datasets are designed for general VQA tasks for autonomous driving. The comprehensive understanding of human behaviors remains unexplored. We show comparison of related datasets in Tab. 1. 2.3 Autonomous Driving Datasets Autonomous driving has been one of the most popular research topics in recent years. There are several datasets that are specially created for developing and evaluating autonomous driving algorithms [9, 7, 8, 79]. These datasets are typically collected from vehicle mounted with multiple sensors, such as multi-view cameras, LiDARs, RaDARs, IMU, etc., supporting autonomous driving tasks such as 2D and 3D object detection, semantic segmentation, depth estimation, and planning. Recently, several works have focused on some specific scenes in autonomous driving, such as the accident [80], snowy scenes [81], and foggy scenes [82]. Some datasets have provided the labeling of several aspects of human behaviors in driving scenarios, such as human motion and trajectory [15, 14, 16], intention of crossing the street [12, 11, 10], or in the forms of general VQA [75, 78, 77, 68]. However, these datasets only investigate some specific human behaviors, and the comprehensive understanding of human behavior remains unexplored."
        },
        {
            "title": "3 The MMHU Dataset\nOverview. As illustrated in Fig. 3, we propose MMHU, a comprehensive human-centric benchmark\nwith rich annotations, emphasizing the criticalness of human behavior understanding in autonomous\ndriving. We built our dataset using high-quality videos from various sources. Then we applied a\nscalable annotation pipeline that only involves minor human effort to get the rich annotations from the\ncollected videos. The annotation for each video clip includes the 3D motion with trajectory, intention,\nhigh- and low-level text description, and critical behavior labels concerning driving safety. We will\npresent our data collection details in Section 3.1. We then introduce the annotation pipelines and",
            "content": "3 Figure 2: Data Collection and Annotation. (Left) We collect data from three sources: the Waymo dataset, the YouTube videos, and the self-collected or paid driving videos. (Right) We demonstrate the annotation pipeline; we first filter and cut the raw videos based on the rough human detection results. Then we reconstruct the SMPL motion for each detected frame. The missing frames are further recovered by an interpolation procedure. For the labeling of text descriptions, we leverage low-level text as bridge between the SMPL parameters and the semantic label. Then we generate the high-level text from the low-level ones. We recognize the critical behavior lists leveraging VLM, based on the visual and text information. We label the behaviors for each human instance using VLM that is fine-tuned on small human-labeled subset. data analysis of human motion and trajectory (Sec. 3.2), text description (Sec. 3.3), and intention and critical behaviors (Sec. 3.4). We show the statistics of the dataset in Sec. 3.5. 3.1 Data Collection Videos Acquisition We collected raw videos of 1.73M frames in total from three kinds of data sources as follows: Autonomous Driving Data contains multi-modality sensor information and rich annotation for generic autonomous driving tasks such as object detection, depth estimation, etc. We collected 1.7 hours videos from Waymo [7], which consist of 73K frames. In-the-wild Data includes first-person driving videos that are publicly available on the Internet. We collected 10 hours of YouTube videos, each with CC license, consisting of 318.25K frames with resolution ranging from 1080p to 2k. Self-collected Data is driving recordings collected by ourselves or from paid sources. We collected 66.5 hours of videos, consisting of 2393.96 frames and the resolution varies from 1080p to 4k. Video Cutting and Filtering Directly applying the annotation pipeline to the entire video can be expensive. We first roughly detect the human presence and filter out the frames that lack human presence. Specifically, we apply human detector on the raw video at 1 FPS, then we separate the raw video into fragments separated by the no-human-presented frames. We then filter out the fragments that are less than 10 seconds. 3.2 Motion and Trajectory Motion Extraction. Human motion provides rich information about human actions and behaviors. We extract motion sequences from the raw videos collected as in Sec. 3.1 and recover human trajectories from the motion sequences. For human motion extraction, we reconstruct the human motions parameterized by the SMPL [31] representation. Formally, given person in frames, their motion is described as SMPL parameter sequence = {St Rnmt {1, ..., }}, Sk defines the motion at time t. We follow the detect-and-reconstruct schema for motion reconstruction from video. We first leverage an object detector to detect humans. Then we track each individual during the detected frames. We finally reconstruct their SMPL parameters following Wham [83]. We employ different procedures to track the individuals, considering different data sources. For sources that provide bounding boxes of humans, e.g., the autonomous driving datasets, we leverage the bounding boxes as prior. We compare the provided bounding boxes with the detected ones, select those that 4 Figure 3: Visualization of the MMHU dataset. For each human instance, the first line shows the video frames that is sampled from the video clips. The human instance is highlighted using red bounding box. We crop and zoom-in the human instance for clearer view. Under each of the frames shows the corresponding human motion rendered as mesh, followed by the text description for the human motion and the behavior labels. have at least 0.2 IoU overlap with one of the detected bounding boxes as reconstruction candidates. Then we delete the frames for each individual whose bounding box intersects others with IoU higher 5 Table 1: Comparison of Related Datasets. We compare our dataset with related dataset of general motion or driving scenes. From left to right, the columns are the dataset name, total frame count or time duration, human count, providing labels of motion, trajectory, VQA pairs, and text descriptions, and the number of behavior classes. represents general motion dataset. Dataset labeled with in the Instance\" column captures the motion from real participants. means upper-bound. Datasets labeled with Unstructured\" in the last column do not provide explicit human behavior labels but involve them in the QA pairs or captions. Our dataset supports all four tasks and provides the most behavior classes among all of the datasets. Dataset Frames / Duration (s) Instances Motion Trajectory Text Behaviors PIE [10] Euro-PVI [11] PMR [14] CityWalker [16] BlindWays [15] 3DPW [84] Human3.6M [34] JAAD [12] Drama [85] CoVLA [75] DriveLM-nuScenes [68] nuScenes-QA [78] MMHU (Ours) 293k / - 83k / - 225k / - - / 110k 300k / 10k 51k / 1.7k 3.6M / - 82k / 33k - / 36k 6M / - 4.8k / - 340k / - 1.73M / 173K 1.3k 7.7k 54 120k 11 7 11 2.2k - - - - 57K VQA 1 1 Unstructured 15 11 General Driving Unstructured General Driving Unstructured General Driving Unstructured General Driving Human-Centric 13 than 0.2, which indicates some occlusion. For the in-the-wild and self-collected sources, we unify the videos to 10 FPS and extracted the 2d keypoints of each individual and fused them with the detected bounding boxes as the tracking results. Figure 4: Statics of MMHU. The average duration of motion sequences is 3s. The most common behavior is crossing the street, while the rarest behavior is using wheelchair. Behavior definition, please refer to Sec. 3.4. Trajectory from Motion. The trajectory of the human is represented as the sequence of point sets {P1, .., Pp, ...PF }, each point set contains points representing the 3D location of human parts Pt = {pk R3k {1, ...n}}. The sequence preceding Pp, denoted as Sp, represents past motion, while the sequence following Pp is referred to as Sf , representing future motion. And Sf is initially padded with zeros. trained model Fpred predicts Sf given the input Sp. We extract the trajectories of each human from their global motion sequence. Motion Completion. The motion sequence of person can lack few frames in the middle due to occlusion. We present missing motion prediction procedure to complete the missing frames. Specifically, given reconstructed SMPL parameter sequence = {S1, S2, ..., Sn}, and missing frames k, + 1, ..., + m, the predicted SMPL parameter ˆSk, ... ˆSk+m is as follows: ˆSk+j = sin((1 sin(θ) )θ) ˆSk1 + sin( θ) sin(θ) ˆSk+m+1, cos(θ) = Sk1 Sk+m+ 3.3 Hierarchical Text Annotation In addition to the parameterized SMPL motion, the semantic understanding of human behaviors is also critical. We use text as semantic level description of the human motion. To narrow the gap between the semantic description and the SMPL parameter, we employ hierarchical text annotation approach. We first convert the SMPL parameters to an element-level text description for 6 each part of the body at each frame in rule-based schema. Then we utilize large language models to aggregate the low-level description of the person over time. Based on the detailed low-level motion description, combined with the video clip, we abstract the high-level descriptions for each motion sequence. The details of hierarchical text annotation are introduced in the supplementary materials. Low-level Text Annotation: As shown in 2, the low-level text describes the movement of each body part in detail. Following [86], we generate the low-level text description for the SMPL motion St at each frame by calculating the angle, distance, position relation, etc. of different body parts. We then aggregate the movement of each body part over time to get the low-level description of human motion. High-level Text Annotation: High-level captions provide semantic-level description of human action and motion. We generate one high-level description for the motion sequence of each person leveraging large vision language models. Specifically, we provide the VLM eight frames uniformly sampled over time, including the images and the low-level text descriptions. 3.4 Critical Behaviors Understanding human behaviors for example, crossing the street is critical for autonomous driving algorithms. We model the critical behaviors as binary attributes, indicating whether person is subject to the corresponding behavior. Critical Behavior Recognition Before we can assign the values to each attribute, we should first answer the question: what behaviors are critical to autonomous driving? One of them might be whether person is going to cross the street, which is known as the intention prediction task in autonomous driving. As illustrated in Fig. 2, we recognize the critical behaviors by leveraging VLMs. Specifically, we sample video clips from the dataset. For each video clip, we ask the VLM to recognize the critical behaviors in the scene. Lastly, the answers are collected, and VLM is instructed to summarize the critical behaviors for autonomous driving. We recognize 13 behaviors, namely walking pets (WP), talking (TS), using phone (UP), using an umbrella (UU), using headphones (UH), carrying items in hand (CI), crossing the street (CR), using wheelchair (WC), using stroller (ST), riding bike (BI), riding scooter (SC), using skateboard (SK), and riding motorcycle (MC). Model Table 2: Motion Generation Evaluation. Given the high FID distance, the generic text-to-motion models cannot properly generate motions in driving scenes. FID Multi Modality Critical Behavior Labling Given the recognized behavior set = {bkk {1, ..., m}}, the label of critical behaviors for person is defined as the subset Bh in which the behaviors hold for the person. For each instance, we enumerate each element in the behavior set bk and construct corresponding question qk. Then we provide the corresponding frames to VLM and ask it about the question qk. Based on the answer, we append bk to the behavior set Bh for the person. Directly applying pre-trained VLMs can suffer from noisy labeling results. To alleviate this, we perform human-inthe-loop labeling strategy. We randomly selected 10% of the dataset and employed human annotators to label whether the given person has certain behaviors. Then we use the human-labeled data to fine-tune the annotation VLM, which will further be applied to the rest of the unlabeled data. The details of the human annotation are described in the supplementary materials. Real MotionDiffuse [46] MotionGPT [87] 0.002 39.275 27.059 - 2.36 5. 3.5 Statistics As shown in Fig. 4 (a) and Tab. 1 (last row), our dataset provides 48 hours human motion sequences and corresponding video clips in total. The frame rate of both the motion sequence and video clip is 10 Hz, thus the total frames sum up to 1.73M. The durations of motion sequences range from 1 to 12 seconds. The mean and median durations are 3.01 and 2 seconds. Regarding the text descriptions, the average length of lowand high-level descriptions is 15 and 33 words, respectively. The world clouds for the two kinds of descriptions are illustrated in Fig. 4 (c) and (d). Considering human behaviors, which is one of the characteristics that distinguishes MMHU from previous ones, the most frequent behavior is crossing the street, followed by carrying items, and the least frequent one is using wheelchair, as illustrated in Fig. 4 (b)."
        },
        {
            "title": "4 Tasks",
            "content": "MMHU supports multiple human-centric tasks. In this section, we will present the definition of tasks and corresponding evaluation metrics in our experiments. 7 Table 3: Evaluation of Human Behavior Visual QA. We construct close-ended questions where the model is asked to select whether the person subjects to certain behaviors. We report the F1 score for each behavior, and then an instance-averaged F1 score is used to evaluate the overall performance. Behavior definition please refer to Sec. 3.4. UH MC Micro-F1 CR WC Baseline WP UU UP SK SC TS ST CI BI Phi-4-multimodal MiniCPM-o-2_6 Dolphins Qwen2-VL-7B Qwen2.5-VL-7B InternVL2-8B InternVL2.5-8B Mantis-8B-SigLIP Aya-Vision-8B Idefics3-8B-Llama3 Pixtral-12b Gemma-3-12B-it Deepseek-vl2-small Kimi-VL-A3B GPT4o-mini 42.9 75.0 42.9 66.7 42.9 33.3 75.0 80.0 46.2 42.9 36.4 53.3 42.9 46.2 85.7 35.6 27.4 2.3 2.8 16.4 1.5 14.5 28.1 23.8 18.5 22.9 36.6 19.9 21.2 67. 24.6 58.6 1.0 63.3 36.4 17.2 32.6 68.6 43.3 19.7 18.4 32.2 54.1 39.4 62.5 90.9 100.0 22.2 100.0 76.9 100.0 100.0 100.0 100.0 90.9 90.9 90.9 76.9 76.9 100.0 15.4 44.4 40.0 25.0 15.4 15.4 15.4 28.6 40.0 15.4 18.2 25.0 15.4 20.0 40.0 39.4 33.4 0.3 30.7 40.6 11.2 27.3 52.5 40.4 23.5 25.6 51.6 30.6 35.8 55.0 26.1 26.4 16.9 66.7 32.4 6.3 24.4 22.6 21.3 24.4 21.7 12.1 37.6 23.0 58.4 100.0 85.7 40.0 100.0 85.7 85.7 100.0 100.0 100.0 85.7 100.0 100.0 85.7 85.7 100. 31.2 77.8 6.5 76.9 34.3 25.0 76.9 93.3 75.0 33.3 37.0 41.0 34.3 46.7 66.7 56.3 88.9 1.7 69.3 47.9 54.7 72.5 91.0 77.6 64.1 58.5 63.3 37.5 66.7 90.1 66.7 80.0 40.0 80.0 66.7 66.7 80.0 80.0 80.0 80.0 66.7 66.7 66.7 66.7 80.0 33.3 50.0 57.1 66.7 25.0 28.6 50.0 50.0 50.0 28.6 33.3 40.0 28.6 33.3 50.0 77.1 73.6 1.3 80.7 73.9 68.8 75.5 87.0 29.1 84.6 78.7 87.1 44.0 85.7 65.36 45.5 52.2 3.1 52.1 44.7 27.7 42.2 58.4 38.6 40.1 41.1 52.9 34.9 47.7 64. Table 4: Evaluation of Motion Prediction Baselines. We evaluate the motion prediction baselines on MMHU-T. We leverage the pre-trained weights and evaluate them on our dataset without fine-tuning. All of the baselines generate plausible trajectory predictions, and PhysMoP achieves the best performance. frame_id PhysMoP [5] AuxFormer [1] CIST-GCN [56] 1 0.4 17.0 18.5 1.7 32.7 25.3 7 9.0 47.8 37.2 MPJPE 9 14.4 60.0 40.8 26.3 71.1 46.2 17 36.2 79.0 46.6 21 45.3 84.3 46.8 54.3 86.1 47.4 Table 5: Benefiting Behavior VQA. By finetuning on MMHU, the baseline model (QWen2.5-VL) shows significant improvement on averaged accuracy and F1-score. Model Accuracy F1-Score Baseline Finetuned 35.31 67.77 44.72 68.54 Motion Prediction. Understanding the historical motion and predicting future ones is essential for the safety of autonomous driving. Following PhysMoP [5], we leverage the sequence of human motion keypoints as the representation. Specifically, human motion involves frames M1, ...Mn, each of which represents the global location of human joints at frame Ii. Each motion frame Mi consists the location of key joint point on human body, i.e. Mi = {pk {1, ..., m}}. The human motion prediction task is about predicting future motion frames from historical ones, i.e., predicting Mt1+1, ...Mt1+t2 from M0, ..Mt1. We employed two widely used metrics to evaluate the baseline methods: (1) Mean Per Joint Position Error (MPJPE), which measures the mean 3D Euclidean distance between the predicted and ground truth joint positions after aligning the root joint; (2) and the ACCL metric, measuring the acceleration error averaged over time to measure the physical plausibility of the predicted motion. Motion Generation. Though motion is an informative and compact representation of human actions, collecting them in the real driving scene is expensive and even dangerous for some behaviors. Motionfrom-text generation can be very efficient and effective way to augment motion data. Specifically, given text description of motion, the algorithm is supposed to generate motion sequences that are subject to the text description. We thus test the capacity of existing current text-to-motion approaches to generate human motions in driving scene. We follow MotionDiffuse [46] to model motion sequences by converting from the SMPL parameters. We leverage the high-level description as the text prompt to generate the motions. We employ FID [88] and multi-modality as evaluation criteria. The former evaluates the distributional distance between the generated motions and the ground-truth motions, while the latter measures joint position differences among 32 motion sequences generated from the same text description. Behavior VQA. Humans in the street can have multiple characteristics and behaviors. Unlike previous related tasks, such as intention prediction, where the behavior is simply classified into binary labels crossing the street or not crossing the street, we propose new behavior set that provides more comprehensive aspects of human behaviors. To make the tasks flexible and easy to extend, we formulate the task as visual question-answering (VQA) task. We label humans with 13 behaviors, which are all binary labels, as mentioned in Sec. 3.4. We construct close-ended questions for all labeled behaviors using template. An example is Is the person in the video riding bike? (A) Yes 8 (B) No.\" We then evaluated the accuracy of the baseline methods. We employed the accuracy and F1-score as evaluation metrics."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate recent methods for human behavior understanding in our dataset. Specifically, we evaluated methods related to human motion prediction, text-to-motion generation, and human behavior VQAs. We analyze their performance on their corresponding tasks. 5.1 Dataset Splitting MMHU is split into three subsets, namely MMHU-V, MMHU-H, and MMHU-T, each consisting of 47k, 9.5k, and 840 human instances, representing the VLM-labeled, human-labeled, and testing data. We sample the human-labeling subset (which should be MMHU-H + MMHU-T) from three strategies. (1) in-video-clip sampling: For some video clips, we randomly sample some individuals from each of them as human-labeling data and the rest as MMHU-V data. (2) in-video sampling: we sample some video clips from each raw video and use all the individuals as human-labeling data, and the rest video clips serve as MMHU-V data. (3) out-of-video sampling: we randomly choose some raw videos, and the entire videos serve as the human-labeling dataset. Similarly, we split the MMHU-H and MMHU-T subsets from the human-labeling data. In the experiments, without specifically mentioning, we use both MMHU-V and MMHU-H as training data, denoted as MMHU. 5.2 Baselines Motion Prediction: For motion prediction, we employ PhysMoP [5], CIST-GCN [56], and AuxFormer [55] as the baselines. Following the settings in [5], we randomly select 50 continuous frames from each motion sequence, using the first 25 frames as the input for the baselines and comparing their output with the remaining 25 frames. Motion Generation: We choose MotionDiffuse [46] and motionGPT [87] as our baseline. For both methods, we provided the high-level descriptions from our dataset as the text prompt. Following motionGPT [87], we only use motion sequences that are within 20 to 196 frames. Behavior VQA: For the 13 critical behaviors, there is no specialist model that is trained to predict all of them. Thus, we employed generalist vision-language models as our baselines. We provided the vision language models 4 to 6 frames sampled from the corresponding video and close-ended question constructed from the behavior labels as input, and evaluated whether the VLMs can accurately recognize these essential behaviors and answer the provided question. We employ multi-image querying VLMs, including Phi-4-multimodal [89], MiniCPM [90], Qwen [91, 92], InternVL [93], Mantis [94], Idefics3 [95], Pixtral [96], Gemma3 [97], Deepseek-VL2 [98], Kimi-VL [99]. 5.3 Baseline Evaluation Motion Prediction: We evaluated the pre-trained baselines on MMHU-T, to unify the time argument used in computing the MPJPE metric, we use the frame id to replace it. All baselines are evaluated on the same frames. The results are shown in Tab. 4. All of the approaches generate plausible results, even though they are not specially trained for driving scenes. Among them, PhysMoP [5] achieves the best performance in our dataset. Motion Generation: As shown in Tab. 2 and Fig. 5 (up row), motion generation models pre-trained on generic human motion datasets are not capable of generating plausible motions in driving scenes, due to the domain gap between the general motions and motions in the street. We further provide visualization results of these baselines in the supplementary material. Behavior VQA: We evaluated the ability of mainstream vision-language models to recognize human behaviors in driving scenes. We constructed closed-ended questions based on the behaviors and provided 4 to 6 frames to the VLMs. We evaluated the VLMs based on their correctness in answering these behavior-related questions. The results are shown in Tab. 3. Improving Human Behavior Understanding 5.4 In this section, we show that MMHU can facilitate versatile tasks of human behavior understanding. By finetuning different baseline models on our dataset, we observe significant performance gains. Motion Prediction. We trained PhysMoP [5] on the mixed data of 3DPW [84] and MMHU, and compared it with that trained on 3DPW only. We then evaluate both variants on the original 3DPW dataset, following the original settings of PhysMoP. As shown in Tab. 6, the model training with our MMHU data generalizes to 3DPW and significantly outperforms the baseline model by 9.49 average MPJPE and 1.1 ACCL. 9 Figure 5: Qualitative comparison of Motion Generation. The baseline model (MotionDiffuse, MD) is not capable to generate proper motions in driving scenes. After fine-tuning on MMHU(second row), the model demonstrates the ability to generate human motions in autonomous driving scenarios. Motion Generation. We show that fine-tuning on MMHU can narrow the domain gap between generic text-to-motion generation to motions in driving scenes. We fine-tune MotionDiffuse [46] on MMHU, and we observe significant improvement on FID, as shown in Tab. 8. The visualization results in Fig. 5 further show the effectiveness of fine-tuning on our dataset. Behavior VQA. We employed QWen2.5-VL [25] as our baseline. We fine-tuned the baseline model on our dataset and evaluated both models using the average accuracy and the F1 score on MMHU-T. As shown in Tab. 5, the fine-tuned model achieves significant performance gain of 15.96% and 15.19% with respect to the average precision and F-1 score. Intention Prediction: Intention prediction answers the question of whether or not person is going to cross the street, which is special aspect of behavior QA. We select TrEP [60] as the baseline. We train the baseline model on the JAAD [100] dataset, which is widely used dataset for intention prediction. We then compared the baseline model to that trained on the mixture of JAAD [100] and MMHU. We observe MMHU significantly contribution to performance gain of the baseline model. All evaluations are conducted on the JAAD [94] test set following the original settings of TrEP [60]. Table 6: Benefiting Motion Prediction. We train the PhysMoP [5] on mixed set of MMHU and 3DPW [84] and compared with that trained on 3DPW only. The evaluation is conducted on 3DPW, following the original setting. Table 7: Benefiting Intention Prediction. We train the baseline TrEP [60] with mixture of JAAD [100] and MMHU. We then evaluate on the JAAD dataset. Compared to training on JAAD alone, training with our dataset performs significantly better. Train Set MPJPE-avg ACCL 3DPW 3DPW+MMHU 47.67 38. 3.8 2.7 Train Set JAAD JAAD+MMHU Accuracy 84.49 91.89 F1-score AuROC 84.45 91.89 92.98 97. Table 8: Benefiting Motion Generation. Models fine-tuned on MMHU (labeled with *) can better generate motions in driving domains and achieves notably lower FID. Model FID Multi Modality Real MotionDiffuse MotionDiffuse* MotionGPT MotionGPT* 0.0020 39.27 1.86 27.06 8.44 - 2.36 2.31 5.42 3."
        },
        {
            "title": "6 Conclusion and Limitation",
            "content": "Conclusion. In this work, we present MMHU, large-scale dataset for human behavior understanding. The MMHU dataset consists of 57k human instances, each of them are richly annotated with 3D motion sequences, text descriptions, and labeling of 13 critical behaviors. We collected driving videos from various sources and developed human-in-the-loop annotation pipeline to get high-quality annotations. We evaluated and analyzed the baseline models on the proposed dataset regarding the task of motion prediction, text-to-motion generation, and human behavior VQA. We also conduct experiments to show how MMHU can benefit these tasks. Limitation. MMHU provided comprehensive dataset for human behavior understanding and developed an annotation pipeline that involves minimal human effort. The capability of VLMs to understand human behavior limits the development of fully automatic annotation pipeline."
        },
        {
            "title": "References",
            "content": "[1] Chenxin Xu, Robby Tan, Yuhong Tan, Siheng Chen, Xinchao Wang, and Yanfeng Wang. Auxiliary tasks benefit 3d skeleton-based human motion prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 95099520, 2023. [2] Nada Osman, Guglielmo Camporese, and Lamberto Ballan. Tamformer: Multi-modal transformer with learned attention mask for early intent prediction. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [3] Chen Xie, Ciyun Lin, Xiaoyu Zheng, Bowen Gong, Dayong Wu, and Antonio López. Gtranspdm: graph-embedded transformer with positional decoupling for pedestrian crossing intention prediction. arXiv preprint arXiv:2409.20223, 2024. [4] Dongfang Yang, Haolin Zhang, Ekim Yurtsever, Keith Redmill, and Ümit Özgüner. Predicting pedestrian crossing intention with feature fusion and spatio-temporal attention. IEEE Transactions on Intelligent Vehicles, 7(2):221230, 2022. [5] Yufei Zhang, Jeffrey Kephart, and Qiang Ji. Incorporating physics principles for precise human motion prediction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 61646174, 2024. [6] Jianwu Fang, Fan Wang, Jianru Xue, and Tat-Seng Chua. Behavioral intention prediction in driving scenes: survey. IEEE Transactions on Intelligent Transportation Systems, 2024. [7] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 24462454, 2020. [8] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621 11631, 2020. [9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):12311237, 2013. [10] Amir Rasouli, Iuliia Kotseruba, Toni Kunic, and John Tsotsos. Pie: large-scale dataset and models for pedestrian intention estimation and trajectory prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 62626271, 2019. [11] Apratim Bhattacharyya, Daniel Olmeda Reino, Mario Fritz, and Bernt Schiele. Euro-pvi: Pedestrian vehicle interactions in dense urban centers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64086417, 2021. [12] Amir Rasouli, Iuliia Kotseruba, and John Tsotsos. Are they going to cross? benchmark dataset and baseline for pedestrian crosswalk behavior. In Proceedings of the IEEE international conference on computer vision workshops, pages 206213, 2017. [13] Timo Von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and moving camera. In Proceedings of the European conference on computer vision (ECCV), pages 601617, 2018. [14] Yichen Wang, Yiyi Zhang, Xinhao Hu, Li Niu, Jianfu Zhang, Yasushi Makihara, Yasushi Yagi, Pai Peng, Wenlong Liao, Tao He, et al. Pedestrian motion reconstruction: large-scale benchmark via mixed reality rendering with multiple perspectives and modalities. In The Thirteenth International Conference on Learning Representations. [15] Hee Jae Kim, Kathakoli Sengupta, Masaki Kuribayashi, Hernisa Kacorri, and Eshed Ohn-Bar. Text to blind motion. Advances in Neural Information Processing Systems, 37:1627216285, 2024. [16] Zhizheng Liu, Joe Lin, Wayne Wu, and Bolei Zhou. Learning to generate diverse pedestrian movements from web videos with noisy labels. In The Thirteenth International Conference on Learning Representations, 2024. [17] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 11 [18] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. [19] Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos, 2024. [20] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, Xiuyu Li, Yunhao Fang, Yukang Chen, Cheng-Yu Hsieh, De-An Huang, An-Chieh Cheng, Vishwesh Nath, Jinyi Hu, Sifei Liu, Ranjay Krishna, Daguang Xu, Xiaolong Wang, Pavlo Molchanov, Jan Kautz, Hongxu Yin, Song Han, and Yao Lu. Nvila: Efficient frontier visual language models, 2024. [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [22] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. [24] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. [25] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [26] Zhongyu Jiang, Haorui Ji, Cheng-Yen Yang, and Jenq-Neng Hwang. 2d human pose estimation calibration and keypoint visibility classification. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 60956099. IEEE, 2024. [27] Vasileios Belagiannis and Andrew Zisserman. Recurrent human pose estimation. In 2017 12th IEEE international conference on automatic face & gesture recognition (FG 2017), pages 468475. IEEE, 2017. [28] Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan, and Erjin Zhou. Rethinking the heatmap regression for bottom-up human pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1326413273, 2021. [29] Yanjie Li, Shoukui Zhang, Zhicheng Wang, Sen Yang, Wankou Yang, Shu-Tao Xia, and Erjin Zhou. Tokenpose: Learning keypoint tokens for human pose estimation. In Proceedings of the IEEE/CVF International conference on computer vision, pages 1131311322, 2021. [30] Sheng Jin, Lumin Xu, Jin Xu, Can Wang, Wentao Liu, Chen Qian, Wanli Ouyang, and Ping Luo. Wholebody human pose estimation in the wild. In European Conference on Computer Vision, pages 196214. Springer, 2020. [31] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia), 34(6):248:1248:16, October 2015. [32] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: Modeling and capturing hands and bodies together. ACM Transactions on Graphics, (Proc. SIGGRAPH Asia), 36(6), November 2017. [33] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. [34] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):13251339, 2013. [35] Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, and Wenjun Zeng. Motionbank: large-scale video motion benchmark with disentangled rule-based annotations. arXiv preprint arXiv:2410.13790, 2024. 12 [36] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF international conference on computer vision, pages 54425451, 2019. [37] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motionx: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36:2526825280, 2023. [38] Buzhen Huang, Yuan Shu, Tianshu Zhang, and Yangang Wang. Dynamic multi-person mesh recovery from uncalibrated multi-view cameras. In 2021 International Conference on 3D Vision (3DV), pages 710720. IEEE, 2021. [39] Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou, and Hujun Bao. Motion capture from internet videos. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 210227. Springer, 2020. [40] Diogo Luvizon, Marc Habermann, Vladislav Golyanik, Adam Kortylewski, and Christian Theobalt. Scene-aware 3d multi-human motion capture from single camera. In Computer Graphics Forum, volume 42, pages 371383. Wiley Online Library, 2023. [41] Jiefeng Li, Siyuan Bian, Chao Xu, Gang Liu, Gang Yu, and Cewu Lu. &d: Learning human dynamics from dynamic camera. In European Conference on Computer Vision, pages 479496. Springer, 2022. [42] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2122221232, 2023. [43] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusion-aware human mesh recovery with dynamic cameras. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1103811049, 2022. [44] Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi Shinoda. Implicit neural representations for variable length human motion generation. In European Conference on Computer Vision, pages 356372. Springer, 2022. [45] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. [46] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern analysis and machine intelligence, 46(6):41154128, 2024. [47] Nishant Rai, Haofeng Chen, Jingwei Ji, Rishi Desai, Kazuki Kozuka, Shun Ishizaka, Ehsan Adeli, and Juan Carlos Niebles. Home action genome: Cooperative compositional action understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11184 11193, 2021. [48] Abhinanda Punnakkal, Arjun Chandrasekaran, Nikos Athanasiou, Alejandra Quiros-Ramirez, and Michael Black. Babel: Bodies, action and behavior with english labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 722731, 2021. [49] Jiang Wang, Zicheng Liu, Ying Wu, and Junsong Yuan. Mining actionlet ensemble for action recognition In 2012 IEEE conference on computer vision and pattern recognition, pages with depth cameras. 12901297. IEEE, 2012. [50] Wanqing Li, Zhengyou Zhang, and Zicheng Liu. Action recognition based on bag of 3d points. In 2010 IEEE computer society conference on computer vision and pattern recognition-workshops, pages 914. IEEE, 2010. [51] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. [52] Juan Carlos Niebles, Chih-Wei Chen, and Li Fei-Fei. Modeling temporal structure of decomposable motion segments for activity classification. In European conference on computer vision, pages 392405. Springer, 2010. [53] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 13 [54] Marcin Marszalek, Ivan Laptev, and Cordelia Schmid. Actions in context. In 2009 IEEE conference on computer vision and pattern recognition, pages 29292936. IEEE, 2009. [55] Lucas Goncalves and Carlos Busso. Auxformer: Robust approach to audiovisual emotion recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 73577361. IEEE, 2022. [56] Edgar Medina, Leyong Loh, Namrata Gurung, Kyung Hun Oh, and Niels Heller. Context-based interpretable spatio-temporal graph convolutional network for human motion forecasting. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 32323241, 2024. [57] Xinshun Wang, Qiongjie Cui, Chen Chen, and Mengyuan Liu. Gcnext: Towards the unity of graph convolutions for human motion prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 56425650, 2024. [58] Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier Alameda-Pineda, and Francesc Moreno-Noguer. Back to mlp: simple baseline for human motion prediction. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 48094819, 2023. [59] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. History repeats itself: Human motion prediction via motion attention. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIV 16, pages 474489. Springer, 2020. [60] Zhengming Zhang, Renran Tian, and Zhengming Ding. Trep: Transformer-based evidential prediction for pedestrian intention with uncertainty. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 35343542, 2023. [61] Neha Sharma, Chhavi Dhiman, and Indu. Visualmotioninteraction-guided pedestrian intention prediction framework. IEEE Sensors Journal, 23(22):2754027548, 2023. [62] Iuliia Kotseruba, Amir Rasouli, and John Tsotsos. Benchmark for evaluating pedestrian action prediction. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 12581268, 2021. [63] Amir Rasouli, Iuliia Kotseruba, and John Tsotsos. Agreeing to cross: How drivers and pedestrians communicate. In IEEE Intelligent Vehicles Symposium (IV), pages 264269, 2017. [64] Joon-Young Kwak, Byoung Chul Ko, and Jae-Yeal Nam. Pedestrian intention prediction based on dynamic fuzzy automata for vehicle driving at nighttime. Infrared Physics & Technology, 81:4151, 2017. [65] Raúl Quintero, Ignacio Parra, David Fernández Llorca, and MA Sotelo. Pedestrian intention and pose prediction through dynamical models and behaviour classification. In 2015 IEEE 18th International Conference on Intelligent Transportation Systems, pages 8388. IEEE, 2015. [66] Nicolas Schneider and Dariu Gavrila. Pedestrian path prediction with recursive bayesian filters: comparative study. In german conference on pattern recognition, pages 174183. Springer, 2013. [67] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving. In European Conference on Computer Vision, pages 403420. Springer, 2024. [68] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beißwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In European Conference on Computer Vision, pages 256274. Springer, 2024. [69] Guangyi Chen, Xiao Liu, Guangrun Wang, Kun Zhang, Philip HS Torr, Xiao-Ping Zhang, and Yansong Tang. Tem-adapter: Adapting image-text pretraining for video question answer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1394513955, 2023. [70] Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven Waslander, Yu Liu, and Hongsheng Li. Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1512015130, 2024. [71] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose Alvarez. Omnidrive: holistic llm-agent framework for autonomous driving with 3d perception, reasoning and planning. arXiv preprint arXiv:2405.01533, 2024. [72] Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multimodal large language model. arXiv preprint arXiv:2402.10828, 2024. 14 [73] Long Chen, Oleg Sinavski, Jan Hünermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable autonomous driving. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1409314100. IEEE, 2024. [74] Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Visual question In European Conference on Computer Vision, pages 252269. answering for autonomous driving. Springer, 2024. [75] Hidehisa Arai, Keita Miwa, Kento Sasaki, Yu Yamaguchi, Kohei Watanabe, Shunsuke Aoki, and Issei Yamamoto. Covla: Comprehensive vision-language-action dataset for autonomous driving. arXiv preprint arXiv:2408.10845, 2024. [76] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, and Li Zhang. ReaIn European son2drive: Towards interpretable and chain-based reasoning for autonomous driving. Conference on Computer Vision, pages 292308. Springer, 2024. [77] Yuichi Inoue, Yuki Yada, Kotaro Tanahashi, and Yu Yamaguchi. Nuscenes-mqa: Integrated evaluation of captions and qa for autonomous driving datasets using markup annotations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 930938, 2024. [78] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. arXiv preprint arXiv:2305.14836, 2023. [79] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The oxford robotcar dataset. The International Journal of Robotics Research, 36(1):315, 2017. [80] Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, and Hongkai Yu. Dada: Driver attention prediction in driving accident scenarios. IEEE transactions on intelligent transportation systems, 23(6):49594971, 2021. [81] Haoyu Chen, Jingjing Ren, Jinjin Gu, Hongtao Wu, Xuequan Lu, Haoming Cai, and Lei Zhu. Snow removal in video: new dataset and novel method. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. [82] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Semantic foggy scene understanding with synthetic data. International Journal of Computer Vision, 126:973992, 2018. [83] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing world-grounded humans with accurate 3d motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20702080, 2024. [84] Timo von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d human pose in the wild using imus and moving camera. In European Conference on Computer Vision (ECCV), sep 2018. [85] Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. Drama: Joint risk localization and captioning in driving. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 10431052, 2023. [86] Ginger Delmas, Philippe Weinzaepfel, Thomas Lucas, Francesc Moreno-Noguer, and Grégory Rogez. Posescript: 3d human poses from natural language. In European Conference on Computer Vision, pages 346362. Springer, 2022. [87] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [88] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [89] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. 15 [90] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. [91] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [92] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [93] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [94] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. [95] Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding vision-language models: insights and future directions., 2024. [96] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [97] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [98] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [99] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. [100] Iuliia Kotseruba, Amir Rasouli, and John Tsotsos. Joint attention in autonomous driving (jaad). arXiv preprint arXiv:1609.04741, 2016. [101] YT-DLP. Open source software available from https://github.com/yt-dlp/yt-dlp. [102] Maxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. Label Studio: Data labeling software, 2020-2025. Open source software available from https://github.com/HumanSignal/labelstudio. [103] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [104] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics."
        },
        {
            "title": "Appendices",
            "content": "We describe the details of the datasets in Sec. A, including the details of the data collection, human annotation, and the visualization results from the dataset. We then describe the details of the experiments we conduct in Sec. B."
        },
        {
            "title": "A Dataset Details",
            "content": "A.1 Data Collection For the YouTube source, we filtered and downloaded videos with the Creative Commons Attribution license (reuse allowed) license using yt-dlp [101]. We also filtered the videos by checking whether they contain the keywords drive or driving in their title. We collect video clips that sum to over 11 hours, consisting of 10k human instances. We list the details of used YouTube channels in Tab. 9. Table 9: Collected YouTube Raw Data Channel Name License Collected Minutes Human Instances TravelRelaxListen planetearthtraveler RoamingBrit Evan-Explores Creative Commons Attribution Creative Commons Attribution Creative Commons Attribution Creative Commons Attribution VietnamSilentRoutes Creative Commons Attribution 179 92 65 147 187 2028 3122 497 62 4812 A.2 Human Annotation We present the details of the human annotators for the MMHU-H and MMHU-T subsets. We employ 12 annotators. Each annotator is provided with 6 frames of the video clip. The target human is cropped and labeled with bounding box. We asked the annotators to check for each behavior that the human holds. We pay the annotator 0.15 CNY per video clip, leading to roughly 50 CNY per hour. We use label studio [102] to build the annotation environment. The English version of the annotation interface is shown in Fig. 6. A.3 Details of Hierarchical Text Annotation To bridge the gap between parameterized SMPL motion and semantic-level understanding, we implement hierarchical text annotation pipeline that translates joint-level motion into structured language descriptions. We use PoseScript [86] to extract joint-wise behavioral descriptions (the low-level description) from SMPL pose sequences. Unlike the original method, which outputs single paragraph per frame summarizing the full-body posture, our approach decomposes captions into joint-specific phrases by isolating text segments corresponding to individual joints. We focus on the most salient joint movements and retain only those joints mentioned in at least 50% of the frames within sequence. To ensure temporal consistency, we align the low-level descriptions of each selected joint across the sequence. These temporally aligned joint descriptions are aggregated using large language model to produce concise summaries of each joints motion over time. The low-level descriptions are utilized to support the subsequent aggregation of high-level semantic behavioral descriptions. We show an example of the low-level (joint-wise) descriptions in Box 1. 17 Figure 6: Annotation Interface. Box 1. Example of Low-Level Descriptions Selected Key Frames for Reference: Low-Level (Joint-wise) Descriptions: Elbow: The elbows are generally bent throughout the sequence, with some variation in the degree of bending. Forearm: The forearm remains mostly horizontal with occasional alignment with the thighs and shins, indicating stable position with some minor adjustments. Knee: The knees are generally bent with slight variations, occasionally straightening or separating at shoulder width, indicating dynamic posture. Left Elbow: The left elbow remains mostly bent throughout the sequence, with occasional variations indicating slight changes in its position. ... Then vision-language model is employed to aggregate the low-level (joint-wise) descriptions with the high-level semantic behavior information from the keyframes. The instuctions for this procedure is demonstrated in Box 2. 18 Box 2. Instruction Template for Aggregated Text Description System Prompt: You are an expert in human motion analysis and natural language refinement. Your task is to reorganize and clarify human pose descriptions derived from SMPL-based 3D models. Ensure all descriptions are structured, concise, and consistent, while strictly preserving the original semanticsdo not add, remove, or alter key motion elements. When provided with low-level pose information (e.g., joint-based summaries or posecodes), use it to enhance the clarity or specificity of the description, especially regarding posture and limb motion. However, maintain overall cohesion and naturalness, with clear focus on the pedestrians observable behavior in context. User Prompt: You are given sequence of images showing pedestrian in green bounding box, captured by front-facing car camera, along with dictionary of low-level joint-based pose descriptions. Your task is to: Write concise, oneto two-sentence summary capturing the pedestrians overall motion and any key changes in action throughout the sequence. Emphasize how the pedestrians behavior evolves over time (e.g., walking, stopping, turning, crouching, changing direction). Use the low-level pose descriptions to refine details when relevant, but avoid excessive mechanical phrasing or redundancy. Describe only what is clearly supported by the visual and pose data do not speculate. Format your final response using the following tags: <description> human pose description here. <description_end> Example: <description> The pedestrian stepped forward with arms raised, paused briefly, then lowered their body and turned slightly to the left. <description_end> Here is the low-level pose description: {Input Low-Level Description}"
        },
        {
            "title": "B Experiments",
            "content": "B.1 Human Behavior VQA Details. For each behavior, we construct closed-ended questions based on the template. We provided 4 to 6 frames to the VLMs. The frames are uniformly sampled from the video clips. To reduce randomness, we will ask the VLMs twice, one direct question and one counter question respectively. The VLM should answer both questions correctly in order to be evaluated as correct on the behavior. We show two example in Box 3 and Box 4 (both answered correctly). We find some VLMs are hard to follow the instructions, thus if the answer has the wrong format, we will roll back at most three times and repeat the same question. An example is shown in Box 5. B.2 Additional Results on Motion Generation To better show the performance improvement of motion generation tasks after finetuning on MMHU dataset, we provide some videos of generated motions along with the supplementary materials. We provided three extra cases to show the fine-tuned model is more capable of generating human motion in the driving scene. The video is named following <casenumber>-<Baseline/Finetune>- <behavior_label>-<Mesh/Skeleton>\". The first case shows that the baseline is not capable of generating feasible motion of riding bike. In the second case, the baseline model only captured the walking\" keyword and ignores the behavior of talking to someone\", while the fine-tuned model can generate more plausible motion with hand postures. In the Third case, we add more detailed description of human pose, and the fine-tuned model shows the ability to follow the more detailed instructions. 19 Box 3. Example of Human Behavior VQA: Carrying Items Frames: System Prompt: You are an expert pedestrian behavior labeler, specializing in analyzing pedestrians behavior on the road. You will perform visual analysis on multiple sequential images over time. Question 1: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. Is the pedestrian carrying items? Please answer or n, only one letter. Answer: Question 2: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. The pedestrian is not carrying items. Is this statement correct? Please answer or n, only one letter. Answer: Box 4. Example of Human Behavior VQA: Using Scooter Frames: System Prompt: You are an expert pedestrian behavior labeler, specializing in analyzing pedestrians behavior on the road. You will perform visual analysis on multiple sequential images over time. Question 1: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. Is the pedestrian using scooter? Please answer or n, only one letter. Answer: Question 2: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. The pedestrian is not using scooter. Is this statement correct? Please answer or n, only one letter. Answer: 20 Box 5. Example of Human Behavior VQA: Using Phone Frames: System Prompt: You are an expert pedestrian behavior labeler, specializing in analyzing pedestrians behavior on the road. You will perform visual analysis on multiple sequential images over time. Question 1: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. Is the pedestrian using phone? Please answer or n, only one letter. Answer: This person is crossing the street. [RollBack] Question 1: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. Is the pedestrian using phone? Please answer or n, only one letter. [RollBack] Answer: Question 2: The images show pedestrian in bounding box with surrounding context. The image is cropped frame from cars front camera. The pedestrian is not using phone. Is this statement correct? Please answer or n, only one letter. Answer: Table 10: Additional Results on Motion Generation Model FID Multi Modality Real MotionGPT Baseline MotionGPT Finetuned 0.0020 27.060.645 8.440.203 - 5.420.124 3.770. B.3 Setting Details in Finetuning Models We provide the details of the fine-tuning procedure used in Sec. 5.4 of the main paper. Motion Prediction. The training data consists of the entire 3DPW dataset (training set) [84] and 30k randomly selected human sequences from MMHU. All training settings follow the default configurations described in these papers. Since the frame rate of 3DPW is 25 FPS, while ours is 10 FPS, we upsample all sequences in our dataset to 50 FPS and then downsample them to 25 FPS. Training is conducted on NVIDIA RTX 4090 GPU. Motion Generation. We select the test set from our MMHU dataset and split it into training, validation, and test subsets with 7:1:2 ratio for this experiment. The main training settings and model architecture follow the default configurations in MotionDiffuse [46]. We use the Adam 21 optimizer with learning rate of 0.0002. For fine-tuning, we set the batch size to 192 and train for 20 epochs using single NVIDIA RTX 6000 Ada GPU. Human Behavior VQA. We apply LoRA [103] fine-tuning to Qwen2.5-3B-Instruct by using LLaMA-Factory Framework [104]. The visual branch is frozen, and LoRA is applied to all other MLP layers of the model. Our LoRA settings are: lora_rank = 8 and lora_alpha = 16. The model is trained for one epoch on subset of the MMHU dataset and evaluated on the MMHU-T dataset. Due to class imbalance, we employed re-sampling to make sure the same training data from each behavior class. The training is conducted on 4 NVIDIA RTX 6000 Ada GPUs with total batch size of 128. The learning rate is set to 1e-4, with warmup ratio of 0.1, and the learning rate scheduler is CosineAnnealingLR. All input images are cropped to 256 256 around the bounding box, or resized to 256 256 if the bounding box is larger than 256 256. Intention Prediction. The training data consists of the JAAD dataset and MMHU training set. All training settings primarily follow the default configurations in Trep [60] on the JAAD dataset. All models are implemented using the Adam optimizer with learning rate of 0.005. We adopt an early stopping strategy with patience of 5, and experiments are conducted on single NVIDIA RTX 6000 Ada GPU."
        }
    ],
    "affiliations": [
        "Brown University",
        "Johns Hopkins University",
        "Texas A&M University",
        "UT Austin"
    ]
}