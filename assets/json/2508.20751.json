{
    "paper_title": "Pref-GRPO: Pairwise Preference Reward-based GRPO for Stable Text-to-Image Reinforcement Learning",
    "authors": [
        "Yibin Wang",
        "Zhimin Li",
        "Yuhang Zang",
        "Yujie Zhou",
        "Jiazi Bu",
        "Chunyu Wang",
        "Qinglin Lu",
        "Cheng Jin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements highlight the importance of GRPO-based reinforcement learning methods and benchmarking in enhancing text-to-image (T2I) generation. However, current methods using pointwise reward models (RM) for scoring generated images are susceptible to reward hacking. We reveal that this happens when minimal score differences between images are amplified after normalization, creating illusory advantages that drive the model to over-optimize for trivial gains, ultimately destabilizing the image generation process. To address this, we propose Pref-GRPO, a pairwise preference reward-based GRPO method that shifts the optimization objective from score maximization to preference fitting, ensuring more stable training. In Pref-GRPO, images are pairwise compared within each group using preference RM, and the win rate is used as the reward signal. Extensive experiments demonstrate that PREF-GRPO differentiates subtle image quality differences, providing more stable advantages and mitigating reward hacking. Additionally, existing T2I benchmarks are limited by coarse evaluation criteria, hindering comprehensive model assessment. To solve this, we introduce UniGenBench, a unified T2I benchmark comprising 600 prompts across 5 main themes and 20 subthemes. It evaluates semantic consistency through 10 primary and 27 sub-criteria, leveraging MLLM for benchmark construction and evaluation. Our benchmarks uncover the strengths and weaknesses of both open and closed-source T2I models and validate the effectiveness of Pref-GRPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 1 5 7 0 2 . 8 0 5 2 : r arXiv preprint PREF-GRPO: PAIRWISE PREFERENCE REWARD-BASED GRPO FOR STABLE TEXT-TO-IMAGE REINFORCEMENT LEARNING Yibin Wang1,2,4, Zhimin Li4, Yuhang Zang3, Yujie Zhou3,5, Jiazi Bu3,5,Chunyu Wang4, Qinglin Lu4, Cheng Jin1,2, Jiaqi Wang2,3 1Fudan University, 2Shanghai Innovation Institute 3Shanghai AI Lab, 4Hunyuan, Tencent, 5Shanghai Jiaotong University Project Page: codegoat24.github.io/UnifiedReward/Pref-GRPO Figure 1: Method Overview. (a) Existing pointwise reward functions assign minimal score differences between generated images, which result in illusory advantage and ultimately lead to reward hacking. (b) PREF-GRPO shifts the training focus from reward score maximization to pairwise preference fitting, enabling stable optimization for T2I generation."
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advancements underscore the significant role of GRPO-based reinforcement learning methods and comprehensive benchmarking in enhancing and evaluating text-to-image (T2I) generation. However, (1) current methods employ pointwise reward models (RM) to score group of generated images and compute their advantages through score normalization for policy optimization. Although effective, this reward score-maximization paradigm is susceptible to reward hacking, where scores increase but image quality deteriorates. This work reveals that the underlying cause is illusory advantage, induced by minimal reward score differences between generated images. After group normalization, these small differences are disproportionately amplified, driving the model to over-optimize for trivial gains and ultimately destabilizing the generation process. To this end, this paper proposes PREF-GRPO, the first pairwise preference reward-based GRPO method for T2I generation, which shifts the optimization objective from traditional reward score maximization to pairwise preference fitting, establishing more stable training paradigm. Specifically, in each step, the images within generated group are pairwise compared using preference RM, and their win rate is calculated as the reward signal for policy optimization. Extensive experiments show that PREF-GRPO effectively differentiates subtle image quality differences, offering more stable advantages than pointwise scoring, thus mitigating the reward hacking problem. *Equal contribution. Corresponding author. 1 arXiv preprint (2) Additionally, existing T2I benchmarks are limited to coarse evaluation criteria, covering only narrow range of sub-dimensions and lacking fine-grained evaluation at the individual sub-dimension level, thereby hindering comprehensive assessment of T2I models. Therefore, this paper proposes UNIGENBENCH, unified T2I generation benchmark. Specifically, our benchmark comprises 600 prompts spanning 5 main prompt themes and 20 subthemes, designed to evaluate T2I models semantic consistency across 10 primary and 27 sub evaluation criteria, with each prompt assessing multiple testpoints. Using the general world knowledge and fine-grained image understanding capabilities of Multi-modal Large Language Model (MLLM), we propose an effective pipeline for benchmark construction and evaluation. Through meticulous benchmarking of both open and closed-source T2I models, we uncover their strengths and weaknesses across various fine-grained aspects, and also demonstrate the effectiveness of our proposed PREF-GRPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent progress highlights the pivotal importance of reinforcement learning (Liu et al., 2025; Li et al., 2025; Xue et al., 2025; He et al., 2025) and comprehensive benchmarking (Ghosh et al., 2023; Huang et al., 2023; Wei et al., 2025) in driving advancements and reliable evaluation of text-to-image (T2I) generation. Specifically, several GRPO-based approaches (Liu et al., 2025; Xue et al., 2025) employ pointwise reward models (RMs) (Wang et al., 2025b; Wu et al., 2023; Kirstain et al., 2023) to score group of generated images in each step, followed by score normalization to compute advantages for policy optimization (Guo et al., 2025), which has proven highly effective in aligning T2I generation with human preferences. With these rapid developments, evaluating T2I models, particularly their instruction-following capability, has become crucial challenge. Current widely adopted benchmarks (Huang et al., 2023; Ghosh et al., 2023), commonly assess T2I models by probing various compositional aspects and rely on CLIP (Radford et al., 2021) based metrics for quantitative evaluation. Recently, TIIF-Bench (Wei et al., 2025) has incorporated additional evaluation dimensions, such as text rendering, to provide more comprehensive assessment. Despite effectiveness, these studies encounter two key limitations: (1) Existing GRPO-based methods use pointwise RMs to achieve reward score maximization, which can provide early gains but often results in reward hacking (recognized by (Liu et al., 2025; Xue et al., 2025)) where scores increase but image quality deteriorates during continual learning, shown in Fig. 2. (2) Current T2I generation benchmarks provide only primary dimension-level coarse evaluation, covering limited range of sub-dimensions and lacking fine-grained assessment at sub-dimension level, shown in Fig. 4. In light of these issues, we posit that (1) reward hacking in GRPO-based methods stems from illusory advantage, which arises from the minimal score differences assigned by RMs between images within the group. When these scores are normalized into advantages, the small gaps are disproportionately amplified. Under reward-maximization objective, such inflated advantages drive the policy to over-optimize for trivial reward cues, and this sustained pressure ultimately steers it toward reward-hacking behaviors that rapidly increase scores but destabilize the generation process (examples shown in Figs. 1 and 2). Besides, if the reward model is even slightly biased, this amplification magnifies these errors, driving the policy to exploit model flaws rather than align with human preferences. (2) The performance of current T2I models across Figure 2: Reward Hacking Visualization. most primary evaluation dimensions (e.g., object attributes and actions) has reached relatively high level, underscoring the necessity of decomposing these broad dimensions into finer-grained sub-tasks for more rigorous and comprehensive assessment. To this end, this work proposes PREF-GRPO, the first preference reward-based GRPO method for stable T2I reinforcement learning, and UNIGENBENCH, unified T2I generation benchmark for fine-grained semantic consistency evaluation. We elaborate on both in the following. 2 arXiv preprint Figure 3: Benchmark Statistics and Evaluation Results. This figure presents (a) prompt themes, (b) subject distribution, and evaluation dimensions (testpoints) of UNIGENBENCH, along with benchmarking results for both open-source and closed-source T2I models. Figure 4: Benchmark Comparison. While current methods only support scoring at the primary dimensions, our benchmark provides fine-grained evaluation across both primary and sub dimensions. (1) PREF-GRPO incorporates pairwise preference RM (PPRM) (Wang et al., 2025a), reformulating the GRPO optimization objective from conventional absolute reward score maximization to pairwise preference fitting. As illustrated in Fig. 1, in each step, given set of generated images, we enumerate all possible image pairs and evaluate them with the PPRM to identify the preferred image in each pair. The win rate of each image (computed as the proportion of pairwise comparisons it preferred) is then used as the reward signal for policy optimization. This design offers three key advantages: (a) Amplified reward variance: driving high-quality images toward win-rates near 1 and lowquality ones toward 0 yields more separable distributions and stable, informative advantage estimates. (b) Enhanced robustness: focus on relative rankings rather than absolute scores reduces overoptimization for marginal score gains and mitigates reward hacking. (c) Preference alignment: pairwise comparisons mirror human judgment for comparable images, producing reward signals that better capture nuanced preferences. Extensive experiments demonstrate that PREF-GRPO can discern subtle variations in image quality, yielding more stable and directional advantages than pointwise scoring, thereby enhancing optimization stability and alleviating reward hacking. (2) Our UNIGENBENCH is built for fine-grained T2I evaluation, encompassing comprehensive evaluation dimensions, diverse prompt themes, and subject categories (see Fig. 3). Unlike existing benchmarks that provide only primary dimension-level coarse evaluation, most of our primary dimensions are further subdivided into fine-grained sub-dimensions (testpoints) shown in Fig. 4. We also construct an automated and effective pipeline based on the powerful Multi-modal Large Language Model (MLLM), i.e., Gemini2.5-pro (Huang & Yang, 2025) for both benchmark construction and T2I model evaluation, as illustrated in Fig. 5. We benchmark popular closed-source models, including GPT-4o (Hurst et al., 2024), Imagen-4.0-Ultra (Saharia et al., 2022), and FLUX-Kontext-Max (Labs., 2024), as well as leading open-source models such as Qwen-Image (Wu et al., 2025a), Hidream (Cai et al., 2025), and Bagel (Deng et al., 2025). Our results, provided in Fig. 3 (e) and (f), show that both openand closed-source models perform relatively well on prompts involving style and world knowledge, but consistently underperform on prompts requiring logical reasoning, such as those containing causal, contrastive, or other complex logical descriptions. Contributions: (1) We present an analysis to reveal the fundamental cause of reward hacking as the illusory advantage problem. (2) Based on our analysis, we propose PREF-GRPO, the first pairwise arXiv preprint preference reward-based GRPO method for stable T2I reinforcement learning, reformulating the optimization objective from conventional absolute reward score maximization to pairwise preference fitting. (3) Extensive experiments demonstrate that PREF-GRPO can discern subtle variations in image quality, producing more stable and directional advantages, thereby enhancing optimization stability and alleviating reward hacking. (4) We introduce UNIGENBENCH, which encompasses comprehensive evaluation dimensions and diverse prompt themes, along with an effective pipeline for benchmark construction and T2I model evaluation. (5) Through meticulous evaluation of openand closed-source T2I models, we reveal their strengths and weaknesses across various aspects."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Reinforcement Learning for T2I Generation is gaining rapid momentum. Early efforts pursued preference-driven objectives (Xie & Gong, 2025; Yang et al., 2024; Wallace et al., 2024). More recently, group relative policy optimization (GRPO) has advanced online RL-enhanced image generation. Flow-GRPO (Tong et al., 2025) and DanceGRPO (Xue et al., 2025) instantiate GRPO on flow-matching models, introducing stochasticity by recasting the original deterministic ODE as an equivalent SDE. While effective, these reward score-maximization methods are prone to reward hacking due to illusory advantage. To this end, we propose PREF-GRPO, which shifts training from reward-score maximization to pairwise preference fitting, yields more stable advantages, and thereby mitigates reward hacking. Existing Benchmark for T2I Evaluatoin have expanded the evaluation of T2I models beyond simple visual fidelity, incorporating compositional reasoning (Ghosh et al., 2023; Huang et al., 2023) and world knowledge(Niu et al., 2025). Recently, (Wei et al., 2025) introduces TIIF-Bench, containing 5k prompts spanning multiple dimensions, i.e., text rendering and style control, rigorously evaluating model robustness to variations in prompt length. However, existing benchmarks largely focus on primary dimensionlevel coarse assessment, covering limited set of sub-tasks and lacking finegrained assessment of these sub-tasks. To address this gap, we propose unified image generation benchmark, UNIGENBENCH, consisting of 600 prompts spanning diverse themes and categories, assessing T2I models across 10 primary and 27 sub-criteria."
        },
        {
            "title": "3 PREF-GRPO",
            "content": "This work introduces PREF-GRPO, aiming to establish more stable RL paradigm for T2I generation, mitigating reward hacking in existing reward score-maximization GRPO methods. In this section, we first present the core idea of GRPO applied to flow matching models in Sec. 3.1, then analyze the root cause of reward hacking, i.e., illusory advantage, in Sec. 3.2, and finally describe our proposed pairwise preference reward-based GRPO method in Sec. 3.3."
        },
        {
            "title": "3.1 FLOW MATCHING GRPO",
            "content": "Flow Matching. Let x0 X0 be data sample from the true distribution and x1 X1 noise sample. Rectified flow (Liu et al., 2022) defines intermediate samples as xt = (1 t)x0 + tx1, [0, 1], and trains velocity field vθ(xt, t) via the flow matching (Lipman et al., 2022) objective: LFM(θ) = Et,x0,x1 (cid:2)v vθ(xt, t) 2 (cid:3), = x1 x0. (1) (2) Beyond training, the iterative denoising process at inference time can be naturally formalized as Markov Decision Process (Black et al., 2023). At each step t, the state is st = (c, t, xt), where denotes the prompt, and the action at corresponds to producing the denoised sample xt1 πθ(xt1xt, c). The transition is deterministic, i.e., st+1 = (c, 1, xt1), with the initial state given by sampling prompt p(c), setting = , and drawing xT (0, I). reward is only provided at the final step: r(x0, c) if = 0, and zero otherwise. GRPO on Flow Matching. GRPO (Guo et al., 2025) introduces group-relative advantage to stabilize policy updates. When applied to flow matching models, for group of generated images 4 arXiv preprint {xi 0}G i=1, the advantage of the i-th image is ˆAi = R(xi 0, c) mean({R(xj 0, c)}G std({R(xj 0, c)}G j=1) j=1) ."
        },
        {
            "title": "The policy is updated by maximizing the regularized objective",
            "content": "JFlow-GRPO(θ) = Ec,{xi} (cid:104) (cid:105) (r, ˆA, θ, ϵ, β) , where (r, ˆA, θ, ϵ, β) ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i="
        },
        {
            "title": "1\nT",
            "content": "T 1 (cid:88) t=0 with ri t(θ) = pθ(xi pθold (xi t1xi t1xi t,c) t,c) . min (cid:0)ri t(θ) ˆAi t, clip(ri t(θ), 1 ϵ, 1 + ϵ) ˆAi (3) (4) (cid:1) βDKL(πθπref), (5) To satisfy GRPOs stochastic exploration requirements, (Liu et al., 2025) convert the deterministic Flow-ODE dxt = vtdt to an equivalent SDE: dxt = (cid:0)vθ(xt, t) + σ2 2t (xt + (1 t)vθ(xt, t))(cid:1)dt + σtdwt, (6) where dwt denotes Wiener process increments and σt controls the stochasticity. Euler-Maruyama discretization gives the update rule: xt+t = xt + (cid:0)vθ(xt, t) + (xt + (1 t)vθ(xt, t))(cid:1)t + σt σ2 2t tϵ, ϵ (0, I). (7) where σt = (cid:113) 1t and is scalar hyper-parameter that controls the noise level. 3.2 ILLUSORY ADVANTAGE IN REWARD SCORE-MAXIMIZATION GRPO METHODS Existing flow matching-based GRPO methods Liu et al. (2025); Xue et al. (2025); Li et al. (2025); He et al. (2025) use pointwise reward models (RMs) Wang et al. (2025b); Radford et al. (2021); Wu et al. (2023) to score group of generated images in each training step. Then, the advantage of each generated image is computed by normalizing its reward score relative to the group, as shown in Eq. 3. This normalization standardizes the advantage across group of samples. However, since existing pointwise RMs tend to assign overly similar reward scores R(xi 0, c) to comparable images within the same group, leading to an extremely small standard deviation σr. Consequently, the resulting normalized advantages can be excessively amplified (See example in Fig. 1). We refer to this phenomenon as illusory advantage. Specifically, let µr denote the mean reward and σr the standard deviation of the rewards in the group. When the rewards are close to each other, σr 0. In such cases, even small difference = R(xi 0, c) µr can lead to large advantage: ˆAi = σr . (8) The disproportionate amplification of small reward differences, i.e., illusory advantage, has several detrimental effects: (1) excessive optimization: even minimal score variations are exaggerated, misleading the policy into over-updating and adopting extreme behaviors, i.e., reward hacking  (Fig. 2)  ; (2) sensitivity to reward noise: the optimization becomes highly susceptible to biases or instabilities in the reward model, prompting the policy to exploit model flaws rather than align with true preferences."
        },
        {
            "title": "3.3 PAIRWISE PREFERENCE REWARD-BASED GRPO",
            "content": "To mitigate the illusory advantage problem in existing methods, we propose PREF-GRPO, which leverages Pairwise Preference Reward Model (PPRM) (Wang et al., 2025a) to reformulate the 5 arXiv preprint optimization objective as pairwise preference fitting. Instead of relying on absolute reward scores, PREF-GRPO evaluates relative preferences among generated images, mirroring the human process of assessing two comparable images. This approach enables the reward signal to better capture nuanced differences in image quality, producing more stable and informative advantages for policy optimization while reducing susceptibility to reward hacking. Specifically, given set of images {xi 0, xj enumerate all possible image pairs (xi each pair. The win rate of image is defined as 0}G i=1 sampled from the policy πθ for prompt c, we 0) and use the PPRM to determine the preferred image in wi ="
        },
        {
            "title": "1\nG − 1",
            "content": "(cid:88) j=i (cid:0)xi 0 xj 0 (cid:1), (9) where () is the indicator function, and xi 0 indicates that image is preferred over according to the PPRM. The win rates are then used as rewards for policy optimization, replacing the absolute rewards in the GRPO objective: 0 xj ˆAi = wi mean({wj}G std({wj}G j=1) j=1) . (10) Compared to reward score maximization, Pref-GRPO offers several advantages: (1) Amplified reward variance: By transforming absolute reward scores into pairwise win-rates, Pref-GRPO inherently increases reward variance across group of generated images. High-quality samples are pushed toward win-rates near 1, while lower-quality samples approach 0, producing reward distribution that is both more discriminative and more robust for advantage estimation, thereby mitigating reward hacking. (2) Robustness to reward noise: Because the optimization relies on relative rankings rather than raw scores, Pref-GRPO substantially mitigates the amplified impact of small reward score fluctuations or biases in the reward model. This reduces the likelihood of the policy exploiting flaws in the reward signal, improving training stability. (3) Alignment with human preference: The pairwise formulation mirrors human perceptual evaluation. When comparing two images of similar quality, human judgments are inherently relative rather than absolute. By emulating this process, Pref-GRPO captures fine-grained quality distinctions often missed by pointwise scoring, providing more faithful and reliable signal for policy improvement."
        },
        {
            "title": "4 UNIGENBENCH",
            "content": "Existing benchmarks (Ghosh et al., 2023; Huang et al., 2023; Wei et al., 2025) exhibit following limitations: (1) Limited coverage within coarse evaluation dimensions: typically covering only few sub-dimensions under each evaluation dimension, which fails to capture the full spectrum of model capabilities. For example, as shown in Fig. 4, current benchmarks include only single sub-dimension for relationships and grammar dimensions, leading to an incomplete and potentially misleading assessment of model performance in these aspects. (2) Absence of sub-dimension-level evaluation: providing scores only at the primary evaluation dimension, without assessing individual sub-dimensions. This lack of granularity limits interpretability and hinders detailed understanding of T2I models strengths and weaknesses. Therefore, we propose UNIGENBENCH, unified image generation benchmark that encompasses diverse prompt themes and comprehensive set of fine-grained evaluation criteria. We will first introduce our design of prompt themes and evaluation criteria in the benchmark (Sec. 4.1), and then elaborate our MLLM-based automated pipeline for prompt generation and T2I evaluation (Sec. 4.2)."
        },
        {
            "title": "4.1 PROMPT THEME AND EVALUATION DIMENSIONS DESIGN",
            "content": "As shown in Fig. 3, UNIGENBENCH covers five major prompt themes: Art, Illustration, Creative Divergence, Design, and Film&Storytelling, further divided into 20 subcategories, alongside diverse subject categories including animals, objects, anthropomorphic characters, scenes, and an Other category for special entities (e.g., robots in science-fiction themes). Unlike coarse metrics in existing benchmarks, we define 10 primary evaluation dimensions and 27 sub-dimensions, covering often overlooked aspects such as logical reasoning, facial expressions, and pronoun reference, enabling fine-grained evaluation and alignment with human intent. See Appendix for more details. 6 arXiv preprint Figure 5: UNIGENBENCH Construction and Evaluation Pipeline. We leverage powerful MLLM for (a) large-scale and diverse prompts generation, and (b) scalable and fine-grained T2I evaluation."
        },
        {
            "title": "4.2 BENCHMARK CONSTRUCTION AND EVALUATION PIPELINE",
            "content": "Having established diverse prompt themes, subject categories, and evaluation dimensions, we further construct an MLLM-based automated pipeline to operationalize the benchmark shown in Fig. 5. This pipeline serves two complementary purposes: (1) generating large-scale, diverse, and high-quality prompts in systematic and controllable manner (Sec. 4.2.1), and (2) enabling scalable, reliable, and fine-grained evaluation of T2I models (Sec. 4.2.2). By leveraging the reasoning and perception capabilities of MLLMs, the pipeline eliminates the need for costly human annotation, while ensuring both efficiency and reliability in benchmark construction and model assessment."
        },
        {
            "title": "4.2.1 PROMPT AND TESTPOINT DESCRIPTION GENERATION",
            "content": "Let denote the set of prompt themes, the set of subject categories, and the set of evaluation dimensions. For each prompt, we sample theme and subject category uniformly at random. Subsequently, subset of testpoints {c1, . . . , ck} C, with [1, 5], is sampled to target specific fine-grained evaluation aspects. The selected tuple (t, s, {c1, . . . , ck}) is input into the MLLM, which generates two outputs: (i) natural language prompt that conforms to the semantic constraints of the selected theme and subject category s, and (ii) structured description set {d1, . . . , dk}, where each di specifies how the corresponding testpoint ci is realized in the prompt. Formally, this process can be expressed as: (p, {d1, . . . , dk}) MLLM (cid:16)"
        },
        {
            "title": "4.2.2 T2I MODEL EVALUATION",
            "content": "p, {di} (cid:12) (cid:12) t, s, {c1, . . . , ck} . (11) (cid:17) Given the generated images {xi} for benchmark prompts {pi}, we evaluate each image using an MLLM. Specifically, the image xi, its corresponding prompt pi, and its testpoint descriptions {di,1, . . . , di,k} are provided as input. The MLLM evaluates each testpoint di,j in the context of xi, producing binary score ri,j {0, 1} and textual rationale ei,j justifying the assessment. This can be formally represented as: (ri,1, . . . , ri,k, ei,1, . . . , ei,k) MLLM (cid:16) {ri,j, ei,j} (cid:12) (cid:12) xi, pi, {di,1, . . . , di,k} . (12) (cid:17) This process ensures that the evaluation captures both the quantitative performance on each testpoint and the qualitative reasoning behind the assessment. After obtaining the scores ri,j for each testpoint di,j in all generated images, we aggregate them to compute scores of sub and primary evaluation dimensions. Specifically, for each sub-dimension c, we define its score as the ratio of the number of times the model successfully satisfies the corresponding testpoint description to the total number of occurrences of that testpoint across the benchmark: Rc = (cid:80) i,j 1{di,j and ri,j = 1} i,j 1{di,j c} (cid:80) , (13) 7 arXiv preprint Figure 6: Qualitative Comparison. We compare PREF-GRPO with several pointwise RM-based GRPO methods, demonstrating its superior performance and effectiveness. Table 1: In-domain Semantic Consistency Comparison on UNIGENBENCH. Gemini2.5-pro is used as the VLM for evaluation. Best scores are in bold, second-best in underlined. Model Overall Style World Know. Attribute Action Relation. Logic.Reason. Grammar Compound Layout Text FLUX.1-dev w/ HPS w/ HPS&CLIP w/ UnifiedReward FLUX+Pref-GRPO 61.30 58.77 61.81 63.62 69.46 83.90 75.20 84.92 86.10 88.40 88.92 88.77 88.98 89.72 90.35 67.84 66.56 68.44 71.55 75.00 62.17 58.94 62.54 63.69 69. 67.26 66.88 68.10 70.42 76.52 30.91 28.18 31.01 32.05 44.09 60.96 58.02 59.36 62.43 63.27 47.04 45.88 50.60 52.32 62.43 71.83 67.91 71.07 73.51 77.61 32.18 31.32 33.07 34.44 47. where 1{} is the indicator function. The overall score for primary dimension is then obtained by averaging the scores of all its sub-dimensions. This procedure ensures that both fine-grained performance on sub-dimensions and broader performance on primary dimensions are captured."
        },
        {
            "title": "5 EXPERIMENT",
            "content": "5."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Baselines: We use FLUX.1-dev (Labs., 2024) as base model and UnifiedReward-Think (Wang et al., 2025a) for pairwise preference RM in PREF-GRPO. For reward-maximization baseline comparison, we employ HPS (Wu et al., 2023), CLIP (Radford et al., 2021), and UnifiedReward (UR) (Wang et al., 2025b). Training and Evaluation: We generate 5k prompts using our pipeline (Fig. 5(a)) for training and evaluate models on UNIGENBENCH. Each test prompt generates four outputs for evaluation. Out-of-domain semantic consistency is assessed with GenEval (Ghosh et al., 2023) and T2I-CompBench (Huang et al., 2023), while image quality is evaluated using UR (Wang et al., 2025b), ImageReward (Xu et al., 2023), PickScore (Kirstain et al., 2023), and Aesthetic (Schuhmann., 2022)."
        },
        {
            "title": "5.2 RESULTS OF PREF-GRPO",
            "content": "Quantitative. As shown in Tabs. 1 and 2, our PREF-GRPO demonstrates substantial improvements in both semantic consistency and image quality. For example, on UNIGENBENCH, relative to URbased score-maximization approaches, Pref-GRPO attains 5.84% increase in the overall score, with further improvements of 12.69% on Text and 12.04% on Logical Reasoning. In image quality evaluation, our method also achieves comprehensive advantages. Qualitative. Examples are shown in Fig. 6. Notably, existing methods exhibit varying degrees of reward hacking. For instance, HPS-optimized images tend to be oversaturated, while UR-optimized images appear darker. We also explore mitigating reward hacking by combining multiple reward scores, i.e., using HPS+CLIP jointly (third row in Fig. 6). While this reduces reward hacking, it does not fully resolve the issue. In contrast, our method mitigates reward hacking while markedly improving semantic generation. Reward Hacking Analysis. We visualize the evolution of image quality scores during training for both UR-based score-maximization methods and PREF-GRPO. As shown in Fig. 2, while UR-based 8 arXiv preprint Table 2: Out-of-Domain Semantic Consistency and Image Quality Evaluations. The best results are in bold, and the second best are underlined. Semantic Consistency Image Quality Model UniGenBench T2I-CompBench GenEval UnifiedReward PickScore ImageReward Aesthetic FLUX.1-dev w/ HPS w/ HPS+CLIP w/ UnifiedReward FLUX+Pref-GRPO 61.30 58.77 61.81 63.62 69. 48.17 46.77 49.18 50.20 51.85 62.92 59.31 64.85 67.28 70.53 3.04 3.09 3.08 3.14 3.26 22.42 22.62 22.61 22.88 23.02 1.27 1.34 1.30 1.38 1.44 6.13 6.20 6.25 6.31 6. Table 3: Benchmarking Results of T2I models on UNIGENBENCH. Gemini2.5-pro is used as the VLM for evaluation. Best scores are in bold, second-best in underlined. Model Overall Style World Know. Attribute Action Relation. Logic.Reason. Grammar Compound Layout Text Keling-Ketu DALL-E-3 FLUX-Pro-Ultra Seedream-3.0 FLUX-Kontext-Max Imagen-4.0-Ultra GPT-4o SDXL Playground 2.5 Emu3 Janus-flow Janus Hunyuan-DiT CogView4 BLIP3-o FLUX.1-dev Bagel Janus-Pro Show-o2 SD-3.5-Large Pref-GRPO HiDream Qwen-Image 65.93 69.18 70.67 78.95 80.00 91.54 92. 39.75 45.61 46.02 46.39 51.23 51.38 56.30 59.87 61.30 61.53 61.61 62.73 62.99 69.46 71.81 78.81 92.27 95.06 90.60 98.10 96.59 99.20 98.57 87.40 89.50 86.80 86.20 89.90 94.10 82.00 92.80 83.90 90.20 90.80 87.20 88.60 88.40 92.50 95.10 86.62 93.51 91.61 95.25 94.19 97.47 98.87 72.63 76.11 77.06 62.50 73.58 80.70 83.07 80.22 88.92 85.60 86.71 86.08 88.92 90.35 94.15 94.30 Closed-source Models 71.66 75.97 76.50 85.58 80.93 92.52 93.59 68.73 69.83 70.53 82.98 77.38 92.20 90.79 70.94 78.06 77.54 80.84 85.08 93.02 94.97 Open-source Models 44.34 52.78 51.39 47.97 54.81 62.71 63.25 63.89 67.84 67.74 67.74 70.51 68.59 75.00 72.97 87.61 34.22 42.68 40.11 43.35 50.38 49.05 57.51 63.97 62.17 61.98 64.26 69.58 62.17 69.77 73.00 84. 44.92 51.52 49.75 50.00 55.08 59.64 62.44 66.50 67.26 70.69 68.40 70.18 69.80 76.52 75.38 79.70 43.75 48.18 43.18 52.73 61.36 79.55 84.97 9.55 16.59 19.32 21.14 26.82 24.55 28.18 39.55 30.91 30.23 37.05 40.91 32.27 44.09 41.14 53.64 71.26 68.07 70.05 61.36 84.23 87.97 91.76 47.33 53.21 52.94 60.29 59.09 55.48 54.81 68.45 60.96 66.44 64.44 61.63 58.96 63.27 63.24 60.29 60.81 70.60 67.78 73.84 78.99 91.37 93. 26.68 35.44 36.86 45.10 46.65 41.62 44.72 53.74 47.04 58.12 62.11 64.69 58.76 62.43 62.63 73.32 77.23 66.67 81.53 87.31 85.04 93.10 91.35 29.85 37.13 44.78 46.46 54.85 44.78 69.22 68.47 71.83 76.49 72.01 75.37 69.03 77.61 78.17 85.82 16.03 25.86 37.36 71.55 61.92 89.08 89.24 0.57 1.15 1.15 0.86 1.15 1.15 17.82 1.15 32.18 7.76 2.59 1.15 32.76 47.13 64.94 74.14 models exhibit rapid score increases, inspection of intermediate results reveals degradation in actual image quality. In contrast, our Pref-GRPO, though fitting pairwise preferences and yielding relatively more gradual score growth, demonstrates consistent and stable improvements in visual quality and effectively mitigates reward hacking. See Appendix A.4 for more analyses."
        },
        {
            "title": "5.3 BENCHMARKING RESULTS ON UNIGENBENCH",
            "content": "As shown in Tab. 3, closed-source models deliver the strongest results: GPT-4o Hurst et al. (2024) and Imagen-4.0-Ultra Saharia et al. (2022) lead across most dimensions, particularly logical reasoning, text rendering, relationship understanding, and compound, indicating robust semantic alignment and understanding. Open-source models are improving: Qwen-Image (Wu et al., 2025a) and HiDream (Cai et al., 2025) consistently rank at the top among open models, with notable strengths in Action, layout, and attribute, narrowing the gap with closed-sourced models. Despite this progress, limitations still remain. Most openand closed-source models have not yet reached saturation on the most challenging dimensions, particularly logical reasoning and text rendering, leaving substantial room for improvement. Moreover, open-source models tend to exhibit greater instability across dimensions, often lagging in grammar and compound tasks. See Appendix B.2 for sub-dimension-level evaluation."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We propose PREF-GRPO, the first pairwise preference reward-based GRPO method, offering more stable T2I reinforcement learning paradigm. Besides, we introduce UNIGENBENCH, unified T2I generation benchmark that encompasses comprehensive dimensions and diverse prompt themes. Extensive experiments validate the effectiveness of our method and the reliability of the benchmark. 9 arXiv preprint"
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025a. Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025b. Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. arXiv preprint arXiv:2105.13290, 2021. Yu Gao, Lixue Gong, Qiushan Guo, Xiaoxia Hou, Zhichao Lai, Fanshi Li, Liang Li, Xiaochen Lian, Chao Liao, Liyang Liu, et al. Seedream 3.0 technical report. arXiv preprint arXiv:2504.11346, 2025. Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. NIPS, 36:5213252152, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. NIPS, 36:7872378747, 2023. Yichen Huang and Lin Yang. Gemini 2.5 pro capable of winning gold at imo 2025. arXiv preprint arXiv:2507.15855, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-apic: An open dataset of user preferences for text-to-image generation. NIPS, 36:3665236663, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In SOSP, pp. 611626, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024a. 10 arXiv preprint Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024b. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation, 2024. Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pp. 87488763, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NIPS, 35:3647936494, 2022. Chrisoph Schuhmann. Laion aesthetics. https://github.com/LAION-AI/aesthetic-predictor, 2022. Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, pp. 82288238, 2024. Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement fine-tuning. arXiv preprint arXiv:2505.03318, 2025a. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025b. 11 arXiv preprint Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In CVPR, pp. 1296612977, 2025b. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. Xin Xie and Dong Gong. Dymo: Training-free diffusion model alignment with dynamic multiobjective scheduling. In CVPR, pp. 1322013230, 2025. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Using human feedback to fine-tune diffusion models without any reward model. In CVPR, pp. 89418951, 2024. 12 arXiv preprint PREF-GRPO A.1 WHY PAIRWISE PREFERENCE-BASED REWARD WORKS This work finds that reward hacking is fundamentally caused by the model overly aligning with the reward models preferences. Specifically, we observe that HPS tends to favor images with saturated colors. However, when the model excessively optimizes this preference, reward hacking occurs, resulting in extreme saturation across all generated images. In contrast, stable optimization should yield subtle adjustments, such as moderately bright colors. Existing works (Liu et al., 2025; Xue et al., 2025) also discuss the issue of reward hacking, recognizing it as pervasive challenge in the field. However, these methods typically attempt to alleviate the problem by adjusting experimental settings, such as incorporating the KL loss (Liu et al., 2025). In contrast, our work reveals that the underlying cause of reward hacking is the issue of illusory advantage. This drives the model to continually over-optimize for trivial reward score improvements, exacerbating reward hacking. Some trivial methods, such as directly scaling down the advantage or scaling up the standard deviation during reward normalization, although they mitigate illusory advantage to some extent, come at the cost of reduced model learning ability. Scaling down the advantage essentially reduces the learning rate by limiting the magnitude of updates. Scaling up the standard deviation dampens the models ability to distinguish meaningful reward differences, leading to less responsive learning process. In contrast, our work explores more stable reward mechanism: pairwise preference fitting. We analyze the rationale behind this mechanism as follows: (1) During GRPO, reward models act as proxies for human judgment, guiding the models training process. However, human evaluators typically make relative comparisons between comparable images, rather than assigning absolute scores to each image. This relative comparison allows for more accurate capture of subtle differences in image quality, ensuring that the model better aligns with human preferences. (2) Moreover, even when occasional errors occur in pairwise preference-based rewards, these errors are not amplified in the same way as errors in reward score maximization learning. This is because pairwise preference fitting provides more stable advantages, ensuring that small errors do not disproportionately influence the optimization process. As result, the models training process is less prone to destabilization. Figure 7: Qualitative Results of UnifiedReward Score-based Winrate as Reward: We convert UnifiedReward scores into win rates as rewards for GRPO, and observe that this effectively mitigates the reward hacking issue. A.2 POINT SCORE-BASED WINRATE V.S. PAIRWISE PREFERENCE-BASED WINRATE To demonstrate that shifting the training objective to our proposed pairwise preference fitting enhances stability and that pairwise comparisons are more reliable than pointwise scores, we conduct an experiment using point score-based win rates as rewards. Specifically, we convert the UnifiedReward scores for group of images into win rates by comparing the scores of each image pair. This win rate then serves as the reward signal for training. As shown in Fig. 7, when the training objective shifts to pairwise preference fitting, the previously dominant dark style in the images is notably alleviated, which validates that pairwise preference fitting stabilizes training. We also provide quantitative results in Tab. 4, demonstrating that although using point score-based win rates as rewards yields significant improvements over reward score maximization, our method using pairwise preference rewards achieves even better results. This confirms that relative comparisons between images are more reliable than absolute point-based scoring. 13 arXiv preprint Table 4: Exploration of Sampling Steps and Joint Optimization. The best results are in bold, and the second best are underlined. Semantic Consistency Image Quality UniGenBench GenEval UnifiedReward PickScore ImageReward Aesthetic Model FLUX.1-dev FLUX+UR (score) FLUX+UR (winrate) Pref-GRPO Pref-GRPO w/ 16 steps w/ 20 steps w/ 25 steps w/ 30 steps 61.30 62. 3.04 22.42 Point Score-based Winrate v.s. Pairwise Preference-based Winrate 63.62 64.32 69.46 68.12 69.23 69.46 69.49 67.28 68.13 70. 3.14 3.20 3.26 22.88 22.91 23.02 Exploration of Sampling Steps during Rollout 67.99 68.92 70.53 70.51 3.12 3.18 3.26 3.22 22.89 22.94 23.02 22. Join Optimization of Pref-GRPO and Reward Score-Maximization Pref-GRPO Pref-GRPO+CLIP 69.46 70.02 70.53 71.26 3.26 3.18 23.02 22. 1.27 1.38 1.39 1.44 1.36 1.48 1.44 1.46 1.44 1.41 6.13 6.31 6.35 6. 6.33 6.43 6.52 6.48 6.52 6.44 A.3 MORE IMPLEMENTATION DETAILS Training is conducted on 64 H20 GPUs with 25 sampling steps, 8 rollouts per prompt from the same initial noise, 4 gradient accumulation steps, and learning rate of 1 105. Following (Liu et al., 2025), we set the hyperparameter = 0.7. We deploy pairwise preference reward server via vLLM (Kwon et al., 2023). For inference, we adopt 30 sampling steps and classifier-free guidance scale of 3.5, consistent with the official Flux configuration. Figure 8: Reward Hacking Visualization of HPS. At around step 160, the image quality begins to degrade, even though the reward score continues to rise, indicating the occurrence of reward hacking. A.4 MORE REWARD HACKING ANALYSIS We further visualize the phenomenon of reward hacking when using HPS (Wu et al., 2023) as the pointwise reward model. As shown in Fig. 8, the reward score increases sharply during training, yet the model quality begins to deteriorate around step 160, manifesting as over-saturated colors. Despite this degradation, the reward score continues to rise. This indicates the presence of the illusory advantage problem, where the model excessively optimizes for marginal improvements in reward. Under prolonged pressure, the model deviates towards hacked trajectory that rapidly inflates reward scores while compromising generation quality. Additionally, we observe that HPS exhibits reward hacking more rapidly compared to UnifiedReward. This is likely because HPS assigns even more minimal reward score differences between generated images, resulting in smaller standard deviation, as shown in Fig. 1, which exacerbates the issue of illusory advantage. 14 arXiv preprint Table 5: Out-of-Domain Performance Comparison on GenEval. The best results are in bold, and the second best are underlined. Model Overall Single Obj. Two Obj. Counting Colors Position Attr. Binding FLUX.1-dev w/ HPS w/ HPS+CLIP w/ UnifiedReward FLUX+Pref-GRPO 62.92 59.31 64.85 67.28 70.53 97.81 97.43 98.12 98. 99.38 79.55 75.00 81.00 82.57 86.36 71.56 62.81 71.81 72.25 74.06 77.66 73.67 78.44 79. 81.12 18.50 21.00 19.00 21.25 26.00 42.25 34.75 40.75 49.50 57.25 Table 6: Out-of-Domain Performance Comparison on T2I-CompBench. The best results are in bold, and the second best are underlined. Overall 3D-Spatial Numeracy Non-Spatial Complex 2D-Spatial Texture Model Shape Color FLUX.1-dev w/ HPS w/ HPS+CLIP w/ UnifiedReward 48.17 46.77 49.18 50.20 77.34 78.17 78.44 78.32 48.32 51.55 53.22 55.13 FLUX+Pref-GRPO 51. 80.27 56.01 62.66 66.13 64.24 67.44 69.12 28.01 22.06 26.90 28.91 28. 40.04 33.75 40.83 40.04 43.95 61.88 56.34 61.58 62.47 65.92 30.67 30.20 30.56 30.88 31. 36.49 35.96 37.69 38.39 39.58 A.5 OUT-OF-DOMAIN SEMANTIC EVALUATION We provide detailed out-of-domain semantic generation evaluations in Tabs. 5 and 6, which highlight the notable improvements of our method compared with existing approaches. A.6 SAMPLING STEPS ANALYSIS We further investigate the impact of the number of sampling steps during rollout on both semantic consistency and image quality. As shown in Tab. 4, increasing the sampling steps from 16 to 25 consistently improves performance across all metrics, with the best results achieved at 25 steps. Although 30 steps yield comparable results to 25, the additional computation brings higher time costs without clear gains. Therefore, we adopt 25 sampling steps as the default setting, which strikes the best balance between effectiveness and efficiency. A."
        },
        {
            "title": "JOINT OPTIMIZATION OF PAIRWISE PREFERENCE FITTING AND REWARD SCORE\nMAXIMIZATION",
            "content": "Although reward score maximization inherently risks reward hacking, we hypothesize that incorporating our pairwise preference fitting mechanism for joint optimization can substantially mitigate this issue. To validate this, we conduct joint optimization using simple yet effective reward model, i.e., CLIP (Radford et al., 2021). As shown in Tab. 4, the integration of CLIP notably improves semantic consistency, but this gain comes at the expense of slightly reduced image quality, highlighting trade-off between semantic alignment and visual fidelity. We also provide qualitative comparison results in Fig. 9, where, despite the quality trade-off, no reward hacking phenomenon is observed. These results indicate that pairwise preference fitting acts as regularizer when combined with reward score maximization, providing principled way to balance semantic accuracy and visual quality while mitigating reward hacking."
        },
        {
            "title": "B UNIGENBENCH",
            "content": "B.1 BENCHMARKING MODELS Closed-source Models. GPT-4o (Hurst et al., 2024), Imagen3.0/4.0-ultra Saharia et al. (2022), Seedream-3.0 (Gao et al., 2025), DALL-E-3 (OpenAI), FLUX-Pro-Ultra/Kontext-Max (Labs., 2024), and Keling-Ketu (Kuaishou). Open-source Models. Qwen-Image (Wu et al., 2025a), Hidream (Cai et al., 2025), Show-o2 (Xie et al., 2025), SD-3.5-Large (Rombach et al., 2021), Janus-Pro (Chen et al., 2025b), Flux.1-dev (Labs., 15 arXiv preprint Figure 9: Qualitative Results of Joint Optimization. Joint training with CLIP improves semantic consistency while slightly degrading perceptual quality, reflecting the inherent trade-off. Figure 10: More Qualitative Comparison. We compare PREF-GRPO with several pointwise RM-based GRPO methods, demonstrating its superior performance and effectiveness. 2024), Bagel (Deng et al., 2025), BLIP3-o (Chen et al., 2025a), CogVideo4 (Ding et al., 2021), Hunyuan-DiT (Li et al., 2024b), Janus (Wu et al., 2025b), Janus-flow (Ma et al., 2024), Emu3 (Wang et al., 2024), Playground2.5 (Li et al., 2024a), and SDXL (Rombach et al., 2021). B.2 FINE-GRAINED EVALUATION RESULTS Existing benchmarks are limited to evaluating only primary dimensions, without capturing the performance of models on more granular aspects. In contrast, our UNIGENBENCH enables finegrained assessment across both primary dimensions and their corresponding sub-dimensions. The detailed evaluation results are provided in Fig. 11. B.3 PROMPT THEMES To comprehensively assess the generative capabilities of T2I models across diverse scenarios, we design the benchmark prompts to achieve broad thematic coverage. As illustrated in Fig. 12, the prompts are organized into five major theme categories: Art, Illustration, Creative Divergence, Design, and Film&Storytelling, which are further divided into 20 subcategories. This hierarchical design ensures comprehensive coverage of practical application scenarios while enabling detailed evaluation across different creative domains. We provide several prompt cases of each prompt theme to facilitate understanding in Fig. 12. 16 arXiv preprint Figure 11: Fine-grained Benchmarking Results of T2I models on UNIGENBENCH. Best scores are in green, second-best in yellow. Figure 12: Prompt Themes of UNIGENBENCH. We provide representative prompt examples for each theme to facilitate understanding. B.4 SUBJECT CATEGORIES As shown in Fig. 3 (b), we further design the benchmark to cover diverse range of subject categories, including animals, objects, anthropomorphic characters, and scenes. Moreover, an Other category is introduced to capture special entities that emerge in specific themes, such as robots in science-fiction themes or sculptures in artistic contexts, thereby ensuring that the benchmark reflects broader spectrum of generation subjects. B.5 EVALUATION DIMENSIONS With the rapid advancement of T2I models, their overall generative performance on mainstream evaluation dimensions, such as object attributes and actions, has already reached relatively high level. We argue that future evaluations should move beyond these coarse dimensions and adopt finer-grained decomposition, thereby more precisely identifying models strengths and weaknesses across specific sub-tasks and providing deeper insights into its true capabilities and limitations. To this end, our benchmark defines 10 primary evaluation dimensions, six of which are further decomposed into fine-grained sub-dimensions, as illustrated in Fig. 13. These include several critical aspects that are largely overlooked by existing benchmarks: Logical Reasoning: Evaluates models ability to handle prompts requiring causal, contrastive, or other simple logical inferences. 17 arXiv preprint Figure 13: Evaluation Dimensions of UNIGENBENCH. We provide representative prompt examples for each evaluation dimension to facilitate understanding. Figure 14: Distribution of Testpoint Counts in Prompts. This figure presents the distribution of the number of testpoints per prompt in UNIGENBENCH. Facial Expressions: Assesses whether generated characters exhibit correct and contextually appropriate emotions. Pronoun Reference: Tests the models capability to resolve ambiguous pronouns (e.g., its, him) correctly. Hand Actions: Examines whether fine-grained hand movements and gestures are accurately rendered. Composition Relations: Measures understanding of made of or composed of relations among objects. Similarity Relations: Evaluates the ability to represent resemblance (e.g., two similar objects, looks like. . . ). Inclusion Relations: Tests comprehension of contains or inside relationships among entities. Grammatical Consistency: Assesses whether multiple objects correctly share attributes or features specified in the prompt (e.g., both red balloons...). We believe that incorporating these fine-grained dimensions is essential for evaluating nuanced semantic comprehension and for ensuring closer alignment with human intent. We also provide several prompt cases of each evaluation dimension in Fig. 13. arXiv preprint B.6 DISTRIBUTION OF TESTPOINT COUNTS IN PROMPTS Unlike other benchmarks that contain thousands of prompts, UniGenBench only requires 600 prompts, each focusing on 1 to 5 specific testpoints, ensuring both breadth and efficiency in evaluation. We visualize the distribution of testpoint counts across prompts, as shown in Fig. 14. B.7 SUPERIORITY OF UNIGENBENCH The superiority of UNIGENBENCH can be summarized as follows: Comprehensive Dimension Evaluation: It spans 10 primary dimensions and 27 subdimensions, offering systematic and in-depth assessment of models capabilities across various aspects. To the best of our knowledge, this is the most comprehensive benchmark in terms of evaluation dimensions. Rich Prompt Theme Coverage: The benchmark includes 5 major prompt themes and 20 sub-themes, covering wide array of generation scenarios, ranging from realistic to creative tasks. This ensures comprehensive evaluation of the models generative capabilities across various scenarios. Efficient and Effective: Unlike other benchmarks Wei et al. (2025); Huang et al. (2023) that require thousands of prompts, UniGenBench utilizes only 600 prompts, each focused on 1 to 5 specific testpoints, ensuring both breadth and efficiency in evaluation. Reliable MLLM Evaluation: Each prompt is paired with detailed testpoint descriptions that clarify how the testpoints are manifested in the prompt, enabling MLLM to perform precise assessments. Unlike other methods Wei et al. (2025); Huang et al. (2023), which often require multiple questions per sample for evaluation, our approach streamlines the process, improving efficiency without compromising accuracy."
        },
        {
            "title": "C ETHICAL STATEMENT",
            "content": "In this work, we affirm our commitment to ethical research practices and responsible innovation. To the best of our knowledge, this study does not involve any data, methodologies, or applications that raise ethical concerns. All experiments and analyses were conducted in compliance with established ethical guidelines, ensuring the integrity and transparency of our research process."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Hunyuan, Tencent",
        "Shanghai AI Lab",
        "Shanghai Innovation Institute",
        "Shanghai Jiaotong University"
    ]
}