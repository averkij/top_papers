{
    "paper_title": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing",
    "authors": [
        "Xiaokun Sun",
        "Zeyu Cai",
        "Hao Tang",
        "Ying Tai",
        "Jian Yang",
        "Zhenyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, a training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blending source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/MorphAny3D.github.io/."
        },
        {
            "title": "Start",
            "content": "MorphAny3D: Unleashing the Power of Structured Latent in 3D Morphing Xiaokun Sun1, Zeyu Cai1, Hao Tang2, Ying Tai1, Jian Yang1, Zhenyu Zhang1 1Nanjing University 2Peking University Corresponding Author xiaokun sun@smail.nju.edu.cn, caizeyu010612@gmail.com, bjdxtanghao@gmail.com {yingtai, csjyang}@nju.edu.cn, zhangjesse@foxmail.com 6 2 0 2 1 ] . [ 1 4 0 2 0 0 . 1 0 6 2 : r Figure 1. MorphAny3D enables smooth, semantically coherent, and aesthetically pleasing transitions from source objects (first column) to target objects (last column), even when they share no semantic or visual relationship (e.g., bee to biplane, Wukong to tree)."
        },
        {
            "title": "Abstract",
            "content": "3D morphing remains challenging due to the difficulty of generating semantically consistent and temporally smooth deformations, especially across categories. We present MorphAny3D, training-free framework that leverages Structured Latent (SLAT) representations for high-quality 3D morphing. Our key insight is that intelligently blend1 ing source and target SLAT features within the attention mechanisms of 3D generators naturally produces plausible morphing sequences. To this end, we introduce Morphing Cross-Attention (MCA), which fuses source and target information for structural coherence, and Temporal-Fused Self-Attention (TFSA), which enhances temporal consistency by incorporating features from preceding frames. An orientation correction strategy further mitigates the pose ambiguity within the morphing steps. Extensive experiments show that our method generates state-of-the-art morphing sequences, even for challenging cross-category cases. MorphAny3D further supports advanced applications such as decoupled morphing and 3D style transfer, and can be generalized to other SLAT-based generative models. Project page: https://xiaokunsun.github.io/ MorphAny3D.github.io/. 1. Introduction Morphing [43, 76, 77] refers to the visual effect of seamlessly transforming source into target through smooth, plausible, and aesthetic deformation sequence. This foundational technique is widely used in animation, film, and game design to enhance creative expression. Based on input dimensionality, morphing is categorized into 2D morphing [2, 4, 36, 90] and 3D morphing [17, 20, 38, 70, 83]. While 2D morphing has advanced significantly with diffusion models [30, 59, 64, 65], 3D morphing remains challenging due to the inherent complexity of modeling smooth and reasonable deformations in three dimensions. Most existing 3D morphing approaches follow two-stage paradigm: (1) establishing dense correspondences [67, 71] between source and target objects using hand-crafted sparse landmarks [17, 60], functional maps [57], optimal transport [63], data priors [20], or extracted 2D features [50]; and (2) interpolating between these correspondences to generate intermediate shapes. While effective in constrained settings, such matching-based methods face several critical limitations. First, they typically prioritize geometric deformation while ignoring the concurrent evolution of textures. Second, they often exhibit limited generalization, particularly in cross-category transformations (e.g., morphing chair into car). As shown in Fig. 2-(a), inaccurate correspondence estimation frequently leads to structurally implausible morphing results. Recent advances in feed-forward 3D generation frameworks [47, 68, 79, 81, 88, 91, 93] have substantially improved the quality and fidelity of 3D synthesis. Notably, Trellis [81] achieves diverse and high-fidelity 3D generation through its Structured Latent (SLAT) representation. Owing to SLATs explicit and regular structure, it shows strong potential for training-free applications such as 3D editing [37, 87], stylization [55], and scene modeling [13]. (a) Figure 2. Comparison of different 3D morphing strategies. Matching-Based 3D Morphing; (b) 2D Morphing + 3D Generation; (c) Direct Interpolation; (d) MorphAny3D. Our method leverages the powerful SLAT to achieve semantically plausible and temporally smooth 3D morphing without any training. α [0, 1] is the deformation weight controlling the morphing progress. naive strategy for 3D morphing with SLAT involves first generating 2D morphing sequence and then lifting each frame independently into 3D using Trellis. However, as illustrated in Fig. 2-(b), this approach fails to ensure temporal consistency due to frame-wise generation and often misrepresents complex 3D deformations. strategy that is more deeply integrated with the generative framework is to directly interpolate the initial noise and conditional features of Trellis, similar to generative-prior-based morphing methods [83, 90]. Yet, as shown in Fig. 2-(c), this strategy lacks explicit constraints on structural plausibility and temporal continuity, often producing suboptimal morphing quality. In summary, while the SLAT representation offers compelling opportunities for 3D morphing, achieving smooth, high-fidelity, and temporally coherent 3D morphing within modern SLAT-based generative frameworks remains an open and pressing challenge. To address these challenges, we present MorphAny3D, novel and robust training-free 3D morphing framework built upon the SLAT representation. As demonstrated in Fig. 1 and Fig. 2-(d), MorphAny3D fully utilizes the 3D generative prior embedded in SLAT to produce structurally plausible and temporally smooth morphing sequences between diverse object categories. Our methodology stems from key observation: rather than interpolating at the noise or condition level, directly aggregating SLAT features within attention mechanisms yields more reasonable and visually smooth 3D deformations. Guided by this insight, we introduce two core attention-based components: (1) Morphing Cross-Attention (MCA): intelligently fuses information from the source and target objects in the cross-attention layers, ensuring the structural coherence and aesthetics of the deformation. (2) Temporal-Fused SelfAttention (TFSA): enhances temporal coherence by incorporating SLAT features from the previous morphing frame into the self-attention mechanism, enabling smooth transitions over time. Furthermore, we propose an orientation correction strategy inspired by the orientation distribution patterns of SLAT-based 3D objects, which effectively resolves abrupt orientation shifts during the morphing process. Extensive experiments demonstrate that our method generates more plausible, smoother, and aesthetically superior 3D morphing sequences compared to existing approaches. Beyond basic morphing, MorphAny3D natively supports advanced applications such as decoupled morphing and 3D stylization, and it can be seamlessly transferred to other SLAT-based models. Our main contributions are summarized as follows: We propose MorphAny3D, the training-free 3D morphing framework based on Structured Latent (SLAT), capable of generating smooth and semantically coherent deformations across diverse 3D object categories. Inspired by SLAT fusion patterns in attention mechanisms, we propose Morphing Cross-Attention and Temporal-Fused Self-Attentiontwo key components that leverage SLAT features across objects and frames to improve morphing quality and temporal consistency. Drawing on the statistical orientation distributions of SLAT-based 3D objects, we propose an orientation correction strategy to mitigate abrupt orientation changes, further enhancing morphing smoothness. 2. Related Work 2D Morphing. Image morphing [36, 42, 43, 62, 95] has long been studied in computer vision and graphics due to its broad applications. Traditional methods [36, 42, 62, 95] rely on hand-crafted features to establish image correspondences, followed by gradual blending to produce smooth transitions. However, they often fail to generate plausible, novel content and are prone to artifacts such as ghosting. With the advent of deep learning, data-driven approaches [2, 23] have improved results by learning 2D generative priors. Nevertheless, their limited model capacity makes crosscategory morphing challenging. The recent success of diffusion models [30, 64, 65] has enabled high-fidelity and diverse image generation. By leveraging pre-trained textto-image models [59], several works [4, 27, 73, 85, 90] have significantly enhanced morphing quality across different categories. Despite these advances, extending such smooth and semantically coherent transformations to 3D content remains an open challenge. 3D Morphing. Most 3D morphing methods rely on correspondences between source and target shapes, followed by the interpolation of matched 3D primitives. The core difficulty lies in accurate 3D matching [12, 61, 67, 71]. Early methods approach this from an axiomatic perspective, incorporating theories such as optimal transport [19, 63, 70] and functional maps [18, 53, 57, 58] to compute correspondences within the same category. In the deep learning era, one line of work [3, 20, 94] adopts data-driven paradigm, enabling the prediction of 3D correspondences using curated 3D datasets. Another category [16, 25, 38, 44, 66] leverages powerful 2D feature extractors [52, 56, 59] to establish 3D correspondences in zero-shot manner. However, these methods struggle to reliably find correspondences across diverse object categories, limiting the practicality of matching-based 3D morphing. 3DMorpher [83] improves cross-category morphing performance by incorporating 3DGS-based 3D generative priors. However, it cannot handle complex geometries due to limitations in its base generator. Moreover, 3DGS-based outputs are not compatible with most commercial 3D software [1, 22, 24]. In this work, inspired by Trellis [81]a milestone in 3D generationwe propose series of simple yet effective modules based on its SLAT representation to achieve aesthetically pleasing and semantically coherent 3D morphing without any training. 3D Generative Models. Advances in 2D diffusion models [30, 45, 48, 54, 64, 65] and the availability of large-scale 3D datasets [10, 11] have driven rapid progress in 3D generation [79, 28, 31, 32, 34, 35, 3941, 46, 47, 68, 69, 74, 78 82, 84, 86, 88, 89, 9193] Among these, Trellis [81] stands out as groundbreaking method for native 3D generation. Its proposed Structured LATent (SLAT) representation not only efficiently encodes rich visual features but is also easily modeled and understood within modern generative frameworks. Crucially, SLAT possesses regular and explicit structure, offering superior extensibility compared to implicit or irregular representations such as VecSet [88], NeRF [49], or 3DGS [33]. growing body of followup work demonstrates that SLAT can be directly transferredwithout retrainingto various downstream tasks, including 3D editing [37, 87], stylization [55], correspondence estimation [15], articulated object modeling [5], scene generation [13], and world synthesis [6, 21]. Despite its strong generalization ability, SLAT has not yet been explored for 3D morphing. In this paper, we bridge this gap by analyzing fusion rules of SLAT features in cross-/selfattention blocks, and propose lightweight yet solid modules and strategies that fully unlock the potential of SLAT for high-quality, training-free 3D morphing. 3. Methodology 3.1. Preliminaries Trellis [81] is robust feed-forward 3D generative model that produces diverse, high-fidelity 3D assets from text or image prompts. Its core encodes 3D models into the Structured LATent (SLAT) representation, i.e., set of local latent vectors {(zi, pi)}L i=1 anchored at active sparse vox3 init and tgt Figure 3. (a) Overview of our method. MorphAny3D generates smooth and high-quality morphing sequence between diverse object categories by leveraging the SLAT representation without any training. (b) Morphing Cross-Attention (MCA) fuses information from the source and target objects in the cross-attention layers to ensure the structural coherence and aesthetics of the deformation. (c) TemporalFused Self-Attention (TFSA) enhances temporal smoothness by incorporating SLAT features from the previous morphing frame into the self-attention mechanism, enabling smooth transitions over time. (d) An orientation correction strategy inspired by statistical orientation distribution patterns in Trellis-generated assets is proposed to resolve abrupt orientation shifts. els pi on the object surface, where each zi RC captures fine-scale geometry and appearance. Trellis employs two-stage generation pipeline based on rectified flow models [45] tailored for SLAT generation: (1) Sparse Structure (SS) Stage: SS flow transformer estimates 643 voxel grid to identify the sparse structure = {pi}L i=1 representing the global shape of assets. (2) Structured Latent (SLAT) Stage: SLAT flow transformer predicts the local latent vectors = {zi}L i=1 for the voxels identified in the SS stage, producing rich geometry and texture details. Finally, the generated SLAT is decoded into standard 3D representations such as meshes, NeRF [49], or 3DGS [33]. In this work, we adopt the image-to-3D variant of Trellis [81], as image conditions facilitate modeling finer details. Notably, as shown in Sec. 4.5, our method generalizes seamlessly to other SLAT-based models with different input modalities. Attention [72] mechanism is fundamental component in modern generative models [48, 54] and is formulated as: be real 3D assets or Trellis-synthesized. For real assets, we obtain their initial noised latents src init, and image conditions csrc, ctgt via 3D inversion [37]. Here, the latent = (fss, fslat) for each object corresponds to Trelliss SS and SLAT stages. For Trellis-generated assets, we reuse cached initial features and conditions from the original generation. The initial noisy feature for frame n, init, is computed by spherical interpolation [90] with deformation weight αn [0, 1], ensuring x0 = xsrc(α0 = 0) and xN = xtgt(αN = 1). We set = 49 (50 frames) with linearly spaced αn = n/N . Further details are provided in our Supp. Mat. Fig. 3-(a) shows the MorphAny3D overview. Based on observed SLAT feature blending patterns in cross-/self-attention blocks (Sec. 3.3, Fig. 4), we introduce two key components to improve morphing plausibility and temporal coherence: Morphing Cross-Attention (Sec. 3.4, Fig. 3-(b)) and Temporal-Fused Self-Attention (Sec. 3.5, Fig. 3-(c)). We also propose an orientation correction strategy (Sec. 3.6, Fig. 3-(d)) derived from statistical analysis of orientation distributions in Trellis outputs to suppress abrupt pose shifts. Attn(Q, K, ) = Softmax (cid:18) QK dk (cid:19) V, (1) where is the query features derived from the latent feature , while and are the key and value features obtained from either external conditions (cross-attention) or the latent feature itself (self-attention). 3.2. Overview Given source object xsrc and target object xtgt, our goal is to generate smooth, high-quality morphing sequence {xn}N n=0 that gradually transforms xsrc into xtgt using the SLAT representation from Trellis [81]. Both objects may 3.3. SLAT Fusion Patterns in Attention Mechanism As shown in Fig. 2, naively applying SLAT to 3D morphing yields poor transitions, highlighting the need to understand how SLAT features influence the morphing process. Prior work [83, 90] shows that fusing source and target keys and values in attention greatly improves 2D/3D morphing qualFigure 5. Attention maps visualization for different attention mechanisms. Red stars denote SLAT head features; pink stars mark their corresponding input regions. Orange boxes highlight KV-Fused CAs incorrect attention focus. MCA preserves correct, semantically consistent attention and avoids KV-Fused CAs artifacts shown in Fig. 4-(b). ange box of Fig. 4-(b). (2) KV-Fused SA effectively improves the smoothness and continuity of the morphing sequence by aggregating 3D latent features from the source and target objects within self-attention (the lowest PPL). (3) Yet, simultaneously applying KV-fused CA and KVfused SAwhile aiming to maximize both plausibility and smoothnesscompromises the plausibility of the resulting deformation (see Fig. 4-(d)). These results indicate that while SLAT feature blending benefits morphing, naive combination harms the plausibility-smoothness trade-off. To address this, we propose two tailored modulesMorphing Cross-Attention and Temporal-Fused Self-Attentionthat preserve the advantages of fusion while avoiding its pitfalls. 3.4. Morphing Cross-Attention (MCA) We hypothesize that the local irrational structure in KVFused CA is caused by patch-wise blending of source and target 2D semantics, which introduces ambiguous conditions that mislead the generation model. Specifically, the blended keys and values in cross-attention are derived from patch-wise DINOv2 [52] features. However, spatially aligned patches between source and target images often lack semantic correspondence, leading to distorted outputs. To verify this hypothesis, we visualize the crossattention maps of head SLAT features (marked by red stars) in Fig. 5. The attention maps from vanilla CA (first two columns) correctly focus on the head regions in the input conditions (indicated by pink stars). In contrast, KV-Fused CA (third column) erroneously attends to background regions (highlighted by an orange box), subsequently using semantically mismatched features to guide head SLAT generationresulting in local structural distortions. To address this issue, we propose Morphing Cross-Attention (MCA) (Fig. 3-(b)). Instead of blending keys and values prior to the attention computation, MCA computes separate attention outputs for the source and target objects and then comFigure 4. Analysis of SLAT fusion patterns in attention for 3D morphing. (a) FID (plausibility) and PPL (smoothness) comparison. (b, c, d) Qualitative results of different fusion strategies (same case as Fig. 2). ity and continuity: KV-Fused-Attn(Qn, src/tgt, src/tgt) = Attn(cid:0)Qn, (1 αn)K src + αnK tgt, (1 αn)V src + αnV tgt(cid:1), (2) where Qn comes from frame-n latent features n, and src/tgt and src/tgt are derived from image conditions (cross-attention) or latent features (self-attention) of source and target objects, respectively. This fusion is shared across all diffusion timesteps and generation stages; we omit corresponding indices for brevity. Given the poor performance of naive SLAT interpolation and the success of KV fusion, we extend this strategy to Trelliss cross-attention (CA) and self-attention (SA), yielding KV-Fused CA and KV-Fused SA. We quantitatively evaluate methods on test set using FID (lower = more plausible) and PPL (lower = smoother) (Fig. 4-(a); see Sec. 4.1 for evaluation details) and qualitatively compare fusion strategies in Fig. 4-(b, c, d). From these results, we draw three key observations: (1) KV-Fused CA significantly enhances the structural and semantic plausibility of 3D morphing by fusing 2D semantic information from the source and target objects within cross-attention (the lowest FID). However, it fails to eliminate localized implausible artifacts, as shown in the or5 bines them to produce the final feature CA: MCA(Qn, src/tgt, src/tgt) = (1 αn)Attn(cid:0)Qn, src, src(cid:1) + αnAttn(cid:0)Qn, tgt, tgt(cid:1). (3) As shown in the last column of Fig. 5, MCA preserves accurate attention on semantically consistent regions by processing source and target features independently, thereby avoiding the artifacts observed in KV-Fused CA. This improvement is further corroborated in Sec. 4.3. 3.5. Temporal-Fused Self-Attention (TFSA) Although MCA yields structurally coherent and visually plausible deformations, temporal smoothness remains limited due to the frame-wise independence inherent in standard generative pipelines. To address this limitationand building on insights from KV-Fused SA discussed in Sec. 3.3we propose Temporal-Fused SelfAttention (TFSA). As illustrated in Fig. 3-(c), TFSA promotes temporally consistent 3D morphing by incorporating features from previously generated frames into the selfattention mechanism. Specifically, when generating the frame, TFSA fuses the keys and values of the current frame with those from the immediately preceding frame to produce the output feature F SA: TFSA(Qn, n, n, n1, n1) = (1 β)Attn(cid:0) Qn, n, n(cid:1) + βAttn(cid:0)Qn, n1, n1), (4) where β [0, 1] controls the influence of the prior frame. We empirically set β = 0.2. Unlike the KV-Fused SA in Fig. 4-(e, f)which blends source and target featuresTFSA fuses features from already plausible neighboring morphing frames, preserving semantic fidelity while enhancing smoothness, as shown in Sec. 4.3 and Fig. 8-(b). 3.6. Orientation Correction Morphing often suffers from abrupt orientation changes (i.e., orientation jumps)especially in intermediate stagescausing visually jarring transitions (Fig. 6-(a)). We represent orientation via the Z-Y-X Euler angles = (yaw, pitch, roll) (in degrees), estimated using OrientAnything [75]. An orientation jump is flagged when the adjacent-frame angular difference exceeds 45, threshold accounting for estimation noise. Analyzing 200 sequences (50 frames each) generated with MCA and TFSA, we find that jumps concentrate around α 0.5 (Fig. 6-(b)), where the conditional features are the most ambiguous. Moreover, is strongly biased toward yaw rotations of 90, 180, and 270, with negligible pitch/roll changes (Fig. 6-(c)), suggesting systematic bias rather than randomness. We hypothesize that this stems from Trelliss internal pose prior. To verify, we estimate the 6 Figure 6. (a) Example of abrupt orientation change during mor- (b) Distribution of α at orientation jumps, peaking near phing. intermediate stages. (c) Adjacent-frame orientation changes at orientation jumps, dominated by 90, 180, and 270 yaw shifts. (d) Orientation distribution of Trellis-generated objects, showing non-canonical poses clustered at the same yaw angles. orientations of 1,000 Trellis-generated objects (Fig. 6-(d)): while most adopt canonical poses, non-canonical ones cluster precisely at the same yaw angles (see cases in Fig. 6-(d)- Yaw)confirming the link between orientation jumps and Trelliss learned pose distribution. 90 , Based on these observations, we propose lightweight orientation correction strategy to mitigate abrupt orientation changes (Fig. 3-(d)). After generating the sparse structure in the SS stage, we create four yaw-rotated candidates: n, 180 , and 270 . We select the candidate that minimizes the Chamfer Distance (CD) to the previous frames structure n1 as the corrected structure ˆP n, which is then passed to the SLAT flow transformer. Since jumps occur mainly mid-sequence (Fig. 6-(b)), this leverages early stable poses to correct later errors. When no jump occurs, the unrotated yields the lowest CD and is retainedensuring non-intrusiveness. As shown in Sec. 4.3, this significantly improves temporal smoothness without sacrificing fidelity. 4. Experiments Due to space limits, we report key settings and results; full details are in the Supp. Mat. We render RGB views from 3DGS and normal maps from Mesh representations. 4.1. Experimental Settings Implementation Details. Our method uses Image-to-3D Trellis [81] without retraining or hyperparameter tuning. Experiments were conducted on single A6000 GPU. Generating each frame takes 30s with 24GB of memory. Baselines. We compare four types: (1) Matching-based Figure 7. Qualitative comparisons. MorphAny3D generates smooth, high-quality 3D morphing sequences across diverse object categories. More results are provided in our Supp. Mat. 3D morphing of 3D or SLAT features (3DInterp, SLATInterp, and use DenseMatcher [94] for correspondence); (2) 2D morphing (DiffMorpher [90], FreeMorph [4]) is lifted to 3D via Trellis; (3) Direct interpolation of noise and conditions (DirectInterp); (4) Modern 3D morphing methodMorphFlow [70]. Since 3DMorpher [83] has not released its full code, we provide only qualitative comparison in our Supp. Mat. Evaluation Metrics. We evaluate 50 diverse source-target pairs from real 3D datasets [14, 37] and Trellis-generated (a) FID [29] measures visual assets using four metrics: plausibility by comparing rendered frames to originals; (b) Perceptual Path Length (PPL) and Perceptual Distance Variance (PDV) [90] assess transition smoothness and temporal homogeneity via average perceptual change and its variance between adjacent frames; (c) Aesthetics Scores (AS) uses vision-language models [26, 51] to rank visual appeal; (d) User Preference (UP) is gathered through user study evaluating quality, smoothness, and realism. 4.2. Evaluation Qualitative Results. As shown in Fig. 7, MorphAny3D generates smooth, high-quality 3D morphing sequences across diverse categories, outperforming all baselines. Matching-based methods (e.g., 3DInterp, SLATInterp, Table 1. Quantitative comparison. Best and second-best in bold and underlined. Method 3DInterp [94] SLATInterp [94] DiffMorpher [90] FreeMorph [4] DirectInterp MorphFlow [70] MorphAny3D FID 409.14 348.31 208.08 164.68 150.94 284.96 111.95 PPL 2.55 6.53 6.65 5.89 3.72 2.41 2.47 PDV AS (%) UP (%) 0.0006 0.0010 0.0021 0.0027 0.0039 0.0009 0.0006 1.00 0.00 5.00 11.00 2.00 0.00 81.00 0.61 1.43 0.82 3.27 5.51 1.63 86. MorphFlow) yield smooth, but often implausible or semantically incoherent transitions. 2D-first approaches (DiffMorpher, FreeMorph) leverage strong 2D priors for structural plausibility but suffer from poor 3D temporal consistency, leading to jerky sequences. DirectInterps feature fusion is ill-suited to SLAT, resulting in suboptimal deformations. In contrast, our method integrates SLAT features with Trelliss 3D generative priors to produce temporally smooth, semantically meaningful, and visually faithful morphs. For example, in the elephant-to-excavator case  (Fig. 7)  , it implicitly aligns the trunk with the boom, creating coherent hybrid that seamlessly blends both concepts. Quantitative Results. As shown in Tab. 1, MorphAny3D achieves state-of-the-art performance, obtaining the best scores in FID, PDV, AS, and UP, and the second-best PPL. Matching-based methods achieve smoothness via linear interpolation but suffer from poor plausibility, as evidenced 7 Figure 8. Ablation study on (a) MCA, (b) TFSA, and (c) OC. Table 2. Ablation study on key components of MorphAny3D. Method KV-Fused CA MCA MCA + TFSA FID 125.47 112.18 113.22 MCA + TFSA + OC 111. PPL 3.82 3.66 2.87 2.47 PDV 0.0013 0.0010 0.0007 0.0006 by an extremely high FID. Our method significantly improves plausibility and visual quality with only marginal smoothness trade-offits PPL is just 0.06 above the lowest. 4.3. Ablation Study Effectiveness of MCA. Fig. 8-(a) shows that MCA suppresses local artifacts (blue box) in KV-Fused CA and improves plausibility. FID drops from 125.47 to 112.18 (Tab. 2). Effectiveness of TFSA. Fig. 8-(b) shows improved temporal coherencee.g., stable crab claws and eyes (green/red boxes). PPL and PDV decrease to 2.87 and 0.0007. Effectiveness of Orientation Correction (OC). Fig. 8-(c) shows that OC mitigates orientation jumps. Quantitatively, it further reduces PPL to 2.47 and PDV to 0.0006. 4.4. Applications Disentangled 3D Morphing. By applying our method selectively to Trelliss SS and SLAT stages, we decouple the global structure from local details. Fig. 9-(a) shows sequences that preserve the source structure with target details (or vice versa). Dual-Target 3D Morphing. Assigning different targets to SS and SLAT enables morphing toward two concepts simultaneously. Fig. 9-(b) shows one target shaping structure, the other detail. 3D Style Transfer. Using style image as the SLAT target transfers style while preserving source structure and aesthetics (Fig. 9-(c)). Figure 9. Applications: (a) Disentangled 3D morphing, (b) DualTarget 3D Morphing and (c) 3D Style Transfer. Figure 10. Generalization experiments on (a) Hi3DGen and (b) Text-to-3D Trellis. Figure 11. Failure cases. 4.5. Generalization Ability We apply MorphAny3D to other SLAT-based modelsHi3DGen [86] and Text-to-3D Trellis [81]. As shown in Fig. 10, it consistently produces smooth, high-quality morphs, confirming its versatility. 5. Conclusion We present MorphAny3D, training-free 3D morphing framework based on Trelliss SLAT representation. By analyzing SLAT fusion in attention, we introduce Morphing Cross-Attention (MCA)fusing source and target features for structural and semantic fidelityand Temporal-Fused Self-Attention (TFSA)leveraging prior frames for temporal coherence. We also propose an orientation correction strategy, guided by statistical pose patterns in Trellis out8 puts, to reduce abrupt orientation shifts. Experiments show that our method achieves state-of-the-art results in generating smooth, high-quality 3D morphing sequences across diverse object categories. It further supports advanced applications such as decoupled morphing and 3D style transfer, and readily generalizes to other SLAT-based models. Limitations and Future Work. MorphAny3D inherits Trelliss limitations and may exhibit artifacts on extremely fine structures (see red boxes in Fig. 11). Future work could adopt stronger 3D generative backbones [28] or enhance SLAT representations to capture finer geometric details."
        },
        {
            "title": "References",
            "content": "[1] Inc. Autodesk. Autodesk maya - 3d animation and modhttps : / / www . autodesk . com / eling software. products/maya, 2024. 3 [2] Hadar Averbuch-Elor, Daniel Cohen-Or, and Johannes Kopf. Smooth image sequences for data-driven morphing. In Comput. Graph. Forum, 2016. 2, 3 [3] Dongliang Cao and Florian Bernard. Self-supervised learning for multimodal non-rigid 3d shape matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1773517744, 2023. 3 [4] Yukang Cao, Chenyang Si, Jinghao Wang, and Ziwei Liu. Freemorph: Tuning-free generalized image morphing with In Proc. IEEE Int. Conf. Comput. Vis., diffusion model. 2025. 2, 3, 7 [5] Chuhao Chen, Isabella Liu, Xinyue Wei, Hao Su, and Freeart3d: Training-free articulated obarXiv preprint Minghua Liu. ject generation using 3d diffusion. arXiv:2510.25765, 2025. 3 [6] Hanke Chen, Yuan Liu, and Minchen Li. Trellisworld: Training-free world generation from object generators. arXiv preprint arXiv:2510.23880, 2025. 3 [7] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2025. 3 [8] Yiwen Chen, Zhihao Li, Yikai Wang, Hu Zhang, Qin Li, Chi Zhang, and Guosheng Lin. Ultra3d: Efficient and highfidelity 3d generation with part attention. arXiv preprint arXiv:2507.17745, 2025. [9] Zhaoxi Chen, Jiaxiang Tang, Yuhao Dong, Ziang Cao, Fangzhou Hong, Yushi Lan, Tengfei Wang, Haozhe Xie, Tong Wu, Shunsuke Saito, et al. 3dtopia-xl: Scaling highquality 3d asset generation via primitive diffusion. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2025. 3 [10] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: In Adv. Neural Inf. Process. universe of 10m+ 3d objects. Syst., 2023. [11] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. Comput. Vis. Pattern Recogn., 2023. 3 In Proc. IEEE Conf. [12] Bailin Deng, Yuxin Yao, Roberto Dyke, and Juyong Zhang. survey of non-rigid 3d registration. In Comput. Graph. Forum, 2022. 3 [13] Wenqi Dong, Bangbang Yang, Zesong Yang, Yuan Li, Tao Hu, Hujun Bao, Yuewen Ma, and Zhaopeng Cui. Hiscene: creating hierarchical 3d scenes with isometric view generation. arXiv preprint arXiv:2504.13072, 2025. 2, 3 [14] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), 2022. [15] Keyu Du, Jingyu Hu, Haipeng Li, Hao Xu, Haibing Huang, Chi-Wing Fu, and Shuaicheng Liu. Hierarchical neural semantic representation for 3d semantic correspondence. arXiv preprint arXiv:2509.17431, 2025. 3 [16] Niladri Shekhar Dutt, Sanjeev Muralikrishnan, and Niloy Mitra. Diffusion 3d features (diff3f): Decorating untextured shapes with distilled semantic features. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2024. 3 [17] Michal Edelstein, Danielle Ezuz, and Mirela Ben-Chen. Enigma: Evolutionary non-isometric geometry matching. arXiv preprint arXiv:1905.10763, 2019. 2 [18] Marvin Eisenberger, Zorah Lahner, and Daniel Cremers. Smooth shells: Multi-scale shape registration with functional In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., maps. 2020. 3 [19] Marvin Eisenberger, Aysim Toker, Laura Leal-Taixe, and Daniel Cremers. Deep shells: Unsupervised shape correspondence with optimal transport. In Adv. Neural Inf. Process. Syst., 2020. 3 [20] Marvin Eisenberger, David Novotny, Gael Kerchenbaum, Patrick Labatut, Natalia Neverova, Daniel Cremers, and Andrea Vedaldi. Neuromorph: Unsupervised shape interpoIn Proceedings of lation and correspondence in one go. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74737483, 2021. 2, [21] Paul Engstler, Aleksandar Shtedritski, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Syncity: Training-free generation of 3d worlds. arXiv preprint arXiv:2503.16420, 2025. 3 [22] Inc. Epic Games. Unreal engine - real-time 3d creation tool. https://www.unrealengine.com/, 2024. 3 [23] Noa Fish, Richard Zhang, Lilach Perry, Daniel Cohen-Or, Eli Shechtman, and Connelly Barnes. Image morphing with perceptual constraints and stn alignment. In Comput. Graph. Forum, 2020. 3 [24] Blender Foundation. Blender - 3d modelling and rendering software. https://www.blender.org/, 2024. 3 [25] William Gao, Noam Aigerman, Thibault Groueix, Vova Kim, and Rana Hanocka. Textdeformer: Geometry manipulation using text guidance. In ACM SIGGRAPH 2023 conference proceedings, 2023. 3 [26] Google DeepMind. Gemini: family of multimodal large language models, 2024. 7 [27] Qiyuan He, Jinghao Wang, Ziwei Liu, and Angela Yao. Aid: arXiv Attention interpolation of text-to-image diffusion. preprint arXiv:2403.17924, 2024. 3 [28] Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Sparseflex: High-resolution Cao, and Yangguang Li. and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025. 3, 9 [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Adv. Neural Inf. Process. Syst., 2017. 7 [30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inf. Process. Syst., 2020. 2, 3 [31] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [32] Team Hunyuan3D, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Penghao Wang, et al. Hunyuan3d-omni: unified framework for controllable generation of 3d assets. arXiv preprint arXiv:2509.21245, 2025. 3 [33] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 2023. 3, 4 [34] Zeqiang Lai, Yunfei Zhao, Haolin Liu, Zibo Zhao, Qingxiang Lin, Huiwen Shi, Xianghui Yang, Mingxin Yang, Shuhui Yang, Yifei Feng, et al. Hunyuan3d 2.5: Towards highfidelity 3d assets generation with ultimate details. arXiv preprint arXiv:2506.16504, 2025. 3 [35] Yushi Lan, Shangchen Zhou, Zhaoyang Lyu, Fangzhou Hong, Shuai Yang, Bo Dai, Xingang Pan, and Chen Change Loy. Gaussiananything: Interactive point cloud latent diffusion for 3d generation. In Proc. Int. Conf. Learn. Represent., 2025. 3 [36] Seung-Yong Lee, KYUNG-YONG CHWA, James Hahn, and Sung Yong Shin. Image morphing using deformation techniques. The Journal of Visualization and Computer Animation, 1996. 2, 3 [37] Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, and Lu Sheng. Voxhammer: Training-free precise and coherent 3d editing in native 3d space. arXiv preprint arXiv:2508.19247, 2025. 2, 3, 4, 7 [38] Mengtian Li, Yunshu Bai, Yimin Chu, Yijun Shen, Zhongmei Li, Weifeng Ge, Zhifeng Xie, and Chaofeng Chen. Gaussianmorphing: Mesh-guided 3d gaussians for semanticaware object morphing. arXiv preprint arXiv:2510.02034, 2025. 2, 3 [39] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman3d: High-fidelity mesh generation with 3d native genarXiv preprint eration and interactive geometry refiner. arXiv:2405.14979, 2024. [40] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape arXiv synthesis using large-scale rectified flow models. preprint arXiv:2502.06608, 2025. [41] Zhihao Li, Yufei Wang, Heliang Zheng, Yihao Luo, and Bihan Wen. Sparc3d: Sparse representation and construction for high-resolution 3d shapes modeling. arXiv preprint arXiv:2505.14521, 2025. 3 [42] Jing Liao, Rodolfo Lima, Diego Nehab, Hugues Hoppe, Pedro Sander, and Jinhui Yu. Automating image morphing using structural similarity on halfway domain. ACM Trans. Graph., 2014. 3 [43] Jianchu Lin, Yinxi Gu, Guangxiao Du, Guoqiang Qu, Xiaobing Chen, Yudong Zhang, Shangbing Gao, Zhen Liu, and Nallappan Gunasekaran. 2d/3d image morphing technology from traditional to modern: survey. Information Fusion, 2024. 2, 3 [44] Haolin Liu, Xiaohang Zhan, Zizheng Yan, Zhongjin Luo, Yuxin Wen, and Xiaoguang Han. Stable-score: stable registration-based framework for 3d shape correspondence. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 917928, 2025. [45] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3, 4 [46] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 3 [47] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2024. 2, 3 [48] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In Proc. Eur. Conf. Comput. Vis., 2024. 3, 4 [49] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proc. Eur. Conf. Comput. Vis., 2020. 3, 4 [50] Luca Morreale, Noam Aigerman, Vladimir Kim, and Niloy Mitra. Neural semantic surface maps. In Comput. Graph. Forum, 2024. [51] OpenAI. ChatGPT: Optimizing language models for dialogue, 2023. 7 [52] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3, 5 [53] Maks Ovsjanikov, Mirela Ben-Chen, Justin Solomon, Adrian Butscher, and Leonidas Guibas. Functional maps: flexible representation of maps between shapes. ACM Trans. Graph., 2012. 3 10 [54] William Peebles and Saining Xie. Scalable diffusion models In Proc. IEEE Int. Conf. Comput. Vis., with transformers. 2023. 3, 4 [55] Zefan Qu, Zhenwei Wang, Haoyuan Wang, Ke Xu, Gerhard Hancke, and Rynson WH Lau. Stylesculptor: Zero-shot style-controllable 3d asset generation with texture-geometry dual guidance. arXiv preprint arXiv:2509.13301, 2025. 2, 3 [56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Mach. Learn., 2021. 3 [57] Jing Ren, Mikhail Panine, Peter Wonka, and Maks Ovsjanikov. Structured regularization of functional map computations. In Comput. Graph. Forum, 2019. 2, 3 [58] Emanuele Rodola, Samuel Rota Bulo, Thomas Windheuser, Matthias Vestner, and Daniel Cremers. Dense non-rigid shape correspondence using random forests. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2014. 3 [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2022. 2, 3 [60] Yusuf Sahillioglu. genetic isometric shape correspondence algorithm with adaptive sampling. ACM Trans. Graph., 2018. [61] Yusuf Sahillioglu. Recent advances in shape correspondence. The Visual Computer, 2020. 3 [62] Eli Shechtman, Alex Rav-Acha, Michal Irani, and Steve Seitz. Regenerative morphing. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2010. 3 [63] Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. ACM Trans. Graph., 2015. 2, 3 [64] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2, and Stefano Ermon. arXiv preprint [65] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2, 3 [66] Mingze Sun, Chen Guo, Puhua Jiang, Shiwei Mao, Yurun Chen, and Ruqi Huang. Srif: Semantic shape registration empowered by diffusion-based image morphing and flow In SIGGRAPH Asia 2024 Conference Papers, estimation. 2024. 3 [67] Gary KL Tam, Zhi-Quan Cheng, Yu-Kun Lai, Frank Langbein, Yonghuai Liu, David Marshall, Ralph Martin, XianFang Sun, and Paul Rosin. Registration of 3d point clouds and meshes: survey from rigid to nonrigid. IEEE Trans. Vis. Comput. Graph., 2012. 2, 3 [68] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In Proc. Eur. Conf. Comput. Vis., 2024. 2, 3 [69] Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from single image. arXiv preprint arXiv:2403.02151, 2024. 3 [70] Chih-Jung Tsai, Cheng Sun, and Hwann-Tzong Chen. Multiview regenerative morphing with dual flows. In Proc. Eur. Conf. Comput. Vis., 2022. 2, 3, 7 [71] Oliver Van Kaick, Hao Zhang, Ghassan Hamarneh, and In Daniel Cohen-Or. survey on shape correspondence. Comput. Graph. Forum, 2011. 2, 3 [72] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Adv. Neural Inf. Process. Syst., 2017. 4 [73] Clinton Wang and Polina Golland. tween images with diffusion models. arXiv:2307.12560, 2023. 3 Interpolating bearXiv preprint [74] Zhengyi Wang, Yikai Wang, Yifei Chen, Chendong Xiang, Shuo Chen, Dajiang Yu, Chongxuan Li, Hang Su, and Jun Zhu. Crm: Single image to 3d textured mesh with convolutional reconstruction model. In Proc. Eur. Conf. Comput. Vis., 2024. 3 [75] Zehan Wang, Ziang Zhang, Tianyu Pang, Chao Du, Hengshuang Zhao, and Zhou Zhao. Orient anything: Learning robust object orientation estimation from rendering 3d models. arXiv preprint arXiv:2412.18605, 2024. 6 [76] George Wolberg. Recent advances in image morphing. Proceedings of CG International96, 1996. 2 [77] George Wolberg. computer, 1998. 2 Image morphing: survey. The visual [78] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In Adv. Neural Inf. Process. Syst., 2024. 3 [79] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. In Adv. Neural Inf. Process. Syst., 2024. 2 [80] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation arXiv preprint made easy with spatial sparse attention. arXiv:2505.17412, 2025. [81] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2025. 2, 3, 4, 6, 8 [82] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. 3 [83] Songlin Yang, Yushi Lan, Honghua Chen, and Xingang Pan. Textured 3d regenerative morphing with 3d diffusion prior. arXiv preprint arXiv:2502.14316, 2025. 2, 3, 4, [84] Xianghui Yang, Huiwen Shi, Bowen Zhang, Fan Yang, Jiacheng Wang, Hongxu Zhao, Xinhai Liu, Xinzhou Wang, Qingxiang Lin, Jiaao Yu, et al. Hunyuan3d 1.0: unified framework for text-to-3d and image-to-3d generation. arXiv preprint arXiv:2411.02293, 2024. 3 [85] Zhaoyuan Yang, Zhengyang Yu, Zhiwei Xu, Jaskirat Singh, Jing Zhang, Dylan Campbell, Peter Tu, and Richard Hartley. Image morphing with perceptuallyuniform sampling using diffusion models. arXiv preprint arXiv:2311.06792, 2023. 3 Impus: [86] Chongjie Ye, Yushuang Wu, Ziteng Lu, Jiahao Chang, Xiaoyang Guo, Jiaqing Zhou, Hao Zhao, and Xiaoguang Han. Hi3dgen: High-fidelity 3d geometry generation from images via normal bridging. In Proc. IEEE Int. Conf. Comput. Vis., 2025. 3, 8 [87] Junliang Ye, Shenghao Xie, Ruowen Zhao, Zhengyi Wang, Hongyu Yan, Wenqiang Zu, Lei Ma, and Jun Zhu. Nano3d: training-free approach for efficient 3d editing without masks. arXiv preprint arXiv:2510.15019, 2025. 2, 3 [88] Biao Zhang, Jiapeng Tang, Matthias Nießner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Trans. Graph., 2023. 2, 3 [89] Bowen Zhang, Yiji Cheng, Jiaolong Yang, Chunyu Wang, Feng Zhao, Yansong Tang, Dong Chen, and Baining Guo. Gaussiancube: structured and explicit radiance reparXiv preprint resentation for 3d generative modeling. arXiv:2403.19655, 2024. [90] Kaiwen Zhang, Yifan Zhou, Xudong Xu, Bo Dai, and Xingang Pan. Diffmorpher: Unleashing the capability of diffusion models for image morphing. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2024. 2, 3, 4, 7 [91] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Trans. Graph., 2024. 2, 3 [92] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In Adv. Neural Inf. Process. Syst., 2023. [93] Zibo Zhao, Zeqiang Lai, Qingxiang Lin, Yunfei Zhao, Haolin Liu, Shuhui Yang, Yifei Feng, Mingxin Yang, Sheng Zhang, Xianghui Yang, et al. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202, 2025. 2, 3 [94] Junzhe Zhu, Yuanchen Ju, Junyi Zhang, Muhan Wang, Zhecheng Yuan, Kaizhe Hu, and Huazhe Xu. Densematcher: Learning 3d semantic correspondence for categoryarXiv preprint level manipulation from single demo. arXiv:2412.05268, 2024. 3, 7 [95] Lei Zhu, Yan Yang, Steven Haker, and Allen Tannenbaum. An image morphing technique based on optimal mass preserving mapping. IEEE Trans. Image Process., 2007."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Peking University"
    ]
}