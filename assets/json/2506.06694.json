{
    "paper_title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning",
    "authors": [
        "Yuan Yuan",
        "Yukun Liu",
        "Chonghua Han",
        "Jie Feng",
        "Yong Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, a scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from a frozen teacher model, and reinforces knowledge retention through a tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a mobility-aware expert routing mechanism, and employs a layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks a crucial step toward unlocking foundation models for mobility, offering a practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models."
        },
        {
            "title": "Start",
            "content": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning Yuan Yuan, Yukun Liu, Chonghua Han, Jie Feng, Yong Li Tsinghua University liyong07@tsinghua.edu.cn 5 2 0 2 7 ] . [ 1 4 9 6 6 0 . 6 0 5 2 : r Abstract Foundation models have revolutionized fields such as natural language processing and computer vision by enabling general-purpose learning across diverse tasks and datasets. However, building analogous models for human mobility remains challenging due to the privacy-sensitive nature of mobility data and the resulting data silos across institutions. To bridge this gap, we propose MoveGCL, scalable and privacy-preserving framework for training mobility foundation models via generative continual learning. Without sharing raw data, MoveGCL enables decentralized and progressive model evolution by replaying synthetic trajectories generated from frozen teacher model, and reinforces knowledge retention through tailored distillation strategy that mitigates catastrophic forgetting. To address the heterogeneity of mobility patterns, MoveGCL incorporates Mixture-of-Experts Transformer with mobility-aware expert routing mechanism, and employs layer-wise progressive adaptation strategy to stabilize continual updates. Experiments on six real-world urban datasets demonstrate that MoveGCL achieves performance comparable to joint training and significantly outperforms federated learning baselines, while offering strong privacy protection. MoveGCL marks crucial step toward unlocking foundation models for mobility, offering practical blueprint for open, scalable, and privacy-preserving model development in the era of foundation models. Keywords Human mobility, foundation models, continual learning"
        },
        {
            "title": "1 Introduction\nIn natural language processing (NLP) [4, 5, 17] and computer vi-\nsion (CV) [8, 27], the rise of large pre-trained foundation models\n(e.g., GPT, Sora) [33] has significantly advanced model sharing and\ngeneral-purpose intelligence through centralized training and open\nrelease. However, in the critical domain of human mobility model-\ning [3, 30], the era of large foundation models has not yet arrived.\nThis is primarily due to the highly sensitive nature of mobility\ntrajectory data, which involves personal privacy [19, 47, 49]. Such\ndata cannot be easily shared or jointly trained across institutions\ndue to privacy constraints and legal restrictions. As a result, mo-\nbility datasets often exist in isolated silos [50, 56], with different\norganizations and researchers relying on their own private datasets,\nhindering data integration, benchmarking, and collaborative model\ndevelopment.",
            "content": "To address the challenge of data silos in mobility modeling, recent studies have proposed different training strategies to leverage multiple datasets, and Figure 1 compares different approaches. The Both authors contributed equally to this work. most common strategy is to train models separately, as shown in Figure 1(a). Recent efforts such as UniTraj [57], TrajBert [37], and TrajFM [24] have explored joint training for unified representation and cross-city generalization (Figure 1(b)), but these models remain tightly coupled with proprietary or limited-quality datasets. TrajFM and TrajBert are typically pre-trained on restricted or private data. While UniTraj uses public data, it suffers from low semantic richness and high sparsity. Federated learning [11, 28] offers potential solution for distributed mobility model training (Figure 1(c)), but its reliance on frequent synchronization and communication poses challenges for scalability and practical deployment. Consequently, these approaches fall short of meeting the diverse, dynamic, and multi-source demands of real-world mobility modeling, and cannot support an open ecosystem of shared models as seen in NLP and CV domains. To truly usher in new era of shareable and sustainable human mobility foundation models, we argue for new collaborative paradigm. This paradigm enables multiple data holders to jointly evolve and continually build foundation model without sharing raw data, while preserving both privacy and generalization capability. However, this vision poses several critical challenges: (1) Privacy constraints. The sensitive nature of mobility trajectory data prohibits direct data sharing. Designing training framework that supports collaboration without exposing raw data is fundamental yet unresolved challenge. (2) Catastrophic forgetting. Without access to past training data, the model can easily forget previously learned knowledge when updated with new mobility data, hindering long-term model evolution. (3) Data heterogeneity. Mobility data exhibits substantial variation across regions, populations, and data sources. practical model must generalize well and dynamically adapt to such heterogeneity. To address these challenges, we propose MoveGCL, scalable training framework for mobility foundation models based on generative continual learning. MoveGCL allows each data holder to evolve shared model locally without exposing raw data, thereby ensuring full privacy preservation. Specifically, MoveGCL starts from pre-trained base model and employs synthetic trajectory replay mechanism: instead of accessing historical data, each participant generates synthetic trajectories that approximate previously seen mobility patterns. This replay process preserves prior knowledge and mitigates catastrophic forgetting. Furthermore, knowledge distillation is applied during replay to reinforce the models ability to retain past capabilities while adapting to new data. To handle the diversity of mobility data, MoveGCL adopts Mixture-ofExperts (MoE) architecture equipped with mobility pattern-aware expert routing mechanism. This design enables the model to dynamically select expert modules tailored to local mobility characteristics. Together, these innovations make MoveGCL practical and Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Figure 1: Method comparison with multiple mobility datasets. privacy-preserving solution for collaboratively building generalizable mobility foundation models across distributed, heterogeneous, and privacy-sensitive data sources. In summary, our key contributions are as follows: We are the first to formalize privacy-preserving collaborative training paradigm towards mobility foundation models, enabling decentralized model evolution without raw data sharing. We propose MoveGCL, novel framework based on generative continual learning. Its core componentsknowledge distillation, mobility-aware expert routing, and layer-wise progressive adaptationenable privacy-preserving, scalable, and adaptive trajectory model training across diverse data sources. Extensive experiments demonstrate that MoveGCL achieves performance comparable to joint training with full data access, and significantly outperforms federated learning baselines without data sharing."
        },
        {
            "title": "2.2 Mobility Foundation Models\nInspired by the success of foundation models in NLP and CV, recent\nefforts have explored pre-trained models for urban and mobility\ndomains [6, 52, 55]. Early studies primarily focused on aggregated",
            "content": "mobility data, leveraging mobility flows across cities to build unified spatio-temporal representations, and have demonstrated strong zero-shot transfer capabilities [21, 22, 46]. In contrast, individuallevel mobility foundation models are less developed. Researchers have explored multi-scale mobility modeling [29, 47, 53], aiming to capture both micro-level behaviors and macro-level patterns essential for generalization. Attempts such as UniTraj [57], TrajBert [37], and TrajFM [24] have explored learning from open trajectory datasets, but these datasets often consist of short-term or non-representative mobility traces that do not reflect regular human movement patterns. As result, current models struggle to capture the full complexity and diversity of real-world individual mobility. Recently, LLMs have also widely utilized in generating human mobility [9, 1315], but the gap between natural language and trajectory data suggests that mobility still requires native foundation models, which can later be aligned with LLMs to bridge symbolic reasoning and physical behavior modeling."
        },
        {
            "title": "3 Preliminaries\n3.1 Data Format\nIn our setting, human mobility data is represented as sequences of\nspatiotemporal tokens, where each token corresponds to a visited\nlocation at a specific time. The spatial domain is typically discretized\ninto a uniform grid (500m Ã— 500m resolution), and the temporal\ndomain is segmented into fixed-length intervals (30 minutes). Each",
            "content": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning Conference acronym XX, June 0305, 2018, Woodstock, NY individual trajectory can be formulated as sequence ğ‘§1, ğ‘§2, . . . , ğ‘§ğ‘‡ , where ğ‘§ğ‘¡ = (ğ‘™ğ‘¡ , ğ‘¡ğ‘¡ ) denotes the location and timestamp of mobility event at time step ğ‘¡. These sequences capture rich behavioral patterns across time and space and form the foundation for model training."
        },
        {
            "title": "3.2 Model Training via Next-Token Prediction\nFollowing the standard practice in language modeling, the training\nobjective for mobility foundation models is formulated as a next-\nlocation prediction task. Given a partial trajectory {ğ‘§1, . . . , ğ‘§ğ‘¡ âˆ’1},\nthe model aims to predict the next location ğ‘™ğ‘¡ , where ğ‘™ğ‘¡ repre-\nsents the spatial component of the upcoming step in the trajec-\ntory. Formally, the training objective is defined as maximizing the\nlog-likelihood of the observed sequence:",
            "content": "L = ğ‘‡ ğ‘¡ =1 log ğ‘ƒ (ğ‘™ğ‘¡ ğ‘§1, . . . , ğ‘§ğ‘¡ 1; ğœƒ ), (1) where ğœƒ denotes the model parameters. This objective allows the model to learn rich dependencies across spatial locations, temporal patterns, and contextual semantics, and serves as the core pretraining strategy for mobility foundation models."
        },
        {
            "title": "Model Development",
            "content": "We adopt continual learning paradigm to train the mobility foundation model. To simulate learning on highly heterogeneous data, each round of continual learning introduces the dataset of new city to the model. Initially, the base model ğ‘“base is trained using the base dataset Dbase. During continual learning, at the beginning of the ğ‘›-th round, the model has already been trained on the dataset Dall,ğ‘›1 = Dbase Dcontinual,ğ‘›1, where Dcontinual,ğ‘›1 = (cid:208)ğ‘›1 ğ‘‘ğ‘– , and each ğ‘‘ğ‘– represents the dataset introduced in the ğ‘–-th ğ‘–=1 round. However, the historical dataset Dall,ğ‘›1 is no longer accessible. The model update at round ğ‘› is performed solely based on the current data ğ‘‘ğ‘› and copy of the model from the previous round, denoted as ğ‘“old,ğ‘›.The continual learning process can be formalized as the following optimization objective: (cid:0)ğœƒ ; ğ‘‘ğ‘›; ğ‘“old,ğ‘› (cid:1) , ğœƒ ğœƒold,ğ‘›, ğœƒnew,ğ‘› = arg min ğœƒ s.t. (2) where ğœƒ denotes the model parameters, and ğœƒold,ğ‘› and ğœƒnew,ğ‘› correspond to the parameters of ğ‘“old,ğ‘› and ğ‘“new,ğ‘›, respectively. The loss functionL (ğœƒ ; ğ‘‘ğ‘›; ğ‘“old,ğ‘›) incorporates constraint terms derived from the previous model ğ‘“old,ğ‘› to mitigate catastrophic forgetting. Rather than reinitializing from scratch, we optimize ğœƒnew,ğ‘› starting from ğœƒold,ğ‘›. Specifically, when ğ‘› = 0, we set ğ‘“old,ğ‘› = ğ‘“base; for ğ‘› > 0, we have ğ‘“old,ğ‘› = ğ‘“new, ğ‘›1."
        },
        {
            "title": "4 MoveGCL\nIn this section, we introduce the overall framework of MoveGCL,\nwhich is shown in Figure 2. MoveGCL is built upon the paradigm\nof generative continual learning, as detailed in Section 4.1. We then\npresent the core model architecture in Section 4.2, which is designed\nto support modular scalability and cross-city adaptability. Finally,\nwe elaborate on the training strategy in Section 4.3, including the",
            "content": "design of layer-wise progressive adaptation mechanism to stabilize continual updates and mitigate forgetting."
        },
        {
            "title": "4.1 Generative Continual Learning\nGenerative Replay with Teacher Model. To retain knowledge\nfrom previously visited cities without storing real-world mobility\ntrajectory data, we design a generative replay strategy, as illustrated\nin Figure 2(a). At each stage, we keep a copy of the previously\ntrained model ğ‘“old, referred to as the teacher model. This teacher\nmodel represents the model trained on earlier mobility datasets. It\nremains frozen during subsequent learning and serves as a knowl-\nedge source to guide the student model ğ‘“new when learning new\ncities.",
            "content": "ğ‘¥new = (cid:2)(ğ‘™ 0 To simulate past mobility behaviors, we employ ğ‘“old to generate synthetic trajectory data ğ‘¥ğ‘ğ‘– old. First, we extract trajectory , ğ‘¡ 0), (ğ‘™ 1), . . . , (ğ‘™ 1 ğ‘– )}ğ¿ from the new dataset, where ğ¿ denotes its length and {(ğ‘™ ğ‘–=0 are the locationtime pairs. Next, for specific previously observed city ğ‘ğ‘– , we sample an initial location from the empirical distribution of actual locations in city ğ‘ğ‘– conditioned on length ğ¿: ğ¿, ğ‘¡ ğ¿)(cid:3), ğ‘– , ğ‘¡ , ğ‘¡ (3) ğ‘™0 ğœŒğ‘ğ‘– locğ¿ , (4) where ğœŒğ‘ğ‘– locğ¿ denotes the empirical distribution of initial locations in previously observed city ğ‘ğ‘– given trajectory length ğ¿. We then replace ğ‘™ 0 with the sampled ğ‘™0 and generate the pseudo old-city trajectory ğ‘¥ğ‘ğ‘– old 0), (ğ‘™1, ğ‘¡ = (cid:2)(ğ‘™0, ğ‘¡ ğ¿)(cid:3), 1), . . . , (ğ‘™ğ¿, ğ‘¡ by drawing subsequent locations autoregressively: ğ‘– }ğ¿ ğ‘–=0 (ğ‘™1, ğ‘™2, . . . , ğ‘™ğ¿) ğ‘“old (cid:12) (cid:12) ğ‘™0, {ğ‘¡ (cid:16) (cid:17) , (5) where each ğ‘¡ new-data trajectory. ğ‘– is taken from the time distribution of the extracted These pseudo-trajectories reflect the mobility patterns learned in earlier stages. We combine all pseudo-trajectories with the real data from the current city, to construct the full training set: Dtrain = ğ‘ (cid:216) ğ‘–=1 ğ›¼ ğ‘ğ‘– old Xnew , (6) where {ğ‘1, ğ‘2, . . . , ğ‘ğ‘ } denotes the ğ‘ previously observed cities, ğ‘ğ‘– old denotes the set of pseudo-trajectories generated for city ğ‘ğ‘– , and Xnew is the set of real trajectories from the current city. Coefficient ğ›¼ > 0 specifies ratio between the number of pseudo-trajectories in each previous city and the number of real trajectories in Xnew. This allows the student model to learn current-city behaviors while retaining knowledge of previously learned cities. Distilling Knowledge to Preserve Mobility Patterns. To further strengthen the models ability to preserve prior knowledge, we introduce knowledge distillation loss that transfers behavioral patterns from the teacher model to the student model, as depicted in Figure 2(b). For each generated pseudo-trajectory ğ‘¥old, we extract the predicted mobility distributions from both models: ğ‘ƒold ( ğ‘¥old) = ğ‘“old ( ğ‘¥old), ğ‘ƒnew ( ğ‘¥old) = ğ‘“new ( ğ‘¥old). (7) Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Figure 2: Overview of the MoveGCL framework: (a) the overall workflow; (b) the implementation of generative continual learning; (c) the model architecture. We minimize the KullbackLeibler (KL) divergence between the teachers and students predicted distributions: LKD = ğ‘¥oldğ‘“old [KL (ğ‘ƒold ( ğ‘¥old) ğ‘ƒnew ( ğ‘¥old))] . (8) For the new data ğ‘¥new, we compute the task loss as the crossentropy between the models predicted distribution and the true labels, referred to as Lcross-entropy. The total training objective of the student model is weighted sum of the task loss on new-city data and the distillation loss: Ltotal = Lcross-entropy + ğœ† LKD, where ğœ† is hyperparameter that balances learning new knowledge and retaining previously acquired behaviors. (9)"
        },
        {
            "title": "4.2 Model Architecture\nTo enable scalable and adaptive learning across heterogeneous\nurban environments, our model is designed with a modular ar-\nchitecture that integrates flexible location encoders/decoders, a\nMixture-of-Experts (MoE) Transformer backbone, and a mobility-\naware expert routing mechanism, as shown in Figure 2(c). This\ndesign ensures the modelâ€™s scalability and adaptability in multi-city\ncontinual learning scenarios.",
            "content": "Unified Location Encoder. Conventional location representations often rely on discrete location IDs, which are inherently cityspecific and hinder cross-city generalization. To overcome this limitation, we design continuous location representation that embeds each location into shared latent space, capturing transferable semantic and spatial properties across cities. This unified representation facilitates knowledge sharing and supports incremental learning across heterogeneous urban environments. Concretely, each location ğ‘™ is represented by feature vector zğ‘™ Rğ‘‘ constructed from three key components: zğ‘™ = ğœ™POI (ğ‘™) ğœ™lat-lon (ğ‘™) ğœ™hot (ğ‘™), (10) where ğœ™POI (ğ‘™) Rğ‘‘1 denotes the Point-of-Interest (POI) embedding, capturing semantic land-use attributes such as residential, commercial, educational, or recreational functions, often indicative of mobility intent and purpose; ğœ™hot (ğ‘™) Rğ‘‘2 is the mobility heat embedding, derived from public available OD flows at each location, which reflects the functional centrality of that location; ğœ™lat-lon (ğ‘™) Rğ‘‘3 is the normalized latitude-longitude embedding, representing the relative spatial position of the location within the city boundary. The overall location representation zğ‘™ is obtained via concatenation () of these features, and is further transformed by shared multi-layer perceptron (MLP). This design is sufficient and generalizable because it captures three complementary views of spatial semantics: (1) semantic functions via POI types, (2) actual mobility signals via visitation popularity, and (3) capture where the location sits in the urban layout. Together, they provide compact yet expressive embedding that generalizes well across different cities with varied spatial structures. Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 3: Illustration of the layer-wise progressive adaptation Mixture-of-Experts Transformer. The Mixture-of-Experts (MoE) architecture comprises router network and multiple expert networks, serving as replacement for the Feed-Forward Network (FFN) within the Transformer [31]. The output of the MoE layer, ğ¹MoE (ğ‘¥), is the weighted sum of the selected expert outputs, where the weights are given by the router networks output: ğ¹MoE (ğ‘¥) = ğ‘˜ ğ‘…ğ‘– (ğ‘¥) ğ¸ğ‘– (ğ‘¥). (11) ğ‘–=1 Here, ğ‘¥ denotes the input to the MoE module, ğ‘˜ is the number of selected experts, ğ‘…ğ‘– (ğ‘¥) is the output of the router network for expert ğ‘– (detailed in Section 4.2), ğ¸ğ‘– (ğ‘¥) is the output of expert ğ‘–. MoveGCL is built upon Mixture-of-Experts (MoE) Transformer blocks, in which each expert module is responsible for capturing specific mobility patterns. During continual learning, we introduce new experts to accommodate knowledge from new cities, and design layer-wise progressive adaptation training strategy (detail in Section 4.3). This partial parameter update strategy injects new knowledge without overwriting existing capabilities, thus alleviating catastrophic forgetting. The modularity of the MoE block also supports elastic expansion of the model as more cities are introduced. Mobility-Aware Expert Routing. For each input trajectory ğ‘¥, we extract set of mobility behavior descriptive features and encode them into obility feature descriptor vector zğ‘š Rğ‘‘ . This feature set comprises: the jump distance ğ‘‘jump (distance between the current point and the previous point in the trajectory); the waiting time ğ‘¡wait (time difference between arrivals at the current and previous points in the trajectory); the quantized radius of gyration ğ‘Ÿgyr; the quantized location entropy ğ»loc; and the city identifier IDcity. These features are embedded via their respective encoders, where ğ‘‘jump and ğ‘¡wait are processed by Transformer-based continuous feature encoder, and ğ‘Ÿgyr, ğ»loc, and IDcity are handled by discrete embedding modules. Finally, the five feature embeddings are concatenated to form the mobility behavior vector: zğ‘š = (cid:34)ğœ™ğ‘‘jump (cid:0)ğ‘‘jump (ğ‘¥)(cid:1), ğœ™ğ‘¡wait (cid:0)ğ‘¡wait (ğ‘¥)(cid:1), ğœ™ğ‘Ÿgyr (cid:1) (cid:0)IDcity (cid:0)ğ‘Ÿgyr (ğ‘¥)(cid:1), (cid:35) . (12) ğœ™ğ»loc (cid:0)ğ»loc (ğ‘¥)(cid:1), ğœ™IDcity Here, ğœ™ğ‘‘jump and ğœ™ğ‘¡wait denote the Transformer encoders for continuous mobility features, while ğœ™ğ‘Ÿgyr , ğœ™ğ»loc , and ğœ™IDcity represent the embedding encoders for discrete features. Within each layer of the MoE transformer blocks, we introduce routing network based on linear transformation to compute the routing weights for each expert, leveraging both the mobility descriptor feature vector zğ‘š and the output of the self-attention submodule at that layer. The routing weights for layer ğ‘– are computed as: ğ‘…ğ‘– (ğ‘¥) = softmax (cid:16) TopK(cid:0)ğ‘Šğ‘–,ğ‘Ÿ (zğ‘š ğ‘‹ğ‘– (ğ‘¥)) + ğ‘ğ‘– (cid:1) (cid:17) , (13) where zğ‘š is the mobility feature descriptor vector defined above; ğ‘‹ğ‘– (ğ‘¥) denotes the output of the self-attention submodule; ğ‘Šğ‘–,ğ‘Ÿ is learnable projection matrice; ğ‘ğ‘– is the bias term; and TopK() retains only the top ğ¾ values (corresponding to the ğ¾ highestscoring experts), setting the remaining expert scores to so that their weights after the softmax operation are effectively zero. This routing strategy achieves two objectives: (1) it promotes functional specialization by directing similar motion patterns to consistent expert subsets; (2) it enables the model to discover and transfer shared mobility structures across cities, thereby enhancing Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. generalization in multi-city scenarios. Additionally, this mobilityaware routing provides structured inductive bias that accelerates model adaptation during incremental learning, allowing new experts to specialize rapidly with minimal interference to retained knowledge. SimilarityBased Decoder. The next-location prediction is performed by computing the similarity between the final output of the Mixture-of-Experts (MoE) Transformer blocks and the representation vectors of all candidate locations in the city. These location representations are generated using Deep & Cross Network (DCN) [40], which consists of both Cross layer and Deep layer to capture feature interactions and nonlinear transformations. The Cross network captures inter-location correlations within city by applying element-wise interactions over location embeddings ğ¸ğ‘™ , producing: ğ¸cross = ğ‘‘ ğ‘–=1 ğ¸ğ‘– ğ‘Šğ‘– ğ¸ğ‘™ + ğ‘ğ‘–, (14) where denotes element-wise multiplication, and ğ‘Šğ‘– , ğ‘ğ‘– are learnable parameters. The Deep layer refines the same ğ¸ğ‘™ using twolayer MLP: ğ¸deep = GELU ((ğ‘Š1ğ¸ğ‘™ + ğ‘1) ğ‘Š2 + ğ‘2) , (15) with weights ğ‘Š1,ğ‘Š2, biases ğ‘1, ğ‘2, and GELU activation. The DCN output is the concatenation of both branches: ğ¸DCN = ğ¸cross ğ¸deep. For next-location prediction, the users historical trajectory is encoded by two cross-city encoders. One generates prediction vector ğ‘ƒ via MoE Transformer blocks; the other produces location embeddings ğ¸DCN via DCN. The similarity score is computed as: (16) Scoresimilarity = ğ‘‘ ğ‘–=1 ğ¼ ğ¸DCN,ğ‘–, (17) where ğ‘‘ is the number of locations. Higher similarity scores indicate higher probabilities of being the next location. This similarity-based decoding strategy ensures scalability, as it decouples the prediction process from fixed output space. Instead of classifying over static set of locations, the model performs representation-level matching, allowing it to generalize across cities with different spatial layouts and dynamically varying numbers of candidate locations."
        },
        {
            "title": "4.3 Layer-Wise Progressive Adaptation\nTo ensure a balance between previously and newly learned knowl-\nedge, MoveGCL employs a layer-wise progressive adaptation strat-\negy, where model parameters are updated in stages, as illustrated\nin Figure 3. For a model composed of ğ‘ layers of MoE transformer\nblocks, the total number of training epochs ğ¸ is evenly divided into\nğ‘ /2 stages, with each stage lasting E\nğ‘ /2 epochs. At each stage, a\npair of symmetrically positioned MoE transformer blocksâ€”one near\nthe input side and the other near the output sideâ€”are unfrozen for\nfine-tuning, while the remaining layers remain frozen. The specific\nprocess is as follows:\nâ€¢ In Stage 1, the outermost layers (closest to the input and output)",
            "content": "are unfrozen. Table 1: Basic statistics of mobility data. City Atlanta Chicago Los Angeles New York Seattle User 114941 148000 161544 170321 88569 Washington D.C 134442 Trajectory 2348218 8051522 16844127 15766369 3362353 11024181 Location 1175 4166 6198 4988 1046 In Stage 2, the second closest layers to the input and output are unfrozen. ... In Stage N/2, the two central layers of the model are unfrozen. Within each stage, only subset of parameters in the unfrozen layers is updatedthe routing modules, newly added experts and previously trained experts that were not frequently activated in the prior generative continual learning phase. To facilitate adaptation to the mobility features across different datasets, parameters of the mobility feature encoder are updated continuously during all stages. Furthermore, to prevent large parameter shifts during the initial stage of parameter updating, all previously trained experts in the input-side MoE transformer layer are kept frozen during Stage 1."
        },
        {
            "title": "5 Results\n5.1 Experimental Settings\nDatasets. We utilize human mobility datasets from multiple cities\nto evaluate the performance of MoveGCL. Specifically, the datasets\ncover over eight hundred thousand users and feature a relatively\nhigh sampling rate compared to currently available public datasets.\nDetailed statistics and descriptions are provided in Table 1. For each\ncity, we randomly sample 120,000 trajectories for training, 40,000\nfor validation, and 40,000 for testing.",
            "content": "Baselines. We compare our method with diverse set of baselines, including traditional mobility models, federated learning-based approaches, and joint learning models with privacy-preserving mechanisms. Traditional approach: This includes Markov models [12] that fit separate transition matrices for different datasets. Deep mobility models: We include LSTM [18], Transformer [38], DeepMove [10], TrajBert [37], and CLET [23] as representative baselines. For each dataset, we train separate model to ensure fair comparison under the same training conditions. Fedarated learning models: We evaluate against PMF [11] and LightTR [28], which leverage federated learning frameworks for human mobility prediction while maintaining data decentralization. We apply their federated learning methods to train our model. Joint models with privacy protection: These methods enable continual learning without accessing previously seen data. Specifically, we consider two variants of our model: MoveGCL (FullTune) unfreezes all experts and routers in the MoE Transformer while keeping the rest of the model frozen, and fine-tunes using only the new citys data. MoveGCL (ExpertTune) incrementally adds one new expert per layer in the MoE Transformer, Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning Conference acronym XX, June 0305, 2018, Woodstock, NY Table 2: Performance comparison between MoveGCL and baseline methods across different cities and Acc@k metrics. Model Atlanta Chicago Los Angeles New York Seattle Washington D.C Markov LSTM Transformer DeepMove TrajBert CLET PVM LightTR MoveGCL (FullTune) MoveGCL (ExpertTune) MoveGCL (WSCALN) Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 0.347 0.183 0.383 0.231 0.356 0.210 0.408 0.242 0.367 0.214 0.390 0.263 0.103 0.131 0.124 0.147 0.146 0.138 0.115 0.169 0.156 0.177 0.159 0.156 0.202 0.259 0.235 0.278 0.234 0.289 0.201 0.275 0.268 0.274 0.277 0. 0.275 0.312 0.318 0.329 0.310 0.313 0.260 0.334 0.300 0.344 0.316 0.341 0.318 0.420 0.397 0.444 0.402 0.454 0.325 0.373 0.353 0.393 0.370 0.422 0.146 0.194 0.175 0.203 0.183 0.200 0.162 0224 0.192 0.247 0.207 0. 0.248 0.269 0.381 0.402 0.151 0.168 0.249 0.269 0.112 0.130 0.190 0. 0.150 0.168 0.257 0.281 0.269 0.296 0.418 0.444 0.217 0.242 0.349 0. 0.188 0.304 0.125 0.206 0.064 0. 0.208 0.329 0.199 0.318 0.147 0. 0.192 0.310 0.132 0.215 0.062 0. 0.207 0.327 0.195 0.322 0.147 0. 0.282 0.421 0.197 0.306 0.157 0. 0.206 0.328 0.324 0.478 0.273 0. unfreezes all experts and routers, and fine-tunes on the new citys data while keeping all other parameters fixed. Evaluation Metrics. We adopt top-ğ‘˜ accuracy as our evaluation metric, defined as acc@ğ‘˜ = 1 ğ‘ ğ‘ 1(cid:0)ğ‘¥ğ‘– ğ‘“ğ‘˜ (ğ‘¥ğ‘– )(cid:1) , (18) ğ‘–=1 where ğ‘ is the total number of samples, ğ‘¥ğ‘– is the ground-truth label for the ğ‘–th sample, ğ‘“ğ‘˜ (ğ‘¥ğ‘– ) denotes the set of the models top-ğ‘˜ predicted labels for sample ğ‘–, and 1() is the indicator function that equals 1 if its argument is true and 0 otherwise. We report on acc@1 and acc@3 to assess the performance of the model. Parameter Settings. The key parameters of our framework fall into three main categories. For the model architecture, we set the temporal embedding dimension to 48. In the mobility encoder, the embedding dimensions of ğ‘‘jump and ğ‘¡wait are both 128, the embedding dimensions of ğ‘Ÿgyr and location entropy ğ»loc are 64, and the embedding dimension of IDcity is 32, with the self-attention modules for both ğ‘‘jump and ğ‘¡wait using hidden dimension of 64. In the trajectory location encoder and the city location encoder, the embedding dimensions of ğœ™POI, ğœ™hot, and ğœ™lat-lon are 256, 128, 128, respectively. The hidden dimension of each MoE transformer block is set to 512. The initial number of experts in each MoE transformer block is 4, and the model comprises 6 layers of MoE transformer blocks. For the base model training phase, the initial model is obtained by training on three cities datasets. During this phase, we use batch size of 16 and train for 30 epochs. The initial learning rate is set to 1.2 105, and the learning rate decays in stepwise fashion during training. For generative continual learning, whenever we introduce new dataset (i.e., new city), we add one new expert to every MoE transformer block. The initial learning rate for this phase is 1.2 104, the batch size is 128, and training also runs for 30 epochs, with the learning rate decaying stepwise throughout. The generative coefficient ğ›¼ is set to 20%. The balance coefficient ğœ† for Ltotal is set to 1."
        },
        {
            "title": "5.2 Overall Performance\nTable 2 presents the performance of MoveGCL compared to state-\nof-the-art baseline methods.\nâ€¢ MoveGCL consistently outperforms traditional deep learn-\ning models trained independently on each dataset, demon-\nstrating strong cross-city scalability. On average, it achieves\na 8% improvement in Acc@1, highlighting its ability to gener-\nalize across diverse urban environments. This result validates\nthe promise of mobility foundation models, which unify knowl-\nedge across cities and reduce redundancy. In contrast, training\nseparate models for each city not only increases computational\nand deployment costs, but also fails to leverage shared mobility\npatterns across domains.",
            "content": "MoveGCL surpasses privacy-preserving federated learning approaches in both accuracy and stability. Compared to domain-specific baselines such as PVM [11] and LightTR [28], MoveGCL achieves significantly higher performance. This advantage stems from its unified generative continual learning framework, which maintains global generalization without suffering from the synchronization overhead and convergence instability inherent in federated setups. This further supports its practicality in real-world multi-party mobility modeling scenarios. MoveGCL effectively balances adaptation to new data while retaining knowledge from previously seen data. We benchmark against two continual learning strategiesFullTune and ExpertTunewhich either fine-tune or expand the model on new datasets. While these methods partially adapt to new cities, they suffer from severe performance degradation on previously seen data, indicating catastrophic forgetting. In contrast, MoveGCL preserves prior knowledge and achieves higher performance on both old and new datasets, demonstrating its ability to support continuous model evolution without sacrificing stability. Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Table 3: Performance comparison across cities and Acc@k metrics for evaluating order invariance. Model Atlanta Chicago Los Angeles New York Seattle Washington D.C WSCALN AWNSC WALNSC Joint Training Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 Acc@1 Acc@3 0.413 0.282 0.407 0.284 0.408 0.286 0.410 0.288 0.421 0.423 0.427 0.428 0.197 0.197 0.191 0.192 0.306 0.308 0.300 0.302 0.478 0.469 0.470 0. 0.273 0.265 0.268 0.270 0.324 0.317 0.318 0.322 0.206 0.200 0.179 0.198 0.328 0.326 0.294 0.320 0.157 0.150 0.153 0.156 0.254 0.246 0.247 0. evaluate the privacy-preserving properties of our approach, following the methodology in [47, 48], we conduct comprehensive analysis from three complementary perspectives: Uniqueness Testing [7, 42]: To evaluate the degree of similarity between the generated data and the real data. Membership Inference Attack [25, 36]: Given trained model and set of samples, membership inference attack assesses whether an classifier can accurately determine which samples were included in the models training set based on the models outputs. Differential Privacy [1, 2]: To ensure that the model does not depend on small subset of training examples, we remove minimal set of training samples and evaluate whether the distribution of model outputs undergoes an obvious change. Uniqueness Testing. We randomly extract subset of trajectories from the training set and use an autoregressive process to generate new trajectories conditioned on each sampled trajectory. We then compute the pairwise similarity between each original sample and its corresponding generated trajectory.If the lengths of two trajectories are different, the similarity score is defined as 0. If they are of equal length, the similarity score is calculated as the proportion of positions where both the timestamp and location ID exactly match. For each generated trajectory, we compute its similarity score with the top-1, top-3, and top-5 most similar real trajectories. Figure 4 presents the cumulative distribution of similarity scores. As shown in the figure, over 95% of the generated trajectories do not have any corresponding real trajectory with similarity score higher than 50%. This indicates that the models outputs are based on the knowledge it has acquired, rather than directly copying trajectories from the training set. Membership Inference Attack. Following the experimental setup in [26, 36], we use the similarity between generated trajectories and their corresponding real input trajectories as the classification feature. For each trajectory ğ‘¥, MoveGCL generates ğ‘¥ autoregressively conditioned on ğ‘¥, and we compute similarity score ğ‘  (ğ‘¥, ğ‘¥) to form the input to the classifier. The classifier is then tasked with determining whether ğ‘¥ was included in the models training set. Positive samples consist of real-world trajectories that were used during training, while negative samples are real trajectories from the same city that were held out. We evaluate the attack success rate, defined as the proportion of samples for which the classifier correctly infers membership status. We employ three widely used classification algorithms: Logistic Regression (LR), Support Vector Machine (SVM) and Random Forest (RF). Figure 4: Similarity score distribution in uniqueness testing."
        },
        {
            "title": "Learning",
            "content": "To assess the robustness of MoveGCL in real-world deployment scenarios, we investigate its sensitivity to the order in which data from different cities is introduced during continual learning. In practice, the arrival of mobility data is often dictated by external factors such as data access regulations, infrastructure development cycles, or institutional collaborations. As result, foundation models intended for long-term, large-scale deployment must remain robust to such variations in data sequencing. We simulate this scenario by reversing the order of datasets used in the continual learning phase. As shown in Table 3, the performance of MoveGCL remains remarkably consistent, with the vast majority of metrics showing deviations of less than 5% across original and reversed sequences. This empirical finding confirms the order-invariant learning behavior of our model, demonstrating that MoveGCL can integrate new city data without disrupting previously acquired knowledge, even when the order of exposure varies significantly. This property is especially crucial for building scalable and unified mobility foundation models, which must support progressive, privacy-preserving knowledge accumulation in noni.i.d. settings where data arrives incrementally and asynchronously. Order robustness is thus key enabler for deploying foundation models that can continually evolve while ensuring stable performance and generalization across diverse urban contexts."
        },
        {
            "title": "5.4 Privacy Evaluation\nAs MoveGCL is built on a generative continual learning framework\nthat does not retain raw data from previous cities, a key question is\nwhether the synthetic data used for replay may inadvertently leak\nprivate information from the original training data. To rigorously",
            "content": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 5: Success rate in membership inference attack Table 4: Differential Privacy statistics by city. Mean Median ğ 0.706 2.671 Atlanta 0.752 2.919 Chicago 0.593 2.988 Los Angeles 0.713 3.394 New York 0.655 Seattle 2.934 0.600 Washington D.C 3.037 75th Percentile 2.212 2.504 2.572 2.001 1.870 1. Figure 5 shows the attack results. As observed, the success rates across all datasets are approximately 50%, indicating that the classifier can hardly determine whether trajectory was part of the training data or not based on the generated sample. These results indicate that our model is not easily susceptible to membership inference attacks. Differential Privacy. For any pair of datasets ğ· and ğ· that differ by only small number of training trajectories, model ğ‘€ is said to satisfy (ğœ€, ğ›¿)-differential privacy if the following condition holds: P(cid:2)ğ‘€ (ğ‘§; ğ·) = ğ‘§(cid:3) ğ‘’ğœ€ P(cid:2)ğ‘€ (ğ‘§; ğ·) = ğ‘§(cid:3) + ğ›¿, (19) where P(cid:2)ğ‘€ (ğ‘§; ğ·) = ğ‘§(cid:3) denotes the probability of observing output ğ‘§ when the model is trained on dataset ğ·, and P(cid:2)ğ‘€ (ğ‘§; ğ·) = ğ‘§(cid:3) is defined analogously for dataset ğ·. Smaller values of ğœ€ and ğ›¿ imply stronger privacy guarantees, since the models output distribution becomes less dependent on any single trajectory. In our experiment, we randomly select subset of trajectories and consider two training scenarios: one in which this subset is included in the training data (ğ·), and one in which it is excluded (ğ·). For each scenario, we train ğ‘€ on the corresponding dataset and then use each selected trajectory as conditioning input to generate multiple synthetic trajectories. We compute similarity score between each generated trajectory and its original conditioning trajectory. These similarity scores are modeled as two Gaussian distributions corresponding to P(cid:2)ğ‘€ (ğ‘§; ğ·)(cid:3) and P(cid:2)ğ‘€ (ğ‘§; ğ·)(cid:3), respectively. Finally, we estimate the privacy-budget parameters ğœ€ from these distributions. As shown in Table 4, without applying any additional privacy-preserving mechanisms, MoveGCL naturally achieves privacy budget of ğœ€ 12 for 75% of randomly Figure 6: Ablation study. w/o KD denotes removal of the knowledge distillation loss; w/o MAER denotes removal of mobility feature from MoE transformers router inputs. sampled trajectories. This level is generally considered an acceptable operating point for generative models [25]; for example, Apple adopts privacy budget of ğœ€ = 4.0 ."
        },
        {
            "title": "5.5 Ablation Studies\nIn this section, we conduct two sets of incremental ablation studies\nbased on MoveGCL (WSCâ†’Aâ†’Lâ†’N). The first set focuses on the\ninput features of the Mobility-Aware Expert Routing module. We\nselectively remove or adjust different dimensions of the location fea-\nture to evaluate the contribution and necessity of each type of input\nin guiding expert routing. The second set targets the incremental\nlearning mechanism itself. We remove the knowledge distillation\nstrategy designed to mitigate catastrophic forgetting in GCL, and\ninstead train the model using only the conventional cross-entropy\nloss. This setup allows us to assess the effectiveness of knowledge\ndistillation in preserving previously learned knowledge.",
            "content": "As shown in Figure 6, removing any input feature from the Mobility-Aware Expert Routing module leads to noticeable performance drop. Similarly, disabling the knowledge distillation strategy in GCL also results in significant decline in model performance. These findings highlight the critical role of each input feature in expert selection, as well as the importance of knowledge distillation in ensuring model stability during continual learning."
        },
        {
            "title": "5.6 Impact of Replay Volume\nIn generative continual learning, synthetic data replay serves as\na key mechanism for preserving previously acquired knowledge\nwithout accessing raw data. A critical hyperparameter in this pro-\ncess is the volume of generated data used during training on new\ncities. While too little replay data may result in catastrophic for-\ngetting, excessive generation increases computational costs and\nmay introduce noise or redundancy. Understanding this trade-off\nis essential for building scalable and efficient mobility foundation\nmodels. To explore this, we vary the amount of generated data and\nevaluate its impact on both knowledge retention (for previously\nseen cities) and adaptation to new cities.",
            "content": "https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. Figure 7: Acc@1 changes at different generated data ratios (ğ›¼), relative to ğ›¼ = 5%. Figure 8: City location embeddings before (original) and after (final) DCN. As shown in Figure 7, the performance on the base cities (WSC) consistently improves with more generated data, demonstrating that replay volume directly influences the models ability to retain past knowledge. In contrast, performance on the newly introduced cities (A, L, N) remains largely stable regardless of replay volume, with no consistent trend of improvement or degradation. These results suggest that while synthetic replay is crucial for mitigating forgetting, it has limited effect on new knowledge acquisition. Thus, allocating moderate amount of generated data offers practical balancesufficient to preserve prior knowledge without incurring unnecessary overheadsupporting the long-term scalability of continual mobility modeling."
        },
        {
            "title": "5.7 In-Depth Analysis\nTo better understand why MoveGCL is capable of unifying diverse\nmobility datasets and effectively handling substantial inter-city\nheterogeneity, we conduct an in-depth analysis of its location em-\nbedding layer. Our goal is to examine whether MoveGCL can learn\nshared spatial representations across cities. To this end, we extract\nthe location embeddings for each city at two stages: (1) after the\ninitial encoder, and (2) after the Deep & Cross Network (DCN). By\ncomparing these two sets of embeddings, we aim to evaluate the",
            "content": "role of DCN in aligning spatial semantics across heterogeneous urban environments. As shown in Figure 8, DCN aligns location embedding distributions in different cities much more closely than the original encoder output. This indicates that DCN successfully captures shared location-feature patterns across urban areas, thereby boosting the models ability to generalize in cross-city settings. Moreover, within each city, the DCN-processed embeddings become less densely clustered than their original counterparts, indicating marked increase in separability among individual locations and further enhancing the models capacity to encode location semantics."
        },
        {
            "title": "6 Conclusion\nIn this work, we present MoveGCL, a scalable and privacy-preserving\nframework for training mobility foundation models via genera-\ntive continual learning. By enabling decentralized model evolution\nwithout sharing raw data, MoveGCL addresses key challenges in\nreal-world human mobility modeling, including data silos, privacy\nconstraints, and heterogeneous mobility distributions. MoveGCL\nrepresents a significant step toward realizing foundation models\nfor mobility by offering a practical and generalizable framework\nthat facilitates collaborative learning across cities and institutions.\nIt paves the way for long-term, privacy-safe, and adaptive modeling",
            "content": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning Conference acronym XX, June 0305, 2018, Woodstock, NY of human movement, with broad implications for urban planning, transportation optimization, and evidence-based policy making. Despite its promising performance, MoveGCL remains constrained by the limited availability of high-quality training data. The development of more large-scale, semantically rich, and geographically diverse mobility datasets will be crucial for further improving the models generalization and robustness. We encourage the broader research community and data-holding institutions to join this collaborative effort, contributing to the creation of open, inclusive, and powerful spatiotemporal foundation models for the mobility domain. References [1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS). ACM, 308318. [2] Galen Andrew, Steve Chien, and Nicolas Papernot. 2019. TensorFlow Privacy: Learning with Differential Privacy for Training Machine Learning Models. https:// blog.tensorflow.org/2019/03/introducing-tensorflow-privacy-learning.html. Accessed: 2025-06-06. [3] Hugo Barbosa, Marc Barthelemy, Gourab Ghoshal, Charlotte James, Maxime Lenormand, Thomas Louail, Ronaldo Menezes, JosÃ© Ramasco, Filippo Simini, and Marcello Tomasini. 2018. Human mobility: Models and applications. Physics Reports 734 (2018), 174. [4] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. DeepSeek LLM: Scaling Open-Source Language Models with Longtermism. CoRR (2024). [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 18771901. [6] Shushman Choudhury, Abdul Rahman Kreidieh, Ivan Kuznetsov, and Neha Arora. 2024. Towards Trajectory-powered Foundation Model of Mobility. In Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Spatial Big Data and AI for Industrial Applications. 14. [7] Yves-Alexandre de Montjoye, CÃ©sar A. Hidalgo, Michel Verleysen, and Vincent D. Blondel. 2013. Unique in the crowd: The privacy bounds of human mobility. Scientific Reports 3 (2013), 1376. doi:10.1038/srep01376 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Fortyfirst international conference on machine learning. [9] Jie Feng, Yuwei Du, Jie Zhao, and Yong Li. 2024. Agentmove: Predicting human mobility anywhere using large language model based agentic framework. arXiv preprint arXiv:2408.13986 (2024). [10] Jie Feng, Yong Li, Zeyu Yang, Qiang Qiu, and Depeng Jin. 2022. Predicting Human Mobility With Semantic Motivation via Multi-Task Attentional Recurrent Networks. IEEE Transactions on Knowledge and Data Engineering 34, 5 (2022), 23602374. doi:10.1109/TKDE.2020.3006048 [11] Jie Feng, Can Rong, Funing Sun, Diansheng Guo, and Yong Li. 2020. PMF: privacy-preserving human mobility prediction framework via federated learning. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 1 (2020), 10:110:21. doi:10.1145/3381006 [12] SÃ©bastien Gambs, Marc-Olivier Killijian, and Miguel NÃºÃ±ez del Prado Cortez. 2012. Next place prediction using mobility Markov chains. In Proceedings of the First Workshop on Measurement, Privacy, and Mobility. ACM, Bern, Switzerland, 38. [13] Letian Gong, Yan Lin, Yiwen Lu, Xuedi Han, Yichen Liu, Shengnan Guo, Youfang Lin, Huaiyu Wan, et al. 2024. Mobility-llm: Learning visiting intentions and travel preference from human mobility data with large language models. Advances in Neural Information Processing Systems 37 (2024), 3618536217. [14] WANG JIAWEI, Renhe Jiang, Chuang Yang, Zengqing Wu, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao, et al. 2024. Large language models as urban residents: An llm agent framework for personal mobility generation. Advances in Neural Information Processing Systems 37 (2024), 124547124574. [15] Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, and Flora Salim. 2025. TrajLLM: Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation. arXiv preprint arXiv:2502.18712 (2025). [16] Gyuhak Kim, Changnan Xiao, Tatsuya Konishi, Zixuan Ke, and Bing Liu. 2022. theoretical study on solving continual learning. Advances in neural information processing systems 35 (2022), 50655079. [17] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems 35 (2022), 2219922213. [18] Dejiang Kong and Fei Wu. 2018. HST-LSTM: hierarchical spatial-temporal long-short term memory network for location prediction. In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI). AAAI Press, Stockholm, Sweden, 23412347. [19] Xiangjie Kong, Qiao Chen, Mingliang Hou, Hui Wang, and Feng Xia. 2023. Mobility trajectory generation: survey. Artificial Intelligence Review 56, Suppl 3 (2023), 30573098. [20] Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. 2019. Learn to grow: continual structure learning framework for overcoming catastrophic forgetting. In International conference on machine learning. PMLR, 39253934. [21] Zhonghang Li, Long Xia, Lei Shi, Yong Xu, Dawei Yin, and Chao Huang. 2024. Opencity: Open spatio-temporal foundation models for traffic prediction. arXiv preprint arXiv:2408.10269 (2024). [22] Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei Yin, and Chao Huang. 2024. Urbangpt: Spatio-temporal large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 53515362. [23] Yuxuan Lin, Hongxu Wan, Shenglin Guo, and Yantao Lin. 2021. Pre-training context and time aware location embeddings from spatial-temporal trajectories for user next location prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 42414248. doi:10.1609/aaai.v35i5.16548 [24] Yan Lin, Tonglong Wei, Zeyu Zhou, Haomin Wen, Jilin Hu, Shengnan Guo, Youfang Lin, and Huaiyu Wan. 2024. TrajFM: vehicle trajectory foundation model for region and task transferability. arXiv preprint arXiv:2408.15251 (2024). [25] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. Using gans for sharing networked time series data: Challenges, initial promise, and open questions. In Proceedings of the ACM internet measurement conference. 464483. [26] Zinan Lin, Alankar Jain, Chen Wang, Giulia Fanti, and Vyas Sekar. 2020. Using GANs for Sharing Networked Time Series Data: Challenges, Initial Promise, and Open Questions. In Proceedings of the ACM Internet Measurement Conference (IMC). 464483. [27] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. 2024. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177 (2024). [28] Ziqiao Liu, Hao Miao, Yan Zhao, Chenxi Liu, Kai Zheng, and Huan Li. 2024. LightTR: Lightweight Framework for Federated Trajectory Recovery. In 2024 IEEE 40th International Conference on Data Engineering (ICDE). 44224434. doi:10. 1109/ICDE60146.2024. [29] Qingyue Long, Yuan Yuan, and Yong Li. 2024. Universal Model for Human Mobility Prediction. arXiv preprint arXiv:2412.15294 (2024). [30] Massimiliano Luca, Gianni Barlacchi, Bruno Lepri, and Luca Pappalardo. 2021. survey on deep learning for human mobility. ACM Computing Surveys (CSUR) 55, 1 (2021), 144. [31] Saeed Masoudnia and Reza Ebrahimpour. 2014. Mixture of experts: literature survey. Artificial Intelligence Review 42 (2014), 275293. [32] New York City Taxi and Limousine Commission. 2023. TLC Trip Record Data. https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page. [Online; accessed 16-November-2023]. [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PmLR, 87488763. [34] Can Rong, Jingtao Ding, and Yong Li. 2024. An interdisciplinary survey on origin-destination flows modeling: Theory and techniques. Comput. Surveys 57, 1 (2024), 149. [35] Markus SchlÃ¤pfer, Lei Dong, Kevin OKeeffe, Paolo Santi, Michael Szell, Hadrien Salat, Samuel Anklesaria, Mohammad Vazifeh, Carlo Ratti, and Geoffrey West. 2021. The universal visitation law of human mobility. Nature 593, 7860 (2021), 522527. [36] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against machine learning models. In Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 318. [37] Junjun Si, Jin Yang, Yang Xiang, Hanqiu Wang, Li Li, Rongqing Zhang, Bo Tu, and Xiangqun Chen. 2023. TrajBERT: BERT-based trajectory recovery with spatial-temporal refinement for implicit sparse trajectories. IEEE Transactions on Mobile Computing 23, 5 (2023), 48494860. [38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS). Curran Associates Inc., Long Beach, California, USA, 60006010. [39] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. 2024. comprehensive survey of continual learning: Theory, method and application. IEEE Transactions Conference acronym XX, June 0305, 2018, Woodstock, NY Yuan et al. on Pattern Analysis and Machine Intelligence (2024). [40] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & cross network for ad click predictions. In Proceedings of the ADKDD17. 17. [41] Buddhi Wickramasinghe, Gobinda Saha, and Kaushik Roy. 2023. Continual learning: review of techniques, challenges, and future directions. IEEE Transactions on Artificial Intelligence 5, 6 (2023), 25262546. [42] Fengli Xu, Zhen Tu, Yong Li, Pengyu Zhang, Xiaoming Fu, and Depeng Jin. 2017. Trajectory recovery from ash: User privacy is not preserved in aggregated mobility data. In Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 1241 1250. [43] Takahiro Yabe, Kota Tsubouchi, Toru Shimizu, Yoshihide Sekimoto, Kaoru Sezaki, Esteban Moro, and Alex Pentland. 2024. YJMob100K: City-scale and longitudinal dataset of anonymized human mobility trajectories. Scientific Data 11, 1 (2024), 397. [44] Dingqi Yang, Bingqing Qu, Jie Yang, and Philippe Cudre-Mauroux. 2019. Revisiting user mobility and social relationships in lbsns: hypergraph embedding approach. In The world wide web conference. 21472157. [45] Jing Yuan, Yu Zheng, Chengyang Zhang, Wenlei Xie, Xing Xie, Guangzhong Sun, and Yan Huang. 2010. T-drive: driving directions based on taxi trajectories. In Proceedings of the 18th SIGSPATIAL International conference on advances in geographic information systems. 99108. [46] Yuan Yuan, Jingtao Ding, Jie Feng, Depeng Jin, and Yong Li. 2024. Unist: prompt-empowered universal model for urban spatio-temporal prediction. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 40954106. [47] Yuan Yuan, Jingtao Ding, Depeng Jin, and Yong Li. 2025. Learning the complexity of urban mobility with deep generative network. PNAS nexus 4, 5 (2025), pgaf081. [48] Yuan Yuan, Jingtao Ding, Huandong Wang, and Depeng Jin. 2024. Generating Daily Activities with Need Dynamics. ACM Transactions on Intelligent Systems and Technology 15, 2 (2024), 29:129:28. doi:10.1145/3637493 [49] Yuan Yuan, Jingtao Ding, Huandong Wang, Depeng Jin, and Yong Li. 2022. Activity trajectory generation via modeling spatiotemporal dynamics. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 47524762. [50] Yuan Yuan, Yuheng Zhang, Jingtao Ding, and Yong Li. 2025. WorldMove, global open data for human mobility. arXiv preprint arXiv:2504.10506 (2025). [51] Junbo Zhang, Yu Zheng, and Dekang Qi. 2017. Deep spatio-temporal residual networks for citywide crowd flows prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 31. [52] Weijia Zhang, Jindong Han, Zhao Xu, Hang Ni, Hao Liu, and Hui Xiong. 2024. Urban foundation models: survey. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 66336643. [53] Yuheng Zhang, Yuan Yuan, Jingtao Ding, Jian Yuan, and Yong Li. 2025. Noise Matters: Diffusion Model-based Urban Mobility Generation with Collaborative Noise Priors. In Proceedings of the ACM on Web Conference 2025. 53525363. [54] Yu Zheng, Hao Fu, Xing Xie, Wei-Ying Ma, and Quannan Li. [n. d.]. Geolife GPS trajectory dataset-User Guide, geolife gps trajectories 1.July 2011, geolife GPS trajectories 1.1. July 2011 geolife GPS trajectories 1.1 ([n. d.]). [55] Zhen Zhou, Ziyuan Gu, Xiaobo Qu, Pan Liu, Zhiyuan Liu, and Wenwu Yu. 2024. Urban mobility foundation model: literature review and hierarchical perspective. Transportation Research Part E: Logistics and Transportation Review 192 (2024), 103795. [56] Yuanshao Zhu, Yongchao Ye, Ying Wu, Xiangyu Zhao, and James Yu. 2023. Synmob: Creating high-fidelity synthetic gps trajectory dataset for urban mobility analysis. Advances in Neural Information Processing Systems 36 (2023), 22961 22977. [57] Yuanshao Zhu, James Jianqiao Yu, Xiangyu Zhao, Xuetao Wei, and Yuxuan Liang. 2024. UniTraj: Universal human trajectory modeling from billion-scale worldwide traces. arXiv preprint arXiv:2411.03859 (2024)."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}