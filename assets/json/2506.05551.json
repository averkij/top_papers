{
    "paper_title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding",
    "authors": [
        "Yan Shu",
        "Hangui Lin",
        "Yexin Liu",
        "Yan Zhang",
        "Gangyan Zeng",
        "Yan Li",
        "Yu Zhou",
        "Ser-Nam Lim",
        "Harry Yang",
        "Nicu Sebe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify a key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose a training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, a coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated question-answer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 1 5 5 5 0 . 6 0 5 2 : r When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding Yan Shu1,* Hangui Lin3,* Yexin Liu2,* Yan Zhang4,5 Gangyan Zeng6 Yan Li3 Yu Zhou7 Ser-Nam Lim8 Harry Yang2 Nicu Sebe1 1UNITN 2HKUST 3UIR 4IIE, CAS 5UCAS 6NJUST 7NKU 8UCF Equal contribution. https://github.com/shuyansy/MLLM-Semantic-Hallucination"
        },
        {
            "title": "Abstract",
            "content": "Large Multimodal Models (LMMs) have achieved impressive progress in visual perception and reasoning. However, when confronted with visually ambiguous or non-semantic scene text, they often struggle to accurately spot and understand the content, frequently generating semantically plausible yet visually incorrect answers, which we refer to as semantic hallucination. In this work, we investigate the underlying causes of semantic hallucination and identify key finding: Transformer layers in LLM with stronger attention focus on scene text regions are less prone to producing semantic hallucinations. Thus, we propose training-free semantic hallucination mitigation framework comprising two key components: (1) ZoomText, coarse-to-fine strategy that identifies potential text regions without external detectors; and (2) Grounded Layer Correction, which adaptively leverages the internal representations from layers less prone to hallucination to guide decoding, correcting hallucinated outputs for non-semantic samples while preserving the semantics of meaningful ones. To enable rigorous evaluation, we introduce TextHalu-Bench, benchmark of over 1,730 samples spanning both semantic and non-semantic cases, with manually curated questionanswer pairs designed to probe model hallucinations. Extensive experiments demonstrate that our method not only effectively mitigates semantic hallucination but also achieves strong performance on public benchmarks for scene text spotting and understanding."
        },
        {
            "title": "Introduction",
            "content": "Scene text, as self-descriptive visual element, conveys rich semantic information that is crucial for downstream applications such as autonomous driving, product analysis, and assistive technologies. Effectively spotting and understanding scene text [17] thus attracts growing attention from the deep learning community. To spot and understand scene texts, traditional approaches [811] rely on multi-stage methods, separately addressing text detection, recognition, and language modeling, which limits their generalization ability in diverse real-world settings. As general solution for vision-language tasks, Large Multimodal Models (LMMs) [1216] have shown remarkable capabilities in image captioning and visual question answering by combining visual encoders with Large Language Models (LLMs). Motivated by this progress, researchers have begun adapting LMMs for OCR-related tasks, including document question answering [1720], GUI analysis agents [21, 22], and unified OCR frameworks [23]. However, whether LMMs can reliably address scene text spotting and understanding remains underexplored. In this work, we investigate this question through TextTrap challenge. As illustrated Preprint. Under review. Figure 1: (a) LMMs hallucinate scene-text answers by relying on semantic priors rather than grounding in the actual visual content. For instance, when we edit MOTEL and PULL to MMOTEL and PULLa, the models still answer the original ones. (b) and (c) illustrate the performance of LMMs on OCRBench and ICDAR 2015, with separate evaluations on semantic and non-semantic text samples. in Fig. 1, LMMs such as Qwen2-VL [12] perform well when scene texts are semantically coherent. However, introducing subtle character-level perturbations that disrupt semantic meaning often leads these models to produce semantically plausible yet visually incorrect answers, phenomenon we refer to as semantic hallucination. Further experiments on ICDAR 2015 [24] and OCRBench [25] provide solid evidence that LMMs frequently hallucinate scene text answers based on semantic priors rather than actual visual grounding. Motivated by the intuition that semantic priors mainly originate from the LLM, we analyze the causes of hallucination from two perspectives. Inspired by prior observations [2628] that different layers in LLMs capture different types of information, we further reveal that these layers exhibit varying tendencies to hallucinate, with certain intermediate layers showing higher likelihood of correctly predicting ground-truth tokens. Building upon this insight, we further quantify and inspect the spatial distribution of attention maps within the LLM, and observe that layers allocating greater attention to ground-truth text regions are less prone to hallucination, thereby suggesting causal relationship between accurate attention allocation and the mitigation of semantic hallucination.. Based on these findings, we propose semantic hallucination mitigation framework composed of two key components: ZoomText, which takes glimpse-refocus steps to first localize contextual regions related to the scene text, and then refines its focus to estimate scene text regions. This coarse-to-fine grounding strategy eliminates the need for external model intervention. Grounded Layer Correction (GLC): Given the anchor regions produced by ZoomText, GLC adaptively selects the transformer layer with the strongest scene text grounding and fuses its hidden state representations into the decoding process. This design helps mitigate hallucinations for non-semantic samples while preserving the semantics of meaningful ones. Notably, our method is training-free and can be seamlessly integrated into existing LMMs to effectively mitigate semantic hallucination in scene text spotting and understanding. Our main contributions are summarized as follows: 1) We identify the problem of semantic hallucination in LMMs when spotting and understanding scene text. We further investigate its underlying causes, revealing that attention drift across different layers within the LLM contributes significantly to hallucination. 2) We propose training-free hallucination mitigation framework that can be seamlessly integrated into existing LMMs without requiring any architectural modifications. 3) We conduct extensive experiments on multiple benchmarks, demonstrating the effectiveness of our method. For example, when applied to the Mini-Monkey [29] and Qwen2.5-VL [12], our framework yields substantial accuracy gains on ST-VQA [5] and TextVQA [30]. Additionally, we introduce 2 TextHalu-Bench, new benchmark designed to evaluate semantic hallucination, where our framework consistently improves existing methods by approximately 4%."
        },
        {
            "title": "2 Related Works",
            "content": "Large Multimodal Models for OCR. LMMs have demonstrated strong performance in general visual understanding tasks such as image captioning [1214, 31, 32], visual question answering [15, 33, 16, 3439], and video understanding [4044]. However, the increasing demand for textgrounded visual reasoning has revealed its limitations in accurate OCR. Recent works have proposed OCR-specific enhancements for LMMs, which can be broadly categorized into three strategies. (1) Resolution-aware processing: UReader introduces shape-adaptive cropping [17], while Monkey [18] and TextMonkey [19] adopt patch-wise division to better handle high-resolution text regions. Ocean-OCR [45] further utilizes native-resolution ViT to support variable input sizes. (2) Token compression and layout encoding: mPLUG-DocOwl [20] and TextHawk2 [46] reduce visual token redundancy while preserving spatial structure. Vary [47] introduces SAM-style [48] visual vocabulary tailored for document and chart understanding. (3) Redesigned OCR paradigms: GOT-OCR [23] proposes new task formulation and architecture specifically optimized for OCR scenarios. Despite these advances, current models still rely heavily on semantic priors and often fail when the input contains visually plausible but meaningless words. This indicates lack of text grounding. Our work investigates this failure mode and proposes an attention-based inter-layer fusion mechanism to enhance robustness in text-level reasoning. Hallucination in Large Multimodal Models. Hallucination in LMMs refers to the generation of outputs that are not grounded in the visual input, often leading to content that is irrelevant or factually incorrect. Prior work has systematically explored hallucination along several dimensions, including object hallucination [4953], knowledge hallucination [54, 52, 55], relational misinterpretation [56, 57, 55, 58], attribute hallucination [56, 55, 58, 53], and hallucination induced by spurious visual patterns [59, 60]. Recent studies have also revealed inconsistencies in model responses across question types [6163]. To mitigate hallucination, various strategies have been proposed, including self-correction decoding [6466], contrastive decoding [67, 68], and adversarial training [62, 54]. However, most of these studies focus on objector fact-centric hallucinations, while OCR-specific hallucinations remain underexplored. In this work, we identify novel form of semantic hallucination in scene text spotting: LMMs can accurately recognize semantically meaningful words, yet fail when those words are replaced with syntactically valid but semantically meaningless tokens. This behavior indicates that models rely heavily on semantic priors rather than truly grounding their predictions in visual evidence."
        },
        {
            "title": "3 Methods",
            "content": "In this section, we first provide background on the generation paradigm of LMMs and analyze the underlying causes of semantic hallucination in scene text spotting and understanding. These analyses reveal that semantic hallucination is closely tied to attention drift within LLMs, where attention deviates from ground-truth text regions. Building upon these insights, we introduce our training-free hallucination mitigation framework. 3.1 Preliminaries of LMMs Generation Most current LMMs adopt the minimalist architecture of LLaVA [34], which comprises visual encoder, vision-language projector, and an LLM. Given an input image, the visual encoder extracts sequence of visual tokens = {v1, v2, . . . , vn}, where denotes the number of output patches. Similarly, the text input is tokenized into sequence of text tokens = {t1, t2, . . . , tm}. These two token sequences are concatenated as = concat(V, ) and fed into the LLM, parameterized by θ, for auto-regressive generation. At each decoding step i, the model predicts the probability distribution over the next token yi in an auto-regressive manner: p(yi V, T, y<i) = softmax (logitθ(yi V, T, y<i)) (1) Figure 2: Visualization of the hallucination-analysis pipeline. For each input image, we (1) identify hallucinated text tokens and compute their layer-wise hallucination-tendency scores, (2) calculate the ratio of the ground-truth text score to the hallucinated text score for each layer, and (3) overlay these normalized ratios onto the corresponding attention maps. We observe that layers with lower propensity to hallucinate concentrate their attention more strongly on the text regions. To generate the final output, decoding strategies such as greedy decoding or beam search are employed to select the next token. The predicted token yi is then appended to the previous input sequence, and the process is repeated until stop condition is met. 3.2 Investigating the Mystery of Semantic Hallucination LMMs are pretrained on large-scale corpora primarily composed of semantically coherent texts, which may impose strong semantic priors on the model. In scene text spotting and understanding, such priors may cause the model to incorrectly interpret visually meaningless or random character patterns as meaningful words. To gain deeper insights into how these hallucinations arise within the model, we focus on the internal processing of the LMM. Prior work shows that different layers capture different types of information [66]. Building on this, we hypothesize that different layers of the LLM may exhibit varying tendencies to produce semantic hallucinations. To validate this hypothesis, we design an analysis pipeline consisting of two steps: Hallucinated Token Extraction. For each generated output, we tokenize both the generated answer and the ground-truth answer using the LMMs predefined text tokenizer. We then compare the two token sequences and identify the first token in the generated sequence that diverges from the ground-truth as hallucinated token. Hallucination Tendency Scoring. We compute the hallucination tendency score at each layer ℓ by comparing the output probabilities of the hallucinated token and its ground-truth counterpart. Specifically, we feed both the hallucinated token yhal and the ground-truth token ygt into the LMMs output head, which consists of linear projection followed by softmax layer, to obtain their respective predicted probabilities: hal = softmax(W outhℓ ℓ hal + b)yhal , gt = softmax(W outhℓ ℓ gt + b)ygt (2) where out and are the parameters of the output head. The hallucination score Sℓ calculated by ℓ hal + ℓ hallucinated output over the correct one at layer ℓ. hal is then hal indicates that the model is more likely to favor the gt). higher Sℓ hal/(P ℓ As shown in Fig. 2, different transformer layers within the LMM exhibit varying tendencies toward semantic hallucination, with more examples provided in the Supplementary Material. Based on this observation, we aim to further investigate the underlying mechanisms driving these differences, particularly focusing on the visual grounding behavior of different layers (i.e., how they 4 Figure 3: Visualization of the ZoomText process and examples. attend to relevant scene text regions). This leads us to pose key question: Is there relationship between transformer layers visual grounding ability (specifically, its attention to scene text regions) and its tendency to produce semantic hallucinations? To answer this question, we propose quantitative measure of visual grounding for each layer, termed the Text-region Attention Score (Aℓ). This score evaluates how much attention transformer layer allocates to ground-truth text regions, which is calculated as: Aℓ = (cid:80) (cid:80) iI iI (cid:80) (cid:80) jT αℓ i,j jI αℓ i,j (3) where denotes the set of all image tokens, and represents those image tokens located within the provided ground-truth text bounding boxes. αℓ i,j is the self-attention weight from the i-th image token to the j-th image token at layer ℓ. Higher values of Aℓ reflect an increased allocation of attention score to the correct text regions, indicative of more robust visual grounding at layer ℓ. Based on Qwen2.5-VL [12] and Mini-Monkey [29], we evaluate our method on OCRBench [25], ST-VQA [5], and TextVQA [30], and analyze the Spearman correlation [69] between layer-wise hallucination tendency scores and their corresponding text-region attention scores. We observe strong negative correlation across all datasets, indicating that layers with lower attention to groundtruth text regions are more susceptible to semantic hallucination. Additional experimental details are provided in Appendix E. 3.3 Toward Semantic Hallucination Mitigation Building on this key observation about semantic hallucination, we aim to leverage the connection between visual grounding ability and hallucination tendency to design an effective mitigation strategy. This naturally raises two questions: 1) How can we estimate scene text regions without relying on additional modules? 2) Once we identify the layer with the strongest scene text grounding, how can we guide the decoding process using this information to reduce hallucinations? ZoomText. Unlike naturally salient objects, scene text is often difficult to localize, especially in the absence of external text detectors. To address this challenge, we propose glimpse-refocus strategy for estimating scene text regions. We begin by observing that scene text frequently appears on semantically meaningful backgrounds, such as signs, posters, or product packaging, which naturally attract model attention during question answering [66]. Leveraging this intuition, we perform glimpse step that identifies text-related regions by computing the query-to-image cross-attention, which measures how much each image token contributes to the query understanding. The highlighted attention regions serve as coarse estimation of potential text positions. Specifically, we extract the softmax-normalized cross-attention from the query tokens to all image tokens at the final layer of the LLM, resulting in Aq2v RHQN , where is the number of attention heads, is the number of query tokens, and is the number of image tokens. We average across heads and query tokens to obtain global image attention map: 5 Atext = 1 HQ (cid:88) (cid:88) h= q=1 A(h,q) q2v RN . (4) We then apply thresholding to select the top-K image tokens as coarse text region candidates. However, not all high-response tokens are truly relevant to the query, as LLMs often utilize certain tokens as registers to aggregate global context across the image. To mitigate this bias toward irrelevant regions, we introduce Refocus step that filters out spurious activations. This step is based on the hypothesis that background or non-semantic tokens exhibit relatively stable attention patterns across layers, as they do not actively participate in the visual reasoning process. Accordingly, we compute normalized attention shift score among the top-K candidate tokens identified in the Glimpse stage, which quantifies how much each tokens importance evolves throughout the forward pass. Let = {s1, . . . , sK} denote the set of top-K image token indices selected from Atext. We extract the self-attention submatrices A(1) v2v RKK from the first and last transformer layers, and then compute normalized attention shift score as: v2v and A(L) Anormalized text = A(L) v2v A(1) A(1) v2v + ϵ v2v (5) where ϵ is small constant for numerical stability. As shown in Fig. 3, most noisy tokens are effectively filtered out, leaving accurate text regions that can be used to guide the decoding process. Grounded Layer Correction. After identifying grounded text regions, we select the most visually grounded transformer layer ℓ = arg maxℓ Aℓ, and propose three strategies to correct the decoding process. All strategies operate on the final-layer hidden states (L) before decoding, producing revised representations ˆH that integrate information from the grounded layer (ℓ). Specifically, we either: (1) replace all hidden states with the grounded ones (Replacement), (2) apply weighted fusion with factor (Fusion), or (3) selectively replace tokens with high grounding scores based on the refined attention map (Selective Replacement). ˆH = (ℓ) (1 w) (L) (L) + (ℓ) , if / S; (ℓ) (Replacement) (Fusion) , if (Selective Replacement) (6) Empirical results (Sec. 5.3) show that among the three strategies, Fusion achieves the best balance between hallucination mitigation and semantic preservation. Accordingly, we adopt Fusion as our default decoding strategy."
        },
        {
            "title": "4 TextHalu-Bench",
            "content": "Previous scene-text benchmarks such as ST-VQA [5] and TextVQA [30] have notable limitations: their test sets are dominated by semantically meaningful and visually clear samples, as shown in Fig. 4. This may overestimate the visual grounding ability of LMMs, as models can often rely on language priors rather than true visual perception to answer correctly. To address these limitations, we introduce TextHalu-Bench, new benchmark comprising 1,730 carefully curated samples by select collected from diverse public datasets, including ICDAR 2013 [70], ICDAR 2015 [24], ICDAR 2019 [71], CCPD [72],MSRA-TD500 [73] ,RoadText [74] and MPSC [75]. The curation process specifically targets instances containing non-semantic text elements, such as isolated numbers, incomplete words, and rare or out-of-vocabulary tokens. Our benchmark covers five representative scenario categoriesBusiness, Industry, Transportation, Public Facilities, and Daily Lifewith balanced distribution and emphasis on visually challenging cases (e.g., occlusions, low-contrast text, unconventional fonts). It features two subtasks: Spotting, which requires models to extract text directly from images, and Understanding, which evaluates whether models can semantically ground the recognized text. The detailed data construction pipeline is provided in the Appendix C. 6 Figure 4: (a) Examples of TextHalu-Bench. (b) Comparison of non-semantic answers ratios between existing scene text benchmarks and TextHalu-Bench. SQ, UQ, and ANS represent spotting, understanding questions, and answers, respectively."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Setup Baselines. To validate the effectiveness of our proposed semantic hallucination mitigation framework, we integrate it into three contemporary open-source LMMs with diverse LLM backbones: MiniMonkey [29], Qwen2.5-VL [76], and LLaVA-NeXT [36]. For all three models, we follow the default configurations provided in their official implementations to ensure fair comparison. In addition, we evaluate our method alongside 10 representative LMMs, including both open-source and proprietary models, across multiple public benchmarks. Benchmarks. In addition to our proposed TextHalu-Bench, we evaluate our method on six public benchmarks encompassing scene text spotting and understanding. ST-VQA [5] and TextVQA [30] focus on real-world images containing scene text, requiring models to understand and reason over both visual and textual information. AI2D [77] centers on scientific diagrams, emphasizing structured reasoning and domain-specific knowledge. OCR-VQA [78] involves book covers and challenges models to incorporate OCR-derived content into question answering. SEED-Bench [79] offers broad suite of vision-language tasks; we evaluate its Text Understanding subset, which tests general VQA and grounding capabilities. Finally, GOT [23] contains 400 natural images with multilingual scene text; we use its scene text subset and report character-level F1 scores. Implementation Details. Our method is training-free and test-time adaptive plug-in module. In ZoomText, we set the number of top image tokens to 128. In Grounded Layer Correction, we adopt the Fusion strategy and set the fusion factor to 0.1. All experiments are conducted on single NVIDIA A800-80G GPU during inference. Importantly, our algorithm introduces no additional modules or trainable parameters. Test-time efficiency analysis is provided in Appendix E. 5.2 Experiment results We conduct extensive experiments on the seven benchmarks, as demonstrated in Tab. 1, in which we derive three primary conclusions. Semantic hallucination remains significant challenge for existing LMMs. On our proposed TextHalu-Bench, even the best-performing proprietary model, GPT-4o, achieves only 45.3 F1 score, while most open-source models perform considerably worse, far below human performance (96.8). This difficulty arises from two key aspects. First, compared to document-based OCR tasks, scene text spotting and understanding are inherently more challenging due to the presence of complex visual distractors and highly diverse text styles. Second, non-semantic texts require accurate visual grounding rather than reliance on semantic priors, an area where many LMMs still suffer from severe hallucinations. These findings highlight the urgency of addressing semantic hallucination and underscore the importance of TextHalu-Bench, which incorporates diverse non-semantic texts to robustly evaluate and analyze the hallucination behavior of LMMs. 7 Table 1: Experimental results on TextHalu-Bench and mainstream scene text spotting and understanding benchmarks. We report the performance on STVQA and GOT by using their official weight. Method LLM TextHalu-Bench STVQATest TextVQAVal GOTScene OCRVQACORE SEEDBenchText AI2D Gemini1.5-Pro [80] GPT-4o [81] - - LLaVA1.5 [34] mPLUG-Owl2 [82] Molmo-D [83] PixtralB [84] Monkey [85] LLaVA-OV [86] Ovis1.6 [87] InternVL2.5 [88] Vicuna-7B LLaMA-7B Qwen2-7B Nemo-12B Qwen-7B Qwen2-7B Llama-3.2-3B InternLM2.5-7B 43.2 45.3 21.4 24.3 24.7 32.8 34.2 21.4 38.4 42. Proprietary Models 61.6 71.0 - - Open-source MLLMs 51.9 49.8 62.3 52.9 54.7 51.9 72.6 75.4 46.0 56.4 67.5 64.3 67.6 78.5 78.2 79.0 - - 38.8 29.8 42.3 35.4 45.7 43.9 25.2 90.0 18.5 18.7 60.6 65.2 15.9 64.7 67.0 64.7 71.2 31.0 76 70.2 36.9 32.1 77.4 47.6 56.0 61.9 52.4 77.1 79.1 85. 55.5 55.7 81.0 79.0 62.5 82.8 84.4 84.2 LLaVA-NeXT [37] LLaVA-NeXT + Ours Llama-3-8B Llama-3-8B Mini-Monkey [89] Mini-Monkey + Ours InternLM2-1.8B InternLM2-1.8B Qwen2.5-VL [76] Qwen2.5-VL + Ours Qwen2.5-3B Qwen2.5-3B 27.9 28.5 (+0.6) 46.5 50.6 (+4.1) 48.3 53.8 (+5.5) 65.1 65.2 (+0.1) 65.3 65.5 (+0.2) 41.9 42.0 (+0.1) 66.7 70.6 (+3.9) 74.1 75.0 (+0.9) 88.8 89.2 (+0.4) 67.3 67.6 (+0.3) 79.1 80.3 (+1.2) 85.2 86.0 (+0.8) 60.7 61.5 (+0.8) 39.7 39.9 (+0.2) 70.2 70.5 (+0.3) 50.0 51.2 (+1.2) 83.3 84.5 (+1.2) 66.7 70.2 (+3.5) 72.8 73.0 (+0.2) 74.8) 74.7 (0.1) 78.1 78.3 (+0.2) Effectiveness of the proposed hallucination mitigation method. We integrate our method into three LMMs with different underlying LLM architectures. Mini-Monkey and Qwen2.5-VL achieve 4.1% and 5.5% improvements in F1 score respectively, indicating that our method effectively helps models remain faithfully grounded on visual cues for scene text spotting and understanding. In contrast, LLaVA-Next shows only marginal improvement of 0.6%, which we attribute to its limited OCR-related capabilities. These results suggest that our method can bring greater benefits when applied to models with stronger scene text perception abilities. Generalization to other benchmarks. Beyond TextHalu-Bench, our method demonstrates promising results on range of public vision-language benchmarks centered on scene text understanding and spotting. All baseline models show consistent improvements when integrated with our framework. Notably, Mini-Monkey achieves an accuracy gain of approximately 4% on ST-VQA, while Qwen2.5-VL improves by around 3% on SEED-Bench. These results suggest that our hallucination mitigation approach serves as generalizable solution, effectively enhancing visual grounding without compromising the original recognition capabilities on semantically valid samples. Table 2: Comparison of different hallucination mitigation methods. Adv. means adversarial training method, and CoT means Chain-of-Thought testing strategy. Methods TextHalu-Bench STVQATest TextVQAVal GOTScene OCRVQACORE SEEDBenchText AI2D Baseline Adv. CoT Ours 46.5 47.5 (+1.0) 46.8 (+0.3) 50.6 (+4.1) 66.7 66.8 (+0.1) 68.2 (+1.5) 70.6 (+3.9) 74.1 73.7 (0.4) 75.2 (+1.1) 75.0 (+0.9) 88.8 89.1 (+0.3) 89.2 (+0.4) 89.2 (+0.4) 39.7 39.9 (+0.2) 39.7 (+0.0) 39.9 (+0.2) 83.3 83.3 (+0.0) 83.3 (+0.0) 84.5 (+1.2) 74.8 74.5 (0.3) 74.9 (+0.1) 74.7 (0.1) 5.3 Ablation Experiment We conduct extensive ablation studies to evaluate the robustness and generalization capability of our proposed semantic hallucination mitigation method. Mini-Monkey is used as the primary baseline, and results on additional models are provided in Appendix F. Comparison with other hallucination mitigation methods. As most existing hallucination mitigation techniques, such as contrastive decoding and self-correcting decoding, are not directly applicable to our setting, we design two tailored baselines for comparison. (1) Training-based adversarial training: Following [62], we construct leading questionanswer pairs to augment the training set with adversarial examples, and retrain the LMM using this data. (2) Training-free Chain-of-Thought (CoT) prompting: We apply CoT prompts to guide the model to first attend to text regions before generating answers. Further implementation details are provided in the Appendix F. As shown in Tab. 2, adversarial training yields only marginal improvements on TextHalu-Bench. While the CoT strategy enhances attention to text regions and improves performance on general scene text tasks, it fails to fundamentally address semantic hallucination. Effectiveness of ZoomText. Our proposed ZoomText module estimates potential scene text regions without relying on external text detectors. We validate its effectiveness in two ways. First, we 8 Table 3: Ablations about the effectiveness of ZoomText. Methods TextHalu-Bench STVQATest TextVQAVal GOTScene OCRVQACORE SEEDBenchText AI2D Baseline with text detector w/o Glimpse w/o Refocus Ours 46.5 50.4 (+3.9) 50.2 (+3.7) 49.8 (+3.3) 50.6 (+4.1) 66.7 70.8 (+4.1) 70.2 (+3.5) 69.5 (+2.8) 70.6 (+3.9) 74.1 75.2 (+1.1) 75.0 (+0.9) 74.9 (+0.8) 75.0 (+0.9) 88.8 89.0 (+0.2) 88.7 (-0.1) 88.7 (-0.1) 89.2 (+0.4) 39.7 39.9 (+0.2) 39.8 (+0.1) 39.7 (+0.0) 39.9 (+0.2) 83.3 83.3 (+0.0) 84.5 (+1.2) 83.3 (+0.0) 84.5 (+1.2) 74.8 74.7 (-0.1) 74.8 (+0.0) 74.8 (+0.0) 74.7 (-0.1) Figure 5: Ablation on the Grounded Layer Correction. (Left) Different layer selection method. (Right) Different correction strategy. Base: Baseline; Repla.: Replacement; S-Repla.: Selective Replacement; Fuse: Fusion. compare ZoomText with baseline that incorporates accurate region proposals obtained from an offthe-shelf pretrained text detector [90]. Second, we ablate ZoomTexts two key components, Glimpse and ReFocus, to evaluate their individual contributions. As shown in Tab. 3, ZoomText achieves performance comparable to models equipped with external detectors, demonstrating its standalone effectiveness. Moreover, we observe that both Glimpse and ReFocus contribute significantly to performance, highlighting the importance of coarse-to-fine region localization. Ablation on Grounded Layer Correction (GLC). To demonstrate the effectiveness of GLC, we first ablate the impact of our layer selection strategy. Specifically, we randomly select layer from the early, middle, and late stages of the LLM and apply the same Fusion strategy. As shown in Fig. 5, intermediate layers can indeed help mitigate hallucination. However, they may also overwrite valid semantic knowledge, as reflected by performance drops on general VQA benchmarks such as ST-VQA. In contrast, our method adaptively selects the layer with the strongest scene text grounding, leading to reduced hallucination on non-semantic samples while preserving the semantic integrity of meaningful ones. Furthermore, we evaluate the three correction strategies introduced in Sec. 3.3: Replacement, Selective Replacement, and Fusion. For fair comparison, all methods operate on the same grounded layer identified by our selection strategy. Naive Replacement performs poorly across all benchmarks, likely due to significant domain gap between training-time representations and directly injected hidden states. In contrast, both Selective Replacement and Fusion effectively reduce hallucinations. However, similar to the trend observed in layer selection, Selective Replacement substantially degrades performance on general scene text understanding tasks. We attribute this to its aggressive overwriting of final-layer hidden states, which may disrupt the learned alignment between visual text and multimodal context. In light of these results, we adopt Fusion, weight-controlled integration, as our default strategy. Details on fusion weight selection are provided in Appendix F."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we identify the problem of semantic hallucination in Large Multimodal Models, where models often produce semantically plausible but visually incorrect answers when spotting and understanding scene text. We analyze its underlying causes and establish strong correlation between accurate intra-layer attention allocation and the reduction of semantic hallucination. Building on this insight, we propose training-free hallucination mitigation framework comprising two key components. First, ZoomText adopts coarse-to-fine strategy to estimate scene text regions without relying on external detectors. Second, Grounded Layer Correction leverages the hidden states from the most visually grounded layer to guide the decoding process. Furthermore, we introduce TextHalu-Bench, benchmark designed to robustly evaluate scene text spotting and understanding in the presence of non-semantic text. Extensive experiments demonstrate the effectiveness and generalizability of our approach across multiple LMMs and benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: Real-time scene text spotting with adaptive bezier-curve network. In CVPR, pages 98099818, 2020. [2] Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao Zhu, Nicholas Yuan, Kai Ding, and Lianwen Jin. Swintextspotter: Scene text spotting via better synergy between text detection and text recognition. In CVPR, pages 45934603, 2022. [3] Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, and Xiang Bai. Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes. In ECCV, pages 6783, 2018. [4] Yan Shu, Wei Wang, Yu Zhou, Shaohui Liu, Aoting Zhang, Dongbao Yang, and Weipinng Wang. Perceiving ambiguity and semantics without recognition: an efficient and effective ambiguous scene text detector. In ACM MM, pages 18511862, 2023. [5] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marçal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In ICCV, pages 42914301, 2019. [6] Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and Manmatha. Latr: Layout-aware transformer for scene-text vqa. In CVPR, pages 1654816558, 2022. [7] Gangyan Zeng, Yuan Zhang, Yu Zhou, and Xiaomeng Yang. Beyond ocr+ vqa: Involving ocr into the flow for robust and accurate textvqa. In ACM MM, pages 376385, 2021. [8] Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, and Ying Xiao. Towards unconstrained end-to-end text spotting. In ICCV, pages 47044714, 2019. [9] Wenhai Wang, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding Liang, ZhiBo Yang, Tong Lu, Chunhua Shen, and Ping Luo. Ae textspotter: Learning visual and linguistic representation for ambiguous text spotting. In ECCV, pages 457473. Springer, 2020. [10] Chuhui Xue, Jiaxing Huang, Wenqing Zhang, Shijian Lu, Changhu Wang, and Song Bai. Contextual text block detection towards scene text understanding. In ECCV, pages 374391. Springer, 2022. [11] Min Liang, Jia-Wei Ma, Xiaobin Zhu, Jingyan Qin, and Xu-Cheng Yin. Layoutformer: Hierarchical text detection towards scene text understanding. In CVPR, pages 1566515674, 2024. [12] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR, pages 2418524198, 2024. [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023. [15] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35:2371623736, 2022. [16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In ICML, pages 1973019742. PMLR, 2023. [17] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, et al. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. arXiv preprint arXiv:2310.05126, 2023. 10 [18] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In CVPR, pages 2676326773, 2024. [19] Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang, and Xiang Bai. Textmonkey: An ocr-free large multimodal model for understanding document. arXiv preprint arXiv:2403.04473, 2024. [20] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, et al. mplug-docowl: Modularized multimodal large language model for document understanding. arXiv preprint arXiv:2307.02499, 2023. [21] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. Cogagent: visual language model for gui agents. In CVPR, pages 1428114290, 2024. [22] Huawen Shen, Chang Liu, Gengluo Li, Xinlong Wang, Yu Zhou, Can Ma, and Xiangyang arXiv preprint Falcon-ui: Understanding gui before following user instructions. Ji. arXiv:2412.09362, 2024. [23] Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General ocr theory: Towards ocr-2.0 via unified end-to-end model. 2024. [24] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In ICDAR, pages 11561160. IEEE, 2015. [25] Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024. [26] Amit Ben-Artzy and Roy Schwartz. Attend first, consolidate later: On the importance of attention in different llm layers. arXiv preprint arXiv:2409.03621, 2024. [27] Jingcheng Niu, Wenjie Lu, and Gerald Penn. Does bert rediscover classical nlp pipeline? In Proceedings of the 29th International Conference on Computational Linguistics, pages 31433153, 2022. [28] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. [29] Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. Mini-monkey: Alleviating the semantic sawtooth effect for lightweight mllms via complementary image pyramid. arXiv preprint arXiv:2408.02034, 2024. [30] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In CVPR, pages 83178326, 2019. [31] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [32] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [33] Yizhang Jin, Jian Li, Yexin Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, et al. Efficient multimodal large language models: survey. arXiv preprint arXiv:2405.10739, 2024. 11 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023. [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. [36] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [37] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024. [38] Muyang He, Yexin Liu, Boya Wu, Jianhao Yuan, Yueze Wang, Tiejun Huang, and Bo Zhao. Efficient multimodal learning from data-centric perspective. arXiv preprint arXiv:2402.11530, 2024. [39] Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong Zhang, Chun-Mei Feng, et al. Mmrc: large-scale benchmark for understanding multimodal large language model in real-world conversation. arXiv preprint arXiv:2502.11903, 2025. [40] Yan Shu, Zheng Liu, Peitian Zhang, Minghao Qin, Junjie Zhou, Zhengyang Liang, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. [41] Xiangrui Liu, Yan Shu, Zheng Liu, Ao Li, Yang Tian, and Bo Zhao. Video-xl-pro: Reconstructive token compression for extremely long video understanding. arXiv preprint arXiv:2503.18478, 2025. [42] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. Mlvu: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. [43] Huaying Yuan, Zheng Liu, Minhao Qin, Hongjin Qian, Shu, Zhicheng Dou, and Ji-Rong Wen. Memory-enhanced retrieval augmentation for long video understanding. arXiv preprint arXiv:2503.09149, 2025. [44] Qi Li, Runpeng Yu, and Xinchao Wang. Vid-sme: Membership inference attacks against large video understanding models, 2025. [45] Song Chen, Xinyu Guo, Yadong Li, Tao Zhang, Mingan Lin, Dongdong Kuang, Youwei Zhang, Lingfeng Ming, Fengyu Zhang, Yuran Wang, et al. Ocean-ocr: Towards general ocr application via vision-language model. arXiv preprint arXiv:2501.15558, 2025. [46] Ya-Qi Yu, Minghui Liao, Jiwen Zhang, and Jihao Wu. Texthawk2: large visionlanguage model excels in bilingual ocr and grounding with 16x fewer tokens. arXiv preprint arXiv:2410.05261, 2024. [47] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao, Zheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and Xiangyu Zhang. Vary: Scaling up the vision vocabulary for large visionlanguage model. In ECCV, pages 408424. Springer, 2024. [48] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. [49] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [50] Suzanne Petryk, David Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph Gonzalez, and Trevor Darrell. Aloha: new measure for hallucination in captioning models. arXiv preprint arXiv:2404.02904, 2024. 12 [51] Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan. How easy is it to fool your multimodal llms? an empirical analysis on deceptive prompts. arXiv preprint arXiv:2402.13220, 2024. [52] Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, and Xirong Li. Phd: prompted visual hallucination evaluation dataset. arXiv preprint arXiv:2403.11116, 2024. [53] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models. arXiv preprint arXiv:2310.14566, 2023. [54] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In ICLR, 2023. [55] Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. Hal-eval: universal and fine-grained hallucination evaluation framework for large vision language models. arXiv preprint arXiv:2402.15721, 2024. [56] Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, and Yueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. arXiv preprint arXiv:2311.13614, 2023. [57] Zhiyang Chen, Yousong Zhu, Yufei Zhan, Zhaowen Li, Chaoyang Zhao, Jinqiao Wang, and Ming Tang. Mitigating hallucination in visual language models with visual supervision. arXiv preprint arXiv:2311.16479, 2023. [58] Haoyi Qiu, Wenbo Hu, Zi-Yi Dou, and Nanyun Peng. Valor-eval: Holistic coverage and faithfulness evaluation of large vision-language models. arXiv preprint arXiv:2404.13874, 2024. [59] Tianyang Han, Qing Lian, Rui Pan, Renjie Pi, Jipeng Zhang, Shizhe Diao, Yong Lin, and Tong Zhang. The instinctive bias: Spurious images lead to hallucination in mllms. arXiv preprint arXiv:2402.03757, 2024. [60] Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, and Rifat Shahriyar. Illusionvqa: challenging optical illusion dataset for vision language models. arXiv preprint arXiv:2403.15952, 2024. [61] Yuan Zhang, Fei Xiao, Tao Huang, Chun-Kai Fan, Hongyuan Dong, Jiawen Li, Jiacong Wang, Kuan Cheng, Shanghang Zhang, and Haoyuan Guo. Unveiling the tapestry of consistency in large vision-language models. arXiv preprint arXiv:2405.14156, 2024. [62] Yexin Liu, Zhengyang Liang, Yueze Wang, Muyang He, Jian Li, and Bo Zhao. Seeing clearly, answering incorrectly: multimodal robustness benchmark for evaluating mllms on leading questions. arXiv preprint arXiv:2406.10638, 2024. [63] Baiqi Li, Zhiqiu Lin, Wenxuan Peng, Jean de Dieu Nyandwi, Daniel Jiang, Zixian Ma, Simran Khanuja, Ranjay Krishna, Graham Neubig, and Deva Ramanan. Naturalbench: Evaluating vision-language models on natural adversarial samples. arXiv preprint arXiv:2410.14669, 2024. [64] Ce Zhang, Zifu Wan, Zhehan Kan, Martin Ma, Simon Stepputtis, Deva Ramanan, Russ Salakhutdinov, Louis-Philippe Morency, Katia Sycara, and Yaqi Xie. Self-correcting decoding with generative feedback for mitigating hallucinations in large vision-language models. arXiv preprint arXiv:2502.06130, 2025. [65] Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. See what you are told: Visual attention sink in large multimodal models. arXiv preprint arXiv:2503.03321, 2025. [66] Chenxi Wang, Xiang Chen, Ningyu Zhang, Bozhong Tian, Haoming Xu, Shumin Deng, and Huajun Chen. Mllm can see? dynamic correction decoding for hallucination mitigation. arXiv preprint arXiv:2410.11779, 2024. 13 [67] Shunqi Mao, Chaoyi Zhang, and Weidong Cai. Through the magnifying glass: Adaptive perception magnification for hallucination-free vlm decoding. arXiv preprint arXiv:2503.10183, 2025. [68] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, pages 1387213882, 2024. [69] Joost CF De Winter, Samuel Gosling, and Jeff Potter. Comparing the pearson and spearman correlation coefficients across distributions and sample sizes: tutorial using simulations and empirical data. Psychological methods, 21(3):273, 2016. [70] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In ICDAR, pages 14841493. IEEE, 2013. [71] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In ICDAR, pages 15711576. IEEE, 2019. [72] Zhenbo Xu, Wei Yang, Ajin Meng, Nanxue Lu, and Huan Huang. Towards end-to-end license plate detection and recognition: large dataset and baseline. In ECCV, pages 255271, 2018. [73] Jiaming Liu, Chengquan Zhang, Yipeng Sun, Junyu Han, and Errui Ding. Detecting text in the wild with deep character embedding network. ArXiv, abs/1901.00363, 2018. [74] Sangeeth Reddy, Minesh Mathew, Lluís Gómez, Marçal Rusiñol, Dimosthenis Karatzas, and C. V. Jawahar. Roadtext-1k: Text detection & recognition dataset for driving videos. 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 1107411080, 2020. [75] Tongkun Guan, Chaochen Gu, Changsheng Lu, Jingzheng Tu, Qi Feng, Kaijie Wu, and Xinping Guan. Industrial scene text detection with refined feature-attentive network. TCSVT, 2022. [76] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [77] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In ECCV, pages 235251. Springer, 2016. [78] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In ICDAR, pages 947952. IEEE, 2019. [79] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seedbench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [80] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [81] OpenAI. Gpt-4o. https://openai.com/index/hello-gpt-4o/, May 2024. [82] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In CVPR, pages 1304013051, 2024. [83] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [84] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [85] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. In CVPR, pages 2676326773, 2024. [86] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [87] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. [88] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [89] Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, and Xiang Bai. Mini-monkey: Alleviating the semantic sawtooth effect for lightweight mllms via complementary image pyramid. arXiv preprint arXiv:2408.02034, 2024. [90] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In ICCV, pages 29612969, 2017. [91] Yan Shu, Weichao Zeng, Fangmin Zhao, Zeyu Chen, Zhenhang Li, Xiaomeng Yang, Yu Zhou, Paolo Rota, Xiang Bai, Lianwen Jin, et al. Visual text processing: comprehensive review and unified evaluation. arXiv preprint arXiv:2504.21682, 2025. [92] Zhenhang Li, Yan Shu, Weichao Zeng, Dongbao Yang, and Yu Zhou. First creating backgrounds then rendering texts: new paradigm for visual text blending. arXiv preprint arXiv:2410.10168, 2024. [93] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In ACM MM, pages 1119811201, 2024. [94] Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. Dataset and neural recurrent sequence labeling model for open-domain factoid question answering, 2016. [95] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. [96] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 292305, Singapore, December 2023. Association for Computational Linguistics. [97] Weichao Zeng, Yan Shu, Zhenhang Li, Dongbao Yang, and Yu Zhou. Textctrl: Diffusion-based scene text editing with prior guidance control. Advances in Neural Information Processing Systems, 37:138569138594, 2024. [98] Liu Yuliang, Jin Lianwen, Zhang Shuaitao, and Zhang Sheng. Detecting curve text in the wild: New dataset and new solution, 2017. [99] Chee Kheng Chng, Chee Seng Chan, and Chenglin Liu. Total-text: Towards orientation robustness in scene text detection. International Journal on Document Analysis and Recognition (IJDAR), 23:3152, 2020."
        },
        {
            "title": "A Overview of Appendix",
            "content": "B: Limitations and Broader Impact. C: Details of TextHalu-Bench. D: Experimental Settings. E: Detailed Experimental Results. F: More Ablation Studies. G: More Visualization Results. Checklist"
        },
        {
            "title": "B Limitations and Broader Impact",
            "content": "Limitations. While our method shows promising performance on scene text spotting and understanding, it still has two key limitations. First, it requires token selection and attention map computation during the prefilling stage, which introduces additional inference time and computational overhead. Second, the effectiveness of our method heavily relies on the underlying OCR perception ability of the base model. As result, it performs suboptimally when applied to LMMs with weak scene text understanding capabilities. Broader Impact. We propose training-free semantic hallucination mitigation framework, which may bring broader impacts across multiple areas. For the OCR community, our method facilitates the adaptation of LMMs to text-intensive tasks, potentially benefiting wide range of downstream applications such as document understanding, autonomous driving, assistive technologies and lowlevel text processing techniques [91, 92] like editing and generation. Additionally, our findings and mitigation strategy provide valuable insights for the development of more reliable and hallucinationresilient multimodal large models. Details of TextHalu-Bench Dataset Collection Process.To promote coverage and diversity, we carefully curated samples across five representative scenario types: Business, Industry, Transportation, Public Facilities, and Daily Life. These categories were selected based on their prevalence in real-world OCR applications and their variance in textual layout, typeface complexity, and visual background noise. In addition, during sample construction, we emphasized the inclusion of challenging edge cases, such as low-contrast text, occlusions, unconventional fonts, or partial visibility, to stress-test the visual grounding ability of MLLMs and better surface hallucination tendencies. Scene Text Spotting Task Definition.Given an image, the model is required to extract all visible textual content from the scene. The task is treated as word-level prediction problem and its output is compared to the ground-truth words using case-insensitive exact match.Spotting examples include questions such as What is the texts in the image?Answer the question in only words you recognize. Scene Text Understanding Task Definition.To measure the higher-level comprehension ability, we adopt multiple-choice format, with at least one correct answer and at most three well-crafted distractors. These distractors are designed with the following strategies: Glyph-based distractors: visually similar characters (e.g., vs. 0, vs. 1) Semantic distractors: misleading but contextually related words (e.g., apple vs. apole) Context-based distractors: co-occurring or spatially nearby words within the same image Understanding task examples include questions such as What is the texts on the boat? A.aa B.bb C.cc D.dd Metric. To quantitatively measure hallucination behavior, we report the average F1 score across both subtasks as our evaluation metric which captures both the models accuracy in extracting visible text and its ability to semantically interpret visual information, distinguishing genuine visual understanding from language-prior hallucination. Table 4: Spearman Correlation between Hallucination tendency score and scene text region attention score, with performance on STVQA and TextVQA. Model Benchmarks OCRBench STVQA TextVQA Mini-Monkey Qwen2.5-VL -0.72 -0.68 -0.78 -0.80 -0.74 -0."
        },
        {
            "title": "D Experimental Settings",
            "content": "Our method is training-free and thus does not require any additional fine-tuning or parameter updates. All models are evaluated under their official default configurations without modification. For evaluation, we test on TextHalu-Bench, ST-VQA, and GOT using our own implementation to ensure consistent handling of OCR and visual inputs. For other benchmarks, we utilize the VLMEvalKit [93] toolkit, and follow the original leaderboard results published by each model for fair comparison. All experiments are conducted on server equipped with 4 NVIDIA A800 GPUs."
        },
        {
            "title": "E More Experimental Results",
            "content": "Figure 6: An example of the correlation (Spearman and Pearson coefficient) between hallucination tendency score and scene text region attention score. Correlation between Hallucination and Attention Distribution in LLMs. Leveraging our automatic hallucinated token identification mechanism, we compute hallucination tendency scores and corresponding scene text region attention scores across all transformer layers for each hallucinated sample. As shown in Tab. 4, Spearman correlation analysis reveals strong negative correlation, indicating that stronger attention to scene text regions is associated with reduced semantic hallucination. layer-wise visualization for representative sample is shown in Fig. 6, where each point corresponds to transformer layer with its hallucination score (x-axis) and scene text attention score (y-axis). Generalization to other domains. To assess the generalization ability of our method beyond the scene text domain, we apply it to four diverse vision-language benchmarks: SEED-Bench consists of 19K multiple-choice questions with accurate human annotations, covering 12 evaluation dimensions across both image and video modalities; RealWorldQA [94] evaluates real-world spatial understanding in physical environments, contributed by XAI; MathVista [95] is challenging benchmark requiring visual mathematical reasoning over charts, diagrams, and textual math problems; POPE [96] focuses on object hallucination, comprising three evaluation tracks: random, popular, and adversarial hallucination. 17 Table 5: Generalization performance of our method on other domains. Method SEEDBench RealWorldQA MathVista POPE Qwen2.5VL Qwen2.5VL + Ours 74 74.1 65.5 65. 61.2 61.4 85.9 86.7 As shown in Tab. 5, our method consistently improves performance across all four benchmarksfor instance, achieving +0.3 accuracy gain on RealWorldQA and +0.8 on POPE. These results suggest that our approach not only enhances scene text understanding but also generalizes well to broader multimodal reasoning tasks, without degrading the pretrained models core alignment or reasoning abilities. Efficiency Analysis. As training-free method, we report the inference time overhead introduced by our approach. As shown in Tab. 6, our method inevitably incurs additional computation in the prefilling stage, where attention maps from all layers are extracted and stored before decoding. However, we argue that this overhead is acceptable, as our approach remains more efficient than other test-time scaling methods such as Chain-of-Thought prompting (introduced in Sec. F). Furthermore, our method does not require any additional modules or external models to assist decoding, maintaining streamlined and lightweight inference process. Table 6: Efficiency analysis of our methods, which use the same prompt to calculate the first token generation time. Method prefilling decoding total Qwen2.5VL Qwen2.5VL + Ours Qwen2.5VL + CoT 0.53 1.08 0.56 1.14 1.15 3. 1.67 2.23 4."
        },
        {
            "title": "F More Ablation Studies",
            "content": "Additional Implementation Details: Comparison with Other Hallucination Mitigation Methods. (1) Adversarial Training. To construct adversarial training data, we employ the image-text editing tool TextCtrl [97] to synthetically perturb the textual content of images from existing scene text datasets, including CTW1500 [98], ICDAR 2015 [24], and TotalText [99]. Following targeted editing strategy, we generate up to three adversarial variants per image, depending on the number of text instances it contains. Editing operations include character-level insertions, deletions, substitutions, and replacements with visually similar but semantically misleading characters. This process yields approximately 10,000 adversarial image-text pairs designed to introduce non-semantic visual perturbations that challenge both grounding and recognition. We fine-tune both Mini-Monkey and Qwen2.5-VL on the augmented dataset, using the original fine-tuning hyperparameters and training for one epoch. The fine-tuned models are then directly evaluated on downstream benchmarks to assess their robustness against semantic hallucination. Figure 7: Visualization of adversarial training data. 18 Table 7: Ablation about the effectiveness of ZoomText on Qwen2.5-VL-3B. Model Methods TextHalu-Bench STVQATest TextVQAVal AI2D OCRVQACORE SEEDBenchText GOTScene Baseline with text detector w/o Glimpse w/o Refocus Ours 48.3 53.4 52.9 53.5 53.8 67.3 67.9 67.3 67.3 67.6 79.1 80.3 78.8 78.9 80.3 78.1 78.2 78.2 78.2 78.2 70.2 70.4 69.8 69.8 70.5 66.7 67.9 70.2 70.2 70. 85.2 85.8 85.2 85.1 86.0 Table 8: Comparison of different hallucination mitigation methods on Qwen2.5-VL-3B. Methods TextHalu-Bench STVQATest TextVQAVal GOTScene OCRVQACORE SEEDBenchText AI2D Baseline Adv. CoT Ours 48.3 49.1 48.5 53. 67.3 67.2 67.7 67.6 79.1 78.6 79.4 80.3 78.1 78.1 78.2 78.2 70.2 70.4 70.4 70.5 66.7 67.9 70.2 70.2 85.2 85.2 85.5 86. (2) Chain-of-Thought (CoT) Prompting. Our CoT-based hallucination mitigation strategy follows two-stage inference process. In the first stage, the model generates an initial answer using standard inference procedures. In the second stage, we feed the model with both the original image and its previously generated answer, along with CoT-style prompt that explicitly instructs the model to reflect on and verify its initial prediction by more carefully grounding it in the visual text regions. We design the following Chain-of-Thought (CoT) prompt to guide the second-stage reasoning: Your previous answer was: {{answer}}. Please carefully examine the text in the image again and verify whether the answer is fully supported by the visual evidence. If necessary, correct the answer based on the actual content in the image. Therefore, as further demonstrated in Tab. 8, Qwen2.5-VL also exhibits consistent improvements, providing additional evidence of the effectiveness of our approach. Additional Effectiveness Results of ZoomText. As shown in Tab. 7, the experimental results further validate the effectiveness of ZoomText. In particular, incorporating both the Glimpse and ReFocus modules leads to notable performance gains, demonstrating the benefit of our progressive refinement strategy for region localization. By incrementally narrowing the models attention to relevant visual areas, ZoomText effectively reduces ambiguity in visual-language alignment. These findings highlight the importance of hierarchical attention in improving scene text spotting and understanding. Analysis of Weighting Strategies for Cross-Layer Hidden State Fusion. To mitigate the limitations of relying solely on the final output layer for text recognition and understanding, we adopt weighted fusion strategy that combines hidden states from different transformer layers, modulated by fusion coefficient λ. We perform grid search over λ {0.1, 0.2, 0.4, 0.6, 0.8} to investigate its effect on performance. As shown in Tab. 9, the optimal performance is achieved at λ = 0.1, resulting in an average accuracy improvement of 1.67% over the baseline. Moreover, the results reveal nuanced trade-off: when the selected hidden layer carries richer visual information, higher values of λ (e.g., λ = 0.6 or 0.8) tend to improve performance on text spotting tasks. However, these higher weights lead to diminished performance in text understanding tasks, likely due to an overemphasis on visual features at the expense of semantic comprehension. This finding underscores the importance of carefully balancing visual and linguistic cues in cross-layer fusion for different types of downstream tasks. Analysis of Hidden Layer Contributions to Fusion. To better understand which layers are most beneficial for visual grounding, we conduct an ablation study examining the effects of fusing hidden states from different model depths. Specifically, we compare five fusion strategies: (a) early layers (layers 010), (b) middle layers (layers 1020), (c) late layers (layers 2035), (d) randomly selected layers, and (e) our proposed layer selection method. As shown in Tab. 10, fusing hidden states from our selected layers yields the most substantial performance gain. These results indicate that not all layers contribute equally to grounding, and that carefully chosen subset can more effectively capture the visual information necessary for hallucination mitigation. 19 Table 9: Analysis of weights for cross-layer hidden state fusion on Qwen2.5VL-3B. Weights TextHalu-Bench STVQATest TextVQAVal AI2D OCRVQACORE SEEDBenchText GOTScene 0 0.1 0.2 0.4 0.6 0.8 48.3 53.8 53.4 50.7 45 27.2 67.3 67.6 66.7 63.5 58.2 42.5 79.1 80.3 77.7 73.2 65.5 45.7 78.1 78.2 78.3 80 80.3 80. 70.2 70.5 67.9 62 56.9 43 66.7 70.2 69 72.6 76.2 69 85.2 86.0 86.2 87.2 85.4 35.8 Table 10: Analysis of layers for hidden state fusion on Qwen2.5VL-3B. Layer Index TextHalu-Bench STVQATest TextVQAVal AI2D OCRVQACORE SEEDBenchText GOTScene 0-10 10-20 20-35 random ours 52.5 52.9 53.1 53.1 53. 67.3 67.4 67 67.1 67.6 78.6 78.9 79 78.9 80.3 78.2 78.2 78.1 78.2 78. 69.7 70 70.4 69.9 70.5 69 70.2 69 70.2 70.2 86.1 85.4 85 85.4 86."
        },
        {
            "title": "G Visualization Results",
            "content": "As shown in Fig. 7 ,we present visualization results that illustrate the effectiveness of our method in solving challenges from semantic hallucination. 20 Figure 8: Visualization results."
        }
    ],
    "affiliations": [
        "HKUST",
        "IIE, CAS",
        "NJUST",
        "NKU",
        "UCAS",
        "UCF",
        "UIR",
        "UNITN"
    ]
}